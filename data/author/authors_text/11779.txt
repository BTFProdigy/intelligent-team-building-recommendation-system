Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 41?44,
Suntec, Singapore, 3 August 2009.
c
?2009 ACL and AFNLP
Combining POMDPs trained with User Simulations and
Rule-based Dialogue Management in a Spoken Dialogue System
Sebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi Roberti
Department of Information Engineering and Computer Science
University of Trento
38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.it
Abstract
Over several years, we have developed an
approach to spoken dialogue systems that
includes rule-based and trainable dialogue
managers, spoken language understanding
and generation modules, and a compre-
hensive dialogue system architecture. We
present a Reinforcement Learning-based
dialogue system that goes beyond standard
rule-based models and computes on-line
decisions of the best dialogue moves. The
key concept of this work is that we bridge
the gap between manually written dia-
log models (e.g. rule-based) and adaptive
computational models such as Partially
Observable Markov Decision Processes
(POMDP) based dialogue managers.
1 Reinforcement Learning-based
Dialogue Management
In recent years, Machine Learning techniques,
in particular Reinforcement Learning (RL), have
been applied to the task of dialogue management
(DM) (Levin et al, 2000; Williams and Young,
2006). A major motivation is to improve robust-
ness in the face of uncertainty, for example due
to speech recognition errors. A further motivation
is to improve adaptivity w.r.t. different user be-
haviour and application/recognition environments.
The Reinforcement Learning framework is attrac-
tive because it offers a statistical model represent-
ing the dynamics of the interaction between sys-
tem and user. This is in contrast to the super-
vised learning approach of learning system be-
haviour based on a fixed corpus (Higashinaka et
al., 2003). To explore the range of dialogue man-
agement strategies, a simulation environment is
required that includes a simulated user (Schatz-
mann et al, 2006) if one wants to avoid the pro-
hibitive cost of using human subjects.
We demonstrate the various parameters that in-
fluence the learnt dialogue management policy by
using pre-trained policies (section 4). The appli-
cation domain is a tourist information system for
accommodation and events in the local area. The
domain of the trained DMs is identical to that of a
rule-based DM that was used by human users (sec-
tion 2), allowing us to compare the two directly.
The state of the POMDP keeps track of the SLU
hypotheses in the form of domain concepts (10 in
the application domain, e.g. main activity, star rat-
ing of hotels, dates etc.) and their values. These
values may be abstracted into ?known/unknown,?
for example, increasing the likelihood that the sys-
tem re-visits a dialogue state which it can exploit.
Representing the verification status of the con-
cepts in the state, influences ? in combination with
the user model (section 1.2) and N best hypotheses
? if the system learns to use clarification questions.
1.1 The exploration/exploitation trade-off in
reinforcement learning
The RL-DM maintains a policy, an internal data
structure that keeps track of the values (accumu-
lated rewards) of past state-action pairs. The goal
of the learner is to optimize the long-term reward
by maximizing the ?Q-Value?Q
pi
(s
t
, a) of a policy
pi for taking action a at time t. The expected cu-
mulative value V of a state s is defined recursively
as V
pi
(s
t
) =
?
a
pi(s
t
, a)
?
s
t+1
P
a
s
t
,s
t+1
[R
a
s
t
,s
t+1
+ ?V
pi
(s
t+1
)].
Since an analytic solution to finding an optimal
value function is not possible for realistic dialogue
scenarios, V (s) is estimated by dialogue simula-
tions.
To optimize Q and populate the policy with ex-
pected values, the learner needs to explore un-
tried actions (system moves) to gain more expe-
riences, and combine this with exploitation of the
41
??
?
?
?
?
?
?
??
??
??
??????
?
??
??
??
?
?
?
??
?
?
?
??
?
?
???
???
??
?
??
?
?
?
?
??
??
?
???????
?
????
?
??
?
?
?
?
?????
?
?
?
?
??
???
?
?????
?
???
??
?
?
?
?
?
?
??
??
??
??
?
?
??
?
?
?
?
??
??
????
?
??
????
??
????
??
?
??
???
???
??
?
?
???
??
????
????
?
??
?
?
?
?
?
??
?
????
???
?
??
?
???
???
???
??
?
????
?
??
?
??
?
?
?
?
?
?
????
?
??
??
??
?
???
??????
??
??
?
??????
??
?
?
??
?????????
?
?
?????
???
?
?
?
??
???
??
?
??????
??
?
?
?
????
?
???
?
?
???????
?
??
?
??
?
?
??
?
??
???
?
?
?????
??????
?
???
?
?
?
?????
??
????
????
?
?
??
?
????
?
??
?
?
?????
??
?
??????????
??
?
?
???
??
?
??????????
?
?
???
??
????
?????????
?
?
???
????
?
?????
??
??
??
???
?????
?
???
???
??
????
?
??
??
?????
????
?????
?
??????
?
????
?????
?
??
???
???
??????????
???
?
??
?
??????
??
????
?????????
?
?????????
????
????
????????
?????
?
???????
?
???????
???????
?????
?????
??????
???????
???????
?
???
?
??????????
?
???????
???????????
???
???????????????????
????????
?
??????
??????
?
???????????
?????????
???????
?
?????????
????????
?
???????
?????
????????????
????
???????????
??????
????????
?????
????????????
?
?????
?
????????
?
?????????
?
????
?
????
?
?????
?
????????
???????
?????????????
?????????
?
?????
?????
?
????????
??????????
?
?????
????????
????
??????
??????
??????????
????????
????????
?
????????
????
?????
?
????
?
?????
?
????
?
???????????
?????
????????
?
??????
?
??????????
??????
??????
????????
??????
??????
?
????????????????
???????
?
?????
?
????
?
?????
?
?????????
?
?????
?
????
?
????
???????
?
???????????
?
?????????
???????
??????
?
???????
???????????
?
?????
?????
?
???????????
??????
?
?????
????????
?
???????
????????????????????
?
??????????
??????
????????
???????
????????
???????????
??????
?
??????
?????????
?
??????
???????????????????
?????????
?
?????????
?
???????
?
???????
?
????
?
????????
???????
??????????
?
???????
?
???
?
?????
?
???????
??????????
?
????
??????
???????????
?????
??????
??????
??????????
???????
??????
????????
?????
???????????
??????
?????????
?????
?
??????
?
???????
?
?????
?
???????
?????
???????????????
??????????
?
?
0 2000 4000 6000 8000 10000
?8
?6
?4
?2
0
x8
gree
dy0.0
_fixe
d_er
ror_s
essio
ns10
000_
maxs
essio
nleng
th4_r
uns1
0
rewa
rd 
# sessions 
(a) 0% exploration, 100% exploitation: learner does not find
optimal dialogue strategy
????
?
?
?
?
?
?
?
?
?
?
??
?
???
?
??
?
?
?
??
?
?
?
?
??
???
?
?
?
???
??
???
??
?
?
?
??
???
???
?
??
?
?
?
??
?
?
???
?
????????
???
?
?
???
?
??
???
????
?
?
??
?
??
??
?
?
?
??
?
??
??
?
?
?
?
?
??
?
??
??
?
??
?
?
????
??
?
?
??
?
?
??
?
??
?
??
???
??
?
?
??
???
?
?
?
?
?
?
?
??
?
???
?
???
?
?
???
??
??
???
??
???
??
???
??
??
?
?
?
?
?
??
??
???
?
?
?
??
?
???
??
?
??
?
??
?
??????
??
?
?
??
????
?????
??
?
?
???
?
?
?
?
?????
?
???
?
???
??
?
???
??
?
??
??
?
?
??
??
?
?
?
?
??
?
?
?
??
?
???
?
?
?
??
?
?
??
??????
??
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?????
?
?
?
?
??
?
???
?
?
?
?
?
?
??????
?
?
?
??
?????
?
?
?
?
??
?
??
??
?????
???
??
?
??
??
?
?
??
??
????
?
??
?
?
?
?
?
???
?
??
?
?
??
?
??????
????
?
??
?
?
?
?
??
?
?
?
??
?
??
???
????
?
?
???
?
??
?
?
?
?
?
?
?
??
?
?
????
???
?
?
?
???
?
????
?????
???
?
???
?
???
??
?
?
?
??
???
?
?
???
???
?
?
???
?
?
??
?
?
??
?
??
?
??
?
?
?
??
?
???
??
?
?
?
???
?
?
???
?
?
?
??
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
???
??
?
?
???
?
?
?
?
??
?
?
?
?
?
?
?
?
????
?
?
????
????
?
?
?
??
??
???
?
?
?
?
?
?
???
?
?
?
?
?
?
?
?
???
?
?
?
??
??
??
??
??
?????
?
??
?
???
?
?
?
?
??
?
?
?
????
??
?
?
?
????
?
???
????
?
?
??
??
?
?
?
?
??
?
??
????
?
?
??
?
???
??
?
??
???
?
??
?
?
?
?
?
?
??
?
?
????
??
??
??
??
????
?
?
??
?
??
?
?
?
?
?
?
?
??
?
?
???
?
?
?
?
?
?
???
?
?
?
?
?
?
??
?
?
??
?
?
????
?
?
?
??
?
??
?
?
?
??
?
?
???
?
?
?
?
??
?
?
?
?
??
?
??????
?
??
?
?
??
?
?
?
???
??
?
?
?
?
?
?
???
?
?
?
?
?
?
?
??
??
?
?
?
??
???
???
?
?
?
?
??
?
?
?
?
?
??
?
?
?
?
?
?
??
???
?
?
?
?
??
??
?
?
??
?
?
??
??
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
????
?
?
??
?
?
?
?
????
?????
???
?
??
?
?
??
??
?
?
?
?
???
?
??
?
?
?
??
?
????
?
??
?
?
?
?
??
??
?
??
?
???
?
?
?
?
?
??
?
?
??
?
???
?
?
?
?
?
??
?
?
?
?
?
???
?
?
?
?
?
????
?
?
?
?
??
??
?
???
??
?
???
?
?
?
?
???
?
??
??
?
?
?
??
?
??
?
?
?
?
?
?
???
??
?
?????
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
?
??
?
?
?
??
?
?
?
?
?
???
?
?
?
?
?
??
????
??
?
?
??
?
?
?
??
?
?
?
????
??
?
?
?
?
???
?
?
???
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
?????
????
??
?
?
???
?
?
?
?
?
?
?
?
?
??
?
?
??
????
?
????
?
????
?
?
??
?
?
?
??
?
???
?
?
??
?
?
?
?
?
???
??????
?
?
?
?
??
?
?
?
???
?
??
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
??
??
?
?
?
?
?
?
?
??
?
?
????
?
?
?
?
??
???
??
?????
?
?
?
??
?
?
?
?
?
?
?
??
?????
?
?
?
??
?
?
?
?
?
??
??
?
?
??
?
?
??
??
?
?
?
?
?
?
?
??
?
?
?
??
??
?
?
?
?
?
??
?
???
?
??
??
?
?
???
??
?
?
???
?
?
?
???
?
?
???
??
??
?
?
??
?
??
???
??
?
?
?
?
?
?
?
?????
?
?
??
???
??
?
?
?
?
?
???
??
??
????
???
?
?
?
?
??
?
?
?
?
?
??
?
?
???
?
??
??
?
?
?
??
??
?
??
??
?
??
??
???????
??
?
?
?
?
??
??
??
?
?
??
???
??
?
???
?
??
??
??
?
?
??????
??
?
???
????
?
????
??
?
??
??
?
?
??
?
?
???
?
?
??
??
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
?
?
??
??
??
??
?
?
??
???
??
????????
??
??
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
???????
??
???
?
?
?
?
?
??????
?
?
?
?
?
?
??
??
?
?
??
?
?
??
?
?
?
?
?
?
?
??
??
???
?
?
?
?
??
?
?
??
?
??
?
??
?
?
????
?
??
?
????
?
?
???
?
?
?
?
?
?
?
????
?
?
???
??
?
?
??
?
?
?
?
??
?
?
?
????
?
?
????
?
?
?
?
?
???
??
?
?
????
??
??
???
?
?
?
??
?
????
???
?
?
?
?
?
??
??
?
?
????
?
?
?
??
?
?
??
??
?
?????
???
?
?
?
?
?
????
?
??
?
??
??
?
?????
????
?
?????
?
?
?
???
??
?
????
???
?
??
?
??
?
???
?
?
?
?
?
?????
?
?
??
??
??
?
?
???
?
?
??
??
?
?
?
?????
?
????
?
??
?
?
?
?
??
?
???
??
??
??
?
?
?
?
???
?
?
?????????
?
?
??
?
?
?
?
???
?
?
?
??
?
?
?
??
??
???
???
?
??
?
?
?
???
?
?
?
?
??
?
?
?
?
?
??
?
??
??
?
?
?
?
?
?
??
?
?
???
??
??
??
?
?
?
?
???????
??
?
??
?
?
?
??
??
??
?
?
?
?
??
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
??
?
?
??
?
?
?
?
?
?
??
??
??
?
?
?
?
??
?
?
??
??
??
?
??
??
??
?
?
?
??
????
?
?
?
?
???
?
?
?
?
?
?
?
??
?
?
?
?
??
?
?
??
?
??
??
???
?
??
??
????
?
?
?
?
?
??
???
?
?
?
??
?
???
???
?
?
?
?
?
?
?
??
?
?
?
?
?
????
??
?
??
?
?
?
?
??
??
?
??
??
?
?
?
????
??????
??
???
?
???
?
??
?
?
?
?
?
?
??
??
???
?
?
?
?
??
?
?
?
????
?
??
?
?
?
??
??
???
?
?
?
??
???
?
???
?
???
????
??
??
?
??
?
??
?
?
??
?
??
?
??
?
?
?
?
??
???
?????
???
?
?
?
?
?
???
?
?
?
?
?
?
?
?
?
?
???
?
?
??
????
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
??
????
??
??
?
?
?
???
???
?
?
?
?
?
?
?
?
???
?
??
?
?
?
??
?
?
?
??
?
???
??
?
?
?
????
?
??
??
?????
?
???
?
????
?
???
?
?
?
???
?
?
?
??
?
?
?
?
?
???
??
??
?
????
?
?
?
?
?
???
?
?
?
?
?
??
?
??
?
?
?
?
?
??
??
?
?
?
?
??
?
??
???
?
?
?
??
??
??
?
?
??
?
??
?
??
?
?
?
?
??
?
?
???
????
?
?
?????
??
?
?
?
???
?
????
???
?
??
?
???
?
?
?
?
??
?
??
??
?
?
???
?
??
?
??
???
?
??
?
?
?
?
??
?
?
??
?
?
?
?
????
?
?
?
???
?
?
??
??
?
??
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
?
??
?
?
???
?
???
?
?
?
?
?
??
?
???
?
?
?
?
??
?
?
?
?
???
?
?
??
?
???
???
??
?
?
?
?
?
?
?
?
??
?
??
?
????
??
?
?
?
?
?
?
???
?
?
?
??
??
??
?
???
?
???
???
??
?
??
??
??????
?
?????
??
?
??????
?
??
?
?
???
???
??
?
??
?
?
?
?
?
??
?
?
?
?
?
???
???
?
??
?
?
?
?
?????
???
???
?
??
?
???
???
?
??
?
?
?
?
??
????
??
?
??
?
??
?
???
???
???
?????
??
??
?
?
??
?
?
???
??
?
??
?
?
???
?
?
?
?
????
?
?
?
?
???
????
?
?
??
??
??
??
?
?
??
?
?
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
????
?????
??
???
?
??
???
???
??
?
??
?
?
??
?
???????
?
???
?
?
??
?
??
??
??
??
?
???
?
?
?
???
?
?
?
?
?
??
?
?
??
?
?
??
?
?
??
?
?
?
??
??
?
?
?
?
???
?
?
?
??
?
?
????
??
?
?
?
?
?
??
??
??
?
???
??
????
?
???????
?
?
?
?
????
?
?
?
???
?
?
?
?
??
?
??
?
??
?
?
?
???
?
?
?
?
???????
?
?
?
?
?
?
?
??
??
?
?
?
?
???
?
???
?
??
???
?
??
?
?
?
????
?
?
?
?
???
?
?
?
?
?
?
?
??
????
?
????
??
?
?
????
?
?
?
?
?
?
??
?
??
???
?
?
?
?
??
?
?
??
??
??
???
???
????
?
?
?????
??
??
??
?
?
?
?
?
??
?
?????
?
?
?
?
?
?
?
?
???
?
??
?
?
?
?
?
??
?
??
?
?
???
??
??
?????
?
??
?
?
?
?
?
??
?
?
?
??
???
?
??
?
?
?
??
?
??
?
?
?
?
?
??
?
?
?
??
??
?
?
?
?
???
??
?
?
?
?
?
?
??
?
?
?
?
?
???
?
??
?
????
?
?
?
??
?????
?
?
???
??
?
?
??
?
?
?
?
??
?
?
?
?
??
??
?
?
???
?
?
??
?
?
???
???
?
?
??
??
?
?
??
?
?
?
?
???
?
?
?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
???
??
?
?
??
?
?
?
??
?
?
?
?
???
?
?
??
????
?
???
???
???
?
?
?
??
?
?
???
?
?
??
?
?
???
??
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?????
?????
?
?
??
?
??
?
??
?????
?
?
?
??
???
?
?
??
??
?
?
??
?
?
??
?
?
?
?
?
?
?
?
?
??
?
??
?
?
??
??
???
?
?
?
?
?
?
????
???
?
?
?
?
???
?
?
?
????
????
?
??
???
?
??
??
?
?
??
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
???
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?????
?
?
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
?
???
??
?
?
?
???
?
?
?
??????
?
?
??
?
?
?
?
????
?
?
???
??
??
??
???
?
?
??
?
???
?
?
?
?
??
??
?
??
?
?
?
?
??
?
?
??
?
?
??
???
?
?
??
?
?
???
??
?
??
?
?
?
?
?
??
?
?
????
??
?
?
??
?
0 2000 4000 6000 8000 10000
?8
?6
?4
?2
0
x8
gree
dy0.2
_fixe
d_er
ror_s
essio
ns10
000_
maxs
essio
nleng
th4_r
uns1
0
# sessions 
rewa
rd 
(b) 20% exploration, 80% exploitation: noticeable increase in
reward, hitting upper bound
Figure 1: Exploration/exploitation trade-off
already known successful actions to also ensure
high reward. In principle there is no distinction
between training and testing. Learning in the RL-
based dialogue manager is strongly dependent on
the chosen exploration/exploitation trade-off. This
is determined by the action selection policy, which
for each system turn decides probabilistically (-
greedy, softmax) if to exploit the currently known
best action of the policy for the believed dialogue
state, or to explore an untried action. Figure 1(a)
shows, for a subdomain of the application domain,
how the reward (expressed as minimizing costs)
reaches an upper bound early during 10,000 sim-
ulated dialogue sessions (each dot represents the
average of 10 rewards at a particular session num-
ber). Note that if the policy provides no matching
state, the system can only explore, and thus a cer-
tain amount of exploration always takes place. In
contrast, with exploration the system is able to find
lower cost solutions (figure 1(b)).
1.2 User Simulation
In order to conduct thousands of simulated dia-
logues, the DM needs to deal with heterogeneous
but plausible user input. For this purpose, we have
designed a User Simulator (US) which bootstraps
likely user behaviors starting from a small corpus
of 74 in-domain dialogs, acquired using the rule-
based version of the SDS (section 2). The task of
the US is to simulate the output of the SLU mod-
ule to the DM, hence providing it with a ranked
list of SLU hypotheses.
A list of possible user goals is stored in a
database table (section 3) using a frame/slot rep-
resentation. For each simulated dialogue, one or
more user goals are randomly selected. The User
Simulator?s task is to mimic a user wanting to per-
form such task(s). At each turn, the US mines the
previous system dialog act to obtain the concepts
required by the DM and obtains the corresponding
values (if any) from the current user goal.
The output of the user model proper is passed
to an error model that simulates the ?noisy chan-
nel? recognition errors based on statistics from the
dialogue corpus. These concern concept values as
well as other dialogue phenomena such as noIn-
put, noMatch and hangUp. If the latter phenomena
occur, they are propagated to the DM directly; oth-
erwise, the following US step is to attach plausible
confidences to concept-value pairs, also based on
the dialogue corpus. Finally, concept-value pairs
are combined in an SLU hypothesis and, as in the
regular SLU module, a cumulative utterance-level
confidence is computed, determining the rank of
each of the n hypotheses. The probability of a
given concept-value observation at time t+1 given
the system act at time t, named a
s,t
, and the ses-
sion user goal g
u
, P (o
t+1
|a
s,t
, g
u
), is obtained by
combining the error model and the user model:
P (o
t+1
|a
u,t+1
) ? P (a
u,t+1
|a
s,t
, g
u
)
where a
u,t+1
is the true user action.
2 Rule-based Dialogue Management
A rule-based dialogue manager was developed as a
meaningful comparison to the trained DM, to ob-
tain training data from human-system interaction
for the user simulator, and to understand the prop-
erties of the domain (Varges et al, 2008). Rule-
based dialog management works in two stages:
retrieving and preprocessing facts (tuples) taken
from a dialogue state database (section 3), and
inferencing over those facts to generate a system
response. We distinguish between the ?context
model? of the first phase ? essentially allowing
42
more recent values for a concept to override less
recent ones ? and the ?dialog move engine? (DME)
of the second phase. In the second stage, accep-
tor rules match SLU results to dialogue context,
for example perceived user concepts to open ques-
tions. This may result in the decision to verify the
application parameter in question, and the action
is verbalized by language generation rules. If the
parameter is accepted, application dependent task
rules determine the next parameter to be acquired,
resulting in the generation of an appropriate re-
quest.
3 Data-centric System Architecture
All data is continuously stored in a database which
web-service based processing modules (such as
SLU, DM and language generation) access. This
architecture also allows us to access the database
for immediate visualization. The system presents
an example of a ?thick? inter-module informa-
tion pipeline architecture. Individual components
exchange data by means of sets of hypotheses
complemented by the detailed conversational con-
text. The database concentrates heterogeneous
types of information at various levels of descrip-
tion in a uniform way. This facilitates dialog eval-
uation, data mining and online learning because
data is available for querying as soon as it has
been stored. There is no need for separate logging
mechanisms. Multiple systems/applications are
available on the same infrastructure due to a clean
separation of its processing modules (SLU, DM,
NLG etc.) from data storage (DBMS), and moni-
toring/analysis/visualization and annotation tools.
4 Visualization Tool
We developed a live web-based dialogue visual-
ization tool that displays ongoing and past di-
alogue utterances, semantic interpretation confi-
dences and distributions of confidences for incom-
ing user acts, the dialogue manager state, and
policy-based decisions and updating. An exam-
ple of the visualization tool is given in figures 3
(dialogue logs) and 4 (annotation view). We are
currently extending the visualization tool to dis-
play the POMDP-related information that is al-
ready present in the dialogue database.
The visualization tool shows how our dedicated
SLU module produces a number of candidate se-
mantic parses using the semantics of a domain on-
tology and the output of ASR.
The visualization of the internal representation
of the POMDP-DM includes the N best dialogue
states after each user utterance and the reranking
of the action set. At the end of each dialogue ses-
sion, the reward and the policy updates are shown,
i.e. new or updated state entries and action val-
ues. Another plot relates the current dialogue?s
reward to the reward of previous dialogues (as in
plots 1(b) and 1(a)).
Users are able to talk with several systems
(via SIP phone connection to the dialogue system
server) and see their dialogues in the visualization
tool. They are able to compare the rule-based
system, a randomly exploring learner that has
not been trained yet, and several systems that
use various pre-trained policies. These policies
are obtained by dialogue simulations with user
models based on data obtained from human-
machine dialogues with the original rule-based
dialogue manager. The web tool is available
at http://cicerone.dit.unitn.it/
DialogStatistics/.
Acknowledgments
This work was partially supported by the Euro-
pean Commission Marie Curie Excellence Grant
for the ADAMACH project (contract No. 022593)
and by LUNA STREP project (contract No.
33549).
References
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus-based discourse understanding in spoken di-
alogue systems. In ACL-03, Sapporo, Japan.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21(2):97?126.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent Information State in a Data-Centric Architec-
ture. In SIGDIAL-08, Columbus, Ohio.
J. D. Williams and S. Young. 2006. Partially Ob-
servable Markov Decision Processes for Spoken Di-
alog Systems. Computer Speech and Language,
21(2):393?422.
43
ASR
TTS
Turn 
Setup
DB
SLU
DM
NLG
http-req
http-req
http-req
http-req
Ids
VXML
   page
ASR results
        HTTP request 
SLU results
DM context/results
VXMLgen
http-req
NLG context/results
(a) Turn-level information flow in the data-centric SDS ar-
chitecture
DB
Simulation
Environment
DM
NLG
User Model
Error Model
User Goals
Corpus
(b) User simulator interface with the dialogue manager
Figure 2: Architecture for interacting with human user (left) and simulated user (right)
Figure 3: Left pane: overview of all dialogues. Right pane: visualization of a system opening prompt fol-
lowed by the user?s activity request. All distinct SLU hypotheses (concept-value combinations) deriving
from ASR are ranked based on concept-level confidence (2 in this turn).
Figure 4: Turn annotation of task success based on previously filled dialog transcriptions (left box).
44
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 156?159,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Leveraging POMDPs trained with User Simulations and
Rule-based Dialogue Management in a Spoken Dialogue System
Sebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi Roberti
Department of Information Engineering and Computer Science
University of Trento
38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.it
Abstract
We have developed a complete spoken di-
alogue framework that includes rule-based
and trainable dialogue managers, speech
recognition, spoken language understand-
ing and generation modules, and a com-
prehensive web visualization interface.
We present a spoken dialogue system
based on Reinforcement Learning that
goes beyond standard rule based models
and computes on-line decisions of the best
dialogue moves. Bridging the gap between
handcrafted (e.g. rule-based) and adap-
tive (e.g. based on Partially Observable
Markov Decision Processes - POMDP) di-
alogue models, this prototype is able to
learn high rewarding policies in a number
of dialogue situations.
1 Reinforcement Learning in Dialogue
Machine Learning techniques, and particularly
Reinforcement Learning (RL), have recently re-
ceived great interest in research on dialogue man-
agement (DM) (Levin et al, 2000; Williams and
Young, 2006). A major motivation for this choice
is to improve robustness in the face of uncertainty
due for example to speech recognition errors. A
second important motivation is to improve adap-
tivity w.r.t. different user behaviour and applica-
tion/recognition environments.
The RL approach is attractive because it offers a
statistical model representing the dynamics of the
interaction between system and user. This con-
trasts with the supervised learning approach where
system behaviour is learnt based on a fixed cor-
pus. However, exploration of the range of dialogue
management strategies requires a simulation en-
vironment that includes a simulated user (Schatz-
mann et al, 2006) in order to avoid the prohibitive
cost of using human subjects.
We demonstrate various parameters that influ-
ence the learnt dialogue management policy by
using pre-trained policies (section 5). The appli-
cation domain is a tourist information system for
accommodation and events in the local area. The
domain of the trained DMs is identical to that of a
rule-based DM that was used by human users (sec-
tion 4), allowing us to compare the two directly.
2 POMDP demonstration system
The POMDP DM implemented in this work is
shown in figure 1: at each turn at time t, the incom-
ingN user act hypotheses an,u split the state space
St to represent the complete set of interpretations
from the start state (N=2). A belief update is per-
formed resulting in a probability assigned to each
state. The resulting ranked state space is used as
a basis for action selection. In our current imple-
mentation, belief update is based on probabilistic
user responses that include SLU confidences. Ac-
tion selection to determine system action am,s is
based on the best state (m is a counter for actions
in action set A). In each turn, the system uses an
-greedy action selection strategy to decide prob-
abilistically if to exploit the policy or explore any
other action at random. (An alternative would be
softmax, for example.) At the end of each dia-
logue/session a reward is assigned and policy en-
tries are added or updated for each state-action
pair involved. These pairs are stored in tabular
form. We perform Monte Carlo updating similar
to (Levin et al, 2000):
Qt(s, a) = R(s, a)/n+Qt?1 ? (n ? 1)/n (1)
where n is the number of sessions, R the reward
and Q the estimate of the state-action value.
At the beginning of each dialogue, a user goal
UG (a set of concept-value pairs) is generated ran-
domly and passed to a user simulator. The user
simulator takes UG and the current dialogue con-
text to produce plausible SLU hypotheses. These
156
Turn?t1?
POLICY:?? Q(s1,a1)?s1,a1?s1,a2?s2,a1?s3,a4?
St1?
a1,s?a2,s?a3,s?a4,s?
policy?lookup?
policy??update?
Ut1?
a1,u?
final?state?
reward?computation?
Turn?t2? Turn?tn?
state?space?
Ut2?
state?space? St2?
?s1,1? ?s1,2??s2,2?a2,u?
a1,s? a2,s?a3,s?
a4,s?
an,s? an,s?an,s?
an,s?
state?space? Stn?user?goal?
start?state?
s1,n?
Q(s1,a2)?Q(s2,a1)?Q(s3,a4)?
UG?
s3,n?
s2,n?
s4,n?s5,n?
s6,n?
Figure 1: POMDP Dialogue Manager
are a subset of the concept-value pairs in UG along
with a confidence estimate bootstrapped from a
small corpus of 74 in-domain dialogs. We assume
that the user ?runs out of patience? after 15 turns
and ends the call.
The system visualizes POMDP-related infor-
mation live for the ongoing dialogue (figure 2).
The visualization tool shows the internal represen-
tation of the dialogue manager including the the
N best dialogue states after each user utterance
and the reranking of the action set. At the end
of each dialogue session, the reward and the pol-
icy updates are shown, i.e. new or updated state
entries and action values. Moreover, the system
generates a plot that relates the current dialogue?s
reward to the reward of previous dialogues.
3 User Simulation
To conduct thousands of simulated dialogues, the
DM needs to deal with heterogeneous but plau-
sible user input. We designed a User Simulator
(US) which bootstraps likely user behaviors start-
ing from a small corpus of 74 in-domain dialogs,
acquired using a rule-based version of the system
(section 4). The role of the US is to simulate
the output of the SLU module to the DM during
the whole interaction, fully replacing the ASR and
SLU modules. This differs from other user sim-
ulation approaches where n-gram models of user
dialog acts are represented.
For each simulated dialogue, one or more user
goals are randomly selected from a list of possible
user goals stored in a database table. A goal is rep-
resented as the set of concept-value pairs defining
a task. Simulation of the user?s behaviour happens
in two stages. First, a user model, i.e. a model
of the user?s intentions at the current stage of the
dialogue, is created. This is done by mining the
previous system move to obtain the concepts re-
quired by the DM and their corresponding values
(if any) from the current user goal. Then, the out-
put of the user model is passed to an error model
that simulates the ?noisy channel? recognition er-
rors based on statistics from the dialogue corpus.
Errors produce perturbations of concept values as
well as phenomena such as noInput, noMatch and
hangUp. If the latter phenomena occur, they are
directly propagated to the DM; otherwise, plau-
sible confidences (based on the dialogue corpus)
are attached to concept-value pairs. The probabil-
ity of a given concept-value observation at time
t + 1 given the system move at time t, as,t, and
the session user goal gu, called P (ot+1|as,t, gu),
is obtained by combining the outputs of the error
model and the user model:
P (ot+1|au,t+1) ? P (au,t+1|as,t, gu)
where au,t+1 is the true user action. Finally,
concept-value pairs are combined in an SLU hy-
pothesis and, as in the regular SLU module, a cu-
mulative utterance-level confidence is computed,
determining the rank of each of the N hypotheses
output to the DM.
4 Rule-based Dialogue Management
A rule-based DM was developed as a meaning-
ful comparison to the trained DM, to obtain train-
ing data from human-system interaction for the
US, and to understand the properties of the do-
main. Rule-based dialog management works in
two stages: retrieving and preprocessing facts (tu-
ples) taken from a dialogue state database, and
inferencing over those facts to generate a system
response. We distinguish between the ?context
model? of the first phase ? essentially allowing
more recent values for a concept to override less
recent ones ? and the ?dialog move engine? of the
second phase. In the second stage, acceptor rules
match SLU results to dialogue context, for ex-
ample perceived user concepts to open questions.
This may result in the decision to verify the ap-
plication parameter in question, and the action is
verbalized by language generation rules. If the
parameter is accepted, application dependent task
157
Figure 2: A screenshot of the online visualization tool. Left: user goal (top), evolving ranked state space
(bottom). Center: per state action distribution at turn ti. Right: consequent reward computation (top) and
policy updates (bottom). See video at http://www.youtube.com/watch?v=69QR0tKKhCw.
158
Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool. Right Pane: visual-
ization of a system opening prompt followed by the user?s activity request. All distinct SLU hypotheses
(concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in this
turn).
rules determine the next parameter to be acquired,
resulting in the generation of an appropriate re-
quest. See (Varges et al, 2008) for more details.
5 Visualization Tool
In addition to the POMDP-related visualization
tool (figure 2), we developed another web-based
dialogue tool for both rule-based and POMDP sys-
tem that displays ongoing and past dialogue ut-
terances, semantic interpretation confidences and
distributions of confidences for incoming user acts
(see dialogue logs in figure 3).
Users are able to talk with several systems
(via SIP phone connection to the dialogue system
server) and see their dialogues in the visualization
tool. They are able to compare the rule-based
system, a randomly exploring learner that has not
been trained yet, and several systems that use vari-
ous pre-trained policies. The web tool is available
at http://cicerone.dit.unitn.it/
DialogStatistics/.
Acknowledgments
This work was partially supported by the Euro-
pean Commission Marie Curie Excellence Grant
for the ADAMACH project (contract No. 022593)
and by LUNA STREP project (contract No.
33549).
References
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21(2):97?126.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proc. 9th SIGdial Workhop
on Discourse and Dialogue, Columbus, Ohio.
J. D. Williams and S. Young. 2006. Partially Ob-
servable Markov Decision Processes for Spoken Di-
alog Systems. Computer Speech and Language,
21(2):393?422.
159
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 213?216,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Investigating Clarification Strategies in a
Hybrid POMDP Dialog Manager
Sebastian Varges and Silvia Quarteroni and Giuseppe Riccardi and Alexei V. Ivanov
Department of Information Engineering and Computer Science
University of Trento, 38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov}@disi.unitn.it
Abstract
We investigate the clarification strategies
exhibited by a hybrid POMDP dialog
manager based on data obtained from a
phone-based user study. The dialog man-
ager combines task structures with a num-
ber of POMDP policies each optimized for
obtaining an individual concept. We in-
vestigate the relationship between dialog
length and task completion. In order to
measure the effectiveness of the clarifica-
tion strategies, we compute concept pre-
cisions for two different mentions of the
concept in the dialog: first mentions and
final values after clarifications and simi-
lar strategies, and compare this to a rule-
based system on the same task. We ob-
serve an improvement in concept precision
of 12.1% for the hybrid POMDP com-
pared to 5.2% for the rule-based system.
1 Introduction
In recent years, probabilistic models of dialog
have been introduced into dialog management, the
part of the spoken dialog system that takes the ac-
tion decision. A major motivation is to improve
robustness in the face of uncertainty, in particu-
lar due to speech recognition errors. The inter-
action is characterized as a dynamic system that
manipulates its environment by performing dialog
actions and perceives feedback from the environ-
ment through its sensors. The original sensory in-
formation is obtained from the speech recognition
(ASR) results which are typically processed by a
spoken language understanding module (SLU) be-
fore being passed on to the dialog manager (DM).
The seminal work of (Levin et al, 2000) mod-
eled dialog management as a Markov Decision
Process (MDP). Using reinforcement learning as
the general learning paradigm, an MDP-based di-
alog manager incrementally acquires a policy by
obtaining rewards about actions it performed in
specific dialog states. As we found in earlier ex-
periments, an MDP can learn to gradually drop the
use of clarification questions if there is no noise.
This is due to the fact that clarifications do not
improve the outcome of the dialog, i.e. the re-
ward. However, with extremely high levels of
noise, the learner prefers to end the dialog imme-
diately (Varges et al, 2009). In contrast to deliber-
ate decision making in the pragmatist tradition of
dialog processing, reinforcement learning can be
regarded as low-level decision making.
MDPs do not account for the observational un-
certainty of the speech recognition results, a key
challenge in spoken dialog systems. Partially Ob-
servable Markov Decision Process (POMDPs) ad-
dress this issue by explicitly modeling how the dis-
tribution of observations is governed by states and
actions.
In this work, we describe the evaluation of a
divide-and-conquer approach to dialog manage-
ment with POMDPs that optimizes policies for
acquiring individual concepts separately. This
makes optimization much easier and allows us to
model the confusability of concrete concept values
explicitly. This also means that different clarifica-
tion strategies are learned for individual concepts
and even individual concept values. The use of the
POMDP policies is orchestrated by an explicit task
structure, resulting in a hybrid approach to dialog
management. The evaluation involved a user study
of 20 subjects in a tourist information domain. The
system is compared against a rule-based baseline
system in the same domain that was also evaluated
with 20 subjects.
2 Hybrid POMDP dialog management
In this section we introduce the hybrid POMDP di-
alog manager that was used in the data collection.
213
2.1 Concept-level POMDPs
The domain is a tourist information system that
uses 5 different policies that can be used in 8
different task roles (see below). For each con-
cept we optimized an individual policy. The
number of states of the POMDP can be lim-
ited to the concept values, for example a loca-
tion name such as trento. The set of ac-
tions consists of a question to obtain the concept
(e.g. question-location), a set of clari-
fication actions (e.g. verify-trento) and a
set of submit actions (e.g. submit-trento).
POMDP modeling including a heuristically set re-
ward structure follows the (simpler) ?tiger prob-
lem? that is well-known in the AI community
(Kaelbling et al, 1998): the system has a num-
ber of actions to obtain further information which
it can try and repeat in any order until it is ready
to commit to a concept value. For optimization we
used the APPL solver (Kurniawati et al, 2008).
2.2 Task structure and dialog management
The use of individual policies is orchestrated by
an explicit task structure that activates and de-
activates them. The task structure is essentially
a directed AND-OR graph with a common root
node. The dialog manager maintains a separate be-
lief distribution for each concept. Figure 1 shows
the general system architecture with a schematic
view of the task structure, and additionally a more
detailed view of an active location node. In the
example, the root node has already finished and
the system is currently obtaining the location for a
lodging task. The term ?role? refers to a concept?s
part in the task, for example a month may be the
check-in or check-out month for accommodation
booking.
At the beginning of a dialog, the task structure is
initialized by activating the root node. A top level
function activates nodes of the task structure and
passes control to that node. Each node maintains
a belief bc for a concept c, which is used to rank
the available actions by computing the inner prod-
uct of policy vectors and belief. The top-ranked
action am is selected by the system, i.e. it is ex-
ploiting the policy, and passed to the natural lan-
guage generator (NLG). Next, the top-ranked SLU
results for the active node and concept are used as
observation zu,c to update the belief to b?c, which
User	 ?
ASR	 ?
TTS	 ?
SLU	 ?
NLG	 ?
PASSIVE	 ?
BLOCKED	 ?
ACTIVE	 ?
BLOCKED	 ?
BLOCKED	 ? BLOCKED	 ?
OPEN	 ? OPEN	 ?
am	 ?
zu,c	 ?
Condi?n :	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?(ac?vity=	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?lodging-??enquiry	 ??	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?lodging-??reserva?n),	 ?ConceptName	 ?	 ?	 ?	 ?	 ?	 ?loca?n,	 ?ConceptRole:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?loca?n-??lodging,	 ?Status:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?ACTIVE,	 ?Belief:	 ?
Ac?on:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?ques?n-??loca?n.	 ?
zu,d	 ?
DM	 ?
Figure 1: System architecture with Task Structure
(task node example in detailed view)
follows the standard method for POMDPs:
b?c(s
?) =
?
s?S
bc(s) T (s, am, s
?) O(a, s?, zu,c)/pzu,c
(1)
where probability b?c(s
?) is the updated belief of
being in state s?, which is computed as the sum of
the probabilities of transitioning from all previous
belief points s to s? by taking machine action am
with probability T (s, am, s?) and observing zu,c
with (smoothed) probability O(am, s?, zu,c). Nor-
malization to obtain a valid probability distribu-
tion is performed by dividing by the probability of
the observation pzu,c .
A concept remains active until a submit action
is selected. At that point, the next active node is
retrieved from the task structure and immediately
used for action selection with an initially uniform
belief. Submit actions are not communicated to
the user but collected and used for the database
query at the end of the dialog.
Overanswering, i.e. the user providing more in-
formation than directly asked for, is handled by de-
layed belief updating: the SLU results are stored
until the first concept of a matching type becomes
active. This is a heuristic rule designed to ensure
that a concept is interpreted in its correct role. Op-
erationally, unused SLU results zu,d (where con-
cept d 6= c) are passed on to the next activated
task node (see also figure 1).
3 Experiments and data analysis
We conducted user studies with two systems in-
volving 20 subjects and 8 tasks in each study.
The systems use a Voice XML platform to drive
ASR and TTS components. Speech recognition is
214
Lodging Task Event Enquiry
TCR #turns TCR #turns
Rule-based DM 75.5% 13.7 66.7% 8.7
(40/53) (?=4.8) (28/42) (?=3.3)
POMDP-DM 78.1% 23.0 84.3% 14.4
(50/64) (?=8.8) (27/32) (?=4.5)
Table 1: Task completion and length metrics
based on statistical language models for the open-
ing prompt, and is grammar-based otherwise. One
system used the hybrid POMDP-DM, the other
is a rule-based dialog manager that uses explicit,
heuristically set confidence thresholds to trigger
the use of clarification questions (Varges et al,
2008).
Dialog length and task completion Table 1
shows task completion rates (?TCR?) and dura-
tions (?#turns?) for the POMDP and rule-based
systems. Task completion in this metric is defined
as the number of tasks of a certain type that were
successfully concluded. Duration is measured in
the number of turn pairs consisting of a system
action followed by a user action. We combine
the counts for two closely related lodging tasks.
The number of tasks is shown in brackets. Table
1 shows that the POMDP-DM successfully con-
cludes more and longer lodging tasks and almost
as many event tasks. In general, the POMDP poli-
cies can be described as more cautious although
obviously the dialog length of the rule system de-
pends on the chosen thresholds.
Concept precision at the value level In order
to measure the effect of the clarification strategies
in both systems, we computed concept precisions
for two different mentions of a concept in a dialog
(table 2): first mentions and final values after clar-
ifications and similar strategies. The rationale for
this metric is that the last mentioned concept value
is the value that the system ultimately obtains from
the user, which is used in the database query:
? if the system decides not to use clarifications,
the only mentioned value is the accepted one,
? if the system verifies and obtains a positive
answer, the last mentioned value is the ac-
cepted one,
? if the system verifies and obtains a negative
answer, the user will mention a new value
(which may or may not be accepted).
Thus, this metric is a uniform way of capturing
the obtained values from systems that internally
Rule-based DM POMDP-DM
first final ?% first final ?%
a) activity 0.78 0.74 -4.1 0.83 0.88 5.0
b) location 0.64 0.74 15.8 0.69 0.73 6.3
c) starrating 0.67 0.70 3.4 0.90 0.97 7.7
d) month 0.85 0.89 4.3 0.76 0.86 12.7
e) day 0.70 0.76 8.3 0.61 0.76 25.3
ALL (a-e) 0.74 0.78 5.2 0.74 0.83 12.1
Clarifications 0.84 0.85 1.5 0.96 0.87 -8.8
Table 2: Concept precision of first vs final value
use very different dialog managers and representa-
tions. The actual precision of a conceptC is calcu-
lated by comparing SLU results to annotations and
counting true positives (matchesM ) and false pos-
itives (separated into mismatches N and entirely
un-annotated concepts U ): Prec(C) = MM+N+U .
Unrecognized concepts, on the other hand, are re-
call related and not counted since they cannot be
part of any system belief.
As table 2 clearly shows, the use of clarification
strategies has a positive effect on concept preci-
sion in both systems. The exception is the preci-
sion of concept activity in the rule-based system
for which the system reprompted rather than ver-
ified.1 In table 2, row ?All? refers to the average
weighted precision of the five concepts. Both sys-
tems start from a similar level of overall precision.
The relative improvement of the POMDP-DM for
all concepts is 12.1%, compared to 5.2% of the
rule-based DM.
We conducted a statistical significance test by
computing the delta in the form of three values for
individual data points, i.e. dialogs, and assigned
+1 for all changes from non-match to match, -1
for a change in the opposite direction and 0 for ev-
erything else (e.g. from mismatch to mismatch).
We found that, although there is a tendency for
the POMDP-DM to perform better, the difference
is not statistically significant at p=0.05 (a possi-
ble explanation is the data size since we are using
human subjects).
We furthermore measured the precision of rec-
ognizing ?yes/no? answers to clarification ques-
tions. In contrast to actual concepts, there is no be-
lief distribution for these in the DM since clarifica-
tion actions are part of the concept POMDP mod-
els. We are thus dealing with individual one-off
recognition results that should be entirely indepen-
dent of each other. However, as table 2 (bottom)
1The second value obtained may be incorrect but above
the confidence threshold; note that the rule system does not
maintain a belief distribution over values.
215
shows, the precision of verifications decreases for
the hybrid POMDP system. A plausible expla-
nation for this is the increasing impatience of the
users due to the longer dialog duration.
Characterization of dialog strategies For
some concepts, the best policy is to ask the
concept question once and then verify once before
committing to the value (assuming the answer is
positive). Other policies verify the same value
twice. Another learned strategy is to ask the orig-
inal concept question twice and then only verify
the value once (assuming that the understood
value was the same in both concept questions). In
other words, the individual concept policies show
different types of strategies regarding uncertainty
handling. This is in marked contrast to the
manually programmed DM that always asks the
concept question once and verifies it if needed
(concept activity being the exception).
HCI and language generation The domain is
sufficiently simple to use template-based genera-
tion techniques to produce the surface forms of
the responses. However, the experiments with the
POMDP-DM highlight some new challenges re-
garding HCI aspects of spoken dialog systems: the
choice of actions may not be ?natural? from the
user?s perspective, for example if the system asks
for a concept twice. However, it should be possi-
ble to better communicate the (change in the) be-
lief to the user.
4 Related work
The pragmatist tradition of dialog processing uses
explicit representations of dialog structure to take
decisions about clarification actions. These mod-
els are more fine-grained and often deal with writ-
ten text, e.g. (Purver, 2006), whereas in spo-
ken dialog systems a major challenge is managing
the uncertainty of the recognition. Reinforcement
learning approaches to dialog management learn
decisions from (often simulated) dialog data in a
less deliberative way. For example, the Hidden In-
formation State model (Young et al, 2010) uses a
reduced summary space that abstracts away many
of the details of observations and dialog state, and
mainly looks at the confidence scores of the hy-
potheses. This seems to imply that clarification
strategies are not tailored toward individual con-
cepts and their values. (Bui et al, 2009) uses fac-
tored POMDP representations that seem closest to
our approach. However, the effect of clarifications
does not seem to have been investigated.
5 Conclusions
We presented evaluation results for a hybrid
POMDP system and compared it to a rule-based
one. The POMDP system achieves higher con-
cept precision albeit at the cost of longer dialogs,
i.e. there is an empirically measurable trade-off
between concept precision and dialog length.
Acknowledgments
This work was partially supported by the Eu-
ropean Commission Marie Curie Excellence
Grant for the ADAMACH project (contract No.
022593).
References
T.H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2009.
A tractable hybrid DDN-POMDP approach to affec-
tive dialogue modeling for probabilistic frame-based
dialogue systems. Natural Language Engineering,
15(2):273?307.
Leslie Pack Kaelbling, Michael L. Littman, and An-
thony R. Cassandra. 1998. Planning and acting in
partially observable stochastic domains. Artificial
Intelligence, 101:99?134.
H. Kurniawati, D. Hsu, andW.S. Lee. 2008. SARSOP:
Efficient point-based POMDP planning by approxi-
mating optimally reachable belief spaces. In Proc.
Robotics: Science and Systems.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
Matthew Purver. 2006. CLARIE: Handling clarifica-
tion requests in a dialogue system. Research on Lan-
guage and Computation, 4(2-3):259?288, October.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, Colum-
bus, Ohio.
Sebastian Varges, Giuseppe Riccardi, Silvia Quar-
teroni, and Alexei V. Ivanov. 2009. The explo-
ration/exploitation trade-off in reinforcement learn-
ing for dialogue management. In Proceedings of
IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU).
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The Hid-
den Information State Model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24:150?174.
216
