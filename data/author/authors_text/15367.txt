Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1466?1475,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Domain-Specific Knowledge from Text
Dirk Hovy, Chunliang Zhang, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, czheng, hovy}@isi.edu
Anselmo Pen?as
UNED NLP and IR Group
Juan del Rosal 16
28040 Madrid, Spain
anselmo@lsi.uned.es
Abstract
Learning by Reading (LbR) aims at enabling
machines to acquire knowledge from and rea-
son about textual input. This requires knowl-
edge about the domain structure (such as en-
tities, classes, and actions) in order to do in-
ference. We present a method to infer this im-
plicit knowledge from unlabeled text. Unlike
previous approaches, we use automatically ex-
tracted classes with a probability distribution
over entities to allow for context-sensitive la-
beling. From a corpus of 1.4m sentences, we
learn about 250k simple propositions about
American football in the form of predicate-
argument structures like ?quarterbacks throw
passes to receivers?. Using several statisti-
cal measures, we show that our model is able
to generalize and explain the data statistically
significantly better than various baseline ap-
proaches. Human subjects judged up to 96.6%
of the resulting propositions to be sensible.
The classes and probabilistic model can be
used in textual enrichment to improve the per-
formance of LbR end-to-end systems.
1 Introduction
The goal of Learning by Reading (LbR) is to enable
a computer to learn about a new domain and then
to reason about it in order to perform such tasks as
question answering, threat assessment, and explana-
tion (Strassel et al, 2010). This requires joint efforts
from Information Extraction, Knowledge Represen-
tation, and logical inference. All these steps depend
on the system having access to basic, often unstated,
foundational knowledge about the domain.
Most documents, however, do not explicitly men-
tion this information in the text, but assume basic
background knowledge about the domain, such as
positions (?quarterback?), titles (?winner?), or ac-
tions (?throw?) for sports game reports. Without
this knowledge, the text will not make sense to the
reader, despite being well-formed English. Luckily,
the information is often implicitly contained in the
document or can be inferred from similar texts.
Our system automatically acquires domain-
specific knowledge (classes and actions) from large
amounts of unlabeled data, and trains a probabilis-
tic model to determine and apply the most infor-
mative classes (quarterback, etc.) at appropriate
levels of generality for unseen data. E.g., from
sentences such as ?Steve Young threw a pass to
Michael Holt?, ?Quarterback Steve Young finished
strong?, and ?Michael Holt, the receiver, left early?
we can learn the classes quarterback and receiver,
and the proposition ?quarterbacks throw passes to
receivers?.
We will thus assume that the implicit knowl-
edge comes in two forms: actions in the form of
predicate-argument structures, and classes as part of
the source data. Our task is to identify and extract
both. Since LbR systems must quickly adapt and
scale well to new domains, we need to be able to
work with large amounts of data and minimal super-
vision. Our approach produces simple propositions
about the domain (see Figure 1 for examples of ac-
tual propositions learned by our system).
American football was the first official evaluation
domain in the DARPA-sponsored Machine Reading
program, and provides the background for a number
1466
of LbR systems (Mulkar-Mehta et al, 2010). Sports
is particularly amenable, since it usually follows a
finite, explicit set of rules. Due to its popularity,
results are easy to evaluate with lay subjects, and
game reports, databases, etc. provide a large amount
of data. The same need for basic knowledge appears
in all domains, though. In music, musicians play in-
struments, in electronics, components constitute cir-
cuits, circuits use electricity, etc.
Teams beat teams
Teams play teams
Quarterbacks throw passes
Teams win games
Teams defeat teams
Receivers catch passes
Quarterbacks complete passes
Quarterbacks throw passes to receivers
Teams play games
Teams lose games
Figure 1: The ten most frequent propositions discovered
by our system for the American football domain
Our approach differs from verb-argument identi-
fication or Named Entity (NE) tagging in several re-
spects. While previous work on verb-argument se-
lection (Pardo et al, 2006; Fan et al, 2010) uses
fixed sets of classes, we cannot know a priori how
many and which classes we will encounter. We
therefore provide a way to derive the appropriate
classes automatically and include a probability dis-
tribution for each of them. Our approach is thus
less restricted and can learn context-dependent, fine-
grained, domain-specific propositions. While a NE-
tagged corpus could produce a general proposition
like ?PERSON throws to PERSON?, our method
enables us to distinguish the arguments and learn
?quarterback throws to receiver? for American foot-
ball and ?outfielder throws to third base? for base-
ball. While in NE tagging each word has only one
correct tag in a given context, we have hierarchical
classes: an entity can be correctly labeled as a player
or a quarterback (and possibly many more classes),
depending on the context. By taking context into
account, we are also able to label each sentence in-
dividually and account for unseen entities without
using external resources.
Our contributions are:
? we use unsupervised learning to train a model
that makes use of automatically extracted
classes to uncover implicit knowledge in the
form of predicate-argument propositions
? we evaluate the explanatory power, generaliza-
tion capability, and sensibility of the proposi-
tions using both statistical measures and human
judges, and compare them to several baselines
? we provide a model and a set of propositions
that can be used to improve the performance
of end-to-end LbR systems via textual enrich-
ment.
2 Methods
INPUT:
Steve Young threw a pass to Michael Holt
1. PARSE INPUT:
2. JOIN NAMES, EXTRACT PREDICATES:
NVN: Steve_Young throw pass 
NVNPN: Steve_Young throw pass to Michael_Holt
3. DECODE TO INFER PROPOSITIONS:
QUARTERBACK throw pass
QUARTERBACK throw pass to RECEIVER
Steve/NNP
Young/NNP
throw/VBD
pass/NN
a/DT
to/TO
Michael/NNP
Holt/NNP
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 2: Illustrated example of different processing steps
Our running example will be ?Steve Young threw
a pass to Michael Holt?. This is an instance of the
underlying proposition ?quarterbacks throw passes
to receivers?, which is not explicitly stated in the
data. A proposition is thus a more general state-
ment about the domain than the sentences it de-
rives. It contains domain-specific classes (quarter-
back, receiver), as well as lexical items (?throw?,
?pass?). In order to reproduce the proposition,
given the input sentences, our system has to not
only identify the classes, but also learn when to
1467
abstract away from the lexical form to the ap-
propriate class and when to keep it (cf. Figure
2, step 3). To facilitate extraction, we focus on
propositions with the following predicate-argument
structures: NOUN-VERB-NOUN (e.g., ?quarter-
backs throw passes?), or NOUN-VERB-NOUN-
PREPOSITION-NOUN (e.g., ?quarterbacks throw
passes to receivers?. There is nothing, though, that
prevents the use of other types of structures as well.
We do not restrict the verbs we consider (Pardo et
al., 2006; Ritter et al, 2010)), which extracts a high
number of hapax structures.
Given a sentence, we want to find the most likely
class for each word and thereby derive the most
likely proposition. Similar to Pardo et al (2006), we
assume the observed data was produced by a process
that generates the proposition and then transforms
the classes into a sentence, possibly adding addi-
tional words. We model this as a Hidden Markov
Model (HMM) with bigram transitions (see Section
2.3) and use the EM algorithm (Dempster et al,
1977) to train it on the observed data, with smooth-
ing to prevent overfitting.
2.1 Data
We use a corpus of about 33k texts on Ameri-
can football, extracted from the New York Times
(Sandhaus, 2008). To identify the articles, we rely
on the provided ?football? keyword classifier. The
resulting corpus comprises 1, 359, 709 sentences
from game reports, background stories, and opin-
ion pieces. In a first step, we parse all documents
with the Stanford dependency parser (De Marneffe
et al, 2006) (see Figure 2, step 1). The output
is lemmatized (collapsing ?throws?, ?threw?, etc.,
into ?throw?), and marked for various dependen-
cies (nsubj, amod, etc.). This enables us to ex-
tract the predicate argument structure, like subject-
verb-object, or additional prepositional phrases (see
Figure 2, step 2). These structures help to sim-
plify the model by discarding additional words like
modifiers, determiners, etc., which are not essen-
tial to the proposition. The same approach is used
by (Brody, 2007). We also concatenate multi-
word names (identified by sequences of NNPs) with
an underscore to form a single token (?Steve/NNP
Young/NNP?? ?Steve Young?).
2.2 Deriving Classes
To derive the classes used for entities, we do not re-
strict ourselves to a fixed set, but derive a domain-
specific set directly from the data. This step is per-
formed simultaneously with the corpus generation
described above. We utilize three syntactic construc-
tions to identify classes, namely nominal modifiers,
copula verbs, and appositions, see below. This is
similar in nature to Hearst?s lexico-syntactic patterns
(Hearst, 1992) and other approaches that derive IS-
A relations from text. While we find it straightfor-
ward to collect classes for entities in this way, we
did not find similar patterns for verbs. Given a suit-
able mechanism, however, these could be incorpo-
rated into our framework as well.
Nominal modifier are common nouns (labeled
NN) that precede proper nouns (labeled NNP), as in
?quarterback/NN Steve/NNP Young/NNP?, where
?quarterback? is the nominal modifier of ?Steve
Young?. Similar information can be gained from ap-
positions (e.g., ?Steve Young, the quarterback of his
team, said...?), and copula verbs (?Steve Young is
the quarterback of the 49ers?). We extract those co-
occurrences and store the proper nouns as entities
and the common nouns as their possible classes. For
each pair of class and entity, we collect counts over
the corpus to derive probability distributions.
Entities for which we do not find any of the above
patterns in our corpus are marked ?UNK?. These
entities are instantiated with the 20 most frequent
classes. All other (non-entity) words (including
verbs) have only their identity as class (i.e., ?pass?
remains ?pass?).
The average number of classes per entity is 6.87.
The total number of distinct classes for entities is
63, 942. This is a huge number to model in our state
space.1 Instead of manually choosing a subset of the
classes we extracted, we defer the task of finding the
best set to the model.
We note, however, that the distribution of classes
for each entity is highly skewed. Due to the unsuper-
vised nature of the extraction process, many of the
extracted classes are hapaxes and/or random noise.
Most entities have only a small number of applicable
classes (a football player usually has one main posi-
1NE taggers usually use a set of only a few dozen classes at
most.
1468
tion, and a few additional roles, such as star, team-
mate, etc.). We handle this by limiting the number of
classes considered to 3 per entity. This constraint re-
duces the total number of distinct classes to 26, 165,
and the average number of classes per entity to 2.53.
The reduction makes for a more tractable model size
without losing too much information. The class al-
phabet is still several magnitudes larger than that for
NE or POS tagging. Alternatively, one could use ex-
ternal resources such as Wikipedia, Yago (Suchanek
et al, 2007), or WordNet++ (Ponzetto and Navigli,
2010) to select the most appropriate classes for each
entity. This is likely to have a positive effect on the
quality of the applicable classes and merits further
research. Here, we focus on the possibilities of a
self-contained system without recurrence to outside
resources.
The number of classes we consider for each entity
also influences the number of possible propositions:
if we consider exactly one class per entity, there will
be little overlap between sentences, and thus no gen-
eralization possible?the model will produce many
distinct propositions. If, on the other hand, we used
only one class for all entities, there will be similar-
ities between many sentences?the model will pro-
duce very few distinct propositions.
2.3 Probabilistic Model
INPUT:
Steve Young threw a pass to Michael Holt
PARSE:
INSTANCES:
Steve_Young throw pass 
Steve_Young throw pass to Michael_Holt
PROPOSITIONS:
Quarterback throw pass
Quarterback throw pass to receiver
Steve
Young
throw
pass
a
to
Michael
Holt
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 3: Graphical model for the running example
We use a generative noisy-channel model to cap-
ture the joint probability of input sentences and their
underlying proposition. Our generative story of how
a sentence s (with words s1, ..., sn) was generated
assumes that a proposition p is generated as a se-
quence of classes p1, ..., pn, with transition proba-
bilities P (pi|pi?1). Each class pi generates a word
si with probability P (si|pi). We allow additional
words x in the sentence which do not depend on any
class in the proposition and are thus generated inde-
pendently with P (x) (cf. model in Figure 3).
Since we observe the co-occurrence counts of
classes and entities in the data, we can fix the emis-
sion parameter P (s|p) in our HMM. Further, we do
not want to generate sentences from propositions, so
we can omit the step that adds the additional words
x in our model. The removal of these words is re-
flected by the preprocessing step that extracts the
structure (cf. Section 2.1).
Our model is thus defined as
P (s,p) =P (p1) ?
n?
i=1
(
P (pi|pi?1) ? P (si|pi)
)
(1)
where si, pi denote the ith word of sentence s and
proposition p, respectively.
3 Evaluation
We want to evaluate how well our model predicts
the data, and how sensible the resulting propositions
are. We define a good model as one that generalizes
well and produces semantically useful propositions.
We encounter two problems. First, since we de-
rive the classes in a data-driven way, we have no
gold standard data available for comparison. Sec-
ond, there is no accepted evaluation measure for this
kind of task. Ultimately, we would like to evaluate
our model externally, such as measuring its impact
on performance of a LbR system. In the absence
thereof, we resort to several complementary mea-
sures, as well as performing an annotation task. We
derive evaluation criteria as follows. A model gener-
alizes well if it can cover (?explain?) all the sentences
in the corpus with a few propositions. This requires
a measure of generality. However, while a proposi-
tion such as ?PERSON does THING?, has excellent
generality, it possesses no discriminating power. We
also need the propositions to partition the sentences
into clusters of semantic similarity, to support effec-
tive inference. This requires a measure of distribu-
tion. Maximal distribution, achieved by assigning
every sentence to a different proposition, however,
is not useful either. We need to find an appropri-
ate level of generality within which the sentences
are clustered into propositions for the best overall
groupings to support inference.
To assess the learned model, we apply the mea-
sures of generalization, entropy, and perplexity (see
1469
Sections 3.2, 3.3, and 3.4). These measures can be
used to compare different systems. We do not at-
tempt to weight or combine the different measures,
but present each in its own right.
Further, to assess label accuracy, we use Ama-
zon?s Mechanical Turk annotators to judge the sen-
sibility of the propositions produced by each sys-
tem (Section 3.5). We reason that if our system
learned to infer the correct classes, then the resulting
propositions should constitute true, general state-
ments about that domain, and thus be judged as sen-
sible.2 This approach allows the effective annotation
of sufficient amounts of data for an evaluation (first
described for NLP in (Snow et al, 2008)).
3.1 Evaluation Data
With the trained model, we use Viterbi decoding to
extract the best class sequence for each example in
the data. This translates the original corpus sen-
tences into propositions. See steps 2 and 3 in Figure
2.
We create two baseline systems from the same
corpus, one which uses the most frequent class
(MFC) for each entity, and another one which uses
a class picked at random from the applicable classes
of each entity.
Ultimately, we are interested in labeling unseen
data from the same domain with the correct class,
so we evaluate separately on the full corpus and
the subset of sentences that contain unknown enti-
ties (i.e., entities for which no class information was
available in the corpus, cf. Section 2.2).
For the latter case, we select all examples con-
taining at least one unknown entity (labeled UNK),
resulting in a subset of 41, 897 sentences, and repeat
the evaluation steps described above. Here, we have
to consider a much larger set of possible classes per
entity (the 20 overall most frequent classes). The
MFC baseline for these cases is the most frequent
of the 20 classes for UNK tokens, while the random
baseline chooses randomly from that set.
3.2 Generalization
Generalization measures how widely applicable the
produced propositions are. A completely lexical ap-
2Unfortunately, if judged insensible, we can not infer
whether our model used the wrong class despite better options,
or whether we simply have not learned the correct label.
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.04 0.01
0.12 0.09
0.25
0.66
Generalization
random
MFC
model
Figure 4: Generalization of models on the data sets
proach, at one extreme, would turn each sentence
into a separate proposition, thus achieving a gener-
alization of 0%. At the other extreme, a model that
produces only one proposition would generalize ex-
tremely well (but would fail to explain the data in
any meaningful way). Both are of course not desir-
able.
We define generalization as
g = 1?
|propositions|
|sentences|
(2)
The results in Figure 4 show that our model is
capable of abstracting away from the lexical form,
achieving a generalization rate of 25% for the full
data set. The baseline approaches do significantly
worse, since they are unable to detect similarities
between lexically different examples, and thus cre-
ate more propositions. Using a two-tailed t-test, the
difference between our model and each baseline is
statistically significant at p < .001.
Generalization on the unknown entity data set is
even higher (65.84%). The difference between the
model and the baselines is again statistically signif-
icant at p < .001. MFC always chooses the same
class for UNK, regardless of context, and performs
much worse. The random baseline chooses between
20 classes per entity instead of 3, and is thus even
less general.
3.3 Normalized Entropy
Entropy is used in information theory to measure
how predictable data is. 0 means the data is com-
pletely predictable. The higher the entropy of our
propositions, the less well they explain the data. We
are looking for models with low entropy. The ex-
treme case of only one proposition has 0 entropy:
1470
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.00 1.000.99 0.99
0.89
0.50
Normalized Entropy
random
MFC
model
Figure 5: Entropy of models on the data sets
we know exactly which sentences are produced by
the proposition.
Entropy is directly influenced by the number of
propositions used by a system.3 In order to compare
different models, we thus define normalized entropy
as
HN =
?
n?
i=0
Pi ? logPi
log n
(3)
where Pi is the coverage of the proposition, or the
percentage of sentences explained by it, and n is the
number of distinct propositions.
The entropy of our model on the full data set is
relatively high with 0.89, see Figure 5. The best
entropy we can hope to achieve given the number
of propositions and sentences is actually 0.80 (by
concentrating the maximum probability mass in one
proposition). The model thus does not perform as
badly as the number might suggest. The entropy of
our model on unseen data is better, with 0.50 (best
possible: 0.41). This might be due to the fact that
we considered more classes for UNK than for regu-
lar entities.
3.4 Perplexity
Since we assume that propositions are valid sen-
tences in our domain, good propositions should have
a higher probability than bad propositions in a lan-
guage model. We can compute this using perplex-
3Note that how many classes we consider per entity influ-
ences how many propositions are produced (cf. Section 2.2),
and thus indirectly puts a bound on entropy.
entropy
Page 1
full data set unknown entities
50.00
51.00
52.00
53.00
54.00
55.00
56.00
57.00
58.00
59.00
60.00 59.52
57.0357.03 57.1556.84
54.92
Perplexity
random
MFC
model
Figure 6: Perplexity of models on the data sets
ity:4
perplexity(data) = 2
? logP (data)
n (4)
where P (data) is the product of the proposition
probabilities, and n is the number of propositions.
We use the uni-, bi-, and trigram counts of the
GoogleGrams corpus (Brants and Franz, 2006) with
simple interpolation to compute the probability of
each proposition.
The results in Figure 6 indicate that the proposi-
tions found by the model are preferable to the ones
found by the baselines. As would be expected, the
sensibility judgements for MFC and model5 (Tables
1 and 2, Section 3.5) are perfectly anti-correlated
(correlation coefficient ?1) with the perplexity for
these systems in each data set. However, due to the
small sample size, this should be interpreted cau-
tiously.
3.5 Sensibility and Label Accuracy
In unsupervised training, the model with the best
data likelihood does not necessarily produce the best
label accuracy. We evaluate label accuracy by pre-
senting subjects with the propositions we obtained
from the Viterbi decoding of the corpus, and ask
them to rate their sensibility. We compare the dif-
ferent systems by computing sensibility as the per-
centage of propositions judged sensible for each sys-
tem. Since the underlying probability distributions
are quite different, we weight the sensibility judge-
ment for each proposition by the likelihood of that
proposition. We report results for both aggregate
4Perplexity also quantifies the uncertainty of the resulting
propositions, where 0 perplexity means no uncertainty.
5We did not collect sensibility judgements for the random
baseline.
1471
accuracy
Page 1
System
90.16 92.13 69.35 70.57 88.84 90.37
94.28 96.55 70.93 70.45 93.06 95.16
100 most frequent random combined
Data set agg maj agg maj agg maj
full
baseline
model
Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible
accuracy
Page 1
System
51.92 51.51 32.39 28.21 50.39 49.66
66.00 69.57 48.14 41.74 64.83 67.76
100 most frequent random combined
Data set agg maj agg maj agg maj
unknown
baseline
model
Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible
sensibility (using the total number of individual an-
swers), and majority sensibility, where each propo-
sition is scored according to the majority of annota-
tors? decisions.
The model and baseline propositions for the full
data set are both judged highly sensible, achieving
accuracies of 96.6% and 92.1% (cf. Table 1). While
our model did slightly better, the differences are not
statistically significant when using a two-tailed test.
The propositions produced by the model from un-
known entities are less sensible (67.8%), albeit still
significantly above chance level, and the baseline
propositions for the same data set (p < 0.01). Only
49.7% propositions of the baseline were judged sen-
sible (cf. Table 2).
3.5.1 Annotation Task
Our model finds 250, 169 distinct propositions,
the MFC baseline 293, 028. We thus have to restrict
ourselves to a subset in order to judge their sensi-
bility. For each system, we sample the 100 most
frequent propositions and 100 random propositions
found for both the full data set and the unknown enti-
ties6 and have 10 annotators rate each proposition as
sensible or insensible. To identify and omit bad an-
notators (?spammers?), we use the method described
in Section 3.5.2, and measure inter-annotator agree-
ment as described in Section 3.5.3. The details of
this evaluation are given below, the results can be
found in Tables 1 and 2.
The 200 propositions from each of the four sys-
6We omit the random baseline here due to size issues, and
because it is not likely to produce any informative comparison.
tems (model and baseline on both full and unknown
data set), contain 696 distinct propositions. We
break these up into 70 batches (Amazon Turk an-
notation HIT pages) of ten propositions each. For
each proposition, we request 10 annotators. Overall,
148 different annotators participated in our annota-
tion. The annotators are asked to state whether each
proposition represents a sensible statement about
American Football or not. A proposition like ?Quar-
terbacks can throw passes to receivers? should make
sense, while ?Coaches can intercept teams? does
not. To ensure that annotators judge sensibility and
not grammaticality, we format each proposition the
same way, namely pluralizing the nouns and adding
?can? before the verb. In addition, annotators can
state whether a proposition sounds odd, seems un-
grammatical, is a valid sentence, but against the
rules (e.g., ?Coaches can hit players?) or whether
they do not understand it.
3.5.2 Spammers
Some (albeit few) annotators on Mechanical Turk
try to complete tasks as quickly as possible with-
out paying attention to the actual requirements, in-
troducing noise into the data. We have to identify
these spammers before the evaluation. One way is
to include tests. Annotators that fail these tests will
be excluded. We use a repetition (first and last ques-
tion are the same), and a truism (annotators answer-
ing ?no? either do not know about football or just
answered randomly). Alternatively, we can assume
that good annotators, who are the majority, will ex-
hibit similar behavior to one another, while spam-
1472
mers exhibit a deviant answer pattern. To identify
those outliers, we compare each annotator?s agree-
ment to the others and exclude those whose agree-
ment falls more than one standard deviation below
the average overall agreement.
We find that both methods produce similar results.
The first method requires more careful planning, and
the resulting set of annotators still has to be checked
for outliers. The second method has the advantage
that it requires no additional questions. It includes
the risk, though, that one selects a set of bad annota-
tors solely because they agree with one another.
3.5.3 Agreement
agreement
Page 1
0.88 0.76 0.82
? 0.45 0.50 0.48
0.66 0.53 0.58
measure 100 most frequent random combined
agreement
G-index
Table 3: Agreement measures for different samples
We use inter-annotator agreement to quantify the
reliability of the judgments. Apart from the simple
agreement measure, which records how often an-
notators choose the same value for an item, there
are several statistics that qualify this measure by ad-
justing for other factors. One frequently used mea-
sure, Cohen?s ?, has the disadvantage that if there
is prevalence of one answer, ? will be low (or even
negative), despite high agreement (Feinstein and Ci-
cchetti, 1990). This phenomenon, known as the ?
paradox, is a result of the formula?s adjustment for
chance agreement. As shown by Gwet (2008), the
true level of actual chance agreement is realistically
not as high as computed, resulting in the counterin-
tuitive results. We include it for comparative rea-
sons. Another statistic, the G-index (Holley and
Guilford, 1964), avoids the paradox. It assumes that
expected agreement is a function of the number of
choices rather than chance. It uses the same general
formula as ?,
(Pa ? Pe)
(1? Pe)
(5)
where Pa is the actual raw agreement measured, and
Pe is the expected agreement. The difference with
? is that Pe for the G-index is defined as Pe = 1/q,
where q is the number of available categories, in-
stead of expected chance agreement. Under most
conditions, G and ? are equivalent, but in the case
of high raw agreement and few categories, G gives a
more accurate estimation of the agreement. We thus
report raw agreement, ?, and G-index.
Despite early spammer detection, there are still
outliers in the final data, which have to be accounted
for when calculating agreement. We take the same
approach as in the statistical spammer detection and
delete outliers that are more than one standard devi-
ation below the rest of the annotators? average.
The raw agreement for both samples combined is
0.82, G = 0.58, and ? = 0.48. The numbers show
that there is reasonably high agreement on the label
accuracy.
4 Related Research
The approach we describe is similar in nature to un-
supervised verb argument selection/selectional pref-
erences and semantic role labeling, yet goes be-
yond it in several ways. For semantic role label-
ing (Gildea and Jurafsky, 2002; Fleischman et al,
2003), classes have been derived from FrameNet
(Baker et al, 1998). For verb argument detec-
tion, classes are either semi-manually derived from
a repository like WordNet, or from NE taggers
(Pardo et al, 2006; Fan et al, 2010). This allows
for domain-independent systems, but limits the ap-
proach to a fixed set of oftentimes rather inappropri-
ate classes. In contrast, we derive the level of gran-
ularity directly from the data.
Pre-tagging the data with NE classes before train-
ing comes at a cost. It lumps entities together which
can have very different classes (i.e., all people be-
come labeled as PERSON), effectively allowing only
one class per entity. Etzioni et al (2005) resolve the
problem with a web-based approach that learns hi-
erarchies of the NE classes in an unsupervised man-
ner. We do not enforce a taxonomy, but include sta-
tistical knowledge about the distribution of possible
classes over each entity by incorporating a prior dis-
tribution P (class, entity). This enables us to gen-
eralize from the lexical form without restricting our-
selves to one class per entity, which helps to bet-
ter fit the data. In addition, we can distinguish sev-
eral classes for each entity, depending on the context
1473
(e.g., winner vs. quarterback). Ritter et al (2010)
also use an unsupervised model to derive selectional
predicates from unlabeled text. They do not assign
classes altogether, but group similar predicates and
arguments into unlabeled clusters using LDA. Brody
(2007) uses a very similar methodology to establish
relations between clauses and sentences, by cluster-
ing simplified propositions.
Pen?as and Hovy (2010) employ syntactic patterns
to derive classes from unlabeled data in the context
of LbR. They consider a wider range of syntactic
structures, but do not include a probabilistic model
to label new data.
5 Conclusion
We use an unsupervised model to infer domain-
specific classes from a corpus of 1.4m unlabeled
sentences, and applied them to learn 250k propo-
sitions about American football. Unlike previous
approaches, we use automatically extracted classes
with a probability distribution over entities to al-
low for context-sensitive selection of appropriate
classes.
We evaluate both the model qualities and sensibil-
ity of the resulting propositions. Several measures
show that the model has good explanatory power and
generalizes well, significantly outperforming two
baseline approaches, especially where the possible
classes of an entity can only be inferred from the
context.
Human subjects on Amazon?s Mechanical Turk
judged up to 96.6% of the propositions for the full
data set, and 67.8% for data containing unseen enti-
ties as sensible. Inter-annotator agreement was rea-
sonably high (agreement = 0.82, G = 0.58, ? =
0.48).
The probabilistic model and the extracted propo-
sitions can be used to enrich texts and support post-
parsing inference for question answering. We are
currently applying our method to other domains.
Acknowledgements
We would like to thank David Chiang, Victoria Fos-
sum, Daniel Marcu, and Stephen Tratz, as well as the
anonymous ACL reviewers for comments and sug-
gestions to improve the paper. Research supported
in part by Air Force Contract FA8750-09-C-0172
under the DARPA Machine Reading Program.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics-Volume 1, pages 86?90. Association
for Computational Linguistics Morristown, NJ, USA.
Thorsten Brants and Alex Franz, editors. 2006. The
Google Web 1T 5-gram Corpus Version 1.1. Number
LDC2006T13. Linguistic Data Consortium, Philadel-
phia.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. In Annual Meeting-Association for Com-
putational Linguistics, volume 45, page 448.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006. Citeseer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Oren Etzioni, Michael Cafarella, Doug. Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
James Fan, David Ferrucci, David Gondek, and Aditya
Kalyanpur. 2010. Prismatic: Inducing knowledge
from a large scale lexicalized relation resource. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 122?127, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet classi-
fication. In Proceedings of EMNLP, volume 3.
Danies Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
1474
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545. Association for Computational Lin-
guistics.
Jasper Wilson Holley and Joy Paul Guilford. 1964. A
Note on the G-Index of Agreement. Educational and
Psychological Measurement, 24(4):749.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Structures.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 59?70.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. In Pro-
ceedings of the NAACL HLT 2010 First International
Workshop on Formalisms and Methodology for Learn-
ing by Reading, pages 15?23, Los Angeles, California,
June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531. Association for Computational
Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Evan Sandhaus, editor. 2008. The New York Times Anno-
tated Corpus. Number LDC2008T19. Linguistic Data
Consortium, Philadelphia.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The DARPA Machine Reading Program-Encouraging
Linguistic and Reasoning Research with a Series of
Reading Tasks. In Proceedings of LREC 2010.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697?706. ACM.
1475
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 546?551,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques
Donald Metzler
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
metzler@isi.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
hovy@isi.edu
Chunliang Zhang
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
czheng@isi.edu
Abstract
Paraphrase generation is an important task
that has received a great deal of interest re-
cently. Proposed data-driven solutions to the
problem have ranged from simple approaches
that make minimal use of NLP tools to more
complex approaches that rely on numerous
language-dependent resources. Despite all of
the attention, there have been very few direct
empirical evaluations comparing the merits of
the different approaches. This paper empiri-
cally examines the tradeoffs between simple
and sophisticated paraphrase harvesting ap-
proaches to help shed light on their strengths
and weaknesses. Our evaluation reveals that
very simple approaches fare surprisingly well
and have a number of distinct advantages, in-
cluding strong precision, good coverage, and
low redundancy.
1 Introduction
A popular idiom states that ?variety is the spice of
life?. As with life, variety also adds spice and appeal
to language. Paraphrases make it possible to express
the same meaning in an almost unbounded number
of ways. While variety prevents language from be-
ing overly rigid and boring, it also makes it difficult
to algorithmically determine if two phrases or sen-
tences express the same meaning. In an attempt to
address this problem, a great deal of recent research
has focused on identifying, generating, and harvest-
ing phrase- and sentence-level paraphrases (Barzi-
lay and McKeown, 2001; Bhagat and Ravichan-
dran, 2008; Barzilay and Lee, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Lin
and Pantel, 2001; Pang et al, 2003; Pasca and Di-
enes, 2005)
Many data-driven approaches to the paraphrase
problem have been proposed. The approaches vastly
differ in their complexity and the amount of NLP re-
sources that they rely on. At one end of the spec-
trum are approaches that generate paraphrases from
a large monolingual corpus and minimally rely on
NLP tools. Such approaches typically make use
of statistical co-occurrences, which act as a rather
crude proxy for semantics. At the other end of
the spectrum are more complex approaches that re-
quire access to bilingual parallel corpora and may
also rely on part-of-speech (POS) taggers, chunkers,
parsers, and statistical machine translation tools.
Constructing large comparable and bilingual cor-
pora is expensive and, in some cases, impossible.
Despite all of the previous research, there have
not been any evaluations comparing the quality of
simple and sophisticated data-driven approaches for
generating paraphrases. Evaluation is not only im-
portant from a practical perspective, but also from
a methodological standpoint, as well, since it is of-
ten more fruitful to devote attention to building upon
the current state-of-the-art as opposed to improv-
ing upon less effective approaches. Although the
more sophisticated approaches have garnered con-
siderably more attention from researchers, from a
practical perspective, simplicity, quality, and flexi-
bility are the most important properties. But are sim-
ple methods adequate enough for the task?
The primary goal of this paper is to take a small
step towards addressing the lack of comparative
evaluations. To achieve this goal, we empirically
546
evaluate three previously proposed paraphrase gen-
eration techniques, which range from very simple
approaches that make use of little-to-no NLP or
language-dependent resources to more sophisticated
ones that heavily rely on such resources. Our eval-
uation helps develop a better understanding of the
strengths and weaknesses of each type of approach.
The evaluation also brings to light additional proper-
ties, including the number of redundant paraphrases
generated, that future approaches and evaluations
may want to consider more carefully.
2 Related Work
Instead of exhaustively covering the entire spectrum
of previously proposed paraphrasing techniques, our
evaluation focuses on two families of data-driven ap-
proaches that are widely studied and used. More
comprehensive surveys of data-driven paraphrasing
techniques can be found in Androutsopoulos and
Malakasiotis (2010) and Madnani and Dorr (2010).
The first family of approaches that we consider
harvests paraphrases from monolingual corpora us-
ing distributional similarity. The DIRT algorithm,
proposed by Lin and Pantel (2001), uses parse tree
paths as contexts for computing distributional sim-
ilarity. In this way, two phrases were considered
similar if they occurred in similar contexts within
many sentences. Although parse tree paths serve as
rich representations, they are costly to construct and
yield sparse representations. The approach proposed
by Pasca and Dienes (2005) avoided the costs asso-
ciated with parsing by using n-gram contexts. Given
the simplicity of the approach, the authors were able
to harvest paraphrases from a very large collection
of news articles. Bhagat and Ravichandran (2008)
proposed a similar approach that used noun phrase
chunks as contexts and locality sensitive hashing
to reduce the dimensionality of the context vectors.
Despite their simplicity, such techniques are suscep-
tible to a number of issues stemming from the distri-
butional assumption. For example, such approaches
have a propensity to assign large scores to antonyms
and other semantically irrelevant phrases.
The second line of research uses comparable or
bilingual corpora as the ?pivot? that binds para-
phrases together (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Pang et al,
2003). Amongst the most effective recent work,
Bannard and Callison-Burch (2005) show how dif-
ferent English translations of the same entry in a
statistically-derived translation table can be viewed
as paraphrases. The recent work by Zhao et al
(Zhao et al, 2009) uses a generalization of DIRT-
style patterns to generate paraphrases from a bilin-
gual parallel corpus. The primary drawback of these
type of approaches is that they require a consider-
able amount of resource engineering that may not be
available for all languages, domains, or applications.
3 Experimental Evaluation
The goal of our experimental evaluation is to ana-
lyze the effectiveness of a variety of paraphrase gen-
eration techniques, ranging from simple to sophis-
ticated. Our evaluation focuses on generating para-
phrases for verb phrases, which tend to exhibit more
variation than other types of phrases. Furthermore,
our interest in paraphrase generation was initially
inspired by challenges encountered during research
related to machine reading (Barker et al, 2007). In-
formation extraction systems, which are key compo-
nent of machine reading systems, can use paraphrase
technology to automatically expand seed sets of re-
lation triggers, which are commonly verb phrases.
3.1 Systems
Our evaluation compares the effectiveness of the
following paraphrase harvesting approaches:
PD: The basic distributional similarity-inspired
approach proposed by Pasca and Dienes (2005)
that uses variable-length n-gram contexts and
overlap-based scoring. The context of a phrase
is defined as the concatenation of the n-grams
immediately to the left and right of the phrase. We
set the minimum length of an n-gram context to be
2 and the maximum length to be 3. The maximum
length of a phrase is set to 5.
BR: The distributional similarity approach proposed
by Bhagat and Ravichandran (2008) that uses noun
phrase chunks as contexts and locality sensitive
hashing to reduce the dimensionality of the contex-
tual vectors.
547
BCB-S: An extension of the Bannard Callison-
Burch (Bannard and Callison-Burch, 2005)
approach that constrains the paraphrases to have the
same syntactic type as the original phrase (Callison-
Burch, 2008). We constrained all paraphrases to be
verb phrases.
We chose these three particular systems because
they span the spectrum of paraphrase approaches, in
that the PD approach is simple and does not rely on
any NLP resources while the BCB-S approach is so-
phisticated and makes heavy use of NLP resources.
For the two distributional similarity approaches
(PD and BR), paraphrases were harvested from the
English Gigaword Fourth Edition corpus and scored
using the cosine similarity between PMI weighted
contextual vectors. For the BCB-S approach, we
made use of a publicly available implementation1.
3.2 Evaluation Methodology
We randomly sampled 50 verb phrases from 1000
news articles about terrorism and another 50 verb
phrases from 500 news articles about American
football. Individual occurrences of verb phrases
were sampled, which means that more common verb
phrases were more likely to be selected and that a
given phrase could be selected multiple times. This
sampling strategy was used to evaluate the systems
across a realistic sample of phrases. To obtain a
richer class of phrases beyond basic verb groups, we
defined verb phrases to be contiguous sequences of
tokens that matched the following POS tag pattern:
(TO | IN | RB | MD | VB)+.
Following the methodology used in previous
paraphrase evaluations (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Kok and Brock-
ett, 2010), we presented annotators with two sen-
tences. The first sentence was randomly selected
from amongst all of the sentences in the evaluation
corpus that contain the original phrase. The second
sentence was the same as the first, except the orig-
inal phrase is replaced with the system generated
paraphrase. Annotators were given the following
options, which were adopted from those described
by Kok and Brockett (2010), for each sentence pair:
0) Different meaning; 1) Same meaning; revised is
1Available at http://www.cs.jhu.edu/?ccb/.
grammatically incorrect; and 2) Same meaning; re-
vised is grammatically correct. Table 1 shows three
example sentence pairs and their corresponding an-
notations according to the guidelines just described.
Amazon?s Mechanical Turk service was used to
collect crowdsourced annotations. For each para-
phrase system, we retrieve (up to) 10 paraphrases
for each phrase in the evaluation set. This yields
a total of 6,465 unique (phrase, paraphrase) pairs
after pooling results from all systems. Each Me-
chanical Turk HIT consisted of 12 sentence pairs.
To ensure high quality annotations and help iden-
tify spammers, 2 of the 12 sentence pairs per HIT
were actually ?hidden tests? for which the correct
answer was known by us. We automatically rejected
any HITs where the worker failed either of these hid-
den tests. We also rejected all work from annotators
who failed at least 25% of their hidden tests. We
collected a total of 51,680 annotations. We rejected
65% of the annotations based on the hidden test fil-
tering just described, leaving 18,150 annotations for
our evaluation. Each sentence pair received a mini-
mum of 1, a median of 3, and maximum of 6 anno-
tations. The raw agreement of the annotators (after
filtering) was 77% and the Fleiss? Kappa was 0.43,
which signifies moderate agreement (Fleiss, 1971;
Landis and Koch, 1977).
The systems were evaluated in terms of coverage
and expected precision at k. Coverage is defined
as the percentage of phrases for which the system
returned at least one paraphrase. Expected precision
at k is the expected number of correct paraphrases
amongst the top k returned, and is computed as:
E[p@k] =
1
k
k?
i=1
pi
where pi is the proportion of positive annotations
for item i. When computing the mean expected
precision over a set of input phrases, only those
phrases that generate one or more paraphrases is
considered in the mean. Hence, if precision were
to be averaged over all 100 phrases, then systems
with poor coverage would perform significantly
worse. Thus, one should take a holistic view of the
results, rather than focus on coverage or precision
in isolation, but consider them, and their respective
tradeoffs, together.
548
Sentence Pair Annotation
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was officially announced in Aden.
0
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was cancelled in Aden.
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem to
protest against the killing of Sharif.
1
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem in
protest of against the killing of Sharif.
It says that foreign companies have no greater right to compensation ? establishing debts at a
1/1 ratio of the dollar to the peso ? than Argentine citizens do.
2
It says that foreign companies have no greater right to compensation ? setting debts at a 1/1
ratio of the dollar to the peso ? than Argentine citizens do.
Table 1: Example annotated sentence pairs. In each pair, the first sentence is the original and the second has a system-
generated paraphrase filled in (denoted by the bold text).
Method C
Lenient Strict
P1 P5 P10 P1 P5 P10
PD 86 .48 .42 .36 .25 .22 .19
BR 84 .83 .65 .52 .16 .17 .15
BCB-S 62 .63 .45 .34 .22 .17 .13
Table 2: Coverage (C) and expected precision at k (Pk)
under lenient and strict evaluation criteria.
Two binarized evaluation criteria are reported.
The lenient criterion allows for grammatical er-
rors in the paraphrased sentence, while the strict
criterion does not.
3.3 Basic Results
Table 2 summarizes the results of our evaluation.
For this evaluation, all 100 verb phrases were run
through each system. The paraphrases returned by
the systems were then ranked (ordered) in descend-
ing order of their score, thus placing the highest
scoring item at rank 1. Bolded values represent the
best result for a given metric.
As expected, the results show that the systems
perform significantly worse under the strict evalu-
ation criteria, which requires the paraphrased sen-
tences to be grammatically correct. None of the ap-
proaches tested used any information from the eval-
uation sentences (other than the fact a verb phrase
was to be filled in). Recent work showed that us-
ing language models and/or syntactic clues from the
evaluation sentence can improve the grammatical-
ity of the paraphrased sentences (Callison-Burch,
Method
Lenient Strict
P1 P5 P10 P1 P5 P10
PD .26 .22 .20 .19 .16 .15
BR .05 .10 .11 .04 .05 .05
BCB-S .24 .25 .20 .17 .14 .10
Table 3: Expected precision at k (Pk) when considering
redundancy under lenient and strict evaluation criteria.
2008). Such approaches could likely be used to im-
prove the quality of all of the approaches under the
strict evaluation criteria.
In terms of coverage, the distributional similarity
approaches performed the best. In another set of ex-
periments, we used the PD method to harvest para-
phrases from a large Web corpus, and found that the
coverage was 98%. Achieving similar coverage with
resource-dependent approaches would likely require
more human and machine effort.
3.4 Redundancy
After manually inspecting the results returned by the
various paraphrase systems, we noticed that some
approaches returned highly redundant paraphrases
that were of limited practical use. For example,
for the phrase ?were losing?, the BR system re-
turned ?are losing?, ?have been losing?, ?have lost?,
?lose?, ?might lose?, ?had lost?, ?stand to lose?,
?who have lost? and ?would lose? within the top 10
paraphrases. All of these are simple variants that
contain different forms of the verb ?lose?. Under
the lenient evaluation criterion almost all of these
paraphrases would be marked as correct, since the
549
same verb is being returned with some grammati-
cal modifications. While highly redundant output
of this form may be useful for some tasks, for oth-
ers (such as information extraction) is it more useful
to identify paraphrases that contain a diverse, non-
redundant set of verbs.
Therefore, we carried out another evaluation
aimed at penalizing highly redundant outputs. For
each approach, we manually identified all of the
paraphrases that contained the same verb as the
main verb in the original phrase. During evalua-
tion, these ?redundant? paraphrases were regarded
as non-related.
The results from this experiment are provided in
Table 3. The results are dramatically different com-
pared to those in Table 2, suggesting that evaluations
that do not consider this type of redundancy may
over-estimate actual system quality. The percent-
age of results marked as redundant for the BCB-S,
BR, and PD approaches were 22.6%, 52.5%, and
22.9%, respectively. Thus, the BR system, which
appeared to have excellent (lenient) precision in our
initial evaluation, returns a very large number of re-
dundant paraphrases. This remarkably reduces the
lenient P1 from 0.83 in our initial evaluation to just
0.05 in our redundancy-based evaluation. The BCB-
S and PD approaches return a comparable number of
redundant results. As with our previous evaluation,
the BCB-S approach tends to perform better under
the lenient evaluation, while PD is better under the
strict evaluation. Estimated 95% confidence inter-
vals show all differences between BCB-S and PD
are statistically significant, except for lenient P10.
Of course, existing paraphrasing approaches do
not explicitly account for redundancy, and hence this
evaluation is not completely fair. However, these
findings suggest that redundancy may be an impor-
tant issue to consider when developing and evalu-
ating data-driven paraphrase approaches. There are
likely other characteristics, beyond redundancy, that
may also be important for developing robust, effec-
tive paraphrasing techniques. Exploring the space
of such characteristics in a task-dependent manner
is an important direction of future work.
3.5 Discussion
In all of our evaluations, we found that the simple
approaches are surprisingly effective in terms of pre-
cision, coverage, and redundancy, making them a
reasonable choice for an ?out of the box? approach
for this particular task. However, additional task-
dependent comparative evaluations are necessary to
develop even deeper insights into the pros and cons
of the different types of approaches.
From a high level perspective, it is also important
to note that the precision of these widely used, com-
monly studied paraphrase generation approaches is
still extremely poor. After accounting for redun-
dancy, the best approaches achieve a precision at 1
of less than 20% using the strict criteria and less than
26% when using the lenient criteria. This suggests
that there is still substantial work left to be done be-
fore the output of these systems can reliably be used
to support other tasks.
4 Conclusions and Future Work
This paper examined the tradeoffs between simple
paraphrasing approaches that do not make use of any
NLP resources and more sophisticated approaches
that use a variety of such resources. Our evaluation
demonstrated that simple harvesting approaches fare
well against more sophisticated approaches, achiev-
ing state-of-the-art precision, good coverage, and
relatively low redundancy.
In the future, we would like to see more em-
pirical evaluations and detailed studies comparing
the practical merits of various paraphrase genera-
tion techniques. As Madnani and Dorr (Madnani
and Dorr, 2010) suggested, it would be beneficial
to the research community to develop a standard,
shared evaluation that would act to catalyze further
advances and encourage more meaningful compara-
tive evaluations of such approaches moving forward.
Acknowledgments
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-3705. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. We would
also like to thank the anonymous reviewers for their
valuable feedback and the Mechanical Turk workers
for their efforts.
550
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, James
Fan, Noah Friedland, Michael Glass, Jerry Hobbs,
Eduard Hovy, David Israel, Doo Soon Kim, Rutu
Mulkar-Mehta, Sourabh Patwardhan, Bruce Porter,
Dan Tecuci, and Peter Yeh. 2007. Learning by read-
ing: a prototype system, performance baseline and
lessons learned. In Proceedings of the 22nd national
conference on Artificial intelligence - Volume 1, pages
280?286. AAAI Press.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 16?
23, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 50?57,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?08, pages
196?205, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Joseph L. Fleiss. 1971. Measuring Nominal Scale
Agreement Among Many Raters. Psychological Bul-
letin, 76(5):378?382.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 145?153, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36:341?387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1, NAACL ?03, pages 102?109, Morristown,
NJ, USA. Association for Computational Linguistics.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(Special Issue 04):503?526.
551
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 280?284,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Better Rule Extraction with Translation Span Alignment 
Jingbo Zhu    Tong Xiao   Chunliang Zhang 
Natural Language Processing Laboratory 
Northeastern University, Shenyang, China 
{zhujingbo,xiaotong,zhangcl}@mail.neu.edu.cn 
 
 
 
 
Abstract 
This paper presents an unsupervised ap-
proach to learning translation span align-
ments from parallel data that improves 
syntactic rule extraction by deleting spuri-
ous word alignment links and adding new 
valuable links based on bilingual transla-
tion span correspondences. Experiments on 
Chinese-English translation demonstrate 
improvements over standard methods for 
tree-to-string and tree-to-tree translation.  
1 Introduction 
Most syntax-based statistical machine translation 
(SMT) systems typically utilize word alignments 
and parse trees on the source/target side to learn 
syntactic transformation rules from parallel data. 
The approach suffers from a practical problem that 
even one spurious (word alignment) link can pre-
vent some desirable syntactic translation rules from 
extraction, which can in turn affect the quality of 
translation rules and translation performance (May 
and Knight 2007; Fossum et al 2008). To address 
this challenge, a considerable amount of previous 
research has been done to improve alignment qual-
ity by incorporating some statistics and linguistic 
heuristics or syntactic information into word 
alignments (Cherry and Lin 2006; DeNero and 
Klein 2007; May and Knight 2007; Fossum et al 
2008; Hermjakob 2009; Liu et al 2010).  
Unlike their efforts, this paper presents a simple 
approach that automatically builds the translation 
span alignment (TSA) of a sentence pair by utiliz-
ing a phrase-based forced decoding technique, and 
then improves syntactic rule extraction by deleting 
spurious links and adding new valuable links based 
on bilingual translation span correspondences. The 
proposed approach has two promising properties.  
S
VP
ADVPNNS
imports
VBZ
have
DT
VBNRB
fallendrasticallythe
??
jianshao
???
dafudu
??
jinkou
?
le
NN
VV AS
AD VP
VP
S
NP Frontier node
Word alignment
 
Figure 1. A real example of Chinese-English sentence 
pair with word alignment and both-side parse trees.  
 
Some blocked Tree-to-string Rules: 
r1: AS(?) ? have 
r2: NN(??) ? the imports 
r3: S (NN:x1 VP:x2) ? x1 x2
Some blocked Tree-to-tree Rules: 
r4: AS(?) ? VBZ(have) 
r5: NN(??) ? NP(DT(the) NNS(imports)) 
r6: S(NN:x1 VP:x2) ? S(NP:x1 VP:x2) 
r7: VP(AD:x1 VP(VV:x2 AS:x3)) 
            ? VP(VBZ:x3 ADVP(RB:x1 VBN:x2)) 
Table 1. Some useful syntactic rules are blocked due to 
the spurious link between ??? and ?the?.  
 
Firstly, The TSAs are constructed in an unsuper-
vised learning manner, and optimized by the trans-
lation model during the forced decoding process, 
without using any statistics and linguistic heuristics 
or syntactic constraints. Secondly, our approach is 
independent of the word alignment-based algo-
rithm used to extract translation rules, and easy to 
implement. 
2 Translation Span Alignment Model 
Different from word alignment, TSA is a process 
of identifying span-to-span alignments between 
parallel sentences. For each translation span pair,  
280
1. Extract phrase translation rules R from the parallel 
corpus with word alignment, and construct a phrase-
based translation model M.  
2. Apply M to implement phrase-based forced decoding 
on each training sentence pair (c, e), and output its 
best derivation d* that can transform c into e.  
3. Build a TSA of each sentence pair (c, e) from its best 
derivation d*, in which each rule r in d* is used to 
form a translation span pair {src(r)<=>tgt(r)}.  
Figure 2. TSA generation algorithm. src(r) and tgt(r) 
indicate the source and target side of rule r.  
 
its source (or target) span is a sequence of source 
(or target) words. Given a source sentence c=c1...cn, 
a target sentence e=e1...em, and its word alignment 
A, a translation span pair ? is a pair of source span 
(ci...cj) and target span (ep...eq)  
)( qp
j
i ec ?=?  
where ? indicates that the source span (ci...cj) and 
the target span (ep...eq) are translational equivalent. 
We do not require that ? must be consistent with 
the associated word alignment A in a TSA model.  
Figure 2 depicts the TSA generation algorithm 
in which a phrase-based forced decoding tech-
nique is adopted to produce the TSA of each sen-
tence pair. In this work, we do not apply syntax-
based forced decoding (e.g., tree-to-string) because 
phrase-based models can achieve the state-of-the-
art translation quality with a large amount of train-
ing data, and are not limited by any constituent 
boundary based constraints for decoding.  
Formally, given a sentence pair (c, e), the 
phrase-based forced decoding technique aims to 
search for the best derivation d* among all consis-
tent derivations that convert the given source sen-
tence c into the given target sentence e with respect 
to the current translation model induced from the 
training data, which can be expressed by 
)|)((Prmaxarg
)(),(
* cdTGTd
edTGTecDd
?=??
=          (1) 
where D(c,e) is the set of candidate derivations that 
transform c to e, and TGT(d) is a function that out-
puts the yield of a derivation d. ? indicates parame-
ters of the phrase-based translation model learned 
from the parallel corpus.  
The best derivation d* produced by forced de-
coding can be viewed as a sequence of translation 
steps (i.e., phrase translation rules), expressed by 
krrrd ???= ...* 21 , 
c = ?? ??? ?? ? 
e =  the imports have drastically fallen 
The best derivation d* produced by forced decoding: 
r1: ?? ? the imports 
r2: ??? ?? ? drastically fallen 
r3: ? ? have 
Generating TSA from d*: 
[??]<=>[the imports]  
[??? ??]<=>[drastically fallen]   
[?]<=>[have] 
Table 2. Forced decoding based TSA generation on the 
example sentence pair in Fig. 1. 
 
where ri indicates a phrase rule used to form d*. 
?is a composition operation that combines rules 
{r1...rk} together to produce the target translation.  
As mentioned above, the best derivation d* re-
spects the input sentence pair (c, e). It means that 
for each phrase translation rule ri used by d*, its 
source (or target) side exactly matches a span of 
the given source (or target) sentence. The source 
side src(ri) and the target side tgt(ri) of each phrase 
translation rule ri in d* form a translation span pair 
{src(ri)<=>tgt(ri)} of (c,e). In other words, the 
TSA of (c,e) is a set of translation span pairs gen-
erated from phrase translation rules used by the 
best derivation d*. The forced decoding based TSA 
generation on the example sentence pair in Figure 
1 can be shown in Table 2. 
3 Better Rule Extraction with TSAs 
To better understand the particular task that we 
will address in this section, we first introduce a 
definition of inconsistent with a translation span 
alignment. Given a sentence pair (c, e) with the 
word alignment A and the translation span align-
ment P, we call a link (ci, ej)?A inconsistent with 
P, if  ci and ej are covered respectively by two dif-
ferent translation span pairs in P and vice versa. 
(ci, ej)?A inconsistent with P  ?
)()(:  
)()(:       
???
???
tgtesrccPOR
tgtesrccP
ji
ji
?????
?????
 
where src(?) and tgt(?) indicate the source and tar-
get span of a translation span pair ?.  
By this, we will say that a link (ci, ej)?A is a 
spurious link if it is inconsistent with the given 
TSA. Table 3 shows that an original link (4?1) 
are covered by two different translation span pairs  
281
Source Target WA TSA 
1: ?? 1: the 1?2 [1,1]<=>[1,2] 
2: ??? 2: imports 2?4 [2,3]<=>[4,5] 
3: ?? 3: have 3?5 [4,4]<=>[3,3] 
4: ? 4: drastically 4?1  
 5: fallen (null)?3  
Table 3. A sentence pair with the original word align-
ment (WA) and the translation span alignment (TSA).  
 
([4,4]<=>[3,3]) and ([1,1] <=>[1,2]), respectively. 
In such a case, we think that this link (4?1) is a 
spurious link according to this TSA, and should be 
removed for rule extraction.   
Given a resulting TSA P, there are four different 
types of translation span pairs, such as one-to-one, 
one-to-many, many-to-one, and many-to-many 
cases. For example, the TSA shown in Table 3 
contains a one-to-one span pair ([4,4]<=>[3,3]), a 
one-to-many span pair ([1,1]<=>[1,2]) and a 
many-many span pair ([2,3]<=>[4,5]). In such a 
case, we can learn a confident link from a one-to-
one translation span pair that is preferred by the 
translation model in the forced decoding based 
TSA generation approach. If such a confident link 
does not exist in the original word alignment, we 
consider it as a new valuable link.  
Until now, a natural way is to use TSAs to di-
rectly improve word alignment quality by deleting 
some spurious links and adding some new confi-
dent links, which in turn improves rule quality and 
translation quality. In other words, if a desirable 
translation rule was blocked due to some spurious 
links, we will output this translation rule. Let?s 
revisit the example in Figure 1 again. The blocked 
tree-to-string r3 can be extracted successfully after 
deleting the spurious link (?, the), and a new tree-
to-string rule r1 can be extracted after adding a new 
confident link (?, have) that is inferred from a 
one-to-one translation span pair [4,4]<=>[3,3].  
4 Experiments 
4.1 Setup 
We utilized a state-of-the-art open-source SMT 
system NiuTrans (Xiao et al 2012) to implement 
syntax-based models in the following experiments. 
We begin with a training parallel corpus of Chi-
nese-English bitexts that consists of 8.8M Chinese 
words and 10.1M English words in 350K sentence 
pairs. The GIZA++ tool was used to perform the  
Method Prec% Rec% F1% Del/Sent Add/Sent
Baseline 83.07 75.75 79.25 - - 
TSA 84.01 75.46 79.51 1.5 1.1 
Table 4. Word alignment precision, recall and F1-score 
of various methods on 200 sentence pairs of Chinese-
English data. 
 
bi-directional word alignment between the source 
and the target sentences, referred to as the baseline 
method. For syntactic translation rule extraction, 
minimal GHKM (Galley et al, 2004) rules are first 
extracted from the bilingual corpus whose source 
and target sides are parsed using the Berkeley 
parser (Petrov et al 2006). The composed rules are 
then generated by composing two or three minimal 
rules. A 5-gram language model was trained on the 
Xinhua portion of English Gigaword corpus. Beam 
search and cube pruning techniques (Huang and 
Chiang 2007) were used to prune the search space 
for all the systems. The base feature set used for all 
systems is similar to that used in (Marcu et al 
2006), including 14 base features in total such as 5-
gram language model, bidirectional lexical and 
phrase-based translation probabilities. All features 
were log-linearly combined and their weights were 
optimized by performing minimum error rate train-
ing (MERT) (Och 2003). The development data set 
used for weight training comes from NIST MT03 
evaluation set, consisting of 326 sentence pairs of 
less than 20 words in each Chinese sentence. Two 
test sets are NIST MT04 (1788 sentence pairs) and 
MT05 (1082 sentence pairs) evaluation sets. The 
translation quality is evaluated in terms of the case-
insensitive IBM-BLEU4 metric.  
4.2 Effect on Word Alignment 
To investigate the effect of the TSA method on 
word alignment, we designed an experiment to 
evaluate alignment quality against gold standard 
annotations. There are 200 random chosen and 
manually aligned Chinese-English sentence pairs 
used to assert the word alignment quality. For 
word alignment evaluation, we calculated precision, 
recall and F1-score over gold word alignment.  
Table 4 depicts word alignment performance of 
the baseline and TSA methods. We apply the TSAs 
to refine the baseline word alignments, involving 
spurious link deletion and new link insertion op-
erations. Table 4 shows our method can yield im-
provements on precision and F1-score, only 
causing a little negative effect on recall.  
282
4.3 Translation Quality 
Method # of Rules MT03 MT04 MT05 
Baseline (T2S) 33,769,071 34.10 32.55 30.15 
TSA (T2S) 32,652,261 
34.61+
(+0.51) 
33.01+
(+0.46)
30.66+
(+0.51)
     
Baseline (T2T) 24,287,206 34.51 32.20 31.78 
TSA (T2T) 24,119,719 
34.85 
(+0.34) 
32.92*
(+0.72)
32.22+ 
(+0.44)
Table 5. Rule sizes and IBM-BLEU4 (%) scores of 
baseline and our method (TSA) in tree-to-string (T2S) 
and tree-to-tree (T2T) translation on Dev set (MT03) 
and two test sets (MT04 and MT05). + and * indicate 
significantly better on performance comparison at p<.05 
and p<.01, respectively.  
 
Table 5 depicts effectiveness of our TSA method 
on translation quality in tree-to-string and tree-to-
tree translation tasks. Table 5 shows that our TSA 
method can improve both syntax-based translation 
systems. As mentioned before, the resulting TSAs 
are essentially optimized by the translation model. 
Based on such TSAs, experiments show that spuri-
ous link deletion and new valuable link insertion 
can improve translation quality for tree-to-string 
and tree-to-tree systems.  
5 Related Work 
Previous studies have made great efforts to incor-
porate statistics and linguistic heuristics or syntac-
tic information into word alignments (Ittycheriah 
and Roukos 2005; Taskar et al 2005; Moore et al 
2006; Cherry and Lin 2006; DeNero and Klein 
2007; May and Knight 2007; Fossum et al 2008; 
Hermjakob 2009; Liu et al 2010). For example, 
Fossum et al (2008) used a discriminatively 
trained model to identify and delete incorrect links 
from original word alignments to improve string-
to-tree transformation rule extraction, which incor-
porates four types of features such as lexical and 
syntactic features. This paper presents an approach 
to incorporating translation span alignments into 
word alignments to delete spurious links and add 
new valuable links.  
Some previous work directly models the syntac-
tic correspondence in the training data for syntactic 
rule extraction (Imamura 2001; Groves et al 2004; 
Tinsley et al 2007; Sun et al 2010a, 2010b; Pauls 
et al 2010). Some previous methods infer syntac-
tic correspondences between the source and the 
target languages through word alignments and con-
stituent boundary based syntactic constraints. Such 
a syntactic alignment method is sensitive to word 
alignment behavior. To combat this, Pauls et al 
(2010) presented an unsupervised ITG alignment 
model that directly aligns syntactic structures for 
string-to-tree transformation rule extraction. One 
major problem with syntactic structure alignment 
is that syntactic divergence between languages can 
prevent accurate syntactic alignments between the 
source and target languages.  
May and Knight (2007) presented a syntactic re-
alignment model for syntax-based MT that uses 
syntactic constraints to re-align a parallel corpus 
with word alignments. The motivation behind their 
methods is similar to ours. Our work differs from 
(May and Knight 2007) in two major respects. 
First, the approach proposed by May and Knight 
(2007) first utilizes the EM algorithm to obtain 
Viterbi derivation trees from derivation forests of 
each (tree, string) pair, and then produces Viterbi 
alignments based on obtained derivation trees. Our 
forced decoding based approach searches for the 
best derivation to produce translation span align-
ments that are used to improve the extraction of 
translation rules. Translation span alignments are 
optimized by the translation model. Secondly, their 
models are only applicable for syntax-based sys-
tems while our method can be applied to both 
phrase-based and syntax-based translation tasks.  
6 Conclusion 
This paper presents an unsupervised approach to 
improving syntactic transformation rule extraction 
by deleting spurious links and adding new valuable 
links with the help of bilingual translation span 
alignments that are built by using a phrase-based 
forced decoding technique. In our future work, it is 
worth studying how to combine the best of our ap-
proach and discriminative word alignment models 
to improve rule extraction for SMT models.  
Acknowledgments 
This research was supported in part by the National 
Science Foundation of China (61073140), the Spe-
cialized Research Fund for the Doctoral Program 
of Higher Education (20100042110031) and the 
Fundamental Research Funds for the Central Uni-
versities in China. 
283
References  
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative 
training. In Proc. of ACL. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Proc. 
of the Third Workshop on Statistical Machine Trans-
lation, pages 44-52. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What's in a translation rule? In Proc. of 
HLT-NAACL 2004, pp273-280. 
Declan Groves, Mary Hearne and Andy Way. 2004. 
Robust sub-sentential alignment of phrase-structure 
trees. In Proc. of COLING, pp1072-1078. 
Ulf Hermjakob. 2009. Improved word alignment with 
statistics and linguistic heuristics. In Proc. of EMNLP, 
pp229-237 
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. In 
Proc. of ACL, pp144-151. 
Kenji Imamura. 2001. Hierarchical Phrase Alignment 
Harmonized with Parsing. In Proc. of NLPRS, 
pp377-384. 
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for Arabic-English ma-
chine translation. In Proc. of HLT/EMNLP. 
Yang Liu, Qun Liu and Shouxun Lin. 2010. Discrimina-
tive word alignment by linear modeling. Computa-
tional Linguistics, 36(3):303-339 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP, pp44-52. 
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc. of 
EMNLP-CoNLL.  
Robert C. Moore, Wen-tau Yih and Andreas Bode. 2006. 
Improved discriminative bilingual word alignment. 
In Proc. of ACL 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL. 
Adam Pauls, Dan Klein, David Chiang and Kevin 
Knight. 2010. Unsupervised syntactic alignment with 
inversion transduction grammars. In Proc. of NAACL, 
pp118-126 
Slav Petrov, Leon Barrett, Roman Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL, pp433-440. 
Jun Sun, Min Zhang and Chew Lim Tan. 2010a. Explor-
ing Syntactic Structural Features for Sub-Tree 
Alignment Using Bilingual Tree Kernels. In Proc. of 
ACL, pp306-315. 
Jun Sun, Min Zhang and Chew Lim Tan. 2010b. Dis-
criminative Induction of Sub-Tree Alignment using 
Limited Labeled Data. In Proc. of COLING, pp1047-
1055. 
Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005. 
A discriminative matching approach to word align-
ment. In Proc. of HLT/EMNLP 
John Tinsley, Ventsislav Zhechev, Mary Hearne and 
Andy Way. 2007. Robust language pair-independent 
sub-tree alignment. In Proc. of MT Summit XI. 
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 2012. 
NiuTrans: An Open Source Toolkit for Phrase-based 
and Syntax-based Machine Translation. In Proceed-
ings of ACL, demonstration session 
284
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 563?568,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Hybrid Approach to Skeleton-based Translation
Tong Xiao??, Jingbo Zhu??, Chunliang Zhang??
? Northeastern University, Shenyang 110819, China
? Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou 310012, China
{xiaotong,zhujingbo,zhangcl}@mail.neu.edu.cn
Abstract
In this paper we explicitly consider sen-
tence skeleton information for Machine
Translation (MT). The basic idea is that
we translate the key elements of the input
sentence using a skeleton translation mod-
el, and then cover the remain segments us-
ing a full translation model. We apply our
approach to a state-of-the-art phrase-based
system and demonstrate very promising
BLEU improvements and TER reductions
on the NIST Chinese-English MT evalua-
tion data.
1 Introduction
Current Statistical Machine Translation (SMT) ap-
proaches model the translation problem as a pro-
cess of generating a derivation of atomic transla-
tion units, assuming that every unit is drawn out
of the same model. The simplest of these is the
phrase-based approach (Och et al, 1999; Koehn
et al, 2003) which employs a global model to
process any sub-strings of the input sentence. In
this way, all we need is to increasingly translate
a sequence of source words each time until the
entire sentence is covered. Despite good result-
s in many tasks, such a method ignores the roles
of each source word and is somewhat differen-
t from the way used by translators. For exam-
ple, an important-first strategy is generally adopt-
ed in human translation - we translate the key ele-
ments/structures (or skeleton) of the sentence first,
and then translate the remaining parts. This es-
pecially makes sense for some languages, such as
Chinese, where complex structures are usually in-
volved.
Note that the source-language structural infor-
mation has been intensively investigated in recent
studies of syntactic translation models. Some of
them developed syntax-based models on complete
syntactic trees with Treebank annotations (Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008),
and others used source-language syntax as soft
constraints (Marton and Resnik, 2008; Chiang,
2010). However, these approaches suffer from
the same problem as the phrase-based counterpart
and use the single global model to handle differ-
ent translation units, no matter they are from the
skeleton of the input tree/sentence or other not-so-
important sub-structures.
In this paper we instead explicitly model the
translation problem with sentence skeleton infor-
mation. In particular,
? We develop a skeleton-based model which
divides translation into two sub-models: a
skeleton translation model (i.e., translating
the key elements) and a full translation model
(i.e., translating the remaining source words
and generating the complete translation).
? We develop a skeletal language model to de-
scribe the possibility of translation skeleton
and handle some of the long-distance word
dependencies.
? We apply the proposed model to Chinese-
English phrase-based MT and demonstrate
promising BLEU improvements and TER re-
ductions on the NIST evaluation data.
2 A Skeleton-based Approach to MT
2.1 Skeleton Identification
The first issue that arises is how to identify the
skeleton for a given source sentence. Many ways
are available. E.g., we can start with a full syntac-
tic tree and transform it into a simpler form (e.g.,
removing a sub-tree). Here we choose a simple
and straightforward method: a skeleton is obtained
by dropping all unimportant words in the origi-
nal sentence, while preserving the grammaticali-
ty. See the following for an example skeleton of a
Chinese sentence.
563
Original Sentence (subscripts represent indices):
z
[1]
per
?
[2]
ton
?Yz
[3]
seawater desalination
?n
[4]
treatment

[5]
of
?
[6]
the cost
3
[7]
5
[8]
5

[9]
yuan

[10]
of
?:
[11]
from
?
[12]
???
[13]
has been further
e?
[14]
reduced
"
[15]
.
(The cost of seawater desalination treatment has
been further reduced from 5 yuan per ton.)
Sentence Skeleton (subscripts represent indices):
?
[6]
the cost
???
[13]
has been further
e?
[14]
reduced
"
[15]
.
(The cost has been further reduced.)
Obviously the skeleton used in this work can be
viewed as a simplified sentence. Thus the prob-
lem is in principle the same as sentence simpli-
fication/compression. The motivations of defin-
ing the problem in this way are two-fold. First,
as the skeleton is a well-formed (but simple) sen-
tence, all current MT approaches are applicable
to the skeleton translation problem. Second, ob-
taining simplified sentences by word deletion is
a well-studied issue (Knight and Marcu, 2000;
Clarke and Lapata, 2006; Galley and McKeown,
2007; Cohn and Lapata, 2008; Yamangil and
Shieber, 2010; Yoshikawa et al, 2012). Many
good sentence simpliciation/compression methods
are available to our work. Due to the lack of space,
we do not go deep into this problem. In Section
3.1 we describe the corpus and system employed
for automatic generation of sentence skeletons.
2.2 Base Model
Next we describe our approach to integrating
skeleton information into MT models. We start
with an assumption that the 1-best skeleton is pro-
vided by the skeleton identification system. Then
we define skeleton-based translation as a task of
searching for the best target string
?
t given the
source string and its skeleton ? :
?
t = argmax
t
P(t|?, s) (1)
As is standard in SMT, we further assume that
1) the translation process can be decomposed in-
to a derivation of phrase-pairs (for phrase-based
models) or translation rules (for syntax-based
models); 2) and a linear function g(?) is used to
assign a model score to each derivation. Let d
s,?,t
(or d for short) denote a translation derivation. The
above problem can be redefined in a Viterbi fash-
ion - we find the derivation
?
dwith the highest mod-
el score given s and ? :
?
d = argmax
d
g(d) (2)
In this way, the MT output can be regarded as the
target-string encoded in
?
d.
To compute g(d), we use a linear combination
of a skeleton translation model g
skel
(d) and a full
translation model g
full
(d):
g(d) = g
skel
(d) + g
full
(d) (3)
where the skeleton translation model handles the
translation of the sentence skeleton, while the full
translation model is the baseline model and han-
dles the original problem of translating the whole
sentence. The motivation here is straightforward:
we use an additional score g
skel
(d) to model the
problem of skeleton translation and interpolate it
with the baseline model. See Figure 1 for an exam-
ple of applying the above model to phrase-based
MT. In the figure, each source phrase is translated
into a target phrase, which is represented by linked
rectangles. The skeleton translation model focus-
es on the translation of the sentence skeleton, i.e.,
the solid (red) rectangles; while the full transla-
tion model computes the model score for all those
phrase-pairs, i.e., all solid and dashed rectangles.
Another note on the model. Eq. (3) provides a
very flexible way for model selection. While we
will restrict ourself to phrase-based translation in
the following description and experiments, we can
choose different models/features for g
skel
(d) and
g
full
(d). E.g., one may introduce syntactic fea-
tures into g
skel
(d) due to their good ability in cap-
turing structural information; and employ a stan-
dard phrase-based model for g
full
(d) in which not
all segments of the sentence need to respect syn-
tactic constraints.
2.3 Model Score Computation
In this work both the skeleton translation model
g
skel
(d) and full translation model g
full
(d) resem-
ble the usual forms used in phrase-based MT, i.e.,
the model score is computed by a linear combina-
tion of a group of phrase-based features and lan-
guage models. In phrase-based MT, the transla-
tion problem is modeled by a derivation of phrase-
pairs. Given a translation model m, a language
model lm and a vector of feature weights w, the
model score of a derivation d is computed by
564
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost
p
h
r
a
s
e
1
p
1
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
) + w
?
lm
? lm
?
(?the cost?)
g(d;w,m, lm) = w
m
? f
m
(p
1
) + w
lm
? lm(?the cost?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment
p
h
r
a
s
e
s
2
&
3
p
1
p
2
p
3
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
) + w
?
lm
? lm
?
(?the cost X?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? p
3
) + w
lm
? lm(?the cost of seawater desalination treatment?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment has been further reduced
p
h
r
a
s
e
s
4
&
5
p
1
p
2
p
3
p
4
p
5
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
? p
4
? p
5
)+
w
?
lm
? lm
?
(?the cost X has been further reduced?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? ... ? p
5
) + w
lm
? lm(?the cost of seawater ... further reduced?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment has been further reduced from
5 yuan
per ton
.
p
h
r
a
s
e
s
6
-
9
p
1
p
2
p
3
p
4
p
5
p
6
p
7
p
8
p
9
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
? p
4
? p
5
? p
9
)+
w
?
lm
? lm
?
(?the cost X has been further reduced X .?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? ... ? p
9
) + w
lm
? lm(?the cost of seawater ... per ton .?)
Figure 1: Example derivation and model scores for a sentence in LDC2006E38. The solid (red) rect-
angles represent the sentence skeleton, and the dashed (blue) rectangles represent the non-skeleton seg-
ments. X represents a slot in the translation skeleton. ? represents composition of phrase-pairs.
g(d;w,m, lm) = w
m
? f
m
(d)+w
lm
? lm(d) (4)
where f
m
(d) is a vector of feature values defined
on d, and w
m
is the corresponding weight vector.
lm(d) andw
lm
are the score and weight of the lan-
guage model, respectively.
To ease modeling, we only consider skeleton-
consistent derivations in this work. A deriva-
tion d is skeleton-consistent if no phrases in d
cross skeleton boundaries (e.g., a phrase where t-
wo of the source words are in the skeleton and
one is outside). Obviously, from any skeleton-
consistent derivation d we can extract a skeleton
derivation d
?
which covers the sentence skeleton
exactly. For example, in Figure 1, the deriva-
tion of phrase-pairs {p
1
, p
2
, ..., p
9
} is skeleton-
consistent, and the skeleton derivation is formed
by {p
1
, p
4
, p
5
, p
9
}.
Then, we can simply define g
skel
(d) and
g
full
(d) as the model scores of d
?
and d:
g
skel
(d) , g(d
?
;w
?
,m, lm
?
) (5)
g
full
(d) , g(d;w,m, lm) (6)
This model makes the skeleton translation and
full translation much simpler because they per-
form in the same way of string translation in
phrase-based MT. Both g
skel
(d) and g
full
(d) share
the same translation model m which can easily
learned from the bilingual data
1
. On the other
hand, it has different feature weight vectors for in-
dividual models (i.e., w and w
?
).
For language modeling, lm is the standard n-
gram language model adopted in the baseline sys-
tem. lm
?
is a skeletal language for estimating the
well-formedness of the translation skeleton. Here
a translation skeleton is a target string where all
segments of non-skeleton translation are general-
ized to a symbol X. E.g., in Figure 1, the trans-
1
In g
skel
(d), we compute the reordering model score on
the skeleton though it is learned from the full sentences. In
this way the reordering problems in skeleton translation and
full translation are distinguished and handled separately.
565
lation skeleton is ?the cost X has been further re-
duced X .?, where two Xs represent non-skeleton
segments in the translation. In such a way of string
representation, the skeletal language model can be
implemented as a standard n-gram language mod-
el, that is, a string probability is calculated by a
product of a sequence of n-gram probabilities (in-
volving normal words and X). To learn the skele-
tal language model, we replace non-skeleton parts
of the target sentences in the bilingual corpus to
Xs using the source sentence skeletons and word
alignments. The skeletal language model is then
trained on these generalized strings in a standard
way of n-gram language modeling.
By substituting Eq. (4) into Eqs. (5) and (6),
and then Eqs. (3) and (2), we have the final model
used in this work:
?
d = argmax
d
(
w
m
? f
m
(d) + w
lm
? lm(d) +
w
?
m
? f
m
(d
?
) + w
?
lm
? lm
?
(d
?
)
)
(7)
Figure 1 shows the translation process and as-
sociated model scores for the example sentence.
Note that this method does not require any new
translation models for implementation. Given a
baseline phrase-based system, all we need is to
learn the feature weights w and w
?
on the devel-
opment set (with source-language skeleton anno-
tation) and the skeletal language model lm
?
on
the target-language side of the bilingual corpus.
To implement Eq. (7), we can perform standard
decoding while ?doubly weighting? the phrases
which cover a skeletal section of the sentence, and
combining the two language models and the trans-
lation model in a linear fashion.
3 Evaluation
3.1 Experimental Setup
We experimented with our approach on Chinese-
English translation using the NiuTrans open-
source MT toolkit (Xiao et al, 2012). Our bilin-
gual corpus consists of 2.7M sentence pairs. Al-
l these sentences were aligned in word level us-
ing the GIZA++ system and the ?grow-diag-final-
and? heuristics. A 5-gram language model was
trained on the Xinhua portion of the English Gi-
gaword corpus in addition to the target-side of the
bilingual data. This language model was used
in both the baseline and our improved system-
s. For our skeletal language model, we trained a
5-gram language model on the target-side of the
bilingual data by generalizing non-skeleton seg-
ments to Xs. We used the newswire portion of the
NIST MT06 evaluation data as our developmen-
t set, and used the evaluation data of MT04 and
MT05 as our test sets. We chose the default fea-
ture set of the NiuTrans.Phrase engine for building
the baseline, including phrase translation proba-
bilities, lexical weights, a 5-gram language mod-
el, word and phrase bonuses, a ME-based lexical-
ized reordering model. All feature weights were
learned using minimum error rate training (Och,
2003).
Our skeleton identification system was built
using the t3 toolkit
2
which implements a state-
of-the-art sentence simplification system. We
used the NEU Chinese sentence simplification
(NEUCSS) corpus as our training data (Zhang
et al, 2013). It contains the annotation of sen-
tence skeleton on the Chinese-language side of
the Penn Parallel Chinese-English Treebank (LD-
C2003E07). We trained our system using the Parts
1-8 of the NEUCSS corpus and obtained a 65.2%
relational F1 score and 63.1% compression rate in
held-out test (Part 10). For comparison, we also
manually annotated the MT development and test
data with skeleton information according to the
annotation standard provided within NEUCSS.
3.2 Results
Table 1 shows the case-insensitive IBM-version
BLEU and TER scores of different systems. We
see, first of all, that the MT system benefits from
our approach in most cases. In both the manual
and automatic identification of sentence skeleton
(rows 2 and 4), there is a significant improvemen-
t on the ?All? data set. However, using different
skeleton identification results for training and in-
ference (row 3) does not show big improvements
due to the data inconsistency problem.
Another interesting question is whether the
skeletal language model really contributes to the
improvements. To investigate it, we removed the
skeletal language model from our skeleton-based
translation system (with automatic skeleton iden-
tification on both the development and test sets).
Seen from row ?lm
?
of Table 1, the removal of
the skeletal language model results in a significan-
t drop in both BLEU and TER performance. It
indicates that this language model is very benefi-
cial to our system. For comparison, we removed
2
http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/
566
Entry MT06 (Dev) MT04 MT05 All
system dev-skel test-skel BLEU TER BLEU TER BLEU TER BLEU TER
baseline - - 35.06 60.54 38.53 61.15 34.32 62.82 36.64 61.54
SBMT manual manual 35.71 59.60 38.99 60.67 35.35 61.60 37.30 60.73
SBMT manual auto 35.72 59.62 38.75 61.16 35.02 62.20 37.03 61.19
SBMT auto auto 35.57 59.66 39.21 60.59 35.29 61.89 37.33 60.80
?lm
?
auto auto 35.23 60.17 38.86 60.78 34.82 62.46 36.99 61.16
?m
?
auto auto 35.50 59.69 39.00 60.69 35.10 62.03 37.12 60.90
s-space - - 35.00 60.50 38.39 61.20 34.33 62.90 36.57 61.58
s-feat. - - 35.16 60.50 38.60 61.17 34.25 62.88 36.70 61.58
Table 1: BLEU4[%] and TER[%] scores of different systems. Boldface means a significant improvement
(p < 0.05). SBMT means our skeleton-based MT system. ?lm
?
(or ?m
?
) means that we remove the
skeletal language model (or translation model) from our proposed approach. s-space means that we
restrict the baseline system to the search space of skeleton-consistent derivations. s-feat. means that we
introduce an indicator feature for skeleton-consistent derivations into the baseline system.
the skeleton-based translation model from our sys-
tem as well. Row ?m
?
of Table 1 shows that the
skeleton-based translation model can contribute to
the overall improvement but there is no big differ-
ences between baseline and ?m
?
.
Apart from showing the effects of the skeleton-
based model, we also studied the behavior of the
MT system under the different settings of search
space. Row s-space of Table 1 shows the BLEU
and TER results of restricting the baseline sys-
tem to the space of skeleton-consistent derivation-
s, i.e., we remove both the skeleton-based trans-
lation model and language model from the SBMT
system. We see that the limited search space is a
little harmful to the baseline system. Further, we
regarded skeleton-consistent derivations as an in-
dicator feature and introduced it into the baseline
system. Seen from row s-feat., this feature does
not show promising improvements. These results
indicate that the real improvements are due to the
skeleton-based model/features used in this work,
rather than the ?well-formed? derivations.
4 Related Work
Skeleton is a concept that has been used in several
sub-areas in MT for years. For example, in confu-
sion network-based system combination it refer-
s to the backbone hypothesis for building confu-
sion networks (Rosti et al, 2007; Rosti et al,
2008); Liu et al (2011) regard skeleton as a short-
ened sentence after removing some of the function
words for better word deletion. In contrast, we de-
fine sentence skeleton as the key segments of a
sentence and develop a new MT approach based
on this information.
There are some previous studies on the use of
sentence skeleton or related information in MT
(Mellebeek et al, 2006a; Mellebeek et al, 2006b;
Owczarzak et al, 2006). In spite of their good
ideas of using skeleton skeleton information, they
did not model the skeleton-based translation prob-
lem in modern SMT pipelines. Our work is a fur-
ther step towards the use of sentence skeleton in
MT. More importantly, we develop a complete ap-
proach to this issue and show its effectiveness in a
state-of-the-art MT system.
5 Conclusion and Future Work
We have presented a simple but effective approach
to integrating the sentence skeleton information
into a phrase-based system. The experimental re-
sults show that the proposed approach achieves
very promising BLEU improvements and TER re-
ductions on the NIST evaluation data. In our fu-
ture work we plan to investigate methods of inte-
grating both syntactic models (for skeleton trans-
lation) and phrasal models (for full translation) in
our system. We also plan to study sophisticated
reordering models for skeleton translation, rather
than reusing the baseline reordering model which
is learned on the full sentences.
Acknowledgements
This work was supported in part by the Nation-
al Science Foundation of China (Grants 61272376
and 61300097), and the China Postdoctoral Sci-
ence Foundation (Grant 2013M530131). The au-
thors would like to thank the anonymous reviewers
for their pertinent and insightful comments.
567
References
David Chiang. 2010. Learning to Translate with
Source and Target Syntax. In Proc. of ACL 2010,
pages 1443-1452.
James Clarke and Mirella Lapata. 2006. Models for
Sentence Compression: A Comparison across Do-
mains, Training Requirements and Evaluation Mea-
sures. In Proc. of ACL/COLING 2006, pages 377-
384.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion. In Proc. of
COLING 2008, pages 137-144.
Jason Eisner. 2003. Learning Non-Isomorphic Tree
Mappings for Machine Translation. In Proc. of ACL
2003, pages 205-208.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov Grammars for Sentence Compres-
sion. In Proc. of HLT:NAACL 2007, pages 180-187.
Liang Huang, Kevin Knight and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006, pages
66-73.
Kevin Knight and Daniel Marcu. 2000. Statistical-
based summarization-step one: sentence compres-
sion. In Proc. of AAAI 2000, pages 703-710.
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
NAACL 2003, pages 48-54.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. of ACL/COLING 2006, pages
609-616.
Shujie Liu, Chi-Ho Li and Ming Zhou. 2011. Statistic
Machine Translation Boosted with Spurious Word
Deletion. In Proc. of Machine Translation Summit
XIII, pages 72-79.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Transla-
tion. In Proc. of ACL:HLT 2008, pages 1003-1011.
Bart Mellebeek, Karolina Owczarzak, Josef van Gen-
abith and Andy Way. 2006. Multi-Engine Machine
Translation by Recursive Sentence Decomposition.
In Proc. of AMTA 2006, pages 110-118.
Bart Mellebeek, Karolina Owczarzak, Declan Groves,
Josef Van Genabith and Andy Way. 2006. A Syn-
tactic Skeleton for Statistical Machine Translation.
In Proc. of EAMT 2006, pages 195-202.
Franz J. Och, Christoph Tillmann and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of EMNLP/VLC
1999, pages 20-28.
Franz J. Och. 2003. Minimum error rate training in s-
tatistical machine translation. In Proc. of ACL 2003,
pages 160-167.
Karolina Owczarzak, Bart Mellebeek, Declan Groves,
Josef van Genabith and Andy Way. 2006. Wrapper
Syntax for Example-Based Machine Translation. In
Proc. of AMTA2006, pages 148-155.
Antti-Veikko I. Rosti, Spyros Matsoukas and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proc. of
ACL 2007, pages 312-319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. of Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li
2012. NiuTrans: An Open Source Toolkit for
Phrase-based and Syntax-based Machine Transla-
tion. In Proc. of ACL 2012, system demonstrations,
pages 19-24.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In
Proc. of ACL 2010, pages 937-947.
Katsumasa Yoshikawa, Ryu Iida, Tsutomu Hirao and
Manabu Okumura. 2012. Sentence Compression
with Semantic Role Constraints. In Proc. of ACL
2012, pages 349-353.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model.
In Proc. of ACL:HLT 2008, pages 559-567.
Chunliang Zhang, Minghan Hu, Tong Xiao, Xue Jiang,
Lixin Shi and Jingbo Zhu. 2013. Chinese Sentence
Compression: Corpus and Evaluation. In Proc.
of Chinese Computational Linguistics and Natural
Language Processing Based on Naturally Annotated
Big Data, pages 257-267.
568
