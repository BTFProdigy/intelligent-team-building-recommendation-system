Japanese Case Structure Analysis 
by Unsupervised Construction of a Case Frame Dictionary 
Daisuke Kawahara, Nobuhiro Kaji and Sadao Kurohashi 
Graduate  School of Intbrm~tics, Kyoto  University 
Yoshida-Honmachi ,  S~kyo-ku, Kyoto,  606-8501, Japan  
{kawahara, kaj i ,  kuro }@p inc. kuee. kyoto-u, ac. jp 
Abstract 
In Japanese, case structure analysis is very im- 
t)ortant to handle several troublesome charac- 
teristics of Japanese snch as scrambling, onfis- 
sion of ease components, mid disappearance of
case markers. However, fi)r lack of a wide- 
coverage ase frame dictionary, it has been dif- 
ficult to perfornl case structure analysis accu- 
rat;ely. Although several methods to construct 
a ease fl'mne dictionary from analyzed corpora 
have been proposed, they cannot avoid data 
sparseness 1)rol)lem. This paper proposes an un- 
supervised method of constructing a case frame 
dictionary from an enormous raw corpus by us- 
ing a robust and accurate parser. It also pro- 
rides a case structure analysis method based on 
the constructed ictionary. 
1 I n t roduct ion  
Syntactic analysis, or parsing has been a main 
objective in Natural Language Processing. In 
case of Jat)anese , however, syntactic analysis 
cannot clarify relations between words ill sen- 
tences because of several troublesome character- 
istics of Japanese such as scrambling, omission 
of case components, and disappearance of case 
markers. Therefore, in Japanese sentence analy- 
sis, case structure analysis is an important issue, 
and a case frame dictionary is necessary for the 
analysis. 
Some research institutes have constructed 
Japanese case frmne dictiouaries manually (Ike- 
hara et al, 1997; Infbrmation-Technology Pro- 
motion Agency, Japan, 1987). However, it is 
quite expensive, or almost impossible to con- 
struct a wide-coverage ease fl'anm dictionary by 
hand. 
Others have tried to construct a case fl'mne 
dictionary automatically from analyzed corpora 
(Utsuro et al, 1998). However, existing syntac- 
tically analyzed corpora are too small to learn a 
dictionary, since case fl'ame iuformation consists 
of relations between ouns and verbs, which rnul- 
tiplies to millions of combinations. 
Based on such a consideration, we took the 
fbllowing unsupervised learning strategy to the 
.Japanese case structure analysis: 
1. At first, a robust and accurate parser is de- 
veloped, which does not utilize a case fl'mne 
dictionary, 
2. a very large corI)us is parsed by the parser, 
3. reliable noun-verb relations are extracted 
from the parse results, and a case frmne dic- 
tionary is constructed from them, and 
4. the dictionary is utilized for case structure 
analysis. 
2 Characteristics of Japanese 
language and necessity of case 
s t ructure  ana lys i s  
In Japanese, postpositions function as case 
markers ( (Ms)  mid a verb is final in a sentence. 
The basic structure of a Japanese sentence is as 
fbllows: 
(1) kate  9a coat wo ki~'u. 
he nominative-CM coat accusative-CM wear 
(lie wears a coat) 
A clause modifier is left to the modified noun 
as follows: 
(2) kate  9 a k i te - i ru  coat 
lie nom-CM wear coat 
(the coat he wears) 
The modified noun followed by a postposition 
then becomes a case component of a matrix verb. 
The typical structure of a Japanese complex sen- 
tence is as fbllows: 
432 
(3) boush, i no irv wa kitc-ir'u 
hat of color tol)ic-marker wear 
coat ni a'wa~'cr"~,. 
coal; dative-CM harmonize 
(c/) harmonizes the color of his/her hat with 
the coat he/she wears) 
In terms of autolnatic analysis, the problen> 
atic characteristics of Japanese sentences can be 
summarized as follows: 
1. Case componenl;s are often scrambled or 
omitted. 
2. Case-marking postpositions disappear when 
case components are accompanied by topic- 
markers or other special 1)ostpositions 
meaning 'just', 'also' and others. 
cx) karv 'wa coat me ti:itc-iv'u. 
he tol}iC-lna.rker coat also wear 
(Ile wears a coat a.lso) 
3. A noun modified 1)y a clause is usually a case 
component for the verb of the mo(litlying 
clause. However, there is no case-marker for 
their relation. In case of sentence 3, there is 
no case-marker for coat in relation to kite- 
ir'u 'wear'. Note that 'hi (dative-CM) of coat: 
ni does not show the case to kitc-ivu 'wear', 
lint to awascr'v, 'harmonize'. 
4. Sentence 3 exhibits a typical structural am- 
1)iguity in a ,lalmnese sentence. That is, 
ir'o "~va 'color topic-marker' possit)ly modi- 
ties kit, c- iru 'wear' or awa.scv'u qmrmonize'. 
In English, sentence structure is rather rigid, 
and word order (the position in relation to the 
verb) clearly defines cases. In Japanese, how- 
ew% the l)roblem 1 above makes word order use- 
less, and CMs constitute the only int'ormation for 
detecting cases. 
Nevertheless, CMs often disapl)ear because of 
the problems 2 and 3, whidl means that sim- 
ple syntactic analysis cmmot clari(5~ cases sui\[i- 
cientl> For eXalnple, given an inlmt sentence: 
(4) har'c wa Dcv, tsch,-go me hano, sv,. 
he topic-marker Germall also sl)eak 
(he speaks German also) 
a simple syntactic analysis just detects both kar'c 
'he' and Dcutsch-go 'German' modifies \]ta'aas~t 
'speak', but tells nothing about which is subject 
and object. This analysis result is not sufficient 
for subsequent NLP applieations like Japanese 
to English machine l;rmmlation. 
Then, what we need to do is a case structure. 
analysis based on a case fl'ame dictiolmry, or a 
subcat, of each verb as follows: 
hanasu 'speak': 
ga (nora) ks're 'he', hire 'person' 
'we (ace) cigo 'English', kotoba 'language' 
~;i?'U, ~%vear': 
ga (nora) kavc qm', hil, o 'person' 
we (ace) fuhu 'cloth', coat 'coat' 
a'tl Jasel'~t 'har l l lonize' :  
ga (nora) kar'c 'he', hito 'person' 
'we (ace) ire ~color' 
ni (dat) fltku 'cloth' 
Consultation of such a dictionary can easily find 
that kar'c 'he' is a nomilmtive case and Dev, tsch,- 
90 'German' is an accusative (:as(', in the sentence 
4. 
Furthermore, a (:ase frame dictionary Call so lve  
the problem 4 above, that is, some part of struc- 
tural ambiguity in sentences. In case of sentence 
3, a t)r()l)er head for 'ir'o wa 'color topic-marker' 
(:all })e selected by consulting case slots of kir'u 
~wear' and those of a'wascru 'harmonize'. 
3 Unsupe,  rv i sed  const ruct ion  o f  a 
case  f ra lne  d ic t ionary  
This s(x:tion explains how to construct a case 
fralll(*, dictionary fl'om corl)ora autonmtica.lly. 
As mentioned in the introduction section, it; 
is quite expensive, or ahnost ilnl)ossible to con- 
struct a wide-coverage case frame dictionary by 
lmnd. In Japanese,, some noun q- copula works 
like an adjective. For example, sa~tsei da 'posi- 
tiveness + Colmla' can take 9a case and 'hi case. 
However, such case frames are rarely covered t)y 
the existing handmade dictionaries 1.
Fm'thermore, existing halldmade dictionaries 
cover typical obligatory cases like ga (nomina- 
tive), wo (accusative), ni (dative), but do not 
cover compound case markers uch as ni-kandz.itc 
'in terms of', 'wo-rncqutte 'concerning' and oth- 
ers. 
Then, we tried to construct an example-based 
case frmne dictionary from corpora, which de- 
lOut method collects case frames not only tbr verbs, 
but also tbr adjectives mM nouns-kcopula. In this paper, 
we use 'verb' instead of 'w;rb/adjective. orllOllll -{- copula.' 
for simplicity. 
433 
Table 1: The accuracy of KNP. 
'wa~ 7tto c lause  clause 
9 a 'we n i  ka'r'a rr~.ade ?lori topic- modif~ying modifying 
noln. ace. dative from to from marker verbs nouIIS 
'lbtal 
91.2% 97.7% 94.2% 83.8% 85.3% 82.8% 88.0% 84.3% 95.5% 91.3% 
scribes what kind of cases each verb has and 
what kind of nouns can fill a case slot. Very large 
syntactically analyzed corpora could be useful to 
construct such a dictionary. However, corpus an- 
notation costs very much and existing analyzed 
corpora are too small from the view point of case 
frame learning. For exmnple, in Kyoto Univer- 
sity Corpus which consists of about 40,000 ana- 
lyzed sentences of newspaper articles, very basic 
verbs like te tsudau 'help' or v, ketsv ,  kcr 'u 'accept' 
appear only 10 times or 15 times respectively. It
is obvious that such small data are insufficient 
for automatic ase frmne learning. That is, case 
frame learning must be done from enormous un- 
analyzed corpora, in unsupervised way 2. 
3.1 Good parser 
NLP research group at Kyoto University has 
been developing a robust and accurate parsing 
system, KNP, over the last ten yem's (Kurohashi 
and Nagao, 1994; Kurohashi and Nagao, 1998). 
This parser has the following advantages: 
? .Japanese is an agglutinative language, and 
several Nnction words (auxiliary verbs, suf- 
fixes, and postpositions) often appear to- 
gether and in many cases compositionality 
does not hold among them. KNP treats 
such function words careflflly and precisely. 
? KNP detects scopes of coordination struc- 
tures well based on their parallelism. 
? KNP employs everal heuristic rules to pro- 
duce mfique parses for the input sentences. 
The accuracy of KNP is shown in Table 1, 
which counted whether each phrase modifies a 
proper head or not. The overall accuracy was 
around 90%, and the accuracy concerning case 
components varies from 82% to 98%. 
21n English, several unsupervised methods have been 
proposed (Manning, 1993; Briscoe and Carroll, 19!)7). 
However, as mentioned in Section 3, automatic Japanese 
case analysis is much harder than English. 
We can collect pairs of verbs and case compo- 
nents from the automatic analyses of large cor- 
pora by KNP. 
3.2 Cop ing wi th  two problems 
The quality of automatic ase frame learning 
could be negatively influenced by the %llowing 
two problems: 
Word sense ambiguity:  A verb sometimes 
has w~rious usages and possibly has several 
case frames depending on its usages. 
Structura l  ambiguity:  KNP performs fairly 
well, but automatic parse results inevitably 
contt~in errors. 
The tbllowing sections explain how to solve 
these problems. 
3.2.1 Word sense ambigu i ty  
If a verb has two or more meanings and their 
case fl'ame patterns differ, we htwe to disam- 
biguate the sense of each occurrence of the verb 
in a corpus first, and collect case components for 
each sense respectively. However, unsupervised 
word sense disambiguation f fl'ee texts is one of 
the most ditficult problems in NLP. At the very 
begimfing, even the definition of word senses is 
open to question. 
To cope with this problem, we made a very 
simple but usefltl assumption: a light verb has 
diffbrent case frames det)ending on its main case 
component; an ordinary verb has a unique case 
frmne even if it has two or more meanings. For 
example, the case frmne of the verb narn  'be- 
come' differs depending on its ni (dative) case 
as %llows: 
. . .  ga  b?}ouki n i  na ' ru  
nora. become ill 
? . .  ga  . . .  to  to rnodach i  n i  na'r"u 
nora. with become a fliend 
In most cases, the main case components are 
placed just in front of the light verbs so that 
the automatic parser can detect their relations 
434 
Tal/le 2: EXmnl)les of the constructed ease frames. 
verl)s 
t, aS?t\]gCl"~l, 
'help' 
yomu 
l'ead' 
case lnarkel"s 
(1,o111) 
,,,,o (it(:(:) 
'r~,i (dat) 
ae (op) 
.qa (no\]n) 
'wo (at(;) 
hi, (dat) 
& (o10 
example  nomls  
husband, person, child, staff, I, SUSl)eet, faculty, ... 
.jol), shol) , farmwork, preparation, election, move, ... 
son, friend, ambassador, meml~er, thank, holid~\y, ... 
volunte(,r, aft'air, otfice, rewar(l, house, headquarters, ...
lX;rson, \], chihl, adult, parent, teacher, ... 
newspaper, book, magazine, article, nov(J, letter, ... 
chiht, person, daughter, teacher, student, reader, ... 
newspaper, book, magazine, library, classroom, bathroom, ... 
reliably. Therefore, as for five major and trou- 
t)lesome light verbs (.~'.,r'u 'do', 'nwr'u, q)ceomo?, 
ar'u 'is . . . ' ,  iu ~s~w', nai 'not'), their case fl'mnes 
are distinguished epending (m their left neigh- 
bouring case components. For other verbs, we 
aSStlllle a \] lnique ease f rame.  
3.2 .2  St ructura l  ambigu i ty  
As shown in '_\['~dfle 1, KNP detects heads of case 
conlt~onents in faMy high accuracy. However, 
in order to collect nmch reliable data, we dis- 
carded moditier-hcad relations in the aul;onmti- 
tal ly Imrsed corpora in the following cases: 
? When CMs of ease conqxments disappear 
because oi" topic markers or others. 
? When the verb is followed 1)y a causative 
auxil iary o1' a passive auxiliary, l;he case tm.t- 
t(:rn is e\]mnged and the 1;race in KNI '  is not 
so rclial)le. 
Based on the conditions al)ove, case compo- 
nents of each verb are collected froln the 1)arscd 
corpora, and the collected ata arc considered as 
case frames of verbs. However, if the f lcquency 
of a CM is very low compared to other CMs, it 
might t)e collected because of parse errors. So, 
we set the threshold for the CM flequency as 
2~,  where m.f means the frequency of the 
1nest folln(t ChJ. i f  the fl'equeney of ~t CM is less 
t lmn the threshold, it is discarded, l.~br exalnple, 
suppose the most frequent CM fin' a verb is we, 
100 times, and the frequency of ni CM tbr the 
verb is 1.6, ni CM is discarded (since it is less 
than the threshold, 20). 
a.3  Const ructed  case  f rmne d ic t ionary  
We applied the al)ow', procedure to Mainichi 
Newst)al)er Corpus (7 years, 3,600,00(} sen- 
tences). Fronl the cortms , case franws of 23,497 
verbs are constructed; the average number of 
ease slots of a verb is 2.8; the average munber 
of cxanqflc nouns in a (:as(: slot is 33.6. Table 2 
shows exmnlfles of constructed ease Dames. 
Although the constructed ata look apl)ropri- 
ate in most cases, it is hard to evaluate a (lictio- 
nary statica.ll> In the next section, we use the 
dictiomu'y in case structure analysis and eval- 
uate the analysis result, wlfich also im\])lies an 
cvahu~.ti(m of the dictionary itself. 
4 Case  s t ructure  ana lys i s  us ing  the  
const ructed  case  f rame d ic t ionary  
4.1. Match ing  o f  an  input  sentence  and  
a case  f ra l l le  
'Jl~e basic 1)ro(:cdure in ('ase strucl;ul"e analysis is 
lo match an inlml sentence with a case frame, 
aS show11 ill lqgUl'C, 1. 
The matching of case conq)onenl:s in an input 
and case slots in a case  fl'alllO is (\[Olle Oll the 
following conditions: 
I. When a ease component has a CM, it must 
be assigned to 1;11o case slot with the same 
CM.  
. When a case COml)Onent does nol: have a 
CM, it can 1)e assigned to the 9a, we, or ni 
CM slot. 
. ()nly one case component can be assigned 
to a case slot (unique case assiglmmnt con- 
straint). 
The conditions above may produce nmltil)le 
matching patterns, and to select the proper one 
alllOng {,llclll, 11Oll118 of case COlllpon(',lltS al'o COlll- 
pared with examph',s in case slots of the (tictio- 
nary. 
435 
syorui wa . 
(5) document topic-marker / 
ka,'e .i .___1 
! 
,,e 1 
~" walashila 
" hand 
\[ (1 handed the document to him.) 
WaRlSU 'hand' 
ga defendam, president .... 
we money, nlelllO, bribe .... 
ni person, suspect, ... 
de affair, office, room .... 
(6) Deutsch-go me 
/ Gcl'nlan also q 
hHI I ( IS I I  
speak " -7  
{';cq/;i'i;ir 
a teacher who speaks also German) 
~ professor, president 1
ni person, friend .... 
- -  we reason English Japanese 
to (sentence) 
Figure 1: Matching of an inl)ut sentence and a case fl:ame. 
Even though a 3,600,000 sentences corpus was 
used for learning, examples in case slots are still 
sparse, and an input noun mostly does not match 
exactly an example in the dictionary. Then, a 
thesaurus i employed to solve this problem. 
In our experiments, NTT Semantic Feature 
Dictionary (Ikehara et al, 1997) is employed as 
a thesaurus. Suppose we calculate the silnilar- 
ity between Wl and w2, their depth is dl and d2 
in the thesaurus, and the depth of their lowest 
(most specitic) common ode is de, the similarity 
score between them is calculated as follows: 
= (4  ? + 
If W 1 and w2 are in the same node of the the- 
saurus, the similarity is 1.0, the maximum score 
based on this criteria. If Wl and w2 are identical, 
the similarity is 1.0, of course. 
The score of case assigmnent is the best sim- 
ilarity between the input noun and examples in 
the case slots. The score of a matching pattern 
is the sum of scores of case assignments in it. If 
two or more patterns meet the above conditions, 
one which has the best score is selected as a final 
result. 
In the case of sentence 5 in Figure 1, karc 7ti 
'he dativc-CM' is assigned to the ni case slot. 
Then, syorui wa 'document topic-marker' can be 
assigned to the ga or wo case slot. By calculating 
similarity between syorui and 9a-slot examples 
and wo-slot exmnples, it; is considered to be as- 
signed to the wo slot. 
In case of sentence 6, none of the case compo- 
nents has a CM. Based on similarity calculation, 
Deutsch,-go is assigned to 'wo, sensei is assigned 
to ga. 
4.2 Pars ing with case structure analysis 
A complex sentence which contains a clausal 
modifier exhit)its a typical structural ambiguity 
of Japanese; case components left to a verb of 
a clausal modifier, Vc, possibly modify V~: or a 
matrix verb Vm. 
For example, in sentence 3, ir'o 'w~L 'color 
topic-inarker' possibly modifies kite-iru 'wear' or 
(l,~l)(\],Sel'~l, q lar i i l on ize ' .  
KNP, a rule-based parser, handles this type of 
ambiguity ~s follows. If a case component is fol- 
lowed by a comma, it is treated as modif\[ying Vm ; 
if not, it is treated as modif\[ying 1~:. Although 
this heuristic rule usually explains real data very 
well, sentence 3 will be analyzed incorrectly. 
Parsing which utilizes a case frame dictionary 
can consider which is a proper head, V~ or Vm, 
tbr an ambiguous case compolmnt by comparing 
examples in the case slots of V~ and 14~. Such 
a consideration nmst be done considering wlmt 
other case components modifly Vc and Vm, since 
the assigned case slot of a case component might 
differ depending on the candidate structure of 
the sentence due to the unique case assignment 
constraint. 
Therefore, it is necessary to expand the struc- 
tural ambiguity and consider all the possible 
structures fbr an input. So, we calculate the 
matching score of all pairs of case components 
and verbs in all possible structures of the sen- 
tence, and select the best structure based on the 
436 
boushi tic; ire wa 
hat color 
bott.~\]ti no
hal ~,~ 
~-- - __  ire wa 
co lo r - -  I 
C(;?lt It\[ 
COat ~5"?rll 
harlllOlliZe 
We ClOth, tllli\]'/)l'lll, CI)la .... ~ WO \] pOWCI', face, }l}ind 
~ . I  
I de party, oily, home .... ni I prcl)lcnce, cItlth .... 
kite-it'lt Co(It I1i HWCLTCI'll 
weal" coat hllrnlolliZC 
hat 
i re wa -2 (distance penalty) 
c? l ? r  ~ \] 
kite-iru \[ 
co.t ,,i ~ 
~ COat IdilA't!rll 
k i ru  'wear' ~. \  awa,~emt 'harllloilizc' 
2 ,:2,,,,,,6 ....
Figure 2: Parsing with east structure analysis. 
sum of the matching scores in it. 
Since the heuristic rule employed ill KNP is 
actually very useful, we in(:orporate it, that is, 
l)enalty score is imposed to the modifier-hea(l re- 
la.tion depending on the distraint between ~t mod- 
ifi(;l" and a head. If a moditier is not followed by 
a comma, the penalty score, 0 , -2 , -4 , -6 ,  ... is 
imposed when a moditler modifie.s the first (nea.r- 
est), second, third, tburth, ... verbs ill a sentence 
respectively; if with a comma, the tmnalty score, 
-2 ,  0, -2,  -4,  ... is impose& 
For example, sentence 3 was analyzed t)y our 
method as shown ill Figure 2. Since the simi- 
lm'ity score between fro ~color' a.nd the 'we-slot 
of uwa.s'cr'u hmunonize is nmch larger t;\]iall theft 
l)etween ire 'eoloff and the ga-slot of lci'r'u. 'wear', 
the correct structure of the selltellee was de- 
tected (the right-lmnd parse of Figure 2). Note 
that, furthermore, both the ease of ire ill reb> 
tion to awascru  'harmonize', and the case of coal, 
in relation to kite-iru 'wear' were dete(:ted cor- 
rectly. 
Structm'al ambiguities often cause a combina- 
torial explosion when a sentence is long. How- 
ever, by detecting the SeOl)eS of coordinate struc- 
tures 1)e%rehand, which off;ell aPl)ear in long 
'l'td)le 3: The at:curacy of case detection. 
(;orre(:I; ill(:orl'e(;t 
\])arsing ease case 
er ror  (lel;e(:l;ion (tete(:tion 
topic-marleer 82 13 5 
clausal modifier 73 18 9 
senl;ences, we can reasonably limit the possil)le 
sl;ructures of the sentence. 
The ~werage analysis peed of tile ext)criments 
described in the next section was about 50 sen- 
tenets/aria. 'File tinm-oul, of one rain. was only 
employed to 7 out of 4,272 test Selltellces. 
4.3 Exper i lnents  and discuss ion 
We used 4,272 sentences of Kyoto University co l  
pus as a test set. We parsed them by our new 
lnethod (Figure 3 shows several examt)les) and 
cheekc, d two 1)oints: case detection of mnbiguous 
case (-omponents and syntactic analysis. 
First, we randomly selected ambiguous ease 
components: 100 l,ol)ic-markcA case components 
all(t 100 (:ase coral)orients moditied by clausal 
437 
ookllrasyo ha 
the Treasury 
3gatsuki kes:~an de 
settlenlellt ill March ,, 
.l'hintakuginkotl kakukmt ga I impr<n'ed bycase iajbmtatiml 
each trust bank 
tsttmitareteirtt 
save lip 
tokubetsu t3,uhokil~ ,1o 
specially reserved money 
mrikuzushi wo 
collstllllptioll 
gai,,'yoltha 
the Foreign Minislcr 
m 
tnitonwru 
allow 
hm~shhula. 
policy 
mikka ni 
on tile third \] 
4 Mexico ga Mexico 
\[Iglppyolt shiRl imln'ored hy cave i@~tmltli?~*i 
illltl(RlnCC ~ ~ \ ]  
i@lre Ixmshi mulo ? 
prevention o1" inl\]alion \[ 
I 
L'eizai misaku ni 
fimmclal pllllcy 
t,wtite 
al~(Rl\[ 
st'Lvlltttl'i ~hifa 
cxphdn 
Figure 3: Exmnt)les of the mmlysis results. 
modifiers, and checked whether their cases were 
correctly detected or not. As shown in Table 3, 
the accuracy of the analysis was fairly good: that 
tbr topic-markers was 82% and that tbr clausal 
modifiers was 73%. 
Then, we compared the parse results of our 
method with those of the original KNP. As a re- 
sult, 565 modifier-head relations differed; in 260 
cases, our method was correct and the original 
KNP was incorrect (by considering the struc- 
tures in the Kyoto University Corpus as a golden 
standard); in 224 cases, vice versa. That is, 
our method was superior to KNP by 36 cases, 
and increased the overall accuracy from 89.8% 
to 89.9%. Since the heuristic rule used in KNP 
is very strong, the improvement was not big. 
The improvement of the accuracy, though small, 
is valuable, because the accuracy around 90% 
seems close to the ceiling of this task. 
5 Conc lus ion  
We proposed an unsupervised construction 
method of a case frame dictionary. We obtained 
a large case fl'alne dictionary, which consists 
of 23,497 verbs. Using this dictionary, we can 
detect ambiguous case components accurately. 
Also since our method employs unsupervised dic- 
tionary learning, it can be easily scaled up. 
Re ferences  
Ted Briscoe and John Carroll. 1997. Automatic 
extraction of subcategorization from corpora. 
In Prvccedings of ANLP-97. 
Satoru Ikehara, Masahiro Miyazaki, Satoshi 
Shirai, Akio Yokoo, Hiromi Nakaiwa, Ken- 
tarou Ogura, and Yoshiflmfi Oyama Yoshi- 
hiko Hayashi, editors. 1997. Japanese Lexi- 
con. Iwananfi Publishing. 
Information-q~chnology Promotion Agency, 
,Japan. 1987. Japanese Verbs : A Guide to 
the H~A Lea:icon of Basic ,Japa~tcsc Verbs. 
S. Kurohashi and M. Nagao. 1994. A syntac- 
tic analysis method of long japanese sentences 
based on the detection of conjunctive struc- 
tures. Computational Linguistics, 20(4). 
S. Kurohashi and M. Nagao. 1998. Build- 
ing a jal)anese parsed corpus while improv- 
ing the t)arsing system. In Prvcccdin.qs of" Th.c 
Fir;st h~,tcr'national Co't@r~ncc on Lwnguage 
R.csources 64 Evaluation, pages 719 724. 
Christopher D. Maturing. 1993. Automatic ac- 
quisition of a large snbcategorization dictio- 
nary froln corpora. In Pr'occcding s of A CL-93. 
Takehito Utsuro, Takashi Miyata, and Yuji Mat- 
sumoto. 1998. General-to-simeific model se- 
lection tbr subcategorization preference. In 
Proceedings of th.c 17th International ConJ'cr- 
cncc on Computational Li'n.quistics and the 
36th Annual Mectin.q of the Association for 
Computational Lin.quistics. 
438 
Japanese Case Frame Construction by Coupling the Verb
and its Closest Case Component
Daisuke Kawahara
Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501,
Japan
kawahara@pine.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501,
Japan
kuro@pine.kuee.kyoto-u.ac.jp
ABSTRACT
This paper describes a method to construct a case frame dic-
tionary automatically from a raw corpus. The main prob-
lem is how to handle the diversity of verb usages. We col-
lect predicate-argument examples, which are distinguished
by the verb and its closest case component in order to deal
with verb usages, from parsed results of a corpus. Since
these couples multiply to millions of combinations, it is dif-
ficult to make a wide-coverage case frame dictionary from a
small corpus like an analyzed corpus. We, however, use a
raw corpus, so that this problem can be addressed. Further-
more, we cluster and merge predicate-argument examples
which does not have different usages but belong to different
case frames because of different closest case components.
We also report on an experimental result of case structure
analysis using the constructed case frame dictionary.
1. INTRODUCTION
Syntactic analysis or parsing has been a main objective in
Natural Language Processing. In case of Japanese, however,
syntactic analysis cannot clarify relations between words in
sentences because of several troublesome characteristics of
Japanese such as scrambling, omission of case components,
and disappearance of case markers. Therefore, in Japanese
sentence analysis, case structure analysis is an important
issue, and a case frame dictionary is necessary for the anal-
ysis.
Some research institutes have constructed Japanese case
frame dictionaries manually [2, 3]. However, it is quite ex-
pensive, or almost impossible to construct a wide-coverage
case frame dictionary by hand.
Others have tried to construct a case frame dictionary
automatically from analyzed corpora. However, existing
syntactically analyzed corpora are too small to learn a dic-
tionary, since case frame information consists of relations
between nouns and verbs, which multiplies to millions of
combinations. Based on such a consideration, we took the
unsupervised learning strategy to Japanese case frame con-
struction1.
To construct a case frame dictionary from a raw corpus,
we parse a raw corpus first, but parse errors are problematic
in this case. However, if we use only reliable modifier-head
relations to construct a case frame dictionary, this problem
can be addressed. Verb sense ambiguity is rather problem-
atic. Since verbs can have different cases and case compo-
nents depending on their meanings, verbs which have dif-
ferent meanings should have different case frames. To deal
with this problem, we collect predicate-argument examples,
which are distinguished by the verb and its closest case com-
ponent, and cluster them. That is, examples are not distin-
guished by verbs such as naru ?make, become? and tsumu
?load, accumulate?, but by couples such as tomodachi ni
naru ?make a friend?, byouki ni naru ?become sick?,nimotsu
wo tsumu ?load baggage?, and keiken wo tsumu ?accumulate
experience?. Since these couples multiply to millions of com-
binations, it is difficult to make a wide-coverage case frame
dictionary from a small corpus like an analyzed corpus. We,
however, use a raw corpus, so that this problem can be ad-
dressed. The clustering process is to merge examples which
does not have different usages but belong to different case
frames because of different closest case components.
2. VARIOUS METHODS FOR CASE FRAME
CONSTRUCTION
We employ the following procedure of case frame construc-
tion from raw corpus (Figure 1):
1. A large raw corpus is parsed by KNP [5], and reliable
modifier-head relations are extracted from the parse
results. We call these modifier-head relations exam-
ples.
2. The extracted examples are distinguished by the verb
and its closest case component. We call these data
example patterns.
3. The example patterns are clustered based on a the-
saurus. We call the output of this process example
case frames, which is the final result of the system.
We call words which compose case components case
examples, and a group of case examples case exam-
ple group. In Figure 1, nimotsu ?baggage?, busshi
1In English, several unsupervised methods have been pro-
posed[7, 1]. However, it is different from those that combi-
nations of nouns and verbs must be collected in Japanese.
example patterns
raw corpus
tagging or
analysis+extraction
of reliable relations
thesaurus
by hand or learning
I. examples example case frames
III. merged frame
II. co-occurrences
IV. semantic case frames
wo
wo
niga
ga
tsumuwonimotsu
busshi
keiken
nikuruma
truck
hikoki
gajugyoin
sensyu
wo
ni
ga
wo
ni
ga
wo
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
nimotsu
kuruma
jugyoin
busshi
truck
keiken
sensyu
nihikoki
woga
ga woni tsumu
tsumusensyu keiken
jugyoin kuruma
truck
hikoki
nimotsu
busshi
ni
niga
wo
wo
wo tsumu
tsumu
tsumu
nimotsu
busshi
keiken
kurumajugyoin
truck
hikoki
gasensyu
wo
wo
wo
wo
wo
ni
ni
ga
ga
ni
kuruma
car
tsumu
load
tsumu
load
tsumu
load
tsumu
load
tsumu
nimotsu
baggage
nimotsu
baggage
jugyoin
worker
busshi
supply
busshi
supply
experience
keikensensyu
player
truck
hikoki
truck
airplane
tsumu
tsumu<mind>
<thing><vehicle><person>
<person>
accumulate
Figure 1: Several methods for case frame construction.
?supply?, and keiken ?experience? are case examples,
and {nimotsu ?baggage?, busshi ?supply?} (of wo case
marker in the first example case frame of tsumu ?load,
accumulate?) is a case example group. A case com-
ponent therefore consists of a case example and a case
marker (CM).
Let us now discuss several methods of case frame construc-
tion as shown in Figure 1.
First, examples (I of Figure 1) can be used individually,
but this method cannot solve the sparse data problem. For
example,
(1) kuruma ni nimotsu wo tsumu
car dat-CM baggage acc-CM load
(load baggage onto the car)
(2) truck ni busshi wo tsumu
truck dat-CM supply acc-CM load
(load supply onto the truck)
even if these two examples occur in a corpus, it cannot
be judged whether the expression ?kuruma ni busshi wo
tsumu? (load supply onto the car) is allowed or not.
Secondly, examples can be decomposed into binomial re-
lations (II of Figure 1). These co-occurrences are utilized
by statistical parsers, and can address the sparse data prob-
lem. In this case, however, verb sense ambiguity becomes a
serious problem. For example,
(3) kuruma ni nimotsu wo tsumu
car dat-CM baggage acc-CM load
(load baggage onto the car)
(4) keiken wo tsumu
experience acc-CM accumulate
(accumulate experience)
from these two examples, three co-occurrences (?kuruma ni
tsumu?, ?nimotsu wo tsumu?, and ?keiken wo tsumu?) are
extracted. They, however, allow the incorrect expression
?kuruma ni keiken wo tsumu? (load experience onto the
car, accumulate experience onto the car).
Thirdly, examples can be simply merged into one frame
(III of Figure 1). However, information quantity of this is
equivalent to that of the co-occurrences (II of Figure 1), so
verb sense ambiguity becomes a problem as well.
We distinguish examples by the verb and its closest case
component. Our method can address the two problems
above: verb sense ambiguity and sparse data.
On the other hand, semantic markers can be used as case
components instead of case examples. These we call seman-
tic case frames (IV of Figure 1). Constructing semantic
case frames by hand leads to the problem mentioned in Sec-
tion 1. Utsuro et al constructed semantic case frames from
a corpus [8]. There are three main differences to our ap-
proach: they use an annotated corpus, depend deeply on a
thesaurus, and did not resolve verb sense ambiguity.
3. COLLECTING EXAMPLES
This section explains how to collect examples shown in
Figure 1. In order to improve the quality of collected exam-
ples, reliable modifier-head relations are extracted from the
parsed corpus.
3.1 Conditions of case components
When examples are collected, case markers, case exam-
ples, and case components must satisfy the following condi-
tions.
Conditions of case markers
Case components which have the following case markers
(CMs) are collected: ga (nominative), wo (accusative), ni
(dative), to (with, that), de (optional), kara (from), yori
(from), he (to), and made (to). We also handle compound
case markers such as ni-tsuite ?in terms of?, wo-megutte
?concerning?, and others.
In addition to these cases, we introduce time case marker.
Case components which belong to the class <time>(see be-
low) and contain a ni, kara, or made CM are merged into
time CM. This is because it is important whether a verb
deeply relates to time or not, but not to distinguish between
surface CMs.
Generalization of case examples
Case examples which have definite meanings are general-
ized. We introduce the following three classes, and use these
classes instead of words as case examples.
<time>
? nouns which mean time
e.g. asa ?morning?, haru ?spring?,
rainen ?next year?
? case examples which contain a unit of time
e.g. 1999nen ?year?, 12gatsu ?month?,
9ji ?o?clock?
? words which are followed by the suffix mae ?before?,
tyu ?during?, or go ?after? and do not have the semantic
marker <place> on the thesaurus
e.g. kaku mae ?before ? ? ? write?,
kaigi go ?after the meeting?
<quantity>
? numerals
e.g. ichi ?one?, ni ?two?, juu ?ten?
? numerals followed by a numeral classifier2 such as tsu,
ko, and nin.
They are expressed with pairs of the class <quan-
tity> and a numeral classifier: <quantity>tsu, <quan-
tity>ko, and <quantity>nin.
e.g. 1tsu ? <quantity>tsu
2ko ? <quantity>ko
<clause>
? quotations (?? ? ? to? ?that ? ? ? ?) and expressions which
function as quotations (?? ? ? koto wo? ?that ? ? ? ?).
e.g. kaku to ?that ? ? ? write?,
kaita koto wo ?that ? ? ? wrote?
Exclusion of ambiguous case components
We do not use the following case components:
? Since case components which contain topic markers
(TMs) and clausal modifiers do not have surface case
markers, we do not use them. For example,
sono giin wa ? ? ? wo teian-shita.
the assemblyman TM acc-CM proposed
wa is a topic marker and giin wa ?assemblyman TM?
depends on teian-shita ?proposed?, but there is no case
marker for giin ?assemblyman? in relation to teian-
shita ?proposed?.
? ? ? wo teian-shiteiru giin ga ? ? ?
acc-CM proposing assemblyman
?? ? ? wo teian-shiteiru? is a clausal modifier and teian-
shiteiru ?proposing? depends on giin ?assemblyman?,
but there is no case marker for giin ?assemblyman? in
relation to teian-shiteiru ?proposing?.
? Case components which contain a ni or de case marker
are sometimes used adverbially. Since they have the
optional relation to their verbs, we do not use them.
e.g. tame ni ?because of?,
mujouken ni ?unconditionally?,
ue de ?in addition to?
For example,
30nichi ni souri daijin ga
30th on prime minister nom-CM
sono 2nin ni
those two people dat-CM
syou wo okutta
award acc-CM gave
2Most nouns must take a numeral classifier when they are
quantified in Japanese. An English equivalent to it is ?piece?.
(On 30th the prime minister gave awards to those two peo-
ple.)
from this sentence, the following example is acquired.
<time>:time-CM daijin:ga
minister:nom-CM
<quantity>nin:ni syou:wo okuru
people:dat-CM award acc-CM give
3.2 Conditions of verbs
We collect examples not only for verbs, but also for adjec-
tives and noun+copulas3 . However, when a verb is followed
by a causative auxiliary or a passive auxiliary, we do not
collect examples, since the case pattern is changed.
3.3 Extraction of reliable examples
When examples are extracted from automatically parsed
results, the problem is that the parsed results inevitably
contain errors. Then, to decrease influences of such errors,
we discard modifier-head relations whose parse accuracies
are low and use only reliable relations.
KNP employs the following heuristic rules to determine a
head of a modifier:
HR1 KNP narrows the scope of a head by finding a clear
boundary of clauses in a sentence. When there is only
one candidate verb in the scope, KNP determines this
verb as the head of the modifier.
HR2 Among the candidate verbs, verbs which rarely take
case components are excluded.
HR3 KNP determines the head according to the preference:
a modifier which is not followed by a comma depends
on the nearest candidate, and a modifier with a comma
depends on the second nearest candidate.
Our approach trusts HR1 but not HR2 and HR3. That is,
modifier-head relations which are decided in HR1 (there is
only one candidate of the head in the scope) are extracted
as examples, but relations which HR2 and HR3 are applied
to are not extracted. The following examples illustrate the
application of these rules.
(5) kare wa kai-tai hon wo
he TM want to buy book acc-CM
takusan mitsuketa node,
a lot found because
Tokyo he okutta.
Tokyo to sent
(Because he found a lot of books which he wants to buy, he
sent them to Tokyo.)
In this example, an example which can be extracted without
ambiguity is ?Tokyo he okutta? ?sent ? to Tokyo? at the end
of the sentence. In addition, since node ?because? is analyzed
as a clear boundary of clauses, the head candidate of hon
wo ?book acc-CM? is only mitsuketa ?find?, and this is also
extracted.
Verbs excluded from head candidates by HR2 possibly
become heads, so we do not use the examples which HR2 is
applied to. For example, when there is a strong verb right
3In this paper, we use ?verb? instead of ?verb/adjective or
noun+copula? for simplicity.
after an adjective, this adjective tends not to be a head of a
case component, so it is excluded from head candidates.
(6) Hi no mawari ga hayaku
fire of spread nom-CM rapidly
sukuidase-nakatta.
could not save
(The fire spread rapidly, so ?
1
could not save ?
2
.)
In this example, the correct head of mawari ga ?spread? is
hayaku ?rapidly?. However, since hayaku ?rapidly? is ex-
cluded from the head candidates, the head of mawari ga
?spread? is analyzed incorrectly.
We show an example of the process HR3:
(7) kare ga shitsumon ni
he nom-CM question acc-CM
sentou wo kitte kotaeta.
lead acc-CM take answered
(He took the lead to answer the question.)
In this example, head candidates of shitsumon ni ?question
acc-CM? are kitte ?take? and kotaeta ?answered?. According
to the preference ?modify the nearer head?, KNP incorrectly
decides the head is kitte ?take?. Like this example, when
there are many head candidates, the decided head is not
reliable, so we do not use examples in this case.
We extracted reliable examples from Kyoto University
Corpus[6], that is a syntactically analyzed corpus, and eval-
uated the accuracy of them. The accuracy of all the case
examples which have the target cases was 90.9%, and the
accuracy of the reliable examples was 97.2%. Accordingly,
this process is very effective.
4. CONSTRUCTION OF EXAMPLE CASE
FRAMES
As shown in Section 2, when examples whose verbs have
different meanings are merged, a case frame which allows an
incorrect expression is created. So, for verbs with different
meanings, different case frames should be acquired.
In most cases, an important case component which decides
the sense of a verb is the closest one to the verb, that is, the
verb sense ambiguity can be resolved by coupling the verb
and its closest case component. Accordingly, we distinguish
examples by the verb and its closest case component. We
call the case marker of the closest case component closest
case marker.
The number of example patterns which one verb has is
equal to that of the closest case components. That is, ex-
ample patterns which have almost the same meaning are
individually handled as follows:
(8) jugyoin:ga kuruma:ni
worker:nom-CM car:dat-CM
nimotsu:wo tsumu
baggage:acc-CM load
(9) {truck,hikoki}:ni
{truck,airplane}:dat-CM
busshi :wo tsumu
supply:acc-CM load
In order to merge example patterns that have almost the
same meaning, we cluster example patterns. The final ex-
( 5 + 8 ) + ( 3 + 2 + 10 )
( 3 + 5 + 8 ) + ( 3 + 2 + 10 )( )1/2 = 0.90ratio of common cases :
0.911.0 0.86
5
10
8
3 2
0.91= 0.941.0 ?  (5? 3) + 0.86 ?  (5? 2)
(5? 3) + (5? 2)
1/2 1/2
1/2 1/2
3 tsumu
tsumu
wo
wo
nimotsu
 busshini
kuruma
{truck  , hikoki  }
jugyoin ga ni
0.92 ?  0.90 = 0.83similarity between
example patterns :
= 0.920.94 ?  ( (5? 3) + (5? 2)  )   + 0.91 ?  (8? 10)
1/2 1/2 1/2 1/4
( (5? 3) + (5? 2)  )  + (8? 10)1/2 1/2 1/2 1/4
similarity between
case example groups :
load
load
baggageworker car
supplyairplanetruck
Figure 2: Example of calculating the similarity be-
tween example patterns (Numerals in the lower
right of examples represent their frequencies.)
ample case frames consist of the example pattern clusters.
The detail of the clustering is described in the following sec-
tion.
4.1 Similarity between example patterns
The clustering of example patterns is performed by using
the similarity between example patterns. This similarity
is based on the similarities between case examples and the
ratio of common cases. Figure 2 shows an example of cal-
culating the similarity between example patterns.
First, the similarity between two examples e
1
, e
2
is calcu-
lated using the NTT thesaurus as follows:
sime(e1, e2) = maxx?s
1
,y?s
2
sim(x,y)
sim(x,y) =
2L
lx + ly
where x, y are semantic markers, and s
1
, s
2
are sets of se-
mantic markers of e
1
, e
2
respectively4. lx, ly are the depths
of x, y in the thesaurus, and the depth of their lowest (most
specific) common node is L. If x and y are in the same node
of the thesaurus, the similarity is 1.0, the maximum score
based on this criterion.
Next, the similarity between the two case example groups
E
1
, E
2
is the normalized sum of the similarities of case ex-
amples as follows:
simE(E1, E2)
=
P
e
1
?E
1
P
e
2
?E
2
?
|e
1
||e
2
| sim
e
(e
1
,e
2
)
P
e
1
?E
1
P
e
2
?E
2
?
|e
1
||e
2
|
where |e
1
| , |e
2
| represent the frequencies of e
1
, e
2
respec-
tively.
The ratio of common cases of example patterns F
1
, F
2
is
4In many cases, nouns have many semantic markers in NTT
thesaurus.
calculated as follows:
cs =
s
P
n
i=1
|E
1cc
i
|+
P
n
i=1
|E
2cc
i
|
Pl
i=1
|E
1c1
i
|+
Pm
i=1
|E
2c2
i
|
where the cases of example pattern F
1
are c1
1
, c1
2
, ? ? ? , c1l,
the cases of example pattern F
2
are c2
1
, c2
2
, ? ? ? , c2m, and
the common cases of F
1
and F
2
is cc
1
, cc
2
, ? ? ? , ccn. E1cc
i
is the case example group of cci in F1. E2cc
i
, E
1c1
i
, and
E
2c2
i
are defined in the same way. The square root in this
equation decreases influences of the frequencies.
The similarity between F
1
and F
2
is the product of the
ratio of common cases and the similarities between case ex-
ample groups of common cases of F
1
and F
2
as follows:
score = cs ?
Pn
i=1
?
wi simE(E1cc
i
, E
2cc
i
)
P
n
i=1
?
wi
wi =
X
e
1
?E
1cc
i
X
e
2
?E
2cc
i
p
|e
1
| |e
2
|
where wi is the weight of the similarities between case ex-
ample groups.
4.2 Selection of semantic markers of example
patterns
The similarities between example patterns are deeply in-
fluenced by semantic markers of the closest case compo-
nents. So, when the closest case components have semantic
ambiguities, a problem arises. For example, when cluster-
ing example patterns of awaseru ?join, adjust?, the pair of
example patterns (te ?hand?, kao, ?face?)5 is created with
the common semantic marker <part of an animal>, and (te
?method?, syouten ?focus?) is created with the common se-
mantic marker <logic, meaning>. From these two pairs, the
pair (te ?hand?, kao ?face?, syouten ?focus?) is created, though
<part of an animal> is not similar to <logic, meaning> at
all.
To address this problem, we select one semantic marker of
the closest case component of each example pattern in order
of the similarity between example patterns as follows:
1. In order of the similarity of a pair, (p, q), of two exam-
ple patterns, we select semantic markers of the closest
case components, np, nq of p, q. The selected semantic
markers sp, sq maximize the similarity between np and
nq .
2. The similarities of example patterns related to p, q are
recalculated.
3. These two processes are iterated while there are pairs
of two example patterns, of which the similarity is
higher than a threshold.
4.3 Clustering procedure
The following is the clustering procedure:
1. Elimination of example patterns which occur infre-
quently
Target example patterns of the clustering are those
whose closest case components occur more frequently
than a threshold. We set this threshold to 5.
5Example patterns are represented by the closest case com-
ponents.
2. Clustering of example patterns which have the same
closest CM
(a) Similarities between pairs of two example pat-
terns which have the same closest CM are calcu-
lated, and semantic markers of closest case com-
ponents are selected. These two processes are it-
erated as mentioned in 4.2.
(b) Each example pattern pair whose similarity is higher
than some threshold is merged.
3. Clustering of all the example patterns
The example patterns which are output by 2 are clus-
tered. In this phase, it is not considered whether the
closest CMs are the same or not. The following exam-
ple patterns have almost the same meaning, but they
are not merged by 2 because of the different closest
CM. This clustering can merge these example patterns.
(10) {busshi,kamotsu}:wo
{supply,cargo}:acc-CM
truck :ni tsumu
truck:dat-CM load
(11) {truck,hikoki}:ni
{truck,airplane}:dat-CM
{nimotsu,busshi}:wo tsumu
{baggage,supply}:acc-CM load
5. SELECTION OF OBLIGATORY CASE
MARKERS
If a CM whose frequency is lower than other CMs, it might
be collected because of parsing errors, or has little relation
to its verb. So, we set the threshold for the CM frequency
as 2
?
mf, where mf means the frequency of the most found
CM. If the frequency of a CM is less than the threshold, it
is discarded. For example, suppose the most frequent CM
for a verb is wo, 100 times, and the frequency of ni CM for
the verb is 16, ni CM is discarded (since it is less than the
threshold, 20).
However, since we can say that all the verbs have ga (nom-
inative) CMs, ga CMs are not discarded. Furthermore, if an
example case frame do not have a ga CM, we supplement
its ga case with semantic marker <person>.
6. CONSTRUCTED CASE FRAME DICTIO-
NARY
We applied the above procedure to Mainichi Newspaper
Corpus (9 years, 4,600,000 sentences). We set the threshold
of the clustering 0.80. The criterion for setting this threshold
is that case frames which have different case patterns or
different meanings should not be merged into one case frame.
Table1 shows examples of constructed example case frames.
From the corpus, example case frames of 71,000 verbs are
constructed; the average number of example case frames of
a verb is 1.9; the average number of case slots of a verb is
1.7; the average number of example nouns in a case slot is
4.3. The clustering led a decrease in the number of example
case frames of 47%.
Table 1: Examples of the constructed case frames(*
means the closest CM).
verb CM case examples
kau1 ga person, passenger
?buy? wo* stock, land, dollar, ticket
de shop, station, yen
kau2 ga treatment, welfare, postcard
wo* anger, disgust, antipathy
...
...
...
yomu1 ga student, prime minister
?read? wo* book, article, news paper
yomu2 ga <person>
wo talk, opinion, brutality
de* news paper, book, textbook
yomu3 ga <person>
wo* future
...
...
...
tadasu1 ga member, assemblyman
?examine? wo* opinion, intention, policy
ni tsuite problem, <clause>, bill
tadasu2 ga chairman, oneself
?improve? wo* position, form
...
...
...
kokuchi1 ga doctor
?inform? ni* the said person
kokuchi2 ga colleague
wo* infection, cancer
ni* patient, family
sanseida1 ga <person>
?agree? ni* opinion, idea, argument
sanseida2 ga <person>
ni* <clause>
As shown in Table1, example case frames of noun+copulas
such as sanseida ?positiveness+copula (agree)?, and com-
pound case markers such as ni-tsuite ?in terms of? of tadasu
?examine? are acquired.
7. EXPERIMENTS AND DISCUSSION
Since it is hard to evaluate the dictionary statically, we
use the dictionary in case structure analysis and evaluate the
analysis result. We used 200 sentences of Mainichi Newspa-
per Corpus as a test set. We analyzed case structures of the
sentences using the method proposed by [4]. As the evalua-
tion of the case structure analysis, we checked whether cases
of ambiguous case components (topic markers and clausal
modifiers) are correctly detected or not. The evaluation re-
sult is presented in Table 2. The baseline is the result by
assigning a vacant case in order of ?ga?, ?wo?, and ?ni?. When
we do not consider parsing errors to evaluate the case de-
tection, the accuracy of our method for topic markers was
96% and that for clausal modifiers was 76%. The baseline
accuracy for topic markers was 91% and that for clausal
modifiers was 62%. Thus we see our method is superior to
the baseline.
Table 2: The accuracy of case detection.
correct case
detection
incorrect case
detection
parsing error
our method
topic marker 85 4 13
clausal modifier 48 15 2
baseline
topic marker 81 8 13
clausal modifier 39 24 2
The following are examples of analysis results6:
(1) 1
ookurasyo
?ga wa ginko ga
the Ministry of Finance TM bank nom-CM
2
tsumitate-teiru
2
ryuhokin
?wo no
deposit reserve fund of
torikuzushi wo
3
mitomeru
consume acc-CM consent
3
houshin
?ni? wo
1
kimeta .
policy acc-CM decide
(The Ministry of Finance decided the policy of con-
senting to consume the reserve fund which the banks
have deposited.)
(2)
korera no
1
gyokai
?wo? wa seijiteki
these industry TM political
hatsugenryoku ga tsuyoi toiu
voice nom-CM strong
tokutyo ga 1 aru .
characteristic nom-CM have
(These industries have the characteristic of
strong political voice.)
Analysis errors are mainly caused by two phenomena. The
first is clausal modifiers which have no case relation to the
modifees such as ?? ? ? wo mitomeru houshin? ?policy of con-
senting ? ? ? ? (? above). The Second is verbs which take two
ga ?nominative? case markers (one is wa superficially) such
as ?gyokai wa ? ? ? toiu tokutyo ga aru? ?industries have the
characteristic of ? ? ? ? (? above). Handling these phenomena
is an area of future work.
8. CONCLUSION
We proposed an unsupervised method to construct a case
frame dictionary by coupling the verb and its closest case
component. We obtained a large case frame dictionary,
which consists of 71,000 verbs. Using this dictionary, we
can detect ambiguous case components accurately. We plan
to exploit this dictionary in anaphora resolution in the fu-
ture.
9. ACKNOWLEDGMENTS
The research described in this paper was supported in
part by JSPS-RFTF96P00502 (The Japan Society for the
Promotion of Science, Research for the Future Program).
6The underlined words with ? are correctly analyzed, but
ones with ? are not. The detected CMs are shown after the
underlines.
10. REFERENCES
[1] T. Briscoe and J. Carroll. Automatic extraction of
subcategorization from corpora. In Proceedings of the
5th Conference on Applied Natural Language
Processing, pages 356?363, 1997.
[2] S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,
H. Nakaiwa, K. Ogura, and Y. O. Y. Hayashi, editors.
Japanese Lexicon. Iwanami Publishing, 1997.
[3] Information-Technology Promotion Agency, Japan.
Japanese Verbs : A Guide to the IPA Lexicon of Basic
Japanese Verbs. 1987.
[4] S. Kurohashi and M. Nagao. A method of case
structure analysis for japanese sentences based on
examples in case frame dictionary. In IEICE
Transactions on Information and Systems, volume
E77-D No.2, 1994.
[5] S. Kurohashi and M. Nagao. A syntactic analysis
method of long japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4), 1994.
[6] S. Kurohashi and M. Nagao. Building a japanese
parsed corpus while improving the parsing system. In
Proceedings of The First International Conference on
Language Resources & Evaluation, pages 719?724, 1998.
[7] C. D. Manning. Automatic acquisition of a large
subcategorization dictionary from corpora. In
Proceedings of the 31th Annual Meeting of ACL, pages
235?242, 1993.
[8] T. Utsuro, T. Miyata, and Y. Matsumoto. Maximum
entropy model learning of subcategorization preference.
In Proceedings of the 5th Workshop on Very Large
Corpora, pages 246?260, 1997.
Fertilization of Case Frame Dictionary
for Robust Japanese Case Analysis
Daisuke Kawahara? and Sadao Kurohashi??
?Graduate School of Information Science and Technology, University of Tokyo
?PRESTO, Japan Science and Technology Corporation (JST)
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes a method of fertilizing a
Japanese case frame dictionary to handle com-
plicated expressions: double nominative sen-
tences, non-gapping relation of relative clauses,
and case change. Our method is divided into
two stages. In the first stage, we parse a large
corpus and construct a Japanese case frame dic-
tionary automatically from the parse results. In
the second stage, we apply case analysis to the
large corpus utilizing the constructed case frame
dictionary, and upgrade the case frame dictio-
nary by incorporating newly acquired informa-
tion.
1 Introduction
To understand a text, it is necessary to find out
relations between words in the text. What is
required to do so is a case frame dictionary. It
describes what kinds of cases each verb has and
what kinds of nouns can fill a case slot. Since
these relations have millions of combinations,
it is difficult to construct a case frame dictio-
nary by hand. We proposed a method to con-
struct a Japanese case frame dictionary auto-
matically by arranging large volumes of parse
results by coupling a verb and its closest case
component (Kawahara and Kurohashi, 2001).
This case frame dictionary, however, could not
handle complicated expressions: double nomi-
native sentences, non-gapping relation of rela-
tive clauses, and case change.
This paper proposes a method of fertiliz-
ing the case frame dictionary to handle these
complicated expressions. We take an iterative
method which consists of two stages. This
means gradual learning of what is understood
by an analyzer in each stage. In the first stage,
we parse a large raw corpus and construct a
Japanese case frame dictionary automatically
from the parse results. This is the method pro-
posed by (Kawahara and Kurohashi, 2001). In
the second stage, we apply case analysis to the
large corpus utilizing the constructed case frame
dictionary, and upgrade the case frame dictio-
nary by incorporating newly acquired informa-
tion.
We conducted a case analysis experiment
with the upgraded case frame dictionary, and
its evaluation showed effectiveness of the fertil-
ization process.
2 Japanese Grammar
We introduce Japanese grammar briefly in this
section.
Japanese is a head-final language. Word or-
der does not play a case-marking role. Instead,
postpositions function as case markers (CMs).
The basic structure of a Japanese sentence is as
follows:
(1) kare
he
ga
nom-CM
hon
book
wo
acc-CM
kaku
write
(he writes a book)
ga and wo are postpositions which mean nom-
inative and accusative, respectively. kare ga
and hon wo are case components, and kaku is a
verb1.
There are two phenomena that case markers
are hidden.
A modifying clause is left to the modified
noun in Japanese. In this paper, we call a
noun modified by a clause clausal modifiee.
A clausal modifiee is usually a case component
for the verb of the modifying clause. There is,
however, no case marker for their relation.
1In this paper, we call verbs, adjectives, and
noun+copulas as verbs for convenience.
(2) hon
book
wo
acc-CM
kaita
write
hito
person
(the person who wrote the book)
(3) kare
he
ga
nom-CM
kaita
write
hon
book
(a book which he wrote)
In (2), hito ?person? has ga ?nominative? rela-
tion to kaita ?write?. In (3), hon ?book? has wo
?accusative? relation to kaita ?write?.
There are some non case-marking postposi-
tions, such as wa and mo. They topicalize or
emphasize noun phrases. We call them topic
markers (TMs) and a phrase followed by one
of them TM phrase.
(4) kare
he
wa
TM
hon
book
wo
acc-CM
kaita
write
(he wrote a book)
(5) kare
he
ga
nom-CM
hon
book
mo
TM
kaita
write
(he wrote a book also)
In (4), wa is interpreted as ga ?nominative?. In
(5), mo is interpreted as wo ?accusative?.
3 Construction of the initial case
frame dictionary
This section describes how to construct the ini-
tial case frame dictionary. This is the first stage
of our two-stage approach, and is performed by
the method proposed by (Kawahara and Kuro-
hashi, 2001). In the rest of this section, we de-
scribe this approach in detail.
The biggest problem in automatic case frame
construction is verb sense ambiguity. Verbs
which have different meanings should have dif-
ferent case frames, but it is hard to disam-
biguate verb senses very precisely. To deal
with this problem, we distinguish predicate-
argument examples, which are collected from
a large corpus, by coupling a verb and its
closest case component. That is, examples
are not distinguished by verbs such as naru
?make/become? and tsumu ?load/accumulate?,
but by couples such as ?tomodachi ni naru?
?make a friend?, ?byouki ni naru? ?become
sick?, ?nimotsu wo tsumu? ?load baggage?, and
?keiken wo tsumu? ?accumulate experience?.
This process makes separate case frames
which have almost the same meaning or usage.
For example, ?nimotsu wo tsumu? ?load bag-
gage? and ?busshi wo tsumu? ?load supply? are
separate case frames. To merge these similar
case frames and increase coverage of the case
frame, we cluster the case frames.
We employ the following procedure for the
automatic case frame construction:
1. A large raw corpus is parsed by a Japanese
parser, and reliable predicate-argument ex-
amples are extracted from the parse re-
sults. Nouns with a TM such as wa or
mo and clausal modifiees are discarded, be-
cause their case markers cannot be under-
stood by syntactic analysis.
2. The extracted examples are bundled ac-
cording to the verb and its closest case com-
ponent, making initial case frames.
3. The initial case frames are clustered using
a similarity measure, resulting in the final
case frames. The similarity is calculated by
using NTT thesaurus.
We constructed a case frame dictionary from
newspaper articles of 20 years (about 20,000,000
sentences).
4 Target expressions
The following expressions could not be handled
with the initial case frame dictionary shown in
section 3, because of lack of information in the
case frame.
Non-gapping relation
This is the case in which the clausal modifiee
is not a case component of the verb in the modi-
fying clause, but is semantically associated with
the clause.
(6) kare ga
he
syudoken wo
initiative
nigiru
have
kaigi
meeting
(the meeting in which he has the initiative)
In this example, kaigi ?meeting? is not a case
component of nigiru ?have?, and there is no case
relation between kaigi and nigiru. We call this
relation non-gapping relation.
Double nominative sentence
This is the case in which the verb has two
nominatives in sentences such as the following.
(7) kuruma
car
wa
TM
engine ga
engine
yoi
good
(the engine of the car is good)
In this example, wa plays a role of nominative,
so yoi ?good? subcategorizes two nominatives:
kuruma ?car? and engine. We call this outer
nominative outer ga and this sentence double
nominative sentence.
Case change
In Japanese, to express the same meaning,
we can use different case markers. We call this
phenomenon case change.
(8) Tom ga
Tom
Mary
Mary
no
of
shiji wo
support
eta
derive
(Tom derived his support from Mary)
In this example, Mary has kara ?from? relation
to eta ?derive?. In this paper, we handle case
change related to no ?of?, such as (no, kara).
The following is an example that outer nom-
inative is related to no case.
(9) kuruma no
car
engine ga
engine
yoi
good
(the engine of the car is good)
The outer nominative of (7) can be nominal
modifier of the inner nominative like this ex-
ample. This is case change of (no, outer ga).
There is a different case from the above that
an NP with no modifying a case component
does not have a case relation to the verb.
(10) kare ga
he
kaigi no
meeting
syudoken wo
initiative
nigiru
have
(he has the initiative in the meeting)
In this example, kaigi ?meeting? has a no rela-
tion to syudoken ?initiative?, but does not have a
case relation to nigiru ?have?. This example is a
transformation of (6), and includes case change
of (no, non-gapping).
5 Fertilization of case frame
dictionary
We construct a fertilized case frame dictionary
from the initial case frame dictionary shown in
section 3, to handle the complicated expressions
described in section 4.
We apply case analysis to a large corpus using
the dictionary, collect information which could
not be acquired by a mere parsing, and upgrade
the case frame dictionary.
The procedure is as follows (figure 1):
1. The initial case frames are acquired by the
method shown in section 3.
2. Case analysis utilizing the case frames ac-
quired in phase 1 is applied to a large cor-
pus, and examples of outer nominative are
collected from case analysis results.
3. Case analysis utilizing the case frames ac-
quired in phase 2 is applied to the large
corpus, and examples of non-gapping rela-
tion are collected similarly.
4. Case similarities are judged to handle case
change.
5.1 Case analysis based on the initial
case frame dictionary
Case analysis of TM phrases and clausal modi-
fiees is indebted to a case frame dictionary. This
section describes an example of case analysis
utilizing the initial case frame dictionary.
(11) sono
that
hon
book
wa
TM
kare ga
he
tosyokan
library
de
in
yonda
read
(he read that book in the library)
Case analysis of this example chooses the fol-
lowing case frame ?tosyokan de yonda? ?read in
the library? (?*? in the case frame means the
closest CM.).
CM examples input
read
nom person, child, ? ? ? he
acc book, paper, ? ? ? book
loc* library, house, ? ? ? library
kare ?he? and tosyokan ?library? correspond to
nominative and locative, respectively, according
to the surface cases. The case marker of TM
phrase ?hon wa? ?book (TM)? cannot be under-
stood by the surface case, but it is interpreted
as wo ?accusative? because of the matching be-
tween ?hon wa? ?book (TM)? and the accusative
case slot of the case frame (underlined in the
case frame).
gano fueru
consumer
bank
company{ }outerga{ }corporationbankJapan { }effectresultprofit}{
(1) case frames based on
    parsing
(2) collection of
    outer nominative
(3) collection of
    non-gapping relation
(4) case similarity judgement
Figure 1: Outline of our method
5.2 Collecting examples of outer
nominative
In the initial case frame construction described
in section 3, the TM phrase was discarded, be-
cause its case marker could not be understood
by parsing. In the example (7), ?engine ga yoi?
?the engine is good? is used to build the initial
case frame, but the TM phrase ?kuruma wa?
?the car? is not used.
Case analysis based on the initial case frame
dictionary tells a case of a TM phrase. Corre-
spondence to outer nominative cannot be under-
stood by the case slot matching, but indirectly.
If the TM cannot correspond to any case slots of
the initial case frame, the TM can be regarded
as outer nominative. For example, in the case
of (7), since the case frame of ?engine ga yoi?
?the engine is good? has only nominative which
corresponds to ?engine?, the TM of ?kuruma
wa? cannot correspond to any case slots and is
recognized as outer nominative. On the other
hand, in the case of (11), the TM of hon wa
is recognized as accusative, because hon ?book?
is similar to the examples of the accusative slot.
We can distinguish and collect outer nominative
examples in this way.
We apply the following procedure to each sen-
tence which has both a TM and ga. To reduce
the influence of parsing errors, the collection
process of these sentences is done under the con-
dition that a TM phrase has no candidates of
its modifying head without its verb.
1. We apply case analysis to a verb which is a
head of a TM phrase. If the verb does not
have the closest case component and can-
not select a case frame, we quit processing
this sentence and proceed to the next sen-
tence. In this phase, the TM phrase is not
made correspondence with a case of the se-
lected case frame.
2. If the case frame does not have any cases
which have no correspondence with the
case components in the input, the TM can-
not correspond to any case slots and is
regarded as outer nominative. This TM
phrase is added to outer nominative exam-
ples of the case frame.
The following is an example of this process.
(12) nagai
long
sumo
sumo
wa
TM
ashi-koshi ni
legs and loins
futan ga
burden
kakaru
impose
(long sumo imposes a burden on legs and
loins)
Case analysis of this example chooses the fol-
lowing case frame ?futan ga kakaru? ?impose a
burden?.
CM examples input
impose nom* burden burdendat heart, legs, loins, ? ? ? legs and loins
futan ?burden? and ashi-koshi ?legs and loins?
correspond to nominative and dative of the case
frame, respectively, and sumo corresponds to no
case marker. Accordingly, the TM of ?sumo
wa? is recognized as outer nominative, and
sumo is added to outer nominative examples of
the case frame ?futan ga kakaru?.
This process made outer nominative of 15,302
case frames (of 597 verbs).
5.3 Collecting examples of non-gapping
relation
Examples of non-gapping relation can be col-
lected in a similar way to outer nominative.
When a clausal modifiee has non-gapping re-
lation, it should not be similar to any exam-
ples of any cases in the case frame, because the
constructed case frames have examples of only
cases except for non-gapping relation. From this
point of view, we apply the following procedure
to each example sentence which contains a mod-
ifying clause. To reduce the influence of pars-
ing errors, the collection process of example sen-
tences is done under the condition that a verb
in a clause has no candidates of its modifying
head without its clausal modifiee (?? ? ? [modify-
ing verb] N1 no N2? is not collected).
1. We apply case analysis to a verb which is
contained by a modifying clause. If the
verb does not have the closest case compo-
nent and cannot select a case frame, we quit
processing this sentence and proceed to the
next sentence. In this phase, the clausal
modifiee is not made correspondence with
a case of the selected case frame.
2. If the similarity between the clausal modi-
fiee and examples of any cases which have
no correspondence with input case com-
ponents does not exceed a threshold, this
clausal modifiee is added to examples of
non-gapping relation in the case frame. We
set the threshold 0.3 empirically.
The following is an example of this process.
(13) gyomu
business
wo itonamu
carry on
menkyo
license
wo syutoku-shita
get
(? got a license to carry on business)
Case analysis of this example chooses the follow-
ing case frame ?{gyomu, business} wo itonamu?
?carry on { work, business }?.
CM examples input
carry on nom bank, company, ? ? ? -acc* work, business business
Nominative of this case frame has no corre-
spondence with a case component of the in-
put, so the clausal modifiee, menkyo ?license?,
is checked whether it can correspond to nom-
inative case examples. In this case, the sim-
ilarity between menkyo ?license? and examples
of nominative is not so high. Consequently, the
relation of menkyo ?license? is recognized as non-
gapping relation, and menkyo is added to exam-
ples of non-gapping relation in the case frame
?{gyomu, business} wo itonamu?.
(14) ihouni
illegally
denwa
telephone
gyomu
business
wo
itonande-ita
carry on
utagai
suspect
(suspect that ? carried on telephone busi-
ness illegally)
In this case, the above case frame is also se-
lected. Since utagai ?suspect? is not similar to
the nominative case examples, it is added to
case examples of non-gapping relation in the
case frame.
This process made non-gapping relation of
23,094 case frames (of 637 verbs).
Collecting examples of non-gapping rela-
tion for all the case frames
Non-gapping relation words which have wide
distribution over verbs can be considered to
have non-gapping relation for all the verbs or
case frames. We add these words to examples
of non-gapping relation of all the case frames.
For example, 5 verbs have menkyo ?license? (ex-
ample (13)) in their non-gapping relation, and
381 verbs have utagai ?suspect? (example (14)).
We, consequently, judge utagai has non-gapping
relation for all the case frames. We call such a
word global non-gapping word.
We treated words which have non-gapping re-
lation for more than 100 verbs as global non-
gapping words. We acquired 128 global non-
gapping words, and the following is the exam-
ples of them (in English).
possibility, necessity, result, course, case,
thought, schedule, outlook, plan, chance,
? ? ?
5.4 Case similarity judgement
To deal with case change, we applied the fol-
lowing process to every case frame with outer
nominative and non-gapping relation.
1. A similarity of every two cases is calculated.
It is the average of similarities between all
the combinations of case examples. But
similarities of couples of basic cases are not
handled, such as (ga, wo), (ga, ni), (wo,
ni), and so on.
2. A couple whose similarity exceeds a thresh-
old is judged to be similar, and is merged
into one case. We set the threshold 0.8 em-
pirically.
The following example is the case when this
process is applied to ?{setsumei, syakumei} wo
motomeru? ?demand {explanation, excuse}?.
CM examples
demand
nom committee, group,
acc* explanation, excuse
dat government, president,
about progress, condition, state,
no progress, reason, content,
In this case frame, the examples of no ?of?2 are
similar to those of ni-tsuite ?about?, and the sim-
ilarity between them is very high, 0.94, so these
case examples are merged into a new case no/ni-
tsuite ?of/about?.
By this process, 6,461 couples of similar cases
are merged. An NP with no modifying a case
component can be analyzed by this merging.
6 Case Analysis
To perform case analysis, we basically employ
the algorithm proposed by (Kurohashi and Na-
gao, 1994). In this section, our case analysis
method of the complicated expressions shown
in section 4 is described.
6.1 Analysis of clausal modifiees
If an clausal modifiee is a function word such
as koto ?(that clause)? or tame ?due?, or a time
expression such as 3 ji ?three o?clock? or saikin
?recently?, it is analyzed as non-gapping rela-
tion.
2In no case in case frames, every noun which modifies
the closest case component of the verb is collected.
The other clausal modifiee can correspond
to ga ?nominative?, wo ?accusative?, ni ?dative?,
outer ga ?outer nominative?, non-gapping rela-
tion, or no ?of?. We decide a corresponding case
which maximizes the score3 of the verb in the
clause. If a clausal modifiee corresponds to ga,
wo, ni, or outer ga, the relation is decided as it
is. If it corresponds to non-gapping relation or
no, the relation is decided as non-gapping re-
lation. In the case of corresponding to no, the
clausal modifiee has no relation to the closest
case component of the verb.
A clausal modifiee can correspond to non-
gapping relation or no under the condition that
similarity between the clausal modifiee and case
examples of non-gapping relation or no is the
maximum value (which means two nouns locate
in the same node in a thesaurus). This is be-
cause a noun which is a little similar to case
examples of non-gapping relation may not have
non-gapping relation.
6.2 Analysis of TM phrases
If a TM phrase is a time expression, it is ana-
lyzed as time case. The other TM phrase can
correspond to ga ?nominative?, wo ?accusative?,
or outer ga ?outer nominative?. We decide a
corresponding case which maximizes the score
of the verb modified by the TM phrase. When
the verb has both a case component with ga and
a TM phrase, the case component with ga cor-
responds to ga in the selected case frame, and
its TM phrase corresponds to wo or outer ga. If
the correspondence between the TM phrase and
outer ga case components gets the best similar-
ity, the input sentence is recognized as a double
nominative sentence.
6.3 Analysis of case change
If the selected case frame of the input verb has
merged cases which include no ?of?, no case in
the input sentence is interpreted as the counter-
part of no between the merged cases. If not, the
no case is considered not to have a case relation
to the verb and has no corresponding case in
the case frame.
3This score is the sum of each similarity between an
input case component and examples of the corresponding
case in the case frame.
Table 1: Case analysis accuracy
clausal
modifiee TM
our method 301/358 307/345
84.0% 88.9%
baseline 287/358 305/345
80.1% 88.4%
Table 2: Non-gapping relation accuracy
precision recall F
our method 82/116 82/92
70.7% 89.1% 78.8%
baseline 88/148 88/92
59.5% 95.7% 73.3%
7 Experiment
We made a case analysis experiment on
Japanese relevance-tagged corpus (Kawahara et
al., 2002). This corpus has correct tags of
predicate-argument relations. We conducted
case analysis on an open test set which consists
of 500 sentences, and evaluated clausal modi-
fiees and TM phrases in these sentences. To
evaluate the real case analysis without influence
of parsing errors, we input the correct structure
of the corpus sentences to the analyzer.
The accuracy of clausal modifiees and TM
phrases is shown in table 1, and the accuracy of
non-gapping relation is shown in table 2. The
baseline of these tables is that if a clausal mod-
ifiee belongs to a non-gapping noun dictionary
in which nouns always having non-gapping re-
lation as clausal modifiees are written, it is an-
alyzed as non-gapping relation.
The accuracy of clausal modifiees increased
by 4%. This shows effectiveness of our fertil-
ization process. However, the accuracy of TM
phrases did not increase. This is because the ac-
curacy of TM phrases which were analyzed us-
ing added outer nominative examples was 4/6,
and its frequency was too low. The accuracy of
case change was 2/4.
8 Related work
There has been some related work analyzing
clausal modifiees and TM phrases. Baldwin et
al. analyzed clausal modifiees with heuristic
rules or decision trees considering various lin-
guistic features (Baldwin et al, 1999). Its ac-
curacy was about 89%. Torisawa analyzed TM
phrases using predicate-argument cooccurences
and word classifications induced by the EM al-
gorithm (Torisawa, 2001). Its accuracy was
about 88% for wa and 84% for mo.
It is difficult to compare the accuracy because
the range of target expressions is different. Un-
like related work, it is promising to utilize our
resultant case frame dictionary for subsequent
analyzes such as ellipsis or discourse analysis.
9 Conclusion
This paper proposed a method of fertilizing
the case frame dictionary to realize an analy-
sis of the complicated expressions, such as dou-
ble nominative sentences, non-gapping relation,
and case change. We can analyze these expres-
sions accurately using the fertilized case frame
dictionary. So far, accuracy of subsequent an-
alyzes such as ellipsis or discourse analysis has
not been so high, because double nominative
sentences and non-gapping relation cannot be
analyzed accurately. It is promising to improve
the accuracy of these analyzes utilizing the fer-
tilized case frame dictionary.
References
Timothy Baldwin, Takenobu Tokunaga, and
Hozumi Tanaka. 1999. The parameter-based
analysis of Japanese relative clause construc-
tions. In IPSJ SIJ Notes 1999-NL-134, pages
55?62.
Daisuke Kawahara and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling
the verb and its closest case component. In
Proceedings of the Human Language Technol-
ogy Conference, pages 204?210.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of
the 3rd International Conference on Lan-
guage Resources and Evaluation, pages 2008?
2013.
Sadao Kurohashi and Makoto Nagao. 1994.
A method of case structure analysis for
Japanese sentences based on examples in
case frame dictionary. In IEICE Transactions
on Information and Systems, volume E77-D
No.2.
Kentaro Torisawa. 2001. An unsupervised
method for canonicalization of Japanese post-
positions. In Proceedings of the 6th Natural
Language Processing Pacific Rim Simposium,
pages 211?218.
Improving Japanese Zero Pronoun Resolution
by Global Word Sense Disambiguation
Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes unsupervised word
sense disambiguation based on automati-
cally constructed case frames and its in-
corporation into our zero pronoun resolu-
tion system. The word sense disambigua-
tion is applied to verbs and nouns. We
consider that case frames define verb senses
and semantic features in a thesaurus define
noun senses, respectively, and perform sense
disambiguation by selecting them based on
case analysis. In addition, according to the
one sense per discourse heuristic, the word
sense disambiguation results are cached and
applied globally to the subsequent words.
We integrated this global word sense disam-
biguation into our zero pronoun resolution
system, and conducted experiments of zero
pronoun resolution on two different domain
corpora. Both of the experimental results
indicated the effectiveness of our approach.
1 Introduction
For a long time, parsing has been a central is-
sue for the area of natural language analyses.
In recent years, its accuracy has improved to
over 90%, and it became the fundamental tech-
nology that is applied to a lot of NLP applica-
tions, such as question answering, text summa-
rization, and machine translation. Accordingly,
anaphora resolution, which is positioned as the
next step of parsing, has been studied actively
(Ng and Cardie, 2002; Yang et al, 2003; Iida
et al, 2003; Isozaki and Hirao, 2003; Kawa-
hara and Kurohashi, 2004). Its performance,
however, is not satisfactory enough to benefit
the NLP applications. We investigated errors
of our Japanese zero pronoun resolution system
(Kawahara and Kurohashi, 2004), and found
that word sense ambiguity causes a major part
of errors.
Our zero pronoun resolution system uti-
lizes the general-purpose thesaurusNihongo Goi
Taikei (Ikehara et al, 1997) (hereafter, NTT
thesaurus) to do matching of example words.
In this thesaurus, one or more semantic features
are given to each word, and similarity between
words can be calculated by comparing closeness
of their semantic features in the thesaurus tree
(Appendix A). Multiple semantic features for
a word, i.e. word sense ambiguity, cause in-
correct matching, and furthermore deteriorate
accuracy of the zero pronoun resolution sys-
tem. For instance, in the thesaurus, ?gobou?
(burdock/priest/temple) has four semantic fea-
tures: <crop>, <vegetable>, <priest> and
<temple>?. If <priest> is used for ?gobou? in
a cooking domain text, though ?gobou? means
?burdock? (a genus of coarse biennial herbs) in
its context, ?gobou? is identified as an agent.
That is, ?gobou? is incorrectly analyzed as an-
tecedents of the following nominative zero pro-
nouns.
If such word sense ambiguity can be resolved,
incorrect matching decreases, and the anaphora
resolution system will improve. Word sense dis-
ambiguation is a basic issue of NLP. Recently,
the evaluation exercises for word sense disam-
biguation such as SENSEVAL-1 (Kilgarriff and
Palmer, 2000) and SENSEVAL-2 (Yarowsky,
2001) have been held, but word sense disam-
biguation has rarely been incorporated into
deep analyses like anaphora resolution.
This paper proposes unsupervised word sense
disambiguation based on automatically con-
structed case frames and its incorporation into
the zero pronoun resolution system. The word
sense disambiguation is applied to verbs? and
nouns. We consider that case frames define
verb senses and semantic features in the the-
saurus define noun senses, respectively. A verb
is disambiguated by selecting a corresponding
case frame to its context, and a noun is disam-
biguated by selecting an appropriate semantic
?In this paper, <> means a semantic feature.
?In this paper, we use ?verb? instead of ?verb, adjective
and noun+copula? for simplicity.
NOUN
CONCRETE ABSTRACT
AGENT PLACE CONCRETE
HUMAN ORGANIZATION
ABSTRACT EVENT ABSTRACT RELATION
TIME POSITION QUANTITY . . . .
Figure 1: The upper levels of the NTT thesaurus.
feature from the ones defined for the noun in
the thesaurus. In addition, according to the one
sense per discourse heuristic, the disambigua-
tion results are cached and applied globally to
the following words in the same text.
The remainder of this paper is organized as
follows. Section 2 briefly describes the NTT the-
saurus and the automatic construction method
of case frames. Section 3 outlines our zero pro-
noun resolution system. Section 4 describes the
method of word sense disambiguation and its
integration into the zero pronoun resolution sys-
tem. Section 5 presents the experiments of the
integrated system. Section 6 summarizes the
conclusions.
2 Resources
We consider that verb and noun senses corre-
spond to case frames and semantic features de-
fined in the NTT thesaurus, respectively. This
section describes the NTT thesaurus and the
case frames briefly.
2.1 NTT thesaurus
NTT Communication Science Laboratories con-
structed a semantic feature tree, whose 3,000
nodes are semantic features, and a nominal dic-
tionary containing about 300,000 nouns, each of
which is given one or more appropriate seman-
tic features. Figure 1 shows the upper levels of
the semantic feature tree.
The similarity between two words is defined
by formula (1) in Appendix A.
2.2 Automatically constructed case
frames
We employ the automatically constructed case
frames (Kawahara and Kurohashi, 2002) as
the basic resource for zero pronoun resolution
and word sense disambiguation. This section
outlines the method of constructing the case
frames.
The biggest problem in automatic case frame
construction is verb sense ambiguity. Verbs
which have different meanings should have dif-
ferent case frames, but it is hard to dis-
ambiguate verb senses precisely. To deal
with this problem, predicate-argument exam-
ples which are collected from a large cor-
pus are distinguished by coupling a verb and
its closest case component. That is, ex-
amples are not distinguished by verbs (e.g.
?tsumu? (load/accumulate)), but by couples
(e.g. ?nimotsu-wo tsumu? (load baggage) and
?keiken-wo tsumu? (accumulate experience)).
This process makes separate case frames
which have almost the same meaning or usage.
For example, ?nimotsu-wo tsumu? (load bag-
gage) and ?busshi-wo tsumu? (load supply) are
similar, but have separate case frames. To cope
with this problem, the case frames are clustered.
Example words are collected for each case
marker, such as ?ga?, ?wo?, ?ni? and
?kara?. They are case-marking postpositions
in Japanese, and usually mean nominative, ac-
cusative, dative and ablative, respectively. We
call such a case marker ?case slot? and example
words in a case slot ?case examples?.
Case examples in a case slot are similar, but
have some incorrect semantic features because
of word sense ambiguity. For instance, ?ni-
motsu? (baggage), ?busshi? (supply) and ?nise-
mono? (imitation) are gathered in a case slot,
and all of them are below the semantic feature
<goods>. On the other hand, ?nisemono? be-
longs to <lie>. <lie> is incorrect for this case
slot, and possibly causes errors in case analysis.
We delete a semantic feature that is not similar
to the other semantic features of its case slot.
To sum up, the procedure for the automatic
case frame construction is as follows.
1. A large raw corpus is parsed by the
Japanese parser, KNP (Kurohashi and
Nagao, 1994b), and reliable predicate-
argument examples are extracted from the
parse results.
2. The extracted examples are bundled ac-
cording to the verb and its closest case com-
ponent, making initial case frames.
3. The initial case frames are clustered using a
similarity measure function. This similar-
ity is calculated by formula (5) in Appendix
B.
4. For each case slot of clustered case frames,
an inappropriate semantic feature that is
not similar to the other semantic features
is discarded.
We constructed two sets of case frames: for
newspaper and cooking domain.
The newspaper case frames are constructed
from about 21,000,000 sentences of newspaper
articles in 20 years (9 years of Mainichi news-
paper and 11 years of Nihonkeizai newspaper).
They consist of 23,000 verbs, and the average
number of case frames for a verb is 14.5.
The cooking case frames are constructed from
about 5,000,000 sentences of cooking domain
that are collected from WWW. They consist
of 5,600 verbs, and the average number of case
frames for a verb is 6.8.
In Figure 1, some examples of the resulting
case frames are shown. In this table, ?CS? means
a case slot. <agent> in the table is a general-
ized case example, which is given to the case
slot where half of the case examples belong to
<agent>. <agent> is also given to ?ga? case
slot that has no case examples, because ?ga?
case components are often omitted, but ?ga?
case slots usually mean nominative.
3 The Outline of the Zero Pronoun
Resolution System
We have proposed a Japanese zero pronoun
resolution system using the case frames, an-
tecedent preference orders, and a machine learn-
ing technique (Kawahara and Kurohashi, 2004).
Its procedure is as follows.
1. Parse an input sentence using the Japanese
parser, KNP.
2. Process each verb in the sentence from left
to right by the following steps.
Table 1: Case frame examples.
CS case examples?
ga <agent>, group, party, ? ? ?youritsu (1) wo <agent>, candidate, applicant(support) ni <agent>, district, election, ? ? ?
ga <agent>youritsu (2) wo <agent>, member, minister, ? ? ?(support) ni <agent>, candidate, successor
... ... ...
orosu (1) ga <agent>
(grate) wo radish
ga <agent>orosu (2) wo money(withdraw) kara bank, post
... ... ...
itadaku (1) ga <agent>
(have) wo soup
ga <agent>itadaku (2) wo advice, instruction, address(be given) kara <agent>, president, circle, ? ? ?
... ... ...
?case examples are expressed only in English for
space limitation.
2.1. Narrow case frames down to corre-
sponding ones to the verb and its clos-
est case component.
2.2. Perform the following processes for
each case frame of the target verb.
i. Match each input case component
with an appropriate case slot of
the case frame. Regard case slots
that have no correspondence as
zero pronouns.
ii. Estimate an antecedent of each
zero pronoun.
2.3. Select a case frame which has the high-
est total score, and output the analysis
result for the case frame.
The rest of this section describes the above
steps (2.1), (2.2.i) and (2.2.ii) in detail.
3.1 Narrowing down case frames
The closest case component plays an important
role to determine the usage of a verb. In par-
ticular, when the closest case is ?wo? or ?ni?,
this trend is clear-cut. In addition, an expres-
sion whose nominative belongs to <agent> (e.g.
?<agent> has accomplished?), does not have
enough clue to decide its usage, namely a case
frame. By considering these aspects, we impose
the following conditions on narrowing down case
frames.
? The closest case component exists, and
must immediately precede its verb.
? The closest case component and the closest
case meet one of the following conditions:
? The closest case is ?wo? or ?ni?.
? The closest case component does
not belong to the semantic marker
<agent>.
? A case frame with the closest case exists,
and the similarity between the closest case
component and examples in the closest case
exceeds a threshold.
We choose the case frames whose similarity
is the highest. If the above conditions are not
satisfied, case frames are not narrowed down,
and the subsequent processes are performed for
each case frame of the target verb. The simi-
larity used here is defined as the best similarity
between the closest case component and exam-
ples in the case slot. The similarity between two
examples is defined as formula (1) in Appendix
A.
Let us consider ?youritsu? (support) in the
second sentence of Figure 2. ?youritsu? has the
case frames shown in Table 1. The input expres-
sion ?kouho-wo youritsu? (support a candidate)
satisfies the above two conditions, and the case
frame ?youritsu (1)? meets the last condition.
Accordingly, this case frame is selected.
3.2 Matching input case components
with case slots in the case frame
We match case components of the target verb
with case slots in the case frame (Kurohashi and
Nagao, 1994a). When a case component has a
case marker, it must be assigned to the case
slot with the same case marker. When a case
component is a topic marked phrase or a clausal
modifiee, which does not have a case marker, it
can be assigned to one of the case slots in the
following table.
topic marked phrases : ga, wo, ga2
clausal modifiees : ga, wo, non-gapping
The conditions above may produce multiple
matching patterns. In this case, one which has
the best score is selected. The score of a match-
ing pattern is defined as the sum of similarities
of case assignments. This similarity is calcu-
lated as the same way described in Section 3.1.
 	
  

 
 





 Automatic Construction of Nominal Case Frames and
its Application to Indirect Anaphora Resolution
Ryohei Sasano, Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo
{ryohei,kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes a method to auto-
matically construct Japanese nominal case
frames. The point of our method is the in-
tegrated use of a dictionary and example
phrases from large corpora. To examine the
practical usefulness of the constructed nom-
inal case frames, we also built a system of
indirect anaphora resolution based on the
case frames. The constructed case frames
were evaluated by hand, and were confirmed
to be good quality. Experimental results of
indirect anaphora resolution also indicated
the effectiveness of our approach.
1 Introduction
What is represented in a text has originally
a network structure, in which several concepts
have tight relations with each other. However,
because of the linear constraint of texts, most
of them disappear in the normal form of texts.
Automatic reproduction of such relations can be
regarded as the first step of ?text understand-
ing?, and surely benefits NLP applications such
as machine translation, automatic abstraction,
and question answering.
One of such latent relationship is indirect
anaphora, functional anaphora, or bridging ref-
erence, such as the following examples.
(1) I bought a ticket. The price was 20 dollars.
(2) There was a house. The roof was white.
Here, ?the price? means ?the price of a ticket?
and ?the roof? means ?the roof of a house.?
Most nouns have their indispensable or req-
uisite entities: ?price? is a price of some goods
or service, ?roof? is a roof of some building,
?coach? is a coach of some sport, and ?virus?
is a virus causing some disease. The relation
between a noun and its indispensable entity is
parallel to that between a verb and its argu-
ments or obligatory cases. In this paper, we call
indispensable entities of nouns obligatory cases.
Indirect anaphora resolution needs a compre-
hensive information or dictionary of obligatory
cases of nouns.
In case of verbs, syntactic structures such as
subject/object/PP in English or case markers
such as ga, wo, ni in Japanese can be utilized
as a strong clue to distinguish several obliga-
tory cases and adjuncts (and adverbs), which
makes it feasible to construct case frames from
large corpora automatically (Briscoe and Car-
roll, 1997; Kawahara and Kurohashi, 2002).
(Kawahara and Kurohashi, 2004) then utilized
the automatically constructed case frames to
Japanese zero pronoun resolution.
On the other hand, in case of nouns, obliga-
tory cases of noun Nh appear, in most cases, in
the single form of noun phrase ?Nh of Nm? in
English, or ?Nm no Nh? in Japanese. This sin-
gle form can express several obligatory cases,
and furthermore optional cases, for example,
?rugby no coach? (obligatory case concerning
what sport), ?club no coach? (obligatory case
concerning which institution), and ?kyonen ?last
year? no coach? (optional case). Therefore, the
key issue to construct nominal case frames is to
analyze ?Nh of Nm? or ?Nm no Nh? phrases to
distinguish obligatory case examples and others.
Work which addressed indirect anaphora in
English texts so far restricts relationships to a
small, relatively well-defined set, mainly part-of
relation like the above example (2), and utilized
hand-crafted heuristic rules or hand-crafted lex-
ical knowledge such as WordNet (Hahn et al,
1996; Vieira and Poesio, 2000; Strube and
Hahn, 1999). (Poesio et al, 2002) proposed
a method of acquiring lexical knowledge from
?Nh of Nm? phrases, but again concentrated on
part-of relation.
In case of Japanese text analysis, (Murata et
al., 1999) proposed a method of utilizing ?Nm
no Nh? phrases for indirect anaphora resolution
of diverse relationships. However, they basically
used all ?Nm no Nh? phrases from corpora, just
excluding some pre-fixed stop words. They con-
fessed that an accurate analysis of ?Nm no Nh?
phrases is necessary for the further improve-
ment of indirect anaphora resolution.
As a response to these problems and follow-
ing the work in (Kurohashi and Sakai, 1999), we
propose a method to construct Japanese nom-
inal case frames from large corpora, based on
an accurate analysis of ?Nm no Nh? phrases
using an ordinary dictionary and a thesaurus.
To examine the practical usefulness of the con-
structed nominal case frames, we also built a
system of indirect anaphora resolution based on
the case frames.
2 Semantic Feature Dictionary
First of all, we briefly introduce NTT Seman-
tic Feature Dictionary employed in this paper.
NTT Semantic Feature Dictionary consists of a
semantic feature tree, whose 3,000 nodes are se-
mantic features, and a nominal dictionary con-
taining about 300,000 nouns, each of which is
given one or more appropriate semantic fea-
tures.
The main purpose of using this dictionary is
to calculate the similarity between two words.
Suppose the word x and y have a semantic fea-
ture sx and sy, respectively, their depth is dx
and dy in the semantic tree, and the depth of
their lowest (most specific) common node is dc,
the similarity between x and y, sim(x, y), is cal-
culated as follows:
sim(x, y) = (dc ? 2)/(dx + dy).
If sx and sy are the same, the similarity is 1.0,
the maximum score based on this criteria.
We also use this dictionary to specify seman-
tic category of words, such as human, time and
place.
3 Semantic Analysis of Japanese
Noun Phrases Nm no Nh
In many cases, obligatory cases of nouns are
described in an ordinary dictionary for human
being. For example, a Japanese dictionary for
children, Reikai Shougaku Kokugojiten, or RSK
(Tajika, 1997), gives the definitions of the word
coach and virus as follows1:
coach a person who teaches technique in some
sport
virus a living thing even smaller than bacte-
ria which causes infectious disease like in-
fluenza
1Although our method handles Japanese noun
phrases by using Japanese definition sentences, in this
paper we use their English translations for the explana-
tion. In some sense, the essential point of our method is
language-independent.
Based on such an observation, (Kurohashi
and Sakai, 1999) proposed a semantic analy-
sis method of ?Nm no Nh?, consisting of the
two modules: dictionary-based analysis (abbre-
viated to DBA hereafter) and semantic feature-
based analysis (abbreviated to SBA hereafter).
This section briefly introduces their method.
3.1 Dictionary-based analysis
Obligatory case information of nouns in an ordi-
nary dictionary can be utilized to solve the dif-
ficult problem in the semantic analysis of ?Nm
no Nh? phrases. In other words, we can say the
problem disappears.
For example, ?rugby no coach? can be inter-
preted by the definition of coach as follows: the
dictionary describes that the noun coach has an
obligatory case sport, and the phrase ?rugby no
coach? specifies that the sport is rugby. That is,
the interpretation of the phrase can be regarded
as matching rugby in the phrase to some sport
in the coach definition. ?Kaze ?cold? no virus?
is also easily interpreted based on the definition
of virus, linking kaze ?cold? to infectious disease.
Dictionary-based analysis (DBA) tries to find
a correspondence between Nm and an obliga-
tory case of Nh by utilizing RSK and NTT Se-
mantic Feature Dictionary, by the following pro-
cess:
1. Look up Nh in RSK and obtain the defini-
tion sentences of Nh.
2. For each word w in the definition sentences
other than the genus words, do the follow-
ing steps:
2.1. When w is a noun which shows an
obligatory case explicitly, like kotog-
ara ?thing?, monogoto ?matter?, nanika
?something?, and Nm does not have a
semantic feature of human or time,
give 0.8 to their correspondence2.
2.2. When w is other noun, calculate the
similarity between Nm and w by us-
ing NTT Semantic Feature Dictionary,
and give the similarity score to their
correspondence.
3. Finally, if the best correspondence score is
0.75 or more, DBA outputs the best corre-
spondence, which can be an obligatory case
of the input; if not, DBA outputs nothing.
2For the present, parameters in the algorithm were
given empirically, not optimized by a learning method.
Table 1: Examples of rules for semantic feature-based analysis.
1. Nm:human, Nh:relative ? <obligatory case(relative)> e.g. kare ?he? no oba ?aunt?
2. Nm:human, Nh:human ? <modification(apposition)> e.g. gakusei ?student? no kare ?he?
3. Nm:organization, Nh:human ? <belonging> e.g. gakkou ?school? no seito ?student?
4. Nm:agent, Nh:event ? <agent> e.g. watashi ?I? no chousa ?study?
5. Nm:material, Nh:concrete ? <modification(material)> e.g. ki ?wood? no hako ?box?
6. Nm:time, Nh:? ? <time> e.g. aki ?autumn? no hatake ?field?
7. Nm:color, quantity, or figure, Nh:? ? <modification> e.g. gray no seihuku ?uniform?
8. Nm:?, Nh:quantity ? <obligatory case(attribute)> e.g. hei ?wall? no takasa ?height?
9. Nm:?, Nh:position ? <obligatory case(position)> e.g. tsukue ?desk? no migi ?right?
10. Nm:agent, Nh:? ? <possession> e.g. watashi ?I? no kuruma ?car?
11. Nm:place or position, Nh:? ? <place> e.g. Kyoto no mise ?store?
??? meets any noun.
In case of the phrase ?rugby no coach?, ?tech-
nique? and ?sport? in the definition sentences
are checked: the similarity between ?technique?
and ?rugby? is calculated to be 0.21, and the
similarity between ?sport? and ?rugby? is cal-
culated to be 1.0. Therefore, DBA outputs
?sport?.
3.2 Semantic feature-based analysis
Since diverse relations in ?Nm no Nh? are han-
dled by DBA, the remaining relations can be
detected by simple rules checking the semantic
features of Nm and/or Nh.
Table 1 shows examples of the rules. For ex-
ample, the rule 1 means that if Nm has a seman-
tic feature human and Nh relative, <obliga-
tory case> relation is assigned to the phrase.
The rules 1, 2, 8 and 9 are for certain oblig-
atory cases. We use these rules because these
relations can be analyzed more accurately by us-
ing explicit semantic features, rather than based
on a dictionary.
3.3 Integration of two analyses
Usually, either DBA or SBA outputs some re-
lation. When both DBA and SBA output some
relations, the results are integrated (basically, if
DBA correspondence score is higher than 0.8,
DBA result is selected; if not, SBA result is se-
lected). In rare cases, neither analysis outputs
any relations, which means analysis failure.
4 Automatic Construction of
Nominal Case Frames
4.1 Collection and analysis of Nm no Nh
Syntactically unambiguous noun phrases ?Nm
no Nh? are collected from the automatic parse
results of large corpora, and they are analyzed
using the method described in the previous sec-
tion.
Table 2: Preliminary case frames for hisashi
?eaves/visor?.
DBA result
1. a roof that stick out above the window of
a house.
[house] hall:2, balcony:1, building:1, ? ? ?
[window] window:2, ceiling:1, counter:1, ? ? ?
2. the fore piece of a cap.
[cap] cap:8, helmet:1, ? ? ?
SBA result
<place> parking:3, store:3, shop:2, ? ? ?
<mod.> concrete:1, metal:1, silver:1, ? ? ?
No semantic analysis result
<other> part:1, light:1, phone:1, ? ? ?
By just collecting the analysis results of each
head word Nh, we can obtain its preliminary
case frames. Table 2 shows preliminary case
frames for hisashi ?eaves/visor?. The upper part
of the table shows the results by DBA. The line
starting with ?[house]? denotes a group of anal-
ysis results corresponding to the word ?house?
in the first definition sentence. For example,
?hall no hisashi? occurs twice in the corpora,
and they were analyzed by DBA to correspond
to ?house.?
The middle part of the table shows the results
by SBA. Noun phrases that have no semantic
analysis result (analysis failure) are bundled and
named <other>, as shown in the last part of the
table.
A case frame should be constructed for each
meaning (definition) of Nh, and groups start-
ing with ?[...]? or ?<...>? in Table 2 are possi-
ble case slots. The problem is how to arrange
the analysis results of DBA and SBA and how
to distinguish obligatory cases and others. The
following sections explain how to handle these
problems.
Table 3: Threshold to select obligatory slots.
type of case slots threshold of probability
analyzed by DBA 0.5% (1/200)
<obligatory case> 2.5% (1/40)
<belonging> 2.5% (1/40)
<possessive> 5% (1/20)
<agent> 5% (1/20)
<place> 5% (1/20)
<other> 10% (1/10)
<modification> not used
<time> not used
Probability = (# of Nm no Nh) / (# of Nh)
4.2 Case slot clustering
One obligatory case might be separated in pre-
liminary case frames, since the definition sen-
tence is sometimes too specific or too detailed.
For example, in the case of hisashi ?eaves/visor?
in Table 2, [house], [window], and <place>
have very similar examples that mean building
or part of building. Therefore, case slots are
merged if similarity of two case slots is more
than 0.5 (case slots in different definition sen-
tences are not merged in any case). Similarity
of two case slots is the average of top 25% sim-
ilarities of all possible pairs of examples.
In the case of Table 2, the similarity between
[house] and [window] is 0.80, and that between
[house] and <place> is 0.67, so that these three
case slots are merged into one case slot.
4.3 Obligatory case selection
Preliminary case frames contain both obliga-
tory cases and optional cases for the head word.
Since we can expect that an obligatory case
co-occurs with the head word in the form of
noun phrase frequently, we can take frequent
case slots as obligatory case of the head word.
However, we have to be careful to set up
the frequency thresholds, because case slots de-
tected by DBA or <obligatory case> by SBA
are more likely to be obligatory; on the other
hand case slots of <modification> or <time>
should be always optional. Considering these
tendencies, we set thresholds for obligatory
cases as shown in Table 3.
In the case of hisashi ?eaves/visor? in Table 2,
[house-window]-<place> slot and [cap] slot are
chosen as the obligatory cases.
4.4 Case frame construction for each
meaning
Case slots that are derived from each definition
sentence constitute a case frame.
If a case slot of <obligatory case> by SBA
or <other> is not merged into case slots in def-
inition sentences, it can be considered that it
indicates a meaning of Nh which is not covered
in the dictionary. Therefore, such a case slot
constitutes an independent case frame.
On the other hand, when other case slots by
SBA such as <belonging> and <possessive>
are remaining, we have to treat them differently.
The reason why they are remaining is that they
are not always described in the definition sen-
tences, but their frequent occurrences indicate
they are obligatory cases. Therefore, we add
these case slots to the case frames derived from
definition sentences.
Table 4 shows several examples of resul-
tant case frames. Hyoujou ?expression? has a
case frame containing two case slots. Hisashi
?eaves/visor? has two case frames according to
the two definition sentences. In case of hiki-
dashi ?drawer?, the first case frame corresponds
to the definition given in the dictionary, and
the second case frame was constructed from the
<other> case slot, which is actually another
sense of hikidashi, missed in the dictionary. In
case of coach, <possessive> is added to the case
frame which was made from the definition, pro-
ducing a reasonable case frame for the word.
4.5 Point of nominal case frame
construction
The point of our method is the integrated
use of a dictionary and example phrases from
large corpora. Although dictionary definition
sentences are informative resource to indicate
obligatory cases of nouns, it is difficult to do
indirect anaphora resolution by using a dictio-
nary as it is, because all nouns in a definition
sentence are not an obligatory case, and only
the frequency information of noun phrases tells
us which is the obligatory case. Furthermore,
sometimes a definition is too specific or detailed,
and the example phrases can adjust it properly,
as in the example of hisashi in Table 2.
On the other hand, a simple method that
just collects and clusters ?Nm no Nh? phrases
(based on some similarity measure of nouns)
can not construct comprehensive nominal case
frames, because of polysemy and multiple oblig-
atory cases. We can see that dictionary defini-
tion can guide the clustering properly even for
such difficult cases.
Table 4: Examples of nominal case frames.
case slot examples
hisashi :1 ?eaves/visor? (the edges of a roof that stick out above the window of a house etc.)
[house, window] parking, store, hall, ? ? ?
hisashi :2 ?eaves/visor? (the fore piece of a cap.)
[cap] cap, helmet, ? ? ?
hyoujou ?expression? (to express one?s feelings on the face or by gestures.)
[one] people, person, citizen, ? ? ?
[feelings] relief, margin, ? ? ?
hikidashi :1 ?drawer? (a boxlike container in a desk or a chest.)
[desk, chest] desk, chest, dresser, ? ? ?
hikidashi :2 ?drawer? <other> credit, fund, saving, ? ? ?
coach (a person who teaches technique in some sport.)
[sport] baseball, swimming, ? ? ?
<belonging> team, club, ? ? ?
kabushiki ?stock? (the total value of a company?s shares.)
[company] company, corporation, ? ? ?
5 Indirect Anaphora Resolution
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames.
An input sentence is parsed using the
Japanese parser, KNP (Kurohashi and Nagao,
1994). Then, from the beginning of the sen-
tence, each noun x is analyzed. When x has
more than one case frame, the process of an-
tecedent estimation (stated in the next para-
graph) is performed for each case frame, and the
case frame with the highest similarity score (de-
scribed below) and assignments of antecedents
to the case frame are selected as a final result.
For each case slot of the target case frame of
x, its antecedent is estimated. A possible an-
tecedent y in the target sentence and the previ-
ous two sentences is checked. This is done one
by one, from the syntactically closer y. If the
similarity of y to the case slot is equal to or
greater than a threshold ? (currently 0.95), it
is assigned to the case slot.
The similarity between y and a case slot is
defined as the highest similarity between y and
an example in the case slot.
For instance, let us consider the sentence
shown in Figure 1. soccer, at the beginning of
the sentence, has no case frame, and is consid-
ered to have no obligatory case.
For the second noun ticket, soccer, which is
a nominal modifier of ticket, is examined first.
The similarity between soccer and the examples
of the case slot [theater, transport] exceeds the
soccer-no
ticket-ga
takai
nedan-de
urareteita.
expensive
price
be sold
case slot examples result
ticket [theater, transport] stage, game,? ? ? soccer
nedan [things] thing, ticket,? ? ? ticket
ticket a printed piece of paper which shows that you have
paid to enter a theater or use a transport
nedan the amount of money for which things are sold or
bought
Figure 1: Indirect anaphora resolution example.
threshold ?, and soccer is assigned to [theater,
transport].
Lastly, for nedan ?price?, its possible an-
tecedents are ticket and soccer. ticket, which
is the closest from nedan, is checked first. The
similarity between ticket and the examples of
the case slot [things] exceeds the threshold ?,
and ticket is judged as the antecedent of nedan.
6 Experiments
We evaluated the automatically constructed
nominal case frames, and conducted an experi-
ment of indirect anaphora resolution.
6.1 Evaluation of case frames
We constructed nominal case frames from news-
paper articles in 25 years (12 years of Mainichi
newspaper and 13 years of Nihonkeizai newspa-
per). These newspaper corpora consist of about
Table 5: Evaluation result of case frames.
precision recall F
58/70 (0.829) 58/68 (0.853) 0.841
25,000,000 sentences, and 10,000,000 ?Nm no
Nh? noun phrases were extracted from them.
The result consists of 17,000 nouns, the average
number of case frames for a noun is 1.06, and
the average number of case slots for a case frame
is 1.09.
We randomly selected 100 nouns that occur
more than 10,000 times in the corpora, and cre-
ated gold standard case frames by hand. For
each test noun, possible case frames were con-
sidered, and for each case frame, obligatory case
slots were given manually. As a result, 68 case
frames for 65 test nouns were created, and 35
test nouns have no case frames.
We evaluated automatically constructed case
frames for these test nouns against the gold
standard case frames. A case frame which has
the same case slots with the gold standard is
judged as correct. The evaluation result is
shown in Table 5: the system output 70 case
frames, and out of them, 58 case frames were
judged as correct.
The recall was deteriorated by the highly re-
stricted conditions in the example collection.
For instance, maker does not have obligatory
case slot for its products. This is because maker
is usually used in the form of compound noun
phrase, ?products maker?, and there are few
occurrences of ?products no maker?. To ad-
dress this problem, not only ?Nm no Nh? but
also ?Nm Nh? (compound noun phrase) and
?Nm ni-kansuru ?in terms of? Nh? should be
collected.
6.2 Experimental results of indirect
anaphora resolution
We conducted a preliminary experiment of
our indirect anaphora resolution system using
?Relevance-tagged corpus? (Kawahara et al,
2002). This corpus consists of Japanese news-
paper articles, and has relevance tags, including
antecedents of indirect anaphors.
We prepared a small test corpus that con-
sists of randomly selected 10 articles. The test
corpus contains 217 nouns. Out of them, 106
nouns are indirect anaphors, and have 108 an-
tecedents, which is because two nouns have dou-
ble antecedents. 49 antecedents directly depend
on their anaphors, and 59 do not. For 91 an-
tecedents out of 108, a case frame of its anaphor
Table 6: Experimental results of indirect
anaphora resolution.
precision recall F
w dep. 40/46 (0.870) 40/59 (0.678) 0.762
w/o dep. 31/61 (0.508) 31/49 (0.633) 0.564
total 71/107 (0.664) 71/108 (0.657) 0.660
includes the antecedent itself or its similar word
(the similarity exceeds the threshold, 0.95). Ac-
cordingly, the upper bound of the recall of our
case-frame-based anaphora resolution is 84.3%
(91/108).
We ran the system on the test corpus, and
compared the system output and the corpus an-
notation. Table 6 shows the experimental re-
sults. In this table, ?w dep.? (with dependency)
is the evaluation of the antecedents that directly
depend on their anaphors. ?w/o dep.? (with-
out dependency) is the case of the antecedents
that do not directly depend on their anaphors.
Although the analysis of ?w dep.? is intrinsi-
cally easier than that of ?w/o dep.?, the recall
of ?w dep.? was not much higher than that
of ?w/o dep.?. The low recall score of ?w dep.?
was caused by nonexistence of case frames which
include the antecedent itself or its similar word.
The antecedents that directly depend on their
anaphors were often a part of compound noun
phrases, such as ?products maker?, which are
not covered by our examples collection.
Major errors in the analyses of the an-
tecedents that do not directly depend on their
anaphors were caused by the following reasons.
Specific/generic usages of nouns
Some erroneous system outputs were caused by
nouns that have both specific and generic us-
ages.
(3) kogaisya-no
subsidiary
kabushiki-wo
stock
baikyaku-shita.
sell
(? sold the stock of the subsidiary.)
In this case, kogaisya ?subsidiary? is an oblig-
atory information for kabushiki ?stock?, which is
specifically used. kogaisya matches the [kaisya
?company?] case slot in Table 4.
However, kabushiki ?stock? in the following ex-
ample is used generically, and does not need spe-
cific company information.
(4)kabushiki
stock
souba-no
price
oshiage
rise
youin-to naru.
factor become
(? become the rise factor of the stock prices.)
Since the current system cannot judge generic
or specific nouns, an antecedent which corre-
sponds to [kaisha ?company?] is incorrectly esti-
mated.
Beyond selectional restriction of case
frames
Selectional restriction based on the case frames
usually worked well, but did not work to distin-
guish candidates both of which belong to Hu-
man or Organization.
(5) Bush bei
American
seiken-wa
administration
Russia-tono
... Bush daitouryou-ga
president
shutyou-shita.
claim
(Bush American administration ... with
Russia ... President Bush claimed ...)
In this example, daitouryou ?president? re-
quires an obligatory case kuni ?nation?. The sys-
tem estimates its antecedent as Russia, though
the correct answer is bei ?America?. This is be-
cause Russia is closer than beikoku. This prob-
lem is somehow related to world knowledge, but
if the system can carefully exploit the context,
it might be able to find the correct answer from
?Bush bei seiken? ?Bush American administra-
tion?.
7 Conclusion
This paper has first proposed an automatic
construction method of Japanese nominal case
frames. This method is based on semantic anal-
ysis of noun phrases ?Nm no Nh? ?Nh of Nm?.
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames. The evaluation indi-
cated the good quality of the constructed case
frames. On the other hand, the accuracy of our
indirect anaphora resolution system is not satis-
factory. In the future, we are planning to make
the case frames more wide-coverage, and im-
prove the indirect anaphora resolution by con-
sidering larger context and more various factors.
References
Ted Briscoe and John Carroll. 1997. Auto-
matic extraction of subcategorization from
corpora. In Proceedings of the 5th Confer-
ence on Applied Natural Language Process-
ing, pages 356?363.
Udo Hahn, Michael Strube, and Katja Markert.
1996. Bridging textual ellipses. In Proceed-
ings of the 16th International Conference on
Computational Linguistics, pages 496?501.
Daisuke Kawahara and Sadao Kurohashi. 2002.
Fertilization of case frame dictionary for ro-
bust Japanese case analysis. In Proceedings of
the 19th International Conference on Compu-
tational Linguistics, pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automati-
cally constructed case frames and structural
preference of antecedents. In Proceedings of
the 1st International Joint Conference on
Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of
the 3rd International Conference on Lan-
guage Resources and Evaluation, pages 2008?
2013.
Sadao Kurohashi and Makoto Nagao. 1994. A
syntactic analysis method of long Japanese
sentences based on the detection of conjunc-
tive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi and Yasuyuki Sakai. 1999.
Semantic analysis of Japanese noun phrases:
A new approach to dictionary-based under-
standing. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 481?488.
Masaki Murata, Hitoshi Isahara, and Makoto
Nagao. 1999. Pronoun resolution in Japanese
sentences using surface expressions and exam-
ples. In Proceedings of the ACL?99 Workshop
on Coreference and Its Applications, pages
39?46.
Massimo Poesio, Tomonori Ishikawa,
Sabine Schulte im Walde, and Renata
Vieira. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proceedings of the
3rd International Conference on Language
Resources and Evaluation, pages 1220?1224.
Michael Strube and Udo Hahn. 1999. Func-
tional centering ? grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Jun-ichi Tajika, editor. 1997. Reikai Syogaku
Kokugojiten. Sanseido.
Renata Vieira and Massimo Poesio. 2000. An
empirically based system for processing defi-
nite descriptions. Computational Linguistics,
26(4):539?592.
PP-Attachment Disambiguation Boosted by
a Gigantic Volume of Unambiguous Examples
Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo,
7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan
{kawahara, kuro}@kc.t.u-tokyo.ac.jp
Abstract. We present a PP-attachment disambiguation method based
on a gigantic volume of unambiguous examples extracted from raw cor-
pus. The unambiguous examples are utilized to acquire precise lexical
preferences for PP-attachment disambiguation. Attachment decisions are
made by a machine learning method that optimizes the use of the lexical
preferences. Our experiments indicate that the precise lexical preferences
work effectively.
1 Introduction
For natural language processing (NLP), resolving various ambiguities is a fun-
damental and important issue. Prepositional phrase (PP) attachment ambigu-
ity is one of the structural ambiguities. Consider, for example, the following
sentences [1]:
(1) a. Mary ate the salad with a fork.
b. Mary ate the salad with croutons.
The prepositional phrase in (1a) ?with a fork? modifies the verb ?ate?, because
?with a fork? describes how the salad is eaten. The prepositional phrase in (1b)
?with croutons? modifies the noun ?the salad?, because ?with croutons? de-
scribes the salad. To disambiguate such PP-attachment ambiguity, some kind of
world knowledge is required. However, it is currently difficult to give such world
knowledge to computers, and this situation makes PP-attachment disambigua-
tion difficult. Recent state-of-the-art parsers perform with the practical accuracy,
but seem to suffer from the PP-attachment ambiguity [2, 3].
For NLP tasks including PP-attachment disambiguation, corpus-based ap-
proaches have been the dominant paradigm in recent years. They can be divided
into two classes: supervised and unsupervised. Supervised methods automati-
cally learn rules from tagged data, and achieve good performance for many NLP
tasks, especially when lexical information, such as words, is given. Such methods,
however, cannot avoid the sparse data problem. This is because tagged data are
not sufficient enough to discriminate a large variety of lexical information. To
deal with this problem, many smoothing techniques have been proposed.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 188?198, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
PP-Attachment Disambiguation Boosted by Unambiguous Examples 189
The other class for corpus-based approaches is unsupervised learning. Unsu-
pervised methods take advantage of a large number of data that are extracted
from large raw corpora, and thus can alleviate the sparse data problem. How-
ever, the problem is their low performance compared with supervised methods,
because of the use of unreliable information.
For PP-attachment disambiguation, both supervised and unsupervised meth-
ods have been proposed, and supervised methods have achieved better perfor-
mance (e.g., 86.5% accuracy by [1]). Previous unsupervised methods tried to ex-
tract reliable information from large raw corpora, but the extraction heuristics
seem to be inaccurate [4, 5]. For example, Ratnaparkhi extracted unambiguous
word triples of (verb, preposition, noun) or (noun, preposition, noun), and re-
ported that their accuracy was 69% [4]. This means that the extracted triples
are not truly unambiguous, and this inaccurate treatment may have led to low
PP-attachment performance (81.9%).
This paper proposes a PP-attachment disambiguation method based on an
enormous amount of truly unambiguous examples. The unambiguous examples
are extracted from raw corpus using some heuristics inspired by the following
example sentences in [6]:
(2) a. She sent him into the nursery to gather up his toys.
b. The road to London is long and winding.
In these sentences, the underlined PPs are unambiguously attached to the
double-underlined verb or noun. The extracted unambiguous examples are uti-
lized to acquire precise lexical preferences for PP-attachment disambiguation.
Attachment decisions are made by a machine learning technique that optimizes
the use of the lexical preferences. The point of our work is to use a ?gigantic?
volume of ?truly? unambiguous examples. The use of only truly unambiguous
examples leads to statistics of high-quality and good performance of disambigua-
tion in spite of the learning from raw corpus. Furthermore, by using a gigantic
volume of data, we can alleviate the influence of the sparse data problem.
The remainder of this paper is organized as follows. Section 2 briefly describes
the globally used training and test set of PP-attachment. Section 3 summarizes
previous work for PP-attachment. Section 4 describes a method of calculating
lexical preference statistics from a gigantic volume of unambiguous examples.
Section 5 is devoted to our PP-attachment disambiguation algorithm. Section
6 presents the experiments of our disambiguation method. Section 7 gives the
conclusions.
2 Tagged Data for PP-Attachment
The PP-attachment data with correct attachment site are available 1. These data
were extracted from Penn Treebank [7] by the IBM research group [8]. Hereafter,
we call these data ?IBM data?. Some examples in the IBM data are shown
in Table 1.
1 Available at ftp://ftp.cis.upenn.edu/pub/adwait/PPattachData/
190 D. Kawahara and S. Kurohashi
Table 1. Some Examples of the IBM data
v n1 p n2 attach
join board as director V
is chairman of N.V. N
using crocidolite in filters V
bring attention to problem V
is asbestos in product N
making paper for filters N
including three with cancer N
Table 2. Various Baselines and Upper Bounds of PP-Attachment Disambiguation
method accuracy
always N 59.0%
N if p is ?of?; otherwise V 70.4%
most likely for each preposition 72.2%
average human (only quadruple) 88.2%
average human (whole sentence) 93.2%
The data consist of 20,801 training and 3,097 test tuples. In addition, a de-
velopment set of 4,039 tuples is provided. Various baselines and upper bounds of
PP-Attachment disambiguation are shown in Table 2. All the accuracies except
the human performances are on the IBM data. The human performances were
reported by [8].
3 Related Work
There have been lots of supervised approaches for PP-attachment disambigua-
tion. Most of them used the IBM data for their training and test data.
Ratnaphakhi et al proposed a maximum entropy model considering words
and semantic classes of quadruples, and performed with 81.6% accuracy [8].
Brill and Resnik presented a transformation-based learning method [9]. They
reported 81.8% accuracy, but they did not use the IBM data 2. Collins and
Brooks used a probabilistic model with backing-off to smooth the probabili-
ties of unseen events, and its accuracy was 84.5% [10]. Stetina and Nagao used
decision trees combined with a semantic dictionary [11]. They achieved 88.1%
accuracy, which is approaching the human accuracy of 88.2%. This great per-
formance is presumably indebted to the manually constructed semantic dictio-
nary, which can be regarded as a part of world knowledge. Zavrel et al em-
ployed a nearest-neighbor method, and its accuracy was 84.4% [12]. Abney et
al. proposed a boosting approach, and yielded 84.6% accuracy [13]. Vanschoen-
winkel and Manderick introduced a kernel method into PP-attachment disam-
2 The accuracy on the IBM data was 81.9% [10].
PP-Attachment Disambiguation Boosted by Unambiguous Examples 191
biguation, and attained 84.8% accuracy [14]. Zhao and Lin proposed a nearest-
neighbor method with contextually similar words learned from large raw corpus
[1]. They achieved 86.5% accuracy, which is the best performance among previ-
ous methods for PP-attachment disambiguation without manually constructed
knowledge bases.
There have been several unsupervised methods for PP-attachment disam-
biguation. Hindle and Rooth extracted over 200K (v, n1, p) triples with ambigu-
ous attachment sites from 13M words of AP news stories [15]. Their disambigua-
tion method used lexical association score, and performed at 75.8% accuracy on
their own data set. Ratnaparkhi collected 910K unique unambiguous triples (v,
p, n2) or (n1, p, n2) from 970K sentences of Wall Street Journal, and pro-
posed a probabilistic model based on cooccurrence values calculated from the
collected data [4]. He reported 81.9% accuracy. As previously mentioned, the
accuracy was possibly lowered by the inaccurate (69% accuracy) extracted ex-
amples. Pantel and Lin extracted ambiguous 8,900K quadruples and unambigu-
ous 4,400K triples from 125M word newspaper corpus [5]. They utilized scores
based on cooccurrence values, and resulted in 84.3% accuracy. The accuracy of
the extracted unambiguous triples are unknown, but depends on the accuracy of
their parser.
There is a combined method of supervised and unsupervised approaches.
Volk combined supervised and unsupervised methods for PP-attachment disam-
biguation for German [16]. He extracted triples that are possibly unambiguous
from 5.5M words of a science magazine corpus, but these triples were not truly
unambiguous. His unsupervised method is based on cooccurrence probabilities
learned from the extracted triples. His supervised method adopted the backed-
off model by Collins and Brooks. This model is learned the model from 5,803
quadruples. Its accuracy on a test set of 4,469 quadruples was 73.98%, and was
boosted to 80.98% by the unsupervised cooccurrence scores. However, his work
was constrained by the availability of only a small tagged corpus, and thus it
is unknown whether such an improvement can be achieved if a larger size of a
tagged set like the IBM data is available.
4 Acquiring Precise Lexical Preferences from Raw
Corpus
We acquire lexical preferences that are useful for PP-attachment disambiguation
from a raw corpus. As such lexical preferences, cooccurrence statistics between
the verb and the prepositional phrase or the noun and the prepositional phrase
are used. These cooccurrence statistics can be obtained from a large raw corpus,
but the simple use of such a raw corpus possibly produces unreliable statistics.
We extract only truly unambiguous examples from a huge raw corpus to acquire
precise preference statistics.
This section first mentions the raw corpus, and then describes how to extract
truly unambiguous examples. Finally, we explain our calculation method of the
lexical preferences.
192 D. Kawahara and S. Kurohashi
4.1 Raw Corpus
In our approach, a large volume of raw corpus is required. We extracted raw
corpus from 200M Web pages that had been collected by a Web crawler for
a month [17]. To obtain the raw corpus, each Web page is processed by the
following tools:
1. sentence extracting
Sentences are extracted from each Web page by a simple HTML parser.
2. tokenizing
Sentences are tokenized by a simple tokenizer.
3. part-of-speech tagging
Tokenized sentences are given part-of-speech tags by Brill tagger [18].
4. chunking
Tagged sentences are chunked by YamCha chunker [19].
By the above procedure, we acquired 1,300M chunked sentences, which con-
sist of 21G words, from the 200M Web pages.
4.2 Extraction of Unambiguous Examples
Unambiguous examples are extracted from the chunked sentences. Our heuristics
to extract truly unambiguous examples were decided in the light of the following
two types of unambiguous examples in [6].
(3) a. She sent him into the nursery to gather up his toys.
b. The road to London is long and winding.
The prepositional phrase ?into the nursery? in (3a) must attach to the verb
?sent?, because attachment to a pronoun like ?him? is not possible. The prepo-
sitional phrase ?to London? in (3b) must attach to the noun ?road?, because
there are no preceding possible heads.
We use the following two heuristics to extract unambiguous examples like
the above.
? To extract an unambiguous triple (v, p, n2) like (3a), a verb followed by a
pronoun and a prepositional phrase is extracted.
? To extract an unambiguous triple (n1, p, n2) like (3b), a noun phrase followed
by a prepositional phrase at the beginning of a sentence is extracted.
4.3 Post-processing of Extracted Examples
The extracted examples are processed in the following way:
? For verbs (v):
? Verbs are reduced to their lemma.
? For nouns (n1, n2):
? 4-digit numbers are replaced with <year>.
PP-Attachment Disambiguation Boosted by Unambiguous Examples 193
? All other strings of numbers were replaced with <num>.
? All words at the beginning of a sentence are converted into lower case.
? All words starting with a capital letter followed by one or more lower
case letters were replaced with <name>.
? All other words are reduced to their singular form.
? For prepositions (p):
? Prepositions are converted into lower case.
As a result, 21M (v, p, n2) triples and 147M (n, p, n2) triples,in total 168M
triples, were acquired.
4.4 Calculation of Lexical Preferences for PP-Attachment
From the extracted truly unambiguous examples, lexical preferences for PP-
attachment are calculated. As the lexical preferences, pointwise mutual informa-
tion between v and ?p n2? is calculated from cooccurrence counts of v and ?p
n2? as follows3:
I(v, pn2) = log
f(v,pn2)
N
f(v)
N
f(pn2)
N
(1)
where N denotes the total number of the extracted examples (168M), f(v) and
f(pn2) is the frequency of v and ?p n2?, respectively, and f(v, pn2) is the cooc-
currence frequency of v and pn2.
Similarly, pointwise mutual information between n1 and ?p n2? is calculated
as follows:
I(n1, pn2) = log
f(n1,pn2)
N
f(n1)
N
f(pn2)
N
(2)
The preference scores ignoring n2 are also calculated:
I(v, p) = log
f(v,p)
N
f(v)
N
f(p)
N
(3)
I(n1, p) = log
f(n1,p)
N
f(n1)
N
f(p)
N
(4)
5 PP-Attachment Disambiguation Method
Our method for resolving PP-attachment ambiguity takes a quadruple (v, n1, p,
n2) as input, and classifies it as V or N. The class V means that the prepositional
3 As in previous work, simple probability ratios can be used, but a preliminary ex-
periment on the development set shows their accuracy is worse than the mutual
information by approximately 1%.
194 D. Kawahara and S. Kurohashi
phrase ?p n2? modifies the verb v. The class N means that the prepositional
phrase modifies the noun n1.
To solve this binary classification task, we employ Support Vector Machines
(SVMs), which have been well-known for their good generalization
performance [20].
We consider the following features:
? LEX: word of each quadruple
To reduce sparse data problems, all verbs and nouns are pre-processed using
the method stated in Section 4.3.
? POS: part-of-speech information of v, n1 and n2
POSs of v, n1 and n2 provide richer information than just verb or noun,
such as inflectional information.
The IBM data, which we use for our experiments, do not contain POS in-
formation. To obtain POS tags of a quadruple, we extracted the original
sentence of each quadruple from Penn Treebank, and applied the Brill tag-
ger to it. Instead of using the correct POS information in Penn Treebank,
we use the POS information automatically generated by the Brill tagger to
keep the experimental environment realistic.
? LP: lexical preferences
Given a quadruple (v, n1, p, n2), four statistics calculated in Section4.4,
I(v, pn2), I(n1, pn2), I(v, p) and I(n1, p), are given to SVMs as features.
6 Experiments and Discussions
We conducted experiments on the IBM data. As an SVM implementation, we em-
ployed SVMlight [21]. To determine parameters of SVMlight, we run our method
on the development data set of the IBM data. As the result, parameter j, which
is used to make much account of training errors on either class [22], is set to
0.65, and 3-degree polynomial kernel is chosen. Table 3 shows the experimen-
tal results for PP-attachment disambiguation. For comparison, we conducted
several experiments with different feature combinations in addition to our pro-
posed method ?LEX+POS+LP?, which uses all of the three types of features.
The proposed method ?LEX+POS+LP? surpassed ?LEX?, which is the stan-
dard supervised model, and furthermore, significantly outperformed all other
Table 3. PP-Attachment Accuracies
LEX POS LP accuracy?
85.34? ?
85.05?
83.73? ?
84.66? ?
86.44? ? ?
87.25
PP-Attachment Disambiguation Boosted by Unambiguous Examples 195
Table 4. Precision and Recall for Each Attachment Site (?LEX+POS+LP? model)
class precision recall
V 1067/1258 (84.82%) 1067/1271 (83.95%)
N 1635/1839 (88.91%) 1635/1826 (89.54%)
Table 5. PP-Attachment Accuracies of Previous Work
method accuracy
our method SVM 87.25%
supervised
Ratnaphakhi et al, 1994 ME 81.6%
Brill and Resnik, 1994 TBL 81.9%
Collins and Brooks, 1995 back-off 84.5%
Zavrel et al, 1997 NN 84.4%
Stetina and Nagao, 1997 DT 88.1%
Abney et al, 1999 boosting 84.6%
Vanschoenwinkel and Manderick, 2003 SVM 84.8%
Zhao and Lin, 2004 NN 86.5%
unsupervised
Ratnaparkhi, 1998 - 81.9%
Pantel and Lin, 2000 - 84.3%
ME: Maximum Entropy, TBL: Transformation-Based Learning,
DT: Decision Tree, NN: Nearest Neighbor
configurations (McNemar?s test; p < 0.05). ?LEX+POS? model was a little
worse than ?LEX?, but ?LEX+POS+LP? was better than ?LEX+LP? (and
also ?POS+LP? was better than ?LP?). From these results, we can see that
?LP? worked effectively, and the combination of ?LEX+POS+LP? was very ef-
fective. Table 4 shows the precision and recall of ?LEX+POS+LP? model for
each class (N and V).
Table 5 shows the accuracies achieved by previous methods. Our performance
is higher than any other previous methods except [11]. The method of Stetina
and Nagao employed a manually constructed sense dictionary, and this conduces
to good performance.
Figure 1 shows the learning curve of ?LEX? and ?LEX+POS+LP? models
while changing the number of tagged data. When using all the training data,
?LEX+POS+LP? was better than ?LEX?by approximately 2%. Under the con-
dition of small data set, ?LEX+POS+LP? was better than ?LEX?by approxi-
mately 5%. In this situation, in particular, the lexical preferences worked more
effectively.
Figure 2 shows the learning curve of ?LEX+POS+LP? model while changing
the number of used unambiguous examples. The accuracy rises rapidly by 10M
unambiguous examples, and then drops once, but after that rises slightly. The
best score 87.28% was achieved when using 77M unambiguous examples.
196 D. Kawahara and S. Kurohashi
 76
 78
 80
 82
 84
 86
 88
 0  5000  10000  15000  20000  25000
Ac
c u
ra
c y
Number of Tagged Data
"LEX+POS+LP"
"LEX"
Fig. 1. Learning Curve of PP-Attachment Disambiguation
 85.2
 85.4
 85.6
 85.8
 86
 86.2
 86.4
 86.6
 86.8
 87
 87.2
 87.4
 0  2e+07  4e+07  6e+07  8e+07  1e+08  1.2e+08  1.4e+08  1.6e+08  1.8e+08
Ac
c u
ra
c y
Number of Used Unambiguous Examples
Fig. 2. Learning Curve of PP-Attachment Disambiguation while changing the number
of used unambiguous examples
7 Conclusions
This paper has presented a corpus-based method for PP-attachment disam-
biguation. Our approach utilizes precise lexical preferences learned from a gi-
gantic volume of truly unambiguous examples in raw corpus. Attachment deci-
sions are made using a machine learning method that incorporates these lexi-
cal preferences. Our experiments indicated that the precise lexical preferences
worked effectively.
PP-Attachment Disambiguation Boosted by Unambiguous Examples 197
In the future, we will investigate useful contextual features for PP-
attachment, because human accuracy improves by around 5% when they see
more than just a quadruple.
Acknowledgements
We would like to thank Prof. Kenjiro Taura for allowing us to use an enormous
volume of Web corpus. We also would like to express our thanks to Tomohide
Shibata for his constructive and fruitful discussions.
References
1. Zhao, S., Lin, D.: A nearest-neighbor method for resolving pp-attachment ambigu-
ity. In: Proceedings of the 1st International Joint Conference on Natural Language
Processing. (2004) 428?434
2. Collins, M.: Head-Driven Statistical Models for Natural Language Parsing. PhD
thesis, University of Pennsylvania (1999)
3. Charniak, E.: A maximum-entropy-inspired parser. In: Proceedings of the 1st
Meeting of the North American Chapter of the Association for Computational
Linguistics. (2000) 132?139
4. Ratnaparkhi, A.: Statistical models for unsupervised prepositional phrase attach-
ment. In: Proceedings of the 17th International Conference on Computational
Linguistics. (1998) 1079?1085
5. Pantel, P., Lin, D.: An unsupervised approach to prepositional phrase attachment
using contextually similar words. In: Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics. (2000) 101?108
6. Manning, C., Schu?tze, H.: Foundations of Statistical Natural Language Processing.
MIT Press (1999)
7. Marcus, M., Santorini, B., Marcinkiewicz, M.: Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics 19 (1994) 313?330
8. Ratnaparkhi, A., Reynar, J., Roukos, S.: A maximum entropy model for preposi-
tional phrase attachment. In: Proceedings of the ARPA Human Language Tech-
nology Workshop. (1994) 250?255
9. Brill, E., Resnik, P.: A rule-based approach to prepositional phrase attachment
disambiguation. In: Proceedings of the 15th International Conference on Compu-
tational Linguistics. (1994) 1198?1204
10. Collins, M., Brooks, J.: Prepositional phrase attachment through a backed-off
model. In: Proceedings of the 3rd Workhop on Very Large Corpora. (1995) 27?38
11. Stetina, J., Nagao, M.: Corpus based pp attachment ambiguity resolution with a
semantic dictionary. In: Proceedings of the 5th Workhop on Very Large Corpora.
(1997) 66?80
12. Zavrel, J., Daelemans, W., Veenstra, J.: Resolving pp attachment ambiguities
with memory-based learning. In: Proceedings of the Workshop on Computational
Natural Language Learning. (1997) 136?144
13. Abney, S., Schapire, R., Singer, Y.: Boosting applied to tagging and pp attach-
ment. In: Proceedings of 1999 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Corpora. (1999) 38?45
198 D. Kawahara and S. Kurohashi
14. Vanschoenwinkel, B., Manderick, B.: A weighted polynomial information gain
kernel for resolving pp attachment ambiguities with support vector machines. In:
Proceedings of the 18th International Joint Conference on Artificial Intelligence.
(2003) 133?138
15. Hindle, D., Rooth, M.: Structural ambiguity and lexical relations. Computational
Linguistics 19 (1993) 103?120
16. Volk, M.: Combining unsupervised and supervised methods for pp attachment
disambiguation. In: Proceedings of the 19th International Conference on Compu-
tational Linguistics. (2002) 1065?1071
17. Takahashi, T., Soonsang, H., Taura, K., Yonezawa, A.: World wide web crawler. In:
Poster Proceedings of the 11th International World Wide Web Conference. (2002)
18. Brill, E.: Transformation-based error-driven learning and natural language process-
ing: A case study in part-of-speech tagging. Computational Linguistics 21 (1995)
543?565
19. Kudo, T., Matsumoto, Y.: Chunking with support vector machines. In: Proceed-
ings of the 2nd Meeting of the North American Chapter of the Association for
Computational Linguistics. (2001) 192?199
20. Vapnik, V.: The Nature of Statistical Learning Theory. Springer (1995)
21. Joachims, T.: 11. In: Making Large-Scale Support Vector Machine Learning Prac-
tical, in Advances in Kernel Methods - Support Vector Learning. MIT Press (1999)
169?184
22. Morik, K., Brockhausen, P., Joachims, T.: Combining statistical learning with a
knowledge-based approach ? a case study in intensive care monitoring. In: Proceed-
ings of the 16th International Conference on Machine Learning. (1999) 268?277
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 682 ? 693, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Automatic Acquisition of Basic Katakana Lexicon  
from a Given Corpus 
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao Kurohashi 
University of Tokyo, 7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan 
{nakazawa, kawahara, kuro}@kc.t.u-tokyo.ac.jp 
Abstract. Katakana, Japanese phonogram mainly used for loan words, is a 
trou-blemaker in Japanese word segmentation. Since Katakana words are heavily 
domain-dependent and there are many Katakana neologisms, it is almost 
impossible to construct and maintain Katakana word dictionary by hand. This 
paper proposes an automatic segmentation method of Japanese Katakana 
compounds, which makes it possible to construct precise and concise Katakana 
word dictionary automati-cally, given only a medium or large size of Japanese 
corpus of some domain. 
1   Introduction
Handling words properly is very important for Natural Language Processing. Words 
are basic unit to assign syntactic/semantic information manually, basic unit to acquire 
knowledge based on frequencies and co-occurrences, and basic unit to access texts in 
Information Retrieval.  
Languages with explicit word boundaries, like white spaces in English, do not suffer 
from this issue so severely, though it is a bit troublesome to handle compounds and 
hyphenation appropriately. On the other hand, languages without explicit boundaries 
such as Japanese always suffer from this issue.  
Japanese character set and their usage. Here, we briefly explain Japanese character set 
and their usage. Japanese uses about 6,000 ideogram, Kanji characters, 83 phonogram, 
Hiragana, and another 86 phonogram, Katakana.  
Kanji is used for Japanese time-honored nouns (including words imported from 
China ancient times) and stems of verbs and adjectives; Hiragana is used for function 
words such as postpositions and auxiliary verbs, and endings of verbs and adjectives; 
Katakana is used for loan words, mostly from the West, as transliterations.  
Japanese is very active to naturalize loan words. Neologisms in special/technical 
domains are often transliterated into Katakana words without translations, or even if 
there are translations, Katakana transliterations are more commonly used in many 
cases. For example,  , transliteration of ?computer? is more commonly 
used than the translation, (keisanki).  
Even for some time-honored Japanese nouns, both Japanese nouns and 
translitera-tions of their English translations are used together these days, and the use of 
        Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus 683 
translit-erations is increasing, such as  , transliteration of ?desk work? vs. 
 (tsukue shigoto). Furthermore, some Japanese nouns, typically the names of 
animals, plants, and food, which can be written in Kanji or Hiragana, are also written in 
Katakana sometimes [4, 6].  
Word segmentation and Katakana words. Let us go back to the word segmentation 
issue. Japanese word segmentation is performed like this: Japanese words are registered 
into the dictionary; given an input sentence, all possible words embedded in the sentence 
and their connections are checked by looking up the dictionary and some connectivity 
grammar; then the most plausible word sequence is selected. The criteria of selecting the 
best word sequence were simple heuristic rules preferring longer words in earlier times, 
and some cost calculation based on manual rules or using some training data, these days.  
Such a segmentation process is in practice not so difficult for Kanji-Hiragana string. 
First of all, since Kanji words and Hiragana words are fairly stable excepting proper 
nouns, they are most perfectly registered in the dictionary. Then, the orthogonal usage of 
Kanji and Hiragana mentioned above makes the segmentation rather simple, as follows:  
                   
(Kare      wa  daigaku      ni  kayou) 
he     postp.   Univ.    postp.     go 
Kanji compound words can cause a segmentation problem. However, since large 
num-ber of Kanji characters lead fairly sparse space of Kanji words, most Kanji 
compounds can be segmented unambiguously.  
A real troublemaker is Katakana words, which are sometimes very long compounds 
such as 	
?extra vergin olive oil? and 
Converting Text into Agent Animations: Assigning Gestures to Text 
Yukiko I. Nakano?   Masashi Okamoto?    Daisuke Kawahara?   Qing Li?   Toyoaki Nishida?
?Japan Science and Technology Agency 
2-5-1 Atago, Minato-ku, Tokyo, 105-6218 Japan
?The University of Tokyo  
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656 Japan
{nakano, okamoto, kawahara, liqing, nishida}@kc.t.u-tokyo.ac.jp 
Abstract 
This paper proposes a method for assigning 
gestures to text based on lexical and syntactic 
information. First, our empirical study identi-
fied lexical and syntactic information strongly 
correlated with gesture occurrence and sug-
gested that syntactic structure is more useful 
for judging gesture occurrence than local syn-
tactic cues. Based on the empirical results, we 
have implemented a system that converts text 
into an animated agent that gestures and 
speaks synchronously. 
1 Introduction  
The significant advances in computer graphics over the 
last decade have improved the expressiveness of ani-
mated characters and have promoted research on inter-
face agents, which serve as mediators of human-
computer interactions. As an interface agent has an em-
bodied figure, it can use its face and body to display 
nonverbal behaviors while speaking.  
Previous studies in human communication suggest 
that gestures in particular contribute to better under-
standing of speech. About 90% of all gestures by 
speakers occur when the speaker is actually uttering 
something (McNeill, 1992). Experimental studies have 
shown that spoken sentences are heard twice as accu-
rately when they are presented along with a gesture 
(Berger & Popelka, 1971). Comprehension of a descrip-
tion accompanied by gestures is better than that accom-
panied by only the speaker?s face and lip movements 
(Rogers, 1978). These previous studies suggest that 
generating appropriate gestures synchronized with 
speech is a promising approach to improving the per-
formance of interface agents. In previous studies of 
multimodal generation, gestures were determined ac-
cording to the instruction content (Andre, Rist, & Mul-
ler, 1999; Rickel & Johnson, 1999), the task situation in 
a learning environment (Lester, Stone, & Stelling, 
1999), or the agent?s communicative goal in conversa-
tion (Cassell et al, 1994; Cassell, Stone, & Yan, 2000). 
These approaches, however, require the contents devel-
oper (e.g., a school teacher designing teaching materi-
als) to be skilled at describing semantic and pragmatic 
relations in logical form. A different approach, (Cassell, 
Vilhjalmsson, & Bickmore, 2001) proposes a toolkit 
that takes plain text as input and automatically suggests 
a sequence of agent behaviors synchronized with the 
synthesized speech. However, there has been little work 
in computational linguistics on how to identify and ex-
tract linguistic information in text in order to generate 
gestures.  
Our study has addressed these issues by considering 
two questions. (1) Is the lexical and syntactic informa-
tion in text useful for generating meaningful gestures? 
(2) If so, how can the information be extracted from the 
text and exploited in a gesture decision mechanism in 
an interface agent? Our goal is to develop a media con-
version technique that generates agent animations syn-
chronized with speech from plain text.  
This paper is organized as follows. The next section 
reviews theoretical issues about the relationships be-
tween gestures and syntactic information. The empirical 
study we conducted based on these issues is described 
in Sec. 3. In Sec. 4 we describe the implementation of 
our presentation agent system, and in the last section we 
discuss future directions.  
2 Linguistic Theories and Gesture Studies 
In this section we review linguistic theories and discuss 
the relationship between gesture occurrence and syntac-
tic information.  
Linguistic quantity for reference: McNeill (McNeill, 
1992) used communicative dynamism (CD), which 
represents the extent to which the message at a given 
point is ?pushing the communication forward? (Firbas, 
1971), as a variable that correlates with gesture occur-
rence. The greater the CD, the more probable the occur-
rence of a gesture. As a measure of CD, McNeill chose 
the amount of linguistic material used to make the refer-
ence (Givon, 1985). Pronouns have less CD than full 
nominal phrases (NPs), which have less CD than modi-
fied full NPs. This implies that the CD can be estimated 
by looking at the syntactic structure of a sentence.  
Theme/Rheme: McNeill also asserted that the theme 
(Halliday, 1967) of a sentence usually has the least CD 
and is not normally accompanied by a gesture. Gestures 
usually accompany the rhemes, which are the elements 
of a sentence that plausibly contribute information 
about the theme, and thus have greater CD. In Japanese 
grammar there is a device for marking the theme explic-
itly. Topic marking postpositions (or ?topic markers?), 
typically ?wa,? mark a nominal phrase as the theme. 
This facilitates the use of syntactic analysis to identify 
the theme of a sentence. Another interesting aspect of 
information structure is that in English grammar, a wh-
interrogative (what, how, etc.) at the beginning of a 
sentence marks the theme and indicates that the content 
of the theme is the focus (Halliday, 1967). However, we 
do not know whether such a special type of theme is 
more likely to co-occur with a gesture or not.  
Given/New: Given and new information demonstrate 
an aspect of theme and rheme. Given information usu-
ally has a low degree of rhematicity, while new infor-
mation has a high degree. This implies that rhematicity 
can be estimated by determining whether the NP is the 
first mention (i.e., new information) or has already been 
mentioned (i.e., old or given information).  
Contrastive relationship: Prevost (1996) reported that 
intonational accent is often used to mark an explicit 
contrast among the salient discourse entities. On the 
basis of this finding and Kendon?s theory about the rela-
tionship between intonation phrases and gesture place-
ments (Kendon, 1972), Cassell & Prevost (1996) 
developed a method for generating contrastive gestures 
from a semantic representation. In syntactic analysis, a 
contrastive relation is usually expressed as a coordina-
tion, which is a syntactic structure including at least two 
conjuncts linked by a conjunction.  
Figure 1 shows an example of the correlation between 
gesture occurrence and the dependency structure of a 
Japanese sentence. Bunsetsu units (8)-(9) and (10)-(13) 
in the figure are conjuncts. A ?bunsetsu unit? in Japa-
nese corresponds to a phrase in English, such as a noun 
phrase or a prepositional phrase. Each conjunct is ac-
companied by a gesture. Bunsetsu (14) is a complement 
containing a verbal phrase; it depends on bunsetsu (15), 
which is an NP. Thus, bunsetsu (15) is a modified full 
NP and thus has large linguistic quantity.  
3 Empirical Study 
To identify linguistic features that might 
be useful for judging gesture occurrence, 
we videotaped seven presentation talks 
and transcribed three minutes for each of 
them. The collected data included 2124 
bunsetsu units and 343 gestures. 
Gesture Annotation: Three coders dis-
cussed how to code the half the data and reached a con-
sensus on gesture occurrence. After this consensus on 
the coding scheme was established1, one of the coders 
annotated the rest of the data. A gesture consists of 
preparation, stroke, and retraction (McNeill, 1992), and 
a stroke co-occurs with the most prominent syllable 
(Kendon, 1972). Thus, we annotated the stroke time as 
well as the start and end time of each gesture.  
Linguistic Analysis: Each bunsetsu unit was automati-
cally annotated with linguistic information using a Japa-
nese syntactic analyzer (Kurohashi & Nagao, 1994)2. 
The information was determined by asked the following 
questions for each bunsetsu unit. 
(a) If it is an NP, is it modified by a clause or a com-
plement? 
(b) If it is an NP, what type of postpositional particle 
marks its end (e.g., ?wa?, ?ga?, ?wo?)? 
(c) Is it a wh-interrogative? 
(d) Are all the content words in the bunsetsu unit have 
mentioned in a preceding sentence? 
(e) Is it a constituent of a coordination? 
Moreover, as we noticed that some lexical entities fre-
quently co-occurred with a gesture in our data, we used 
the syntactic analyzer to annotate additional lexical in-
formation based on the following questions.  
(f) Is the bunsetsu unit an emphatic adverbial phrase 
(e.g., very, extremely), or is it modified by a pre-
ceding emphatic adverb (e.g., very important is-
sue)? 
(g) Does it include a cue word (e.g., now, therefore)? 
(h) Does it include a numeral (e.g., thousands of people, 
99 times)? 
We then investigated the correlation between these 
lexical and syntactic features and the occurrence of ges-
ture strokes.  
Result: The results are summarized in Table 1. The 
baseline gesture occurrence frequency was 10.1% per 
bunsetsu unit (a gesture occurred once about every ten 
                                                          
1 Inter-coder reliability among the three coders in catego-
rizing the gestures (beat, iconic, etc.) was sufficiently high 
(Kappa = 0.81). Although we did not measure agreement on 
gesture occurrence itself, this result suggests that the coders 
had very similar schemes for recognizing gestures.  
2 To prevent the effects of parsing errors, errors in syntac-
tic dependency analysis were corrected manually for about 
13% of the data.  
shindo-[ga] atae-rareru-to-ka sore-[ni] kawaru kasokudo-[ga] atae-rareru-to iu-youna jyoukyou-de
<parallel>
<nominal>
<complement>
<verbal>
(8) (9) (10) (11) (12) (13) (14) (15)
?a situation where seismic intensity is given, or degree of acceleration is given?
Figure 1: Example analysis of syntactic dependency  
Underlined phrases are accompanied by gestures, and strokes occur at dou-
ble-underlined parts. Case markers are enclosed by square brackets [ ]. 
Table 1. Summary of results 
Case Syntactic/lexical information of a bunsetsu unit 
Gesture 
occurrence
C1 (a) NP modified by clause 0.382 
C2 
Quantity of 
modification Pronouns, other 
types of NPs 
(b) Case marker = ?wo? 
& (d) New information 
0.281 
C3 (c) WH-interrogative 0.414 
C4 (e) Coordination 0.477 
C5 (f) Emphatic adverb itself 0.244 
C6 
Emphatic 
adverbial phrase (f?) Following emphatic adverb 0.350 
C7 (g) Cue word 0.415 
C8 (h) Numeral 0.393 
C9 Other (baseline) 0.101 
 
bunsetsu units). A gesture stroke most frequently co-
occurred with a bunsetsu unit forming a coordination 
(47.7%). When an NP was modified by a full clause, it 
was accompanied by a gesture 38.2% of the time. For 
the other types of noun phrases, including pronouns, 
when an accusative case marked with case marker ?wo? 
was new information (i.e., it was not mentioned in a 
previous sentence), a gesture co-occurred with the 
phrase 28.1% of the time. Moreover, gesture strokes 
frequently co-occurred with wh-interrogatives (41.4%), 
cue words (41.5%), and numeral words (39.3%). Ges-
ture strokes frequently occurred right after emphatic 
adverbs (35%) rather than with the adverb (24.4%).  
These cases listed in Table 1 had a 3 to 5 times higher 
probability of gesture occurrence than the baseline and 
accounted for 75% of all the gestures observed in the 
data. Our results suggest that these types of lexical and 
syntactic information can be used to distinguish be-
tween where a gesture should be assigned and where 
one should not be assigned. They also indicate that the 
syntactic structure of a sentence more strongly affects 
gesture occurrence than theme or rheme and than given 
or new information specified by local grammatical cues, 
such as topic markers and case markers.  
4 System Implementation 
4.1 Overview 
We used our results to build a presentation agent system, 
SPOC (Stream-oriented Public Opinion Channel).? This 
system enables a user to embody a story (written text) 
as a multimodal presentation featuring video, graphics, 
speech, and character animation. A snapshot of the 
SPOC viewer is shown in Figure 2.  
In order to implement a storyteller in SPOC, we de-
veloped an agent behavior generation system we call 
?CAST (Conversational Agent System for neTwork 
applications).? Taking text input, CAST automatically 
selects agent gestures and other nonverbal behaviors, 
calculates an animation schedule, and produces synthe-
sized voice output for the agent. As shown in Figure 2, 
CAST consists of four main components: (1) the Agent 
Behavior Selection Module (ABS), (2) the Language 
Tagging Module (LTM), (3) the agent animation system, 
and (4) a text-to-speech engine (TTS). The received text 
input is first sent to the ABS. The ABS selects appro-
priate gestures and facial expressions based on the lin-
guistic information calculated by the LTM. It then 
obtains the timing information from the TTS and calcu-
lates a time schedule for the set of agent actions. The 
output from the ABS is a set of animation instructions 
that can be interpreted and executed by the agent anima-
tion system. 
4.2 Determining Agent Behaviors 
Tagging linguistic information: First, the LTM parses 
the input text and calculates the linguistic information 
described in Sec. 3. For example, bunsetsu (9) in Figure 
1 has the following feature set. 
{Text-ID: 1, Sentence-ID: 1, Bunsetsu-ID: 9, Govern: 8, De-
pend-on: 13, Phrase-type: VP, Linguistic-quantity: NA, Case-
marker: NA, WH-interrogative: false, Given/New: new, Coor-
dinate-with: 13, Emphatic-Adv: false, Cue-Word: false, Nu-
meral: false} 
The text ID of this bunsetsu unit is 1, the sentence ID 
is 1, the bunsetsu ID is 9. This bunsetsu governs bun-
setsu 8 and depends on bunsetsu 13. It conveys new 
information and, together with bunsetsu 13, forms a 
parallel phrase.  
Assigning gestures: Then, for each bunsetsu unit, the 
ABS decides whether to assign a gesture or not based 
on the empirical results shown in Table 1. For example, 
bunsetsu unit (9) shown above matches case C4 in Ta-
ble 1, where a bunsetsu unit is a constituent of coordina-
tion. In this case, the system assigns a gesture to the 
bunsetsu with 47.7 % probability. In the current imple-
mentation, if a specific gesture for an emphasized con-
cept is defined in the gesture animation library (e.g., a 
gesture animation expressing ?big?), it is preferred to a 
?beat gesture? (a simple flick of the hand or fingers up 
and down (McNeill, 1992)). If a specific gesture is not 
defined, a beat gesture is used as the default. 
Animation ID
Start/end time
Agent Behavior 
Selection Module 
(ABS) Language Tagging 
Module (LTM)
Text-to-Speech 
(TTS)
S-POC Viewer
Input 
text
CAST
Video
Graphics
Graphics + Camera work
Agent Animation 
System
This is 
our ?
This is 
our 
Figure 2: Overview of CAST and SPOC 
The output of the ABS is stored in XML format. The 
type of action and the start and end times of the action 
are indicated by XML tags. In the example shown in 
Figure 3, the agent first gazes towards the user. It then 
performs contrast gestures at the second and sixth bun-
setsu units and a beat gesture at the eighth bunsetsu unit.  
Finally, the ABS transforms the XML into a time 
schedule by accessing the TTS engine and estimating 
the phoneme and bunsetsu boundary timings. The 
scheduling technique is similar to that described by 
(Cassell et al, 2001). The ABS also assigns visemes for 
the lip-sync and the facial expressions, such as head 
movement, eye gaze, blink, and eyebrow movement.  
5 Discussion and Conclusion 
We have addressed the issues related to assigning ges-
tures to text and converting the text into agent anima-
tions synchronized with speech. First, our empirical 
study identified useful lexical and syntactic information 
for assigning gestures to plain text. Specifically, when a 
bunsetsu unit is a constituent of coordination, gestures 
occur almost half the time. Gestures also frequently co-
occur with nominal phrases modified by a clause. These 
findings suggest that syntactic structure is a stronger 
determinant of gesture occurrence than theme or rheme 
and given or new information specified by local gram-
matical cues.  
We plan to enhance our model by incorporating more 
general discourse level information, though the current 
system exploits cue words as a very partial kind of dis-
course information. For instance, gestures frequently 
occur at episode boundaries. Pushing and popping of a 
discourse segment (Grosz & Sidner, 1986) may also 
affect gesture occurrence. Therefore, by integrating a 
discourse analyzer into the LTM, more general struc-
tural discourse information can be used in the model. 
Another important direction is to evaluate the effective-
ness of agent gestures in actual human-agent interaction. 
We expect that if our model can generate gestures with 
appropriate timing for emphasizing important words 
and phrases, users can perceive agent presentations as 
being more alive and comprehensible. We plan to con-
duct a user study to examine this hypothesis.  
References 
Andre, E., Rist, T., & Muller, J. (1999). Employing AI meth-
ods to control the behavior of animated interface agents. Ap-
plied Artificial Intelligence, 13, 415-448. 
Berger, K. W., & Popelka, G. R. (1971). Extra-facial Gestures 
in Relation to Speech-reading. Journal of Communication 
Disorders, 3, 302-308. 
Cassell, J. et al (1994). Animated Conversation: Rule-Based 
Generation of Facial Expression, Gesture and Spoken Intona-
tion for Multiple Conversational Agents. Paper presented at 
the SIGGRAPH '94. 
Cassell, J., & Prevost, S. (1996). Distribution of Semantic 
Features Across Speech and Gesture by Humans and Com-
puters. Paper presented at the Workshop on the Integration of 
Gesture in Language and Speech. 
Cassell, J., Stone, M., & Yan, H. (2000). Coordination and 
Context-Dependence in the Generation of Embodied Conver-
sation. Paper presented at the INLG 2000. 
Cassell, J., Vilhjalmsson, H., & Bickmore, T. (2001). BEAT: 
The Behavior Expression Animation Toolkit. Paper presented 
at the SIGGRAPH 01. 
Firbas, J. (1971). On the Concept of Communicative Dyna-
mism in the Theory of Functional Sentence Perspective. Phi-
lologica Pragensia, 8, 135-144. 
Givon, T. (1985). Iconicity, Isomorphism and Non-arbitrary 
Coding in Syntax. In J. Haiman (Ed.), Iconicity in Syntax (pp. 
187-219): John Benjamins. 
Grosz, B., & Sidner, C. (1986). Attention, Intentions, and the 
Structure of Discourse. Computational Linguistics, 12(3), 175-
204. 
Halliday, M. A. K. (1967). Intonation and Grammar in British 
English. The Hague: Mouton. 
Kendon, A. (1972). Some Relationships between Body Mo-
tion and Speech. In A. W. Siegman & B. Pope (Eds.), Studies 
in Dyadic Communication (pp. 177-210). Elmsford, NY: Per-
gamon Press. 
Kurohashi, S., & Nagao, M. (1994). A Syntactic Analysis 
Method of Long Japanese Sentences Based on the Detection 
of Conjunctive Structures. Computational Linguistics, 20(4), 
507-534. 
Lester, J. C., Stone, B., & Stelling, G. (1999). Lifelike Peda-
gogical agents for Mixed-Initiative Problem Solving in Con-
structivist Learning Environments. User Modeling and User-
Adapted Interaction, 9(1-2), 1-44. 
McNeill, D. (1992). Hand and Mind: What Gestures Reveal 
about Thought. Chicago, IL/London, UK: The University of 
Chicago Press. 
Prevost, S. A. (1996). An Informational Structural Approach 
to Spoken Language Generation. Paper presented at the 34th 
Annual Meeting of the Association for Computational Lin-
guistics, Santa Cruz, CA. 
Rickel, J., & Johnson, W. L. (1999). Animated Agents for 
Procedural Training in Virtual Reality: Perception, Cognition 
and Motor Control. Applied Artificial Intelligence, 13(4-5), 
343-382. 
Rogers, W. (1978). The Contribution of Kinesic Illustrators 
towards the Comprehension of Verbal Behavior within Utter-
ances. Human Communication Research, 5, 54-62. 
 
<Gaze type="towards">
shindo-ga
<Gesture_right type="contrast" handshape_right="stroke1@2">
atae-rareru-to-ka
</Gesture_right> 
sore-ni
kawaru
kasokudo-ga
<Gesture_right type="contrast" handshape_right="stroke2@2">
atae-rareru-to
</Gesture_right> 
iu-youna
<Gesture_right type="best" handshape_right="stroke1">
jyoukyou-de
</Gesture_right>
?  
Figure 3: Example of CAST output
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 176?183,
New York, June 2006. c?2006 Association for Computational Linguistics
A Fully-Lexicalized Probabilistic Model
for Japanese Syntactic and Case Structure Analysis
Daisuke Kawahara? and Sadao Kurohashi?
Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
We present an integrated probabilistic
model for Japanese syntactic and case
structure analysis. Syntactic and case
structure are simultaneously analyzed
based on wide-coverage case frames that
are constructed from a huge raw corpus in
an unsupervised manner. This model se-
lects the syntactic and case structure that
has the highest generative probability. We
evaluate both syntactic structure and case
structure. In particular, the experimen-
tal results for syntactic analysis on web
sentences show that the proposed model
significantly outperforms known syntactic
analyzers.
1 Introduction
Case structure (predicate-argument structure or log-
ical form) represents what arguments are related to
a predicate, and forms a basic unit for conveying the
meaning of natural language text. Identifying such
case structure plays an important role in natural lan-
guage understanding.
In English, syntactic case structure can be mostly
derived from word order. For example, the left ar-
gument of the predicate is the subject, and the right
argument of the predicate is the object in most cases.
Blaheta and Charniak proposed a statistical method
?Currently, National Institute of Information and Communi-
cations Technology, JAPAN, dk@nict.go.jp
?Currently, Graduate School of Informatics, Kyoto Univer-
sity, kuro@i.kyoto-u.ac.jp
for analyzing function tags in Penn Treebank, and
achieved a really high accuracy of 95.7% for syn-
tactic roles, such as SBJ (subject) and DTV (da-
tive) (Blaheta and Charniak, 2000). In recent years,
there have been many studies on semantic structure
analysis (semantic role labeling) based on PropBank
(Kingsbury et al, 2002) and FrameNet (Baker et al,
1998). These studies classify syntactic roles into se-
mantic ones such as agent, experiencer and instru-
ment.
Case structure analysis of Japanese is very differ-
ent from that of English. In Japanese, postpositions
are used to mark cases. Frequently used postposi-
tions are ?ga?, ?wo? and ?ni?, which usually mean
nominative, accusative and dative. However, when
an argument is followed by the topic-marking post-
position ?wa?, its case marker is hidden. In addi-
tion, case-marking postpositions are often omitted in
Japanese. These troublesome characteristics make
Japanese case structure analysis very difficult.
To address these problems and realize Japanese
case structure analysis, wide-coverage case frames
are required. For example, let us describe how to
apply case structure analysis to the following sen-
tence:
bentou-wa taberu
lunchbox-TM eat
(eat lunchbox)
In this sentence, taberu (eat) is a verb, and bentou-
wa (lunchbox-TM) is a case component (i.e. argu-
ment) of taberu. The case marker of ?bentou-wa?
is hidden by the topic marker (TM) ?wa?. The an-
alyzer matches ?bentou? (lunchbox) with the most
176
suitable case slot (CS) in the following case frame
of ?taberu? (eat).
CS examples
taberu ga person, child, boy, ? ? ?wo lunch, lunchbox, dinner, ? ? ?
Since ?bentou? (lunchbox) is included in ?wo? ex-
amples, its case is analyzed as ?wo?. As a result, we
obtain the case structure ??:ga bentou:wo taberu?,
which means that ?ga? (nominative) argument is
omitted, and ?wo? (accusative) argument is ?bentou?
(lunchbox). In this paper, we run such case structure
analysis based on example-based case frames that
are constructed from a huge raw corpus in an unsu-
pervised manner.
Let us consider syntactic analysis, into which our
method of case structure analysis is integrated. Re-
cently, many accurate statistical parsers have been
proposed (e.g., (Collins, 1999; Charniak, 2000) for
English, (Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2002) for Japanese). Since they somehow
use lexical information in the tagged corpus, they are
called ?lexicalized parsers?. On the other hand, un-
lexicalized parsers achieved an almost equivalent ac-
curacy to such lexicalized parsers (Klein and Man-
ning, 2003; Kurohashi and Nagao, 1994). Accord-
ingly, we can say that the state-of-the-art lexicalized
parsers are mainly based on unlexical (grammatical)
information due to the sparse data problem. Bikel
also indicated that Collins? parser can use bilexical
dependencies only 1.49% of the time; the rest of
the time, it backs off to condition one word on just
phrasal and part-of-speech categories (Bikel, 2004).
This paper aims at exploiting much more lexical
information, and proposes a fully-lexicalized proba-
bilistic model for Japanese syntactic and case struc-
ture analysis. Lexical information is extracted not
from a small tagged corpus, but from a huge raw cor-
pus as case frames. This model performs case struc-
ture analysis by a generative probabilistic model
based on the case frames, and selects the syntactic
structure that has the highest case structure proba-
bility.
2 Automatically Constructed Case Frames
We employ automatically constructed case frames
(Kawahara and Kurohashi, 2002) for our model of
Table 1: Case frame examples (examples are ex-
pressed only in English for space limitation.).
CS examples
ga <agent>, group, party, ? ? ?
youritsu (1) wo <agent>, candidate, applicant
(support) ni <agent>, district, election, ? ? ?
ga <agent>
youritsu (2) wo <agent>, member, minister, ? ? ?
(support) ni <agent>, candidate, successor
...
...
...
itadaku (1) ga <agent>
(have) wo soup
ga <agent>
itadaku (2) wo advice, instruction, address
(be given) kara <agent>, president, circle, ? ? ?
...
...
...
case structure analysis. This section outlines the
method for constructing the case frames.
A large corpus is automatically parsed, and case
frames are constructed from modifier-head exam-
ples in the resulting parses. The problems of auto-
matic case frame construction are syntactic and se-
mantic ambiguities. That is to say, the parsing re-
sults inevitably contain errors, and verb senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from re-
liable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are dis-
ambiguated by a couple of a verb and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by verbs (e.g., ?tsumu? (load/accumulate)), but by
couples (e.g., ?nimotsu-wo tsumu? (load baggage)
and ?keiken-wo tsumu? (accumulate experience)).
Modifier-head examples are aggregated in this way,
and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?nimotsu-wo tsumu? (load baggage) and ?busshi-wo
tsumu? (load supply) are similar, they are clustered.
The similarity is measured using a thesaurus (Ike-
hara et al, 1997).
Using this gradual procedure, we constructed case
frames from the web corpus (Kawahara and Kuro-
177
hashi, 2006). The case frames were obtained from
approximately 470M sentences extracted from the
web. They consisted of 90,000 verbs, and the aver-
age number of case frames for a verb was 34.3.
In Figure 1, some examples of the resulting case
frames are shown. In this table, ?CS? means a case
slot. <agent> in the table is a generalized example,
which is given to the case slot where half of the ex-
amples belong to <agent> in a thesaurus (Ikehara
et al, 1997). <agent> is also given to ?ga? case
slot that has no examples, because ?ga? case com-
ponents are usually agentive and often omitted.
3 Integrated Probabilistic Model for
Syntactic and Case Structure Analysis
The proposed method gives a probability to each
possible syntactic structure T and case structure L
of the input sentence S, and outputs the syntactic
and case structure that have the highest probability.
That is to say, the system selects the syntactic struc-
ture Tbest and the case structure Lbest that maximize
the probability P (T,L|S):
(Tbest, Lbest) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S)
P (S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant.
3.1 Generative Model for Syntactic and Case
Structure Analysis
We propose a generative probabilistic model based
on the dependency formalism. This model considers
a clause as a unit of generation, and generates the
input sentence from the end of the sentence in turn.
P (T,L, S) is defined as the product of a probability
for generating a clause Ci as follows:
P (T,L, S) =
?
i=1..n
P (Ci|bhi) (2)
where n is the number of clauses in S, and bhi isCi?s
modifying bunsetsu1. The main clause Cn at the end
1In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English,
and ?eojeol? in Korean.
Figure 1: An Example of Probability Calculation.
of a sentence does not have a modifying head, but
we handle it by assuming bhn = EOS (End Of Sen-
tence).
For example, consider the sentence in Figure 1.
There are two possible dependency structures, and
for each structure the product of probabilities indi-
cated below of the tree is calculated. Finally, the
model chooses the highest-probability structure (in
this case the left one).
Ci is decomposed into its predicate type fi (in-
cluding the predicate?s inflection) and the rest case
structure CSi. This means that the predicate in-
cluded in CSi is lemmatized. Bunsetsu bhi is also
decomposed into the content part whi and the type
fhi .
P (Ci|bhi) = P (CSi, fi|whi , fhi)
= P (CSi|fi, whi , fhi)P (fi|whi , fhi)
? P (CSi|fi, whi)P (fi|fhi) (3)
The last equation is derived because the content part
in CSi is independent of the type of its modifying
head (fhi), and in most cases, the type fi is indepen-
dent of the content part of its modifying head (whi).
For example, P (bentou-wa tabete|syuppatsu-shita)
is calculated as follows:
P (CS(bentou-wa taberu)|te, syuppatsu-suru)P (te|ta.)
We call P (CSi|fi, whi) generative model for case
structure and P (fi|fhi) generative model for predi-
cate type. The following two sections describe these
models.
3.2 Generative Model for Case Structure
We propose a generative probabilistic model of case
structure. This model selects a case frame that
178
Figure 2: An example of case assignment CAk.
matches the input case components, and makes cor-
respondences between input case components and
case slots.
A case structure CSi consists of a predicate vi,
a case frame CFl and a case assignment CAk.
Case assignment CAk represents correspondences
between input case components and case slots as
shown in Figure 2. Note that there are various pos-
sibilities of case assignment in addition to that of
Figure 2, such as corresponding ?bentou? (lunch-
box) with ?ga? case. Accordingly, the index k of
CAk ranges up to the number of possible case as-
signments. By splitting CSi into vi, CFl and CAk,
P (CSi|fi, whi) is rewritten as follows:
P (CSi|fi, whi) = P (vi, CFl, CAk|fi, whi)
= P (vi|fi, whi)
? P (CFl|fi, whi , vi)
? P (CAk|fi, whi , vi, CFl)
? P (vi|whi)
? P (CFl|vi)
? P (CAk|CFl, fi) (4)
The above approximation is given because it is
natural to consider that the predicate vi depends on
its modifying headwhi , that the case frameCFl only
depends on the predicate vi, and that the case assign-
ment CAk depends on the case frame CFl and the
predicate type fi.
The probabilities P (vi|whi) and P (CFl|vi) are
estimated from case structure analysis results of a
large raw corpus. The remainder of this section il-
lustrates P (CAk|CFl, fi) in detail.
3.2.1 Generative Probability of Case
Assignment
Let us consider case assignment CAk for each
case slot sj in case frame CFl. P (CAk|CFl, fi)
can be decomposed into the following product de-
pending on whether a case slot sj is filled with an
input case component (content part nj and type fj)
or vacant:
P (CAk|CFl, fi) =
?
sj :A(sj)=1
P (A(sj) = 1, nj , fj |CFl, fi, sj)
?
?
sj :A(sj)=0
P (A(sj) = 0|CFl, fi, sj)
=
?
sj :A(sj)=1
{
P (A(sj) = 1|CFl, fi, sj)
?P (nj , fj |CFl, fi, A(sj) = 1, sj)
}
?
?
sj :A(sj)=0
P (A(sj) = 0|CFl, fi, sj) (5)
where the function A(sj) returns 1 if a case slot sj
is filled with an input case component; otherwise 0.
P (A(sj) = 1|CFl, fi, sj) and P (A(sj) =
0|CFl, fi, sj) in equation (5) can be rewritten as
P (A(sj) = 1|CFl, sj) and P (A(sj) = 0|CFl, sj),
because the evaluation of case slot assignment de-
pends only on the case frame. We call these proba-
bilities generative probability of a case slot, and they
are estimated from case structure analysis results of
a large corpus.
Let us calculate P (CSi|fi, whi) using the ex-
ample in Figure 1. In the sentence, ?wa? is
a topic marking (TM) postposition, and hides
the case marker. The generative probability of
case structure varies depending on the case slot
to which the topic marked phrase is assigned.
For example, when a case frame of ?taberu?
(eat) CFtaberu1 with ?ga? and ?wo? case slots is
used, P (CS(bentou-wa taberu)|te, syuppatsu-suru)
is calculated as follows:
P1(CS(bentou-wa taberu)|te, syuppatsu-suru) =
P (taberu|syuppatsu-suru)
? P (CFtaberu1|taberu)
? P (bentou,wa|CFtaberu1, te, A(wo) = 1,wo)
? P (A(wo) = 1|CFtaberu1,wo)
? P (A(ga) = 0|CFtaberu1, ga) (6)
179
P2(CS(bentou-wa taberu)|te, syupatsu-suru) =
P (taberu|syuppatsu-suru)
? P (CFtaberu1|taberu)
? P (bentou,wa|CFtaberu1, te, A(ga) = 1, ga)
? P (A(ga) = 1|CFtaberu1, ga)
? P (A(wo) = 0|CFtaberu1,wo) (7)
Such probabilities are computed for each case frame
of ?taberu? (eat), and the case frame and its cor-
responding case assignment that have the highest
probability are selected.
We describe the generative probability of a case
component P (nj , fj |CFl, fi, A(sj) = 1, sj) below.
3.2.2 Generative Probability of Case
Component
We approximate the generative probability of a
case component, assuming that:
? a generative probability of content part nj is in-
dependent of that of type fj ,
? and the interpretation of the surface case in-
cluded in fj does not depend on case frames.
Taking into account these assumptions, the genera-
tive probability of a case component is approximated
as follows:
P (nj , fj |CFl, fi, A(sj) = 1, sj) ?
P (nj |CFl, A(sj) = 1, sj) P (fj |sj , fi) (8)
P (nj |CFl, A(sj) = 1, sj) is the probability of
generating a content part nj from a case slot sj in a
case frame CFl. This probability is estimated from
case frames.
Let us consider P (fj |sj , fi) in equation (8). This
is the probability of generating the type fj of a case
component that has a correspondence with the case
slot sj . Since the type fj consists of a surface case
cj2, a punctuation mark (comma) pj and a topic
marker ?wa? tj , P (fj |sj , fi) is rewritten as follows
2A surface case means a postposition sequence at the end of
bunsetsu, such as ?ga?, ?wo?, ?koso? and ?demo?.
(using the chain rule):
P (fj |sj , fi) = P (cj , tj , pj |sj , fi)
= P (cj |sj , fi)
? P (pj |sj , fi, cj)
? P (tj |sj , fi, cj , pj)
? P (cj |sj)
? P (pj |fi)
? P (tj |fi, pj) (9)
This approximation is given by assuming that cj
only depends on sj , pj only depends on fj , and tj
depends on fj and pj . P (cj |sj) is estimated from the
Kyoto Text Corpus (Kawahara et al, 2002), in which
the relationship between a surface case marker and
a case slot is annotated by hand.
In Japanese, a punctuation mark and a topic
marker are likely to be used when their belong-
ing bunsetsu has a long distance dependency. By
considering such tendency, fi can be regarded as
(oi, ui), where oi means whether a dependent bun-
setsu gets over another head candidate before its
modifying head vi, and ui means a clause type of
vi. The value of oi is binary, and ui is one of the
clause types described in (Kawahara and Kurohashi,
1999).
P (pj |fi) = P (pj |oi, ui) (10)
P (tj |fi, pj) = P (tj |oi, ui, pj) (11)
3.3 Generative Model for Predicate Type
Now, consider P (fi|fhi) in the equation (3). This is
the probability of generating the predicate type of a
clause Ci that modifies bhi . This probability varies
depending on the type of bhi .
When bhi is a predicate bunsetsu, Ci is a subor-
dinate clause embedded in the clause of bhi . As for
the types fi and fhi , it is necessary to consider punc-
tuation marks (pi, phi) and clause types (ui, uhi).
To capture a long distance dependency indicated by
punctuation marks, ohi (whether Ci has a possible
head candidate before bhi) is also considered.
PV Bmod(fi|fhi) = PV Bmod(pi, ui|phi , uhi , ohi)
(12)
When bhi is a noun bunsetsu, Ci is an embedded
clause in bhi . In this case, clause types and a punc-
tuation mark of the modifiee do not affect the prob-
ability.
PNBmod(fi|fhi) = PNBmod(pi|ohi) (13)
180
Table 2: Data for parameter estimation.
probability what is generated data
P (pj |oi, uj) punctuation mark Kyoto Text Corpus
P (tj |oi, ui, pj) topic marker Kyoto Text Corpus
P (pi, ui|phi , uhi , ohi) predicate type Kyoto Text Corpus
P (cj |sj) surface case Kyoto Text Corpus
P (vi|whi) predicate parsing results
P (nj |CFl, A(sj) = 1, sj) words case frames
P (CFl|vi) case frame case structure analysis results
P (A(sj) = {0, 1} |CFl, sj) case slot case structure analysis results
Table 3: Experimental results for syntactic analysis.
baseline proposed
all 3,447/3,976 (86.7%) 3,477/3,976 (87.4%)
NB?VB 1,310/1,547 (84.7%) 1,328/1,547 (85.8%)
TM 244/298 (81.9%) 242/298 (81.2%)
others 1,066/1,249 (85.3%) 1,086/1,249 (86.9%)
NB?NB 525/556 (94.4%) 526/556 (94.6%)
VB?VB 593/760 (78.0%) 601/760 (79.1%)
VB?NB 453/497 (91.1%) 457/497 (92.0%)
4 Experiments
We evaluated the syntactic structure and case struc-
ture outputted by our model. Each parameter is es-
timated using maximum likelihood from the data
described in Table 2. All of these data are not
existing or obtainable by a single process, but ac-
quired by applying syntactic analysis, case frame
construction and case structure analysis in turn. The
process of case structure analysis in this table is a
similarity-based method (Kawahara and Kurohashi,
2002). The case frames were automatically con-
structed from the web corpus comprising 470M sen-
tences, and the case structure analysis results were
obtained from 6M sentences in the web corpus.
The rest of this section first describes the exper-
iments for syntactic structure, and then reports the
experiments for case structure.
4.1 Experiments for Syntactic Structure
We evaluated syntactic structures analyzed by the
proposed model. Our experiments were run on
hand-annotated 675 web sentences 3. The web sen-
tences were manually annotated using the same cri-
teria as the Kyoto Text Corpus. The system input
was tagged automatically using the JUMAN mor-
phological analyzer (Kurohashi et al, 1994). The
syntactic structures obtained were evaluated with re-
3The test set is not used for case frame construction and
probability estimation.
gard to dependency accuracy ? the proportion of
correct dependencies out of all dependencies except
for the last dependency in the sentence end 4.
Table 3 shows the dependency accuracy. In
the table, ?baseline? means the rule-based syn-
tactic parser, KNP (Kurohashi and Nagao, 1994),
and ?proposed? represents the proposed method.
The proposed method significantly outperformed the
baseline method (McNemar?s test; p < 0.05). The
dependency accuracies are classified into four types
according to the bunsetsu classes (VB: verb bun-
setsu, NB: noun bunsetsu) of a dependent and its
head. The ?NB?VB? type is further divided into
two types: ?TM? and ?others?. The type that is most
related to case structure is ?others? in ?NB?VB?.
Its accuracy was improved by 1.6%, and the error
rate was reduced by 10.9%. This result indicated
that the proposed method is effective in analyzing
dependencies related to case structure.
Figure 3 shows some analysis results, where the
dotted lines represent the analysis by the baseline
method, and the solid lines represent the analysis by
the proposed method. Sentence (1) and (2) are in-
correctly analyzed by the baseline but correctly ana-
lyzed by the proposed method.
There are two major causes that led to analysis
errors.
Mismatch between analysis results and annota-
tion criteria
In sentence (3) in Figure 3, the baseline
method correctly recognized the head of ?iin-wa?
(commissioner-TM) as ?hirakimasu? (open). How-
ever, the proposed method incorrectly judged it as
?oujite-imasuga? (offer). Both analysis results can
be considered to be correct semantically, but from
4Since Japanese is head-final, the second last bunsetsu un-
ambiguously depends on the last bunsetsu, and the last bunsetsu
has no dependency.
181
? ?
(1) mizu-ga takai tokoro-kara hikui tokoro-he nagareru.
water-nom high ground-abl low ground-all flow
(Water flows from high ground to low ground.)
? ?
(2) ... Kobe shi-ga senmonchishiki-wo motsu volunteer-wo bosyushita ...
Kobe city-nom expert knowledge-acc have volunteer-acc recruited
(Kobe city recruited a volunteer who has expert knowledge, ...)
??
(3) iin-wa, jitaku-de minasan-karano gosoudan-ni oujite-imasuga, ... soudansyo-wo hirakimasu
commissioner-TM at home all of you consultation-dat offer window open
(the commissioner offers consultation to all of you at home, but opens a window ...)
Figure 3: Examples of analysis results.
Table 4: Experimental results for case structure anal-
ysis.
baseline proposed
TM 72/105 (68.6%) 82/105 (78.1%)
clause 107/155 (69.0%) 121/155 (78.1%)
the viewpoint of our annotation criteria, the latter is
not a syntactic relation, but an ellipsis relation. To
address this problem, it is necessary to simultane-
ously evaluate not only syntactic relations but also
indirect relations, such as ellipses and anaphora.
Linear weighting on each probability
We proposed a generative probabilistic model,
and thus cannot optimize the weight of each proba-
bility. Such optimization could be a way to improve
the system performance. In the future, we plan to
employ a machine learning technique for the opti-
mization.
4.2 Experiments for Case Structure
We applied case structure analysis to 215 web sen-
tences which are manually annotated with case
structure, and evaluated case markers of TM phrases
and clausal modifiees by comparing them with the
gold standard in the corpus. The experimental re-
sults are shown in table 4, in which the baseline
refers to a similarity-based method (Kawahara and
Kurohashi, 2002). The experimental results were re-
ally good compared to the baseline. It is difficult to
compare the results with the previous work stated in
the next section, because of different experimental
settings (e.g., our evaluation includes parse errors in
incorrect cases).
5 Related Work
There have been several approaches for syntactic
analysis handling lexical preference on a large scale.
Shirai et al proposed a PGLR-based syntactic
analysis method using large-scale lexical preference
(Shirai et al, 1998). Their system learned lexical
preference from a large newspaper corpus (articles
of five years), such as P (pie|wo, taberu), but did
not deal with verb sense ambiguity. They reported
84.34% accuracy on 500 relatively short sentences
from the Kyoto Text Corpus.
Fujio and Matsumoto presented a syntactic anal-
ysis method based on lexical statistics (Fujio and
Matsumoto, 1998). They made use of a probabilistic
model defined by the product of a probability of hav-
ing a dependency between two cooccurring words
and a distance probability. The model was trained
on the EDR corpus, and performed with 86.89% ac-
curacy on 10,000 sentences from the EDR corpus 5.
On the other hand, there have been a number
of machine learning-based approaches using lexical
preference as their features. Among these, Kudo
and Matsumoto yielded the best performance (Kudo
and Matsumoto, 2002). They proposed a chunking-
based dependency analysis method using Support
Vector Machines. They used two-fold cross valida-
tion on the Kyoto Text Corpus, and achieved 90.46%
5The evaluation includes the last dependencies in the sen-
tence end, which are always correct.
182
accuracy 5. However, it is very hard to learn suffi-
cient lexical preference from several tens of thou-
sands sentences of a hand-tagged corpus.
There has been some related work analyzing
clausal modifiees and TM phrases. For exam-
ple, Torisawa analyzed TM phrases using predicate-
argument cooccurences and word classifications in-
duced by the EM algorithm (Torisawa, 2001). Its
accuracy was approximately 88% for ?wa? and 84%
for ?mo?. It is difficult to compare the accuracy
of their system to ours, because the range of tar-
get expressions is different. Unlike related work,
it is promising to utilize the resultant case frames
for subsequent analyzes such as ellipsis or discourse
analysis.
6 Conclusion
We have described an integrated probabilistic model
for syntactic and case structure analysis. This model
takes advantage of lexical selectional preference of
large-scale case frames, and performs syntactic and
case analysis simultaneously. The experiments indi-
cated the effectiveness of our model. In the future,
by incorporating ellipsis resolution, we will develop
an integrated model of syntactic, case and ellipsis
analysis.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley FrameNet Project. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics and
the 36th Annual Meeting of the Association for Computa-
tional Linguistics, pages 86?90.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Don Blaheta and Eugene Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Meeting of
the North American Chapter of the Association for Compu-
tational Linguistics, pages 234?240.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 88?96.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, Yoshifumi
Oyama, and Yoshihiko Hayashi, editors. 1997. Japanese
Lexicon. Iwanami Publishing.
Daisuke Kawahara and Sadao Kurohashi. 1999. Corpus-based
dependency analysis of Japanese sentences using verb bun-
setsu transitivity. In Proceedings of the 5th Natural Lan-
guage Processing Pacific Rim Symposium, pages 387?391.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of
case frame dictionary for robust Japanese case analysis. In
Proceedings of the 19th International Conference on Com-
putational Linguistics, pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proceedings of the 5th International Conference on
Language Resources and Evaluation.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida. 2002.
Construction of a Japanese relevance-tagged corpus. In Pro-
ceedings of the 3rd International Conference on Language
Resources and Evaluation, pages 2008?2013.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn TreeBank. In Pro-
ceedings of the Human Language Technology Conference.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proceedings of the
Conference on Natural Language Learning, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese morpho-
logical analyzer JUMAN. In Proceedings of the Interna-
tional Workshop on Sharable Natural Language, pages 22?
28.
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi
Tanaka. 1998. An empirical evaluation on statistical parsing
of Japanese sentences using lexical association statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 80?87.
Kentaro Torisawa. 2001. An unsupervised method for canon-
icalization of Japanese postpositions. In Proceedings of the
6th Natural Language Processing Pacific Rim Simposium,
pages 211?218.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency model using posterior
context. In Proceedings of the 6th International Workshop
on Parsing Technology, pages 321?322.
183
 
				Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 425?432
Manchester, August 2008
Coordination Disambiguation without Any Similarities
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
The use of similarities has been one of the
main approaches to resolve the ambigui-
ties of coordinate structures. In this pa-
per, we present an alternative method for
coordination disambiguation, which does
not use similarities. Our hypothesis is
that coordinate structures are supported
by surrounding dependency relations, and
that such dependency relations rather yield
similarity between conjuncts, which hu-
mans feel. Based on this hypothesis, we
built a Japanese fully-lexicalized genera-
tive parser that includes coordination dis-
ambiguation. Experimental results on web
sentences indicated the effectiveness of our
approach, and endorsed our hypothesis.
1 Introduction
The interpretation of coordinate structures directly
affects the meaning of the text. Addressing co-
ordination ambiguities is fundamental to natu-
ral language understanding. Previous studies on
coordination disambiguation suggested that con-
juncts in coordinate structures have syntactic or
semantic similarities, and dealt with coordination
ambiguities using (sub-)string matching, part-of-
speech matching, semantic similarities, and so
forth (Agarwal and Boggess, 1992). Semantic sim-
ilarities are acquired from thesauri (Kurohashi and
Nagao, 1994; Resnik, 1999) or distributional simi-
larity (Chantree et al, 2005).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
For instance, consider the following example:
(1) eat Caesar salad and Italian pasta
The above methods detect the similarity between
salad and pasta using a thesaurus or distributional
similarity, and identify the coordinate structure
that conjoins salad and pasta. They do not use the
information of the word eat.
On the other hand, this coordinate structure can
be analyzed by using selectional preference of eat.
Since eat is likely to have salad and pasta as its ob-
jects, it is plausible that salad and pasta are coor-
dinated. Such selectional preferences are thought
to support the construction of coordinate structures
and to yield similarity between conjuncts on the
contrary.
We present a method of coordination disam-
biguation without using similarities. Coordinate
structures are supported by their surrounding de-
pendency relations that provide selectional prefer-
ences. These relations implicitly work as similari-
ties, and thus it is not necessary to use similarities
explicitly.
In this paper, we focus on Japanese. Coor-
dination disambiguation is integrated in a fully-
lexicalized generative dependency parser (Kawa-
hara and Kurohashi, 2007). For the selectional
preferences, we use lexical knowledge, such as
case frames, which is extracted from a large raw
corpus.
The remainder of this paper is organized as fol-
lows. Section 2 summarizes previous work related
to coordination disambiguation and its integration
into parsing. Section 3 briefly describes the back-
ground of this study. Section 4 overviews our idea,
and section 5 describes our model in detail. Sec-
tion 6 is devoted to our experiments. Finally, sec-
tion 7 gives the conclusions.
425
2 Related Work
Previous work on coordination disambiguation has
focused mainly on finding the scope of coordinate
structures.
There are several methods that use similari-
ties between the heads of conjuncts. Similari-
ties are obtained from manually assigned seman-
tic tags (Agarwal and Boggess, 1992), a the-
saurus (Resnik, 1999) and a distributional the-
saurus (Chantree et al, 2005). Other approaches
used cooccurrence statistics. To determine the at-
tachments of ambiguous coordinate noun phrases,
Goldberg (1999) applied a cooccurrence-based
probabilistic model, and Nakov and Hearst (2005)
used web-based frequencies. The performance of
these methods ranges from 50% to 80%.
Of the above approaches, Resnik (1999) and
Nakov and Hearst (2005) considered the statistics
of noun-noun modification. For example, the co-
ordinate structure ?((mail and securities) fraud)? is
guided by the estimation that mail fraud is a salient
compound nominal phrase. On the other hand, the
coordinate structure ?(corn and (peanut butter))? is
led because corn butter is not a familiar concept.
They did not use the selectional preferences of the
predicates that the conjuncts depend on. There-
fore, this idea is subsumed into ours.
The previously described methods focused on
coordination disambiguation. Some research has
been undertaken that integrated coordination dis-
ambiguation into parsing.
Several techniques have considered the charac-
teristics of coordinate structures in a generative or
reranking parser. Dubey et al (2006) proposed
an unlexicalized PCFG parser that modified PCFG
probabilities to condition the existence of syntactic
parallelism. Hogan (2007) improved a generative
lexicalized parser by considering the symmetry be-
tween words in each conjunct. As for a reranking
parser, Charniak and Johnson (2005) incorporated
some features of syntactic parallelism in coordi-
nate structures into their MaxEnt reranking parser.
Nilsson et al tried to transform the tree rep-
resentation of a treebank into a more suitable
representation for data-driven dependency parsers
(Nilsson et al, 2006; Nilsson et al, 2007). One
of their targets is the representation of coordinate
structures. They succeeded in improving a deter-
ministic parser, but failed for a globally optimized
discriminative parser.
Kurohashi and Nagao proposed a Japanese pars-
ing method that included coordinate structure de-
tection (Kurohashi and Nagao, 1994). Their
method first detects coordinate structures in a sen-
tence, and then determines the dependency struc-
ture of the sentence under the constraints of the
detected coordinate structures. Their method cor-
rectly analyzed 97 out of 150 Japanese sentences.
Kawahara and Kurohashi (2007) integrated this
method into a generative parsing model. Shimbo
and Hara (2007) considered many features for co-
ordination disambiguation and automatically opti-
mized their weights, which were heuristically de-
termined in Kurohashi and Nagao (1994), using a
discriminative learning model.
A number of machine learning-based ap-
proaches to Japanese parsing have been developed.
Among them, the best parsers are the SVM-based
dependency analyzers (Kudo and Matsumoto,
2002; Sassano, 2004). In particular, Sassano added
some features to improve his parser by enabling
it to detect coordinate structures (Sassano, 2004).
However, the added features did not contribute to
improving the parsing accuracy. Tamura et al
(2007) learned not only standard modifier-head
relations but also ancestor-descendant relations.
With this treatment, their method can indirectly
improve the handling of coordinate structures in
limited cases.
3 Background
3.1 Japanese Grammar
Let us first briefly introduce Japanese grammar.
The structure of a Japanese sentence can be de-
scribed well by the dependency relation between
bunsetsus. A bunsetsu is a basic unit of depen-
dency, consisting of one or more content words and
the following zero or more function words. A bun-
setsu corresponds to a base phrase in English and
eojeol in Korean. The Japanese language is head-
final, that is, a bunsetsu depends on another bun-
setsu to its right (but not necessarily the adjacent
bunsetsu).
For example, consider the following sentence
1
:
(2) ane-to
sister-CMI
gakkou-ni
school-ALL
itta
went
(went to school with (my) sister)
1
In this paper, we use the following abbreviations:
NOM (nominative), ACC (accusative), ABL (ablative),
ALL (allative), CMI (comitative), CNJ (conjunction) and
TM (topic marker).
426
This sentence consists of three bunsetsus. The fi-
nal bunsetsu, itta, is a predicate, and the other bun-
setsus, ane-to and gakkou-ni, are its arguments.
Their endings, to and ni, are postpositions that
function as case markers.
3.2 Treebank
To evaluate our method, we use a web corpus that
is manually annotated using the criteria of the Ky-
oto Text Corpus (Kurohashi and Nagao, 1998).
The Kyoto Text Corpus is syntactically annotated
in dependency formalism, and consists of 40K
Japanese newspaper sentences. The web corpus,
which is used in our evaluation, consists of 759
sentences extracted from the web.
Under the annotation criteria of the Kyoto Text
Corpus, the last bunsetsu in a pre-conjunct depends
on the last bunsetsu in a post-conjunct, as shown in
the dependency trees of Figure 1.
4 Our Idea of Addressing Coordination
Ambiguities
The target of our approach is nominal coordinate
structures. Consider, for example, the follow-
ing sentence, which contains a nominal coordinate
structure.
(3) jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
(increase of population and pollution of air
were stimulated)
In this sentence, the postposition to is a coordinate
conjunction
2
. In Japanese, a coordinate conjunc-
tion is attached to a verb or noun, forming a bun-
setsu, like case-marking postpositions. We call a
bunsetsu that contains a coordinate conjunction co-
ordination key bunsetsu.
The coordinate structure in example (3) has four
possible scopes as depicted in Figure 1. In this
figure, our parser generates the constituent words
according to the arrows in the reverse direction.
Note that the words that have ?1/2? marks are gen-
erated from multiple words, because they depend
2
Note that the postposition to can be used as a coordinate
conjunction and also a comitative case marker as in exam-
ple (2). The detection of coordinate conjunctions is a task of
coordination disambiguation as well as the identification of
coordination scopes. Both of these tasks are simultaneously
handled in our method.
on a coordinate structure. In this case, their gen-
erative probabilities, which are described later, are
averaged.
The scope patterns in Figure 1 can be written in
English as follows:
a. (population increase) and (air pollution)
b. population (increase and (air pollution))
c. ((population increase) and air) pollution
d. population (increase and air) pollution
In (a) and (b), two arguments, zouka (increase)
and osen (pollution), are generated from the verb
sokushin-sareta (stimulated), and are eligible for
the ga (NOM) words of the verb sokushin-sareta
(stimulated). However, (b) is not appropriate,
because we cannot say the nominal compound
?jinkou-no osen? (pollution of population). In (c)
and (d), the heads of conjuncts, zouka (increase)
and taiki (air), are generated from osen (pollu-
tion). These cases are also inappropriate, because
we cannot say the nominal compound ?zouka-no
osen? (pollution of increase). Accordingly, in this
case, the correct scope, (a), is derived based on the
selectional preferences of predicates and nouns.
In this framework, we require selectional prefer-
ences. We use case frames for predicates (Kawa-
hara and Kurohashi, 2006) and occurrences of
noun-noun modifications for nouns. Both of them
are extracted from a large amount of raw text.
5 Our Model of Coordination
Disambiguation
This section describes an integrated model of co-
ordination disambiguation in a generative parsing
framework. First, we describe resources for selec-
tional preferences, and then illustrate our model of
coordination disambiguation.
5.1 Resources for Selectional Preferences
As the resources of selectional preferences to
support coordinate structures, we use automati-
cally constructed case frames and cooccurrences
of noun-noun modifications.
5.1.1 Automatically Constructed Case
Frames
We employ automatically constructed case
frames (Kawahara and Kurohashi, 2006). This
section outlines the method for constructing the
case frames.
427
zouka-to
increase-CNJ
jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(a) jinkou-no
populuation-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
(b)
jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(c) jinkou-no
population-GEN
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(d)
C
1/2
1/2
Figure 1: Four possible coordination scopes for example (3). Rounded rectangles represent conjuncts.
The solid arrows represent dependency trees. The dotted arrows represent the additional processes of
generation for coordinate structures. Note that the arrows with coordinate relation (?C? mark) do not
participate in generation instead.
Table 1: Acquired case frames of yaku. Example
words are expressed only in English due to space
limitation. The number following each word de-
notes its frequency.
CS examples
ga I:18, person:15, craftsman:10, ? ? ?
yaku (1)
wo bread:2484, meat:1521, cake:1283, ? ? ?
(bake)
de oven:1630, frying pan:1311, ? ? ?
yaku (2) ga teacher:3, government:3, person:3, ? ? ?
(have wo fingers:2950
difficulty) ni attack:18, action:15, son:15, ? ? ?
ga maker:1, distributor:1
yaku (3)
wo data:178, file:107, copy:9, ? ? ?
(burn)
ni R:1583, CD:664, CDR:3, ? ? ?
.
.
.
.
.
.
.
.
.
A large corpus is automatically parsed, and case
frames are constructed from modifier-head exam-
ples in the resulting parses. The problems of auto-
matic case frame construction are syntactic and se-
mantic ambiguities. That is to say, the parsing re-
sults inevitably contain errors, and verb senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from
reliable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are disam-
biguated by a pair consisting of a verb and its clos-
est case component. Such pairs are explicitly ex-
pressed on the surface of text, and are thought to
play an important role in sentence meanings. For
instance, examples are distinguished not by verbs
(e.g., ?yaku? (bake/broil/have difficulty)), but by
pairs (e.g., ?pan-wo yaku? (bake bread), ?niku-wo
yaku? (broil meat), and ?te-wo yaku? (have diffi-
culty)). Modifier-head examples are aggregated in
this way, and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?pan-wo yaku? (bake bread) and ?niku-wo yaku?
(broil meat) are similar, they are clustered. The
similarity is measured using a thesaurus (The Na-
tional Institute for Japanese Language, 2004).
Using this gradual procedure, we constructed
case frames from a web corpus (Kawahara and
Kurohashi, 2006). The case frames were ob-
tained from approximately 500M sentences ex-
tracted from the web corpus. They consisted of
90,000 verbs, and the average number of case
frames for a verb was 34.3.
In Table 1, some examples of the resulting case
frames of the verb yaku are listed. In this table,
?CS? indicates a case slot.
428
ane-to
sister-CNJ
otouto-wo
brother-ACC
yondainvited
C(b)ane-tosister-CMI
otouto-wo
brother-ACC
yondainvited
(a) to
wo wo
wo
Figure 2: Dependency trees and generation pro-
cesses for example (4). This example sentence has
two possible dependency structures according to
the interpretation of to: comitative in (a) and co-
ordinate conjunction in (b).
5.1.2 Cooccurrences of Noun-noun
Modifications
Adnominal nouns have selectional preferences
to nouns, and thus this characteristic is useful for
coordination disambiguation (Resnik, 1999). We
collect dependency relations between nouns from
automatic parses of the web corpus. As a re-
sult, 10.7M unique dependency relations were ob-
tained.
5.2 Our Model
We employ a probabilistic generative dependency
parser (Kawahara and Kurohashi, 2007) as a base
model. This base model measures similarities
between conjuncts in the same way as (Kuro-
hashi and Nagao, 1994), and calculates probabil-
ities of generating these similarities. Our proposed
model, however, does not do both of them. Our
model purely depends on selectional preferences
provided by automatically acquired lexical knowl-
edge.
Our model gives probabilities to all the possible
dependency structures for an input sentence, and
selects the structure that has the highest probabil-
ity. For example, consider the following sentence:
(4) ane-to
sister-CNJ
otouto-wo
brother-ACC
yonda
invited
(invited (my) sister and brother)
For this sentence, our model assesses the two de-
pendency structures (a) and (b) in Figure 2. In our
model, both of the pre-conjunct and post-conjunct
are generated from the predicate. That is, in (b),
both ane (sister) and otouto (brother) with wo
(ACC) are generated from yonda (invited). To
identify the correct structure, (b), it is essential
that both ane (sister) and otouto (brother) are el-
igible for the accusative words of yonda (invited).
Therefore, selectional preferences play an impor-
tant role in coordination disambiguation. On the
other hand, in (a), ane (sister) with to (CMI) is
generated from yonda (invited), and also otouto
(brother) with wo (ACC) is generated from yonda.
However, yonda is not likely to have the to case
slot, so the probability of (a) is lower than that of
(b). Our model can finally select the correct struc-
ture, (b), which has the highest probability. This
kind of assessment is also performed to resolve
the scope ambiguities of coordinate structures as
shown in Figure 1.
This model gives a probability to each possible
dependency structure, T , and case structure, L, of
the input sentence, S, and outputs the dependency
and case structure that have the highest probability.
That is to say, the model selects the dependency
structure, T
best
, and the case structure, L
best
, that
maximize the probability, P (T,L|S):
(T
best
, L
best
) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S)
P (S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant.
The model considers a clause as a generation
unit and generates the input sentence from the end
of the sentence in turn. The probability P (T,L, S)
is defined as the product of probabilities for gener-
ating clause C
i
as follows:
P (T,L, S) =
?
C
i
?S
P (C
i
, rel
ih
i
|C
h
i
) (2)
C
h
i
is C
i
?s modifying clause, and rel
ih
i
is the de-
pendency relation between C
i
and C
h
i
. The main
clause, C
n
, at the end of a sentence does not have
a modifying head, but a virtual clause C
h
n
= EOS
(End Of Sentence) is added. Dependency relation
rel
ih
i
is classified into two types, C (coordinate)
and D (normal dependency).
Clause C
i
is decomposed into its clause type,
f
i
, (including the predicate?s inflection and func-
tion words) and its remaining content part C
i
?
.
Clause C
h
i
is also decomposed into its content
part, C
h
i
?
, and its clause type, f
h
i
.
P (C
i
, rel
ih
i
|C
h
i
) = P (C
i
?
, f
i
, rel
ih
i
|C
h
i
?
, f
h
i
)
? P (C
i
?
, rel
ih
i
|f
i
, C
h
i
?
)? P (f
i
|f
h
i
)
? P (C
i
?
|rel
ih
i
, f
i
, C
h
i
?
)? P (rel
ih
i
|f
i
)
? P (f
i
|f
h
i
) (3)
429
Equation (3) is derived using appropriate approx-
imations described in Kawahara and Kurohashi
(2007).
We call P (C
i
?
|rel
ih
i
, f
i
, C
h
i
?
) generative prob-
ability of a content part, and P (rel
ih
i
|f
i
) gener-
ative probability of a dependency relation. The
following two subsections describe these probabil-
ities.
5.2.1 Generative Probability of Dependency
Relation
The most important feature to determine
whether two clauses are coordinate is a coordina-
tion key. Therefore, we consider a coordination
key, k
i
, as clause type f
i
. The generative prob-
ability of a dependency relation, P (rel
ih
i
|f
i
), is
defined as follows:
P (rel
ih
i
|f
i
) = P (rel
ih
i
|k
i
) (4)
We classified coordination keys into 52 classes ac-
cording to the classification described in (Kuro-
hashi and Nagao, 1994). If type f
i
does not
contain a coordination key, the relation is always
D (normal dependency), that is, P (rel
ih
i
|f
i
) =
P (D|?) = 1.
The generative probability of a dependency re-
lation was estimated from the Kyoto Text Corpus
using maximum likelihood.
5.2.2 Generative Probability of Content Part
The generative probability of a content part
changes according to the class of a content part,
C
i
?
. We classify C
i
?
into two classes: predicate
clause and nominal phrase.
If C
i
?
is a predicate clause, C
i
?
represents a case
structure. We consider that a case structure con-
sists of a predicate, v
i
, a case frame, CF
l
, and
a case assignment, CA
k
. Case assignment CA
k
represents correspondences between the input case
components and the case slots shown in Figure 3.
Thus, the generative probability of a content part
is decomposed as follows:
P
v
(C
i
?
|rel
ih
i
, f
i
, C
h
i
?
)
= P (v
i
, CF
l
, CA
k
|rel
ih
i
, f
i
, C
h
i
?
)
? P (v
i
|rel
ih
i
, f
i
, w
h
i
)
? P (CF
l
|v
i
)
? P (CA
k
|CF
l
, f
i
) (5)
These generative probabilities are estimated from
case frames themselves and parsing results of a
large web corpus.
bentou-wa
tabete
(lunchbox)
(eat)
?
lunchbox, bread, ?wo
man, student, ?ga
taberu1 (eat)
Case Frame CF
l
Case 
Assignment
CA
k
(no correspondence)
Dependency Structure of S
Figure 3: Example of case assignment.
If C
i
?
is a nominal phrase and consists of a noun
n
i
, we consider the following probability instead
of equation (5):
P
n
(C
i
?
|rel
ih
i
, f
i
, C
h
i
?
) ? P (n
i
|rel
ih
i
, f
i
, w
h
i
)
This is because a noun does not have a case frame
or any case components in the current framework.
Since we do not use cooccurrences of coordinate
phrases as used in the base model, rel
ih
i
is always
D (normal dependency). This probability is esti-
mated from the cooccurrences of noun-noun mod-
ifications using maximum likelihood.
6 Experiments
We evaluated the dependency structures that were
output by our model. The case frames used in this
paper were automatically constructed from 500M
Japanese sentences obtained from the web.
In this work, the parameters related to unlexical
types were calculated from the Kyoto Text Corpus,
which is a small tagged corpus of newspaper ar-
ticles, and lexical parameters were obtained from
a huge web corpus. To evaluate the effectiveness
of our model, our experiments were conducted us-
ing web sentences. As the test corpus, we used
759 web sentences
3
, which are described in sec-
tion 3.2. We also used the Kyoto Text Corpus as
a development corpus to optimize the smoothing
parameters. The system input was automatically
tagged using the JUMAN morphological analyzer
4
.
We used two baseline systems for compara-
tive purposes: a rule-based dependency parser
(Kurohashi and Nagao, 1994) and the probabilistic
generative model of dependency, coordinate and
case structure analysis (Kawahara and Kurohashi,
2007)
5
.
6.1 Evaluation of Dependency Structures
We evaluated the dependency structures that were
analyzed by the proposed model. Evaluating the
3
The test set was not used to construct case frames or es-
timate probabilities.
4
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
5
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
430
Table 2: Experimental results of dependency structures. ?all? represents the accuracy of all the depen-
dencies, and ?coordination key? represents the accuracy of only the coordination key bunsetsus.
rule-coord-w/sim prob-coord-w/sim prob-coord-wo/sim
all 3,821/4,389 (87.1%) 3,852/4,389 (87.8%) 3,877/4,389 (88.3%)
coordination key 878/1,106 (79.4%) 881/1,106 (79.7%) 897/1,106 (81.1%)
scope ambiguity of coordinate structures is sub-
sumed within this dependency evaluation. The de-
pendency structures obtained were evaluated with
regard to dependency accuracy ? the proportion
of correct dependencies out of all dependencies
except for the last one in the sentence end
6
. Ta-
ble 2 lists the dependency accuracy. In this table,
?rule-coord-w/sim? represents a rule-based depen-
dency parser; ?prob-coord-w/sim? represents the
probabilistic parser of dependency, coordinate and
case structure (Kawahara and Kurohashi, 2007);
and ?prob-coord-wo/sim? represents our proposed
model. ?all? represents the overall accuracy, and
?coordination key? represents the accuracy of only
the coordination key bunsetsus. The proposed
model, ?prob-coord-wo/sim?, significantly outper-
formed both ?rule-coord-w/sim? and ?prob-coord-
w/sim? (McNemar?s test; p < 0.05) for ?all?.
Figure 4 shows some analyses that are cor-
rectly analyzed by the proposed method. For
example, in sentence (1), our model can rec-
ognize the correct coordinate structure that con-
joins ?densya-no hassyaaizu? (departure signals
of trains) and ?keitaidenwa-no tyakushinon? (ring
tones of cell phones). This is because the case
frame of ?ongaku-ni naru? (become music) is
likely to generate ?hassyaaizu? (departure signal)
and ?tyakushinon? (ring tone).
To compare our results with a state-of-the-art
discriminative dependency parser, we input the
same test corpus into an SVM-based Japanese
dependency parser, CaboCha
7
(Kudo and Mat-
sumoto, 2002). Its dependency accuracy was
86.7% (3,807/4,389), which is close to that of
?rule-coord-w/sim?. This low accuracy is at-
tributed to the lack of the consideration of coor-
dinate structures. Though dependency structures
are closely related to coordinate structures, the
CaboCha parser failed to incorporate coordination
features. Another cause of the low accuracy is
the out-of-domain training corpus. That is, the
parser is trained on a newspaper corpus, whereas
6
Since Japanese is head-final, the second to last bunsetsu
unambiguously depends on the last bunsetsu, and the last bun-
setsu has no dependency.
7
http://chasen.org/?taku/software/cabocha/
the test corpus is obtained from the web, because
of the non-availability of a tagged web corpus that
is large enough to train a supervised parser.
6.2 Discussion
We presented a method for coordination dis-
ambiguation without using similarities, and this
method achieved better performance than the
conventional approaches based on similarities.
Though we do not use similarities, we implicitly
consider similarities between conjuncts. This is
because the heads of pre- and post-conjuncts share
a case marker and a predicate, and thus they are es-
sentially similar. Our idea is related to the notion
of distributional similarity. Chantree et al (2005)
applied the distributional similarity proposed by
Lin (1998) to coordination disambiguation. Lin
extracted from a corpus dependency triples of two
words and the grammatical relationship between
them, and considered that similar words are likely
to have similar dependency relations. The differ-
ence between Chantree et al (2005) and ours is
that their method does not use the information of
verbs in the sentence under consideration, but use
only the cooccurrence information extracted from
a corpus.
On the other hand, the disadvantage of our
model is that it cannot consider the parallelism of
conjuncts, which still seems to exist in especially
strong coordinate structures. Handling of such par-
allelism is an open question of our model.
The generation process adopted in this work
is similar to the design of dependency structure
described in Hudson (1990), which lets the con-
juncts have a dependency relation to the predi-
cate. Nilsson et al (2006) mentioned this notion,
but did not consider this idea in their experiments
of tree transformations for data-driven dependency
parsers. In addition, it is not necessary for our
method to transform dependency trees in pre- and
post-processes, because we just changed the pro-
cess of generation in the generative parser.
7 Conclusion
In this paper, we first came up with a hypoth-
esis that coordinate structures are supported by
431
? ?
(1) densya-no hassyaaizu-ya, keitaidenwa-no tyakushinon-madega ongaku-ni naru-hodoni, ...
train-GEN departure signal cell phone-GEN ring tone-also music-ACC become
(departure signals of trains and ring tones of cell phones become music, ...)
? ?
(2) nabe-ni dashijiru 3 kappu-to, nokori-no syouyu, mirin, sake-wo irete, ...
pot-DAT stock three cups-and remainder-GEN soy mirin sake-ACC pour
(pour three cups of stock and remaining soy, mirin and sake to the pot, ...)
Figure 4: Examples of correct analyses. The dotted lines represent the analysis by the baseline, ?prob-
coord-w/sim?, and the solid lines represent the analysis by the proposed method, ?prob-coord-wo/sim?.
surrounding dependency relations. Based on this
hypothesis, we built an integrated probabilistic
model for coordination disambiguation and depen-
dency/case structure analysis. This model does
not make use of similarities to analyze coordinate
structures, but takes advantage of selectional pref-
erences from a huge raw corpus and large-scale
case frames. The experimental results indicate
the effectiveness of our model, and thus support
our hypothesis. Our future work involves incorpo-
rating ellipsis resolution to develop an integrated
model for syntactic, case, and ellipsis analysis.
References
Agarwal, Rajeev and Lois Boggess. 1992. A simple but use-
ful approach to conjunct identification. In Proceedings of
ACL1992, pages 15?21.
Chantree, Francis, Adam Kilgarriff, Anne de Roeck, and Al-
istair Wills. 2005. Disambiguating coordinations us-
ing word distribution information. In Proceedings of
RANLP2005.
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of ACL2005, pages 173?180.
Dubey, Amit, Frank Keller, and Patrick Sturt. 2006. Inte-
grating syntactic priming into an incremental probabilistic
parser, with an application to psycholinguistic modeling.
In Proceedings of COLING-ACL2006, pages 417?424.
Goldberg, Miriam. 1999. An unsupervised model for statis-
tically determining coordinate phrase attachment. In Pro-
ceedings of ACL1999, pages 610?614.
Hogan, Deirdre. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proceedings of
ACL2007, pages 680?687.
Hudson, Richard. 1990. English Word Grammar. Blackwell.
Kawahara, Daisuke and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance com-
puting. In Proceedings of LREC2006.
Kawahara, Daisuke and Sadao Kurohashi. 2007. Proba-
bilistic coordination disambiguation in a fully-lexicalized
Japanese parser. In Proceedings of EMNLP-CoNLL2007,
pages 306?314.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceedings
of CoNLL2002, pages 29?35.
Kurohashi, Sadao and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on the
detection of conjunctive structures. Computational Lin-
guistics, 20(4):507?534.
Kurohashi, Sadao and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing sys-
tem. In Proceedings of LREC1998, pages 719?724.
Lin, Dekang. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL98, pages
768?774.
Nakov, Preslav and Marti Hearst. 2005. Using the web as
an implicit training set: Application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP2005, pages
835?842.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2006. Graph
transformations in data-driven dependency parsing. In
Proceedings of COLING-ACL2006, pages 257?264.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007. General-
izing tree transformations for inductive dependency pars-
ing. In Proceedings of ACL2007, pages 968?975.
Resnik, Philip. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to problems
of ambiguity in natural language. Journal of Artificial In-
telligence Research, 11:95?130.
Sassano, Manabu. 2004. Linear-time dependency analysis
for Japanese. In Proceedings of COLING2004, pages 8?
14.
Shimbo, Masashi and Kazuo Hara. 2007. A discriminative
learning model for coordinate conjunctions. In Proceed-
ings of EMNLP-CoNLL2007, pages 610?619.
Tamura, Akihiro, Hiroya Takamura, and Manabu Oku-
mura. 2007. Japanese dependency analysis using the
ancestor-descendant relation. In Proceedings of EMNLP-
CoNLL2007, pages 600?609.
The National Institute for Japanese Language. 2004. Bun-
ruigoihyo. Dainippon Tosho, (In Japanese).
432
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 769?776
Manchester, August 2008
A Fully-Lexicalized Probabilistic Model
for Japanese Zero Anaphora Resolution
Ryohei Sasano
?
Graduate School of Information Science
and Technology, University of Tokyo
ryohei@nlp.kuee.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communication Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Infomatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents a probabilistic model
for Japanese zero anaphora resolution.
First, this model recognizes discourse en-
tities and links all mentions to them. Zero
pronouns are then detected by case struc-
ture analysis based on automatically con-
structed case frames. Their appropriate
antecedents are selected from the entities
with high salience scores, based on the
case frames and several preferences on
the relation between a zero pronoun and
an antecedent. Case structure and zero
anaphora relation are simultaneously de-
termined based on probabilistic evaluation
metrics.
1 Introduction
Anaphora resolution is one of the most important
techniques in discourse analysis. In English, def-
inite noun phrases such as the company and overt
pronouns such as he are anaphors that refer to pre-
ceding entities (antecedents). On the other hand,
in Japanese, anaphors are often omitted and these
omissions are called zero pronouns. We focus
on zero anaphora resolution of Japanese web cor-
pus, in which anaphors are often omitted and zero
anaphora resolution plays an important role in dis-
course analysis.
Zero anaphora resolution can be divided into
two phases. The first phase is zero pronoun detec-
tion and the second phase is zero pronoun resolu-
tion. Zero pronoun resolution is similar to coref-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
* Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
erence resolution and pronoun resolution, which
have been studied for many years (e.g. Soon et
al. (2001); Mitkov (2002); Ng (2005)). Isozaki and
Hirao (2003) and Iida et al (2006) focused on zero
pronoun resolution assuming perfect pre-detection
of zero pronouns. However, we consider that zero
pronoun detection and resolution have a tight rela-
tion and should not be handled independently. Our
proposed model aims not only to resolve zero pro-
nouns but to detect zero pronouns.
Zero pronouns are not expressed in a text and
have to be detected prior to identifying their an-
tecedents. Seki et al (2002) proposed a proba-
bilistic model for zero pronoun detection and res-
olution that uses hand-crafted case frames. In
order to alleviate the sparseness of hand-crafted
case frames, Kawahara and Kurohashi (2004) in-
troduced wide-coverage case frames to zero pro-
noun detection that are automatically constructed
from a large corpus. They use the case frames as
selectional restriction for zero pronoun resolution,
but do not utilize the frequency of each example of
case slots. However, since the frequency is shown
to be a good clue for syntactic and case structure
analysis (Kawahara and Kurohashi, 2006), we con-
sider the frequency also can benefit zero pronoun
detection. Therefore we propose a probabilistic
model for zero anaphora resolution that fully uti-
lizes case frames. This model directly consid-
ers the frequency and estimates case assignments
for overt case components and antecedents of zero
pronoun simultaneously.
In addition, our model directly links each zero
pronoun to an entity, while most existing mod-
els link it to a certain mention of an entity. In
our model, mentions and zero pronouns are treated
similarly and all of them are linked to correspond-
ing entities. In this point, our model is similar to
769
Table 1: Examples of Constructed Case Frames.
case slot examples generalized examples with rate
ga (subjective) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?
tsumu (1)
wo (objective) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?
(load)
ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2)
ga (subjective) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate)
wo (objective) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
.
.
.
.
.
.
.
.
.
ga (subjective) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (objective) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
.
.
.
.
.
.
.
.
.
the coreference model proposed by Luo (2007) and
that proposed by Yang et al (2008). Due to this
characteristic, our model can utilize information
beyond a mention and easily consider salience (the
importance of an entity).
2 Construction of Case Frames
Case frames describe what kinds of cases each
predicate has and what kinds of nouns can fill
these case slots. We construct case frames from
a large raw corpus by using the method proposed
by Kawahara and Kurohashi (2002), and use them
for case structure analysis and zero anaphora res-
olution. This section shows how to construct the
case frames.
2.1 Basic Method
After a large corpus is parsed by a Japanese parser,
case frames are constructed from modifier-head
examples in the resulting parses. The problems of
case frame construction are syntactic and seman-
tic ambiguities. That is to say, the parsing results
inevitably contain errors and predicate senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from
reliable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are disam-
biguated by coupling a predicate and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by predicates (e.g., ?tsumu (load/accumulate))?,
but by couples (e.g., ?nimotsu-wo tsumu (load bag-
gage)? and ?keiken-wo tsumu (accumulate experi-
ence))?. Modifier-head examples are aggregated in
this way, and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?nimotsu-wo tsumu (load baggage)? and ?busshi-
wo tsumu (load supplies)? are similar, they are
clustered. The similarity is measured using a
thesaurus (The National Language Institute for
Japanese Language, 2004). Using this gradual pro-
cedure, we constructed case frames from approx-
imately 1.6 billion sentences extracted from the
web. In Table 1, some examples of the resulting
case frames are shown.
2.2 Generalization of Examples
By using case frames that are automatically con-
structed from a large corpus, sparseness problem
is alleviated to some extent, but still remains. For
instance, there are thousands of named entities
(NEs), which cannot be covered intrinsically. To
deal with this sparseness problem, we general-
ize the examples of case slots. Kawahara and
Kurohashi also give generalized examples such
as ?agent? but only a few types. We generalize
case slot examples based on categories of common
nouns and NE classes.
First, we use the categories that Japanese mor-
phological analyzer JUMAN
1
adds to common
nouns. In JUMAN, about twenty categories are de-
fined and tagged to common nouns. For example,
?ringo (apple),? ?inu (dog)? and ?byoin (hospi-
tal)? are tagged as ?FOOD,? ?ANIMAL? and ?FA-
CILITY,? respectively. For each category, we cal-
culate the rate of categorized example among all
case slot examples, and add it to the case slot as
?[CT:FOOD]:0.07.?
We also generalize NEs. We use a common
standard NE definition for Japanese provided by
IREX workshop (1999). IREX defined eight NE
classes as shown in Table 2. We first recognize
NEs in the source corpus by using an NE recog-
nizer (Sasano and Kurohashi, 2008), and then con-
struct case frames from the NE-recognized corpus.
1
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
770
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
As well as categories, for each NE class, we calcu-
late the NE rate among all case slot examples, and
add it to the case slot as ?[NE:PERSON]:0.12.?
The generalized examples are also included in
Table 1. This information is utilized to estimate the
case assignment probability, which will be men-
tioned in Section 3.2.3.
3 Zero Anaphora Resolution Model
In this section, we propose a probabilistic model
for Japanese zero anaphora resolution.
3.1 Overview
The outline of our model is as follows:
1. Parse an input text using the Japanese parser
KNP
2
and recognize NEs.
2. Conduct coreference resolution and link each
mention to an entity or create new entity.
3. For each sentence, from the end of the sen-
tence, analyze each predicate by the follow-
ing steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondence
between each input case component and
an case slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondence
between each zero pronoun candidate
and an existing entity.
(e) For each possible case frame, estimate
each correspondence probabilistically,
and select the most likely case frame and
correspondence.
In this paper, we concentrate on three case slots
for zero anaphora resolution: ?ga (subjective),?
?wo (objective)? and ?ni (dative),? which cover
about 90% of zero anaphora.
2
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
Morphological analysis, NE recognition, syn-
tactic analysis and coreference resolution are con-
ducted as pre-processes for zero anaphora resolu-
tion. Therefore, the model has already recognized
existing entities before zero anaphora resolution.
For example, let us consider the following text:
(i) Toyota-wa 1997-nen hybrid car Prius-wo
hatsubai(launch). 2000-nen-karaha kaigai
(overseas)-demo hanbai(sell)-shiteiru.
(Toyota launched the hybrid car Prius in 1997. ?
1
started selling ?
2
overseas in 2000.)
Figure 1 shows the analysis process for this text.
There are three mentions
3
in the first sentence, and
the two mentions, hybrid car and Prius, appear in
apposition. Thus, after the pre-processes, two enti-
ties, {Toyota} and {hybrid-car, Prius}, are created.
Then, case structure analysis for the predicate
hatsubai (launch) is conducted. First, one of the
case frames of hatsubai (launch) is temporarily se-
lected and each input case component is assigned
to an appropriate case slot. For instance, case com-
ponent Toyota is assigned to ga case slot and Prius
is assigned to wo case slot
4
. In this case, though
there is a mention hybrid-car that is not a case
component of hatsubai (launch) by itself, it refers
to the same entity as Prius refers. Thus, there is no
entity that is not linked to hatsubai (launch), and
no further analysis is conducted.
Now, let us consider the second sentence. A
mention kaigai (overseas) appears and a new entity
{kaigai} is created. Then, case structure analysis
for the predicate hanbai (sell) is conducted. There
is only one overt case component kaigai (over-
seas), and it is assigned to a case slot of the se-
lected case frame of hanbai (sell). For instance,
the case frame hanbai(1) in Table 1 is selected and
kaigai (overseas) is assigned to de (locative) case
slot. In this case, the remaining case slots ga, wo
and ni are considered as zero pronouns, and all
possible correspondences between zero pronouns
and remaining entities are considered. As a result
of probabilistic estimation, the entity {Toyota} is
assigned to ga case, the entity {hybrid-car, Prius}
is assigned to wo case and no entity is assigned to
ni case.
Now, we show how to estimate the correspon-
dence probabilistically in the next subsection.
3
In this paper, we do not consider time expressions, such
as 1997, as mentions.
4
Note that since there are some non case-making postposi-
tions in Japanese, such as ?wa? and ?mo,? several correspon-
dences can be considered.
771
Toyota-wa
Prius-wo
hybrid car
hatsubai.
kaigai-demo
hanbai-shiteiru.
1997-nen
2000-nen-karawa
{Toyota, ?
?
}
{hybrid car, 
Prius, ?2 }
{kaigai}
Entities
(overseas)
(launch)
(sell)
hatsubai (launch)
ga
subjective
company, SONY, firm, ? 
[NE:ORGANIZATION] 0.15, ?
wo
objective
product, CD, model, car,  ?
[CT:ARTIFACT] 0.40, ?
de      
locative
area, shop, world, Japan, ?
[CT:FACILITY] 0.13, ?
hanbai (sell)
ga
subjective
company, Microsoft, ? 
[NE:ORGANIZATION] 0.16, ?
wo     
objective
goods, product, ticket, ? 
[CT:ARTIFACT] 0.40, ?
ni
dative
customer, company, user, ? 
[CT:PERSON] 0.28, ?
de      
locative
shop, bookstore, site, ? 
[CT:FACILITY] 0.40, ?
:direct case assignment
:indirect case assignment (zero anaphora)
Case framesInput sentences
Toyota launched the hybrid car Prius in 1997. ?
?
started selling ?2 overseas in 2000.
Figure 1: An Example of Case Assignment CA
k
.
3.2 Probabilistic Model
The proposed model gives a probability to each
possible case frame CF and case assignment CA
when target predicate v, input case components
ICC and existing entities ENT are given. It also
outputs the case frame and case assignment that
have the highest probability. That is to say, our
model selects the case frame CF
best
and the case
assignment CA
best
that maximize the probability
P (CF,CA|v, ICC,ENT ):
(CF
best
, CA
best
)
= argmax
CF,CA
P (CF,CA|v, ICC,ENT ) (1)
Though case assignment CA usually represents
correspondences between input case components
and case slots, in our model it also represents
correspondences between antecedents of zero pro-
nouns and case slots. Hereafter, we call the former
direct case assignment (DCA) and the latter indi-
rect case assignment (ICA). Then, we transform
P (CF
l
, CA
k
|v, ICC,ENT ) as follows:
P (CF
l
, CA
k
|v, ICC,ENT )
=P (CF
l
|v, ICC,ENT )
? P (DCA
k
|v, ICC,ENT,CF
l
)
? P (ICA
k
|v, ICC,ENT,CF
l
, DCA
k
)
?P (CF
l
|v, ICC) ? P (DCA
k
|ICC,CF
l
)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (2)
=P (CF
l
|v)?P (DCA
k
, ICC|CF
l
)/P (ICC|v)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (3)
(
? P (CF
l
|v, ICC) =
P (CF
l
, ICC|v)
P (ICC|v)
=
P (ICC|CF
l
, v) ? P (CF
l
|v)
P (ICC|v)
=
P (ICC|CF
l
) ? P (CF
l
|v)
P (ICC|v)
,
(? CF
l
contains the information about v.)
P (DCA
k
|ICC,CF
l
)
=
P (DCA
k
, ICC|CF
l
)
P (ICC|CF
l
)
)
Equation (2) is derived because we assume that
the case frame CF
l
and direct case assignment
DCA
k
are independent of existing entities ENT ,
and indirect case assignment ICA
k
is independent
of input case components ICC.
Because P (ICC|v) is constant, we can say that
our model selects the case frame CF
best
and the
direct case assignment DCA
best
and indirect case
assignment ICA
best
that maximize the probability
P (CF,DCA, ICA|v, ICC,ENT ):
(CF
best
, DCA
best
, ICA
best
) =
argmax
CF,DCA,ICA
(
P (CF |v) ? P (DCA, ICC|CF )
?P (ICA|ENT,CF,DCA)
)
(4)
The probability P (CF
l
|v), called generative
probability of a case frame, is estimated from
case structure analysis of a large raw corpus. The
following subsections illustrate how to calculate
P (DCA
k
, ICC|CF
l
) and P (ICA
k
|ENT,CF
l
,
DCA
k
).
772
3.2.1 Generative Probability of Direct Case
Assignment
For estimation of generative probability of di-
rect case assignment P (DCA
k
, ICC|CF
l
), we
follow Kawahara and Kurohashi?s (2006) method.
They decompose P (DCA
k
, ICC|CF
l
) into the
following product depending on whether a case
slot s
j
is filled with an input case component or
vacant:
P (DCA
k
, ICC|CF
l
) =
?
s
j
:A(s
j
)=1
P (A(s
j
) = 1, n
j
, c
j
|CF
l
, s
j
)
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
)
=
?
s
j
:A(s
j
)=1
{
P (A(s
j
) = 1|CF
l
, s
j
)
? P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
}
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
) (5)
where the function A(s
j
) returns 1 if a case slot s
j
is filled with an input case component; otherwise
0, n
j
denotes the content part of the case compo-
nent, and c
j
denotes the surface case of the case
component.
The probabilities P (A(s
j
) = 1|CF
l
, s
j
) and
P (A(s
j
) = 0|CF
l
, s
j
) are called generative prob-
ability of a case slot, and estimated from case
structure analysis of a large raw corpus as well as
generative probability of a case frame.
The probability P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1) is
called generative probability of a case component
and estimated as follows:
P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
?P (n
j
|CF
l
, s
j
, A(s
j
)=1)?P (c
j
|s
j
, A(s
j
)=1) (6)
P (n
j
|CF
l
, s
j
, A(s
j
) = 1) means the gener-
ative probability of a content part n
j
from a
case slot s
j
in a case frame CF
l
, and esti-
mated by using the frequency of a case slot
example in the automatically constructed case
frames. P (c
j
|s
j
, A(s
j
) = 1) is approximated by
P (c
j
|case type of(s
j
), A(s
j
)=1) and estimated
from the web corpus in which the relationship be-
tween a surface case marker and a case slot is an-
notated by hand.
3.2.2 Probability of Indirect Case Assignment
To estimate probability of indirect case assign-
ment P (ICA
k
|ENT,CF
l
, DCA
k
) we also de-
compose it into the following product depending
Table 3: Location Classes of Antecedents.
intra-sentence: case components of
L
1
: parent predicate of V
z
L
2
: parent predicate of V
z
? (parallel)
L
3
: child predicate of V
z
L
4
: child predicate of V
z
(parallel)
L
5
: parent predicate of parent noun phrase of V
z
L
6
: parent predicate of parent predicate of V
z
(parallel)
L
7
: other noun phrases following V
z
L
8
: other noun phrases preceding V
z
inter-sentence: noun phrases in
L
9
: 1 sentence before
L
10
: 2 sentences before
L
11
: 3 sentences before
L
12
: more than 3 sentences before
on whether a case slot s
j
is filled with an entity
ent
j
or vacant:
P (ICA
k
|ENT,CF
l
, DCA
k
) =
?
s
j
:A
?
(s
j
)=1
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
?
?
s
j
:A
?
(s
j
)=0
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) (7)
where the function A
?
(s
j
) returns 1 if a case slot
s
j
is filled with an entity ent
j
; otherwise 0. Note
that we only consider case slots ga, wo and ni that
is not filled with an input case component. We
approximate P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
and P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) as follows:
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 1, ent
j
|ent
j
, CF
l
, s
j
)
= P (A
?
(s
j
) = 1|ent
j
, CF
l
, s
j
) (8)
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 0|case type of(s
j
)) (9)
Equation (8) is derived because we assume
P (A
?
(s
j
) = 1|CF
l
, s
j
) is independent of exist-
ing entities that are not assigned to s
j
. Equation
(9) is derived because we assume P (A
?
(s
j
) = 0)
is independent of ENT and CF
l
, and only de-
pends on the case type of s
j
, such as ga, wo and ni.
P (A
?
(s
j
)=0|case type of(s
j
)) is the probability
that a case slot has no correspondence after zero
anaphora resolution and estimated from anaphoric
relation tagged corpus.
Let us consider the probability P (A
?
(s
j
) =
1|ent
j
, CF
l
, s
j
). We decompose ent
j
into content
part n
j
m
, surface case c
j
n
and location class l
j
n
.
Here, location classes denote the locational rela-
tions between zero pronouns and their antecedents.
We defined twelve location classes as described in
Table 3. In Table 3, V
z
means a predicate that has
a zero pronoun. Note that we also consider the
773
locations of zero pronouns that are linked to the
target entity as location class candidates. Now we
roughly approximate P (A
?
(s
j
)=1|ent
j
, CF
l
, s
j
)
as follows:
P (A
?
= 1|ent
j
, CF
l
, s
j
)
=P (A
?
= 1|n
j
m
, c
j
n
, l
j
n
, CF
l
, s
j
)
=
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
,A
?
=1)?P (A
?
=1|CF
l
, s
j
)
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
|CF
l
, s
j
)
?
P (c
j
n
|CF
l
, s
j
, A
?
=1)
P (c
j
n
|CF
l
, s
j
)
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
) (10)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
)
?
P (c
j
n
|case type of(s
j
), A
?
=1)
P (c
j
n
)
? P (A
?
=1|l
j
n
, case type of(s
j
)) (11)
(
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
)
=
P (A
?
=1, l
j
n
|CF
l
, s
j
)
P (l
j
n
|CF
l
, s
j
)
=P (A
?
=1|CF
l
, l
j
n
, s
j
)
)
Note that because ent
j
is often mentioned more
than one time, there are several combinations of
content part n
j
m
, surface case c
j
n
and location
class l
j
n
candidates. We select the pair of m and n
with the highest probability.
Equation (10) is derived because we as-
sume n
j
m
, c
j
n
and l
j
n
are independent of each
other. Equation (11) is derived because we ap-
proximate P (A
?
= 1|CF
l
, l
j
n
, s
j
) as P (A
?
=
1|l
j
n
, case type of(s
j
)), and assume P (n
j
m
) and
P (c
j
n
) are independent of CF
l
and s
j
. Since these
approximation is too rough, specifically, P (n
j
m
)
and P (c
j
n
) tend to be somewhat smaller than
P (n
j
m
|CF
l
, s
j
) and P (c
j
n
|CF
l
, s
j
) and equation
(11) often becomes too large, we introduce a
parameter ?(? 1) and use the ?-times value as
P (A
?
= 1|ent
j
, CF
l
, s
j
).
The first term of equation (11) represents how
likely an entity that contains n
j
m
as a content part
is considered to be an antecedent, the second term
represents how likely an entity that contains c
j
n
as
a surface case is considered to be an antecedent,
and the third term gives the probability that an
entity that appears in location class l
j
n
is an an-
tecedent.
The probabilities P (n
j
m
) and P (c
j
n
) are esti-
mated from a large raw corpus. The probabili-
ties P (c
j
n
|case type of(s
j
)) and P (A
?
= 1|l
j
n
,
case type of(s
j
)) are estimated from the web
corpus in which the relationship between an an-
tecedent of a zero pronoun and a case slot, and the
relationship between its surface case marker and a
case slot are annotated by hand. Then, let us con-
sider the probability P (n
j
m
|CF
l
, s
j
, A
?
(s
j
) = 1)
in the next subsection.
3.2.3 Probability of Component Part of Zero
Pronoun
P (n
j
m
|CF
l
, s
j
, A
?
=1) is similar to P (n
j
|CF
l
,
s
j
, A=1) and can be estimated approximately from
case frames using the frequencies of case slot ex-
amples. However, while A
?
(s
j
) = 1 means s
j
is
not filled with input case component but filled with
an entity as the result of zero anaphora resolution,
case frames are constructed by extracting only the
input case component. Therefore, the content part
of a zero anaphora antecedent n
j
m
is often not in-
cluded in the case slot examples. To cope with this
problem, we utilize generalized examples.
When one mention of an entity is tagged any
category or recognized as an NE, we also use the
category or the NE class as the content part of the
entity. For examples, if an entity {Prius} is recog-
nized as an artifact name and assigned to wo case
of the case frame hanbai(1) in Table 1, the system
also calculates:
P (NE :ARTIFACT |hanbai(1),wo, A
?
(wo)=1)
P (NE :ARTIFACT )
besides:
P (Prius|hanbai(1),wo, A
?
(wo) = 1)
P (Prius)
and uses the higher value.
3.3 Salience Score
Previous works reported the usefulness of salience
for anaphora resolution (Lappin and Leass, 1994;
Mitkov et al, 2002). In order to consider salience
of an entity, we introduce salience score, which is
calculated by the following set of simple rules:
? +2 : mentioned with topical marker ?wa?.
? +1 : mentioned without topical marker ?wa?.
? +0.5 : assigned to a zero pronoun.
? ?0.7 : beginning of each sentence.
For examples, we consider the salience score of
the entity {Toyota} in (i) in Section 3.1. In the
first sentence, since {Toyota} is mentioned with
topical marker ?wa?, the salience score is 2. At the
beginning of the second sentence it becomes 1.4,
774
Table 4: Data for Parameter Estimation.
probability data
P (n
j
) raw corpus
P (c
j
) raw corpus
P (c
j
|case type of(s
j
), A(s
j
)=1) tagged corpus
P (c
j
|case type of(s
j
), A
?
(s
j
)=1) tagged corpus
P (n
j
|CF
l
, s
j
, A(s
j
)=1) case frames
P (n
j
|CF
l
, s
j
, A
?
(s
j
)=1) case frames
P (CF
l
|v
i
) case structure analysis
P (A(s
j
)={0, 1} |CF
l
, s
j
) case structure analysis
P (A
?
(s
j
)=0|case type of(s
j
)) tagged corpus
P (A
?
(s
j
)=1|l
j
, case type of(s
j
)) tagged corpus
Table 5: Experimental Results.
R P F
Kawahara & Kurohashi .230 (28/122) .173 (28/162) .197
Proposed (? = 1) .426 (52/122) .271 (52/192) .331
(? = 1/2) .410 (50/122) .373 (50/134) .391
(? = 1/4) .295 (36/122) .419 (36/86) .346
and after assigned to the zero pronoun of ?hanbai?
it becomes 1.9. Note that we use the salience score
not as a probabilistic clue but as a filter to consider
the target entity as a possible antecedent. When we
use the salience score, we only consider the entities
that have the salience score no less than 1.
4 Experiments
4.1 Setting
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences).
We selected 20 documents for test and used the
other 166 documents for calculating several proba-
bilities. Since the anaphoric relations in some web
documents were not so clear and too difficult to
recognize, we did not select such documents for
test. In the 20 test documents, 122 zero anaphora
relations were tagged between one of the mentions
of the antecedent and the target predicate that had
the zero pronoun.
Each parameter for proposed model was esti-
mated using maximum likelihood from the data
described in Table 4. The case frames were auto-
matically constructed from web corpus comprising
1.6 billion sentences. The case structure analysis
was conducted on 80 million sentences in the web
corpus, and P (n
j
) and P (c
j
)were calculated from
the same 80 million sentences.
In order to concentrate on zero anaphora resolu-
tion, we used the correct morphemes, named enti-
ties, syntactic structures and coreferential relations
that were annotated by hand. Since correct corefer-
ential relations were given, the number of created
entities was same between the gold standard and
the system output because zero anaphora resolu-
tion did not create new entities.
4.2 Experimental Results
We conducted experiments of zero anaphora reso-
lution. As the parameter ? introduced in Section
3.2.2., we tested 3 values 1, 1/2, and 1/4. For
comparison, we also tested Kawahara and Kuro-
hashi?s (2004) model. The experimental results are
shown in Table 5, in which recall R, precision P
and F-measure F were calculated by:
R =
# of correctly recognized zero anaphora
# of zero anaphora tagged in corpus
,
P =
# of correctly recognized zero anaphora
# of system outputted zero anaphora
,
F =
2
1/R + 1/P
.
Kawahara and Kurohashi?s model achieved al-
most 50% as F-measure against newspaper arti-
cles. However, as a result of our experiment
against web documents, it achieved only about
20% as F-measure. This may be because anaphoric
relations in web documents were not so clear as
those in newspaper articles and more difficult to
recognize. As to the parameter ?, the larger ?
tended to output more zero anaphora, and the high-
est F-measure was achieved against ? = 1/2.
When using ? = 1/2, there were 72 (=122?50)
zero pronouns that were tagged in the corpus and
not resolved correctly. Only 12 of them were cor-
rectly detected and assigned to a wrong entity, that
is, 60 of them were not even detected. Therefore,
we can say our recall errors were mainly caused by
the low recall of zero pronoun detection.
In order to confirm the effectiveness of gener-
alized examples of case slots and salience score,
we also conducted experiments under several con-
ditions. We set ? = 1/2 in these experiments. The
results are shown in Table 6, in which CT means
generalized categories, NE means generalized NEs
and SS means salience score.
Without using any generalized examples, the F-
measure is less than Kawahara and Kurohashi?s
method, which use similarity to deal with sparse-
ness of case slot examples, and we can con-
firm the effectiveness of the generalized examples.
While generalized categories much improved the
F-measure, generalized NEs contribute little. This
may be because the NE rate is smaller than com-
mon noun rate, and so the effect is limited.
We also confirmed that the salience score filter
improved F-measure. Moreover, by using salience
score filter, the zero anaphora resolution becomes
about ten times faster. This is because the system
775
Table 6: Experiments under Several Conditions.
CT NE SS R P F
?
.131 (16/122) .205 (16/78) .160
? ?
.164 (20/122) .247 (20/81) .197
? ?
.402 (49/122) .368 (49/133) .384
? ?
.385 (47/122) .196 (47/240) .260
? ? ?
.410 (50/122) .373 (50/134) .391
can avoid checking entities with low salience as
antecedent candidates.
4.3 Comparison with Previous Works
We compare our accuracies with (Seki et al,
2002). They achieved 48.9% in precision, 88.2%
in recall, and 62.9% in F-measure for zero pro-
noun detection, and 54.0% accuracy for antecedent
estimation on 30 newspaper articles, that is, they
achieved about 34% in F-measure for whole zero
pronoun resolution. It is difficult to directly com-
pare their results with ours due to the difference
of the corpus, but our method achieved 39% in
F-measure and we can confirm that our model
achieves reasonable performance considering the
task difficulty.
5 Conclusion
In this paper, we proposed a probabilistic model
for Japanese zero anaphora resolution. By us-
ing automatically constructed wide-coverage case
frames that include generalized examples and in-
troducing salience score filter, our model achieves
reasonable performance against web corpus. As
future work, we plan to conduct large-scale ex-
periments and integrate this model to a fully-
lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis (Kawahara and
Kurohashi, 2006).
References
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploiting syntactic patterns as clues in zero-
anaphora resolution. In Proceedings of COL-
ING/ACL 2006, pages 625?632.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Isozaki, Hideki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 184?191.
Kawahara, Daisuke and Sadao Kurohashi. 2002.
Fertilization of Case Frame Dictionary for Robust
Japanese Case Analysis. In Proceedings of the 19th
International Conference on Computational Linguis-
tics, pages 425?431.
Kawahara, Daisuke and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of the 1st International
Joint Conference on Natural Language Processing
(IJCNLP-04), pages 334?341.
Kawahara, Daisuke and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 176?183.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Luo, Xiaoqiang. 2007. Coreference or not: A
twin model for coreference resolution. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 73?80.
Mitkov, Ruslan, Richard Evans, and Constantin Or?asan.
2002. A new, fully automatic version of mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002).
Ng, Vincent. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of the 43rd Annual Meeting
of the Asssociation for Computational Linguistics,
pages 157?164.
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP-08), pages 607?612.
Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics, pages
911?917.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
The National Language Institute for Japanese Lan-
guage. 2004. Bunruigoihyo. Dainippon Tosho, (In
Japanese).
Yang, Xiaofeng, Jian Su, Jun Lang, Ghew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-mention
model for coreference resolution with inductive logic
programming. In Proceedings of ACL-08: HLT,
pages 843?851.
776
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1049?1056
Manchester, August 2008
Chinese Dependency Parsing with Large Scale Automatically 
Constructed Case Structures 
Kun Yu 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kunyu@nlp.kuee.kyo
to-u.ac.jp 
Daisuke Kawahara 
National Institute of Informa-
tion and Communications 
Technology, Japan 
dk@nict.go.jp 
Sadao Kurohashi 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kuro@i.kyoto-
u.ac.jp 
 
Abstract 
This paper proposes an approach using 
large scale case structures, which are 
automatically constructed from both a 
small tagged corpus and a large raw cor-
pus, to improve Chinese dependency 
parsing. The case structure proposed in 
this paper has two characteristics: (1) it 
relaxes the predicate of a case structure to 
be all types of words which behaves as a 
head; (2) it is not categorized by semantic 
roles but marked by the neighboring 
modifiers attached to a head. Experimen-
tal results based on Penn Chinese Tree-
bank show the proposed approach 
achieved 87.26% on unlabeled attach-
ment score, which significantly outper-
formed the baseline parser without using 
case structures. 
1 Introduction 
Case structures (i.e. predicate-argument struc-
tures) represent what arguments can be attached 
to a predicate, which are very useful to recognize 
the meaning of natural language text. Research-
ers have applied case structures to Japanese syn-
tactic analysis and improved parsing accuracy 
successfully (Kawahara and Kurohashi, 2006(a); 
Abekawa and Okumura, 2006). However, few 
works focused on using case structures in Chi-
nese parsing. Wu (2003) proposed an approach 
to learn the relations between verbs and nouns 
and applied these relations to a Chinese parser. 
Han et al (2004) presented a method to acquire 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
the sub-categorization of Chinese verbs and used 
them in a PCFG parser. 
Normally, case structures are categorized by 
semantic roles for verbs. For example, Kawahara 
and Kurohashi (2006(b)) constructed Japanese 
case structures which were marked by post posi-
tions. Wu (2003) classified the Chinese verb-
noun relations as ?verb-object? and ?modifier-
head?. In this paper, we propose a new type of 
Chinese case structure, which is different from 
those presented in previous work (Wu, 2003; 
Han et al, 2004; Kawahara and Kurohashi, 
2006(a); Abekawa and Okumura, 2006) in two 
aspects:  
(1) It relaxes the predicate of a case structure 
to be all types of words which behaves as a head; 
(2) It is not categorized by semantic roles 
but marked by the neighboring modifiers at-
tached to a head. The sibling modification infor-
mation remembers the parsing history of a head 
node, which is useful to correct the parsing error 
such as a verb ? (see) is modified by two nouns 
?? (film) and ?? (introduction) as objects 
(see Figure 1). 
 
Figure 1. Dependency trees of an example sen-
tence (I see the introduction of a film). 
We automatically construct large scale case 
structures from both a small tagged corpus and a 
large raw corpus. Then, we apply the large scale 
case structures to a Chinese dependency parser to 
improve parsing accuracy.  
The Chinese dependency parser using case 
structures is evaluated by Penn Chinese Tree-
bank 5.1 (Xue et al, 2002). Results show that the 
1049
automatically constructed case structures helped 
increase parsing accuracy by 2.13% significantly.  
The rest of this paper is organized as follows: 
Section 2 describes the proposed Chinese case 
structure and the construction method in detail; 
Section 3 describes a Chinese dependency parser 
using constructed case structures; Section 4 lists 
the experimental results with a discussion in sec-
tion 5; Related work is introduced in Section 6; 
Finally, Section 7 gives a brief conclusion and 
the direction of future work. 
2 Chinese Case Structure and its Con-
struction 
2.1 A New Type of Chinese Case Structure 
We propose a new type of Chinese case structure 
in this paper, which is represented as the combi-
nation of a case pattern and a case element (see 
Figure 2). Case element remembers the bi-lexical 
dependency relation between all types of head-
modifier pairs, which is also recognized in previ-
ous work (Wu, 2003; Han et al, 2004; Kawahara 
and Kurohashi, 2006(a); Abekawa and Okumura, 
2006). Case pattern keeps the pos-tag sequence 
of all the modifiers attached to a head to remem-
ber the parsing history of a head node.  
 
Figure 2. An example of constructed case 
structure. 
2.2 Construction Corpus 
We use 9,684 sentences from Penn Chinese 
Treebank 5.1 as the tagged corpus, and 7,338,028 
sentences written in simplified Chinese from 
Chinese Gigaword (Graff et al, 2005) as the raw 
corpus for Chinese case structure construction.  
Before constructing case structures from the 
raw corpus, we need to get the syntactic analysis 
of it. First, we do word segmentation and pos-
tagging for the sentences in Chinese Gigaword 
by a Chinese morphological  analyzer 
(Nakagawa and Uchimoto, 2007). Then a 
Chinese deterministic syntactic analyzer (Yu et 
al., 2007) is used to parse the whole corpus. 
To guarantee the accuracy of constructed case 
structures, we only use the sentences with less 
than k words from Chinese Gigaword. It is based 
on the assumption that parsing short sentences is 
more accurate than parsing long sentences. The 
performance of the deterministic parser used for 
analyzing Chinese Gigaword (see Figure 3) 
shows smaller k ensures better parsing quality 
but suffers from lower sentence coverage. 
Referring to Figure 2, we set k as 30. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Performance of the deterministic parser 
with different k on 1,800 sentences2. 
2.3 Case Pattern Construction 
A case pattern consists of a sequence of pos-tags 
indicating the order of all the modifiers attached 
to a head (see Figure 1), which can be repre-
sented as following.  
>=< ?? rnnlmmi posposposposposposcp ],,...,[,],...,,[ 1111
Here, 
lmm pospospos ],...,,[ 11? means the pos-tag 
sequence of the modifiers attached to a head 
from the left side, and 
rnn pospospos ],,...,[ 11 ? means 
the pos-tag sequence of the modifiers attached to 
a head from the right side. 
We use the 33 pos-tags defined in Penn Chi-
nese Treebank (Xue et al, 2002) to describe a 
case pattern, and make following modifications: 
? group common noun, proper noun and 
pronoun together and mark them as ?noun?; 
? group predicative adjective and all the 
other verbs together and mark them as ?verb?; 
? only regard comma, pause, colon and 
semi-colon as punctuations and mark them as 
?punc?, and neglect other punctuations. 
                                                 
2 UAS means unlabeled attachment score (Buchholz and 
Marsi, 2006). The sentences used for this evaluation are 
from Penn Chinese Treebank with gold word segmentation 
and pos-tag. 
0 10 20 30 40 50 60 70 80 90 100 110
20
30
40
50
60
70
80
90
100
%
k
 UAS
 Sentence Coverage
1050
? group cardinal number and ordinal number 
together and mark them as ?num?; 
? keep the original definition for other pos-
tags but label them by new tags, such as labeling 
?P? as ?prep? and labeling ?AD? as ?adv?. 
The task of case pattern construction is to ex-
tract cpi for each head from both the tagged cor-
pus and the raw corpus. As we will introduce 
later, the Chinese dependency parser using case 
structures applies CKY algorithm for decoding. 
Thus the following substrings of cpi are also ex-
tracted for each head as horizontal Markoviza-
tion during case pattern construction. 
],1[],,1[                                                
],,...,[,],...,,[ 1111
njmk
pospospospospospos rjjlkk
???
>< ??  
],1[  ,],...,,[ 11 mkpospospos lkk ??>< ?  
],1[  ,],,...,[ 11 njpospospos rjj ??>< ?  
2.4 Case Element Construction 
As introduced in Section 2.1, a case element 
keeps the lexical preference between a head and 
its modifier. Therefore, the task of case element 
construction is to extract head-modifier pairs 
from both the tagged corpus and the raw corpus.  
Although only the sentences with less than k 
(k=30) words from Chinese Gigaword are used 
as raw corpus to guarantee the accuracy, there 
still exist some dependency relations with low 
accuracy in these short sentences because of the 
non-perfect parsing quality. Therefore, we apply 
a head-modifier (HM) classifier to the parsed 
sentences from Chinese Gigaword to further ex-
tract head-modifier pairs with high quality. This 
HM classifier is based on SVM classification. 
Table 1 lists the features used in this classifier.  
Feature Description 
Poshead/ 
Posmod 
Pos-tag pair of head and modifier 
Distance Distance between head and modifier 
HasComma If there exists comma between head and  modifier, set as 1; otherwise as 0 
HasColon If there exists colon between head and modifier, set as 1; otherwise as 0 
HasSemi If there exists semi-colon between head and modifier, set as 1; otherwise as 0 
Table 1. Features for HM classifier. 
The HM classifier is trained on 3500 sentences 
from Penn Chinese Treebank with gold-standard 
word segmentation and pos-tag. All the sentences 
are parsed by the same Chinese deterministic 
parser used for Chinese Gigaword analysis. The 
correct dependency relations created by the 
parser are looked as positive examples and the 
left dependency relations are used as negative 
examples. TinySVM 3  is selected as the SVM 
toolkit. A polynomial kernel is used and degree 
is set as 2. Tested on 346 sentences, which are 
from Penn Chinese Treebank and parsed by the 
same deterministic parser with gold standard 
word segmentation and pos-tag, this HM classi-
fier achieved 96.77% on precision and 46.35% 
on recall. 
3 A Chinese Dependency Parser Using 
Case Structures 
3.1 Parsing Model 
We develop a lexicalized Chinese dependency 
parser to use constructed case structures. This 
parser gives a probability P(T|S) to each possible 
dependency tree T of an input sentence 
S=w1,w2,?,wn (wi is a node representing a word 
with its pos-tag), and outputs the dependency 
tree T* that maximizes P(T|S) (see equation 1). 
CKY algorithm is used to decode the dependency 
tree from bottom to up. 
)|(maxarg STPT
T
=?  (1)
To use case structures, P(T|S) is divided into 
two parts (see equation 2): the probability of a 
sentence S generating a root node wROOT, and the 
product of the probabilities of a node wi generat-
ing a case structure CSi. 
?
=
?=
m
i
iiROOT wCSPSwPSTP
1
)|()|()|(  (2)
As introduced in Section 2, a case structure 
CSi is composed of a case pattern cpi and a case 
element cmi. Thus 
),|()|(                  
)|,()|(
iiiii
iiiii
cpwcmPwcpP
wcmcpPwCSP
?=
=  
(3)
A case element cmi consists of a set of de-
pendencies {Dj}, in which each Dj is a tuple <wj, 
disj, commaj>. Here wj means a modifier node, 
disj means the distance between wj and its head, 
and commaj means the number of commas be-
tween wj and its head. Assuming any Dj and Dk 
are independent of each other when they belong 
to the same case element, P(cmi|wi,cpi) can be 
written as  
),|,,(                        
),|(),|(
iij
j
jj
ii
j
jiii
cpwcommadiswP
cpwDPcpwcmP
?
?
=
=
(4)
                                                 
3 http://chasen.org/~taku/software/TinySVM/ 
1051
Finally, P(wj,disj,commaj | wi,cpi) is divided as 
),,|,(),|(
),|,,(
ijijjiij
iijjj
cpwwcommadisPcpwwP
cpwcommadiswP
?=  (5) 
Maximum likelihood estimation is used to es-
timate P(wROOT|S) on training data set with the 
smoothing method used in (Collins, 1996). The 
estimation of P(cpi|wi), P(wj|wi,cpi), and P(disj, 
commaj |wi,wj,cpi) will be introduced in the fol-
lowing subsections. 
3.2 Estimating P(cpi|wi) by Case Patterns 
Three steps are used to estimate P(cpi|wi) by 
maximum likelihood estimation using the con-
structed case patterns: 
? Estimate P(cpi|wi) only by the case pat-
terns from the tagged corpus and represent it as 
)|(? iitagged wcpP ; 
? Estimate P(cpi|wi) only by the case pat-
terns from the raw corpus and represent it as 
)|(? iiraw wcpP ; 
? Estimate P(cpi|wi) by equation 6, in which 
?pattern is calculated by equation 7 to set proper 
ratio for the probabilities estimated by the case 
patterns from different corpora.  
)|(?)1()|(?
)|(?
iirawpatterniitaggedpattern
ii
wcpPwcpP
wcpP
??+?= ?? (6)
1++++
+++=
rawrawtaggedtagged
rawrawtaggedtagged
pattern ????
?????  
(7)
In equation 7, ?tagged and ?raw mean the occur-
rence of a lexicalized node wi=<lexi, posi> gener-
ating cpi in the tagged or raw corpus, ?tagged and 
?raw mean the occurrence of a back-off node 
wi=<posi> generating cpi in the tagged or raw 
corpus. To overcome the data sparseness prob-
lem, we not only apply the smoothing method 
used in (Collins, 1996) for a lexicalized head to 
back off it to its part-of-speech, but also assign a 
very small value to P(cpi|wi) when there is no cpi 
modifying wi in the constructed case patterns. 
3.3 Estimating P(wj|wi,cpi) and P(disj, com-
maj |wi,wj,cpi) by Case Elements 
To estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by maximum likelihood estimation, we 
also use three steps: 
? Estimate the two probabilities only by the 
case elements from the tagged corpus and repre-
sent them as ),|(? iijtagged cpwwP  and 
),,|,(? ijijjtagged cpwwcommadisP ; 
? Estimate the two probabilities only by the 
case elements from the raw corpus, and represent 
them as ),|(? iijraw cpwwP  and 
),,|,(? ijijjraw cpwwcommadisP ; 
? Estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by equation 8 and equation 9. 
),|(?)1(   
 ),|(?
),|(?
iijrawelement
iijtaggedelement
iij
cpwwP
cpwwP
cpwwP
??
+?=
?
?  
(8)
),,|,(?)1(   
),,|,(?
),,|,(?
ijijjrawelement
ijijjtaggedelement
ijijj
cpwwcommadisP
cpwwcommadisP
cpwwcommadisP
??
+?=
?
?  
(9)
The smoothing method used in (Collins, 1996) 
is applied during estimation.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Parsing accuracy with different ?element 
on development data set. 
In order to set proper ratio for the probabilities 
estimated by the case elements from different 
corpora, we use a parameter ?element in equation 8 
and 9. The appropriate setting (?element =0.4) is 
learned by a development data set (see Figure 4). 
4 Evaluation Results 
4.1 Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set to 
evaluate the proposed approach. 9,684 sentences 
from Section 001-270 and 400-931, which are 
also used for constructing case structures, are 
used as training data. 346 sentences from Section 
271-300 are used as testing data. 334 sentences 
from Section 301-325 are used as development 
data. Penn2Malt4 is used to transfer the phrase 
structure of Penn Chinese Treebank to depend-
ency structure. Gold-standard word segmentation 
and pos-tag are applied in all the experiments. 
                                                 
4 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
86.0
86.5
87.0
87.5
88.0
88.5
 
 
U
A
S 
(%
)
?
element
1052
Unlabeled attachment score (UAS) (Buchholz 
and Marsi, 2006) is used as evaluation metric. 
Because of the difficulty of assigning correct 
head to Chinese punctuation, we calculate UAS 
only on the dependency relations in which the 
modifier is not punctuation. 
4.2 Results 
Three parsers were evaluated in this experiment: 
? ?baseline?: a parser not using case struc-
tures, where P(T|S) is calculated by equation 11 
and P(wj|wi) and P(disj, commaj |wi,wj) are esti-
mated by training data set only.  
?
?
??
??
??=
?=
jinji
jijjijROOT
jinji
ijjjROOT
wwcommadisPwwPSwP
wcommadiswPSwP
STP
],,1[,
],,1[,
)),|,()|(()|(
)|,,()|(
)|(
(11) 
? ?w/ case elem?: a parser only using case 
element, which also calculates P(T|S) by equa-
tion 11 but estimates P(wj|wi) and P(disj, commaj 
|wi,wj) by constructed case elements.  
? ?proposed?: the parser introduced in Sec-
tion 3, which uses both case elements and case 
patterns. 
The evaluation results on testing data set (see 
Table 2) shows the proposed parser achieved 
87.26% on UAS, which was 2.13% higher than 
that of the baseline parser. This improvement is 
regarded as statistically significant (McNemar?s 
test: p<0.0005). Besides, Table 2 shows only us-
ing case elements increased parsing accuracy by 
1.30%. It means both case elements and case pat-
terns gave help to parsing accuracy, and case 
elements contributed more in the proposed ap-
proach. 
Parsing 
model baseline w/ case elem proposed 
UAS (%) 85.13 86.43 (+1.30) 87.26 (+2.13)
Table 2. Parsing accuracy of different parsing 
models. 
Figure 5 and Figure 6 show the dependency 
trees of two example sentences created by both 
the baseline parser and the proposed parser. In 
Figure 5, the baseline parser incorrectly assigned 
??/NN (signing) as the head of ??/NN (co-
operation). However, after using the case ele-
ment ??/NN (project) ? ??/NN, the cor-
rect head of ??/NN was found by the pro-
posed parser. Figure 6 shows the baseline parser 
recognized ??/VV (opening) as the head of ?
/P (as) incorrectly. But in the proposed parser, 
the probability of ??/VV generating the case 
pattern ?[prep, punc, prep]l? was much lower 
than that of ??/VV  generating the case pattern 
?[prep]l?. Therefore, the proposed parser rejected 
the incorrect dependency that?/P modified?
?/VV  and got the correct head of ?/P as ??
/VV (show) successfully. 
5 Discussion 
5.1 Influence of the Number of Case Struc-
tures on Parsing Accuracy 
During case structure construction, we only used 
the sentences with less than k (k=30) words from 
Chinese Gigaword as the raw corpus. Enlarging k 
will introduce more sentences from Chinese Gi-
gaword and increase the number of case struc-
tures. Table 4 lists the number of case structures 
and parsing accuracy of the proposed parser on 
testing data set with different k5. It shows enlarg-
ing the number of case structures is a possible 
way to increase parsing accuracy. But simply 
setting larger k did not help parsing, because it 
decreased the parsing accuracy of Chinese Gi-
gaword and consequently decreased the accuracy 
of constructed case structures. Using good parse 
selection (Reichart and Rappoport, 2007; Yates 
et al, 2006) on the syntactic analysis of Chinese 
Gigaword is a probable way to construct more 
case structures without decreasing their accuracy. 
We will consider about it in the future. 
k 10 20 30 40 
# of Case Ele-
ment (M) 0.66 1.14 1.81 2.75
# of Case Pat-
tern (M) 0.57 1.55 3.91 8.48
UAS (%) 85.16 86.42 87.26 87.07
Table 4. Case structure number and parsing 
accuracy with different k. 
5.2 Influence of the Case Structure Con-
struction Corpus on Parsing Accuracy 
We also evaluated the proposed parser on testing 
data set using case structures constructed from 
different corpora.  
Results (see Table 5) show that parsing accu-
racy was improved greatly only when using case 
structures constructed from both the two corpora. 
The case structures constructed from either of a 
                                                 
5 Considering about the time expense of case structure con-
struction, we only did test for k ?40. 
1053
single corpus only gave a little help to parsing. It 
is because among all the case structures used 
during testing (see Table 6), 19.57% case ele-
ments were constructed from the tagged corpus 
only and 54.18% case patterns were constructed 
from the raw corpus only. The incorrect head-
modifier pairs extracted from Chinese Gigaword 
is a possible reason for the fact that some case 
elements only existing in the tagged corpus. En-
hancing good parse selection on Chinese Giga-
word could improve the quality of extracted 
head-modifier pairs and solve this problem. In 
addition, the strict definition of case pattern is a 
probable reason that makes more than half of the 
case patterns only exist in the raw corpus and 
18.18% case patterns exist in neither of the two 
corpora. We will modify the representation of 
case pattern to make it more flexible to the num-
ber of modifiers to resolve this issue in the future. 
Corpus Tagged Raw Tagged+Raw
UAS (%) 85.25 85.90 87.26 
Table 5. Parsing accuracy with case structures 
constructed from different corpora. 
Corpus Tagged Raw Tagged+Raw None
% of case 
element  19.57 8.95 68.07 3.41
% of case 
pattern 0.03 54.18 27.61 18.18
Table 6. Ratio of case structures constructed 
from different corpora. 
 
Figure 5. Dependency trees of an example sentence (The signing of China-US cooperation high tech 
project ?). 
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV ?/DEC  ???/NN  ?/AD  ??/VV  ... 
(a) dependency tree created by the baseline parser
(b) dependency tree created by the proposed parser
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV  ?/DEC  ???/NN  ?/AD  ??/VV ... 
 
Figure 6. Dependency trees of an example sentence (As introduced, the exhibition opening in the 
stadium will show?). 
5.3 Parsing Performance with Real Pos-tag 
Gold standard word segmentation and pos-tag 
are applied in previous experiments. However, 
parsing accuracy will be affected by the incorrect 
word segmentation and pos-tag in the real appli-
cations. Currently, the best performance of Chi-
nese word segmentation has achieved 99.20% on 
F-score, but the best accuracy of Chinese pos-
tagging was 96.89% (Jin and Chen, 2008). 
Therefore, we think pos-tagging is more crucial 
for applying parser in real task compared with 
word segmentation. Considering about this, we 
evaluated the parsing models introduced in Sec-
tion 4 with real pos-tag in this experiment. 
 
 
 
Parsing model baseline proposed 
UAS (%) 80.91 82.90 (+1.99) 
Table 7. Parsing accuracy of different parsing 
models with real pos-tag. 
An HMM-based pos-tagger is used to get pos-
tag for testing sentences with gold word segmen-
tation. The pos-tagger was trained on the same 
training data set described in Section 4.1 and 
achieved 93.70% F-score on testing data set. Re-
sults (see Table 7) show that even if with real 
pos-tags, the proposed parser still outperformed 
the baseline parser significantly. However, the 
results in Table 7 indicate that incorrect pos-tag 
affected the parsing accuracy of the proposed 
parser greatly. Some researchers integrated pos-
1054
tagging into parsing and kept n-best pos-tags to 
reduce the effect of pos-tagging errors on parsing 
accuracy (Cao et al, 2007). We will also con-
sider about this in our future work. 
6 Related Work 
To our current knowledge, there were few works 
about using case structures in Chinese parsing, 
except for the work of Wu (2003) and Han et al 
(2004). Compared with them, our proposed ap-
proach presents a new type of case structures for 
all kinds of head-modifier pairs, which not only 
recognizes bi-lexical dependency but also re-
members the parsing history of a head node. 
Parsing history has been used to improve pars-
ing accuracy by many researchers (Yamada and 
Matsumoto, 2003; McDonald and Pereira, 2006). 
Yamada and Matsumoto (2003) showed that 
keeping a small amount of parsing history was 
useful to improve parsing performance in a shift-
reduce parser. McDonald and Pereira (2006) ex-
panded their first-order spanning tree model to be 
second-order by factoring the score of the tree 
into the sum of adjacent edge pair scores. In our 
proposed approach, the case patterns remember 
the neighboring modifiers for a head node like 
McDonald and Pereira?s work. But it keeps all 
the parsing histories of a head, which is different 
from only keeping adjacent two modifiers in 
(McDonald and Pereira, 2006). Besides, to use 
the parsing histories in CKY decoding, our ap-
proach applies horizontal Markovization during 
case pattern construction. In general, the success 
of using case patterns in Chinese parsing in his 
paper proves again that keeping parsing history is 
crucial to improve parsing performance, no mat-
ter in which way and to which parsing model it is 
applied. 
There were also some works that handled lexi-
cal preference for Chinese parsing in other ways. 
For example, Cheng et al (2006) and Hall et al 
(2007) applied shift-reduce deterministic parsing 
to Chinese. Sagae and Tsujii (2007) generalized 
the standard deterministic framework to prob-
abilistic parsing by using a best-first search strat-
egy. In these works, lexical preferences were 
introduced as features for predicting parsing ac-
tion. Besides, Bikel and Chiang (2000) applied 
two lexicalized parsing models developed for 
English to Penn Chinese Treebank. Wang et al 
(2005) proposed a completely lexicalized bot-
tom-up generative parsing model to parse Chi-
nese, in which a word-similarity-based smooth-
ing was introduced to replace part-of-speech 
smoothing. 
7 Conclusion and Future Work 
This paper proposes an approach to use large 
scale case structures, which are automatically 
constructed from both a small tagged corpus and 
the syntactic analysis of a large raw corpus, to 
improve Chinese dependency parsing. The pro-
posed case structures not only recognize the lexi-
cal preference between all types of head-modifier 
pairs, but also keep the parsing history of a head 
word. Experimental results show the proposed 
approach improved parsing accuracy signifi-
cantly. Besides, although we only apply the pro-
posed approach to Chinese dependency parsing 
currently, the same idea could be adapted to 
other languages easily because it doesn?t use any 
language specific knowledge. 
There are several future works under consid-
eration, such as modifying the representation of 
case patterns to make it more robust, enhancing 
good parse selection on the analysis of raw cor-
pus, and integrating pos-tagging into parsing 
model.  
References 
T.Abekawa and M.Okumura. 2006. Japanese De-
pendency Parsing Using Co-occurrence Informa-
tion and a Combination of Case Elements. In Pro-
ceedings of   the joint conference of the 
International Committee on Computational Lin-
guistics and the Association for Computational 
Linguistics 2006. pp. 833-840. 
D.Bikel. 2004. Intricacies of Collins? Parsing Model. 
Computational Linguistics, 30(4): 479-511. 
D.Bikel and D.Chiang. 2000. Two Statistical Parsing 
Models Applied to the Chinese Treebank. In Pro-
ceedings of the 2nd Chinese Language Processing 
Workshop. pp. 1-6. 
S.Buchholz and E.Marsi. 2006. CoNLL-X Shared 
Task on Multilingual Dependency Parsing. In Pro-
ceedings of the 10th Conference on Computational 
Natural Language Learning.  
H.Cao et al. 2007. Empirical Study on Parsing Chi-
nese Based on Collins? Model. In Proceedings of 
the 10th Conference of the Pacific Association for 
Computational Linguisitcs. pp. 113-119. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2006. Multi-
lingual Dependency Parsing at NAIST. In Proceed-
ings of the 10th Conference on Computational 
Natural Language Learning. pp. 191-195. 
1055
M.Collins. 1996. A New Statistical Parser Based on 
Bigram Lexical Dependencies. In Proceedings of 
the 34th Annual Meeting of the Association for 
Computational Linguistics. pp. 184-191. 
M.Collins. 1999. Head-Driven Statistical Models for 
Natural Language Parsing. Ph.D Thesis. University 
of Pennsylvania. 
D.Graff et al. 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia. 
J.Hall et al 2007. Single Malt or Blended? A Study in 
Multilingual Parser Optimization. In Proceedings 
of the shared task at the Conference on Computa-
tional Natural Language Learning 2007. pp. 933-
939. 
X.Han et al. 2004. Subcategorization Acquisition and 
Evaluation for Chinese Verbs. In Proceedings of 
the 20th International Conference on Computa-
tional Linguistics.  
G.Jin and X.Chen. 2008. The Fourth International 
Chinese Language Processing Bakeoff: Chinese 
Word Segmentation, Named Entity Recognition 
and Chinese Pos Tagging. In Proceedings of the 6th 
SIGHAN Workshop on Chinese Language Process-
ing. 
D.Kawahara and S.Kurohashi. 2006 (a). A Fully-
lexicalized Probabilistic Model for Japanese Syn-
tactic and Case frame Analysis. In Proceedings of 
the Human Language Technology conference - 
North American chapter of the Association for 
Computational Linguistics annual meeting 2006. 
pp. 176-183. 
D.Kawahara and S.Kurohashi. 2006 (b). Case Frame 
Compilation from the Web Using High-
performance Computing. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation. 
T.Kudo and Y.Matsumoto. 2002. Japanese Depend-
ency Analysis Using Cascaded Chunking. In Pro-
ceedings of the Conference on Natural Language 
Learning. pp. 29-35. 
R.McDonald and F.Pereira. 2006. Online Learning of 
Approximate Dependency Parsing Algorithm. In 
Proceedings of the 11th Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
R.McDonald and J.Nivre. 2007. Characterizing the 
Errors of Data-driven Dependency Parsing Models. 
In Proceedings of the Conference on Empirical 
Methods in Natural Language Processing Confer-
ence on Computational Natural Language Learn-
ing 2007. 
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Ap-
proach to Word Segmentation and POS Tagging. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics. pp. 
217-220. 
R.Reichart and A.Rappoport. 2007. An Ensemble 
Method for Selection of High Quality Parses. In 
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics. pp. 408-
415. 
K.Sagae and J.Tsujii. 2007. Dependency Parsing and 
Domain Adaptation with LR Models and Parser 
Ensembles. In Proceedings of the shared task at 
the Conference on Computational Natural Lan-
guage Learning 2007. pp. 1044-1050. 
Q.Wang, D.Schuurmans, and D.Lin. 2005. Strictly 
Lexical Dependency Parsing. In Proceedings of the 
9th International Workshop on Parsing Technolo-
gies. pp. 152-159.  
A.Wu. 2003. Learning Verb-Noun Relations to Im-
prove Parsing. In Proceedings of the 2nd SIGHAN 
Workshop on Chinese Language Processing. pp. 
119-124. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a 
Large-Scale Annotated Chinese Corpus. In Pro-
ceedings of the 18th International Conference on 
Computational Linguistics. 
N.Xue and M.Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing. 
N.Xue and M.Palmer. 2005. Automatic Semantic 
Rule Labeling for Chinese Verbs. In Proceedings 
of the 19th International Joint Conference on Artifi-
cial Intelligence.  
H.Yamada and Y.Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. 
In Proceedings of the 7th International Workshop 
on Parsing Technologies. 
A.Yates, S.Schoenmackers, and O.Etzioni. 2006. De-
tecting Parser Errors Using Web-based Semantic 
Filters. In Proceedings of the 2006 Conference on 
Empirical Methods in Natural Language Process-
ing. pp. 27-34. 
J.You and K.Chen. 2004. Automatic Semantic Role 
Assignment for a Tree Structure. In Proceedings of 
the 3rd SIGHAN Workshop on Chinese Language 
Processing. 
K.Yu, S.Kurohashi, and H.Liu. 2007. A Three-step 
Deterministic Parser for Chinese Dependency Pars-
ing. In Proceedings of the Human Language Tech-
nologies: the Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics 2007. pp. 201-204. 
 
1056
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 306?314, Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Coordination Disambiguation
in a Fully-lexicalized Japanese Parser
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
This paper describes a probabilistic model
for coordination disambiguation integrated
into syntactic and case structure analy-
sis. Our model probabilistically assesses
the parallelism of a candidate coordinate
structure using syntactic/semantic similari-
ties and cooccurrence statistics. We inte-
grate these probabilities into the framework
of fully-lexicalized parsing based on large-
scale case frames. This approach simulta-
neously addresses two tasks of coordination
disambiguation: the detection of coordinate
conjunctions and the scope disambiguation
of coordinate structures. Experimental re-
sults on web sentences indicate the effective-
ness of our approach.
1 Introduction
Coordinate structures are a potential source of syn-
tactic ambiguity in natural language. Since their in-
terpretation directly affects the meaning of the text,
their disambiguation is important for natural lan-
guage understanding.
Coordination disambiguation consists of the fol-
lowing two tasks:
? the detection of coordinate conjunctions,
? and finding the scope of coordinate structures.
In English, for example, coordinate structures are
triggered by coordinate conjunctions, such as and
and or. In a coordinate structure that consists of
more than two conjuncts, commas, which have var-
ious usages, also function like coordinate conjunc-
tions. Recognizing true coordinate conjunctions
from such possible coordinate conjunctions is a task
of coordination disambiguation (Kurohashi, 1995).
The other is the task of identifying the range of co-
ordinate phrases or clauses.
Previous work on coordination disambiguation
has focused on the task of addressing the scope am-
biguity (e.g., (Agarwal and Boggess, 1992; Gold-
berg, 1999; Resnik, 1999; Chantree et al, 2005)).
Kurohashi and Nagao proposed a similarity-based
method to resolve both of the two tasks for Japanese
(Kurohashi and Nagao, 1994). Their method, how-
ever, heuristically detects coordinate conjunctions
by considering only similarities between possible
conjuncts, and thus cannot disambiguate the follow-
ing cases1:
(1) a. kanojo-to
she-cmi
gakkou-ni
school-acc
itta
went
(? went to school with her)
b. kanojo-to
she-cnj
watashi-ga
I-nom
goukaku-shita
passed an exam
(she and I passed an exam)
In sentence (1a), postposition ?to? is used as a comi-
tative case marker, but in sentence (1b), postposition
?to? is used as a coordinate conjunction.
To resolve this ambiguity, predicative case frames
are required. Case frames describe what kinds of
1In this paper, we use the following abbreviations:
nom (nominative), acc (accusative), abl (ablative), cmi (comi-
tative), cnj (conjunction) and TM (topic marker).
306
Table 1: Case frame examples (Examples are writ-
ten in English. Numbers following each example
represent its frequency.).
CS Examples
ga I:18, person:15, craftsman:10, ? ? ?
yaku (1) wo bread:2484, meat:1521, cake:1283, ? ? ?
(broil) de oven:1630, frying pan:1311, ? ? ?
yaku (2) ga teacher:3, government:3, person:3, ? ? ?
(have wo fingers:2950
difficulty) ni attack:18, action:15, son:15, ? ? ?
ga maker:1, distributor:1
yaku (3) wo data:178, file:107, copy:9, ? ? ?
(burn) ni R:1583, CD:664, CDR:3, ? ? ?
...
...
...
ga dolphin:142, student:50, fish:28, ? ? ?
oyogu (1) wo sea:1188, underwater:281, ? ? ?
(swim) de crawl:86, breaststroke:49, stroke:24, ? ? ?
...
...
...
ga I:4, man:4, person:4, ? ? ?
migaku (1) wo tooth:5959, molar:27, foretooth:12
(brush) de brush:38, salt:13, powder:12, ? ? ?
...
...
...
nouns are related to each predicate. For example, a
case frame of ?iku? (go) has a ?to? case slot filled
with the examples such as ?kanojo? (she) or human.
On the other hand, ?goukaku-suru? (pass an exam)
does not have a ?to? case slot but does have a ?ga?
case slot filled with ?kanojo? (she) and ?watashi?
(I). These case frames provide the information for
disambiguating the postpositions ?to? in sentences
(1a) and (1b): (1a) is not coordinate and (1b) is co-
ordinate.
This paper proposes a method for integrating co-
ordination disambiguation into probabilistic syntac-
tic and case structure analysis. This method simul-
taneously addresses the two tasks of coordination
disambiguation by utilizing syntactic/semantic par-
allelism in possible coordinate structures and lexi-
cal preferences in large-scale case frames. We use
the case frames that were automatically constructed
from the web (Table 1). In addition, cooccurrence
statistics of coordinate conjuncts are incorporated
into this model.
2 Related Work
Previous work on coordination disambiguation has
focused mainly on finding the scope of coordinate
structures.
Agarwal and Boggess proposed a method for
identifying coordinate conjuncts (Agarwal and
Boggess, 1992). Their method simply matches parts
of speech and hand-crafted semantic tags of the
head words of the coordinate conjuncts. They tested
their method using the Merck Veterinary Manual
and found their method had an accuracy of 81.6%.
Resnik described a similarity-based approach for
coordination disambiguation of nominal compounds
(Resnik, 1999). He proposed a similarity measure
based on the notion of shared information content.
He conducted several experiments using the Penn
Treebank and reported an F-measure of approxi-
mately 70%.
Goldberg applied a cooccurrence-based proba-
bilistic model to determine the attachments of am-
biguous coordinate phrases with the form ?n1 p n2
cc n3? (Goldberg, 1999). She collected approxi-
mately 120K unambiguous pairs of two coordinate
words from a raw newspaper corpus for a one-year
period and estimated parameters from these statis-
tics. Her method achieved an accuracy of 72% using
the Penn Treebank.
Chantree et al presented a binary classifier for co-
ordination ambiguity (Chantree et al, 2005). Their
model is based on word distribution information
obtained from the British National Corpus. They
achieved an F-measure (? = 0.25) of 47.4% using
their own test set.
The previously described methods focused on co-
ordination disambiguation. Some research has been
undertaken that integrated coordination disambigua-
tion into parsing.
Kurohashi and Nagao proposed a Japanese pars-
ing method that included coordinate structure detec-
tion (Kurohashi and Nagao, 1994). Their method
first detects coordinate structures in a sentence, and
then heuristically determines the dependency struc-
ture of the sentence under the constraints of the de-
tected coordinate structures. Their method correctly
analyzed 97 Japanese sentences out of 150.
Charniak and Johnson used some features of syn-
tactic parallelism in coordinate structures for their
MaxEnt reranking parser (Charniak and Johnson,
2005). The reranker achieved an F-measure of
91.0%, which is higher than that of their genera-
tive parser (89.7%). However, they used a numer-
ous number of features, and the contribution of the
307
Table 2: Expressions that indicate coordinate struc-
tures.
(a) coordinate noun phrase:
,(comma) to ya toka katsu oyobi ka aruiwa ...
(b) coordinate predicative clause:
-shi ga oyobi ka aruiwa matawa ...
(c) incomplete coordinate structure:
,(comma) oyobi narabini aruiwa ...
parallelism features is unknown.
Dubey et al proposed an unlexicalized PCFG
parser that modified PCFG probabilities to condi-
tion the existence of syntactic parallelism (Dubey
et al, 2006). They obtained an F-measure increase
of 0.4% over their baseline parser (73.0%). Experi-
ments with a lexicalized parser were not conducted
in their work.
A number of machine learning-based approaches
to Japanese parsing have been developed. Among
them, the best parsers are the SVM-based depen-
dency analyzers (Kudo and Matsumoto, 2002; Sas-
sano, 2004). In particular, Sassano added some fea-
tures to improve his parser by enabling it to detect
coordinate structures (Sassano, 2004). However, the
added features did not contribute to improving the
parsing accuracy. This failure can be attributed to
the inability to consider global parallelism.
3 Coordination Ambiguity in Japanese
In Japanese, the bunsetsu is a basic unit of depen-
dency that consists of one or more content words and
the following zero or more function words. A bun-
setsu corresponds to a base phrase in English and
?eojeol? in Korean.
Coordinate structures in Japanese are classified
into three types. The first type is the coordinate noun
phrase.
(2) nagai
long
enpitsu-to
pencil-cnj
keshigomu-wo
eraser-acc
katta
bought
(bought a long pencil and an eraser)
We can find these phrases by referring to the words
listed in Table 2-a.
The second type is the coordinate predicative
clause, in which two or more predicates form a co-
ordinate structure.
bn
An: Partial matrix
A = (a(i, j))
Coordination key bunsetsu
a(n, m)
a(pm-n, n+1)
a path
Similarity betweenbn and bm
Figure 1: Method using triangular matrix.
(3) kanojo-to
she-cmi
kekkon-shi
married
ie-wo
house-acc
katta
bought
(married her and bought a house)
We can find these clauses by referring to the words
and ending forms listed in Table 2-b.
The third type is the incomplete coordinate struc-
ture, in which some parts of coordinate predicative
clauses are present.
(4) Tom-wa
Tom-TM
inu-wo,
dog-acc
Jim-wa
Jim-TM
neko-wo
cat-acc
kau
buys
(Tom (buys) a dog, and Jim buys a cat)
We can find these structures by referring to the
words listed in Table 2-c and also the correspon-
dence of case-marking postpositions.
For all of these types, we can detect the possibility
of a coordinate structure by looking for a coordina-
tion key bunsetsu that accompanies one of the words
listed in Table 2 (in total, we have 52 coordination
expressions). That is to say, the left and right sides of
a coordination key bunsetsu constitute possible pre-
and post-conjuncts, and the key bunsetsu is located
at the end of the pre-conjunct. The size of the con-
juncts corresponds to the scope of the coordination.
4 Calculating Similarity between Possible
Coordinate Conjuncts
We assess the parallelism of potential coordinate
structures in a probabilistic parsing model. In this
308
puroguramingu gengo-wa 2 2 0 2 2 2 0 0 2 0 (prog. language)
mondai kaiketsu-no 2 0 2 4 2 0 0 2 0 (problem solution)
arugorizumu-wo 0 2 2 4 0 0 2 0 (algorithm)hyogen dekiru 0 0 0 2 4 0 2 (can express)kijutsuryoku-to 2 2 0 0 2 0 (descriptive power)keisanki-no 2 0 0 2 0 (computer)kinou-wo 0 0 2 0 (function)jubun-ni 2 0 2 (sufficiently)kudou dekiru 0 2 (can drive)
wakugumi-ga 0 (framework)hitsuyou-dearu. (require)
(Programming language requires descriptive power to express an algorithm for 
solving problems and a framework to sufficiently drive functions of a computer.)
post-conjunct
pre-conjunct
Figure 2: Example of calculating path scores.
section, we describe a method for calculating simi-
larities between potential coordinate conjuncts.
To measure the similarity between potential pre-
and post-conjuncts, a lot of work on the coordi-
nation disambiguation used the similarity between
conjoined heads. However, not only the conjoined
heads but also other components in conjuncts have
some similarity and furthermore structural paral-
lelism. Therefore, we use a method to calculate the
similarity between two whole coordinate conjuncts
(Kurohashi and Nagao, 1994). The remainder of this
section contains a brief description of this method.
To calculate similarity between two series of bun-
setsus, a triangular matrix, A, is used (illustrated in
Figure 1).
A = (a(i, j)) (0 ? i ? l; i ? j ? l) (1)
where l is the number of bunsetsus in a sentence,
diagonal element a(i, j) is the i-th bunsetsu, and el-
ement a(i, j) (i < j) is the similarity value between
bunsetsus bi and bj . A similarity value between
two bunsetsus is calculated on the basis of POS
matching, exact word matching, and their semantic
closeness in a thesaurus tree (Kurohashi and Nagao,
1994). We use the Bunruigoihyo thesaurus, which
contains 96,000 Japanese words (The National In-
stitute for Japanese Language, 2004).
To detect a coordinate structure involving a key
bunsetsu, bn, we consider only a partial matrix (de-
noted An), that is, the upper right part of bn (Figure
1).
An = (a(i, j)) (0 ? i ? n;n + 1 ? j ? l) (2)
To specify correspondences between bunsetsus in
potential pre- and post-conjuncts, a path is defined
as follows:
path ::= (a(p1,m), a(p2,m? 1), . . . ,
a(pm?n, n + 1)) (3)
where n+1 ? m ? l, a(p1,m) 6= 0, p1 = n, pi ?
pi+1, (1 ? i ? m? n? 1).
That is, a path represents a series of elements from
a non-zero element in the lowest row in An to an
element in the leftmost column in An. The path has
an only element in each column and extends toward
the upper left. The series of bunsetsus on the left side
of the path and the series under the path are potential
conjuncts for key bn. Figure 2 shows an example of
a path.
A path score is defined based on the following cri-
teria:
? the sum of each element?s points on the path
? penalty points when the path extends non-
diagonally (which causes conjuncts of unbal-
anced lengths)
? bonus points on expressions signaling the be-
ginning or ending of a coordinate structure,
such as ?kaku? (each) and nado? (and so on)
? the total score of the above criteria is divided
by the square root of the number of bunsetsus
covered by the path for normalization
The score of each path is calculated using a dy-
namic programming method. We consider each path
as a candidate of pre- and post-conjuncts.
309
5 Integrated Probabilistic Model for
Syntactic, Coordinate and Case
Structure Analysis
This section describes a method of integrating coor-
dination disambiguation into a probabilistic parsing
model. The integrated model is based on a fully-
lexicalized probabilistic model for Japanese syntac-
tic and case structure analysis (Kawahara and Kuro-
hashi, 2006b).
5.1 Outline of the Model
This model gives a probability to each possible de-
pendency structure, T , and case structure, L, of the
input sentence, S, and outputs the syntactic, coordi-
nate and case structure that have the highest proba-
bility. That is to say, the model selects the syntactic
structure, T best, and the case structure, Lbest, that
maximize the probability, P (T,L|S):
(T best, Lbest) = argmax (T,L)P (T,L|S)
= argmax (T,L)
P (T,L, S)
P (S)
= argmax (T,L)P (T,L, S) (4)
The last equation is derived because P (S) is con-
stant.
The model considers a clause as a generation unit
and generates the input sentence from the end of the
sentence in turn. The probability P (T,L, S) is de-
fined as the product of probabilities for generating
clause Ci as follows:
P (T,L, S) =
?
i=1..nP (Ci, relihi |Chi) (5)
where n is the number of clauses in S, Chi is Ci?s
modifying clause, and relihi is the dependency re-
lation between Ci and Chi . The main clause, Cn,
at the end of a sentence does not have a modify-
ing head, but a virtual clause Chn = EOS (End Of
Sentence) is inserted. Dependency relation relihi is
first classified into two types C (coordinate) and D
(normal dependency), and C is further divided into
five classes according to the binned similarity (path
score) of conjuncts. Therefore, relihi can be one of
the following six classes.
relihi = {D,C0, C1, C2, C3, C4} (6)
For instance, C0 represents a coordinate relation
with a similarity of less than 1, and C4 represents
a coordinate relation with a similarity of 4 or more.
bentou-wa
tabete-te
kaet-ta(go home)
bentou-wa
tabete-te
kaet-ta(go home) EOSEOS)|,( EOSDtakaetP ? )|,( EOSDtakaetwabentouP ??)|,( takaetDtetabewabentouP ??? )|,( takaetwabentouDtetabeP ???
(eat)
(lunchbox)
(eat)
(lunchbox)
)|,( EOSDtakaetP ? )|,( EOSDtakaetwabentouP ??)|0,( takaetCtetabewabentouP ??? )|0,( takaetwabentouCtetabeP ???
(1) (3)
(4)(2)
Dependency structure Dependency structure21,TT 43 ,TT
DT :1 0:2 CT DT :3 0:4 CT
Figure 3: Example of probability calculation.
For example, consider the sentence shown in Fig-
ure 3. There are four possible dependency structures
in this figure, and the product of the probabilities
for each structure indicated below the tree is calcu-
lated. Finally, the model chooses the structure with
the highest probability (in this case T 1 is chosen).
Clause Ci is decomposed into its clause type,
f i, (including the predicate?s inflection and function
words) and its remaining content part Ci?. Clause
Chi is also decomposed into its content part, Chi ?,
and its clause type, fhi .
P (Ci, relihi |Chi) = P (Ci
?, f i, relihi |Chi
?, fhi)
= P (Ci?, relihi |f i, Chi
?, fhi)? P (f i|Chi
?, fhi)
? P (Ci?, relihi |f i, Chi
?)? P (f i|fhi) (7)
Equation (7) is derived because the content part, Ci?,
is usually independent of its modifying head type,
fhi , and in most cases, the type, f i, is independent
of the content part of its modifying head, Chi .
We call P (Ci?, relihi |f i, Chi ?) generative prob-
ability of a case and coordinate structure, and
P (f i|fhi) generative probability of a clause type.
The latter is the probability of generating func-
tion words including topic markers and punctuation
marks, and is estimated using a syntactically an-
notated corpus in the same way as (Kawahara and
Kurohashi, 2006b).
The generative probability of a case and coordi-
nate structure can be rewritten as follows:
P (Ci?, relihi |f i, Chi
?)
= P (Ci?|relihi , f i, Chi
?)? P (relihi |f i, Chi
?)
? P (Ci?|relihi , f i, Chi
?)? P (relihi |f i) (8)
310
Equation (8) is derived because dependency rela-
tions (coordinate or not) heavily depend on mod-
ifier?s types including coordination keys. We call
P (Ci?|relihi , f i, Chi ?) generative probability of a
case structure, and P (relihi |f i) generative proba-
bility of a coordinate structure. The following two
subsections describe these probabilities.
5.2 Generative Probability of Coordinate
Structure
The most important feature to decide whether two
clauses are coordinate is coordination keys. There-
fore, we consider a coordination key, ki, as clause
type f i. The generative probability of a coordinate
structure, P (relihi |f i), is defined as follows:
P (relihi |f i) = P (relihi |ki) (9)
We classified coordination keys into 52 classes ac-
cording to the classification proposed by (Kurohashi
and Nagao, 1994). If type f i does not contain a co-
ordination key, the relation is always D (normal de-
pendency), that is P (relihi |f i) = P (D|?) = 1.
The generative probability of a coordinate struc-
ture was estimated from a syntactically annotated
corpus using maximum likelihood. We used the
Kyoto Text Corpus (Kurohashi and Nagao, 1998),
which consists of 40K Japanese newspaper sen-
tences.
5.3 Generative Probability of Case Structure
We consider that a case structure consists of a pred-
icate, vi, a case frame, CF l, and a case assignment,
CAk. Case assignment CAk represents correspon-
dences between the input case components and the
case slots shown in Figure 4. Thus, the generative
probability of a case structure is decomposed as fol-
lows:
P (Ci?|relihi , f i, Chi
?)
= P (vi, CF l, CAk|relihi , f i, Chi
?)
= P (vi|relihi , f i, Chi
?)
? P (CF l|relihi , f i, Chi
?, vi)
? P (CAk|relihi , f i, Chi
?, vi, CF l)
? P (vi|relihi , f i, whi)
? P (CF l|vi)
? P (CAk|CF l, f i) (10)
bentou-wa
tabete
(lunchbox)
(eat)
?
lunchbox, bread, ?wo
man, student, ?ga
taberu1 (eat)
Case Frame CF
l
Case 
Assignment
CA
k
(no correspondence)
Dependency Structure of S
Figure 4: Example of case assignment.
The above approximation is given because it is nat-
ural to consider that the predicate vi depends on its
modifying head whi instead of the whole modifying
clause, that the case frame CF l only depends on the
predicate vi, and that the case assignment CAk de-
pends on the case frame CF l and the clause type f i.
The generative probabilities of case frames and
case assignments are estimated from case frames
themselves in the same way as (Kawahara and Kuro-
hashi, 2006b). The remainder of this section de-
scribes the generative probability of a predicate,
P (vi|relihi , f i, whi).
The generative probability of a predicate cap-
tures cooccurrences of coordinate or non-coordinate
phrases. This kind of information is not handled
in case frames, which aggregate only predicate-
argument relations.
The generative probability of a predicate mainly
depends on a coordination key in the clause type, f i,
as well as the generative probability of a coordinate
structure. We define this probability as follows:
P (vi|relihi , f i, whi) = P (vi|relihi , ki, whi)
If Ci? is a nominal clause and consists of a noun
ni, we consider the following probability in stead of
equation (10):
Pn(Ci?|relihi , f i, Chi
?) ? P (ni|relihi , f i, whi)
This is because a noun does not have a case frame
and any case components in the current framework.
To estimate these probabilities, we first applied a
conventional parsing system with coordination dis-
ambiguation to a huge corpus, and collected coor-
dinate bunsetsus from the parses. We used KNP2
(Kurohashi and Nagao, 1994) as the parser and a
web corpus consisting of 470M Japanese sentences
(Kawahara and Kurohashi, 2006a). The generative
probability of a predicate was estimated from the
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
311
collected coordinate bunsetsus using maximum like-
lihood.
5.4 Practical Issue
The proposed model considers all the possible de-
pendency structures including coordination ambigu-
ities. To reduce this high computational cost, we in-
troduced the CKY framework to the search.
Each parameter in the model is smoothed by using
several back-off levels in the same way as (Collins,
1999). Smoothing parameters are optimized using a
development corpus.
6 Experiments
We evaluated the coordinate structures and depen-
dency structures that were outputted by our model.
The case frames used in this paper were automati-
cally constructed from 470M Japanese sentences ob-
tained from the web. Some examples of the case
frames are listed in Table 1 (Kawahara and Kuro-
hashi, 2006a).
In this work, the parameters related to unlexical
types are calculated from a small tagged corpus of
newspaper articles, and lexical parameters are ob-
tained from a huge web corpus. To evaluate the ef-
fectiveness of our fully-lexicalized model, our ex-
periments are conducted using web sentences. As
the test corpus, we prepared 759 web sentences 3.
The web sentences were manually annotated using
the same criteria as the Kyoto Text Corpus. We also
used the Kyoto Text Corpus as a development corpus
to optimize the smoothing parameters. The system
input was automatically tagged using the JUMAN
morphological analyzer 4.
We used two baseline systems for comparative
purposes: the rule-based dependency parser, KNP
(Kurohashi and Nagao, 1994), and the probabilis-
tic model of syntactic and case structure analysis
(Kawahara and Kurohashi, 2006b), in which coor-
dination disambiguation is the same as that of KNP.
6.1 Evaluation of Detection of Coordinate
Structures
First, we evaluated detecting coordinate structures,
namely whether a coordination key bunsetsu triggers
3The test set was not used to construct case frames and esti-
mate probabilities.
4http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
Table 3: Experimental results of detection of coor-
dinate structures.
baseline proposed
precision 366/460 (79.6%) 361/435 (83.0%)
recall 366/447 (81.9%) 361/447 (80.8%)
F-measure ? (80.7%) ? (81.9%)
a coordinate structure. Table 3 lists the experimen-
tal results. The F-measure of our method is slightly
higher than that of the baseline method (KNP). In
particular, our method achieved good precision.
6.2 Evaluation of Dependency Parsing
Secondly, we evaluated the dependency structures
analyzed by the proposed model. Evaluating the
scope ambiguity of coordinate structures is sub-
sumed within this dependency evaluation. The de-
pendency structures obtained were evaluated with
regard to dependency accuracy ? the proportion of
correct dependencies out of all dependencies except
for the last dependency in the sentence end 5. Ta-
ble 4 lists the dependency accuracy. In this table,
?syn? represents the rule-based dependency parser,
KNP, ?syn+case? represents the probabilistic parser
of syntactic and case structure (Kawahara and Kuro-
hashi, 2006b), and ?syn+case+coord? represents our
proposed model. The proposed model significantly
outperformed both of the baseline systems (McNe-
mar?s test; p < 0.01).
In the table, the dependency accuracies are clas-
sified into four types on the basis of the bunsetsu
classes (PB: predicate bunsetsu and NB: noun bun-
setsu) of a dependent and its head. ?syn+case?
outperformed ?syn?. In particular, the accuracy
of predicate-argument relations (?NB?PB?) was
improved, but the accuracies of ?NB?NB? and
?PB?PB? decreased. ?syn+case+coord? outper-
formed the two baselines for all of the types. Not
only the accuracy of predicate-argument relations
(?NB?PB?) but also the accuracies of coordinate
noun/predicate bunsetsus (related to ?NB?NB? and
?PB?PB?) were improved. These improvements
are conduced by the integration of coordination dis-
ambiguation and syntactic/case structure analysis.
5Since Japanese is head-final, the second last bunsetsu un-
ambiguously depends on the last bunsetsu, and the last bunsetsu
has no dependency.
312
Table 4: Experimental results of dependency parsing.
syn syn+case syn+case+coord
all 3,833/4,436 (86.4%) 3,852/4,436 (86.8%) 3,893/4,436 (87.8%)
NB?PB 1,637/1,926 (85.0%) 1,664/1,926 (86.4%) 1,684/1,926 (87.4%)
NB?NB 1,032/1,136 (90.8%) 1,029/1,136 (90.6%) 1,037/1,136 (91.3%)
PB?PB 654/817 (80.0%) 647/817 (79.2%) 659/817 (80.7%)
PB?NB 510/557 (91.6%) 512/557 (91.9%) 513/557 (92.1%)
To compare our results with a state-of-the-art dis-
criminative dependency parser, we input the same
test corpus into an SVM-based Japanese dependency
parser, CaboCha6(Kudo and Matsumoto, 2002).
Its dependency accuracy was 86.3% (3,829/4,436),
which is equivalent to that of ?syn? (KNP). This low
accuracy is attributed to the out-of-domain training
corpus. That is, the parser is trained on a newspa-
per corpus, whereas the test corpus is obtained from
the web, because of the non-availability of a tagged
web corpus that is large enough to train a supervised
parser.
6.3 Discussion
Figure 5 shows some analysis results, where the
dotted lines represent the analysis by the baseline,
?syn+case?, and the solid lines represent the analysis
by the proposed method, ?syn+case+coord?. These
sentences are incorrectly analyzed by the baseline
but correctly analyzed by the proposed method. For
instance, in sentence (1), the noun phrase coordina-
tion of ?apurikeesyon? (application) and ?doraiba?
(driver) can be correctly analyzed. This is because
the case frame of ?insutooru-sareru? (installed) is
likely to generate ?doraiba?, and ?apurikeesyon?
and ?doraiba? are likely to be coordinated.
One of the causes of errors in dependency parsing
is the mismatch between analysis results and anno-
tation criteria. As per the annotation criteria, each
bunsetsu has only one modifying head. Therefore, in
some cases, even if analysis results are semantically
correct, they are judged as incorrect from the view-
point of the annotation. For example, in sentence
(4) in Figure 6, the baseline method, ?syn?, correctly
recognized the head of ?iin-wa? (commissioner-TM)
as ?hirakimasu? (open). However, the proposed
method incorrectly judged it as ?oujite-imasuga?
(offer). Both analysis results can be considered to
be semantically correct, but from the viewpoint of
6http://chasen.org/?taku/software/cabocha/
our annotation criteria, the latter is not a syntactic
relation (i.e., incorrect), but an ellipsis relation. This
kind of error is caused by the strong lexical prefer-
ence considered in our method.
To address this problem, it is necessary to simul-
taneously evaluate not only syntactic relations but
also indirect relations, such as ellipses and anaphora.
This kind of mismatch also occurred for the detec-
tion of coordinate structures.
Another errors were caused by an inherent char-
acteristic of generative models. Generative models
have some advantages, such as their application to
language models. However, it is difficult to incor-
porate various features that seem to be useful for
addressing syntactic and coordinate ambiguity. We
plan to apply discriminative reranking to the n-best
parses produced by our generative model in the same
way as (Charniak and Johnson, 2005).
7 Conclusion
This paper has described an integrated probabilistic
model for coordination disambiguation and syntac-
tic/case structure analysis. This model takes advan-
tage of lexical preference of a huge raw corpus and
large-scale case frames and performs coordination
disambiguation and syntactic/case analysis simulta-
neously. The experiments indicated the effective-
ness of our model. Our future work involves incor-
porating ellipsis resolution to develop an integrated
model for syntactic, case, and ellipsis analysis.
Acknowledgment
This research is partially supported by special coor-
dination funds for promoting science and technol-
ogy.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Proceed-
ings of ACL1992, pages 15?21.
313
??
(1) insutooru-sareteiru apurikeesyon-oyobi doraiba-tono kyougou-niyori dousa-shinai baai-ga arimasu.
installed application driver conflict not work case-nom exist
(due to the conflict between installed application and driver, there is a case that (it) does not work.)
? ?
(2) ... kuroji-wa 41oku-doru-to, zennen-yori 10oku-doru gensyou-shita.
surplus-TM 4.1 billion dollars preceding year-abl 1 billion dollars reduced
(... surplus was 4.1 billion dollars and was reduced by 1 billion dollars from the preceding year.)
??
(3) ... gurupu-wa sugu ugokidasu-node wakaru-nodaga, ugokidasa-nai gurupu-mo aru.
group-TM soon start to work see not start to work group also be
(... can see the groups that start to work soon, but there are groups that do not start to work.)
Figure 5: Examples of correct analysis results. The dotted lines represent the analysis by the baseline,
?syn+case?, and the solid lines represent the analysis by the proposed method, ?syn+case+coord?.
??
(4) iin-wa, jitaku-de minasan-karano gosoudan-ni oujite-imasuga, ... soudansyo-wo hirakimasu
commissioner-TM at home all of you consultation-acc offer window open
(the commissioner offers consultation to all of you at home, but opens a window ...)
Figure 6: An example of incorrect analysis results caused by the mismatch between analysis results and
annotation criteria.
Francis Chantree, Adam Kilgarriff, Anne de Roeck, and
Alistair Wills. 2005. Disambiguating coordinations
using word distribution information. In Proceedings
of RANLP2005.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL2005, pages 173?180.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Amit Dubey, Frank Keller, and Patrick Sturt. 2006. In-
tegrating syntactic priming into an incremental prob-
abilistic parser, with an application to psycholinguis-
tic modeling. In Proceedings of COLING-ACL2006,
pages 417?424.
Miriam Goldberg. 1999. An unsupervised model for
statistically determining coordinate phrase attachment.
In Proceedings of ACL1999, pages 610?614.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using
high-performance computing. In Proceedings of
LREC2006.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proceedings of
HLT-NAACL2006, pages 176?183.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL2002, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a Japanese parsed corpus while improving the parsing
system. In Proceedings of LREC1998, pages 719?724.
Sadao Kurohashi. 1995. Analyzing coordinate structures
including punctuation in English. In Proceedings of
IWPT1995, pages 136?147.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of COLING2004,
pages 8?14.
The National Institute for Japanese Language. 2004.
Bunruigoihyo. Dainippon Tosho, (In Japanese).
314
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 992?1001,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Construction of an Idiom Corpus and its Application to Idiom Identification
based on WSD incorporating Idiom-Specific Features
Chikara Hashimoto
Graduate School of Science and Engineering
Yamagata University
Yonezawa, Yamagata, 992-8510, JAPAN
ch@yz.yamagata-u.ac.jp
Daisuke Kawahara
National Institute of Information and
Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
dk@nict.go.jp
Abstract
Some phrases can be interpreted either id-
iomatically (figuratively) or literally in con-
text, and the precise identification of idioms
is indispensable for full-fledged natural lan-
guage processing (NLP). To this end, we have
constructed an idiom corpus for Japanese.
This paper reports on the corpus and the re-
sults of an idiom identification experiment us-
ing the corpus. The corpus targets 146 am-
biguous idioms, and consists of 102,846 sen-
tences, each of which is annotated with a lit-
eral/idiom label. For idiom identification, we
targeted 90 out of the 146 idioms and adopted
a word sense disambiguation (WSD) method
using both common WSD features and idiom-
specific features. The corpus and the experi-
ment are the largest of their kind, as far as we
know. As a result, we found that a standard
supervised WSD method works well for the
idiom identification and achieved an accuracy
of 89.25% and 88.86% with/without idiom-
specific features and that the most effective
idiom-specific feature is the one involving the
adjacency of idiom constituents.
1 Introduction
Some phrases like kick the bucket are ambiguous
with regard to whether they carry literal or idiomatic
meaning in a certain context. This ambiguity needs
to be resolved in the same manner as ambiguous
words that have been dealt with in the WSD liter-
ature. We term the resolution of the literal/idiomatic
ambiguity as idiom identification, hereafter.
Idiom identification is classified into two kinds;
one is for idiom types and the other is for idiom to-
kens. With the former, phrases that can be inter-
preted as idioms are found in text corpora, typically
for compiling idiom dictionaries. On the other hand,
the latter helps identify a phrase in context as a true
idiom or a phrase that should be interpreted literally
(a literal phrase, henceforth). In this paper, we deal
with the latter, i.e., idiom token identification.
Despite the recent enthusiasm for multiword ex-
pressions (MWEs) (Gre?goire et al, 2007; Gre?goire
et al, 2008), the idiom token identification is in an
early phase of its development. Given that many
NLP tasks like machine translation or parsing have
been developed as a result of the availability of lan-
guage resources, idiom token identification should
also be developed when adequate idiom resources
are provided. To this end, we have constructed a
Japanese idiom corpus. We have also conducted
an idiom identification experiment using the corpus
that we hope will be a good reference point for fu-
ture studies on the task. We drew on a standard
WSD framework with machine learning exploiting
both features commonly used in the WSD studies
and idiom-specific features. This paper reports in
detail the corpus and the result of the experiment;
herein, it must be noted that to the best of our knowl-
edge, the corpus and the experiment are the largest
ever of their kind.
We only deal with the ambiguity between lit-
eral and idiomatic interpretations. However, some
phrases have two or more idiomatic meanings with-
out context. For example, a Japanese idiom te-o
dasu (hand-ACC stretch)1 can be interpreted as ei-
1ACC is the accusative case marker. Likewise we use the
following notation in this paper; NOM for the nominative case
992
ther ?punch,? ?steal? or ?make moves on.? This kind
of ambiguity should be placed on the agenda.
We do not tackle the problem of what constitutes
the notion of ?idiom.? We simply regard phrases
listed in Sato (2007) as idioms.
The reminder of this paper is organized as fol-
lows. In ?2 we present related works. ?3 shows the
target idioms. After the idiom corpus is described
in ?4, we detail our idiom identification method and
experiment in ?5. Finally ?6 concludes the paper.
2 Related Work
There have only been a few works on the con-
struction of an idiom corpus. In this regard, Birke
and Sarkar (2006) and Cook et al (2008) are no-
table exceptions. Birke and Sarkar (2006) auto-
matically constructed a corpus of English idiomatic
expressions (words that can be used non-literally).
They targeted 50 expressions and collected about
6,600 examples. They call the corpus TroFi Exam-
ple Base, which is available on the Web.2 Cook
et al (2008) compiled a corpus of English verb-
noun combinations (VNCs) tokens. Their corpus
deals with 53 VNC expressions and consists of about
3,000 example sentences. Like ours, they assigned
each example with a label indicating whether an ex-
pression in the example is used literally or idiomati-
cally. Our corpus can be regarded as the Japanese
idiom counterpart of these works. However, note
that our corpus targets 146 idioms and consists of
as many as 102,846 example sentences. Another ex-
ception is Tsuchiya et al (2006), who manually con-
structed an example database of Japanese compound
functional expressions named MUST. They provide
it on the Web.3 Some of the compound functional
expressions in Japanese are ambiguous like idioms
are.4
marker, DAT for the dative case marker, and GEN for the genitive
case marker. FROM and TO stand for the Japanese counterparts
of from and to. NEG represents a verbal negation morpheme.
2http://www.cs.sfu.ca/?anoop/students/jbirke/
3http://nlp.iit.tsukuba.ac.jp/must/
4For example, (something)-ni-atatte ((something)-DAT-
run.into) means either ?run into (something)? or ?on the occa-
sion of (something).? The former is the literal interpretation and
the latter is the idiomatic interpretation of the compound func-
tional expression.
The SAID dataset5 provides data about the syn-
tactic flexibility of English idioms. It does not con-
cern itself with idiom token identification. How-
ever, as in Hashimoto et al (2006b), Hashimoto et
al. (2006a) and Cook et al (2007) among others, the
syntactic behavior of idioms is an important clue to
idiom token identification.
Previous studies have mostly focused on the id-
iom type identification (Lin, 1999; Krenn and Evert,
2001; Baldwin et al, 2003; Shudo et al, 2004; Fa-
zly and Stevenson, 2006). However, there has been a
growing interest in idiom token identification in re-
cent times (Katz and Giesbrecht, 2006; Hashimoto
et al, 2006b; Hashimoto et al, 2006a; Birke and
Sarkar, 2006; Cook et al, 2007). Katz and Gies-
brecht (2006) compared the word vector of an id-
iom in context and that of the constituent words of
the idiom using LSA in order to determine if the
expression is idiomatic. Hashimoto et al (2006b)
and Hashimoto et al (2006a) (HSU henceforth) fo-
cused their attention on the differences in gram-
matical constraints imposed on idioms and their lit-
eral counterparts such as the possibility of passiviza-
tion, and developed handcrafted rules for Japanese
idiom identification. Although their task is ex-
actly the same as ours and we draw on the gram-
matical knowledge provided by them, the scale of
their experiment is very small, since only 108 sen-
tences were used for idiom identification in their pa-
per. Further, unlike HSU, we employ matured WSD
technologies. Cook et al (2007) (CFS henceforth)
propose an unsupervised method for English on the
basis of the observation that idioms tend to be ex-
pressed in a small number of fixed forms.
These studies used only the characteristics of id-
ioms (or MWEs). On the other hand, we exploit
a WSD method, for which there have been many
studies and matured technologies, in addition to the
characteristics of idioms. Birke and Sarkar (2006)
also used WSD. However, they employed an unsu-
pervised method, while ours is a completely super-
vised one.
Apart from idioms, Uchiyama et al (2005) con-
ducted the token classification of Japanese com-
pound verbs exploiting supervised method.
5
http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T10
993
3 Target Idioms
For this study, we selected 146 idioms through the
following procedure. 1 We extracted basic idioms
from Sato (2007). Sato compiled about 3,600 basic
idioms of Japanese from five books: two dictionaries
for elementary school, two idiom dictionaries, and
one linguistics book on idioms. We extracted those
idioms that were described in more than two of these
five books. The total number of such idioms added
up to 926. 2 From among these idioms, we chose
ambiguous ones.6 As a result, 146 idioms were se-
lected.
As for 2 , sometimes it is not trivial to determine
if an idiom is ambiguous or not. Some idioms are
rarely interpreted literally, while others, in all likeli-
hood, take on the literal meaning. Is it meaningful to
regard them as ambiguous and deal with them in this
study? If not, how does one assuredly distinguish
truly ambiguous idioms from those that are mostly
interpreted either literally or figuratively? This can
only be done if there is an accurate idiom identifica-
tion system.
After all, we asked two native speakers of
Japanese (Group A) to classify idioms into two
classes: 1) truly ambiguous ones and 2) completely
unambiguous or practically unambiguous ones. On
the basis of the classification, one of the authors
made final judgments.
To verify how stable this ambiguity endorsement
was, we asked another two other native speakers
of Japanese (Group B) to perform the same task
and calculated the Kappa statistic between the two
speakers. First, we sampled 101 idioms from the 926
chosen earlier. Then, the two members of Group B
classified the sampled idioms into the two classes.
The Kappa statistic was found to be 0.6576, which
indicates middling stability.
Tables 2 and 3 list some of the target idioms.
4 Idiom Corpus
4.1 Corpus Specification
The corpus is designed for the idiom token iden-
tification task. That is, each example sentence in
the corpus is annotated with a label that indicates
6Some idioms like by and large do not have a literal mean-
ing. They are not dealt with in this paper.
whether the corresponding phrase in the example is
used as an idiom or a literal phrase. We call the for-
mer the positive example and the latter the negative
example. More specifically, the corpus consists of
lines that each represent one example. A line con-
sists of four fields as follows: 1 Label indicates
whether the example is positive or negative. Label i
is used for positive examples and l for negative ones.
2 ID denotes the idiom that is included in the exam-
ple. In this study, each idiom has a unique num-
ber, which is based on Sato (2007). 3 Lemma also
shows the idiom in the example. We assigned each
idiom its canonical (or standard) form on the basis
of Sato (2007). 4 Example is the example itself.
Given below is a sample of a negative example of
goma-o suru (sesame-ACC crush) ?flatter?.
? l 1417 ??? ????? ? ? ?
The third field is the lemma of the idiom. The last
one is the example that says ?crushing sesame in a
mortar...?
Before working on the corpus construction, we
prepared a reference by which human annotators
could consistently distinguish between the literal
and figurative meanings of idioms. To be more pre-
cise, this reference specified literal and idiomatic
meanings for each idiom like dictionaries do. For
example, the entry for goma-o suru in the reference
is as follows.
Idiom: To flatter people.
Literal: To crush sesame.
As for the corpus size, we continued to anno-
tate examples for each idiom, regardless of the pro-
portion of idioms and literal phrases, until the total
number of examples for each idiom reached 1,000.7
In the case of a shortage of original data, we anno-
tated as many examples as possible. The original
data were sourced from the Japanese Web corpus
(Kawahara and Kurohashi, 2006).
4.2 Corpus Construction
We constructed the corpus in the following man-
ner: 1 From the Web corpus, we collected exam-
ple sentences that contained one of our target id-
ioms whichever meaning (positive or negative) they
7For idioms that we sampled for preliminary annotation, we
annotated more than 1,000 examples.
994
 0
 10
 20
 30
 40
 50
 60
 0  500  1000  1500  2000  2500  3000
# 
of
 T
yp
es
# of Examples
Figure 1: Distribution of the number of examples
take on. Concretely speaking, we automatically col-
lected sentences in which constituent words of one
of our targets appeared in a canonical dependency
relationship by using KNP8, a Japanese dependency
parser. 2 We classified the collected examples as
positive and negative. This was done by human an-
notators and was based on the reference to distin-
guish the two meanings. For annotation, longer ex-
amples were given higher priority than shorter ex-
amples. Note that we discarded examples that were
collected by mistake due to dependency parsing er-
rors and those that lacked a context that could help
them be interpreted correctly.
This was done by the two members of Group A
and took 230 hours.
4.3 Status of Corpus
The corpus consists of 102,846 examples.9 Figure
1 shows the distribution of the number of examples.
For 68 idioms, we annotated more than 1,000 exam-
ples. However, we annotated less than 100 examples
for 17 idioms because of inadequate original data.
The average number of words in a sentence is 46.
Idiom in Figure 2 shows the distribution of sen-
tence length (the number of words) in the corpus.
Web and News indicate the sentence length in the
Web and a newspaper corpora, respectively. This is
drawn from Kawahara and Kurohashi (2006). As
8http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
9Note that the figures reported here are for the corpus of the
2008-06-25 version and will be slightly changed over time.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
76-8071-7566-7061-6556-6051-5546-5041-4536-4031-3526-3021-2516-2011-156-101-5
S
en
te
nc
e 
R
at
io
# of Words in a Sentence
Web
News
Idiom
Figure 2: Distribution of sentence length
you see, our corpus contains many more long sen-
tences. This is because longer sentences were given
priority for annotation, as stated in ?4.2. Figure 3
shows the longest and shortest examples each for lit-
eral and idiomatic meanings of goma-o suru drawn
from the corpus.
To determine how consistent the positive/negative
annotation is across different human annotators, we
sampled 1,421 examples from the corpus, asked the
two members of Group B to do the same annota-
tion, and calculated the Kappa statistic between the
two. The value was 0.8519, which indicates very
high agreement.
The corpus is available on the Web.10 Currently
we provide the list of the basic Japanese idioms we
are dealing with, the idiom corpus, and the vector
representation data used for the idiom identification
experiment. The corpus is protected under the BSD
license.
5 Idiom Identification Experiment
5.1 Method of Idiom Identification
We adopted a standard WSD method using machine
learning. More specifically, we used SVM (Vap-
nik, 1995) with a quadratic kernel implemented in
TinySVM.11 The features we used are classified into
either those that have been commonly used in WSD
on the lines of Lee and Ng (2002) (LN hereafter),
10http://openmwe.sourceforge.jp/
11http://www.chasen.org/?taku/software/TinySVM/
995
? ????????????????????????????????????????
????????????????????????????????????????
?????????????????????????????????????????
????? ??? ???????????????????????????
????????????????????????????????????
(But I suspect that the show managers of IT ventures will remain sly and audacious, and survive
by flattering manufacturers, bending over themselves to accede to the demands of governmental
agencies, and talking glibly about buzz terms, without intelligence but with vitality, just like
the brokers of prostitutes in the Edo period were, because Gresham?s law of 1562 says that any
circulating currency consisting of both good and bad money quickly becomes dominated by the
bad money.)
? ? ??? ???????
(Just like a pretty official flattering his boss.)
? ?????????????????????????????????????? ?
?? ?????????????????????????????????????
???????????????????????????????
(In order to mash boiled soybeans, it is the best to use a meat chopper, but if you don?t have one,
use the thing to crush sesame, or put them into a plastic bag, cover it with a towel, then mash it
with a glass bottle, which is easier.)
? ??? ????
(Crushing sesame, then adding seasonings to it.)
Figure 3: The longest and shortest examples for both literal and idiomatic meanings of goma-o suru
or those that have been designed for Japanese idiom
identification proposed by HSU.12
? Common WSD Features
f1: POS of three words on the left side of idiom
and three words on the right side
f2: Local collocations
f3: Single words in the surrounding context
f4a: Lemma of the rightmost word among
those words that are the dependents of the
leftmost constituent word of idiom13
f4b: POS of the rightmost word among those
words that are the dependents of the left-
most constituent word of idiom
f5a: Lemma of the word which the rightmost
constituent word of idiom is the dependent
of
12Remember that HSU implemented them in handcrafted
rules. We adapted them to a machine learning framework.
13Note that Japanese is a head final language.
f5b: POS of the word which the rightmost con-
stituent word of idiom is the dependent of
f6: Hypernyms of words in the surrounding
context
f7: Domains of words (Hashimoto and Kuro-
hashi, 2007; Hashimoto and Kurohashi,
2008) in the surrounding context
? Idiom-Specific Features
f8: Adnominal modification flag
f9: Topic case marking flag
f10: Voice alternation flag
f11: Negation flag
f12: Volitional modality flag
f13: Adjacency flag
We used JUMAN,14 a morphological analyzer of
Japanese, and KNP to extract these features.
14http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
996
f2 and f3 are the same as those described in LN.
But f1 is slightly different in that we did not use the
P0 of LN. f4 and f5 roughly correspond to the syn-
tactic relations of LN. We adapted it to Japanese id-
ioms along with some simplifications. In the case of
the example ofmune-o utu (chest-ACC hit) ?impress?
below,15 f4 is the POS and lemma of tyousyu and f5
corresponds to those of uta.16
? tyousyu-no
audience-GEN
mune-o
chest-ACC
utu
hit
utukusi
beautiful
uta
song
?A beautiful song that impresses the audience?
f6 and f7 are available from JUMAN?s output.
For example, the hypernym of tyousyu (audience)
is human and its domain is culture/media.
Those of uta (song) are abstract-thing and
culture/recreation. They are not used in
LN, but they are known to be useful for WSD
(Tanaka et al, 2007; Magnini et al, 2002).
f8 indicates whether a nominal constituent of an
idiom, if any, undergoes adnominal modification. f9
indicates whether one of Japanese topic case mark-
ers is attached to a nominal constituent of an idiom,
if any. f10 is turned on when a passive or causative
suffix is attached to a verbal constituent of an idiom,
if any.17 f11 and f12 are similar to f10. The former
is used for negated forms and the latter for volitional
modality suffixes of a predicate part of an idiom, if
any.18 Volitional modality includes expressions like
order, request, permission, prohibition, and volition.
Finally, f13 indicates whether the constituents of an
idiom is adjacent to each other.
As discussed in HSU, the idiom-specific fea-
tures are effective to distinguish idioms from lit-
eral phrases. For example, the idiom goma-o suru
does not allow adnominal modification, while its lit-
eral counterpart does. Similarly, the idiom mune-o
utu cannot take volitional modality unlike its literal
counterpart.
15The arrows indicate dependency relations.
16Functional words attaching to either the f4 word or the f5
word are ignored. In the example, no (GEN) is ignored.
17Passivization is indicated by the suffix (r)are in Japanese.
But the same suffix is also used for honorification, potentials
and spontaneous potentials. Since it is beyond the current tech-
nology, we gave up distinguishing them.
18Note that f10, f11 and f12 are applied to only those idioms
that can be used as predicates.
5.2 Experimental Condition
In the experiment, we dealt with 90 idioms for which
more than 50 examples for both idiomatic and literal
usages were available.19 We conducted experiments
for each idiom.
The performance measure is the accuracy.
Accuracy =
# of examples correctly identified
# of all example
The baseline system uniformly regards all ex-
amples as either positive or negative depending on
which is more dominant in the idiom corpus. Natu-
rally, this is prepared for each idiom.
Baseline =
max(# of positive, # of negative)
# of all example
The accuracy and the baseline accuracy for each
idiom are calculated in a 10-fold cross validation
style; we split examples of an idiom into 10 pieces
in advance of the experiment.
Also, we calculated the overall accuracy and
baseline accuracy from the individual results. We
summed up all accuracy scores of all the 90 idioms
and then divided it by 90, which is called the macro-
average. We did this for the baseline accuracy, too.
Another performance measure is the relative error
reduction (RER).20
RER =
ER of baseline ? ER of system
ER of baseline
The overall RER is calculated from the overall ac-
curacy and baseline by the above formula.
5.3 Experimental Result
Table 1 shows the overall performance. The first col-
umn is the baseline accuracy (%). The second col-
umn is the accuracy (%) and relative error reduction
(%) of the system without the idiom-specific fea-
tures. The third column is those of the system with
the idiom features. Tables 2 and 3 show the individ-
ual results of the 90 idioms. The first column shows
19Some examples were unavailable due to the feature extrac-
tion failure. Thus, examples used for the experiment are fewer
in number than those included in the corpus.
20ER stands for Error Rate in the formula.
997
Table 2: Individual Results (1/2)
Type Base (Pos ; Neg) w/o I (RER) w/ I (RER)
??? (blue.vein-ACC emerge) ?burst a blood vessel? 83.38 (286 ; 57) 86.32 (17.68) 86.61 (19.45)
??? (sit cross-legged) ?rest on one?s laurels? 62.45 (587 ; 353) 92.66 (80.45) 92.87 (81.02)
?? (leg-NOM attach) ?find a clue to solving a case? 72.21 (184 ; 478) 77.20 (17.96) 79.62 (26.68)
?? (leg-NOM go.out) ?run over the budget? 77.59 (188 ; 651) 92.61 (67.01) 93.08 (69.13)
??? (one?s feet-ACC look.down) ?see someone coming? 57.53 (420 ; 310) 85.89 (66.77) 85.75 (66.45)
?? (leg-ACC wash) ?wash one?s hands of ...? 68.47 (632 ; 291) 92.65 (76.68) 92.65 (76.69)
??? (leg-ACC stretch) ?go a little further? 80.24 (727 ; 179) 95.26 (76.03) 95.38 (76.59)
?? (head-NOM ache) ?harass oneself about ...? 57.87 (158 ; 217) 83.94 (61.89) 83.94 (61.89)
??? (head-ACC fold) ?tear one?s hair out? 87.28 (796 ; 116) 91.35 (31.99) 91.35 (31.99)
??? (head-ACC lift) ?rear its head? 83.14 (804 ; 163) 93.40 (60.83) 93.50 (61.45)
?? (fat-NOM put.on) ?warm up to one?s work? 83.69 (196 ; 1006) 92.94 (56.69) 92.94 (56.69)
?? (oil-ACC sell) ?shoot the breeze? 86.67 (507 ; 78) 92.63 (44.70) 92.63 (44.70)
?? (oil-ACC squeeze) ?rake someone over the coals? 66.83 (69 ; 139) 84.64 (53.71) 86.14 (58.23)
?? (net-ACC spread) ?wait expectantly? 70.10 (366 ; 858) 81.28 (37.41) 80.96 (36.31)
??? (breath-NOM choke.up) ?stifling? 71.61 (681 ; 270) 79.82 (28.91) 79.50 (27.80)
??? (one-FROM ten-TO) ?all without exception? 92.00 (770 ; 67) 93.48 (18.51) 93.48 (18.51)
?? (color-ACC lose) ?turn pale? 73.32 (262 ; 720) 84.23 (40.91) 84.23 (40.91)
??? (arm-NOM go.up) ?develop one?s skill? 57.06 (481 ; 362) 84.47 (63.85) 88.75 (73.80)
?? (tail-ACC pull) ?have a lasting effect? 87.72 (843 ; 118) 93.14 (44.15) 93.35 (45.84)
?? (face-ACC present) ?show up? 84.48 (697 ; 128) 88.60 (26.49) 88.82 (27.93)
??? (shoulder-ACC juxtapose) ?on a par? 89.38 (842 ; 100) 93.20 (35.97) 93.10 (34.97)
??? (corner-NOM remove) ?become mature? 57.45 (370 ; 274) 78.35 (49.13) 78.04 (48.39)
?? (lip-ACC bite) ?bite one?s lip? 70.89 (587 ; 241) 78.40 (25.78) 79.36 (29.10)
?? (mouth-ACC cut) ?break the ice? 51.50 (210 ; 223) 84.83 (68.73) 83.69 (66.36)
???? (mouth-ACC sharpen) ?pout? 86.33 (663 ; 105) 87.61 (9.40) 87.35 (7.47)
??? (neck-NOM turn-NEG) ?up to one?s neck? 66.63 (619 ; 310) 86.41 (59.28) 86.22 (58.71)
?? (neck-ACC cut) ?give the axe? 53.90 (449 ; 384) 89.93 (78.15) 89.80 (77.88)
??? (neck-ACC twist) ?think hard? 93.16 (885 ; 65) 94.11 (13.85) 93.79 (9.23)
??? (thing-DAT depend) ?perhaps? 67.15 (231 ; 113) 96.50 (89.35) 97.35 (91.94)
??? (sesame-ACC crush) ?flatter? 50.29 (87 ; 88) 92.75 (85.42) 90.99 (81.88)
??? (back-ACC train) ?turn one?s back? 66.70 (597 ; 298) 89.06 (67.14) 89.06 (67.14)
?? (blood-NOM flow) ?humane? 50.18 (422 ; 419) 82.41 (64.70) 83.24 (66.37)
?? (midair-DAT float) ?? 58.07 (382 ; 529) 88.03 (71.46) 88.69 (73.03)
?? (dirt-NOM attach) ?be defeated in sumo wrestling? 72.66 (70 ; 186) 79.48 (24.97) 78.76 (22.33)
?? (hand-NOM reach) ?afford? ?reach an age? ?attentive? 80.76 (470 ; 112) 87.66 (35.85) 87.66 (35.85)
?? (hand-NOM there.isn?t) ?have no remedy? 86.94 (799 ; 120) 92.61 (43.38) 92.83 (45.06)
??? (hand-NOM get.away) ?get one?s work done? 53.49 (360 ; 414) 92.37 (83.59) 92.36 (83.57)
?? (hand-DAT ride) ?fall into someone?s trap? 61.05 (372 ; 583) 92.86 (81.68) 93.49 (83.30)
??? (hand-DAT insert) ?obtain? 53.21 (373 ; 328) 93.44 (85.99) 93.59 (86.29)
??? (hand-ACC hang) ?give a lot of care? 70.57 (241 ; 578) 91.19 (70.04) 91.31 (70.46)
?? (hand-ACC cut) ?break away? 57.85 (468 ; 341) 91.08 (78.83) 91.08 (78.83)
?? (hand-ACC take) ?give every possible help (to learn)? 88.89 (91 ; 728) 92.74 (34.67) 92.62 (33.56)
?? (hand-ACC grasp) ?conclude an alliance? 90.51 (73 ; 696) 95.44 (51.93) 95.17 (49.16)
??? (hand-ACC stretch) ?extend one?s business? 89.55 (95 ; 814) 94.01 (42.69) 94.22 (44.72)
??? (hand-ACC open.up) ?extend one?s business? 70.52 (579 ; 242) 89.17 (63.26) 90.15 (66.57)
?? (hand-ACC turn) ?take measures? 68.86 (246 ; 544) 93.04 (77.64) 93.92 (80.49)
?? (mountain.pass-ACC go.over) ?get over the hump? 72.18 (685 ; 264) 89.28 (61.46) 89.49 (62.23)
?? (mud-ACC daub) ?drag someone through mud? 74.38 (543 ; 187) 91.64 (67.38) 91.92 (68.45)
?? (wave-DAT ride) ?catch a wave? 86.23 (783 ; 125) 93.05 (49.55) 92.94 (48.74)
??? (heat-NOM get.cool) ?fever goes down? 89.90 (890 ; 100) 92.02 (21.00) 92.22 (23.00)
??? (heat-ACC raise) ?go ape? 92.52 (903 ; 73) 94.50 (26.45) 94.71 (29.21)
??? (heat-ACC feed.in) ?enthuse? 85.06 (723 ; 127) 90.71 (37.80) 91.76 (44.88)
??? (root-ACC take.down) ?take root? 85.83 (824 ; 136) 93.23 (52.21) 93.23 (52.21)
?? (root-ACC spread) ?take root? 60.00 (564 ; 376) 87.66 (69.15) 87.66 (69.15)
???? (bus-DAT miss) ?miss the boat? 76.97 (199 ; 665) 90.50 (58.74) 92.36 (66.81)
???? (baton-ACC give) ?have someone succeed his position? 65.33 (471 ; 250) 81.70 (47.23) 82.25 (48.81)
??? (nasal.breathing-NOM heavy) ?full of big talk? 52.77 (286 ; 256) 75.33 (47.77) 76.62 (50.50)
??? (nose-NOM high) ?proud? 50.27 (659 ; 652) 81.01 (61.81) 82.30 (64.42)
??? (nose-ACC break) ?humble (someone)? 56.60 (69 ; 90) 69.58 (29.91) 74.92 (42.20)
??? (nose-ACC make.a.sound) ?make light of ...? 55.72 (536 ; 426) 80.79 (56.63) 81.21 (57.57)
???? (belly-ACC cut) ?have a heart-to-heart talk? 95.62 (1265 ; 58) 96.68 (24.16) 96.68 (24.16)
????? (teeth-ACC clench) ?grit one?s teeth? 65.54 (194 ; 102) 71.97 (18.66) 71.63 (17.66)
?? (human-ACC eat) ?look down on someone? 74.95 (727 ; 243) 87.01 (48.15) 87.01 (48.15)
???? (spark-ACC spread) ?fight heatedly? 75.99 (728 ; 230) 89.57 (56.56) 89.68 (57.00)
998
Table 3: Individual Results (2/2)
Type Base (Pos ; Neg) w/o I (RER) w/ I (RER)
??? (painting.brush-ACC add) ?correct (writings or paintings)? 75.80 (213 ; 68) 83.99 (33.84) 84.70 (36.79)
??? (ship-ACC row) ?nod? 50.76 (167 ; 162) 75.82 (50.88) 76.37 (52.01)
???? (bone-NOM break) ?have difficulty? 62.30 (575 ; 348) 94.14 (84.46) 94.14 (84.47)
???? (bone-ACC bury) ?make it one?s final home? 82.82 (757 ; 157) 89.84 (40.85) 90.60 (45.31)
???? (bone-ACC break) ?make efforts? 60.89 (350 ; 545) 92.74 (81.43) 92.96 (82.01)
???? (curtain-NOM open) ?start? 55.64 (533 ; 425) 86.32 (69.17) 86.22 (68.94)
??? (right-FROM left) ?passing through without staying? 73.88 (794 ; 2246) 89.90 (61.34) 89.87 (61.21)
?? (water-AND oil) ?oil and water? 55.66 (1053 ; 839) 83.19 (62.10) 85.84 (68.07)
??? (water-DAT flush) ?forgive and forget? 67.08 (652 ; 320) 85.91 (57.19) 89.40 (67.81)
??? (body-DAT put.on) ?learn? 90.29 (725 ; 78) 96.51 (64.11) 96.39 (62.82)
??? (ear-NOM ache) ?make one?s ears burn? 59.49 (333 ; 489) 88.69 (72.08) 89.54 (74.19)
??? (ear-DAT insert) ?get word of ...? 74.89 (501 ; 168) 89.50 (58.20) 90.38 (61.67)
???? (fruit-ACC bear) ?bear fruit? 89.39 (826 ; 98) 95.79 (60.33) 95.68 (59.31)
??? (chest-NOM ache) ?suffer heartache? 93.59 (876 ; 60) 95.82 (34.78) 95.93 (36.46)
???? (chest-NOM expand) ?feel one?s heart leap? 55.58 (338 ; 423) 94.08 (86.68) 94.48 (87.57)
???? (chest-ACC hit) ?impress? 92.39 (801 ; 66) 96.45 (53.34) 96.68 (56.39)
??? (germ-NOM come.out) ?close to making the top? 56.57 (377 ; 491) 91.33 (80.03) 91.55 (80.55)
?? (eye-NOMthere.isn?t) ?have a passion for ...? 91.81 (829 ; 74) 95.70 (47.47) 95.25 (42.05)
??? (scalpel-ACC insert) ?take drastic measures? 88.96 (741 ; 92) 96.28 (66.30) 96.28 (66.30)
?? (eye-DAT enter) ?catch sight of ...? 84.76 (623 ; 112) 90.22 (35.79) 91.16 (41.97)
??? (eye-ACC cover) ?be in a shambles? 87.24 (725 ; 106) 91.45 (32.99) 92.06 (37.72)
??? (eye-ACC awake) ?snap out of ..? 83.26 (118 ; 587) 87.92 (27.85) 88.64 (32.12)
??? (eye-ACC close) ?turn a blind eye? 70.13 (533 ; 227) 90.26 (67.40) 90.26 (67.40)
???? (eye-ACC thin) ?one?s eyes light up? 53.44 (115 ; 132) 75.20 (46.74) 75.11 (46.54)
???? (finger-ACC suck) ?look enviously? 92.50 (876 ; 71) 95.68 (42.41) 95.58 (41.09)
??? (bow-ACC draw) ?defy? 88.06 (138 ; 1018) 95.51 (62.41) 95.43 (61.68)
Table 1: Overall Result
Base w/o I (RER) w/ I (RER)
72.92 88.86 (58.87) 89.25 (60.30)
the target idioms. The second column shows base-
line accuracy (%) and the numbers of positive and
negative examples for each idiom. The accuracy (%)
and relative error reduction (%) of the system with-
out the idiom-specific features are described in the
third column. The fourth column is those of the sys-
tem with the idiom features. Bold face indicates a
better performance.
All in all, we see relatively high baseline perfor-
mances. Nevertheless, both systems outperformed
the baseline. Especially, the system without the
idiom-specific features has a noticeable lead over the
baseline, showing that WSD technologies are effec-
tive in the idiom identification. Incorporating the id-
iom features into the system improved the overall
performance, which is statistically significant (Mc-
Nemar test, p<0.01). But performances of some id-
ioms slightly degraded by the incorporation of the
idiom features.
Table 4: Overall Results without Using One of the Idiom
Features
Feature Type Acc
All 89.25
?f8 (w/o Adnominal modification flag) 89.24
?f9 (w/o Topic case marking flag) 89.22
?f10 (w/o Voice alternation flag) 89.15
?f11 (w/o Negation flag) 89.17
?f12 (w/o Volitional modality flag) 89.19
?f13 (w/o Adjacency flag) 89.09
Table 4 shows overall results without using one of
the idiom features.21 As you see, the adjacency flag
(f13) contributes to idiom identification accuracy the
most.22 On the other hand, the adnominal modifica-
tion flag (f8) contributes to the task only slightly.23
21The first row shows the result with all idiom features used,
just for ease of reference.
22Note that greater performance drop indicates greater con-
tribution.
23This result is inconsistent with the result obtained in HSU,
where they reported that grammatical constraints involving ad-
nominal modification was most effective. This inconsistency
might be attributed to the differences of datasets being used for
idiom identification experiment. HSU used only 108 sentences
999
Table 5: Results reported in CFS
Accu RER
Baseline 61.9 ?
Unsupervised 72.4 27.6
Supervised 76.2 37.5
Table 5 shows the results reported in CFS. Their
baseline system regards all instances as idioms. The
performance of the supervised one is obtained by the
method of Katz and Giesbrecht (2006). Though we
cannot simply compare this with our results due to
the difference in experimental conditions, this im-
plies that our WSD-based method was equally good
or possibly better than their methods that are tailored
to MWEs.
6 Conclusion
In this paper, we reported on the idiom corpus we
have constructed and the idiom identification exper-
iment using the corpus.
As mentioned in ?4.3, some idioms are short of
examples in the current idiom corpus. We plan to
collect more examples by using different characters.
In the Japanese language, there are basically three
character systems: Hiragana, Katakana, and Chinese
characters. Thus, you can write an idiom in different
characters. For example, mune-o utu (chest-ACC hit)
?impress? can be either???? or??? .
In spite of its imperfection, we are sure that we
can learn a lot about the idiom identification from
the corpus, since, as far as we know, it is the largest-
ever one, and so is the idiom identification experi-
ment reported in ?5.
Also, we showed that a standard supervised WSD
method works well for the idiom identification.
Our system achieved the accuracy of 89.25% and
88.86% with/without idiom-specific features.
Though we dealt with as many as 90 idioms, prac-
tical NLP systems are required to deal with many
more idioms. Toward a scalable idiom identifica-
tion, we have to develop an unsupervised or semi-
supervised method. The unsupervised method of
for the experiment, while 75,011 sentences were used for our
experiment. Also, the dataset of HSU came from newspaper
articles, while our dataset came from the web.
Birke and Sarkar (2006) requires WordNet. Fortu-
nately, the Japanese WordNet is now available (Isa-
hara et al, 2008), thus we can try their method.
Also, CFS propose a language-independent unsu-
pervised method. These could be of help.
At any rate, our idiom corpus will play an im-
portant role in the development of unsupervised or
semi-supervised methods, and the experimental re-
sults obtained in this study will be a good reference
point to evaluate those methods.
Acknowledgments
This work was conducted as a part of the collabora-
tive research project of Kyoto University and NTT
Communication Science Laboratories.
The work was supported from NTT Communica-
tion Science Laboratories and JSPS Grants-in-Aid
for Young Scientists (B) 19700141.
We would like to thank the members of the collab-
orative research group of Kyoto University and NTT
Communication Science Laboratories and Francis
Bond for their stimulating discussion. Our thanks
go as well to Prof. Sato Satoshi, who kindly gave us
the list of basic idioms of Japanese.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions, pages 89?96.
Julia Birke and Anoop Sarkar. 2006. A clustering
approach for the nearly unsupervised recoginition of
nonliteral language. In Proceedings of the 11th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL 2006), pages 329?
336.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the ACL 2007
Workshop on A Broader Perspective on Multiword Ex-
pressions, pages 41?48.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2008. The VNC-Tokens Dataset. In Proceedings of
the LREC Workshop Towards a Shared Task for Multi-
word Expressions (MWE2008), pages 19?22.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the 11th Conference
1000
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2006), pages 337?344.
Nicole Gre?goire, Stefan Evert, and Su Nam Kim, editors.
2007. Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions. Association for
Computational Linguistics, Prague.
Nicole Gre?goire, Stefan Evert, and Brigitte Krenn, edi-
tors. 2008. Proceedings of the LREC Workshop To-
wards a Shared Task for Multiword Expressions. ACL
Special Interest Group on the Lexicon (SIGLEX),
Marrakech.
Chikara Hashimoto and Sadao Kurohashi. 2007. Con-
struction of Domain Dictionary for Fundamental Vo-
cabulary. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?07) Poster, pages 137?140.
Chikara Hashimoto and Sadao Kurohashi. 2008. Blog
Categorization Exploiting Domain Dictionary and Dy-
namically Estimated Domains of Unknown Words. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?08) Short
paper, Poster, pages 69?72.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006a. Detecting Japanese idioms with a linguistically
rich dictionary. Language Resources and Evaluation,
40(3?4):243?252.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006b. Japanese Idiom Recognition: Drawing a Line
between Literal and Idiomatic Meanings. In The Joint
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL 2006)
Poster, pages 353?360, Sydney, July.
Hitoshi Isahara, Francis Bond, Kiyotaka Uchimoto,
Masao Utiyama, and Kyoko Kanzaki. 2008. Devel-
opment of the Japanese WordNet. In The sixth inter-
national conference on Language Resources and Eval-
uation (LREC2008).
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the Workshop, COLING/ACL 2006, Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, pages 12?19, July.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources and
Evaluation (LREC-06), pages 1344?1347.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? a case study on extracting pp-verb
collocations. In Proceedings of the ACL-01 Workshop
on Collocations, pages 39?46.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP
?02: Proceedings of the ACL-02 conference on Em-
pirical methods in natural language processing, pages
41?48.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceeding of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 317?324.
BernardoMagnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The Role of Domain Infor-
mation in Word Sense Disambiguation. Natural Lan-
guage Engineering, special issue on Word Sense Dis-
ambiguation, 8(3):359?373.
Satoshi Sato. 2007. Compilation of a comparative list
of basic Japanese idioms from five sources. In IPSJ
2007-NL-178, pages 1?6. (in Japanese).
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi,
and Kenji Yoshimura. 2004. MWEs as Non-
propositional Content Indicators. In the 2nd ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae
Fujita, and Chikara Hashimoto. 2007. Word Sense
Disambiguation Incorporating Lexical and Structural
Semantic Information. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 477?
485.
Masatoshi Tsuchiya, Takehito Utsuro, Suguru Mat-
suyoshi, Satoshi Sato, and Seiichi Nakagawa. 2006.
Development and analysis of an example database of
Japanese compound functional expressions. Trans-
actions of Information Processing Society of Japan,
47(6):1728?1741. (in Japanese).
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, Special Issue on
Multiword Expressions, 19(4):497?512.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer-Verlag, New York.
1001
Dependency Parsing with Short Dependency Relations in Unlabeled Data
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jp
Abstract
This paper presents an effective dependency
parsing approach of incorporating short de-
pendency information from unlabeled data.
The unlabeled data is automatically parsed
by a deterministic dependency parser, which
can provide relatively high performance for
short dependencies between words. We then
train another parser which uses the informa-
tion on short dependency relations extracted
from the output of the first parser. Our pro-
posed approach achieves an unlabeled at-
tachment score of 86.52, an absolute 1.24%
improvement over the baseline system on
the data set of Chinese Treebank.
1 Introduction
In dependency parsing, we attempt to build the
dependency links between words from a sen-
tence. Given sufficient labeled data, there are sev-
eral supervised learning methods for training high-
performance dependency parsers(Nivre et al, 2007).
However, current statistical dependency parsers pro-
vide worse results if the dependency length be-
comes longer (McDonald and Nivre, 2007). Here
the length of a dependency from word w
i
and word
w
j
is simply equal to |i ? j|. Figure 1 shows the
F
1
score1 provided by a deterministic parser rela-
tive to dependency length on our testing data. From
1precision represents the percentage of predicted arcs of
length d that are correct and recall measures the percentage of
gold standard arcs of length d that are correctly predicted.
F
1
= 2? precision? recall/(precision + recall)
the figure, we find that F
1
score decreases when de-
pendency length increases as (McDonald and Nivre,
2007) found. We also notice that the parser pro-
vides good results for short dependencies (94.57%
for dependency length = 1 and 89.40% for depen-
dency length = 2). In this paper, short dependency
refers to the dependencies whose length is 1 or 2.
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30
F1
Dependency Length
baseline
Figure 1: F-score relative to dependency length
Labeled data is expensive, while unlabeled data
can be obtained easily. In this paper, we present an
approach of incorporating unlabeled data for depen-
dency parsing. First, all the sentences in unlabeled
data are parsed by a dependency parser, which can
provide state-of-the-art performance. We then ex-
tract information on short dependency relations from
the parsed data, because the performance for short
dependencies is relatively higher than others. Fi-
nally, we train another parser by using the informa-
tion as features.
The proposed method can be regarded as a semi-
supervised learning method. Currently, most semi-
88
supervised methods seem to do well with artificially
restricted labeled data, but they are unable to outper-
form the best supervised baseline when more labeled
data is added. In our experiments, we show that our
approach significantly outperforms a state-of-the-art
parser, which is trained on full labeled data.
2 Motivation and previous work
The goal in dependency parsing is to tag dependency
links that show the head-modifier relations between
words. A simple example is in Figure 2, where the
link between a and bird denotes that a is the depen-
dent of the head bird.
I    see    a    beautiful    bird    .
Figure 2: Example dependency graph.
We define that word distance of word w
i
and word
w
j
is equal to |i ? j|. Usually, the two words in a
head-dependent relation in one sentence can be adja-
cent words (word distance = 1) or neighboring words
(word distance = 2) in other sentences. For exam-
ple, ?a? and ?bird? has head-dependent relation in
the sentence at Figure 2. They can also be adjacent
words in the sentence ?I see a bird.?.
Suppose that our task is Chinese dependency
parsing. Here, the string ????JJ(Specialist-
level)/? ?NN(working)/? ?NN(discussion)?
should be tagged as the solution (a) in Figure
3. However, our current parser may choose the
solution (b) in Figure 3 without any additional
information. The point is how to assign the head for
????(Specialist-level)?. Is it ???(working)?
or ???(discussion)??
  
  
  
(b)
(a)
Figure 3: Two solutions for ????(Specialist-
level)/??(working)/??(discussion)?
As Figure 1 suggests, the current dependency
parser is good at tagging the relation between ad-
jacent words. Thus, we expect that dependencies
of adjacent words can provide useful information
for parsing words, whose word distances are longer.
When we search the string ????(Specialist-
level)/??(discussion)? at google.com, many rele-
vant documents can be retrieved. If we have a good
parser, we may assign the relations between the two
words in the retrieved documents as Figure 4 shows.
We can find that ???(discussion)? is the head of
????(Specialist-level)? in many cases.
1)?525	26
///,//?
2)?TSUBAKI: An Open Search Engine Infrastructure for
Developing New Information Access Methodology
Keiji Shinzato?, Tomohide Shibata?, Daisuke Kawahara?,
Chikara Hashimoto?? and Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University
?National Institute of Information and Communications Technology
??Department of Informatics, Yamagata University
{shinzato, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
dk@nict.go.jp ch@yz.yamagata-u.ac.jp
Abstract
As the amount of information created by
human beings is explosively grown in the
last decade, it is getting extremely harder
to obtain necessary information by conven-
tional information access methods. Hence,
creation of drastically new technology is
needed. For developing such new technol-
ogy, search engine infrastructures are re-
quired. Although the existing search engine
APIs can be regarded as such infrastructures,
these APIs have several restrictions such as a
limit on the number of API calls. To help the
development of new technology, we are run-
ning an open search engine infrastructure,
TSUBAKI, on a high-performance comput-
ing environment. In this paper, we describe
TSUBAKI infrastructure.
1 Introduction
As the amount of information created by human be-
ings is explosively grown in the last decade (Uni-
versity of California, 2003), it is getting extremely
harder to obtain necessary information by con-
ventional information access methods, i.e., Web
search engines. This is obvious from the fact that
knowledge workers now spend about 30% of their
day on only searching for information (The Del-
phi Group White Paper, 2001). Hence, creation of
drastically new technology is needed by integrating
several disciplines such as natural language process-
ing (NLP), information retrieval (IR) and others.
Conventional search engines such as Google and
Yahoo! are insufficient to search necessary informa-
tion from the current Web. The problems of the con-
ventional search engines are summarized as follows:
Cannot accept queries by natural language sen-
tences: Search engine users have to represent their
needs by a list of words. This means that search
engine users cannot obtain necessary information if
they fail to represent their needs into a proper word
list. This is a serious problem for users who do not
utilize a search engine frequently.
Cannot provide organized search results: A
search result is a simple list consisting of URLs,
titles and snippets of web pages. This type of re-
sult presentation is obviously insufficient consider-
ing explosive growth and diversity of web pages.
Cannot handle synonymous expressions: Exist-
ing search engines ignore a synonymous expression
problem. Especially, since Japanese uses three kinds
of alphabets, Hiragana, Katakana and Kanji, this
problem is more serious. For instance, although both
Japanese words ????? and ???? mean child,
the search engines provide quite different search re-
sults for each word.
We believe that new IR systems that overcome the
above problems give us more flexible and com-
fortable information access and that development
of such systems is an important and interesting re-
search topic.
To develop such IR systems, a search engine in-
frastructure that plays a low-level layer role (i.e., re-
trieving web pages according to a user?s query from
a huge web page collection) is required. The Appli-
cation Programming Interfaces (APIs) provided by
189
commercial search engines can be regarded as such
search engine infrastructures. The APIs, however,
have the following problems:
1. The number of API calls a day and the num-
ber of web pages included in a search result are
limited.
2. The API users cannot know how the acquired
web pages are ranked because the ranking mea-
sure of web pages has not been made public.
3. It is difficult to reproduce previously-obtained
search results via the APIs because search en-
gine?s indices are updated frequently.
These problems are an obstacle to develop new IR
systems using existing search engine APIs.
The research project ?Cyber Infrastructure for the
Information-explosion Era1? gives researchers sev-
eral kinds of shared platforms and sophisticated
tools, such as an open search engine infrastructure,
considerable computational environment and a grid
shell software (Kaneda et al, 2002), for creation of
drastically new IR technology. In this paper, we de-
scribe an open search engine infrastructure TSUB-
AKI, which is one of the shared platforms devel-
oped in the Cyber Infrastructure for the Information-
explosion Era project. The overview of TSUBAKI is
depicted in Figure 1. TSUBAKI is built on a high-
performance computing environment consisting of
128 CPU cores and 100 tera-byte storages, and it
can provide users with search results retrieved from
approximately 100 million Japanese web pages.
The mission of TSUBAKI is to help the develop-
ment of new information access methodology which
solves the problems of conventional information ac-
cess methods. This is achieved by the following
TSUBAKI?s characteristics:
API without any restriction: TSUBAKI pro-
vides its API without any restrictions such as the
limited number of API calls a day and the number
of results returned from an API per query, which are
the typical restrictions of the existing search engine
APIs. Consequently, TSUBAKI API users can de-
velop systems that handle a large number of web
pages. This feature is important for dealing with the
Web that has the long tail aspect.
1http://i-explosion.ex.nii.ac.jp/i-explosion/ctr.php/m/Inde-
xEng/a/Index/
	







 	
Learning Reliability of Parses for Domain Adaptation of
Dependency Parsing
Daisuke Kawahara and Kiyotaka Uchimoto
National Institute of Information and Communications Technology,
3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan
{dk, uchimoto}@nict.go.jp
Abstract
The accuracy of parsing has exceeded 90%
recently, but this is not high enough to use
parsing results practically in natural lan-
guage processing (NLP) applications such as
paraphrase acquisition and relation extrac-
tion. We present a method for detecting re-
liable parses out of the outputs of a single
dependency parser. This technique is also
applied to domain adaptation of dependency
parsing. Our goal was to improve the per-
formance of a state-of-the-art dependency
parser on the data set of the domain adap-
tation track of the CoNLL 2007 shared task,
a formidable challenge.
1 Introduction
Dependency parsing has been utilized in a variety
of natural language processing (NLP) applications,
such as paraphrase acquisition, relation extraction
and machine translation. For newspaper articles, the
accuracy of dependency parsers exceeds 90% (for
English), but it is still not sufficient for practical use
in these NLP applications. Moreover, the accuracy
declines significantly for out-of-domain text, such as
weblogs and web pages, which have commonly been
used as corpora. From this point of view, it is impor-
tant to consider the following points to use a parser
practically in applications:
? to select reliable parses, especially for knowl-
edge acquisition,
? and to adapt the parser to new domains.
This paper proposes a method for selecting reli-
able parses from parses output by a single depen-
dency parser. We do not use an ensemble method
based on multiple parsers, but use only a single
parser, because speed and efficiency are important
when processing a massive volume of text. The
resulting highly reliable parses would be useful to
automatically construct dictionaries and knowledge
bases, such as case frames (Kawahara and Kuro-
hashi, 2006). Furthermore, we incorporate the reli-
able parses we obtained into the dependency parser
to achieve domain adaptation.
The CoNLL 2007 shared task tackled domain
adaptation of dependency parsers for the first time
(Nivre et al, 2007). Sagae and Tsujii applied an
ensemble method to the domain adaptation track
and achieved the highest score (Sagae and Tsujii,
2007). They first parsed in-domain unlabeled sen-
tences using two parsers trained on out-of-domain
labeled data. Then, they extracted identical parses
that were produced by the two parsers and added
them to the original (out-of-domain) training set to
train a domain-adapted model.
Dredze et al yielded the second highest score1
in the domain adaptation track (Dredze et al, 2007).
However, their results were obtained without adap-
tation. They concluded that it is very difficult to sub-
stantially improve the target domain performance
over that of a state-of-the-art parser. To confirm
this, we parsed the test set (CHEM) of the domain
adaptation track by using one of the best dependency
parsers, second-order MSTParser (McDonald et al,
1Dredze et al achieved the second highest score on the
CHEM test set for unlabeled dependency accuracy.
709
2006)2. Though this parser was trained on the pro-
vided out-of-domain (Penn Treebank) labeled data,
surprisingly, its accuracy slightly outperformed the
highest score achieved by Sagae and Tsujii (unla-
beled dependency accuracy: 83.58 > 83.42 (Sagae
and Tsujii, 2007)). Our goal is to improve a state-
of-the-art parser on this domain adaptation track.
Dredze et al also indicated that unlabeled de-
pendency parsing is not robust to domain adaptation
(Dredze et al, 2007). This paper therefore focuses
on unlabeled dependency parsing.
2 Related Work
We have already described the domain adaptation
track of the CoNLL 2007 shared task. For the mul-
tilingual dependency parsing track, which was the
other track of the shared task, Nilsson et al achieved
the best performance using an ensemble method
(Hall et al, 2007). They used a method of com-
bining several parsers? outputs in the framework of
MST parsing (Sagae and Lavie, 2006). This method
does not select parses, but considers all the output
parses with weights to decide a final parse of a given
sentence.
Reichart and Rappoport also proposed an ensem-
ble method to select high-quality parses from the
outputs of constituency parsers (Reichart and Rap-
poport, 2007a). They regarded parses as being of
high quality if 20 different parsers agreed. They did
not apply their method to domain adaptation or other
applications.
Reranking methods for parsing have a relation
to parse selection. They rerank the n-best parses
that are output by a generative parser using a lot
of lexical and syntactic features (Collins and Koo,
2005; Charniak and Johnson, 2005). There are
several related methods for 1-best outputs, such
as revision learning (Nakagawa et al, 2002) and
transformation-based learning (Brill, 1995) for part-
of-speech tagging. Attardi and Ciaramita proposed
a method of tree revision learning for dependency
parsing (Attardi and Ciaramita, 2007).
As for the use of unlabeled data, self-training
methods have been successful in recent years. Mc-
Closky et al improved a state-of-the-art con-
stituency parser by 1.1% using self-training (Mc-
2http://sourceforge.net/projects/mstparser/
Table 1: Labeled and unlabeled data provided for
the shared task. The labeled PTB data is used for
training, and the labeled BIO data is used for devel-
opment. The labeled CHEM data is used for the final
test.
name source labeled unlabeled
PTB Penn Treebank 18,577 1,625,606
BIO Penn BioIE 200 369,439
CHEM Penn BioIE 200 396,128
Closky et al, 2006a). They also applied self-training
to domain adaptation of a constituency parser (Mc-
Closky et al, 2006b). Their method simply adds
parsed unlabeled data without selecting it to the
training set. Reichart and Rappoport applied self-
training to domain adaptation using a small set of
in-domain training data (Reichart and Rappoport,
2007b).
Van Noord extracted bilexical preferences from a
Dutch parsed corpus of 500M words without selec-
tion (van Noord, 2007). He added some features into
an HPSG (head-driven phrase structure grammar)
parser to consider the bilexical preferences, and ob-
tained an improvement of 0.5% against a baseline.
Kawahara and Kurohashi extracted reliable de-
pendencies from automatic parses of Japanese sen-
tences on the web to construct large-scale case
frames (Kawahara and Kurohashi, 2006). Then
they incorporated the constructed case frames into a
probabilistic dependency parser, and outperformed
their baseline parser by 0.7%.
3 The Data Set
This paper uses the data set that was used in the
CoNLL 2007 shared task (Nivre et al, 2007). Table
1 lists the data set provided for the domain adapta-
tion track.
We pre-processed all the unlabeled sentences us-
ing a conditional random fields (CRFs)-based part-
of-speech tagger. This tagger is trained on the
PTB training set that consists of 18,577 sentences.
The features are the same as those in (Ratnaparkhi,
1996). As an implementation of CRFs, we used
CRF++3. If a method of domain adaptation is ap-
plied to the tagger, the accuracy of parsing unlabeled
sentences will improve (Yoshida et al, 2007). This
3http://crfpp.sourceforge.net/
710
paper, however, does not deal with domain adapta-
tion of a tagger but focuses on that of a parser.
4 Learning Reliability of Parses
Our approach assesses automatic parses of a single
parser in order to select only reliable parses from
them. We compare automatic parses and their gold-
standard ones, and regard accurate parses as positive
examples and the remainder as negative examples.
Based on these examples, we build a binary classi-
fier that classifies each sentence as reliable or not.
To precisely detect reliable parses, we make use of
several linguistic features inspired by the notion of
controlled language (Mitamura et al, 1991). That is
to say, the reliability of parses is judged based on the
degree of sentence difficulty.
Before describing our base dependency parser and
the algorithm for detecting reliable parses, we first
explain the data sets used for them. We prepared
the following three labeled data sets to train the base
dependency parser and the reliability detector.
PTB base train: training set for the base parser:
14,862 sentences
PTB rel train: training set for reliability detector:
2,500 sentences4
BIO rel dev: development set for reliability detec-
tor: 200 sentences (= labeled BIO data)
PTB base train is used to train the base depen-
dency parser, and PTB rel train is used to train our
reliability detector. BIO rel dev is used for tuning
the parameters of the reliability detector.
4.1 Base Dependency Parser
We used the MSTParser (McDonald et al, 2006),
which achieved top results in the CoNLL 2006
(CoNLL-X) shared task, as a base dependency
parser. To enable second-order features, the param-
eter order was set to 2. The other parameters were
set to default. We used PTB base train (14,862 sen-
tences) to train this parser.
4.2 Algorithm to Detect Reliable Parses
We built a binary classifier for detecting reliable sen-
tences from a set of automatic parses produced by
41,215 labeled PTB sentences are left as another develop-
ment set for the reliability detector, but they are not used in this
paper.
the base dependency parser.
We used support vector machines (SVMs) as a bi-
nary classifier with a third-degree polynomial ker-
nel. We parsed PTB rel train (2,500 sentences) us-
ing the base parser, and evaluated each sentence with
the metric of unlabeled dependency accuracy. We
regarded the sentences whose accuracy is better than
a threshold, ? , as positive examples, and the others
as negative ones. In this experiment, we set the ac-
curacy threshold ? at 100%. As a result, 736 out of
2,500 examples (sentences) were judged to be posi-
tive.
To evaluate the reliability of parses, we take ad-
vantage of the following features that can be related
to the difficulty of sentences.
sentence length: The longer the sentence is, the
poorer the parser performs (McDonald and Nivre,
2007). We determine sentence length by the number
of words.
dependency lengths: Long-distance dependen-
cies exhibit bad performance (McDonald and Nivre,
2007). We calculate the average of the dependency
length of each word.
difficulty of vocabulary: It is hard for super-
vised parsers to learn dependencies that include low-
frequency words. We count word frequencies in the
training data and make a word list in descending or-
der of frequency. For a given sentence, we calculate
the average frequency rank of each word.
number of unknown words: Similarly, depen-
dency accuracy for unknown words is notoriously
poor. We count the number of unknown words in a
given sentence.
number of commas: Sentences with multiple
commas are difficult to parse. We count the num-
ber of commas in a given sentence.
number of conjunctions (and/or): Sentences
with coordinate structures are also difficult to parse
(Kurohashi and Nagao, 1994). We count the num-
ber of coordinate conjunctions (and/or) in a given
sentence.
To apply these features to SVMs in practice, the
numbers are binned at a certain interval for each fea-
ture. For instance, the number of conjunctions is
split into four bins: 0, 1, 2 and more than 2.
711
Table 2: Example BIO sentences judged as reliable. The underlined words have incorrect modifying heads.
dep. accuracy sentences judged as reliable
12/12 (100%) No mutations resulting in truncation of the APC protein were found .
12/13 (92%) Conventional imaging techniques did not show two in 10 of these patients .
6/6 (100%) Pancreatic juice was sampled endoscopically .
11/12 (92%) The specificity of p53 mutation for pancreatic cancer is very high .
9/10 (90%) K-ras mutations are early genetic changes in colon cancer .
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 80  82  84  86  88  90  92  94  96  98  100
Sen
tenc
e co
vera
ge (%
)
Dependency accuracy (%)
Figure 1: Accuracy-coverage curve on BIO rel dev.
4.3 Experiments on Detecting Reliable Parses
We conducted an experiment on detecting the reli-
ability of parses. Our detector was applied to the
automatic parses of BIO rel dev, and only reliable
parses were selected from them. When parsing this
set, the POS tags contained in the set were substi-
tuted with automatic POS tags because it is prefer-
able to have the same environment as when applying
the parser to unlabeled data.
We evaluated unlabeled dependency accuracy of
the extracted parses. The accuracy-coverage curve
shown in Figure 1 was obtained by changing the soft
margin parameter C 5 of SVMs from 0.0001 to 10.
In this figure, the coverage is the ratio of selected
sentences out of all the sentences (200 sentences),
and the accuracy is unlabeled dependency accuracy.
A coverage of 100% indicates that the accuracy of
200 sentences without any selection was 80.85%.
If the soft margin parameter C is set to 0.001,
we can obtain 19 sentences out of 200 at a depen-
dency accuracy of 93.85% (183/195). The average
sentence length was 10.3 words. Out of obtained
19 sentences, 14 sentences achieved a dependency
accuracy of 100%, and thus the precision of the reli-
ability detector itself was 73.7% (14/19). Out of 200
sentences, 36 sentences were correctly parsed by the
5A higher soft margin value allows more classification er-
rors, and thus leads to the increase of recall and the decrease of
precision.
base parser, and thus the recall is 38.9% (14/36).
Table 2 shows some sentences that were evaluated
as reliable using the above setting (C = 0.001). Ma-
jor errors were caused by prepositional phrase (PP)-
attachment. To improve the accuracy of detecting
reliable parses, it would be necessary to consider the
number of PP-attachment ambiguities in a given sen-
tence as a feature.
5 Domain Adaptation of Dependency
Parsing
For domain adaptation, we adopt a self-training
method. We combine in-domain unlabeled (auto-
matically labeled) data with out-of-domain labeled
data to make a training set. There are many possible
methods for combining unlabeled and labeled data
(Daume? III, 2007), but we simply concatenate unla-
beled data with labeled data to see the effectiveness
of the selected reliable parses. The in-domain unla-
beled data to be added are selected by the reliability
detector. We set the soft margin parameter at 0.001
to extract highly reliable parses. As mentioned in
the previous section, the accuracy of selected parses
was approximately 94%.
We parsed the unlabeled sentences of BIO and
CHEM (approximately 400K sentences for each) us-
ing the base dependency parser that is trained on the
entire PTB labeled data. Then, we applied the reli-
ability detector to these parsed sentences to obtain
31,266 sentences for BIO and 31,470 sentences for
CHEM. We call the two sets of obtained sentences
?BIO pool? and ?CHEM pool?.
For each training set of the experiments described
below, a certain number of sentences are randomly
selected from the pool and combined with the entire
out-of-domain (PTB) labeled data.
5.1 Experiment on BIO Development Data
We first conducted an experiment of domain adapta-
tion using the BIO development set.
712
 83
 83.5
 84
 84.5
 85
 0  5000  10000  15000  20000  25000
Acc
ura
cy (%
)
Number of Unlabeled Sentences
reliable parses
randomly selected parses
without addition
Figure 2: Dependency accuracies on BIO when the
number of added unlabeled data is changed.
Figure 2 shows how the accuracy changes when
the number of added reliable parses is changed. The
solid line represents our proposed method, and the
dotted line with points represents a baseline method.
This baseline is a self-training method that simply
adds unlabeled data without selection to the PTB
labeled data. Each experimental result is the aver-
age of five trials done to randomly select a certain
number of parses from the BIO pool. The horizontal
dotted line (84.07%) represents the accuracy of the
parser without adding unlabeled data (trained only
on the PTB labeled data).
From this figure, we can see that the proposed
method always outperforms the baseline by approxi-
mately 0.4%. The best accuracy was achieved when
18,000 unlabeled parses were added. However, if
more than 18,000 sentences are added, the accuracy
declines. This can be attributed to the balance of the
number of labeled data and unlabeled data. Since
the number of added unlabeled data is more than
the number of labeled data, the entire training set
might be unreliable, though the accuracy of added
unlabeled data is relatively high. To address this
problem, it is necessary to weigh labeled data or
to change the way information from acquired unla-
beled data is handled.
5.2 Experiment on CHEM Test Data
The addition of 18,000 sentences showed the high-
est accuracy for the BIO development data. To adapt
the parser to the CHEM test set, we used 18,000 reli-
able unlabeled sentences from the CHEM pool with
the PTB labeled sentences to train the parser. Ta-
ble 3 lists the experimental results. In this table, the
Table 3: Experimental results on CHEM test data.
system accuracy
PTB+unlabel (18,000 sents.) 84.12
only PTB (baseline) 83.58
1st (Sagae and Tsujii, 2007) 83.42
2nd (Dredze et al, 2007) 83.38
3rd (Attardi et al, 2007) 83.08
third row lists the three highest scores of the domain
adaptation track of the CoNLL 2007 shared task.
The baseline parser was trained only on the PTB
labeled data (as described in Section 1). The pro-
posed method (PTB+unlabel (18,000 sents.)) out-
performed the baseline by approximately 0.5%, and
also beat all the systems submitted to the domain
adaptation track. These systems include an en-
semble method (Sagae and Tsujii, 2007) and an
approach of tree revision learning with a selec-
tion method of only using short training sentences
(shorter than 30 words) (Attardi et al, 2007).
6 Discussion and Conclusion
This paper described a method for detecting reliable
parses out of the outputs of a single dependency
parser. This technique was also applied to domain
adaptation of dependency parsing.
To extract reliable parses, we did not adopt an en-
semble method, but used a single-parser approach
because speed and efficiency are important in pro-
cessing a gigantic volume of text to benefit knowl-
edge acquisition. In this paper, we employed the
MSTParser, which can process 3.9 sentences/s on a
XEON 3.0GHz machine in spite of the time com-
plexity of O(n3). If greater efficiency is required,
it is possible to apply a pre-filter that removes long
sentences (e.g., longer than 30 words), which are
seldom selected by the reliability detector. In ad-
dition, our method does not depend on a particu-
lar parser, and can be applied to other state-of-the-
art parsers, such as Malt Parser (Nivre et al, 2006),
which is a feature-rich linear-time parser.
In general, it is very difficult to improve the accu-
racy of the best performing systems by using unla-
beled data. There are only a few successful studies,
such as (Ando and Zhang, 2005) for chunking and
(McClosky et al, 2006a; McClosky et al, 2006b) on
constituency parsing. We succeeded in boosting the
accuracy of the second-order MST parser, which is
713
a state-of-the-art dependency parser, in the CoNLL
2007 domain adaptation task. This was a difficult
challenge as many participants in the task failed to
obtain any meaningful gains from unlabeled data
(Dredze et al, 2007). The key factor in our success
was the extraction of only reliable information from
unlabeled data.
However, that improvement was not satisfactory.
In order to achieve more gains, it is necessary to ex-
ploit a much larger number of unlabeled data. In this
paper, we adopted a simple method to combine un-
labeled data with labeled data. To use this method
more effectively, we need to balance the labeled and
unlabeled data very carefully. However, this method
is not scalable because the training time increases
significantly as the size of a training set expands. We
can consider the information from more unlabeled
data as features of machine learning techniques. An-
other approach is to formalize a probabilistic model
based on unlabeled data.
References
Rie Ando and Tong Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Proceed-
ings of ACL2005, pages 1?9.
Giuseppe Attardi and Massimiliano Ciaramita. 2007. Tree re-
vision learning for dependency parsing. In Proceedings of
NAACL-HLT2007, pages 388?395.
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, Atanas
Chanev, and Massimiliano Ciaramita. 2007. Multilingual
dependency parsing and domain adaptation using DeSR. In
Proceedings of EMNLP-CoNLL2007, pages 1112?1118.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing. Computational Linguis-
tics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of ACL2005, pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Hal Daume? III. 2007. Frustratingly easy domain adaptation. In
Proceedings of ACL2007, pages 256?263.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman
Ganchev, Joa?o V. Grac?a, and Fernando Pereira. 2007. Frus-
tratingly hard domain adaptation for dependency parsing. In
Proceedings of EMNLP-CoNLL2007, pages 1051?1055.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit, Bea?ta
Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single
malt or blended? a study in multilingual parser optimization.
In Proceedings of EMNLP-CoNLL2007, pages 933?939.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for Japanese syntactic and
case structure analysis. In Proceedings of HLT-NAACL2006,
pages 176?183.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL2006, pages 152?159.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL2006, pages 337?344.
Ryan McDonald and Joakim Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In Pro-
ceedings of EMNLP-CoNLL2007, pages 122?131.
Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006.
Multilingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings of CoNLL-X, pages 216?220.
TerukoMitamura, Eric Nyberg, and Jaime Carbonell. 1991. An
efficient interlingua translation system for multi-lingual doc-
ument production. In Proceedings of MT Summit III, pages
55?61.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto. 2002. Re-
vision learning and its application to part-of-speech tagging.
In Proceedings of ACL2002, pages 497?504.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?l sen Eryi git, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with support vector machines. In Proceed-
ings of CoNLL-X, pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The
CoNLL 2007 shared task on dependency parsing. In Pro-
ceedings of EMNLP-CoNLL2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP1996,
pages 133?142.
Roi Reichart and Ari Rappoport. 2007a. An ensemble method
for selection of high quality parses. In Proceedings of
ACL2007, pages 408?415.
Roi Reichart and Ari Rappoport. 2007b. Self-training for
enhancement and domain adaptation of statistical parsers
trained on small datasets. In Proceedings of ACL2007, pages
616?623.
Kenji Sagae and Alon Lavie. 2006. Parser combination by
reparsing. In Proceedings of the Companion Volume to HLT-
NAACL2006, pages 129?132.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser ensem-
bles. In Proceedings of EMNLP-CoNLL2007, pages 1044?
1050.
Gertjan van Noord. 2007. Using self-trained bilexical prefer-
ences to improve disambiguation accuracy. In Proceedings
of IWPT2007, pages 1?10.
Kazuhiro Yoshida, Yoshimasa Tsuruoka, Yusuke Miyao, and
Jun?ichi Tsujii. 2007. Ambiguous part-of-speech tagging
for improving accuracy and domain portability of syntactic
parsers. In Proceedings of IJCAI-07, pages 1783?1788.
714
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521?529,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Effect of Corpus Size on Case Frame Acquisition
for Discourse Analysis
Ryohei Sasano
Graduate School of Informatics,
Kyoto University
sasano@i.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communications Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper reports the effect of corpus size on
case frame acquisition for discourse analysis
in Japanese. For this study, we collected a
Japanese corpus consisting of up to 100 bil-
lion words, and constructed case frames from
corpora of six different sizes. Then, we ap-
plied these case frames to syntactic and case
structure analysis, and zero anaphora resolu-
tion. We obtained better results by using case
frames constructed from larger corpora; the
performance was not saturated even with a
corpus size of 100 billion words.
1 Introduction
Very large corpora obtained from the Web have
been successfully utilized for many natural lan-
guage processing (NLP) applications, such as prepo-
sitional phrase (PP) attachment, other-anaphora res-
olution, spelling correction, confusable word set dis-
ambiguation and machine translation (Volk, 2001;
Modjeska et al, 2003; Lapata and Keller, 2005; At-
terer and Schu?tze, 2006; Brants et al, 2007).
Most of the previous work utilized only the sur-
face information of the corpora, such as n-grams,
co-occurrence counts, and simple surface syntax.
This may be because these studies did not require
structured knowledge, and for such studies, the size
of currently available corpora is considered to have
been almost enough. For instance, while Brants et
al. (2007) reported that translation quality continued
to improve with increasing corpus size for training
language models at even size of 2 trillion tokens, the
increase became small at the corpus size of larger
than 30 billion tokens.
However, for more complex NLP tasks, such as
case structure analysis and zero anaphora resolution,
it is necessary to obtain more structured knowledge,
such as semantic case frames, which describe the
cases each predicate has and the types of nouns that
can fill a case slot. Note that case frames offer not
only the knowledge of the relationships between a
predicate and its particular case slot, but also the
knowledge of the relationships among a predicate
and its multiple case slots. To obtain such knowl-
edge, very large corpora seem to be necessary; how-
ever it is still unknown how much corpora would be
required to obtain good coverage.
For examples, Kawahara and Kurohashi pro-
posed a method for constructing wide-coverage case
frames from large corpora (Kawahara and Kuro-
hashi, 2006b), and a model for syntactic and case
structure analysis of Japanese that based upon case
frames (Kawahara and Kurohashi, 2006a). How-
ever, they did not demonstrate whether the coverage
of case frames was wide enough for these tasks and
how dependent the performance of the model was on
the corpus size for case frame construction.
This paper aims to address these questions. We
collect a very large Japanese corpus consisting of
about 100 billion words, or 1.6 billion unique sen-
tences from the Web. Subsets of the corpus are ran-
domly selected to obtain corpora of different sizes
ranging from 1.6 million to 1.6 billion sentences.
We construct case frames from each corpus and ap-
ply them to syntactic and case structure analysis, and
zero anaphora resolution, in order to investigate the
521
relationships between the corpus size and the perfor-
mance of these analyses.
2 Related Work
Many NLP tasks have successfully utilized very
large corpora, most of which were acquired from
the Web (Kilgarriff and Grefenstette, 2003). Volk
(2001) proposed a method for resolving PP attach-
ment ambiguities based upon Web data. Modjeska
et al (2003) used the Web for resolving nominal
anaphora. Lapata and Keller (2005) investigated the
performance of web-based models for a wide range
of NLP tasks, such as MT candidate selection, ar-
ticle generation, and countability detection. Nakov
and Hearst (2008) solved relational similarity prob-
lems using the Web as a corpus.
With respect to the effect of corpus size on NLP
tasks, Banko and Brill (2001a) showed that for
content sensitive spelling correction, increasing the
training data size improved the accuracy. Atterer
and Schu?tze (2006) investigated the effect of cor-
pus size in combining supervised and unsupervised
learning for two types of attachment decision; they
found that the combined system only improved the
performance of the parser for small training sets.
Brants et al (2007) varied the amount of language
model training data from 13 million to 2 trillion to-
kens and applied these models to machine transla-
tion systems. They reported that translation qual-
ity continued to improve with increasing corpus size
for training language models at even size of 2 tril-
lion tokens. Suzuki and Isozaki (2008) provided ev-
idence that the use of more unlabeled data in semi-
supervised learning could improve the performance
of NLP tasks, such as POS tagging, syntactic chunk-
ing, and named entities recognition.
There are several methods to extract useful infor-
mation from very large corpora. Search engines,
such as Google and Altavista, are often used to ob-
tain Web counts (e.g. (Nakov and Hearst, 2005;
Gledson and Keane, 2008)). However, search en-
gines are not designed for NLP research and the re-
ported hit counts are subject to uncontrolled vari-
ations and approximations. Therefore, several re-
searchers have collected corpora from the Web by
themselves. For English, Banko and Brill (2001b)
collected a corpus with 1 billion words from vari-
ety of English texts. Liu and Curran (2006) created
a Web corpus for English that contained 10 billion
words and showed that for content-sensitive spelling
correction the Web corpus results were better than
using a search engine. Halacsy et al (2004) created
a corpus with 1 billion words for Hungarian from
the Web by downloading 18 million pages. Others
utilize publicly available corpus such as the North
American News Corpus (NANC) and the Gigaword
Corpus (Graff, 2003). For instance, McClosky et al
(2006) proposed a simple method of self-training a
two phase parser-reranker system using NANC.
As for Japanese, Kawahara and Kurohashi
(2006b) collected 23 million pages and created a
corpus with approximately 20 billion words. Google
released Japanese n-gram constructed from 20 bil-
lion Japanese sentences (Kudo and Kazawa, 2007).
Several news wires are publicly available consisting
of tens of million sentences. Kotonoha project is
now constructing a balanced corpus of the present-
day written Japanese consisting of 50 million words
(Maekawa, 2006).
3 Construction of Case Frames
Case frames describe the cases each predicate has
and what nouns can fill the case slots. In this study,
case frames we construct case frames from raw cor-
pora by using the method described in (Kawahara
and Kurohashi, 2006b). This section illustrates the
methodology for constructing case frames.
3.1 Basic Method
After parsing a large corpus by a Japanese parser
KNP1, we construct case frames from modifier-head
examples in the resulting parses. The problems for
case frame construction are syntactic and semantic
ambiguities. In other words, the resulting parses in-
evitably contain errors and predicate senses are in-
trinsically ambiguous. To cope with these problems,
we construct case frames from reliable modifier-
head examples.
First, we extract modifier-head examples that had
no syntactic ambiguity, and assemble them by cou-
pling a predicate and its closest case component.
That is, we assemble the examples not by predi-
cates, such as tsumu (load/accumulate), but by cou-
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
522
Table 1: Examples of Constructed Case Frames.
Case slot Examples Generalized examples with rate
ga (nominative) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?tsumu (1) wo (accusative) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?(load) ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2) ga (nominative) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate) wo (accusative) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
... ... ...
ga (nominative) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (accusative) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
... ... ...
ples, such as nimotsu-wo tsumu (load baggage) and
keiken-wo tsumu (accumulate experience). Such
couples are considered to play an important role
for constituting sentence meanings. We call the as-
sembled examples as basic case frames. In order
to remove inappropriate examples, we introduce a
threshold ? and use only examples that appeared no
less than ? times in the corpora.
Then, we cluster the basic case frames to merge
similar case frames. For example, since nimotsu-
wo tsumu (load baggage) and busshi-wo tsumu (load
supplies) are similar, they are merged. The similar-
ity is measured by using a Japanese thesaurus (The
National Language Institute for Japanese Language,
2004). Table 1 shows examples of constructed case
frames.
3.2 Generalization of Examples
When we use hand-crafted case frames, the data
sparseness problem is serious; by using case frames
automatically constructed from a large corpus, it was
alleviated to some extent but not eliminated. For in-
stance, there are thousands of named entities (NEs)
that cannot be covered intrinsically. To deal with
this problem, we generalize the examples of the case
slots. Kawahara and Kurohashi also generalized ex-
amples but only for a few types. In this study, we
generalize case slot examples based upon common
noun categories and NE classes.
First, we generalize the examples based upon the
categories that tagged by the Japanese morpholog-
ical analyzer JUMAN2. In JUMAN, about 20 cat-
egories are defined and tagged to common nouns.
For example, ringo (apple), inu (dog) and byoin
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
(hospital) are tagged as FOOD, ANIMAL and FA-
CILITY, respectively. For each category, we calcu-
late the ratio of the categorized example among all
case slot examples, and add it to the case slot (e.g.
[CT:FOOD]:0.07).
We also generalize the examples based upon NE
classes. We use a common standard NE defini-
tion for Japanese provided by the IREX (1999).
We first recognize NEs in the source corpus by
using an NE recognizer (Sasano and Kurohashi,
2008); and then construct case frames from the NE-
recognized corpus. Similar to the categories, for
each NE class, we calculate the NE ratio among all
the case slot examples, and add it to the case slot
(e.g. [NE:PERSON]:0.12). The generalized exam-
ples are also included in Table 1.
4 Discourse Analysis with Case Frames
In order to investigate the effect of corpus size
on complex NLP tasks, we apply the constructed
cases frames to an integrated probabilistic model
for Japanese syntactic and case structure analysis
(Kawahara and Kurohashi, 2006a) and a probabilis-
tic model for Japanese zero anaphora resolution
(Sasano et al, 2008). In this section, we briefly de-
scribe these models.
523
4.1 Model for Syntactic and Case Structure
Analysis
Kawahara and Kurohashi (2006a) proposed an in-
tegrated probabilistic model for Japanese syntactic
and case structure analysis based upon case frames.
Case structure analysis recognizes predicate argu-
ment structures. Their model gives a probability to
each possible syntactic structure T and case struc-
ture L of the input sentence S, and outputs the syn-
tactic and case structure that have the highest proba-
bility. That is to say, the system selects the syntactic
structure Tbest and the case structure Lbest that max-
imize the probability P (T,L|S):
(Tbest, Lbest) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant. P (T,L, S) is defined as the product of a prob-
ability for generating a clause Ci as follows:
P (T,L, S) = ?
i=1..n
P (Ci|bhi) (2)
where n is the number of clauses in S, and bhi is
Ci?s modifying bunsetsu3. P (Ci|bhi) is approxi-
mately decomposed into the product of several gen-
erative probabilities such as P (A(sj) = 1|CFl, sj)
and P (nj |CFl, sj , A(sj) = 1), where the function
A(sj) returns 1 if a case slot sj is filled with an input
case component; otherwise 0. P (A(sj)=1|CFl, sj)
denotes the probability that the case slot sj is filled
with an input case component, and is estimated from
resultant case structure analysis of a large raw cor-
pus. P (nj |CFl, sj , A(sj) = 1) denotes the proba-
bility of generating a content part nj from a filled
case slot sj in a case frame CFl, and is calculated
by using case frames. For details see (Kawahara and
Kurohashi, 2006a).
4.2 Model for Zero Anaphora Resolution
Anaphora resolution is one of the most important
techniques for discourse analysis. In English, overt
pronouns such as she and definite noun phrases such
as the company are anaphors that refer to preced-
ing entities (antecedents). On the other hand, in
3In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English.
Japanese, anaphors are often omitted; these omis-
sions are called zero pronouns. Zero anaphora res-
olution is the integrated task of zero pronoun detec-
tion and zero pronoun resolution.
We proposed a probabilistic model for Japanese
zero anaphora resolution based upon case frames
(Sasano et al, 2008). This model first resolves
coreference and identifies discourse entities; then
gives a probability to each possible case frame CF
and case assignment CA when target predicate v,
input case components ICC and existing discourse
entities ENT are given, and outputs the case frame
and case assignment that have the highest probabil-
ity. That is to say, this model selects the case frame
CFbest and the case assignment CAbest that maxi-
mize the probability P (CF,CA|v, ICC,ENT ):
(CF best, CAbest)
= argmax
(CF,CA)
P (CF,CA|v, ICC,ENT ) (3)
P (CF,CA|v, ICC,ENT ) is approximately de-
composed into the product of several probabilities.
Case frames are used for calculating P (nj |CFl,
sj , A(sj) = 1), the probability of generating a con-
tent part nj from a case slot sj in a case frame
CFl, and P (nj |CFl, sj , A?(sj)=1), the probability
of generating a content part nj of a zero pronoun,
where the function A?(sj) returns 1 if a case slot sj
is filled with an antecedent of a zero pronoun; other-
wise 0.
P (nj |CFl, sj , A?(sj)=1) is similar to P (nj |CFl,
sj , A(sj)=1) and estimated from the frequencies of
case slot examples in case frames. However, while
A?(sj)=1 means sj is not filled with an overt argu-
ment but filled with an antecedent of zero pronoun,
case frames are constructed from overt predicate ar-
gument pairs. Therefore, the content part nj is often
not included in the case slot examples. To cope with
this problem, this model also utilizes generalized ex-
amples to estimate P (nj |CFl, sj , A(sj) = 1). For
details see (Sasano et al, 2008).
5 Experiments
5.1 Construction of Case Frames
In order to investigate the effect of corpus size,
we constructed case frames from corpora of dif-
ferent sizes. We first collected Japanese sentences
524
Table 4: Statistics of the Constructed Case Frames.
Corpus size (sentences) 1.6M 6.3M 25M 100M 400M 1.6G
# of predicate 2460 6134 13532 27226 42739 65679
(type) verb 2039 4895 10183 19191 28523 41732
adjective 154 326 617 1120 1641 2318
noun with copula 267 913 2732 6915 12575 21629
average # of case frames for a predicate 15.9 12.2 13.3 16.1 20.5 25.3
average # of case slots for a case frame 2.95 3.44 3.88 4.21 4.69 5.08
average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6
average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64
average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84
File size(byte) 8.9M 20M 56M 147M 369M 928M
Table 3: Corpus Sizes and Thresholds.
Corpus size for caseframe construction 1.6M 6.3M 25M 100M 400M 1.6G(sentences)
Threshold ?introduced in Sec. 3.1 2 3 4 5 7 10
Corpus size toestimate generative 1.6M 3.2M 6.3M 13M 25M 50Mprobability (sentences)
from the Web using the method proposed by Kawa-
hara and Kurohashi (2006b). We acquired approx-
imately 6 billion Japanese sentences consisting of
approximately 100 billion words from 100 million
Japanese web pages. After discarding duplicate sen-
tences, which may have been extracted from mirror
sites, we acquired a corpus comprising of 1.6 bil-
lion (1.6G) unique Japanese sentences consisting of
approximately 25 billion words. The average num-
ber of characters and words in each sentence was
28.3, 15.6, respectively. Then we randomly selected
subsets of the corpus for five different sizes; 1.6M,
6.3M, 25M, 100M, and 400M sentences to obtain
corpora of different sizes.
We constructed case frames from each corpus. We
employed JUMAN and KNP to parse each corpus.
We changed the threshold ? introduced in Section
3.1 depending upon the size of the corpus as shown
in Table 3. Completing the case frame construc-
tion took about two weeks using 600 CPUs. Ta-
ble 4 shows the statistics for the constructed case
frames. The number of predicates, the average num-
ber of examples and unique examples for a case slot,
and whole file size were confirmed to be heavily de-
pendent upon the corpus size. However, the average
number of case frames for a predicate and case slots
for a case frame did not.
5.2 Coverage of Constructed Case Frames
5.2.1 Setting
In order to investigate the coverage of the resul-
tant case frames, we used a syntactic relation, case
structure, and anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences).
This corpus was manually annotated using the same
criteria as Kawahara et al (2004). There were 2,390
annotated relationships between predicates and their
direct (not omitted) case components and 837 zero
anaphoric relations in the corpus.
We used two evaluation metrics depending upon
whether the target case component was omitted or
not. For the overt case component of a predicate, we
judged the target component was covered by case
frames if the target component itself was included in
the examples for one of the corresponding case slots
of the case frame. For the omitted case component,
we checked not only the target component itself but
also all mentions that refer to the same entity as the
target component.
5.2.2 Coverage of Case Frames
Figure 1 shows the coverage of case frames for
the overt argument, which would have tight relations
with case structure analysis. The lower line shows
the coverage without considering generalized exam-
ples, the middle line shows the coverage considering
generalized NE examples, and the upper line shows
the coverage considering all generalized examples.
Figure 2 shows the coverage of case frames for
the omitted argument, which would have tight rela-
tions with zero anaphora resolution. The upper line
shows the coverage considering all generalized ex-
amples, which is considered to be the upper bound
of performance for the zero anaphora resolution sys-
525
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.897
0.683
0.649
+NE,CT match
+ NE match
exact match
Figure 1: Coverage of CF (overt argument).
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.892
0.608
0.472
+NE,CT match
+ NE match
exact match
Figure 2: Coverage of CF (omitted argument).
tem described in Section 4.2. Comparing with Fig-
ure 1, we found two characteristics. First, the lower
and middle lines of Figure 2 were located lower than
the corresponding lines in Figure 1. This would re-
flect that some frequently omitted case components
are not described in the case frames because the case
frames were constructed from only overt predicate
argument pairs. Secondly, the effect of generalized
NE examples was more evident for the omitted ar-
gument reflecting the important role of NEs in zero
anaphora resolution.
Both figures show that the coverage was improved
by using larger corpora and there was no saturation
even when the largest corpus of 1.6 billion sentences
was used. When the largest corpus and all general-
ized examples were used, the case frames achieved a
coverage of almost 90% for both the overt and omit-
ted argument.
Figure 3 shows the coverage of case frames for
each predicate type, which was calculated for both
overt and omitted argument considering all general-
ized examples. The case frames for verbs achieved
a coverage of 93.0%. There were 189 predicate-
argument pairs that were not included case frames;
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
verb
adjective
noun with copula
0.930
0.788
0.545
Figure 3: Coverage of CF for Each Predicate Type.
11 pairs of them were due to lack of the case frame
of target predicate itself, and the others were due
to lack of the corresponding example. For adjec-
tive, the coverage was 78.8%. The main cause of
the lower coverage would be that the predicate argu-
ment relations concerning adjectives that were used
in restrictive manner, such as ?oishii sushi? (deli-
cious sushi), were not used for case frame construc-
tion, although such relations were also the target of
the coverage evaluation. For noun with copula, the
coverage was only 54.5%. However, most predicate
argument relations concerning nouns with copula
were easily recognized from syntactic preference,
and thus the low coverage would not quite affect the
performance of discourse analysis.
5.3 Syntactic and Case Structure Analysis
5.3.1 Accuracy of Syntactic Analysis
We investigated the effect of corpus size for syn-
tactic analysis described in Section 4.1. We used
hand-annotated 759 web sentences, which was used
by Kawahara and Kurohashi (2007). We evaluated
the resultant syntactic structures with regard to de-
pendency accuracy, the proportion of correct depen-
dencies out of all dependencies4.
Figure 4 shows the accuracy of syntactic struc-
tures. We conducted these experiments with case
frames constructed from corpora of different sizes.
We also changed the corpus size to estimate gen-
erative probability of a case slot in Section 4.1 de-
pending upon the size of the corpus for case frame
construction as shown in Table 3. Figure 4 also in-
4Note that Kawahara and Kurohashi (2007) exclude the de-
pendency between the last two bunsetsu, since Japanese is head-
final and thus the second last bunsetsu unambiguously depends
on the last bunsetsu.
526
0.886
0.888
0.890
0.892
0.894
0.896
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.894
1.6G
p < 0.1
100M
25M
p < 0.01
6.3M
1.6M
400M
p < 0.1
25M
p < 0.01
6.3M
1.6M
100M
p < 0.1
6.3M
1.6M
25M
p < 0.1
6.3M
1.6M6.3M1.6M
with case frames
w/o case frames
Figure 4: Accuracy of Syntactic Analysis. (McNemar?s
test results are also shown under each data point.)
cludes McNemar?s test results. For instance, the dif-
ference between the corpus size of 1.6G and 100M
sentences is significant at the 90% level (p = 0.1),
but not significant at the 99% level (p = 0.01).
In Figure 4, ?w/o case frames? shows the accu-
racy of the rule-based syntactic parser KNP that does
not use case frames. Since the model described
in Section 4.1 assumes the existence of reasonable
case frames, when we used case frames constructed
from very small corpus, such as 1.6M and 6.3M sen-
tences, the accuracy was lower than that of the rule-
based syntactic parser. Moreover, when we tested
the model described in Section 4.1 without any case
frames, the accuracy was 0.885.
We confirmed that better performance was ob-
tained by using case frames constructed from larger
corpora, and the accuracy of 0.8945 was achieved
by using the case frames constructed from 1.6G sen-
tences. However the effect of the corpus size was
limited. This is because there are various causes
of dependency error and the case frame sparseness
problem is not serious for syntactic analysis.
We considered that generalized examples can
benefit for the accuracy of syntactic analysis, and
tried several models that utilize these examples.
However, we cannot confirm any improvement.
5.3.2 Accuracy of Case Structure Analysis
We conducted case structure analysis on 215 web
sentences in order to investigate the effect of cor-
pus size for case structure analysis. The case mark-
ers of topic marking phrases and clausal modifiers
5It corresponds to 0.877 in Kawahara and Kurohashi?s
(2007) evaluation metrics.
0.400
0.500
0.600
0.700
0.800
0.900
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.784
Figure 5: Accuracy of Case Structure Analysis.
Table 5: Corpus Sizes for Case Frame Construction and
Time for Syntactic and Case Structure Analysis.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 850 1244 1833 2696 3783 5553
were evaluated by comparing them with the gold
standard in the corpus. Figure 5 shows the experi-
mental results. We confirmed that the accuracy of
case structure analysis strongly depends on corpus
size for case frame construction.
5.3.3 Analysis Speed
Table 5 shows the time for analyzing syntactic
and case structure of 759 web sentences. Although
the time for analysis became longer by using case
frames constructed from a larger corpus, the growth
rate was smaller than the growth rate of the size for
case frames described in Table 4.
Since there is enough increase in accuracy of case
structure analysis, we can say that case frames con-
structed larger corpora are desirable for case struc-
ture analysis.
5.4 Zero Anaphora Resolution
5.4.1 Accuracy of Zero Anaphora Resolution
We used an anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences)
to evaluate zero anaphora resolution. We used first
51 documents for test and used the other 135 doc-
uments for calculating several probabilities. In the
51 test documents, 233 zero anaphora relations were
annotated between one of the mentions of the an-
tecedent and corresponding predicate that had zero
pronoun.
In order to concentrate on evaluation for zero
anaphora resolution, we used the correct mor-
527
0.00
0.10
0.20
0.30
0.40
0.50
1M 10M 100M 1000M
F
-m
ea
su
re
Corpus Size (Number of Sentences)
0.417
0.330
0.313
+NE,CT match
+ NE match
exact match
Figure 6: F-measure of Zero Anaphora Resolution.
phemes, named entities, syntactic structures and
coreference relations that were manually annotated.
Since correct coreference relations were given, the
number of created entities was the same between the
gold standard and the system output because zero
anaphora resolution did not create new entities.
The experimental results are shown in Figure 6, in
which F-measure was calculated by:
R = # of correctly recognized zero anaphora# of zero anaphora annotated in corpus ,
P = # of correctly recognized zero anaphora# of system outputted zero anaphora ,
F = 21/R + 1/P .
The upper line shows the performance using all
generalized examples, the middle line shows the
performance using only generalized NEs, and the
lower line shows the performance without using
any generalized examples. While generalized cat-
egories much improved the F-measure, generalized
NEs contributed little. This tendency is similar to
that of coverage of case frames for omitted argument
shown in Figure 2. Unlike syntactic and case struc-
ture analysis, the performance for the zero anaphora
resolution is quite low when using case frames con-
structed from small corpora, and we can say case
frames constructed from larger corpora are essential
for zero anaphora resolution.
5.4.2 Analysis Speed
Table 6 shows the time for resolving zero
anaphora in 51 web documents consisting of 278
sentences. The time for analysis became longer by
using case frames constructed from larger corpora,
Table 6: Corpus Sizes for Case Frame Construction and
Time for Zero Anaphora Resolution.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 538 545 835 1040 1646 2219
which tendency is similar to the growth of the time
for analyzing syntactic and case structure.
5.5 Discussion
Experimental results of both case structure analy-
sis and zero anaphora resolution show the effective-
ness of a larger corpus in case frame acquisition for
Japanese discourse analysis. Up to the corpus size
of 1.6 billion sentences, or 100 billion words, these
experimental results still show a steady increase in
performance. That is, we can say that the corpus
size of 1.6 billion sentences is not enough to obtain
case frames of sufficient coverage.
These results suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring unstructured statistics of a corpus,
such as n-grams, and co-occurrence counts; and for
complex NLP tasks such as case structure analysis
and zero anaphora resolution, the currently available
corpus size is not sufficient.
Therefore, to construct more wide-coverage case
frames by using a larger corpus and reveal howmuch
corpora would be required to obtain sufficient cov-
erage is considered as future work.
6 Conclusion
This paper has reported the effect of corpus size
on case frame acquisition for syntactic and case
structure analysis, and zero anaphora resolution in
Japanese. We constructed case frames from cor-
pora of six different sizes ranging from 1.6 million
to 1.6 billion sentences; and then applied these case
frames to Japanese syntactic and case structure anal-
ysis, and zero anaphora resolution. Experimental re-
sults showed better results were obtained using case
frames constructed from larger corpora, and the per-
formance showed no saturation even when the cor-
pus size was 1.6 billion sentences.
The findings suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring surface statistics of a corpus; and
for complex NLP tasks the currently available cor-
pus size is not sufficient.
528
References
Michaela Atterer and Hinrich Schu?tze. 2006. The ef-
fect of corpus size in combining supervised and un-
supervised training for disambiguation. In Proc. of
COLING-ACL?06, pages 25?32.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proc. of HLT?01.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proc. of ACL?01, pages 26?33.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP-CoNLL?07,
pages 858?867.
Ann Gledson and John Keane. 2008. Using web-search
results to measure word-group similarity. In Proc. of
COLING?08, pages 281?288.
David Graff. 2003. English Gigaword. Technical Report
LDC2003T05, Linguistic Data Consortium, Philadel-
phia, PA USA.
Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras
Rung, Istvan Szakadat, and Vikto Tron. 2004. Creat-
ing open language resources for Hungarian. In Proc.
of LREC?04, pages 203?210.
IREX Committee, editor. 1999. Proc. of the IREX Work-
shop.
Daisuke Kawahara and Sadao Kurohashi. 2006a. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proc. of HLT-
NAACL?06, pages 176?183.
Daisuke Kawahara and Sadao Kurohashi. 2006b.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC?06, pages
1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2007.
Probabilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proc. of EMNLP-
CoNLL?07, pages 306?314.
Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-
hashi. 2004. Toward text understanding: Integrat-
ing relevance-tagged corpora and automatically con-
structed case frames. In Proc. of LREC?04, pages
1833?1836.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistic, 29(3):333?347.
Taku Kudo and Hideto Kazawa. 2007. Web Japanese N-
gram version 1, published by Gengo Shigen Kyokai.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2:1:1?31.
Vinci Liu and James R. Curran. 2006. Web text corpus
for natural language processing. In Proc. of EACL?06,
pages 233?240.
Kikuo Maekawa. 2006. Kotonoha, the corpus develop-
ment project of the National Institute for Japanese lan-
guage. In Proc. of the 13th NIJL International Sympo-
sium, pages 55?62.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proc. of
HLT-NAACL?06, pages 152?159.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proc. of EMNLP-2003,
pages 176?183.
Preslav Nakov and Marti Hearst. 2005. A study of using
search engine page hits as a proxy for n-gram frequen-
cies. In Proc. of RANLP?05.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. of ACL-HLT?08, pages 452?460.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proc. of IJCNLP?08, pages 607?
612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING?08, pages 769?776.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-HLT?08,
pages 665?673.
The National Language Institute for Japanese Language.
2004. Bunruigoihyo. Dainippon Tosho, (In Japanese).
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Proc. of the
Corpus Linguistics, pages 601?606.
529
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 205?208,
Prague, June 2007. c?2007 Association for Computational Linguistics
Minimally Lexicalized Dependency Parsing
Daisuke Kawahara and Kiyotaka Uchimoto
National Institute of Information and Communications Technology,
3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan
{dk, uchimoto}@nict.go.jp
Abstract
Dependency structures do not have the infor-
mation of phrase categories in phrase struc-
ture grammar. Thus, dependency parsing
relies heavily on the lexical information of
words. This paper discusses our investiga-
tion into the effectiveness of lexicalization
in dependency parsing. Specifically, by re-
stricting the degree of lexicalization in the
training phase of a parser, we examine the
change in the accuracy of dependency re-
lations. Experimental results indicate that
minimal or low lexicalization is sufficient
for parsing accuracy.
1 Introduction
In recent years, many accurate phrase-structure
parsers have been developed (e.g., (Collins, 1999;
Charniak, 2000)). Since one of the characteristics of
these parsers is the use of lexical information in the
tagged corpus, they are called ?lexicalized parsers?.
Unlexicalized parsers, on the other hand, achieved
accuracies almost equivalent to those of lexicalized
parsers (Klein and Manning, 2003; Matsuzaki et al,
2005; Petrov et al, 2006). Accordingly, we can
say that the state-of-the-art lexicalized parsers are
mainly based on unlexical (grammatical) informa-
tion due to the sparse data problem. Bikel also in-
dicated that Collins? parser can use bilexical depen-
dencies only 1.49% of the time; the rest of the time,
it backs off to condition one word on just phrasal and
part-of-speech categories (Bikel, 2004).
This paper describes our investigation into the ef-
fectiveness of lexicalization in dependency parsing
instead of phrase-structure parsing. Usual depen-
dency parsing cannot utilize phrase categories, and
thus relies on word information like parts of speech
and lexicalized words. Therefore, we want to know
the performance of dependency parsers that have
minimal or low lexicalization.
Dependency trees have been used in a variety of
NLP applications, such as relation extraction (Cu-
lotta and Sorensen, 2004) and machine translation
(Ding and Palmer, 2005). For such applications, a
fast, efficient and accurate dependency parser is re-
quired to obtain dependency trees from a large cor-
pus. From this point of view, minimally lexicalized
parsers have advantages over fully lexicalized ones
in parsing speed and memory consumption.
We examined the change in performance of de-
pendency parsing by varying the degree of lexical-
ization. The degree of lexicalization is specified by
giving a list of words to be lexicalized, which appear
in a training corpus. For minimal lexicalization, we
used a short list that consists of only high-frequency
words, and for maximal lexicalization, the whole list
was used. Consequently, minimally or low lexical-
ization is sufficient for dependency accuracy.
2 Related Work
Klein and Manning presented an unlexicalized
PCFG parser that eliminated all the lexicalized pa-
rameters (Klein and Manning, 2003). They manu-
ally split category tags from a linguistic view. This
corresponds to determining the degree of lexicaliza-
tion by hand. Their parser achieved an F1 of 85.7%
for section 23 of the Penn Treebank. Matsuzaki et al
and Petrov et al proposed an automatic approach to
205
Dependency accuracy (DA) Proportions of words, except
punctuation marks, that are assigned the correct heads.
Root accuracy (RA) Proportions of root words that are cor-
rectly detected.
Complete rate (CR) Proportions of sentences whose depen-
dency structures are completely correct.
Table 1: Evaluation criteria.
splitting tags (Matsuzaki et al, 2005; Petrov et al,
2006). In particular, Petrov et al reported an F1 of
90.2%, which is equivalent to that of state-of-the-art
lexicalized parsers.
Dependency parsing has been actively studied in
recent years (Yamada and Matsumoto, 2003; Nivre
and Scholz, 2004; Isozaki et al, 2004; McDon-
ald et al, 2005; McDonald and Pereira, 2006;
Corston-Oliver et al, 2006). For instance, Nivre
and Scholz presented a deterministic dependency
parser trained by memory-based learning (Nivre and
Scholz, 2004). McDonald et al proposed an on-
line large-margin method for training dependency
parsers (McDonald et al, 2005). All of them per-
formed experiments using section 23 of the Penn
Treebank. Table 2 summarizes their dependency ac-
curacies based on three evaluation criteria shown in
Table 1. These parsers believed in the generalization
ability of machine learners and did not pay attention
to the issue of lexicalization.
3 Minimally Lexicalized Dependency
Parsing
We present a simple method for changing the de-
gree of lexicalization in dependency parsing. This
method restricts the use of lexicalized words, so it is
the opposite to tag splitting in phrase-structure pars-
ing. In the remainder of this section, we first de-
scribe a base dependency parser and then report ex-
perimental results.
3.1 Base Dependency Parser
We built a parser based on the deterministic algo-
rithm of Nivre and Scholz (Nivre and Scholz, 2004)
as a base dependency parser. We adopted this algo-
rithm because of its linear-time complexity.
In the algorithm, parsing states are represented by
triples ?S, I,A?, where S is the stack that keeps the
words being under consideration, I is the list of re-
DA RA CR
(Yamada and Matsumoto, 2003) 90.3 91.6 38.4
(Nivre and Scholz, 2004) 87.3 84.3 30.4
(Isozaki et al, 2004) 91.2 95.7 40.7
(McDonald et al, 2005) 90.9 94.2 37.5
(McDonald and Pereira, 2006) 91.5 N/A 42.1
(Corston-Oliver et al, 2006) 90.8 93.7 37.6
Our Base Parser 90.9 92.6 39.2
Table 2: Comparison of parser performance.
maining input words, and A is the list of determined
dependencies. Given an input word sequence, W ,
the parser is first initialized to the triple ?nil,W, ??1.
The parser estimates a dependency relation between
two words (the top elements of stacks S and I). The
algorithm iterates until the list I is empty. There are
four possible operations for a parsing state (where t
is the word on top of S, n is the next input word in
I , and w is any word):
Left In a state ?t|S, n|I,A?, if there is no depen-
dency relation (t ? w) in A, add the new de-
pendency relation (t ? n) into A and pop S
(remove t), giving the state ?S, n|I,A ? (t ?
n)?.
Right In a state ?t|S, n|I,A?, if there is no depen-
dency relation (n ? w) in A, add the new de-
pendency relation (n ? t) into A and push n
onto S, giving the state ?n|t|S, I,A?(n ? t)?.
Reduce In a state ?t|S, I,A?, if there is a depen-
dency relation (t ? w) in A, pop S, giving the
state ?S, I,A?.
Shift In a state ?S, n|I,A?, push n onto S, giving
the state ?n|S, I,A?.
In this work, we used Support Vector Machines
(SVMs) to predict the operation given a parsing
state. Since SVMs are binary classifiers, we used the
pair-wise method to extend them in order to classify
our four-class task.
The features of a node are the word?s lemma,
the POS/chunk tag and the information of its child
node(s). The lemma is obtained from the word form
using a lemmatizer, except for numbers, which are
replaced by ??num??. The context features are the
two preceding nodes of node t (and t itself), the two
succeeding nodes of node n (and n itself), and their
1We use ?nil? to denote an empty list and a|A to denote a
list with head a and tail A.
206
 87
 87.2
 87.4
 87.6
 87.8
 88
 88.2
 88.4
 0  1000  2000  3000  4000  5000
Ac
cur
acy
 (%)
Number of Lexicalized Words
Figure 1: Dependency accuracies on the WSJ while
changing the degree of lexicalization.
child nodes (lemmas and POS tags). The distance
between nodes n and t is also used as a feature.
We trained our models on sections 2-21 of the
WSJ portion of the Penn Treebank. We used sec-
tion 23 as the test set. Since the original treebank is
based on phrase structure, we converted the treebank
to dependencies using the head rules provided by
Yamada 2. During the training phase, we used intact
POS and chunk tags3. During the testing phase, we
used automatically assigned POS and chunk tags by
Tsuruoka?s tagger4(Tsuruoka and Tsujii, 2005) and
YamCha chunker5(Kudo and Matsumoto, 2001).
We used an SVMs package, TinySVM6,and trained
the SVMs classifiers using a third-order polynomial
kernel. The other parameters are set to default.
The last row in Table 2 shows the accuracies of
our base dependency parser.
3.2 Degree of Lexicalization vs. Performance
The degree of lexicalization is specified by giving
a list of words to be lexicalized, which appear in
a training corpus. For minimal lexicalization, we
used a short list that consists of only high-frequency
words, and for maximal lexicalization, the whole list
was used.
To conduct the experiments efficiently, we trained
2http://www.jaist.ac.jp/?h-yamada/
3In a preliminary experiment, we tried to use automatically
assigned POS and chunk tags, but we did not detect significant
difference in performance.
4http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/
5http://chasen.org/?taku-ku/software/yamcha/
6http://chasen.org/?taku-ku/software/TinySVM/
 83.6
 83.8
 84
 84.2
 84.4
 84.6
 84.8
 85
 0  1000  2000  3000  4000  5000
Ac
cur
acy
 (%)
Number of Lexicalized Words
Figure 2: Dependency accuracies on the Brown Cor-
pus while changing the degree of lexicalization.
our models using the first 10,000 sentences in sec-
tions 2-21 of the WSJ portion of the Penn Treebank.
We used section 24, which is usually used as the
development set, to measure the change in perfor-
mance based on the degree of lexicalization.
We counted word (lemma) frequencies in the
training corpus and made a word list in descending
order of their frequencies. The resultant list con-
sists of 13,729 words, and the most frequent word is
?the?, which occurs 13,252 times, as shown in Table
3. We define the degree of lexicalization as a thresh-
old of the word list. If, for example, this threshold is
set to 1,000, the top 1,000 most frequently occurring
words are lexicalized.
We evaluated dependency accuracies while
changing the threshold of lexicalization. Figure 1
shows the result. The dotted line (88.23%) repre-
sents the dependency accuracy of the maximal lex-
icalization, that is, using the whole word list. We
can see that the decrease in accuracy is less than
1% at the minimal lexicalization (degree=100) and
the accuracy of more than 3,000 degree slightly ex-
ceeds that of the maximal lexicalization. The best
accuracy (88.34%) was achieved at 4,500 degree and
significantly outperformed the accuracy (88.23%) of
the maximal lexicalization (McNemar?s test; p =
0.017 < 0.05). These results indicate that maximal
lexicalization is not so effective for obtaining accu-
rate dependency relations.
We also applied the same trained models to the
Brown Corpus as an experiment of parser adapta-
tion. We first split the Brown Corpus portion of
207
rank word freq. rank word freq.
1 the 13,252 1,000 watch 29
2 , 12,858
...
...
...
...
...
... 2,000 healthvest 12
100 week 261
...
...
...
...
...
... 3,000 whoop 7
500 estate 64
...
...
...
...
...
...
Table 3: Word list.
the Penn Treebank into training and testing parts in
the same way as (Roark and Bacchiani, 2003). We
further extracted 2,425 sentences at regular intervals
from the training part and used them to measure the
change in performance while varying the degree of
lexicalization. Figure 2 shows the result. The dot-
ted line (84.75%) represents the accuracy of maxi-
mal lexicalization. The resultant curve is similar to
that of the WSJ experiment7. We can say that our
claim is true even if the testing corpus is outside the
domain.
3.3 Discussion
We have presented a minimally or lowly lexical-
ized dependency parser. Its dependency accuracy is
close or almost equivalent to that of fully lexicalized
parsers, despite the lexicalization restriction. Fur-
thermore, the restriction reduces the time and space
complexity. The minimally lexicalized parser (de-
gree=100) took 12m46s to parse the WSJ develop-
ment set and required 111 MB memory. These are
36% of time and 45% of memory reduction, com-
pared to the fully lexicalized one.
The experimental results imply that training cor-
pora are too small to demonstrate the full potential
of lexicalization. We should consider unsupervised
or semi-supervised ways to make lexicalized parsers
more effective and accurate.
Acknowledgment
This research is partially supported by special coor-
dination funds for promoting science and technol-
ogy.
7In the experiment on the Brown Corpus, the difference be-
tween the best accuracy and the baseline was not significant.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL2000, pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric
Ringger. 2006. Multilingual dependency parsing using
bayes point machines. In Proceedings of HLT-NAACL2006,
pages 160?167.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL2004,
pages 423?429.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probabilistic synchronous dependency insertion gram-
mars. In Proceedings of ACL2005, pages 541?548.
Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao. 2004.
A deterministic word dependency analyzer enhanced with
preference learning. In Proceedings of COLING2004, pages
275?281.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of ACL2003, pages 423?
430.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of NAACL2001, pages
192?199.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Proceedings
of ACL2005, pages 75?82.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of EACL2006, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of ACL2005, pages 91?98.
Joakim Nivre and Mario Scholz. 2004. Deterministic de-
pendency parsing of English text. In Proceedings of COL-
ING2004, pages 64?70.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL2006, pages 433?
440.
Brian Roark and Michiel Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In Proceed-
ings of HLT-NAACL2003, pages 205?212.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirectional
inference with the easiest-first strategy for tagging sequence
data. In Proceedings of HLT-EMNLP2005, pages 467?474.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT2003, pages 195?206.
208
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 1?4,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WISDOM: A Web Information Credibility Analysis System 
Susumu Akamine?  Daisuke Kawahara?  Yoshikiyo Kato? 
Tetsuji Nakagawa?  Kentaro Inui?  Sadao Kurohashi??  Yutaka Kidawara? 
?National Institute of Information and Communications Technology 
? Graduate School of Informatics, Kyoto University 
{akamine, dk, ykato, tnaka, inui, kidawara}@nict.go.jp, kuro@i.kyoto-u.ac.jp 
 
 
 
 
Abstract 
We demonstrate an information credibility 
analysis system called WISDOM. The purpose 
of WISDOM is to evaluate the credibility of in-
formation available on the Web from multiple 
viewpoints. WISDOM considers the following 
to be the source of information credibility: in-
formation contents, information senders, and 
information appearances. We aim at analyzing 
and organizing these measures on the basis of 
semantics-oriented natural language processing 
(NLP) techniques. 
1. Introduction 
As computers and computer networks become 
increasingly sophisticated, a vast amount of in-
formation and knowledge has been accumulated 
and circulated on the Web. They provide people 
with options regarding their daily lives and are 
starting to have a strong influence on govern-
mental policies and business management. How-
ever, a crucial problem is that the information 
available on the Web is not necessarily credible. 
It is actually very difficult for human beings to 
judge the credibility of the information and even 
more difficult for computers. However, comput-
ers can be used to develop a system that collects, 
organizes, and relativises information and helps 
human beings view information from several 
viewpoints and judge the credibility of the in-
formation. 
Information organization is a promising en-
deavor in the area of next-generation Web search. 
The search engine Clusty provides a search result 
clustering1, and Cuil classifies a search result on 
the basis of query-related terms2. The persuasive 
technology research project at Stanford Universi-
ty discussed how websites can be designed to 
influence people?s perceptions (B. J. Fogg, 2003). 
However, as per our knowledge, no research has 
been carried out for supporting the human judg-
ment on information credibility and information 
organization systems for this purpose. 
In order to support the judgment of informa-
tion credibility, it is necessary to extract the 
background, facts, and various opinions and their 
                                                 
1 http://clusty.com/, http://clusty.jp/  
distribution for a given topic. For this purpose, 
syntactic and discourse structures must be ana-
lyzed, their types and relations must be extracted, 
and synonymous and ambiguous expressions 
should be handled properly.  
Furthermore, it is important to determine the 
identity of the information sender and his/her 
specialty as criteria for credibility, which require 
named entity recognition and total analysis of 
documents. 
In this paper, we describe an information cre-
dibility analysis system called WISDOM, which 
automatically analyzes and organizes the above 
aspects on the basis of semantically oriented 
NLP techniques. WISDOM currently operates 
over 100 million Japanese Web pages. 
2. Overview of WISDOM 
We consider the following three criteria for the 
judgment of information credibility.  
(1) Credibility of information contents,  
(2) Credibility of the information sender, and  
(3) Credibility estimated from the document 
style and superficial characteristics. 
In order to help people judge the credibility of 
information from these viewpoints, we have been 
developing an information analysis system called 
WISDOM. Figure 1 shows the analysis result of 
WISDOM on the analysis topic ?Is bio-ethanol 
good for the environment?? Figure 2 shows the 
system architecture of WISDOM. 
Given an analysis topic (query), WISDOM 
sends the query to the search engine TSUBAKI 
(Shinzato et al, 2008), and TSUBAKI returns a 
list of the top N relevant Web pages (N is usually 
set to 1000). 
Then, those pages are automatically analyzed, 
and major and contradictory expressions and eva-
luative expressions are extracted. Furthermore, 
the information senders of the Web pages, which 
were analyzed beforehand, are collected and the 
distribution is calculated. 
The WISDOM analysis results can be viewed 
from several viewpoints by changing the tabs 
using a Web browser. The leftmost tab, ?Sum-
mary,? shows the summary of the analysis, with 
major phrases and major/contradictory state-
ments first.  
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Query: ?Is bio-ethanol good for the environment?? Summary 
Figure 1. An analysis example of the information credibility analysis system WISDOM. 
 
 
Figure 2. System architecture of WISDOM. 
 
By referring to these phrases and statements, 
a user can grasp the important issues related to 
the topic at a glance. The pie diagram indicates 
the distribution of the information sender class 
spread over 1000 pages, such as company, indus-
try group, and government. The names of the 
information senders of the class can be viewed 
by placing the cursor over a class region. The last 
bar chart shows the distribution of positive and 
negative opinions related to the topic spread over 
1000 pages, for all and for each sender class. For 
example, with regard to ?Bio-ethanol,? we can 
see that the number of positive opinions is more 
than that of negative opinions, but it is the oppo-
site in the case of some sender classes. Several 
display units in the Summary tab are cursor sen-
sitive, providing links to more detailed informa-
tion (e.g., the page list including a major state-
Sender 
Opinion 
Search Result Major/Contradictory Expressions
2
ment, the page list of a sender class, and the page 
list containing negative opinions). 
The ?Search Result? tab shows the search re-
sult by TSUBAKI, i.e., ranking the relevant pag-
es according to the TSUBAKI criteria. The ?Ma-
jor/Contradictory Expressions? tab shows the list 
of major phrases and major/contradictory state-
ments about the given topic and the list of pages 
containing the specified phrase or statement. The 
?Opinion? tab shows the analysis result of the 
evaluative expressions, classified according to 
for/against, like/dislike, merit/demerit, and others, 
and it also shows the list of pages containing the 
specified type of evaluative expressions. The 
?Sender? tab classifies the pages according to the 
class of the information sender, for example, a 
user can view the pages created only by the gov-
ernment.  
Furthermore, the superficial characteristics of 
pages called as information appearance are ana-
lyzed beforehand and can be viewed in WIS-
DOM, such as whether or not the contact address 
is shown in the page and the privacy policy is on 
the page, the volume of advertisements on the 
page, the number of images, and the number of 
in/out links. 
As shown thus far, given an analysis topic, 
WISDOM collects and organizes the relevant 
information available on the Web and provides 
users with multi-faceted views. We believe that 
such a system can considerably support the hu-
man judgment of information credibility. 
3. Data Infrastructure  
We usually utilize 100 million Japanese Web 
pages as the analysis target. The Web pages have 
been converted into the standard formatted Web 
data, an XML format. The format includes sever-
al metadata such as URLs, crawl dates, titles, and 
in/out links. A text in a page is automatically 
segmented into sentences (note that the sentence 
boundary is not clear in the original HTML file), 
and the analysis results obtained by a morpholog-
ical analyzer, parser, and synonym analyzer are 
also stored in the standard format. Furthermore, 
the site operator, the page author, and informa-
tion appearance (e.g., contact address, privacy 
policy, volume of advertisements, and images) 
are automatically analyzed and stored in the 
standard format. 
4. Extraction of Major Expressions and 
Their Contradictions 
For the organization of information contents, 
WISDOM extracts and presents the major ex-
pressions and their contradictions on a given 
analysis topic (Kawahara et al, 2008). Major 
expressions are defined as expressions occurring 
at a high frequency in the set of Web pages on 
the analysis topic. They are classified into two: 
noun phrases and predicate-argument structures 
(statements). Contradictions are the predicate-
argument structures that contradict the major ex-
pressions. For the Japanese phrase yutori kyouiku 
(cram-free education), for example, tsumekomi 
kyouiku (cramming education) and ikiru chikara 
(life skills) are extracted as the major noun 
phrases; yutori kyouiku-wo minaosu (reexamine 
cram-free education) and gakuryokuga teika-suru 
(scholastic ability deteriorates), as the major pre-
dicate-argument structures; and gakuryoku-ga 
koujousuru (scholastic ability ameliorates), as its 
contradiction. This kind of summarized informa-
tion enables a user to grasp the facts and argu-
ments on the analysis topic available on the Web. 
We use 1000 Web pages for a topic retrieved 
from the search engine TSUBAKI. Our method 
of extracting major expressions and their contra-
dictions consists of the following steps: 
1. Extracting candidates of major expressions: 
The candidates of major expressions are ex-
tracted from each Web page in the search result. 
From the relevant sentences to the analysis topic 
that consist of approximately 15 sentences se-
lected from each Web page, compound nouns, 
parenthetical expressions, and predicate-
argument structures are extracted as the candi-
dates of the major expressions. 
2. Distilling major expressions: 
Simply presenting expressions at a high fre-
quency is not always information of high quality. 
This is because scattering synonymous expres-
sions such as karikyuramu (curriculum) and 
kyouiku katei (course of study) and entailing ex-
pressions such as IWC and IWC soukai (IWC 
plenary session), all of which occur frequently, 
hamper the understanding process of users. Fur-
ther, synonymous predicate-argument structures 
such as gakuryoku-ga teika-suru (scholastic 
ability deteriorates) and gakuryoku-ga sagaru 
(scholastic ability lowers) have the same problem. 
To overcome this problem, we distill major ex-
pressions by merging spelling variations with 
morphological analysis, merging synonymous 
expressions automatically acquired from an ordi-
nary dictionary and the Web, and merging ex-
pressions that can be entailed by another expres-
sion. 
3. Extracting contradictory expressions: 
Predicate-argument structures that negate the 
predicate of major ones and that replace the pre-
dicate of major ones with its antonym are ex-
tracted as contradictions. For example, gakuryo-
ku-ga teika-shi-nai (scholastic ability does not 
deteriorate) and gakuryokuga koujou-suru (scho-
lastic ability ameliorates) are extracted as the 
contradictions to gakuryoku-ga teikasuru (scho-
lastic ability deteriorates). This process is per-
formed using an antonym lexicon, which consists 
of approximately 2000 pairs; these pairs are ex-
tracted from an ordinary dictionary. 
5. Extraction of Evaluative Information 
The extraction and classification of evaluative 
information from texts are important tasks with 
3
many applications and they have been actively 
studied recently (Pang and Lee, 2008). Most pre-
vious studies on opinion extraction or sentiment 
analysis deal with only subjective and explicit 
expressions. For example, Japanese sentences 
such as watashi-wa apple-ga sukida (I like ap-
ples) and kono seido-ni hantaida (I oppose the 
system) contain evaluative expressions that are 
directly expressed with subjective expressions. 
However, sentences such as kono shokuhin-wa 
kou-gan-kouka-ga aru (this food has an anti-
cancer effect) and kono camera-wa katte 3-ka-de 
kowareta (this camera was broken 3 days after I 
bought it) do not contain subjective expressions 
but contain negative evaluative expressions. 
From the viewpoint of information credibility, it 
appears important to deal with a wide variety of 
evaluative information including such implicit 
evaluative expressions (Nakagawa et al, 2008). 
A corpus annotated with evaluative informa-
tion was developed for evaluative information 
analysis studies. Fifty topics such as ?Bio-
ethanol? and ?Pension plan? were chosen. For 
each topic, 200 sentences containing the topic 
word were collected from the Web to construct 
the corpus totaling 10,000 sentences. For each 
sentence, annotators judged whether or not the 
sentence contained evaluative expressions. When 
evaluative expressions were identified, the evalu-
ative expressions, their holders, their sentiment 
polarities (positive or negative), and their relev-
ance to the topic were annotated. 
We developed an automatic analyzer of evalu-
ative information using the corpus. We per-
formed experiments of sentiment polarity classi-
fication using Support Vector Machines. Word 
forms, POS tags, and sentiment polarities from 
an evaluative word dictionary of all the words in 
evaluative expressions were used as features, and 
an accuracy of 83% was obtained. From the error 
analysis, we found that it was difficult to classify 
domain-specific evaluative expressions; we are 
now planning the automatic acquisition of evalu-
ative word dictionaries. 
6. Information Sender Analysis 
The source of information (or information sender) 
is one of the important elements when judging the 
credibility of information. It is rather easy for human 
beings to identify the information sender of a Web 
page. When reading a Web page, whether it is deli-
berate or not, we attribute some characteristics to the 
information sender and accordingly form our atti-
tudes toward the information. However, the state-of-
the-art search engines do not provide facilities to 
organize a vast amount of information on the basis 
of the information sender. If we can organize the 
information on a topic on the basis of who or what 
type the information sender is, it would enable the 
user to grasp an overview of the topic or to judge the 
credibility of relevant information. 
WISDOM automatically identifies the site op-
erators of Web pages and classifies them into 
predefined categories of information sender 
called information sender class. A site operator 
of a Web page is the governing body of a website 
on which the page is published. The information 
sender class categorizes the information sender 
on the basis of axes such as individuals vs. or-
ganizations and profit vs. nonprofit organizations. 
The list below shows the categories of informa-
tion sender class. 
 
 
 
1. Organization (cont?d) 
  (c) Press 
    i. Broadcasting Station 
    ii. Newspaper 
    iii. Publisher 
2. Individual 
  (a) Real Name 
  (b) Anonymous,  
Screen Name 
 
1. Organization 
  (a) Profit Organization 
    i. Company 
    ii. Industry Group 
  (b) Nonprofit Organization 
    i. Academic Society 
    ii. Government 
    iii. Political Organization 
    iv. Public Service Corp., 
         Nonprofit Organization 
    v. University 
    vi. Voluntary Association 
   vii. Education Institution
WISDOM allows the user to organize the in-
formation on the basis of the information sender 
class assigned to each Web page. Technical de-
tails of the information sender analysis employed 
in WISDOM can be found in (Kato et al, 2008). 
7. Conclusions 
This paper has described an information analy-
sis system called WISDOM. As shown in this pa-
per, WISDOM already provides a reasonably nice 
organized view for a given topic and can serve as a 
useful tool for handling informational queries and 
for supporting human judgment of information 
credibility. WISDOM is freely available at 
http://wisdom-nict.jp/.  
References 
B. J. Fogg. 2003. Persuasive Technology: Using Com-
puters to Change What We Think and Do (The Mor-
gan Kaufmann Series in Interactive Technologies). 
Morgan Kaufmann. 
K. Shinzato, T. Shibata, D. Kawahara, C. Hashimoto, 
and S. Kurohashi 2008. TSUBAKI: An open search 
engine infrastructure for developing new information 
access methodology. In Proceedings of IJCNLP2008. 
D. Kawahara, S. Kurohashi, and K. Inui 2008. Grasping 
major statements and their contradictions toward in-
formation credibility analysis of web contents. In 
Proceedings of  WI?08. 
B. Pang and L. Lee 2008. Opinion mining and senti-
ment analysis, Foundations and Trends in Informa-
tion Retrieval, Volume 2, Issue 1-2, 2008. 
T. Nakagawa, T. Kawada, K. Inui, and S. Kurohashi 
2008. Extracting subjective and objective evaluative 
expressions from the web. In Proceedings of 
ISUC2008. 
Y. Kato, D. Kawahara, K. Inui, S. Kurohashi, and T. 
Shibata 2008. Extracting the author of web pages. In 
Proceedings of WICOW2008. 
4
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108?116,
Paris, October 2009. c?2009 Association for Computational Linguistics
Capturing Consistency between Intra-clause and Inter-clause Relations
in Knowledge-rich Dependency and Case Structure Analysis
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We present a method for dependency and
case structure analysis that captures the
consistency between intra-clause relations
(i.e., case structures or predicate-argument
structures) and inter-clause relations. We
assess intra-clause relations on the basis
of case frames and inter-clause relations
on the basis of transition knowledge be-
tween case frames. Both knowledge bases
are automatically acquired from a mas-
sive amount of parses of a Web corpus.
The significance of this study is that the
proposed method selects the best depen-
dency and case structure that are con-
sistent within each clause and between
clauses. We confirm that this method con-
tributes to the improvement of dependency
parsing of Japanese.
1 Introduction
The approaches of dependency parsing basically
assess the likelihood of a dependency relation be-
tween two words or phrases and subsequently
collect all the assessments for these pairs as the
dependency parse of the sentence. To improve
dependency parsing, it is important to consider
as broad a context as possible, rather than a
word/phrase pair.
In the recent evaluation workshops (shared
tasks) of multilingual dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007),
transition-based and graph-based methods
achieved good performance by incorporating rich
context. Transition-based dependency parsers
consider the words following the word under
consideration as features of machine learning
(Kudo and Matsumoto, 2002; Nivre and Scholz,
2004; Sassano, 2004). Graph-based dependency
parsers consider sibling and grandparent nodes,
i.e., second-order and higher-order features
(McDonald and Pereira, 2006; Carreras, 2007;
Nakagawa, 2007).
It is desirable to consider a wider-range phrase,
clause, or a whole sentence, but it is difficult to
judge whether the structure of such a wide-range
expression is linguistically correct. One of the rea-
sons for this is the scarcity of the knowledge re-
quired to make such a judgment. When we use
the Penn Treebank (Marcus et al, 1993), which is
one of the largest corpora among the available ana-
lyzed corpora, as training data, even bi-lexical de-
pendencies cannot be learned sufficiently (Bikel,
2004). To circumvent such scarcity, for instance,
Koo et al (2008) proposed the use of word classes
induced by clustering words in a large raw cor-
pus. They succeeded in improving the accuracy of
a higher-order dependency parser.
On the other hand, some researchers have pro-
posed other approaches where linguistic units such
as predicate-argument structures (also known as
case structures and logical forms) are considered
instead of arbitrary nodes such as sibling nodes.
To solve the problem of knowledge scarcity, they
learned knowledge of such predicate-argument
structures from a very large number of automat-
ically analyzed corpora (Abekawa and Okumura,
2006; Kawahara and Kurohashi, 2006b). While
Abekawa and Okumura (2006) used only co-
occurrence statistics of verbal arguments, Kawa-
hara and Kurohashi (2006b) assessed predicate-
argument structures by checking case frames,
which are semantic frames that are automatically
compiled for each predicate sense from a large raw
corpus. These methods outperformed the accuracy
of supervised dependency parsers.
In such linguistically-motivated approaches,
well-formedness within a clause was considered,
but coherence between clauses was not con-
sidered. Even if intra-clause relations (i.e., a
predicate-argument structure within a clause) are
108
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
( b 1 )
( a 1 )
( c 1 )
( a 2 )
( b 2 )
( c 2 )
( a 3 )
( b 3 )
( c 3 )
3 3 3
3 3 3
3 3 3
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
Figure 1: Possible dependency and case structures of sentence (1).
optimized, they might not be optimum when look-
ing at clause pairs or sequences. To improve the
accuracy of dependency parsing, we propose a
method for dependency and case structure analy-
sis that considers the consistency between intra-
clause and inter-clause relations. This method an-
alyzes intra-clause relations on the basis of case
frames and inter-clause relations on the basis of
transition knowledge between case frames. These
two knowledge sources are automatically acquired
from a massive amount of parses of a Web corpus.
The contributions of this paper are two-fold.
First, we acquire transition knowledge not be-
tween verbs or verb phrases but between case
frames, which are semantically disambiguated
representations. Second, we incorporate the tran-
sition knowledge into dependency and case struc-
ture analysis to capture the consistency between
intra-clause and inter-clause relations.
The remainder of this paper is organized as
follows. Section 2 illustrates our idea. Section
3 describes a method for acquiring the transi-
tion knowledge. Section 4 explains the proposed
method of incorporating the acquired transition
knowledge into a probabilistic model of depen-
dency and case structure analysis. Section 5 re-
ports experimental results. Section 6 gives the
conclusions.
109
2 Idea of Capturing Consistency between
Intra-clause and Inter-clause Relations
We propose a method for generative dependency
parsing that captures the consistency between
intra-clause and inter-clause relations.
Figure 1 shows the ambiguities of dependency
and case structure of pointo-wa (point-TOP) in the
following sentence:
(1) pointo-wa,
point-TOP
hitotsu-ni
one-DAT
matomete
pack
takuhaibin-de
courier-CMI
okuru
send
koto-desu
be that
(The point is that (we) pack (one?s bag-
gage) and send (it) using courier service.)
The correct structure is (c1), which is surrounded
by the dotted rectangle. Structures (c2), (c3)
and so on have the same dependency structure as
(c1), but have incorrect case structures, in which
incorrect case frames are selected. Note that
matomeru:5, okuru:6 and so on in the figure rep-
resent the IDs of the case frames.
The parser of Kawahara and Kurohashi (2006b)
(and also conventional Japanese parsers) erro-
neously analyzes the head of pointo-wa (point-
TOP)1 as matomete (organize), whereas the cor-
rect head is koto-desu (be that), as shown in struc-
ture (a1) in Figure 1.
This error is caused by the incorrect selection
of the case frame matomeru:6 (organize), which is
shown in Table 1. This case frame locally matches
the input predicate-argument structure ?pointo-wa
hitotsu-ni matomeru? (organize points). There-
fore, this method considers only intra-clause re-
lations, and falls into local optimum.
If we consider the wide range of two clauses,
this error can be corrected. In structure (a1)
in Figure 1, the generative probability of case
frame transition, P (matomeru:6|okuru:6), is con-
sidered. This probability value is very low, be-
cause there are few relations between the case
frame matomeru:6 (organize) and the case frame
okuru:6 (send baggage) in corpora.
Consequently, structure (c1) is chosen as the
correct one, where both intra-clause and inter-
clause relations can be interpreted by the case
1In this paper, we use the following abbreviations:
NOM (nominative), ACC (accusative), ABL (ablative),
CMI (comitative) and TOP (topic marker).
Table 1: Case frame examples for matomeru and
okuru. ?CS? represents case slot. Argument words
are written only in English. ?<num>? represents
the class of numerals.
case frame ID CS example words
... ... ...
matomeru:5
(pack)
ga I, person, ...
wo baggage, luggage, variables, ...
ni <num>, pieces, compact, ...
matomeru:6
(organize)
ga doctor, ...
wo point, singularity, ...
ni <num>, pieces, below, ...
... ... ...
okuru:1
(send)
ga person, I, ...
wo mail, message, information, ...
ni friend, address, direction, ...
de mail, post, postage, ...
... ... ...
okuru:6
(send)
ga woman, ...
wo baggage, supply, goods, ...
ni person, Japan, parental house, ...
de mail, post, courier, ...
... ... ...
frames and the transition knowledge between case
frames.
3 Acquiring Transition Knowledge
between Case Frames
We automatically acquire large-scale transition
knowledge of inter-clause relations from a raw
corpus. The following two points are different
from previous studies on the acquisition of inter-
clause knowledge such as entailment/synonym
knowledge (Lin and Pantel, 2001; Torisawa, 2006;
Pekar, 2006; Zanzotto et al, 2006), verb relation
knowledge (Chklovski and Pantel, 2004), causal
knowledge (Inui et al, 2005) and event relation
knowledge (Abe et al, 2008):
? the unit of knowledge is disambiguated and
generalized
The unit in previous studies was a verb or a
verb phrase, in which verb sense ambiguities
still remain. Our unit is case frames that are
semantically disambiguated.
? the variation of relations is not limited
Although previous studies focused on lim-
ited kinds of semantic relations, we compre-
hensively collect generic relations between
clauses.
110
In this section, we first describe our unit of
transition knowledge, case frames, briefly. We
then detail the acquisition method of the transition
knowledge, and report experimental results. Fi-
nally, we refer to related work to the acquisition of
such knowledge.
3.1 The Unit of Transition Knowledge: Case
Frames
In this paper, we regard case frames as the unit of
transition knowledge. Case frames are constructed
from unambiguous structures and are semantically
clustered according to their meanings and usages.
Therefore, case frames can be a less ambiguous
and more generalized unit than a verb and a verb
phrase. Due to these characteristics, case frames
are a suitable unit for acquiring transition knowl-
edge and weaken the influence of data sparseness.
3.1.1 Automatic Construction of Case
Frames
We employ the method of Kawahara and Kuro-
hashi (2006a) to automatically construct case
frames. In this section, we outline the method for
constructing the case frames.
In this method, a large raw corpus is auto-
matically parsed, and the case frames are con-
structed from argument-head examples in the re-
sulting parses. The problems in automatic case
frame construction are syntactic and semantic am-
biguities. In other words, the parsing results in-
evitably contain errors, and verb senses are intrin-
sically ambiguous. To cope with these problems,
case frames are gradually constructed from reli-
able argument-head examples.
First, argument-head examples that have no
syntactic ambiguity are extracted, and they are dis-
ambiguated by a pair comprising a verb and its
closest case component. Such pairs are explic-
itly expressed on the surface of the text and can be
considered to play an important role in conveying
the meaning of a sentence. For instance, exam-
ples are distinguished not by verbs (e.g., ?tsumu?
(load/accumulate)), but by pairs (e.g., ?nimotsu-
wo tsumu? (load baggage) and ?keiken-wo tsumu?
(accumulate experience)). argument-head exam-
ples are aggregated in this manner, and they yield
basic case frames.
Thereafter, the basic case frames are clustered
in order to merge similar case frames, including
similar case frames that are made from scram-
bled sentences. For example, since ?nimotsu-
wo tsumu? (load baggage) and ?busshi-wo tsumu?
(load supply) are similar, they are clustered to-
gether. The similarity is measured by using a dis-
tributional thesaurus based on the study described
in Lin (1998).
3.2 Acquisition of Transition Knowledge
from Large Corpus
To acquire the transition knowledge, we collect the
clause pairs in a large raw corpus that have a de-
pendency relation and represent them as pairs of
case frames. For example, from the following sen-
tence, a case frame pair, (matomeru:5, okuru:6), is
extracted.
(2) nimotsu-wo
baggage-ACC
matomete,
pack
takuhaibin-de
courier-CMI
okutta
sent
(packed one?s baggage and sent (it) using
courier service)
These case frames are determined by applying
a conventional case structure analyzer (Kawa-
hara and Kurohashi, 2006b), which selects the
case frames most similar to the input expres-
sions ?nimotu-wo matomeru? (pack baggage) and
?takuhaibin-de okuru? (send with courier service)
from among the case frames of matomeru (or-
ganize/settle/pack/...) and okuru (send/remit/see
off/...); some of the case frames of matomeru and
okuru are listed in Table 1.
We adopt the following steps to acquire the tran-
sition knowledge between case frames:
1. Apply dependency and case structure analy-
sis to assign case frame IDs to each clause in
a large raw corpus.
2. Collect clause pairs that have a dependency
relation, and represent them as pairs of case
frame IDs.
3. Count the frequency of each pair of case
frame IDs; these statistics are used in the
analysis described in Section 4.
At step 2, we collect both syntactically ambigu-
ous and unambiguous relations in order to allevi-
ate data sparseness. The influence of a small num-
ber of dependency parsing errors would be hidden
by a large number of correct (unambiguous) rela-
tions.
111
Table 2: Examples of automatically acquired transition knowledge between case frames.
pairs of case frame IDs meaning freq.
(okuru:1, okuru:6) (send mails, send baggage) 186
(aru:1, okuru:6) (have, send baggage) 150
(suru:1, okuru:6) (do, send baggage) 134
(issyoda:10, okuru:6) (get together, send baggage) 118
(kaku:1, okuru:6) (write, send baggage) 115
... ... ...
(matomeru:5, okuru:6) (pack, send baggage) 12
(dasu:3, okuru:6) (indicate, send baggage) 12
... ... ...
3.3 Experiments of Acquiring Transition
Knowledge between Case Frames
To obtain the case frames and the transition knowl-
edge between case frames, we first built a Japanese
Web corpus by using the method of Kawahara and
Kurohashi (2006a). We first crawled 100 million
Japanese Web pages, and then, we extracted and
unduplicated Japanese sentences from the Web
pages. Consequently, we developed a Web corpus
consisting of 1.6 billion Japanese sentences.
Using the procedure of case frame construction
presented in Section 3.1.1, we constructed case
frames from the whole Web corpus. They con-
sisted of 43,000 predicates, and the average num-
ber of case frames for a predicate was 22.2.
Then, we acquired the transition knowledge be-
tween case frames using 500 million sentences of
the Web corpus. The resulting knowledge con-
sisted of 108 million unique case frame pairs. Ta-
ble 2 lists some examples of the acquired transition
knowledge. In the acquired transition knowledge,
we can find various kinds of relation such as en-
tailment, cause-effect and temporal relations.
Let us compare this result with the results of
previous studies. For example, Chklovski and
Pantel (2004) obtained 29,165 verb pairs for sev-
eral semantic relations in VerbOcean. The tran-
sition knowledge acquired in this study is several
thousand times larger than that in VerbOcean. It
is very difficult to make a meaningful compari-
son, but it can be seen that we have succeeded in
acquiring generic transition knowledge on a large
scale.
3.4 Related Work
In order to realize practical natural language pro-
cessing (NLP) systems such as intelligent dialog
systems, a lot of effort has been made to develop
world knowledge or inference knowledge. For ex-
ample, in the CYC (Lenat, 1995) and Open Mind
(Stork, 1999) projects, such knowledge has been
obtained manually, but it is difficult to manually
develop broad-coverage knowledge that is suffi-
cient for practical use in NLP applications.
On the other hand, the automatic acquisition of
such inference knowledge from corpora has at-
tracted much attention in recent years. First, se-
mantic knowledge between entities has been au-
tomatically obtained (Girju and Moldovan, 2002;
Ravichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). For example, Pantel and Pennac-
chiotti (2006) proposed the Espresso algorithm,
which iteratively acquires entity pairs and extrac-
tion patterns using reciprocal relationship between
entities and patterns.
As for the acquisition of the knowledge be-
tween events or clauses, which is most relevant
to this study, many approaches have been adopted
to acquire entailment knowledge. Lin and Pan-
tel (2001) and Szpektor and Dagan (2008) learned
entailment rules based on distributional similar-
ity between instances that have a relation to a
rule. Torisawa (2006) extracted entailment knowl-
edge using coordinated verb pairs and noun-verb
co-occurrences. Pekar (2006) also collected en-
tailment knowledge with discourse structure con-
straints. Zanzotto et al (2006) obtained entailment
knowledge using nominalized verbs.
There have been some studies on relations other
than entailment relations. Chklovski and Pan-
tel (2004) obtained verb pairs that have one of
five semantic relations by using a search engine.
Inui et al (2005) classified the occurrences of
the Japanese connective marker tame. Abe et al
112
(2008) learned event relation knowledge for two
semantic relations. They first gave seed pairs of
verbs or verb phrases and extracted the patterns
that matched these seed pairs. Subsequently, by
using the Espresso algorithm (Pantel and Pennac-
chiotti, 2006), this process was iterated to augment
both instances and patterns. The acquisition unit
in these studies was a verb or a verb phrase.
In contrast to these studies, we obtained generic
transition knowledge between case frames without
limiting target semantic relations.
4 Incorporating Transition Knowledge
into Dependency and Case Structure
Analysis
We employ the probabilistic generative model of
dependency and case structure analysis (Kawahara
and Kurohashi, 2006b) as a base model. We incor-
porate the obtained transition knowledge into this
base parser.
Our model assigns a probability to each possi-
ble dependency structure, T , and case structure,
L, of the input sentence, S, and outputs the de-
pendency and case structure that have the highest
probability. In other words, the model selects the
dependency structure T best and the case structure
Lbest that maximize the probability P (T,L|S) or
its equivalent, P (T,L, S), as follows:
(T best, Lbest) = argmax (T,L)P (T,L|S)
= argmax (T,L)P (T,L, S)P (S)
= argmax (T,L)P (T,L, S). (1)
The last equation follows from the fact that P (S)
is constant.
In the model, a clause (or predicate-argument
structure) is considered as a generation unit and
the input sentence is generated from the end of the
sentence. The probability P (T,L, S) is defined
as the product of the probabilities of generating
clauses Ci as follows:
P (T,L, S) = ? Ci?SP (Ci|Ch), (2)
where Ch is the modifying clause of Ci. Since the
Japanese language is head final, the main clause at
the end of a sentence does not have a modifying
head; we account for this by assuming Ch = EOS
(End Of Sentence).
The probability P (Ci|Ch) is defined in a man-
ner similar to that in Kawahara and Kurohashi
(2006b). However, the difference between the
probability in the above-mentioned study and that
in our study is the generative probability of the
case frames, i.e., the probability of generating a
case frame CF i from its modifying case frame
CF h. The base model approximated this proba-
bility as the product of the probability of gener-
ating a predicate vi from its modifying predicate
vh and the probability of generating a case frame
CF i from the predicate vi as follows:
P (CF i|CF h) ?
P (vi|vh)? P (CF i|vi). (3)
Our proposed model directly estimates the proba-
bility P (CF i|CF h) and considers the transition
likelihood between case frames. This probabil-
ity is calculated from the transition knowledge be-
tween case frames using maximum likelihood.
In practice, to avoid the data sparseness prob-
lem, we interpolate the probability P (CF i|CF h)
with the probability of generating predicates,
P (vi|vh), as follows:
P ?(CF i|CF h) ?
?P (CF i|CF h) + (1? ?)P (vi|vh), (4)
where ? is determined using the frequencies of the
case frame pairs, (CF i, CF h), in the same man-
ner as in Collins (1999).
5 Experiments
We evaluated the dependency structures that were
output by our new dependency parser. The case
frames used in these experiments are the same as
those described in Section 3.3, which were au-
tomatically constructed from 1.6 billion Japanese
sentences obtained from the Web.
In this study, the parameters related to unlexi-
cal types were calculated from the Kyoto Univer-
sity Text Corpus, which is a small tagged corpus
of newspaper articles, and lexical parameters were
obtained from a large Web corpus. To evaluate the
effectiveness of our model, our experiments were
conducted using sentences obtained from the Web.
As a test corpus, we used 759 Web sentences2,
which were manually annotated using the same
criteria as those in the case of the Kyoto Univer-
sity Text Corpus. We also used the Kyoto Univer-
sity Text Corpus as a development corpus to op-
timize some smoothing parameters. The system
2The test set was not used to construct case frames and
estimate probabilities.
113
Table 3: The dependency accuracies in our experiments.
syn syn+case syn+case+cons
all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%)
NP?VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%)
NP?NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%)
VP?VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%)
VP?NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%)
input was automatically tagged using the JUMAN
morphological analyzer 3.
We used two baseline systems for the purposes
of comparison: a rule-based dependency parser
(Kurohashi and Nagao, 1994) and the probabilistic
generative model of dependency and case struc-
ture analysis (Kawahara and Kurohashi, 2006b)4.
We use the above-mentioned case frames also in
the latter baseline parser, which also requires au-
tomatically constructed case frames.
5.1 Evaluation of Dependency Structures
We evaluated the obtained dependency structures
in terms of phrase-based dependency accuracy ?
the proportion of correct dependencies out of all
dependencies5.
Table 3 lists the dependency accuracies. In this
table, ?syn? represents the rule-based dependency
parser, ?syn+case? represents the probabilistic
parser of syntactic and case structure (Kawahara
and Kurohashi, 2006b)6, and ?syn+case+cons?
represents our proposed model. In the table, the
dependency accuracies are classified into four cat-
egories on the basis of the phrase classes (VP:
verb phrase7 and NP: noun phrase) of a dependent
and its head. The parser ?syn+case+cons? signif-
icantly outperformed the two baselines for ?all?
(McNemar?s test; p < 0.05). In particular, the ac-
curacy of the intra-clause (predicate-argument) re-
lations (?NP?VP?) was improved by 1.5% from
?syn? and by 0.4% from ?syn+case.? These im-
3http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
4http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
5Since Japanese is head-final, the second to last phrase
unambiguously depends on the last phrase. However, we in-
clude such dependencies into our evaluation as in most of
previous studies.
6The accuracy described in Kawahara and Kurohashi
(2006b) is different from that of this paper due to the different
evaluation measure excluding the unambiguous dependencies
of the second last phrases.
7VP includes not only verbs but also adjectives and nouns
with copula.
provements are due to the incorporation of the
transition knowledge into syntactic/case structure
analysis.
In order to compare our results with a state-of-
the-art discriminative dependency parser, we in-
put the test corpus into an SVM-based Japanese
dependency parser, CaboCha8(Kudo and Mat-
sumoto, 2002), which was trained using the Kyoto
University Text Corpus. Its dependency accuracy
was 88.6% (4,540/5,122), which is close to that of
?syn.? This low accuracy is attributed to the lack
of knowledge of both intra-clause and inter-clause
relations. Another cause of the low accuracy is the
out-of-domain training corpus. In other words, the
parser was trained on a newspaper corpus, while
the test corpus was obtained from theWeb because
a tagged Web corpus that is large enough to train
a supervised parser is not available.
5.2 Discussions
Figure 2 shows some improved analyses; here, the
dotted lines represent the results of the analysis
performed using the baseline ?syn + case,? and
the solid lines represent the analysis performed
using the proposed method, ?syn+case+cons.?
These sentences are incorrectly analyzed by the
baseline but correctly analyzed by the proposed
method. For example, in sentence (a), the head of
gunegunemichi-wo (winding road-ACC) was cor-
rectly analyzed as yurareru (be jolted). This is
because the case frame of ?basu-ni yurareru? (be
jolted by bus) is likely to generate tatsu (stand)
that does not take the wo (ACC) slot. In this man-
ner, by considering the transition knowledge be-
tween case frames, the selection of case frames
became accurate, and thus, the accuracy of the
dependencies within clauses (predicate-argument
structures) was improved.
In the case of the dependencies between pred-
icates (VP?VP), however, only small improve-
8http://chasen.org/?taku/software/
cabocha/
114
? ?(a) gunegunemichi-wo tattamama basu-ni yurareru toko-wo kakugoshimashita.
winding road-ACC stand bus-DAT be jolted (that)-ACC be resolved
(be resolved to be jolted standing on the bus by the winding road.)
??(b) nanika-wo eru tame-ni suteta mono-nimo miren-wo nokoshiteiru.
something-ACC get for discarded thing-also lingering desire-ACC retain
(retain a lingering desire also for the thing that was discarded to get something.)
??(c) senbei-no hako-wa, kankaku-wo akete chinretsusareteiruno-ga mata yoi.
rice cracker-GEN box-TOP interval-ACC place be displayed-NOM also good
(It is also good that boxes of rice cracker are displayed placing an interval.)
Figure 2: Improved examples.
? ?(d) ketsuron-kara itteshimaeba, kaitearukoto-wa machigattenaishi, juyouna kotodato-wa wakaru.
conclusion-ABL say content-TOP not wrong important (that)-TOP understand
(Saying from conclusions, the content is not wrong and (I) understand that (it) is important)
Figure 3: An erroneous example.
ments were achieved by using the transition
knowledge between case frames. This is mainly
because the heads of the predicates are intrinsi-
cally ambiguous in many cases.
For example, in sentence (d) in Figure 3, the
correct head of itteshimaeba (say) is wakaru (un-
derstand) as designated by the solid line, but our
model incorrectly judged the head to be machigat-
teinaishi, (not wrong) as designated by the dotted
line. However, in this case, both the phrases that
are being modified are semantically related to the
modifier. To solve this problem, it is necessary to
re-consider the evaluation metrics of dependency
parsing.
6 Conclusion
In this paper, we have described a method for ac-
quiring the transition knowledge of inter-clause re-
lations and a method for incorporating this knowl-
edge into dependency and case structure analy-
sis. The significance of this study is that the pro-
posed parsing method selects the best dependency
and case structures that are consistent within each
clause and between clauses. We confirmed that
this method contributed to the improvement of the
dependency parsing of Japanese.
The case frames that are acquired from 1.6 bil-
lion Japanese sentences have been made freely
available to the public9. In addition, we are prepar-
ing to make the acquired transition knowledge ac-
cessible on the Web.
In future, we will investigate the iteration of
knowledge acquisition and parsing based on the
acquired knowledge. Since our parser is a gener-
ative model, we are expecting a performance gain
by the iteration. Furthermore, we would like to ex-
plore the use of the transition knowledge between
case frames to improve NLP applications such as
recognizing textual entailment (RTE) and sentence
generation.
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning
cooccurrence patterns and fertilizing cooccurrence
samples with verbal nouns. In Proceedings of IJC-
NLP2008, pages 497?504.
Takeshi Abekawa and Manabu Okumura. 2006.
Japanese dependency parsing using co-occurrence
information and a combination of case elements. In
Proceedings of COLING-ACL2006, pages 833?840.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
9http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/caseframe-e.html
115
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMNLP-CoNLL2007 Shared Task, pages 957?
961.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP2004, pages
33?40.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Roxana Girju and Dan Moldovan. 2002. Mining an-
swers for causation questions. In Proceedings of
AAAI Spring Symposium.
Takashi Inui, Kentaro Inui, and Yuji Matsumoto.
2005. Acquiring causal knowledge from text us-
ing the connective marker tame. ACM Transactions
on Asian Language Information Processing (ACM-
TALIP), 4(4):435?474.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using
high-performance computing. In Proceedings of
LREC2006.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceed-
ings of HLT-NAACL2006, pages 176?183.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08:HLT, pages 595?603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of CoNLL2002, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507?534.
Douglas B. Lenat. 1995. CYC: A large-scale invest-
ment in knowledge infrastructure. Communications
of the ACM, 38(11):32?38.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL2006, pages 81?88.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In Proceedings of
EMNLP-CoNLL2007 Shared Task, pages 952?956.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING2004, pages 64?70.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL2007, pages 915?932.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL2006, pages 113?120.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of HLT-NAACL2006,
pages 49?56.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL2002, pages 41?47.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of COLING2004,
pages 8?14.
David G. Stork. 1999. Character and document re-
search in the open mind initiative. In Proceedings
of International Conference on Document Analysis
and Recognition, pages 1?12.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING2008, pages 849?856.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using Japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL2006, pages 57?64.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of COLING-
ACL2006, pages 849?856.
116
Coling 2010: Poster Volume, pages 534?542,
Beijing, August 2010
Identifying Contradictory and Contrastive Relations between Statements
to Outline Web Information on a Given Topic
Daisuke Kawahara? Kentaro Inui?? Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Graduate School of Information Sciences, Tohoku University
?Graduate School of Informatics, Kyoto University
dk@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
We present a method for producing a
bird?s-eye view of statements that are ex-
pressed on Web pages on a given topic.
This method aggregates statements that
are relevant to the topic, and shows con-
tradictory and contrastive relations among
them. This view of contradictions and
contrasts helps users acquire a top-down
understanding of the topic. To realize
this, we extract such statements and re-
lations, including cross-document implicit
contrastive relations between statements,
in an unsupervised manner. Our experi-
mental results indicate the effectiveness of
our approach.
1 Introduction
The quantity of information on theWeb is increas-
ing explosively. Online information includes news
reports, arguments, opinions, and other coverage
of innumerable topics. To find useful information
from such a mass of information, people gener-
ally use conventional search engines such as Ya-
hoo! and Google. They input keywords to a search
engine as a query and obtain a list of Web pages
that are relevant to the keywords. They then use
the list to check several dozen top-ranked Web
pages one by one.
This method of information access does not
provide a bird?s-eye view of the queried topic;
therefore it can be highly time-consuming and dif-
ficult for a user to gain an overall understanding of
what is written on the topic. Also, browsing only
top-ranked Web pages may provide the user with
biased information. For example, when a user
direct contrastive statement ?A is more P than B?
contrastive keyword pair (A, B)
contradictory relation ?A is P? ? ?A is not P?
contrastive relation ?A is P? ? ?B is P (not P)?
Table 1: Overview of direct contrastive state-
ments, contrastive keyword pairs and contradic-
tory/contrastive relations. Note that ?P? is a pred-
icate.
searches for information on ?agaricus,? claimed
to be a health food, using a conventional search
engine, many commercial pages touting its health
benefits appear at the top of the ranks, while other
pages remain low-ranked. The user may miss an
existing Web page that indicates its unsubstanti-
ated health benefits, and could be unintentionally
satisfied by biased or one-sided information.
This paper proposes a method for produc-
ing a bird?s-eye view of statements that are ex-
pressed on Web pages on a given query (topic).
In particular, we focus on presenting contradic-
tory/contrastive relations and statements on the
topic. This presentation enables users to grasp
what arguing points exist and furthermore to see
contradictory/contrastive relations between them
at a glance. Presenting these relations and state-
ments is thought to facilitate users? understanding
of the topic. This is because people typically think
about contradictory and contrastive entities and is-
sues for decision-making in their daily lives.
Our system presents statements and relations
that are important and relevant to a given topic,
including the statements and relations listed in Ta-
ble 1. Direct contrastive statements compare two
entities or issues in a single sentence. The con-
trasted entities or issues are also extracted as con-
trastive keyword pairs. In addition to them, our
534
sekken-wa gosei senzai-to chigai, kankyo-ni yoi. 
!"#$%&'%("")%*"+%,-.%./0&+"/1./,%#'%2"1$#+.)%,"%'3/,-.42%).,.+(./,5!
gosei senzai-de yogore-ga ochiru (15) 
6#'-%',#&/'%6&,-%'3/,-.42%).,.+(./,!
gosei senzai-ni dokusei-ga aru (9) 
'3/,-.42%).,.+(./,%-#'%,"7&2&,3! gosei senzai-ni dokusei-ga nai (2) '3/,-.42%).,.+(./,%&'%/",%,"7&2!
sekken-de yogore-ga ochi-nai (6) 
/",%+.1"0.%',#&/%6&,-%'"#$! sekken-de yogore-ga ochiru (4) +.1"0.%',#&/'%6&,-%'"#$!
goseisenzai-de te-ga areru (7) 
13%-#/)%(.,'%+"8(-%6&,-%'3/).,!
gosei senzai-wa kaimen kasseizai-wo fukumu (5) 
'3/,-.42%).,.+(./,%2"/,#&/'%'8+*#2,#/,!
[direct contrastive statement]!
contrastive relation!
contradictory relation!
Legend:!
Figure 1: Examples of statements on ?gosei senzai? (synthetic detergent), which are represented by
rounded rectangles. Each statement is linked with the pages from which it is extracted. The number in
a parenthesis represents the number of pages.
system shows contradictory and contrastive rela-
tions between statements. Contradictory relations
are the relations between statements that are con-
tradictory about an entity or issue. Contrastive
relations are the relations between statements in
which two entities or issues are contrasted.
In particular, we have the following two novel
contributions.
? We identify contrastive relations between
statements, which consist of in-document
and cross-document implicit relations.
These relations complement direct con-
trastive statements, which are explicitly
mentioned in a single sentence.
? We precisely extract direct contrastive state-
ments and contrastive keyword pairs in an
unsupervised manner, whereas most previ-
ous studies used supervised methods (Jindal
and Liu, 2006b; Yang and Ko, 2009).
Our system focuses on the Japanese language.
For example, Figure 1 shows examples of ex-
tracted statements on the topic ?gosei senzai?
(synthetic detergent). Rounded rectangles repre-
sent statements relevant to this topic. The first
statement is a direct contrastive statement, which
refers to a contrastive keyword pair, ?gosei sen-
zai? (synthetic detergent) and ?sekken? (soap).
The pairs of statements connected with a broad
arrow have contradictory relations. The pairs of
statements connected with a thin arrow have con-
trastive relations. Users not only can see what is
written on this topic at a glance, but also can check
out the details of a statement by following its links
to the original pages.
2 Related Work
Studies have been conducted on automatic extrac-
tion of direct contrastive sentences (comparative
sentences) for English (Jindal and Liu, 2006b) and
for Korean (Yang and Ko, 2009). They prepared a
set of keywords that serve as clues to direct con-
trastive sentences and proposed supervised tech-
niques on the basis of tagged corpora. We pro-
pose an unsupervised method for extracting direct
contrastive sentences without constructing tagged
corpora.
From direct contrastive sentences, Jindal and
Liu (2006a) and Satou and Okumura (2007) pro-
posed methods for extracting quadruples of (tar-
get, basis, attribute, evaluation). Jindal and Liu
(2006a) extracted these quadruples and obtained
an F-measure of 70%-80% for the extraction of
?target? and ?basis.? Since this extraction was
535
not their main target, they did not perform er-
ror analysis on the extracted results. Satou and
Okumura (2007) extracted quadruples from blog
posts. They provided a pair of named entities
for ?target? and ?basis,? whereas we automati-
cally identify such pairs. Ganapathibhotla and Liu
(2008) proposed a method for detecting which en-
tities (?target? and ?basis?) in a direct contrastive
statement are preferred by its author.
There is also related work that focuses on non-
contrastive sentences. Ohshima et al (2006) ex-
tracted coordinated terms, which are semantically
broader than our contrastive keyword pairs, using
hit counts from a search engine. They made use
of syntactic parallelism among coordinated terms.
Their task was to input one of coordinated terms
as a query, which is different from ours. Soma-
sundaran and Wiebe (2009) presented a method
for recognizing a stance in online debates. They
formulated this task as debate-side classification
and solved it by using automatically learned prob-
abilities of polarity.
To aggregate statements and detect relations be-
tween them, one of important modules is recogni-
tion of synonymous, entailed, contradictory and
contrastive statements. Studies on rhetorical
structure theory (Mann and Thompson, 1988) and
recognizing textual entailment (RTE) deal with
these relations. In particular, evaluative work-
shops on RTE have been held and this kind of re-
search has been actively studied (Bentivogli et al,
2009). The recent workshops of this series set up
a task that recognizes contradictions. Harabagiu
et al (2006), de Marneffe et al (2008), Voorhees
(2008), and Ritter et al (2008) focused on rec-
ognizing contradictions. For example, Harabagiu
et al (2006) used negative expressions, antonyms
and contrast discourse relations to recognize con-
tradictions. These methods only detect relations
between given sentences, and do not create a
bird?s-eye view.
To create a kind of bird?s-eye view, Kawahara et
al. (2008), Statement Map (Murakami et al, 2009)
and Dispute Finder (Ennals et al, 2010) identi-
fied various relations between statements includ-
ing contradictory relations, but do not handle con-
trastive relations, which are one of the important
relations for taking a bird?s-eye view on a topic.
Lerman and McDonald (2009) proposed a method
for generating contrastive summaries about given
two entities on the basis of KL-divergence. This
study is related to ours in the aspect of extracting
implicit contrasts, but contrastive summaries are
different from contrastive relations between state-
ments in our study.
3 Our Method
We propose a method for grasping overall infor-
mation on the Web on a given query (topic). This
method extracts and presents statements that are
relevant to a given topic, including direct con-
trastive statements and contradictory/contrastive
relations between these statements.
As a unit for statements, we use a predicate-
argument structure (also known as a case structure
and logical form). A predicate-argument struc-
ture represents a ?who does what? event. Pro-
cesses such as clustering, summarization, compar-
ison with other knowledge and logical consistency
verification, which are required for this study and
further analysis, are accurately performed on the
basis of predicate-argument structures. The ex-
traction of our target relations and statements is
performed via identification and aggregation of
synonymous, contrastive, and contradictory rela-
tions between predicate-argument structures.
As stated in section 1, we extract direct con-
trastive statements, contrastive keyword pairs, rel-
evant statements, contrastive relations and contra-
dictory relations. We do this with the following
steps:
1. Extraction and aggregation of predicate-
argument structures
2. Extraction of contrastive keyword pairs and
direct contrastive statements
3. Identification of contradictory relations
4. Identification of contrastive relations
Below, we first describe our method of extract-
ing and aggregating predicate-argument struc-
tures. Then, we explain our method of extract-
ing direct contrastive statements with contrastive
keyword pairs, and identifying contradictory and
contrastive relations in detail.
536
3.1 Extraction and Aggregation of
Predicate-argument Structures
A predicate-argument structure consists of a pred-
icate and one or more arguments that have a de-
pendency relation to the predicate.
We extract predicate-argument structures from
automatic parses of Web pages on a given topic
by using the method of Kawahara et al (2008).
We apply the following procedure to Web pages
that are retrieved from the TSUBAKI (Shinzato
et al, 2008) open search engine infrastructure, by
inputting the topic as a query.
1. Extract important sentences from each Web
page. Important sentences are defined as sen-
tences neighboring the topic word(s).
2. Obtain results of morphological analysis
(JUMAN1) and dependency parsing (KNP2)
of the important sentences, and extract
predicate-argument structures from them.
3. Filter out functional and meaningless
predicate-argument structures, which are
not relevant to the topic. Pointwise mutual
information between the entire Web and the
target Web pages for a topic is used.
Note that the analyses in step 2 are performed be-
forehand and stored in an XML format (Shinzato
et al, 2008).
Acquired predicate-argument structures vary
widely in their representations of predicates and
arguments. In particular, many separate predicate-
argument structures have the same meaning due to
spelling variations, transliterations, synonymous
expressions and so forth. To cope with this prob-
lem, we apply ?keyword distillation? (Shibata
et al, 2009), which is a process of absorbing
spelling variations, synonymous expressions and
keywords with part-of relations on a set of Web
pages about a given topic. As a knowledge source
to merge these expressions, this process uses a
knowledge base that is automatically extracted
from an ordinary dictionary and the Web. For
instance, the following predicate-argument struc-
tures are judged to be synonymous3.
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
3In this paper, we use the following abbreviations:
(1) a. sekken-wo
soap-ACC
tsukau
use
b. sopu-wo
soap-ACC
tsukau
use
c. sekken-wo
soap-ACC
shiyou-suru
utilize
We call the predicate-argument structures that
are obtained as the result of the above proce-
dure statement candidates. The final output of
our system consists of direct contrastive state-
ments (with contrastive keyword pairs), top-N
statements (major statements) in order of fre-
quency of statement candidates, and statements
with contradictory/contrastive relations. Contra-
dictory/contrastive relations are identified against
major statements by searching statement candi-
dates.
Another outcome of keyword distillation is a re-
sultant set of keywords that are important and rel-
evant to the topic. We call this set of keywords
relevant keywords, which also include words or
phrases in the query. Relevant keywords are used
to extract contrastive keyword pairs.
3.2 Extraction of Contrastive Keyword Pairs
and Direct Contrastive Statements
We extract contrastive keyword pairs from con-
trastive constructs, which are manually speci-
fied as patterns of predicate-argument structures.
Statements that contain contrastive constructs are
defined as direct contrastive statements.
For example, the following sentence is a typi-
cal direct contrastive statement, which contains a
contrastive verb ?chigau? (differ).
(2) sekken-wa
soap-TOP
gosei senzai-to
synthetic detergent-ABL
chigai,
differ
? ? ?
(soap differs from synthetic detergent, ? ? ?)
From this sentence, a contrastive keyword pair,
?sekken? (soap) and ?gosei senzai? (synthetic de-
tergent), is extracted. The above sentence is ex-
tracted as a direct contrastive statement.
We preliminarily evaluated this simple pattern-
based method and found that it has the following
three problems.
NOM (nominative), ACC (accusative), DAT (dative),
ABL (ablative), CMI (comitative), GEN (genitive) and
TOP (topic marker).
537
? Keyword pairs that are mentioned in a con-
trastive construct are occasionally not rele-
vant to the given topic.
? Non-contrastive keyword pairs are erro-
neously extracted due to omissions of at-
tributes and targets of comparisons.
? Non-contrastive keyword pairs that have an
is-a relation are erroneously extracted.
To deal with the first problem, we filter out key-
word pairs that are contrastive but that are not rel-
evant to the topic. For this purpose, we apply fil-
tering by using relevant keywords, which are de-
scribed in section 3.1.
As an example of non-contrastive keyword
pairs (the second problem), from the following
sentence, a keyword pair, ?tokkyo seido? (patent
system) and ?nihon? (Japan), is incorrectly ex-
tracted by the pattern-based method.
(3) amerika-no
America-GEN
tokkyo seido-wa
patent system-TOP
nihon-to
Japan-ABL
kotonari,
different
? ? ?
(patent system of America is different from ? of
Japan ? ? ?)
In this sentence, ?nihon? (Japan) has a meaning of
?nihon-no tokkyo seido? (patent system of Japan).
That is to say, ?tokkyo seido? (patent system),
which is the attribute of comparison, is omitted.
In this study, in addition to patterns of con-
trastive constructs, we use checking and filtering
on the basis of similarity. The use of similarity
is inspired by the semantic parallelism between
contrasted keywords. As this similarity, we em-
ploy distributional similarity (Lin, 1998), which
is calculated using automatic dependency parses
of 100 million Japanese Web pages. By search-
ing similar keywords from the above sentence, we
successfully extract a contrastive keyword pair,
?amerika? (America) and ?nihon? (Japan), and
the above sentence as a direct contrastive state-
ment.
Similarly, a target of comparison can be omitted
as in the following sentence.
(4) nedan-wa
price-TOP
gosei senzai-yori
synthetic detergent-ABL
takaidesu
high
(price of ? is higher than synthetic detergent)
In this example, the similarity between ?nedan?
(price) and ?gosei senzai? (synthetic detergent) is
lower than a threshold, and this sentence and the
extracts from it are filtered out.
As for the third problem, we may extract non-
contrastive keyword pairs that have an is-a rela-
tion. From the following sentence, we incorrectly
extract a contrastive keyword pair, ?konbini? (con-
venience store) and ?7-Eleven,? which cannot be
filtered out due to its high similarity.
(5) 7-Eleven-wa
7-Eleven-TOP
hokano
other
konbini-to
convenience store-ABL
kurabete,
compare
? ? ?
(7-Eleven is ? ? ? compared to other convenience
stores)
To deal with this problem, we use a filter on the
basis of a set of words that indicate the existence
of hypernyms, such as ?hokano? (other) and ip-
panno (general). We prepare six words for this
purpose.
To sum up, we use the following procedure to
identify contrast keyword pairs.
1. Extract predicate-argument structures that do
not match the above is-a patterns and match
one of the following patterns. They are ex-
tracted from the statement candidates.
? X-wa Y-to {chigau | kotonaru | kuraberu}
(X {differ | vary | compare} from/with Y)
? X-wa Y-yori [adjective]
(X is more ? ? ? than Y)
Note that each of X and Y is a noun phrase
in the argument position.
2. Extract (x, y) that satisfies both the follow-
ing conditions as a contrastive keyword pair.
Note that (x, y) is part of a word sequence in
(X, Y), respectively.
? Both x and y are included in a set of rel-
evant keywords.
? (x, y) has the highest similarity among
any other candidates of (x, y), and this
similarity is higher than a threshold.
Note that the threshold is determined based on a
preliminary experiment using a set of synonyms
(Aizawa, 2007). We extract the sentence that con-
tains the predicate-argument structure used in step
1 as a direct contrastive statement.
538
3.3 Identification of Contradictory Relations
We identify contradictory relations between state-
ment candidates. In this paper, contradictory re-
lations are defined as the following two types
(Kawahara et al, 2008).
negation of predicate
If the predicate of a candidate statement is
negated, its contradiction has the same or synony-
mous predicate without negation. If not, its con-
tradiction has the same or synonymous predicate
with negation.
(6) a. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoi
good
b. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoku-nai
not good
antonym of predicate
The predicate of a contradiction is an antonym
of that of a candidate statement. To judge antony-
mous relations, we use an antonym lexicon ex-
tracted from a Japanese dictionary (Shibata et al,
2008). This lexicon consists of approximately
2,000 entries.
(7) a. gosei senzai-ga
synthetic detergent-NOM
anzen-da
safe
b. gosei senzai-ga
synthetic detergent-NOM
kiken-da
dangerous
To identify contradictory relations between
statements in practice, we search statement can-
didates that satisfy one of the above conditions
against major statements.
3.4 Identification of Contrastive Relations
We identify contrastive relations between state-
ment candidates. In this paper, we define a con-
trastive relation as being between a pair of state-
ment candidates whose arguments are contrastive
keyword pairs and whose predicates have synony-
mous or contradictory relations. Contradictory re-
lations of predicates are defined in the same way
as section 3.3.
In the following example, (a, b) and (a, c) have
a contrastive relation. Also, (b, c) has a contradic-
tory relation.
(8) a. gosei senzai-de
synthetic detergent-CMI
yogore-ga
stain-NOM
ochiru
wash
Topic: bio-ethanol
? (bio-ethanol fuel, gasoline)
(bio-ethanol car, electric car)
Topic: citizen judgment system
? (citizen judgment system, jury system)
(citizen judgment system, lay judge system)
Topic: patent system
? (patent system, utility model system)
(large enterprise, small enterprise)
Topic: Windows Vista
? (Vista, XP)
Table 2: Examples of extracted contrastive key-
word pairs (translated into English).
b. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochiru
wash
c. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochi-nai
not wash
The process of identifying contrastive relations
between statements is performed in the same way
as the identification of contradictory relations.
That is to say, we search statement candidates
that satisfy the definition of contrastive relations
against major statements.
4 Experiments
We conducted experiments for extracting con-
trastive keyword pairs, direct contrastive state-
ments and contradictory/contrastive relations on
50 topics, such as age of adulthood, anticancer
drug, bio-ethanol, citizen judgment system, patent
system and Windows Vista.
We retrieve at most 1,000 Web pages for a topic
from the search engine infrastructure, TSUBAKI.
As major statements, we extract 10 statement can-
didates in order of frequency.
Below, we first evaluate the extracted con-
trastive keyword pairs and direct contrastive state-
ments, and then evaluate the identified contradic-
tory and contrastive relations between statements.
4.1 Evaluation of Contrastive Keyword Pairs
and Direct Contrastive Statements
Contrastive keyword pairs and direct contrastive
statements were extracted on 30 of 50 topics. 99
direct contrastive statements and 73 unique con-
trastive keyword pairs were obtained on 30 topics.
The average number of obtained contrastive key-
word pairs for a topic was approximately 2.4. Ta-
539
Topic: ?tyosakuken hou? (copyright law)
?syouhyouken-wa tyosakuken-yori zaisantekina kachi-wo motsu.?
The trademark right has more financial value than the copyright.
?tyosakuken hou-de hogo-sareru? ? ?tyosakuken hou-de hogo-sare-nai?protected by the copyright law not protected by the copyright law
?tyosakuken-wo shingai-suru? ? ?tyosakuken-wo shingai-shi-nai?infringe the copyright not infringe the copyright
? ?syouhyouken-wo shingai-shi-nai?not infringe the trademark right
Topic: ?genshiryoku hatsuden syo? (nuclear power plant)
?genshiryoku hatsuden syo-wa karyoku hatsuden syo-to chigau.?
Nuclear power plants are different from thermoelectric power plants.
?CO2-wo hassei-shi-nai? ? ?CO2-wo hassei-suru?not emit carbon dioxide emit carbon dioxide
?genpatsu-wo tsukuru? ? ?genshiryoku hatsuden syo-wo tsukura-nai?construct a nuclear power plant not construct a nuclear power plant
? ?karyoku hatsuden syo-wo tsukuru?construct a thermoelectric power plant
Table 3: Examples of identified direct contrastive statements, contradictory relations and contrastive
relations. The sentences with two underlined parts are direct contrastive statements. The arrows ???
and ??? represent a contradictory relation and a contrastive relation, respectively.
ble 2 lists examples of obtained contrastive key-
word pairs. We successfully extracted not only
contrastive keyword pairs including topic words,
but also those without them.
Our manual evaluation of the extracted con-
trastive keyword pairs found that 89% (65/73) of
the contrastive keyword pairs are actually con-
trasted in direct contrastive statements. Correct
contrastive keyword pairs were extracted on 28 of
30 topics. We also evaluated the contrastive key-
word pairs extracted without similarity filtering.
In this case, 190 contrastive keyword pairs on 41
topics were extracted and 44% (84/190) of them
were correct. Correct contrastive keyword pairs
were extracted on 31 of 41 topics. Therefore, sim-
ilarity filtering did not largely decrease the recall,
but significantly increased the precision.
We have eight contrastive keyword pairs that
were incorrectly extracted by our proposed
method. These contrastive keyword pairs acciden-
tally have similarity that is higher than the thresh-
old. Major errors were caused by the ambiguity of
Japanese ablative keyword ?yori.?
(9) heisya-wa
our company-TOP
bitWallet sya-yori
bitWallet, Inc.-ABL
Edy gifuto-no
Edy gift-GEN
gyomu itaku-wo
entrustment-ACC
ukete-imasu
have
(Our company is entrusted with Edy gift by bitWal-
let, Inc.)
In this example, ?yori? means not the basis of
contrast but the source of action. The similar-
ity filtering usually prevents incorrect extraction
from such a non-contrastive sentence. However,
in this case, the pair of ?heisya? (our company)
and ?bitWallet sya? (bitWallet, Inc.) was not fil-
tered due to the high similarity between them. To
cope with this problem, it is necessary to use lin-
guistic knowledge such as case frames.
4.2 Evaluation of Contradictory and
Contrastive Relations
Contradictory relations were identified on 49 of
50 topics. For 49 topics, 268 contradictory re-
lations were identified. The average number of
identified contradictory relations for a topic was
5.5. Contrastive relations were identified on 18
of 30 topics, on which contrastive keyword pairs
were extracted. For the 18 topics, 60 contrastive
relations were identified. The average number of
identified contrastive relations for a topic was 3.3.
Table 3 lists examples of the identified contra-
dictory and contrastive relations as well as direct
contrastive statements. We manually evaluated
the identified contradictory relations and the con-
trastive relations that were identified for correct
contrastive keyword pairs. As a result, we con-
cluded that they completely obey our definitions.
We also classified each of the obtained contra-
dictory and contrastive relations into two classes:
?cross-document? and ?in-document.? ?Cross-
540
Topic: age of adulthood
lower the age of adulthood to 18
? lower the voting age to 18
Topic: anticancer drug
anticancer drugs have side effects
? anticancer drugs have effects
Table 4: Examples of unidentified contrastive re-
lations (translated into English).
document? means that a contradictory/contrastive
relation is obtained not from a single page but
across multiple pages. If a relation can be
obtained from both, we classified it into ?in-
document.? As a result, 67% (179/268) of contra-
dictory relations and 70% (42/60) of contrastive
relations were ?cross-document.? We can see that
many cross-document implicit relations that can-
not be retrieved from a single page were success-
fully identified.
4.3 Discussions
We successfully identified contradictory relations
on almost all the topics. However, out of 50 top-
ics, we extracted contrastive keyword pairs on 30
topics and contrastive relations on 18 topics. To
investigate the resultant contrastive relations from
the viewpoint of recall, we manually checked
whether there were unidentified contrastive rela-
tions among 100 statement candidates for each
topic. We actually checked 20 topics and found
six unidentified contrastive relations in total. Ta-
ble 4 lists examples of the unidentified contrastive
relations. Out of 20 topics, in total, 44 contrastive
relations are manually discovered on 13 topics,
but out of 13 topics, 38 contrastive relations are
identified on eight topics by our method. There-
fore, we achieved a recall of 86% (38/44) at rela-
tion level and 62% (8/13) at topic level. We can
see that our method was able to cover a relatively
wide range of contrastive relations on the topics
on which our method successfully extracted con-
trastive keyword pairs.
To detect such unidentified contrastive rela-
tions, it is necessary to robustly extract contrastive
keyword pairs. In the future, we will employ a
bootstrapping approach to identify patterns of di-
rect contrastive statements and contrastive key-
!"#$"%&'(#)*$+#'#,-#&'!
+.$&.'$!"#$"%&'(#)*$+#'#,-#&'!
!"#$"./0!
1/"($12'($"%&'(#)*$+#'#,-#&'!
1/"($12'($"./0!
3!#,%4$"%&'(#)*$+#'#,-#&'!
5678$92#1$.:$;!"#$"%&'(#)*$+#'#,-#&'<!
5678$92#1$.:$;+.$&.'$!"#$"%&'(#)*$+#'#,-#&'<!
Figure 2: A view of major, contradictory and con-
trastive statements in WISDOM.
word pairs. We will also use patterns of con-
trastive discourse structures as well as those of
predicate-argument structures.
5 Conclusion
This paper has described a method for producing a
bird?s-eye view of statements that are expressed in
Web pages on a given topic. This method aggre-
gates statements relevant to the topic and shows
the contradictory/contrastive relations and state-
ments among them.
In particular, we successfully extracted direct
contrastive statements in an unsupervised man-
ner. We specified only several words for the
extraction patterns and the filtering. Therefore,
our method for Japanese is thought to be easily
adapted to other languages. We also proposed
a novel method for identifying contrastive rela-
tions between statements, which included cross-
document implicit relations. These relations com-
plemented direct contrastive statements.
We have incorporated our proposed method
into an information analysis system, WISDOM4
(Akamine et al, 2009), which can show multi-
faceted information on a given topic. Now, this
system can show contradictory/contrastive rela-
tions and statements as well as their contexts as
a view of KWIC (keyword in context) (Figure 2).
This kind of presentation facilitates users? under-
standing of an input topic.
4http://wisdom-nict.jp/
541
References
Aizawa, Akiko. 2007. On calculating word similarity
using web as corpus. In Proceedings of IEICE Tech-
nical Report, SIG-ICS, pages 45?52 (in Japanese).
Akamine, Susumu, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
hashi, and Yutaka Kidawara. 2009. WISDOM:
A web information credibility analysis system. In
Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 1?4.
Bentivogli, Luisa, Ido Dagan, Hoa Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In Proceedings of TAC 2009 Workshop.
de Marneffe, Marie-Catherine, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047.
Ennals, Rob, Beth Trushkowsky, and John Mark
Agosta. 2010. Highlighting disputed claims on the
web. In Proceedings of WWW 2010.
Ganapathibhotla, Murthy and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of COLING 2008, pages 241?248.
Harabagiu, Sanda, Andrew Hickl, and Finley Laca-
tusu. 2006. Negation, contrast and contradiction
in text processing. In Proceedings of AAAI-06.
Jindal, Nitin and Bing Liu. 2006a. Identifying com-
parative sentences in text documents. In Proceed-
ings of SIGIR 2006.
Jindal, Nitin and Bing Liu. 2006b. Mining compar-
ative sentences and relations. In Proceedings of
AAAI-06.
Kawahara, Daisuke, Sadao Kurohashi, and Kentaro
Inui. 2008. Grasping major statements and their
contradictions toward information credibility analy-
sis of web contents. In Proceedings of WI?08, short
paper, pages 393?397.
Lerman, Kevin and Ryan McDonald. 2009. Con-
trastive summarization: An experiment with con-
sumer reviews. In Proceedings of NAACL-HLT
2009, Companion Volume: Short Papers, pages
113?116.
Lin, Dekang. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774.
Mann, William and Sandra Thompson. 1988. Rhetor-
ical structure theory: toward a functional theory of
text organization. Text, 8(3):243?281.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009. Statement map: Assisting
information credibility analysis by visualizing argu-
ments. In Proceedings of WICOW 2009.
Ohshima, Hiroaki, Satoshi Oyama, and Katsumi
Tanaka. 2006. Searching coordinate terms with
their context from the web. In Proceedings of WISE
2006, pages 40?47.
Ritter, Alan, Stephen Soderland, Doug Downey, and
Oren Etzioni. 2008. It?s a contradiction ? no, it?s
not: A case study using functional relations. In Pro-
ceedings of EMNLP 2008, pages 11?20.
Satou, Toshinori and Manabu Okumura. 2007. Ex-
traction of comparative relations from Japanese we-
blog. In IPSJ SIG Technical Report 2007-NL-181,
pages 7?14 (in Japanese).
Shibata, Tomohide, Michitaka Odani, Jun Harashima,
Takashi Oonishi, and Sadao Kurohashi. 2008.
SYNGRAPH: A flexible matching method based on
synonymous expression extraction from an ordinary
dictionary and a web corpus. In Proceedings of IJC-
NLP 2008, pages 787?792.
Shibata, Tomohide, Yasuo Banba, Keiji Shinzato, and
Sadao Kurohashi. 2009. Web information organi-
zation using keyword distillation based clustering.
In Proceedings of WI?09, short paper, pages 325?
330.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. TSUBAKI: An open search engine in-
frastructure for developing new information access
methodology. In Proceedings of IJCNLP 2008,
pages 189?196.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In Proceed-
ings of ACL-IJCNLP 2009, pages 226?234.
Voorhees, Ellen M. 2008. Contradictions and justi-
fications: Extensions to the textual entailment task.
In Proceedings of ACL-08: HLT, pages 63?71.
Yang, Seon and Youngjoong Ko. 2009. Extract-
ing comparative sentences from korean text docu-
ments using comparative lexical patterns and ma-
chine learning techniques. In Proceedings of ACL-
IJCNLP 2009 Conference Short Papers, pages 153?
156.
542
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 269?278, Dublin, Ireland, August 23-29 2014.
Rapid Development of a Corpus with Discourse Annotations
using Two-stage Crowdsourcing
Daisuke Kawahara
??
Yuichiro Machida
?
Tomohide Shibata
??
Sadao Kurohashi
??
Hayato Kobayashi
?
Manabu Sassano
?
?
Graduate School of Informatics, Kyoto University
?
CREST, Japan Science and Technology Agency
?
Yahoo Japan Corporation
{dk, shibata, kuro}@i.kyoto-u.ac.jp, machida@nlp.ist.i.kyoto-u.ac.jp,
{hakobaya, msassano}@yahoo-corp.jp
Abstract
We present a novel approach for rapidly developing a corpus with discourse annotations using
crowdsourcing. Although discourse annotations typically require much time and cost owing to
their complex nature, we realize discourse annotations in an extremely short time while retaining
good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experi-
ment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run.
Based on this corpus, we also develop a supervised discourse parser and evaluate its performance
to verify the usefulness of the acquired corpus.
1 Introduction
Humans understand text not by individually interpreting clauses or sentences, but by linking such a text
fragment with another in a particular context. To allow computers to understand text, it is essential to
capture the precise relations between these text fragments. This kind of analysis is called discourse
parsing or discourse structure analysis, and is an important and fundamental task in natural language
processing (NLP). Systems for discourse parsing are, however, available only for major languages, such
as English, owing to the lack of corpora with discourse annotations.
For English, several corpora with discourse annotations have been developed manually, consuming a
great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al.,
2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson,
2005). Discourse parsers trained on these corpora have also been developed and practically used. To
create the same resource-rich environment for another language, a quicker method than the conventional
time-consuming framework should be sought. One possible approach is to use crowdsourcing, which
has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008;
Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource
the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of
spans with a certain relation and identifying the relation between the pair.
In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the proce-
dure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for
crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a
discourse unit for the span is a costly process, and thus we adopt a clause as the discourse unit, since this
is reliable enough to be automatically detected. We also limit the length of each target document to three
sentences and at most five clauses to facilitate the annotation task. Secondly, we detect and annotate
clause pairs in a document that hold logical discourse relations. However, since this is too complicated
to assign as one task using crowdsourcing, we divide the task into two steps: determining the existence
of logical discourse relations and annotating the type of relation. Our two-stage approach is a robust
method in that it confirms the existence of the discourse relations twice. We also designed the tagset
of discourse relations for crowdsourcing, which consists of two layers, where the upper layer contains
the following three classes: ?CONTINGENCY,? ?COMPARISON? and ?OTHER.? Although the task
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
269
settings are simplified for crowdsourcing, the obtained corpus and knowledge of discourse parsing could
be still useful in general discourse parsing.
In our experiments, we crowdsourced discourse annotations for Japanese, for which there are no pub-
licly available corpora with discourse annotations. The resulting corpus consists of 10,000 documents,
each of which comprises three sentences extracted from the web. Carrying out this two-stage crowd-
sourcing task took less than eight hours. The time elapsed was significantly shorter than the conventional
corpus building method.
We also developed a discourse parser by exploiting the acquired corpus with discourse annotations.
We learned a machine learning-based model for discourse parsing based on this corpus and evaluated its
performance. An F1 value of 37.9% was achieved for contingency relations, which would be roughly
comparable with state-of-the-art discourse parsers on English. This result indicates the usefulness of the
acquired corpus. The resulting discourse parser would be effectively exploited in NLP applications, such
as sentiment analysis (Zirn et al., 2011) and contradiction detection (Murakami et al., 2009; Ennals et
al., 2010).
The novel contributions of this study are summarized below:
? We propose a framework for developing a corpus with discourse annotations using two-stage crowd-
sourcing, which is both cheap and quick to execute, but still retains good quality of the annotations.
? We construct a Japanese discourse corpus in an extremely short time.
? We develop a discourse parser based on the acquired corpus.
The remainder of this paper is organized as follows. Section 2 introduces related work, while Section
3 describes our proposed framework and reports the experimental results for the creation of a corpus with
discourse annotations. Section 4 presents a method for discourse parsing based on the corpus as well as
some experimental results. Section 5 concludes the paper.
2 Related Work
Snow et al. (2008) applied crowdsourcing to five NLP annotation tasks, but the settings of these tasks
are very simple. There have also been several attempts to construct language resources with complex
annotations using crowdsourcing. Negri et al. (2011) proposed a method for developing a cross-lingual
textual entailment (CLTE) corpus using crowdsourcing. They tackled this complex data creation task by
dividing it into several simple subtasks: sentence modification, type annotation and sentence translation.
The creative CLTE task and subtasks are quite different from our non-creative task and subtasks of
discourse annotations. Fossati et al. (2013) proposed FrameNet annotations using crowdsourcing. Their
method is a single-step approach to only detect frame elements. They verified the usefulness of their
approach through an experiment on a small set of verbs with only two frame ambiguities per verb.
Although they seem to be running a larger-scale experiment, its result has not been revealed yet. Hong
and Baker (2011) presented a crowdsourcing method for selecting FrameNet frames, which is a part of
the FrameNet annotation process. Since their task is equivalent to word sense disambiguation, it is not
very complex compared to the whole FrameNet annotation process. These FrameNet annotations are
still different from discourse annotations, which are our target. To the best of our knowledge, there have
been no attempts to crowdsource discourse annotations.
There are several manually-crafted corpora with discourse annotation for English, such as the Penn
Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse
Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles.
Several attempts have been made to manually create corpora with discourse annotations for languages
other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (news-
paper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents;
1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres;
267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences com-
pared with the English corpora containing several tens of thousands sentences.
270
In recent years, there have been many studies on discourse parsing on the basis of the above hand-
annotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty
et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing
can be attributed to the existence of corpora with discourse annotations. However, the target language is
mostly English since English is the only language that has large-scale discourse corpora. To develop and
improve discourse parsers for languages other than English, it is necessary to build large-scale annotated
corpora, especially in a short period if possible.
3 Development of Corpus with Discourse Annotations using Crowdsourcing
3.1 Corpus Specifications
We develop a tagged corpus in which pairs of discourse units are annotated with discourse relations.
To achieve this, it is necessary to determine target documents, discourse units, and a discourse relation
tagset. The following subsections explain the details of these three aspects.
3.1.1 Target Text and Discourse Unit
In previous studies on constructing discourse corpora, the target documents were mainly newspaper
texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper
corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on
newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts
of other domains.
To address this problem, we set out to create an annotated corpus covering a variety of domains.
Since the web contains many documents across a variety of domains, we use the Diverse Document
Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus
consists of the first three sentences of a Japanese web page, making these short documents suitable for
our discourse annotation method based on crowdsourcing.
We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourc-
ing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically
identified, do not need to be manually modified since they are thought to be reliable enough. Clause
identification is performed using the rules of Shibata and Kurohashi (2005). For example, the following
rules are used to identify clauses as our discourse units:
? clauses that function as a relatively strong boundary in a sentence are adopted,
? relative clauses are excluded.
Since workers involved in our crowdsourcing task need to judge whether clause pairs have discourse
relations, the load of these workers increases combinatorially as the number of clauses in a sentence
increases. To alleviate this problem, we limit the number of clauses in a document to five. This limitation
excludes only about 5% of the documents in the original corpus.
Our corpus consists of 10,000 documents corresponding to 30,000 sentences. The total number of
clauses in this corpus is 39,032, and thus the average number of clauses in a document is 3.9. The total
number of clause pairs is 59,426.
3.1.2 Discourse Relation Tagset
One of our supposed applications of discourse parsing is to automatically generate a bird?s eye view of a
controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010),
which identify various relations between statements, including contradictory relations. We assume that
expansion relations, such as elaboration and restatement, and temporal relations are not important for this
purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations
independently of causal relations. We also suppose that temporal relations can be annotated separately
for NLP applications that require temporal information. We determined the tagset of discourse relations
271
Upper type Lower type Example
CONTINGENCY
Cause/Reason ???????????????????
[since (I) pushed the button] [hot water was turned on]
Purpose ?????????????????????
[to pass the exam] [(I) studied a lot]
Condition ?????????????????
[if (you) push the button] [hot water will be turned on]
Ground ??????????????????????????
[here is his/her bag] [he/she would be still in the company]
COMPARISON
Contrast ?????????????????????????????
[at that restaurant, sushi is good] [ramen is so-so]
Concession ??????????????????????????
[that restaurant is surely good] [the price is high]
OTHER (Other) ???????????????????
[After being back home] [it began to rain]
Table 1: Discourse relation tagset with examples.
by referring to the Penn Discourse Treebank. This tagset consists of two layers, where the upper layer
contains three classes and the lower layer seven classes as follows:
? CONTINGENCY
? Cause/Reason (causal relations and not conditional relations)
? Purpose (purpose-action relations where the purpose is not necessarily accomplished)
? Condition (conditional relations)
? Ground (other contingency relations including pragmatic cause/condition)
? COMPARISON (same as the Penn Discourse Treebank)
? Contrast
? Concession
? OTHER (other weak relation or no relation)
Note that we do not consider the direction of relations to simplify the annotation task for crowdsourcing.
Table 1 shows examples of our tagset.
Therefore, our task is to annotate clause pairs in a document with one of the discourse relations given
above. Sample annotations of a document are shown below. Here, clause boundaries are shown by ?::?
and clause pairs that are not explicitly marked are allocated the ?OTHER? relation.
Cause/Reason ?????::??????????::????????????::???????
???????::??????????????
... [the surgery of my father ended safely] [(I) am relieved a little bit]
Contrast ???????????????????????::????????????
????????????????????????::???????????
??????::????????
... [There is tailwind to live,] [there is also headwind.]
3.2 Two-stage Crowdsourcing for Discourse Annotations
We create a corpus with discourse annotations using two-stage crowdsourcing. We divide the annotation
task into the following two subtasks: determining whether a clause pair has a discourse relation excluding
?OTHER,? and then, ascertaining the type of discourse relation for a clause pair that passes the first stage.
272
Probability Number
= 1.0 64
> 0.99 554
> 0.9 1,065
> 0.8 1,379
> 0.5 2,655
> 0.2 4,827
> 0.1 5,895
> 0.01 9,068
> 0.001 12,277
> 0.0001 15,554
Table 2: Number of clause pairs resulting from the judgments of discourse relation existence.
3.2.1 Stage 1: Judgment of Discourse Relation Existence
This subtask determines whether each clause pair in a document has one of the following discourse
relations: Cause/Reason, Purpose, Condition, Ground, Contrast, and Concession (that is, all the relations
except ?OTHER?). Workers are shown examples of these relations and asked to determine only the
existence thereof.
In this subtask, an item presented to a worker at a particular time consists of all the judgments of
clause pairs in a document. By adopting this approach, each worker considers the entire document when
making his/her judgments.
3.2.2 Stage 2: Judgment of Discourse Relation Type
This subtask involves ascertaining the discourse relation type for a clause pair that passes the first stage.
The result of this subtask is one of the seven lower types in our discourse relation tagset. Workers
are shown examples of these types and then asked to select one of the relations. If a worker chooses
?OTHER,? this corresponds to canceling the positive determination of the existence of the discourse
relation in stage one.
In this subtask, an item is the judgment of a clause pair. That is, if a document contains more than
one clause pair that must be judged, the judgments for this document are divided into multiple items,
although this is rare.
3.3 Experiment and Discussion
We conducted an experiment of the two-stage crowdsourcing approach using Yahoo! Crowdsourcing.
1
To increase the reliability of the produced corpus, we set the number of workers for each item for each
task to 10. The reason why we chose this value is as follows. While Snow et al. (2008) claimed that an
average of 4 non-expert labels per item in order to emulate expert-level label quality, the quality of some
tasks increased by increasing the number of workers to 10. We also tested hidden gold-standard items
once every 10 items to examine worker?s quality. If a worker failed these items in serial, he/she would
have to take a test to continue the task.
We obtained judgments for the 59,426 clause pairs in the 10,000 documents of our corpus in the
first stage of crowdsourcing, i.e., the subtask of determining the existence of discourse relations. We
calculated the probability of each label using GLAD
2
(Whitehill et al., 2009), which was proved to
be more reliable than the majority voting. This probability corresponds to the probability of discourse
relation existence of each clause pair. Table 2 lists the results. We set a probability threshold to select
those clause pairs whose types were to be judged in the second stage of crowdsourcing. With this
threshold set to 0.01, 9,068 clause pairs (15.3% of all the clause pairs) were selected. The threshold was
set fairly low to allow low-probability judgments to be re-examined in the second stage.
1
http://crowdsourcing.yahoo.co.jp/
2
http://mplab.ucsd.edu/?jake/OptimalLabelingRelease1.0.3.tar.gz
273
Lower type All prob > 0.8
Cause/Reason 2,104 1,839 (87.4%)
Purpose 755 584 (77.4%)
Condition 1,109 925 (83.4%)
Ground 442 273 (61.8%)
Contrast 437 354 (81.0%)
Concession 80 49 (61.3%)
Sum of the above discourse relations 4,927 4,024 (81.7%)
Other 4,141 3,753 (90.6%)
Total 9,068 7,777 (85.8%)
Table 3: Results of the judgments of lower discourse relation types.
Upper type All prob > 0.8
CONTINGENCY 4,439 3,993 (90.0%)
COMPARISON 516 417 (80.8%)
Sum of the above discourse relations 4,955 4,410 (89.0%)
OTHER 4,113 3,753 (91.2%)
Total 9,068 8,163 (90.0%)
Table 4: Results of the judgments of upper discourse relation types.
The discourse relation types of the 9,068 clause pairs were determined in the second stage of crowd-
sourcing. We extended GLAD (Whitehill et al., 2009) for application to multi-class tasks, and calculated
the probability of the labels of each clause pair. We assigned the label (discourse relation type) with the
highest probability to each clause pair. Table 3 gives some statistics of the results. The second column in
this table denotes the numbers of each discourse relation type, while the third column gives the numbers
of each type of clause pair with a probability higher than 0.80. Table 4 gives statistics of the results when
the lower discourse relation types are merged into the upper types. Table 5 shows some examples of the
resulting annotations.
Carrying out the two separate subtasks using crowdsourcing took approximately three hours and five
hours with 1,458 and 1,100 workers, respectively. If we conduct this task at a single stage, it would take
approximately 33 (5 hours / 0.153) hours. It would be four times longer than our two-stage approach.
Such single-stage approach is also not robust since it does not have a double check mechanism, with
which the two-stage approach is equipped. We spent 111 thousand yen and 113 thousand yen (approx-
imately 1,100 USD, respectively) for these subtasks, which would be extremely less expensive than the
projects of conventional discourse annotations.
For the examples in Table 5, we confirmed that the discourse relation types of the top four examples
were surely correct. However, we judged the type (Contrast) of the bottom example as incorrect. Since
the second clause is an instantiation of the first clause, the correct type should be ?Other.? We found such
errors especially in the clause pairs with a probability lower than 0.80.
4 Development of Discourse Parser based on Acquired Discourse Corpus
To verify the usefulness of the acquired corpus with discourse annotations, we developed a supervised
discourse parser based on the corpus, and evaluated its performance. We built two discourse parsers using
the annotations of the lower and upper discourse relation types, respectively. From the annotations in the
first stage of crowdsourcing (i.e., judging the existence of discourse relations), we assigned annotations
with a probability less than 0.01 as ?OTHER.? Of the annotations acquired in the second stage (i.e.,
judging discourse relation types), we adopted those with a probability greater than 0.80 and discarded
the rest. After this preprocessing, we obtained 58,135 (50,358 + 7,777) instances of clause pairs for
the lower-type discourse parser and 58,521 (50,358 + 8,163) instances of clause pairs for the upper-type
274
Prob # W Type Document
1.00 6/10 Cause/Reason ???????????????????????????????
??????????????????????????????
????
... [Since the flower blooms in the fifth lunar month] [it is called ?Sat-
suki.?] ...
0.99 4/10 Condition ??????????????????????????????
???????????????????????????????
?????????????????????????????
??????
[If you click the balloon on the map] [you can see the recommended
route] ...
0.81 3/10 Purpose ?????????????????????????????
?????????????????????????????
??????????????????????????????
?????????????????????????????
... [And seeking ?Great harvest?] [each country is engaged in a war]
0.61 2/10 Cause/Reason ??????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
?????
... [by transmitting power to the front and rear axle with the combina-
tion of gears and shafts] [(it) drives the four wheels.]
0.54 3/10 Contrast ?????????????????????????????
??????????????????????????????
???????????????????????????
... [a scramble for customers by department stores would be severe.]
[What comes out is the possibility of the closure of Fukuoka Mit-
sukoshi.]
Table 5: Examples of Annotations. The first column denotes the estimated label probability and the
second column denotes the number of workers that assigned the designated type. In the fourth column,
the clause pair annotated with the type is marked with?? ([ ] in English translations).
discourse parser. Of these, 4,024 (6.9%) and 4,410 (7.5%) instances, respectively, had one of the types
besides ?OTHER.? We conducted experiments using five-fold cross validation on these instances.
To extract features of machine learning, we applied the Japanese morphological analyzer, JUMAN,
3
and the Japanese dependency parser, KNP,
4
to the corpus. We used the features listed in Table 6, which
are usually used for discourse parsing.
We adopted Opal (Yoshinaga and Kitsuregawa, 2010)
5
for the machine learning implementation. This
tool enables online learning using a polynomial kernel. As parameters for Opal, we used the passive-
aggressive algorithm (PA-I) with a polynomial kernel of degree two as a learner and the extension to
multi-class classification (Matsushima et al., 2010). The numbers of classes were seven and three for the
lower- and upper-type discourse parsers, respectively. We set the aggressiveness parameter C to 0.001,
which generally achieves good performance for many classification tasks. Other parameters were set to
the default values of Opal.
To measure the performance of the discourse parsers, we adopted precision, recall and their harmonic
mean (F1). These metrics were calculated as the proportion of the number of correct clause pairs to the
3
http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN
4
http://nlp.ist.i.kyoto-u.ac.jp/EN/?KNP
5
http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
275
Name Description
clause distance clause distance between two clauses
sentence distance sentence distance between two clauses
bag of words bag of words (lemmas) for each clause
predicate a content word (lemma) of the predicate of each clause
conjugation form of predicate a conjugation form of the predicate of each clause
conjunction a conjunction if it is located at the beginning of a clause
word overlapping ratio an overlapping ratio of words between the two clauses
clause type a lexical type output by KNP for each clause (about 100 types)
topic marker existence existence of a topic marker in each clause
topic marker cooccurrence existence of a topic marker in both clauses
Table 6: Features for our discourse parsers.
Type Precision Recall F1
Cause/Reason 0.623 (441/708) 0.240 (441/1,839) 0.346
Purpose 0.489 (44/90) 0.075 (44/584) 0.131
Condition 0.581 (256/441) 0.277 (256/925) 0.375
Ground 0.000 (0/12) 0.000 (0/273) 0.000
Contrast 0.857 (6/7) 0.017 (6/354) 0.033
Concession 0.000 (0/0) 0.000 (0/49) 0.000
Other 0.944 (53,702/56,877) 0.992 (53,702/54,111) 0.968
Table 7: Performance of our lower-type discourse parser.
Type Precision Recall F1
CONTINGENCY 0.625 (1,084/1,735) 0.272 (1,084/3,993) 0.379
COMPARISON 0.412 (7/17) 0.017 (7/417) 0.032
OTHER 0.942 (53,454/56,769) 0.988 (53,454/54,111) 0.964
Table 8: Performance of our upper-type discourse parser.
number of all recognized or gold-standard ones for each discourse relation type. Tables 7 and 8 give the
accuracies for the lower- and upper-type discourse parsers, respectively.
From Table 8, we can see that our upper-type discourse parser achieved an F1 of 37.9% for contingency
relations. It is difficult to compare our results with those in previous work due to the use of different data
set and different languages. We, however, anticipate that our results would be comparable with those
of state-of-the-art English discourse parsers. For example, the end-to-end discourse parser of Lin et al.
(2012) achieved an F1 of 20.6% ? 46.8% on the Penn Discourse Treebank.
We also obtained a low F1 for comparison relations. This tendency is similar to the previous results
on the Penn Discourse Treebank. The biggest cause of this low F1 is the lack of unambiguous explicit
discourse connectives for these relations. Although there are explicit discourse connectives in Japanese,
many of them have multiple meanings and cannot be used as a direct clue for discourse relation detection
(e.g., as described in Kaneko and Bekki (2014)). As reported in Pitler et al. (2009) and other studies,
the identification of implicit discourse relations are notoriously difficult. To improve its performance, we
need to incorporate external knowledge sources other than the training data into the discourse parsers.
A promising way is to use large-scale knowledge resources that are automatically acquired from raw
corpora.
276
5 Conclusion
We presented a rapid approach for building a corpus with discourse annotations and a discourse parser
using two-stage crowdsourcing. The acquired corpus is made publicly available and can be used for
research purposes.
6
This corpus can be used not only to build a discourse parser but also to evaluate
its performance. The availability of the corpus with discourse annotations will accelerate the develop-
ment and improvement of discourse parsing. In the future, we intend integrating automatically acquired
knowledge from corpora into the discourse parsers to further enhance their performance. We also aim to
apply our framework to other languages without available corpora with discourse annotations.
References
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a corpus of temporal-
causal structure. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
pages 908?915.
Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disam-
biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pages 69?73.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo Sierra. 2011. On the development of the RST Spanish
treebank. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 1?10.
Rob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In
Proceedings of the 19th international conference on World Wide Web, pages 341?350.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 60?68. Association for Computational Linguistics.
Marco Fossati, Claudio Giuliano, and Sara Tonelli. 2013. Outsourcing FrameNet to the crowd. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser
evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), pages 169?172.
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus
annotated with semantic relations. In Proceedings of 26th Pacific Asia Conference on Language Information
and Computing, pages 535?544.
Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser
using support vector machine classification. Dialogue & Discourse, 1(3):1?33.
Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ?real? WSD? In Proceedings of the 5th Linguistic
Annotation Workshop, pages 30?37.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential
rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages 486?496.
Kimi Kaneko and Daisuke Bekki. 2014. Building a Japanese corpus of temporal-causal-discourse structures
based on SDRT for extracting causal relations. In Proceedings of the EACL 2014 Workshop on Computational
Approaches to Causality in Language (CAtoCL), pages 33?39.
6
http://nlp.ist.i.kyoto-u.ac.jp/EN/?DDLC
277
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 476?485.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1?34.
Shin Matsushima, Nobuyuki Shimizu, Kazuhiro Yoshida, Takashi Ninomiya, and Hiroshi Nakagawa. 2010. Exact
passive-aggressive algorithm for multiclass classification using support class. In Proceedings of 2010 SIAM
International Conference on Data Mining (SDM2010), pages 303?314.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Pro-
ceedings of the 3rd Workshop on Information Credibility on the Web, pages 43?50.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide
and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679.
Thiago Alexandre Salgueiro Pardo, Maria das Grac?as Volpe Nunes, and Lucia Helena Machado Rino. 2004.
Dizer: An automatic discourse analyzer for Brazilian Portuguese. In Advances in Artificial Intelligence?SBIA
2004, pages 224?234. Springer.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation, pages 2961?2968.
Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis.
In Proceedings of Second International Joint Conference on Natural Language Processing, pages 754?766.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg. 2008. Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 254?263.
Manfred Stede. 2004. The Potsdam commentary corpus. In Proceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 96?102.
Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 566?574.
Jacob Whitehill, Paul Ruvolo, Ting fan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should
count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22,
pages 2035?2043.
FlorianWolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational
Linguistics, 31(2):249?287.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING2010), pages
1245?1253.
C?acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis
with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing,
pages 336?344.
278
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 924?934,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Japanese Zero Reference Resolution
Considering Exophora and Author/Reader Mentions
Masatsugu Hangyo Daisuke Kawahara Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
{hangyo,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp
Abstract
In Japanese, zero references often occur and
many of them are categorized into zero ex-
ophora, in which a referent is not mentioned in
the document. However, previous studies have
focused on only zero endophora, in which
a referent explicitly appears. We present a
zero reference resolution model considering
zero exophora and author/reader of a docu-
ment. To deal with zero exophora, our model
adds pseudo entities corresponding to zero
exophora to candidate referents of zero pro-
nouns. In addition, we automatically detect
mentions that refer to the author and reader of
a document by using lexico-syntactic patterns.
We represent their particular behavior in a dis-
course as a feature vector of a machine learn-
ing model. The experimental results demon-
strate the effectiveness of our model for not
only zero exophora but also zero endophora.
1 Introduction
Zero reference resolution is the task of detecting and
identifying omitted arguments of a predicate. Since
the arguments are often omitted in Japanese, zero
reference resolution is essential in a wide range of
Japanese natural language processing (NLP) appli-
cations such as information retrieval and machine
translation.
(1) ????
pasta-NOM
???
like
??
everyday
(??)
(?-NOM)
(??)
(?-ACC)
?????
eat
(Liking pasta, (?) eats (?) every day)
For example, in example (1) , the accusative argu-
ment of the predicate ?????? (eat) is omitted .1
The omitted argument is called a zero pronoun. In
this example, the zero pronoun refers to ?????
(pasta).
Zero reference resolution is divided into two sub-
tasks: zero pronoun detection and referent identifi-
cation. Zero pronoun detection is the task that de-
tects omitted zero pronouns from a document. In
example (1), this task detects that there are the zero
pronouns in the accusative and nominative cases of
?????? (eat) and there is no zero pronoun in
the dative case of ??????. Referent identifica-
tion is the task that identifies the referent of a zero
pronoun. In example (1), this task identifies that the
referent of the zero pronoun in the accusative case of
?????? is ????? (pasta). These two subtasks
are often resolved simultaneously and our proposed
model is a unified model.
Many previous studies (Imamura et al, 2009;
Sasano et al, 2008; Sasano and Kurohashi, 2011)
have treated only zero endophora, which is a phe-
nomenon that a referent is mentioned in a document,
such as ????? (pasta) in example (1). However,
zero exophora, which is a phenomenon that a ref-
erent does not appear in a document, often occurs in
Japanese when a referent is an author or reader of a
document or an indefinite pronoun. For example, in
example (1), the referent of the zero pronoun of the
nominative case of ?????? (eat) is the author of
1In this paper, we use the following abbreviations: NOM
(nominative), ABL(ablative), ACC (accusative), DAT (dative),
ALL (allative), GEN (genitive), CMI (comitative), CNJ (con-
junction), INS(instrumental) and TOP (topic marker).
924
Zero pronoun Referent Examplein the document
Zero endophora Exist Exist ??????????? (????)??????(I like cafes and go (to a cafe) everyday.)
Zero exophora Exist Not exist
??????? ([reader]?)
????????????
(I would like to explain the advantage (to [reader]).)
No zero reference Not exist Not exist
????????????? (??)?????
(You can have a relaxing time.)
*There is no dative case.
Table 1: Examples of zero endophora, zero exophora and no zero reference.
the document, but the author is not mentioned ex-
plicitly.
(2) ???
recently
?????
PC-INS
???
movie-ACC
([unspecified:person]?)
([unspecified:person]-NOM)
????
can see
(Recently, (people) can see movies by a PC.)
Similarly, in example (2), the referent of the zero
pronoun of the nominative case of ????? (can
see) is an unspecified person.2
Most previous studies have neglected zero ex-
ophora, as though a zero pronoun does not exist in
a sentence. However, such a rough approximation
has impeded the zero reference resolution research.
In Table 1, in ?zero exophora,? the dative case of
the predicate has the zero pronoun, but in ?no zero
reference,? the dative case of the predicate does not
have a zero pronoun. Treating them with no dis-
tinction causes a decrease in accuracy of machine
learning-based zero pronoun detection due to a gap
between the valency of a predicate and observed ar-
guments of the predicate. In this work, to deal with
zero exophora explicitly, we provide pseudo entities
such as [author], [reader] and [unspecified:person]
as candidate referents of zero pronouns.
In the referent identification, selectional prefer-
ences of a predicate (Sasano et al, 2008; Sasano and
Kurohashi, 2011) and contextual information (Iida
et al, 2006) have been widely used. The author and
reader (A/R) of a document have not been used for
contextual clues because the A/R rarely appear in
the discourse in corpora based on newspaper arti-
cles, which are main targets of the previous studies.
However, in other domain documents such as blog
2In the following examples, omitted arguments are put in
parentheses and exophoric referents are put in square brackets.
articles and shopping sites, the A/R often appear in
the discourse. The A/R tend to be omitted and there
are many clues for the referent identification about
the A/R such as honorific expressions and modal-
ity expressions. Therefore, it is important to deal
with the A/R of a document explicitly for the refer-
ent identification.
The A/R appear as not only the exophora but also
the endophora.
(3) ? author ?
I-TOP
???
Kyoto-DAT
(??)
(I-NOM)
????
will go
???????
have thought
(I have thought (I) will go to Kyoto.)
??? reader ?
you all-TOP
???
where-DAT
?????
want to go
(????)
(you all-NOM)
(??)
(I-DAT)
????????
let me know
(Please let (me) know where do you want to go.)
In example (3), ??? (I), which is explicitly men-
tioned in the document, is the author of the docu-
ment and ????? (you all) is the reader. In this pa-
per, we call these expressions, which refer to the au-
thor and reader, author mention and reader men-
tion. We treat them explicitly to improve the per-
formance of zero reference resolution. Since the
A/R are mentioned as various expressions besides
personal pronouns in Japanese, it is difficult to de-
tect the A/R mentions based merely on lexical in-
formation. In this work, we automatically detect
the A/R mentions by using a learning-to-rank al-
gorithm(Herbrich et al, 1998; Joachims, 2002) that
uses lexico-syntactic patterns as features.
Once the A/R mentions can be detected, their in-
formation is useful for the referent identification.
925
The A/R mentions have both a property of the dis-
course element mentioned in a document and a prop-
erty of the zero exophoric A/R. In the first sentence
of example (3), it can be estimated that the referent
of the zero pronoun of the nominative case of ???
?? (will go) from a contextual clue that ??? (I) is
the topic of this sentence and a syntactic clues that ?
?? (I) depends on ???????? (have thought)
over the predicate ????? (will go).3 Such con-
textual clues can be available only for the discourse
entities that are mentioned explicitly. On the other
hand, in the second sentence, since ???????
?? (let me know) is a request form, it can be as-
sumed that the referent of the zero pronoun of the
nominative case is ??? (I), which is the author,
and the one of the dative case is ???? (you all),
which is the reader. The clues such as request forms,
honorific expressions and modality expressions are
available for the author and reader. In this work, to
represent such aspect of the A/R mentions, both the
endophora and exophora features are given to them.
In this paper, we propose a zero reference reso-
lution model considering the zero exophora and the
author/reader mentions, which resolves the zero ref-
erence as a part of a predicate-argument structure
analysis.
2 Related Work
Several approaches to Japanese zero reference reso-
lution have been proposed.
Iida et al (2006) proposed a zero reference resolu-
tion model that uses the syntactic relations between
a zero pronoun and a candidate referent as a feature.
They deal with zero exophora by judging that a zero
pronoun does not have anaphoricity. However, the
information of zero pronoun existences is given and
thus they did not address zero pronoun detection.
Zero reference resolution has been tackled as a
part of predicate-argument structure analysis. Ima-
mura et al (2009) proposed a predicate-argument
structure analysis model based on a log-linear model
that simultaneously conducts zero endophora resolu-
tion. They assumed a particular candidate referent,
NULL, and when the analyzer selected this refer-
ent, the analyzer outputs ?zero exophora or no zero
3Since ??? (I) depends on ???????? (have thought),
the relation between ??? (I) and ????? (will go) is the zero
reference.
pronoun,? in which they are treated without distinc-
tion. Sasano et al (2008) proposed a probabilis-
tic predicate-argument structure analysis model in-
cluding zero endophora resolution by using wide-
coverage case frames constructed from a web cor-
pus. Sasano and Kurohashi (2011) extended the
Sasano et al (2008)?s model by focusing on zero en-
dophora. Their model is based on a log-linear model
that uses case frame information and the location of
a candidate referent as features. In their work, zero
exophora is not treated and they assumed that a zero
pronoun is absent when there is no referent in a doc-
ument.
For languages other than Japanese, zero pronoun
resolution methods have been proposed for Chinese,
Portuguese, Spanish and other languages. In Chi-
nese, Kong and Zhou (2010) proposed tree-kernel
based models for three subtasks: zero pronoun de-
tection, anaphoricity decision and referent selection.
In Portuguese and Spanish, only a subject word is
omitted and zero pronoun resolution has been tack-
led as a part of coreference resolution. Poesio et
al. (2010) and Rello et al (2012) detected omitted
subjects and made a decision whether the omitted
subject has anaphoricity or not as preprocessing of
coreference resolution systems.
3 Baseline Model
In this section, we describe a baseline zero refer-
ence resolution system. In our model, the zero refer-
ence resolution is conducted as a part of predicate-
argument structure (PAS) analysis. The PAS con-
sists of a case frame and an alignment between case
slots and referents. The case frames are constructed
for each meaning of a predicate. Each case frame
describes surface cases that each predicate has (case
slot) and words that can fill each case slot (exam-
ple). In this study, the case frames are constructed
from 6.9 billion Web sentences by using Kawahara
and Kurohashi (2006a)?s method.
The baseline model does not treat zero exophora
as the previous studies. The baseline model analyzes
a document in the following procedure in the same
way as the previous study (Sasano and Kurohashi,
2011).4
4For learning, the previous study used a log-linear model,
but we use a learning-to-rank model. In our preliminary exper-
926
 
????
Kyoto station-DAT
??
stand
?????
curry shop-NOM
????
like
????
the shop
??
often
?????
go
(I like a curry shop in Kyoto station and often go to the shop.)
???
Today-TOP
????
you all-DAT
(?????)
(curry shop-ACC)
??????
will introduce
(Today, I will introduce (the shop) to you.)
Discourse entities 
{??? (Kyoto station)}, {???? (curry shop),??? (the shop)}, {?? (today)},
{??? (you all)}
 
Candidate predicate-argument structures of ??????? in the baseline model 
[1-1] case frame:[???? (1)], { NOM:Null, ACC:Null, DAT:???, TIME:?? }
[1-2] case frame:[???? (1)], { NOM:Null, ACC:????, DAT:???, TIME:?? }
[1-3] case frame:[???? (1)], { NOM:???, ACC:????, DAT:???, TIME:?? }
.
.
.
[2-1] case frame:[???? (2)], { NOM:Null, ACC:Null, DAT:???, TIME:?? }
[2-2] case frame:[???? (2)], { NOM:Null, ACC:????, DAT:???, TIME:?? }
.
.
.
 
 
Figure 1: Examples of discourse entities and predicate-argument structures
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Analyze the predicate-argument structure for
each predicate using the following steps:
(a) Generate candidate predicate-argument
structures.
(b) Calculate the score of each predicate-
argument structure and select the one with
the highest score.
We illustrate the details of the above procedure.
First, we describe how to set the discourse entities
in step 2. In our model, we treat referents of a zero
pronoun using a unit called discourse entity, which
is what mentions in a coreference chain are bound
into. In Figure 1, we treat ?????? (curry shop)
and ????? (the shop), which are in a coreference
chain, as one discourse entity. In Figure 1, the dis-
course entity {????, ??? } is selected for
the referent of the accusative case of the predicate ?
?????? (will introduce).
Next, we illustrate the PAS analysis in step 3. In
step 3a, possible combinations of the case frame
(cf ) and the alignment (a) between case slots and
iment of the baseline model, there is little difference between
the results of these methods.
discourse entities are listed. First, one case frame is
selected from case frames for the predicate. Next,
overt arguments, which have dependency relations
with the predicate, are aligned to a case slot of the
case frame. Finally, each of zero pronouns of re-
maining case slots is assigned to a discourse entity
or is not assigned to any discourse entities. The case
slot whose zero pronoun is not assigned to any dis-
course entities corresponds to the case that does not
have a zero pronoun. In Figure 1, we show the ex-
amples of candidate PASs. In these examples, [??
?? (1)] and [???? (2)] are case frames corre-
sponding to each meaning of ??????. Referents
of each case slot are actually selected from discourse
entities but are explained as a representative word
for illustration. ?Null? indicates that a case slot is
not assigned to any discourse entities. Since align-
ments between case slots and discourse entities of
the PAS [1-2] and [2-2] are the same but their case
frames are different, we deal with them as discrete
PASs. In this case, however, the results of zero ref-
erence resolution are the same.
We represent each PAS as a feature vector, which
is described in section 3.1, and calculate a score of
each PAS with the learned weights. Finally, the sys-
tem outputs the PAS with the highest score.
927
Type Value Description
Log Probabilities that {words, categories and named entity types} of e is assigned to c of cf
Log Generative probabilities of {words, categories and named entity types} of e
Log PMIs between {words, categories and named entity types} of e and c of cf
Case Log Max of PMIs between {words, categories and named entity types} of e and c of cf
frame Log Probability that c of cf is assigned to any words
Log Ratio of examples of c to ones of cf
Binary c of cf is {adjacent and obligate} case
Predicate
Binary Modality types of p
Binary Honorific expressions of p
Binary Tenses of p
Binary p is potential form
Binary Modifier of p (predicate, noun and end of sentence)
Binary p is {dynamic and stative} verb
Context
Binary Named entity types of e
Integer Number of mentions about e in t
Integer Number of mentions about e {before and after} p in t
Binary e is mentioned with post position ??? in a target sentence
Binary Sentence distances between e and p
Binary Location categories of e (Sasano and Kurohashi, 2011)
Binary e is mentioned at head of a target sentence
Binary e is mentioned with post position {??? and ??? } at head of a target sentence
Binary e is mentioned at head of the first sentence
Binary e is mentioned with post position ??? at head of the first sentence
Binary e is mentioned at end of the first sentence
Binary e is mentioned with copula at end of the first sentence
Binary e is mentioned with noun phrase stop at end of the first sentence
Binary Salience score of e is larger than 1 (Sasano and Kurohashi, 2011)
other Binary c is assigned
Table 2: The features of ?assigned(cf, c? e, p, t)
3.1 Feature Representation of
Predicate-Argument Structure
When text t and target predicate p are given and PAS
(cf, a) is chosen, we represent a feature vector of the
PAS as ?(cf, a, p, t). ?(cf, a, p, t) consists of a fea-
ture vector ?overt-PAS(cf, a, p, t) and feature vec-
tors ?(cf, c/e, p, t). Where ?overt-PAS(cf, a, p, t)
corresponds to alignment between case slots and
overt (not omitted) arguments and ?(cf, c/e, p, t)
represents that a case slot c is assigned to a discourse
entity e. If a case slot is assigned to an overt entity,
?(cf, c/e, p, t) is set to a zero vector.
Each feature vector ?(cf, c/e, p, t) consists
of ?A(cf, c/e, p, t) and ?NA(cf, c/Null, p, t).
?A(cf, c/e, p, t) becomes active when the case
slot c is assigned to the discourse entity e and
?NA(cf, c/Null, p, t) becomes active when the
case slot c is not assigned to any discourse entities.
For example, the PAS [1-2] in Figure 1 is repre-
sented as:
(?overt-PAS(???? (1), {NOM:Null,ACC:Null,
NOM:???,TIME:?? }),0?A ,
?NA(???? (1),NOM/Null),
?A(???? (1),ACC/????),
0?NA ,0?A ,0?NA). 5
In our feature representation, the second and third
terms correspond to the nominative case, the forth
and fifth ones correspond to the accusative and the
sixth and seventh ones correspond to the dative
case.
We present the details of ?overt-PAS(cf, a, p, t),
?A(cf, c/e, p, t) and ?NA(cf, c/Null, p, t). We use
a score of the probabilistic PAS analysis (Kawahara
and Kurohashi, 2006b) to ?overt-PAS(cf, a, p, t).
We list the features of ?A(cf, c/e, p, t) in Table 2
and the features of ?NA(cf, c/Null, p, t) in Table
5In the following example, p and t are sometimes omitted
and 0?is 0 vector that has the same dimension as ?.
928
Type Value Description
Case frame
Log Probability that c of cf is
not assigned
Log Ratio of number of examples
of c to ones of cf
Binary c of cf is{adjacent and obligate} case
Table 3: The features of ?NA(cf, c/Null, p, t)
3.
3.2 Weight Learning
In the previous section, we defined the feature vec-
tor ?(cf, a, p, t), which represents a PAS. In this
section, we illustrate the learning method of the
weight vector corresponding to the feature vector.
The weight vector is learned by using a learning-to-
rank algorithm.
In a corpus, gold-standard alignments a? are man-
ually annotated but case frames are not annotated.
Since the case frames are constructed for each mean-
ing, some of them are unsuitable for a usage of a
prdicate in a context. If training data includes PASs
(cf, a?) whose cf is such case a frame as correct
instances, these are harmful for training. Hence,
we treat a case frame cf? which is selected by a
heuristic method as a correct case frame and remove
(cf, a?) which has other cf .
In particular, we make ranking data for the learn-
ing for each target predicate p in the following steps.
1. List possible PASs (cf, a) for predicate p.
2. Calculate a probabilistic zero reference resolu-
tion score for each (cf, a?) and define the one
with highest score as (cf?, a?).
3. Remove (cf, a?) except (cf?, a?) from the
learning instance.
4. Make ranking data that (cf?, a?) has a higher
rank than other (cf, a).
In the above steps, we make ranking data for each
predicate and use the ranking data collected from all
target predicates as training data.
4 Corpus
In this work, we use Diverse Document Leads Cor-
pus (DDLC) (Hangyo et al, 2012) for experiments.
In DDLC, documents collected from the web are
annotated with morpheme, syntax, named entity,
coreference, PAS and A/R mention. Morpheme,
syntax, named entity, coreference and PAS are an-
notated on the basis of Kyoto University Text Cor-
pus (Kawahara et al, 2002). The PAS annotation in-
cludes zero reference information and the exophora
referents are defined as five elements, [author],
[reader], [US(unspecified):person], [US:matter] and
[US:situation]. The A/R mentions are annotated
to head phrases of compound nouns when the A/R
mentions consist of compound nouns. If the A/R
is mentioned by multiple expressions, only one of
them is annotated with the A/R mention tag and all
of these mentions are linked by a coreference chain.
In other words, the A/R mentions are annotated to
discourse entities. In the web site of an organiza-
tion such as a company, the site administrator often
writes the document on behalf of the organization.
In such a case, the organization is annotated as the
author.
5 Author/Reader Mention Detection
A/R mentions, which refer to A/R of a document,
have different properties from other discourse enti-
ties. The A/R are mentioned as very various expres-
sions such as personal pronouns, proper expressions
and role expressions.
(4) ??????
Hello
??????
project team-GEN
?? author ???
am Umetsuji
(Hello, I?m Umetsuji on the project team.)
(5) ???
problem-NOM
???
exist
??? author ??
to moderator
?????????
let me know
(Please let me know if there are any problems.)
In example (4), the author is mentioned as ????
(Umetsuji), which is the name of the author, and in
example (5), the author is mentioned as ?????
(moderator), which expresses the status of the au-
thor. Likewise, the reader is sometimes mentioned
as ????? (customer) and others. However, since
such expressions often refer to someone other than
the A/R, whether an expression indicates the A/R of
a document depends on the context of the document.
In English and other languages, the A/R mentions
can be detected from coreference information be-
cause it can be assumed that the expression that has
929
a coreference relation with first or second personal
pronoun is the A/R mention. However, since the
A/R tend to be omitted and personal pronouns are
rarely used in Japanese, it is difficult to detect the
A/R mentions from coreference information. Be-
cause of these reasons, it is difficult to detect which
discourse entity is the A/R mention from lexical in-
formation of the entities. In this study, the A/R men-
tions are detected from lexico-syntactic (LS) pat-
terns in the document. We use a learning-to-rank
algorithm to detect A/R mentions by using the LS
patterns as features.
5.1 Author/Reader Detection Model
We use a learning-to-rank method for detecting A/R
mentions. This method learns the ranking that en-
tities of the A/R mentions have a higher rank than
other discourse entities. Here, it is an important
point that there are no A/R mentions in some doc-
uments. The documents in which the A/R mentions
do not appear are classified into two types. The first
type is a document that the A/R do not appear in
the discourse of the document such as newspaper ar-
ticles and novels. The second type is a document
that the A/R appear in the discourse but all of their
mentions are omitted. For example, in Figure 1, the
author appears in the discourse (e.g. the nominative
argument of ?like?) but is not mentioned explicitly.
We introduce two pseudo entities corresponding to
these types. The first pseudo entity ?no A/R men-
tion (discourse)? represents the document that the
A/R do not appear in the discourse. It is considered
that the document that the A/R do not appear have
characteristics of writing style such that honorific
expressions and request expressions are rarely used.
This pseudo entity is represented as a document vec-
tor that consists of LS pattern features of the whole
document, which reflect a writing style of a doc-
ument. The second pseudo entity ?no A/R men-
tion (omitted)? represents the document in which all
mentions of the A/R are omitted and this pseudo en-
tity is represented as 0 vector. Since a decision score
of this pseudo entity is allways 0, discourse entities
whose score is lower than the score of this pseudo
entity can be treated as a negative example in a bi-
nary classification.
When there are A/R mentions in a document, we
make ranking data where the discourse entity of
the A/R mention has a higher rank than other dis-
course entities and ?no A/R mention? pseudo enti-
ties. When the A/R do not appear in the discourse,
we make ranking data where ?no A/R mention (dis-
course)? has a higher rank than all discourse enti-
ties and ?no A/R mention (omitted)?. When the A/R
appear in the discourse but all mentions are omit-
ted, we make ranking data where ?no A/R mention
(omitted)? has a higher rank than all discourse en-
tities and ?no A/R mention (discourse)?. We judge
that the A/R appear in the discourse if the A/R ap-
pear as a referent of zero reference in gold-standard
PASs and this judgment is used only in the training
phase. After making the ranking data for each doc-
ument, all of the ranking data are merged and the
merged data is fed into the learning-to-rank model.
For the A/R mention detection, we calculate the
score of all discourse entities and the pseudo entities
and select the discourse entity with the highest score
to the A/R mention. If any ?no A/R mention? have
the highest score, we decide that there are no A/R
mentions in the document.
5.2 Lexico-Syntactic Patterns
For each discourse entity, phrases of the discourse
entity, its parent and their dependency relations are
used to make LS patterns that represent the discourse
entity. When a discourse entity is mentioned multi-
ple times, the phrases of all mentions are used to
make the LS patterns. LS patterns of phrases are
made by generalizing these phrases on various lev-
els (types). LS patterns of dependencies are made
from combining the LS patterns of phrases.
Table 4 lists generalization types. On the word
type, we make a phrase LS pattern by generalizing
each content word and jointing them. For example, a
LS pattern of the phrase ????? generalized on the
<representative form> is ????. The word+ type
is the same as word except all content words are gen-
eralized on the <part of speech and conjugation>.
For example, a LS pattern of the dependency rela-
tion ????????? generalized on the <named
entity> is ?NE:PERSON+?? verb:past?. We also
use the LS patterns of generalized individual mor-
phemes. On the phrase type, each phrase is gener-
alized according to the information assigned to the
phrase and all content words are generalized on the
<part of speech and conjugation> if the information
930
Unit Type Example (original phrase)
word
<no generalization> ?? (??)
<original form> ??? (??)
<representative form> ?? (???)
<part of speech and conjugation> verb:past (???)
word+
<category> Category:PERSON+? (??)
<named entity> NE:PERSON+? (???)
<first person pronoun> FirstPersonPronoun+? (??)
<second person pronoun> SecondPersonPronoun+? (????)
phrase
<modality> modality:request (??????????)
<honorific expression> honorific:modest (??????)
<attached words> ???? (??????????)
Table 4: Generalization types of the LS patterns
is not assigned to the phrase.
For ?no A/R mention (discourse)? instance, the
above features of all mentions, including verbs and
adjectives, and their dependencies in the document
are gathered and used as the features representing
the instance.
6 Zero Reference Resolution Considering
Exophora and Author/Reader Mentions
In this section, we describe the zero reference reso-
lution system that considers the zero exophora and
the A/R mentions. The proposed model resolves
zero reference as a part of the PAS analysis based
on the baseline model.
The proposed model analyzes the PASs in the fol-
lowing steps:
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Detect the A/R mentions of the document.
4. Set pseudo entities from the estimated A/R
mentions.
5. Analyze the PAS for each predicate using the
same procedure as the baseline model.
The differences form baseline model are the estima-
tion of the A/R mentions in step 3 and the setting of
pseudo entities in step 4.
6.1 Pseudo Entities and Author/Reader
Mentions for Zero Exophora
In the baseline model, referents of zero pronouns
are selected form discourse entities. The proposed
model adds pseudo entities([author], [reader],
[US:person] (unspecified:person) and [US:others]
(unspecified:others)6) to deal with zero exophora.
When the A/R mentions appear in a document,
the A/R pseudo entities raise an issue. The zero en-
dophora are given priority to zero exophora. In other
words, the A/R mentions are selected to the referents
in preference to pseudo entities when there are A/R
mentions. Therefore, when the system estimates that
A/R mentions appear, the A/R pseudo entities are
not created.
In the PAS analysis, referents are selected from
discourse entities and the pseudo entities. A zero
reference is the zero exophora when a case slot is
assigned to pseudo entities. Candidate PASs of ??
????? in Figure 1 are shown in Figure 2.
6.2 Feature Representation of Predicate
Argument Structure
In the same way as the baseline model, the
proposed model represents a PAS as a fea-
ture vector that consists of the feature vector
?overt-PAS(cf, a, p, t) and the feature vectors
?(cf, c/e, p, t). The difference from the baseline
model is a composition of ?A(cf, c/e, p, t). In the
proposed model, each ?A(cf, c/e) is composed of
vectors, ?discourse(cf, c/e), ?[author ](cf, c/e),
?[reader ](cf, c/e), ?[US :person](cf, c/e),
?[US :others](cf, c/e) and ?max(cf, c/e). Their
contents and dimensions are the same and similar to
?A(cf, c/e) of the baseline model the except for the
6We merge [US:matter] and [US:situation] because of the
small amount of [US:situation] in the corpus.
931
 
[1-1] case frame:[???? (1)], { NOM:[author], ACC:Null, DAT:??? reader, TIME:?? }
[1-2] case frame:[???? (1)], { NOM:[US:person], ACC:Null, DAT:??? reader, TIME:?? }
[1-3] case frame:[???? (1)], { NOM:[author], ACC:????, DAT:??? reader, TIME:?? }
[1-4] case frame:[???? (1)], { NOM:???, ACC:????, DAT:??? reader, TIME:?? }
[1-5] case frame:[???? (1)], { NOM:[author], ACC:[US:others], DAT:??? reader, TIME:?? }
.
.
.
[2-1] case frame:[???? (2)], { NOM:[author], ACC:Null, DAT:??? reader, TIME:?? }
[2-2] case frame:[???? (2)], { NOM:[US:person], ACC:Null, DAT:??? reader, TIME:?? }
.
.
.
 
Figure 2: Candidate predicate-argument structures of ??????? in the proposed model
Expressions Categories
author ? (I),?? (we),? (I),? (I), PERSON, ORGANIZATION?? (our company),?? (our company),?? (our shop)
reader ??? (you),? (customer),? (you),?? (you all), PERSON??? (you all),? (person),?? (people)
US:person ? (person),?? (people) PERSON
US:others ?? (thing)??? (situation) all categories exceptPERSON and ORGANIZATION
Table 5: Expressions and categories for pseudo entities
addition of a few features described in section 6.3.
?discourse corresponds to the discourse entities,
which are mentioned explicitly and becomes active
when e is a discourse entity including the A/R men-
tions. ?discourse is the same as ?A of the base-
line model and the difference is explained in section
6.3. ?[author ] and ?[reader ] become active when e is
[author]/[reader] or the discourse entity correspond-
ing to the A/R mention. In particular, when e is
the discourse entity corresponding to the A/R men-
tion, both ?discourse and ?[author ]/?[reader ] become
active. This representation gives the A/R mentions
the properties of the discourse entity and the A/R.
?[US :person] and ?[US :others] become active when e
is [US:person] and [US:others].
Because ?[author ], ?[reader ], ?[US :person] and
?[US :others] correspond to the pseudo entities, which
are not mentioned explicitly, we cannot use word in-
formation such as expressions and categories. We
assume that the pseudo entities have expressions and
categories shown in Table 5 and use these to cal-
culate case frame features. Finally, ?max consists
of the highest value of correspondent feature of the
above feature vectors.
6.3 Author/Reader Mention Score
We add A/R mention score features to the feature
vector ?A(cf, c/e, p, t) described in Table 2. The
A/R mention scores are the discriminant function
scores of the A/R mention detection. When e is the
A/R mention, we set the A/R mention score to the
feature.
7 Experiments
7.1 Experimental Settings
We used 1,000 documents from DDLC and per-
formed 5-fold cross-validation. 1,440 zero en-
dophora and 1,935 zero exophora are annotated in
these documents. 258 documens are annotated with
author mentions and 105 documens are annotated
with reader mentions. We used gold-standard (man-
ually annotated) morphemes, named entities, depen-
dency structures and coreference relations to focus
on the A/R detection and the zero reference resolu-
tion. We used SV M rank7 for the learning-to-rank
method of the A/R detection and the PAS analysis.
The categories of words are given by the morpho-
logical analyzer JUMAN8. Named entities and pred-
icate features (e.g., honorific expressions, modality)
7http://www.cs.cornell.edu/people/tj/svm light/svm rank.html
8http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
932
System output
Exist NoneCorrect Wrong
Gold Exist 140 6 112
-standard None - 38 704
Table 6: Result of the author mention detection
System output
Exist NoneCorrect Wrong
Gold Exist 56 2 47
-standard None - 23 872
Table 7: Result of the reader mention detection
are given by the syntactic parser KNP.9
7.2 Results of Author/Reader Mention
Detection
We show the results of the author and reader men-
tion detection in Table 6 and Table 7. In these tables,
?exist? indicates numbers of documents in which the
A/R mentions are manually annotated or our system
estimated that some discourse entities are A/R men-
tions. From these results, the A/R mentions includ-
ing ?none? can be predicted to accuracies of approx-
imately 80%. On the other hand, the recalls are not
particularly high: the recall of author is 140/258 and
the recall of reader is 56/105. This is because the
documents in which the A/R do not appear are more
than the ones in which the A/R appear and the sys-
tem prefers to output ?no author/reader mention? as
the result of training.
7.3 Results of Zero Reference Resolution
We show the results of zero reference resolution
in Table 8 and Table 9. The difference between
the baseline and the proposed model is statistically
significant (p < 0.05) from the McNemar?s test.
In Table 8, we evaluate only the zero endophora
for comparison to the baseline model, which deals
with only the zero endophora. ?Proposed model
(estimate)? shows the result of the proposed model
which estimated the A/R mentions and ?Proposed
model (gold-standard)? shows the result of the pro-
posed model which is given the A/R mentions of
gold-standard from the corpus.
From Table 8, considering the zero exophora and
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Recall Precision F1
Baseline 0.269 0.377 0.314
Proposed model 0.282 0.448 0.346(estimate)
Proposed model 0.388 0.522 0.445(gold-standard)
Table 8: Results of zero endophora resolution
Recall Precision F1
Baseline 0.115 0.377 0.176
Proposed model 0.317 0.411 0.358(estimate)
Proposed model 0.377 0.485 0.424(gold-standard)
Table 9: Results of zero reference resolution
the A/R mentions improves accuracy of zero en-
dophora resolution as well as zero reference reso-
lution including zero exophora.
From Table 8 and Table 9, the proposed model
given the gold-standard A/R mentions achieves ex-
traordinarily high accuracies. This result indicates
that improvement of the A/R mention detection im-
proves the accuracy of zero reference resolution in
the proposed model.
8 Conclusion
This paper presented a zero reference resolution
model considering exophora and author/reader men-
tions. In the experiments, our proposed model
achieves higher accuracy than the baseline model.
As future work, we plan to improve the au-
thor/reader detection model to improve the zero ref-
erence resolution.
References
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kuro-
hashi. 2012. Building a diverse document leads
corpus annotated with semantic relations. In Pro-
ceedings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 535?
544, Bali,Indonesia, November. Faculty of Computer
Science, Universitas Indonesia.
Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra,
and Klaus Obermayer. 1998. Learning preference re-
lations for information retrieval. In ICML-98 Work-
shop: text categorization and machine learning, pages
80?84.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
933
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 625?632, Sydney, Australia, July.
Association for Computational Linguistics.
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009.
Discriminative approach to predicate-argument struc-
ture analysis with zero-anaphora resolution. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 85?88, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation, pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for japanese syn-
tactic and case structure analysis. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 176?183, New York
City, USA, June. Association for Computational Lin-
guistics.
Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida.
2002. Construction of a japanese relevance-tagged
corpus. In Proc. of The Third International Confer-
ence on Language Resources Evaluation, May.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882?891, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
italian. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov.
2012. Elliphant: Improved automatic detection of zero
subjects and impersonal constructions in spanish. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 706?715. Association for Computational
Linguistics.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 758?766, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 769?776,
Manchester, UK, August. Coling 2008 Organizing
Committee.
934
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213?1223,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Knowledge Acquisition for Case Alternation
between the Passive and Active Voices in Japanese
Ryohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura1
1 Precision and Intelligence Laboratory, Tokyo Institute of Technology
2 Graduate School of Informatics, Kyoto University
{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jp
Abstract
We present a method for automatically acquir-
ing knowledge for case alternation between
the passive and active voices in Japanese. By
leveraging several linguistic constraints on al-
ternation patterns and lexical case frames ob-
tained from a large Web corpus, our method
aligns a case frame in the passive voice to a
corresponding case frame in the active voice
and finds an alignment between their cases.
We then apply the acquired knowledge to a
case alternation task and prove its usefulness.
1 Introduction
Predicate-argument structure analysis is one of the
fundamental techniques for many natural language
applications such as recognition of textual entail-
ment, information retrieval, and machine transla-
tion. In Japanese, the relationship between a pred-
icate and its argument is usually represented by us-
ing case particles1 (Kawahara and Kurohashi, 2006;
Taira et al, 2008; Yoshikawa et al, 2011). However,
since case particles vary depending on the voices,
we have to take case alternation into account to rep-
resent predicate-argument structure. There are thus
two major types of representations: one uses surface
cases, and the other uses normalized-cases for the
base form of predicates. For example, while the Ky-
oto University Text Corpus (Kawahara et al, 2004),
one of the major Japanese corpora that contains an-
notations of predicate-argument structures, adopts
1Japanese is a head-final language. Word order does not
mark syntactic relations. Instead, postpositional case particles
function as case markers.
the former representation, the NAIST Text Corpora
(Iida et al, 2007), another major Japanese corpus,
adopts the latter representation.
Examples (1) and (2) describe the same event in
the passive and active voices, respectively. When
we use surface cases to represent the relationship be-
tween the predicate and its argument in Example (1),
the case of ?? (woman)? is ga2 and the case of ??
(man)? is ni.2 On the other hand, when we use the
normalized-cases for the base form, the case of ??
(woman)? is wo2 and the case of ?? (man)? is ga,
which are the same as the surface cases in the active
voice as in Example (2).
(1) ?? ?? ????????
woman-ga man-ni was pushed down
(A woman was pushed down by a man.)
(2) ?? ?? ???????
man-ga woman-wo pushed down
(A man pushed down a woman.)
Both representations have their own advantages.
Surface case analysis is easier than normalized-case
analysis, especially when we consider omitted ar-
guments, which are also called zero anaphors (Na-
gao and Hasida, 1998). In Japanese, zero anaphora
frequently occurs, and the omitted unnormalized-
case of a zero anaphor is often the same as the
surface case of its antecedent (Sasano and Kuro-
hashi, 2011). Therefore, surface case analysis suits
zero anaphora resolution. On the other hand, when
2Ga, wo, and ni are typical Japanese postpositional case par-
ticles. In most cases, they indicate nominative, accusative, and
dative, respectively.
1213
we focus on the resulting predicate argument struc-
tures, the normalized-case structure is more useful.
Specifically, since a normalized-case structure rep-
resents the same meaning in the same representa-
tion, normalized-case analysis is useful for recog-
nizing textual entailment and information retrieval.
Therefore, we need a system that first analyzes
surface cases and then alternates the surface cases
with normalized-cases. In particular, we focus on
the transformation of the passive voice into the ac-
tive voice in this paper. Passive-to-active voice
transformation in English can be performed system-
atically, which does not depend on lexical infor-
mation in most cases. However, in Japanese, the
method of transformation depends on lexical infor-
mation. For example, while the case particle ni in
Example (1) is alternated with ga in the active voice,
the case particle ni in Example (3) is not alternated in
the active voice as in Example (4) even though both
their predicates are ???????? (be pushed
down).?
(3) ?? ?? ????????
woman-ga sea-ni was pushed down
(A woman was pushed down into the sea.)
(4) ?? ?? ???????
woman-wo sea-ni pushed down
(? pushed down a woman into the sea.)
The ni case in Example (1) indicates agent. On
the other hand, the ni case in Example (3) indicates
direction. To determine the difference is important
for many NLP applications including machine trans-
lation. In fact, Google Translate (GT)3 translates
Examples (1) and (3) as ?Woman was pushed down
in the man? and ?Woman was pushed down in the
sea,? respectively, which may be because GT cannot
distinguish between the roles of ni in Examples (1)
and (3).
(5) ?? ?? ?????
prize-ga man-ni was awarded
(A prize was awarded to a man.)
In example (5), although the ni-case argument
?? (man)? is the same as in Example (1), the case
particle ni indicates recipient and is not alternated
in the active voice. These examples show that case
3http://translate.google.com, accessed 2013-2-20.
alternation between the passive and active voices in
Japanese depends on not only predicates but also ar-
guments, and we have to consider their combina-
tions. Since it is impractical to manually describe
the case alternation rules for all combinations of
predicates and arguments, we have to acquire such
knowledge automatically.
Thus, in this paper, we present a method for ac-
quiring the knowledge for case alternation between
the passive and active voices in Japanese. Our
method leverages several linguistic constraints on al-
ternation patterns and lexical case frames obtained
from a large Web corpus, which are constructed for
each meaning and voice of each predicate.
2 Related Work
Levin (1993) grouped English verbs into classes on
the basis of their shared meaning components and
syntactic behavior, defined in terms of diathesis al-
ternations. Hence, diathesis alternations have been
the topic of interest for a number of researchers
in the field of automatic verb classification, which
aims to induce possible verb frames from corpora
(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-
nis et al 2008; Schulte im Walde et al 2008; Li and
Brew 2008; Sun and Korhonen 2009; Theijssen et al
2012). Baroni and Lenci (2010) used distributional
slot similarity to distinguish between verbs undergo-
ing the causative-inchoative alternations, and verbs
that do not alternate.
There is some work on passive-to-active voice
transformation in Japanese. Baldwin and Tanaka
(2000) empirically identified the range and fre-
quency of basic verb alternation, including active-
passive alternation, in Japanese. They automatically
extracted alternation types by using hand-crafted
case frames but did not evaluate the quality. Kondo
et al (2001) dealt with case alternation between the
passive and active voices as a subtask of paraphras-
ing a simple sentence. They manually introduced
case alternation rules on the basis of verb types and
case patterns and transformed passive sentences into
active sentences.
Murata et al (2006) developed a machine-
learning-based method for Japanese case alterna-
tion. They extracted 3,576 case particles in passive
sentences from the Kyoto University Text Corpus
1214
Case particle Grammatical function
ga nominative
wo accusative
ni dative
de locative, instrumental
kara ablative
no genitive
Table 1: Examples of Japanese postpositional case parti-
cles and their typical grammatical functions.
and tagged their cases in the active voice. Then,
they trained SVM classifiers using the tagged cor-
pus. Their features for training SVM were made
by using several lexical resources such as IPAL
(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo
(NLRI, 1993), and the output of Kondo et al?s
method.
3 Lexicalized Case Frames
To acquire knowledge for case alternation, we ex-
ploit lexicalized case frames that are automatically
constructed from 6.9 billion Web sentences by using
Kawahara and Kurohashi (2002)?s method. In short,
their method first parses the input sentences, and
then constructs case frames by collecting reliable
modifier-head relations from the resulting parses.
These case frames are constructed for each predi-
cate like PropBank frames (Palmer et al, 2005), for
each meaning of the predicate like FrameNet frames
(Fillmore et al, 2003), and for each voice. However,
neither pseudo-semantic role labels such as Arg1 in
PropBank nor information about frames defined in
FrameNet are included in these case frames. Each
case frame describes surface cases that each predi-
cate has and instances that can fill a case slot, which
is fully lexicalized like the subcategorization lexicon
VALEX (Korhonen et al, 2006).
We list some Japanese postpositional case parti-
cles with their typical grammatical functions in Ta-
ble 1 and show examples of case frames in Table
2.4 Ideally, one case frame is constructed for each
meaning and voice of the target predicate. However,
since Kawahara and Kurohashi?s method is unsuper-
vised, several case frames are actually constructed
4Niyotte in Table 2 is a Japanese functional phrase that in-
dicates agent in this case. We treat niyotte as a case particle in
this paper for the sake of simplicity.
Case Frame: ????????-4 (be pushed down-4)?
{?? (woman):5,? (I):2,? (woman):2, ? ? ? }-ga
{? (sea):229,? (bottom):115,? (pond):51, ? ? ? }-ni
{??(stepmother):2,????(Pegasus):2, ? ? ? }-niyotte
? ? ?
Case Frame: ????????-5 (be pushed down-5)?
{?? (Kyoko):3,?? (manager):1, ? ? ? }-ga
{?? (someone):143,??? (somebody):85, ? ? ? }-ni
{?? (stair):20,? (ship):7,? (cliff):7, ? ? ? }-kara
? ? ?
Case Frame: ??????-2 (push down-2)?
{? (man):14,?? (lion):5,? (tiger):3, ? ? ? }-ga
{?(child):316,??(child):81,?(person):51, ? ? ? }-wo
{? (sea):580,? (ravine):576,? (river):352 ? ? ? }-ni
? ? ?
Case Frame: ??????-4 (push down-4)?
{?? (someone):14,???? (lion):5, ? ? ? }-ga
{? (person):257,? (I):214,? (child):137, ? ? ? }-wo
{? (cliff):53,?? (stair):28, ? ? ? }-kara
? ? ?
Table 2: Examples of case frames for ???????
? (be pushed down)? and ?????? (push down).?
Words in curly braces denote instances that can fill cor-
responding cases and the numbers following these words
denote their frequency in the corpus.
for each meaning and voice. For example, 59 and
eight case frames were respectively constructed for
the predicate in the passive voice ????????
(be pushed down)? and in the active voice ????
?? (push down)? from 6.9 billion Web sentences.
Table 2 shows the 4th and 5th case frames for ???
????? (be pushed down)? and the 2nd and 4th
case frames for ?????? (push down).?
Table 3 shows an example of case frames for
??? (hit),? which includes no-case. Here, the
Japanese postpositional case particle ?no? roughly
corresponds to ?of,? that is, ?X no Y? means ?Y of
X,? and thus no-case is not an argument of the target
predicate. While Kawahara and Kurohashi?s method
basically collects arguments of the target predicate,
the phrase of no-case that modifies the direct object
of the predicate is also collected as no-case. This
is because, as we will show in the next section, this
phrase can be represented as ga-case in the passive
voice.
1215
Case Frame: ???-2 (hit-2)?
{? (man):51,? (fist):30,?? (someone):23, ? ? ? }-ga
{?? (myself):360,? (I):223, ? ? ? }-no
{? (head):5424,? (face):3215, ? ? ? }-wo
{? (fist):316,?? (palm):157,?? (fist):126, ? ? ? }-de
? ? ?
Table 3: An example of case frames for ??? (hit).?
4 Passive-Active Transformation in
Japanese
Morphologically speaking, the passive voice in
Japanese is expressed by using the auxiliary verbs
??? (reru)? and ???? (rareru),? whose past
forms are ??? (reta)? and ???? (rareta),? re-
spectively. For example, the verb in the base form
?????? (tsukiotosu, push down)? is trans-
formed into the past passive form ???????
? (tsukiotosa-reta, was pushed down).? Case al-
ternations accompany passive-active transformation
in Japanese. There are only two case alternations
at most in passive-active transformation. One is the
case represented as ga in the passive voice, and the
other is the case represented as ga in the active voice.
Japanese passive sentences can be classified into
three types in accordance with what is represented
as ga-case in the passive voice: direct passive, in-
direct passive, and possessor passive.
In direct passive sentence, the object of the pred-
icate in the active voice is represented as ga-case.
Examples (1), (3), and (5) are all direct passive sen-
tences. The case that is represented as ga in the ac-
tive voice is usually represented as ni, niyotte, kara,
or de in the passive sentence. In the first sentence of
Examples (6) and (7),5 ga-cases in the active voice
are represented as niyotte and kara, respectively. On
the other hand, ga-case in the passive sentence is al-
ternated with wo or ni as shown with broken lines in
the second sentence of Examples (6) and (7).
(6) P: ???...... ????? ??????
cause-ga..... man-niyotte was identified
(The cause was identified by a man.)
A: ?? ???...... ?????
man-ga cause-wo...... identified
(A man identified the cause.)
5?P? denotes a passive sentence and ?A? denotes the corre-
sponding active sentence in these examples.
(7) P: ??...... ??? ????????
man-ga..... woman-kara was talked to
(A man was talked to by a woman.)
A: ?? ??...... ??????
woman-ga man-ni.... talked to
(A woman talked to a man.)
Indirect passive is also called adversative pas-
sive, in which an indirectly influenced agent is repre-
sented with ga. For example, ?? (I),? the argument
represented with ga in the first sentence of Exam-
ple (8), does not appear in the active voice, i.e. the
second sentence of Example (8). In the case of in-
direct passive, ga-case in the active sentence is al-
ways alternated with ni-case in the passive sentence
as shown with solid lines in Examples (8).
(8) P: ??...... ??? ?????
I-ga..... child-ni was cried
(I?ve got a child crying.)
A: ??? ????(A child cried.)
child-ga cried
Possessor passive is similar to indirect passive in
that the argument represented with ga-case does not
appear as an argument of the predicate in the ac-
tive voice. Therefore, possessor passive is some-
times treated as a kind of indirect passive. How-
ever, in the case of possessor passive, the argument
appears in the active sentence as a possessor of the
direct object. For example, the ga-case argument
?? (woman)? in the passive sentence of Example
(9) does not appear as an argument of the predicate
???? (hit)? in the active sentence but appears in
the phrase that modifies the direct object ?? (head)?
with the case particle no, which indicates that ??
(woman)? is the possessor of ?? (head).?
(9) P: ??...... ?? ?? ?????
woman-ga..... man-ni head-wo was hit
(A woman was hit on the head by a man.)
A:?? ??...... ?? ????
man-ga woman-no..... head-wo hit
(A man hit the head of a woman.)
In conclusion, the number of case alternation pat-
terns accompanying passive-active transformation in
Japanese is limited. Ga-case in the passive voice can
1216
be alternated only with either wo, ni, or no, or does
not appear in the active voice. Ga-case in the active
voice can be represented only by ni, niyotte, kara,
or de in the passive voice. Hence, it is sufficient to
consider only their combinations.
5 Knowledge Acquisition for Case
Alternation
5.1 Task Definition
Our objective is to acquire knowledge for case al-
ternation between the passive and active voices in
Japanese. We leverage lexical case frames obtained
from a large Web corpus by using Kawahara and
Kurohashi (2002)?s method and align cases of a case
frame in the passive voice and cases of a case frame
in the active voice. As described in Section 2, sev-
eral case frames are constructed for each voice of
each predicate. Our task consists of the following
two subtasks:
1. Identify a corresponding case frame in the ac-
tive voice.
2. Find an alignment between cases of case
frames in the passive and active voice.
Figure 1 shows the overview of our task. If a case
frame in the passive voice is input, we identify a cor-
responding case frame in the active voice, and find
an alignment between cases by using the algorithm
described in Section 5.3. In this example, an active
case frame ??????-4 (push down-4)? is iden-
tified as a corresponding case frame for the input
passive case frame ????????-5 (be pushed
down-5)? and ga, ni, and kara-cases in the passive
case frame are aligned to wo, ga, and kara-cases in
the active case frame, respectively.
5.2 Clues for Knowledge Acquisition
We exploit three clues for corresponding case frame
identification and case alignment as follows:
1. Semantic similarity between the instances of
the aligned cases: simSEM .
2. Case distribution similarity between the corre-
sponding case frames: simDIST .
3. Preference of alternation patterns: fPP .
&DVH)UDPH SXVKGRZQ
^DZRUG 	ZRUGV `JD
^
,SHUVRQ`ZR
^ ERWWRPKHOO`QL

&DVH)UDPH SXVKGRZQ
^PDQ OLRQ `JD
^FKLOGFKLOG`ZR
^VHDUDYLQHProceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?67,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Inducing Example-based Semantic Frames
from a Massive Amount of Verb Uses
Daisuke Kawahara
?
Daniel W. Peterson
?
Octavian Popescu
?
Martha Palmer
?
?
Kyoto University, Kyoto, Japan
?
University of Colorado at Boulder, Boulder, CO, USA
?
Fondazione Bruno Kessler, Trento, Italy
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu
Abstract
We present an unsupervised method for in-
ducing semantic frames from verb uses in
giga-word corpora. Our semantic frames
are verb-specific example-based frames
that are distinguished according to their
senses. We use the Chinese Restau-
rant Process to automatically induce these
frames from a massive amount of verb in-
stances. In our experiments, we acquire
broad-coverage semantic frames from two
giga-word corpora, the larger comprising
20 billion words. Our experimental results
indicate the effectiveness of our approach.
1 Introduction
Semantic frames are indispensable knowledge for
semantic analysis or text understanding. In the
last decade, semantic frames, such as FrameNet
(Baker et al., 1998) and PropBank (Palmer et al.,
2005), have been manually elaborated. These
resources are effectively exploited in many nat-
ural language processing (NLP) tasks, includ-
ing not only semantic parsing but also ma-
chine translation (Boas, 2002), information ex-
traction (Surdeanu et al., 2003), question answer-
ing (Narayanan and Harabagiu, 2004), paraphrase
acquisition (Ellsworth and Janin, 2007) and recog-
nition of textual entailment (Burchardt and Frank,
2006).
There have been many attempts to automati-
cally acquire frame knowledge from raw corpora
with the goal of either adding frequency informa-
tion to an existing resource or of inducing simi-
lar frames for other languages. Most of these ap-
proaches, however, focus on syntactic frames, i.e.,
subcategorization frames (e.g., (Manning, 1993;
Briscoe and Carroll, 1997; Korhonen et al., 2006;
Lippincott et al., 2012; Reichart and Korhonen,
2013)). Since subcategorization frames represent
argument patterns of verbs and are purely syn-
tactic, expressions that have the same subcatego-
rization frame can have different meanings (e.g.,
metaphors). Semantics-oriented NLP applications
based on frames, such as paraphrase acquisition
and machine translation, require consistency in the
meaning of each frame, and thus these subcatego-
rization frames are not suitable for these semantic
tasks.
Recently, there have been a few studies on au-
tomatically acquiring semantic frames (Materna,
2012; Materna, 2013). Materna induced seman-
tic frames (called LDA-Frames) from triples of
(subject, verb, object) in the British National
Corpus (BNC) based on Latent Dirichlet Allo-
cation (LDA) and the Dirichlet Process. LDA-
Frames capture limited linguistic phenomena of
these triples, and are defined across verbs based
on probabilistic topic distributions.
This paper presents a method for automati-
cally building verb-specific semantic frames from
a large raw corpus. Our semantic frames are verb-
specific like PropBank and semantically distin-
guished. A frame has several syntactic case slots,
each of which consists of words that are eligible to
fill the slot. For example, let us show three seman-
tic frames of the verb ?observe?:
1
observe:1
nsubj:{we, author, ...} dobj:{effect, result, ...}
prep in:{study, case, ...} ...
observe:2
nsubj:{teacher, we, ...} dobj:{child, student, ...}
prep in:{classroom, school, ...} ...
observe:3
nsubj:{child, people, ...} dobj:{bird, animal, ...}
prep at:{range, time, ...} ...
1
In this paper, we use the dependency relation names
of the Stanford collapsed dependencies (de Marneffe et al.,
2006) as the notations of case slots. For instance, ?nsubj?
means a nominal subject, ?dobj? means a direct object, ?iboj?
means an indirect object, ?ccomp? means a clausal comple-
ment and ?prep *? means a preposition.
58
Frequencies, which are not shown in the above ex-
amples, are attached to each semantic frame, case
slot and word, and can be effectively exploited for
the applications of these semantic frames. The fre-
quencies of words in each case slot become good
sources of selectional preferences.
Our novel contributions are summarized as fol-
lows:
? induction of semantic frames based on the
Chinese Restaurant Process (Aldous, 1985)
from only automatic parses of a web-scale
corpus,
? exploitation of the assumption of one sense
per collocation (Yarowsky, 1993) to make the
computation feasible,
? providing broad-coverage knowledge for se-
lectional preferences, and
? evaluating induced semantic frames by us-
ing an existing annotated corpus with verb
classes.
2 Related Work
The most closely related work to our semantic
frames are LDA-Frames, which are probabilistic
semantic frames automatically induced from a raw
corpus (Materna, 2012; Materna, 2013). He used a
model based on LDA and the Dirichlet Process to
cluster verb instances of a triple (subject, verb, ob-
ject) to produce semantic frames and slots. Both
of these are represented as a probabilistic distri-
bution of words across verbs. He applied this
method to the BNC and acquired 427 frames and
144 slots (Materna, 2013). These frames are over-
generalized across verbs and might be difficult
to provide with fine-grained selectional prefer-
ences. In addition, Grenager and Manning (2006)
proposed a method for inducing PropBank-style
frames from Stanford typed dependencies ex-
tracted from raw corpora. Although these frames
are based on typed dependencies and more seman-
tic than subcategorization frames, they are not dis-
tinguished in terms of the senses of words filling a
case slot.
There are hand-crafted semantic frames in the
lexicons of FrameNet (Baker et al., 1998) and
PropBank (Palmer et al., 2005). Corpus Pattern
Analysis (CPA) frames (Hanks, 2012) are another
manually created repository of patterns for verbs.
Each pattern represents a prototypical word usage
as extracted by lexicographers from the BNC. Cre-
ating CPA is time consuming, but our proposed
method may be employed to assist in the creation
of this type of resource, as shown in Section 4.4.
Our task can be regarded as clustering of verb
instances. In this respect, the models of Parisien
and Stevenson are related to our method (Parisien
and Stevenson, 2009; Parisien and Stevenson,
2010). Parisien and Stevenson (2009) proposed
a Dirichlet Process model for clustering usages
of the verb ?get.? Later, Parisien and Stevenson
(2010) proposed a Hierarchical Dirichlet Process
model for jointly clustering argument structures
(i.e., subcategorization frames) and verb classes.
However, their argument structures are not seman-
tic but syntactic, and also they did not evaluate the
resulting frames. There have also been related ap-
proaches to clustering verb types (Vlachos et al.,
2009; Sun and Korhonen, 2009; Falk et al., 2012;
Reichart and Korhonen, 2013). These methods in-
duce verb clusters in which multiple verbs partic-
ipate, and do not consider the polysemy of verbs.
Our objective is different from theirs.
Another line of related work is unsupervised
semantic parsing or semantic role labeling (Poon
and Domingos, 2009; Lang and Lapata, 2010;
Lang and Lapata, 2011a; Lang and Lapata, 2011b;
Titov and Klementiev, 2011; Titov and Klemen-
tiev, 2012). These approaches basically clus-
ter predicates and their arguments to distinguish
predicate senses and semantic roles of arguments.
Modi et al. (2012) extended the model of Titov and
Klementiev (2012) to jointly induce semantic roles
and frames using the Chinese Restaurant Process,
which is also used in our approach. However,
they did not aim at building a lexicon of semantic
frames, but at distinguishing verbs that have dif-
ferent senses in a relatively small annotated cor-
pus. Applying this method to a large corpus could
produce a frame lexicon, but its scalability would
be a big problem.
For other languages than English, Kawahara
and Kurohashi (2006a) proposed a method for au-
tomatically compiling Japanese semantic frames
from a large web corpus. They applied con-
ventional agglomerative clustering to predicate-
argument structures using word/frame similarity
based on a manually-crafted thesaurus. Since
Japanese is head-final and has case-marking post-
positions, it seems easier to build semantic frames
with it than with other languages such as English.
They also achieved an improvement in depen-
dency parsing and predicate-argument structure
59
analysis by using their resulting frames (Kawahara
and Kurohashi, 2006b).
3 Method for Inducing Semantic Frames
Our objective is to automatically induce verb-
specific example-based semantic frames. Each se-
mantic frame consists of a partial set of syntactic
slots: nsubj, dobj, iobj, ccomp and prep *. Each
slot consists of words with frequencies, which
could provide broad-coverage selectional prefer-
ences.
Frames for a verb should be semantically distin-
guished. That is to say, each frame should consist
of predicate-argument structures that have consis-
tent usages or meanings.
Our procedure to automatically generate seman-
tic frames from verb usages is as follows:
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based on
the assumption of one sense per collocation
to get a set of initial frames, and
3. apply clustering to the initial frames based
on the Chinese Restaurant Process to produce
the final semantic frames.
Each of these steps is described in the following
sections in detail.
3.1 Extracting Predicate-argument
Structures from a Raw Corpus
We first apply dependency parsing to a large raw
corpus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al., 2006).
2
Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ?
Here, we do not distinguish adjuncts from argu-
ments. All extracted dependents of a verb are han-
dled as arguments. This distinction is left for fu-
ture work, but this will be performed using slot
2
http://nlp.stanford.edu/software/lex-parser.shtml
Sentences:
They observed the effects of ...
This statistical ability to observe an effect ...
We did not observe a residual effect of ...
He could observe the results at the same time ...
My first opportunity to observe the results of ...
You can observe beautiful birds ...
Children may then observe birds ...
.
.
.
Predicate-argument structures:
nsubj:they observe dobj:effect
observe dobj:effect
nsubj:we observe dobj:effect
nsubj:he observe dobj:result prep at:time
observe dobj:result
nsubj:you observe dobj:bird
nsubj:child observe dobj:bird
.
.
.
Initial frames:
nsubj:{they, we, ...} observe dobj:{effect}
nsubj:{he, ...} observe dobj:{result} prep at:{time}
nsubj:{you, child, ...} observe dobj:{bird}
.
.
.
Figure 1: Examples of predicate-argument struc-
tures and initial frames for the verb ?observe.?
frequencies in the applications of semantic frames
or the method proposed by Abend and Rappoport
(2010).
We apply the following processes to extracted
predicate-argument structures:
? A verb and an argument are lemmatized, and
only the head of an argument is preserved for
compound nouns.
? Phrasal verbs are also distinguished from
non-phrasal verbs. For example, ?look up?
has independent frames from ?look.?
? The passive voice of a verb is distinguished
from the active voice, and thus these have in-
dependent frames. Passive voice is detected
using the part-of-speech tag ?VBN? (past
participle). The alignment between frames of
active and passive voices will be done after
the induction of frames using the model of
Sasano et al. (2013) in the future.
? ?xcomp? (open clausal complement) is re-
named to ?ccomp? (clausal complement) and
?xsubj? (controlling subject) is renamed to
?nsubj? (nominal subject). This is because
60
these usages as predicate-argument structures
are not different.
? A capitalized argument with the part-of
speech ?NNP? (singular proper noun) or
?NNPS? (plural proper noun) is general-
ized to ?name?. Similarly, an argument of
?ccomp? is generalized to ?comp? since the
content of a clausal complement is not impor-
tant.
Extracted predicate-argument structures are
collected for each verb and the subsequent pro-
cesses are applied to the predicate-argument struc-
tures of each verb. Figure 1 shows examples of
predicate-argument structures for ?observe.?
3.2 Constructing Initial Frames from
Predicate-argument Structures
A straightforward way to produce semantic frames
is to cluster the extracted predicate-argument
structures directly. Since our objective is to com-
pile broad-coverage semantic frames, a massive
amount of predicate-argument structures should
be fed into the clustering. It would take prohibitive
computational costs to conduct the sampling pro-
cedure, which is described in the next section.
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:
3
dobj, ccomp, nsubj, prep ?, iobj.
This selection of a predominant argument order
above is justified by relative comparisons of the
discriminative power of the different slots for CPA
frames (Popescu, 2013). If a predicate-argument
structure does not have any of the above slots, it is
discarded.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
3
If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
word, e.g., ?dobj:effect?) are merged into an ini-
tial frame (Figure 1). After this process, we dis-
card minor initial frames that occur fewer than 10
times.
For example, we have 732,292 instances
(predicate-argument structures) for the verb ?ob-
serve? in the web corpus that is used in our exper-
iment (its details are described in Section 4.1). As
the result of this merging process, we obtain 6,530
initial frames, which become an input for the clus-
tering. This means that this process accelerates the
speed of clustering more than 100 times.
The precision of this process will be evaluated
in Section 4.3.
3.3 Clustering using Chinese Restaurant
Process
We cluster initial frames for each verb to produce
final semantic frames using the Chinese Restau-
rant Process (Aldous, 1985). We regard each ini-
tial frame as an instance in the usual clustering of
the Chinese Restaurant Process.
We calculate the posterior probability of a se-
mantic frame f
j
given an initial frame v
i
as fol-
lows:
P (f
j
|v
i
) ?
{
n(f
j
)
N+?
? P (v
i
|f
j
) f
j
?= new
?
N+?
? P (v
i
|f
j
) f
j
= new,
(1)
where N is the number of initial frames for the
target verb and n(f
j
) is the current number of ini-
tial frames assigned to the semantic frame f
j
. ?
is a hyper-parameter that determines how likely
it is for a new semantic frame to be created. In
this equation, the first term is the Dirichlet process
prior and the second term is the likelihood of v
i
.
P (v
i
|f
j
) is defined based on the Dirichlet-
Multinomial distribution as follows:
P (v
i
|f
j
) =
?
w?V
P (w|f
j
)
count(v
i
,w)
, (2)
where V is the vocabulary in all case slots cooc-
curring with the verb. It is distinguished by
the case slot, and thus consists of pairs of slots
and words, e.g., ?nsubj:child? and ?dobj:bird.?
count(v
i
, w) is the number of w in the initial
frame v
i
.
P (w|f
j
) is defined as follows:
P (w|f
j
) =
count(f
j
, w) + ?
?
t?V
count(f
j
, t) + |V | ? ?
, (3)
61
where count(f
j
, w) is the current number of w in
the frame f
j
, and ? is a hyper-parameter of Dirich-
let distribution. For a new semantic frame, this
probability is uniform (1/|V |).
We use Gibbs sampling to realize this cluster-
ing.
4 Experiments and Evaluations
4.1 Experimental Settings
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we first crawled the
web. We extracted sentences from each web
page that seems to be written in English based
on the encoding information. Then, we selected
sentences that consist of at most 40 words, and
removed duplicated sentences. From this pro-
cess, we obtained a corpus of one billion sen-
tences, totaling approximately 20 billion words.
We focused on verbs whose frequency was more
than 1,000. There were 19,649 verbs, includ-
ing phrasal verbs, and separating passive and ac-
tive constructions. We extracted 2,032,774,982
predicate-argument structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition)
to induce semantic frames. This corpus consists
of approximately 180 million sentences, which to-
taling four billion words. There were 7,356 verbs
after applying the same frequency threshold as the
web corpus. We extracted 423,778,278 predicate-
argument structures from this corpus.
We set the hyper-parameters ? in (1) and ? in
(3) to 1.0. The frame assignments for all the com-
ponents were initialized randomly. We took 100
samples for each initial frame and selected the
frame assignment that has the highest probability.
These parameters were determined according to a
preliminary experiment to manually examine the
quality of resulting frames.
4.2 Experimental Results
We executed the per-verb clustering tasks on a PC
cluster. It finished within a few hours for most
verbs, but it took a couple of days for very frequent
verbs, such as ?get? and ?say.? The clustering pro-
duced an average number of semantic frames per
verb of 15.2 for the web corpus and 18.5 for the
Gigaword corpus. Examples of induced semantic
frames from the web corpus are shown in Table 1.
slot instances
nsubj i:5850, we:5201, he:3796, you:3669, ...
dobj what:7091, people:2272, this:2262, ...
observe:1
prep in way:254, world:204, life:194, ...
.
.
.
nsubj we:11135, you:1321, i:1317, ...
dobj change:5091, difference:2719, ...
observe:2
prep in study:622, case:382, cell:362, ...
.
.
.
nsubj student:3921, i:2240, we:2174, ...
dobj child:2323, class:2184, student:2025, ...
observe:3
prep in classroom:555, action:509, ...
.
.
.
nsubj we:44833, i:6873, order:4051, ...
dobj card:28835, payment:22569, ...
accept:1
prep for payment:1166, convenience:1147, ...
.
.
.
nsubj i:10568, we:9300, you:5106, ...
dobj that:14180, this:12061, it:7756, ...
accept:2
prep as part:1879, fact:1085, truth:926, ...
.
.
.
nsubj people:7459, he:6696, we:5515, ...
dobj christ:13766, jesus:6528, it:5612, ...
accept:3
prep as savior:5591, lord:597, one:469, ...
.
.
.
Table 1: Examples of resulting frames for the verb
?observe? and ?accept? induced from the web cor-
pus. The number following an instance word rep-
resents its frequency.
4.3 Evaluation of Induced Semantic Frames
We evaluate precision and coverage of induced se-
mantic frames. To measure the precision of in-
duced semantic frames, we adopt the purity met-
ric, which is usually used to evaluate clustering re-
sults. However, the problem is that it is impossible
to assign gold-standard classes to the huge num-
ber of instances. To automatically measure the
purity of the induced semantic frames, we make
use of the SemLink corpus (Loper et al., 2007), in
which VerbNet classes (Kipper-Schuler, 2005) and
PropBank/FrameNet frames are assigned to each
instance. We make a test set that contains 157 pol-
ysemous verbs that occur 10 or more times in the
SemLink corpus (sections 02-21 of the Wall Street
Journal). We first add these instances to the in-
stances from a raw corpus and apply clustering to
these merged instances. Then, we compare the in-
duced semantic frames of the SemLink instances
with their gold-standard classes. We adopt Verb-
Net classes and PropBank frames as gold-standard
classes.
For each group of verb-specific semantic
frames, we measure the purity of the frames as the
percentage of SemLink instances belonging to the
majority gold class in their respective cluster. Let
62
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.802 0.917 0.952 0.854 0.870
VerbNet Initial frames 0.985 0.982 0.755 0.812 0.855 0.889
Induced sem frames 0.900 0.901 0.886 0.928 0.893 0.914
against One frame 0.901 0.872 ? ? 0.909 0.910
PropBank Initial frames 0.994 0.993 ? ? 0.858 0.893
Induced sem frames 0.965 0.949 ? ? 0.924 0.939
Table 2: Evaluation results of semantic frames from the web corpus against VerbNet classes and Prop-
Bank frames. ?Mac? means a macro average and ?Mic? means a micro average.
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.804 0.855 0.920 0.826 0.858
VerbNet Initial frames 0.985 0.981 0.666 0.758 0.795 0.855
Induced sem frames 0.916 0.909 0.796 0.880 0.852 0.894
against One frame 0.901 0.874 ? ? 0.877 0.896
PropBank Initial frames 0.994 0.993 ? ? 0.798 0.859
Induced sem frames 0.968 0.953 ? ? 0.874 0.915
Table 3: Evaluation results of semantic frames from the Gigaword corpus against VerbNet classes and
PropBank frames. ?Mac? means a macro average and ?Mic? means a micro average.
N denote the total number of SemLink instances
of the target verb, G
j
the set of instances belong-
ing to the j-th gold class and F
i
the set of instances
belonging to the i-th frame. The purity (PU) can
then be written as follows:
PU =
1
N
?
i
max
j
|G
j
? F
i
|. (4)
For example, a frame of the verb ?observe? con-
tains 11 SemLink instances, and eight out of them
belong to the class SAY-37.7, which is the ma-
jority class among these 11 instances. PU is cal-
culated by summing up such counts over all the
frames of this verb.
Usually, inverse purity or collocation is used
to measure the recall of normal clustering tasks.
However, these recall measures do not fit our task.
This is because it is not a real error to have similar
separate frames. Instead, we want to avoid hav-
ing so many frames that we cannot provide broad-
coverage selectional preferences due to sparsity.
To judge this aspect, we measure coverage.
The coverage (CO) measures to what extent
predicate-argument structures of the target verb in
a test set are included in one of frames of the verb.
We use the predicate-argument structures of the
above 157 verbs from the SemLink corpus, which
are the same ones used in the evaluation of PU.
We judge a predicate-argument structure as cor-
rect if all of its argument words (of the target slot
described in Section 3.1) are included in the corre-
sponding slot of a frame. If the clustering gets bet-
ter, the value of CO will get higher, because merg-
ing instances by clustering alleviates data sparsity.
These per-verb scores are aggregated into an
overall score by averaging over all verbs. We use
two ways of averaging: a macro average and a mi-
cro average. The macro average is a simple av-
erage of scores for individual verbs. The micro
average is obtained by weighting the scores for in-
dividual verbs proportional to the number of in-
stances for that verb. Finally, we use the harmonic
mean (F
1
) of purity and coverage as a single mea-
sure of clustering quality.
For comparison, we adopt the following two
baseline methods:
One frame a frame into which all the instances
for a verb are merged
Initial frames the initial frames without cluster-
ing (described in Section 3.2)
Table 2 and Table 3 list evaluation results for
semantic frames induced from the web corpus and
the Gigaword corpus, respectively.
4
Note that CO
does not consider gold-standard classes, and thus
the values of CO are the same for the VerbNet
4
We did not adopt inverse purity, but its values for the
induced semantic frames range from 0.42 to 0.49.
63
and PropBank evaluations. The induced frames
outperformed the two baseline methods in terms
of F
1
in most cases. While the coverage of the
web frames was higher than that of the Giga-
word frames, as expected, the purity of the web
frames was slightly lower than that of the Giga-
word frames. This degradation might be caused
by the noise in the web corpus.
The purity of the initial frames was around
98%-99%, which means that there were few cases
that the one-sense-per-collocation assumption was
violated.
Modi et al. (2012) reported a purity of 77.9%
for the assignment of FrameNet frames to the
FrameNet corpus. We also conducted the above
purity evaluation against FrameNet frames for 140
verbs.
5
We obtained a macro average of 92.9%
and a micro average of 89.2% for the web frames,
and a macro average of 93.2% and a micro average
of 89.8% for the Gigaword frames. It is difficult
to directly compare these results with Modi et al.
(2012), but our frame assignments seem to have
higher accuracy.
4.4 Evaluation against CPA Frames
Corpus Pattern Analysis (CPA) is a technique for
linking word usage to prototypical syntagmatic
patterns.
6
The resource was built manually by in-
vestigating examples in the BNC, and the set of
corpus examples used to induce each pattern is
given. For example, the following three patterns
describe the usage of the verb ?accommodate.?
[Human 1] accommodate [Human 2]
[Building] accommodate [Eventuality]
[Human] accommodate [Self] to [Eventuality]
In this paper, we use CPA to evaluate the quality
of the automatically induced frames. By compar-
ing the induced frames to CPA patterns, we can
evaluate the correctness and relevance of this ap-
proach from a human point of view. To do that,
we associate semantic features to the set of words
in each slot in the frames, using SUMO (Niles
and Pease, 2001). For example, take the follow-
ing frame for the verb ?accomplish?:
accomplish:1
nsubj:{you, leader, employee, ...}
dobj:{developing, progress, objective, ...}.
5
Since FrameNet frames are not assigned to all the verbs
of SemLink, the number of verbs is different from the evalu-
ations against VerbNet and PropBank.
6
http://deb.fi.muni.cz/pdev/
all K-means
Entropy (E) 0.790 0.516
Recovery Rate (RC) 0.347 0.630
Purity (P ) 0.462 0.696
Table 4: CPA Evaluation.
Using SUMO, we map this frame to the following:
nsubj: [Human]
dobj: [SubjectiveAssessmentAttribute],
which corresponds to pattern 3 for ?accomplish?
in CPA.
We also associate SUMO attributes to the CPA
patterns with more than 10 examples (716 verbs).
There are many patterns of SUMO attributes for
any CPA frame or induced frame, since each
filler word in a particular slot can have more
than one SUMO attribute. We filter out the
non-discriminative SUMO attributes following the
technique described in Popescu (2013). Using
this, we obtain SUMO attributes for both CPA
clusters and induced frames, and we can use the
standard entropy-based measures to evaluate the
match between the two types of patterns: E ? en-
tropy, RC ? recovery rate, and P ? purity (Li et
al., 2004):
E =
K
?
j=1
m
j
m
? e
j
, RC = 1 ?
K,L
?
j,i=1
p
ij
m
i
, (5)
P =
K
?
j=1
m
j
m
? p
j
, p
j
= max
i
p
ij
, (6)
e
j
=
L
?
i=1
p
ij
log
2
p
ij
, p
ij
=
m
ij
m
i
, (7)
where m
j
is the number of induced frames corre-
sponding to topic j, m
ij
is the number of induced
frames in cluster j and annotated with the CPA
pattern i, m is the total number of induced frames,
L is the number of CPA patterns, and K is the
number of induced frames.
We also consider a K-means clustering process,
with K set as 2 or 3 depending on the number of
SUMO-attributed patterns. The K-means evalu-
ation is carried out considering only the centroid
of the cluster, which corresponds to the prototypi-
cal induced semantic frame with SUMO attributes.
We compute E, RC and P using formulae (5) -
(7) for each verb and then compute the macro av-
erage, considering all the frames and only the K-
means centroids, respectively. The results for the
induced web frames are displayed in Table 4.
64
The evaluation method presented here over-
comes some of the drawbacks of the previous ap-
proaches (Materna, 2012; Materna, 2013). First,
we did not limit the evaluation to the most frequent
patterns. Second, the mapping was carried out au-
tomatically and not by hand. The results above
compare favorably with the previous approaches,
especially considering that no filtering procedures
were applied to the induced frames. We anticipate
that the results based on the prototypical induced
frames with SUMO attributes would be competi-
tive. Our post-analysis revealed that the entropy
can be lowered further if an automatic filtering
based on frequencies is applied.
4.5 Evaluation of the Quality of Selectional
Preferences
We also investigated the quality of selectional
preferences within the induced semantic frames.
The only publicly available test data for selectional
preferences, to our knowledge, is from Chambers
and Jurafsky (2010). This data consists of quadru-
ples (verb, relation, word, confounder) and does
not contain their context.
7
A typical way for using our semantic frames is
to select an appropriate frame for an input sen-
tence and judge the eligibility of the word uses
against the selected frame. However, due to the
lack of context for the above data, it is difficult to
select a corresponding semantic frame for a test
quadruple and thus the induced semantic frames
cannot be naturally applied to this data. To in-
vestigate the potential for selectional preferences
of the semantic frames, we approximately match
a quadruple with each of the semantic frames of
the verb and select the frame that has the highest
probability as follows:
P (w) = max
i
P (w|v, rel, f
i
), (8)
where w is the word or confounder, v is the verb,
rel is the relation and f
i
is a semantic frame. By
comparing the probabilities of the word and the
confounder, we select either of them according to
the higher probability. For tie breaking in the case
that no frames are found for the verb or both the
word and confounder are not found in the case slot,
we randomly select either of them in the same way
as Chambers and Jurafsky (2010).
We use the ?neighbor frequency? set, which is
the most difficult among the three sets included
7
A document ID of the English Gigaword corpus is avail-
able, but it is difficult to recover the context of each instance
from this information.
in the data. It contains 6,767 quadruples and the
relations consist of three classes: subject, object
and preposition, which has no distinction of ac-
tual prepositions. To link these relations with our
case slots, we manually aligned the subject with
the nsubj (nominal subject) slot, the object with
the dobj (direct object) slot and the preposition
with prep * (all the prepositions) slots. For the
preposition relation, we choose the highest prob-
ability among all the preposition slots in a frame.
To match the generalized ?name? with the word in
a quadruple, we change the word to ?name? if it is
capitalized and not a capitalized personal pronoun.
Our semantic frames from the Gigaword corpus
achieved an accuracy of 81.7%
8
and those from
the web corpus achieved an accuracy of 80.2%.
This slight deterioration seems to come from the
noise in the web corpus. The best performance
in Chambers and Jurafsky (2010) is 81.7% on
this ?neighbor frequency? set, which was achieved
by conditional probabilities with the Erk (2007)?s
smoothing method calculated from the English Gi-
gaword corpus. Our approach for selectional pref-
erences does not use smoothing like Erk (2007),
but it achieved equivalent performance to the pre-
vious work. If we applied our semantic frames to a
verb instance with its context, a more precise judg-
ment of selectional preferences would be possible
with appropriate frame selection.
5 Conclusion
This paper has described an unsupervised method
for inducing semantic frames from instances of
each verb in giga-word corpora. This method is
clustering based on the Chinese Restaurant Pro-
cess. The resulting frame data are open to the pub-
lic and also can be searched by inputting a verb via
our web interface.
9
As applications of the resulting frames, we plan
to integrate them into syntactic parsing, semantic
role labeling and verb sense disambiguation. For
instance, Kawahara and Kurohashi (2006b) im-
proved accuracy of dependency parsing based on
Japanese semantic frames automatically induced
from a large raw corpus. It is valuable and promis-
ing to apply our semantic frames to these NLP
tasks.
8
Since the dataset was created from the NYT 2001 portion
of the English Gigaword Corpus, we built semantic frames
again from the Gigaword corpus except this part.
9
http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
65
Acknowledgments
This work was supported by Kyoto University
John Mung Program and JST CREST. We grate-
fully acknowledge the support of the National Sci-
ence Foundation Grant NSF 1116782 - RI: Small:
A Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 226?236.
David Aldous. 1985. Exchangeability and related top-
ics.
?
Ecole d?
?
Et?e de Probabilit?es de Saint-Flour XIII
?1983, pages 1?198.
Collin Baker, Charles J. Fillmore, and John Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 86?90.
Hans C. Boas. 2002. Bilingual framenet dictionaries
for machine translation. In Proceedings of the 3rd
International Conference on Language Resources
and Evaluation, pages 1364?1371.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing, pages 356?363.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating textual entailment with LFG and FrameNet
frames. In Proceedings of the 2nd PASCAL Recog-
nizing Textual Entailment Workshop, pages 92?97.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 445?453.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Michael Ellsworth and Adam Janin. 2007. Mu-
taphrase: Paraphrasing with framenet. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 143?150.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216?223.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying french verbs using french and en-
glish lexical resources. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 854?863.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1?
8.
Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. Input,
Process and Product: Developments in Teaching
and Language Corpora, pages 54?69.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176?183.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345?352.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939?947.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1117?1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1320?1331.
Tao Li, Sheng Ma, and Mitsunori Ogihara. 2004.
Entropy-based criterion in categorical clustering. In
Proceedings of the 21st International Conference on
Machine Learning, volume 4, pages 536?543.
66
Thomas Lippincott, Anna Korhonen, and Diarmuid
?
O S?eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420?429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Christopher Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, pages 235?
242.
Ji?r?? Materna. 2012. LDA-Frames: An unsupervised
approach to generating semantic frames. In Alexan-
der Gelbukh, editor, Proceedings of the 13th Inter-
national Conference CICLing 2012, Part I, volume
7181 of Lecture Notes in Computer Science, pages
376?387. Springer Berlin / Heidelberg.
Ji?r?? Materna. 2013. Parameter estimation for LDA-
Frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482?486.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1?7.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693?701.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Proceedings of the International
Conference on Formal Ontology in Information Sys-
tems, pages 2?9.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Christopher Parisien and Suzanne Stevenson. 2009.
Modelling the acquisition of verb polysemy in chil-
dren. In Proceedings of the CogSci2009 Workshop
on Distributional Semantics beyond Concrete Con-
cepts, pages 17?22.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd annual
meeting of the Cognitive Science Society.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Octavian Popescu. 2013. Learning corpus patterns us-
ing finite state automata. In Proceedings of the 10th
International Conference on Computational Seman-
tics, pages 191?203.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862?872.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi,
and Manabu Okumura. 2013. Automatic knowl-
edge acquisition for case alternation between the
passive and active voices in Japanese. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1213?1223.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445?1455.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
74?82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266?271.
67
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030?1040,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Step-wise Usage-based Method for Inducing
Polysemy-aware Verb Classes
Daisuke Kawahara
?
Daniel W. Peterson
?
Martha Palmer
?
?
Kyoto University, Kyoto, Japan
?
University of Colorado at Boulder, Boulder, CO, USA
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu
Abstract
We present an unsupervised method for in-
ducing verb classes from verb uses in giga-
word corpora. Our method consists of
two clustering steps: verb-specific seman-
tic frames are first induced by clustering
verb uses in a corpus and then verb classes
are induced by clustering these frames.
By taking this step-wise approach, we can
not only generate verb classes based on a
massive amount of verb uses in a scalable
manner, but also deal with verb polysemy,
which is bypassed by most of the previous
studies on verb clustering. In our exper-
iments, we acquire semantic frames and
verb classes from two giga-word corpora,
the larger comprising 20 billion words.
The effectiveness of our approach is veri-
fied through quantitative evaluations based
on polysemy-aware gold-standard data.
1 Introduction
A verb plays a primary role in conveying the
meaning of a sentence. Capturing the sense of a
verb is essential for natural language processing
(NLP), and thus lexical resources for verbs play
an important role in NLP.
Verb classes are one such lexical resource.
Manually-crafted verb classes have been devel-
oped, such as Levin?s classes (Levin, 1993) and
their extension, VerbNet (Kipper-Schuler, 2005),
in which verbs are organized into classes on the
basis of their syntactic and semantic behavior.
Such verb classes have been used in many NLP ap-
plications that need to consider semantics in par-
ticular, such as word sense disambiguation (Dang,
2004), semantic parsing (Swier and Stevenson,
2005; Shi andMihalcea, 2005) and discourse pars-
ing (Subba and Di Eugenio, 2009).
There have also been many attempts to auto-
matically acquire verb classes with the goal of ei-
ther adding frequency information to an existing
resource or of inducing similar verb classes for
other languages. Most of these approaches assume
that all target verbs are monosemous (Stevenson
and Joanis, 2003; Schulte im Walde, 2006; Joa-
nis et al, 2008; Li and Brew, 2008; Sun et al,
2008; Sun and Korhonen, 2009; Vlachos et al,
2009; Parisien and Stevenson, 2010; Parisien and
Stevenson, 2011; Falk et al, 2012; Lippincott et
al., 2012; Reichart and Korhonen, 2013; Sun et al,
2013). This monosemous assumption, however, is
not realistic because many frequent verbs actually
have multiple senses. Moreover, to the best of our
knowledge, none of the following approaches at-
tempt to quantitatively evaluate soft clusterings of
verb classes induced by polysemy-aware unsuper-
vised approaches (Korhonen et al, 2003; Lapata
and Brew, 2004; Li and Brew, 2007; Schulte im
Walde et al, 2008).
In this paper, we propose an unsupervised
method for inducing verb classes that is aware
of verb polysemy. Our method consists of two
clustering steps: verb-specific semantic frames are
first induced by clustering verb uses in a cor-
pus and then verb classes are induced by clus-
tering these frames. By taking this step-wise ap-
proach, we can not only induce verb classes with
frequency information from a massive amount of
verb uses in a scalable manner, but also deal with
verb polysemy.
Our novel contributions are summarized as fol-
lows:
? induce both semantic frames and verb classes
from a massive amount of verb uses by a scal-
able method,
? explicitly deal with verb polysemy,
? discover effective features for each of the
clustering steps, and
? quantitatively evaluate a soft clustering of
verbs.
1030
!"#$%&'(#)*#+%!"#%#,#-!( %%%$&.%&'(#)*#%$&.)%-"/0+%
-"/0+)#1%&'(#)*#+%'/)+( %%%2#%&'(#)*#+%1/-#%'/)+(%
3#%&'(#)*#+%!"#%)#(.0! %%%%2#%&'(#)*#+%445%6#&60#%
7 % % % % %%%7%
7 % % % % %%%7!
&'(#)*#84%
9!"#$:%"#:%7;%&'(#)*#%
%%%%%%%9#,#-!:%)#(.0!:%7;!
&'(#)*#8<%
9$&.:%2#:%7;%&'(#)*#%
%%%%%%9-"/0+:%6#&60#:%7;!
&'(#)*#8=%
92#:%-"/0+:%7;%&'(#)*#%
%%%%%%%%%9'/)+:%2/0+0/>#:%7;!
2?!-"8@%
9A:%2#:%7;%2?!-"%
%%%%%%%%%9'/)+:%?1/B?0:%7;!
7!
/1*#(CD?!#E=@FG! (/D"!E=5F<!
2#%2?!-"%&.)%'/)+(%
A%2?!-"#+%!"#%B&*/#%
H#%2/00%2?!-"%!"#%D?B#%
7%
7!
7!
I#)'%-0?((#(8!
J#B?1C-%>)?B#(8!
I#)'%.(#(8!
Figure 1: Overview of our two-step approach. Verb-specific semantic frames are first induced from verb
uses (lower part) and then verb classes are induced from the semantic frames (upper part). The labels of
verb classes are manually assigned here for better understanding.
2 Related Work
As stated in Section 1, most of the previous studies
on verb clustering assume that verbs are monose-
mous. A typical method in these studies is to rep-
resent each verb as a single data point and apply
classification (e.g., Joanis et al (2008)) or clus-
tering (e.g., Sun and Korhonen (2009)) to these
data points. As a representation for a data point,
distributions of subcategorization frames are often
used, and other semantic features (e.g., selectional
preferences) are sometimes added to improve the
performance.
Among these studies on monosemous verb clus-
tering (i.e., predominant class induction), there
have been several Bayesian methods. Vlachos
et al (2009) proposed a Dirichlet process mix-
ture model (DPMM; Neal (2000)) to cluster verbs
based on subcategorization frame distributions.
They evaluated their result with a gold-standard
test set, where a single class is assigned to a verb.
Parisien and Stevenson (2010) proposed a hierar-
chical Dirichlet process (HDP; Teh et al (2006))
model to jointly learn argument structures (sub-
categorization frames) and verb classes by using
syntactic features. Parisien and Stevenson (2011)
extended their model by adding semantic features.
They tried to account for verb learning by children
and did not evaluate the resultant verb classes.
Modi et al (2012) extended the model of Titov
and Klementiev (2012), which is an unsupervised
model for inducing semantic roles, to jointly in-
duce semantic roles and frames across verbs using
the Chinese Restaurant Process (Aldous, 1985).
All of the above methods considered verbs to be
monosemous and did not deal with verb polysemy.
Our approach also uses Bayesian methods, but is
designed to capture verb polysemy.
We summarize a few studies that consider poly-
semy of verbs in the rest of this section.
Miyao and Tsujii (2009) proposed a supervised
method that can handle verb polysemy. Their
method represents a verb?s syntactic and seman-
tic features, and learns a log-linear model from
the SemLink corpus (Loper et al, 2007). Boleda
et al (2007) also proposed a supervised method
for Catalan adjectives considering the polysemy of
adjectives.
The most closely related work to our polysemy-
aware task of unsupervised verb class induction is
the work of Korhonen et al (2003), who used dis-
tributions of subcategorization frames to cluster
verbs. They adopted the Nearest Neighbor (NN)
and Information Bottleneck (IB) methods for clus-
tering. In particular, they tried to consider verb
polysemy by using the IB method, which is a soft
clustering method (Tishby et al, 1999). However,
the verb itself is still represented as a single data
point. After performing soft clustering, they noted
that most verbs fell into a single class, and they
decided to assign a single class to each verb by
hardening the clustering. They considered multi-
ple classes only in the gold-standard data used for
their evaluations. We also evaluate our induced
verb classes on this gold-standard data, which was
created on the basis of Levin?s classes (Levin,
1993).
Lapata and Brew (2004) and Li and Brew
(2007) proposed probabilistic models for calculat-
ing prior probabilities of verb classes for a verb.
These models are approximated to condition not
1031
on verbs but on subcategorization frames. As
mentioned in Li and Brew (2007), it is desirable
to extend the model to depend on verbs to fur-
ther improve accuracy. They conducted several
evaluations including predominant class induction
and token-level verb sense disambiguation, but did
not evaluate multiple classes output by their mod-
els. Schulte im Walde et al (2008) also applied
probabilistic soft clustering to verbs by incorporat-
ing subcategorization frames and selectional pref-
erences based on WordNet. This model is based
on the Expectation-Maximization algorithm and
the Minimum Description Length principle. Since
they focused on the incorporation of selectional
preferences, they did not evaluate verb classes but
evaluated only selectional preferences using a lan-
guage model-based measure.
Materna proposed LDA-frames, which are de-
fined across verbs and can be considered to be
a kind of verb class (Materna, 2012; Materna,
2013). LDA-frames are probabilistic semantic
frames automatically induced from a raw corpus.
He used a model based on latent Dirichlet alo-
cation (LDA; Blei et al (2003)) and the Dirichlet
process to cluster verb instances of a triple (sub-
ject, verb, object) to produce semantic frames and
roles. Both of these are represented as a proba-
bilistic distribution of words across verbs. He ap-
plied this method to the BNC and acquired 1,200
frames and 400 roles (Materna, 2012). He did not
evaluate the resulting frames as verb classes.
In sum, there have been no studies that quantita-
tively evaluate polysemous verb classes automati-
cally induced by unsupervised methods.
3 Our Approach
3.1 Overview
Our objective is to automatically learn semantic
frames and verb classes from a massive amount
of verb uses following usage-based approaches.
Although Bayesian approaches are a possible so-
lution to simultaneously induce frames and verb
classes from a corpus as used in previous stud-
ies, it has prohibitive computational cost. For in-
stance, Parisien and Stevenson applied HDP only
to a small-scale child speech corpus that contains
170K verb uses to jointly induce subcategoriza-
tion frames and verb classes (Parisien and Steven-
son, 2010; Parisien and Stevenson, 2011). Ma-
terna applied an LDA-based method to the BNC,
which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to
be verb classes (Materna, 2012; Materna, 2013).
However, it would take three months for this ex-
periment using this 100 million word corpus.
1
Al-
though it is best to use the largest possible cor-
pus for this kind of knowledge acquisition tasks
(Sasano et al, 2009), it is infeasible to scale to
giga-word corpora using such joint models.
In this paper, we propose a two-step approach
for inducing semantic frames and verb classes.
First, we make multiple data points for each verb
to deal with verb polysemy (cf. polysemy-aware
previous studies still represented a verb as one
data point (Korhonen et al, 2003; Miyao and Tsu-
jii, 2009)). To do that, we induce verb-specific
semantic frames by clustering verb uses. Then,
we induce verb classes by clustering these verb-
specific semantic frames across verbs. An interest-
ing point here is that we can use exactly the same
method for these two clustering steps.
Our procedure to automatically induce verb
classes from verb uses is summarized as follows:
1. induce verb-specific semantic frames by clus-
tering predicate-argument structures for each
verb extracted from automatic parses as
shown in the lower part of Figure 1, and
2. induce verb classes by clustering the induced
semantic frames across verbs as shown in the
upper part of Figure 1.
Each of these two steps is described in the follow-
ing sections in detail.
3.2 Inducing Verb-specific Semantic Frames
We induce verb-specific semantic frames from
verb uses based on the method of Kawahara et al
(2014). Our semantic frames consist of case slots,
each of which consists of word instances that can
be filled. The procedure for inducing these seman-
tic frames is as follows:
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based
on the assumption of one sense per colloca-
tion (Yarowsky, 1993) to get a set of initial
frames, and
1
In our replication experiment, it took a week to perform
70 iterations usingMaterna?s code and an Intel Xeon E5-2680
(2.7GHz) CPU. To reach 1,000 iterations, which are reported
to be optimum, it would take three months.
1032
3. apply clustering to the initial frames based
on the Chinese Restaurant Process (Al-
dous, 1985) to produce verb-specific seman-
tic frames.
These three steps are briefly described below.
3.2.1 Extracting Predicate-argument
Structures from a Raw Corpus
We apply dependency parsing to a large raw cor-
pus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al, 2006).
2
Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ?
In this process, the verb and arguments are lem-
matized, and only the head of an argument is pre-
served for compound nouns.
Predicate-argument structures are collected for
each verb and the subsequent processes are ap-
plied to the predicate-argument structures of each
verb.
3.2.2 Constructing Initial Frames from
Predicate-argument Structures
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:
3
dobj, ccomp, nsubj, prep ?, iobj.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
word, e.g., ?dobj:effect?) are merged into an ini-
tial frame. After this process, we discard minor
initial frames that occur fewer than 10 times.
2
http://nlp.stanford.edu/software/lex-parser.shtml
3
If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
3.2.3 Clustering Method
We cluster initial frames for each verb to pro-
duce semantic frames using the Chinese Restau-
rant Process (Aldous, 1985), regarding each initial
frame as an instance.
We calculate the posterior probability of a clus-
ter c
j
given an initial frame f
i
as follows:
P (c
j
|f
i
) ?
{
n(c
j
)
N+?
? P (f
i
|c
j
) c
j
?= new
?
N+?
? P (f
i
|c
j
) c
j
= new,
(1)
whereN is the number of initial frames for the tar-
get verb and n(c
j
) is the current number of initial
frames assigned to the cluster c
j
. ? is a hyper-
parameter that determines how likely it is for a
new cluster to be created. In this equation, the first
term is the Dirichlet process prior and the second
term is the likelihood of f
i
.
P (f
i
|c
j
) is defined based on the Dirichlet-
Multinomial distribution as follows:
P (f
i
|c
j
) =
?
w?V
P (w|c
j
)
count(f
i
,w)
, (2)
where V is the vocabulary in all case slots cooc-
curring with the verb and count(f
i
, w) is the num-
ber of w in the initial frame f
i
. The original
method in Kawahara et al (2014) defined w as
pairs of slots and words, e.g., ?nsubj:child? and
?dobj:bird,? but does not consider slot-only fea-
tures, e.g., ?nsubj? and ?dobj,? which ignore lex-
ical information. Here we experiment with both
representations and compare the results.
P (w|c
j
) is defined as follows:
P (w|c
j
) =
count(c
j
, w) + ?
?
t?V
count(c
j
, t) + |V | ? ?
, (3)
where count(c
j
, w) is the current number of w
in the cluster c
j
, and ? is a hyper-parameter of
Dirichlet distribution. For a new cluster, this prob-
ability is uniform (1/|V |).
We regard each output cluster as a semantic
frame, by merging the initial frames in a clus-
ter into a semantic frame. In this way, semantic
frames for each verb are acquired.
We use Gibbs sampling to realize this cluster-
ing.
3.3 Inducing Verb Classes from Semantic
Frames
To induce verb classes across verbs, we apply
clustering to the induced verb-specific semantic
1033
frames. We can use exactly the same clustering
method as described in Section 3.2.3 by using se-
mantic frames for multiple verbs as an input in-
stead of initial frames for a single verb. This is
because an initial frame has the same structure as
a semantic frame, which is produced by merging
initial frames. We regard each output cluster as a
verb class this time.
For the features, w, in equation (2), we try the
two representations again: slot-only features and
slot-word pair features. The representation using
only slots corresponds to the consideration of only
syntactic argument patterns. The other representa-
tion using the slot-word pairs means that semantic
similarity based on word overlap is naturally con-
sidered by looking at lexical information. We will
compare in our experiments four possible combi-
nations: two feature representations for each of the
two clustering steps.
4 Experiments and Evaluations
We first describe our experimental settings and de-
fine evaluation metrics to evaluate induced soft
clusterings of verb classes. Then, we con-
duct type-level multi-class evaluations, type-level
single-class evaluations and token-level multi-
class evaluations. These two levels of evaluations
are performed by considering the work of Reichart
et al (2010) on clustering evaluation. Finally, we
discuss the results of our full experiments.
4.1 Experimental Settings
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sen-
tences from crawled web pages that are judged to
be written in English based on the encoding infor-
mation. Then, we selected sentences that consist
of at most 40 words, and removed duplicated sen-
tences. From this process, we obtained a corpus of
one billion sentences, totaling approximately 20
billion words. We focused on verbs whose fre-
quency in the web corpus was more than 1,000.
There were 19,649 verbs, including phrasal verbs,
and separating passive and active constructions.
We extracted 2,032,774,982 predicate-argument
structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition).
This corpus consists of approximately 180 mil-
lion sentences, which totaling four billion words.
There were 7,356 verbs after applying the same
frequency threshold as the web corpus. We ex-
tracted 423,778,278 predicate-argument structures
from this corpus.
We set the hyper-parameters ? in (1) and ? in
(3) to 1.0. The cluster assignments for all the com-
ponents were initialized randomly. We took 100
samples for each input frame and selected the clus-
ter assignment that has the highest probability.
4.2 Evaluation Metrics
To measure the precision and recall of a cluster-
ing, modified purity and inverse purity (also called
collocation or weighted class accuracy) are com-
monly used in previous studies on verb clustering
(e.g., Sun and Korhonen (2009)). However, since
these measures are only applicable to a hard clus-
tering, it is necessary to extend them to be applica-
ble to a soft clustering, because in our task a verb
can belong to multiple clusters or classes.
4
We
propose a normalized version of modified purity
and inverse purity. This kind of normalization for
soft clusterings was performed for other evalua-
tion metrics as in Springorum et al (2013).
To measure the precision of a clustering, a nor-
malized version of modified purity is defined as
follows. Suppose K is the set of automatically in-
duced clusters and G is the set of gold classes. Let
K
i
be the verb vector of the i-th cluster and G
j
be
the verb vector of the j-th gold class. Each com-
ponent of these vectors is a normalized frequency,
which equals a cluster/class attribute probability
given a verb. Where there is no frequency in-
formation available for class distribution, such as
the gold-standard data described in Section 4.3,
we use a uniform distribution across the verb?s
classes. The core idea of purity is that each clus-
ter K
i
is associated with its most prevalent gold
class. In addition, to penalize clusters that consist
of only one verb, such singleton clusters in K are
considered as errors, as is usual with modified pu-
rity. The normalized modified purity (nmPU) can
then be written as follows:
nmPU =
1
N
?
i s.t. |K
i
|>1
max
j
?
K
i
(K
i
? G
j
), (4)
?
K
i
(K
i
? G
j
) =
?
v?K
i
?G
j
c
iv
, (5)
4
Korhonen et al (2003) evaluated hard clusterings based
on a gold standard with multiple classes per verb. They re-
ported only precision measures including modified purity,
and avoided extending the evaluation metrics for soft clus-
terings.
1034
verb classes verb classes
place 9 drop 9, 45, 004, 47,
51, A54, A30dye 24, 21, 41
focus 31, 45 bake 26, 45
stare 30 persuade 002
lay 9 sparkle 43
build 26, 45 pour 9, 43, 26, 57,
13, 31force 002, 11
glow 43 invent 26, 27
Table 1: An excerpt of the gold-standard verb
classes for several verbs from Korhonen et al
(2003). The classes starting with ?0? were de-
rived from the LCS database, those starting with
?A? were defined by Korhonen et al, and the other
classes were from Levin?s classes. A bolded class
is the predominant class for each verb.
where N denotes the total number of verbs, |K
i
|
denotes the number of positive components in
K
i
, and c
iv
denotes the v-th component of K
i
.
?
K
i
(K
i
? G
j
) means the total mass of the set of
verbs in K
i
?G
j
, given by summing up the values
in K
i
. In case of evaluating a hard clustering, this
is equal to |K
i
? G
j
| because all the values of c
iv
are equal to 1.
As usual, the following normalized inverse pu-
rity (niPU) is used to measure the recall of a clus-
tering:
niPU =
1
N
?
j
max
i
?
G
j
(K
i
? G
j
). (6)
Finally, we use the harmonic mean (F
1
) of nmPU
and niPU as a single measure of clustering quality.
4.3 Type-level Multi-class Evaluations
We first evaluate our induced verb classes on the
test set created by Korhonen et al (2003) (Table 1
of their paper) which was created by considering
verb polysemy on the basis of Levin?s classes and
the LCS database (Dorr, 1997). It consists of 62
classes and 110 verbs, out of which 35 verbs are
monosemous and 75 verbs are polysemous. The
average number of verb classes per verb is 2.24.
An excerpt from this data is shown in Table 1.
As our baselines, we adopt two previously pro-
posed methods. We first implemented a soft clus-
tering method for verb class induction proposed by
Korhonen et al (2003). They used the information
bottleneck (IB) method for assigning probabilities
of classes to each verb. Note that Korhonen et al
(2003) actually hardened the clusterings and left
method K nmPU niPU F
1
IB (k=35, t=0.10) 35.0 53.59 51.44 52.44
IB (k=35, t=0.05) 35.0 53.67 52.62 53.10
IB (k=35, t=0.02) 35.0 54.42 54.43 54.40
IB (k=35, t=0.01) 35.0 54.60 55.54 55.04
IB (k=42, t=0.10) 41.6 55.42 49.46 52.24
IB (k=42, t=0.05) 41.8 55.55 49.97 52.59
IB (k=42, t=0.02) 42.0 56.19 51.24 53.58
IB (k=42, t=0.01) 42.0 56.80 51.92 54.24
LDA-frames (t=0.10) 100 47.52 56.83 51.76
LDA-frames (t=0.05) 165 50.46 67.94 57.91
LDA-frames (t=0.02) 306 49.98 75.50 60.14
LDA-frames (t=0.01) 458 49.55 82.71 61.97
Gigaword/S-S 272.8 63.46 67.66 65.49
Gigaword/S-SW 36.4 31.49 95.70 47.38
Gigaword/SW-S 186.2 63.52 64.18 63.84
Gigaword/SW-SW 30.0 36.27 94.66 52.40
web/S-S 363.6 61.32 78.64 68.90
web/S-SW 52.2 35.80 99.30 52.62
web/SW-S 212.2 66.26 77.38 71.39
web/SW-SW 55.0 36.70 96.25 53.13
Table 2: Type-level multi-class evaluations. K rep-
resents the (average) number of induced classes.
?S? denotes the use of slot-only features and ?SW?
denotes the use of slot-word pair features. For ex-
ample, ?SW-S? means that slot-word pair features
are used for semantic frame induction and slot-
only features are used for verb class induction.
the evaluations of soft clusterings for their future
work. For input data, we employ VALEX (Ko-
rhonen et al, 2006), which is a publicly-available
large-scale subcategorization lexicon.
5
By follow-
ing the method of Korhonen et al (2003), preposi-
tional phrases (pp) are parameterized for two fre-
quent subcategorization frames (NP and NP PP),
and the unfiltered raw frequencies of subcatego-
rization frames are used as features to represent
a verb. It is necessary to specify the number of
clusters, k, for the IB method beforehand, and
we adopt 35 and 42 clusters according to their re-
ported high accuracies. To output multiple classes
for each verb, we set a threshold, t, for class at-
tribute probabilities. That is, classes that have a
higher class attribute probability than the thresh-
old are output for each verb. We report the results
of the following threshold values: 0.01, 0.02, 0.05
and 0.10.
The other baseline is LDA-frames (Materna,
2012). We use the induced LDA-frames that are
5
http://ilexir.co.uk/applications/valex/
1035
predominant class eval multiple class eval
method K mPU iPU F
1
mPU niPU F
1
NN 24 46.36 52.73 49.34 52.73 46.85 49.62
IB (k=35) 34.8 42.73 51.82 46.82 51.64 46.83 49.09
IB (k=42) 41.0 47.45 50.91 49.11 55.27 45.45 49.87
LDA-frames 53 30.00 47.27 36.71 41.82 44.28 43.01
Gigaword/S 9.6 25.64 71.27 37.70 32.91 64.71 43.62
Gigaword/SW 10.6 30.36 71.09 42.25 39.82 66.92 49.70
web/S 20.4 42.73 61.46 50.31 54.91 57.12 55.86
web/SW 11.8 34.36 71.82 46.40 49.09 67.01 56.50
Table 3: Type-level single-class evaluations against predominant/multiple classes. K represents the (av-
erage) number of induced classes.
available on the web site.
6
This frame data was in-
duced from the BNC and consists of 1,200 frames
and 400 semantic roles. Again, we set a threshold
for frame attribute probabilities.
We report results using our methods with four
feature combinations (slot-only (S) and slot-word
pair (SW) features each used for both the frame-
generation and verb-class clustering steps) for
both the Gigaword and web corpora. Table 2 lists
evaluation results for the baseline methods and our
methods.
7
The results of the IB baseline and our
methods are obtained by averaging five runs.
We can see that ?web/SW-S? achieved the best
performance and obtained a higher F
1
than the
baselines by more than nine points. ?Web/SW-
S? uses the combination of slot-word pair fea-
tures for clustering verb-specific frames and slot-
only features for clustering across verbs. Inter-
estingly, this result indicates that slot distributions
are more effective than lexical information in slot-
word pairs for inducing verb classes similar to the
gold standard. This result is consistent with ex-
pectations, given a gold standard based on Levin?s
verb classes, which are organized according to the
syntactic behavior of verbs. The use of slot-word
pairs for verb class induction generally merged too
many frames into each class, apparently due to ac-
cidental word overlaps across verbs.
The verb classes induced from the web corpus
achieved a higher F
1
than those from the Gigaword
corpus. This can be attributed to the larger size of
the web corpus. The employment of this kind of
huge corpus is enabled by our scalable method.
6
http://nlp.fi.muni.cz/projekty/lda-frames/
7
Although we do not think that the classes with very small
attribute probabilities are meaningful, the F
1
scores for lower
thresholds than 0.01 converged to about 66 in the case of
LDA-frames.
4.4 Type-level Single-class Evaluations
against Predominant/Multiple Classes
Since we focus on the handling of verb polysemy,
predominant class induction for each verb is not
our main objective. However, we wish to compare
our method with previous work on the induction of
a predominant (monosemous) class for each verb.
To output a single class for each verb by us-
ing our proposed method, we skip the induction
of verb-specific semantic frames and instead cre-
ate a single frame for each verb by merging all
predicate-argument structures of the verb. Then,
we apply clustering to these frames across verbs.
For clustering features, we again compare two rep-
resentations: slot-only features (S) and slot-word
pair features (SW).
We evaluate the single-class output for each
verb based on the predominant gold-standard
classes, which are defined for each verb in the
test set of Korhonen et al (2003). This data con-
tains 110 verbs and 33 classes. We evaluate these
single-class outputs in the same manner as Korho-
nen et al (2003), using the gold standard with mul-
tiple classes, which we also use for our multi-class
evaluations.
As we did with the multi-class evaluations, we
adopt modified purity (mPU), inverse purity (iPU)
and their harmonic mean (F
1
) as the metrics for the
evaluation with predominant classes. It is not nec-
essary to normalize these metrics when we treat
verbs as monosemous, and evaluate against the
predominant sense. When we evaluate against the
multiple classes in the gold standard, we do nor-
malize the inverse purity.
For baselines, we once more adopt the Nearest
Neighbor (NN) and Information Bottleneck (IB)
methods proposed by Korhonen et al (2003), and
LDA-frames proposed by Materna (2012). The
1036
clusterings with the NN and IB methods are ob-
tained by using the VALEX subcategorization lex-
icon. To harden the clusterings of the IB method
and the LDA-frames, the class with the highest
probability is selected for each verb. This hard-
ening process is exactly the same as Korhonen et
al. (2003). Note that our results of the NN and IB
methods are different from those reported in their
paper since the data source is different.
8
Table 3 lists accuracies of baseline methods and
our methods. Our proposed method using the web
corpus achieved comparable performance with the
baseline methods on the predominant class evalu-
ation and outperformed them on the multiple class
evaluation. More sophisticated methods for pre-
dominant class induction, such as the method of
Sun and Korhonen (2009) using selectional pref-
erences, could produce better single-class outputs,
but have difficulty in producing polysemy-aware
verb classes.
From the result, we can see that the induced
verb classes based on slot-only features did not
achieve a higher F
1
than those based on slot-word
pair features in many cases. This result is differ-
ent from that of multi-class evaluations in Section
4.3. We speculate that slot distributions are not so
different among verbs when all uses of a verb are
merged into one frame, and thus their discrimina-
tion power is lower than that in the intermediate
construction of semantic frames.
4.5 Token-level Multi-class Evaluations
We conduct token-level multi-class evaluations us-
ing 119 verbs, which appear 100 or more times in
sections 02-21 of the SemLink WSJ corpus. These
119 verbs cover 102 VerbNet classes, and 48 of
them are polysemous in the sense of being in more
than one VerbNet class. Each instance of these 119
verbs in this corpus belongs to one of 102 Verb-
Net classes. We first add these instances to the
instances from a raw corpus and apply the two-
step clustering to these merged instances. Then,
we compare the induced verb classes of the Sem-
Link instances with their gold-standard VerbNet
classes. We report the values of modified purity
(mPU), inverse purity (iPU) and their harmonic
mean (F
1
). It is not necessary to normalize these
metrics because the clustering of these instances is
hard.
8
Korhonen et al (2003) reported that the highest modified
purity was 49% against predominant classes and 60% against
multiple classes.
method K mPU iPU F
1
Gigaword/S-NIL ? 93.43 20.06 33.03
Gigaword/SW-NIL ? 94.45 41.07 57.25
Gigaword/S-S 512.2 75.06 45.26 56.47
Gigaword/SW-S 260.6 73.98 56.45 64.04
web/S-NIL ? 93.70 32.96 48.76
web/SW-NIL ? 94.51 44.95 60.92
web/S-S 500.0 72.25 52.48 60.79
web/SW-S 255.2 72.65 61.00 66.31
Table 4: Token-level evaluations against VerbNet
classes. K represents the average number of in-
duced classes.
For clustering features, we compare two fea-
ture combinations: ?S-S? and ?SW-S,? which
achieved high performance in the type-level multi-
class evaluations (Section 4.3). The results of
these methods are obtained by averaging five runs.
For a baseline, we use verb-specific semantic
frames without clustering across verbs (?S-NIL?
and ?SW-NIL?), where these frames are consid-
ered to be verb classes but not shared across verbs.
Table 4 lists accuracies of these methods for the
two corpora. We can see that ?SW-S? achieved
a higher F
1
than ?S-S? and the baselines without
verb class induction (?S-NIL? and ?SW-NIL?).
Modi et al (2012) induced semantic frames
across verbs using the monosemous assumption
and reported an F
1
of 44.7% (77.9% PU and
31.4% iPU) for the assignment of FrameNet
frames to the FrameNet corpus. We also con-
ducted the above evaluation against FrameNet
frames for 75 verbs.
9
We achieved an F
1
of
62.79% (66.97% mPU and 59.09% iPU) for
?web/SW-S,? and an F
1
of 60.06% (65.58% mPU
and 55.39% iPU) for ?Gigaword/SW-S.? It is dif-
ficult to directly compare these results with Modi
et al (2012), but our induced verb classes seem to
have higher F
1
accuracy.
4.6 Full Experiments and Discussions
We finally induce verb classes from the semantic
frames of 1,667 verbs, which appear at least once
in sections 02-21 of the WSJ corpus. Based on
the best results in the above evaluations, we in-
duced semantic frames using slot-word pair fea-
tures, and then induced verb classes using slot-
only features. We ended with 38,481 semantic
frames and 699 verb classes from the Gigaword
9
Since FrameNet frames are not assigned to all verbs of
SemLink, the number of verbs is different from the evalua-
tions against VerbNet classes.
1037
class semantic frames
Class 1 rave:1, talk:1
Class 2 need:2, say:2
Class 3 smell:1, sound:1
Class 4 concentrate:1, focus:1
Class 5 express:2, inquire:62, voice:1
Class 6 revolve:1, snake:2, wrap:2
Class 7 hand:1, hand:3, hand:4
Class 8 depend:1, rely:1, rely:3
Class 9 collaborate:1, compete:2, work:1
Class 10 coach:3, teach:3, teach:4
Class 11 dance:1, react:1, stick:1
Class 12 advise:8, express:4, quiz:10, voice:2
Class 13 give:18, grant:6, offer:11, offer:12
Class 14 keep:14, keep:18, stay:4, stay:488
Class 15 cuff:5, fasten:2, tie:1, tie:4
Class 16 arrange:3, book:4, make:27, reserve:5
Class 17 deport:6, differ:1, fluctuate:1, vary:1
Class 18 peek:1, peek:3, peer:1, peer:7, ...
Class 19 groan:1, growl:1, hiss:1, moan:1, purr:1
Class 20 inform:1, notify:2, remind:1, beware:1, ...
Table 5: Examples of induced verb classes. Un-
derlined semantic frames are shown in Table 6.
corpus, and 61,903 semantic frames and 840 verb
classes from the web corpus. It took two days to
induce verb classes from the Gigaword corpus and
three days from the web corpus.
Examples of verb classes and semantic frames
induced from the web corpus are shown in Table
5 and Table 6. While there are many classes with
consistent meanings, such as ?Class 4? and ?Class
16,? some classes have mixed meanings. For in-
stance, ?Class 2? consists of the semantic frames
?need:2? and ?say:2.? These frames were merged
due to the high syntactic similarity of constituting
slot distributions, which are comprised of a sub-
ject and a sentential complement. To improve the
quality of verb classes, it is necessary to develop
a clustering model that can consider syntactic and
lexical similarity in a balanced way.
5 Conclusion
We presented a step-wise unsupervised method
for inducing verb classes from instances in giga-
word corpora. This method first clusters predicate-
argument structures to induce verb-specific se-
mantic frames and then clusters these semantic
frames across verbs to induce verb classes. Both
clustering steps are performed with exactly the
same method, which is based on the Chinese
Restaurant Process. The resulting semantic frames
and verb classes are open to the public and also can
be searched via our web interface.
10
10
http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
slot instance words
nsubj you:2150273, i:7678, we:4599, ...
need:2
ccomp ?s?:2193321
nsubj she:1705781, he:20693, i:9422, ...
say:2
ccomp ?s?:1829616
nsubj i:11100, he:10323, we:6373, ...
dobj me:30646, you:27678, us:21642, ...
inform:1
prep of decision:846, this:759, situation:688, ...
.
.
.
nsubj we:7505, you:3439, i:1035, ...
dobj you:18604, us:7281, them:3649, ...
notify:2
prep of change:1540, problem:496, status:386, ...
.
.
.
Table 6: Examples of induced semantic frames.
The number following an instance word denotes
its frequency and ?s? denotes a sentential comple-
ment.
From the results, we can see that the combi-
nation of the slot-word pair features for cluster-
ing verb-specific frames and the slot-only features
for clustering across verbs is the most effective
and outperforms the baselines by approximately
10 points. This indicates that slot distributions
are more effective than lexical information in slot-
word pairs for the induction of verb classes, when
Levin-style classes are used for evaluation. This
is consistent with Levin?s principle of organizing
verb classes according to the syntactic behavior of
verbs.
As applications of the resulting semantic frames
and verb classes, we plan to integrate them into
syntactic parsing, semantic role labeling and verb
sense disambiguation. For instance, Kawahara
and Kurohashi (2006) improved accuracy of de-
pendency parsing based on Japanese semantic
frames automatically induced from a raw corpus.
It is also valuable and promising to apply the in-
duced verb classes to NLP applications as used in
metaphor identification (Shutova et al, 2010) and
argumentative zoning (Guo et al, 2011).
Acknowledgments
This work was supported by Kyoto University
John Mung Program and JST CREST. We also
gratefully acknowledge the support of the National
Science Foundation Grant NSF-IIS-1116782, A
Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
1038
References
David Aldous. 1985. Exchangeability and related top-
ics.
?
Ecole d?
?
Et?e de Probabilit?es de Saint-Flour XIII
?1983, pages 1?198.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. the Journal of
Machine Learning Research, 3:993?1022.
Gemma Boleda, Sabine Schulte im Walde, and Toni
Badia. 2007. Modelling polysemy in adjective
classes by multi-label classification. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 171?180.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Bonnie J. Dorr. 1997. Large-scale dictionary con-
struction for foreign language tutoring and inter-
lingual machine translation. Machine Translation,
12(4):271?322.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying French verbs using French and
English lexical resources. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 854?863.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176?183.
Daisuke Kawahara, Daniel W. Peterson, Octavian
Popescu, and Martha Palmer. 2014. Inducing
example-based semantic frames from a massive
amount of verb uses. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Zvika
Marx. 2003. Clustering polysemic subcategoriza-
tion frame distributions semantically. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 64?71.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345?352.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?73.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. The University
of Chicago Press.
Jianguo Li and Chris Brew. 2007. Disambiguating
Levin verbs using untagged data. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Pro-
ceedings of ACL-08: HLT, pages 434?442.
Thomas Lippincott, Anna Korhonen, and Diarmuid
?
O S?eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420?429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Ji?r?? Materna. 2012. LDA-frames: An unsupervised ap-
proach to generating semantic frames. In Proceed-
ings of the 13th International Conference CICLing
2012, Part I, pages 376?387.
Ji?r?? Materna. 2013. Parameter estimation for LDA-
frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482?486.
Yusuke Miyao and Jun?ichi Tsujii. 2009. Supervised
learning of a probabilistic lexicon of verb semantic
classes. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1328?1337.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1?7.
1039
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249?
265.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd Annual
Meeting of the Cognitive Science Society.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
learned verb classes. In Proceedings of the 33rd An-
nual Meeting of the Cognitive Science Society.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862?872.
Roi Reichart, Omri Abend, and Ari Rappoport. 2010.
Type level clustering evaluation: New measures and
a POS induction case study. In Proceedings of the
14th Conference on Computational Natural Lan-
guage Learning, pages 77?87.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case frame
acquisition for discourse analysis. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
521?529.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of ACL-08: HLT, pages 496?
504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Computational
Linguistics and Intelligent Text Processing, pages
100?111. Springer.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1002?1010.
Sylvia Springorum, Sabine Schulte im Walde, and Ja-
son Utt. 2013. Detecting polysemy in hard and
soft cluster analyses of German preposition vector
spaces. In Proceedings of the 6th International Joint
Conference on Natural Language Processing, pages
632?640.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 71?78.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Automatic classification of English verbs us-
ing rich syntactic features. In Proceedings of the
3rd International Joint Conference on Natural Lan-
guage Processing, pages 769?774.
Lin Sun, Diana McCarthy, and Anna Korhonen. 2013.
Diathesis alternation approximation for verb clus-
tering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Short Papers, pages 736?741.
Robert Swier and Suzanne Stevenson. 2005. Exploit-
ing a verb lexicon in automatic semantic role la-
belling. In Proceedings of Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
883?890.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101(476).
Naftali Tishby, Fernando C. Pereira, and William
Bialek. 1999. The information bottleneck method.
In Proceedings of the 37th Annual Allerton Confer-
ence on Communication, Control and Computing,
pages 368?377.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
Dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometri-
cal Models of Natural Language Semantics, pages
74?82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266?271.
1040
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 253?258,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Chinese Morphological Analysis with Character-level POS Tagging 
 
Mo Shen?, Hongxiao Liu?, Daisuke Kawahara?, and Sadao Kurohashi? 
?Graduate School of Informatics, Kyoto University, Japan 
?School of Computer Science, Fudan University, China 
shen@nlp.ist.i.kyoto-u.ac.jp {dk,kuro}@i.kyoto-u.ac.jp 
12210240027@fudan.edu.cn 
 
  
 
Abstract 
The focus of recent studies on Chinese word 
segmentation, part-of-speech (POS) tagging 
and parsing has been shifting from words to 
characters. However, existing methods have 
not yet fully utilized the potentials of Chinese 
characters. In this paper, we investigate the 
usefulness of character-level part-of-speech 
in the task of Chinese morphological analysis. 
We propose the first tagset designed for the 
task of character-level POS tagging. We pro-
pose a method that performs character-level 
POS tagging jointly with word segmentation 
and word-level POS tagging. Through exper-
iments, we demonstrate that by introducing 
character-level POS information, the perfor-
mance of a baseline morphological analyzer 
can be significantly improved. 
1 Introduction 
In recent years, the focus of research on Chinese 
word segmentation, part-of-speech (POS) tag-
ging and parsing has been shifting from words 
toward characters. Character-based methods 
have shown superior performance in these tasks 
compared to traditional word-based methods (Ng 
and Low, 2004; Nakagawa, 2004; Zhao et al, 
2006; Kruengkrai et al, 2009; Xue, 2003; Sun, 
2010). Studies investigating the morphological-
level and character-level internal structures of 
words, which treat character as the true atom of 
morphological and syntactic processing, have 
demonstrated encouraging results (Li, 2011; Li 
and Zhou, 2012; Zhang et al, 2013). This line of 
research has provided great insight in revealing 
the roles of characters in word formation and 
syntax of Chinese language. 
However, existing methods have not yet fully 
utilized the potentials of Chinese characters. 
While Li (2011) pointed out that some characters  
Character-level 
Part-of-Speech 
Examples of Verb 
verb + noun ?? (invest : throw + wealth) 
noun + verb ?? (feel sorry : heart + hurt) 
verb + adjective 
?? (realize : recognize + 
clear) 
adjective + verb ?? (hate : pain + hate) 
verb + verb 
?? (inspect : examine + re-
view) 
Table 1. Character-level POS sequence as a 
more specified version of word-level POS: an 
example of verb. 
can productively form new words by attaching to 
existing words, these characters consist only a 
portion of all Chinese characters and appear in 
35% of the words in Chinese Treebank 5.0 
(CTB5) (Xue et al, 2005). Zhang (2013) took 
one step further by investigating the character-
level structures of words; however, the machine 
learning of inferring these internal structures re-
lies on the character forms, which still suffers 
from data sparseness.  
In our view, since each Chinese character is in 
fact created as a word in origin with complete 
and independent meaning, it should be treated as 
the actual minimal morphological unit in Chinese 
language, and therefore should carry specific 
part-of-speech. For example, the character ??? 
(beat) is a verb and the character ??? (broken) is 
an adjective. A word on the other hand, is either 
single-character, or a compound formed by sin-
gle-character words. For example, the verb ??
?? (break) can be seen as a compound formed 
by the two single-character words with the con-
struction ?verb + adjective?. 
Under this treatment, we observe that words 
with the same construction in terms of character-
level POS tend to also have similar syntactic 
roles. For example, the words having the con-
253
struction ?verb + adjective? are typically verbs, 
and those having the construction ?adjective + 
noun? are typically nouns, as shown in the fol-
lowing examples:  
 
(a) verb : verb + adjective  
????(break) : ???(beat) + ???(broken) 
????(update) : ???(replace) + ???(new) 
????(bleach) : ???(wash) + ???(white) 
 
(b) noun : adjective + noun 
????(theme) : ???(main) + ???(topic) 
????(newcomer) : ???(new) + ???(person) 
????(express) : ???(fast) + ???(car) 
 
This suggests that character-level POS can be 
used as cues in predicting the part-of-speech of 
unknown words. 
Another advantage of character-level POS is 
that, the sequence of character-level POS in a 
word can be seen as a more fine-grained version 
of word-level POS. An example is shown in Ta-
ble 1. The five words in this table are very likely 
to be tagged with the same word-level POS as 
verb in any available annotated corpora, while it 
can be commonly agreed among native speakers 
of Chinese that the syntactic behaviors of these 
words are different from each other, due to their 
distinctions in word constructions. For example, 
verbs having the construction ?verb + noun? (e.g. 
??) or ?verb + verb? (e.g. ??) can also be 
nouns in some context, while others cannot; And 
verbs having the constructions ?verb + adjective? 
(e.g. ??) require exact one object argument, 
while others generally do not. Therefore, com-
pared to word-level POS, the character-level 
POS can produce information for more expres-
sive features during the learning process of a 
morphological analyzer. 
In this paper, we investigate the usefulness of 
character-level POS in the task of Chinese mor-
phological analysis. We propose the first tagset 
designed for the task of character-level POS tag-
ging, based on which we manually annotate the 
entire CTB5. We propose a method that performs 
character-level POS tagging jointly with word 
segmentation and word-level POS tagging. 
Through experiments, we demonstrate that by 
introducing character-level POS information, the 
performance of a baseline morphological analyz-
er can be significantly improved. 
 
 
 
 
Tag Part-of-Speech Example 
n noun ??/NN (bill) 
v verb ??/VV (publish) 
j adj./adv. ??/VA (vast)  
t numerical ????/CD (3.14) 
m quantifier ?/CD ?/M (a piece of) 
d date ???/NT (1995) 
k proper noun ??/NR (sino-US) 
b prefix ???/NN (vice mayor) 
e suffix 
???/NN (construction 
inductry) 
r transliteration ????/NR (?rp?d) 
u punctuation 
???????/NR 
(Charles Dickens) 
f foreign chars X??/NN (X-ray) 
o onomatopoeia ??/AD (rumble) 
s surname 
???/NR (Wang 
Xinmin) 
p pronoun ??/PN (they) 
c other functional ??/VV (be used for) 
Table 2. Tagset for character-level part-of-
speech tagging. The underlined characters in 
the examples correspond to the tags on the 
left-most column. The CTB-style word-level 
POS are also shown for the examples. 
2 Character-level POS Tagset 
We propose a tagset for the task of character-
level POS tagging. This tagset contains 16 tags, 
as illustrated in Table 2. The tagset is designed 
by treating each Chinese character as a single-
character word, and each (multi-character) word 
as a phrase of single-character words. Some of 
these tags are directly derived from the common-
ly accepted word-level part-of-speech, such as 
noun, verb, adjective and adverb. It should be 
noted that, for single-character words, the differ-
ence between adjective and adverb can almost be 
ignored, because for any of such words that can 
be used as an adjective, it usually can also be 
used as an adverb. Therefore, we have merged 
these two tags into one.  
On the other hand, some other tags are de-
signed specifically for characters, such as trans-
literation, surname, prefix and suffix. Unlike 
some Asian languages such as Japanese, there is 
no explicit character set in Chinese that are used 
exclusively for expressing names of foreign per-
sons, places or organizations. However, some 
characters are used much more frequently than 
others in these situations. For example, in the 
person?s name ?????? (?rp?d), all the four 
characters can be frequently observed in words  
254
 
Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by blue 
bold lines. 
 
 
Word Length 1 2 3 4 5 6 7 or more 
Tags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...ME 
Table 3. Word representation with a 6-tag tagset: S, B, B2, B3, M, E 
 
of transliterations. Similarly, surnames in Chi-
nese are also drawn from a set of limited number 
of characters. We therefore assign specific tags 
for this kind of character sets. The tags for pre-
fixes and suffixes are motivated by the previous 
studies (Li, 2011; Li and Zhou, 2012). 
We have annotated character-level POS for all 
words in CTB5 1 . Fortunately, character-level 
POS in most words are independent of context, 
which means it is sufficient to annotate word 
forms unless there is an ambiguity. The annota-
tion was conducted by two persons, where each 
one of them was responsible for about 70% of 
the documents in the corpus. The redundancy 
was set for the purposes of style unification and 
quality control, on which we find that the inter- 
annotator agreement is 96.2%. Although the an-
notation also includes the test set, we blind this 
portion in all the experiments.  
                                                 
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN 
3 Chinese Morphological Analysis with 
Character-level POS 
3.1 System Description 
Previous studies have shown that jointly pro-
cessing word segmentation and POS tagging is 
preferable to pipeline processing, which can 
propagate errors (Nakagawa and Uchimoto, 2007; 
Kruengkrai et al, 2009). Based on these studies, 
we propose a word-character hybrid model 
which can also utilize the character-level POS 
information. This hybrid model constructs a lat-
tice that consists of word-level and character-
level nodes from a given input sentence. Word-
level nodes correspond to words found in the 
system?s lexicon, which has been compiled from 
training data. Character-level nodes have special 
tags called position-of-character (POC) that indi-
cate the word-internal position (Asahara, 2003; 
Nakagawa, 2004). We have adopted the 6-tag 
tagset, which (Zhao et al, 2006) reported to be 
optimal. This tagset is illustrated in Table 3. 
Figure 2 shows an example of a lattice for the 
Chinese sentence: ????????? (Chen 
Deming answers to journalists? questions). The 
correct path is marked with blue bold lines. The 
255
Category Template Condition 
Baseline-unigram ?  ? ?  ? ?     ? ?     ? ?            ? ?          ?     ?                    ? 
 ?      ? ?      ? ?     ? ?     ? ?     ?     ?          ? ?         ? ?        ? ?        ? ?         ? 
Baseline-bigram ?      ? ?      ? ?      ? ?      ? ?          ? ?         ? 
       
 ?          ? ?         ? ?             ? ?          ? ?         ? 
 ?          ? ?         ? ?             ? ?           ? 
 ?             ? ?               ? ?                ? 
 ?      ? ?      ? ?          ? ?         ?         ?          ? ?         ? ?             ? 
 ?      ? Otherwise 
Proposed-unigram ?         ?    
Proposed-bigram ?              ? ?                  ?         
 ?             ? ?                 ? 
 ?                      ? ?                     ?        
 ?                     ? ?                    ?  
 ?          ? ?             ?        
 ?          ? ?              ?        
 ?                  ? ?                 ? ?                     ?        
Table 4. Feature templates. The ?Condition? column describes when to apply the templates:    
and   denote the previous and the current word-level node;     and    denote the previous and 
the current character-level node;     and    denote the previous and the current node of any 
types. Word-level nodes represent known words that can be found in the system?s lexicon. 
 
upper part of the lattice (word-level nodes) rep-
resents known words, where each node carries 
information such as character form, character-
level POS , and word-level POS. A word that 
contains multiple characters is represented by a 
sub-lattice (the dashed rectangle in the figure), 
where a path stands for a possible sequence of 
character-level POS for this word. For example, 
the word ???? (journalist) has two possible 
paths of character-level POS: ?verb + suffix? and 
?noun + suffix?. Nodes that are inside a sub-
lattice cannot be linked to nodes that are outside, 
except from the boundaries. The lower part of 
the lattice (character-level nodes) represents un-
known words, where each node carries a posi-
tion-of-character tag, in addition to other types of 
information that can also be found on a word-
level node. A sequence of character-level nodes 
are considered as an unknown word if and only if 
the sequence of POC tags forms one of the cases 
listed in Table 3. This table also illustrates the 
permitted transitions between adjacent character-
level nodes. We use the standard dynamic pro-
gramming technique to search for the best path in 
the lattice. We use the averaged perceptron (Col-
lins, 2002), an efficient online learning algorithm, 
to train the model. 
3.2 Features 
We show the feature templates of our model in 
Table 4. The features consist of two categories: 
baseline features, which are modified from the 
templates proposed in (Kruengkrai et al, 2009); 
and proposed features, which encode character-
level POS information.  
Baseline features: For word-level nodes that 
represent known words, we use the symbols  ,   
and   to denote the word form, POS tag and 
length of the word, respectively. The functions 
         and        return the first and last 
character of  . If   has only one character, we 
omit the templates that contain          or 
      . We use the subscript indices 0 and -1 to 
indicate the current node and the previous node 
during a Viterbi search, respectively. For charac-
ter-level nodes,   denotes the surface character, 
and   denotes the combination of POS and POC 
(position-of-character) tags.  
Proposed features: For word-level nodes, the 
function           returns the pair of the char-
acter-level POS tags of the first and last charac-
ters of  , and          returns the sequence of 
character-level POS tags of . If either the pair 
or the sequence of character-level POS is ambig-
uous, which means there are multiple paths in the 
sub-lattice of the word-level node, then the val-
ues on the current best path (with local context) 
during the Viterbi search will be returned. If   
has only one character, we omit the templates 
that contain          . For character-level nodes, 
the function       returns its character-level 
POS. The subscript indices 0 and -1 as well as 
256
other symbols stand for the same meaning as 
they are in the baseline features.  
4 Evaluation 
4.1 Settings 
To evaluate our proposed method, we have con-
ducted two sets of experiments on CTB5: word 
segmentation, and joint word segmentation and 
word-level POS tagging. We have adopted the 
same data division as in (Jiang et al, 2008a; 
Jiang et al, 2008b; Kruengkrai et al, 2009; 
Zhang and Clark, 2010; Sun, 2011): the training 
set, dev set and test set have 18,089, 350 and 348 
sentences, respectively. The models applied on 
all test sets are those that result in the best per-
formance on the CTB5 dev set. 
We have annotated character-level POS in-
formation for all 508,768 word tokens in CTB5. 
As mentioned in section 2, we blind the annota-
tion in the test set in all the experiments. To learn 
the characteristics of unknown words, we built 
the system?s lexicon using only the words in the 
training data that appear at least 3 times. We ap-
plied a similar strategy in building the lexicon for 
character-level POS, where the threshold we 
choose is 2. These thresholds were tuned using 
the development data.  
We have used precision, recall and the F-score 
to measure the performance of the systems. Pre-
cision ( ) is defined as the percentage of output 
tokens that are consistent with the gold standard 
test data, and recall ( ) is the percentage of to-
kens in the gold standard test data that are recog-
nized in the output. The balanced F-score ( ) is 
defined as  
     
   
. 
4.2 Experimental Results 
We compare the performance between a baseline 
model and our proposed approach. The results of 
the word segmentation experiment and the joint 
experiment of segmentation and POS tagging are 
shown in Table 5(a) and Table 5(b), respectively. 
Each row in these tables shows the performance 
of the corresponding system. ?CharPos? stands 
for our proposed model which has been de-
scribed in section 3. ?Baseline? stands for the 
same model except it only enables features from 
the baseline templates. 
The results show that, while the differences 
between the baseline model and the proposed 
model in word segmentation accuracies are small, 
the proposed model achieves significant im-
provement in the experiment of joint segmentati- 
(a) Word Segmentation Results 
System P R F 
Baseline 97.48 98.44 97.96 
CharPOS 97.55 98.51 98.03 
 
(b) Joint Segmentation and POS Tagging Results 
System P R F 
Baseline 93.01 93.95 93.48 
CharPOS 93.42 94.18 93.80 
Table 5. Experimental results on CTB5. 
 
System Segmentation Joint 
Baseline 97.96 93.48 
CharPOS 98.03 93.80 
Jiang2008a 97.85 93.41 
Jiang2008b 97.74 93.37 
Kruengkrai2009 97.87 93.67 
Zhang2010 97.78 93.67 
Sun2011 98.17 94.02 
Table 6. Comparison with previous studies on 
CTB5. 
on and POS tagging2. This suggests that our pro-
posed method is particularly effective in predict-
ing the word-level POS, which is consistent with 
our observations mentioned in section 1. 
In Table 6 we compare our approach with 
morphological analyzers in previous studies. The 
accuracies of the systems in previous work are 
directly taken from the original paper. As the 
results show, despite the fact that the perfor-
mance of our baseline model is relatively weak 
in the joint segmentation and POS tagging task, 
our proposed model achieves the second-best 
performance in both segmentation and joint tasks. 
5 Conclusion 
We believe that by treating characters as the true 
atoms of Chinese morphological and syntactic 
analysis, it is possible to address the out-of-
vocabulary problem that word-based methods 
have been long suffered from. In our error analy-
sis, we believe that by exploring the character-
level POS and the internal word structure (Zhang 
et al, 2013) at the same time, it is possible to 
further improve the performance of morphologi-
cal analysis and parsing. We will address these 
issues in our future work. 
  
                                                 
2        in McNemar?s test. 
257
Reference 
Masayuki Asahara. 2003. Corpus-based Japanese 
Morphological Analysis. Nara Institute of Science 
and Technology, Doctor?s Thesis. 
Michael Collins. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of EMNLP, pages 1?8. 
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?. 
2008a. A Cascaded Linear Model for Joint Chinese 
Word Segmentation and Part-of-speech Tagging. 
In Proceedings of ACL. 
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word 
Lattice Reranking for Chinese Word Segmentation 
and Part-of-speech Tagging. In Proceedings of COL-
ING. 
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi 
Kazama, YiouWang, Kentaro Torisawa, and Hi-
toshi Isahara. 2009. An Error-Driven Word-
Character Hybird Model for Joint Chinese Word 
Segmentation and POS Tagging. In Proceedings of 
ACL-IJCNLP, pages 513-521. 
Zhongguo Li. 2011. Parsing the Internal Structure of 
Words: A New Paradigm for Chinese Word Seg-
mentation. In Proceedings of ACL-HLT, pages 
1405?1414. 
Zhongguo Li and Guodong Zhou. 2012. Unified De-
pendency Parsing of Chinese Morphological and 
Syntactic Structures. In Proceedings of EMNLP, 
pages 1445?1454. 
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings of 
EMNLP, pages 277?284. 
Tetsuji Nakagawa. 2004. Chinese and japanese word 
segmentation using word-level and character-level 
information. In Proceedings of COLING, pages 
466?472.  
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. 
Hybrid Approach to Word Segmentation and Pos 
Tagging. In Proceedings of ACL Demo and Poster 
Sessions, pages 217-220. 
Weiwei Sun. 2010. Word-based and Character-based 
Word Segmentation Models: Comparison and 
Combination. In Proceedings of COLING Poster 
Sessions, pages 1211?1219. 
Weiwei Sun. 2011. A Stacked Sub-word Model for 
Joint Chinese Word Segmentation and Part-of-
speech Tagging. In Proceedings of ACL-HLT, 
pages 1385?1394. 
Nianwen Xue. 2003. Chinese Word Segmentation as 
Character Tagging. In International Journal of 
Computational Linguistics and Chinese Language 
Processing. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207?238. 
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang 
Lu. 2006. Effective Tag Set Selection in Chinese 
Word Segmentation via Conditional Random Field 
Modeling. In Proceedings of PACLIC, pages 87-94. 
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting 
Liu. 2013. Chinese Parsing Exploiting Characters. 
In Proceedings of ACL, page 125-134. 
Yue Zhang and Stephen Clark. 2010. A Fast Decoder 
for Joint Word Segmentation and POS-tagging Us-
ing a Single Discriminative Model. In Proceedings 
of EMNLP, pages 843?852. 
258
