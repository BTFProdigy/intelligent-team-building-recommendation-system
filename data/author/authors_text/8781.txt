Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 18?25,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LEILA: Learning to Extract Information by Linguistic Analysis
Fabian M. Suchanek
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
suchanek@mpii.mpg.de
Georgiana Ifrim
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
ifrim@mpii.mpg.de
Gerhard Weikum
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
weikum@mpii.mpg.de
Abstract
One of the challenging tasks in the con-
text of the Semantic Web is to automati-
cally extract instances of binary relations
from Web documents ? for example all
pairs of a person and the corresponding
birthdate. In this paper, we present LEILA,
a system that can extract instances of ar-
bitrary given binary relations from natu-
ral language Web documents ? without
human interaction. Different from previ-
ous approaches, LEILA uses a deep syn-
tactic analysis. This results in consistent
improvements over comparable systems
(such as e.g. Snowball or TextToOnto).
1 Introduction
1.1 Motivation
Search engines, question answering systems and
classification systems alike can greatly profit from
formalized world knowledge. Unfortunately, man-
ually compiled collections of world knowledge
(such as e.g. WordNet (Fellbaum, 1998)) often
suffer from low coverage, high assembling costs
and fast aging. In contrast, the World Wide Web
provides an enormous source of knowledge, as-
sembled by millions of people, updated constantly
and available for free. Since the Web data con-
sists mostly of natural language documents, a first
step toward exploiting this data would be to ex-
tract instances of given target relations. For exam-
ple, one might be interested in extracting all pairs
of a person and her birthdate (the birthdate-
relation), pairs of a company and the city of its
headquarters (the headquarters-relation) or
pairs of an entity and the concept it belongs to (the
instanceOf-relation). The task is, given a set
of Web documents and given a target relation, ex-
tracting pairs of entities that are in the target rela-
tion. In this paper, we propose a novel method for
this task, which works on natural language Web
documents and does not require human interac-
tion. Different from previous approaches, our ap-
proach involves a deep linguistic analysis, which
helps it to achieve a superior performance.
1.2 Related Work
There are numerous Information Extraction (IE)
approaches, which differ in various features:
? Arity of the target relation: Some systems are
designed to extract unary relations, i.e. sets of
entities (Finn and Kushmerick, 2004; Califf and
Mooney, 1997). In this paper we focus on the
more general binary relations.
? Type of the target relation: Some systems
are restricted to learning a single relation,
mostly the instanceOf-relation (Cimiano
and Vo?lker, 2005b; Buitelaar et al, 2004).
In this paper, we are interested in extracting
arbitrary relations (including instanceOf).
Other systems are designed to discover new
binary relations (Maedche and Staab, 2000).
However, in our scenario, the target relation is
given in advance.
? Human interaction: There are systems that re-
quire human intervention during the IE process
(Riloff, 1996). Our work aims at a completely
automated system.
? Type of corpora: There exist systems that can
extract information efficiently from formatted
data, such as HTML-tables or structured text
(Graupmann, 2004; Freitag and Kushmerick,
2000). However, since a large part of the Web
consists of natural language text, we consider in
this paper only systems that accept also unstruc-
tured corpora.
? Initialization: As initial input, some systems
require a hand-tagged corpus (J. Iria, 2005;
Soderland et al, 1995), other systems require
text patterns (Yangarber et al, 2000) or tem-
plates (Xu and Krieger, 2003) and again oth-
ers require seed tuples (Agichtein and Gravano,
2000; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005) or tables of target concepts
(Cimiano and Vo?lker, 2005a). Since hand-
18
labeled data and manual text patterns require
huge human effort, we consider only systems
that use seed pairs or tables of concepts.
Furthermore, there exist systems that use the
whole Web as a corpus (Etzioni et al, 2004) or that
validate their output by the Web (Cimiano et al,
2005). In order to study different extraction tech-
niques in a controlled environment, however, we
restrict ourselves to systems that work on a closed
corpus for this paper.
One school of extraction techniques concen-
trates on detecting the boundary of interesting en-
tities in the text, (Califf and Mooney, 1997; Finn
and Kushmerick, 2004; Yangarber et al, 2002).
This usually goes along with the restriction to
unary target relations. Other approaches make
use of the context in which an entity appears
(Cimiano and Vo?lker, 2005a; Buitelaar and Ra-
maka, 2005). This school is mostly restricted to
the instanceOf-relation. The only group that
can learn arbitrary binary relations is the group
of pattern matching systems (Etzioni et al, 2004;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Brin, 1999; Soderland, 1999; Xu et
al., 2002; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005). Surprisingly, none of these sys-
tems uses a deep linguistic analysis of the cor-
pus. Consequently, most of them are extremely
volatile to small variations in the patterns. For ex-
ample, the simple subordinate clause in the fol-
lowing example (taken from (Ravichandran and
Hovy, 2002)) can already prevent a surface pat-
tern matcher from discovering a relation between
?London? and the ?river Thames?: ?London, which has
one of the busiest airports in the world, lies on the banks
of the river Thames.?
1.3 Contribution
This paper presents LEILA (Learning to Extract
Information by Linguistic Analysis), a system that
can extract instances of an arbitrary given binary
relation from natural language Web documents
without human intervention. LEILA uses a deep
analysis for natural-language sentences as well as
other advanced NLP methods like anaphora reso-
lution, and combines them with machine learning
techniques for robust and high-yield information
extraction. Our experimental studies on a variety
of corpora demonstrate that LEILA achieves very
good results in terms of precision and recall and
outperforms the prior state-of-the-art methods.
1.4 Link Grammars
There exist different approaches for parsing nat-
ural language sentences. They range from sim-
ple part-of-speech tagging to context-free gram-
mars and more advanced techniques such as Lex-
ical Functional Grammars, Head-Driven Phrase
Structure Grammars or stochastic approaches. For
our implementation, we chose the Link Grammar
Parser (Sleator and Temperley, 1993). It is based
on a context-free grammar and hence it is simpler
to handle than the advanced parsing techniques.
At the same time, it provides a much deeper se-
mantic structure than the standard context-free
parsers. Figure 1 shows a simplified example of
a linguistic structure produced by the link parser
(a linkage).
A linkage is a connected planar undirected
graph, the nodes of which are the words of the sen-
tence. The edges are called links. They are labeled
with connectors. For example, the connector subj
in Figure 1 marks the link between the subject and
the verb of the sentence. The linkage must ful-
fill certain linguistic constraints, which are given
by a link grammar. The link grammar specifies
which word may be linked by which connector to
preceding and following words. Furthermore, the
parser assigns part-of-speech tags, i.e. symbols
identifying the grammatical function of a word in
a sentence. In the example in Figure 1, the let-
ter ?n? following the word ?composers? indentifies
?composers? as a noun.
Chopin was.v     great  among the composers.n of   his  time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 1: A simple linkage
Figure 2 shows how the Link Parser copes with a
more complex example. The relationship between
the subject ?London? and the verb ?lies? is not dis-
rupted by the subordinate clause:
London, which has one of the busiest airports, lies on the banks of the river Thames. 
subj
mod subj obj prep
prepObj
det
sup mod
prepObj
det mod
prepObj
det grp
Figure 2: A complex linkage
We say that a linkage expresses a relation r, if
the underlying sentence implies that a pair of enti-
ties is in r. Note that the deep grammatical anal-
ysis of the sentence would allow us to define the
meaning of the sentence in a theoretically well-
founded way (Montague, 1974). For this paper,
however, we limit ourselves to an intuitive under-
standing of the notion of meaning.
We define a pattern as a linkage in which two
19
words have been replaced by placeholders. Figure
3 shows a pattern derived from the linkage in Fig-
ure 1 by replacing ?Chopin? and ?composers? by the
placeholders ?X? and ?Y?.
    X       was.v       great  among the       Y        of  his    time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 3: A pattern
We call the (unique) shortest path from one
placeholder to the other the bridge, marked in bold
in the figure. The bridge does not include the
placeholders. Two bridges are regarded as equiva-
lent, if they have the same sequence of nodes and
edges, although nouns and adjectives are allowed
to differ. For example, the bridge in Figure 3 and
the bridge in Figure 4 (in bold) are regarded as
equivalent, because they are identical except for
a substitution of ?great? by ?mediocre?. A pattern
matches a linkage, if an equivalent bridge occurs
in the linkage. For example, the pattern in Figure
3 matches the linkage in Figure 4.
Mozart was.v clearly mediocre  among the composers.n.
subj
compl
mod
prepObj
detmod
Figure 4: A matching linkage
If a pattern matches a linkage, we say that the
pattern produces the pair of words that the link-
age contains in the position of the placeholders.
In Figure 4, the pair ?Mozart? / ?composers? is pro-
duced by the pattern in Figure 3.
2 System Description
2.1 Document Pre-Processing
LEILA accepts HTML documents as input. To
allow the system to handle date and number ex-
pressions, we normalize these constructions by
regular expression matching in combination with
a set of functions. For example, the expression
?November 23rd to 24th 1998? becomes ?1998-11-23
to 1998-11-24? and the expression ?0.8107 acre-feet?
becomes ?1000 cubic-meters?. Then, we split the
original HTML-document into two files: The first
file contains the proper sentences with the HTML-
tags removed. The second file contains the non-
grammatical parts, such as lists, expressions us-
ing parentheses and other constructions that can-
not be handled by the Link Parser. For example,
the character sequence ?Chopin (born 1810) was a
great composer? is split into the sentence ?Chopin
was a great composer? and the non-grammatical in-
formation ?Chopin (born 1810)?. The grammatical
file is parsed by the Link Parser.
The parsing allows for a restricted named entity
recognition, because the parser links noun groups
like ?United States of America? by designated con-
nectors. Furthermore, the parsing allows us to do
anaphora resolution. We use a conservative ap-
proach, which simply replaces a third person pro-
noun by the subject of the preceding sentence.
For our goal, it is essential to normalize nouns
to their singular form. This task is non-trivial,
because there are numerous words with irregular
plural forms and there exist even word forms that
can be either the singular form of one word or the
plural form of another. By collecting these excep-
tions systematically from WordNet, we were able
to stem most of them correctly with our Plural-to-
Singular Stemmer (PlingStemmer1). For the non-
grammatical files, we provide a pseudo-parsing,
which links each two adjacent items by an artifi-
cial connector. As a result, the uniform output of
the preprocessing is a sequence of linkages, which
constitutes the input for the core algorithm.
2.2 Core Algorithm
As a definition of the target relation, our algorithm
requires a function (given by a Java method) that
decides into which of the following categories a
pair of words falls:
? The pair can be an example for the target re-
lation. For instance, for the birthdate-
relation, the examples can be given by a list of
persons with their birth dates.
? The pair can be a counterexample. For the
birthdate-relation, the counterexamples can
be deduced from the examples (e.g. if ?Chopin?
/ ?1810? is an example, then ?Chopin? / ?2000?
must be a counterexample).
? The pair can be a candidate. For birthdate,
the candidates would be all pairs of a proper
name and a date that are not an example or a
counterexample.
? The pair can be none of the above.
The core algorithm proceeds in three phases:
1. In the Discovery Phase, it seeks linkages in
which an example pair appears. It replaces the
two words by placeholders, thus producing a
pattern. These patterns are collected as positive
patterns. Then, the algorithm runs through the
sentences again and finds all linkages that match
1available at http://www.mpii.mpg.de/ ?suchanek
20
a positive pattern, but produce a counterexam-
ple. The corresponding patterns are collected as
negative patterns2.
2. In the Training Phase, statistical learning is ap-
plied to learn the concept of positive patterns.
The result of this process is a classifier for pat-
terns.
3. In the Testing Phase, the algorithm considers
again all sentences in the corpus. For each link-
age, it generates all possible patterns by replac-
ing two words by placeholders. If the two words
form a candidate and the pattern is classified as
positive, the produced pair is proposed as a new
element of the target relation (an output pair).
In principle, the core algorithm does not depend on
a specific grammar or a specific parser. It can work
on any type of grammatical structures, as long as
some kind of pattern can be defined on them. It is
also possible to run the Discovery Phase and the
Testing Phase on different corpora.
2.3 Learning Model
The central task of the Discovery Phase is deter-
mining patterns that express the target relation.
These patterns are generalized in the Training
Phase. In the Testing Phase, the patterns are used
to produce the output pairs. Since the linguistic
meaning of the patterns is not apparent to the sys-
tem, the Discovery Phase relies on the following
hypothesis: Whenever an example pair appears
in a sentence, the linkage and the corresponding
pattern express the target relation. This hypoth-
esis may fail if a sentence contains an example
pair merely by chance, i.e. without expressing the
target relation. Analogously, a pattern that does
express the target relation may occasionally pro-
duce counterexamples. We call these patterns false
samples. Virtually any learning algorithm can deal
with a limited number of false samples.
To show that our approach does not depend
on a specific learning algorithm, we implemented
two classifiers for LEILA: One is an adaptive k-
Nearest-Neighbor-classifier (kNN) and the other
one uses a Support Vector Machine (SVM). These
classifiers, the feature selection and the statistical
model are explained in detail in (Suchanek et al,
2006). Here, we just note that the classifiers yield
a real valued label for a test pattern. This value
can be interpreted as the confidence of the classifi-
cation. Thus, it is possible to rank the output pairs
of LEILA by their confidence.
2Note that different patterns can match the same linkage.
3 Experiments
3.1 Setup
We ran LEILA on different corpora with increasing
heterogeneity:
? Wikicomposers: The set of all Wikipedia arti-
cles about composers (872 HTML documents).
We use it to see how LEILA performs on a docu-
ment collection with a strong structural and the-
matic homogeneity.
? Wikigeography: The set of all Wikipedia
pages about the geography of countries (313
HTML documents).
? Wikigeneral: A set of random Wikipedia arti-
cles (78141 HTML documents). We chose it to
assess LEILA?s performance on structurally ho-
mogenous, but thematically random documents.
? Googlecomposers: This set contains one doc-
ument for each baroque, classical, and roman-
tic composer in Wikipedia?s list of composers,
as delivered by a Google ?I?m feeling lucky?
search for the composer?s name (492 HTML
documents). We use it to see how LEILA per-
forms on a corpus with a high structural hetero-
geneity. Since the querying was done automat-
ically, the downloaded pages include spurious
advertisements as well as pages with no proper
sentences at all.
We tested LEILA on different target relations with
increasing complexity:
? birthdate: This relation holds between a person
and his birth date (e.g. ?Chopin? / ?1810?). It is
easy to learn, because it is bound to strong sur-
face clues (the first element is always a name,
the second is always a date).
? synonymy: This relation holds between two
names that refer to the same entity (e.g.
?UN?/?United Nations?). The relation is more so-
phisticated, since there are no surface clues.
? instanceOf: This relation is even more sophis-
ticated, because the sentences often express it
only implicitly.
We compared LEILA to different competitors. We
only considered competitors that, like LEILA, ex-
tract the information from a corpus without using
other Internet sources. We wanted to avoid run-
ning the competitors on our own corpora or on our
own target relations, because we could not be sure
to achieve a fair tuning of the competitors. Hence
we ran LEILA on the corpora and the target rela-
tions that our competitors have been tested on by
their authors. We compare the results of LEILA
with the results reported by the authors. Our com-
petitors, together with their respective corpora and
relations, are:
21
? TextToOnto3: A state-of-the-art representative
for non-deep pattern matching. The system pro-
vides a component for the instanceOf rela-
tion and takes arbitrary HTML documents as in-
put. For completeness, we also consider its suc-
cessor Text2Onto (Cimiano and Vo?lker, 2005a),
although it contains only default methods in its
current state of development.
? Snowball (Agichtein and Gravano, 2000):
A recent representative of the slot-extraction
paradigm. In the original paper, Snowball has
been tested on the headquarters relation.
This relation holds between a company and the
city of its headquarters. Snowball was trained
on a collection of some thousand documents
and then applied to a test collection. For copy-
right reasons, we only had access to the test col-
lection (150 text documents).
? (Cimiano and Vo?lker, 2005b) present a new sys-
tem that uses context to assign a concept to
an entity. We will refer to this system as the
CV-system. The approach is restricted to the
instanceOf-relation, but it can classify in-
stances even if the corpus does not contain ex-
plicit definitions. In the original paper, the sys-
tem was tested on a collection of 1880 files from
the Lonely Planet Internet site4.
For the evaluation, the output pairs of the sys-
tem have to be compared to a table of ideal pairs.
One option would be to take the ideal pairs from a
pre-compiled data base. The problem is that these
ideal pairs may differ from the facts expressed in
the documents. Furthermore, these ideal pairs do
not allow to measure how much of the document
content the system actually extracted. This is why
we chose to extract the ideal pairs manually from
the documents. In our methodology, the ideal pairs
comprise all pairs that a human would understand
to be elements of the target relation. This involves
full anaphora resolution, the solving of reference
ambiguities, and the choice of truly defining con-
cepts. For example, we accept Chopin as instance
of composer but not as instance of member,
even if the text says that he was a member of some
club. Of course, we expect neither the competi-
tors nor LEILA to achieve the results in the ideal
table. However, this methodology is the only fair
way of manual extraction, as it is guaranteed to
be system-independent. If O denotes the multi-
set of the output pairs and I denotes the multi-set
of the ideal pairs, then precision, recall, and their
3http://www.sourceforge.net/projects/texttoonto
4http://www.lonelyplanet.com/
harmonic mean F1 can be computed as
recall = |O ? I||I| precision =
|O ? I|
|O|
F1 = 2 ? recall ? precisionrecall + precision .
To ensure a fair comparison of LEILA to Snow-
ball, we use the same evaluation as employed in
the original Snowball paper (Agichtein and Gra-
vano, 2000), the Ideal Metric. The Ideal Metric
assumes the target relation to be right-unique (i.e.
a many-to-one relation). Hence the set of ideal
pairs is right-unique. The set of output pairs can
be made right-unique by selecting the pair with the
highest confidence for each first component. Du-
plicates are removed from the ideal pairs and also
from the output pairs. All output pairs that have
a first component that is not in the ideal set are
removed.
There is one special case for the CV-system,
which uses the Ideal Metric for the non-right-
unique instanceOf relation. To allow for a fair
comparison, we used the Relaxed Ideal Metric,
which does not make the ideal pairs right-unique.
The calculation of recall is relaxed as follows:
recall = |O ? I||{x|?y : (x, y) ? I}|
Due to the effort, we could extract the ideal pairs
only for a sub-corpus. To ensure significance in
spite of this, we compute confidence intervals for
our estimates: We interpret the sequence of out-
put pairs as a repetition of a Bernoulli-experiment,
where the output pair can be either correct (i.e.
contained in the ideal pairs) or not. The parameter
of this Bernoulli-distribution is the precision. We
estimate the precision by drawing a sample (i.e.
by extracting all ideal pairs in the sub-corpus). By
assuming that the output pairs are identically in-
dependently distributed, we can calculate a confi-
dence interval for our estimation. We report confi-
dence intervals for precision and recall for a con-
fidence level of ? = 95%. We measure precision
at different levels of recall and report the values
for the best F1 value. We used approximate string
matching techniques to account for different writ-
ings of the same entity. For example, we count
the output pair ?Chopin? / ?composer? as correct,
even if the ideal pairs contain ?Frederic Chopin? /
?composer?. To ensure that LEILA does not just
reproduce the example pairs, we list the percent-
age of examples among the output pairs. During
our evaluation, we found that the Link Grammar
parser does not finish parsing on roughly 1% of
the files for unknown reasons.
22
Table 1: Results with different relations
Corpus Relation System #D #O #C #I Precision Recall F1 %E
Wikicomposers birthdate LEILA(SVM) 87 95 70 101 73.68%? 8.86% 69.31%? 9.00% 71.43% 4.29%
Wikicomposers birthdate LEILA(kNN) 87 90 70 101 78.89%? 8.43% 70.30%? 8.91% 74.35% 4.23%
Wikigeography synonymy LEILA(SVM) 81 92 74 164 80.43%? 8.11% 45.12%? 7.62% 57.81% 5.41%
Wikigeography synonymy LEILA(kNN) 81 143 105 164 73.43%? 7.24% 64.02%? 7.35% 68.40% 4.76%
Wikicomposers instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03% 6.62%
Wikicomposers instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30% 7.34%
Wikigeneral instanceOf LEILA(SVM) 287 921 304 912 33.01%? 3.04% 33.33%? 3.06% 33.17% 3.62%
Googlecomposers instanceOf LEILA(SVM) 100 787 210 1334 26.68%? 3.09% 15.74%? 1.95% 19.80% 4.76%
Googlecomposers instanceOf LEILA(kNN) 100 840 237 1334 28.21%? 3.04% 17.77%? 2.05% 21.80% 8.44%
Googlec.+Wikic. instanceOf LEILA(SVM) 100 563 203 1334 36.06%? 3.97% 15.22%? 1.93% 21.40% 5.42%
Googlec.+Wikic. instanceOf LEILA(kNN) 100 826 246 1334 29.78%? 3.12% 18.44%? 2.08% 22.78% 7.72%
#O ? number of output pairs #D ? number of documents in the hand-processed sub-corpus
#C ? number of correct output pairs %E ? proportion of example pairs among the correct output pairs
#I ? number of ideal pairs Recall and Precision with confidence interval at ? = 95%
3.2 Results
3.2.1 Results on different relations
Table 1 summarizes our experimental results
with LEILA on different relations. For the birth-
date relation, we used Edward Morykwas? list of
famous birthdays5 as examples. As counterexam-
ples, we chose all pairs of a person that was in the
examples and an incorrect birthdate. All pairs of
a proper name and a date are candidates. We ran
LEILA on the Wikicomposer corpus. LEILA per-
formed quite well on this task. The patterns found
were of the form ?X was born in Y ? and ?X (Y )?.
For the synonymy relation we used all pairs
of proper names that share the same synset in
WordNet as examples (e.g. ?UN?/?United Na-
tions?). As counterexamples, we chose all pairs of
nouns that are not synonymous in WordNet (e.g.
?rabbit?/?composer?). All pairs of proper names are
candidates. We ran LEILA on the Wikigeography
corpus, because this set is particularly rich in syn-
onyms. LEILA performed reasonably well. The
patterns found include ?X was known as Y ? as well
as several non-grammatical constructions such as
?X (formerly Y )?.
For the instanceOf relation, it is difficult to se-
lect example pairs, because if an entity belongs
to a concept, it also belongs to all super-concepts.
However, admitting each pair of an entity and one
of its super-concepts as an example would result in
far too many false positives. The problem is to de-
termine for each entity the (super-)concept that is
most likely to be used in a natural language defini-
tion of that entity. Psychological evidence (Rosch
et al, 1976) suggests that humans prefer a certain
layer of concepts in the taxonomy to classify en-
tities. The set of these concepts is called the Ba-
sic Level. Heuristically, we found that the low-
est super-concept in WordNet that is not a com-
pound word is a good approximation of the ba-
5http://www.famousbirthdates.com
sic level concept for a given entity. We used all
pairs of a proper name and the corresponding ba-
sic level concept of WordNet as examples. We
could not use pairs of proper names and incorrect
super-concepts as counterexamples, because our
corpus Wikipedia knows more meanings of proper
names than WordNet. Therefore, we used all pairs
of a common noun and an incorrect super-concept
from WordNet as counterexamples. All pairs of
a proper name and a WordNet concept are candi-
dates.
We ran LEILA on the Wikicomposers corpus.
The performance on this task was acceptable, but
not impressive. However, the chances to obtain a
high recall and a high precision were significantly
decreased by our tough evaluation policy: The
ideal pairs include tuples deduced by resolving
syntactic and semantic ambiguities and anaphoras.
Furthermore, our evaluation policy demands that
non-defining concepts like member not be cho-
sen as instance concepts. In fact, a high propor-
tion of the incorrect assignments were friend,
member, successor and predecessor, de-
creasing the precision of LEILA. Thus, compared
to the gold standard of humans, the performance
of LEILA can be considered reasonably good. The
patterns found include the Hearst patterns (Hearst,
1992) ?Y such as X?, but also more complex pat-
terns like ?X was known as a Y ?, ?X [. . . ] as Y ?, ?X
[. . . ] can be regarded as Y ? and ?X is unusual among
Y ?. Some of these patterns could not have been
found by primitive regular expression matching.
To test whether thematic heterogeneity influ-
ences LEILA, we ran it on the Wikigeneral corpus.
Finally, to try the limits of our system, we ran it on
the Googlecomposers corpus. As shown in Table
1, the performance of LEILA dropped in these in-
creasingly challenging tasks, but LEILA could still
produce useful results. We can improve the results
on the Googlecomposers corpus by adding the Wi-
kicomposers corpus for training.
23
The different learning methods (kNN and SVM)
performed similarly for all relations. Of course, in
each of the cases, it is possible to achieve a higher
precision at the price of a lower recall. The run-
time of the system splits into parsing (? 40s for
each document, e.g. 3:45h for Wikigeography)
and the core algorithm (2-15min for each corpus,
5h for the huge Wikigeneral).
3.2.2 Results with different competitors
Table 2 shows the results for comparing LEILA
against various competitors (with LEILA in bold-
face). We compared LEILA to TextToOnto and
Text2Onto for the instanceOf relation on the
Wikicomposers corpus. TextToOnto requires an
ontology as source of possible concepts. We gave
it the WordNet ontology, so that it had the same
preconditions as LEILA. Text2Onto does not re-
quire any input. Text2Onto seems to have a preci-
sion comparable to ours, although the small num-
ber of found pairs does not allow a significant con-
clusion. Both systems have drastically lower recall
than LEILA.
For Snowball, we only had access to the test
corpus. Hence we trained LEILA on a small por-
tion (3%) of the test documents and tested on
the remaining ones. Since the original 5 seed
pairs that Snowball used did not appear in the col-
lection at our disposal, we chose 5 other pairs
as examples. We used no counterexamples and
hence omitted the Training Phase of our algorithm.
LEILA quickly finds the pattern ?Y -based X?. This
led to very high precision and good recall, com-
pared to Snowball ? even though Snowball was
trained on a much larger training collection.
The CV-system differs from LEILA, because its
ideal pairs are a table, in which each entity is as-
signed to its most likely concept according to a hu-
man understanding of the text ? independently of
whether there are explicit definitions for the entity
in the text or not. We conducted two experiments:
First, we used the document set used in Cimiano
and Vo?lker?s original paper (Cimiano and Vo?lker,
2005a), the Lonely Planet corpus. To ensure a
fair comparison, we trained LEILA separately on
the Wikicomposers corpus, so that LEILA cannot
have example pairs in its output. For the evalu-
ation, we calculated precision and recall with re-
spect to an ideal table provided by the authors.
Since the CV-system uses a different ontology, we
allowed a distance of 4 edges in the WordNet hi-
erarchy to count as a match (for both systems).
Since the explicit definitions that our system relies
on were sparse in the corpus, LEILA performed
worse than the competitor. In a second experi-
ment, we had the CV-system run on the Wikicom-
posers corpus. As the CV-system requires a set
of target concepts, we gave it the set of all con-
cepts in our ideal pairs. Furthermore, the sys-
tem requires an ontology on these concepts. We
gave it the WordNet ontology, pruned to the tar-
get concepts with their super-concepts. We evalu-
ated by the Relaxed Ideal Metric, again allowing
a distance of 4 edges in the WordNet hierarchy to
count as a match (for both systems). This time,
our competitor performed worse. This is because
our ideal table is constructed from the definitions
in the text, which our competitor is not designed
to follow. These experiments only serve to show
the different philosophies in the definition of the
ideal pairs for the CV-system and LEILA. The CV-
system does not depend on explicit definitions, but
it is restricted to the instanceOf-relation.
4 Conclusion and Outlook
We addressed the problem of automatically ex-
tracting instances of arbitrary binary relations
from natural language text. The key novelty of our
approach is to apply a deep syntactic analysis to
this problem. We have implemented our approach
and showed that our system LEILA outperforms
existing competitors.
Our current implementation leaves room for fu-
ture work. For example, the linkages allow for
more sophisticated ways of resolving anaphoras
or matching patterns. LEILA could learn nu-
merous interesting relations (e.g. country /
president or isAuthorOf) and build up an
ontology from the results with high confidence.
LEILA could acquire and exploit new corpora on
its own (e.g., it could read newspapers) and it
could use its knowledge to acquire and structure
its new knowledge more efficiently. We plan to
exploit these possibilities in our future work.
4.1 Acknowledgements
We would like to thank Eugene Agichtein for his
caring support with Snowball. Furthermore, Jo-
hanna Vo?lker and Philipp Cimiano deserve our
sincere thanks for their unreserved assistance with
their system.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gravano.
2000. Snowball: extracting relations from large plain-text
collections. In ACM 2000, pages 85?94, Texas, USA.
[Brin1999] Sergey Brin. 1999. Extracting patterns and rela-
tions from the world wide web. In Selected papers from
the Int. Workshop on the WWW and Databases, pages
172?183, London, UK. Springer-Verlag.
[Buitelaar and Ramaka2005] P. Buitelaar and S. Ramaka.
2005. Unsupervised ontology-based semantic tagging
24
Table 2: Results with different competitors
Corpus M Relation System #D #O #C #I Prec Rec F1
Snowball corp. S headquarters LEILA(SVM) 54 92 82 165 89.13%? 6.36% 49.70%? 7.63% 63.81%
Snowball corp. S headquarters LEILA(kNN) 54 91 82 165 90.11%? 6.13% 49.70%? 7.63% 64.06%
Snowball corp. S headquarters Snowball 54 144 49 165 34.03%? 7.74% 29.70%? 6.97% 31.72%
Snowball corp. I headquarters LEILA(SVM) 54 50 48 126 96.00%? 5.43% 38.10%? 8.48% 54.55%
Snowball corp. I headquarters LEILA(kNN) 54 49 48 126 97.96%? 3.96% 38.10%? 8.48% 54.86%
Snowball corp. I headquarters Snowball 54 64 31 126 48.44%?12.24% 24.60%? 7.52% 32.63%
Wikicomposers S instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03%
Wikicomposers S instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30%
Wikicomposers S instanceOf Text2Onto 87 36 18 1127 50.00% 1.60%? 0.73% 3.10%
Wikicomposers S instanceOf TextToOnto 87 121 47 1127 38.84%? 8.68% 4.17%? 1.17% 7.53%
Wikicomposers R instanceOf LEILA(SVM) 87 336 257 744 76.49%? 4.53% 34.54%? 3.42% 47.59%
Wikicomposers R instanceOf LEILA(kNN) 87 367 276 744 75.20%? 4.42% 37.10%? 3.47% 49.68%
Wikicomposers R instanceOf CV-system 87 134 30 744 22.39% 4.03%? 1.41% 6.83%
Lonely Planet R instanceOf LEILA(SVM) ? 159 42 289 26.42%? 6.85% 14.53%? 4.06% 18.75%
Lonely Planet R instanceOf LEILA(kNN) ? 168 44 289 26.19%? 6.65% 15.22%? 4.14% 19.26%
Lonely Planet R instanceOf CV-system ? 289 92 289 31.83%? 5.37% 31.83%? 5.37% 31.83%
M ? Metric (S: Standard, I: Ideal Metric, R: Relaxed Ideal Metric). Other abbreviations as in Table 1
for knowledge markup. In W. Buntine, A. Hotho, and
Stephan Bloehdorn, editors, Workshop on Learning in Web
Search at the ICML 2005.
[Buitelaar et al2004] P. Buitelaar, D. Olejnik, and M. Sin-
tek. 2004. A protege plug-in for ontology extraction from
text based on linguistic analysis. In ESWS 2004, Herak-
lion, Greece.
[Califf and Mooney1997] M. Califf and R. Mooney. 1997.
Relational learning of pattern-match rules for informa-
tion extraction. ACL-97 Workshop in Natural Language
Learning, pages 9?15.
[Cimiano and Vo?lker2005a] P. Cimiano and J. Vo?lker.
2005a. Text2onto - a framework for ontology learn-
ing and data-driven change discovery. In A. Montoyo,
R. Munozand, and E. Metais, editors, Proc. of the 10th Int.
Conf. on Applications of Natural Language to Information
Systems, pages 227?238, Alicante, Spain.
[Cimiano and Vo?lker2005b] P. Cimiano and J. Vo?lker.
2005b. Towards large-scale, open-domain and ontology-
based named entity classification. In Int. Conf. on Recent
Advances in NLP 2005, pages 166?172.
[Cimiano et al2005] P. Cimiano, G. Ladwig, and S. Staab.
2005. Gimme the context: Contextdriven automatic se-
mantic annotation with cpankow. In Allan Ellis and Tat-
suya Hagino, editors, WWW 2005, Chiba, Japan.
[Etzioni et al2004] O. Etzioni, M. Cafarella, D. Downey,
S. Kok, A. Popescu, T. Shaked, S. Soderland, D. S. Weld,
and A. Yates. 2004. Web-scale information extraction
in knowitall (preliminary results). In WWW 2004, pages
100?110.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
[Finn and Kushmerick2004] A. Finn and N. Kushmerick.
2004. Multi-level boundary classification for information
extraction. In ECML 2004, pages 111?122.
[Freitag and Kushmerick2000] D. Freitag and N. Kushmer-
ick. 2000. Boosted wrapper induction. In American Nat.
Conf. on AI 2000.
[Graupmann2004] Jens Graupmann. 2004. Concept-based
search on semi-structured data exploiting mined semantic
relations. In EDBT Workshops, pages 34?43.
[Hearst1992] A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In ICCL 1992, Nantes,
France.
[J. Iria2005] F. Ciravegna J. Iria. 2005. Relation extraction
for mining the semantic web.
[Maedche and Staab2000] A. Maedche and S. Staab. 2000.
Discovering conceptual relations from text. In W. Horn,
editor, ECAI 2000, pages 85?94, Berlin, Germany.
[Mann and Yarowsky2005] Gideon Mann and David
Yarowsky. 2005. Multi-field information extraction and
cross-document fusion. In ACL 2005.
[Montague1974] R. Montague. 1974. Universal grammar.
In Formal Philosophy. Selected Papers of Richard Mon-
tague. Yale University Press.
[Ravichandran and Hovy2002] D. Ravichandran and
E. Hovy. 2002. Learning surface text patterns for a
question answering system. In ACL 2002, Philadelphia,
USA.
[Riloff1996] E. Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. Annual Conf. on AI
1996, pages 1044?1049.
[Rosch et al1976] E. Rosch, C.B. Mervis, W.D. Gray, D.M.
Johnson, and P. Boyes-Bream. 1976. Basic objects in
natural categories. Cognitive Psychology, pages 382?439.
[Ruiz-Casado et al2005] Maria Ruiz-Casado, Enrique Al-
fonseca, and Pablo Castells. 2005. Automatic extraction
of semantic relationships for wordnet by means of pattern
learning from wikipedia. In NLDB 2006, pages 67?79.
[Sleator and Temperley1993] D. Sleator and D. Temperley.
1993. Parsing english with a link grammar. 3rd Int. Work-
shop on Parsing Technologies.
[Soderland et al1995] S. Soderland, D. Fisher, J. Aseltine,
and W. Lehnert. 1995. Crystal: Inducing a conceptual
dictionary. IJCAI 1995, pages 1314?1319.
[Soderland1999] S. Soderland. 1999. Learning information
extraction rules for semi-structured and free text. Machine
Learning, pages 233?272.
[Suchanek et al2006] Fabian M. Suchanek, Georgiana
Ifrim, and Gerhard Weikum. 2006. Combining Linguistic
and Statistical Analysis to Extract Relations from Web
Documents. In SIGKDD 2006.
[Xu and Krieger2003] F. Xu and H. U. Krieger. 2003. In-
tegrating shallow and deep nlp for information extraction.
In RANLP 2003, Borovets, Bulgaria.
[Xu et al2002] F. Xu, D. Kurz, J. Piskorski, and
S. Schmeier. 2002. Term extraction and mining
term relations from free-text documents in the financial
domain. In Int. Conf. on Business Information Systems
2002, Poznan, Poland.
[Yangarber et al2000] R. Yangarber, R. Grishman,
P. Tapanainen, and S. Huttunen. 2000. Automatic
acquisition of domain knowledge for information extrac-
tion. In ICCL 2000, pages 940?946, Morristown, NJ,
USA. Association for Computational Linguistics.
[Yangarber et al2002] R. Yangarber, W. Lin, and R. Grish-
man. 2002. Unsupervised learning of generalized names.
In ICCL 2002, pages 1?7, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
25
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 913?921,
Beijing, August 2010
The Bag-of-Opinions Method for Review Rating Prediction from Sparse
Text Patterns
Lizhen Qu
Max-Planck Institute
for Informatics
lqu@mpii.mpg.de
Georgiana Ifrim
Bioinformatics Research
Centre
ifrim@birc.au.dk
Gerhard Weikum
Max-Planck Institute
for Informatics
weikum@mpii.mpg.de
Abstract
The problem addressed in this paper is to
predict a user?s numeric rating in a prod-
uct review from the text of the review. Un-
igram and n-gram representations of text
are common choices in opinion mining.
However, unigrams cannot capture impor-
tant expressions like ?could have been bet-
ter?, which are essential for prediction
models of ratings. N-grams of words, on
the other hand, capture such phrases, but
typically occur too sparsely in the train-
ing set and thus fail to yield robust pre-
dictors. This paper overcomes the limita-
tions of these two models, by introducing
a novel kind of bag-of-opinions represen-
tation, where an opinion, within a review,
consists of three components: a root word,
a set of modifier words from the same sen-
tence, and one or more negation words.
Each opinion is assigned a numeric score
which is learned, by ridge regression,
from a large, domain-independent cor-
pus of reviews. For the actual test case
of a domain-dependent review, the re-
view?s rating is predicted by aggregat-
ing the scores of all opinions in the re-
view and combining it with a domain-
dependent unigram model. The paper
presents a constrained ridge regression al-
gorithm for learning opinion scores. Ex-
periments show that the bag-of-opinions
method outperforms prior state-of-the-art
techniques for review rating prediction.
1 Introduction
1.1 Motivation
Opinion mining and sentiment analysis has be-
come a hot research area (Pang and Lee, 2008).
There is ample work on analyzing the sentiments
of online-review communities where users com-
ment on products (movies, books, consumer elec-
tronics, etc.), implicitly expressing their opinion
polarities (positive, negative, neutral), and also
provide numeric ratings of products (Titov and
McDonald, 2008b; Lerman et al, 2009; Hu and
Liu, 2004; Titov and McDonald, 2008a; Pang
and Lee, 2005; Popescu and Etzioni, 2005a). Al-
though ratings are more informative than polari-
ties, most prior work focused on classifying text
fragments (phrases, sentences, entire reviews) by
polarity. However, a product receiving mostly 5-
star reviews exhibits better customer purchase be-
havior compared to a product with mostly 4-star
reviews. In this paper we address the learning and
prediction of numerical ratings from review texts,
and we model this as a metric regression problem
over an appropriately defined feature space.
Formally, the input is a set of rated documents
(i.e., reviews), {xi, yi}Ni=1, where xi is a sequence
of word-level unigrams (w1, ..., wl) and yi ? R is
a rating. The goal is to learn a function f(x) that
maps the word vector x into a numerical rating y?,
which indicates both the polarity and strength of
the opinions expressed in a document.
Numerical review rating prediction is harder
than classifying by polarity. Consider the follow-
ing example from Amazon book reviews:
The organization of the book is hard to follow
and the chapter titles are not very helpful, so go-
ing back and trying to find information is quite
913
difficult.
We note that there are many subjective words
(hard, helpful, difficult) modified by opinion mod-
ifiers such as (very, quite) and negation words like
(not). For rating prediction, considering opin-
ion modifiers is crucial; very helpful is a much
stronger sentiment than helpful. Negation words
also need attention. As pointed out by Liu and
Seneff (2009) we cannot simply reverse the polar-
ity. For example, if we assign a higher positive
score to very helpful than to helpful, simply re-
versing the sign of the scores would incorrectly
suggest that not helpful is less negative than not
very helpful.
The widely used unigram (bag-of-words)
model (Pang and Lee, 2005; Snyder and Barzilay,
2007; Goldberg and Zhu, 2006; Ganu et al, 2009)
cannot properly capture phrase patterns. Con-
sider the following example: not so helpful vs.
not so bad. In a unigram-based regression model
each unigram gets a weight indicating its polarity
and strength. High positive/negative weights are
strongly positive/negative clues. It is reasonable
to assign a positive weight to helpful and a nega-
tive weight to bad. The fundamental problem of
unigrams arises when assigning a weight to not.
If not had a strongly negative weight, the posi-
tive weight of helpful would be strongly reduced
while the negative weight of bad would be ampli-
fied (by combining weights). This clearly fails to
capture the true intentions of the opinion phrases.
The same problem holds for so, which is an inten-
sifier that should keep the same sign as the word
it modifies. We refer to this limitation of the uni-
gram model as polarity incoherence.
A promising way of overcoming this weakness
is to include n-grams, generalizing the bag-of-
words model into a bag-of-phrases model (Bac-
cianella et al, 2009; Pang and Lee, 2008). How-
ever, regression models over the feature space
of all n-grams (for either fixed maximal n or
variable-length phrases) are computationally ex-
pensive in their training phase. Moreover and
most importantly for our setting, including n-
grams in the model results in a very high dimen-
sional feature space: many features will then oc-
cur only very rarely in the training data. There-
fore, it is difficult if not impossible to reliably
learn n-gram weights from limited-size training
sets. We refer to this problem as the n-gram spar-
sity bottleneck. In our experiments we inves-
tigate the effect of using bigrams and variable-
length ngrams for improving review rating predic-
tion.
1.2 Contribution
To overcome the above limitations of unigram and
n-gram features, we have developed a novel kind
of bag-of-opinions model, which exploits domain-
independent corpora of opinions (e.g., all Amazon
reviews), but is finally applied for learning predic-
tors on domain-specific reviews (e.g., movies as
rated in IMDB or Rottentomatoes). A document
is represented as a bag of opinions each of which
has three components: a root word, a set of modi-
fier words and one or more negation words. In the
phrase not very helpful, the opinion root is help-
ful, one (of potentially many) opinion modifier(s)
is very, and a negation word is not. We enforce po-
larity coherence by the design of a learnable func-
tion that assigns a score to an opinion.
Our approach generalizes the cumulative linear
offset model (CLO) presented in (Liu and Seneff,
2009). The CLO model makes several restrictive
assumptions, most notably, that all opinion scores
within one document are the same as the overall
document rating. This assumption does not hold
in practice, not even in reviews with extremely
positive/negative ratings. For example, in a 5-
star Amazon review the phrases most impressive
book and it helps explain should receive different
scores. Otherwise, the later transfer step to dif-
ferent domains would yield poor predictions. Due
to this restriction, CLO works well on particular
types of reviews that have pro/con entries listing
characteristic major opinions about the object un-
der review. For settings with individual reviews
whose texts do not exhibit any specific structure,
the CLO model faces its limitations.
In our bag-of-opinions method, we address the
learning of opinion scores as a constrained ridge
regression problem. We consider the opinion
scores in a given review to be drawn from an
unknown probability distribution (so they do not
have to be the same within a document). We es-
timate the review rating based on a set of statis-
914
tics (e.g., expectation, variance, etc.) derived from
the scores of opinions in a document. Thus, our
method has a sound statistical foundation and can
be applied to arbitrary reviews with mixed opin-
ion polarities and strengths. We avoid the n-gram
sparsity problem by the limited-size structured
feature space of (root,modifiers,negators) opin-
ions.
We treat domain-independent and domain-
dependent opinions differently in our system. In
the first step we learn a bag-of-opinions model on
a large dataset of online reviews to obtain scores
for domain-independent opinions. Since the po-
larity of opinions is not bound to a topic, one
can learn opinion scores from a pooled corpus
of reviews for various categories, e.g., movies,
books, etc., and then use these scored opinions
for predicting the ratings of reviews belonging
to a particular category. In order to also capture
domain-dependent information (possibly comple-
mentary to the opinion lexicon used for learn-
ing domain-independent opinions), we combine
the bag-of-opinions model with an unigram model
trained on the domain-dependent corpus. Since
domain-dependent training is typically limited,
we model it using unigram models rather than
bag-of-opinions. By combining the two models,
even if an opinion does not occur in the domain-
dependent training set but it occurs in a test re-
view, we can still accurately predict the review rat-
ing based on the globally learned opinion score. In
some sense our combined learning scheme is sim-
ilar to smoothing in standard learning techniques,
where the estimate based on a limited training
set is smoothed using a large background corpus
(Zhai and Lafferty, 2004).
In summary, the contributions of this paper are
the following:
1. We introduce the bag-of-opinions model, for
capturing the influence of n-grams, but in a
structured way with root words, modifiers,
and negators, to avoid the explosion of the
feature space caused by explicit n-gram mod-
els.
2. We develop a constrained ridge regression
method for learning scores of opinions from
domain-independent corpora of rated re-
views.
3. For transferring the regression model to
newly given domain-dependent applications,
we derive a set of statistics over opinion
scores in documents and use these as fea-
tures, together with standard unigrams, for
predicting the rating of a review.
4. Our experiments with Amazon reviews from
different categories (books, movies, music)
show that the bag-of-opinions method out-
performs prior state-of-the-art techniques.
2 Bag-of-Opinions Model
In this section we first introduce the bag-of-
opinions model, followed by the method for
learning (domain-independent) model parameters.
Then we show how we annotate opinions and how
we adapt the model to domain-dependent data.
2.1 Model Representation
We model each document as a bag-of-opinions
{opk}Kk=1, where the number of opinionsK varies
among documents. Each opinion opk consists
of an opinion root wr, r ? SR, a set of opin-
ion modifiers {wm}Mm=1, m ? SM and a set of
negation words {wz}Zz=1, z ? SZ , where the sets
SR, SM , SZ are component index sets of opinion
roots, opinion modifiers and negation words re-
spectively. The union of these sets forms a global
component index set S ? Nd, where d is the di-
mension of the index space. The opinion root de-
termines the prior polarity of the opinion. Modi-
fiers intensify or weaken the strength of the prior
polarity. Negation words strongly reduce or re-
verse the prior polarity. For each opinion, the
set of negation words consists of at most a nega-
tion valence shifter like not (Kennedy and Inkpen,
2006) and its intensifiers like capitalization of the
valence shifter. Each opinion component is asso-
ciated with a score. We assemble the scores of
opinion elements into an opinion-score by using
a score function. For example, in the opinion not
very helpful, the opinion root helpful determines
the prior polarity positive say with a score 0.9, the
modifier very intensifies the polarity say with a
915
score 0.5. The prior polarity is further strongly re-
duced by the negation word not with e.g., a score
-1.2. Then we sum up the scores to get a score of
0.2 for the opinion not very helpful.
Formally, we define the function score(op) as
a linear function of opinion components, which
takes the form
score(op) = sign(r)?rxr
+
M?
m=1
sign(r)?mxm
+
Z?
z=1
sign(r)?zxz (1)
where {xz, xm, xr} are binary variables denoting
the presence or absence of negation words, modi-
fiers and opinion root. {?z, ?m, ?r} are weights of
each opinion elements. sign(r) : wr ? {?1, 1}
is the opinion polarity function of the opinion root
wr. It assigns a value 1/-1 if an opinion root is
positive/negative. Due to the semantics of opin-
ion elements, we have constraints that ?r ? 0
and ?z ? 0. The sign of ?m is determined in the
learning phase, since we have no prior knowledge
whether it intensifies or weakens the prior polar-
ity.
Since a document is modeled as a bag-of-
opinions, we can simply consider the expec-
tation of opinion scores as the document rat-
ing. If we assume the scores are uniformly dis-
tributed, the prediction function is then f(x) =
1
K
?K
k=1 score(opk) which assigns the average of
opinion scores to the document x.
2.2 Learning Regression Parameters
We assume that we can identify the opinion roots
and negation words from a subjectivity lexicon. In
this work we use MPQA (Wilson et al, 2005). In
addition, the lexicon provides the prior polarity of
the opinion roots. In the training phase, we are
given a set of documents with ratings {xi, yi}Ni=1,
and our goal is to find an optimal function f?
whose predictions {y?i}Ni=1 are as close as possi-
bile to the original ratings {yi}Ni=1. Formally, we
aim to minimize the following loss function:
L = 12N
N?
i=1
(f(xi)? yi)2 (2)
where f(xi) is modeled as the average score of
opinions in review xi.
First, we rewrite score(op) as the dot
product ??,p? between a weight vector
? = [?z,?m, ?r] and a feature vector
p = [sign(r)xz, sign(r)xm, sign(r)xr].
In order to normalize the vectors, we
rewrite the weight and feature vectors in
the d dimensional vector space of all root
words, modifiers and negation words. Then
? = [..,?z, 0, ..,?m, 0, .., ?r, 0..] ? Rd and p =
[sign(r)xz, 0, .., sign(r)xm, 0, .., sign(r)xr, ...] ?
Rd. The function f(xi) can then be written as
the dot product ??,vi?, where vi = 1Ki
?Ki
k=1 pk,
with Ki the number of opinions in review xi.
By using this feature representation, the learning
problem is equivalent to:
min
?
L(?) = 12N
N?
i=1
(??,vi?+ ?0 ? yi)2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (3)
where ? ? Rd, ? = [?z,?m,?r]. ?0 is the inter-
cept of the regression function, which is estimated
as the mean of the ratings in the training set. We
define a new variable y?i = yi ? ?0.
In order to avoid overfitting, we add an l2 norm
regularizer to the loss function with the parameter
? > 0.
LR(?) = 12N
N?
i=1
(??,vi? ? y?i)2 +
?
2 ? ? ?
2
2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (4)
We solve the above optimization problem by Al-
gorithm 1 using coordinate descent. The proce-
dure starts with ?0 = 0, ?0 ? Rd. Then it up-
dates iteratively every coordinate of the vector ?
until convergence. Algorithm 1 updates every co-
ordinate ?j , j ? {1, 2, ..., d} of ? by solving the
following one-variable sub-problem:
minlj??j?cjLR(?1, ..., ?j , ..., ?d)
916
where lj and cj denote the lower and upper
bounds of ?j . If j ? SZ , lj = ?? and cj = 0.
If j ? SR, lj = 0 and cj = ?. Otherwise both
bounds are infinity.
According to (Luo and Tseng, 1992), the solu-
tion of this one-variable sub-problem is
??j = max{lj ,min{cj , gj}}
where
gj =
1
N
?N
i=1 vij(y?i ?
?
l 6=j ?lvl)
1
N
?N
i=1 v2ij + ?
Here gj is the close form solution of standard
ridge regression at coordinate j (for details see
(Friedman et al, 2008)). We prove the conver-
gence of Algorithm 1, by the following theorem
using techniques in (Luo and Tseng, 1992).
Theorem 1 A sequence of ? generated by Algo-
rithm 1 globally converges to an optimal solution
?? ? ?? of problem (4), where ?? is the set of
optimal solutions.
Proof: Luo and Tseng (1992) show that coordi-
nate descent for constrained quadratic functions
in the following form converges to one of its global
optimal solutions.
min? h(?) = ??,Q??/2 + ?q,??
s.t. ET? ? b
where Q is a d?d symmetric positive-definite ma-
trix, E is a d? d matrix having no zero column, q
is a d-vector and b is a d-vector.
We rewrite LR in matrix form as
1
2N (y? ?V?)
T (y? ?V?) + ?2?
T?
= 12N (V?)
T (V?) + ?2?
T? ? 12N ((V?)
T y?
? 12N y?
T (V?)) + 12N y?
T y?
= ??,Q??/2 + ?q,??+ constant
where
Q = BTB,B =
[ ?
1
NV?
?Id?d
]
,q = ?1N (V
T y?)
where Id?d is the identity matrix. Because ? >
0, all columns of B are linearly independent. As
Q = BTB and symmetric, Q is positive definite.
We define E as a d ? d diagonal matrix with
all entries on the main diagonal equal to 1 except
eii = ?1, i ? SZ and b is a d-vector with all
entries equal to ?? except bi = 0, for i ? SZ or
i ? SR.
Because the almost cyclic rule is applied to
generate the sequence {?t}, the algorithm con-
verges to a solution ?? ? ??.
Algorithm 1 Constrained Ridge Regression
1: Input: ? and {vn, y?n}Nn=1
2: Output: optimal ?
3: repeat
4: for j = 1, ..., d do
5: gj =
1
N
PN
i=1 vij(y?i?
P
l 6=j ?lvl)
1
N
PN
i=1 v2ij+?6:
??j =
?
?
?
0, if j ? SR and gj < 0
0, if j ? SZ and gj > 0
gj , else
7: end for
8: until Convergence condition is satisfied
2.3 Annotating Opinions
The MPQA lexicon contains separate lexicons for
subjectivity clues, intensifiers and valence shifters
(Wilson et al, 2005), which are used for identify-
ing opinion roots, modifiers and negation words.
Opinion roots are identified as the positive and
negative subjectivity clues in the subjectivity lex-
icon. In the same manner, intensifiers and va-
lence shifters of the type {negation, shiftneg} are
mapped to modifiers and negation words. Other
modifier candidates are adverbs, conjunctions and
modal verbs around opinion roots. We consider
non-words modifiers as well, e.g., punctuations,
capitalization and repetition of opinion roots. If
the opinion root is a noun, adjectives are also in-
cluded into modifier sets.
The automatic opinion annotation starts with
locating the continous subjectivity clue sequence.
Once we find such a sequence and at least one
of the subjectivity clue is positive or negative, we
search to the left up to 4 words for negation words
and modifier candidates, and stop if encountering
another opinion root. Similarly, we search to the
917
right up to 3 unigrams for modifiers and stop if
we find negation words or any other opinion roots.
The prior polarity of the subjectivity sequence is
determined by the polarity of the last subjectivity
clue with either positive or negative polarity in the
sequence. The other subjectivity clues in the same
sequence are treated as modifiers.
2.4 Adaptation to Domain-Dependent Data
The adaptation of the learned (domain-
independent) opinion scores to the target
domain and the integration of domain-dependent
unigrams is done in a second ridge-regression
task. Note that this is a simpler problem than
typical domain-adaptation, since we already know
from the sentiment lexicon which are the domain-
independent features. Additionally, its relatively
easy to obtain a large mixed-domain corpus for
reliable estimation of domain-independent opin-
ion scores (e.g., use all Amazon product reviews).
Furthermore, we need a domain-adaptation step
since domain-dependent and domain-independent
data have generally different rating distributions.
The differences are mainly reflected in the
intercept of the regression function (estimated
as the mean of the ratings). This means that
we need to scale the positive/negative mean of
the opinion scores differently before using it
for prediction on domain-dependent reviews.
Moreover, other statistics further characterize the
opinion score distribution. We use the variance
of opinion scores to capture the reliability of
the mean, multiplied by the negative sign of the
mean to show how much it strengthens/weakens
the estimation of the mean. The mean score of
the dominant polarity (major exp) is also used
to reduce the influence of outliers. Because
positive and negative means should be scaled
differently, we represent positive and negative
values of the mean and major exp as 4 different
features. Together with variance, they are the 5
statistics of the opinion score distribution. The
second learning step on opinion score statistics
and domain-dependent unigrams as features,
re-weights the importance of domain-independent
and domain-dependent information according to
the target domain bias.
3 Experimental Setup
We performed experiments on three target do-
mains of Amazon reviews: books, movies
(DVDs), and music (CDs). For each domain,
we use ca. 8000 Amazon reviews for evalua-
tion; an additional set of ca. 4000 reviews are
withheld for parameter tuning (regularization pa-
rameter, etc.). For learning weights for domain-
independent opinions, we use a mixed-domain
corpus of ca. 350,000 reviews from Amazon
(electronics, books, dvds, etc.); this data is dis-
joint from the test sets and contains no reviews
from the music domain. In order to learn un-
biased scores, we select about the same number
of positive and negative reviews (where reviews
with more/less than 3 stars are regarded as posi-
tive/negative). The regularization parameters used
for this corpus are tuned on withheld data with ca.
6000 thematically mixed reviews.1.
We compare our method, subsequently referred
to as CRR-BoO (Constrained Ridge Regression
for Bag-of-Opinions), to a number of alternative
state-of-the-art methods. These competitors are
varied along two dimensions: 1) feature space,
and 2) training set. Along the first dimension,
we consider a) unigrams coined uni, b) unigrams
and bigrams together, coined uni+bi, c) variable-
length n-grams coined n-gram, d) the opinion
model by (Liu and Seneff, 2009) coined CLO (cu-
mulative linear offset model). As learning pro-
cedure, we use ridge regression for a), b), and
d), and bounded cyclic regression, coined BCR,
for c). Along the second - orthogonal - di-
mension, we consider 3 different training sets:
i) domain-dependent training set coined DD, ii)
the large mixed-domain training set coined MD,
iii) domain-dependent training set and the large
mixed-domain training set coined DD+MD. For
the DD+MD training set, we apply our two stage
approach for CRR-BoO and CLO, i.e., we use
the mixed-domain corpus for learning the opinion
scores in the first stage, and integrate unigrams
from DD in a second domain-adaptation stage.
We train the remaining feature models directly on
the combination of the whole mixed-domain cor-
1All datasets are available from
http://www.mpi-inf.mpg.de/?lqu
918
feature models uni uni+bi n-gram CLO CRR-BoO
DD
book 1.004 0.961 0.997 1.469 0.942
dvd 1.062 1.018 1.054 1.554 0.946
music 0.686 0.672 0.683 0.870 0.638
MD
book 1.696 1.446 1.643 1.714 1.427
dvd 1.919 1.703 1.858 1.890 1.565
music 2.395 2.160 2.340 2.301 1.731
DD+MD
book 1.649 1.403 1.611 1.032 0.884
dvd 1.592 1.389 1.533 1.086 0.928
music 1.471 1.281 1.398 0.698 0.627
Table 1: Mean squared error for rating prediction methods on Amazon reviews.
pus and the training part of DD.
The CLO model is adapted as follows. Since
bags-of-opinions generalize CLO, adjectives and
adverbs are mapped to opinion roots and modi-
fiers, respectively; negation words are treated the
same as CLO. Subsequently we use our regression
technique. As Amazon reviews do not contain pro
and con entries, we learn from the entire review.
For BCR, we adapt the variable-length n-grams
method of (Ifrim et al, 2008) to elastic-net-
regression (Friedman et al, 2008) in order to ob-
tain a fast regularized regression algorithm for
variable-length n-grams. We search for signifi-
cant n-grams by incremental expansion in back-
ward direction (e.g., expand bad to not bad). BCR
pursues a dense solution for unigrams and a sparse
solution for n-grams. Further details on the BCR
learning algorithm will be found on a subsequent
technical report.
As for the regression techniques, we show
only results with ridge regression (for all fea-
ture and training options except BCR). It outper-
formed -support vector regression (SVR) of lib-
svm (Chang and Lin, 2001), lasso (Tibshirani,
1996), and elastic net (Zou and Hastie, 2005) in
our experiments.
4 Results and Discussion
Table 1 shows the mean square error (MSE) from
each of the three domain-specific test sets. The er-
ror is defined as MSE = 1N
?N
i=1(f(xi) ? yi)2.
The right most two columns of the table show re-
sults for the full-fledge two-stage learning for our
method and CLO, with domain-dependent weight
learning and the domain adaptation step. The
other models are trained directly on the given
training sets. For the DD and DD+MD train-
ing sets, we use five-fold cross-validation on the
domain-specific sets. For the MD training set, we
take the domain-specific test sets as hold-out data
for evaluation.
Table 1 clearly shows that our CRR-BoO
method outperforms all alternative methods by a
significant margin. Most noteworthy is the mu-
sic domain, which is not covered by the mixed-
domain corpus. As expected, unigrams only per-
form poorly, and adding bigrams leads only to
marginal improvements. BCR pursues a dense
solution for unigrams and a sparse solution for
variable-length n-grams, but due to the sparsity
of occurence of long n-grams, it filters out many
interesting-but-infrequent ngrams and therefore
performs worse than the dense solution of the
uni+bi model. The CLO method of (Liu and Sen-
eff, 2009) shows unexpectedly poor performance.
Its main limitation is the assumption that opinion
scores are identical within one document. This
does not hold in documents with mixed opinion
polarities. It also results in conflicts for opinion
components that occur in both positive and nega-
tive documents. In contrast, CRR-BoO naturally
captures the mixture of opinions as a bag of pos-
itive/negative scores. We only require that the
mean of opinion scores equals the overall docu-
ment rating.
The right most column of Table 1 shows that
our method can be improved by learning opinion
scores from the large mixed-domain corpus. How-
919
opinion score
good 0.18
recommend 1.64
most difficult -1.66
but it gets very good! 2.37
would highly recommend 2.73
would not recommend -1.93
Table 2: Example opinions learned from the Ama-
zon mixed-domain corpus.
ever, the high error rates of the models learned di-
rectly on the MD corpus show that direct training
on the mixed-domain data can introduce a signifi-
cant amount of noise into the prediction models.
Although the noise can be reduced by learning
from MD and DD together, the performance is
still worse than when learning directly from the
domain-dependent corpora. Additionally, when
the domain is not covered by the mixed-domain
corpus (e.g., music), the results are even worse.
Thus, the two stages of our method (learning
domain-independent opinion scores plus domain-
adaptation) are decisive for a good performance,
and the sentiment-lexicon-based BoO model leads
to robust learning of domain-independent opinion
scores.
Another useful property of BoO is its high in-
terpretability. Table 2 shows example opinion
scores learned from the mixed-domain corpus.
We observe that the scores corelate well with our
intuitive interpretation of opinions.
Our CRR-BoO method is highly scalable.
Excluding the preprocessing steps (same for
all methods), the learning of opinion compo-
nent weights from the ca. 350,000 domain-
independent reviews takes only 11 seconds.
5 Related Work
Rating prediction is modeled as an ordinal re-
gression problem in (Pang and Lee, 2005; Gold-
berg and Zhu, 2006; Snyder and Barzilay, 2007).
They simply use the bag-of-words model with re-
gression algorithms, but as seen previously this
cannot capture the expressive power of phrases.
The resulting models are not highly interpretable.
Baccianella et al (2009) restrict the n-grams to
the ones having certain POS patterns. However,
the long n-grams matching the patterns still suffer
from sparsity. The same seems to hold for sparse
n-gram models (BCR in this paper) in the spirit
of Ifrim et al (2008). Although sparse n-gram
models can explore arbitrarily large n-gram fea-
ture spaces, they can be of little help if the n-grams
of interests occur sparsely in the datasets.
Since our approach can be regarded as learning
a domain-independent sentiment lexicon, it is re-
lated to the area of automatically building domain-
independent sentiment lexicons (Esuli and Sebas-
tiani, 2006; Godbole et al, 2007; Kim and Hovy,
2004). However, this prior work focused mainly
on the opinion polarity of opinion words, neglect-
ing the opinion strength. Recently, the lexicon
based approaches were extended to learn domain-
dependent lexicons (Kanayama and Nasukawa,
2006; Qiu et al, 2009), but these approaches
also neglect the aspect of opinion strength. Our
method requires only the prior polarity of opinion
roots and can thus be used on top of those meth-
ods for learning the scores of domain-dependent
opinion components. The methods proposed in
(Hu and Liu, 2004; Popescu and Etzioni, 2005b)
can also be categorized into the lexicon based
framework because their procedure starts with a
set of seed words whose polarities are propagated
to other opinion bearing words.
6 Conclusion and Future Work
In this paper we show that the bag-of-opinions
(BoO) representation is better suited for captur-
ing the expressive power of n-grams while at the
same time overcoming their sparsity bottleneck.
Although in this paper we use the BoO represen-
tation to model domain-independent opinions, we
believe the same framework can be extended to
domain-dependent opinions and other NLP appli-
cations which can benefit from modelling n-grams
(given that the n-grams are decomposable in some
way). Moreover, the learned model can be re-
garded as a domain-independent opinion lexicon
with each entry in the lexicon having an associated
score indicating its polarity and strength. This in
turn has potential applications in sentiment sum-
marization, opinionated information retrieval and
opinion extraction.
920
References
Baccianella, S., A. Esuli, and F. Sebastiani. 2009.
Multi-facet rating of product reviews. In ECIR.
Springer.
Chang, C.C. and C.J. Lin, 2001. LIBSVM: a
library for support vector machines. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Esuli, A. and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In LREC, pages 417?422.
Friedman, J., T. Hastie, and R. Tibshirani. 2008.
Regularization paths for generalized linear models
via coordinate descent. Technical report, Techni-
cal Report, Available at http://www-stat. stanford.
edu/jhf/ftp/glmnet. pdf.
Ganu, G., N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In 12th International Workshop on the
Web and Databases.
Godbole, Namrata, Manjunath Srinivasaiah, and
Steven Skiena. 2007. Large-scale sentiment anal-
ysis for news and blogs. In ICWSM.
Goldberg, A. B. and X.J. Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-
based semi-supervised learning for sentiment cat-
egorization. In HLT-NAACL 2006 Workshop on
Textgraphs: Graph-based Algorithms for Natural
Language Processing.
Hu, M.Q. and B. Liu. 2004. Mining and summarizing
customer reviews. In CIKM, pages 168?177. ACM
New York,USA.
Ifrim, G., G. Bakir, and G. Weikum. 2008. Fast logis-
tic regression for text categorization with variable-
length n-grams. In KDD, pages 354?362, New
York,USA. ACM.
Kanayama, H. and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In EMNLP, pages 355?363.
Kennedy, A. and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING, pages 1367?1373.
Lerman, K., S. Blair-Goldensohn, and R. McDonald.
2009. Sentiment summarization: Evaluating and
learning user preferences. In EACL, pages 514?522.
ACL.
Liu, J.J. and S. Seneff. 2009. Review Sentiment
Scoring via a Parse-and-Paraphrase Paradigm. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
161?169. ACL.
Luo, Z.Q. and Q. Tseng. 1992. On the convergence of
the coordinate descent method for convex differen-
tiable minimization. Journal of Optimization The-
ory and Applications, 72(1):7?35.
Pang, B. and L. Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, page 124. ACL.
Pang, B. and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Popescu, A.M. and O. Etzioni. 2005a. Extracting
product features and opinions from reviews. In
HLT/EMNLP, volume 5, pages 339?346. Springer.
Popescu, A.M. and O. Etzioni. 2005b. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP, volume 5, pages 339?
346. Springer.
Qiu, G., B. Liu, J.J. Bu, and C. Chen. 2009. Ex-
panding Domain Sentiment Lexicon through Dou-
ble Propagation. In IJCAI.
Snyder, B. and R. Barzilay. 2007. Multiple as-
pect ranking using the good grief algorithm. In
NAACL/HLT, pages 300?307.
Tibshirani, R. 1996. Regression shrinkage and selec-
tion via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Titov, I. and R. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In HLT/ACL, pages 308?316.
Titov, I. and R. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In WWW,
pages 111?120. ACM.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT/ACL, pages 347?354.
Zhai, C. X. and J. Lafferty. 2004. A study of smooth-
ing methods for language models applied to infor-
mation retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
Zou, H. and T. Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of
the Royal Statistical Society Series B(Statistical
Methodology), 67(2):301?320.
921
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2195?2204, Dublin, Ireland, August 23-29 2014.
HARPY: Hypernyms and Alignment of Relational Paraphrases
Adam Grycner
Max-Planck Institute for Informatics
Campus E1.4, 66123
Saarbr?ucken, Germany
agrycner@mpi-inf.mpg.de
Gerhard Weikum
Max-Planck Institute for Informatics
Campus E1.4, 66123
Saarbr?ucken, Germany
weikum@mpi-inf.mpg.de
Abstract
Collections of relational paraphrases have been automatically constructed from large text cor-
pora, as a WordNet counterpart for the realm of binary predicates and their surface forms. How-
ever, these resources fall short in their coverage of hypernymy links (subsumptions) among the
synsets of phrases. This paper closes this gap by computing a high-quality alignment between
the relational phrases of the Patty taxonomy, one of the largest collections of this kind, and the
verb senses of WordNet. To this end, we devise judicious features and develop a graph-based
alignment algorithm by adapting and extending the SimRank random-walk method. The re-
sulting taxonomy of relational phrases and verb senses, coined HARPY, contains 20,812 synsets
organized into a Directed Acyclic Graph (DAG) with 616,792 hypernymy links. Our empirical as-
sessment, indicates that the alignment links between Patty and WordNet have high accuracy, with
Mean Reciprocal Rank (MRR) score 0.7 and Normalized Discounted Cumulative Gain (NDCG)
score 0.73. As an additional extrinsic value, HARPY provides fine-grained lexical types for the
arguments of verb senses in WordNet.
1 Introduction
Motivation: This paper addresses the task of discovering and organizing paraphrases of relations be-
tween entities (Lin and Pantel, 2001; Fader et al., 2011; Nakashole et al., 2012; Moro and Navigli, 2012;
Alfonseca et al., 2013). This task involves understanding that the phrases ?travels to?, ?visits? and ?on
her tour through? (relating a person and a country) are synonymous and that ?leader of? and ?works
with? (relating a person and an organization) are in a hypernymy relation: the former is subsumed by
the latter. This kind of lexical knowledge can be harnessed for advanced tasks like question answering
(Fader et al., 2013), search over web tables (Gupta et al., 2014), or event mining over news (Alfonseca
et al., 2013).
Work along these lines has developed large repositories of relational paraphrases, most notably, the
collections ReVerb (Fader et al., 2011), Patty (Nakashole et al., 2012), and WiSeNet (Moro and Navigli,
2012). The largest of these, Patty, contains ca. 350,000 synsets of phrases, each annotated with ontolog-
ical types of their two arguments (e.g., person ? country, or politician ? political party). However, the
subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links between phrases,
and the entire taxonomy is kind of fragmented into a many-rooted DAG (directed acyclic graph). More-
over, the synsets are rather noisy in the long tail with low confidence. WiSeNet, an alternative resource,
has ca. 40,000 synsets and no hypernymy links.
WordNet (Fellbaum, 1998), on the other hand, is a very rich resource on synonymy and hypernymy.
However, its coverage of binary relations (as opposed to unary predicates, mostly nouns) is restricted
to (mostly) single-word verbs. WordNet has ca. 13,767 verb synsets, organized into a hierarchy with
13,239 hypernymy links. Unlike Patty, though, WordNet does not associate verb senses with a lexical
type signature for the subject and object arguments of a verb, and it is sparse in multi-word phrases.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2195
Resources like VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) aim to overcome these
deficiencies, but are much smaller.
Goal and Approach: In this paper, our goal is to overcome the limitations of resources like Patty
and WordNet. We want to reconcile the wealth of Patty?s multi-word paraphrases with lexical typing, on
one hand, and the clean hypernymy organization of WordNet verbs, on the other hand. To this end, we
compute an alignment between the phrase synsets that Patty provides with the verb senses of WordNet.
This has mutual benefits: 1) we enhance many Patty phrases with the clean hypernyms of WordNet,
this way augmenting the subsumption hierarchy, and 2) we extend WordNet verb senses with the lexical
type signatures derived from Patty. Our approach uses a variety of features from both of the two aligned
resources, as well as further auxiliary sources. Algorithmically, we build on an advanced notion of
random walks over graphs, known as SimRank (Jeh and Widom, 2002).
Contributions: Our method is able to construct a high-quality taxonomy of relational paraphrases,
coined HARPY, that combines the richness of Patty with the clean hierarchy of WordNet. The algorithm
for computing the alignment is efficient and robust. One can think of the alignment as a way of sense-
disambiguating Patty phrases by mapping them to WordNet. HARPY links 20,812 of the Patty phrases
to WordNet. Conversely, 4,789 out of 13,767 WordNet verb senses are enriched with information from
Patty. We evaluate the quality of HARPY by extensive sampling with human assessment. We also
demonstrate its benefit by the extrinsic use-case of annotating WordNet verb senses with lexical type
signatures. All experimental data and the HARPY resource will be available on a public web site.
2 Related Work
With the proliferation of knowledge bases, like Freebase (Google Knowledge Graph), DBpedia, YAGO,
or ConceptNet, there is a wealth of resources about entities and semantic classes (i.e., unary predicates
and their instances). In contrast, the systematic compilation of paraphrases for relations (i.e., binary
predicates) has received much less attention. Some of the knowledge-base projects, especially those that
center on Open Information Extraction, make intensive use of surface patterns (e.g., verbal phrases) that
indicate relations (e.g., (Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012; Speer and Havasi,
2012; Wu et al., 2012)); however, they do not organize these patterns into a WordNet-style taxonomy.
Prior work towards such taxonomies go back to the projects DIRT (Lin and Pantel, 2001), VerbOcean
(Chklovski and Pantel, 2004), and VerbNet (Kipper et al., 2008). However, the resulting resources were
mostly restricted to single verbs. ReVerb (Fader et al., 2011) extended these approaches by automatically
mining entire phrases from Web contents, but still with focus on verbal structures. Patty (Nakashole et
al., 2012) used sequence mining algorithms for gathering a general class of relational phrases, organizing
them into synsets, and inferring lexical type signatures. WiseNet (Moro and Navigli, 2012) harnessed
phrases from Wikipedia articles and clustered them into synsets of relational phrases. All of these works
are fairly limited in their coverage of subsumptions (hypernymy) between relational phrases.
There is ample work on computing alignments among different kinds of lexical thesauri, dictionar-
ies, taxonomies, ontologies, and other forms of linguistic or semantic resources. Prominent cases along
these lines include the alignments between FrameNet and WordNet (Ferr?andez et al., 2010), VerbNet
and PropBank (Palmer, 2009), Wikionary and WordNet (Meyer and Gurevych, 2012), and across mul-
tilingual WordNets and/or Wikipedia editions (e.g., (de Melo and Weikum, 2009; Navigli and Ponzetto,
2012)). For aligning ontologies based on OWL and RDF logics, there is a series of annual benchmark
competitions (Grau et al., 2013). Most approaches are based on relatedness measures and context simi-
larities between words or concepts and their neighborhoods in the respective resources (e.g., (Banerjee
and Pedersen, 2003; Budanitsky and Hirst, 2006; Gabrilovich and Markovitch, 2007)). Algorithmically,
this translates into a nearest-neighbor (most-similar) assignment between entries of different resources.
More sophisticated methods use similarities merely to assign weights to relatedness edges in a graph,
and then employ random walks on such a graph (e.g., (Pilehvar et al., 2013)). The prevalent method
of this kind uses Personalized Page Rank (Haveliwala, 2002)), computing stationary probabilities for
reaching nodes in one resource when starting random walks on a given node of the other resources (with
randomized restarts).
2196
Computing alignments between resources can sometimes be viewed as a task of disambiguation words
or concepts in one resource by mapping them to the other resource (e.g., mapping Wiktionary entries
onto WordNet senses). Thus, the huge body of work on word sense disambiguation (WSD) is relevant,
too. Methodologically, this research also relies, to a large extent, on relatedness/similarity measures and
random walks on appropriately constructed graphs. See (Navigli, 2009) for an extensive survey.
There is remotely related work on several other tasks in computational linguistics and text mining.
These include semantic relatedness between concepts or words (e.g., (Gabrilovich and Markovitch, 2007;
Pilehvar et al., 2013)), type inference for the arguments of a phrase (e.g., (Kozareva and Hovy, 2010;
Nakashole et al., 2013)), and entailment among verbs (e.g., (Hashimoto et al., 2009)). The SemEval-2010
task on classification of semantic relations (Hendrickx et al., 2010) addressed the problem of predicting
the relation for a given sentence and pair of nominals, but was limited to a small prespecified set of
relations.
3 Constructing a Candidate Alignment Graph
The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb
synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section
4 will then discuss the actual alignment algorithm.
Vertices of the CAG represent
? synsets of relational phrases in Patty, or phrases for short,
? verb senses from WordNet, verbs for short,
? features of either phrases or verbs.
Edges of the CAG correspond to relations between phrases, verbs, and features. We consider three types
of relations here: similarity, hypernymy, and vertex-features. Edges are weighted (see below).
Vertex Types: There are 6 kinds of vertices in the CAG. Since we aim to connect Patty phrases with
WordNet verbs, these two are the main kinds of vertices. Additionally, the graph contains feature vertices
representing noun senses from WordNet (nouns for short), surface verbs as occurring in sample texts,
sentence frames from WordNet, and specifically derived phrase-verb vertices connecting phrases and
verbs. The latter are constructed by combining each phrase with its top-10 most similar verb senses. To
this end, we retrieve all verb synsets from WordNet and rank the verb synsets by the cosine similarity
between the support sentences that Patty provides for its phrases (i.e., sentences from Wikipedia that
contain instances of a phrase) and the usage examples in WordNet glosses. The resulting vertices are
labeled by the combination of phrase id and verb-sense id. Having these combinations as vertices, rather
than simply connecting phrases and verbs via edges, leads to a CAG structure that is better suited for our
random walk algorithms (see Section 4). Table 1 gives examples for the 6 vertex types.
Relational Phrase Verb Sense Noun Sense Surface Verb Sentence Frame Phrase-Verb Pair
[person] succeeded
[person]
succeed2#verb king1#noun succeed Somebody ----s
somebody
(phrase 1,
verb sense 2)
[musician] played
jazz with [musician]
play3#verb music1#noun play Somebody ----s
something
(phrase 2,
verb sense 3)
Table 1: Examples of vertex types
Edge Types: Edges in the graph represent 3 different types of relationships between vertices:
? For all relational phrases, all verb senses from WordNet and also all noun senses (as feature vertices),
we capture their hypernymy relations as edges.
? We connect phrase-verb vertices with their constituents, phrase vertices and verb vertices, by simi-
larity edges, with weights derived from the similarity computation.
? The remaining edges connect phrases or verbs with their respective feature vertices. There are 6
kinds of such vertex-feature edges, explained next.
2197
Verb Features: The following features are associated with verb senses. A lemma edge connects a verb
sense with one or more surface-verb vertices, as given in WordNet glosses. A domain edge edge connects
a verb sense with noun senses that describe the usage domain of the verb (e.g. literature, politics). This
information is retrieved from WordNet and the WordNet Domains project (Bentivogli et al., 2004). While
the latter does not provide sense-disambiguated information, we need to add a mechanism which maps
domain information to its WordNet noun sense counterpart. Therefore, we map domain surface nouns to
their most frequent senses.
In addition, we harness the WordNet links of type derivationally related form to construct further edges
between verb senses and noun-sense features in our CAG. The last type of edges for verb-sense features
are sentence frame edges, between verb vertices and feature vertices of type sentence frame. WordNet for
each verb sense provides information about its sentence frames. There are defined 35 possible sentence
frames.
Phrase Features: Relational phrases are associated with the following features. A verb-in-phrase
edge connects a phrase with a surface verb whenever the phrase contains the verb after lemmatization.
Analogously to the domain edges for verb senses, we introduce Wikipedia-category edges between re-
lational phrases and noun senses. Patty provides us with Wikipedia articles where instances of a phrase
occur. We consider all Wikipedia categories of such an article as a source for related noun senses.
We use ontological types of the articles and the categories and their mappings to Wordnet provided by
the YAGO project (Suchanek et al., 2007). Finally, we also introduce sentence-frame edges between
relational phrases and sentence-frame feature vertices. To avoid polluting the CAG with overly noisy
connections, we apply specific tests. First, we check if the lexical argument types of a phrase and a
frame are compatible (e.g., musician is compatible with person, but not with location). Second, we com-
pare characteristic prepositions in the phrase and the frame. We create and edge only if these additional
tests are affirmative.
Examples of vertices connected by the different edge types with verb vertices and phrase vertices are
shown in Table 2 and 3, respectively.
Hypernymy Similarity Lemma Domain Derivationally
Related Form
Sentence Frame
replace2#verb (phrase 1,
verb sense 2)
?succeed?,
?come after?
politics1#noun successor1#noun Somebody ----s
somebody
Table 2: Vetices connected by different edges with vertex ?succeed2#verb? of type verb.
Hypernymy Similarity Verbs in phrase Wikipedia Category Sentence Frame
[person] replaced [person] (phrase 1, verb sense 2) ?succeed? politician1#noun Somebody ----s
somebody
Table 3: Vetices connected by different edges with vertex ?[person] succeeded [person]? of type phrase.
EdgeWeights: All edges in the graph are weighted. The weights are derived from frequency counts of
features and/or similarity scores, or are simply set to 1 for binary cases (e.g., hypernymy edges). Lemma
edges between verb senses and surface verbs vertices are weighted in proportion to the frequency count
of a verb sense, as given by WordNet. Wikipedia-category edges have weights based on the number
of occurrences of a relational phrase in Wikipedia articles and the frequencies of categories. Similarity
edges have weights set according to the cosine similarity between examples of a verb sense and examples
of a relational phrase.
Finally, we normalize all weights in the graph by requiring that the sum of weights of the incoming
edges is equal to 1 for every vertex. For the verb and phrase vertices, we perform an additional nor-
malization so that each kind of edge has the same impact in terms of the total edge weight per edge
kind.
The above procedure leads to a CAG with 238,437 vertices and 4,776,116 edges. Figure 1 shows an
excerpt for illustration.
2198
  
succeed2#verb
replace2#verb[person] replaced [person]
[person] succeeded [person]
[person] took throne after [person]
Hypernymy
Hypernymy
Hypernymypolitics1#noun
 successor1#noun
succeed
 Somebody ----s somebody
([person] succeeded [person],succeed2#verb)
           Sentence Frame Sentence Frame    Similarity Similarity
                  Wik
ipedia CategoryWikipedia
 Category Domain
Derivationally Related Form      Verbs in phrase Lemma
Relational phrases Features Verb senses
Figure 1: Excerpt from Candidate Alignment Graph
4 Alignment Algorithm
Our algorithm runs on the directed candidate alignment graph (CAG). Intuitively, it aims to find ?strong
paths? between relational-phrase vertices and verb-sense vertices. We use random-walk methods to this
end. For each relational phrase, we compute scores and a ranked list of verb senses to which the phrase
likely corresponds. The top-ranked verb would ideally be the desired alignment.
SimRank: We employ the SimRank algorithm (Jeh and Widom, 2002), an advanced form of random
walks. SimRank computes similarity scores between a pair of vertices in a weighted graph, based on
the neighborhoods of the two vertices. The definition, formally given in Equation 1, is recursive: two
vertices are similar if their neighborhoods are similar. In the standard SimRank equation, I
i
(a) represents
the i
th
(incoming) neighbor of vertex a, and C is a constant dampening factor.
s(a, b) =
C
|I(a)| |I(b)|
|I(a)|
?
i=1
|I(b)|
?
j=1
s (I
i
(a), I
j
(b)) (1)
SimRank helps capturing long-distance dependencies between vertices in a graph. This would not be
achieved by simpler similarity measures of context vectors. Note that SimRank is quite different from
(Personalized) PageRank methods; SimRank can be seen as a random walk over pairs of nodes, not over
individual nodes. During the CAG construction, we tried to keep the path lengths between phrase vertices
and verb vertices uniform for all kinds of feature vertices, to avoid biasing the influence of specific
features. Since the SimRank similarity is based on two random walks meeting, the method works best
when all paths between source-target node pairs have even length. With this property SimRank produces
better results; we introduced explicit phrase-verb vertices for this reason.
SimRank with Fingerprints: Unfortunately, SimRank has very high computational complexity: the
run-time of a straightforward implementation is O(Kn
4
), where n is the number of vertices in the graph
and K is the number of iterations in an iterative fixpoint computation (in the style of the Jacobi method).
However, there are much faster approximations of SimRank. We use a variant known as SimRank with
fingerprints (Fogaras and R?acz, 2005) To approximate the SimRank score for two vertices, this method
computes the expected first meeting time for two random walks originating from the two vertices (with
randomized restarts). To this end, the method precomputes a fingerprint for each vertex a: a data structure
holding the visiting probabilities of vertices for standard random walks originating in a. A fast imple-
mentation actually runs random walks a specified number of times, to estimate the visiting probabilities.
For two vertices a and b, the expected number of hops until their random walks meet in a common vertex
is then efficiently computed from the fingerprints of a and b. Moreover, this method allows computing
the SimRank score for a pair of vertices on demand, only for vertex pairs of interest, rather than having
to compute all O(n
2
) scores.
The original SimRank method works with unweighted graphs. In our setting, we modify transition
probabilities according to edge weights. Our extended SimRank variant is equivalent to Equation 2,
2199
where W (a, b) denotes the weight of the edge between a and b. This equation is similar to the weighted
variant of (Antonellis et al., 2007).
s
w
(a, b) = C ?
|I(a)|
?
i=1
|I(b)|
?
j=1
W (a, I
i
(a)) ?W (b, I
j
(b)) ? s
w
(I
i
(a), I
j
(b)) (2)
Unlike the original SimRank method, we also incorporate random jumps in the underlying random-
walk model. Each vertex has a different random jump probability, explained next.
Random Jumps: The original SimRank definition favors vertices with smaller neighborhoods. To
avoid this bias, we introduce a form of smoothing on the graph. Whenever a phrase vertex or verb vertex
lacks some of the feature types that other vertices may have, we introduce an option for random jumps
from the given vertex to any other vertex in the graph. For each missing kind of feature (e.g., domain
feature or sentence-frame feature), we assign a probability mass of , a small constant, for a random
jump. So if several features are missing, there is an accumulated probability for a jump. The target
of a random jump is always chosen with uniform distribution. A final normalization of edge weights
(with linear adjustment) ensures that the possible transitions from a vertex form a proper probability
distribution. he method works also without smoothing (i.e., setting the constant to 0), but the results tend
to be worse. The results are not very sensitive to the exact choice of the random-jump parameter.
Filtering and Candidate Pruning: The target of our alignment is the WordNet verb hierarchy, but
not all relational phrases can be mapped into this target space. Therefore, we restrict ourselves to a subset
of relational phrases that contain exactly one verb. This eliminates noun phrases (e.g. ?father of?) and
phrases that contain multiple verbs (e.g. ?succeed and died?, ?succeeded in persuading?). Noun phrases
should be aligned to the WordNet noun hierarchy and it should be treated as a different task (using e.g.
state-of-the-art work (Ponzetto and Navigli, 2010)). Multi-verb phrases often pose semantic difficulties.
Note that the verbs in these phrases are always transitive verbs, as Patty is derived from subject-phrase-
object structures in large corpora. We also used the cardinalities of the support sentences in Patty for
pruning the noisy tail of phrases, by dropping all phrases that have only a single instance.
To avoid computing SimRank scores for every pair of vertices, we prune the search space as follows.
We consider only pairs of relational phrases and verb senses which contain the same surface verb (with
lemmatization).
Deriving Hypernymy Links: Once we have alignments between phrases and verbs, we derive hy-
pernymy relations among phrases as follows. Whenever phrases p
1
and p
2
are aligned with verb senses
v
1
and v
2
, respectively, and v
1
is a direct or transitive hypernym of v
2
, we infer that p
1
is a hypernym
of p
2
. We consider transitive hypernyms because not every WordNet verb sense has a phrase aligned
with it; without transitivity we would obtain a very sparse hierarchy. By the acyclicity of the WordNet
hypernymy structure, the process yields a proper DAG. However, the output contains redundant links
(direct ones and transitive ones connecting the same pair of phrases); these are subsequently eliminated
by a transitive reduction algorithm (Aho et al., 1972).
5 Evaluation
We evaluated the quality of the HARPY alignments by manual assessment of a large sample set, and
compared it against several alternative methods.
Baselines: We compared our SimRank-based method against the following baselines, each given the
same feature set:
? Cosine Similarity: for each relational phrase and verb sense, we create a contextual vector (in the
spirit of distributional semantics) consisting of the features described in Section 3, with tf-idf-based
weights (Manning et al., 2008). The alignment ranking is computed by the cosine similarity of tf-
idf-weighted contextual vectors.
? Modified Adsorption (MAD): a label propagation algorithm (Talukdar and Crammer, 2009) run on
the candidate alignment graph. In our setting, each relational phrase is a label. Initially, only the
respective phrase vertices have this label. The algorithm propagates labels to other vertices, based on
2200
the graph?s edge weights. The top-k results for the alignment of a phrase are the verb senses with the
highest probability for the phrase label. We use the Junto Label Propagation Toolkit
1
.
? Personalized PageRank (PPR): a method for random walks with random jumps back to the start
vertex (Haveliwala, 2002). For each phrase, a separate PPR is performed. The ranking of verb senses
is produced by the visiting probabilities according to the PPR scores.
? Most Frequent Sense (MSF): For each phrase, we consider only verb senses that contain the same
surface verb (with lemmatization), and rank them by the WordNet frequency information.
Assessment: We retrieved a random subset of 261 relational phrases considered for alignment, and
showed the results of the different alignment methods to two human judges. For each relational phrase,
we displayed its textual form, list of usage examples, and the top-5 ranked list of verb senses computed
by each method under comparison. Each verb sense was enriched with information about its lemmas, its
gloss, and examples. The evaluators were asked to identify the verb sense that is semantically equivalent
to the given relational phrase (including the option of saying ?none?).
Quality Measures: As all methods compute a ranked list of verb senses for a given phrase where
exactly one list item is correct, we use quality measures geared for such rankings: Mean Reciprocal
Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG). In addition, we report on the
precision for top-k results, for small k (1, 3, or 5). Here, a top-k result is considered good if the correct
verb senses appears among the top-k alignments, for a given phrase.
Results: The results are shown in Table 4. Our method outperforms all baselines. Among the com-
petitors, MFS shows the best performance. This is not so surprising; MFS is rarely outperformed in
word sense disambiguation (McCarthy et al., 2004; Navigli and Lapata, 2010). Our gains over MFS are
remarkable. In total, HARPY aligned 20,812 phrases to 4,789 verb senses, and also obtained 616,792
hypernymy links between phrases.
The evaluation process led to high inter-judge agreement, with Cohen?s Kappa around 0.678. The
number of samples, 261, was large enough for statistical significance: we performed a paired t-test for
MRR, NDCG and Precision@1 of the SimRank results against each of the baselines, and obtained
p-values below 0.05.
SimRank MFS PPR MAD Cosine
MRR 0.698 0.664 0.553 0.463 0.252
NDCG 0.733 0.705 0.584 0.51 0.279
Precision@1 0.571 0.517 0.41 0.318 0.161
Precision@3 0.793 0.778 0.644 0.594 0.307
Precision@5 0.874 0.866 0.736 0.67 0.391
Table 4: Evaluation
Tables 5 and 6 shows example results that HARPY computed. Table 5 has correct outputs. We see
that HARPY manages to distinguish between the sport, musical, and theatrical senses of the verb ?play?.
As shown in Table 6, HARPY also produces some spurious results, with various factors contributing to
these errors. For example, the phrase ?covered on album? was aligned with the first sense of ?cover?
since there is no musical sense for ?cover? in WordNet. Other errors arise from mistakes in the original
Patty repository of relational phrases. For example, the travel sense of the verb ?head? was aligned with
the phrase ?head of? because ?head of? and ?head to? were in the same Patty synset. Yet another cause
of problems is the extremely fine granularity of WordNet: even for humans it is often hard to distinguish
between love as a state of liking and love as being enamored.
6 Extrinsic Study: Lexical Types for WordNet Verbs
As an extrinsic use-case for the HARPY resource, we studied the task of inferring lexical types for the
subject and object arguments of a WordNet verb sense. For a given verb sense, we propagate the type
signature of the relational phrase with the highest alignment score.
1
http://code.google.com/p/junto/
2201
Relational phrase Verb Sense WordNet definition
[musician] played with [musician] play3 play on an instrument
[actor] played [[det]] role in [event] act3 play a role or part
[person] played hockey for [organization] play1 participate in games or sport
[person] was shooting [person] shoot2 kill by firing a missile
[movie] be shot in [city] film1 make a film or photograph of something
[composition] written by [composer] compose2 write music
[writer] writing at [organization] write1 produce a literary work
Table 5: Correct examples
Relational phrase Verb Sense WordNet definition
[person] covered on album [artifact] cover1 provide with a covering or cause to be covered
[person] head of [artifact] head1 to go or travel towards
[person] becomes convinced that [person] become1 enter or assume a certain state or condition
[person] is loved by [person] love1 have a great affection or liking for
[wrestler] wrestled in [organization] wrestle1 combat to overcome an opposing tendency or force
Table 6: Wrong alignment examples
For comparison, this procedure is performed with the HARPY alignments as well as the alignments by
the baseline methods. We showed a uniformly sampled set of 261 results to human judges, who assessed
as valid or invalid. Additionally, we had a set of the 100 most-confident results (those derived from the
highest alignment scores) assessed in the same manner.
For the uniform samples, the type signature derived from HARPY had a precision of 0.46, whereas
the best of the baselines (PPR and Cosine) achieved 0.39. For the top-100 samples, HARPY achieved a
precision of 0.81. Table 7 shows some example results, demonstrating the added value beyond WordNet.
Domain Range Verb Sense WordNet definition
country country export1 sell or transfer abroad
person country head2 be in charge of
organization organization own1 have ownership or possession of
person person predate1 be earlier in time; go back further
saint organization reverence1 regard with feelings of respect and reverence
person artifact rush5 run with the ball, in football
organization person sustain4 supply with necessities and support
musician musician play3 play on an instrument
football player athlete pass20 throw (a ball) to another player
singer composer inspire2 supply the inspiration for
ruler country suppress1 to put down by force or authority
architect city design2 plan something for a specific role or purpose or effect
priest saint canonize2 treat as a sacred person
country country ally with1 unite formally; of interest groups or countries
company organization deal13 sell
artifact computer game port8 modify (software) for use on a different machine or platform
Table 7: Type inference examples by HARPY
7 Conclusion
HARPY is a new resource that aligns lexically typed multi-word phrases for binary relations with Word-
Net verb senses. By judiciously devising appropriate features and adapting and extending an advanced
random-walk method, SimRank, we achieved high-quality alignments, as shown in our evaluation. This
creates added value for both the resource of relational phrases, Patty, and WordNet. Phrases are now
organized into a clean hypernymy hierarchy, an important aspect on which the Patty work fell short.
WordNet verb senses, on the other hand, are extended by a rich set of paraphrases and also by lexical
type signatures inherited from the phrases. We believe that this new resource is a useful asset for com-
putational linguistics. As a future work, we plan to align additional resources like WiseNet (Moro and
Navigli, 2012), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2008). The HARPY resource is
publicly available at www.mpi-inf.mpg.de/yago-naga/patty/.
2202
References
Alfred V. Aho, M. R. Garey, Jeffrey D. Ullman 1972. The Transitive Reduction of a Directed Graph. SIAM J.
Comput., 131?137.
Enrique Alfonseca, Daniele Pighin, and Guillermo Garrido. 2013. HEADY: News headline abstraction through
event pattern clustering. ACL (1), 1243?1253.
Ioannis Antonellis, Hector Garcia-Molina, and Chi-Chao Chang. 2007. Simrank++: Query rewriting through link
analysis of the click graph. CoRR, abs/0712.0499.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. COLING-ACL,
86?90.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. IJCAI,
805?810.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the WordNet Domains
hierarchy: Semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic
Ressources, MLR ?04, 101?108, Stroudsburg, PA, USA. Association for Computational Linguistics.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1): 13?47.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Toward an architecture for Never-Ending Language Learning. AAAI
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations.
EMNLP, 33?40.
Gerard de Melo and Gerhard Weikum. 2009. Towards a universal wordnet by learning from combined evidence.
CIKM, 513?522.
Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction.
EMNLP, 1535?1545.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question
answering. ACL (1), 1608?1618.
Christiane Fellbaum, George Miller (Editors). 1998. WordNet An Electronic Lexical Database. The MIT Press.
?
Oscar Ferr?andez, Michael Ellsworth, Rafael Mu?noz, and Collin F. Baker. 2010. Aligning FrameNet and WordNet
based on semantic neighborhoods. LREC.
D?aniel Fogaras and Bal?azs R?acz. 2005. Scaling link-based similarity search. WWW, 641?650.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. IJCAI, 1606?1611.
Bernardo Cuenca Grau, Zlatan Dragisic, Kai Eckert, J?er?ome Euzenat, Alfio Ferrara, Roger Granada, Valentina
Ivanova, Ernesto Jim?enez-Ruiz, Andreas Oskar Kempf, Patrick Lambrix, Andriy Nikolov, Heiko Paulheim,
Dominique Ritze, Franc?ois Scharffe, Pavel Shvaiko, C?assia Trojahn dos Santos, and Ondrej Zamazal. 2013.
Results of the ontology alignment evaluation initiative 2013. Ontology Matching, volume 1111 of CEUR Work-
shop Proceedings, 61?100.
Rahul Gupta, Alon Halevy, Xuezhi Wang, Steven Whang, and Fei Wu. 2014. Biperpedia: An ontology for search
applications. Proc. 40th Int?l Conf. on Very Large Data Bases (PVLDB). 505?516 .
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the web. EMNLP, 1172?1181.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. WWW, 517?526.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. Proceedings of SemEval-2, Uppsala, Sweden.
Glen Jeh and Jennifer Widom. 2002. SimRank: a measure of structural-context similarity. KDD, 538?543.
2203
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English
verbs. Language Resources and Evaluation, 42(1):21?40.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Learning arguments and supertypes of semantic relations using
recursive patterns. ACL, 1482?1491.
Dekang Lin and Patrick Pantel. 2001. DIRT @SBT@discovery of inference rules from text. KDD, 323?328.
Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch?utze 2008. Scoring, Term Weighting, and the Vector
Space Model. Introduction to Information Retrieval. Cambridge University Press, Cambridge, England, 2008,
pp. 109?133.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for
information extraction. EMNLP-CoNLL, 523?534.
Diana McCarthy, Rob Koeling, Julie Weeds, and John A. Carroll. 2004. Finding predominant word senses in
untagged text. ACL, 279?286.
Christian M. Meyer and Iryna Gurevych. 2012. To exhibit is not to loiter: A multilingual, sense-disambiguated
Wiktionary for measuring verb similarity. COLING, 1763?1780.
Andrea Moro and Roberto Navigli. 2012. WiseNet: building a Wikipedia-based semantic network with ontolo-
gized relations. CIKM, 1672?1676.
Ndapandula Nakashole, Gerhard Weikum, and Fabian M. Suchanek. 2012. PATTY: A taxonomy of relational
patterns with semantic types. EMNLP-CoNLL, 1135?1145.
Ndapandula Nakashole, Tomasz Tylenda, and Gerhard Weikum. 2013. Fine-grained semantic typing of emerging
entities. ACL (1), 1488?1497.
Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word
sense disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., 32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network. Artif. Intell., 193:217?250.
Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2).
Martha. Palmer. 2009. SemLink: Linking PropBank, VerbNet and FrameNet. In Proceedings of the Generative
Lexicon ConferenceGenLex-09, Pisa, Italy, Sept.
Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, disambiguate and walk: A unified
approach for measuring semantic similarity. ACL (1), 1341?1351.
Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-Rich Word Sense Disambiguation Rivaling Su-
pervised Systems. ACL, 1522-1531.
Robert Speer and Catherine Havasi. 2012. Representing general relational knowledge in ConceptNet 5. LREC,
pages 3679?3686.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: a core of semantic knowledge. WWW,
697?706.
Partha Pratim Talukdar and Koby Crammer. 2009. New regularized algorithms for transductive learning.
ECML/PKDD (2), volume 5782 of Lecture Notes in Computer Science, pages 442?457. Springer.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Qili Zhu. 2012. Probase: a probabilistic taxonomy for text
understanding. SIGMOD Conference, 481?492.
2204
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782?792,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Robust Disambiguation of Named Entities in Text
Johannes Hoffart1, Mohamed Amir Yosef1, Ilaria Bordino2, Hagen Fu?rstenau3,
Manfred Pinkal3, Marc Spaniol1, Bilyana Taneva1, Stefan Thater3, Gerhard Weikum1
1 Max Planck Institute for Informatics, Saarbru?cken, Germany
2 Yahoo! Research Lab, Barcelona, Spain
3 Saarland University, Saarbru?cken, Germany
{jhoffart,mamir,mspaniol,btaneva,weikum}@mpi-inf.mpg.de
bordino@yahoo-inc.com {hagenf,pinkal,stth}@coli.uni-sb.de
Abstract
Disambiguating named entities in natural-
language text maps mentions of ambiguous
names onto canonical entities like people or
places, registered in a knowledge base such as
DBpedia or YAGO. This paper presents a ro-
bust method for collective disambiguation, by
harnessing context from knowledge bases and
using a new form of coherence graph. It unifies
prior approaches into a comprehensive frame-
work that combines three measures: the prior
probability of an entity being mentioned, the
similarity between the contexts of a mention
and a candidate entity, as well as the coherence
among candidate entities for all mentions to-
gether. The method builds a weighted graph of
mentions and candidate entities, and computes
a dense subgraph that approximates the best
joint mention-entity mapping. Experiments
show that the new method significantly outper-
forms prior methods in terms of accuracy, with
robust behavior across a variety of inputs.
1 Introduction
1.1 Motivation
Web pages, news articles, blog postings, and other
Internet data contain mentions of named entities such
as people, places, organizations, etc. Names are often
ambiguous: the same name can have many different
meanings. For example, given a text like ?They per-
formed Kashmir, written by Page and Plant. Page
played unusual chords on his Gibson.?, how can we
tell that ?Kashmir? denotes a song by Led Zeppelin
and not the Himalaya region (and that Page refers
to guitarist Jimmy Page and not to Google founder
Larry Page, and that Gibson is a guitar model rather
than the actor Mel Gibson)?
Establishing these mappings between the mentions
and the actual entities is the problem of named-entity
disambiguation (NED).
If the possible meanings of a name are known up-
front - e.g., by using comprehensive gazetteers such
as GeoNames (www.geonames.org) or knowledge
bases such as DBpedia (Auer07), Freebase (www.
freebase.com), or YAGO (Suchanek07), which
have harvested Wikipedia redirects and disambigua-
tion pages - then the simplest heuristics for name res-
olution is to choose the most prominent entity for a
given name. This could be the entity with the longest
Wikipedia article or the largest number of incoming
links in Wikipedia; or the place with the most inhab-
itants (for cities) or largest area, etc. Alternatively,
one could choose the entity that uses the mention
most frequently as a hyperlink anchor text. For the
example sentence given above, all these techniques
would incorrectly map the mention ?Kashmir? to the
Himalaya region. We refer to this suite of methods
as a popularity-based (mention-entity) prior.
Key to improving the above approaches is to con-
sider the context of the mention to be mapped, and
compare it - by some similarity measure - to contex-
tual information about the potential target entities.
For the example sentence, the mention ?Kashmir?
has context words like ?performed? and ?chords? so
that we can compare a bag-of-words model against
characteristic words in the Wikipedia articles of the
different candidate entities (by measures such as co-
sine similarity, weighted Jaccard distance, KL diver-
gence, etc.). The candidate entity with the highest
similarity is chosen. Alternatively, labeled training
data can be harnessed to learn a multi-way classifier,
and additional features like entire phrases, part-of-
speech tags, dependency-parsing paths, or nearby
782
hyperlinks can be leveraged as well. These methods
work well for sufficiently long and relatively clean
input texts such as predicting the link target of a Wi-
kipedia anchor text (Milne08). However, for short or
more demanding inputs like news, blogs, or arbitrary
Web pages, relying solely on context similarity can-
not achieve near-human quality. Similarity measures
based on syntactically-informed distributional mod-
els require minimal context only. They have been
developed for common nouns and verbs (Thater10),
but not applied to named entities.
The key to further improvements is to jointly con-
sider multiple mentions in an input and aim for a col-
lective assignment onto entities (Kulkarni09). This
approach should consider the coherence of the re-
sulting entities, in the sense of semantic relatedness,
and it should combine such measures with the con-
text similarity scores of each mention-entity pair. In
our example, one should treat ?Page?, ?Plant? and
?Gibson? also as named-entity mentions and aim to
disambiguate them together with ?Kashmir?.
Collective disambiguation works very well when a
text contains mentions of a sufficiently large number
of entities within a thematically homogeneous con-
text. If the text is very short or is about multiple, un-
related or weakly related topics, collective mapping
tends to produce errors by directing some mentions
towards entities that fit into a single coherent topic
but do not capture the given text. For example, a text
about a football game between ?Manchester? and
?Barcelona? that takes place in ?Madrid? may end up
mapping either all three of these mentions onto foot-
ball clubs (i.e., Manchester United, FC Barcelona,
Real Madrid) or all three of them onto cities. The
conclusion here is that none of the prior methods
for named-entity disambiguation is robust enough to
cope with such difficult inputs.
1.2 Contribution
Our approach leverages recently developed knowl-
edge bases like YAGO as an entity catalog and a
rich source of entity types and semantic relationships
among entities. These are factored into new measures
for the similarity and coherence parts of collectively
disambiguating all mentions in an input text. For
similarity, we also explore an approach that lever-
ages co-occurrence information obtained from large,
syntactically parsed corpora (Thater10).
We cast the joint mapping into the following graph
problem: mentions from the input text and candidate
entities define the node set, and we consider weighted
edges between mentions and entities, capturing con-
text similarities, and weighted edges among entities,
capturing coherence. The goal on this combined
graph is to identify a dense subgraph that contains
exactly one mention-entity edge for each mention,
yielding the most likely disambiguation. Such graph
problems are NP-hard, as they generalize the well-
studied Steiner-tree problem. We develop a greedy
algorithm that provides high-quality approximations,
and is customized to the properties of our mention-
entity graph model.
In addition to improving the above assets for the
overall disambiguation task, our approach gains in
robustness by using components selectively in a self-
adapting manner. To this end, we have devised the
following multi-stage procedure.
? For each mention, we compute popularity priors
and context similarities for all entity candidates
as input for our tests.
? We use a threshold test on the prior to decide
whether popularity should be used (for mentions
with a very high prior) or disregarded (for men-
tions with several reasonable candidates).
? When both the entity priors and the context simi-
larities are reasonably similar in distribution for
all the entity candidates, we keep the best candi-
date and remove all others, fixing this mention
before running the coherence graph algorithm.
We then run the coherence graph algorithm on all
the mentions and their remaining entity candidates.
This way, we restrict the coherence graph algorithm
to the critical mentions, in situations where the goal
of coherence may be misleading or would entail high
risk of degradation.
The paper makes the following novel contribu-
tions: 1) a framework for combining popularity pri-
ors, similarity measures, and coherence into a robust
disambiguation method; 2) new measures for defin-
ing mention-entity similarity; 3) a new algorithm
for computing dense subgraphs in a mention-entity
graph, which produces high-quality mention-entity
mappings; 4) an empirical evaluation on a demand-
ing corpus (based on additional annotations for the
dataset of the CoNLL 2003 NER task), with signifi-
783
cant improvements over state-of-the-art opponents.
2 State of the Art
Recognizing named entities (NER tagging) in natural-
language text has been extensively addressed in NLP
research. The output is labeled noun phrases. How-
ever, these are not yet canonical entities, explicitly
and uniquely denoted in a knowledge repository.
Approaches that use Wikipedia for explicit disam-
biguation date back to (Bunescu06) and have been
further pursued by (Cucerzan07; Han09; Milne08;
Nguyen08; Mihalcea07). (Bunescu06) defined a sim-
ilarity measure that compared the context of a men-
tion to the Wikipedia categories of an entity candi-
date. (Cucerzan07; Milne08; Nguyen08) extended
this framework by using richer features for the simi-
larity comparison. (Milne08) additionally introduced
a supervised classifier for mapping mentions to en-
tities, with learned feature weights rather than using
the similarity function directly. (Milne08) introduced
a notion of semantic relatedness between a mention?s
candidate entities and the unambiguous mentions in
the textual context. The relatedness values are de-
rived from the overlap of incoming links in Wikipedia
articles. (Han09) considered another feature: the re-
latedness of common noun phrases in a mention?s
context, matched against Wikipedia article names.
While these features point towards semantic coher-
ence, the approaches are still limited to mapping each
mention separately. Nonetheless, this line of feature-
rich similarity-driven methods achieved very good
results in experiments, especially for the task of pre-
dicting Wikipedia link targets for a given href anchor
text. On broader input classes such as news articles
(called ?wikification in the wild? in (Milne08)), the
precision was reported to be about 75 percent.
The first work with an explicit collective-learning
model for joint mapping of all mentions has been
(Kulkarni09). This method starts with a supervised
learner for a similarity prior, and models the pair-
wise coherence of entity candidates for two different
mentions as a probabilistic factor graph with all pairs
as factors. The MAP (maximum a posteriori) es-
timator for the joint probability distribution of all
mappings is shown to be an NP-hard optimization
problem, so that (Kulkarni09) resorts to approxima-
tions and heuristics like relaxing an integer linear
program (ILP) into an LP with subsequent round-
ing or hill-climbing techniques. The experiments in
(Kulkarni09) show that this method is superior to the
best prior approaches, most notably (Milne08). How-
ever, even approximate solving of the optimization
model has high computational costs.
Coreference resolution is the task of mapping
mentions like pronouns or short phrases to a pre-
ceding, more explicit, mention. Recently, interest
has arisen in cross-document coreference resolution
(Mayfield09), which comes closer to NED, but does
not aim at mapping names onto entities in a knowl-
edge base. Word sense disambiguation (McCarthy09;
Navigli09) is the more general task of mapping con-
tent words to a predefined inventory of word senses.
While the NED problem is similar, it faces the chal-
lenges that the ambiguity of entity names tends to be
much higher (e.g., mentions of common lastnames
or firstname-only).
Projects on automatically building knowledge
bases (Doan08) from natural-language text include
KnowItAll (Banko07), YAGO and its tool SOFIE
(Suchanek09; Nakashole11), StatSnowball (Zhu09),
ReadTheWeb (Carlson10), and the factor-graph work
by (Wick09). Only SOFIE maps names onto canon-
ical entities; the other projects produce output with
ambiguous names. SOFIE folds the NED into its
MaxSat-based reasoning for fact extraction. This ap-
proach is computationally expensive and not intended
for online disambiguation of entire texts.
3 Framework
Mentions and Ambiguity: We consider an input
text (Web page, news article, blog posting, etc.) with
mentions (i.e., surface forms) of named entities (peo-
ple, music bands, songs, universities, etc.) and aim
to map them to their proper entries in a knowledge
base, thus giving a disambiguated meaning to entity
mentions in the text. We first identify noun phrases
that potentially denote named entities. We use the
Stanford NER Tagger (Finkel05) to discover these
and segment the text accordingly.
Entity Candidates: For possible entities (with
unique canonical names) that a mention could denote,
we harness existing knowledge bases like DBpedia
or YAGO. For each entity they provide a set of short
names (e.g., ?Apple? for Apple Inc. and para-
784
phrases (e.g., ?Big Apple? for New York City).
In YAGO, these are available by the means relation,
which in turn is harvested from Wikipedia disam-
biguation pages, redirects, and links.
Popularity Prior for Entities: Prominence or
popularity of entities can be seen as a probabilistic
prior for mapping a name to an entity. The most com-
mon way of estimating this are the Wikipedia-based
frequencies of particular names in link anchor texts
referring to specific entities, or number of inlinks.
Context Similarity of Mentions and Entities:
The key for mapping mentions onto entities are the
contexts on both sides of the mapping. We consider
two different approaches. First, for each mention,
we construct a context from all words in the entire
input text. This way, we can represent a mention
as a set of (weighted) words or phrases that it co-
occurs with. Second, we alternatively consider simi-
larity scores based on syntactically-parsed contexts,
based on (Thater10). On the entity side of the map-
ping, we associate each entity with characteristic
keyphrases or salient words, precomputed from Wi-
kipedia articles and similar sources. For example,
Larry Page would have keyphrases like ?Stan-
ford?, ?search engine?, etc., whereas Jimmy Page
may have keyphrases ?Gibson guitar?, ?hard rock?,
etc. Now we can define and compute similarity mea-
sures between a mention and an entity candidate,
e.g., the weighted word overlap, the KL divergence,
n-gram-based measures, etc. In addition, we may
use syntactic contextualization techniques, based on
dependency trees, that suggest phrases that are typi-
cally used with the same verb that appears with the
mention in the input text (Thater10).
Coherence among Entities: On the entity side,
each entity has a context in the underlying knowl-
edge base(s): other entities that are connected via
semantic relationships (e.g., memberOf) or have the
same semantic type (e.g., rock musician). An
asset that knowledge bases like DBpedia and YAGO
provide us with is the same-as cross-referencing to
Wikipedia. This way, we can quantify the coherence
between two entities by the number of incoming links
that their Wikipedia articles share. When we consider
candidate entities for different mentions, we can now
define and compute a notion of coherence among the
corresponding entities, e.g., by the overlap among
their related entities or some form of type distance.
Coherence is a key asset because most texts deal with
a single or a few semantically related topics such as
rock music or Internet technology or global warming,
but not everything together.
Overall Objective Function: To aim for the best
disambiguation mappings, our framework combines
prior, similarity, and coherence measures into a
combined objective function: for each mention mi,
i = 1..k, select entity candidates eji , one per men-
tion, such that
? ?
?
i=1..k
prior(mi, eji)+
? ?
?
i=1..k
sim(cxt(mi), cxt(eji))+
? ? coh(ej1 ? cnd(m1) . . . ejk ? cnd(mk)) = max!
where ? + ? + ? = 1, cnd(mi) is the set of pos-
sible meanings of mi, cxt( ) denotes the context of
mentions and entities, respectively, and coh( ) is the
coherence function for a set of entities.
Section 4 gives details on each of these three com-
ponents. For robustness, our solution selectively en-
ables or disables the three components, based on tests
on the mentions of the input text; see Section 5.
4 Features and Measures
4.1 Popularity Prior
As mentioned above, our framework supports multi-
ple forms of popularity-based priors, but we found a
model based on Wikipedia link anchors to be most
effective: For each surface form that constitutes an
anchor text, we count how often it refers to a partic-
ular entity. For each name, these counts provide us
with an estimate for a probability distribution over
candidate entities. For example, ?Kashmir? refers to
Kashmir (the region) in 90.91% of all occurrences
and in 5.45% to Kashmir (Song).
4.2 Mention-Entity Similarity
Keyphrase-based Similarity: On the mention side,
we use all tokens in the document (except stopwords
and the mention itself) as context. We experimented
with a distance discount to discount the weight of
tokens that are further away, but this did not improve
the results for our test data.
On the entity side, the knowledge base knows au-
thoritative sources for each entity, for example, the
785
corresponding Wikipedia article or an organizational
or individual homepage. These are the inputs for
an offline data-mining step to determine character-
istic keyphrases for each entity and their statistical
weights. We describe this only for Wikipedia as in-
put corpus, the approach extends to other inputs. As
keyphrase candidates for an entity we consider its
corresponding Wikipedia article?s link anchors texts,
including category names, citation titles, and external
references. We extended this further by considering
also the titles of articles linking to the entity?s article.
All these phrases form the keyphrase set of an entity:
KP (e).
For each word w that occurs in a keyphrase, we
compute a specificity weight with regard to the given
entity: the MI (mutual information) between the en-
tity e and the keyword w, calculating the joint proba-
bilities for MI as follows:
p(e, w) =
??w ?
(
KP (e) ??e??INe KP (e?)
)??
N
reflecting if w is contained in the keyphrase set of e
or any of the keyphrase sets of an entity linking to e,
IN(e), with N denoting the total number of entities.
The joint probabilities for the cases p(e, w?), p(e?, w),
p(e?, w?) are calculated accordingly.
Keyphrases may occur only partially in an input
text. For example, the phrase ?Grammy Award win-
ner? associated with entity Jimmy Page may oc-
cur only in the form ?Grammy winner? near some
mention ?Page?. Therefore, our algorithm for the
similarity of mention m with regard to entity e com-
putes partial matches of e?s keyphrases in the text.
This is done by matching individual words and re-
warding their proximity in an appropriate score. To
this end we compute, for each keyphrase, the shortest
window of words that contains a maximal number
of words of the keyphrase. We refer to this window
as the phrase?s cover (cf. (Taneva11)). For example,
matching the text ?winner of many prizes including
the Grammy? results in a cover length of 7 for the
keyphrase ?Grammy award winner?. By this ratio-
nale, the score of partially matching phrase q in a text
is set to:
score(q) = z
(?
w?cover weight(w)?
w?q weight(w)
)2
where z = # matching wordslength of cover(q) andweight(w) is eitherthe MI weight (defined above) or the collection-wide
IDF weight of the keyphrase word w. Note that the
second factor is squared, so that there is a superlinear
reduction of the score for each word that is missing
in the cover.
For the similarity of a mention m to candidate
entity e, this score is aggregated over all keyphrases
of e and all their partial matches in the text, leading
to the similarity score
simscore(m, e) =
?
q?KP (e)
score(q)
Syntax-based Similarity: In addition to surface
features of words and phrases, we leverage informa-
tion about the immediate syntactic context in which
an entity mention occurs. For example, in the sen-
tence ?Page played unusual chords?, we can extract
the fact that the mention ?Page? is the subject of the
verb ?play?. Using a large text corpus for training,
we collect statistics about what kinds of entities tend
to occur as subjects of ?play?, and then rank the can-
didate entities according to their compatibility with
the verb.
Specifically, we employ the framework of
(Thater10), which allows us to derive vector represen-
tations of words in syntactic contexts (such as being
the subject of a particular verb). We do not directly
apply this model to derive contextualized representa-
tions of entity mentions, as information about specific
proper names is very sparse in corpora like GigaWord
or Wikipedia. Instead, we consider a set of substi-
tutes for each possible entity e, which we take as its
context cxt(e). For this, we use the WordNet synsets
associated with the entity?s YAGO types and all their
hypernyms. For each substitute, we compute a stan-
dard distributional vector and a contextualized vector
according to (Thater10). Syntax-based similarity be-
tween cxt(e) and the context cxt(m) of the mention
is then defined as the sum of the scalar-product simi-
larity between these two vectors for each substitute.
This results in high similarity if the syntactic contex-
tualization only leads to small changes of the vectors,
reflecting the compatibility of the entity?s substitutes.
In our example, we compute a vector for ?gui-
tarist? as subject of ?play?, and another one for ?en-
trepreneur? in the same context. The former is more
786
compatible with the given context than the latter, lead-
ing to higher similarity for the entity Jimmy Page.
4.3 Entity-Entity Coherence
As all entities of interest are registered in a knowl-
edge base (like YAGO), we can utilize the semantic
type system, which is usually a DAG of classes. The
simples measure is the distance between two entities
in terms of type and subclassOf edges.
The knowledge bases also provide same-as cross-
referencing to Wikipedia, amd we quantify the coher-
ence between two entities by the number of incom-
ing links that their Wikipedia articles share. This
approach has been refined by Milne and Witten
(Milne08), taking into account the total number N of
entities in the (Wikipedia) collection:
mw coh(e1, e2) =
1? log (max(|INe1 |, |INe2 |))? log(|INe1 ? INe2 |)log(|N |)? log (min(|INe1 |, |INe2 |))
if > 0 and else set to 0.
5 Graph Model and Algorithms
5.1 Mention-Entity Graph
From the popularity, similarity, and coherence mea-
sures discussed in Section 4, we construct a weighted,
undirected graph with mentions and candidate enti-
ties as nodes. As shown in the example of Figure 1,
the graph has two kinds of edges:
? A mention-entity edge is weighted with a similar-
ity measure or a combination of popularity and
similarity measure. Our experiments will use a
linear combination with coefficients learned from
withheld training data.
? An entity-entity edge is weighted based on
Wikipedia-link overlap, or type distance, or some
combination along these lines.
Our experiments will focus on anchor-based pop-
ularity, keyphrase-based and/or syntactic similarity,
and link-based coherence (mw coh). The mention-
entity graph is dense on the entities side and often has
hundreds or thousands of nodes, as the YAGO knowl-
edge base offers many candidate entities for common
mentions (e.g., country names that could also denote
sports teams, common lastnames, firstnames, etc.).
5.2 Graph Algorithm
Given a mention-entity graph, our goal is to com-
pute a dense subgraph that would ideally contain all
mention nodes and exactly one mention-entity edge
for each mention, thus disambiguating all mentions.
We face two main challenges here. The first is how
to specify a notion of density that is best suited for
capturing the coherence of the resulting entity nodes.
The seemingly most natural approach would be to
measure the density of a subgraph in terms of its total
edge weight. Unfortunately, this will not work ro-
bustly for the disambiguation problem. The solution
could be dominated by a few entity nodes with very
high weights of incident edges, so the approach could
work for prominent targets, but it would not achieve
high accuracy also for the long tail of less prominent
and more sparsely connected entities. We need to
capture the weak links in the collective entity set of
the desired subgraph. For this purpose, we define
the weighted degree of a node in the graph to be the
total weight of its incident edges. We then define the
density of a subgraph to be equal to the minimum
weighted degree among its nodes. Our goal is to
compute a subgraph with maximum density, while
observing constraints on the subgraph structure.
The second critical challenge that we need to face
is the computational complexity. Dense-subgraph
problems are almost inevitably NP-hard as they gen-
eralize the Steiner-tree problem. Hence, exact algo-
rithms on large input graphs are infeasible.
To address this problem, we adopt and extend an
approximation algorithm of (Sozio10) for the prob-
lem of finding strongly interconnected, size-limited
groups in social networks. The algorithm starts from
the full mention-entity graph and iteratively removes
the entity node with the smallest weighted degree.
Among the subgraphs obtained in the various steps,
the one maximizing the minimum weighted degree
will be returned as output. To guarantee that we
arrive at a coherent mention-entity mapping for all
mentions, we enforce each mention node to remain
connected to at least one entity. However, this con-
straint may lead to very suboptimal results.
For this reason, we apply a pre-processing phase to
prune the entities that are only remotely related to the
mention nodes. For each entity node, we compute the
distance from the set of all mention nodes in terms
787
They performed 
Kashmir,  
written by  
Page    
and Plant.   
Page played  
unusual chords  
on his Gibson. 
?? Led Zeppelin 
?? Hard rock 
?? Electric guitar 
?? Session guitarist 
?? Led Zeppelin 
?? Gibson 
?? Jimmy Page 
signature model 
?? Hard rock 
Kashmir (song) 
Kashmir (region) 
Larry Page 
Jimmy Page 
Page, Arizona 
Robert Plant 
Gibson Les Paul 
Gibson, Missouri 
Figure 1: Mention-Entity Graph Example
of the sum of the corresponding squared shortest-
path distances. We then restrict the input graph to
the entity nodes that are closest to the mentions. An
experimentally determined good choice for the size
of this set is five times the number of the mention
nodes. Then the iterative greedy method is run on
this smaller subgraph. Algorithm 1 summarizes this
procedure, where an entity is taboo if it is the
last candidate for a mention it is connected to.
Algorithm 1: Graph Disambiguation Algorithm
Input: weighted graph of mentions and entities
Output: result graph with one edge per mention
begin
pre?processing phase;
foreach entity do
calculate distance to all mentions;
keep the closest (5? mentions count)
entities, drop the others;
main loop;
while graph has non-taboo entity do
determine non-taboo entity node
with lowest weighted degree, remove it
and all its incident edges;
if minimum weighted degree increased
then
set solution to current graph;
post?processing phase;
process solution by local search or full
enumeration for best configuration;
The output of the main loop would often be close
to the desired result, but may still have more than one
mention-entity edge for one or more mentions. At
this point, however, the subgraph is small enough to
consider an exhaustive enumeration and assessment
of all possible solutions. This is one of the options
that we have implemented as post-processing step.
Alternatively, we can perform a faster local-search
algorithm. Candidate entities are randomly selected
with probabilities proportional to their weighted de-
grees. This step is repeated for a prespecified number
of iterations, and the best configuration with the high-
est total edge-weight is used as final solution.
5.3 Robustness Tests
The graph algorithm generally performs well. How-
ever, it may be misled in specific situations, namely,
if the input text is very short, or if it is thematically
heterogeneous. To overcome these problems, we in-
troduce two robustness tests for individual mentions
and, depending on the tests? outcomes, use only a
subset of our framework?s features and techniques.
Prior test: Our first test ensures that the popularity
prior does not unduly dominate the outcome if the
true entities are dominated by false alternatives. We
check, for each mention, whether the popularity prior
for the most likely candidate entity is above some
threshold ?, e. g. above 90% probability. If this is not
the case, then the prior is completely disregarded for
computing the mention-entity edge weights. Other-
wise, the prior is combined with the context-based
similarity computation to determine edge weights.
788
We never rely solely on the prior.
Coherence test: As a test for whether the coher-
ence part of our framework makes sense or not,
we compare the popularity prior and the similarity-
only measure, on a per-mention basis. For each
mention, we compute the L1 distance between the
popularity-based vector of candidate probabilities
and the similarity-only-based vector of candidate
probabilities:
?
i=1..k
|prior(m, ei)? simscore(m, ei)|
This difference is always between 0 and 2. If it ex-
ceeds a specified threshold ? (e.g., 1), the disagree-
ment between popularity and similarity-only indi-
cates that there is a situation that coherence may be
able to fix. If, on the other hand, there is hardly any
disagreement, using coherence as an additional as-
pect would be risky for thematically heterogeneous
texts and should better be disabled. In that case, we
choose an entity for the mention at hand, using the
combination of prior and similarity. Only the win-
ning entity is included in the mention-entity graph, all
other candidates are omitted for the graph algorithm.
The robustness tests and the resulting adaptation of
our method are fully automated.
6 Experiments
6.1 Setup
System: All described methods are implemented in
a prototype system called AIDA (Accurate Online
Disambiguation of Named Entities). We use the Stan-
ford NER tagger (Finkel05) to identify mentions in
input texts, the YAGO2 knowledge base (Hoffart11)
as a repository of entities, and the English Wikipe-
dia edition (as of 2010-08-17) as a source of mining
keyphrases and various forms of weights. The graph
algorithm makes use of Webgraph (Boldi04).
Datasets: There is no established benchmark for
NED. The best prior work (Kulkarni09)) compiled
its own hand-annotated dataset, sampled from online
news. Unfortunately, this data set is fairly small (102
short news articles, about 3,500 proper noun men-
tions). Moreover, its entity annotations refer to an old
version of Wikipedia. To avoid unfair comparisons,
we created our own dataset based on CoNLL 2003
articles 1,393
mentions (total) 34,956
mentions with no entity 7,136
words per article (avg.) 216
mentions per article (avg.) 25
distinct mentions per article (avg.) 17
mentions with candidate in KB (avg.) 21
entities per mention (avg) 73
initial annotator disagreement (%) 21.1
Table 1: CoNLL Dataset Properties
data, extensively used in prior work on NER tagging
(Sang03).
This consists of proper noun annotations for 1393
Reuters newswire articles. We hand-annotated all
these proper nouns with corresponding entities in
YAGO2. Each mention was disambiguated by two
students and resolved by us in case of conflict. This
data set is referred to as CoNLL in the following
and fully available at http://www.mpi-inf.mpg.
de/yago-naga/aida/. Table 1 summarizes prop-
erties of the dataset.
Methods under comparison: Our framework in-
cludes many variants of prior methods from the lit-
erature. We report experimental results for some of
them. AIDA?s parameters were tuned by line-search
on 216 withheld development documents. We found
the following to work best:
? threshold for prior test: ? = 0.9
? weights for popularity, similarity, coherence:
? = 0.43, ? = 0.47, ? = 0.10
? initial number of entites in graph: 5 ? #mentions
? threshold for coherence test: ? = 0.9
We checked the sensitivity of the hyper-parameter
settings and found the influence of variations to be
small, e. g. when varying ? within the range [0.5,1.3],
the changes in precision@1.0 are within 1%.
The baseline for our experiments is the collective-
inference method of (Kulkarni09), which outper-
forms simpler methods (such as (Milne08)). We
refer to this method as Kul CI. Since program code
for this method is not available, we re-implemented
it using the LP solver CPLEX for the optimization
problem with subsequent rounding, as described in
(Kulkarni09). In addition, we compare against (our
re-implementation of) the method of (Cucerzan07),
789
Our Methods Competitors
sim-k prior
sim-k
prior
sim-s
sim-k
sim-s
r-prior
sim-k
r-prior
sim-k
coh
r-prior
sim-k
r-coh
prior Cuc Kul s Kul sp Kul CI
Macro P@1.0 76.53 75.75 71.43 76.40 80.71 80.73 81.91 71.24 43.74 58.06 76.74 76.74
Micro P@1.0 76.09 70.72 66.09 76.13 79.57 81.77 81.82 65.84 51.03 63.42 72.31 72.87
MAP 66.98 83.99 85.97 67.00 85.91 89.05 87.31 86.63 40.06 63.90 86.50 85.44
Table 2: Experimental results on CoNLL (all values in %)
referred to as Cuc. For all methods, weights for
combining components were obtained by training
a SVM classifier on 946 withheld CoNLL training
documents.
Performance measures: The key measures in our
evaluation are precision and recall. We consider
the precision-recall curve, as there is an inherent
trade-off between the two measures. Precision is the
fraction of mention-entity assignments that match
the ground-truth assignment. Recall is the fraction
of the ground-truth assignments that our method(s)
could compute. Both measures can aggregate over of
all mentions (across all texts) or over all input texts
(each with several mentions). The former is called
micro-averaging, the latter macro-averaging.
As we use a knowledge base with millions of enti-
ties, we decided to neglect the situation that a mention
may refer to an unknown entity not registered in the
knowledge base. We consider only mention-entity
pairs where the ground-truth gives a known entity,
and thus ignore roughly 20% of the mentions without
known entity in the ground-truth. This simplifies the
calculation of aggregated precision-recall measures
like (interpolated) MAP (mean average precision):
MAP = 1m
?
i=1..m
precision@ im
where precision@ im is the precision at a specificrecall level. This measure is equivalent to the area
under the precision-recall curve.
For constructing the precision-recall curve, we sort
the mention-entity pairs in descending order of con-
fidence, so that x% recall refers to the x% with the
highest confidence. We use each method?s mention-
entity similarity for the confidence values.
6.2 Results
The results of AIDA vs. the collective-inference
method of (Kulkarni09) and the entity disambigua-
tion method of (Cucerzan07) on 229 test documents
are shown in Table 21. The table includes variants
of our framework, with different choices for the sim-
ilarity and coherence computations. The shorthand
notation for the combinations in the table is as fol-
lows: prior: popularity prior; r-prior: popularity
prior with robustness test; sim-k: keyphrase based
similarity measure; sim-s: syntax-based similarity;
coh: graph coherence; r-coh: graph coherence with
robustness test.
The shorthand names for competitors are: Cuc:
(Cucerzan07) similarity measure; Kul s: (Kulka-
rni09) similarity measure only; Kul sp: Kul s com-
bined with plus popularity prior; Kul CI: Kul sp com-
bined with coherence. All coherence methods use
the Milne-Witten inlink overlap measure mw coh.
The most important measure is macro/micro preci-
son@1.0, which corresponds to the overall correct-
ness of the methods for all mentions that are assigned
to an entity in the ground-truth data. Our sim-k pre-
cision is already very good. Combining it with the
syntax-based similarity improves micro-averaged pre-
cision@1.0, but the macro-averaged results are a bit
worse. Thus, the more advanced configurations of
AIDA did not use syntax-based similarity. Uncondi-
tionally combining prior and sim-k degrades the qual-
ity, but including the prior robustness test (r-prior
sim-k) improves the results significantly. The preci-
sion for our best method, the prior- and coherence-
tested Keyphrase-based mention-entity similarity (r-
prior sim-k r-coh), significantly outperforms all com-
petitors (with a p-value of a paired t-test< 0.01). Our
macro-averaged precision@1.0 is 81.91%, whereas
Kul CI only achieves 76.74%. Even r-prior sim-
k, without any coherence, significantly outperforms
12 of the 231documents in the original test set could not be
processed by Kul CI due to memory limitations. All results are
given for the subset, for the sake of comparability. Results for
the complete set are available on our website.
790
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.7
0.8
0.9
r-prior sim-k r-coh
r-prior sim-k
Kul CI Kul sp
prior
recall
pre
cis
ion
Figure 2: Experimental results on CoNLL: precision-recall curves
Kul CI (with coherence) with a p-value of < 0.01.
In micro-average precision@1.0, the differences are
even higher, showing that we perform better through-
out all documents.
The macro-averaged precision-recall curves in Fig-
ure 2 show that the best AIDA method performs
particularly well in the tail of high recall values. The
MAP underlines the robustness of our best methods.
The high MAP for the prior method is because
we rank by mention-entity edge weight; for prior
this is simply the prior probability. As the prior is
most probably correct for mentions with a very high
prior for their most popular entity (by definition), the
initial ranking of the prior is very good, but drops
more sharply. We believe that the main difficulty in
named entity disambiguation lies exactly in the ?long
tail? of not-so-prominent entities.
We also tried the (Milne08) web service on a sub-
set of our test collection, but this was obviously
geared for Wikipedia linkage and performed poorly.
6.3 Discussion
Our keyphrase-based similarity measure performs
better than the Kul s measure, which is a combina-
tion of 4 different entity contexts (abstract tokens,
full text tokens, inlink anchor tokens, inlink anchor
tokens + surrounding tokens), 3 similarity measures
(Jaccard, dot product, and tf.idf cosine similarity),
and the popularity prior. Adding the prior to our
similarity measure by linear combination degrades
the performance. We found that our measure already
captures a notion of popularity because popular enti-
ties have more keyphrases and can thus accumulate
a higher total score. The popularity should only be
used when one entitiy has a very high probability, and
introducing the robustness test for the prior achieved
this, improving on both our similarity and Kul sp.
Unconditionally adding the notion of coherence
among entities improves the micro-average precision,
but not the macro-average. Investigating potential
problems, we found that the coherence can be led
astray when parts of the document form a coherent
cluster of entities, and other entities are then forced
to be coherent to this cluster. To overcome this is-
sue, we introduced the coherence robustness test,
and the results with r-coh show that it makes sense
to fix an entity for a mention when the prior and
similarity are in reasonable agreement. Adding this
coherence test leads to a signigicant (p-value < 0.05)
improvement over the non-coherence based measures
in both micro- and macro-average precision. Our ex-
periments showed that when adding this coherence
test, around 23 of the mentions are solved using localsimilarity only and are assigned an entity before run-
ning the graph algorithm. In summary, we observed
that the AIDA configuration with r-prior, keyphrase-
based sim-k, and r-coh significantly outperformed all
competitors.
7 Conclusions and Future Work
The AIDA system provides an integrated NED
method using popularity, similarity, and graph-based
coherence, and includes robustness tests for self-
adaptive behavior. AIDA performed significantly bet-
ter than state-of-the-art baselines. The system is fully
implemented and accessible online (http://www.
mpi-inf.mpg.de/yago-naga/aida/). Our fu-
ture work will consider additional semantic proper-
ties between entities (types, memberOf/partOf, etc.)
for further enhancing the coherence algorithm.
Acknowledgements
This work has been partially supported by the German Sci-
ence Foundation (DFG) through the Cluster of Excellence
on ?Multimodal Computing and Interaction? and the Eu-
ropean Union through the 7th Framework IST Integrated
Project ?LivingKnowledge? (no. 231126). We also thank
Mauro Sozio for the discussion on the graph algorithm.
791
References
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DB-
pedia: A Nucleus for a Web of Open Data. ISWC 2007
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Paolo Boldi and Sebastiano Vigna. The WebGraph frame-
work I: Compression techniques. WWW 2004, soft-
ware at http://webgraph.dsi.unimi.it/
Razvan C. Bunescu, Marius Pasca: Using Encyclopedic
Knowledge for Named entity Disambiguation. EACL
2006
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010
Silviu Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. EMNLP-CoNLL 2007
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, Shiv-
akumar Vaithyanathan. (Eds.). Special issue on infor-
mation extraction. SIGMOD Record, 37(4), 2008.
Jenny Rose Finkel, Trond Grenager, Christopher Man-
ning: Incorporating Non-local Information into Infor-
mation Extraction Systems by Gibbs Sampling. ACL
2005, software at http://nlp.stanford.edu/
software/CRF-NER.shtml
Xianpei Han, Jun Zhao: Named entity disambiguation
by leveraging wikipedia semantic knowledge. CIKM
2009.
Johannes Hoffart, Fabian Suchanek, Klaus Berberich, Ed-
win Lewis-Kelham, Gerard de Melo, Gerhard Weikum:
YAGO2: Exploring and Querying World Knowledge in
Time, Space, Context, and Many Languages. Demo Pa-
per, WWW 2011, data at http://www.mpi-inf.
mpg.de/yago-naga/yago/
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
Soumen Chakrabarti: Collective annotation of Wikipe-
dia entities in web text. KDD 2009
James Mayfield et al: Corss-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
AAAI Spring Symposium on Learning by Reading and
Learning to Read, 2009.
Diane McCarthy. Word Sense Disambiguation: An
Overview. Language and Linguistics Compass 3(2):
537-558, Wiley, 2009
Rada Mihalcea, Andras Csomai: Wikify!: Linking Docu-
ments to Encyclopedic Knowledge. CIKM 2007
David N. Milne, Ian H. Witten: Learning to Link with
Wikipedia. CIKM 2008
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Roberto Navigli: Word sense disambiguation: A survey.
ACM Comput. Surv., 41(2), 2009
Hien T. Nguyen, Tru H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. RIVF
2008
Erik F. Tjong Kim Sang, Fien De Meulder: Introduction to
the CoNLL-2003 Shared Task: Language-Independent
Named Entity Recognition. CoNLL 2003
Mauro Sozio, Aristides Gionis: The Community-search
Problem and How to Plan a Successful Cocktail Party.
KDD 2010
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian Suchanek, Mauro Sozio, Gerhard Weikum: SOFIE:
a Self-Organizing Framework for Information Extrac-
tion. WWW 2009
Bilyana Taneva, Mouna Kacimi, and Gerhard Weikum:
Finding Images of Rare and Ambiguous Entities. Tech-
nical Report MPI-I-2011-5-002, Max Planck Institute
for Informatics, 2011.
Stefan Thater, Hagen Fu?rstenau, Manfred Pinkal. Contex-
tualizing Semantic Representations using Syntactically
Enriched Vector Models. ACL 2010
Michael L. Wick, Aron Culotta, Khashayar Rohani-
manesh, Andrew McCallum: An Entity Based Model
for Coreference Resolution. SDM 2009: 365-376
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a Statistical Approach to Extract-
ing Entity Relationships. WWW 2009
792
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 149?159, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Weakly Supervised Model for Sentence-Level Semantic Orientation
Analysis with Multiple Experts
Lizhen Qu and Rainer Gemulla and Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
{lqu,rgemulla,weikum}@mpi-inf.mpg.de
Abstract
We propose the weakly supervised Multi-
Experts Model (MEM) for analyzing the se-
mantic orientation of opinions expressed in
natural language reviews. In contrast to most
prior work, MEM predicts both opinion po-
larity and opinion strength at the level of in-
dividual sentences; such fine-grained analysis
helps to understand better why users like or
dislike the entity under review. A key chal-
lenge in this setting is that it is hard to ob-
tain sentence-level training data for both po-
larity and strength. For this reason, MEM is
weakly supervised: It starts with potentially
noisy indicators obtained from coarse-grained
training data (i.e., document-level ratings), a
small set of diverse base predictors, and, if
available, small amounts of fine-grained train-
ing data. We integrate these noisy indicators
into a unified probabilistic framework using
ideas from ensemble learning and graph-based
semi-supervised learning. Our experiments in-
dicate that MEM outperforms state-of-the-art
methods by a significant margin.
1 Introduction
Opinion mining is concerned with analyzing opin-
ions expressed in natural language text. For example,
many internet websites allow their users to provide
both natural language reviews and numerical ratings
to items of interest (such as products or movies).
In this context, opinion mining aims to uncover the
relationship between users and (features of) items.
Preferences of users to items can be well understood
by coarse-grained methods of opinion mining, which
focus on analyzing the semantic orientation of doc-
uments as a whole. To understand why users like or
dislike certain items, however, we need to perform
more fine-grained analysis of the review text itself.
In this paper, we focus on sentence-level analy-
sis of semantic orientation (SO) in online reviews.
The SO consists of polarity (positive, negative, or
other1) and strength (degree to which a sentence is
positive or negative). Both quantities can be ana-
lyzed jointly by mapping them to numerical ratings:
Large negative/positive ratings indicate a strong neg-
ative/positive orientation. A key challenge in fine-
grained rating prediction is that fine-grained train-
ing data for both polarity and strength is hard to
obtain. We thus focus on a weakly supervised set-
ting in which only coarse-level training data (such
as document ratings and subjectivity lexicons) and,
optionally, a small amount of fine-grained training
data (such as sentence polarities) is available.
A number of lexicon-based approaches for phrase-
level rating prediction has been proposed in the liter-
ature (Taboada et al2011; Qu et al2010). These
methods utilize a subjectivity lexicon of words along
with information about their semantic orientation;
they focus on phrases that contain words from the
lexicon. A key advantage of sentence-level methods
is that they are able to cover all sentences in a review
and that phrase identification is avoided. To the best
of our knowledge, the problem of rating prediction
at the sentence level has not been addressed in the
literature. A naive approach would be to simply aver-
age phrase-level ratings. Such an approach performs
1We assign polarity other to text fragments that are off-topic
or not directly related to the entity under review.
149
poorly, however, since (1) phrases are analyzed out
of context (e.g., modal verbs or conditional clauses),
(2) domain-dependent information about semantic
orientation is not captured in the lexicons, (3) only
phrases that contain lexicon words are covered. Here
(1) and (2) lead to low precision, (3) to low recall.
To address the challenges outlined above, we pro-
pose the weakly supervised Multi-Experts Model
(MEM) for sentence-level rating prediction. MEM
starts with a set of potentially noisy indicators of SO
including phrase-level predictions, language heuris-
tics, and co-occurrence counts. We refer to these
indicators as base predictors; they constitute the set
of experts used in our model. MEM is designed
such that new base predictors can be easily integrated.
Since the information provided by the base predictors
can be contradicting, we use ideas from ensemble
learning (Dietterichl, 2002) to learn the most con-
fident indicators and to exploit domain-dependent
information revealed by document ratings. Thus, in-
stead of averaging base predictors, MEM integrates
their features along with the available coarse-grained
training data into a unified probabilistic model.
The integrated model can be regarded as a Gaus-
sian process (GP) model (Rasmussen, 2004) with
a novel multi-expert prior. The multi-expert prior
decomposes into two component distributions. The
first component distribution integrates sentence-local
information obtained from the base predictors. It
forms a special realization of stacking (Dzeroski and
Zenko, 2004) but uses the features from the base pre-
dictors instead of the actual predictions. The second
component distribution propagates SO information
across similar sentences using techniques from graph-
based semi-supervised learning (GSSL) (Zhu et al
2003; Belkin et al2006). It aims to improve the
predictions on sentences that are not covered well
enough by our base predictors. Traditional GSSL al-
gorithms support either discrete labels (classification)
or numerical labels (regression); we extend these
techniques to support both types of labels simulta-
neously. We use a novel variant of word sequence
kernels (Cancedda et al2003) to measure sentence
similarity. Our kernel takes the relative positions of
words but also their SO and synonymity into account.
Our experiments indicate that MEM significantly
outperforms prior work in both sentence-level rating
prediction and sentence-level polarity classification.
2 Related Work
There exists a large body of work on analyzing the
semantic orientation of natural language text. Our
approach is unique in that it is weakly supervised,
predicts both polarity and strength, and operates on
the sentence level.
Supervised approaches for sentiment analysis fo-
cus mainly on opinion mining at the document
level (Pang and Lee, 2004; Pang et al2002; Pang
and Lee, 2005; Goldberg and Zhu, 2006), but have
also been applied to sentence-level polarity classifi-
cation in specific domains (Mao and Lebanon, 2006;
Pang and Lee, 2004; McDonald et al2007). In
these settings, a sufficient amount of training data is
available. In contrast, we focus on opinion mining
tasks with little or no fine-grained training data.
The weakly supervised HCRF model (Ta?ckstro?m
and McDonald, 2011b; Ta?ckstro?m and McDonald,
2011a) for sentence-level polarity classification is per-
haps closest to our work in spirit. Similar to MEM,
HCRF uses coarse-grained training data and, when
available, a small amount of fine-grained sentence
polarities. In contrast to MEM, HCRF does not pre-
dict the strength of semantic orientation and ignores
the order of words within sentences.
There exists a large number of lexicon-based meth-
ods for polarity classification (Ding et al2008; Choi
and Cardie, 2009; Hu and Liu, 2004; Zhuang et al
2006; Fu and Wang, 2010; Ku et al2008). The
lexicon-based methods of (Taboada et al2011; Qu
et al2010) also predict ratings at the phrase level;
these methods are used as experts in our model.
MEM leverages ideas from ensemble learning (Di-
etterichl, 2002; Bishop, 2006) and GSSL meth-
ods (Zhu et al2003; Zhu and Ghahramani, 2002;
Chapelle et al2006; Belkin et al2006). We extend
GSSL with support for multiple, heterogenous labels.
This allows us to integrate our base predictors as well
as the available training data into a unified model
that exploits that strengths of algorithms from both
families.
3 Base Predictors
Each of our base predictors predicts the polarity or
the rating of a single phrase. As indicated above,
we do not use these predictions directly in MEM but
instead integrate the features of the base predictors
150
(see Sec. 4.4). MEM is designed such that new base
predictors can be integrated easily.
Our base predictors use a diverse set of available
web and linguistic resources. The hope is that this di-
versity increases overall prediction performance (Di-
etterichl, 2002): The statistical polarity predictor fo-
cuses on local syntactic patterns; it is based on corpus
statistics for SO-carrying words and opinion topic
words. The heuristic polarity predictor uses manu-
ally constructed rules to achieve high precision but
low recall. Both the bag-of-opinions rating predictor
and the SO-CAL rating predictor are based on lexi-
cons. The BoO predictor uses a lexicon trained from
a large generic-domain corpus and is recall-oriented;
the SO-CAL predictor uses a different lexicon with
manually assigned weights and is precision-oriented.
3.1 Statistical Polarity Predictor
The polarity of an SO-carrying word strongly de-
pends on its target word. For example, consider the
phrase ?I began this novel with the greatest of hopes
[...]?. Here, ?greatest? has a positive semantic orien-
tation in all subjectivity lexicons, but the combination
?greatest of hopes? often indicates a negative senti-
ment. We refer to a pair of SO-carrying word (?great-
est?) and a target word (?hopes?) as an opinion-target
pair. Our statistical polarity predictor learns the po-
larity of opinions and targets jointly, which increases
the robustness of its predictions.
Syntactic dependency relations of the form
A
R
?? B are a strong indicator for opinion-target
pairs (Qiu et al2009; Zhuang et al2006); e.g.,
?great?
nmod
?????product?. To achieve high precision,
we only consider pairs connected by the follow-
ing predefined set of shortest dependency paths:
verb
subj
??? noun, verb
obj
?? noun, adj
nmod
???? noun,
adj
prd
??? verb
subj
??? noun. We only retain opinion-
target pairs that are sufficiently frequent.
For each extracted pair z, we count how often it
co-occurs with each document polarity y ? Y , where
Y = {positive, negative, other} denotes the set of po-
larities. If z occurs in a document but is preceded by
a negator, we treat it as a co-occurrence of opposite
document polarity. If z occurs in a document with po-
larity other, we count the occurrence with only half
weight, i.e., we increase both #z and #(other, z)
by 0.5. These documents are typically a mixture of
positive and negative opinions so that we want to
reduce their impact. The marginal distribution of
polarity label y given that z occurs in a sentence is
estimated as P (y | z) = #(y, z)/#z. The predictor
is trained using the text and ratings of the reviews in
the training data, i.e., without relying on fine-grained
annotations.
The statistical polarity predictor can be used to pre-
dict sentence-level polarities by averaging the phrase-
level predictions. As discussed previously, such an
approach is problematic; we use it as a baseline ap-
proach in our experimental study. We also employ
phrase-level averaging to estimate the variance of
base predictors; see Sec. 4.3. Denote by Z(x) the set
of opinion-target pairs in sentence x. To predict the
sentence polarity y ? Y , we take the Bayesian aver-
age of the phrase-level predictors: P (y | Z(x)) =
?
z?Z(x) P (y | z)P (z) =
?
z?Z(x) P (y, z). Thus
the most likely polarity is the one with the highest
co-occurrence count.
3.2 Heuristic Polarity Predictor
Heuristic patterns can also serve as base predictors.
In particular, we found that some authors list positive
and negative aspects separately after keywords such
as ?pros? and ?cons?. A heuristic that exploits such
patterns achieved a high precision (> 90%) but low
recall (< 5%) in our experiments.
3.3 Bag-of-Opinions Rating Predictor
We leverage the bag-of-opinion (BoO) model of Qu et
al. (2010) as a base predictor for phrase-level ratings.
The BoO model was trained from a large generic
corpus without fine-grained annotations.
In BoO, an opinion consists of three components:
an SO-carrying word (e.g., ?good?), a set of intensi-
fiers (e.g., ?very?) and a set of negators (e.g., ?not?).
Each opinion is scored based on these words (repre-
sented as a boolean vector b) and the polarity of the
SO-carrying word (represented as sgn(r) ? {?1, 1})
as indicated by the MPQA lexicon of Wilson et
al. (2005). In particular, the score is computed as
sgn(r)?Tb, where ? is the learned weight vector.
The sign function sgn(r) ensures consistent weight
assignment for intensifiers and negators. For exam-
ple, an intensifier like ?very? can obtain a large posi-
tive or a large negative weight depending on whether
it is used with a positive or negative SO-carrying
151
word, respectively.
3.4 SO-CAL Rating Predictor
The Semantic Orientation Calculator (SO-CAL) of
Taboada et al2011) also predicts phrase-level rat-
ings via a scoring function similar to the one of BoO.
The SO-CAL predictor uses a manually created lexi-
con, in which each word is classified as either an SO-
carrying word (associated with a numerical score), an
intensifier (associated with a modifier on the numer-
ical score), or a negator. SO-CAL employs various
heuristics to detect irrealis and to correct for the pos-
itive bias inherent in most lexicon-based classifiers.
Compared to BoO, SO-CAL has lower recall but
higher precision.
4 Multi-Experts Model
Our multi-experts model incorporates features from
the individual base predictors, coarse-grained labels
(i.e., document ratings or polarities), similarities be-
tween sentences, and optionally a small amount of
sentence polarity labels into an unified probabilistic
model. We first give an overview of MEM, and then
describe its components in detail.
4.1 Model Overview
Denote by X = {x1, . . . ,xN} a set of sentences.
We associate each sentence xi with a set of initial
labels y?i, which are strong indicators of semantic
orientation: the coarse-grained rating of the corre-
sponding document, the polarity label of our heuristic
polarity predictor, the phrase-level ratings from the
SO-CAL predictor, and optionally a manual polarity
label. Note that the number of initial labels may vary
from sentence to sentence and that initial labels are
heterogeneous in that they refer to either polarities
or ratings. Let Y? = {y?1, . . . , y?N}. Our goal is to
predict the unobserved ratings r = {r1, . . . , rN} of
each sentence.
Our multi-expert model is a probabilistic model
for X, Y?, and r. In particular, we model the rating
vector r via a multi-expert prior PE(r | X,?) with
parameter ? (Sec. 4.2). PE integrates both features
from the base predictors and sentence similarities.
We correlate ratings to initial labels via a set of con-
ditional distributions Pb(y?b | r), where b denotes the
type of initial label (Sec. 4.3). The posterior of r is
then given by
P (r | X, Y?,?) ?
?
b
Pb(y?
b | r)PE(r | X,?).
Note that the posterior is influenced by both the multi-
expert prior and the set of initial labels.
We use MAP inference to obtain the most likely
rating of each sentence, i.e., we solve
argmin
r,?
?
?
b
log(Pb(y?
b | r))? log(PE(r | X,?)),
where as before ? denotes the model parameters. We
solve the above optimization problem using cyclic
coordinate descent (Friedman et al2008).
4.2 Multi-Expert Prior
The multi-expert prior PE(r | X,?) consists of two
component distributions N1 and N2. Distribution
N1 integrates features from the base predictors, N2
incorporates sentence similarities to propagate infor-
mation across sentences.
In a slight abuse of notation, denote by xi the set of
features for the i-th sentence. Vector xi contains the
features of all the base predictors but also includes bi-
gram features for increased coverage of syntactic pat-
terns; see Sec. 4.4 for details about the feature design.
Letm(xi) = ?Txi be a linear predictor for ri, where
? is a real weight vector. Assuming Gaussian noise,
ri follows a Gaussian distribution N1(ri | mi, ?2)
with mean mi = m(xi) and variance ?2. Note that
predictor m can be regarded as a linear combination
of base predictors because both m and each of the
base predictors are linear functions. By integrating
all features into a single function, the base predictors
are trained jointly so that weight vector ? automati-
cally adapts to domain-dependent properties of the
data. This integrated approach significantly outper-
formed the alternative approach of using a weighted
vote of the individual predictions made by the base
predictors. We regularize the weight vector ? us-
ing a Laplace prior P (? | ?) with parameter ? to
encourage sparsity.
Note that the bigram features in xi partially cap-
ture sentence similarity. However, such features can-
not be extended to longer subsequences such as tri-
grams due to data sparsity: useful features become
as infrequent as noisy terms. Moreover, we would
152
like to capture sentence similarity using gapped (i.e.,
non-consecutive) subsequences. For example, the
sentences ?The book is an easy read.? and ?It is easy
to read.? are similar but do not share any consecutive
bigrams. They do share the subsequence ?easy read?,
however. To capture this similarity, we make use of a
novel sentiment-augmented variant of word sequence
kernels (Cancedda et al2003). Our kernel is used
to construct a similarity matrix W among sentences
and the corresponding regularized Laplacian L?. To
capture the intuition that similar sentences should
have similar ratings, we introduce a Gaussian prior
N2(r | 0, L??1) as a component into our multi-expert
prior; see Sec. 4.5 for details and a discussion of
why this prior encourages similar ratings for similar
sentences.
Since the two component distributions feature dif-
ferent expertise, we take their product and obtain the
multi-expert prior
PE(r | X,?) ? N1(r |m, I?
2)N2(r | 0, L??1)P (? | ?),
where m = (m1, . . . ,mN ). Note that the normal-
izing constant of PE can be ignored during MAP
inference since it does not depend on ?.
4.3 Incorporating Initial Labels
Recall that the initial labels Y? are strong indica-
tors of semantic orientation associated with each
sentence; they correspond to either discrete polarity
labels or to continuous rating labels. This hetero-
geneity constitutes the main difficulty for incorporat-
ing the initial labels via the conditional distributions
Pb(y?b | r). We assume independence throughout so
that Pb(y?b | r) =
?
i Pb(y?
b
i | ri).
Rating Labels For continuous labels, we assume
Gaussian noise and set Pb(y?bi | ri) = N (y?
b
i | ri, ?
b
i ),
where variance ?bi is a type- and sentence-dependent.
For SO-CAL labels, we simply set ?SO-CALi =
?SO-CAL, where ?SO-CAL is a hyperparameter. The
SO-CAL scores have limited influence in our overall
model; we found that more complex designs lead to
little improvement. We proceed differently for docu-
ment ratings. Our experiment suggests that document
ratings constitute the most important indicator of the
SO of a sentence. Thus sentence ratings should be
close to document ratings unless strong evidence to
the contrary exists. In other words, we want variance
?Doci to be small.
When no manually created sentence-level polar-
ity labels are available, we set the value of ?Doci de-
pending on the polarity class. In particular, we set
?Doci = 1 for both positive and negative documents,
and ?Doci = 2 for neutral documents. The reasoning
behind this choice is that sentence ratings in neu-
tral documents express higher variance because these
documents often contain a mixture of positive and
negative sentences.
When a small set of manually created sentence
polarity labels is available, we train a classifier that
predicts whether the sentence polarity coincides with
the document polarity. If so, we set the corresponding
variance ?Doci to a small value; otherwise, we choose
a larger value. In particular, we train a logistic regres-
sion classifier (Bishop, 2006) using the following
binary features: (1) an indicator variable for each
document polarity, and (2) an indicator variable for
each triple of base predictor, predicted polarity, and
document polarity (set to 1 if the polarities match).
We then set ?Doci = (?pi)
?1, where pi is the probabil-
ity of matching polarities obtained from the classifier
and ? is a hyperparameter that ensures correct scal-
ing.
Polarity Labels We now describe how to model
the correlation between the polarity of a sentence and
its rating. An simple and effective approach is to
partition the range of ratings into three consecutive
partitions, one for each polarity class. We thus consid-
ering the polarity classes {positive, other, negative}
as ordered and formulate polarity classification as an
ordinal regression problem (Chu and Ghahramani,
2006). We immediately obtain the distribution
Pb(y?
b
i = pos | ri) = ?
(
ri ? b+
?
?b
)
Pb(y?
b
i = oth | ri) = ?
(
b+ ? ri
?
?b
)
? ?
(
b? ? ri
?
?b
)
Pb(y?
b
i = neg | ri) = ?
(
b? ? ri
?
?b
)
,
where b+ and b? are the partition boundaries between
positive/other and other/negative, respectively,2 ?(x)
denotes the cumulative distribution function of the
2We set b+ = 0.3 and b? = ?0.3 to calibrate to SO-CAL,
which treats ratings in [?0.3, 0, 3] as polarity other.
153
Figure 1: Distribution of polarity given rating.
Gaussian distribution, and variance ?b is a hyper-
parameter. It is easy to verify that
?
y?bi?Y
p(y?bi |
ri) = 1. The resulting distribution is shown in Fig. 1.
We can use the same distribution to use MEM for
sentence-level polarity classification; in this case, we
pick the polarity with the highest probability.
4.4 Incorporating Base Predictors
Base predictors are integrated into MEM via compo-
nent N1(ri | mi, ?2) of the multi-expert prior (see
Sec. 4.2). Recall that mi is a linear function of the
features xi of each sentence. In this section, we dis-
cuss how xi is constructed from the features of the
base predictors. New base predictors can be inte-
grated easily by exposing their features to MEM.
Most base predictors operate on the phrase level;
our goal is to construct features for the entire sen-
tence. Denote by nbi the number of phrases in the
i-th sentence covered by base predictor b, and let
obij denote a set of associated features. Features o
b
ij
may or may not correspond directly to the features
of base predictor b; see the discussion below. A
straightforward strategy is to set xbi = (n
b
i)
?1?
j o
b
ij .
We proceed slightly differently and average the fea-
tures associated with phrases of positive prior polar-
ity separately from those of phrases with negative
prior polarity (Taboada et al2011). We then con-
catenate the averaged feature vectors, i.e., we set
xbi = (o?
b,pos
ij o?
b,neg
ij ), where o?
b,p
ij denotes the average
of the feature vectors obij associated with phrases of
prior polarity p. This procedure allows us to learn
a different weight for each feature depending on its
context (e.g., the weight of intensifier ?very? may dif-
fer for positive and negative phrases). We construct
xi by concatenating the sentence-level features xbi of
each base predictor and a feature vector of bigrams.
To integrate a base predictor, we only need to
specify the relevant features and, if applicable, prior
phrase polarities. For our choice of base predictors,
we use the following features:
SO-CAL predictor. The prior polarity of a SO-
CAL phrase is given by the polarity of its SO-
carrying word in the SO-CAL lexicon. The feature
vector oSO-CALij consists of the weight of the SO-
carrying word from the lexicon as well the set of
negator words, irrealis marker words, and intensifier
words in the phrase. Moreover, we add the first two
words preceding the SO-carrying word as context
features (skipping nouns, negators, irrealis markers,
and intensifiers, and stopping at clause boundaries).
All words are encoded as binary indicator features.
BoO predictor. Similar to SO-CAL, we deter-
mine the prior polarity of a phrase based on the BoO
dictionary. In contrast to SO-CAL, we directly use
the BoO score as a feature because the BoO predictor
weights have been trained on a very large corpus and
are thus reliable. We also add irrealis marker words
in the form of indicator features.
Statistical polarity predictor. Recall that the sta-
tistical polarity predictor is based on co-occurrence
counts of opinion-topic pairs and document polar-
ities. We treat each opinion-topic pair as a phrase
and use the most frequently co-occurring polarity
as the phrase?s prior polarity. We use the logarithm
of the co-occurrence counts with positive, negative,
and other polarity as features; this set of features per-
formed better than using the co-occurrence counts or
estimated class probabilities directly. We also add
the same type of context features as for SO-CAL, but
rescale each binary feature by the logarithm of the
occurrence count #z of the opinion-topic pair (i.e.,
the features take values in {0, log #z}).
4.5 Incorporating Sentence Similarities
The component distribution N2(r | 0, L??1) in the
multi-expert prior encourages similar sentences to
have similar ratings. The main purpose of N2 is to
propagate information from sentences on which the
base predictors perform well to sentences for which
base prediction is unreliable or unavailable (e.g., be-
154
cause they do not contain SO-carrying words). To
obtain this distribution, we first construct an N ?N
sentence similarity matrix W using a sentiment-
augmented word sequence kernel (see below). We
then compute the regularized graph Laplacian L? =
L+I/?2 based on the unnormalized graph Laplacian
L = D?W (Chapelle et al2006), where D be a
diagonal matrix with dii =
?
j wij and hyperparam-
eter ?2 controls the scale of sentence ratings.
To gain insight into distribution N2, observe that
N2(r | 0, L??1)
? exp
(
?
1
2
?
i,j
wij(ri ? rj)
2 ? ?r?22/?
2
)
.
The left term in the exponent forces the ratings of
similar sentences to be similar: the larger the sen-
tence similarity wij , the more penalty is paid for dis-
similar ratings. For this reason, N2 has a smoothing
effect. The right term is an L2 regularizer and encour-
ages small ratings; it is controlled by hyperparameter
?2.
The entries wij in the sentence similarity matrix
determine the degree of smoothing for each pair of
sentence ratings. We compute these values by a novel
sentiment-augmented word sequence kernel, which
extends the well-known word sequence kernel of Can-
cedda et al2003) by (1) BoO weights to strengthen
the correlation of sentence similarity and rating sim-
ilarity and (2) synonym resolution based on Word-
Net (Miller, 1995).
In general, a word sequence kernel computes a
similarity score of two sequences based on their
shared subsequences. In more detail, we first de-
fine a score function for a pair of shared subse-
quences, and then sum up these scores to obtain
the overall similarity score. Consider for example
the two sentences ?The book is an easy read.? (s1)
and ?It is easy to read.? (s2) along with the shared
subsequence ?is easy read? (u). Observe that the
words ?an? and ?to? serve as gaps as they are not
part of the subsequence. We represent subsequence
u in sentence s via a real-valued projection function
?u(s). In our example, ?u(s1) = ?is?
g
an?easy?read
and ?u(s2) = ?is?easy?
g
to?read. The decay factors
?w ? (0, 1] for matching words characterize the
importance of a word (large values for significant
words). On the contrary, decay factors ?gw ? (0, 1]
for gap words are penalty terms for mismatches
(small values for significant words). The score of
subsequence u is defined as ?u(s1)?u(s2). Thus
two shared subsequences have high similarity if they
share significant words and few gaps. Following Can-
cedda et al2003), we define the similarity between
two sequences as
kn(si, sj) =
?
u??n
?u(si)?u(sj),
where ? is a finite set of words and n denotes the
length of the considered subsequences. This sim-
ilarity function can be computed efficiently using
dynamic programming.
To apply the word sequence kernel, we need to
specify the decay factors. A traditional choice is
?w = log( NNw )/ log(N), where Nw is the document
frequency of the word w and N is the total number
of documents. This IDF decay factor is not well-
suited to our setting: Important opinion words such
as ?great? have a low IDF value due to their high
document frequency. To overcome this problem,
we incorporate additional weights for SO-carrying
words using the BoO lexicon. To do so, we first
rescale the BoO weights into [0, 1] using the sig-
moid g(w) = (1 + exp(?a?w + b))?1, where ?w
denotes the BoO weight of word w.3 We then set
?w = min(log( NNw )/ log(N) + g(w), 0.9). The de-
cay factor for gaps is given by ?gw = 1 ? ?w. Thus
we strongly penalize gaps that consist of infrequent
words or opinion words.
To address data sparsity, we incorporate synonyms
and hypernyms from WordNet into our kernel. In
particular, we represent words found in WordNet by
their first two synset names (for verbs, adjectives,
nouns) and their direct hypernym (nouns only). Two
words are considered the same when their synsets
overlap. Thus, for example, ?writer? has the same
representation as ?author?.
To build the similarity matrix W, we construct
a k-nearest-neighbor graph for all sentences.4 We
consider subsequences consisting of three words (i.e.,
wij = k3(si, sj)); longer subsequences are overly
sparse, shorter subsequences are covered by the bi-
grams features in N1.
3We set a = 2 and b = 1 in our experiments.
4We use k = 15 and only consider neighbors with a similar-
ity above 0.001.
155
5 Experiments
We evaluated both MEM and a number of alternative
approaches for both sentence-level polarity classifi-
cation and sentence-level strength prediction across
a number of domains. We found that MEM out-
performs state-of-the-art approaches by a significant
margin.
5.1 Experimental Setup
We implemented MEM as well as the HCRF classi-
fier of (Ta?ckstro?m and McDonald, 2011a; Ta?ckstro?m
and McDonald, 2011b), which is the best-performing
estimator of sentence-level polarity in the weakly-
supervised setting reported in the literature. We train
both methods using (1) only coarse labels (MEM-
Coarse, HCRF-Coarse) and (2) additionally a small
number of sentence polarities (MEM-Fine, HCRF-
Fine5). We also implemented a number of baselines
for both polarity classification and strength predic-
tion: a document oracle (DocOracle) that simply uses
the document label for each sentence, the BoO rat-
ing predictor (BaseBoO), and the SO-CAL rating pre-
dictor (BaseSO-CAL). For polarity classification, we
compare our methods also to the statistical polarity
predictor (Basepolarity). To judge on the effectiveness
of our multi-export prior for combining base predic-
tors, we take the majority vote of all base predic-
tors and document polarity as an additional baseline
(Majority-Vote). Similarly, for strength prediction,
we take the arithmetic mean of the document rat-
ing and the phrase-level predictions of BaseBoO and
BaseSO-CAL as a baseline (Mean-Rating). We use the
same hyperparameter setting for MEM across all our
experiments.
We evaluated all methods on Amazon reviews
from different domains using the corpus of Ding et al
(2008) and the test set of Ta?ckstro?m and McDonald
(2011a). For each domain, we constructed a large bal-
anced dataset by randomly sampling 33,000 reviews
from the corpus of Ding et al2008). We chose
the books, electronics, and music domains for our
experiments; the dvd domain was used for develop-
ment. For sentence polarity classification, we use the
test set of Ta?ckstro?m and McDonald (2011a), which
5We used the best-performing model that fuses HCRF-Coarse
and the supervised model (McDonald et al2007) by interpola-
tion.
contains roughly 60 reviews per domain (20 for each
polarity). For strength evaluation, we created a test
set of 300 pairs of sentences per domain from the
polarity test set. Each pair consisted of two sentences
of the same polarity; we manually determined which
of the sentences is more positive. We chose this pair-
wise approach because (1) we wanted the evaluation
to be invariant to the scale of the predicted ratings,
and (2) it much easier for human annotators to rank
a pair of sentences than to rank a large collection of
sentences.
We followed Ta?ckstro?m and McDonald (2011b)
and used 3-fold cross-validation, where each fold
consisted of a set of roughly 20 documents from the
test set. In each fold, we merged the test set with the
reviews from the corresponding domain. For MEM-
Fine and HCRF-Fine, we use the data from the other
two folds as fine-grained polarity annotations. For
our experiments on polarity classification, we con-
verted the predicted ratings of MEM, BaseBoO, and
BaseSO-CAL into polarities by the method described
in Sec. 4.3. We compare the performance of each
method in terms of accuracy, which is defined as the
fraction of correct predictions on the test set (correct
label for polarity / correct ranking for strength). All
reported numbers are averages over the three folds. In
our tables, boldface numbers are statistically signifi-
cant against all other methods (t-test, p-value 0.05).
5.2 Results for Polarity Classification
Table 1 summarizes the results of our experiments for
sentence polarity classification. The base predictors
perform poorly across all domains, mainly due to
the aforementioned problems associated with aver-
aging phrase-level predictions. In fact, DocOracle
performs almost always better than any of the base
predictors. However, accurracy increases when we
combine base predictors and DocOracle using ma-
jority voting, which indicates that ensemble methods
work well.
When no fine-grained annotations are available
(HCRF-Coarse, MEM-Coarse), both MEM-Coarse
and Majority-Vote outperformed HCRF-Coarse,
which in turn has been shown to outperform a num-
ber of lexicon-based methods as well as classifiers
trained on document labels (Ta?ckstro?m and McDon-
ald, 2011a). MEM-Coarse also performs better than
Majority-Vote. This is because MEM propagates
156
Book Electronics Music Avg
Basepolarity 43.7 40.3 43.8 42.6
BaseBoO 50.9 48.9 52.6 50.8
BaseSO-CAL 44.6 50.2 45.0 46.6
DocOracle 51.9 49.6 59.3 53.6
Majority-Vote 53.7 53.4 58.7 55.2
HCRF-Coarse 52.2 53.4 57.2 54.3
MEM-Coarse 54.4 54.9 64.5 57.9
HCRF-Fine 55.9 61.0 58.7 58.5
MEM-Fine 59.7 59.6 63.8 61.0
Table 1: Accuracy of polarity classification per do-
main and averaged across domains.
evidence across similar sentences, which is espe-
cially useful when no explicit SO-carrying words
exist. Also, MEM learns weights of features of base
predictors, which leads to a more adaptive integration,
and our ordinal regression formulation for polarity
prediction allows direct competition among positive
and negative evidence for improved accuracy.
When we incorporate a small amount of sentence
polarity labels (HCRF-Fine, MEM-Fine), the accu-
racy of all models greatly improves. HCRF-Fine has
been shown to outperform the strongest supervised
method on the same dataset (McDonald et al2007;
Ta?ckstro?m and McDonald, 2011b). MEM-Fine falls
short of HCRF-Fine only in the electronics domain
but performs better on all other domains. In the book
and music domains, where MEM-Fine is particularly
effective, many sentences feature complex syntac-
tic structure and SO-carrying words are often used
without reference to the quality of the product (but to
describe contents, e.g., ?a love story? or ?a horrible
accident?).
Our models perform especially well when they are
applied to sentences containing no or few opinion
words from lexicons. Table 2 reports the evaluation
results for both sentences containing SO-carrying
words from either MPQA or SO-CAL lexicons and
for sentences containing no such words. The re-
sults explain why our model falls short of HCRF-
Fine in the electronics domain: reviews of electronic
products contain many SO-carrying words, which
almost always express opinions. Nevertheless, MEM-
Fine handles sentences without explicit SO-carrying
words well across all domains; here the propagation
of information across sentences helps to learn the SO
Book Electronics Music
op fact op fact op fact
HCRF-Fine 55.7 55.9 63.3 54.6 59.0 57.4
MEM-Fine 58.9 62.4 60.7 56.7 64.5 60.8
Table 2: Accuracy of polarity classification for sen-
tences with opinion words (op) and without opinion
words (fact).
of facts (such as ?short battery life?).
We found that for all methods, most of the errors
are caused by misclassifying positive/negative sen-
tences as other and vice versa. Moreover, sentences
with polarity opposite to the document polarity are
hard cases if they do not feature frequent strong pat-
terns. Another difficulty lies in off-topic sentences,
which may contain explicit SO-carrying words but
are not related to the item under review. This is one
of the main reasons for the poor performance of the
lexicon-based methods.
Overall, we found that MEM-Fine is the method of
choice. Thus our multi-expert model can indeed bal-
ance the strength of the individual experts to obtain
better estimation accuracy.
5.3 Results for Strength Prediction
Table 3 shows the accuracy results for strength pre-
diction. Here our models outperformed all baselines
by a large margin. Although document ratings are
strong indicators in the polarity classification task,
they lead to worse performance than lexicon-based
methods. The main reason for this drop in accuracy
is that the document oracle assigns the same rating
to all sentences within a review. Thus DocOracle
cannot rank sentences from the same review, which
is a severe limitation. This shortage can be partly
compensated by averaging the base predictions and
document rating (Mean-Rating). Note that it is non-
trivial to apply existing ensemble methods for the
weights of individual base predictors because of the
absence of the sentence ratings as training labels. In
contrast, our MEM models use indirect supervision
to adaptively assign weights to the features from base
predictors. Similar to polarity classification, a small
amount of sentence polarity labels often improved
the performance of MEM.
157
Book Electronics Music Avg
BaseBoO 58.3 51.6 53.5 54.5
BaseSO-CAL 60.6 57.1 47.6 55.1
DocOracle 45.1 36.2 41.4 40.9
Mean-Rating 70.3 57.0 60.8 62.7
MEM-Coarse 68.7 60.5 69.5 66.2
MEM-Fine 72.4 63.3 67.2 67.6
Table 3: Accuracy of strength prediction.
6 Conclusion
We proposed the Multi-Experts Model for analyz-
ing both opinion polarity and opinion strength at
the sentence level. MEM is weakly supervised; it
can run without any fine-grained annotations but is
also able to leverage such annotations when avail-
able. MEM is driven by a novel multi-expert prior,
which integrates a number of diverse base predictors
and propagates information across sentences using a
sentiment-augmented word sequence kernel. Our ex-
periments indicate that MEM achieves better overall
accuracy than alternative methods.
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples.
The Journal of Machine Learning Research, 7:2399?
2434.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning, volume 4. Springer New York.
Nicola Cancedda, E?ric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
Oliver Chapelle, Bernhard Scho?lkopf, and Alexander Zien.
2006. Semi-Supervised Learning. MIT Press.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, volume 2, pages 590?598.
Wei Chu and Zoubin Ghahramani. 2006. Gaussian pro-
cesses for ordinal regression. Journal of Machine
Learning Research, 6(1):1019.
Thomas G. Dietterichl. 2002. Ensemble learning. The
Handbook of Brain Theory and Neural Networks, pages
405?408.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the International Conference on Web
Search and Data Mining, pages 231?240.
Saso Dzeroski and Bernard Zenko. 2004. Is combining
classifiers with stacking better than selecting the best
one? Machine Learning, 54(3):255?273.
Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear mod-
els via coordinate descent. Technical report.
Guohong Fu and Xin Wang. 2010. Chinese sentence-
level sentiment classification based on fuzzy sets. In
Proceedings of the International Conference on Com-
putational Linguistics, pages 312?319. Association for
Computational Linguistics.
Andrew B. Goldberg and Xiaojun Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 168?177.
Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen,
and Hsin-Hsi Chen. 2008. Sentence-level opinion anal-
ysis by copeopi in ntcir-7. In Proceedings of NTCIR-7
Workshop Meeting.
Yi Mao and Guy Lebanon. 2006. Isotonic Conditional
Random Fields and Local Sentiment Flow. Advances
in Neural Information Processing Systems, pages 961?
968.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, volume 45, page 432.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?41.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting on Association for Computational Linguistics,
pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 124?131.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 79?86.
158
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding Domain Sentiment Lexicon through Dou-
ble Propagation. In International Joint Conference on
Artificial Intelligence, pages 1199?1204.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010.
The bag-of-opinions method for review rating predic-
tion from sparse text patterns. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 913?921.
Carl Edward Rasmussen. 2004. Gaussian processes in
machine learning. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011a. Dis-
covering Fine-Grained Sentiment with Latent Variable
Structured Prediction Models. In Proceedings of the
European Conference on Information Retrieval, pages
368?374.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 569?574.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Human Language
Technology Conference and the Conference on Empir-
ical Methods in Natural Language Processing, pages
347?354.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propagation.
Technical report.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference on Machine Learning, pages 912?
919.
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management, pages 43?50.
159
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 379?390, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Natural Language Questions for the Web of Data
Mohamed Yahya1, Klaus Berberich1, Shady Elbassuoni2
Maya Ramanath3, Volker Tresp4, Gerhard Weikum1
1 Max Planck Institute for Informatics, Germany
2 Qatar Computing Research Institute
3 Dept. of CSE, IIT-Delhi, India 4 Siemens AG, Corporate Technology, Munich, Germany
{myahya,kberberi,weikum}@mpi-inf.mpg.de
selbassuoni@qf.org.qa
ramanath@cse.iitd.ac.in volker.tresp@siemens.com
Abstract
The Linked Data initiative comprises struc-
tured databases in the Semantic-Web data
model RDF. Exploring this heterogeneous
data by structured query languages is tedious
and error-prone even for skilled users. To ease
the task, this paper presents a methodology
for translating natural language questions into
structured SPARQL queries over linked-data
sources.
Our method is based on an integer linear pro-
gram to solve several disambiguation tasks
jointly: the segmentation of questions into
phrases; the mapping of phrases to semantic
entities, classes, and relations; and the con-
struction of SPARQL triple patterns. Our so-
lution harnesses the rich type system provided
by knowledge bases in the web of linked data,
to constrain our semantic-coherence objective
function. We present experiments on both the
question translation and the resulting query
answering.
1 Introduction
1.1 Motivation
Recently, very large, structured, and semantically
rich knowledge bases have become available. Ex-
amples are Yago (Suchanek et al2007), DBpe-
dia (Auer et al2007), and Freebase (Bollacker et
al., 2008). DBpedia forms the nucleus of the Web of
Linked Data (Heath and Bizer, 2011), which inter-
connects hundreds of RDF data sources with a total
of 30 billion subject-property-object (SPO) triples.
The diversity of linked-data sources and their high
heterogeneity make it difficult for humans to search
and discover relevant information. As linked data
is in RDF format, the standard approach would be
to run structured queries in triple-pattern-based lan-
guages like SPARQL, but only expert programmers
are able to precisely specify their information needs
and cope with the high heterogeneity of the data
(and absence or very high complexity of schema in-
formation). For less initiated users the only option
to query this rich data is by keyword search (e.g.,
via services like sig.ma (Tummarello et al2010)).
None of these approaches is satisfactory. Instead, the
by far most convenient approach would be to search
in knowledge bases and the Web of linked data by
means of natural-language questions.
As an example, consider a quiz question like
?Which female actor played in Casablanca and is
married to a writer who was born in Rome??.
The answer could be found by querying sev-
eral linked data sources together, like the IMDB-
style LinkedMDB movie database and the DB-
pedia knowledge base, exploiting that there are
entity-level sameAs links between these collections.
One can think of different formulations of the
example question, such as ?Which actress from
Casablanca is married to a writer from Rome??. A
possible SPARQL formulation, assuming a user fa-
miliar with the schema of the underlying knowl-
edge base(s), could consist of the following six
triple patterns (joined by shared-variable bind-
ings): ?x hasGender female, ?x isa actor, ?x
actedIn Casablanca (film), ?x marriedTo ?w,
?w isa writer, ?w bornIn Rome. This complex
query, which involves multiple joins, would yield
good results, but it is difficult for the user to come
379
up with the precise choices for relations, classes, and
entities. This would require familiarity with the con-
tents of the knowledge base, which no average user
is expected to have. Our goal is to automatically cre-
ate such structured queries by mapping the user?s
question into this representation. Keyword search is
usually not a viable alternative when the information
need involves joining multiple triples to construct
the final result, notwithstanding good attempts like
that of Pound et al2010). In the example, the obvi-
ous keyword query ?female actress Casablanca mar-
ried writer born Rome? lacks a clear specification of
the relations among the different entities.
1.2 Problem
Given a natural language question qNL and a knowl-
edge base KB, our goal is to translate qNL into a
formal query qFL that captures the information need
expressed by qNL.
We focus on input questions that put the em-
phasis on entities, classes, and relations between
them. We do not consider aggregations (counting,
max/min, etc.) and negations. As a result, we gener-
ate structured queries of the form known as conjunc-
tive queries or select-project-join queries in database
terminology. Our target language is SPARQL 1.0,
where the above focus leads to queries that consist of
multiple triple patterns, that is, conjunctions of SPO
search conditions. We do not use any pre-existing
query templates, but generate queries from scratch
as they involve a variable number of joins with a-
priori unknown join structure.
A major challenge is in the ambiguity of
the phrases occurring in a natural-language ques-
tion. Phrases can denote entities (e.g., the city
of Casablanca or the movie Casablanca), classes
(e.g., actresses, movies, married people), or rela-
tions/properties (e.g., marriedTo between people,
played between people and movies). A priori, we do
not know if a phrase should be mapped to an entity,
a class, or a relation. In fact, some phrases may de-
note any of these three kinds of targets. For example,
a phrase like ?wrote score for? in a question about
film music composers, could map to the composer-
film relation wroteSoundtrackForFilm, to the class
of movieSoundtracks (a subclass of music pieces),
or to an entity like the movie ?The Score?. Depend-
ing on the choice, we may arrive at a structurally
good query (with triple patterns that can actually
be joined) or at a meaningless and non-executable
query (with disconnected triple patterns). This gen-
eralized disambiguation problem is much more chal-
lenging than the more focused task of named entity
disambiguation (NED). It is also different from gen-
eral word sense disambiguation (WSD), which fo-
cuses on the meaning of individual words (e.g., map-
ping them to WordNet synsets).
1.3 Contribution
In our approach, we introduce new elements towards
making translation of questions into SPARQL triple
patterns more expressive and robust. Most impor-
tantly, we solve the disambiguation and mapping
tasks jointly, by encoding them into a comprehen-
sive integer linear program (ILP): the segmentation
of questions into meaningful phrases, the mapping
of phrases to semantic entities, classes, and rela-
tions, and the construction of SPARQL triple pat-
terns. The ILP harnesses the richness of large knowl-
edge bases like Yago2 (Hoffart et al2011b), which
has information not only about entities and relations,
but also about surface names and textual patterns
by which web sources refer to them. For example,
Yago2 knows that ?Casablanca? can refer to the city
or the film, and ?played in? is a pattern that can de-
note the actedIn relation. In addition, we can lever-
age the rich type system of semantic classes. For ex-
ample, knowing that Casablanca is a film, for trans-
lating ?played in? we can focus on relations with a
type signature whose range includes films, as op-
posed to sports teams, for example. Such informa-
tion is encoded in judiciously designed constraints
for the ILP. Although we intensively harness Yago2,
our approach does not depend on a specific choice of
knowledge base or language resource for type infor-
mation and phrase/name dictionaries. Other knowl-
edge bases such as DBpedia can be easily plugged
in.
Based on these ideas, we have developed a frame-
work and system, called DEANNA (DEep Answers
for maNy Naturally Asked questions), that com-
prises a full suite of components for question de-
composition, mapping constituents into the seman-
tic concept space, generating alternative candidate
mappings, and computing a coherent mapping of all
constituents into a set of SPARQL triple patterns that
380
can be directly executed on one or more linked data
sources.
2 Background
We use the Yago2 knowledge base, with its rich
type system, as a semantic backbone. Yago2 is com-
posed of instances of binary relations derived from
Wikipedia and WordNet. The instances, called facts,
provide both ontological information and instance
data. Figure 1 shows sample facts from Yago2. Each
fact is composed of semantic items that can be di-
vided into relations, entities, and classes. Entities
and classes together are referred to as concepts.
Subject Predicate Object
film subclassOfproduction
Casablanca (film)type film
?Casablanca? means Casablanca (film)
?Casablanca? means Casablanca, Morocco
Ingrid Bergman actedIn Casablanca (film)
Figure 1: Sample knowledge base
Examples of relations are type, subclassOf, and
actedIn. Each relation has a type signature: classes
for the relation?s domain and range. Classes, such as
person and film group entities. Entities are repre-
sented in canonical form such as Ingrid Bergman
and Casablanca (film). A special type of entities
are literals, such as strings, numbers, and dates.
3 Framework
Given a natural language question, Figure 2 shows
the tasks DEANNA performs to translate a ques-
tion into a structured query. The first three steps
prepare the input for constructing a disambiguation
graph for mapping the phrases in a question onto
entities, classes, and relations, in a coherent man-
ner. The fourth step formulates this generalized dis-
ambiguation problem as an ILP with complex con-
straints and computes the best solution using an
ILP solver. Finally, the fifth and sixth step together
use the disambiguated mapping to construct an exe-
cutable SPARQL query.
A question sentence is a sequence of tokens,
qNL = (t0, t1, ..., tn). A phrase is a contiguous sub-
sequence of tokens (ti, ti+1, ..., ti+l) ? qNL, 0 ?
i, 0 ? l ? n. The input question is fed into the fol-
lowing pipeline of six steps:
1. Phrase detection. Phrases are detected that
potentially correspond to semantic items such as
?Who?, ?played in?, ?movie? and ?Casablanca?.
2. Phrase mapping to semantic items. This in-
cludes finding that the phrase ?played in? can ei-
ther refer to the semantic relation actedIn or to
playedForTeam and that the phrase ?Casablanca?
can potentially refer to Casablanca (film) or
Casablanca, Morocco. This step merely constructs
a candidate space for the mapping. The actual dis-
ambiguation is addressed by step 4, discussed below.
3. Q-unit generation. Intuitively, a q-unit is a triple
composed of phrases. Their generation and role will
be discussed in detail in the next section.
4. Joint disambiguation, where the ambiguities in
the phrase-to-semantic-item mapping are resolved.
This entails resolving the ambiguity in phrase bor-
ders, and above all, choosing the best fitting can-
didates from the semantic space of entities, classes,
and relations. Here, we determine for our running
example that ?played in? refers to the semantic re-
lation actedIn and not to playedForTeam and the
phrase ?Casablanca? refers to Casablanca (film)
and not Casablanca, Morocco.
5. Semantic items grouping to form semantic
triples. For example, we determine that the relation
marriedTo connects person referred to by ?Who?
and writer to form the semantic triple person
marriedTo writer. This is done via q-units.
6. Query generation. For SPARQL queries, seman-
tic triples such as person marriedTo writer have
to be mapped to suitable triple patterns with appro-
priate join conditions expressed through common
variables: ?x type person, ?x marriedTo ?w, and
?w type writer for the example.
3.1 Phrase Detection
A detected phrase p is a pair < Toks, l > where
Toks is a phrase and l is a label, l ?
{concept, relation}, indicating whether a phrase is
a relation phrase or a concept phrase. Pr is the set of
all detected relation phrases and Pc is the set of all
detected concept phrases.
One special type of detected relation phrase is
the null phrase, where no relation is explicitly men-
tioned, but can be induced. The most prominent ex-
ample of this is the case of adjectives, such as ?Aus-
tralian movie?, where we know there is a relation
being expressed between ?Australia? and ?movie?.
We use multiple detectors for detecting phrases of
381
 qu
est
ion
ph
ras
es
str
uct
ure
d
qu
ery
 
Concept
&Relation
Phrase
Detection  
Joint
Disambi-
guation
  
ma
pp
ing
s
can
did
ate
gra
ph
 
entities & namesclasses & subclassesrelations & pattternsincl. dictionaries & statistics
  
Knowledge Base
 
Concept
&Relation
Phrase
Mapping
1 2 3 4
     
5
  SemanticItems
Grouping
Query
Gene-
ration
6
sel
ect
ed
s-n
od
es
trip
le
pa
tte
rns
 
Q-unit  
Genera- 
tion
Figure 2: Architecture of DEANNA.
different types. For concept detection, we use a de-
tector that works against a phrase-concept dictionary
which looks as follows:
{?Rome?,?eternal city?} ? Rome
{?Casablanca?} ? Casablanca (film)
We experimented with using third-party named en-
tity recognizers but the results were not satisfactory.
This dictionary was mostly constructed as part of
the knowledge base, independently of the question-
to-query translation task in the form of instances of
the means relation in Yago2, an example of which is
shown in Figure 1
For relation detection, we experimented with var-
ious approaches. We mainly rely on a relation detec-
tor based on ReVerb (Fader et al2011) with addi-
tional POS tag patterns, in addition to our own which
looks for patterns in dependency parses.
3.2 Phrase Mapping
After phrases are detected, each phrase is mapped
to a set of semantic items. The mapping of concept
phrases also relies on the phrase-concept dictionary.
To map relation phrases, we rely on a corpus of
textual patterns to relation mappings of the form:
{?play?,?star in?,?act?,?leading role?} ? actedIn
{?married?, ?spouse?,?wife?} ? marriedTo
Distinct phrase occurrences will map to different
semantic item instances. We discuss why this is im-
portant when we discuss the construction of the dis-
ambiguation graph and variable assignment in the
structured query.
3.3 Dependency Parsing & Q-Unit Generation
Dependency parsing identifies triples of to-
kens, or triploids, ?trel, targ1, targ2?, where
trel, targ1, targ2 ? qNL are seeds for phrases, with
the triploid acting as a seed for a potential SPARQL
triple pattern. Here, trel is the seed for the relation
phrase, while targ1 and targ2 are seeds for the two
arguments. At this point, there is no attempt to
assign subject/object roles to the arguments.
Triploids are collected by looking for specific de-
pendency patterns in dependency graphs (de Marn-
effe et al2006). The most prominent pattern we
look for is a verb and its arguments. Other patterns
include adjectives and their arguments, preposition-
ally modified tokens and objects of prepositions.
By combining triploids with detected phrases, we
obtain q-units. A q-unit is a triple of sets of phrases,
?{prel ? Pr}, {parg1 ? Pc}, {parg2 ? Pc}?, where
trel ? prel and similarly for arg1 and arg2. Concep-
tually, one can view a q-unit as a placeholder node
with three sets of edges, each connecting the same
q-node to a phrase that corresponds to a relation or
concept phrase in the same q-unit. This notion of
nodes and edges will be made more concrete when
we present our disambiguation graph construction.
3.4 Disambiguation of Phrase Mappings
The core contribution of this paper is a framework
for disambiguating phrases into semantic items ?
covering relations, classes, and entities in a unified
manner. This can be seen as a joint task combining
382
named entity disambiguation for entities, word sense
disambiguation for classes (common nouns), and re-
lation extraction. The next section presents the dis-
ambiguation framework in detail.
3.5 Query Generation
Once phrases are mapped to unique semantic items,
we proceed to generate queries in two steps. First,
semantic items are grouped into triples. This is done
using the triploids generated earlier. The power of
using a knowledge base is that we have a rich type
system that allows us to tell if two semantic items
are compatible or not. Each relation has a type sig-
nature and we check whether the candidate items are
compatible with the signature.
We did not assign subject/object roles in triploids
and q-units because a natural language relation
phrase might express the inverse of a semantic rela-
tion, e.g., the natural language expression ?directed
by? and the relation isDirectorOf with respect to
the movies domain are inverses of each other. There-
fore, we check which assignment of arg1 and arg2
is compatible with the semantic relation. If both ar-
rangements are compatible, then we give preference
to the assignment given by the dependency parsers.
Once semantic items are grouped into triples, it
is an easy task to expand them to SPARQL triple
patterns. This is done by replacing each seman-
tic class with a distinct type-constrained variable.
Note that this is the reason why each distinct phrase
maps to a distinct instance of a semantic class, to
ensure correct variable assignment. This becomes
clear when we consider the question ?Which singer
is married to a singer??, which requires two distinct
variables each constrained to bind to an entity of
type singer.
4 Joint Disambiguation
The goal of the disambiguation step is to compute
a partial mapping of phrases onto semantic items,
such that each phrase is assigned to at most one
semantic item. This step also resolves the phrase-
boundary ambiguity, by enforcing that only non-
overlapping phrases are mapped. As the result of
disambiguating one phrase can influence the map-
ping of other phrases, we consider all phrases jointly
in one big disambiguation task.
In the following, we construct a disambiguation
graph that encodes all possible mappings. We im-
pose a variety of complex constraints (mutual ex-
clusion among overlapping phrases, type constraints
among the selected semantic items, etc.), and define
an objective function that aims to maximize the joint
quality of the mapping. The graph construction it-
self may resemble similar models used in NED (e.g.,
(Milne and Witten, 2008; Kulkarni et al2009; Hof-
fart et al2011a)). Recall, however, that our task is
more complex because we jointly consider entities,
classes, and relations in the candidate space of pos-
sible mappings. Because of this complication and
to capture our complex constraints, we do not em-
ploy graph algorithms, but model the general disam-
biguation problem as an ILP.
4.1 Disambiguation Graph
Joint disambiguation takes place over a disambigua-
tion graph DG = (V,E), where V = Vs ? Vp ? Vq
and E = Esim ? Ecoh ? Eq, where:
? Vs is the set of semantic items, vs ? Vs is an
s-node.
? Vp is the set of phrases, vp ? Vp is called a p-
node. We denote the set of p-nodes correspond-
ing to relation phrases by Vrp and the set of p-
nodes corresponding to concept phrases by Vrc .
? Vq is a set of placeholder nodes for q?units,
called q-nodes. They represent phrase triples.
? Esim ? Vp ? Vs is a set of weighted similarity
edges that capture the strength of the mapping
of a phrase to a semantic item.
? Ecoh ? Vs ? Vs is a set of weighted coherence
edges that capture the semantic coherence be-
tween two semantic items. Semantic coherence
is discussed in more detail later in this section.
? Eq ? Vq?Vp?d, where d ? {rel, arg1, arg2}
is a q-edge. Each such edge connects a place-
holder q-node to a p-node with a specific role
as a relation, or one of the two arguments. A
q-unit, as presented earlier, can be seen as a q-
node along with its outgoing q-edges.
Figure 3 shows the disambiguation graph for our
running example (excluding coherence edges be-
tween s-nodes).
4.2 Edge Weights
We next describe how the weights on similarity
edges and semantic coherence edges are defined.
383
q1
q
2
q
3
a writer
Casablanca
played
played in
Who
married
married to
is married to
was born
born
Rome
c:writer
r:bornIn
r:bornOnDate
e:Max_Born
e:Born_(film)
e:Sydne_Rome
r:Rome
e:White_House
e:Casablanca
e:Casablanca_(film)
e:Played_(film)
r:actedIn
r:hasMusicalRole
c:person
e:Married_(series)
c: married_person
r:marriedTo
q-nodes p-nodes
s-nodes
arg1 arg2
rel
Figure 3: Disambiguation graph for the running example.
4.2.1 Semantic Coherence
Semantic coherence, Cohsem, captures to what
extent two semantic items occur in the same context.
This is different from semantic similarity (Simsem),
which is usually evaluated using the distance be-
tween nodes in a taxonomy (Resnik, 1995). While
we expect Simsem(George Bush, Woody Allen) to
be higher than Simsem(Woody Allen, Terminator)
we would like Cohsem(Woody Allen, Terminator),
both of which are from the entertainment domain, to
be higher than Cohsem(George Bush, Woody Allen).
For Yago2, we characterize an entity e by its in-
links InLinks(e): the set of Yago2 entities whose
corresponding Wikipedia pages link to the entity.
To be able to compare semantic items of different
semantic types (entities, relations, and classes), we
need to extend this to classes and relations. For class
c with entities e, its inlinks are defined as follows:
InLinks(c) =
?
e?c Inlinks(e)
For relations, we only consider those that map en-
tities to entities (e.g. actedIn, produced), for which
we define the set of inlinks as follows:
InLinks(r) =
?
(e1,e2)?r
(InLinks(e1) ? InLinks(e2))
The intuition behind this is that when the two argu-
ments of an instance of the relation co-occur, then
the relation is being expressed.
We define the semantic coherence (Cohsem) be-
tween two semantic items s1 and s2 as the Jaccard
coefficient of their sets of inlinks.
4.2.2 Similarity Weights
Similarity weights are computed differently for
entities, classes, and relations. For entities, we use a
normalized prior score based on how often a phrase
refers to a certain entity in Wikipedia. For classes,
we use a normalized prior that reflects the number
of members in a class. Finally, for relations, similar-
ity reflects the maximum n-gram similarity between
the phrase and any of the relation?s surface forms.
We use Lucene for indexing and searching the rela-
tion surface forms.
4.3 Disambiguation Graph Processing
The result of disambiguation is a subgraph of the
disambiguation graph, yielding the most coherent
mappings. We employ an ILP to this end. Before
describing our ILP, we state some necessary defini-
tions:
? Triple dimensions: d ? {rel, arg1, arg2}
? Tokens: T = {t0, t1, ..., tn}.
? Phrases: P = {p0, p1, ..., pk}.
? Semantic items: S = {s0, s1, ..., sl}.
? Token occurrences: P(t) = {p ? P | t ? p}.
? Xi ? {0, 1} indicates if p-node i is selected.
? Yij ? {0, 1} indicates if p-node i maps to s-
node j.
? Zkl ? {0, 1} indicates if s-nodes k, l are both
selected so that their coherence edge matters.
? Qmnd ? {0, 1} indicates if the q-edge between
q-node m and p-node n for d is selected.
? Cj , Ej and Rj are {0, 1} constants indicating
if s-node j is a class, entity, or relation, resp.
? wij is the weight for a p?s similarity edge.
? vkl is the weight for an s?s semantic coherence
edge.
? trc ? {0, 1} indicates if the relation s-node r is
type-compatible with the concept s-node c.
Given the above definitions, our objective func-
tion is
maximize ?
?
i,j wijYij + ?
?
k,l vklZkl+
?
?
m,n,dQmnd
384
subject to the following constraints:
1. A p-node can be assigned to one s-node at most:
?
j Yij ? 1, ?i
2. If a p-s similarity edge is chosen, then the respec-
tive p-node must be chosen:
Yij ? Xi, ?j
3. If s-nodes k and l are chosen (Zkl = 1), then there
are p-nodes mapping to each of the s-nodes k and l
( Yik = 1 for some i and Yjl = 1 for some j):
Zkl ?
?
i Yik and Zkl ?
?
j Yjl
4. No token can appear as part of two phrases:
?
i?P(t)Xi ? 1, ?t ? T
5. At most one q-edge is selected for a dimension:
?
nQmnd ? 1, ?m, d
6. If the q-edge mnd is chosen (Qmnd = 1) then
p-node n must be selected:
Qmnd ? Xn, ?m, d
7. Each semantic triple should include a relation:
Er ? Qmn?d +Xn? + Yn?r ? 2 ?m,n?, r, d = rel
8. Each triple should have at least one class:
Cc1 + Cc2 ? Qmn??d1 +Xn?? + Yn???c1+
Qmn???d2 +Xn??? + Yn???c2 ? 5,
?m,n??, n???, r, c1, c2, d1 = arg1, d2 = arg2
This is not invoked for existential questions that
return Boolean answers and are translated to ASK
queries in SPARQL. An example is the question
?Did Tom Cruise act in Top Gun??, which can be
translated to ASK {Tom Cruise actedIn Top Gun}.
9. Type constraints are respected (through q-edges):
trc1 + trc2 ? Qmn?d1 +Xn? + Yn?r+
Qmn??d2 +Xn?? + Yn???c1+
Qmn???d3 +Xn??? + Yn???c2 ? 7
?m,n?, n??, n???, r, c1, c2,
d1 = rel, d2 = arg1, d3 = arg2
The above is a sophisticated ILP, and most likely
NP-hard. However, even with ten thousands of vari-
ables it is within the regime of modern ILP solvers.
In our experiments, we used Gurobi (Gur, 2011), and
achieved run-times ? typically of a few seconds.
q
1
q
2
q
3
a writer
Casablanca
played in
Who
is married to
was born
Rome
c:writer
r:bornIn
r:Rome
e:Casablanca
r:actedIn
c:person
r:marriedTo
q-nodes p-nodes
s-nodes
Figure 4: Computed subgraph for the running example.
Figure 4 shows the resulting subgraph for the dis-
ambiguation graph of Figure 3. Note how common
p-nodes between q-units capture joins.
5 Evaluation
5.1 Datasets
Our experiments are based on two collections of
questions: the QALD-1 task for question answer-
ing over linked data (QAL, 2011) and a collection
of questions used in (Elbassuoni et al2011; El-
bassuoni et al2009) in the context of the NAGA
project, for informative ranking of SPARQL query
answers (Elbassuoni et al2009) evaluated the
SPARQL queries, but the underlying questions are
formulated in natural language.) The NAGA collec-
tion is based on linking data from IMDB with the
Yago2 knowledge base. This is an interesting linked-
data case: IMDB provides data about movies, actors,
directors, and movie plots (in the form of descrip-
tive keywords and phrases); Yago2 adds semantic
types and relational facts for the participating enti-
ties. Yago2 provides nearly 3 million concepts and
100 relations, of which 41 lie within the scope of
our framework.
Typical example questions for these two col-
lections are: ?Which software has been published
by Mean Hamster Software?? for QALD-1, and
?Which director has won the Academy Award for
Best director and is married to an actress that has
won the Academy Award for Best Actress?? for
NAGA. For both collections, some questions are
out-of-scope for our setting, because they mention
entities or relations that are not available in the un-
derlying datasets, contain date or time comparisons,
or involve aggregation such as counting. After re-
385
moving these questions, our test set consists of 27
QALD-1 training questions out of a total of 50 and
44 NAGA questions, out of a total of 87. We used
the 19 questions from the QALD-1 test set that are
within the scope of our method for tuning the hyper-
parameters (?, ?, ?) in the ILP objective function.
5.2 Evaluation Metrics
We evaluated the output of DEANNA at three
stages in the processing pipeline: a) after the dis-
ambiguation of phrases, b) after the generation of
the SPARQL query, and c) after obtaining answers
from the underlying linked-data sources. This way,
we could obtain insights into our building blocks,
in addition to assessing the end-to-end performance.
In particular, we could assess the goodness of the
question-to-query translation independently of the
actual answer quality which may depend on partic-
ularities of the underlying datasets (e.g., slight mis-
matches between query terminology and the names
in the data.)
At each of the three stages, the output was shown
to two human assessors who judged whether an out-
put item was good or not. If the two were in dis-
agreement, then a third person resolved the judg-
ment.
For the disambiguation stage, the judges looked
at each q-node/s-node pair, in the context of the
question and the underlying data schemas, and de-
termined whether the mapping was correct or not
and whether any expected mappings were missing.
For the query-generation stage, the judges looked
at each triple pattern and determined whether the
pattern was meaningful for the question or not and
whether any expected triple pattern was missing.
Note that, because our approach does not use any
query templates, the same question may generate se-
mantically equivalent queries that differ widely in
terms of their structure. Hence, we rely on our eval-
uation metrics that are based on triple patterns, as
there is no gold-standard query for a given ques-
tion. For the query-answering stage, the judges were
asked to identify if the result sets for the generated
queries are satisfactory.
With these assessments, we computed over-
all quality measures by both micro-averaging and
macro-averaging. Micro-averaging aggregates over
all assessed items (e.g., q-node/s-node pairs or triple
patterns) regardless of the questions to which they
belong. Macro-averaging first aggregates the items
for the same question, and then averages the quality
measure over all questions.
For a question q and item set s in one of the stages
of evaluation, let correct(q, s) be the number of cor-
rect items in s, ideal(q) be the size of the ideal item
set and retrieved(q, s) be the number of retrieved
items, we define coverage and precision as follows:
cov(q, s) = correct(q, s)/ideal(q)
prec(q, s) = correct(q, s)/retrieved(q, s).
5.3 Results & Discussion
5.3.1 Disambiguation
Table 1 shows the results for disambiguation in
terms of macro and micro coverage and precision.
For both datasets, coverage is high as few mappings
are missing. We obtain perfect precision for QALD-
1 as no mapping that we generate is incorrect, while
for NAGA we generate few incorrect mappings.
5.3.2 Query Generation
Table 2 shows the same metrics for the generated
triple patterns. The results are similar to those for
disambiguation. Missing or incorrect triple patterns
can be attributed to (i) incorrect mappings in the dis-
ambiguation stage or (ii) incorrect detection of de-
pendencies between phrases despite having the cor-
rect mappings.
5.3.3 Question Answering
Table 3 shows the results for query answering.
Here, we attempt to generate answers to questions
by executing the generated queries over the datasets.
The table shows the number of questions for which
the system successfully generated SPARQL queries
(#queries), and among those, how many resulted
in satisfactory answers as judged by our evalua-
tors (#satisfactory). Answers were considered un-
satisfactory when: 1) the generated SPARQL query
was wrong, 2) the result set was empty due to the
incompleteness of the underlying knowledge base,
or 3) a small fraction of the result set was relevant
to the question. For both sets of questions, most of
the queries that were perceived unsatisfactory were
ones that returned no answers. Table 4 shows a
set of example QALD questions, the corresponding
SPARQL queries and sample answers.
386
Benchmark QALD-1 NAGA
covmacro 0.973 0.934
precmacro 1.000 0.934
covmicro 0.963 0.945
precmicro 1.000 0.941
Table 1: Disambiguation
Benchmark QALD-1 NAGA
covmacro 0.975 0.894
precmacro 1.000 0.941
covmicro 0.956 0.847
precmicro 1.000 0.906
Table 2: Query generation
Benchmark QALD-1 NAGA
#questions 27 44
#queries 20 41
#satisfactory 10 15
#relaxed +3 +3
Table 3: Query answering
Question Generated Query Sample Answers
1. Who was the wife of President
Lincoln?
?x marriedTo Abraham Lincoln .
?x type person
Mary Todd Lincoln
2. In which films did Julia Roberts
as well as Richard Gere play?
?x type movie . Richard Gere actedIn ?x .
Julia Roberts actedIn ?x
Runaway Bride
Pretty Woman
3. Which actors were born in
Germany?
?x type actor . ?x bornIn Germany NONE
Table 4: Example questions, the generated SPARQL queries and their answers
Queries that produced no answers, such as the
third query in Table 4 were further relaxed using an
incarnation of the techniques described in (Elbas-
suoni et al2009), by retaining the triple patterns
expressing type constraints and relaxing all other
triple patterns. Relaxing a triple pattern was done
by replacing all entities with variables and casting
entity mentions into keywords that are attached to
the relaxed triple pattern. For example, the QALD
question ?Which actors were born in Germany??
was translated into the following SPARQL query:
?x type actor . ?x bornIn Germany which pro-
duced no answers when run over the Yago2 knowl-
edge base since the relation bornIn relates peo-
ple to cities and not countries in Yago2. The
query was then relaxed into: ?x type actor . ?x
bornIn ?z[Germany]. This relaxed (and keyword-
augmented) triple-pattern query was then processed
the same way as triple-pattern queries without any
keywords. The results of such query were then
ranked based on how well they match the keyword
conditions specified in the relaxed query using the
ranking model in (Elbassuoni et al2009). Using
this technique, the top ranked results for the relaxed
query were all actors born in German cities as shown
in Table 5.
After relaxation, the judges again assessed the re-
sults of the relaxed queries and determined whether
they were satisfactory or not. The number of addi-
tional queries that obtained satisfactory answers af-
ter relaxation are shown under #relaxed in Table 3.
The evaluation data, in addition to a demonstra-
tion of our system (Yahya et al2012), can be found
at http://mpi-inf.mpg.de/yago-naga/deanna/.
6 Related Work
Question answering has a long history in NLP and
IR research. The Web and Wikipedia have proved to
be a valuable resource for answering fact-oriented
questions. State-of-the-art methods (Hirschman and
Gaizauskas, 2001; Kwok et al2001; Zheng, 2002;
Katz et al2007; Dang et al2007; Voorhees, 2003)
cast the user?s question into a keyword query to a
Web search engine (perhaps with phrases for loca-
tion and person names or other proper nouns). Key
to finding good results is to retrieve and rank sen-
tences or short passages that contain all or most key-
words and are likely to yield good answers. Together
with trained classifiers for the question type (and
thus the desired answer type), this methodology per-
forms fairly well for both factoid and list questions.
IBM?s Watson project (Ferrucci et al2010)
demonstrated a new kind of deep QA. A key ele-
ment in Watson?s approach is to decompose com-
plex questions into several cues and sub-cues,
with the aim of generating answers from matches
for the various cues (tapping into the Web and
Wikipedia). Knowledge bases like DBpedia (Auer
et al2007), Freebase (Bollacker et al2008), and
Yago (Suchanek et al2007)) are used for both an-
swering parts of questions that can be translated to
structured form (Chu-Carroll et al2012) and type-
checking possible answer candidates and thus filter-
ing out spurious results (Kalyanpur et al2011).
The recent QALD-1 initiative (QAL, 2011) pro-
posed a benchmark task to translate questions into
SPARQL queries over linked-data sources like DB-
pedia and MusicBrainz. FREyA (Damljanovic et
al., 2011), the best performing system, relies on
387
Q: ?x type actor . ?x wasBornIn ?z[Germany]
Martin Lawrence type actor . Martin Lawrence wasBornIn Frankfurt am Main
Robert Schwentke type actor . Robert Schwentke wasBornIn Stuttgart
Willy Millowitsch type actor . Willy Millowitsch wasBornIn Cologne
Jerry Zaks type actor . Jerry Zaks wasBornIn Stuttgart
Table 5: Top-4 results for the QALD question ?Which actors were born in Germany?? after relaxation
interaction with the user to interpret the question.
Earlier work on mapping questions into structured
queries includes the work by Frank et al2007) and
Unger and Cimiano (2011). Frank et al2007) used
lexical-conceptual templates for query generation.
However, this work did not address the crucial issue
of disambiguating the constituents of the question.
In Pythia, Unger and Cimiano (2011) relied on an
ontology-driven grammar for the question language
so that questions could be directly mapped onto the
vocabulary of the underlying ontology. Such gram-
mars are obviously hard to craft for very large, com-
plex, and evolving knowledge bases. Nalix is an at-
tempt to bring question answering to XML data (Li,
Yang, and Jagadish, 2007) by mapping questions to
XQuery expressions, relying on human interaction
to resolve possible ambiguity.
Very recently, Unger et al2012) developed a
template-based approach based on Pythia, where
questions are automatically mapped to structured
queries in a two step process. First, a set of query
templates are generated for a question, independent
of the knowledge base, determining the structure of
the query. After that, each template is instantiated
with semantic items from the knowledge base. This
performs reasonably well for the QALD-1 bench-
mark: out of 50 test questions, 34 could be mapped,
and 19 were correctly answered.
Efforts on user-friendly exploration of struc-
tured data include keyword search over relational
databases (Bhalotia et al2002) and structured key-
word search (Pound et al2010). The latter is a com-
promise between full natural language and struc-
tured queries, where the user provides the structure
and the system takes care of the disambiguation of
keyword phrases.
Our joint disambiguation method was inspired
by recent work on NED (Milne and Witten, 2008;
Kulkarni et al2009; Hoffart et al2011a) and
WSD (Navigli, 2009). In contrast to this prior work
on related problems, our graph construction and
constraints are more complex, as we address the
joint mapping of arbitrary phrases onto entities,
classes, or relations. Moreover, instead of graph al-
gorithms or factor-graph learning, we use an ILP for
solving the ambiguity problem. This way, we can ac-
commodate expressive constraints, while being able
to disambiguate all phrases in a few seconds.
DEANNA uses dictionaries of names and phrases
for entities, classes, and relations. Spitkovsky and
Chang (2012) recently released a huge dictionary of
pairs of phrases and Wikipedia links, derived from
Google?s Web index. For relations, Nakashole et al
(2012) released PATTY, a large taxonomy of pat-
terns with semantic types.
7 Conclusions and Future Work
We presented a method for translating natural-
language questions into structured queries. The nov-
elty of this method lies in modeling several map-
ping stages as a joint ILP problem. We harness type
signatures and other information from large-scale
knowledge bases. Although our model, in princi-
ple, leads to high combinatorial complexity, we ob-
served that the Gurobi solver could handle our ju-
diciously designed ILP very efficiently. Our experi-
mental studies showed very high precision and good
coverage of the query translation, and good results
in the actual question answers.
Future work includes relaxing some of the limita-
tions that our current approach still has. First, ques-
tions with aggregations cannot be handled at this
point. Second, queries sometimes return empty an-
swers although they perfectly capture the original
question, but the underlying data sources are incom-
plete or represent the relevant information in an un-
expected manner. We plan to extend our approach of
combining structured data with textual descriptions,
and generate queries that combine structured search
predicates with keyword or phrase matching.
388
References
Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyga-
niak, R.; and Ives, Z. G. 2007. DBpedia: A Nucleus
for a Web of Open Data. In ISWC/ASWC.
Bhalotia, G.; Hulgeri, A.; Nakhe, C.; Chakrabarti, S.; and
Sudarshan, S. 2002. Keyword Searching and Brows-
ing in Databases using BANKS. In ICDE.
Bollacker, K. D.; Evans, C.; Paritosh, P.; Sturge, T.; and
Taylor, J. 2008. Freebase: a Collaboratively Created
Graph Database for Structuring Human Knowledge.
In SIGMOD.
Chu-Carroll, J.; Fan, J.; Boguraev, B. K.; Carmel, D.; and
Sheinwald, D.; Welty, C. 2012. Finding needles in the
haystack: Search and candidate generation. In IBM J.
Res. & Dev., vol 56, no.3/4.
Damljanovic, D.; Agatonovic, M.; and Cunningham, H.
2011. FREyA: an Interactive Way of Querying Linked
Data using Natural Language.
Dang, H. T.; Kelly, D.; and Lin, J. J. 2007. Overview of
the trec 2007 question answering track. In TREC.
de Marneffe, M. C.; Maccartney, B.; and Manning, C. D.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
Elbassuoni, S.; Ramanath, M.; Schenkel, R.; Sydow, M.;
and Weikum, G. 2009. Language-model-based rank-
ing for queries on rdf-graphs. In CIKM.
Elbassuoni, S.; Ramanath, M.; and Weikum, G. 2011.
Query relaxation for entity-relationship search. In
ESWC.
Fader, A.; Soderland, S.; and Etzioni, O. 2011. Iden-
tifying relations for open information extraction. In
EMNLP.
Ferrucci, D. A.; Brown, E. W.; Chu-Carroll, J.; Fan, J.;
Gondek, D.; Kalyanpur, A.; Lally, A.; Murdock, J. W.;
Nyberg, E.; Prager, J. M.; Schlaefer, N.; and Welty,
C. A. 2010. Building Watson: An Overview of the
DeepQA Project. AI Magazine 31(3).
Frank, A.; Krieger, H.-U.; Xu, F.; Uszkoreit, H.; Crys-
mann, B.; Jo?rg, B.; and Scha?fer, U. 2007. Question
Answering from Structured Knowledge Sources. J.
Applied Logic 5(1).
Gurobi Optimization, Inc. 2012. Gurobi Optimizer Ref-
erence Manual. http://www.gurobi.com/.
Heath, T., and Bizer, C. 2011. Linked Data: Evolving
the Web into a Global Data Space. San Rafael, CA:
Morgan & Claypool, 1 edition.
Hirschman, L., and Gaizauskas, R. 2001. Natural Lan-
guage Question Answering: The View from Here. Nat.
Lang. Eng. 7.
Hoffart, J.; Mohamed, A. Y.; Bordino, I.; Fu?rstenau, H.;
Pinkal, M.; Spaniol, M.; Taneva, B.; Thaterm S.; and
Weikum, G. 2011. Robust Disambiguation of Named
Entities in Text. In EMNLP.
Hoffart, J.; Suchanek, F. M.; Berberich, K.; Lewis-
Kelham, E.; de Melo, G.; and Weikum, G. 2011.
Yago2: exploring and querying world knowledge in
time, space, context, and many languages. In WWW
(Companion Volume).
Kalyanpur, A.; Murdock, J. W.; Fan, J.; and Welty, C. A.
2011. Leveraging community-built knowledge for
type coercion in question answering. In International
Semantic Web Conference.
Katz, B.; Felshin, S.; Marton, G.; Mora, F.; Shen, Y. K.;
Zaccak, G.; Ammar, A.; Eisner, E.; Turgut, A.; and
Westrick, L. B. 2007. CSAIL at TREC 2007 Ques-
tion Answering. In TREC.
Kulkarni, S.; Singh, A.; Ramakrishnan, G.; and
Chakrabarti, S. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Kwok, C. C. T.; Etzioni, O.; and Weld, D. S. 2001. Scal-
ing Question Answering to the Web. In WWW.
Li, Y.; Yang, H.; and Jagadish, H. V. 2007. NaLIX:
A Generic Natural Language Search Environment for
XML Data. ACM Trans. Database Syst. 32(4).
Milne, D. N., and Witten, I. H. 2008. Learning to link
with wikipedia. In CIKM.
Ndapandula Nakashole, Gerhard Weikum and Fabian
Suchanek 2012. PATTY: A Taxonomy of Relational
Patterns with Semantic Types. In EMNLP.
Navigli, R. 2009. Word sense disambiguation: A survey.
ACM Comput. Surv. 41(2).
Pound, J.; Ilyas, I. F.; and Weddell, G. E. 2010. Ex-
pressive and Flexible Access to Web-extracted Data: A
Keyword-based Structured Query Language. In SIG-
MOD.
2011. 1st Workshop on Question Answering over
Linked Data (QALD-1). http://www.sc.cit-ec.uni-
bielefeld.de/qald-1.
Resnik, P. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. In IJCAI.
Spitkovsky, V. I. Spitkovsky; Chang, A. X. ; 2012. A
Cross-Lingual Dictionary for English Wikipedia Con-
cepts. In LREC.
Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007.
Yago: a core of semantic knowledge. In WWW.
Tummarello, G.; Cyganiak, R.; Catasta, M.; Danielczyk,
S.; Delbru, R.; and Decker, S. 2010. Sig.ma: Live
views on the web of data. J. Web Sem. 8(4).
Unger, C.; and Cimiano, P. 2011. Pythia: Compositional
Meaning Construction for Ontology-Based Question
Answering on the Semantic Web. In NLDB.
Unger, C.; Bu?hmann, L.; Lehmann, J.; Ngonga Ngomo,
A.-C.; Gerber, D.; and Cimiano, P. 2012. Template-
based question answering over RDF data. In WWW.
Voorhees, E. M. 2003. Overview of the trec 2003 ques-
tion answering track. In TREC.
389
Yahya, M.; Berberich, K.; Elbassuoni, S.; Ramanath, M.;
Tresp, V.; and Weikum, G. 2012. Deep answers
for naturally asked questions on the web of data. In
WWW.
Zheng, Z. 2002. AnswerBus Question Answering Sys-
tem. In HLT.
390
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1135?1145, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
PATTY: A Taxonomy of Relational Patterns with Semantic Types
Ndapandula Nakashole, Gerhard Weikum, Fabian Suchanek
Max Planck Institute for Informatics
Saarbru?cken, Germany
{nnakasho,weikum,suchanek}@mpi-inf.mpg.de
Abstract
This paper presents PATTY: a large resource
for textual patterns that denote binary relations
between entities. The patterns are semanti-
cally typed and organized into a subsumption
taxonomy. The PATTY system is based on ef-
ficient algorithms for frequent itemset mining
and can process Web-scale corpora. It har-
nesses the rich type system and entity popu-
lation of large knowledge bases. The PATTY
taxonomy comprises 350,569 pattern synsets.
Random-sampling-based evaluation shows a
pattern accuracy of 84.7%. PATTY has 8,162
subsumptions, with a random-sampling-based
precision of 75%. The PATTY resource
is freely available for interactive access and
download.
1 Introduction
Motivation. WordNet (Fellbaum 1998) is one of the
most widely used lexical resources in computer sci-
ence. It groups nouns, verbs, and adjectives into sets
of synonyms, and arranges these synonyms in a tax-
onomy of hypernyms. WordNet is limited to single
words. It does not contain entire phrases or pat-
terns. For example, WordNet does not contain the
pattern X is romantically involved with Y. Just like
words, patterns can be synonymous, and they can
subsume each other. The pattern X is romantically
involved with Y is synonymous with the pattern X is
dating Y. Both are subsumed by X knows Y. Patterns
for relations are a vital ingredient for many appli-
cations, including information extraction and ques-
tion answering. If a large-scale resource of relational
patterns were available, this could boost progress in
NLP and AI tasks.
Yet, existing large-scale knowledge bases are
mostly limited to abstract binary relationships be-
tween entities, such as ?bornIn? (Auer 2007; Bol-
lacker 2008; Nastase 2010; Suchanek 2007). These
do not correspond to real text phrases. Only the Re-
Verb system (Fader 2011) yields a larger number of
relational textual patterns. However, no attempt is
made to organize these patterns into synonymous
patterns, let ale into a taxonomy. Thus, the pat-
terns themselves do not exhibit semantics.
Goal. Our goal in this paper is to systematically
compile relational patterns from a corpus, and to im-
pose a semantically typed structure on them. The
result we aim at is a WordNet-style taxonomy of
binary relations. In particular, we aim at patterns
that contain semantic types, such as ?singer? sings
?song?. We also want to automatically generalize
syntactic variations such as sings her ?song? and
sings his ?song?, into a more general pattern sings
[prp] ?song? with POS tag [prp]. Analogously but
more demandingly, we want to automatically infer
that the above patterns are semantically subsumed
by the pattern ?musician? performs on ?musical
composition? with more general types for the entity
arguments in the pattern.
Compiling and organizing such patterns is chal-
lenging for the following reasons. 1) The number
of possible patterns increases exponentially with the
length of the patterns. For example, the string ?Amy
sings ?Rehab?? can give rise to the patterns ?singer?
sings ?song?, ?person? sings ?artifact?, ?person?
[vbz] ?entity?, etc. If wildcards for multiple words
are allowed (such as in ?person? sings * ?song?), the
number of possible patterns explodes. 2) A pattern
1135
can be semantically more general than another pat-
tern (when one relation is implied by the other re-
lation), and it can also be syntactically more gen-
eral than another pattern (by the use of placehold-
ers such as [vbz]). These two subsumption orders
have a non-obvious interplay, and none can be ana-
lyzed without the other. 3) We have to handle pattern
sparseness and coincidental matches. If the corpus
is small, e.g., the patterns ?singer? later disliked her
song ?song? and ?singer? sang ?song?, may apply to
the same set of entity pairs in the corpus. Still, the
patterns are not synonymous. 4) Computing mutual
subsumptions on a large set of patterns may be pro-
hibitively slow. Moreover, due to noise and vague
semantics, patterns may even not form a crisp tax-
onomy, but require a hierarchy in which subsump-
tion relations have to be weighted by statistical con-
fidence measures.
Contributions. In this paper, we present PATTY, a
large resource of relational patterns that are arranged
in a semantically meaningful taxonomy, along with
entity-pair instances. More precisely, our contribu-
tions are as follows:
1) SOL patterns: We define an expressive fam-
ily of relational patterns, which combines syntac-
tic features (S), ontological type signatures (O), and
lexical features (L). The crucial novelty is the addi-
tion of the ontological, semantic dimension to pat-
terns. When compared to a state-of-the-art pattern
language, we found that SOL patterns yield higher
recall while achieving similar precision.
2) Mining algorithms: We present efficient and
scalable algorithms that can infer SOL patterns and
subsumptions at scale, based on instance-level over-
laps and an ontological type hierarchy.
3) A large Lexical resource:. On the Wikipe-
dia corpus, we obtained 350,569 pattern synsets
with 84.7% precision. We make our pat-
tern taxonomy available for further research at
www.mpi-inf.mpg.de/yago-naga/patty/ .
The paper is structured as follows. Section 2 dis-
cusses related work. Section 3 outlines the basic
machinery for pattern extraction. Section 4 intro-
duces our SOL pattern model. Sections 5 and 6
present the syntactic and semantic generalization of
patterns. Section 7 explains how to arrange the pat-
terns into a taxonomy. Section 8 reports our experi-
mental findings.
2 Related Work
A wealth of taxonomic knowledge bases (KBs)
about entities and their semantic classes have be-
come available. These are very rich in terms of
unary predicates (semantic classes) and their entity
instances. However, the number of binary relations
(i.e., relation types, not instances) in these KBs is
usually small: Freebase (Bollacker 2008) has a few
thousand hand-crafted relations. WikiNet (Nastase
2010) has automatically extracted ca. 500 relations
from Wikipedia category names. DBpedia (Auer
2007) has automatically compiled ca. 8000 names
of properties from Wikipedia infoboxes, but these
include many involuntary semantic duplicates such
as surname and lastname. In all of these projects,
the resource contains the relation names, but not the
natural language patterns for them. The same is true
for other projects along these lines (Navigli 2010;
Philpot 2008; Ponzetto 2007; Suchanek 2007).
In contrast, knowledge base projects that auto-
matically populate relations from Web pages also
learn surface patterns for the relations: examples
are TextRunner/ReVerb (Banko 2007; Fader 2011),
NELL (Carlson 2010; Mohamed11), Probase (Wu
2011), the dynamic lexicon approach by (Hoffmann
2010; Wu 2008), the LDA-style clustering approach
by (Yao 2011), and projects on Web tables (Li-
maye 2010; Venetis 2011). Of these, only TextRun-
ner/ReVerb and NELL have made large pattern col-
lections publicly available.
ReVerb (Fader 2011) constrains patterns to verbs
or verb phrases that end with prepositions, while
PATTY can learn arbitrary patterns. More impor-
tantly, all methods in the TextRunner/ReVerb family
are blind to the ontological dimension of the enti-
ties in the patterns. Therefore, there is no notion of
semantic typing for relation phrases as in PATTY.
NELL (Carlson 2010) is based on a fixed set
of prespecified relations with type signatures, (e.g.,
personHasCitizenship: ?person? ? ?country?), and
learns to extract suitable noun-phrase pairs from a
large Web corpus. In contrast, PATTY discovers pat-
terns for relations that are a priori unknown.
1136
In OntExt (Mohamed11), the NELL architecture
was extended to automatically compute new re-
lation types (beyond the prespecified ones) for a
given type signature of arguments, based on a clus-
tering technique. For example, the relation mu-
sicianPlaysInstrument is found by clustering pat-
tern co-occurrences for the noun-phrase pairs that
fall into the specific type signature ?musician? ?
?musicinstrument?. This technique works for one
type signature at a time, and does not scale up to
mining a large corpus. Also, the technique is not
suitable for inferring semantic subsumptions. In
contrast, PATTY efficiently acquires patterns from
large-scale corpora and organizes them into a sub-
sumption hierarchy.
Class-based attribute discovery is a special case
of mining relational patterns (e.g., (Alfonseca 2010;
Pasca 2007; Pasca 2008; Reisinger 2009)). Given a
semantic class, such as movies or musicians, the task
is to determine relevant attributes, such as cast and
budget for movies, or albums and biography for mu-
sicians, along with their instances. Unlike PATTY?s
patterns, the attributes are not typed. They come
with a prespecified type for the domain, but without
any type for the range of the underlying relation.
There are further relation-centric tasks in NLP
and text mining that have commonalities with our
endeavor, but differ in fundamental ways. The
SemEval-2010 task on classification of semantic re-
lations between noun-phrase pairs (Hendrickx 2010)
aimed at predicting the relation for a given sentence
and pair of nominals, but used a fixed set of prespec-
ified relations. Another task in this research avenue
is to characterize and predict the argument types for
a given relation or pattern (Kozareva 2010; Nakov
2008). This is closer to KB population and less re-
lated to our task of discovering relational patterns
and systematically organizing them.
From a linguistic perspective, there is ample
work on patterns for unary predicates of the form
class(entity). This includes work on entailment of
classes, i.e., on is-a and subclassOf relationships.
Entailment among binary predicates of the form re-
lation(entity1, entity2) has received less attention
(Lin 2001; Chklovski 2004; Hashimoto 2009; Be-
rant 2011). These works focus solely on verbs, while
PATTY learns arbitrary phrases for patterns.
Several lexical resources capture verb categories
and entailment: WordNet 3.0 (Fellbaum 1998) con-
tains about 13,000 verb senses, with troponymy and
entailment relations; VerbNet (Kipper 2008) is a hi-
erarchical lexicon with more than 5,000 verb senses
in ca. 300 classes, including selectional preferences.
Again, all of these resources focus solely on verbs.
ConceptNet 5.0 (Havasi 2007) is a thesaurus of
commonsense knowledge built as a crowdsourcing
endeavor. PATTY, in contrast, is constructed fully
automatically from large corpora. Automatic learn-
ing of paraphrases and textual entailment has re-
ceived much attention (see the survey of (Androut-
sopoulos 2010)), but does not consider fine-grained
typing for binary relations, as PATTY does.
3 Pattern Extraction
This section explains how we obtain basic textual
patterns from the input corpus. We first apply the
Stanford Parser (Marneffe 2006) to the individual
sentences of the corpus to obtain dependency paths.
The dependency paths form a directed graph, with
words being nodes and dependencies being edges.
For example, the sentence ?Winehouse effortlessly
performed her song Rehab.? yields the following de-
pendency paths:
nsubj(performed-3, Winehouse-1)
advmod(performed-3, effortlessly-2)
poss(Rehab-6, her-4)
nn(Rehab-6, song-5)
dobj(performed-3, Rehab-6)
While our method also works with patterns obtained
from shallow features such as POS tags, we found
that dependency paths improve pattern extraction
precision especially on long sentences.
We then detect mentions of named entities in the
parsed corpus. For this purpose, we use a dictio-
nary of entities. This can be any resource that con-
tains named entities with their surface names and se-
mantic types (Auer 2007; Suchanek 2007; Hoffart
2011; Bollacker 2008). In our experiments, we used
the YAGO2 knowledge base (Hoffart 2011). We
match noun phrases that contain at least one proper
noun against the dictionary. For disamiguation, we
1137
use a simple context-similarity prior, as described
in (Suchanek 2009). We empirically found that this
technique has accuracy well above 80% (and higher
for prominent and thus frequently occurring enti-
ties). In our example, the entity detection yields the
entities Amy Winehouse and Rehab (song).
Whenever two named entities appear in the same
sentence, we extract a textual pattern. For this pur-
pose, we traverse the dependency graph to get the
shortest path that connects the two entities. In the
example, the shortest path between ?Winehouse?
and ?Rehab? is: Winehouse nsubj performed dobj
Rehab. In order to capture only relations that refer
to subject-relation-object triples, we only consider
shortest paths that start with subject-like dependen-
cies, such as nsubj, rcmod and partmod. To re-
flect the full meaning of the patterns, we expand the
shortest path with adverbial and adjectival modifiers,
for example the advmod dependency. The sequence
of words on the expanded shortest path becomes our
final textual pattern. In the example, the textual pat-
tern is Amy Winehouse effortlessly performed Rehab
(song).
4 SOL Pattern Model
Textual patterns are tied to the particular surface
form of the text. Therefore, we transform the textual
patterns into a new type of patterns, called syntactic-
ontologic-lexical patterns (SOL patterns). SOL pat-
terns extend lexico-syntactic patterns by ontological
type signatures for entities. The SOL pattern lan-
guage is expressive enough to capture fine-grained
relational patterns, yet simple enough to be dealt
with by efficient mining algorithms at Web scale.
A SOL pattern is an abstraction of a textual pat-
tern that connects two entities of interest. It is a
sequence of words, POS-tags, wildcards, and onto-
logical types. A POS-tag stands for a word of the
part-of-speech class. We introduce the special POS-
tag [word], which stands for any word of any POS
class. A wildcard, denoted ?, stands for any (pos-
sibly empty) sequence of words. Wildcards are es-
sential to avoid overfitting of patterns to the corpus.
An ontological type is a semantic class name (such
as ?singer?) that stands for an instance of that class.
Every pattern contains at least two types, and these
are designated as entity placeholders.
A string and a pattern match, if there is an order-
preserving bijection from sequences of words in the
string to items in the pattern, so that each item can
stand for the respective sequence of words. For ex-
ample, the pattern ?person??s [adj] voice * ?song?
matches the strings ?Amy Winehouse?s soft voice
in ?Rehab?? and ?Elvis Presley?s solid voice in his
song ?All shook up??. The type signature of a pat-
tern is the pair of the entity placeholders. In the ex-
ample, the type signature is person ? song. The
support set of a pattern is the set of pairs of entities
that appear in the place of the entity placeholders
in all strings in the corpus that match the pattern.
In the example, the support set of the pattern could
be {(Amy,Rehab), (Elvis, AllShookUp)}. Each
pair is called a support pair of the pattern.
Pattern B is syntactically more general than pat-
tern A if every string that matches A also matches
B. Pattern B is semantically more general than A
if the support set of B is a superset of the support
set of A. If A is semantically more general than B
and B is semantically more general than A, the pat-
terns are called synonymous. A set of synonymous
patterns is called a pattern synset. Two patterns, of
which neither is semantically more general than the
other, are called semantically different.
To generate SOL patterns from the textual pat-
terns, we decompose the textual patterns into n-
grams (n consecutive words). A SOL pattern con-
tains only the n-grams that appear frequently in the
corpus and the remaining word sequences are re-
placed by wildcards. For example, in the sentence
?was the first female to run for the governor of?
might give rise to the pattern * the first female * gov-
ernor of, if ?the first female? and ?governor of? are
frequent in the corpus.
To find the frequent n-grams efficiently, we apply
the technique of frequent itemset mining (Agrawal
1993; Srikant 1996): each sentence is viewed as a
?shopping transaction? with a ?purchase? of several
n-grams, and the mining algorithm computes the n-
gram combinations with large co-occurrence sup-
port1. These n-grams allow us to break down a sen-
1Our implementation restricts n-grams to length 3 and uses
up to 4 n-grams per sentence
1138
tence into wildcard-separated subsequences, which
yields an SOL pattern. We generate multiple pat-
terns with different types, one for each combination
of types that the detected entities have in the under-
lying ontology.
We quantify the statistical strength of a pattern by
means of its support set. For a given pattern p with
type signature t1 ? t2, the support of p is the size
of its support set. For confidence, we compare the
support-set sizes of p and an untyped variant pu of
p, in which the types ?t1? and ?t2? are replaced by
the generic type ?entity?. We define the confidence
of p as the ratio of the support-set sizes of p and pu.
5 Syntactic Pattern Generalization
Almost every pattern can be generalized into a syn-
tactically more general pattern in several ways: by
replacing words by POS-tags, by introducing wild-
cards (combining more n-grams), or by generaliz-
ing the types in the pattern. It is not obvious which
generalizations will be reasonable and useful. We
observe, however, that generalizing a pattern may
create a pattern that subsumes two semantically dif-
ferent patterns. For example, the generalization
?person? [vb] ?person? subsumes the two semanti-
cally different patterns ?person? loves ?person? and
?person? hates ?person?. This means that the pattern
is semantically meaningless.
Therefore, we proceed as follows. For every pat-
tern, we generate all possible generalizations. If a
generalization subsumes multiple patterns with dis-
joint support sets, we abandon the generalized pat-
tern. Otherwise, we add it to our set of patterns.
6 Semantic Pattern Generalization
The main difficulty in generating semantic subsump-
tions is that the support sets may contain spurious
pairs or be incomplete, thus destroying crisp set in-
clusions. To overcome this problem, we designed
a notion of a soft set inclusion, in which one set S
can be a subset of another set B to a certain degree.
One possible measure for this degree is the confi-
dence, i.e., the ratio of elements in S that are in B,
deg(S ? B) = |S ? B|/|S|. However, if a support
set S has only few elements due to sparsity, it may
become a subset of another support setB, even if the
two patterns are semantically different. Therefore,
one has to take into account also the support, i.e., the
size of the set S. Traditionally, this is done through a
weighted trade-off between confidence and support.
To avoid the weight tuning, we instead devised
a probabilistic model. We interpret S as a random
sample from the ?true? support set S? that the pattern
would have on an infinitely large corpus. We want
to estimate the ratio of elements of S? that are in
B. This ratio is a Bernoulli parameter that can be
estimated from the ratio of elements of the sample S
that are in B. We compute the Wilson score interval
[c ? d, c + d] (Brown 2001) for the sample. This
interval guarantees that with a given probability (set
a priori, usually to ? = 95%), the true ratio falls into
the interval [c ? d, c + d]. If the sample is small, d
is large and c is close to 0.5. If the sample is large,
d decreases and c approaches the naive estimation
|S ? B|/|S|. Thereby, the Wilson interval center
naturally balances the trade-off between confidence
and the support. Hence we define deg(S ? B) = c.
This estimator may degrade when the sample size
is too small We can alternatively use a conservative
estimator deg(S ? B) = c?d, i.e., the lower bound
of the Wilson score interval. This gives a low score
to the case where S ? B if we have few samples (S
is small).
7 Taxonomy Construction
We now have to arrange the patterns in a semantic
taxonomy. A baseline solution would compare ev-
ery pattern support set to every other pattern support
set in order to determine inclusion, mutual inclusion,
or independence. This would be prohibitively slow.
For this reason, we make use of a prefix-tree for fre-
quent patterns (Han 2005). The prefix-tree stores
support sets of patterns. We then developed an algo-
rithm for obtaining set intersections from the prefix-
tree.
7.1 Prefix-Tree Construction
Suppose we have pattern synsets and their support
sets as shown in Table 1. An entity pair in a support
set is denoted by a letter. For example, in the sup-
port set for the pattern ?Politican? was governor
of ?State?, the entry ?A,80? may denote the entity
1139
ID Pattern Synset & Support Sets
P1 ?Politician? was governor of ?State?
A,80 B,75 C,70
P2 ?Politician? politician from ?State?
A,80 B,75 C,70 D,66 E,64
P3 ?Person? daughter of ?Person?
F,78 G,75 H,66
P4 ?Person? child of ?Person?
I,88 J,87 F,78 G,75 K,64
Table 1: Pattern Synsets and their Support Sets
Root 
A p1,p2 
B 
C 
D 
p1,p2 
p1,p2 
p2 
E p2 
F 
G 
H 
p3 I 
J 
F 
p4 
G p4 
K p4 
p4 
p4 p3 p3 
Figure 1: Prefix-Tree for the Synsets in Table 1.
pair Arnold Schwarzenegger, California, with an oc-
currence frequency 80. The contents of the support
sets are used to construct a prefix-tree, where nodes
are entity pairs. If synsets have entity pairs in com-
mon, they share a common prefix; thus the shared
parts can be represented by one prefix-path in the
tree. This enables subsumptions to be directly ?read
off? from the tree, while representing the tree in a
compact manner. To increase the chance of shared
prefixes, entity pairs are inserted into the tree in de-
creasing order of occurrence frequency.
The prefix-tree of support sets is a prefix-tree aug-
mented with synset information stored at the nodes.
Each node (entity pair) stores the identifiers of the
pattern sysnets whose support sets contain that en-
tity pair. In addition, each node stores a link to the
next node with the same entity pair.
Figure 1 shows the tree for the pattern synsets
in Table 1. The left-most path contains synsets P1
and P2. The two patterns have a prefix in common,
thus they share the same path. This is reflected by
the synsets stored in the nodes in the path. Synsets
P2 and P3 belong to two different paths due to dis-
similar prefixes although they have common nodes.
Instead, their common nodes are connected by the
same-entity-pair links shown as dotted lines in Fig-
ure 1. These links are created whenever the entity
pair already exists in the tree but with a prefix differ-
ent from the prefix of the synset being added to the
tree. The size of the tree is at most the total num-
ber of entity pairs making up the supports sets of the
synsets. The height of the tree is at most the size of
the the largest support set.
7.2 Mining Subsumptions from the Prefix-Tree
To efficiently mine subsumptions from the prefix-
tree, we have to avoid comparing every path to every
other path as this introduces the same inefficiencies
that the baseline approach suffers from.
From the construction of the tree it follows that
for any node Ni in the tree, all paths containing Ni
can be found by following node Ni?s links includ-
ing the same-entity-pair links. By traversing the en-
tire path of a synset Pi, we can reach all the pattern
synsets sharing common nodes with Pi. This leads
to our main insight: if we start traversing the tree
bottom up, starting at the last node in P ?is support
set, we can determine exactly which paths are sub-
sumed by Pi. Traversing the tree this way for all
patterns gives us the sizes of the support set intersec-
tion. The determined intersection sizes can then be
used in the Wilson estimator to determine the degree
of semantic subsumption and semantic equivalence
of patterns.
7.3 DAG Construction
Once we have generated subsumptions between re-
lational patterns, there might be cycles in the graph
we generate. We ideally want to remove the minimal
total number of subsumptions whose removal results
in an a directed acyclic graph (DAG). This task is
related to the minimum feedback-arc-set problem:
given a directed graph, we want to remove the small-
est set of edges whose removal makes the remaining
graph acyclic. This is a well known NP-hard prob-
lem (Kann 1992). We use a greedy algorithm for
1140
removing cycles and eliminating redundancy in the
subsumptions, thus effectively constructing a DAG.
Starting with a list of subsumption edges ordered by
decreasing weights, we construct the DAG bottom-
up by adding the highest-weight subsumption edge.
This step is repeated for all subsumptions, where we
add a subsumption to the DAG only if it does not
introduce cycles or redundancy. Redundancy occurs
when there already exists a path, by transitivity of
subsumptions, between pattern synsets linked by the
subsumption. This process finally yields a DAG of
pattern synsets ? the PATTY taxonony.
8 Experimental Evaluation
8.1 Setup
The PATTY extraction and mining algorithms were
run on two different input corpora: the New York
Times archive (NYT) which includes about 1.8 Mil-
lion newspaper articles from the years 1987 to 2007,
and the English edition of Wikipedia (WKP), which
contains about 3.8 Million articles (as of June 21,
2011). Experiments were carried out, for each cor-
pus, with two different type systems: a) the type sys-
tem of YAGO2, which consists of about 350,000 se-
mantic classes from WordNet and the Wikipedia cat-
egory system, and b) the two-level domain/type hier-
archy of Freebase which consists of 85 domains and
a total of about 2000 types within these domains.
All relational patterns and their respective entity
pairs are stored in a MongoDB database. We evalu-
ated PATTY along four dimensions: quality of pat-
terns, quality of subsumptions, coverage, and de-
sign alternatives. These dimensions are discussed
in the following four subsections. We also per-
formed an extrinsic study to demonstrate the use-
fulness of PATTY for paraphrasing the relations
of DBpedia and YAGO2. In terms of runtimes,
he most expensive part is the pattern extraction,
where we identify pattern candidates through de-
pendency parsing and perform entity recognition
on the entire corpus. This phase runs about a
day for Wikipedia a cluster. All other phases of
the PATTY system take less than an hour. All
experimental data is available on our Web site at
www.mpi-inf.mpg.de/yago-naga/patty/.
8.2 Precision of Relational Patterns
To assess the precision of the automatically mined
patterns (patterns in this section always mean pattern
synsets), we sampled the PATTY taxonomy for each
combination of input corpus and type system. We
ranked the patterns by their statistical strength (Sec-
tion 4), and evaluated the precision of the top 100
pattern synsets. Several human judges were shown
a sampled pattern synset, its type signature, and a
few example instances, and then stated whether the
pattern synset indicates a valid relation or not. Eval-
uators checked the correctness of the type signature,
whether the majority of patterns in the synset is rea-
sonable, and whether the instances seem plausible.
If so, the synset was flagged as meaningful. The re-
sults of this evaluation are shown in column four of
Table 2, with a 0.9-confidence Wilson score inter-
val (Brown 2001). In addition, the same assessment
procedure was applied to randomly sampled synsets,
to evaluate the quality in the long tail of patterns.
The results are shown in column five of Table 2. For
the top 100 patterns, we achieve above 90% preci-
sion for Wikipedia, and above 80% for 100 random
samples.
Corpus Types Patterns Top 100 Random
NYT
YAGO2 86,982 0.89?0.06 0.72?0.09
Freebase 809,091 0.87 ?0.06 0.71?0.09
WKP
YAGO2 350,569 0.95?0.04 0.85?0.07
Freebase 1,631,531 0.93?0.05 0.80?0.08
Table 2: Precision of Relational Patterns
From the results we make two observations. First,
Wikipedia patterns have higher precision than those
from the New York Times corpus. This is because
some the language in the news corpus does not ex-
press relational information; especially the news on
stock markets produced noisy patterns picked up by
PATTY. However, we still manage to have a preci-
sion of close to 90% for the top 100 patterns and
around 72% for random sample on the NYT cor-
pus. The second observation is that the YAGO2
type system generally led to higher precision than
the Freebase type system. This is because YAGO2
has finer grained, ontologically clean types, whereas
Freebase has broader categories with a more liberal
1141
assignment of entities to categories.
8.3 Precision of Subsumptions
We evaluated the quality of the subsumptions by
assessing 100 top-ranked as well as 100 randomly
selected subsumptions. As shown in Table 3, a
large number of the subsumptions are correct. The
Wikipedia-based PATTY taxonomy has a random-
sampling-based precision of 75%.
Corpus Types # Edges Top 100 Random
NYT
YAGO2 12,601 0.86?0.07 0.68?0.09
Freebase 80,296 0.89?0.06 0.41?0.09
WKP
YAGO2 8,162 0.83?0.07 0.75?0.07
Freebase 20,339 0.85?0.07 0.62?0.09
Table 3: Quality of Subsumptions
Example subsumptions from Wikipedia are:
? ?person? nominated for ?award? =
?person? winner of ?award?
? ?person? ? s wife ?person? =
?person? ?s widow ?person?
8.4 Coverage
To evaluate the coverage of PATTY, we would need
a complete ground-truth resource that contains all
possible binary relations between entities. Unfor-
tunately, there is no such resource2. We tried to
approximate such a resource by manually compil-
ing all binary relations between entities that ap-
pear in Wikipedia articles of a certain domain. We
chose the domain of popular music, because it offers
a plethora of non-trivial relations (such as addict-
edTo(person,drug), coveredBy(musician,musician),
dedicatedSongTo(musician,entity))). We considered
the Wikipedia articles of five musicians (Amy Wine-
house, Bob Dylan, Neil Young, John Coltrane, Nina
Simone). For each page, two annotators hand-
extracted all relationship types that they would spot
in the respective articles. The annotators limited
themselves to relations where at least one argument
type is ?musician?. Then we formed the intersection
of the two annotators? outputs (i.e., their agreement)
2Lexical resources such as WordNet contain only verbs, but
not binary relations such as is the president of. Other resources
are likely incomplete.
as a reasonable gold standard for relations identifi-
able by skilled humans. In total, the gold-standard
set contains 163 relations.
We then compared our relational patterns to the
relations included in four major knowledge bases,
namely, YAGO2, DBpedia (DBP), Freebase (FB),
and NELL, limited to the specific domain of music.
Table 4 shows the absolute number of relations cov-
ered by each resource. For PATTY, the patterns were
derived from the Wikipedia corpus with the YAGO2
type system.
gold standard PATTY YAGO2 DBP FB NELL
163 126 31 39 69 13
Table 4: Coverage of Music Relations
PATTY covered 126 of the 163 gold-standard re-
lations. This is more than what can be found in large
semi-curated knowledge bases such as Freebase,
and twice as much as Wikipedia-infobox-based re-
sources such as DBpedia or YAGO offer. Some
PATTY examples that do not appear in the other re-
sources at all are:
? ?musician? PRP idol ?musician? for the relation
hasMusicalIdol
? ?person? criticized by ?organization? for
critizedByMedia
? ?person? headliner ?artifact? for headlinerAt
? ?person? successfully sued ?person? for suedBy
? ?musician? wrote hits for ?musician? for wrote-
HitsFor,
This shows (albeit anecdotically) that PATTY?s pat-
terns contribute added value beyond today?s knowl-
edge bases.
8.5 Pattern Language Alternatives
We also investigated various design alternatives to
the PATTY pattern language. We looked at three
main alternatives: the first is verb-phrase-centric
patterns advocated by ReVerb (Fader 2011), the sec-
ond is the PATTY language without type signatures
(just using sets of n-grams with syntactic general-
izations), and the third one is the full PATTY lan-
guage. The results for the Wikipedia corpus and the
1142
Reverb-style patterns PATTY without types PATTY full
# Patterns 5,996 184,629 350,569
Patterns Precision 0.96?0.03 0.74?0.08 0.95?0.04
# Subsumptions 74 15,347 8,162
Subsumptions Precision 0.79 ?0.09 0.58?0.09 0.83?0.07
# Facts 192,144 6,384,684 3,890,075
Facts Precision. 0.86 ?0.07 0.64?0.09 0.88 ?0.06
Table 5: Results for Different Pattern Language Alternatives
Relation Paraphrases Precision Sample Paraphrases
DBPedia/artist 83 0.96?0.03 [adj] studio album of, [det] song by . . .
DBPedia/associatedBand 386 0.74?0.11 joined band along, plays in . . .
DBPedia/doctoralAdvisor 36 0.558?0.15 [det] student of, under * supervision . . .
DBPedia/recordLabel 113 0.86?0.09 [adj] artist signed to, [adj] record label . . .
DBPedia/riverMouth 31 0.83?0.12 drains into, [adj] tributary of . . .
DBPedia/team 1,108 0.91?0.07 be * traded to, [prp] debut for . . .
YAGO/actedIn 330 0.88?0.08 starred in * film, [adj] role for . . .
YAGO/created 466 0.79?0.10 founded, ?s book . . .
YAGO/isLeaderOf 40 0.53?0.14 elected by, governor of . . .
YAGO/holdsPoliticalPosition 72 0.73?0.10 [prp] tenure as, oath as . . .
Table 6: Sample Results for Relation Paraphrasing
YAGO2 type system are shown in Table 5; preci-
sion figures are based on the respective top 100 pat-
terns or subsumption edges. We observe from these
results that the type signatures are crucial for pre-
cision. Moreover, the number of patterns, subsump-
tions and facts found by verb-phrase-centric patterns
(ReVerb (Fader 2011)), are limited in recall. Gen-
eral pattern synsets with type signatures, as newly
pursued in this paper, substantially outperform the
verb-phrase-centric alternative in terms of pattern
and subsumption recall while yielding high preci-
sion.
8.6 Extrinsic Study: Relation Paraphrasing
To further evaluate the usefulness of PATTY, we per-
formed a study on relation paraphrasing: given a re-
lation from a knowledge base, identify patterns that
can be used to express that relation. Paraphrasing
relations with high-quality patterns is important for
populating knowledge bases and counters the prob-
lem of semantic drifting caused by ambiguous and
noisy patterns.
We considered relations from two knowledge
bases, DBpedia and YAGO2, focusing on relations
that hold between entities and do not include literals.
PATTY paraphrased 225 DBpedia relations with a
total of 127,811 patterns, and 25 YAGO2 relations
with a total of 43,124 patterns. Among these we
evaluated a random sample of 1,000 relation para-
phrases. Table 6 shows precision figures for some
selected relations, along anecdotic example patterns.
Some relations are hard to capture precisely. For
DBPedia/doctoralAdvisor, e.g., PATTY picked up
patterns like ?worked with? as paraphrases. These
are not entirely wrong, but we evaluated them as
false because they are too general to indicate the
more specific doctoral advisor relation.
Overall, however, the paraphrasing precision is
high. Our evaluation showed an average precision
of 0.76?0.03 across all relations.
9 Conclusion and Future Directions
This paper presented PATTY, a large resource of text
patterns. Different from existing resources, PATTY
organizes patterns into synsets and a taxonomy, sim-
ilar in spirit to WordNet. Our evaluation shows
that PATTY?s patterns are semantically meaning-
ful, and that they cover large parts of the relations
of other knowledge bases. The Wikipedia-based
version of PATTY contains 350,569 pattern synsets
at a precision of 84.7%, with 8,162 subsumptions,
at a precision of 75%. The PATTY resource is
1143
freely available for interactive access and download
at www.mpi-inf.mpg.de/yago-naga/patty/.
Our approach harnesses existing knowledge bases
for entity-type information. However, PATTY is not
tied to a particular choice for this purpose. In fact,
it would be straightforward to adjust PATTY to us-
ing surface-form noun phrases rather than disam-
biguated entities, as long as we have means to infer
at least coarse-grained types (e.g., person, organiza-
tion, location). An interesting future direction is to
study this generalized setting. We would also like
to investigate the enhanced interplay of information
extraction and pattern extraction, and possible appli-
cations for question answering.
References
Ion Androutsopoulos, Prodromos Malakasiotis: A Sur-
vey of Paraphrasing and Textual Entailment Methods.
Journal of Artificial Intelligence Research 38: 135?
187, 2010
Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami:
Mining Association Rules between Sets of Items in
Large Databases. SIGMOD Conference 1993
Enrique Alfonseca, Marius Pasca, Enrique Robledo-
Arnuncio: Acquisition of instance attributes via la-
beled and related instances. SIGIR 2010
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DBpe-
dia: A Nucleus for a Web of Open Data. ISWC 2007,
data at http://dbpedia.org
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Jonathan Berant, Ido Dagan, Jacob Goldberger: Global
Learning of Typed Entailment Rules. ACL 2011
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, Jamie Taylor: Freebase: a collaboratively
created graph database for structuring human knowl-
edge. SIGMOD Conference 2008, data at http://
freebase.com
Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta: In-
terval Estimation for a Binomial Proportion. Statistical
Science 16: 101?133, 2001
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010, data at http://rtw.ml.
cmu.edu/rtw/
Timothy Chklovski, Patrick Pantel: VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
EMNLP 2004; data available at http://demo.
patrickpantel.com/demos/verbocean/
Anthony Fader, Stephen Soderland, Oren Etzioni:
Identifying Relations for Open Information Extrac-
tion. EMNLP 2011, data at http://reverb.cs.
washington.edu
Christiane Fellbaum (Editor): WordNet: An Electronic
Lexical Database. MIT Press, 1998
Jiawei Han, Jian Pei , Yiwen Yin : Mining frequent pat-
terns without candidate generation. SIGMOD 2000.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, Jun?ichi Kazama:
Large-Scale Verb Entailment Acquisition from the
Web. EMNLP 2009
Catherine Havasi, Robert Speer, and Jason Alonso. Con-
ceptNet 3: a Flexible, Multilingual Semantic Net-
work for Common Sense Knowledge, RANLP 2007;
data available at http://conceptnet5.media.
mit.edu/
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco
Pennacchiotti, Lorenza Romano, Stan Szpakowicz:
SemEval-2010 Task 8: Multi-Way Classification of
Semantic Relations Between Pairs of Nominals, 5th
ACL International Workshop on Semantic Evaluation,
2010; data available at http://www.isi.edu/
?kozareva/downloads.html
Johannes Hoffart, Fabian Suchanek, Klaus Berberich,
Edwin Lewis-Kelham, Gerard de Melo, Ger-
hard Weikum: YAGO2: Exploring and Query-
ing World Knowledge in Time, Space, Con-
text, and Many Languages. WWW 2011, data at
http://yago-knowledge.org
Raphael Hoffmann, Congle Zhang, Daniel S. Weld:
Learning 5000 Relational Extractors. ACL 2010
Vigo Kann: On the approximability of NP-complete opti-
mization problems. PhD thesis, Department of Numer-
ical Analysis and Computing Science, Royal Institute
of Technology, Stockholm. 1992.
Karin Kipper, Anna Korhonen, Neville Ryant,
Martha Palmer, A Large-scale Classification of
English Verbs, Language Resources and Evalua-
tion Journal, 42(1): 21-40, 2008, data available at
http://verbs.colorado.edu/?mpalmer/
projects/verbnet/downloads.html
Zornitsa Kozareva, Eduard H. Hovy: Learning Argu-
ments and Supertypes of Semantic Relations Using
Recursive Patterns. ACL 2010
Girija Limaye, Sunita Sarawagi, Soumen Chakrabarti:
Annotating and Searching Web Tables Using Entities,
Types and Relationships. PVLDB 3(1): 1338-1347
(2010)
Dekang Lin, Patrick Pantel: DIRT: discovery of inference
rules from text. KDD 2001
1144
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC
2006.
Thahir Mohamed, Estevam R. Hruschka Jr., Tom M.
Mitchell: Discovering Relations between Noun Cat-
egories. EMNLP 2011
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Preslav Nakov, Marti A. Hearst: Solving Relational Simi-
larity Problems Using the Web as a Corpus. ACL 2008
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Ca?cilia Zirn, Anas Elghafari: WikiNet: A Very Large
Scale Multi-Lingual Concept Network. LREC 2010,
data at http://www.h-its.org/english/
research/nlp/download/wikinet.php
Roberto Navigli, Simone Paolo Ponzetto: BabelNet:
Building a Very Large Multilingual Semantic Net-
work. ACL 2010 data at http://lcl.uniroma1.
it/babelnet/
Marius Pasca, Benjamin Van Durme: What You Seek Is
What You Get: Extraction of Class Attributes from
Query Logs. IJCAI 2007
Marius Pasca, Benjamin Van Durme: Weakly-Supervised
Acquisition of Open-Domain Classes and Class At-
tributes from Web Documents and Query Logs. ACL
2008
Andrew Philpot, Eduard Hovy, Patrick Pantel: The
Omega Ontology, in: Ontology and the Lexicon,
Cambridge University Press, 2008, data at http:
//omega.isi.edu/
Simone Paolo Ponzetto, Michael Strube: Deriving a
Large-Scale Taxonomy from Wikipedia. AAAI 2007,
data at http://www.h-its.org/english/
research/nlp/download/wikitaxonomy.
php
Joseph Reisinger, Marius Pasca: Latent Variable Models
of Concept-Attribute Attachment. ACL/AFNLP 2009
Ramakrishnan Srikant, Rakesh Agrawal: Mining Se-
quential Patterns: Generalizations and Performance
Improvements. EDBT 1996
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian M. Suchanek, Mauro Sozio, Gerhard Weikum:
SOFIE: a self-organizing framework for information
extraction. WWW 2009
Lin Sun, Anna Korhonen: Hierarchical Verb Clustering
Using Graph Factorization. EMNLP 2011
Petros Venetis, Alon Y. Halevy, Jayant Madhavan, Marius
Pasca, Warren Shen, Fei Wu, Gengxin Miao, Chung
Wu: Recovering Semantics of Tables on the Web.
PVLDB 4(9): 528-538, 2011
Tom White: Hadoop: The Definitive Guide, 2nd Edition.
O?Reilly, 2010.
Fei Wu, Daniel S. Weld: Automatically refining the wiki-
pedia infobox ontology. WWW 2008
Wentao Wu, Hongsong Li, Haixun Wang, Kenny Q. Zhu:
Towards a Probabilistic Taxonomy of Many Concepts.
Technical Report MSR-TR-2011-25, Microsoft Re-
search, 2011
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew
McCallum: Structured Relation Discovery using Gen-
erative Models. EMNLP 2011
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a statistical approach to extracting
entity relationships. WWW 2009
1145
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374?385,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Werdy: Recognition and Disambiguation of Verbs and Verb Phrases
with Syntactic and Semantic Pruning
Luciano Del Corro Rainer Gemulla
Max-Planck-Institut f?ur Informatik
Saarbr?ucken, Germany
{ delcorro, rgemulla, weikum }@mpi-inf.mpg.de
Gerhard Weikum
Abstract
Word-sense recognition and disambigua-
tion (WERD) is the task of identifying
word phrases and their senses in natural
language text. Though it is well under-
stood how to disambiguate noun phrases,
this task is much less studied for verbs
and verbal phrases. We present Werdy,
a framework for WERD with particular
focus on verbs and verbal phrases. Our
framework first identifies multi-word ex-
pressions based on the syntactic structure
of the sentence; this allows us to recog-
nize both contiguous and non-contiguous
phrases. We then generate a list of can-
didate senses for each word or phrase, us-
ing novel syntactic and semantic pruning
techniques. We also construct and lever-
age a new resource of pairs of senses for
verbs and their object arguments. Finally,
we feed the so-obtained candidate senses
into standard word-sense disambiguation
(WSD) methods, and boost their precision
and recall. Our experiments indicate that
Werdy significantly increases the perfor-
mance of existing WSD methods.
1 Introduction
Understanding the semantics of words and multi-
word expressions in natural language text is an
important task for automatic knowledge acquisi-
tion. It serves as a fundamental building block
in a wide area of applications, including semantic
parsing, question answering, paraphrasing, knowl-
edge base construction, etc. In this paper, we
study the task of word-sense recognition and dis-
ambiguation (WERD) with a focus on verbs and
verbal phrases. Verbs are the central element in a
sentence, and the key to understand the relations
between sets of entities expressed in a sentence.
We propose Werdy, a method to (i) automati-
cally recognize in natural language text both sin-
gle words and multi-word phrases that match en-
tries in a lexical knowledge base (KB) like Word-
Net (Fellbaum, 1998), and (ii) disambiguate these
words or phrases by identifying their senses in the
KB. WordNet is a comprehensive lexical resource
for word-sense disambiguation (WSD), covering
nouns, verbs, adjectives, adverbs, and many multi-
word expressions. In the following, the notion of
an entry refers to a word or phrase in the KB,
whereas a sense denotes the lexical synset of the
entry?s meaning in the given sentence.
A key challenge for recognizing KB entries in
natural language text is that entries often consist of
multiple words. In WordNet-3.0 more than 40%
of the entries are multi-word. Such entries are
challenging to recognize accurately for two main
reasons: First, the components of multi-word en-
tries in the KB (such as fiscal year) often consist
of components that are themselves KB entries (fis-
cal and year). Second, multi-word entries (such
as take a breath) may not appear consecutively in
a sentence (?He takes a deep breath.?). Werdy
addresses the latter problem by (conceptually)
matching the syntactic structure of the KB entries
to the syntactic structure of the input sentence.
To address the former problem, Werdy identifies
all possible entries in a sentence and passes them
to the disambiguation phase (take, breath, take a
breath, . . . ); the disambiguation phase provides
more information about which multi-word entries
to keep. Thus, our method solves the recognition
and the disambiguation tasks jointly.
Once KB entries have been identified, Werdy
374
disambiguates each entry against its possible
senses. State-of-the-art methods for WSD (Nav-
igli, 2009) work fairly well for nouns and noun
phrases. However, the disambiguation of verbs
and verbal phrases has received much less atten-
tion in the literature.
WSD methods can be roughly categorized into
(i) methods that are based on supervised training
over sense-annotated corpora (e.g., Zhong and Ng
(2010)), and (ii) methods that harness KB?s to as-
sess the semantic relatedness among word senses
for mapping entries to senses (e.g., Ponzetto and
Navigli (2010)). For these methods, mapping
verbs to senses is a difficult task since verbs tend
to have more senses than nouns. In WordNet, in-
cluding monosemous words, there are on average
1.24 senses per noun and 2.17 per verb.
To disambiguate verbs and verbal phrases,
Werdy proceeds in multiple steps. First, Werdy
obtains the set of candidate senses for each recog-
nized entry from the KB. Second, it reduces the
set of candidate entries using novel syntactic and
semantic pruning techniques. The key insight be-
hind our syntactic pruning is that each verb sense
tends to occur in a only limited number of syn-
tactic patterns. For example, the sentence ?Al-
bert Einstein remained in Princeton? has a sub-
ject (?Albert Einstein?), a verb (?remained?) and
an adverbial (?in Princeton?), it follows an SVA
clause pattern. We can thus safely prune verb
senses that do not match the syntactic structure of
the sentence. Moreover, each verb sense is com-
patible with only a limited number of semantic
argument types (such as location, river, person,
musician, etc); this phenomena is called selec-
tional preference or selectional restriction. Senses
that are compatible only with argument types not
present in the sentence can be pruned. Our prun-
ing steps are based on the idea that a verb selects
the categories of its arguments both syntactically
(c-selection) and semantically (s-selection). In the
final step, Werdy employs a state-of-the-art gen-
eral WSD method to select the most suitable sense
from the remaining candidates. Since incorrect
senses have already been greatly pruned, this step
significantly gains accuracy and efficiency over
standard WSD.
Our semantic pruning technique builds on a
newly created resource of pairs of senses for verbs
and their object arguments. For example, the
WordNet verb sense ?play-1? (i.e., the 1st sense of
the verb entry ?play?) selects as direct object the
noun sense ?sport-1?. We refer to this novel re-
source as the VO Sense Repository, or VOS repos-
itory for short.
1
It is constructed from the Word-
Net gloss-tags corpus, the SemCor dataset, and a
small set of manually created VO sense pairs.
We evaluated Werdy on the SemEval-2007
coarse-grained WSD task (Navigli et al., 2007),
both with and without automatic recognition of en-
tries. We found that our techniques boost state-of-
the-art WSD methods and obtain high-quality re-
sults. Werdy significantly increases the precision
and recall of the best performing baselines.
The rest of the paper is organized as follows.
Section 2 gives an overview of Werdy compo-
nents. Section 3 presents the entry recognition,
and Sections 4 and 5 discuss our novel syntac-
tic and semantic pruning techniques. Section 6
presents the Semantic VO Repository and how we
constructed it. Section 7 gives the results of our
evaluation. Section 8 discusses related work.
2 Overview of Werdy
Werdy consists of four steps: (i) entry recognition,
(ii) syntactic pruning, (iii) semantic pruning, and
(iv) word-sense disambiguation. The novel con-
tribution of this paper is in the first three steps,
and in the construction of the VO sense repository.
Each of these steps operates on the clause level,
i.e., we first determine the set of clauses present
in the input sentence and then process clauses sep-
arately. A clause is a part of a sentence that ex-
presses some statement or coherent piece of infor-
mation. Clauses are thus suitable minimal units
for automatic text understanding tasks (Del Corro
and Gemulla, 2013); see Sec.3 for details.
In the entry-recognition step (Sec. 3), Werdy
obtains for the input sentence a set of potential
KB entries along with their part-of-speech tags.
The candidate senses of each entry are obtained
from WordNet. For instance, in the sentence ?He
takes a deep and long breath?, the set of potential
entries includes take (verb, 44 candidate senses),
take a breath (verb, 1 candidate sense), and breath
(noun, 5 candidate senses). Note that in contrast to
Werdy, most existing word-sense disambiguation
methods assume that entries have already been
(correctly) identified.
1
The VOS repository, Werdy?s source code, and results of
our experimental study are available at http://people.
mpi-inf.mpg.de/
?
corrogg/.
375
In the syntactic-pruning step (Sec. 4), we elim-
inate candidate senses that do not agree with
the syntactic structure of the clause. It is well-
established that the syntactic realization of a
clause is intrinsically related with the sense of
its verb (Quirk et al., 1985; Levin, 1993; Hanks,
1996; Baker et al., 1998; Palmer et al., 2005).
Quirk et al. (1985) identified seven possible clause
types in the English language (such as ?subject
verb adverbial?, SVA). We make use of techniques
inspired by Del Corro and Gemulla (2013) to iden-
tify the clause type of each clause in the sen-
tence. We then match the clause type with the set
of WordNet frames (e.g., ?somebody verb some-
thing?) that WordNet provides for each verb sense,
and prune verb senses for which there is no match.
In the semantic-pruning step (Sec. 5), we fur-
ther prune the set of candidate senses by taking the
semantic types of direct objects into account. Sim-
ilarly to the syntactic relation mentioned above,
a verb sense also imposes a (selectional) restric-
tion on the semantic type of its arguments (Quirk
et al., 1985; Levin, 1993; Hanks, 1996; Baker et
al., 1998; Palmer et al., 2005). For instance, the
verb play with sense participate in games or sports
requires an object argument of type ?game-1?
2
,
?game-3?, or ?sport-1?. Senses that do not match
the arguments found in the clause are pruned.
This step is based on the newly constructed VOS
Repository (Sec. 6). Note that when there is no di-
rect object, only the syntactic pruning step applies.
3 Entry Recognition
The key challenge in recognizing lexical KB en-
tries in text is that entries are not restricted to sin-
gle words. In addition to named entities (such as
people, places, etc.), KB?s contain multi-word ex-
pressions. For example, WordNet-3.0 contains en-
tries such as take place (verb), let down (verb),
take into account (verb), be born (verb), high
school (noun), fiscal year (noun), and Prime Min-
ister (noun). Note that each individual word in a
multi-word entry is usually also an entry by itself,
and can even be part of several multi-word en-
tries. To ensure correct disambiguation, all poten-
tial multi-word entries need to be recognized (Fin-
layson and Kulkarni, 2011), even when they do not
appear as consecutive words in a sentence.
Werdy addresses these challenges by explor-
ing the syntactic structure of both the input sen-
2
We use the notation ?WordNet entry-sense number?.
He takes my hand and a deep breath .
nsubj poss
dobj
cc
det
amod
conj
root
Figure 1: An example dependency parse
tence and the lexical KB entries. The structure
of the sentence is captured in a dependency parse
(DP). Given a word in a sentence, Werdy con-
ceptually generates all subtrees of the DP starting
at that word, and matches them against the KB.
This process can be performed efficiently as Word-
Net entries are short and can be indexed appro-
priately. To match the individual words of a sen-
tence against the words of a KB entry, we follow
the standard approach and perform lemmatization
and stemming (Finlayson, 2014). To further han-
dle personal pronouns and possessives, we follow
Arranz et al. (2005) and normalize personal pro-
nouns (I, you, my, your, . . . ) to one?s, and reflex-
ive pronouns (myself, yourself, . . . ) to oneself.
Consider the example sentence ?He takes my
hand and a deep breath?. We first identify the
clauses and their DP?s (Fig. 1) using the method
of Del Corro and Gemulla (2013), which also
processes coordinating conjunctions. We obtain
clauses ?He takes my hand? and ?He takes a deep
breath?, which we process separately. To obtain
possible entries for the first clause, we start with
its head word (take) and incrementally consider
its descendants (take hand, take one?s hand, . . . ).
The exploration is terminated as early as possible;
for example, we do not consider take one?s hand
because there is no WordNet entry that contains
both take and hand. For the second clause, we
start with take (found in WordNet), then expand
to take breath (not found but can occur together),
then take a breath (found), then take a deep breath
(not found, cannot occur together) and so on.
Note that the word ?take? in the sentence re-
fer to two different entries and senses: ?take? for
the first clause and ?take a breath? for the sec-
ond clause. In this stage no decisions are made
about selecting entries and disambiguating them;
these decisions are made in the final WSD stage
of Werdy.
We tested Werdy?s entry-recognizer on the
SemEval-2007 corpus. We detected the correct en-
376
Pattern Clause type Example WN frame example [frame number]
SV
i
SV AE died. Somebody verb [2]
SV
e
A SVA AE remained in Princeton. Somebody verb PP [22]
SV
c
C SVC AE is smart. Somebody verb adjective [6]
SV
mt
O SVO AE has won the Nobel Prize. Somebody verb something [8]
SV
dt
O
i
O SVOO RSAS gave AE the Nobel Prize. Somebody verb somebody something [14]
SV
ct
OA SVOA The doorman showed AE to his office. Somebody verb somebody PP [20]
SV
ct
OC SVOC AE declared the meeting open. Something verb something adjective/noun [5]
S: Subject, V: Verb, C: Complement, O: Direct object, O
i
: Indirect object, A: Adverbial, V
i
: Intransitive verb, V
c
: Copular verb,
V
c
: Extended-copular verb, V
mt
: Monotransitive verb, V
dt
: Ditransitive verb, V
ct
: Complex-transitive verb
Table 1: Clause types and examples of matching WordNet frames
tries for all but two verbs (out of more than 400).
The two missed entries (take up and get rolling)
resulted from incorrect dependency parses.
4 Syntactic Pruning
Once the KB entries have been recognized, Werdy
prunes the set of possible senses of each verb entry
by considering the syntactic structure of the clause
in which the entry occurs. This pruning is based
on the observation that each verb sense may occur
only in a limited number of ?clause types?, each
having specific semantic functions (Quirk et al.,
1985). When the clause type of the sentence is
incompatible with a candidate sense of an entry,
this sense is eliminated.
Werdy first detects in the input sentence the set
of clauses and their constituents. A clause con-
sists of one subject (S), one verb (V), and option-
ally an indirect object (O), a direct object (O), a
complement (C) and one or more adverbials (A).
Not all combinations of clause constituents ap-
pear in the English language. When we classify
clauses according to the grammatical function of
their constituents, we obtain only seven different
clause types (Quirk et al., 1985); see Table 1. For
example, the sentence ?He takes my hand? is of
type SVO; here ?He? is the subject, ?takes? the
verb, and ?my hand? the object. The clause type
can (in principle) be determined by observing the
verb type and its complementation (Del Corro and
Gemulla, 2013).
For instance, consider the SVA clause ?The stu-
dent remained in Princeton?. The verb remain has
four senses in WN: (1) stay the same; remain in
a certain state (e.g., ?The dress remained wet?),
(2) continue in a place, position, or situation (?He
remained dean for another year?), (3) be left; of
persons, questions, problems (?There remains the
question of who pulled the trigger?) or (4) stay be-
hind (?The hostility remained long after they made
up?). The first sense of remain requires an SVC
pattern; the other cases require either SV or SVA.
Our example clause is of type SVA so that we can
safely prune the first sense.
WordNet provides an important resource for ob-
taining the set of clause types that are compatible
with each sense of a verb. In particular, each verb
sense in WordNet is annotated with a set of frames
(e.g., ?somebody verb something?) in which they
may occur, capturing both syntactic and semantic
constraints. There are 35 different frames in to-
tal.
3
We manually assigned a set of clause types to
each frame (e.g., SVO to frame ?somebody verb
something?). Table 1 shows an example frame for
each of the seven clause types. On average, each
WordNet-3.0 verb sense is associated with 1.57
frames; the maximum number of frames per sense
is 9. The distribution of frames is highly skewed:
More than 61% of the 21,649 frame annotations
belong to one of four simple SVO frames (num-
bers 8, 9, 10 and 11), and 22 out of the 35 frames
have less than 100 instances. This skew makes
the syntactic pruning step effective for non-SVO
clauses, but less effective for SVO clauses.
Werdy directly determines a set of possible
frame types for each clause of the input sentence.
Our approach is based on the clause-type detection
method of Del Corro and Gemulla (2013), but we
also consider additional information that is cap-
tured in frames but not in clause types. For ex-
ample, we distinguish different realizations of ob-
jects (such as clausal objects from non-clausal ob-
jects), which are not captured in the clause type.
Given the DP of a clause, Werdy identifies the
3
See http://wordnet.princeton.edu/
wordnet/man/wninput.5WN.html.
377
Clause
Object?
Q
1
Complement?
Adverbial?
Q
2
Q
3
Frames
4,6,7
Frames
1-3
Frames
1,2,12,13,22,27
No
Yes
No
Yes
No
Dir. and in-
direct object?
Complement?
that-clause?
infinitive/
to-infinitive?
Adverbial?
Q
7
Q
8
Q
9
Q
10
Q
11
Frames
14,15
Frame
5
Frames
26,34
Frames
24,28,29,32,35
Frames
1,2,8-11,33
Frames
1,2,8-11,15-21, 30, 31,33
Yes No
Yes
No
Yes
No
Yes
Yes
No
No
Yes
Figure 2: Flow chart for frame detection
set of WN frames that can potentially match the
clause as outlined in the flowchart of Fig. 2. Werdy
walks through the flowchart; for each question,
we check for the presence or absence of a specific
constituent of a clause (e.g., a direct object for Q
1
)
and proceed appropriately until we obtain a set of
possible frames. This set is further reduced by
considering additional information in the frames
(not shown; e.g., that the verb must end on ?-ing?).
For our example clause ?The student remained
in Princeton?, we first identify possible frames
{ 1, 2, 12, 13, 22, 27 } using the flowchart (Q
1
no,
Q
2
no, Q
3
yes); using the additional information
in the frames, Werdy then further prunes this set
to { 1, 2, 22 }. The corresponding set of remaining
candidate sense for remain is as given above, i.e.,
{ ?remain-2?, ?remain-3?, ?remain-4? }.
Our mapping of clause types to WordNet frames
is judiciously designed for the way WordNet is or-
ganized. For instance, frames containing adver-
bials generally do not specify whether or not the
adverbial is obligatory; here we are conservative
in that we do not prune such frames if the input
clause does not contain an adverbial. As another
example, some frames overlap or subsume each
other; e.g, frame ?somebody verb something? (8)
subsumes ?somebody verb that clause? (26). In
some word senses annotated with the more general
frame, the more specific one can also apply (e.g.,
?point out-1? is annotated with 8 but not 26; 26
can apply), in others it does not (e.g., ?play-1? is
also annotated with 8 but not 26; but here 26 can-
not apply). To ensure the effectiveness of syntactic
pruning, we only consider the frames that are di-
rectly specified in WordNet. This procedure often
produces the desired results; in a few cases, how-
ever, we do prune the correct sense (e.g., frame 26
for clause ?He points out that . . . ?).
5 Semantic Pruning
A verb sense imposes a restriction on the semantic
type of the arguments it may take and vice versa
(Quirk et al., 1985; Levin, 1993; Hanks, 1996;
Baker et al., 1998; Palmer et al., 2005; Kipper et
al., 2008). This allows us to further prune the verb
candidate set by discarding verb senses whose se-
mantic argument is not present in the clause.
WordNet frames potentially allow a shallow
type pruning based on the semantics provided for
the clause constituents. However we could solely
distinguish people (?somebody?) from things
(?something?), which is too crude to obtain sub-
stantial pruning effects. Moreover, this distinction
is sometimes ambiguous.
Instead, we have developed a more powerful
approach to semantic pruning based on our VOS
repository. We remove from the verb candidate set
those senses whose semantic argument cannot be
present in the sentence. For instance, consider the
clause ?The man plays football.? Suppose that we
know that the verb entry play with sense ?play-
1? (?participate in sports?) takes an object of type
?sport-1?; i.e., we have a tuple ?play-1, sport-1?
in our repository. Then, we check whether any
of the possible senses of football?(i) sport or (ii)
ball?is of type ?sport-1?. Here the first sense has
the correct type (the second sense does not); thus
we retain ?play-1? as a possible sense for the verb
entry play. Next, suppose that we consider sense
?play-3? (?play on an instrument?), which accord-
ing to our corpus takes ?instrument-6? as argument
(i.e., there is a tuple ?play-3, instrument-6? in our
VOS repository). Since none of the senses of foot-
ball is of type ?instrument-6?, we can safely drop
378
?play-3? from our candidate set. We perform this
procedure for every verb sense in the candidate set.
Semantic pruning makes use of both VOS
repository and the hypernym structure of the noun
senses in WordNet. For each sentence, we obtain
the possible senses of the direct-object argument
of the verb. We then consider each candidate sense
of the verb (e.g., ?play-1?), and check whether any
of its compatible object-argument senses (from
our repository) is a hypernym of any of the possi-
ble senses of its actual object argument (in the sen-
tence); e.g., ?sport-1? is a hypernym of ?football-
1?. If so, we retain the verb?s candidate sense. If
not, either the candidate sense of the verb is in-
deed incompatible with the object argument in the
sentence, or our repository is incomplete. To han-
dle incompleteness to some extent, we also con-
sider hyponyms of the object-argument senses in
our repository; e.g., if we observe object sport in a
sentence and have verb-sense argument ?football-
1? in our corpus, we consider this a match. If the
hyponyms lead to a match, we retain the verb?s
candidate sense; otherwise, we discard it.
6 Verb-Object Sense Repository
We use three different methods to construct the
repository. In particular, we harness the sense-
annotated WordNet glosses
4
as well as the sense-
annotated SemCor corpus (Landes et al., 1998).
5
The major part of the VOS repository was ac-
quired from WordNet?s gloss tags. According
to Atkins and Rundell (2008), noun definitions
should be expressed in terms of the class to which
they belong, and verb definitions should refer to
the types of the subjects or objects related to the
action. Based on this rationale, we extracted all
noun senses that appear in the gloss of each verb
sense; each of these noun senses is treated as a
possible sense of the object argument of the cor-
responding verb sense. For example, the gloss of
?play-1? is ?participate in games or sports;? each
noun is annotated with its senses (2 and 3 for
?games?, 1 for ?sports?). We extract tuples ?play-
1, game-2?, ?play-1, game-3?, and ?play-1, sport-
1? from this gloss. Note that we only extract
direct-object arguments, i.e., we do not consider
the type of the subject argument of a verb sense.
Since the constituents of the predicate are much
4
http://wordnet.princeton.edu/
glosstag.shtml
5
http://web.eecs.umich.edu/
?
mihalcea/
downloads.html
more important than the subject to determine or
describe a verb sense, lexical resources rarely con-
tain information on the subject (Atkins and Run-
dell, 2008). Similarly, WordNet glosses typically
do not provide any information about adverbials.
Overall, we collected arguments for 8,657 verb
senses (out of WordNet?s 13,767 verb senses) and
a total of 13,050 ?verb-#, object-#?-pairs.
We leveraged the sense-annotated SemCor cor-
pus to further extend our VOS repository. We
parsed each sentence in the corpus to obtain
the respective pairs of verb sense and object
sense. Since sentences are often more specific
than glosses, and thus less helpful for construct-
ing our repository, we generalized the so-found
object senses using a heuristic method. In particu-
lar, we first obtained all the object senses of each
verb sense, and then repeatedly generalized sets of
at least two senses that share a direct hypernym
to this hypernym. The rationale is that we only
want to generalize if we have some evidence that
a more general sense may apply; we thus require
at least two hyponyms before we generalize. Us-
ing this method, we collected arguments for 1,516
verb senses and a total of 4,131 sense pairs.
Finally, we noticed that the most frequent
senses used in the English language are usually
so general that their glosses do not contain any
relevant semantic argument. For instance, one of
the most frequent verbs is ?see-1?, which has gloss
?perceive by ?sight-3??. The correct semantic ar-
gument ?entity-1? is so general that it is omitted
from the gloss. In fact, our gloss-tag extractor
generates tuple ?see-1, sight-3?, which is incorrect.
We thus manually annotated the 30 most frequent
verb senses with their object argument types.
Our final repository contains arguments for
9,335 verb senses and a total of 17,181 pairs. Pairs
from SemCor tend to be more specific because
they refer to text occurrences. The assumption of
taking the nouns of the glosses as arguments seems
to be mostly correct, although some errors may
be introduced. Consider the pair ?play-28, stream-
2? extracted from the gloss ?discharge or direct
or be discharged or directed as if in a continu-
ous ?stream-2??. Also, in some cases, the glosses
may refer to adverbials as in ?play-14, location-1?,
taken from gloss ?perform on a certain ?location-
1??. Note that if an argument is missing from our
repository, we may prune the correct sense of the
verb. If, however, there is an additional, incorrect
379
argument in the repository, the correct verb sense
is retained but pruning may be less effective.
7 Evaluation
Dataset. We tested Werdy on the SemEval-2007
coarse-grained dataset.
6
It consists of five sense-
annotated documents; the sense annotations refer
to a coarse-grained version of WordNet. In addi-
tion to sense annotations, the corpus also provides
the corresponding KB entries (henceforth termed
?gold entries?) as well as a POS tag. We restrict
our evaluation to verbs that act as clause heads. In
total, 461 such verbs were recognized by ClausIE
(Del Corro and Gemulla, 2013) and the Stanford
Parser (Klein and Manning, 2003).
7
WSD Algorithms. For the final step of Werdy,
we used the KB-based WSD algorithms of
Ponzetto and Navigli (2010) and It-Makes-
Sense (Zhong and Ng, 2010), a state-of-the-art
supervised system that was the best performer in
SemEval-2007. Each method only labels entries
for which it is sufficiently confident.
Simplified Extended Lesk (SimpleExtLesk). A
version of Lesk (1986). Each entry is assigned the
sense with highest term overlap between the en-
try?s context (words in the sentence) and both the
sense?s gloss (Kilgarriff and Rosenzweig, 2000)
as well as the glosses of its neighbors (Baner-
jee and Pedersen, 2003). A sense is output only
if the overlap exceeds some threshold; we used
thresholds in the range of 1?20 in our experi-
ments. There are many subtleties and details
in the implementation of SimpleExtLesk so we
used two different libraries: a Java implementation
of WordNet::Similarity (Pedersen et al., 2004),
8
which we modified to accept a context string, and
DKPro-WSD (Miller et al., 2013) version 1.1.0,
with lemmatization, removal of stop words, paired
overlap enabled and normalization disabled.
Degree Centrality. Proposed by Navigli and La-
pata (2010). The method collects all paths con-
necting each candidate sense of an entry to the set
of candidate senses of the words the entry?s con-
text. The candidate sense with the highest degree
in the resulting subgraph is selected. We imple-
mented this algorithm using the Neo4j library.
9
6
The data is annotated with WordNet 2.1 senses; we
converted the annotations to WordNet-3.0 using DKPro-
WSD (Miller et al., 2013).
7
Version 3.3.1, model englishRNN.ser.gz
8
http://www.sussex.ac.uk/Users/drh21/
9
http://www.neo4j.org/
We used a fixed threshold of 1 and vary the search
depth in range 1?20. We used the candidate senses
of all nouns and verbs in a sentence as context.
It-Makes-Sense (IMS). A state-of-the-art, pub-
licly available supervised system (Zhong and Ng,
2010) and a refined version of Chan et al. (2007),
which ranked first in the SemEval-2007 coarse
grained task. We modified the code to accept KB
entries and their candidate senses. We tested both
in WordNet-2.1 and 3.0; for the later we mapped
Werdy?s set of candidates to WordNet-2.1.
Most Frequent Sense (MFS). Selects the most
frequent sense (according to WordNet frequen-
cies) among the set of candidate senses of an en-
try. If there is a tie, we do not label. Note that
this procedure differs slightly from the standard of
picking the entry with the smallest sense id. We
do not follow this approach since it cannot handle
well overlapping entries.
MFS back-off. When one of the above meth-
ods fails to provide a sense label (or provides more
than one), we used the MFS method above with a
threshold of 1. This procedure increased the per-
formance in all cases.
Methodology. The disambiguation was per-
formed with respect to coarse-grained sense clus-
ters. The score of a cluster is the sum of the indi-
vidual scores of its senses (except for IMS which
provides only one answer per word); the cluster
with the highest score was selected. Our source
code and the results of our evaluation are publicly
available
10
.
The SemEval-2007 task was not designed for
automatic entry recognition, for each word or
multi-word expression it provides the WordNet
entry and the POS tag. We proceeded as follows
to handle multi-word entries. In the WSD step, we
considered the candidate senses of all recognized
entries that overlap with the gold entry. For exam-
ple, we considered the candidate senses of entries
take, breath, and take a breath for gold entry take
a breath.
The SemEval-2007 task uses WordNet-2.1 but
Werdy uses WordNet-3.0. We mapped both the
sense keys and clusters from WordNet-2.1 to
WordNet-3.0. All senses in WordNet-3.0 that
could not be mapped to any cluster were consider
to belong each of them to a single sense cluster.
Note that this procedure is fair: for such senses
10
http://people.mpi-inf.mpg.de/
?
corrogg/
380
Algorithm Gold Pruning MFS threshold Verbs (clause heads) F1
Entry back-off /depth P R F1 points
Degree + - + 5 73.54 73.54 73.54
Centrality + + + 11 79.61 79.61 79.61 + 6.07
+ - - 5 73.99 71.58 72.77
+ + - 8 79.91 78.52 79.21 + 6.44
- - + 5 70.41 70.41 70.41
- + + 10 76.46 76.46 76.46 + 6.05
- - - 4 71.05 68.90 69.96
- + - 10 76.81 75.81 76.30 + 6.34
SimpleExtLesk + - + 6 77.28 75.27 76.26
(DKPro) + + + 5 81.90 80.48 81.18 + 4.92
+ - - 1 73.70 52.28 61.17
+ + - 1 81.99 64.21 72.02 + 10.85
- - + 5 74.33 72.57 73.44
- + + 5 79.30 77.75 78.52 + 5.08
- - - 1 69.85 50.54 58.65
- + - 1 78.69 62.20 69.48 + 10.83
SimpleExtLesk + - + 5 77.11 75.27 76.18
(WordNet::Sim) + + + 5 80.57 79.18 79.87 + 3.69
+ - - 1 74.82 68.98 71.78
+ + - 1 79.04 75.27 77.11 + 5.33
- - + 6 74.12 72.35 73.22
- + + 7 77.97 76.46 77.21 + 3.99
- - - 1 71.36 65.66 68.39
- + - 1 76.20 71.92 74.00 + 5.61
MFS + - - 1 76.61 74.62 75.60
+ + - 1 80.35 78.96 79.65 + 4.05
- - - 1 73.67 71.92 72.79
- + - 1 77.75 76.24 76.99 + 4.20
IMS + - + n.a. 79.60 79.60 79.60
(WordNet-2.1) + + + n.a. 80.04 80.04 80.04 + 0.44
- - + n.a. 76.21 75.05 75.63
- + + n.a. 77.53 76.36 76.94 + 1.31
IMS + - + n.a. 78.96 78.96 78.96
(WordNet-3.0) + + + n.a. 79.83 79.83 79.83 + 0.87
- - + n.a. 75.77 74.62 75.19
- + + n.a. 77.53 76.36 76.94 + 1.75
Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
the disambiguation is equivalent to a fine-grained
disambiguation, which is harder.
Results. Our results are displayed in Table 2.
We ran each algorithm with the gold KB entries
provided by in the dataset (+ in column ?gold en-
try) as well as the entries obtained by our method
of Sec. 3 (-). We also enabled (+) and disabled
(-) the pruning steps as well as the MFS back-off
strategy. The highest F1 score was achieved by
SimpleExtLesk (DKPro) with pruning and MFS
back-off: 81.18 with gold entries and 78.52 with
automatic entry recognition. In all cases, our syn-
tactic and semantic pruning strategy increased per-
formance (up to +10.85 F1 points). We next dis-
cuss the impact of the various steps of Werdy in
detail.
Detailed Analysis. Table 3 displays step-by-
step results for DKPro?s SimpleExtLesk, for MFS,
as well as SimpleExtLesk with MFS back-off, the
best performing strategy. The table shows results
when only some Werdy?s steps are used. We start
from a direct use of the respective algorithm with
the gold entries of SemEval-2007 after each hor-
izontal line, and then successively add the Werdy
steps indicated in the table.
When no gold entries were provided, perfor-
mance dropped due to the increase of sense can-
didates for multi-word expressions, which include
the possible senses of the expression itself as well
as the senses of the entry?s parts that are them-
381
Steps Performed threshold P R F1 F1 points
SimpleExtLesk (DKPro)
Plain with gold entries 1 73.70 52.28 61.17
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Syntactic Pruning 1 76.47 58.84 66.50 + 7.85
+ Semantic Pruning 1 78.69 62.20 69.48 + 2.98
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Semantic Pruning 1 73.85 55.39 63.30 + 4.65
+ Syntactic Pruning 1 79.33 61.21 69.10 + 7.93
+ Semantic Pruning 1 81.99 64.21 72.02 + 2.92
+ Semantic Pruning 1 78.11 56.90 65.84 + 4.67
MFS
Plain with gold entries 1 76.61 74.62 75.60
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Syntactic Pruning 1 75.77 74.14 74.95 + 2.16
+ Semantic Pruning 1 77.75 76.24 76.99 + 2.04
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Semantic Pruning 1 77.09 75.43 76.25 + 3.46
+ Syntactic Pruning 1 78.46 76.94 77.69 + 2.09
+ Semantic Pruning 1 80.35 78.96 79.65 + 1.96
+ Semantic Pruning 1 79.91 78.02 78.95 + 3.35
SimpleExtLesk (DKPro) with MFS back-off
Plain with gold entries 6 77.28 75.27 76.26
+ Entry Recognition 6 74.33 72.57 73.44 - 2.82
+ Syntactic Pruning 5 76.65 75.00 75.82 + 2.38
+ Semantic Pruning 5 79.30 77.75 78.52 + 2.70
+ Entry Recognition 5 74.33 72.57 73.44 - 2.82
+ Semantic Pruning 5 78.19 76.51 77.34 +3.90
+ Syntactic Pruning 5 79.34 77.80 78.56 + 2.30
+ Semantic Pruning 5 81.90 80.48 81.18 + 2.62
+ Semantic Pruning 5 81.02 79.09 80.04 + 3.78
Table 3: Step-by-step results
selves WordNet entries. Our entry recognizer
tends to do a good job since it managed to cor-
rectly identify all the relevant entries except in two
cases (i.e. ?take up? and ?get rolling?), in which
the dependency parse was incorrect. The drop in
F1 for our automatic entry recognition was mainly
due to incorrect selection of the correct entry of a
set of alternative, overlapping entries.
Syntactic pruning did not prune the correct
sense in most cases. In 16 cases (with gold en-
tries), however, the correct sense was pruned. Five
of these senses were pruned due to incorrect de-
pendency parses, which led to incorrect frame
identification. In two cases, the sense was not
annotated with the recognized frame in WordNet,
although it seemed adequate. In the remaining
cases, a general frame from WordNet was incor-
rectly omitted. Improvements to WordNet?s frame
annotations may thus make syntactic pruning even
more effective.
Semantic pruning also improves performance.
Here the correct sense was pruned for 11 verbs,
mainly due to the noisiness and incompleteness
of our VOS repository. Without using gold en-
tries, we found in total 237 semantic matches be-
tween possible verbs senses and possible object
senses (200 with gold entries). We also found that
our manual annotations in the VOS repository (see
Sec. 6) did not affect our experiments.
The results show that syntactic and semantic
pruning are beneficial for verb sense disambigua-
tion, but also stress the necessity to improve ex-
isting resources. Ideally, each verb sense would
be annotated with both the possible clause types
or syntactic patterns in which it can occur as well
as the possible senses of its objects. Annotations
for subjects and adverbial arguments may also be
beneficial.
382
8 Related Work
WSD is a classification task where for every word
there is a set of possible senses given by some ex-
ternal resource (as a KB). Two types of methods
can be distinguished in WSD. Supervised systems
(Dang and Palmer, 2005; Dligach and Palmer,
2008; Chen and Palmer, 2009; Zhong and Ng,
2010) use a classifier to assign senses to words,
mostly relying on manually annotated data for
training. In principle, these systems suffer from
low coverage since the training data is usually
sparse. Some authors have tried to overcome this
limitation by exploiting linked resources as train-
ing data (Shen et al., 2013; Cholakov et al., 2014).
The second WSD approach corresponds to the
so-called KB methods (Agirre and Soroa, 2009;
Ponzetto and Navigli, 2010; Miller et al., 2012;
Agirre et al., 2014). They rely on a back-
ground KB (typically WordNet or extended ver-
sions (Navigli and Ponzetto, 2012)), where related
senses appear close to each other. KB-based al-
gorithms often differ in the way the KB is ex-
plored. It has been shown that a key point to en-
hance performance is the amount of semantic in-
formation in the KB (Ponzetto and Navigli, 2010;
Miller et al., 2012). Our framework fits this line of
work since it is also unsupervised and enriches the
background knowledge in order to enhance perfor-
mance of standard WSD algorithms. A compre-
hensive overview of WSD systems can be found
in Navigli (2009) and Navigli (2012).
To bring WSD to real-world applications, the
mapping between text and KB entries is a funda-
mental first step. It has been pointed that the ex-
istence of multi-word expressions imposes multi-
ple challenges to text understanding tasks (Sag et
al., 2002). The problem has been addressed by
Arranz et al. (2005) and Finlayson and Kulkarni
(2011). They find multi-word entries by match-
ing word sequences allowing some morphological
and POS variations according to predefined pat-
terns. Our method differs in that we can recognize
KB entries that appear discontinuously and in that
we do not select the correct entry but generate a
set of potential entries.
Linguists have noted the link between verb
senses and the syntactic structure and argument
types (Quirk et al., 1985; Levin, 1993; Hanks,
1996), and supervised WSD systems were devel-
oped to capture this relation (Dang and Palmer,
2005; Chen and Palmer, 2009; Dligach and
Palmer, 2008; Cholakov et al., 2014). In Dang
and Palmer (2005) and Chen and Palmer (2009),
it is shown that WSD tasks can be improved with
features that capture the syntactic structure and in-
formation about verb arguments and their types.
They use features as shallow named entity recog-
nition and the hypernyms of the possible senses
of the noun arguments. Dang and Palmer (2005)
also included features extracted from PropBank
(Palmer et al., 2005) from role labels and frames.
Dligach and Palmer (2008) generated a corpus of
verb and their arguments (both surface forms),
which was used to incorporate a semantic feature
to the supervised system.
In our work, we also incorporate syntactic and
semantic information. Instead of learning the re-
lation between the verb senses and the syntactic
structure, however we incorporate it explicitly us-
ing the WordNet frames, which provide informa-
tion about which verb sense should be consider
for a given syntactic pattern. We also incorporate
explicitly the semantic relation between each verb
sense and its arguments using our VOS repository.
Different resources of semantic arguments for
automatic text understanding tasks have been con-
structed (Baker et al., 1998; Palmer et al., 2005;
Kipper et al., 2008; Gurevych et al., 2012; Nakas-
hole et al., 2012; Flati and Navigli, 2013). In
(Baker et al., 1998; Palmer et al., 2005; Kipper
et al., 2008; Gurevych et al., 2012), the classifica-
tion of verbs and arguments is focused toward se-
mantic or thematic roles. Nakashole et al. (2012)
uses semantic types to construct a taxonomy of bi-
nary relations and Flati and Navigli (2013) col-
lected semantic arguments for given textual ex-
pressions. For instance, given the verb ?break?,
they extract a pattern ?break ?body part-1??. In
contrast to existing resources, our VOS repository
disambiguates both the verb sense and the senses
of its arguments.
9 Conclusion
We presented Werdy, a framework for word-sense
recognition and disambiguation with a particular
focus on verbs and verbal phrases. Our results
indicate that incorporating syntactic and seman-
tic constraints improves the performance of verb
sense disambiguation methods. This stresses the
necessity of extending and improving the available
syntactic and semantic resources, such as Word-
Net or our VOS repository.
383
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of EACL, pages 33?41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57?84.
Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation.
In Computational Linguistics and Intelligent Text
Processing, volume 3406 of Lecture Notes in Com-
puter Science, pages 250?262.
B. T. Sue Atkins and Michael Rundell. 2008. The Ox-
ford Guide to Practical Lexicography. Oxford Uni-
versity Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL, pages 86?90.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI, pages 805?810.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253?256.
Jinying Chen and Martha Palmer. 2009. Improving
english verb sense disambiguation performance with
linguistically motivated features and clear sense dis-
tinction boundaries. Language Resources and Eval-
uation, 43(2):181?208.
Kostadin Cholakov, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Automated verb sense labelling
based on linked lexical resources. In Proceedings of
EACL, pages 68?77.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL, pages 42?49.
Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of WWW, pages 355?366.
Dmitriy Dligach and Martha Palmer. 2008. Improv-
ing verb sense disambiguation with automatically
retrieved semantic knowledge. In Proceedings of
ICSC, pages 182?189.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of MWE, pages 20?
24.
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of GWC.
Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings of ACL, pages 1222?1232.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby - a large-scale unified
lexical-semantic resource based on lmf. In Proceed-
ings of EACL, pages 580?590.
Patrick Hanks. 1996. Contextual dependency and lex-
ical sets. International Journal of Corpus Linguis-
tics, 1(1):75?98.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english senseval. Com-
puters and the Humanities, 34(1-2):15?48.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42(1):21?40.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423?430.
Shari Landes, Claudia Leacock, and Randee I. Tengi,
1998. Building Semantic Concordances. MIT
Press.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of SIGDOC, pages 24?26.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING,
pages 1781?1796.
Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A
generalized uima-based framework for word sense
disambiguation. In Proceedings of ACL: System
Demonstrations, pages 37?42.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
EMNLP, pages 1135?1145.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. EEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678?692.
384
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193(0):217?
250.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30?35.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41(2):10:1?
10:69.
Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
Proceedings of SOFSEM, pages 115?129.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: Measuring the
relatedness of concepts. In Proceedings of HLT-
NAACL: Demonstration Papers, pages 38?41.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522?1531.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Proceed-
ings of CICLing, pages 1?15.
Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to fine grained sense disambiguation in
wikipedia. In Proceedings of *SEM, pages 22?31.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proceedings of ACL: System
Demonstrations, pages 78?83.
385
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 844?853,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Untangling the Cross-Lingual Link Structure of Wikipedia
Gerard de Melo
Max Planck Institute for Informatics
Saarbru?cken, Germany
demelo@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
weikum@mpi-inf.mpg.de
Abstract
Wikipedia articles in different languages
are connected by interwiki links that are
increasingly being recognized as a valu-
able source of cross-lingual information.
Unfortunately, large numbers of links are
imprecise or simply wrong. In this pa-
per, techniques to detect such problems are
identified. We formalize their removal as
an optimization task based on graph re-
pair operations. We then present an al-
gorithm with provable properties that uses
linear programming and a region growing
technique to tackle this challenge. This
allows us to transform Wikipedia into a
much more consistent multilingual regis-
ter of the world?s entities and concepts.
1 Introduction
Motivation. The open community-maintained en-
cyclopedia Wikipedia has not only turned the In-
ternet into a more useful and linguistically di-
verse source of information, but is also increas-
ingly being used in computational applications as
a large-scale source of linguistic and encyclope-
dic knowledge. To allow cross-lingual navigation,
Wikipedia offers cross-lingual interwiki links that
for instance connect the Indonesian article about
Albert Einstein to the corresponding articles in
over 100 other languages. Such links are extraor-
dinarily valuable for cross-lingual applications.
In the ideal case, a set of articles connected di-
rectly or indirectly via such links would all de-
scribe the same entity or concept. Due to concep-
tual drift, different granularities, as well as mis-
takes made by editors, we frequently find con-
cepts as different as economics and manager in the
same connected component. Filtering out inaccu-
rate links enables us to exploit Wikipedia?s multi-
linguality in a much safer manner and allows us to
create a multilingual register of named entities.
Contribution. Our research contributions are:
1) We identify criteria to detect inaccurate connec-
tions in Wikipedia?s cross-lingual link structure.
2) We formalize the task of removing such links
as an optimization problem. 3) We introduce an
algorithm that attempts to repair the cross-lingual
graph in a minimally invasive way. This algorithm
has an approximation guarantee with respect to
optimal solutions. 4) We show how this algorithm
can be used to combine all editions of Wikipedia
into a single large-scale multilingual register of
named entities and concepts.
2 Detecting Inaccurate Links
In this paper, we model the union of cross-lingual
links provided by all editions of Wikipedia as an
undirected graph G = (V,E) with edge weights
w(e) for e ? E. In our experiments, we simply
honour each individual link equally by defining
w(e) = 2 if there are reciprocal links between the
two pages, 1 if there is a single link, and 0 other-
wise. However, our framework is flexible enough
to deal with more advanced weighting schemes,
e.g. one could easily plug in cross-lingual mea-
sures of semantic relatedness between article texts.
It turns out that an astonishing number of con-
nected components in this graph harbour inac-
curate links between articles. For instance, the
Esperanto article ?Germana Imperiestro? is about
German emporers and another Esperanto article
?Germana Imperiestra Regno? is about the Ger-
man Empire, but, as of June 2010, both are linked
to the English and German articles about the Ger-
man Empire. Over time, some inaccurate links
may be fixed, but in this and in large numbers of
other cases, the imprecise connection has persisted
for many years. In order to detect such cases, we
need to have some way of specifying that two ar-
ticles are likely to be distinct.
844
Figure 1: Connected component with inaccurate
links (simplified)
2.1 Distinctness Assertions
Figure 1 shows a connected component that con-
flates the concept of television as a medium with
the concept of TV sets as devices. Among other
things, we would like to state that ?Television? and
?T.V.? are distinct from ?Television set? and ?TV
set?. In general, we may have several sets of enti-
ties Di,1, . . . , Di,li , for which we assume that any
two entities u,v from different sets are pairwise
distinct with some degree of confidence or weight.
In our example, Di,1 = {?Television?,?T.V.?}
would be one set, andDi,2 = {?Television set?,?TV
set?} would be another set, which means that we
are assuming ?Television?, for example, to be dis-
tinct from both ?Television set? and ?TV set?.
Definition 1. (Distinctness Assertions) Given a
set of nodes V , a distinctness assertion is a col-
lection Di = (Di,1, . . . , Di,li) of pairwise dis-
joint (i.e. Di,j ? Di,k = ? for j 6= k) sub-
sets Di,j ? V that expresses that any two nodes
u ? Di,j , v ? Di,k from different subsets (j 6= k)
are asserted to be distinct from each other with
some weight w(Di) ? R.
We found that many components with inaccurate
links can be identified automatically with the fol-
lowing distinctness assertions.
Criterion 1. (Distinctness between articles from
the same Wikipedia edition) For each language-
specific edition of Wikipedia, a separate asser-
tion (Di,1, Di,2, . . . ) can be made, where each
Di,j contains an individual article together with
its respective redirection pages. Two articles from
the same Wikipedia very likely describe distinct
concepts unless they are redirects of each other.
For example, ?Georgia (country)? is distinct from
?Georgia (U.S. State)?. Additionally, there are also
redirects that are clearly marked by a category or
template as involving topic drift, e.g. redirects
from songs to albums or artists, from products to
companies, etc. We keep such redirects in a Di,j
distinct from the one of their redirect targets.
Criterion 2. (Distinctness between categories
from the same Wikipedia edition) For each
language-specific edition of Wikipedia, a separate
assertion (Di,1, Di,2, . . . ) is made, where each
Di,j contains a category page together with any
redirects. For instance, ?Category:Writers? is dis-
tinct from ?Category:Writing?.
Criterion 3. (Distinctness for links with anchor
identifiers) The English ?Division by zero?, for in-
stance, links to the German ?Null#Division?. The
latter is only a part of a larger article about the
number zero in general, so we can make a dis-
tinctness assertion to separate ?Division by zero?
from ?Null?. In general, for each interwiki link or
redirection with an anchor identifier, we add an as-
sertion (Di,1, Di,2) where Di,1,Di,2 represent the
respective articles without anchor identifiers.
These three types of distinctness assertions are
instantiated for all articles and categories of all
Wikipedia editions. The assertion weights are tun-
able; the simplest choice is using a uniform weight
for all assertions (note that these weights are dif-
ferent from the edge weights in the graph). We
will revisit this issue in our experiments.
2.2 Enforcing Consistency
Given a graph G representing cross-lingual links
between Wikipedia pages, as well as distinctness
assertions D1, . . . , Dn with weights w(Di), we
may find that nodes that are asserted to be dis-
tinct are in the same connected component. We
can then try to apply repair operations to recon-
cile the graph?s link structure with the distinctness
asssertions and obtain global consistency. There
are two ways to modify the input, and for each
we can also consider the corresponding weights
as a sort of cost that quantifies how much we are
changing the original input:
a) Edge cutting: We may remove an edge e ?
E from the graph, paying cost w(e).
b) Distinctness assertion relaxation: We may
remove a node v ? V from a distinctness as-
sertion Di, paying cost w(Di).
845
Removing edges allows us to split connected com-
ponents into multiple smaller components, thereby
ensuring that two nodes asserted to be distinct are
no longer connected directly or indirectly. In Fig-
ure 1, for instance, we could delete the edge from
the Spanish ?TV set? article to the Japanese ?televi-
sion? article. In constrast, removing nodes from
distinctness assertions means that we decide to
give up our claim of them being distinct, instead
allowing them to share a connected component.
Our reliance on costs is based on the assump-
tion that the link structure or topology of the graph
provides the best indication of which cross-lingual
links to remove. In Figure 1, we have distinct-
ness assertions between nodes in two densely con-
nected clusters that are tied together only by a sin-
gle spurious link. In such cases, edge removals
can easily yield separate connected components.
When, however, the two nodes are strongly con-
nected via many different paths with high weights,
we may instead opt for removing one of the two
nodes from the distinctness assertion.
The aim will be to balance the costs for remov-
ing edges from the graph with the costs for remov-
ing nodes from distinctness assertions to produce
a consistent solution with a minimal total repair
cost. We accommodate our knowledge about dis-
tinctness while staying as close as possible to what
Wikipedia provides as input.
This can be formalized as the Weighted
Distinctness-Based Graph Separation (WDGS)
problem. Let G be an undirected graph with a set
of vertices V and a set of edges E weighted by
w : E ? R. If we use a set C ? V to spec-
ify which edges we want to cut from the original
graph, and sets Ui to specify which nodes we want
to remove from distinctness assertions, we can be-
gin by defining WDGS solutions as follows.
Definition 2. (WDGS Solution). Given a graph
G = (V,E) and n distinctness assertionsD1, . . . ,
Dn, a tuple (C,U1, . . . , Un) is a valid WDGS so-
lution if and only if ?i, j, k 6= j, u ? Di,j \ Ui,
v ? Di,k \ Ui: P(u, v, E \ C) = ?, i.e. the set of
paths from u to v in the graph (V,E \C) is empty.
Definition 3. (WDGS Cost). Let w : E ? R
be a weight function for edges e ? E, and w(Di)
(i = 1 . . . n) be weights for the distinctness as-
sertions. The (total) cost of a WDGS solution
S = (C,U1, . . . , Un) is then defined as
c(S) = c(C,U1, . . . , Un)
=
[
?
e?C
w(e)
]
+
[
n?
i=1
|Ui|w(Di)
]
Definition 4. (WDGS). A WDGS problem instance
P consists of a graph G = (V,E) with edge
weights w(e) and n distinctness assertions D1,
. . . , Dn with weights w(Di). The objective con-
sists in finding a solution (C,U1, . . . , Un) with
minimal cost c(C,U1, . . . , Un).
It turns out that finding optimal solutions effi-
ciently is a hard problem (proofs in Appendix A).
Theorem 1. WDGS is NP-hard and APX-hard. If
the Unique Games Conjecture (Khot, 2002) holds,
then it is NP-hard to approximate WDGS within
any constant factor ? > 0.
3 Approximation Algorithm
Due to the hardness of WDGS, we devise a
polynomial-time approximation algorithm with an
approximation factor of 4 ln(nq + 1) where n is
the number of distinctness assertions and q =
max
i,j
|Di,j |. This means that for all problem in-
stances P , we can guarantee
c(S(P ))
c(S?(P ))
? 4 ln(nq + 1),
where S(P ) is the solution determined by our al-
gorithm, and S?(P ) is an optimal solution. Note
that this approximation guarantee is independent
of how long each Di is, and that it merely repre-
sents an upper bound on the worst case scenario.
In practice, the results tend to be much closer to
the optimum, as will be shown in Section 4.
Our algorithm first solves a linear program (LP)
relaxation of the original problem, which gives
us hints as to which edges should most likely be
cut and which nodes should most likely be re-
moved from distinctness assertions. Note that this
is a continuous LP, not an integer linear program
(ILP); the latter would not be tractable due to the
large number of variables and constraints of the
problem. After solving the linear program, a new
? extended ? graph is constructed and the optimal
LP solution is used to define a distance metric on
it. The final solution is obtained by smartly se-
lecting regions in this extended graph as the in-
dividual output components, employing a region
846
growing technique in the spirit of the seminal work
by Leighton and Rao (1999). Edges that cross the
boundaries of these regions are cut.
Definition 5. Given a WDGS instance, we define a
linear program of the following form:
minimize
?
e?E
dew(e) +
n?
i=1
li?
j=1
?
v?Di,j
ui,vw(Di)
subject to
pi,j,v = ui,v ?i, j<li, v ? Di,j (1)
pi,j,v + ui,v ? 1 ?i, j<li, v ?
S
k>j
Di,k (2)
pi,j,v ? pi,j,u + de ?i, j<li, e=(u,v) ? E (3)
de ? 0 ?e ? E (4)
ui,v ? 0 ?i, v ?
liS
j=1
Di,j (5)
pi,j,v ? 0 ?i, j<li, v?V (6)
The LP uses decision variables de and ui,v, and
auxiliary variables pi,j,v that we refer to as poten-
tial variables. The de variables indicate whether
(in the continuous LP: to what degree) an edge
e should be deleted, and the ui,v variables indi-
cate whether (to what degree) v should be removed
from a distinctness assertion Di. The LP objec-
tive function corresponds to Definition 3, aiming
to minimize the total costs. A potential variable
pi,j,v reflects a sort of potential difference between
an assertionDi,j and a node v. If pi,j,v = 0, then v
is still connected to nodes in Di,j . Constraints (1)
and (2) enforce potential differences between Di,j
and all nodes in Di,k with k > j. For instance,
for distinctness between ?New York City? and ?New
York? (the state), they might require ?New York?
to have a potential of 1, while ?New York City?
has a potential of 0. The potential variables are
tied to the deletion variables de for edges in Con-
straint (3) as well as to the ui,v in Constraints (1)
and (2). This means that the potential difference
pi,j,v + ui,v ? 1 can only be obtained if edges are
deleted on every path between ?New York City? and
?New York?, or if at least one of these two nodes is
removed from the distinctness assertion (by setting
the corresponding ui,v to non-zero values). Con-
straints (4), (5), (6) ensure non-negativity.
Having solved the linear program, the next ma-
jor step is to convert the optimal LP solution into
the final ? discrete ? solution. We cannot rely
on standard rounding methods to turn the optimal
fractional values of the de and ui,v variables into
a valid solution. Often, all solution variables have
small values and rounding will merely produce an
empty (C,U1, . . . , Un) = (?, ?, . . . , ?). Instead,
a more sophisticated technique is necessary. The
optimal solution of the LP can be used to define
an extended graph G? with a distance metric d be-
tween nodes. The algorithm then operates on this
graph, in each iteration selecting regions that be-
come output components and removing them from
the graph. A simple example is shown in Figure 2.
The extended graph contains additional nodes and
edges representing distinctness assertions. Cutting
one of these additional edges corresponds to re-
moving a node from a distinctness assertion.
Definition 6. Given G = (V,E) and distinct-
ness assertions D1, . . . , Dn with weights w(Di),
we define an undirected graph G? = (V ?, E?)
where V ? = V ? {vi,v | i = 1 . . . n, w(Di) >
0, v ?
?
j Di,j}, E
? = {e ? E | w(e) > 0} ?
{(v, vi,v) | v ? Di,j , w(Di) > 0}. We accordingly
extend the definition of w(e) to additionally cover
the new edges by defining w(e) = w(Di) for e =
(v, vi,v). We also extend it for sets S of edges by
defining w(S) =
?
e?S w(e). Finally, we define a
node distance metric
d(u, v) =
?
??????????
??????????
0 u = v
de (u, v) ? E
ui,v u = vi,v
ui,u v = vi,u
min
p?
P(u,v,E?)
?
(u?,v?)
?p
d(u?, v?) otherwise,
where P(u, v, E?) denotes the set of acyclic paths
between two nodes in E?. We further fix
c?f =
?
(u,v)?E?
d(u, v)w(e)
as the weight of the fractional solution of the LP
(c?f is a constant based on the original E?, irre-
spective of later modifications to the graph).
Definition 7. Around a given node v in G?, we
consider regions R(v, r) ? V with radius r. The
cut C(v, r) of a given region is defined as the set
of edges in G? with one endpoint within the region
and one outside the region:
R(v, r) = {v? ? V ? | d(v, v?) ? r}
C(v, r) = {e ? E? | |e ?R(v, r)| = 1}
For sets of nodes S ? V , we define R(S, r) =
?
v?S
R(v, r) and C(S, r) =
?
v?S
C(v, r).
847
Figure 2: Extended graph with two added nodes
v1,u, v1,v representing distinctness between ?Tele-
visio?n? and ?Televisor?, and a region around v1,u
that would cut the link from the Japanese ?Televi-
sion? to ?Televisor?
Definition 8. Given q = max
i,j
|Di,j |, we approxi-
mate the optimal cost of regions as:
c?(v, r) =
?
e=(u,u?)?E?:
e?R(v,r)
d(u, u?)w(e) (1)
+
?
e?C(v,r)
v??e?R(v,r)
(r ? d(v, v?))w(e)
c?(S, r) =
1
nq
c?f +
?
v?S
c?(v, r) (2)
The first summand accounts for the edges en-
tirely within the region, and the second one ac-
counts for the edges in C(v, r) to the extent that
they are within the radius. The definition of c?(S, r)
contains an additional slack component that is re-
quired for the approximation guarantee proof.
Based on these definitions, Algorithm 3.1 uses
the LP solution to construct the extended graph.
It then repeatedly, as long as there is an unsatis-
fied assertion Di, chooses a set S of nodes con-
taining one node from each relevant Di,j . Around
the nodes in S it simultaneously grows |S| regions
with the same radius, a technique previously sug-
gested by Avidor and Langberg (2007). These re-
gions are essentially output components that de-
termine the solution. Repeatedly choosing the
radius that minimizes w(C(S,r))c?(S,r) allows us to ob-
tain the approximation guarantee, because the dis-
tances in this extended graph are based on the so-
lution of the LP. The properties of this algorithm
are given by the following two theorems (proofs in
Appendix A).
Theorem 2. The algorithm yields a valid WDGS
solution (C,U1, . . . , Un).
Theorem 3. The algorithm yields a solution
(C,U1, . . . , Un) with an approximation factor of
4 ln(nq + 1) with respect to the cost of the op-
timal WDGS solution (C?, U?1 , . . . , U
?
n), where n
is the number of distinctness assertions and q =
max
i,j
|Di,j |. This solution can be obtained in poly-
nomial time.
4 Results
4.1 Wikipedia
We downloaded February 2010 XML dumps of
all available editions of Wikipedia, in total 272
editions that amount to 86.5 GB uncompressed.
From these dumps we produced two datasets.
Dataset A captures cross-lingual interwiki links
between pages, in total 77.07 million undirected
edges (146.76 million original links). Dataset
B additionally includes 2.2 million redirect-based
edges. Wikipedia deals with interwiki links to
redirects transparently, however there are many
redirects with titles that do not co-refer, e.g. redi-
rects from members of a band to the band, or from
aspects of a topic to the topic in general. We only
included redirects in the following cases:
? the titles of redirect and redirect target match
after Unicode NFKD normalization, diacrit-
ics removal, case conversion, and removal of
punctuation characters
? the redirect uses certain templates or cate-
gories that indicate co-reference with the tar-
get (alternative names, abbreviations, etc.)
We treated them like reciprocal interwiki links by
assigning them a weight of 2.
4.2 Application of Algorithm
The choice of distinctness assertion weights de-
pends on how lenient we wish to be towards con-
ceptual drift, allowing us to opt for more fine- or
more coarse-grained distinctions. In our experi-
ments, we decided to prefer fine-grained concep-
tual distinctions, and settled on a weight of 100.
We analysed over 20 million connected com-
ponents in each dataset, checking for distinctness
assertions. For the roughly 110,000 connected
components with relevant distinctness assertions,
848
Algorithm 3.1 WDGS Approximation Algorithm
1: procedure SELECT(V,E, V ?, E?, w,D1, . . . , Dn, l1, . . . , ln)
2: solve linear program given by Definition 5 . determine optimal fractional solution
3: construct G? = (V ?, E?) . extended graph (Definition 6)
4: C ? {e ? E | w(e) = 0} . cut zero-weighted edges
5: Ui ?
li?1?
j=1
Di,j ?i : w(Di) = 0 . remove zero-weighted Di
6: while ?i, j, k > j, u ? Di,j , v ? Di,k : P(vi,u, vi,v, E?) 6= ? do . find unsatisfied assertion
7: S ? ? . set of nodes around which regions will be grown
8: for all j in 1 . . . li ? 1 do . arbitrarily choose node from each Di,j
9: if ?v ? Di,j : vi,v ? V ? then S ? S ? vi,v
10: D ? {d(u, v) ? 12 | u ? S, v ? V
?} ? {12} . set of distances
11: choose  such that ?d, d? ? D : 0 <  |d? d?| . infinitesimally small
12: r ? argmin
r=d?: d?D\{0}
w(C(S, r))
c?(S, r)
. choose optimal radius (ties broken arbitrarily)
13: V ? ? V ? \R(S, r) . remove regions from G?
14: E? ? {e ? E? | e ? V ?}
15: C ? C ? (C(S, r) ? E) . update global solution
16: for all i? in 1 . . . n do
17: Ui? ? Ui? ? {v | (vi?,v, v) ? C(S, r)}
18: for all j in 1 . . . li? do Di?,j ? Di?,j ? V ? . prune distinctness assertions
19: return (C,U1, . . . , Un)
we applied our algorithm, relying on the commer-
cial CPLEX tool to solve the linear programs. In
most cases, the LP solving took less than a second,
however the LP sizes grow exponentially with the
number of nodes and hence the time complex-
ity increases similarly. In about 300 cases per
dataset, CPLEX took too long and was automat-
ically killed or the linear program was a priori
deemed too large to complete in a short amount
of time. For these cases, we adopted an alternative
strategy described later on.
Table 1 provides the experimental results for the
two datasets. Dataset B is more connected and
thus has fewer connected components with more
pairs of nodes asserted to be distinct by distinct-
ness assertions. The LP given by Definition 5
provides fractional solutions that constitute lower
bounds on the optimal solution (cf. also Lemma
5 in Appendix A), so the optimal solution can-
not have a cost lower than the fractional LP solu-
tion. Table 1 shows that in practice, our algorithm
achieves near-optimal results.
4.3 Linguistic Adequacy
The near-optimal results of our algorithm apply
with respect to our problem formalization, which
aims at repairing the graph in a minimally inva-
Table 1: Algorithm Results
Dataset A Dataset B
Connected
components
23,356,027 21,161,631
? with distinctness
assertions
112,857 113,714
? algorithm applied
successfully
112,580 113,387
Distinctness
assertions
380,694 379,724
Node pairs con-
sidered distinct
916,554 1,047,299
Lower bound on
optimal cost
1,255,111 1,245,004
Cost of our solution 1,306,747 1,294,196
Factor 1.04 1.04
Edges to be deleted
(undirected)
1,209,798 1,199,181
Nodes to be merged 603 573
sive way. It may happen, however, that the graph?s
topology is misleading, and that in a specific case
deleting many cross-lingual links to separate two
entities is more appropriate than looking for a
conservative way to separate them. This led us
849
to study the linguistic adequacy. Two annotators
evaluated 200 randomly selected separated pairs
from Dataset A consisting of an English and a
German article, with an inter-annotator agreement
(Cohen ?) of 0.656. Examples are given in Table
2. We obtained a precision of 87.97% ? 0.04%
(Wilson score interval) against the consensus an-
notation. Many of the errors are the result of ar-
ticles having many inaccurate outgoing links, in
which case they may be assigned to the wrong
component. In other cases, we noted duplicate ar-
ticles in Wikipedia.
Occasionally, we also observed differences in
scope, where one article would actually describe
two related concepts in a single page. Our algo-
rithm will then either make a somewhat arbitrary
assignment to the component of either the first or
second concept, or the broader generalization of
the two concepts becomes a separate, more gen-
eral connected component.
4.4 Large Problem Instances
When problem instances become too large, the lin-
ear programs can become too unwieldy for lin-
ear optimization software to cope with on current
hardware. In such cases, the graphs tend to be very
sparsely connected, consisting of many smaller,
more densely connected subgraphs. We thus in-
vestigated graph partitioning heuristics to decom-
pose larger graphs into smaller parts that can more
easily be handled with our algorithm. The METIS
algorithms (Karypis and Kumar, 1998) can de-
compose graphs with hundreds of thousands of
nodes almost instantly, but favour equally sized
clusters over lower cut costs. We obtained parti-
tionings with costs orders of magnitude lower us-
ing the heuristic by Dhillon et al (2007).
4.5 Database of Named Entities
The partitioning heuristics allowed us to process
all entries in the complete set of Wikipedia dumps
and produce a clean output set of connected com-
ponents where each Wikipedia article or category
belongs to a connected component consisting of
pages about the same entity or concept. We can re-
gard these connected components as equivalence
classes. This means that we obtain a large-scale
multilingual database of named entities and their
translations. We are also able to more safely trans-
fer information cross-lingually between editions.
For example, when an article a has a category c in
the French Wikipedia, we can suggest the corre-
sponding Indonesian category for the correspond-
ing Indonesian article.
Moreover, we believe that this database will
help extend resources like DBPedia and YAGO
that to date have exclusively used the English
Wikipedia as their repository of entities and
classes. With YAGO?s category heuristics, even
entirely non-English connected components can
be assigned a class in WordNet as long as at least
one of the relevant categories has an English page.
So, the French Wikipedia article on the Dutch
schooner ?JR Tolkien?, despite the lack of a cor-
responding English article, can be assigned to the
WordNet synset for ?ship?. Using YAGO?s plu-
ral heuristic to distinguish classes (Einstein is a
physicist) from topic descriptors (Einstein belongs
to the topic physics), we determined that over 4.8
million connected components can be linked to
WordNet, greatly surpassing the 3.2 million arti-
cles covered by the English Wikipedia alone.
5 Related Work
A number of projects have used Wikipedia as a
database of named entities (Ponzetto and Strube,
2007; Silberer et al, 2008). The most well-
known are probably DBpedia (Auer et al, 2007),
which serves as a hub in the Linked Data Web,
Freebase1, which combines human input and au-
tomatic extractors, and YAGO (Suchanek et al,
2007), which adds an ontological structure on top
of Wikipedia?s entities. Wikipedia has been used
cross-lingually for cross-lingual IR (Nguyen et al,
2009), question answering (Ferra?ndez et al, 2007)
as well as for learning transliterations (Pasternack
and Roth, 2009), among other things.
Mihalcea and Csomai (2007) have studied pre-
dicting new links within a single edition of
Wikipedia. Sorg and Cimiano (2008) considered
the problem of suggesting new cross-lingual links,
which could be used as additional inputs in our
problem. Adar et al (2009) and Bouma et al
(2009) show how cross-lingual links can be used
to propagate information from one Wikipedia?s in-
foboxes to another edition.
Our aggregation consistency algorithm uses
theoretical ideas put forward by researchers study-
ing graph cuts (Leighton and Rao, 1999; Garg et
al., 1996; Avidor and Langberg, 2007). Our prob-
lem setting is related to that of correlation cluster-
ing (Bansal et al, 2004), where a graph consist-
1http://www.freebase.com/
850
Table 2: Examples of separated concepts
English concept German concept
(translated)
Explanation
Coffee percolator French Press different types of brewing devices
Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger
of Baqa al-Gharbiyye and Jatt
Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek
mythology
Old Belarusian language Ruthenian language the second is often considered slightly
broader
ing of positively and negatively labelled similar-
ity edges is clustered such that similar items are
grouped together, however our approach is much
more generic than conventional correlation clus-
tering. Charikar et al (2005) studied a variation
of correlation clustering that is similar to WDGS,
but since a negative edge would have to be added
between each relevant pair of entities in a distinct-
ness assertion, the approximation guarantee would
only be O(log(n |V |2)). Minimally invasive re-
pair operations on graphs have also been stud-
ied for graph similarity computation (Zeng et al,
2009), where two graphs are provided as input.
6 Conclusions and Future Work
We have presented an algorithmic framework for
the problem of co-reference that produces consis-
tent partitions by intelligently removing edges or
allowing nodes to remain connected. This algo-
rithm has successfully been applied to Wikipedia?s
cross-lingual graph, where we identified and elim-
inated surprisingly large numbers of inaccurate
connections, leading to a large-scale multilingual
register of names.
In future work, we would like to investigate
how our algorithm behaves in extended settings,
e.g. we can use heuristics to connect isolated,
unconnected articles to likely candidates in other
Wikipedias using weighted edges. This can be
extended to include mappings from multiple lan-
guages to WordNet synsets, with the hope that
the weights and link structure will then allow the
algorithm to make the final disambiguation deci-
sion. Additional scenarios include dealing with
co-reference on the Linked Data Web or mappings
between thesauri. As such resources are increas-
ingly being linked to Wikipedia and DBpedia, we
believe that our techniques will prove useful in
making mappings more consistent.
A Proofs
Proof (Theorem 1). We shall reduce the mini-
mum multicut problem to WDGS. The hardness
claims then follow from Chawla et al (2005).
Given a graph G = (V,E) with a positive cost
c(e) for each e ? E, and a set D = {(si, ti) | i =
1 . . . k} of k demand pairs, our goal is to find
a multicut M with respect to D with minimum
total cost
?
e?M c(e). We convert each demand
pair (si, ti) into a distinctness assertion Di =
({si}, {ti}) with weight w(Di) = 1+
?
e?E c(e).
An optimal WDGS solution (C,U1, . . . , Uk) with
cost c then implies a multicut C with the same
weight, because each w(Di) >
?
e?E c(e), so
all demand pairs will be satisfied. C is a minimal
multicut because any multicut C ? with lower cost
would imply a valid WDGS solution (C ?, ?, . . . , ?)
with a cost lower than the optimal one, which is a
contradiction.
Lemma 4. The linear program given by Defini-
tion 5 enforces that for any i,j,k 6= j,u ? Di,j ,
v ? Di,k, and any path v0, . . . , vt with v0 = u,
vt = v we obtain ui,u+
?t?1
l=0 d(vl,vl+1)+ui,v ? 1.
The integer linear program obtained by aug-
menting Definition 5 with integer constraints
de, ui,v, pi,j,v ? {0, 1} (for all applicable e, i, j,
v) produces optimal solutions (C,U1, . . . , Uk) for
WDGS problems, obtained as C = ({e ? E | de =
1}, Ui = {v | ui,v = 1}.
Proof. Without loss of generality, let us assume
that j < k. The LP constraints give us pi,j,vt ?
pi,j,vt?1 +d(vt?1,vt), . . . , pi,j,v1 ? pi,j,v0 +d(v0,v1),
as well as pi,j,v0 = ui,u and pi,j,vt + ui,v ? 1.
Hence 1 ? pi,j,vt+ui,v ? ui,u+
?t?1
l=0 d(vl,vl+1)+
ui,v.
With added integrality constraints, we obtain ei-
ther u ? Ui, v ? Ui, or at least one edge along any
path from u to v is cut, i.e. P(u, v, E \ C) = ?.
851
This proves that any ILP solution enduces a valid
WDGS solution (Definition 2).
Clearly, the integer program?s objective func-
tion minimizes c(C,U1, . . . , Un) (Definition 3) if
C = ({e ? E | de = 1}, Ui = {v | ui,v = 1}.
To see that the solutions are optimal, it thus suf-
fices to observe that any optimal WDGS solution
(C?, U?1 , . . . , U
?
n) yields a feasible ILP solution
de = IC?(e), ui,v = IU?i (v).
Proof (Theorem 2). ri < 12 holds for any ra-
dius ri chosen by the algorithm, so for any re-
gion R(v0, r) grown around a node v0, and any
two nodes u, v within that region, the triangle in-
equality gives us d(u, v) ? d(u, v0) + d(v0, v) <
1
2 +
1
2 = 1 (maximal distance condition). At
the same time, by Lemma 4 and Definition 6 for
any u ? Di,j , v ? Di,k (j 6= k), we obtain
d(vi,u, vi,v) = d(vi,u, u) + d(u, v) + d(v, vi,v) ?
1. With the maximal distance condition above, this
means that vi,u and vi,v cannot be in the same re-
gion. Hence u, v cannot be in the same region,
unless the edge from vi,u to u is cut (in which case
u will be placed in Ui) or the edge from v to vi,v
is cut (in which case v will be placed in Ui). Since
each region is separated from other regions via C,
we obtain that ?i, j, k 6= j, u, v: u ? Di,j \ Ui,
v ? Di,k \ Ui implies P(u, v, E \ C) = ?, so a
valid solution is obtained.
Lemma 5 (essentially due to Garg et al (1996)).
For any i where ?j, k > j, u ? Di,j , v ? Di,k :
P(vi,u, vi,v, E?) 6= ? and w(Di) > 0, there exists
an r such that w(C(S, r)) ? 2 ln(nq + 1) c?(S, r),
0 ? r < 12 for any set S consisting of vi,v nodes.
Proof. Define w(S, r) =
?
v?S w(C(v, r)). We
will prove that there exists an appropriate r with
w(C(S, r)) ? w(S, r) ? 2 ln(nq+1) c?(S, r). As-
sume, for reductio ad absurdum, that ?r ? [0, 12) :
w(S, r) > 2 ln(nq + 1)c?(S, r). As we expand
the radius r, we note that c?(S, r) ddr = w(S, r)
whereever c? is differentiable with respect to r.
There are only a finite number of points r1,. . . ,rl?1
in (0, 12) where this is not the case (namely, when
?u ? S, v ? V ? : d(u, v) = ri). Also note
that c? increases monotonically for increasing val-
ues of r, and that it is universally greater than
zero (since there is a path between vi,u, vi,v). Set
r0 = 0, rl = 12 and choose  such that 0 <  
min{rj+1 ? rj | j < l}. Our assumption then
implies:
l?
j=1
? rj?
rj?1+
w(S,r)
c?(S,r) dr
>
[
l?
j=1
rj ? rj?1 ? 2
]
2 ln(nq + 1)
l?
j=1
ln c?(S, rj ? )? ln c?(S, rj?1 + )
>
(
1
2 ? 2l
)
2 ln(nq + 1)
ln c?(S, 12 ? )? ln c?(S, 0)
> (1? 4l) ln(nq + 1)
c?(S, 12?)
c?(S,0) > (nq + 1)
1?4l
c?(S, 12 ? ) > (nq + 1)
1?4lc?(S, 0)
For small , the right term can get arbitrarily close
to (nq+1)c?(S, 0) ? c?f + c?(S, 0), which is strictly
larger than c?(S, 12 ? ) no matter how small  be-
comes, so the initial assumption is false.
Proof (Theorem 3). Let Si, ri denote the set
S and radius r chosen in particular iterations,
and ci the corresponding costs incurred: ci =
w(C(Si, r) ? E) + |Ui|w(Di) = w(C(Di, r)).
Note that any ri chosen by the algorithm will in
fact fulfil the criterion described by Lemma 5, be-
cause ri is chosen to minimize the ratio between
the two terms, and the minimizing r ? [0, 12)
must be among the r considered by the algo-
rithm (w(C(Di, r)) only changes at one of those
points, so the minimum is reached by approach-
ing the points from the left). Hence, we obtain
ci ? 2 ln(n+ 1)c?(Si, ri). For our global solution,
note that there is no overlap between the regions
chosen within an iteration, since regions have a
radius strictly smaller than 12 , while vi,u, vi,v for
u ? Di,j , v ? Di,k, j 6= k have a distance of
at least 1. Nor is there any overlap between re-
gions from different iterations, because in each it-
eration the selected regions are removed from G?.
Globally, we therefore obtain c(C,U1, . . . , Un) =?
i ci < 2 ln(nq + 1)
?
i c?(Si, ri) ? 2 ln(nq +
1)2c?f (observe that i ? nq). Since c?f is the ob-
jective score for the fractional LP relaxation solu-
tion of the WDGS ILP (Lemma 4), we obtain c?f ?
c(C?, U?1 , . . . , U
?
n), and thus c(C,U1, . . . , Un) <
4 ln(n+ 1)c(C?, U?1 , . . . , U
?
n).
To obtain a solution in polynomial time, note
that the LP size is polynomial with respect to nq
and may be solved using a polynomial algorithm
(Karmarkar, 1984). The subsequent steps run in
O(nq) iterations, each growing up to |V | regions
using O(|V |2) uniform cost searches.
852
References
Eytan Adar, Michael Skinner, and Daniel S. Weld.
2009. Information arbitrage across multi-lingual
Wikipedia. In Ricardo A. Baeza-Yates, Paolo Boldi,
Berthier A. Ribeiro-Neto, and Berkant Barla Cam-
bazoglu, editors, Proceedings of the 2nd Interna-
tional Conference on Web Search and Web Data
Mining, WSDM 2009, pages 94?103. ACM.
So?ren Auer, Chris Bizer, Jens Lehmann, Georgi Kobi-
larov, Richard Cyganiak, and Zachary Ives. 2007.
DBpedia: a nucleus for a web of open data. In
Aberer et al, editor, The Semantic Web, 6th Interna-
tional Semantic Web Conference, 2nd Asian Seman-
tic Web Conference, ISWC 2007 + ASWC 2007, Bu-
san, Korea, November 11?15, 2007, Lecture Notes
in Computer Science 4825. Springer.
Adi Avidor and Michael Langberg. 2007. The multi-
multiway cut problem. Theoretical Computer Sci-
ence, 377(1-3):35?42.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89?113.
Gosse Bouma, Sergio Duarte, and Zahurul Islam.
2009. Cross-lingual alignment and completion of
Wikipedia templates. In CLIAWS3 ?09: Proceed-
ings of the Third International Workshop on Cross
Lingual Information Access, pages 21?29, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. 2005. Clustering with qualitative informa-
tion. Journal of Computer and System Sciences,
71(3):360?383.
Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yu-
val Rabani, and D. Sivakumar. 2005. On the hard-
ness of approximating multicut and sparsest-cut. In
In Proceedings of the 20th Annual IEEE Conference
on Computational Complexity, pages 144?153.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors.
a multilevel approach. IEEE Trans. Pattern Anal.
Mach. Intell., 29(11):1944?1957.
Sergio Ferra?ndez, Antonio Toral, O?scar Ferra?ndez, An-
tonio Ferra?ndez, and Rafael Mun?oz. 2007. Ap-
plying Wikipedia?s multilingual knowledge to cross-
lingual question answering. In NLDB, pages 352?
363.
Naveen Garg, Vijay V. Vazirani, and Mihalis Yan-
nakakis. 1996. Approximate max-flow min-
(multi)cut theorems and their applications. SIAM
Journal on Computing (SICOMP), 25:698?707.
Narendra Karmarkar. 1984. A new polynomial-time
algorithm for linear programming. In STOC ?84:
Proceedings of the 16th Annual ACM Symposium on
Theory of Computing, pages 302?311, New York,
NY, USA. ACM.
George Karypis and Vipin Kumar. 1998. A fast and
high quality multilevel scheme for partitioning irreg-
ular graphs. SIAM Journal on Scientific Computing,
20(1):359?392.
Subhash Khot. 2002. On the power of unique 2-prover
1-round games. In STOC ?02: Proceedings of the
34th Annual ACM Symposium on Theory of Com-
puting, pages 767?775, New York, NY, USA. ACM.
Tom Leighton and Satish Rao. 1999. Multicommodity
max-flow min-cut theorems and their use in design-
ing approximation algorithms. Journal of the ACM,
46(6):787?832.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the 16th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2007),
pages 233?242, New York, NY, USA. ACM.
D. Nguyen, A. Overwijk, C. Hauff, R.B. Trieschnigg,
D. Hiemstra, and F.M.G. Jong de. 2009. Wiki-
Translate: query translation for cross-lingual infor-
mation retrieval using only Wikipedia. In Carol
Peters, Thomas Deselaers, Nicola Ferro, and Julio
Gonzalo, editors, Evaluating Systems for Multilin-
gual and Multimodal Information Access, Lecture
Notes in Computer Science 5706, pages 58?65.
Jeff Pasternack and Dan Roth. 2009. Learning bet-
ter transliterations. In CIKM ?09: Proceeding of the
18th ACM Conference on Information and Knowl-
edge Management, pages 177?186, New York, NY,
USA. ACM.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
AAAI 2007: Proceedings of the 22nd Conference
on Artificial Intelligence, pages 1440?1445. AAAI
Press.
Carina Silberer, Wolodja Wentland, Johannes Knopp,
and Matthias Hartung. 2008. Building a multilin-
gual lexical resource for named entity disambigua-
tion, translation and transliteration. In European,
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of Wikipedia - a
classification-based approach. In Proceedings of the
AAAI 2008 Workshop on Wikipedia and Artifical In-
telligence.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International
World Wide Web conference, WWW, New York, NY,
USA. ACM Press.
Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang,
Jianhua Feng, and Lizhu Zhou. 2009. Comparing
stars: On approximating graph edit distance. Pro-
ceedings of the VLDB Endowment, 2(1):25?36.
853
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 233?237,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coupling Label Propagation and Constraints for Temporal Fact Extraction
Yafang Wang, Maximilian Dylla, Marc Spaniol and Gerhard Weikum
Max Planck Institute for Informatics, Saarbru?cken, Germany
{ywang|mdylla|mspaniol|weikum}@mpi-inf.mpg.de
Abstract
The Web and digitized text sources contain
a wealth of information about named entities
such as politicians, actors, companies, or cul-
tural landmarks. Extracting this information
has enabled the automated construction of large
knowledge bases, containing hundred millions
of binary relationships or attribute values about
these named entities. However, in reality most
knowledge is transient, i.e. changes over time,
requiring a temporal dimension in fact extrac-
tion. In this paper we develop a methodology
that combines label propagation with constraint
reasoning for temporal fact extraction. Label
propagation aggressively gathers fact candi-
dates, and an Integer Linear Program is used
to clean out false hypotheses that violate tem-
poral constraints. Our method is able to im-
prove on recall while keeping up with preci-
sion, which we demonstrate by experiments
with biography-style Wikipedia pages and a
large corpus of news articles.
1 Introduction
In recent years, automated fact extraction from Web
contents has seen significant progress with the emer-
gence of freely available knowledge bases, such as
DBpedia (Auer et al, 2007), YAGO (Suchanek et
al., 2007), TextRunner (Etzioni et al, 2008), or
ReadTheWeb (Carlson et al, 2010a). These knowl-
edge bases are constantly growing and contain cur-
rently (by example of DBpedia) several million enti-
ties and half a billion facts about them. This wealth
of data allows to satisfy the information needs of
advanced Internet users by raising queries from key-
words to entities. This enables queries like ?Who is
married to Prince Charles?? or ?Who are the team-
mates of Lionel Messi at FC Barcelona??.
However, factual knowledge is highly ephemeral:
Royals get married and divorced, politicians hold
positions only for a limited time and soccer players
transfer from one club to another. Consequently,
knowledge bases should be able to support more
sophisticated temporal queries at entity-level, such
as ?Who have been the spouses of Prince Charles
before 2000?? or ?Who are the teammates of Lionel
Messi at FC Barcelona in the season 2011/2012??.
In order to achieve this goal, the next big step is to
distill temporal knowledge from the Web.
Extracting temporal facts is a complex and time-
consuming endeavor. There are ?conservative? strate-
gies that aim at high precision, but they tend to suffer
from low recall. On the contrary, there are ?aggres-
sive? approaches that target at high recall, but fre-
quently suffer from low precision. To this end, we
introduce a method that allows us to gain maximum
benefit from both ?worlds? by ?aggressively? gath-
ering fact candidates and subsequently ?cleaning-up?
the incorrect ones. The salient properties of our ap-
proach and the novel contributions of this paper are
the following:
? A temporal fact extraction strategy that is able
to efficiently gather thousands of fact candidates
based on a handful of seed facts.
? An ILP solver incorporating constraints on tem-
poral relations among events (e.g., marriage of
a person must be non-overlapping in time).
? Experiments on real world news and Wikipedia
articles showing that we gain recall while keep-
ing up with precision.
2 Related Work
Recently, there have been several approaches that
aim at the extraction of temporal facts for the auto-
mated construction of large knowledge bases, but
233
time-aware fact extraction is still in its infancy. An
approach toward fact extraction based on coupled
semi-supervised learning for information extraction
(IE) is NELL (Carlson et al, 2010b). However, it
does neither incorporate constraints nor temporal-
ity. TIE (Ling and Weld, 2010) binds time-points
of events described in sentences, but does not dis-
ambiguate entities or combine observations to facts.
A pattern-based approach for temporal fact extrac-
tion is PRAVDA (Wang et al, 2011), which utilizes
label propagation as a semi-supervised learning strat-
egy, but does not incorporate constraints. Similarly,
TOB is an approach of extracting temporal business-
related facts from free text, which requires deep pars-
ing and does not apply constraints as well (Zhang et
al., 2008). In contrast, CoTS (Talukdar et al, 2012)
introduces a constraint-based approach of coupled
semi-supervised learning for IE, however not focus-
ing on the extraction part. Building on TimeML
(Pustejovsky et al, 2003) several works (Verhagen et
al., 2005; Mani et al, 2006; Chambers and Jurafsky,
2008; Verhagen et al, 2009; Yoshikawa et al, 2009)
identify temporal relationships in free text, but don?t
focus on fact extraction.
3 Framework
Facts and Observations. We aim to extract factual
knowledge transient over time from free text. More
specifically, we assume time T = [0, Tmax ] to
be a finite sequence of time-points with yearly
granularity. Furthermore, a fact consists of a
relation with two typed arguments and a time-
interval defining its validity. For instance, we write
worksForClub(Beckham,RMadrid)@[2003, 2008)
to express that Beckham played for Real Madrid
from 2003 to 2007. Since sentences containing a
fact and its full time-interval are sparse, we consider
three kinds of textual observations for each relation,
namely begin, during, and end. ?Beckham signed
for Real Madrid from Manchester United in 2003.?
includes both the begin observation of Beckham be-
ing with Real Madrid as well as the end observation
of working for Manchester. A positive seed fact is a
valid fact of a relation, while a negative seed fact is
incorrect (e.g., for relation worksForClub, a positive
seed fact is worksForClub(Beckham,RMadrid),
while worksForClub(Beckham,BMunich) is a
negative seed fact).
Framework. As depicted in Figure 1, our framework
is composed of four stages, where the first collects
candidate sentences, the second mines patterns from
the candidates sentences, the third extracts temporal
facts from the sentences utilizing the patterns and the
last removes noisy facts by enforcing constraints.
Preprocessing. We retrieve all sentences from the
corpus comprising at least two entities and a temporal
expression, where we use YAGO for entity recogni-
tion and disambiguation (cf. (Hoffart et al, 2011)).
Figure 1: System Overview
Pattern Analysis. A pattern is a n-gram based fea-
ture vector. It is generated by replacing entities
by their types, keeping only stemmed nouns, verbs
converted to present tense and the last preposition.
For example, considering ?Beckham signed for Real
Madrid from Manchester United in 2003.? the cor-
responding pattern for the end occurrence is ?sign
for CLUB from?. We quantify the strength of each
pattern by investigating how frequent the pattern oc-
curs with seed facts of a particular relation and how
infrequent it appears with negative seed facts.
Fact Candidate Gathering. Entity pairs that co-
occur with patterns whose strength is above a mini-
mum threshold become fact candidates and are fed
into the next stage of label propagation.
4 T-Fact Extraction
Building on (Wang et al, 2011) we utilize Label
Propagation (Talukdar and Crammer, 2009) to deter-
mine the relation and observation type expressed by
each pattern.
Graph. We create a graph G = (VF ??VP , E) having
one vertex v ? VF for each fact candidate observed
in the text and one vertex v ? VP for each pattern.
Edges between VF and VP are introduced whenever a
fact candidate appeared with a pattern. Their weight
is derived from the co-occurrence frequency. Edges
234
among VP nodes have weights derived from the n-
gram overlap of the patterns.
Labels. Moreover, we use one label for each observa-
tion type (begin, during, and end) of each relation and
a dummy label representing the unknown relation.
Objective Function. Let Y ? R|V|?|Labels|+ de-
note the graph?s initial label assignment, and Y? ?
R|V|?|Labels|+ stand for the estimated labels of all ver-
tices, Sl encode the seed?s weights on its diagonal,
and R?l contain zeroes except for the dummy label?s
column. Then, the objective function is:
L(Y?) =
?
`
[
(Y?` ? Y??`)TS`(Y?` ? Y??`)
+?1Y?T?`LY??` + ?2?Y??` ?R?`?
2
]
(1)
Here, the first term (Y?` ? Y??`)TS`(Y?` ? Y??`)
ensures that the estimated labels approximate the
initial labels. The labeling of neighboring vertices
is smoothed by ?1Y?T?`LY??`, where L refers to the
Laplacian matrix. The last term is a L2 regularizer.
5 Cleaning of Fact Candidates
To prune noisy t-facts, we compute a consistent sub-
set of t-facts with respect to temporal constraints (e.g.
joining a sports club takes place before leaving a
sports club) by an Integer Linear Program (ILP).
Variables. We introduce a variable xr ? {0, 1} for
each t-fact candidate r ? R, where 1 means the can-
didate is valid. Two variables xf,b, xf,e ? [0, Tmax ]
denote begin (b) and end (e) of time-interval of a fact
f ? F . Note, that many t-fact candidates refer to the
same fact f , since they share their entity pairs.
Objective Function. The objective function intends
to maximize the number of valid raw t-facts, where
wr is a weight obtained from the previous stage:
max
?
r?R
wr ? xr
Intra-Fact Constraints. xf,b and xf,e encode a
proper time-interval by adding the constraint:
?f ? F xf,b < xf,e
Considering only a single relation, we assume the
setsRb,Rd, andRe to comprise its t-fact candidates
with respect to the begin, during, and end observa-
tions. Then, we introduce the constraints
?l ? {b, e}, r ? Rl tl ? xr ? xf,l (2)
?l ? {b, e}, r ? Rl xf,l ? tl ? xr + (1? xr)Tmax (3)
?r ? Rd xf,b ? tb ? xr + (1? xr)Tmax (4)
?r ? Rd te ? xr ? xf,e (5)
where f has the same entity pair as r and tb, te are
begin and end of r?s time-interval. Whenever xr is
set to 1 for begin or end t-fact candidates, Eq. (2)
and Eq. (3) set the value of xf,b or xf,e to tb or te,
respectively. For each during t-fact candidate with
xr = 1, Eq. (4) and Eq. (5) enforce xf,b ? tb and
te ? xf,e.
Inter-Fact Constraints. Since we can refer to a fact
f ?s time interval by xf,b and xf,e and the connectives
of Boolean Logic can be encoded in ILPs (Karp,
1972), we can use all temporal constraints expressible
by Allen?s Interval Algebra (Allen, 1983) to specify
inter-fact constraints. For example, we leverage this
by prohibiting marriages of a single person from
overlapping in time.
Previous Work. In comparison to (Talukdar et al,
2012), our ILP encoding is time-scale invariant. That
is, for the same data, if the granularity of T is
changed from months to seconds, for example, the
size of the ILP is not affected. Furthermore, because
we allow all relations of Allen?s Interval Algebra, we
support a richer class of temporal constraints.
6 Experiments
Corpus. Experiments are conducted in the soccer
and the celebrity domain by considering the works-
ForClub and isMarriedTo relation, respectively. For
each person in the ?FIFA 100 list? and ?Forbes 100
list? we retrieve their Wikipedia article. In addition,
we obtained about 80,000 documents for the soccer
domain and 370,000 documents for the celebrity do-
main from BBC, The Telegraph, Times Online and
ESPN by querying Google?s News Archive Search1
in the time window from 1990-2011. All hyperpa-
rameters are tuned on a separate data-set.
Seeds. For each relation we manually select the 10
positive and negative fact candidates with highest
occurrence frequencies in the corpus as seeds.
Evaluation. We evaluate precision by randomly sam-
pling 50 (isMarriedTo) and 100 (worksForClub) facts
for each observation type and manually evaluating
them against the text documents. All experimental
data is available for download from our website2.
6.1 Pipeline vs. Joint Model
Setting. In this experiment we compare the perfor-
mance of the pipeline being stages 3 and 4 in Figure
1news.google.com/archivesearch
2www.mpi-inf.mpg.de/yago-naga/pravda/
235
1 and a joint model in form of an ILP solving the
t-fact extraction and noise cleaning at the same time.
Hence, the joint model resembles (Roth and Yih,
2004) extended by Section 5?s temporal constraints.
Re
lat
ion Observation
Label Propagation ILP for T-Fact Extraction
Precision # Obs. Precision # Obs.
wo
rk
sF
or
Cl
ub begin 80% 2537 81% 2426 W
ithoutNoiseCleaning
during 78% 2826 86% 1153
end 65% 440 50% 550
isM
ar
rie
dT
o begin 52% 195 28% 232
during 76% 92 6% 466
end 62% 50 2% 551
wo
rk
sF
or
Cl
ub begin 85% 2469 87% 2076
W
ithNoiseCleaning
during 85% 2761 79% 1434
end 74% 403 72% 275
isM
ar
rie
dT
o begin 64% 177 74% 67
during 79% 89 88% 61
end 70% 47 71% 28
Table 1: Pipeline vs. Joint Model
Results. Table 1 shows the results on the pipeline
model (lower-left), joint model (lower-right), label-
propagation w/o noise cleaning (upper-left), and ILP
for t-fact extraction w/o noise cleaning (upper-right).
Analysis. Regarding the upper part of Table 1 the
pattern-based extraction works very well for works-
ForClub, however it fails on isMarriedTo. The reason
is, that the types of worksForClub distinguish the
patterns well from other relations. In contrast, isMar-
riedTo?s patterns interfere with other person-person
relations making constraints a decisive asset. When
comparing the joint model and the pipeline model,
the former sacrifices recall in order to keep up with
the latter?s precision level. That is because the joint
model?s ILP decides with binary variables on which
patterns to accept. In contrast, label propagation ad-
dresses the inherent uncertainty by providing label
assignments with confidence numbers.
6.2 Increasing Recall
Setting. In a second experiment, we move the t-fact
extraction stage away from high precision towards
higher recall, where the successive noise cleaning
stage attempts to restore the precision level.
Results. The columns of Table 2 show results for
different values of ?1 of Eq. (1). From left to right,
we used ?1 = e?1, 0.6, 0.8 for worksForClub and
?1 = e?2, e?1, 0.6 for isMarriedTo. The table?s up-
per part reports on the output of stage 3, whereas the
lower part covers the facts returned by noise cleaning.
Analysis. For the conservative setting label propa-
gation produces high precision facts with only few
inconsistencies, so the noise cleaning stage has no
effect, i.e. no pruning takes place. This is the set-
ting usual pattern-based approaches without cleaning
stage are working in. In contrast, for the standard set-
ting (coinciding with Table 1?s left column) stage 3
yields less precision, but higher recall. Since there are
more inconsistencies in this setup, the noise cleaning
stage accomplishes precision gains compensating for
the losses in the previous stage. In the relaxed setting
precision drops too low, so the noise cleaning stage is
unable to figure out the truly correct facts. In general,
the effects on worksForClub are weaker, since in this
relation the constraints are less influential.
Conservative Standard Relaxed
Prec. # Obs. Prec. # Obs. Prec. # Obs.
wo
rk
sF
or
Cl
ub begin 83% 2443 80% 2537 80% 2608 W
ithoutNoiseCleaning
during 81% 2523 78% 2826 76% 2928
end 77% 377 65% 440 62% 501
isM
ar
rie
dT
o begin 72% 112 52% 195 44% 269
during 90% 63 76% 92 52% 187
end 67% 37 62% 50 36% 116
wo
rk
sF
or
Cl
ub begin 83% 2389 85% 2469 84% 2536
W
ithNoiseCleaning
during 88% 2474 85% 2761 75% 2861
end 79% 349 72% 403 70% 463
isM
ar
rie
dT
o begin 72% 111 64% 177 46% 239
during 90% 62 79% 89 54% 177
end 69% 36 68% 47 38% 110
Table 2: Increasing Recall.
7 Conclusion
In this paper we have developed a method that com-
bines label propagation with constraint reasoning
for temporal fact extraction. Our experiments have
shown that best results can be achieved by applying
?aggressive? label propagation with a subsequent ILP
for ?clean-up?. By coupling both approaches we
achieve both high(er) precision and high(er) recall.
Thus, our method efficiently extracts high quality
temporal facts at large scale.
236
Acknowledgements
This work is supported by the 7th Framework IST
programme of the European Union through the fo-
cused research project (STREP) on Longitudinal An-
alytics of Web Archive data (LAWA) under contract
no. 258105.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?843,
November.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, and Zachary Ives. 2007. Dbpedia: A nu-
cleus for a web of open data. In In 6th Intl Semantic
Web Conference, Busan, Korea, pages 11?15. Springer.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010a. Toward an architecture for never-ending lan-
guage learning. In AAAI, pages 1306?1313.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010b.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM 2010).
Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In EMNLP, pages 698?706.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, Decem-
ber.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol, Ste-
fan Thater, and Gerhard Weikum. 2011. Robust disam-
biguation of named entities in text. In Proc. of EMNLP
2011: Conference on Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK, July
2731, pages 782?792.
Richard M. Karp. 1972. Reducibility among combinato-
rial problems. In Complexity of Computer Computa-
tions, pages 85?103.
Xiao Ling and Daniel S. Weld. 2010. Temporal infor-
mation extraction. In Proceedings of the AAAI 2010
Conference, pages 1385 ? 1390, Atlanta, Georgia, USA,
July 11-15. Association for the Advancement of Artifi-
cial Intelligence.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In In ACL-06, pages 17?18.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Robust
specification of event and temporal expressions in text.
In New Directions in Question Answering, pages 28?
34.
Dan Roth and Wen-Tau Yih, 2004. A Linear Programming
Formulation for Global Inference in Natural Language
Tasks, pages 1?8.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th International
Conference on World Wide Web, pages 697?706, New
York, NY, USA. ACM.
Partha Pratim Talukdar and Koby Crammer. 2009. New
regularized algorithms for transductive learning. In
Proceedings of the European Conference on Machine
Learning and Knowledge Discovery in Databases: Part
II, ECML PKDD ?09, pages 442?457, Berlin, Heidel-
berg. Springer-Verlag.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts.
In Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining (WSDM), Seattle,
Washington, USA, February. Association for Comput-
ing Machinery.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert Knip-
pen, Seok Bae Jang, Jessica Littman, Anna Rumshisky,
John Phillips, and James Pustejovsky. 2005. Automat-
ing temporal annotation with TARSQI. In ACL ?05:
Proceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 81?84, Morristown, NJ,
USA. Association for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark
Hepple, Jessica Moszkowicz, and James Pustejovsky.
2009. The tempeval challenge: identifying temporal
relations in text. Language Resources and Evaluation,
43:161?179.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM ?11,
pages 837?846, New York, NY, USA. ACM.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume 1
- Volume 1, ACL ?09, pages 405?413, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Qi Zhang, Fabian Suchanek, and Gerhard Weikum. 2008.
TOB: Timely ontologies for business relations. In 11th
International Workshop on Web and Databases 2008
(WebDB 2008), Vancouver, Canada. ACM.
237
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
UWN: A Large Multilingual Lexical Knowledge Base
Gerard de Melo
ICSI Berkeley
demelo@icsi.berkeley.edu
Gerhard Weikum
Max Planck Institute for Informatics
weikum@mpi-inf.mpg.de
Abstract
We present UWN, a large multilingual lexi-
cal knowledge base that describes the mean-
ings and relationships of words in over 200
languages. This paper explains how link pre-
diction, information integration and taxonomy
induction methods have been used to build
UWN based on WordNet and extend it with
millions of named entities from Wikipedia.
We additionally introduce extensions to cover
lexical relationships, frame-semantic knowl-
edge, and language data. An online interface
provides human access to the data, while a
software API enables applications to look up
over 16 million words and names.
1 Introduction
Semantic knowledge about words and named enti-
ties is a fundamental building block both in vari-
ous forms of language technology as well as in end-
user applications. Examples of the latter include
word processor thesauri, online dictionaries, ques-
tion answering, and mobile services. Finding se-
mantically related words is vital for query expan-
sion in information retrieval (Gong et al, 2005),
database schema matching (Madhavan et al, 2001),
sentiment analysis (Godbole et al, 2007), and ontol-
ogy mapping (Jean-Mary and Kabuka, 2008). Fur-
ther uses of lexical knowledge include data cleaning
(Kedad and M?tais, 2002), visual object recognition
(Marsza?ek and Schmid, 2007), and biomedical data
analysis (Rubin and others, 2006).
Many of these applications have used English-
language resources like WordNet (Fellbaum, 1998).
However, a more multilingual resource equipped
with an easy-to-use API would not only enable us to
perform all of the aforementioned tasks in additional
languages, but also to explore cross-lingual applica-
tions like cross-lingual IR (Etzioni et al, 2007) and
machine translation (Chatterjee et al, 2005).
This paper describes a new API that makes lexical
knowledge about millions of items in over 200 lan-
guages available to applications, and a correspond-
ing online user interface for users to explore the data.
We first describe link prediction techniques used to
create the multilingual core of the knowledge base
with word sense information (Section 2). We then
outline techniques used to incorporate named enti-
ties and specialized concepts (Section 3) and other
types of knowledge (Section 4). Finally, we describe
how the information is made accessible via a user in-
terface (Section 5) and a software API (Section 6).
2 The UWN Core
UWN (de Melo and Weikum, 2009) is based on
WordNet (Fellbaum, 1998), the most popular lexi-
cal knowledge base for the English language. Word-
Net enumerates the senses of a word, providing a
short description text (gloss) and synonyms for each
meaning. Additionally, it describes relationships be-
tween senses, e.g. via the hyponymy/hypernymy re-
lation that holds when one term like ?publication? is
a generalization of another term like ?journal?.
This model can be generalized by allowing words
in multiple languages to be associated with a mean-
ing (without, of course, demanding every meaning
be lexicalized in every language). In order to ac-
complish this at a large scale, we automatically link
151
terms in different languages to the meanings already
defined in WordNet. This transforms WordNet into
a multilingual lexical knowledge base that covers
not only English terms but hundreds of thousands
of terms from many different languages.
Unfortunately, a straightforward translation runs
into major difficulties because of homonyms and
synonyms. For example, a word like ?bat? has 10
senses in the English WordNet, but a German trans-
lation like ?Fledermaus? (the animal) only applies to
a small subset of those senses (cf. Figure 1). This
challenge can be approached by disambiguating us-
ing machine learning techniques.
Figure 1: Word sense ambiguity
Knowledge Extraction An initial input knowl-
edge base graph G0 is constructed by ex-
tracting information from existing wordnets,
translation dictionaries including Wiktionary
(http://www.wiktionary.org), multilingual thesauri
and ontologies, and parallel corpora. Additional
heuristics are applied to increase the density of the
graph and merge near-duplicate statements.
Link Prediction A sequence of knowledge graphs
Gi are iteratively derived by assessing paths from
a new term x to an existing WordNet sense z via
some English translation y covered by WordNet. For
instance, the German ?Fledermaus? has ?bat? as a
translation and hence initially is tentatively linked to
all senses of ?bat? with a confidence of 0. In each
iteration, the confidence values are then updated to
reflect how likely it seems that those links are cor-
rect. The confidences are predicted using RBF-
kernel SVM models that are learnt from a training
set of labelled links between non-English words and
senses. The feature space is constructed using a se-
ries of graph-based statistical scores that represent
properties of the previous graph Gi?1 and addition-
ally make use of measures of semantic relatedness
and corpus frequencies. The most salient features
xi(x, z) are of the form:
?
y??(x,Gi?1)
?(x, y) sim?x(y, z) (1)
?
y??(x,Gi?1)
?(x, y) sim?x(y, z)
sim?x(y, z) + dissimx(y, z)
(2)
The formulae consider the out-neighbourhood y ?
?(x,Gi?1) of x, i.e. its translations, and then ob-
serve how strongly each y is tied to z. The function
sim? computes the maximal similarity between any
sense of y and the current sense z. The dissim func-
tion computes the sum of dissimilarities between
senses of y and z, essentially quantifying how many
alternatives there are to z. Additional weighting
functions ?, ? are used to bias scores towards senses
that have an acceptable part-of-speech and senses
that are more frequent in the SemCor corpus.
Relying on multiple iterations allows us to draw
on multilingual evidence for greater precision and
recall. For instance, after linking the German ?Fled-
ermaus? to the animal sense of ?bat?, we may be able
to infer the same for the Turkish translation ?yarasa?.
Results We have successfully applied these tech-
niques to automatically create UWN, a large-scale
multilingual wordnet. Evaluating random samples
of term-sense links, we find (with Wilson-score in-
tervals at ? = 0.05) that for French the preci-
sion is 89.2% ? 3.4% (311 samples), for German
85.9% ? 3.8% (321 samples), and for Mandarin
Chinese 90.5% ? 3.3% (300 samples). The over-
all number of new term-sense links is 1,595,763, for
822,212 terms in over 200 languages. These figures
can be grown further if the input is extended by tap-
ping on additional sources of translations.
3 MENTA: Named Entities and
Specialized Concepts
The UWN Core is extended by incorporating large
amounts of named entities and language- and
domain-specific concepts from Wikipedia (de Melo
and Weikum, 2010a). In the process, we also obtain
152
human-readable glosses in many languages, links to
images, and other valuable information. These ad-
ditions are not simply added as a separate knowl-
edge base, but fully connected and integrated with
the core. In particular, we create a mapping between
Wikipedia and WordNet in order to merge equiva-
lent entries and we use taxonomy construction meth-
ods in order to attach all new named entities to their
most likely classes, e.g. ?Haight-Ashbury? is linked
to a WordNet sense of the word ?neighborhood?.
Information Integration Supervised link predic-
tion, similar to the method presented in Section 2, is
used in order to attach Wikipedia articles to semanti-
cally equivalent WordNet entries, while also exploit-
ing gloss similarity as an additional feature. Addi-
tionally, we connect articles from different multilin-
gual Wikipedia editions via their cross-lingual inter-
wiki links, as well as categories with equivalent ar-
ticles and article redirects with redirect targets.
We then consider connected components of di-
rectly or transitively linked items. In the ideal case,
such a connected component consists of a number
of items all describing the same concept or entity, in-
cluding articles from different versions of Wikipedia
and perhaps also categories or WordNet senses.
Unfortunately, in many cases one obtains con-
nected components that are unlikely to be correct,
because multiple articles from the same Wikipedia
edition or multiple incompatible WordNet senses are
included in the same component. This can be due
to incorrect links produced by the supervised link
prediction, but often even the original links from
Wikipedia are not consistent.
In order to obtain more consistent connected com-
ponents, we use combinatorial optimization meth-
ods to delete certain links. In particular, for each
connected component to be analysed, an Integer
Linear Program formalizes the objective of mini-
mizing the costs for deleted edges and the costs for
ignoring soft constraints. The basic aim is that of
deleting as few edges as possible while simultane-
ously ensuring that the graph becomes as consistent
as possible. In some cases, there is overwhelming
evidence indicating that two slightly different arti-
cles should be grouped together, while in other cases
there might be little evidence for the correctness of
an edge and so it can easily be deleted with low cost.
While obtaining an exact solution is NP-hard and
APX-hard, we can solve the corresponding Linear
Program using a fast LP solver like CPLEX and sub-
sequently apply region growing techniques to obtain
a solution with a logarithmic approximation guaran-
tee (de Melo and Weikum, 2010b).
The clean connected components resulting from
this process can then be merged to form aggregate
entities. For instance, given WordNet?s standard
sense for ?fog?, water vapor, we can check which
other items are in the connected component and
transfer all information to the WordNet entry. By
extracting snippets of text from the beginning of
Wikipedia articles, we can add new gloss descrip-
tions for fog in Arabic, Asturian, Bengali, and many
other languages. We can also attach pictures show-
ing fog to the WordNet word sense.
Taxonomy Induction The above process con-
nects articles to their counterparts in WordNet. In
the next step, we ensure that articles without any di-
rect counterpart are linked to WordNet as well, by
means of taxonomic hypernymy/instance links (de
Melo and Weikum, 2010a).
We generate individual hypotheses about likely
parents of entities. For instance, articles are con-
nected to their Wikipedia categories (if these are not
assessed to be mere topic descriptors) and categories
are linked to parent categories, etc. In order to link
categories to possible parent hypernyms in Word-
Net, we adapt the approach proposed for YAGO
(Suchanek et al, 2007) of determining the headword
of the category name and disambiguating it.
Since we are dealing with a multilingual scenario
that draws on articles from different multilingual
Wikipedia editions that all need to be connected to
WordNet, we apply an algorithm that jointly looks
at an entity and all of its parent candidates (not just
from an individual article, but all articles in the same
connected component) as well as superordinate par-
ent candidates (parents of parents, etc.), as depicted
in Figure 2. We then construct a Markov chain based
on this graph of parents that also incorporates the
possibility of random jumps from any parent back
to the current entity under consideration. The sta-
tionary probability of this Markov chain, which can
be obtained using random walk methods, provides
us a ranking of the most likely parents.
153
Figure 2: Noisy initial edges (left) and cleaned, integrated output (right), shown in a simplified form
Figure 3: UWN with named entities
Results Overall, we obtain a knowledge base with
5.4 million concepts or entities and 16.7 million
words or names associated with them from over
200 languages. Over 2 million named entities come
only from non-English Wikipedia editions, but their
taxonomic links to WordNet still have an accuracy
around 90%. An example excerpt is shown in Fig-
ure 3, with named entities connected to higher-level
classes in UWN, all with multilingual labels.
4 Other Extensions
Word Relationships Another plugin provides
word relationships and properties mined from Wik-
tionary. These include derivational and etymologi-
cal word relationships (e.g. that ?grotesque? comes
from the Italian ?grotta?: grotto, artificial cave), al-
ternative spellings (e.g. ?encyclop?dia? for ?en-
cyclopedia?), common misspellings (e.g. ?minis-
cule? for ?minuscule?), pronunciation information
(e.g. how to pronounce ?nuclear?), and so on.
Frame-Semantic Knowledge Frame semantics is
a cognitively motivated theory that describes words
in terms of the cognitive frames or scenarios that
they evoke and the corresponding participants in-
volved in them. For a given frame, FrameNet
provides definitions, involved participants, associ-
ated words, and relationships. For instance, the
Commerce_goods-transfer frame normally
involves a seller and a buyer, among other things,
and different words like ?buy? and ?sell? can be cho-
sen to describe the same event.
Such detailed knowledge about scenarios is
largely complementary in nature to the sense re-
lationships that WordNet provides. For instance,
WordNet emphasizes the opposite meaning of the
words ?happy? and ?unhappy?, while frame seman-
tics instead emphasizes the cognitive relatedness of
words like ?happy?, ?unhappy?, ?astonished?, and
?amusement?, and explains that typical participants
include an experiencer who experiences the emo-
tions and external stimuli that evoke them. There
have been individual systems that made use of both
forms of knowledge (Shi and Mihalcea, 2005; Cop-
pola and others, 2009), but due to their very different
nature, there is currently no simple way to accom-
plish this feat. Our system addresses this by seam-
lessly integrating frame semantic knowledge into the
system. We draw on FrameNet (Baker et al, 1998),
the most well-known computational instantiation of
frame semantics. While the FrameNet project is
generally well-known, its use in practical applica-
154
tions has been limited due to the lack of easy-to-use
APIs and because FrameNet alne does not cover as
many words as WordNet. Our API simultaneously
provides access to both sources.
Language information For a given language, this
extension provides information such as relevant
writing systems, geographical regions, identifica-
tion codes, and names in many different languages.
These are all integrated into WordNet?s hypernym
hierarchy, i.e. from language families like the Sinitic
languages one may move down to macrolanguages
like Chinese, and then to more specific forms like
Mandarin Chinese, dialect groups like Ji-Lu Man-
darin, or even dialects of particular cities.
The information is obtained from ISO standards,
the Unicode CLDR as well as Wikipedia and then
integrated with WordNet using the information in-
tegration strategies described above (de Melo and
Weikum, 2008). Additionally, information about
writing systems is taken from the Unicode CLDR
and information about individual characters is ob-
tained from the Unicode, Unihan, and Hanzi Data
databases. For instance, the Chinese character ???
is connected to its radical component ??? and to its
pronunciation component ???.
5 Integrated Query Interface and Wiki
We have developed an online interface that provides
access to our data to interested researchers (yago-
knowledge.org/uwn/ ), as shown in Figure 4.
Interactive online interfaces offer new ways of in-
teracting with lexical knowledge that are not possi-
ble with traditional print dictionaries. For example,
a user wishing to find a Spanish word for the concept
of persuading someone not to believe something
might look up the word ?persuasion? and then navi-
gate to its antonym ?dissuasion? to find the Spanish
translation. A non-native speaker of English looking
up the word ?tercel? might find it helpful to see pic-
tures available for the related terms ?hawk? or ?fal-
con? ? a Google Image search for ?tercel? merely de-
livers images of Toyota Tercel cars.
While there have been other multilingual inter-
faces to WordNet-style lexical knowledge in the past
(Pianta et al, 2002; Atserias and others, 2004), these
provide less than 10 languages as of 2012. The most
similar resource is BabelNet (Navigli and Ponzetto,
2010), which contains multilingual synsets but does
not connect named entities from Wikipedia to them
in a multilingual taxonomy.
Figure 4: Part of Online Interface
6 Integrated API
Our goal is to make the knowledge that we have de-
rived available for use in applications. To this end,
we have developed a fully downloadable API that
can easily be used in several different programming
languages. While there are many existing APIs for
WordNet and other lexical resources (e.g. (Judea et
al., 2011; Gurevych and others, 2012)), these don?t
provide a comparable degree of integrated multilin-
gual and taxonomic information.
Interface The API can be used by initializing an
accessor object and possibly specifying the list of
plugins to be loaded. Depending on the particular
application, one may choose only Princeton Word-
Net and the UWN Core, or one may want to in-
clude named entities from Wikipedia and frame-
semantic knowledge derived from FrameNet, for in-
stance. The accessor provides a simple graph-based
lookup API as well as some convenience methods
for common types of queries.
An additional higher-level API module imple-
ments several measures of semantic relatedness. It
also provides a simple word sense disambiguation
method that, given a tokenized text with part-of-
155
speech and lemma annotations, selects likely word
senses by choosing the senses (with matching part-
of-speech) that are most similar to words in the con-
text. Note that these modules go beyond existing
APIs because they operate on words in many differ-
ent languages and semantic similarity can even be
assessed across languages.
Data Structures Under the hood, each plugin re-
lies on a disk-based associative array to store the
knowledge base as a labelled multi-graph. The out-
going labelled edges of an entity are saved on disk in
a serialized form, including relation names and rela-
tion weights. An index structure allows determining
the position of such records on disk.
Internally, this index structure is implemented as
a linearly-probed hash table that is also stored ex-
ternally. Note that such a structure is very efficient
in this scenario, because the index is used as a read-
only data store by the API. Once an index has been
created, write operations are no longer performed,
so B+ trees and similar disk-based balanced tree in-
dices commonly used in relational database manage-
ment systems are not needed. The advantage is that
this enables faster lookups, because retrieval opera-
tions normally require only two disk reads per plu-
gin, one to access a block in the index table, and
another to access a block of actual data.
7 Conclusion
UWN is an important new multilingual lexical re-
source that is now freely available to the community.
It has been constructed using sophisticated knowl-
edge extraction, link prediction, information integra-
tion, and taxonomy induction methods. Apart from
an online querying and browsing interface, we have
also implemented an API that facilitates the use of
the knowledge base in applications.
References
Jordi Atserias et al 2004. The MEANING multilingual
central repository. In Proc. GWC 2004.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc.
COLING-ACL 1998.
Niladri Chatterjee, Shailly Goyal, and Anjali Naithani.
2005. Resolving pattern ambiguity for English to
Hindi machine translation using WordNet. In Proc.
Workshop Translation Techn. at RANLP 2005.
Bonaventura Coppola et al 2009. Frame detection over
the Semantic Web. In Proc. ESWC.
Gerard de Melo and Gerhard Weikum. 2008. Language
as a foundation of the Semantic Web. In Proc. ISWC.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proc. CIKM 2009.
Gerard de Melo and Gerhard Weikum. 2010a. MENTA:
Inducing multilingual taxonomies from Wikipedia. In
Proc. CIKM 2010.
Gerard de Melo and Gerhard Weikum. 2010b. Untan-
gling the cross-lingual link structure of Wikipedia. In
Proc. ACL 2010.
Oren Etzioni, Kobi Reiter, Stephen Soderland, and Mar-
cus Sammer. 2007. Lexical translation with applica-
tion to image search on the Web. In Proc. MT Summit.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for news
and blogs. In Proc. ICWSM.
Zhiguo Gong, Chan Wa Cheang, and Leong Hou U.
2005. Web query expansion by WordNet. In Proc.
DEXA 2005.
Iryna Gurevych et al 2012. Uby: A large-scale uni-
fied lexical-semantic resource based on LMF. In Proc.
EACL 2012.
Yves R. Jean-Mary and Mansur R. Kabuka. 2008. AS-
MOV: Results for OAEI 2008. In Proc. OM 2008.
Alex Judea, Vivi Nastase, and Michael Strube. 2011.
WikiNetTk ? A tool kit for embedding world knowl-
edge in NLP applications. In Proc. IJCNLP 2011.
Zoubida Kedad and Elisabeth M?tais. 2002. Ontology-
based data cleaning. In Proc. NLDB 2002.
Jayant Madhavan, P. Bernstein, and E. Rahm. 2001.
Generic schema matching with Cupid. In Proc. VLDB.
Marcin Marsza?ek and C. Schmid. 2007. Semantic hier-
archies for visual object recognition. In Proc. CVPR.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proc. ACL 2010.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing an aligned
multilingual database. In Proc. GWC.
Daniel L. Rubin et al 2006. National Center for Biomed-
ical Ontology. OMICS, 10(2):185?98.
Lei Shi and Rada Mihalcea. 2005. Putting the pieces to-
gether: Combining FrameNet, VerbNet, and WordNet
for robust semantic parsing. In Proc. CICLing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proc. WWW 2007.
156
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1488?1497,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fine-grained Semantic Typing of Emerging Entities
Ndapandula Nakashole, Tomasz Tylenda, Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
{nnakasho,ttylenda,weikum}@mpi-inf.mpg.de
Abstract
Methods for information extraction (IE)
and knowledge base (KB) construction
have been intensively studied. However, a
largely under-explored case is tapping into
highly dynamic sources like news streams
and social media, where new entities are
continuously emerging. In this paper, we
present a method for discovering and se-
mantically typing newly emerging out-of-
KB entities, thus improving the freshness
and recall of ontology-based IE and im-
proving the precision and semantic rigor
of open IE. Our method is based on a prob-
abilistic model that feeds weights into in-
teger linear programs that leverage type
signatures of relational phrases and type
correlation or disjointness constraints. Our
experimental evaluation, based on crowd-
sourced user studies, show our method
performing significantly better than prior
work.
1 Introduction
A large number of knowledge base (KB) con-
struction projects have recently emerged. Promi-
nent examples include Freebase (Bollacker 2008)
which powers the Google Knowledge Graph, Con-
ceptNet (Havasi 2007), YAGO (Suchanek 2007),
and others. These KBs contain many millions of
entities, organized in hundreds to hundred thou-
sands of semantic classes, and hundred millions
of relational facts between entities. However, de-
spite these impressive advances, there are still ma-
jor limitations regarding coverage and freshness.
Most KB projects focus on entities that appear in
Wikipedia (or other reference collections such as
IMDB), and very few have tried to gather entities
?in the long tail? beyond prominent sources. Vir-
tually all projects miss out on newly emerging en-
tities that appear only in the latest news or social
media. For example, the Greenlandic singer Nive
Nielsen has gained attention only recently and is
not included in any KB (a former Wikipedia article
was removed because it ?does not indicate the im-
portance or significance of the subject?), and the
resignation of BBC director Entwistle is a recently
new entity (of type event).
Goal. Our goal in this paper is to discover emerg-
ing entities of this kind on the fly as they become
noteworthy in news and social-media streams. A
similar theme is pursued in research on open infor-
mation extraction (open IE) (Banko 2007; Fader
2011; Talukdar 2010; Venetis 2011; Wu 2012),
which yields higher recall compared to ontology-
style KB construction with canonicalized and se-
mantically typed entities organized in prespecified
classes. However, state-of-the-art open IE meth-
ods extract all noun phrases that are likely to de-
note entities. These phrases are not canonical-
ized, so the same entity may appear under many
different names, e.g., ?Mr. Entwistle?, ?George
Entwistle?, ?the BBC director?, ?BBC head En-
twistle?, and so on. This is a problem because
names and titles are ambiguous, and this hampers
precise search and concise results.
Our aim is for all recognized and newly dis-
covered entities to be semantically interpretable
by having fine-grained types that connect them
to KB classes. The expectation is that this will
boost the disambiguation of known entity names
and the grouping of new entities, and will also
strengthen the extraction of relational facts about
entities. For informative knowledge, new entities
must be typed in a fine-grained manner (e.g., gui-
tar player, blues band, concert, as opposed to crude
types like person, organization, event).
Strictly speaking, the new entities that we cap-
1488
ture are typed noun phrases. We do not attempt
any cross-document co-reference resolution, as
this would hardly work with the long-tail na-
ture and sparse observations of emerging entities.
Therefore, our setting resembles the established
task of fine-grained typing for noun phrases (Fleis-
chmann 2002), with the difference being that we
disregard common nouns and phrases for promi-
nent in-KB entities and instead exclusively focus
on the difficult case of phrases that likely denote
new entities. The baselines to which we compare
our method are state-of-the-art methods for noun-
phrase typing (Lin 2012; Yosef 2012).
Contribution. The solution presented in this
paper, called PEARL, leverages a repository of
relational patterns that are organized in a type-
signature taxonomy. More specifically, we har-
ness the PATTY collection consisting of more
than 300,000 typed paraphrases (Nakashole 2012).
An example of PATTY?s expressive phrases is:
?musician? * cover * ?song? for a musician per-
forming someone else?s song. When extract-
ing noun phrases, PEARL also collects the co-
occurring PATTY phrases. The type signatures of
the relational phrases are cues for the type of the
entity denoted by the noun phrase. For example,
an entity named Snoop Dogg that frequently co-
occurs with the ?singer? * distinctive voice in *
?song? pattern is likely to be a singer. Moreover,
if one entity in a relational triple is in the KB and
can be properly disambiguated (e.g., a singer), we
can use a partially bound pattern to infer the type
of the other entity (e.g., a song) with higher confi-
dence.
In this line of reasoning, we also leverage the
common situation that many input sentences con-
tain one entity registered in the KB and one novel
or unknown entity. Known entities are recognized
and mapped to the KB using a recent tool for
named entity disambiguation (Hoffart 2011). For
cleaning out false hypotheses among the type can-
didates for a new entity, we devised probabilistic
models and an integer linear program that consid-
ers incompatibilities and correlations among entity
types.
In summary, our contribution in this paper is
a model for discovering and ontologically typ-
ing out-of-KB entities, using a fine-grained type
system and harnessing relational paraphrases with
type signatures for probabilistic weight computa-
tion. Crowdsourced quality assessments demon-
strate the accuracy of our model.
2 Detection of New Entities
To detect noun phrases that potentially refer to en-
tities, we apply a part-of-speech tagger to the in-
put text. For a given noun phrase, there are four
possibilities: a) The noun phrase refers to a gen-
eral concept (a class or abstract concept), not an
individual entity. b) The noun phrase is a known
entity that can be directly mapped to the knowl-
edge base. c) The noun phrase is a new name for
a known entity. d) The noun phrase is a new entity
not known to the knowledge base at all. In this pa-
per, our focus is on case d); all other cases are out
of the scope of this paper.
We use an extensive dictionary of surface forms
for in-KB entities (Hoffart 2012), to determine if
a name or phrase refers to a known entity. If a
phrase does not have any match in the dictionary,
we assume that it refers to a new entity. To decide
if a noun phrase is a true entity (i.e., an individ-
ual entity that is a member of one or more lexi-
cal classes) or a non-entity (i.e., a common noun
phrase that denotes a class or a general concept),
we base the decision on the following hypothesis
(inspired by and generalizing (Bunescu 2006): A
given noun phrase, not known to the knowledge
base, is a true entity if its headword is singular
and is consistently capitalized (i.e., always spelled
with the first letter in upper case).
3 Typing Emerging Entities
To deduce types for new entities we propose to
align new entities along the type signatures of pat-
terns they occur with. In this manner we use the
patterns to suggest types for the entities they occur
with. In particular, we infer entity types from pat-
tern type signatures. Our approach builds on the
following hypothesis:
Hypothesis 3.1 (Type Alignment Hypothesis)
For a given pattern such as ?actor??s character
in ?movie?, we assume that an entity pair (x, y)
frequently occurring with the pattern in text
implies that x and y are of the types ?actor? and
?movie?, respectively.
Challenges and Objective. While the type align-
ment hypothesis works as a starting point, it in-
troduces false positives. Such false positives stem
1489
from the challenges of polysemy, fuzzy pattern
matches, and incorrect paths between entities.
With polysemy, the same lexico-syntactic pattern
can have different type signatures. For example,
the following are three different patterns: ?singer?
released ?album?, ?music band? released ?album?,
?company? released ?product?. For an entity pair
(x, y) occurring with the pattern ?released?, x can
be one of three different types.
We cannot expect that the phrases we extract in
text will be exact matches of the typed relational
patterns learned by PATTY. Therefore, for better
recall, we must accept fuzzy matches. Quite often
however, the extracted phrase matches multiple re-
lational patterns to various degrees. Each of the
matched relational patterns has its own type sig-
nature. The type signatures of the various matched
patterns can be incompatible with one another.
The problem of incorrect paths between entities
emerges when a pair of entities occurring in the
same sentence do not stand in a true subject-object
relation. Dependency parsing does not adequately
solve the issue. Web sources contain a plethora
of sentences that are not well-formed. Such sen-
tences mislead the dependency parser to extract
wrong dependencies.
Our solution takes into account polysemy, fuzzy
matches, as well as issues stemming from poten-
tial incorrect-path limitations. We define and solve
the following optimization problem:
Definition 1 (Type Inference Optimization)
Given all the candidate types for x, find the
best types or ?strongly supported? types for x.
The final solution must satisfy type disjointness
constraints. Type disjointness constraints are
constraints that indicate that, semantically, a pair
of types cannot apply to the same entity at the
same time. For example, a ?university? cannot be
a ?person?.
We also study a relaxation of type disjointness
constraints through the use of type correlation con-
straints. Our task is therefore twofold: first, gen-
erate candidate types for new entities; second, find
the best types for each new entity among its can-
didate types.
4 Candidate Types for Entities
For a given entity, candidate types are types that
can potentially be assigned to that entity, based on
the entity?s co-occurrences with typed relational
patterns.
Definition 2 (Candidate Type) Given a new en-
tity x which occurs with a number of patterns
p1, p2, ..., pn, where each pattern pi has a type sig-
nature with a domain and a range: if x occurs on
the left of pi, we pick the domain of pi as a candi-
date type for x; if x occurs on the right of pi, we
pick the range of pi as a candidate type for x.
For each candidate type, we compute confi-
dence weights. Ideally, if an entity occurs with
a pattern which is highly specific to a given type
then the candidate type should have high con-
fidence. For example ?is married to? is more
specific to people then ?expelled from?. A per-
son can be expelled from an organization but a
country can also be expelled from an organization
such as NATO. There are various ways to com-
pute weights for candidate types. We first intro-
duce a uniform weight approach and then present a
method for computing more informative weights.
4.1 Uniform Weights
We are given a new entity x which occurs with
phrases (x phrase1 y1), (x phrase2 y2), ..., (x
phrasen yn). Suppose these occurrences lead
to the facts (x, p1, y1), (x, p2, y2),..., (x, pn, yn).
The pis are the typed relational patterns extracted
by PATTY. The facts are generated by matching
phrases to relational patterns with type signa-
tures. The type signature of a pattern is denoted
by:
sig(pi) = (domain(pi), range(pi))
We allow fuzzy matches, hence each fact comes
with a match score. This is the similarity degree
between the phrase observed in text and the typed
relational pattern.
Definition 3 (Fuzzy Match Score) Suppose we
observe the surface string: (x phrase y) which
leads to the fact: x, pi, y. The fuzzy match similar-
ity score is: sim(phrase, pi), where similarity is
the n-gram Jaccard similarity between the phrase
and the typed pattern.
The confidence that x is of type domain is de-
fined as follows:
Definition 4 (Candidate Type Confidence)
For a given observation (x phrase y), where
1490
phrase matches patterns p1, ..., pn, with domains
d1, ..., db which are possibly the same:
typeConf(x, phrase, d) =
?
{pi:domain(pi)=d}
(
sim(phrase, pi)
)
Observe that this sums up over all patterns that
match the phrase.
To compute the final confidence for
typeConf(x, domain), we aggregate the
confidences over all phrases occurring with x.
Definition 5 (Aggregate Confidence) For
a set of observations (x, phrase1, y1),
(x, phrase2, y2), ..., (x, phrasen, yn), the
aggregate candidate type confidence is given by:
aggTypeConf(x, d) =
?
phrasei
typeConf(x, phrasei, d)
=
?
phrasei
?
{pj :domain(pj)=d}
(sim(phrasei, pj))
The confidence for the range
typeConf(x, range) is computed analogously.
All confidence weights are normalized to values
in [0, 1].
The limitation of the uniform weight approach
is that each pattern is considered equally good for
suggesting candidate types. Thus this approach
does not take into account the intuition that an en-
tity occurring with a pattern which is highly spe-
cific to a given type is a stronger signal that the
entity is of the type suggested. Our next approach
addresses this limitation.
4.2 Co-occurrence Likelihood Weight
Computation
We devise a likelihood model for computing
weights for entity candidate types. Central to this
model is the estimation of the likelihood of a given
type occurring with a given pattern.
Suppose using PATTY methods we mined a
typed relational pattern ?t1? p ?t2?. Suppose that
we now encounter a new entity pair (x, y) occur-
ring with a phrase that matches p. We can com-
pute the likelihood of x and y being of types t1
and t2, respectively, from the likelihood of p co-
occurring with entities of types t1, t2. Therefore
we are interested in the type-pattern likelihood,
defined as follows:
Definition 6 (Type-Pattern Likelihood) The
likelihood of p co-occurring with an entity pair
(x, y) of the types (t1, t2) is given by:
P [t1, t2|p] (1)
where t1 and t2 are the types of the arguments ob-
served with p from a corpus such as Wikipedia.
P [t1, t2|p] is expanded as follows:
P [t1, t2|p] =
P [t1, t2, p]
P [p] . (2)
The expressions on the right-hand side of Equa-
tion 2 can be directly estimated from a corpus.
We use Wikipedia (English), for corpus-based es-
timations. P [t1, t2, p] is the relative occurrence
frequency of the typed pattern among all entity-
pattern-entity triples in a corpus (e.g., the frac-
tion of ?musican? plays ?song? among all triples).
P[p] is the relative occurrence frequency of the un-
typed pattern (e.g., plays) regardless of the argu-
ment types. For example, this sums up over both
?musican? plays ?song? occurrences and ?actor?
plays ?fictional character?. If we observe a fact
where one argument name can be easily disam-
biguated to a knowledge-base entity so that its type
is known, and the other argument is considered to
be an out-of-knowledge-base entity, we condition
the joint probability of t1, p, and t2 in a different
way:
Definition 7 (Conditional Type-PatternLikelihood)
The likelihood of an entity of type t1 occurring
with a pattern p and an entity of type t2 is given
by:
P [t1|t2, p] =
P [t1, t2, p]
P [p, t2]
(3)
where the P [p, t2] is the relative occurrence fre-
quency of a partial triple, for example, ?*? plays
?song?.
Observe that all numbers refer to occurrence
frequencies. For example, P [t1, p, t2] is a frac-
tion of the total number of triples in a corpus.
Multiple patterns can suggest the same type for
an entity. Therefore, the weight of the assertion
that y is of type t, is the total support strength from
all phrases that suggest type t for y.
Definition 8 (Aggregate Likelihood) The aggre-
gate likelihood candidate type confidence is given
1491
by:
typeConf(x, domain)) =
?
phrasei
?
pj
(
sim(phrasei, pj) ??
)
Where ? = P [t1, t2|p] or P [t1|t2, p] or P [t2|t1, p]
The confidence weights are normalized to values
in [0, 1]. So far we have presented a way of gener-
ating a number of weighted candidate types for x.
In the next step we pick the best types for an entity
among all its candidate types.
4.3 Integer Linear Program Formulation
Given a set of weighted candidate types, our goal
is to pick a compatible subset of types for x. The
additional asset that we leverage here is the com-
patibility of types: how likely is it that an entity
belongs to both type ti and type tj . Some types
are mutually exclusive, for example, the type loca-
tion rules out person and, at finer levels, city rules
out river and building, and so on. Our approach
harnesses these kinds of constraints. Our solution
is formalized as an Integer Linear Program (ILP).
We have candidate types for x: t1, .., tn. First, we
define a decision variable Ti for each candidate
type i = 1, . . . , n. These are binary variables:
Ti = 1 means type ti is selected to be included
in the set of types for x, Ti = 0 means we discard
type ti for x.
In the following we develop two variants of this
approach: a ?hard? ILP with rigorous disjointness
constraints, and a ?soft? ILP which considers type
correlations.
?Hard? ILP with Type Disjointness Con-
straints. We infer type disjointness constraints
from the YAGO2 knowledge base using occur-
rence statistics. Types with no overlap in entities
or insignificant overlap below a specified thresh-
old are considered disjoint. Notice that this intro-
duces hard constraints whereby selecting one type
of a disjoint pair rules out the second type. We de-
fine type disjointness constraints Ti + Tj ? 1 for
all disjoint pairs ti, tj (e.g. person-artifact, movie-
book, city-country, etc.). The ILP is defined as
follows:
objective
max?i Ti ? wi
type disjointness constraint
?(ti, tj)disjoint Ti + Tj ? 1
The weights wi are the aggregrated likelihoods
as specified in Definition 8.
?Soft? ILP with Type Correlations. In many
cases, two types are not really mutually exclusive
in the strict sense, but the likelihood that an en-
tity belongs to both types is very low. For exam-
ple, few drummers are also singers. Conversely,
certain type combinations are boosted if they are
strongly correlated. An example is guitar players
and electric guitar players. Our second ILP con-
siders such soft constraints. To this end, we pre-
compute Pearson correlation coefficients for all
type pairs (ti, tj) based on co-occurrences of types
for the same entities. These values vij ? [?1, 1]
are used as weights in the objective function of
the ILP. We additionally introduce pair-wise deci-
sion variables Yij , set to 1 if the entity at hand be-
longs to both types ti and tj , and 0 otherwise. This
coupling between the Yij variables and the Ti, Tj
variables is enforced by specific constraints. For
the objective function, we choose a linear combi-
nation of per-type evidence, using weights wi as
before, and the type-compatibility measure, using
weights vij . The ILP with correlations is defined
as follows:
objective
max ??i Ti ? wi + (1? ?)
?
ij Yij ? vij
type correlation constraints
?i,j Yij + 1 ? Ti + Tj
?i,j Yij ? Ti
?i,j Yij ? Tj
Note that both ILP variants need to be solved
per entity, not over all entities together. The ?soft?
ILP has a size quadratic in the number of candidate
types, but this is still a tractable input for modern
solvers. We use the Gurobi software package to
compute the solutions for the ILP?s. With this de-
sign, PEARL can efficiently handle a typical news
article in less than a second, and is well geared for
keeping up with high-rate content streams in real
time. For both the ?hard? and ?soft? variants of
the ILP, the solution is the best types for entity x
satisfying the constraints.
1492
5 Evaluation
To define a suitable corpus of test data, we ob-
tained a stream of news documents by subscrib-
ing to Google News RSS feeds for a few topics
over a six-month period (April 2012 ? Septem-
ber 2012). This produced 318, 434 documents.
The topics we subscribed to are: Angela Merkel,
Barack Obama, Business, Entertainment, Hillary
Clinton, Joe Biden, Mitt Romney, Newt Gingrich,
Rick Santorum, SciTech and Top News. All our ex-
periments were carried out on this data. The type
system used is that of YAGO2, which is derived
from WordNet. Human evaluations were carried
out on Amazon Mechanical Turk (MTurk), which
is a platform for crowd-sourcing tasks that require
human input. Tasks on MTurk are small question-
naires consisting of a description and a set of ques-
tions.
Baselines. We compared PEARL against two
state-of-the-art baselines: i). NNPLB (No Noun
Phrase Left Behind), is the method presented in
(Lin 2012), based on the propagation of types
for known entities through salient patterns occur-
ring with both known and unknown entities. We
implemented the algorithm in (Lin 2012) in our
framework, using the relational patterns of PATTY
(Nakashole 2012) for comparability. For assess-
ment we sampled from the top-5 highest ranked
types for each entity. In our experiments, our im-
plementation of NNPLB achieved precision values
comparable to those reported in (Lin 2012). ii).
HYENA (Hierarchical tYpe classification for En-
tity NAmes), the method of (Yosef 2012), based
on a feature-rich classifier for fine-grained, hierar-
chical type tagging. This is a state-of-the-art rep-
resentative of similar methods such as (Rahman
2010; Ling 2012).
Evaluation Task. To evaluate the quality of types
assigned to emerging entities, we presented turk-
ers with sentences from the news tagged with out-
of-KB entities and the types inferred by the meth-
ods under test. The turkers task was to assess the
correctness of types assigned to an entity mention.
To make it easy to understand the task for the turk-
ers, we combined the extracted entity and type into
a sentence. For example if PEARL inferred that
Brussels Summit is an political event, we generate
and present the sentence: Brussels Summit is an
event. We allowed four possible assessment val-
ues: a) Very good output corresponds to a perfect
result. b) Good output exhibits minor errors. For
instance, the description G20 Summit is an orga-
nization is wrong, because the summit is an event,
but G20 is indeed an organization. The problem in
this example is incorrect segmentation of a named
entity. c) Wrong for incorrect types (e.g., Brussels
Summit is a politician). d) Not sure / do not know
for other cases.
Comparing PEARL to Baselines. Per method,
turkers evaluated 105 entity-type pair test sam-
ples. We first sampled among out-of-KB entities
that were mentioned frequently in the news cor-
pus: in at least 20 different news articles. Each
test sample was given to 3 different turkers for as-
sessment. Since the turkers did not always agree
if the type for a sample is good or not, we ag-
gregate their answers. We use voting to decide
whether the type was assigned correctly to an en-
tity. We consider the following voting variants:
i) majority ?very good? or ?good?, a conservative
notion of precision: precisionlower. ii) at least
one ?very good? or ?good?, a liberal notion of
precision: precisionupper. Table 1 shows preci-
sion for PEARL-hard, PEARL-soft, NNPLB, and
HYENA, with a 0.9-confidence Wilson score in-
terval (Brown 2001). PEARL-hard outperformed
PEARL-soft and also both baselines. HYENA?s
relatively poor performance can be attributed to
the fact that its features are mainly syntactic such
as bi-grams and part-of-speech tags. Web data is
challenging, it has a lot of variations in syntac-
tic formulations. This introduces a fair amount
of ambiguity which can easily mislead syntactic
features. Leveraging semantic features as done
by PEARL could improve HYENA?s performance.
While the NNPLB method performs better than
HYENA, in comparison to PEARL-hard, there is
room for improvement. Like HYENA, NNPLB
assigns negatively correlated types to the same en-
tity. This limitation could be addressed by apply-
ing PEARL?s ILPs and probabilistic weights to the
candidate types suggested by NNPLB.
To compute inter-judge agreement we calcu-
lated Fleiss? kappa and Cohen?s kappa ?, which
are standard measures. The usual assumption for
Fleiss?? is that labels are categorical, so that each
disagreement counts the same. This is not the case
in our settings, where different labels may indicate
partial agreement (?good?, ?very good?). There-
1493
Precisionlower Precisionupper
PEARL-hard 0.77?0.08 0.88?0.06
PEARL-soft 0.53?0.09 0.77?0.09
HYENA 0.26?0.08 0.56?0.09
NNPLB 0.46?0.09 0.68?0.09
Table 1: Comparison of PEARL to baselines.
? F leiss Cohen
0.34 0.45
Table 2: Lower bound estimations for inter-judge
agreement kappa: Fleiss? ? & adapted Cohen?s ?.
fore the ? values in Table 2 are lower-bound esti-
mates of agreement in our experiments; the ?true
agreement? seems higher. Nevertheless, the ob-
served Fleiss ? values show that the task was fairly
clear to the turkers; values > 0.2 are generally
considered as acceptable (Landis 1977). Cohen?s
? is also not directly applicable to our setting. We
approximated it by finding pairs of judges who as-
sessed a significant number of the same entity-type
pairs.
Precisionlower Precisionupper
Freq. mentions 0.77?0.08 0.88?0.06
All mentions 0.65?0.09 0.77?0.08
Table 3: PEARL-hard performance on a sample of
frequent entities (mention frequency? 20) and on
a sample of entities of all mention frequencies.
Mention Frequencies. We also studied PEARL-
hard?s performance on entities of different men-
tion frequencies. The results are shown in Ta-
ble 3. Frequently mentioned entities provide
PEARL with more evidence as they potentially oc-
cur with more patterns. Therefore, as expected,
precision when sampling over all entities drops
a bit. For such infrequent entities, PEARL does
not have enough evidence for reliable type assign-
ments.
Variations of PEARL. To quantify how various
aspects of our approach affect performance, we
studied a few variations. The first method is the
full PEARL-hard. The second method is PEARL
with no ILP (denoted No ILP), only using the
probabilistic model. The third variation is PEARL
without probabilistic weights (denoted Uniform
Figure 1: Variations of the PEARL method.
Weights). From Figure 1, it is clear that both the
ILP and the weighting model contribute signifi-
cantly to PEARL?s ability to make precise type as-
signments. Sample results from PEARL-hard are
shown in Table 4.
NDCG. For a given entity mention e, an entity-
typing system returns a ranked list of types
{t1, t2, ..., tn}. We evaluated ranking quality us-
ing the top-5 ranks for each method. These assess-
ments were aggregated into the normalized dis-
counted cumulative gain (NDCG), a widely used
measure for ranking quality. The NDCG values
obtained are 0.53, 0.16, and 0.16, for PEARL-
hard, HYENA, and NNPLB, respectively. PEARL
clearly outperforms the baselines on ranking qual-
ity, too.
6 Related Work
Tagging mentions of named entities with lexical
types has been pursued in previous work. Most
well-known is the Stanford named entity recog-
nition (NER) tagger (Finkel 2005) which assigns
coarse-grained types like person, organization, lo-
cation, and other to noun phrases that are likely to
denote entities. There is fairly little work on fine-
grained typing, notable results being (Fleischmann
2002; Rahman 2010; Ling 2012; Yosef 2012).
These methods consider type taxonomies similar
to the one used for PEARL, consisting of several
hundreds of fine-grained types. All methods use
trained classifiers over a variety of linguistic fea-
tures, most importantly, words and bigrams with
part-of-speech tags in a mention and in the textual
context preceding and following the mention. In
addition, the method of (Yosef 2012) (HYENA)
utilizes a big gazetteer of per-type words that oc-
cur in Wikipedia anchor texts. This method out-
performs earlier techniques on a variety of test
1494
Entity Inferred Type Sample Source Sentence (s)
Lochte medalist Lochte won America?s lone gold ...
Malick director ... the red carpet in Cannes for Malick?s 2011 movie ...
Bonamassa musician Bonamassa recorded Driving Towards the Daylight in Las Vegas ...
... Bonamassa opened for B.B. King in Rochester , N.Y.
Analog Man album Analog Man is Joe Walsh?s first solo album in 20 years.
Melinda Liu journalist ... in a telephone interview with journalist Melinda Liu of the Daily Beast.
RealtyTrac publication Earlier this month, RealtyTrac reported that ...
Table 4: Sample types inferred by PEARL.
cases; hence it served as one of our baselines.
Closely related to our work is the recent ap-
proach of (Lin 2012) (NNPLB) for predicting
types for out-of-KB entities. Noun phrases in the
subject role in a large collection of fact triples
are heuristically linked to Freebase entities. This
yields type information for the linked mentions.
For unlinkable entities the NNPLB method (in-
spired by (Kozareva 2011)) picks types based on
co-occurrence with salient relational patterns by
propagating types of linked entities to unlinkable
entities that occur with the same patterns. Unlike
PEARL, NNPLB does not attempt to resolve in-
consistencies among the predicted types. In con-
trast, PEARL uses an ILP with type disjointness
and correlation constraints to solve and penalize
such inconsistencies. NNPLB uses untyped pat-
terns, whereas PEARL harnesses patterns with
type signatures. Furthermore, PEARL computes
weights for candidate types based on patterns and
type signatures. Weight computations in NNPLB
are only based on patterns. NNPLB only assigns
types to entities that appear in the subject role of
a pattern. This means that entities in the object
role are not typed at all. In contrast, PEARL in-
fers types for entities in both the subject and object
role.
Type disjointness constraints have been studied
for other tasks in information extraction (Carlson
2010; Suchanek 2009), but using different formu-
lations.
7 Conclusion
This paper addressed the problem of detecting and
semantically typing newly emerging entities, to
support the life-cycle of large knowledge bases.
Our solution, PEARL, draws on a collection of
semantically typed patterns for binary relations.
PEARL feeds probabilistic evidence derived from
occurrences of such patterns into two kinds of
ILPs, considering type disjointness or type corre-
lations. This leads to highly accurate type predic-
tions, significantly better than previous methods,
as our crowdsourcing-based evaluation showed.
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta:
Interval Estimation for a Binomial Proportion. Sta-
tistical Science 16: pages 101?133, 2001.
R. C. Bunescu, M. Pasca: Using Encyclopedic Knowl-
edge for Named entity Disambiguation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy, 2006.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
S. Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
1495
CoNLL), pages 708?716, Prague, Czech Republic,
2007.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
J.R. Finkel, T. Grenager, C. Manning. 2005. Incorpo-
rating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 363?370,
Ann Arbor, Michigan, 2005.
Michael Fleischman, Eduard H. Hovy: Fine Grained
Classification of Named Entities. In Proceedings
the International Conference on Computational Lin-
guistics, COLING 2002.
X. Han, J. Zhao: Named Entity Disambiguation by
Leveraging Wikipedia Semantic Knowledge. In Pro-
ceedings of 18th ACM Conference on Information
and Knowledge Management (CIKM), pages 215 ?
224,Hong Kong, China, 2009.
C. Havasi, R. Speer, J. Alonso. ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
Sebastian Hellmann, Claus Stadler, Jens Lehmann,
Sren Auer: DBpedia Live Extraction. OTM Confer-
ences (2) 2009: 1209-1223.
J. Hoffart, M. A. Yosef, I.Bordino and H. Fuerstenau,
M. Pinkal, M. Spaniol, B.Taneva, S.Thater, Gerhard
Weikum: Robust Disambiguation of Named Entities
in Text. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 782?792, Edinburgh, UK, 2011.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
J. Hoffart, F. Suchanek, K. Berberich, G. Weikum:
YAGO2: A Spatially and Temporally Enhanced
Knowledge Base from Wikipedia. Artificial Intelli-
gence 2012.
Z. Kozareva, L. Voevodski, S.-H.Teng: Class Label
Enhancement via Related Instances. EMNLP 2011:
118-128
J. R. Landis, G. G. Koch: The measurement of observer
agreement for categorical data in Biometrics. Vol.
33, pp. 159174, 1977.
C. Lee, Y-G. Hwang, M.-G. Jang: Fine-grained
Named Entity Recognition and Relation Extraction
for Question Answering. In Proceedings of the 30th
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR), pages 799?800, Amsterdam, The Nether-
lands, 2007.
T. Lin, Mausam , O. Etzioni: No Noun Phrase Left
Behind: Detecting and Typing Unlinkable Entities.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 893?903, Jeju, South Ko-
rea, 2012.
Xiao Ling, Daniel S. Weld: Fine-Grained Entity
Recognition. In Proceedings of the Conference on
Artificial Intelligence (AAAI), 2012
D. N. Milne, I. H. Witten: Learning to Link with Wi-
kipedia. In Proceedings of 17th ACM Conference on
Information and Knowledge Management (CIKM),
pages 509-518, Napa Valley, California, USA, 2008.
N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.
V. Nastase, M. Strube, B. Boerschinger, Ca?cilia Zirn,
Anas Elghafari: WikiNet: A Very Large Scale
Multi-Lingual Concept Network. In Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation(LREC), Malta, 2010.
H. T. Nguyen, T. H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. In Pro-
ceedings of the IEEE International Conference on
Research, Innovation and Vision for the Future in
Computing & Communication Technologies (RIVF),
pages 247?254, Ho Chi Minh City, Vietnam, 2008.
Feng Niu, Ce Zhang, Christopher Re, Jude W. Shav-
lik: DeepDive: Web-scale Knowledge-base Con-
struction using Statistical Learning and Inference. In
the VLDS Workshop, pages 25-28, 2012.
A. Rahman, Vincent Ng: Inducing Fine-Grained Se-
mantic Classes via Hierarchical and Collective Clas-
sification. In Proceedings the International Con-
ference on Computational Linguistics (COLING),
pages 931-939, 2010.
F. M. Suchanek, G. Kasneci, G. Weikum: Yago: a
Core of Semantic Knowledge. In Proceedings of the
16th International Conference on World Wide Web
(WWW) pages, 697-706, Banff, Alberta, Canada,
2007.
1496
F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631?640,
Madrid, Spain, 2009.
P.P. Talukdar, F. Pereira: Experiments in Graph-Based
Semi-Supervised Learning Methods for Class-
Instance Acquisition. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1473-1481, 2010.
P. Venetis, A. Halevy, J. Madhavan, M. Pasca, W. Shen,
F. Wu, G. Miao, C. Wu: Recovering Semantics of
Tables on the Web. In Proceedings of the VLDB En-
dowment, PVLDB 4(9), pages, 528?538. 2011.
W. Wu, H. Li, H. Wang, K. Zhu: Probase: A
Probabilistic Taxonomy for Text Understanding. In
Proceedings of the International Conference on
Management of Data (SIGMOD), pages 481?492,
Scottsdale, AZ, USA, 2012.
M. A. Yosef, S. Bauer, J. Hoffart, M. Spaniol, G.
Weikum: HYENA: Hierarchical Type Classifica-
tion for Entity Names. In Proceedings the In-
ternational Conference on Computational Linguis-
tics(COLING), to appear, 2012.
1497
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133?138,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
HYENA-live: Fine-Grained Online Entity Type Classification from
Natural-language Text
Mohamed Amir Yosef1 Sandro Bauer2 Johannes Hoffart1
Marc Spaniol1 Gerhard Weikum1
(1) Max-Planck-Institut fu?r Informatik, Saarbru?cken, Germany
(2) Computer Laboratory, University of Cambridge, UK
{mamir|jhoffart|mspaniol|weikum}@mpi-inf.mpg.de
sandro.bauer@cl.cam.ac.uk
Abstract
Recent research has shown progress in
achieving high-quality, very fine-grained
type classification in hierarchical tax-
onomies. Within such a multi-level type
hierarchy with several hundreds of types at
different levels, many entities naturally be-
long to multiple types. In order to achieve
high-precision in type classification, cur-
rent approaches are either limited to certain
domains or require time consuming multi-
stage computations. As a consequence, ex-
isting systems are incapable of performing
ad-hoc type classification on arbitrary input
texts. In this demo, we present a novel Web-
based tool that is able to perform domain
independent entity type classification under
real time conditions. Thanks to its efficient
implementation and compacted feature rep-
resentation, the system is able to process
text inputs on-the-fly while still achieving
equally high precision as leading state-of-
the-art implementations. Our system offers
an online interface where natural-language
text can be inserted, which returns seman-
tic type labels for entity mentions. Further
more, the user interface allows users to ex-
plore the assigned types by visualizing and
navigating along the type-hierarchy.
1 Introduction
Motivation
Web contents such as news, blogs and other so-
cial media are full of named entities. Each en-
tity belongs to one or more semantic types as-
sociated with it. For instance, an entity such as
Bob Dylan should be assigned the types Singer,
Musician, Poet, etc., and also the correspond-
ing supertype(s) (hypernyms) in a type hierarchy,
in this case Person. Such fine-grained typing of
entities in texts can be a great asset for various
NLP tasks including semantic role labeling, sense
disambiguation and named entity disambiguation
(NED). For instance, noun phrases such as ?song-
writer Dylan?, ?Google founder Page?, or ?rock
legend Page? can be easily mapped to the entities
Bob Dylan, Larry Page, and Jimmy Page if their re-
spective types Singer, BusinessPerson, and
Guitarist are available (cf. Figure 1 for an il-
lustrative example).
with 100,000$, Google wasFunded
"
founded by Brin and Page "
his firstplayed on" guitar in 1952Page "
Business_people
Entrepreneur
Entertainer
Musician
Figure 1: Fine-grained entity type classification
Problem Statement
Type classification is not only be based on hier-
archical sub-type relationships (e.g. Musician
isA Person), but also has to do on multi-labeling.
Within a very fine-grained type hierarchy, many en-
tities naturally belong to multiple types. For exam-
ple, a guitarist is also a musician and a person, but
may also be a singer, an actor, or even a politician.
Consequently, entities should not only be assigned
the most (fine-grained) label associated to them,
but with all labels relevant to them. So we face
a hierarchical multi-label classification problem
(Tsoumakas et al, 2012).
Contribution
This paper introduces HYENA-live, which allows
an on-the-fly computation of semantic types for en-
tity mentions, based on a multi-level type hierarchy.
Our approach uses a suite of features for a given
entity mention, such as neighboring words and bi-
133
grams, part-of-speech tags, and also phrases from a
large gazetteer derived from state-of-the-art knowl-
edge bases. In order to perform ?live? entity type
classification based on ad-hoc text inputs, several
performance optimizations have been undertaken
to operate under real-time conditions.
2 Entity Type Classification Systems
State-of-the-art tools for named entity recognition
such as the Stanford NER Tagger (Finkel et al,
2005) compute semantic tags only for a small set of
coarse-grained types: Person, Location, and
Organization (plus tags for non-entity phrases
of type time, money, percent, and date). However,
we are not aware of any online tool that performs
fine-grained typing of entity mentions. The most
common workaround to perform entity classifica-
tion is a two-stage process: in first applying an on-
line tool for Named-Entity Disambiguation (NED),
such as DBpedia Spotlight (Mendes et al, 2011)
or AIDA (Yosef et al, 2011; Hoffart et al, 2011),
in order to map the mentions onto canonical enti-
ties and subsequently query the knowledge base for
their types. In fact, (Ling and Weld, 2012) followed
this approach when comparing their entity classi-
fication system results against those obtained by
an adoption of the Illinois? Named-Entity Linking
system (NEL) (Ratinov et al, 2011) and reached
the conclusion that while NEL performed decently
for prominent entities, it could not scale to cover
long tail ones. Specifically, entity typing via NED
has three major drawbacks:
1. NED is an inherently hard problem, especially
with highly ambiguous mentions. As a conse-
quence, accurate NED systems come at a high
computation costs.
2. NED only works for those mentions that cor-
respond to a canonical entity within a knowl-
edge base. However, this fails for all out-of-
knowledge-base entities like unregistered per-
sons, start-up companies, etc.
3. NED heavily depends on the quality of the un-
derlying knowledge base. Yet, only very few
knowledge bases have comprehensive class
labeling of entities. Even more, in the best
case, coverage drops sharply for relatively un-
common entities.
We decided to adopt one of the existing ap-
proaches to make it suitable for online querying.
We considered five systems. In the rest of this
section we will briefly describe each of them.
(Fleischman and Hovy, 2002) is one of the earli-
est approaches to perform entity classification into
subtypes of PERSON. They developed a decision-
tree classifier based on contextual features that can
be automatically extracted from the text. In order
to account for scarcity of labeled training data, they
tapped on WordNet synonyms to achieve higher
coverage. While their approach is fundamentally
suitable, their type system is very restricted. In or-
der to account for more fine-grained classes, more
features need to be added to their feature set.
(Ekbal et al, 2010) considered 141 subtypes of
WordNet class PERSON and developed a maximum
entropy classifier exploiting the words surrounding
the mentions together with their POS tags and other
contextual features. Their type hierarchy is fine-
grained, but still limited to sub classes of PERSON.
In addition, their experimental results have been
flagged as non-reproducible in the ACL Anthology.
(Altaf ur Rahman and Ng, 2010) considered a
two-level type hierarchy consisting of 29 top-level
classes and a total of 92 sub-classes. These include
many non-entity types such as date, time, percent,
money, quantity, ordinal, cardinal, etc. They in-
corporated a hierarchical classifier using a rich fea-
ture set and made use of WordNet sense tagging.
However, the latter requires human interception,
which is not suitable for ad-hoc processing of out-
of-domain texts.
(Ling and Weld, 2012) developed FIGER,
which classifies entity mentions onto a two-level
taxonomy based on the Freebase knowledge base
(Bollacker et al, 2008). This results in a two-level
hierarchy with top-level topics and 112 types. They
trained a CRF for the joint task of recognizing en-
tity mentions and inferring type tags. Although
they handle multi-label assignment, their test data
is sparse. Many classes are absent and plenty of
instances come with only a single label (e.g. 216
of the 562 entities were of type PERSON without
subtypes). Further, their results are instance based,
which does not guarantee that the quality of their
system will be reproducible for all the 112 types in
their taxonomy.
(Yosef et al, 2012) is the most recent work in
multi-label type classification. The HYENA sys-
tem incorporates a large hierarchy of 505 classes
134
organized under 5 top level classes, with 100 de-
scendant classes under each of them. The hierarchy
reaches a depth of up to 9 levels in some parts.
The system is based on an SVM classifier using a
comprehensive set of features and provides results
for all classes of a large data set. In their exper-
iments the superiority of the system in terms of
precision and recall has been shown. However, the
main drawback of HYENA comes from its large
hierarchy and the extensive set of features extracted
from the fairly large training corpus it requires. As
a result, on-the-fly type classification with HYENA
is impossible in its current implementation.
We decided to build on top of HYENA sys-
tem by spotting the bottlenecks in the architec-
ture and modifying it accordingly to be suitable
for online querying. In Section 3 we explain in
details HYENA?s type taxonomy and their feature
portfolio. Later on, we explain the engineering
undertaken in order to develop the on-the-fly type
classification system HYENA-live (cf. Section 4).
3 Type Hierarchy and Feature Set
3.1 Fine-grained Taxonomy
The type system is an automatically gathered fine-
grained taxonomy of 505 classes. The classes are
organized under 5 top level classes, with 100 de-
scendant classes under each. The YAGO knowl-
edge base (Hoffart et al, 2013) is selected to de-
rive the taxonomy from because of its highly pre-
cise classification of entities into WordNet classes,
which is a result of the accurate mapping YAGO
has from Wikipedia Categories to WordNet synsets.
We start with five top classes namely PERSON,
LOCATION, ORGANIZATION, EVENT and
ARTIFACT. Under each top class, the most 100
prominent descendant classes are picked. Promi-
nence is estimated by the number of YAGO entities
tagged with this class. This results in a very-fine
grained taxonomy of 505 types, represented as a
directed acyclic graph with 9 levels in its deepest
parts. While the classes are picked from the YAGO
type system, the approach is generic and can be
applied to derive type taxonomies from other
knowledge bases such as Freebase or DBpedia
(Auer et al, 2007) as in (Ling and Weld, 2012).
3.2 Feature Set
For the sake of generality and applicability to ar-
bitrary text, we opted for features that can be au-
tomatically extracted from the input text without
any human interaction, or manual annotation. The
extracted features fall under five categories, which
we briefly explain in the rest of this section.
Mention String
We derive four features from the entity mention
string. The mention string itself, a noun phrase
consisting of one or more consecutive words. The
other three features are unigrams, bigrams, and
trigrams that overlap with the mention string.
Sentence Surrounding Mention
We also exploit a bounded-size window around the
mention to extract four features: all unigrams, bi-
grams, and trigrams. Two versions of those features
are extracted, one to account for the occurrence of
those tokens around the mention, and another to ac-
count for the position at which they occurred with
respect to the mention (before or after). In addition,
unigrams are also included with their absolute dis-
tance ignoring whether before of after the mention.
Our demo is using a conservative threshold for the
size of the window which is three tokens on each
side of the mention.
Mention Paragraph
We also leverage the entire paragraph of the men-
tion. This gives additional topical cues about the
mention type (e.g., if the paragraph is about a mu-
sic concert, this is a cue for mapping people names
to musician types). We create three features here:
unigrams, bigrams, and trigrams without including
any distance information. In our demo, we extract
those features from a bounded window of size 2000
characters before and after the mention.
Grammatical Features
We exploit the semantics of the text by extracting
four features. First, we use part-of-speech tags of
the tokens in a size-bounded window around the
mention in distance and absolute distance versions.
Second and third, we create a feature for the first
occurrence of a ?he? or ?she? pronoun in the same
sentence and in the subsequent sentence following
the mention, along with the distance to the mention.
Finally, we use the closest verb-preposition pair
preceding the mention as another feature.
Gazetteer Features
We leverage YAGO2 knowledge base even further
by building a type-specific gazetteer of words oc-
135
# of articles 50,000
# of instances (all types) 1,613,340
# of location instances 489,003 (30%)
# of person instances 426,467 (26.4%)
# of organization instances 219,716 (13.6%)
# of artifact instances 204,802 (12.7%)
# of event instances 176,549 (10.9%)
# instances in 1 top-level class 1,131,994 (70.2%)
# instances in 2 top-level classes 182,508 (11.3%)
# instances in more than 2 top-level classes 6,492 (0.4%)
# instances not in any class 292,346 (18.1%)
Table 1: Properties of the labeled data used for training HYENA-live
curring in the names of the entities of that type.
YAGO2 knowledge base comes with an exten-
sive dictionary of name-entity pairs extracted from
Wikipedia redirects and link-anchor texts. We con-
struct, for each type, a binary feature that indicates
if the mention contains a word occurring in this
type?s gazetteer. Note that this is a fully automated
feature construction, and it does by no means de-
termine the mention type(s) already, as most words
occur in the gazetteers of many different types. For
example, ?Alice? occurs in virtually every subclass
of Person but also in city names like ?Alice Springs?
and other locations, as well as in songs, movies,
and other products or organizations.
4 System Implementation
4.1 Overview
As described in Section 3, HYENA classifies men-
tions of named entities onto a hierarchy of 505
types using large set of features. A random sub-
set of the English Wikipedia has been used for
training HYENA. By exploiting Wikipedia anchor
links, mentions of named entities are automati-
cally disambiguated to their correct entities. Each
Wikipedia named entity has a corresponding YAGO
entity labeled with an accurate set of types, and
hence we effortlessly obtain a huge training data
set (cf. data properties in Table 1).
We build type-specific classifiers using the SVM
software LIBLINEAR (cf. http://liblinear.
bwaldvogel.de/). Each model comes with a com-
prehensive feature set. While larger models (with
more features) improve the accuracy, they signifi-
cantly affect the applicability of the system. A sin-
gle model file occupies around 150MB disk space
leading to a total of 84.7GB for all models. As
a consequence, there is a substantial setup time
to load all models in memory and a high-memory
server (48 cores with 512GB of RAM) is required
for computation. An analysis showed that each sin-
gle feature contributes to the overall performance
of HYENA, but only a tiny subset of all features is
relevant for a single classifier. Therefore, most of
the models are extremely sparse.
4.2 Sparse Models Representation
There are several workarounds applicable to batch
mode operations, e.g. by performing classifications
per level only. However, this is not an option for
on-the-fly computations. For that reason we opted
for a sparse-model representation.
LIBLINEAR model files are normalized textual
files: a header (data about the model and the to-
tal number of features), followed by listing the
weights assigned to each feature (line number in-
dicates the feature ID). Each model file has been
post-processed to produce 2 files:
? A compacted model file containing only fea-
tures of non-zero weights. Its header reflects
the reduced number of features.
? A meta-data file. It maps the new features IDs
to the original feature IDs.
Due to the observed sparsity in the model files,
particularly at deeper levels, there is a significant
decrease in disk space consumption for the com-
pacted model files and hence in the memory re-
quirements.
4.3 Sparse Models Classification
By switching to the sparse model representation the
architecture of the whole system is affected. In par-
ticular, modified versions of feature vectors need
to be generated for each classifier; this is because
136
,nSXt 
7e[t
)eatXre 
([traFtor
&lassifiFation 
0odels
6Sarse 0odels 
0eta'ata
6Sarse 0odel 
5eSresentation
3ost 
3roFessing
)eatXre 9eFtor
&lassifier 
'eFision
0odel6SeFifiF 
)eatXre 9eFtor
Figure 2: Modified system architecture designed for handling sparse models
a lot of features have been omitted from specific
classifiers (those with zero weights). Consequently,
the feature IDs need to be mapped to the new fea-
ture space of each classifier. The conceptual design
of the new architecture is illustrated in Figure 4.2.
5 Demo Presentation
HYENA-live has been fully implemented as a Web
application. Figure 5 shows the user interface of
HYENA-live in a Web browser:
1) On top, there is a panel where a user can input
any text, e.g. by copy-and-paste from news ar-
ticles. We employ the Stanford NER Tagger to
identify noun phrases as candidates of entity
mentions. Alternatively, users can flag entity
mentions by double brackets (e.g. ?Harry is
the opponent of [[you know who]]?). For the
sake of simplicity, detected entity mentions by
HYENA-live are highlighted in yellow. Each
mention is clickable to study its type classifi-
cation results.
2) The output of type classification is shown in-
side a tabbed widget. Each tab corresponds
to a detected mention by the system and tabs
are sorted by the order of occurrence in the
input text. To open a tab, the tab header or the
corresponding mention in the input area needs
to be clicked.
3) The type classification of a mention is shown
as a color-coded interactive tree. While the
original type hierarchy is a directed acyclic
graph, for the ease of navigation the classifi-
cation output has been converted into a tree.
In order to do so, nodes that belong to more
than a parent have been duplicated. There are
three different types of nodes:
? Green Nodes: referring to a class that has
been accepted by the classifier. These
nodes can be further expanded in order
to check which sub-classes have been
accepted or rejected by HYENA-live.
? Red Nodes: corresponding to a class that
was rejected by the classifier, and hence
HYENA-live did not traverse deeper to
test its sub-classes.
? White Nodes: matching classes that have
not been tested. These nodes are either
known upfront (e.g. ENTITY) or their
super class was rejected by the system.
It is worth noting that HYENA-live automati-
cally adjusts the layouting so that as much as
possible of the hierarchy is shown to the user.
For the sake of explorability, this is being dy-
namically adjusted once the user decides to
navigate along a certain (child-)node.
The system is available online at:
d5gate.ag5.mpi-sb.mpg.de/webhyena/.
The data transfer between the client and the server
is done via JSON objects. Hence, we also provide
HYENA-live as a JSON compliant entity classi-
fication Web-service. As a result, the back-end
becomes easily interchangeable (e.g. by a different
classification technique or a different type taxon-
omy) with minimum modifications required on the
user interface side.
Acknowledgments
This work is supported by the 7th Framework IST programme
of the European Union through the focused research project
(STREP) on Longitudinal Analytics of Web Archive data
(LAWA) under contract no. 258105.
137
Figure 3: Interactively exploring the types of the ?Battle of Waterloo? in the HYENA-live interface
References
Md. Altaf ur Rahman and Vincent Ng. 2010. Inducing
fine-grained semantic classes via hierarchical and
collective classification. In COLING, pages 931?
939.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, and Zachary Ives. 2007. Dbpedia: A nu-
cleus for a web of open data. In ISWC, pages 11?15.
Springer.
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD, pages 1247?1250.
Asif Ekbal, Eva Sourjikova, Anette Frank, and Si-
mone P. Ponzetto. 2010. Assessing the challenge of
fine-grained named entity recognition and classifica-
tion. In Named Entities Workshop, pages 93?101.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL, pages 363?370.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In COLING,
pages 1?7.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fu?rstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In EMNLP, pages 782?792.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194(0):28 ?
61.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In AAAI, pages 94?100.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva,
and Christian Bizer. 2011. Dbpedia spotlight:
shedding light on the web of documents. In I-
SEMANTICS, pages 1?8.
Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms for
disambiguation to wikipedia. In ACL, pages 1375?
1384.
Grigorios Tsoumakas, Min-Ling Zhang, and Zhi-Hua
Zhou. 2012. Introduction to the special issue on
learning from multi-label data. Machine Learning,
88(1-2):1?4.
Mohamed Amir Yosef, Johannes Hoffart, Ilaria Bor-
dino, Marc Spaniol, and Gerhard Weikum. 2011.
AIDA: An online tool for accurate disambiguation
of named entities in text and tables. PVLDB,
4(12):1450?1453.
Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical Type Classification for Entity
Names. In COLING, pages 1361?1370.
138
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction
with Latent Structural SVM
Lizhen Qu
Max Planck Institute
for Informatics
lqu@mpi-inf.mpg.de
Yi Zhang
Nuance Communications
yi.zhang@nuance.com
Rui Wang
DFKI GmbH
mars198356@hotmail.com
Lili Jiang
Max Planck Institute
for Informatics
ljiang@mpi-inf.mpg.de
Rainer Gemulla
Max Planck Institute
for Informatics
rgemulla@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute
for Informatics
weikum@mpi-inf.mpg.de
Abstract
Extracting instances of sentiment-oriented re-
lations from user-generated web documents is
important for online marketing analysis. Un-
like previous work, we formulate this extrac-
tion task as a structured prediction problem
and design the corresponding inference as an
integer linear program. Our latent structural
SVM based model can learn from training cor-
pora that do not contain explicit annotations of
sentiment-bearing expressions, and it can si-
multaneously recognize instances of both bi-
nary (polarity) and ternary (comparative) re-
lations with regard to entity mentions of in-
terest. The empirical evaluation shows that
our approach significantly outperforms state-
of-the-art systems across domains (cameras
and movies) and across genres (reviews and
forum posts). The gold standard corpus that
we built will also be a valuable resource for
the community.
1 Introduction
Sentiment-oriented relation extraction (Choi et al.,
2006) is concerned with recognizing sentiment po-
larities and comparative relations between entities
from natural language text. Identifying such rela-
tions often requires syntactic and semantic analysis
at both sentence and phrase level. Most prior work
on sentiment analysis consider either i) subjective
sentence detection (Yu and K?bler, 2011), ii) po-
larity classification (Johansson and Moschitti, 2011;
Wilson et al., 2005), or iii) comparative relation
identification (Jindal and Liu, 2006; Ganapathib-
hotla and Liu, 2008). In practice, however, differ-
ent types of sentiment-oriented relations frequently
coexist in documents. In particular, we found that
more than 38% of the sentences in our test corpus
contain more than one type of relations. The iso-
lated analysis approach is inappropriate because i) it
sacrifices acuracy by ignoring the intricate interplay
among different types of relations; ii) it could lead to
conflicting predictions such as estimating a relation
candidate as both negative and comparative. There-
fore, in this paper, we identify instances of both sen-
timent polarities and comparative relations for enti-
ties of interest simultaneously. We assume that all
the mentions of entities and attributes are given, and
entities are disambiguated. It is a widely used as-
sumption when evaluating a module in a pipeline
system that the outputs of preceding modules are
error-free.
To the best of our knowledge, the only exist-
ing system capable of extracting both comparisons
and sentiment polarities is a rule-based system pro-
posed by Ding et al. (2009). We argue that it is
better to tackle the task by using a unified model
with structured outputs. It allows us to consider a
set of correlated relation instances jointly and char-
acterize their interaction through a set of soft and
hard constraints. For example, we can encode con-
straints to discourage an attribute to participate in
a polarity relation and a comparative relation at the
same time. As a result, the system extracts a set of
correlated instances of sentiment-oriented relations
from a given sentence. For example, with the sen-
tence about the camera Canon 7D, ?The sensor is
great, but the price is higher than Nikon D7000.?
the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155?168. Action Editor: Janyce Wiebe.
Submitted 6/2013; Revised 11/2013; Published 4/2014. c?2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textit-
price).
However, constructing a fully annotated train-
ing corpus for this task is labor-intensive and re-
quires strong linguistic background. We minimize
this overhead by applying a simplified annotation
scheme, in which annotators mark mentions of en-
tities and attributes, disambiguate the entities, and
label instances of relations for each sentence. Based
on the new scheme, we have created a small Senti-
ment Relation Graph (SRG) corpus for the domains
of cameras and movies, which significantly differs
from the corpora used in prior work (Wei and Gulla,
2010; Kessler et al., 2010; Toprak et al., 2010;
Wiebe et al., 2005; Hu and Liu, 2004) in the follow-
ing ways: i) both sentiment polarities and compar-
ative relations are annotated; ii) all mentioned en-
tities are disambiguated; and iii) no subjective ex-
pressions are annotated, unless they are part of entity
mentions.
The new annotation scheme raises a new chal-
lenge for learning algorithms in that they need to
automatically find textual evidences for each anno-
tated relation during training. For example, with the
sentence ?I like the Rebel a little better, but that is
another price jump?, simply assigning a sentiment-
bearing expression to the nearest relation candidate
is insufficient, especially when the sentiment is not
explicitly expressed.
In this paper, we propose SENTI-LSSVM, a latent
structural SVM based model for sentiment-oriented
relation extraction. SENTI-LSSVM is applied to find
the most likely set of the relation instances expressed
in a given sentence, where the latent variables are
used to assign the most appropriate textual evidences
to the respective instances.
In summary, the contributions of this paper are the
following:
? We propose SENTI-LSSVM: the first unified sta-
tistical model with the capability of extracting
instances of both binary and ternary sentiment-
oriented relations.
? We design a task-specific integer linear pro-
gramming (ILP) formulation for inference.
? We construct a new SRG corpus as a valuable
asset for the evaluation of sentiment relation
extraction.
? We conduct extensive experiments with on-
line reviews and forum posts, showing that
SENTI-LSSVM model can effectively learn from
a training corpus without explicitly annotated
subjective expressions and that its performance
significantly outperforms state-of-the-art sys-
tems.
2 Related Work
There are ample works on analyzing sentiment po-
larities and entity comparisons, but the majority of
them studied the two tasks in isolation.
Most prior approaches for fine-grained sentiment
analysis focus on polarity classification. Super-
vised approaches on expression-level analysis re-
quire the annotation of sentiment-bearing expres-
sions as training data (Jin et al., 2009; Choi
and Cardie, 2010; Johansson and Moschitti, 2011;
Yessenalina and Cardie, 2011; Wei and Gulla,
2010). However, the corresponding annotation pro-
cess is time-consuming. Although sentence-level
annotations are easier to obtain, the analysis at this
level cannot cope with sentences conveying relations
of multiple types (McDonald et al., 2007; T?ckstr?m
and McDonald, 2011; Socher et al., 2012). Lexicon-
based approaches require no training data (Ku et al.,
2006; Kim and Hovy, 2006; Godbole et al., 2007;
Ding et al., 2008; Popescu and Etzioni, 2005; Liu et
al., 2005) but suffer from inferior performance (Wil-
son et al., 2005; Qu et al., 2012). In contrast, our
method requires no annotation of sentiment-bearing
expressions for training and can predict both senti-
ment polarities and comparative relations.
Sentiment-oriented comparative relations have
been studied in the context of user-generated dis-
course (Jindal and Liu, 2006; Ganapathibhotla and
Liu, 2008). Approaches rely on linguistically moti-
vated rules and assume the existence of independent
keywords in sentences which indicate comparative
relations. Therefore, these methods fall short of ex-
tracting comparative relations based on domain de-
pendent information.
Both Johansson and Moschitti (2011) and Wu et
al. (2011) formulate fine-grained sentiment analy-
sis as a learning problem with structured outputs.
However, they focus only on polarity classification
156
of expressions and require annotation of sentiment-
bearing expressions for training as well.
While ILP has been previously applied for infer-
ence in sentiment analysis (Choi and Cardie, 2009;
Somasundaran and Wiebe, 2009; Wu et al., 2011),
our task requires a complete ILP reformulation due
to 1) the absence of annotated sentiment expressions
and 2) the constraints imposed by the joint extrac-
tion of both sentiment polarity and comparative re-
lations.
3 System Overview
This section gives an overview of the whole system
for extracting sentiment-oriented relation instances.
Prior to presenting the system architecture, we in-
troduce the essential concepts and the definitions of
two kinds of directed hypergraphs as the represen-
tation of correlated relation instances extracted from
sentences.
3.1 Concepts and Definitions
Entity. An entity is an abstract or concrete thing,
which needs not be of material existence. An entity
in this paper refers to either a product or a brand.
Attribute. An attribute is an object closely associ-
ated with or belonging to an entity, such as the lens
of digital camera.
Sentiment-Oriented Relation. A sentiment-
oriented relation is either a sentiment polarity or a
comparative relation, defined on tuples of entities
and attributes. A sentiment polarity relation conveys
either a positive or a negative attitude towards enti-
ties or their attributes, whereas a comparative rela-
tion indicates the preference of one entity over the
other entity w.r.t. an attribute.
Relation Instance. An instance of sentiment polar-
ity takes the form r(entity, attribute) with r ? {pos-
itive, negative}, such as positive(Canon 7D, sen-
sor). The polarity instances expressed in the form
of unary relations, such as ?Nikon D7000 is ex-
cellent.?, are denoted as binary relations r(entity,
whole), where the attribute whole indicates the en-
tity as a whole. In contrast, an instance of compar-
ative relation is in the form of preferred{entity, en-
tity, attribute}, e.g. preferred(Canon 7D, Nikon
D7000, price). For brevity, we refer to an instance
set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances
of the remaining relations, we represent them as
other{entity, attribute}, such as textitpartOf{wheel,
car}. These relations include objective relations
and the subjective relations other than sentiment-
oriented relations.
Mention-Based Relation Instances. A mention-
based relation instance refers to a tuple of entity
mentions with a certain relation. This concept is in-
troduced as the representation of instances in a sen-
tence by replacing entities with the corresponding
entity mentions, such as positive(?Canon SD880i?,
?wide angle view?).
Figure 1: An example of MRG.
Mention-Based Relation Graph. A mention-based
relation graph (or MRG ) represents a collection of
mention-based relation instances expressed in a sen-
tence. As illustrated in Figure 1, an MRG is a di-
rected hypergraph G = ?M,E? with a vertex set
M and an edge set E. A vertex mi ? M denotes
a mention of an entity or an attribute occurring ei-
ther within the sentence or in its context. We say
that a mention is from the context if it is mentioned
in the previous sentence or is an attribute implied
in the current sentence. An instance of a binary re-
lation in an MRG takes the form of a binary edge
el = (mi,ma), where mi and ma denote an en-
tity mention and an attribute mention respectively,
and the type l ? {positive, negative, other}. A
ternary edge el indicating comparative relation is
represented as el = (mi,mj ,ma), where two en-
tity mentions mi and mj are compared with respect
to the attribute mention ma. We define the type
l ? {better,worse} to indicate two possible direc-
tions of the relation and assume mi occurs before
mj . As a result, we have a set L of five relation
types: positive, negative, better, worse or other. Ac-
cording to these definitions, the annotations in the
SRG corpus are actually MRGs and disambiguated
entities. If there are multiple mentions referring to
the same entity, annotators are asked to choose the
157
most obvious one because it saves annotation time
and is less demanding for the entity recognition and
diambiguation modules.
Figure 2: An example of eMRG. The textual evi-
dences are wrapped by green dashed boxes.
Evidentiary Mention-Based Relation Graph. An
evidentiary mention-based relation graph, coined
eMRG , extends an MRG by associating each edge
with a textual evidence to support the corresponding
relation assertions (see Figure 2). Consequently, an
edge in an eMRG is denoted by a pair (a, c), where
a represents a mention-based relation instance and
c is the associated textual evidence. It is also re-
ferred to as an evidentiary edge. represented as
el = (mi,mj ,ma), an MRG as an evidentiary MRG
(eMRG) and the edges of eMRGs as evidentiary
edges, as shown in Figure 2.
3.2 System Architecture
Figure 3: System architecture.
As illustrated by Figure 3, at the core of our sys-
tem is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs
from sentences. For a given sentence with known
entity mentions, we select all possible mention sets
as relation candidates, where each set includes at
least one entity mention. Then we associate each
relation candidate with a set of constituents or the
whole sentence as the textual evidence candidates
(cf. Section 6.1). Subsequently, the inference com-
ponent aims to find the most likely eMRG from all
possible combinations of mention-based relation in-
stances and their textual evidences (cf. Section 6.2).
The representation eMRG is chosen because it char-
acterizes exactly the model outputs by letting each
edge correspond to an instance of mention-based re-
lation and the associated textual evidence. Finally,
the model parameters of this model are learned by
an online algorithm (cf. Section 7).
Since instance sets of sentiment-oriented relations
(sSoRs) are the expected outputs, we can obtain
sSoRs from MRGs by using a simple rule-based al-
gorithm. The algorithm essentially maps the men-
tions from an MRG into entities and attributes in an
sSoR and label the corresponding tuples with the re-
lation types of the edges from an MRG. For instances
of comparative relation, the label better or worse is
mapped to the relation type preferred.
4 SENTI-LSSVM Model
The task of sentiment-oriented relation extraction
is to determine the most likely sSoR in a sentence.
Since sSoRs are derived from the corresponding
MRGs as described in Section 3, the task is reduced
to find the most likely MRG for each sentence. Since
an MRG is created by assigning relation types to a
subset of all relation candidates, which are possible
tuples of mentions with unknown relation types, the
number of MRGs can be extremely high.
To tackle the task, one solution is to employ
an edge-factored linear model in the framework of
structural SVM (Martins et al., 2009; Tsochantaridis
et al., 2004). The model suggests that a bag of fea-
tures should be specified for each relation candidate,
and then the model predicts the most likely candi-
date sets along with their relation types to form the
optimal MRGs. As we observed, for a relation can-
didate, the most informative features are the words
near its entity mentions in the original text. How-
158
ever, if we represent a candidate by all these words,
it is very likely that the instances of different relation
types share overly similar features, because a men-
tion is often involved in more than one relation can-
didate, as shown in Figure 2. As a consequence, the
instances of different relations represented by overly
similar features can easily confuse the learning algo-
rithm. Thus, it is critical to select proper constituents
or sentences as textual evidences for each relation
candidate in both training and testing.
Consequently, we divide the task of sentiment-
oriented relation extraction into two subtasks : i)
identifying the most likely MRGs; ii) assigning
proper textual evidences to each edge of MRGs to
support their relation assertions. It is desirable to
carry out the two subtasks jointly as these two sub-
tasks could enhance each other. First, the identifi-
cation of relation types requires proper textual ev-
idences; second, the soft and hard constraints im-
posed by the correlated relation instances facilitate
the recognition of the corresponding textual evi-
dences. Since the eMRGs are created by attaching
every MRG with a set of textual evidences, tackling
the two subtasks simultaneously is equivalent to se-
lecting the most likely eMRG from a set of eMRG
candidates. It is challenging because our SRG corpus
does not contain any annotation of textual evidences.
Formally, let X denote the set of all available sen-
tences, and we define y ? Y(x)(x ? X ) as the set
of labeled edges of an MRG and Y = ?x?XY(x).
Since the assignments of textual evidences are not
observed, an assignment of evidences to y is de-
noted by a latent variable h ? H(x) and H =
?x?XH(x). Then (y, h) corresponds to an eMRG,
and (a, c) ? (y, h) is a labeled edge a attached
with a textual evidence c. Given a labeled dataset
D = {(x1, y1), ..., (xn, yn)} ? (X ? Y)n, we aim
to learn a discriminant function f : X ? Y?H that
outputs the optimal eMRG (y, h) ? Y(x)?H(x) for
a given sentence x.
Due to the introduction of latent variables, we
adopt the latent structural SVM (Yu and Joachims,
2009) for structural classification. Our discriminant
function is defined as
f(x) = argmax(y,h)?Y(x)?H(x)?>?(x, y, h) (1)
where ?(x, y, h) is the feature function of an eMRG
(y, h) and ? is the corresponding weight vector.
To ensure tractability, we also employ edge-based
factorization for our model. Let Mp denote a set of
entity mentions and yr(mi) be a set of edges labeled
with sentiment-oriented relations incident to mi, the
factorization of ?(x, y, h) is given as
?(x, y, h) =
?
(a,c)?(y,h)
?e(x, a, c) + (2)
?
mi?Mp
?
a,a??yr(mi),a 6=a?
?c(a, a?)
where ?e(x, a, c) is a local edge feature function
for a labeled edge a attached with a textual evidence
c and ?c(a, a?) is a feature function capturing co-
occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.
5 Feature Space
The following features are used in the feature func-
tions (Equation 2):
Unigrams: As mentioned before, a textual evi-
dence attached to an edge in MRG is either a word,
phrase or sentence. We consider all lemmatized un-
igrams in the textual evidence as unigram features.
Context: Since web users usually express related
sentiments about the same entity across sentence
boundaries, we describe the sentiment flow using a
set of contextual binary features. For example, if en-
tity A is mentioned in both the previous sentence and
the current sentence, a set of contextual binary fea-
tures are used to indicate all possible combinations
of the current and the previous mentioned sentiment-
oriented relations regarding to entity A.
Co-occurrence: We have mentioned the co-
occurrence feature in Equation 2, indicated by
?c(a, a?). It captures the co-occurrence of two la-
beled edges incident to the same entity mention.
Note that the co-occurrence feature function is con-
sidered only if there is a contrast conjunction such as
?but? between the non-shared entity mentions inci-
dent to the two labeled edges.
Senti-predictors: Following the idea of (Qu et
al., 2012), we encode the prediction results from
the rule-based phrase-level multi-relation predic-
tor (Ding et al., 2009) and from the bag-of-opinions
predictor (Qu et al., 2010) as features based on the
textual evidence. The output of the first predictor
is an integer value, while the output of the second
predictor is a sentiment relation, such as ?positive?,
159
?negative?, ?better? or ?worse?. We map the rela-
tional outputs into integer values and then encode
the outputs from both predictors as senti-predictor
features.
Others: The commonly used part-of-speech tags
are also included as features. Moreover, for an edge
candidate, a set of binary features are used to denote
the types of the edge and its entity mentions. For in-
stance, a binary feature indicates whether an edge is
a binary edge related to an entity mentioned in con-
text. To characterize the syntactic dependencies be-
tween two adjacent entity mentions, we use the path
in the dependency tree between the heads of the cor-
responding constituents, the number of words and
other mentions in-between as features. Additionally,
if the textual evidence is a constituent, its feature
w.r.t. an edge is the dependency path to the clos-
est mention of the edge that does not overlap with
this constituent.
6 Structural Inference
In order to find the best eMRG for a given sentence
with a well trained model, we need to determine
the most likely relation type for each relation candi-
date and support the corresponding assertions with
proper textual evidences. We formulate this task
as an Integer Linear Programming (ILP). Instead of
considering all constituents of a sentence, we empir-
ically select a subset as textual evidences for each
relation candidate.
6.1 Textual Evidence Candidates Selection
Textual evidences are selected based on the con-
stituent trees of sentences parsed by the Stanford
parser (Klein and Manning, 2003). For each men-
tion in a sentence, we first locate a constituent in
the tree with the maximal overlap by Jaccard sim-
ilarity. Starting from this constituent, we consider
two types of candidates: type I candidates are con-
stituents at the highest level which contain neither
any word of another mention nor any contrast con-
junctions such as ?but?; type II candidates are con-
stituents at the highest level which cover exactly two
mentions of an edge and do not overlap with any
other mentions. For a binary edge connecting an en-
tity mention and an attribute mention, we consider
a type I candidate starting from the attribute men-
tion. For a binary edge connecting two entity men-
tions, we consider type I candidates starting from
both mentions. Moreover, for a comparative ternary
edge, we consider both type I and type II candidates
starting from the attribute mention. This strategy is
based on our observation that these candidates of-
ten cover the most important information w.r.t. the
covered entity mentions.
6.2 ILP Formulation
We formulate the inference problem of finding the
best eMRG as an ILP problem due to its convenient
integration of both soft and hard constraints.
Given the model parameters ?, we reformulate
the score of an eMRG in the discriminant function
(1) as follows,
?>?(x, y, h) =
?
(a,c)?(y,h)
saczac +
?
mi?Mp
?
a,a??yr(mi),a 6=a?
saa?zaa?
where sac = ?>?e(x, a, c) denotes the score of a
labeled edge a attached with a textual evidence c,
saa? = ?>?c(a, a?) is the edge co-occurrence score,
the binary variable zac indicates the presence or ab-
sence of the corresponding edge, and zaa? indicates
if two edges co-occurr. As not every edge set can
form an eMRG, we require that a valid eMRG should
satisfy a set of linear constraints, which form our
constraint space. Then function (1) is equivalent to
max
z?B s
>z + ?zd
s.t. A
?
?
z
?
?
?
? ? d
z,?, ? ? B
where B = 2S with S = {0, 1}, and ? and ? are
auxiliary binary variables that help define the con-
straint space. The above optimization problem takes
exactly the form of an ILP because both the con-
straints and the objective function are linear, and all
variables take only integer values.
In the following, we consider two types of con-
straint space, 1) an eMRG with only binary edges and
2) an eMRG with both binary and ternary edges.
160
eMRG with only Binary Edges: An eMRG has
only binary edges if a sentence contains no attribute
mention or at most one entity mention. We expect
that each edge has only one relation type and is sup-
ported by a single textual evidence. To facilitate the
formulation of constraints, we introduce ?el to de-
note the presence or absence of a labeled edge el,
and ?ec to indicate if a textual evidence c is assigned
to an unlabeled edge e. Then the binary variable for
the corresponding evidentiary edge zelc = ?ec ? ?el ,
where the ILP formulation of conjunction can be
found in (Martins et al., 2009).
Let Ce denote the set of textual evidence candi-
dates of an unlabeled edge e. The constraint of at
most one textual evidence per edge is formulated as:
?
c?Ce
?ec ? 1 (3)
Once a textual evidence is assigned to an edge,
their relation labels should match and the number
of labeled edges must agree with the number of at-
tached textual evidences. Further, we assume that a
textual evidence c conveys at most one relation so
that an evidence will not be assigned to the relations
of different types, which is the main problem for the
structural SVM based model. Let ?cl indicate that
the textual evidence c is labeled by the relation type
l. The corresponding constraints are expressed as,
?
l?Le
?el =
?
c?Ce
?ec; zelc ? ?cl;
?
l?L
?cl ? 1
where Le denotes the set of all possible labels for
an unlabeled edge e, and L is the set of all relation
types of MRGs (cf. Section 3).
In order to avoid a textual evidence being overly
reused by multiple relation candidates, we first pe-
nalize the assignment of a textual evidence c to a
labeled edge a by associating the corresponding zac
with a fixed negative cost ?? in the objective func-
tion. Then the selection of one textual evidence per
edge a is encouraged by associating ? to zdc in the
objective function, where zdc =
?
e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves as
a candidate. The disjunction zdc is expressed as:
zdc ? ?e, e ? Sc
zdc ?
?
e?Sc
?e
(a) Binary edge structure
(b) Ternary edge structure
Figure 4: Alternative structures associated with an
attribute mention.
This soft constraint not only encourages one textual
evidence per edge, but also keeps it eligible for mul-
tiple assignments.
For any two labeled edge a and a? incident
to the same entity mention, the edge-to-edge co-
occurrence is described by zca,a? = za ? za? .
eMRG with both Binary and Ternary Edges: If
there are more than one entity mentions and at least
one attribute mention in a sentence, an eMRG can
potentially have both binary and ternary edges. In
this case, we assume that each mention of attributes
can participate either in binary relations or in ternary
relations. The assumption holds in more than 99.9%
of the sentences in our SRG corpus, thus we describe
it as a set of hard constraints. Geometrically, the as-
sumption can be visualized as the selection between
two alternative structures incident to the same at-
tribute mention, as shown in Figure 4. Note that,
in the binary edge structure, we include not only the
edges incident to the attribute mention but also the
edge between the two entity mentions.
Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mention
mi. Variable ? bmi =
?
el?Sbmi
?el indicates whether
the attribute mention is associated with a binary
edge structure or not. In the same manner, we use
? tmi =
?
el?Stmi
?el to indicate the association of the
an attribute mention mi with an ternary edge struc-
ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is
161
formulated as ? bmi + ? tmi = 1. As this influencesonly the edges incident to an attribute mention, we
keep all the constraints introduced in the previous
section unchanged except for constraint (3), which
is modified as
?
c?Ce
?ec ? ? bmi ;
?
c?Ce
?ec ? ? tmi
Therefore, we can have either binary edges or
ternary edges for an attribute mention.
7 Learning Model Parameters
Given a set of training sentences D =
{(x1, y1), . . . , (xn, yn)}, the best weight vec-
tor ? of the discriminant function (1) is found by
solving the following optimization problem:
min
?
1
n
n?
i=1
[ max
(y?,h?)?Y(x)?H(x)
(?>?(x, y?, h?)+?(h?, y?, y))
? max
h??H(x)
?>?(x, y, h?)] + ?|?|] (4)
where ?(h?, y?, y) is a loss function measuring the dis-
crepancies between an eMRG (y, h?) with gold stan-
dard edge labels y and an eMRG (y?, h?) with inferred
labeled edges y? and textual evidences h?. Due to the
sparse nature of the lexical features, we apply L1
regularizer to the weight vector ?, and the degree of
sparsity is controlled by the hyperparameter ?.
Since the L1 norm in the above optimization
problem is not differentiable at zero, we apply the
online forward-backward splitting (FOBOS) algo-
rithm (Duchi and Singer, 2009). It requires two steps
for updating the weight vector ? by using a single
training sentence x on each iteration t.
?t+ 12 = ?t ? ?t?t
?t+1 = arg min
?
1
2?? ? ?t?
2 + ?t?|?|
where ?t is the subgradient computed without con-
sidering the L1 norm and ?t is the learning rate.
For a labeled sentence x, ?t = ?(x, y??, h??) ?
?(x, y, h??), where the feature functions of the corre-
sponding eMRGs are inferred by solving (y??, h??) =
arg max(h?,y?)?H(x)?Y(x)[?
>?(x, y?, h?) + ?(h?, y?, y)]
and (y, h??) = arg maxh??H(x) ?>?(x, y, h?), as in-
dicated in the optimization problem (4).
The former inference problem is similar to the
one we considered in the previous section except
the inclusion of the loss function. We incorporate
the loss function into the ILP formulation by defin-
ing the loss between an MRG (y, h) and a gold stan-
dard MRG as the sum of per-edge costs. In our ex-
periments, we consider a positive cost ? for each
wrongly labeled edge a, so that if an edge a has a
different label from the gold standard, we add ? to
the coefficient sac of the corresponding variable zac
in the objective function of the ILP formulation.
In addition, since the non-positive weights of edge
labels in the initial learning phrase often lead to
eMRGs with many unlabeled edges, which harms the
learning performance, we fix it by adding a con-
straint for the minimal number of labeled edges in
an eMRG, ?
a?A
?
c?Ca
?ac ? ? (5)
where A is the set of all labeled edge candidates and
? denotes the minimal number of labeled edges.
Empirically, the best way to determine ? is to
make it equal to the maximal number of labeled
edges in an eMRG with the restriction that a tex-
tual evidence can be assigned to at most one edge.
By considering all the edge candidates A and all the
textual evidence candidates C as two vertex sets in a
bipartite graph G? = ?V = (A,C), E? (with edges in
E indicating which textual evidence can be assigned
to which edge), ? corresponds to exactly the size of
a maximum matching of the bipartite graph1.
To find the optimal eMRG (y, h??), for the gold la-
bel k of each edge, we consider the following set of
constraints for inference since the labels of the edges
are known for the training data,
?
c?Ce
?ec ? 1; ?ec ? lck
?
k??L
lck? ? 1;
?
e?Sc
?ec ? 1
We include also the soft constraints, which avoid
a textual evidence being overly reused by multiple
relations, and the constraints similar to (5) to ensure
a minimal number of labeled edges and a minimal
number of sentiment-oriented relations.
1It is computed by the Hopcroft-Karp algorithm (Hopcroft
and Karp, 1973) in our implementation.
162
8 SRG Corpus
For evaluation we constructed the SRG corpus,
which in total consists of 1686 manually annotated
online reviews and forum posts in the digital camera
and movie domains2. For each domain, we maintain
a set of attributes and a list of entity names.
The annotation scheme for the sentiment repre-
sentation asserts minimal linguistic knowledge from
our annotators. By focusing on the meanings of the
sentences, the annotators make decisions based on
their language intuition, not restricted by specific
syntactic structures. Taking the example in Figure
2, the annotators only need to mark the mentions of
entities and attributes from both the sentences and
the context, disambiguate them, and label (?Canon
7D?, ?Nikon D7000?, price) as worse and (?Canon
7D?, ?sensor?) as positive, whereas in prior work,
people have annotated the sentiment-bearing expres-
sions such as ?great? and link them to the respective
relation instances as well. This also enables them
to annotate instances of both sentiment polarity and
comparative relaton, which are conveyed by not only
explicit sentiment-bearing expressions like ?excel-
lent performance?, but also factual expressions im-
plying evaluations such as ?The 7V has 10x optical
zoom and the 9V has 16x.?.
Camera Movie
Reviews Forums Reviews Forums
positive 386 1539 879 905
negative 165 363 529 331
comparison 30 480 39 35
Table 1: Distribution of relation instances in SRG corpus.
14 annotators participated in the annotation
project. After a short training period, annotators
worked on randomly assigned documents one at a
time. For product reviews, the system lists all rel-
evant information about the entity and the prede-
fined attributes. For forum posts, the system shows
only the attribute list. For each sentence in a doc-
ument, the annotator first determines if it refers to
an entity of interest. If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Ama-
zon.com; the 667 camera forum posts are downloaded from fo-
rum.digitalcamerareview.com; the 138 movie reviews and 774
forum posts are from imdb.com and boards.ie respectively
as off-topic. Otherwise, the annotator will identify
the most obvious mentions, disambiguate them, and
mark the MRGs. We evaluate the inter-annotator
agreement on sSoRs in terms of Cohen?s Kappa
(?) (Cohen, 1968). An average Kappa value of 0.698
was achieved on a randomly selected set consisting
of 412 sentences.
Table 1 shows the corpus distribution after nor-
malizing them into sSoRs. Camera forum posts con-
tain the largest proportion of comparisons because
they are mainly about the recommendation of dig-
ital cameras. In contrast, web users are much less
interested in comparing movies, in both reviews and
forums. In all subsets, positive relations play a dom-
inant role since web users intend to express more
positive attitudes online than negative ones (Pang
and Lee, 2007).
9 Experiments
This section describes the empirical evaluation of
SENTI-LSSVM together with two competitive base-
lines on the SRG corpus.
9.1 Experimental Setup
We implemented a rule-based baseline (DING-
RULE) and a structural SVM (Tsochantaridis et
al., 2004) baseline (SENTI-SSVM) for comparison.
The former system extends the work of Ding et
al. (2009), which designed several linguistically-
motivated rules based on a sentiment polarity lexi-
con for relation identification and assumes there is
only one type of sentiment relation in a sentence. In
our implementation, we keep all the rules of (Ding et
al., 2009) and add one phrase-level rule when there
are more than one mention in a sentence. The ad-
ditional rule assigns sentiment-bearing words and
negators to its nearest relation candidates based on
the absolute surface distance between the words and
the corresponding mentions. In this case, the phrase-
level sentiment-oriented relations depend only on
the assigned sentiment words and negators. The lat-
ter system is based on a structural SVM and does
not consider the assignment of textual evidences to
relation instances during inference. The textual fea-
tures of a relation candidate are all lexical and sen-
timent predictor features within a surface distance
of four words from the mentions of the candidate.
163
Thus, this baseline does not need the inference con-
straints of SENTI-LSSVM for the selection of textual
evidences. To gain more insights into the model,
we also evaluate the contribution of individual fea-
tures of SENTI-LSSVM. In addition, to show if identi-
fying sentiment polarities and comparative relations
jointly works better than tackling each task on its
own, we train SENTI-LSSVM for each task separately
and combine their predictions according to compat-
ibility rules and the corresponding graph scores.
For each domain and text genre, we withheld 15%
documents for development and use the remaining
for cross validation. The hyperparameters of all sys-
tems are tuned on the development datasets. For all
experiments of SENTI-LSSVM, we use ? = 0.0001
for the L1 regularizer in Eq.(4) and ? = 0.05 for
the loss function; and for SENTI-SSVM, ? = 0.0001
and ? = 0.01. Since the relation type of off-topic
sentences is certainly other, we evaluate all systems
with 5-fold cross-validation only on the on-topic
sentences in the evaluation dataset. Since the same
sSoR can have several equivalent MRGs and the rela-
tion type other is not of our interest, we evaluate the
sSoRs in terms of precision, recall and F-measure.
All reported numbers are averages over the 5 folds.
9.2 Results
Table 2 shows the complete results of all sys-
tems. Here our model SENTI-LSSVM outperformed
all baselines in terms of the average F-measure
scores and recalls by a large margin. The F-measure
on movie reviews is about 14% over the best base-
line. The rule-based system has higher precision
than recall in most cases. However, simply increas-
ing the coverage of the domain independent senti-
ment polarity lexicon might lead to worse perfor-
mance (Taboada et al., 2011) because many sen-
timent oriented relations are conveyed by domain
dependent expressions and factual expressions im-
plying evaluations, such as ?This camera does not
have manual control.? Compared to DING-RULE,
SENTI-SSVM performs better in the camera domain
but worse for the movies due to many misclassi-
fication of negative relation instances as other. It
also wrongly predicted more positive instances as
other than SENTI-LSSVM. We found that the recalls
of these instances are low because they often have
overly similar features with the instances of the type
other linking to the same mentions. The problem
gets worse in the movie domain since i) many sen-
tences contain no explicit sentiment-bearing words;
ii) the prior polarity of the sentiment-bearing words
do not agree with their contextual polarity in the
sentences. Consider the following example from a
forum post about the movie ?Superman Returns?:
?Have a look at Superman: the Animated Series or
Justice League Unlimited . . . that is how the char-
acters of Superman and Lex Luthor should be.?. In
contrast, our model minimizes the overlapping fea-
tures by assigning them to the most likely relation
candidates. This leads to significantly better per-
formance. Although SENTI-SSVM has low recall for
both positive and negative relations, it achieves the
highest recall for the comparative relation among all
systems in the movie domain and camera reviews.
Since less than 1% of all instances are for compara-
tive relations in these document sets and all models
are trained to optimize the overall accuracy, SENTI-
LSSVM intends to trade off the minority class for the
overall better performance. This advantage disap-
pears on the camera forum posts, where the number
of instances of comparative relation is 12 times more
than that in the other data sets.
All systems perform better in predicting positive
relations than the negative ones. This corresponds
well to the empirical findings in (Wilson, 2008) that
people intend to use more complex expressions for
negative sentiments than their affirmative counter-
parts. It is also in accordance with the distribution of
these relations in our SRG corpus which is randomly
sampled from the online documents. For learning
systems, it can also be explained by the fact that the
training data for positive relations are considerably
more than those for negative ones. The comparative
relation is the hardest one to process since we found
that many corresponding expressions do not contain
explicit keywords for comparison.
To understand the performance of the key fea-
ture groups in our model better, we remove each
group from the full SENTI-LSSVM system and eval-
uate the variations with movie reviews and camera
forum posts, which have relatively balanced distri-
bution of relation types. As shown in Table 3, the
features from the sentiment predictors make signif-
icant contributions for both datasets. The differ-
ent drops of the performance indicate that the po-
164
Positive Negative Comparison Micro-average
P R F P R F P R F P R F
Ca
me
ra
Fo
rum
DING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0
SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9
SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4
Ca
me
ra
Re
-
vie
w DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3
SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7
Mo
vie
Fo
rum
DING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2
SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6
SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0
Mo
vie Re
-
vie
w DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7
SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9
Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM. Boldface figures are statistically
significantly better than all others in the same comparison group under t-test with p = 0.05.
Feature Models Movie Reviews Camera Forums
full system 62.9 45.4
?unigram 63.2 (+0.3) 41.2 (-4.2)
?context 54.5 (-8.4) 46.0 (+0.6)
?co-occurrence 62.6 (-0.3) 44.9 (-0.5)
?senti-predictors 61.3 (-1.6) 34.3 (-11.1)
Table 3: Micro-average F-measure of SENTI-LSSVM
with different feature models
larities predicted by rules are more consistent in
camera forum posts than in movie reviews. Due
to the complexity of expressions in the movie re-
views our model cannot benefit from the unigram
features but these features are a good compensation
for the sentiment predictor features in camera fo-
rum posts. The sharp drop by removing the context
features from our model on movie reviews indicates
that the sentiments in movie reviews depend highly
on the relations of the previous sentences. In con-
trast, the sentiment-oriented relations of the previ-
ous sentences could be a reason of overfitting for
camera forum data. The edge co-occurrence fea-
tures do not play an important role in our model
since the number of co-occurred sentiment-oriented
relations in the sentences with contrast conjunctions
like ?but? is small. However, we found that allow-
ing the co-occurrence of any sentiment-oriented re-
lations would harm the performance of the model.
In addition, our experiments showed that the sep-
arated approach, which trains a model for senti-
ment polarities and comparative relations respec-
tively, leads to a decrease by almost 1% in terms of
the F-measure averaged over all four datasets. The
largest drop of F-measure is 3% on camera forum
posts, since this dataset contains the largest propor-
tion of comparative relations. We found that the er-
rors are increased when the trained models make
conflicting predictions. In this case, the joint ap-
proach can take all factors into account and make
more consistent decisions than the separated ap-
proaches.
10 Conclusion
We proposed SENTI-LSSVM model for extracting in-
stances of both sentiment polarities and comparative
relations. For evaluating and training the model, we
created an SRG corpus by using a lightweight an-
notation scheme. We showed that our model can
automatically find textual evidences to support its
relation predictions and achieves significantly bet-
ter F-measure scores than alternative state-of-the-art
methods.
References
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
165
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 590?598, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual meeting of
the Association for Computational Linguistics, pages
269?274. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 431?
439, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or Par-
tial Credit. Psychological bulletin, 70(4):213.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining, pages 231?240, New
York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity
discovery and assignment for opinion mining applica-
tions. In Proceedings of the ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
1125?1134.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
The Journal of Machine Learning Research, 10:2899?
2934.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, pages 241?248, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs (system demonstration). In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media.
John E Hopcroft and Richard M Karp. 1973. An n?5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on computing, 2(4):225?231.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Proceedings of the
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 168?177, New York, NY,
USA. ACM.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1195?
1204, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings of the 21st In-
ternational Conference on Artificial Intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities?
exploration of pipelines and joint models. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics, volume 11, pages 101?106.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sent-
ment corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, SST ?06, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in
news and blog corpora. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 100?107.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351, New
York, NY, USA. ACM.
Andr? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Annual meeting of the Association for Computational
Linguistics, pages 342?350.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
166
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Chu-Ren
Huang and Dan Jurafsky, editors, Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), ACL Anthology, pages 913?
921, Beijing, China. Tsinghua University Press.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum. 2012.
A weakly supervised model for sentence-level seman-
tic orientation analysis with multiple experts. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 149?159,
Jeju Island, Korea, July. Proceedings of the Annual
meeting of the Association for Computational Linguis-
tics.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1201?1211.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 226?234.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar T?ckstr?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the 33rd
European conference on Advances in information re-
trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.
Springer-Verlag.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 575?584,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, pages 104?112.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the Annual meeting of the Association
for Computational Linguistics, pages 404?413.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained subjectivity
and sentiment analysis: recognizing the intensity, po-
larity, and attitudes of private states. Ph.D. thesis,
UNIVERSITY OF PITTSBURGH.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based sen-
timent representation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1332?1341.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, page 147.
Ning Yu and Sandra K?bler. 2011. Filling the gap:
Semi-supervised learning for opinion detection across
domains. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
200?209. Association for Computational Linguistics.
167
168
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 187?195,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
AIDArabic
A Named-Entity Disambiguation Framework for Arabic Text
Mohamed Amir Yosef, Marc Spaniol, Gerhard Weikum
Max-Planck-Institut f?ur Informatik, Saarbr?ucken, Germany
{mamir|mspaniol|weikum}@mpi-inf.mpg.de
Abstract
There has been recently a great progress in
the field of automatically generated knowl-
edge bases and corresponding disambigua-
tion systems that are capable of mapping
text mentions onto canonical entities. Ef-
forts like the before mentioned have en-
abled researchers and analysts from vari-
ous disciplines to semantically ?understand?
contents. However, most of the approaches
have been specifically designed for the En-
glish language and - in particular - sup-
port for Arabic is still in its infancy. Since
the amount of Arabic Web contents (e.g.
in social media) has been increasing dra-
matically over the last years, we see a
great potential for endeavors that support
an entity-level analytics of these data. To
this end, we have developed a framework
called AIDArabic that extends the existing
AIDA system by additional components
that allow the disambiguation of Arabic
texts based on an automatically generated
knowledge base distilled from Wikipedia.
Even further, we overcome the still exist-
ing sparsity of the Arabic Wikipedia by ex-
ploiting the interwiki links between Arabic
and English contents in Wikipedia, thus,
enriching the entity catalog as well as dis-
ambiguation context.
1 Introduction
1.1 Motivation
Internet data including news articles and web pages,
contain mentions of named-entities such as people,
places, organizations, etc. While in many cases
the intended meanings of the mentions is obvi-
ous (and unique), in many others, the mentions
are ambiguous and have many different possible
meanings. Therefore, Named-Entity Disambigua-
tion (NED) is essential for many application in the
domain of Information Retrieval (such as informa-
tion extraction). It also enables producing more
useful and accurate analytics. The problem has
been exhaustively studied in the literature. The
essence of all NED techniques is using background
information extracted from various sources (e.g.
Wikipedia), and use such information to know the
correct/intended meaning of the mention.
The Arabic content is enormously growing on
the Internet, nevertheless, background ground in-
formation is clearly lacking behind other languages
such as English. Consider Wikipedia for example,
while the English Wikipedia contains more than 4.5
million articles, the Arabic version contains less
than 0.3 million ones
1
. As a result, and up to our
knowledge, there is no serious work that has been
done in the area of performing NED for Arabic
input text.
1.2 Problem statement
NED is the problem of mapping ambiguous names
of entities (mentions) to canonical entities regis-
tered in an entity catalog (knowledgebase) such as
Freebase (www.freebase.com), DBpedia (Auer et
al., 2007), or Yago (Hoffart et al., 2013). For ex-
ample, given the text ?I like to visit Sheikh Zayed.
Despite being close to Cairo, it is known to be a
quiet district?, or in Arabic,? qJ



??@

?PA K


	
P I
.
k

@
	
?? A ?E
.
Q

?
	
?? ?
	
?Q ?A K
.
Z?Y??A K
.
	
Q



?

J


K ?


??
	
? . Y K


@
	
P

?Q?A

?? @?. When processing this text automatically,
we need to be able to tell that Sheikh Zayed de-
notes the the city in Egypt
2
, not the mosque in
Abu Dhabi
3
or the President of the United Arab
1
as of July 2014
2
http://en.wikipedia.org/wiki/Sheikh Zayed City
http://ar.wikipedia.org/wiki/YK


@
	
P_ qJ



?.? @_

?
	
JK


Y?
3
http://en.wikipedia.org/wiki/Sheikh Zayed Mosque
http://ar.wikipedia.org/wiki/YK


@
	
P_ qJ



?.? @_ ??Ag
.
187
Emirates
4
. In order to automatically establish such
mappings, the machine needs to be aware of the
characteristic description of each entity, and try to
find the most suitable one given the input context.
In our example, knowing that the input text men-
tioned the city of Cairo favors the Egyptian city
over the mosque in Abu Dhabi, for example. In
principle, state-of-the-art NED frameworks require
main four ingredients to solve this problem:
? Entity Repository: A predefined universal
catalog of all entities known to the NED
framework. In other words, each mention in
the input text must be mapped to an entity in
the repository, or to null indicating the correct
entity is not included in the repository.
? Name-Entity Dictionary: It is a many-to-
many relation between possible mentions and
the entities in the repository. It connects an
entity with different possible mentions that
might be used to refer to this entity, as well as
connecting a mention with all potential candi-
date entity it might denote.
? Entity-Descriptions: It keeps per entity a
bag of characteristic keywords or keyphrases
that distinguishes an entity from another. In
addition, they come with scoring scheme that
signify the specificity of such keyword to that
entity.
? Entity-Entity Relatedness Model: For co-
herent text, the entities that are used for map-
ping all the mentions in the input text, should
be semantically related. For that reason, an
entity-entity relatedness model is required to
asses the coherence.
For the English language, all of the ingredi-
ents mentioned above are richly available. For
instance, the English Wikipedia is a comprehen-
sive up-to-date resource. Many NED systems
use Wikipedia as their entity repository. Further-
more, many knowledge bases are extracted from
Wikipedia as well. When trying to apply the exist-
ing NED approaches on the Arabic text, we face
the following challenges:
? Entity Repository: There is no such compre-
hensive entity catalog. Arabic Wikipedia is an
4
http://en.wikipedia.org/wiki/Zayed bin Sultan Al Nahyan
http://ar.wikipedia.org/wiki/
	
?AJ


?
	
E_ ?

@_
	
?A???_
	
?K
.
_ YK


@
	
P
order of magnitude smaller than the English
one. In addition, many entities in the Arabic
Wikipedia are specific to the Arabic culture
with no corresponding English counterpart.
As a consequence, even many prominent enti-
ties are missing from the Arabic Wikipedia.
? Name-Entity Dictionary: Most of the name-
entity dictionary entries originate from man-
ual input (e.g. anchor links). Like outlined
before, Arabic Wikipedia has fewer resources
to extract name-entity mappings, caused by
the lack of entities and lack of manual input.
? Entity-Descriptions: As already mentioned,
there is a scarcity of anchor links in the Arabic
Wikipedia. Further, the categorization system
of entities is insufficient, Both are essential
sources of building the entities descriptions.
Hence, it is more challenging to produce com-
prehensive description of each entity.
? Entity-Entity Relatedness Model: Related-
ness estimation among entities is usually com-
puted using the overlap in the entities descrip-
tion and/or link structure of Wikipedia. Due to
the previously mentioned scarcity of contents
in the Arabic Wikipedia, it is also difficult to
accurately estimate the entity-entity related-
ness.
As a consequence, the main challenge in per-
forming NED on Arabic text is the lack of a com-
prehensive entity catalog together with rich descrip-
tions of each entity. We considered our open source
AIDA system
5
(Hoffart et al., 2011)- mentioned as
state-of-the-art NED System by (Ferrucci, 2012) -
as a starting point and modified its data acquisition
pipeline in order to generate a schema suitable for
performing NED on Arabic text.
1.3 Contribution
We developed an approach to exploit and fuse cross-
lingual evidences to enrich the background informa-
tion we have about entities in Arabic to build a com-
prehensive entity catalog together with their con-
text that is not restricted to the Arabic Wikipedia.
Our contributions can be summarized in the follow-
ing points:
? Entity Repository: We switched to
YAGO3(Mahdisoltani et al., 2014), the
5
https://www.github.com/yago-naga/aida
188
multilingual version of YAGO2s. YAGO3
comes with a more comprehensive catalog
that covers entities from different languages
(extracted from different Wikipedia dumps).
While we selected YAGO3 to be our back-
ground knowledge base, any multi-lingual
knowledge base such as Freebase could be
used as well.
? Name-Entity Dictionary: We compiled a
dictionary from YAGO3 and Freebase to pro-
vide the potential candidate entities for each
mention string. While the mention is in Ara-
bic, the entity can belong to either the English
or the Arabic Wikipedia.
? Entity-Descriptions: We harnessed different
ingredients in YAGO3, and Wikipedia to pro-
duce a rich entity context schema. For the
sake of precision, we did not employ any au-
tomated translation.
? Entity-Entity Relatedness Model: We
fused the link structure of both the English
and Arabic Wikipedia?s to compute a com-
prehensive relatedness measure between the
entities.
2 Related Work
NED is one of the classical NLP problems that
is essential for many Information Retrieval tasks.
Hence, it has been extensively addressed in NLP
research. Most of NED approaches use Wikipedia
as their knowledge repository. (Bunescu and Pasca,
2006) defined a similarity measure that compared
the context of a mention to the Wikipedia cate-
gories of the entity candidate. (Cucerzan, 2007;
Milne and Witten, 2008; Nguyen and Cao, 2008)
extended this framework by using richer features
for similarity comparison. (Milne and Witten,
2008) introduced the notion of semantic related-
ness and estimated it using the the co-occurrence
counts in Wikipedia. They used the Wikipedia link
structure as an indication of occurrence. Below,
we give a brief overview on the most recent NED
systems:
The AIDA system is an open source system
that employs contextual features extracted from
Wikipedia (Hoffart et al., 2011; Yosef et al., 2011).
It casts the NED problem into a graph problem
with two types of nodes (mention nodes, and en-
tity nodes). The weights on the edges between the
mentions and the entities are the contextual similar-
ity between mention?s context and entity?s context.
The weights on the edges between the entities are
the semantic relatedness among those entities. In a
subsequent process, the graph is iteratively reduced
to achieve a dense sub-graph where each mention
is connected to exactly one entity.
The CSAW system uses local scores computed
from 12 features extracted from the context sur-
rounding the mention, and the candidate entities
(Kulkarni et al., 2009). In addition, it computes
global scores that captures relatedness among anno-
tations. The NED is then formulated as a quadratic
programming optimization problem, which nega-
tively affects the performance. The software, how-
ever, is not available.
DBpedia Spotlight uses Wikipedia anchors, ti-
tles and redirects to search for mentions in the input
text (Mendes et al., 2011). It casts the context of the
mention and the entity into a vector-space model.
Cosine similarity is then applied to identify the
candidate with the highest similarity. Nevertheless,
their model did not incorporate any semantic relat-
edness among entities. The software is currently
available as a service.
TagMe 2 exploits the Wikipedia link structure to
estimate the relatedness among entities (Ferragina
and Scaiella, 2010). It uses the measure defined by
(Milne and Witten, 2008) and incorporates a voting
scheme to pick the right mapping. According to
the authors, the system is geared for short input
text with limited context. Therefore, the approach
favors coherence among entities over contextual
similarity. TagMe 2 is available a service.
Illinois Wikifier formulates NED as an opti-
mization problem with an objective function de-
signed for higher global coherence among all men-
tions (Ratinov et al., 2011). In contrast to AIDA
and TagMe 2, it does not incorporate the link struc-
ture of Wikipedia to estimate the relatedness among
entities. Instead, it uses normalized Google sim-
ilarity distance (NGD) and pointwise mutual in-
formation. The software is as well available as a
service.
Wikipedia Miner is a machine-learning based
approach (Milne and Witten, 2008). It exploits
three features in order to train the classifier. The
features it employs are prior probability that a men-
tion refers to a specific entity, properties extracted
from the mention context, and finally the entity-
entity relatedness. The software of Wikipedia
189
YAGO3
English 
Wikipedia
Arabic 
Wikipedia
YAGO
Extractor
Entities 
Dictionary
Categories 
Dictionary
Standard AIDA 
Builder
Mixed 
AIDA
Schema
Translator
Mixed
AIDA
Schema
Filter
Arabic
AIDA
Schema
Freebase
Freebase-to-YAGO 
Dictionary
Original AIDA Pipeline
Extraction AIDA Schema Building Translation Filtration
Figure 1: AIDArabic Architecture
Miner is available on their Website.
The approaches mentioned before have been de-
veloped for English language NED. As such, none
of them is ready to handle Arabic input without
major modification.
As of now, no previous research exploits cross-
lingual resources to enable NED for Arabic text.
Nevertheless, cross-lingual resources have been
used to improve Arabic NER (Darwish, 2013).
They used Arabic and English Wikipedia together
with DBpedia in order to build a large Arabic-
English dictionary for names. This augments the
Arabic names with a capitalization feature, which
is missing in the Arabic language.
3 Architecture
In order to build AIDArabic, we have extended the
pipeline used for building an English AIDA schema
from the YAGO knowledge base. The new archi-
tecture is shown in Figure 1 and indicates those
components, that have been added for AIDArabic.
These are pre- and post-processing stages to the
original AIDA schema extractor. The new pipeline
can be divided into the following stages:
Extraction
We have configured a dedicated YAGO3 extrac-
tor to provide the data necessary for AIDAra-
bic. To this end, we feed the English and Arabic
Wikipedia?s into YAGO3 extractor to provide three
major outputs:
? Entity Repository: A comprehensive set of
entities that exist in both, the English and Ara-
bic Wikipedia?s. In addition, the correspond-
ing anchortexts, categories as well as links
from and/to each entity.
? Entity Dictionary: This is an automatically
compiled mappings that captures the inter-
wiki links among the English and the Arabic
Wikipedia?s.
? Categories Dictionary: This is also an auto-
matically harvested list of mappings between
the English and Arabic Wikipedia categories.
More details about data generated by each and
every extractor will be given in Section 4.
AIDA Schema Building
In this stage we invoke the original AIDA schema
builder without any language information. How-
ever, we additionally add the Freebase knowledge
base to AIDA and map Freebase entities to YAGO3
entities. Freebase is used here solely to harness its
coverage of multi-lingual names of different enti-
ties. It is worth noting that Freebase is used merely
to enrich YAGO3, but the set of entities are gath-
ered from YAGO. In other words, if there is an
entity in Freebase without a YAGO counter part, it
gets discarded.
Translation
Although it is generally viable to use machine trans-
lation or ?off the shelf? English-Arabic dictionaries
to translate the context of entities. However, we
confine ourselves to the dictionaries extracted from
Wikipedia that maps entities as well as categories
190
from English to Arabic. This is done in order to
achieve a high precision derived from the manual
labor inherent in interwiki links and assigned cate-
gories.
Filtration
This is a final cleaning stage. Despite translating
the context of entities using the Wikipedia-based
dictionaries as comprehensive as possible, a con-
siderable amount of context information remains
in English (e.g. those English categories that do
not have an Arabic counterpart). To this end, any
remaining leftovers in English are being discarded.
4 Implementation
This section explains the implementation of the
pipeline described in Section 3. We first high-
light the differences between YAGO2 and YAGO3,
which justify the switch of the underlying knowl-
edge base. Then, we present the techniques we
have developed in order to build the dictionary be-
tween mentions and candidate entities. After that,
we explain the context enrichment for Arabic enti-
ties by exploiting cross-lingual evidences. Finally,
we briefly explain the entity-entity relatedness mea-
sure applied for disambiguation. In the following
table (cf. Table 1 for details) we summarize the
terminology used in the following section.
4.1 Entity Repository
YAGO3 has been specifically designed as a multi-
lingual knowledge base. Hence, standard YAGO3
extractors take as an input a set of Wikipedia dumps
from different languages, and produce a unified
repository of named entities across all languages.
This is done by considering inter-wiki links. If an
entity in language l ? L ? {en} has an English
counter part, the English one is kept instead of
that in language l, otherwise, the original entity
is kept. For example, in our repository, the entity
used to represent Egypt is ?Egypt? coming from
the English Wikipedia instead of ?ar/Q???? coming
from the Arabic Wikpedia. However, the entity that
refers to the western part of Cairo is identified as
?ar/

?Q?A

?? @ H
.
Q
	
?? because it has no counter-part in
the English Wikipedia. Formally, the set of entities
in YAGO3 are defined as follows:
E = E
en
? E
ar
After the extraction is done, YAGO3 generates
an entity dictionary for each and every language.
This dictionary translates any language specific
entity into the one that is used in YAGO3 (whether
the original one, or the English counter part).
Based on the the previous example, the following
entries are created in the dictionary:
ar/Q??? ? Egypt
ar/

?Q?A

?? @ H
.
Q
	
? ? ar/

?Q?A

?? @ H
.
Q
	
?
Such a dictionary is essential for all further pro-
cessing we do over YAGO3 to enrich the Arabic
knowledge base using the English one. It is worth
noting here, that this dictionary is completely au-
tomatically harvested from the inter-wiki links in
Wikipedia, and hence no automated machine trans-
lation and/or transliteration are invoked (e.g. for
Person Names, Organization Names, etc.). While
this may harm the coverage of our linkage, it guar-
antees the precision of our mapping at the same
time. This is thanks to the high quality of inter-
wiki between named-entities in Wikipedia.
4.2 Name-Entity Dictionary
The dictionary in the context of NED refers to the
relation that connects strings to canonical entities.
In other words, given a mention string, the dictio-
nary provides a list of potential canonical entities
this string may refer to. In our original implemen-
tation of AIDA, this dictionary was compiled from
four sources extracted from Wikipedia (titles, dis-
ambiguation pages, redirects, and anchor texts).
We used the same sources after adapting them to
the Arabic domain, and added to them entries com-
ing from Freebase. In the following, we briefly
summarize the main ingredients used to populate
our dictionary:
? Titles: The most natural possible name of a
canonical entity is the title of its correspond-
ing page in Wikipedia. This is different from
the entity ID itself. For example, in our exam-
ple for the entity ?Egypt? that gets its id from
the English Wikipeida, we consider the title
?Q???? coming from the Arabic Wikipedia.
? Disambiguation Pages: These pages
are called in the Arabic Wikipedia
?iJ


	
??

J? @

HAj
	
???. They are dedicated
pages to list the different possible meanings
of a specific name. We harness all the links
in a disambiguation page and add them as
191
l A language in Wikipedia
L Set of all languages in Wikipedia
e
en
An entity originated from the English WIkipedia
e
ar
An entity originated from the Arabic WIkipedia
e An entity in the final collection of YAGO3
E Set of the corresponding entities
Cat
en
(e) Set of Categories of an entity e in the English Wikipedia
Cat
ar
(e) Set of Categories of an entity e in the Arabic Wikipedia
Inlink
en
(e) Set of Incoming Links to an entity e in the English Wikipedia
Inlink
ar
(e) Set of Incoming Links to an entity e in the Arabic Wikipedia
Trans(S) Translation of each element in S from English to Arabic using the appropriate dictionaries
en?ar
Table 1: Terminology
potential entities for that name. To this end,
we extract our content solely from the Arabic
Wikipedia. For instance, the phrase ?

?
	
J K


Y?
YK


@
	
P? has a disambiguation page that lists all
the cities that all called Zayed including the
ones in Egypt, Bahrain and United Arab Emi-
rates.
? Redirects: ?

HCK


?m
