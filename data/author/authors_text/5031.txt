Chinese Word Segmentation with Multiple Postprocessors
in HIT-IRLab
Huipeng Zhang     Ting Liu    Jinshan Ma    Xiantao Liao 
Information Retrieval Lab, Harbin Institute of Technology, Harbin, 150001 CHINA 
{zhp,tliu,mjs,taozi}@ir.hit.edu.cn
Abstract
This paper presents the results of the
system IRLAS1 from HIT-IRLab in the
Second International Chinese Word
Segmentation Bakeoff. IRLAS consists
of several basic components and multi-
ple postprocessors. The basic compo-
nents include basic segmentation,
factoid recognition, and named entity
recognition. These components main-
tain a segment graph together. The
postprocessors include merging of ad-
joining words, morphologically derived
word recognition, and new word identi-
fication. These postprocessors do some
modifications on the best word se-
quence which is selected from the seg-
ment graph. Our system participated in
the open and closed tracks of PK cor-
pus and ranked #4 and #3 respectively.
Our scores were very close to the high-
est level. It proves that our system has
reached the current state of the art.
1 Introduction
IRLAS participated in both the open and closed
tracks of PK corpus. The sections below descript
in detail the components of our system and the
tracks we participated in.
The structure of this paper is as follows. Sec-
tion 2 presents the system description. Section 3
describes in detail the tracks we participated in.
Section 4 gives some experiments and discus-
sions. Section 5 enumerates some external fac-
1 IRLAS is the abbreviation for ?Information Retrieval
Lab Lexical Analysis System?.
tors that affect our performance. Section 6 gives
our conclusion.
2 System Description 
2.1 Basic Segmentation
When a line is input into the system, it is first
split into sentences separated by period. The
reason to split a line into sentences is that in
named entity recognition, the processing of sev-
eral shorter sentences can reach a higher named
entity recall rate than that of a long sentence.
The reason to split the line only by period is for
the simplicity for programming, and the sen-
tences separated by period are short enough to
process.
Then every sentence is segmented into single
atoms. For example, a sentence like ?HIT-IRLab
?????? SIGHAN ?????? will be
segmented as ?HIT-IRLab/?/?/?/?/?/?
/SIGHAN/?/?/?/?/??.
After atom segmentation, a segment graph is
created. The number of nodes in the graph is the
number of atoms plus 1, and every atom corre-
sponds to an arc in the graph.
Then all the words in the dictionary2 that ap-
pear in the sentence will be added to the seg-
ment graph. The graph contains various
information such as the bigram possibility of
every word. Figure 1 shows the segment graph
of the above sentence after basic segmentation.
2.2 Factoid Recognition 
After basic segmentation, a graph with all the
atoms and all the words in the dictionary is set
up. On this basis, we find out all the factoids
2 The dictionary is trained with training corpus.
172
Figure 1: The segment graph 
Note: the probability of each word is not shown in the graph.
such as numbers, times and e-mails with a set of
rules. Then, we also add all these factoids to the
segment graph.
2.3 Named Entity Recognition 
Then we will recognize the named entities such
as persons and locations. First, we select N3 best
paths from the segment graph with Dijkstra al-
gorithm. Then for every path of the N+1 paths4
(N best paths and the atom path), we perform a
process of Roles Tagging with HMM model
(Zhang et al 2003). The process of it is much
like that of Part of Speech Tagging. Then with
the best role sequence of every path, we can find
out all the named entities and add them to the
segment graph as usual. Take the sentence ??
????????? for example. After basic
segmentation and factoid recognition, the N+1
paths are as follows:
?/?/?/?/?/?/??/?
?/?/?/?/?/?/?/?/?
Then for each path, the process of Roles
Tagging is performed and the following role
sequences are generated:
X/S/W/N/O/O/O/O5
X/S/W/N/O/O/O/O/O
From these role sequences, we can find out
that ?XSW? is a 3-character Chinese name. So
the word ????? is recognized as a person
name and be added to the segment graph. 
3 N is a constant which is 8 in our system.
4 It may be smaller than N+1 if the sentence is short
enough; exactly, N+1 is the upper bound of the path num-
ber.
5 X, S, W, N and O are all roles for person name recogni-
tion, X is surname, S is the first character of given name,
W is the second character of given name, N is the word
following a person name, and O is other remote context.
We defined 17 roles for person name recognition and 10
roles for location name recognition.
2.4 Merging of Adjoining Words
After the steps above, the segment graph is
completed and a best word sequence is gener-
ated with Dijkstra algorithm. This merging op-
eration and all the following operations are done
to the best word sequence.
There are many inconsistencies in the PK
corpus. For example, in PK training corpus, the
word ???? sometimes is considered as one
word, but sometimes is considered as two sepa-
rate words as ?? ??. The inconsistencies
lower the system?s performance to some extent.
To solve this problem, we first train from the
training corpus the probability of a word to be
one word and the probability to be two separate
words. Then we perform a process of merging:
if two adjoining words in the best word se-
quence are more likely to be one word, then we
just merge them together. 
2.5 Morphologically Derived Word Recog-
nition
To deal with the words with the postfix like
???, ???, ??? and so on, we perform the
process to merge the preceding word and the
postfix into one word. We train a list of post-
fixes from the training corpus. Then we scan the
best word sequence, if there is a single character
word that appears in the postfix list, we merge
the preceding word and this postfix into one
word. For example, a best word sequence like
??? ? ? ? ??? will be converted to
???? ? ? ??? after this operation.
2.6 New Word Identification
As for the words that are not in the dictionary
and cannot be identified with the steps above,
we perform a process of New Word Identifica-
tion (NWI). We train from the training corpus
the probability of a word to be independent and
the probability to be a special part of another
word. In our system, we only consider the words
that have one or two characters. Then we scan
173
the best word sequence, if the product of the
probabilities of two adjoining words exceed a
threshold, then we merge the two words into one
word.
Take the word ???? for example. It is
segmented as ?? ?? after all the above steps
since this word is not in the dictionary. We find
that the word ??? has a probability of 0.83 to
be the first character of a two character word,
and the word ??? has a probability of 0.94 to be
the last character of a two character word. The
product of them is 0.78 which is larger than 0.65,
which is the threshold in our system. So the
word ???? is recognized as a single word.
3 Tracks
3.1 Closed Track
As for the PK closed track, we first extract all
the common words and tokens from the training
corpus and set up a dictionary of 55,335 entries.
Then we extract every kind of named entity re-
spectively. With these named entities, we train
parameters for Roles Tagging. We also train all
the other parameters mentioned in Section 2
with the training corpus. 
3.2 Open Track
The PK open track is similar to closed one. In
open track, we use all the 6 months corpus of
People?s Daily and set up a dictionary of
107,749 entries. Additionally, we find 101 new
words from the Web and add them to the dic-
tionary. We train the parameters of named entity
recognition with a person list and a location list
in our laboratory. The training of other parame-
ters is the same with closed track.
4 Experiments and Discussions 
We do several experiments on PK test corpus to
see the contribution of each postprocessor. We
cut off one postprocessor at a time from the
complete system and record its F-score. The
evaluation results are shown in Table 1. In the
table, MDW represents Morphologically De-
rived Word Recognition, and NWI represents
New Word Identification.
PK open PK closed 
Complete
System 96.5% 94.9%
Without
Merging 96.3% 94.7%
Without
MDW 96.6% 94.4%
Without
NWI 96.5% 94.9%
Table 1: Evaluation results of IRLAS with each
postprocessor cut off at a time
From Table 1, we can come to some interest-
ing facts:
! The Merging of Adjoining Words has good
effect on both open and closed tracks. So
we can conclude that this module can solve
the problem of inconsistent training corpus
to some extent.
! Morphologically Derived Word Recogni-
tion does some harm in open track, but it
has a very good effect in closed track.
Maybe it is because that in open track, we
can make a comparatively larger dictionary
since we can use any resource we have. So
most MDWs6 are in the dictionary and the
MDWs that are not in the dictionary are
mostly difficult to recognize. So it does
more harm than good in many cases. But in
closed track, we have a small dictionary
and many common MDWs are not in the
dictionary. So it does much more good in
closed track.
! New Word Identification is minimal in both
open and closed tracks. Maybe it is because
that the above steps have recognized the
most OOV words and it is hard to recognize 
any more new words.
5 External Factors That Affect Our
Performance
The difference on the definition of words is the
main factor that affects our performance. In
many cases such as ??????, ????, ???
?? are all considered as one word in our system
but not so in the PK gold standard corpus. An-
other factor is the inconsistencies in training
corpus, although this problem has been solved to
some extent with the module of merging. But
6 It refers to Morphologically Derived Words.
174
because the inconsistencies also exist in test cor-
pus and there are some instances that a word is
more likely to be a single word in training cor-
pus but more likely to be separated into two
words in test corpus. For example, the word ??
?? is more likely to be a single word in training
corpus but is more likely to be separated into
two words in test corpus. There is another factor
that affects MDW, many postfixes in our system
are not considered as postfixes in PK gold stan-
dard corpus. For example, the word ????? is
recognized as a MDW in our system since ???
is a postfix, however, it is segmented into two
separate words as ??? ?? in PK gold stan-
dard corpus. 
6 Conclusion
Through the second SIGHAN bakeoff, we find
the segmentation model and the algorithm in our
system is effective and the multiple postproces-
sors we use can also enhance the performance of
our system. At the same time, we also find some
problems of us. It also has potential for us to
improve our system. Take MDW for example,
we can make use of more features such as the
POS and the length of the preceding word to
enhance the recall and precision rate. The bake-
off points out the direction for us to improve our
system.
References
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai, Chinese Named Entity Recognition Us-
ing Role Model. International Journal of Computa-
tional Linguistics and Chinese Language
Processing, 2003, Vol.8(2)
Andi Wu, Zixin Jiang, 2000. Statistically-Enhanced
New Word Identification in a Rule-Based Chinese
System. In Proceedings of the Second Chinese
Language Processing Workshop, pp. 46-51,
HKUST, Hong Kong.
Huaping Zhang, Hongkui Yu, Deyi Xiong, Qun Liu,
HHMM-based Chinese Lexical Analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, July 11-12,
2003, Sapporo, Japan.
Aitao Chen, Chinese Word Segmentation Using
Minimal Linguistics Knowledge. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing, July 11-12, 2003, Sapporo, Ja-
pan.
Andi Wu, Chinese Word Segmentation in MSR-NLP.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, July 11-12,
2003, Sapporo, Japan.
175
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 211?215, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing Based on Dynamic Local Optimization
Ting Liu Jinshan Ma Huijia Zhu Sheng Li
Information Retrieval Lab
Harbin Institute of Technology
Harbin, 150001, China
{tliu,mjs,hjzhu,ls}@ir.hit.edu.cn
Abstract
This paper presents a deterministic pars-
ing algorithm for projective dependency
grammar. In a bottom-up way the al-
gorithm finds the local optimum dynam-
ically. A constraint procedure is made
to use more structure information. The
algorithm parses sentences in linear time
and labeling is integrated with the parsing.
This parser achieves 63.29% labeled at-
tachment score on the average in CoNLL-
X Shared Task.
1 Introduction
Recently, dependency grammar has gained renewed
attention in the parsing community. Good results
have been achieved in some dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al, 2004).
With the availability of many dependency treebanks
(van der Beek et al, 2002; Hajic? et al, 2004;
Bo?hmova? et al, 2003; Kromann, 2003; Dz?eroski et
al., 2006) and more other treebanks which can be
converted to dependency annotation (Brants et al,
2002; Nilsson et al, 2005; Chen et al, 2003; Kawata
and Bartels, 2000), multi-lingual dependency pars-
ing is proposed in CoNLL shared task (Buchholz et
al., 2006).
Many previous works focus on unlabeled parsing,
in which exhaustive methods are often used (Eis-
ner, 1996). Their global searching performs well
in the unlabeled dependency parsing. But with the
increase of parameters, efficiency has to be consid-
ered in labeled dependency parsing. Thus determin-
istic parsing was proposed as a robust and efficient
method in recent years. Such method breaks the
construction of dependency tree into a series of ac-
tions. A classifier is often used to choose the most
probable action to assemble the dependency tree.
(Yamada and Matsumoto, 2003) defined three ac-
tions and used a SVM classifier to choose one of
them in a bottom-up way. The algorithm in (Nivre
et al, 2004) is a blend of bottom-up and top-down
processing. Its classifier is trained by memory-based
learning.
Deterministic parsing derives an analysis without
redundancy or backtracking, and linear time can be
achieved. But when searching the local optimum in
the order of left-to-right, some wrong reduce may
prevent next analysis with more possibility. (Jin et
al., 2005) used a two-phase shift-reduce to decrease
such errors, and improved the accuracy of long dis-
tance dependencies.
In this paper a deterministic parsing based on dy-
namic local optimization is proposed. According to
the probabilities of dependency arcs, the algorithm
dynamically finds the one with the highest probabil-
ities instead of dealing with the sentence in order.
A procedure of constraint which can integrate more
structure information is made to check the rational-
ity of the reduce. Finally our results and error anal-
ysis are presented.
2 Dependency Probabilities
An example of Chinese dependency tree is showed
in Figure1. The tree can be represented as a directed
graph with nodes representing word tokens and arcs
211
Figure 1: A Chinese dependency tree
representing dependency relations. The assumption
that the arcs are independent on each other often is
made so that parsing can be handled easily. On the
other side the independence assumption will result
in the loss of information because dependencies are
interrelated on each other actually. Therefore, two
kinds of probabilities are used in our parser. One is
arc probabilities which are the possibility that two
nodes form an arc, and the other is structure proba-
bilities which are used to describe some specific syn-
tactic structures.
2.1 Arc Probabilities
A dependency arc A
i
can be expressed as a 4-tuple
A
i
= <Node
i
, Node
j
, D, R>. Node
i
and Node
j
are
nodes that constitute the directed arc. D is the direc-
tion of the arc, which can be left or right. R is rela-
tion type labeled on the arc. Under the independence
assumption that an arc depends on its two nodes we
can calculate arc probability given two nodes. In our
paper the arc probabilities are calculated as follows:
P
1
= P(R,D|CTag
i
, CTag
j
, Dist)
P
2
= P(R,D|FTag
i
, FTag
j
)
P
3
= P(R,D|CTag
i
, Word
j
)
P
4
= P(R,D|Word
i
, CTag
j
)
P
5
= P(R,D|Word
i
,CTag
i
, Word
j
,CTag
j
)
P
6
= P(R,D|CTag
i?1
, CTag
i
, CTag
j
, CTag
j+1
)
Where CTag is coarse-grained part of speech tag
and FTag is fine-grained tag. As to Word we choose
its lemma if it exists. Dist is the distance between
Node
i
and Node
j
. It is divided into four parts:
Dist = 1 if j-i = 1
Dist = 2 if j-i = 2
Dist = 3 if 3?j-i?6
Dist = 4 if j-i > 6
All the probabilities are obtained by maximum
likelihood estimation from the training data. Then
interpolation smoothing is made to get the final arc
probabilities.
2.2 Structure Probabilities
Structure information plays the critical role in syn-
tactic analysis. Nevertheless the flexibility of syn-
tactic structures and data sparseness pose obstacles
to us. Especially some structures are related to spe-
cific language and cannot be employed in multi-
lingual parsing. We have to find those language-
independent features.
In valency theory ?valence? represents the num-
ber of arguments that a verb is able to govern. In
this paper we extend the range of verbs and argu-
ments to all the words. We call the new ?valence?
Governing Degree (GD), which means the ability of
one node governing other nodes. In Figure1, the GD
of node ???? is 2 and the GDs of two other nodes
are 0. The governing degree of nodes in dependency
tree often shows directionality. For example, Chi-
nese token ??? always governs one left node. Fur-
thermore, we subdivide the GD into Left Governing
Degree (LGD) and Right Governing Degree (RGD),
which are the ability of words governing their left
children or right children. In Figure 1 the LGD and
RGD of verb ???? are both 1.
In the paper we use the probabilities of GD
over the fine-grained tags. The probabilities of
P(LDG|FTag) and P(RGD|FTag) are calculated
from training data. Then we only reserve the FTags
with large probability because their GDs are stable
and helpful to syntactic analysis. Other FTags with
small probabilities are unstable in GDs and cannot
provide efficient information for syntactic analysis.
If their probabilities are less than 0.65 they will be
ignored in our dependency parsing.
3 Dynamic local optimization
Many previous methods are based on history-based
models. Despite many obvious advantages, these
methods can be awkward to encode some constrains
within their framework (Collins, 2000). Classifiers
are good at encoding more features in the determin-
istic parsing (Yamada and Matsumoto, 2003; Nivre
et al, 2004). However, such algorithm often make
more probable dependencies be prevented by pre-
ceding errors. An example is showed in Figure 2.
Arc a is a frequent dependency and b is an arc with
more probability. Arc b will be prevented by a if the
reduce is carried out in order.
212
Figure 2: A common error in deterministic parsing
3.1 Our algorithm
Our deterministic parsing is based on dynamic local
optimization. The algorithm calculates the arc prob-
abilities of two continuous nodes, and then reduces
the most probable arc. The construction of depen-
dency tree includes four actions: Check, Reduce,
Delete, and Insert. Before a node is reduced, the
Check procedure is made to validate its correctness.
Only if the arc passes the Check procedure it can
be reduced. Otherwise the Reduce will be delayed.
Delete and Insert are then carried out to adjust the
changed arcs. The complete algorithm is depicted
as follows:
Input Sentence: S = (w
1
, w
2
,l, w
n
)
Initialize:
for i = 1 to n
R
i
= GetArcProb(w
i
,w
i+1
);
Push(R
i
) onto Stack;
Sort(Stack);
Start:
i = 0;
While Stack.empty = false
R = Stack.top+i;
if Check(R) = true
Reduce(R);
Delete(R?);
Insert(R?);
i = 0;
else
i++;
The algorithm has following advantages:
? Projectivity can be guaranteed. The node is
only reduced with its neighboring node. If a
node is reduced as a leaf it will be removed
from the sentence and doesn?t take part in next
Reduce. So no cross arc will occur.
? After n-1 pass a projective dependency tree is
complete. Algorithm is finished in linear time.
? The algorithm always reduces the node with the
Figure 3: Adjustment
highest probability if it passes the Check. No
any limitation on order thus the spread of errors
can be mitigated effectively.
? Check is an open process. Various constrains
can be encoded in this process. Structural con-
strains, partial parsed information or language-
dependent knowledge can be added.
Adjustment is illustrated in Figure 3, where ??
?? is reduced and arc R? is deleted. Then the algo-
rithm computes the arc probability of R? and inserts
it to the Stack.
3.2 Checking
The information in parsing falls into two kinds:
static and dynamic. The arc probabilities in 2.1 de-
scribe the static information which is not changed in
parsing. They are obtained from the training data in
advance. The structure probabilities in 2.2 describe
the dynamic information which varies in the process
of parsing. The use of dynamic information often
depends on what current dependency tree is.
Besides the governing degree, Check procedure
also uses another dynamic information?Sequential
Dependency. Whether current arc can be reduced is
relating to previous arc. In Figure 3 the reduce of the
arc R depends on the arc R?. If R? has been delayed
or its probability is little less than that of R, arc R
will be delayed.
If the arc doesn?t pass the Check it will be de-
layed. The delayed time ranges from 1 to Length
which is the length of sentence. If the arc is delayed
Length times it will be blocked. The Reduce will be
delayed in the following cases:
?
?
GD(Node
i
) > 0 and its probability is P. If
GD(Node
i
) = 0 and Node
i
is made as child
in the Reduce, the Node
i
will be delayed
Length*P times.
?
?
GD(Node
i
) ? m (m > 0) and its probability
is P. If GD(Node
i
) = m and Node
i
is made as
parent in the Reduce, the Node
i
will be delayed
Length*P times.
213
Figure 4: Token score with size of training data
Figure 5: Token score with sentence length
? P(R?) > ?P(R), the current arc R will be de-
layed Length*(P(R?)/P(R)) times. R? is the pre-
ceding arc and ? = 0.60.
? If arc R? is blocking, the arc R will be delayed.
?
GD is empirical value and GD is current value.
4 Experiments and analysis
Our parsing results and average results are listed
in the Table 1. It can be seen that the attachment
scores vary greatly with different languages. A gen-
eral analysis and a specific analysis are made respec-
tively in this section.
4.1 General analysis
We try to find the properties that make the differ-
ence to parsing results in multi-lingual parsing. The
properties of all the training data can be found in
(Buchholz et al, 2006). Intuitively the size of train-
ing data and average length of per sentence would
be influential on dependency parsing. The relation
of these properties and scores are showed in the Fig-
ure 4 and 5.
From the charts we cannot assuredly find the
properties that are proportional to score. Whether
Czech language with the largest size of training data
or Chinese with the shortest sentence length, don?t
achieve the best results. It seems that no any factor is
determining to parsing results but all the properties
exert influence on the dependency parsing together.
Another factor that maybe explain the difference
of scores in multi-lingual parsing is the characteris-
tics of language. For example, the number of tokens
with HEAD=0 in a sentence is not one for some lan-
guages. Table 1 shows the range of governing de-
gree of head. This statistics is somewhat different
with that from organizers because we don?t distin-
guish the scoring tokens and non-scoring tokens.
Another characteristic is the directionality of de-
pendency relations. As Table 1 showed, many rela-
tions in treebanks are bi-directional, which increases
the number of the relation actually. Furthermore, the
flexibility of some grammatical structures poses dif-
ficulties to language model. For instance, subject
can appear in both sides of the predicates in some
treebanks which tends to cause the confusion with
the object (Kromann, 2003; Afonso et al, 2002;
Civit Torruella and Mart?? Anton??n, 2002; Oflazer et
al., 2003; Atalay et al, 2003).
As to our parsing results, which are lower than all
the average results except for Danish. That can be
explained from the following aspects:
(1) Our parser uses a projective parsing algorithm
and cannot deal with the non-projective tokens,
which exist in all the languages except for Chinese.
(2) The information provided by training data is not
fully employed. Only POS and lemma are used. The
morphological and syntactic features may be helpful
to parsing.
(3) We haven?t explored syntactic structures in depth
for multi-lingual parsing and more structural fea-
tures need to be used in the Check procedure.
4.2 Specific analysis
Specifically we make error analysis to Chinese and
Turkish. In Chinese result we found many errors
occurred near the auxiliary word ???(DE). We call
the noun phrases with ??? DE Structure. The word
??? appears 355 times in the all 4970 dependencies
of the test data. In Table 2 the second row shows the
frequencies of ?DE? as the parent of dependencies.
The third row shows the frequencies while it is as
child. Its error rate is 33.1% and 43.4% in our re-
sults respectively. Furthermore, each head error will
result in more than one errors, so the errors from DE
Structures are nearly 9% in our results.
214
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu
our 50.74 75.29 58.52 77.70 59.36 68.11 70.84 71.13 57.21 65.08 63.83 41.72
ave 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95
NH 17 1 28 4 9 1 14 1 11 1 1 5
BD 27/24 78/55 82/72 54/24 26/17 46/40 7/2 55/40 26/23 21/19 64/54 26/23
Table 1: The second and third rows are our scores and average scores. The fourth row lists the maximal
number of tokens with HEAD=0 in a sentence. The last row lists the number of relations/the number of
bi-directional relations of them (Our statistics are slightly different from that of organizers).
gold system error headerr
parent 320 354 106 106
child 355 355 154 74
Table 2: Chinese DE Structure Errors
The high error rate is due to the flexibility of DE
Structure. The children of DE can be nouns and
verbs, thus the ambiguities will occur. For example,
the sequence ?V N1 DE N2? is a common ambigu-
ious structure in Chinese. It needs to be solved with
semantic knowledge to some extent. The errors of
DE being child are mostly from noun compounds.
For example, the string ????????? results
in the error: ?DE? as the child of ????. It will be
better that noun compounds are processed specially.
Our results and average results achieve the low-
est score on Turkish. We try to find some reasons
through the following analysis. Turkish is a typi-
cal head-final language and 81.1% of dependencies
are right-headed. The monotone of directionality in-
creases the difficulties of identification. Another dif-
ficulty is the diversity of the same pair. Taking noun
and pronoun as example, which only achieve the ac-
curacy of 25% and 28% in our results, there are 14
relations in the noun-verb pairs and 11 relations in
the pronoun-verb pairs. Table 3 illustrates the distri-
bution of some common relations in the test data.
The similarity of these dependencies makes our
parser only recognize 23.3% noun-verb structures
and 21.8% pronoun-verb structures. The syntactic
or semantic knowledge maybe helpful to distinguish
these similar structures.
5 Conclusion
This paper has applied a deterministic algorithm
based on dynamic local optimization to multi-
total obj sub mod D.A L.A
Noun-V 1300 494 319 156 102 78
Pron-V 215 91 60 9 37 3
Table 3: The distribution of some common relations
lingual dependency parsing. Through the error
analysis for some languages, we think that the dif-
ference between languages is a main obstacle posed
on multi-lingual dependency parsing. Adopting
different learners according to the type of languages
may be helpful to multi-lingual dependency parsing.
Acknowledgement This work was supported
by the National Natural Science Foundation of
China under Grant No. 60435020?60575042 and
60503072.
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
M.X. Jin, M.Y. Kim, and J.H. Lee. 2005. Two-phase
shift-reduce deterministic dependency parser of chi-
nese. In Proc. of IJCNLP: Companion Volume includ-
ing Posters/Demos and tutorial abstracts.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of the Eighth Conf. on
Computational Natural Language Learning (CoNLL),
pages 49?56.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of the 8th Intern. Workshop on Parsing Technologies
(IWPT).
215
