Maytag: A multi-staged approach to identifying
complex events in textual data
Conrad Chang, Lisa Ferro, John Gibson, Janet Hitzeman, Suzi Lubar, Justin Palmer,
Sean Munson, Marc Vilain, and Benjamin Wellner
The MITRE Corporation
202 Burlington Rd.
Bedford, MA 01730 USA
contact: mbv@mitre.org (Vilain)
Abstract
We present a novel application of NLP
and text mining to the analysis of finan-
cial documents. In particular, we de-
scribe an implemented prototype, May-
tag, which combines information extrac-
tion and subject classification tools in an
interactive exploratory framework. We
present experimental results on their per-
formance, as tailored to the financial do-
main, and some forward-looking exten-
sions to the approach that enables users
to specify classifications on the fly.
1 Introduction
Our goal is to support the discovery of complex
events in text. By complex events, we mean
events that might be structured out of multiple
occurrences of other events, or that might occur
over a span of time. In financial analysis, the
domain that concerns us here, an example of
what we mean is the problem of understanding
corporate acquisition practices. To gauge a
company?s modus operandi in acquiring other
companies, it isn?t enough to know just that an
acquisition occurred, but it may also be impor-
tant to understand the degree to which it was
debt-leveraged, or whether it was performed
through reciprocal stock exchanges.
In other words, complex events are often
composed of multiple facets beyond the basic
event itself. One of our concerns is therefore to
enable end users to access complex events
through a combination of their possible facets.
Another key characteristic of rich domains
like financial analysis, is that facts and events are
subject to interpretation in context. To a finan-
cial analyst, it makes a difference whether a
multi-million-dollar loss occurs in the context of
recurring operations (a potentially chronic prob-
lem), or in the context of a one-time event, such
as a merger or layoff. A second concern is thus
to enable end users to interpret facts and events
through automated context assessment.
The route we have taken towards this end is to
model the domain of corporate finance through
an interactive suite of language processing tools.
Maytag, our prototype, makes the following
novel contribution. Rather than trying to model
complex events monolithically, we provide a
range of multi-purpose information extraction
and text classification methods, and allow the
end user to combine these interactively. Think
of it as Boolean queries where the query terms
are not keywords but extracted facts, events, en-
tities, and contextual text classifications.
2 The Maytag prototype
Figure 1, below, shows the Maytag prototype
in action. In this instance, the user is browsing a
particular document in the collection, the 2003
securities filings for 3M Corporation. The user
has imposed a context of interpretation by select-
ing the ?Legal matters? subject code, which
causes the browser to only retrieve those portions
of the document that were statistically identified
as pertaining to law suits. The user has also se-
lected retrieval based on extracted facts, in this
case monetary expenses greater than $10 million.
This in turn causes the browser to further restrict
retrieval to those portions of the document that
contain the appropriate linguistic expressions,
e.g., ?$73 million pre-tax charge.?
As the figure shows, the granularity of these
operations in our browser is that of the para-
graph, which strikes a reasonable compromise
between providing enough context to interpret
retrieval results, but not too much. It is also ef-
131
fective at enabling combination of query terms.
Whereas the original document contains 5161
paragraphs, the number of these that were tagged
with the ?Legal matters? code is 27, or .5 percent
of the overall document. Likewise, the query for
expenses greater than $10 million restricts the
return set to 26 paragraphs (.5 percent). The
conjunction of both queries yields a common
intersection of only 4 paragraphs, thus precisely
targeting .07 percent of the overall document.
Under the hood, Maytag consists of both an
on-line component and an off-line one. The on-
line part is a web-based GUI that is connected to
a relational database via CGI scripts (html,
JavaScript, and Python). The off-line part of the
system hosts the bulk of the linguistic and statis-
tical processing that creates document meta-data:
name tagging, relationship extraction, subject
identification, and the like. These processes are
applied to documents entering the text collection,
and the results are stored as meta-data tables.
The tables link the results of the off-line process-
ing to the paragraphs in which they were found,
thereby supporting the kind of extraction- and
classification-based retrieval shown in Figure 1.
3 Extraction in Maytag
As is common practice, Maytag approaches
extraction in stages. We begin with atomic
named entities, and then detect structured
entities, relationships, and events. To do so, we
rely on both rule-based and statistical means.
3.1 Named entities
In Maytag, we currently extract named entities
with a tried-but-true rule-based tagger based on
the legacy Alembic system (Vilain, 1999). Al-
though we?ve also developed more modern sta-
tistical methods (Burger et al 1999, Wellner &
Vilain, 2006), we do not currently have adequate
amounts of hand-marked financial data to train
these systems. We therefore found it more con-
venient to adapt the Alembic name tagger by
manual hill climbing. Because this tagger was
originally designed for a similar newswire task,
we were able to make the port using relatively
small amounts of training data. We relied on two
100+ page-long Securities filings (singly anno-
tated), one for training, and the other for test, on
which we achieve an accuracy of F=94.
We found several characteristics of our finan-
cial data to be especially challenging. The first is
the widespread presence of company name look-
alikes, by which we mean phrases like ?Health
Care Markets? or ?Business Services? that may
look like company names, but in fact denote
business segments or the like. To circumvent
this, we had to explicitly model non-names, in
effect creating a business segment tagger that
captures company name look-alikes and prevents
them from being tagged as companies.
Another challenging characteristic of these fi-
nancial reports is their length, commonly reach-
ing hundreds of pages. This poses a quandary
Figure 1: The Maytag interface
132
for the way we handle discourse effects. As with
most name taggers, we keep a ?found names? list
to compensate for the fact that a name may not
be clearly identified throughout the entire span of
the input text. This list allows the tagger to
propagate a name from clear identifying contexts
to non-identified occurrences elsewhere in the
discourse. In newswire, this strategy boosts re-
call at very little cost to precision, but the sheer
length of financial reports creates a dispropor-
tionate opportunity for found name lists to intro-
duce precision errors, and then propagate them.
3.2 Structured entities, relations, and events
Another way in which financial writing differs
from general news stories is the prevalence of
what we?ve called structured entities, i.e., name-
like entities that have key structural attributes.
The most common of these relate to money. In
financial writing, one doesn?t simply talk of
money: one talks of a loss, gain or expense, of
the business purpose associated therewith, and of
the time period in which it is incurred. Consider:
Worldwide expenses for environmental
compliance [were] $163 million in 2003.
To capture such cases as this, we?ve defined a
repertoire of structured entities. Fine-grained
distinctions about money are encoded as color of
money entities, with such attributes as their color
(in this case, an operating expense), time stamp,
and so forth. We also have structured entities for
expressions of stock shares, assets, and debt.
Finally, we?ve included a number of constructs
that are more properly understood as relations
(job title) or events (acquisitions).
3.3 Statistical training
Because we had no existing methods to address
financial events or relations, we took this oppor-
tunity to develop a trainable approach. Recent
work has begun to address relation and event
extraction through trainable means, chiefly SVM
classification (Zelenko et al 2003, Zhou et al
2005). The approach we?ve used here is classi-
fier-based as well, but relies on maximum en-
tropy modeling instead.
Most trainable approaches to event extraction
are entity-anchored: given a pair of relevant enti-
ties (e.g., a pair of companies), the object of the
endeavor is to identify the relation that holds be-
tween them (e.g., acquisition or subsidiary). We
turn this around: starting with the head of the
relation, we try to find the entities that fill its
constituent roles. This is, unavoidably, a
strongly lexicalized approach. To detect an
event such as a merger or acquisition, we start
from indicative head words, e.g., ?acquire,?
?purchases,? ?acquisition,? and the like.
The process proceeds in two stages. Once
we?ve scanned a text to find instances of our in-
dicator heads, we classify the heads to determine
whether their embedding sentence represents a
valid instance of the target concept. In the case
of acquisitions, this filtering stage eliminates
such non-acquisitions as the use of the word
?purchases? in ?the company purchases raw ma-
terials.? If a head passes this filter, we find the
fillers of its constituent roles through a second
classification stage
The role stage uses a shallow parser to chunk
the sentence, and considers the nominal chunks
and named entities as candidate role fillers. For
acquisition events, for example, these roles in-
clude the object of the acquisition, the buying
agent, the bought assets, the date of acquisition,
and so forth (a total of six roles). E.g.
In the fourth quarter of 2000 (WHEN), 3M
[AGENT] also acquired the multi-layer inte-
grated circuit packaging line [ASSETS] of
W.L. Gore and Associates [OBJECT].
The maximum entropy role classifier relies on
a range of feature types: the semantic type of the
phrase (for named entities), the phrase vocabu-
lary, the distance to the target head, and local
context (words and phrases).
Our initial evaluation of this approach has
given us encouraging first results. Based on a
hand-annotated corpus of acquisition events,
we?ve measured filtering performance at F=79,
and role assignment at F=84 for the critical case
of the object role. A more recent round of ex-
periments has produced considerably higher per-
formance, which we will report on later this year.
4 Subject Classification
Financial events with similar descriptions can
mean different things depending on where these
events appear in a document or in what context
they appear. We attempt to extract this important
contextual information using text classification
methods. We also use text classification methods
to help users to more quickly focus on an area
where interesting transactions exist in an interac-
tive environment. Specifically, we classify each
paragraph in our document collection into one of
several interested financial areas. Examples in-
clude: Accounting Rule Change, Acquisitions
and Mergers, Debt, Derivatives, Legal, etc.
133
4.1 Experiments
In our experiments, we picked 3 corporate an-
nual reports as the training and test document set.
Paragraphs from these 3 documents, which are
from 50 to 150 pages long, were annotated with
the types of financial transactions they are most
related to. Paragraphs that did not fall into a
category of interest were classified as ?other?.
The annotated paragraphs were divided into ran-
dom 4x4 test/training splits for this test. The
?other? category, due to its size, was sub-
sampled to the size of the next-largest category.
As in the work of Nigam et al(2002) or Lodhi
et al(2002), we performed a series of experi-
ments using maximum entropy and support vec-
tor machines. Besides including the words that
appeared in the paragraphs as features, we also
experimented with adding named entity expres-
sions (money, date, location, and organization),
removal of stop words, and stemming. In gen-
eral, each of these variations resulted in little dif-
ference compared with the baseline features con-
sisting of only the words in the paragraphs.
Overall results ranged from F-measures of 70-75
for more frequent categories down to above 30-
40 for categories appearing less frequently.
4.2 Online Learning
We have embedded our text classification
method into an online learning framework that
allows users to select text segments, specify
categories for those segments and subsequently
receive automatically classified paragraphs simi-
lar to those already identified. The highest con-
fidence paragraphs, as determined by the classi-
fier, are presented to the user for verification and
possible re-classification.
Figure 1, at the start of this paper, shows the
way this is implemented in the Maytag interface.
Checkboxes labeled pos and neg are provided
next to each displayed paragraph: by selecting
one or the other of these checkboxes, users indi-
cate whether the paragraph is to be treated as a
positive or a negative example of the category
they are elaborating. In our preliminary studies,
we were able to achieve the peak performance
(the highest F1 score) within the first 20 training
examples using 4 different categories.
5 Discussion and future work
The ability to combine a range of analytic
processing tools, and the ability to explore their
results interactively are the backbone of our ap-
proach. In this paper, we?ve covered the frame-
work of our Maytag prototype, and have looked
under its hood at our extraction and classification
methods, especially as they apply to financial
texts. Much new work is in the offing.
Many experiments are in progress now to as-
sess performance on other text types (financial
news), and to pin down performance on a wider
range of events, relations, and structured entities.
Another question we would like to address is
how best to manage the interaction between clas-
sification and extraction: a mutual feedback
process may well exist here.
We are also concerned with supporting finan-
cial analysis across multiple documents. This
has implications in the area of cross-document
coreference, and is also leading us to investigate
visual ways to define queries that go beyond the
paragraph and span many texts over many years.
Finally, we are hoping to conduct user studies
to validate our fundamental assumption. Indeed,
this work presupposes that interactive application
of multi-purpose classification and extraction
techniques can model complex events as well as
monolithic extraction tools ? laMUC.
Acknowledgements
This research was performed under a MITRE
Corporation sponsored research project.
References
Zhou, G., Su J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction.
Proc. of the 43rd ACL Conf, Ann Arbor, MI.
Nigam, K., Lafferty, J., and McCallum, A. 1999. Us-
ing maximum entropy for text classification. Proc.
of IJCAI ?99 Workshop on Information Filtering.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini,
and N., Watkins, C. 2002. Text classification using
string kernels. Journal of Machine Learning Re-
search, Vol. 2, pp. 419-444.
Vilain, M. and Day, D. 1996. Finite-state Phrase Pars-
ing by Rule Sequences, Proc. of COLING-96.
Vilain, M. 1999. Inferential information extraction.
In Pazienza, M.T. & Basili, R., Information Ex-
traction. Springer Verlag.
Wellner, B., and Vilain, M. (2006) Leveraging ma-
chine readable dictionaries in discriminative se-
quence models. Proc. of LREC 2006 (to appear).
Zelenko D., Aone C. and Richardella. 2003. Kernel
methods for relation extraction. Journal of Ma-
chine Learning Research. pp1083-1106.
134
Proceedings of NAACL HLT 2007, Companion Volume, pages 181?184,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Entity Extraction is a Boring Solved Problem ? or is it? 
Marc Vilain The MITRE Corporation Burlington Rd Bedford MA 01730 USA mbv@mitre.org 
Jennifer Su The MITRE Corporation and Cornell University Ithaca NY 14853 USA jfs29@cornell.edu 
Suzi Lubar The MITRE Corporation Burlington Rd Bedford MA 01730 USA slubar@mitre.org  
Abstract This paper presents empirical results that contradict the prevailing opinion that en-tity extraction is a boring solved problem.  In particular, we consider data sets that resemble familiar MUC/ACE data, and re-port surprisingly poor performance for both commercial and research systems.  We then give an error analysis that sug-gests research challenges for entity ex-traction that are neither boring nor solved. 
1 Background Entity extraction or named entity recognition, as it is sometimes called, is a known and familiar prob-lem.  Named entity (NE) tagging has been the sub-ject of numerous shared-task evaluations, including the seminal MUC 6, MUC 7 and MET evaluations, the CoNLL shared task, the SIGHAN bake-offs, and the ACE evaluations.  With this track record, and with commercial vendors now selling named-entity tagging for a fee, many naturally consider entity extraction to be an essentially solved problem.  The present paper challenges this view. The main issue, as we see it, is transfer: NE tag-gers developed for a specific corpus tend not to perform well on other data sets.  Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befud-dles taggers built for journalistic text.  Minkov et al(2005) further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest. Because named entity taggers are so strongly based on surface features, it isn?t surprising to ob-
serve poor tagger transfer across texts with signifi-cantly different styles or with unrelated content.  In this paper, we report on the more surprising result that transfer issues arise even for texts with closely aligned content or closely aligned styles. In particular, we consider a range of primarily business-related texts that are, on the face of it, close in style and/or substance to the journalistic stories in existing NE data sets, MUC 6 in particular.  We thus would have expected these texts to sup-port good transfer performance from taggers con-figured to the MUC task.  Instead, we found the same kinds of performance drops as Kosseim and Poibeau had noted for informal texts.  Our aim here is to shed light on the how and why of this. 2 Scope of the present study We begin with a disclaimer.  Our goal is not so much to present new technical solutions to NE rec-ognition, as to draw attention to those aspects of the problem that remain unsolved.  We cover two main thrusts: (i) a black-box evaluation of several NE taggers (commercial and research systems); and (ii) an error analysis of system performance. 2.1 Evaluation data Our evaluation data set contains three distinct sec-tions.  The largest component consists of publicly-available financial reports filed with the Securities and Exchange Commission (SEC), in particular the 2003 forms 10-K filed by eight Fortune 500 com-panies.  These corporate annual reports share the same subject matter as much business news: sales, profits, acquisitions, business strategies and the like.  They take, however, a more technical slant and are rich in accounting jargon.  They are also longer, ranging in our study from 22 to 54 pages. 
181
Preliminary exploration with our own MUC 6 tagger showed these SEC filings to be particularly hard to tag.  Because their sheer length and techni-cal emphasis seemed implicated in this poor per-formance, we assembled a second corpus of forty Web-hosted business stories from such news pro-viders as MS-NBC, CNN Money, and Motley Fool.  These stories focus on the same eight companies as our 10-K data set, but are shorter and less techni-cal, thus allowing us to isolate length and techni-cality as factors in tagging business texts. The final portion of our test set consists of ten news stories that were selected to closely match the kind of data used in past MUC evaluations.  They were drawn from the New York Times (NYT) and Wall Street Journal (WSJ) on-line editions, and fo-cus on current events, thus providing one more comparable dimension of evaluation.1 2.2 Evaluated systems Five systems participated in our study, represent-ing a range of commercial tools and research pro-totypes.  Two of these are state-of-the-art hand-built systems based on rule/pattern interpreters.  Two are open-source statistical systems, one based on HMMs, and the other on CRFs; both were trained on the MUC 6 data set.  The final system is our own legacy MUC-style tagger, noted as Ariel in Table 1.  Except as noted below, all the systems were run out of the box, with no adaptation to the data. License and privacy concerns prevent us from identifying all the systems; instead this paper re-ports most results anonymously, using the names of Disney heroines as system pseudonyms.  We have, however exposed the identity of our own system out of fairness, as it benefited somewhat from earlier tuning to SEC forms 10-K. 2.3 Evaluation method We attempted to replicate the procedure used in the MUC evaluations, extending it only as required by                                                 1 We will make the non-copyrighted part of our corpus (the 10-Ks) available to other researchers. 
the characteristics of the taggers.  The test data were formatted as in MUC 6, and where SGML markup ran afoul of system I/O characteristics, we remapped the data manually, resolving, e.g., cross-ing tags that may have strayed into the output. To provide scores that could be compared with the MUC evaluations, we created MUC6-compliant answer keys (Sundheim, 1995), and remapped sys-tem output to this standard.  We removed system responses that were considered non-taggable in MUC (e.g., URLs) and conflated fine-grained dis-tinctions not made in MUC (e.g., remapping coun-try tags to location).  Scores were assessed with the venerable MUC scorer, which provides partial credit for system responses that match the key in type but not extent, or vice-versa.  The scorer also provides a full error analysis, separately character-izing each error in a system response. 3 Findings Table 2, overleaf, presents our overall findings, aggregated across the three primary entity types: person, organization, and location (the ENAMEX types in the MUC standard).  We generally did not measure the MUC TIMEX (dates, times) and NUMEX types (moneys, percents) because: (i) neither of the statistical systems generate them; (ii) those systems that do generate them tend to do well; (iii) they are overwhelmingly more frequent in the SEC data than in news, thus skewing results.  For completeness? sake, however, Table 2 does provide all-entity news scores in parentheses for those systems that happened to generate the full set of MUC-6 entities. Turning now to actual performance measure-ments, Table 2 does not present an especially pretty picture.  Aside from two systems? runs on the MUC-like current events, all the scores are sub-stantially below those obtained by competitive MUC systems, which typically reached F scores in the mid-90s, with a high of F=96 at MUC-6. SEC.  The worst performances were turned in for SEC filings, as shown in the first block of rows in Table 2.  While precision is generally poor, re-call is even worse.  One reason for this is the very frequent rightwards shortenings of company names (e.g., from 3M Corporation to the Corporation), in contrast to the leftwards shortening (e.g., 3M) fa-vored in news texts.  Ariel had been tuned to tag all these cases, but the other systems only tagged a scattershot fraction.  To isolate the contribution of 
Pocahontas Rule-based Belle Rule-based Jasmine Statistical, HMM, MUC-trained Mulan Statistical, CRF, MUC-trained Ariel Rule-based, 10-K tuning Table 1: system pseudonyms. 
182
these cases to system recall error, we recalculated the scores by making the cases optional.  The scorer removes missing optional responses from the recall denominator, and as expected recall im-proved; see the second block in Table 2.  Business news.  The most consistent perform-ance across systems was achieved with business news, with scores ranging in F=69-80.  This is a huge improvement over the gaping F=36-75 range we saw with SEC filings (F=43-75 with optional short names).  This confirms that length and finan-cial jargon are implicated in the poor performance on forms 10-K.  Nonetheless, these improved scores are still 15-20 points lower than the better MUC scores.  Is business language just hard to tag? MUC-like news.  Our attempt to replicate the MUC evaluation data yields an equivocal answer.  Two systems (Pocahontas and Ariel) achieved MUC6-level scores; it may not be coincidental that both are next-generation versions of systems that participated at MUC.  Of the other systems, MUC-trained Mulan also showed substantial improve-ment going from business news to current events. While it is good news that three of the systems that were explicitly trained on MUC (manually or statistically) did well on MUC-like data, it is disqui-eting to see how poorly this training generalized to other news texts. 4 Factors affecting performance A finer analysis of our three data sets helps trian-gulate the factors leading to the systematic per-formance differences shown in Table 2. Prevalence of organizations.  One factor espe-cially stands out: as Table 3 shows, organizations 
are twice as prevalent in the business sources as in the MUC-like data.  As organization scores gener-ally trail scores for persons and locations (Table 4), this partly explains why business texts are hard. Kinds of organizations.  But that does not ex-plain it all.  The profiles in Figure 1 show that cur-rent events favor government/quasi-government names (e.g.,?Congress,? ?Hamas?).  They are less linguistically productive than the corporate and quasi-corporate names in business texts, and so are more amenable to being explicitly listed in name gazetteers.  Florian et al(2003) note the effective-ness of gazetteers for tagging the CoNLL corpus. Editorial standards.  Our business news data reflect a growing portion of Web-hosted texts that relax the journalistic editorial rules of traditional news sources such as the NYT or WSJ.  For in-stance, our data show the same frequent omission of corporate designators (e.g. ?inc.?) that Kosseim noted in informal text.  Whereas news sources of record will generally mention a company?s desig-nator at least once in a story, our business data fre-quently fail to do so at all, thus removing a key name-tagging cue.  By tracing the Ariel rule base, we found that the absence of any designator was implicated in 81% of the system?s recall error for organization names. Length.  Name taggers often overcome this kind of missing evidence by second-passing a text, propagating name mentions identified in the first 
 Pocahontas Belle Jasmine Mulan Ariel R=58 R=28 R=50 R=50 R=71 P=65 P=52 P=43 P=56 P=79 SEC filings F=61.1 F=36.4 F=42.7 F=52.6 F=74.5 R=71 R=36 R=55 R=60 R=71 P=65 P=52 P=40 P=56 P=79 SEC filings, ?the Corp.? optional F=68.0 F=42.8 F=46.2 F=57.9 F=74.7 R=80 (82) R=64 (69) R=76 R=65 R=71 (75) P=80 (79) P=86 (83) P=63 P=74 P=74 (75) Business news F=80.1 (81) F=73.5 (75) F=69.1 F=69.2 F=72.3 (75) R=94 (94) R=59 (63) R=79 R=79 R=89 (91) P=94 (93) P=82 (80) P=70 P=92 P=91 (92) Current events (MUC-like) F=94.3 (94) F=68.5 (71) F=74.5 F=84.9 F=90.4 (92) Table 2: aggregated extraction scores, ENAMEX only, unless parenthesized (in parens = all entities). 
 SEC Business MUC Org 70% 65% 29% Per 9% 23% 35% Loc 21% 12% 36% Table 3: Relative distribution of entity types 
183
pass to matching but undetected mentions (Mik-heev, 1999).  This strategy runs foul, though, when the first pass produces precision errors, as these too can get propagated.  Document length is implicated in this through the greater cumulative likelihood of making an error on the first pass and of finding a mention that matches the error on the second pass. Quasi-names and non-names.  A final factor that especially afflicts the Forms 10-K is the simi-larity of names and non-names.  Non-taggable product names (?AMD Athlon?) often look like le-gitimate subsidiaries, while valid operating divi-sions (?Health Care?) are often hard to distinguish from generic designations of market segments.  5 Implications for further research. What surprised us most in conducting this study was to find so obvious a transfer gap among what appear to be very similar text sources.  We were also surprised by the involvement in this of relaxed editorial standards around seeming trivia (like the keyword ?inc.?)  This suggests, for one, that cur-rent techniques remain too dependent on skin-deep word co-occurrence features.  It also suggests that the editorially pristine news texts used in so much NE research may be atypically easy to tag. While name-tagging programs may struggle with editorially informal texts, the absence of sur-
face contextual cues poses no noticeable challenge to human readers.  What cues are left, and there are many, are semantic in nature: predicate-argument structure, selectional restrictions, organization of the lexicon, etc.  Recent efforts to create common propositional banks and lexical ontologies may thus have much to offer.  Indeed, current research in these areas is just beginning to trickle down to the name-tagging problem (Mohit & Hwa, 2005). Another key issue is ensuring tagging coher-ency at the whole-document level.  This might help alleviate the kind of error propagation with dual-pass strategies that particularly afflicts long docu-ments.  Recent applications of statistical co-reference models are beginning to show promise (Finkel et al 2005; Ji & Grishman, 2005). Lastly, we can see this whole study as a particu-lar challenge case for transfer learning, and indeed such work as Sutton and McCallum?s (2005) has looked at the name-tagging task from a transfer learning standpoint. It may thus be that today?s exciting emerging work in ?unsolved? areas ? semantics, reference, and learning ? could come to play a key role in what is sometimes maligned as yesterday?s boring solved problem. References Finkel J R, Grenager T, Manning C (2005). Incorporat-ing non-local information into information extraction systems by Gibbs sampling, Proc ACL, Ann Arbor. Florian R, Ittycheriah A, Jing H, Zhang T (2003). Named entity recognition through classifier combina-tion, Proc CoNLL, Edmonton. Ji H, Grishman R (2005). Improving name tagging by reference resolution and relation detection, Proc ACL. Kosseim L, Poibeau T (2001). Extraction de noms pro-pres ? partir de textes vari?s.  Proc. TALN, Toulouse. Mikheev A, Moens M, Grover C (1999). Named entity recognition without gazetteers.  Proc. EACL, Bergen. Minkov E, Wang R, Cohen W (2005). Extracting per-sonal names from email. Proc HLT/EMNLP, Vancouver. Mohit B, Hwa R (2005) Syntax-based semi-supervised named entity tagging.  Proc ACL, Ann Arbor MI. Sundheim B (1995), ed. Proc MUC-6, Columbia MD. Sutton C, McCallum A (2005). Composition of CRFs for transfer learning, Proc HLT/EMNLP, Vancouver. 
 Poca. Belle Jasm. Mul. Ariel S org F=62 F=10 F=46 F=53 F=83   opt F=74 F=14 F=52 F=61 F=83 S per F=75 F=65 F=49 F=64 F=60 S loc F=79 F=77 F=49 F=74 F=78 B org F=77 F=72 F=70 F=63 F=66 B. per F=90 F=85 F=70 F=69 F=79 B. loc F=78 F=76 F=59 F=75 F=73 M org F=90 F=58 F=48 F=80 F=80 M per F=99 F=90 F=84 F=81 F=92 M loc F=98 F=81 F=74 F=89 F=95 Table 4: Type subcores (S=SEC, B=biz., M=MUC) 
184
In: Proceedings of CoNLL-2000 and LLL-2000, pages 160-162, Lisbon, Portugal, 2000. 
Phrase Parsing with Rule Sequence Processors: 
an Appl icat ion to the Shared CoNLL Task 
Marc  V i la in  and Dav id  Day  
The MITRE Corporation 
Bedford, MA 01730, USA 
{mbv, day}Omitre,  org 
For several years, chunking has been an inte- 
gral part of MITRE's approach to information 
extraction. Our work exploits chunking in two 
principal ways. First, as part of our extraction 
system (Alembic) (Aberdeen et al, 1995), the 
chunker delineates descriptor phrases for entity 
extraction. Second, as part of our ongoing re- 
search in parsing, chunks provide the first level 
of a stratified approach to syntax - the second 
level is defined by grammatical relations, much 
as in the SPARKLE effort (Carroll et al, 1997). 
Because of our ongoing work with chunking, 
we were naturally interested in evaluating our 
approach on the common CoNLL task. In this 
note, we thus present three different evaluations 
of our work on phrase-level parsing. The first 
is a baseline of sorts, our own version of the 
"chunking as tagging" approach introduced by 
Ramshaw and Marcus (Ramshaw and Marcus, 
1995). The second set of results reports the 
performance of a trainable rule-based system, 
the Alembic phrase rule parser. As a point of 
comparison, we also include a third set of mea- 
sures produced by running the standard Alem- 
bic chunker on the common task with little or 
no adaptation. 
1 Chunking as Tagging 
For this first experiment, we coerced our part- 
of-speech tagger to generate chunk labels. We 
did so in what can only count as the most rudi- 
mentary way: by training the tagger to map 
part-of-speech labels to the chunk labels of the 
common task. The learning procedure is a re- 
implementation f Brill's transformation-based 
approach (Brill, 1993), extended to cover ap- 
proximately an order of magnitude more rule 
schemata. As input, the training corpus was 
tagged with the parts-of-speech from the corn- 
N 
1000 
2000 
4000 
accuracy precision 
86 77 
89 82 
89 81 
recall FB= 1 
77 77 
82 82 
81 81 
Table 1: Performance of the brute-force re- 
tagging approach 
mon data set: these provided an initial labeling 
of the data which was then directly converted 
to chunk labels through the action of transfor- 
mation rules (Brill's so-called contextual rules). 
Because the learning procedure is none the 
swiftest, we restricted ourselves to subsets of 
the training data, acquiring rules from the first 
1000, 2000, and 4000 sentences of the training 
set. In each case, we acquired 500 transforma- 
tion rules. We measured the following perfor- 
mance of these rules on the test set. 
These results are hardly stellar, falling some 
10 points of F below the performance of previ- 
ous approaches to noun group detection. To be 
sure, the chunking task is more demanding than 
the simple identification of noun group bound- 
aries, so one would expect lower performance on 
the harder problem. But the rudimentary way 
in which we implemented the approach is likely 
also to blame. 
There are a number of clear-cut ways in which 
we could attempt o improve our performance 
using this approach. In particular, we would ex- 
pect to obtain better esults if we did not oblit- 
erate the part-of-speech of a lexeme in the pro- 
cess of tagging it with a chunk label. Indeed, 
in our experiments, the learning procedure ac- 
quired transformations that simply replaced the 
part-of-speech tag with a chunking tag, thereby 
inhibiting potentially useful downstream rules 
for accessing the part-of-speech information of 
160 
a chunk-tagged word. 
2 Chunk ing  w i th  the  Phrase  Ru le  
Parser  
Our main interest in this common evaluation, 
however, was not to set new high-water marks 
with the approach of Ramshaw and Marcus, but 
to exercise our phrase rule parser. 
The Alembic phrase rule parser (Vilain and 
Day, 1996) provides the core of the system's 
syntactic processing. In our extraction appli- 
cations, the phraser (as we call it) initially tags 
named entities and other fixed-class constructs 
(like titles). The phraser also treats as atomic 
units the stereotypical combinations of named 
entities that one finds in newswire text, e.g., 
the person-title-organization apposition "U.N. 
secretary general Kofi Anan". The three com- 
ponents of the apposition axe initially parsed 
as fixed-class entities, and are then combined 
to form a single person-denoting phrase. These 
preliminary parsing steps provide part of the in- 
put to the chunker, which is itself implemented 
as a phrase rule parser. 
The architecture of the parser is based on 
Brill's approach. The parser follows a sequence 
of rules in order to build phrases out of parse is- 
lands. These islands are initially introduced by 
instantiating partial phrases around individual 
lexemes (useful for name tagging), or around 
runs of certain parts of speech (useful for both 
name tagging and chunking). It is the job of the 
phrase parsing rules to grow the boundaries of 
these phrases to the left or right, and to assign 
them a type, e.g., a name tag or a chunk la- 
bel. As with other rule sequence processors, the 
phraser proceeds in sequence through its cata- 
logue of rules, applying each in turn wherever it
matches, and then discarding it to proceed on 
to the next rule in the sequence. 
For example, in name tagging, we seed 
initial phrases around runs of capitalized 
words. A phrase such as "meetings in Paris 
and Rome" would produce an initial phrase 
analysis of "meetings in <?>Par is</?> and 
<?>Rome</?>",  where the "?" on the phrases 
are initial labels that indicate the phrase has 
not received a type. 
The patterns that are implemented byphrase 
parsing rules are similar to those in Brill's 
transformation-based p-o-s tagger. A rule can 
test for the presence of a given part of speech, of 
a lexeme, of a list of lexemes, and so on. These 
tests are themselves anchored to a specific locus 
(a phrase or lexeme) and are performed relative 
to that locus. As actions, the rules can grow the 
boundaries of a phrase, and set or modify its la- 
bel. For example, a typical name tagging rule 
would assign a LOCATION tag to any phrase 
preceded by the preposition "in". And indeed, 
this very rule tends to emerge as the very first 
rule acquired in training a phraser-based name 
tagger. We show it here with no further com- 
ment, trusting that its syntax is self-evident. 
(def-phraser-rule 
:conditions (:left-i :lex "in") 
:actions (:set-label :LOCATION)) 
In our particular example ("meetings in 
<?>Par is</?> and <?>Rome</?>") ,  this 
rule would re-label the <?> phrase around Paris 
with the LOCATION tag. A subsequent rule 
might then exploit the coordination to infer 
that "Rome" is a location as well, implement- 
ing the transformation "LOCATION and <?>" 
--+ "LOCATION and LOCATION". This incre- 
mental patching of errors is the hallmark of 
Brill's approach. 
An interesting property of this rule language 
is that the phraser can be operated either as 
a trainable procedure, using standard error- 
driven transformation learning, or as a hand- 
engineered system. For the purpose of the com- 
mon CoNLL task, let us first present our results 
for the trainable case. 
We again approached the task in a relatively 
rudimentary way, in this case by applying the 
phrase rule learning procedure with no partic- 
ular adaptation to the task. Indeed, the proce- 
dure can be parameterized by word lists which 
it can then exploit to improve its performance. 
Since our main interest here was to see our base- 
line performance on the task, we did not harvest 
such word lists from the training data (there is 
an automated way to do this). We ran a num- 
ber of training runs based on different partitions 
of the training data, with the following overall 
performance on test data, averaged across runs. 
accuracy 89 \]precision 89 I recall 
161 
test data precision The constituents hat were most accurately 
recognized were noun groups (F=88), with verb 
groups a close second (F=87). These were 
followed by the ostensibly easy cases of PP's 
(F=86), SBAR's (F=79), and ADVP's (F=75). 
Our lowest performing constituent for which 
the learning procedure actually generated rules 
was ADJP's (F=37), with no rules generated 
to identify CONJP's, INTJ's, LST's, or PRT's 
(F=0 in all these cases). 
In general, precision, tended to be several 
points of F higher than recall, and in the case 
of ADJP's average precision was 76 compared 
to average recall of 24! 
3 Chunk ing  w i th  the  
Hand-Eng ineered  System 
As a point of comparison, we also applied our 
hand-engineered chunker to the CoNLL task. 
We expected that it would not perfbrm at its 
best on this task, since it was designed with 
a significantly different model of chunking in 
mind, and indeed, unmodified, it produced is- 
appointing results: 
accuracy precision recall\[ Ffl- 1 
84 80 75 \[ 77 
The magnitude of our error term was some- 
thing of a surprise. With production runs 
on standard newswire stories (several hundred 
words in lengths) the chunker typically produces 
fewer errors per story than one can count on one 
hand. The discrepancy with the results mea- 
sured on the CoNLL task is of course due to 
the fact that our manually engineered parser 
was designed to produce chunks to a different 
standard. 
The standard was carefully defined so as to 
be maximally informative to downstream pro- 
cessing. Generally speaking, this means that it 
tends to make distinctions that are not made in 
the CoNLL data, e.g., splitting verbal runs such 
as "failed to realize" into individual verb groups 
when more than one event is denoted. 
Our curiosity about these discrepancies i
now piqued. As a point of further investiga- 
tion, we intend to apply the phraser's training 
procedure to adapt the manual chunker to the 
CoNLL task. With transformation-based rule 
sequences, this is easy to do: one merely trains 
the procedure to transform the output required 
ADJP  
ADVP 
CONJP  
INT J  
LST  
NP  
PP  
PRT  
SBAR 
VP  
all 
75.89% 
80.64% 
0.00% 
0.00% 
0.00% 
87.85% 
91.77% 
0.00% 
91.36% 
90.34% 
88.82% 
recall Fp=l 
24.43% 36.96 
70.21% 75.06 
0.00% 0.00 
0.00% 0.00 
0.0O% 0.00 
87.77% 87.81 
80.42% 85.72 
0.00% 0.00 
69.16% 78.72 
84.13% 87.13 
82.91% 85.76 
Table 2: The results of the phrase rule parser. 
for the one task into that required for the other. 
The rules acquired in this way are then sim- 
ply tacked on to the end of the original rule 
sequence (a half dozen such rules written by 
hand bring the performance of the chunker up 
to F=82, for example). 
A more interesting point of investigation, 
however, would be to analyze the discrepan- 
cies between current chunk standards from the 
standpoint of syntactic and semantic riteria. 
We look forward to reporting on this at some 
future point. 
Re ferences  
J. Aberdeen, J. Burger, D. Day, L. Hirschman, 
P. Robinson, and M. Vilain. 1995. Mitre: De- 
scription of the alembic system used for muc-6. 
In Proc. 6th Message Understanding Conference 
(MUC-6). Defense Advanced Research Projects 
Agency, November. 
E. Brill. 1993. A Corpus-based Approach to Lan- 
guage Learning. Ph.D. thesis, U. Pennsylvania. 
J. Carroll, T. Briscoe, N. Calzolari, S. Fed- 
erici, S. Montemagni, V. Pirrelli, G. Grefen- 
stette, A. Sanfilippo, G. Carroll, and M. Rooth. 
1997. SPARKLE work package 1, specifica- 
tion of phrasal parsing, final report. Avail- 
able at http://www, i lc .  pi. cnr. i t /spark le / -  
sparkle, htm, November. 
L. Ramshaw and M. Marcus. 1995. Text chunking 
using transformation-based learning. In Proc. of 
the 3rd Workshop on Very Large Corpora, pages 
82-94, Cambridge, MA, USA. 
M. Vilain and D. Day. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
16th Intl. Conference on Computational Linguis- 
tics (COLING-96). 
162 
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 192?200,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A simple feature-copying approach for long-distance dependencies 
Marc Vilain, Jonathan Huggins, and Ben Wellner The MITRE Corporation 202 Burlington Rd Bedford, MA 01730 (USA) {mbv,jhuggins,wellner}@mitre.org 
 
 
Abstract 
This paper is concerned with statistical meth-ods for treating long-distance dependencies.  We focus in particular on a case of substantial recent interest: that of long-distance depend-ency effects in entity extraction.  We intro-duce a new approach to capturing these effects through a simple feature copying preprocess, and demonstrate substantial performance gains on several entity extraction tasks. 1 Long-distance dependencies The linguistic phenomena known as long-distance dependencies have a long history in computational linguistics.  Originally arising in phrase-structure grammar, the term aptly describes phenomena that are not strictly grammatical, and has thus gained currency in other endeavors, including that of con-cern to us here: entity extraction.  The common thread, however, is simply that the treatment of a linguistic constituent ? might be influenced by the treatment of a non-local constituent ?. In phrase-structure grammar, dependencies arise between matrix phrases and the gapped phrases that they dominate, as in ?the cake that I hope you?ll serve ??.  The idea that these are long-distance dependencies arises from the fact that the separation between linked constituents can be arbi-trarily increased while their dependency continues to hold (as in ?the cake that I hope you?ll ask Fred to tell Joan to beg Maryanne to serve ??). With entity extraction, long-distance dependen-cies typically occur between mentions of the same entity.  Consider, for example, the italicized refer-ences to Thomas White in this newswire excerpt:  
Bank of America on Friday named Thomas White head of global markets.  White has been global head of credit products. The fact that the first of these mentions is easily understood as person-denoting has substantial bearing on interpreting the second mention as per-son-denoting as well.  But while local evidence for personhood is abundant for the first instance (e.g., the given name ?Thomas? or the verb ?named?), the evidence local to the second instance is weak, and it is highly unlikely that a learning procedure would on its own acquire the relevant 5-gram con-text (? has been ?JJ ?title).  The dependency between these instances of White is thus a significant factor in interpreting both as names. It is well known that capturing this kind of de-pendency can dramatically improve the perform-ance of entity extraction systems.  In this paper, we pursue a very simple method that enables statistical models to exploit these long-distance dependencies for entity extraction.  The method obtains compa-rable or better results than those achieved by more elaborate techniques, and while we focus here on the specific case of entity extraction, we believe that the method is simple and reliable enough to apply generally to other long-distance phenomena.  2 Approaches to name dependencies The problem of capturing long-distance dependen-cies between names has a traditional heuristic solu-tion.  This method, which goes back to systems participating in the original MUC-6 evaluation (Sundheim, 1995), is based on a found names list.  The method requires two passes through the input.  A first pass captures named entities based on local 
192
evidence, and enters these names into a found names registry.  A second pass identifies candidate entities that were missed by the first pass, and compares them to entries in the registry.  Where there is string overlap between the candidate and a previously found name, the entity type assigned to the existing entry is copied to the candidate. Overall, this is an effective strategy, and we used it ourselves in a rule-based name tagger from the MUC-6 era (Vilain and Day, 1996).  The strat-egy?s Achilles heel, however, is what happens when erroneous entries are added to the found names list.  These can get copied willy-nilly, thereby drastically increasing the scope of what may originally have started as a single local error.  Clearly, the approach is begging to be given a firmer evidence-weighing foundation. 2.1 A statistical hybrid An early such attempt at reformulating the ap-proach is due to Minkheev et al(1999).   As with previous approaches, Mikheev and his colleagues use a rule-based first pass to populate a found-names list.  The second pass, however, is based on a maximum entropy classifier that labels non-first-passed candidates based on evidence accrued from matching entries on the found-names list.  The sta-tistical nature of the decision eliminates some of the failure modes of the heuristic found-names strategy, and in particular, prevents the copying of single errors committed in the first pass.  The ma-jor weakness of the approach, however, is the heu-ristic first pass.  Minkheev et alnote that their method is most effective with a high-precision found-names list, implemented as a tightly con-trolled (but incomplete) rule-based first pass. 2.2 Fully-statistical models Several more recent efforts have attempted to re-move the need for a heuristic first-pass tagger, and have thus cast the problem as one-pass statistical models (Bunescu and Mooney, 2004; Sutton and McCallum, 2004; Finkel et al 2005).  While the technical details differ, all three methods approach the problem through conditional random fields (CRFs).  In order to capture the long-distance de-pendencies between name instances, these ap-proaches extend the linear-chain sequence models that are typically used for extracting entities with a CRF (Sha and Pereira, 2003).  The resulting models 
consist of sentence-length sequences interlinked on those words that might potentially have long-distance interactions.  Because of the graph-like nature of these models, the simplifying assump-tions of linear-chain CRFs no longer hold.  Since complete parameter estimation is intractable under these conditions, these three approaches introduce approximate methods for parameter estimation or decoding (Perceptron training for the first, loopy belief propagation for the first two, Gibbs sampling and simulated annealing for the third). Krishnan and Manning (2006) provide a lucid critique of these extended models and of their computational ramifications.  In a nutshell, their critique centers on the complexity of constructing the linked graphs (which they deemed high), the stability of Perceptron training (potentially unsta-ble), and the run-time cost of simulated annealing (undesirably high).  Since these undesirable prop-erties are directly due to the treatment of long-distance dependencies through graphical models, it is natural to ask whether graphical models are ac-tually required to capture these dependencies. 2.3 Avoiding non-sequential dependencies In point of fact, Krishnan and Manning (2006) pre-sent an alternative to these graph-based methods.  In particular, they break the explicit links that mu-tually condition non-adjacent lexemes, and instead rely on separate passes in a way that is reminiscent of earlier methods.  A first-pass CRF is used to identify entities based solely on local information.  The entity labels assigned by this first CRF are summarized in terms of lexeme-by-lexeme major-ity counts; these counts are then passed to a second CRF in the form of lexical features. Consider, for example, a financial news source, where we would expect that a term like ?Bank? might be assigned a preponderance of ORG labels by the first-pass CRF.  This would be signaled to the second-pass CRF through a token majority fea-ture that would take on the value ORG for all in-stances of the lexeme ?Bank?. This effectively aggregates local first-pass labeling decisions that apply to this lexeme, and makes the second-pass CRF sensitive to these first-pass decisions.  Further refinements capture cases where a lexeme?s label diverges from the token majority, for example: ?Left Bank,? where ?Bank? will be assigned a LOC-valued entity majority feature whenever it ap-
193
pears in that particular word sequence. By captur-ing long-distance dependencies through lexical features, Krishnan and Manning avoid the need for graphical models, thus regaining tractability. How well does this work?  Returning to our earlier example, the idea behind these majority count features is that a term like ?White? might be assigned the PER label by the first CRF when it ap-pears in the context ?Thomas White.?  Say, for the sake of argument, that sufficiently many instances of ?White? are labeled PER by the first pass to sum to a majority.  The second-stage CRF might then be expected to exploit the majority count features for ?White? to PER-label any instances of White that were left unlabeled in the first pass (or that were given erroneous first-pass labels). The method would be expected to fail, how-ever, in cases where the first pass yields a majority of erroneous labels.  Krishnan and Manning sug-gest that this is a fairly unlikely scenario, and dem-onstrate that their approach effectively captures long-distance name dependencies for the CoNLL English name-tagging task.  They measured a best-in-class error reduction of 13.3% between their two-pass method and a single-stage CRF equipped with comparable features.  3 A contradictory data set Just how unlikely, however, is the majority-error scenario that Krishnan and Manning discount?  As it turns out, we encountered precisely this scenario while working with a corpus that is closely related to the CoNLL data used by Krishnan and Manning. The corpus in question was drawn from the on-line edition of Reuters business news.  The articles cover a range of business topics: mergers and ac-quisitions (M+A), stock valuations, management change, and so forth.  This corpus is highly perti-nent to this discussion, as the CoNLL English data are also Reuters news stories, drawn from the gen-eral news distribution.  Our business data thus rep-resent a natural branch of the overall CoNLL data. A characteristic of these Reuters business sto-ries that distinguishes them from general news is the prevalence of organization names, in particular company names.  In these data, instances of com-pany names significantly outnumber the next-most-common entities (money, dates, and the like).  Even state-of-the-art CRFs trained on these data therefore err on the side of generating companies, 
meaning that in the absence of countermanding evidence (such as the presence of a person?s given name), an entity will tend to be labeled ORG by default.  Our earlier ?Thomas White? example is a case in point: where the full name would typically be labeled PER, last-name-only instances (?White?) might go unlabeled or be marked ORGs. Table 1, above, shows a qualitative analysis of this phenomenon for PER entities in our M+A test set.  The table considers person-denoting entities with three or more instances in the test set (n=35), and summarizes the majority accuracy of the labels assigned to them by a feature-rich 1-pass CRF.  Of these thirty-five cases, we eliminate from consid-eration six trivial test cases that are present unam-biguously in the training data (e.g., ?Carl Icahn?), since the CRF will effectively memorizes these cases during training.  Of the remaining twenty-nine non-trivial cases, not quite half of them (45%) were accurately labeled by the CRF for the majority of their instances.  A larger number of entities ei-ther received an incorrect majority label (38%) or were equivocally labeled, receiving an equal num-ber of correct and incorrect tags (17%). For this data set then, majority count features are poor models of the long-distance dependencies between person names, as they are just about as likely to predict the wrong label as the correct one. 4 A feature-copying alternative A further analysis of our business news test sample revealed an intriguing fact.  While in the absence of compelling evidence, the CRF might label a mention of a person entity as an org (or leave it unlabeled), for those mentions where compelling evidence existed, the CRF generally got it right.  By compelling evidence, we mean such linguistic cues as the presence of a given name, contextual prox-imity to agentive verbs (e.g. ?said?), and so forth. This suggests an alternative approach to captur-ing these kinds of long-distance dependencies be-
Label accuracy count % test cases Trivially correct (present in both test and training) 6 ? Majority correct, test only 13 45% Majority incorrect, test only 11 38% Equivocal, test only 5 17% Table 1: effectiveness of majority counts as predictors of entity type, Reuters business news sample 
194
tween names.  In contrast to previous approaches, what is needed is not so much a way of coordinat-ing non-local decisions about an entity?s label, as a way of coordinating non-local evidence pertinent to the labeling decision.  That is, instead of condi-tioning the labeling decision of a lexeme on the labeling decisions for that lexeme elsewhere in the corpus, we ought to condition the decision on the key evidence supporting those decisions. 4.1 Displaced features Our approach operates by identifying those fea-tures of a CRF that are most predictive over a cor-pus.  Each of those features is then duplicated: for a given token ?, one version of the feature applies directly to ?, while the other version applies to all other instances where ??s word form appears in the current document.  In particular, what we duplicate is the indicator function for a feature.  The local version of an indicator ? signals true if it applies locally to ?, while the displaced version ?d signals true if it applies to any token ?? that is an instance of the same word from as ?. To make this concrete, consider our opening example, now indexed with word positions: Thomas7 White8 ? White13 has14 been15 ? Say that ? is a feature indicator that is true of a token ?i just in case the token to its left, ?i-1, is a given name.  In this instance, ?(White8) is true and ?(White13) is false.  Then ?d, the displaced version of ?, will be true of ?i just in case there is some token ?j with the same word form such that ?(?j) is true.  In this instance ?d(White8) and ?d(White13) are both true by virtue of ? being true of White8. This feature displacement scheme introduces non-local evidence into labeling decisions, effec-tively capturing the long-distance dependencies exhibited by name-tagging tasks.  The method dif-fers from previous approaches in that the models are not made conditional on non-local decisions (as in the case of graphical models), nor are they made conditional on aggregated first-pass decisions (as in Krishnan & Manning), but rather are made con-ditional on non-local evidence (displaced features). 4.2 Identifying features to displace Because a typical entity extraction model can use tens or hundreds of thousands of features, it is not practical to displace every one of them.  Though 
technically this only doubles the number of fea-tures under consideration, the lexical indexing rap-idly gets out of hand.  In addition, training and run times increase and, in our experience, a risk of over-fitting emerges.  In point of fact, however, capturing long-distance name dependencies does not require us to replicate every last bit of feature-borne evidence.  Instead, we only need to displace the evidence that is most reliably predictive. To select predictive features to displace, we?ve had most success with a method based on informa-tion gain.  Specifically, we use a one-time pre-process that measures feature gain relative to a corpus.  The pre-process considers the same com-plement of feature schemas as are used by the ac-tual CRF, and grounds the schemas on a training corpus to instantiate free lexical and P-O-S parame-ters.  Gain for the instantiated features is measured through K-L divergence, and the n features with highest gain are then selected for displacement (with n typically ranging from 1,000 to 10,000). As in (Schneider, 2004), gain for a given fea-ture ?, is found through a variant of the familiar Kullback-Leibler divergence formula, 
? 
D
KL
(P ||Q) = p(x
i
)log
2
p(x
i
)
q(x
i
)
i
?
 For our purposes, the xi are the non-null entity labels defined for the training set (PER, ORG, etc.), P is the probability distribution of the labels over the training set, Q is the distribution of the labels over tokens for which ? applies, and p and q are their respective smoothed probability estimates (Laplace smoothing).  Note in particular that this formulation excludes the null label (?not an en-tity?).  This effectively means that K-L divergence is giving us a measure of the degree to which a feature predicts one or more non-null entity labels.  Because the null label is generally the dominant label in named-entity tasks, including the null label in the calculation of K-L divergence tends to overwhelm the statistics, and leads to the selection of uninformative features that predict non-entities. Figure 1 demonstrates the effectiveness of this feature selection method, along with sensitivity to the threshold parameter.  The figure charts F-score on a Reuters business news task (M+A) as a func-tion of the number of displaced features.  From a baseline of F=89.3, performance improves rapidly with the addition of displaced features to the CRF model, reaching a maximum of F=91.4 with the 
195
addition of 1,000 displaced features.  Performance then fluctuates asymptotically around this level. The chart also shows comparable growth curves for two alternative feature selection methods.  The feature count method is similar to feature gain, but instead of ranking features with K-L divergence, it ranks them according to the number of times they match against the corpus.  Feature weight does not use a schema-grounding first pass to generate can-didate features, but trains a CRF model on the cor-pus, and then ranks features according to the weight assigned to them in the model.  In prelimi-nary experiments, neither of these methods yielded as high-performing a set of displaced features as feature gain.  Additionally their growth curves ex-hibit sensitivity to parameter setting, which sug-gests a risk of over-fitting.  For these reasons, we did not pursue these approaches further. Note finally that the feature schemas we con-sider for displacement only encode local evidence (see Table 2 below).  In particular, they do not en-code the assigned label of a word form, as this would effectively introduce the kind of graphical conditional dependencies that lie outside the scope of linear-chain CRF methods. 4.3 Training and decoding Aside from two pre-processing steps, training or decoding a CRF with displaced features is no dif-ferent from training or decoding one with only conventional features.  As to the pre-processing steps, the first applies to the corpus overall, as we must initially select a collection of locally predic-tive features to displace.  The second step applies on a per-document basis and consists of the crea-tion of the inverted lexical indices that are used to trigger indicator functions for displaced features. 
While these additional steps complicate training and decoding somewhat, they have little effect on actual decoding run times.  Most importantly, they retain the linear-chain properties of the CRF, and therefore do not require the graphical modeling and involved parameter estimation called for by most previous approaches.  In addition, the training logistics are of a lesser magnitude than those re-quired by Krishnan and Manning?s approach, since training their second-stage model first requires round-robin training of one-fold-left-out classifiers that estimate first-stage majority counts. 5 Experimental design To evaluate the effectiveness of feature copying with long-distance dependencies, we undertook a number of information extraction experiments.  We focused on the traditional name-tagging task, relying on both current and archival data sets.  For each data set, we trained entity-extraction models that corresponded to three different strategies for capturing long-distance dependencies. ? Baseline model: a feature-rich CRF trained with only local features and no long-distance dependency features; ? Feature-copying model: a CRF trained with the same local features, along with displaced versions of high-gain features; ? Majority model: a re-implementation of the Krishnan and Manning strategy, using the same feature set as the baseline CRF as well as their majority count features. We used held-out development test sets to tune the selection of displaced features, in particular, the number of features to displace. 5.1 CRF configurations We used the Carafe open-source implementation of sequence-based conditional random fields.1  Carafe has achieved competitive results for standard se-quence modeling tasks (Wellner & Vilain, 2006, Wellner et al 2007), and allows for flexible feature design.  Carafe provides several learning methods, including a fast gradient descent method using pe-riodic step-size adjustment (Huang et al 2007).  Preliminary trials, however, produced better results                                                        1 http://sourceforge.net/projects/carafe 
Figure 1: F score on the Reuters M+A task, as a  function of number of displaced features 
196
with conditional log-likelihood learning (L-BFGS optimization).  We used this latter method here, L2-regularized by a spherical Gaussian prior with variance set to 10.0 (based on preliminary trials). Our baseline CRF was given a feature set that has proven its mettle in the literature (see Table 2).  Along with contextual n-grams and the like, these features capture linguistic regularities through membership in vocabulary lists, e.g., first names, major geographical names, honorifics, etc.  They also include hand-engineered lists from our legacy rule-based tagger, e.g., head word lists for organi-zation names, lists of agentive verbs that reliably apply to persons, date atoms, and more.  For part-of-speech features, we either accepted the parts of speech provided with a data set, or generated them with our implementation of Brill?s method (Brill, 1994).  For the majority count features, we used document and corpus versions the token and entity features described by Krishnan and Manning, but did not re-implement their super-entity feature. 5.2 Experimental data We evaluated our approach on five different data sets: our current corpus of Web-harvested Reuters business news, as well as four archival data sets that have been reported on by other researchers.  The business news data consist of a training corpus of mergers and acquisition stories (M+A), devel-opment and evaluation test sets for M+A and test sets for three additional topics: hot stocks (HS), new initiatives (NI), and general business news (BN).   Table 3 provides an overview of our data sets and of some salient distinctions between them. All five extraction tasks require the reporting of three core entity types: persons, organizations, and locations; additional required types are noted in the table.  The reporting guidelines for the first four tasks are closely related: Reuters business and MUC-6 were annotated to the same original MUC-6 
standard, while MUC-7 and MNET extend the MUC-6 standard slightly.  The CoNLL standard alone calls for a catch-all (and troublesome) MISC entity. 5.3 Scoring metrics Previous results on these data sets have been re-ported using one of two scoring methods: strict match (CoNLL) or match with partial credit, as cal-culated by the MUC scorer (MUC-6, MUC-7, and MNET).  To enable comparisons to previously pub-lished work, we report our results with the metric appropriate to each data set (we use the MUC scorer for Reuters).  These scoring distinctions are perti-nent only to comparisons of absolute performance.  In this paper, the interest is with relative compari-sons across approaches to long-distance dependen-cies, for which the scorers are kept constant. 6 Experimental results Table 4 summarizes our experimental results for the seven test sets annotated to the MUC-6 standard or its close variants (we will consider the CoNLL task separately).  Along with F scores for our base-line CRF, the table presents F scores and baseline-relative error reduction (?E) for two approaches to long-distance name dependencies: feature dis-placement (disp) and the Krishnan and Manning strategy (K+M).  We were pleased to see that fea-ture displacement proved effective for all of the extraction tasks.  As the table shows, the addition of displaced features consistently reduced the re-sidual error term left by the baseline CRF trained only with local features.  For the English-language corpora, the error reduction ranged from a low of 11 % for the Reuters NI task to a high of 39% for the MUC-6 task.  The error reduction for the Span-ish-language MNET task was lowest of all, at 8.9%. For all the English tasks, we consistently achieved better results with feature displacement 
lexical unigrams w-2 ? w+2 lexical bigrams w-2,w-1 ? w+1,w+2 P-O-S unigrams p-2 ? p+2 P-O-S bigrams p-2,p-1 ? p+1,p+2 substrings .*s or s.* ||s||?4 linguistic word lists gazetteers, date atoms, ? regular expressions caps., digits, ? ?corp.? nearby also ?ltd.? ? Table 2: Baseline features; wi and pi respectively de-note lexeme and P-O-S in relative position i. 
Corpus Language NU TM MI  Topics MUC-6 English ? ?  mostly politics MUC-7 English ? ?r  mostly politics MNET Spanish ? ?r  mostly politics Reuters English ? ?  business CoNLL English   ? all news Table 3: Data set characteristics.  All include persons, organizations, and locations; some have nu-meric forms (NU), dates and times (TM) where r indicates relative dates, or misc (MI). 
197
than with our version of Krishnan and Manning?s approach (we were not able to obtain Spanish K+M results by publication time).  In each case, dis-placement produced a greater reduction in baseline error than did majority counts.  Furthermore, be-cause both approaches start from the same baseline CRF, the resulting raw performance was conse-quently also higher for displacement.  Note in par-ticular the Reuters M+A test set: these are the data for which Table 1 suggests that majority counts would be poor predictors of long-distance effects.  This prediction is in fact borne out by our results. 6.1 Effects of linguistic engineering  We were interested to note that the feature dis-placement method achieved both highest perform-ance and highest error reduction for the MUC-6 corpus (F=92.8, ?E=39.3%) and for two of the Reuters test sets: M+A (F=91.4, ?E=20.0%) and BN (F=91.8, ?E=21.6%).  The MUC-6 F-score, in par-ticular, is comparable to those of hand-built MUC-era systems; in fact, it exceeds the score of our own hand-built MUC-6 system (Aberdeen et al 1995). What is apparently happening is that these three data sets are well matched to a group of linguisti-cally inspired lexical features with which we trained our baseline CRF.  In particular, our base-line features include gazetteers and word lists hand-selected for identifying entities based on lo-cal context: first names, agentive verbs, date at-oms, etc.  This played out in two significant ways.  First, these linguistic features tended to elevate baseline performance (see Table 4).  Second, these same features also proved effective when dis-placed, as demonstrated by the substantial error reduction with displacement. Feature displacement thus further rewards sound feature engineering. 6.2 Other MUC-related results The MUC-7 and Reuters hot stocks data (HS) pro-vide informative contrasts.  For these data, feature displacement provided error reduction of ?E=13.9% and 13.4% respectively, which is less 
than for the top three data sets.  It is interesting to note that in both cases, the baseline score is also lower, suggesting again that the performance of feature copying follows the performance of base-line tagging.  In the case of Reuters HS, the evalua-tion data contained many out-of-training references to stock indices, which depressed baseline scores.  Similar development-to-evaluation divergences have also been noted with the MUC-7 corpus. 6.3 The CoNLL task Our results for the CoNLL task, reported in Table 5 below, provide a different point of contrast.  The middle two rows of the table present the same ex-perimental configurations as have been discussed so far.  For this data set, we note that feature dis-placement does not perform as well as our re-implementation of Krishnan and Manning?s strat-egy in terms of both absolute score and error re-duction.  Likewise, published results for other approaches mostly outperform displacement (see the first three rows in Table 5). One possible explanation lies with the linguistic features with which we approached CoNLL: these are the same ones we originally developed for MUC-6.  As noted earlier the CoNLL standard di-verges in several ways from MUC-6.  In particular, CoNLL calls for a MISC entity that covers a range of name-like entities, e.g., events. MISC also, how-ever, captures names that are trapped by tokeniza-tion (?London-based?), as well as some MUC organizations (sports leagues).  This suggests that adapting our features to the CONLL task might help. 
MUC-6 MUC-7 MNET Reuters M+A Reuters BN Reuters HS Reuters NI  F ?E F ?E F ?E F ?E F ?E F ?E F ?E baseline 88.2 ? 84.0 ? 88.9 ? 89.3 ? 89.5 ? 85.4 ? 88.8 ? disp. 92.8 39% 86.2 14% 89.9 8.9% 91.4 20% 91.8 22% 87.3 13% 90.1 11% K+M 91.5 28% 85.2 7.4% ? ? 90.4 11% 91.0 14% 86.3 6.2% 89.2 2.8%  Table 4: Performance on seven test sets annotated to variants of the MUC-6 standard (MUC scorer). 
  base F LDD F ?E Bunescu + Mooney 2004 80.09 82.30 11.1% Finkel et al2005 85.51 86.86 9.3% Krishnan + Manning 2006 85.29 87.34 13.3% K+M (re-impl, MUC feats.) 84.3 86.0 10.7% displacement (MUC feats.) 84.3 85.8 9.6% displ. (CoNLL feats.) 85.24 86.55 8.9% displ. (CoNLL feats. + DS) 86.57 87.39 6.1% Table 5: Performance on the CoNLL task; LDD designates  use of long-distance dependency method. 
198
The final two rows in Table 5 present attempts to tune our features to CoNLL.  This includes some features (the ?CoNLL feats? in Table 5) indicating story topic, all-caps headline contexts, presence in a sporting result table, and similar idiosyncrasies.  In addition, we also used features based on dis-tributional similarity word lists (DS in the table) provided with the Stanford NER package.2 While these feature engineering efforts proved effective, what we found surprised us.  As Table 5 shows, the CoNLL features do substantially raise baseline performance, with the full set of new fea-tures producing a baseline (F=86.6) that outper-forms previously published baselines by over a point of F score.  In keeping with our observations for the MUC-annotated text, we would then have expected to see a comparable increase in the per-formance of displaced features, i.e., a jump in error reduction relative to the baseline.  Instead, we found just the reverse.  Whereas displacement ac-counts for a 1.5 point gain in F (?E=9.6%) with the MUC baseline features, with the beter CoNLL fea-tures, the gain due to displacement falls to 0.82 points of F (?E=6.1%).  While the final result with displacement (F=87.39) slightly edges out the pre-vious high water mark of F=87.35 (Krishnan and Manning, 2005), the pattern is puzzling and not in keeping with our seven other data sets. One possible explanations lies again with the CoNLL standard.  The standard calls explicitly for inconsistent annotation of the same entity when used in different contexts.  Along with place names being called MISC in hyphenated contexts (noted above), some places must be called ORG when used to refer to sports teams ? except in results tables, where they are sometimes LOC.  Such inconsisten-cies subvert the notion of long-distance dependen-cies by making these dependencies contradictory, thereby reducing the potential value of displace-ment as a means for improving performance. 7 Conclusions Earlier in this paper, we introduced the notion of long-distance dependencies through their original codification in the context of phrase-structure grammars.  By an interesting historical twist, the original solution to these grammatical long-distance effects, known as gap threading (Pereira,                                                        2 http://nlp.stanford.edu/software/CRF-NER.shtml 
1981), involved what is essentially a feature-copying operation, namely unification of constitu-ent features.  It is gratifying to note that the method presented here has illustrious predecessors. Regarding the particular task of interest here, entity extraction, this paper conclusively shows that a simple feature-copying method provides an effective method for capturing long-distance de-pendencies between names.  For the MUC-6 task, in particular, this error reduction is enough to lift a middle-of-the-pack performance from our baseline CRF to a level that would have placed it among the handful of top performers at the MUC-6 evaluation. As noted, the method is also substantially more manageable than earlier approaches.  It avoids the intractability of graphical models and also avoids the approximations required by methods that rely on these models.  It also adds only minimal proc-essing time at training and run times.  This pro-vides a practical alternative to the method of Krishnan and Manning, who require twelve sepa-rate training runs to create their models, and fur-ther require a time-consuming run-time process to mediate between their first and second stage CRFs. We intend to take this work in two directions.  First, we would like to get to the bottom of why the method did not do better with the CoNLL and MNET tasks.  As noted earlier, our hypothesis is that we would expect greater exploitation of long-distance dependencies if we first improved the performance of the baseline CRF, especially by improving the acuity of task-related features.  While it is not a key interest of ours to achieve best-in-class per-formance on historical evaluations, it is the case that we seek a better understanding of the range of application of the feature copying method. Another direction of interest is to consider other problems that exhibit long-distance dependencies that might be addressed by feature copying.  Word sense disambiguation is one such case, especially given Yarowsky?s maxim regarding one sense per discourse, a consistency notion that seems tailor-made for treatment as long-distance dependencies (Yarowsky, 1995).  Likewise, we are curious about the applicability of the method to reference resolu-tion, another key task with long-distance effects. Meanwhile, we believe that this method pro-vides a practical approach for capturing long-distance effects in one of the most practical and useful application of human language technologies, entity extraction. 
199
References John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995. Description of the Alembic system as used for MUC-6. Pcdgs of the 6th Message Understanding Conference (MUC-6). Eric Brill. 1994. Some advances in rule-based part-of-speech tagging. Pcdgs. AAAI-94. Razvan Bunescu and Raymond J. Mooney. 2004. Col-lective information extraction with relational Markov networks. Pcdgs. of the 42nd ACL.  Barcelona. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sam-pling. Pcdgs. of the 43rd ACL.  Ann Arbor, MI. Han-Shen Huang, Yu-Ming Chang, and Chun-Nan Hsu. 2007. Training conditional random fields by periodic step size adaptation for large-scale text mining.  Pcdgs. 7th Intl. Conf. on Data Mining (ICDM-2007). Vijay Krishnan and Christopher D. Manning. 2006. An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition. Pcdgs. of the 21st COLING and 44th ACL.  Sidney. Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers.  Pcdgs. of the 9th EACL. Bergen. Fernando Pereira. 1981. Extraposition grammars.  American Jnl. of Computational Linguistics, 4(7). Karl-Michael Schneider. 2004. A new feature selection score for multinomial naive Bayes text classification based on KL-divergence.  In Companion to the Pcdgs. of the 42nd ACL.  Barcelona. Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields.  Pcdgs. of NAACL-HLT 2003. Edmonton, CA. Beth Sundheim, ed. 1995.  Pcdgs. of the 6th Message Understanding Conference (MUC-6). Columbia, MD. Charles Sutton and Andrew McCallum. 2004. Collec-tive segmentation and labeling of distant entities in information extraction.  Pcdgs. ICML Workshop on Statistical Relational Learning. Marc Vilain and David Day. 1996. Finite-state phrse parsing by rule sequences. Pcdgs. of the 16th Confer-ence on Computational Lingusitics (COLING-96). Ben Wellner, Matt Huyck, Scott Mardis, John Aber-deen, Alex Morgan, Leon Peskin, Alex Yeh, Janet Hitzeman, and Lynette Hirschman. 2007. Rapidly re-targetable approaches to de-identification.  Journal of the Americal Medical Informatics Association; 14(5). Ben Wellner and Marc Vilain. (2006). Leveraging ma-chine-readable dictionaries in discriminative se-quence models. In Pcdgs. of the 5th Language Resources and Evaluation Conf. (LREC 2006). Genoa. David Yarowsky. 1995.  Unsupervised word sense dis-ambiguation rivaling supervised methods.  Pcdgs. Of 33rd ACL.  Cambridge, MA. 
200
