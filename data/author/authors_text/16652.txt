NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 47?55,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Communication strategies for a computerized caregiver for individuals with
Alzheimer?s disease
Frank Rudzicz1,2, ? and Rozanne Wilson1 and Alex Mihailidis2 and Elizabeth Rochon1
1 Department of Speech-Language Pathology,
2 Department of Occupational Science and Occupational Therapy
University of Toronto
Toronto Canada
Carol Leonard
School of Rehabilitation Sciences
University of Ottawa
Ottawa Canada
Abstract
Currently, health care costs associated with
aging at home can be prohibitive if individ-
uals require continual or periodic supervision
or assistance because of Alzheimer?s disease.
These costs, normally associated with human
caregivers, can be mitigated to some extent
given automated systems that mimic some of
their functions. In this paper, we present in-
augural work towards producing a generic au-
tomated system that assists individuals with
Alzheimer?s to complete daily tasks using ver-
bal communication. Here, we show how to
improve rates of correct speech recognition
by preprocessing acoustic noise and by mod-
ifying the vocabulary according to the task.
We conclude by outlining current directions of
research including specialized grammars and
automatic detection of confusion.
1 Introduction
In the United States, approximately $100 billion are
spent annually on the direct and indirect care of in-
dividuals with Alzheimer?s disease (AD), the major-
ity of which is attributed to long-term institutional
care (Ernst et al, 1997). As the population ages, the
incidence of AD will double or triple, with Medi-
care costs alone reaching $189 billion in the US by
2015 (Bharucha et al, 2009). Given the growing
need to support this population, there is an increas-
ing interest in the design and development of tech-
nologies that support this population at home and
extend ones quality of life and autonomy (Mihailidis
et al, 2008).
?Contact: frank@cs.toronto.edu
Alzheimer?s disease is a type of progres-
sive neuro-degenerative dementia characterized by
marked declines in mental acuity, specifically in
cognitive, social, and functional capacity. A decline
in memory (short- and long-term), executive capac-
ity, visual-spacial reasoning, and linguistic ability
are all typical effects of AD (Cummings, 2004).
These declines make the completion of activities of
daily living (e.g., finances, preparing a meal) diffi-
cult and more severe declines often necessitate care-
giver assistance. Caregivers who assist individuals
with AD at home are common, but are often the pre-
cursor to placement in a long-term care (LTC) facil-
ity (Gaugler et al, 2009).
We are building systems that automate, where
possible, some of the support activities that currently
require family or formal (i.e., employed) caregivers.
Specifically, we are designing an intelligent dialog
component that can engage in two-way speech com-
munication with an individual in order to help guide
that individual towards the completion of certain
daily household tasks, including washing ones hands
and brushing ones teeth. A typical installation setup
in a bathroom, shown in figure 1, consists of video
cameras that track a user?s hands and the area in and
around the sink, as well as microphones, speakers,
and a screen that can display prompting informa-
tion. Similar installations are being tested in other
household rooms as part of the COACH project (Mi-
hailidis et al, 2008), according to the task; this is
an example of ambient intelligence in which tech-
nology embedded in the environment is sensitive to
the activities of the user with it (Spanoudakis et al,
2010).
47
Our goal is to encode in software the kinds of
techniques used by caregivers to help their clients
achieve these activities; this includes automati-
cally identifying and recovering from breakdowns
in communication and flexibly adapting to the in-
dividual over time. Before such a system can be de-
ployed, the underlying models need to be adjusted
to the desired population and tasks. Similarly, the
speech output component would need to be pro-
grammed according to the vocabularies, grammars,
and dialog strategies used by caregivers. This paper
presents preliminary experiments towards dedicated
speech recognition for such a system. Evaluation
data were collected as part of a larger project exam-
ining the use of communication strategies by formal
caregivers while assisting residents with moderate to
severe AD during the completion of toothbrushing
(Wilson et al, 2012).
2 Background ? communication strategies
Automated communicative systems that are more
sensitive to the emotive and the mental states of their
users are often more successful than more neutral
conversational agents (Saini et al, 2005). In order to
be useful in practice, these communicative systems
need to mimic some of the techniques employed
by caregivers of individuals with AD. Often, these
caregivers are employed by local clinics or medical
institutions and are trained by those institutions in
ideal verbal communication strategies for use with
those having dementia (Hopper, 2001; Goldfarb and
Pietro, 2004). These include (Small et al, 2003) but
are not limited to:
1. Relatively slow rate of speech rate.
2. Verbatim repetition of misunderstood prompts.
3. Closed-ended questions (i.e., that elicit yes/no
responses).
4. Simple sentences with reduced syntactic com-
plexity.
5. Giving one question or one direction at a time.
6. Minimal use of pronouns.
These strategies, though often based on observa-
tional studies, are not necessarily based on quantita-
tive empirical research and may not be generalizable
across relevant populations. Indeed, Tomoeda et al
(1990) showed that rates of speech that are too slow
(a) Environmental setup
(b) On-screen prompting
Figure 1: Setup and on-screen prompting for COACH.
The environment includes numerous sensors including
microphones and video cameras as well as a screen upon
which prompts can be displayed. In this example, the
user is prompted to lather their hands after having applied
soap. Images are copyright Intelligent Assistive Technol-
ogy and Systems Lab).
may interfere with comprehension if they introduce
48
problems of short-term retention of working mem-
ory. Small, Andersen, and Kempler (1997) showed
that paraphrased repetition is just as effective as ver-
batim repetition (indeed, syntactic variation of com-
mon semantics may assist comprehension). Further-
more, Rochon, Waters, and Caplan (2000) suggested
that the syntactic complexity of utterances is not
necessarily the only predictor of comprehension in
individuals with AD; rather, correct comprehension
of the semantics of sentences is inversely related to
the increasing number of propositions used ? it is
preferable to have as few clauses or core ideas as
possible, i.e., one-at-a-time.
Although not the empirical subject of this pa-
per, we are studying methods of automating the
resolution of communication breakdown. Much of
this work is based on the Trouble Source-Repair
(TSR) model in which difficulties in speaking, hear-
ing, or understanding are identified and repairs are
initiated and carried out (Schegloff, Jefferson, and
Sacks, 1977). Difficulties can arise in a number
of dimensions including phonological (i.e., mispro-
nunciation), morphological/syntactic (e.g., incorrect
agreement among constituents), semantic (e.g., dis-
turbances related to lexical access, word retrieval,
or word use), and discourse (i.e., misunderstanding
of topic, shared knowledge, or cohesion) (Orange,
Lubinsky, and Higginbotham, 1996). The major-
ity of TSR sequences involve self-correction of a
speaker?s own error, e.g., by repetition, elaboration,
or reduction of a troublesome utterance (Schegloff,
Jefferson, and Sacks, 1977). Orange, Lubinsky,
and Higginbotham (1996) showed that while 18%
of non-AD dyad utterances involved TSR, whereas
23.6% of early-stage AD dyads and 33% of middle-
stage AD dyads involved TSR. Of these, individu-
als with middle-stage AD exhibited more discourse-
related difficulties including inattention, failure to
track propositions and thematic information, and
deficits in working memory. The most common
repair initiators and repairs given communication
breakdown involved frequent ?wh-questions and hy-
potheses (e.g., ?Do you mean??). Conversational
partners of individuals with middle-stage AD initi-
ated repair less frequently than conversational part-
ners of control subjects, possibly aware of their de-
teriorating ability, or to avoid possible further con-
fusion. An alternative although very closely related
paradigm for measuring communication breakdown
is Trouble Indicating Behavior (TIB) in which the
confused participant implicitly or explicitly requests
aid. In a study of 7 seniors with moderate/severe de-
mentia and 3 with mild/moderate dementia, Watson
(1999) showed that there was a significant difference
in TIB use (? < 0.005) between individuals with
AD and the general population. Individuals with
AD are most likely to exhibit dysfluency, lack of up-
take in the dialog, metalinguistic comments (e.g., ?I
can?t think of the word?), neutral requests for repeti-
tion, whereas the general population are most likely
to exhibit hypothesis formation to resolve ambiguity
(e.g., ?Oh, so you mean that you had a good time??)
or requests for more information.
2.1 The task of handwashing
Our current work is based on a study completed by
Wilson et al (2012) towards a systematic observa-
tional representation of communication behaviours
of formal caregivers assisting individuals with mod-
erate to severe AD during hand washing. In that
study, caregivers produced 1691 utterances, 78% of
which contained at least one communication strat-
egy. On average, 23.35 (? = 14.11) verbal strate-
gies and 7.81 (? = 5.13) non-verbal strategies were
used per session. The five most common communi-
cation strategies employed by caregivers are ranked
in table 1. The one proposition strategy refers to
using a single direction, request, or idea in the utter-
ance (e.g. ?turn the water on?). The closed-ended
question strategy refers to asking question with a
very limited, typically binary, response (e.g., ?can
you turn the taps on??) as opposed to questions elic-
iting a more elaborate response or the inclusion of
additional information. The encouraging comments
strategy refers to any verbal praise of the resident
(e.g., ?you are doing a good job?). The paraphrased
repetition strategy is the restatement of a misunder-
stood utterance using alternative syntactic or lexical
content (e.g., ?soap up your hands....please use soap
on your hands?). There was no significant difference
between the use of paraphrased and verbatim repe-
tition of misunderstood utterances. Caregivers also
reduced speech rate from an average baseline of 116
words per minute (s.d. 36.8) to an average of 36.5
words per minute (s.d. 19.8).
The least frequently used communication strate-
49
Number of occurrences % use of strategy Uses per session
Verbal strategy Overall Successful Overall Successful Mean SD
One proposition 619 441 35 36 8.6 6.7
Closed-ended question 215 148 12 12 3.0 3.0
Encouraging comments 180 148 10 12 2.9 2.5
Use of resident?s name 178 131 10 11 2.8 2.5
Paraphrased repetition 178 122 10 10 3.0 2.5
Table 1: Most frequent verbal communication strategies according to their number of occurrences in dyad communi-
cation. The % use of strategy is normalized across all strategies, most of which are not listed. These results are split
according to the total number of uses and the number of uses in successful resolution of a communication breakdown.
Mean (and standard deviation) of uses per session are given across caregivers. Adapted with permission from Wilson
et al (2012).
gies employed by experienced caregivers involved
asking questions that required verification of a res-
ident?s request or response (e.g., ?do you mean
that you are finished??), explanation of current ac-
tions (e.g., ?I am turning on the taps for you?), and
open-ended questions (e.g., ?how do you wash your
hands??).
The most common non-verbal strategies em-
ployed by experienced caregivers were guided touch
(193 times, 122 of which were successful) in which
the caregiver physically assists the resident in the
completion of a task, demonstrating action (113
times. 72 of which were successful) in which an
action is illustrated or mimicked by the caregiver,
handing an object to the resident (107 times, 85 of
which were successful), and pointing to an object
(105 times, 95 of which were successful) in which
the direction to an object is visually indicated by
the caregiver. Some of these strategies may be em-
ployed by the proposed system; for example, videos
demonstrating an action may be displayed on the
screen shown in figure 1(a), which may replace to
some extent the mimicry by the caregiver. A pos-
sible replication of the fourth most common non-
verbal strategy may be to highlight the required ob-
ject with a flashing light, a spotlight, or by display-
ing it on screen; these solutions require tangential
technologies that are beyond the scope of this cur-
rent study, however.
3 Data
Our experiments are based on data collected by Wil-
son et al (submitted) with individuals diagnosed
with moderate-to-severe AD who were recruited
from long-term care facilities (i.e., The Harold and
Grace Baker Centre and the Lakeside Long-Term
Care Centre) in Toronto. Participants had no pre-
vious history of stroke, depression, psychosis, alco-
holism, drug abuse, or physical aggression towards
caregivers. Updated measures of disease severity
were taken according to the Mini-Mental State Ex-
amination (Folstein, Folstein, and McHugh, 1975).
The average cognitive impairment among 7 individ-
uals classified as having severe AD (scores below
10/30) was 3.43 (? = 3.36) and among 6 individ-
uals classified as having moderate AD (scores be-
tween 10/30 and 19/30) was 15.8 (? = 4.07). The
average age of residents was 81.4 years with an aver-
age of 13.8 years of education and 3.1 years of resi-
dency at their respective LTC facility. Fifteen formal
caregivers participated in this study and were paired
with the residents (i.e., as dyads) during the comple-
tion of activities of daily living. All but one care-
giver were female and were comfortable with En-
glish. The average number of years of experience
working with AD patients was 12.87 (? = 9.61).
The toothbrushing task follows the protocol of the
handwashing task. In total, the data consists of 336
utterances by the residents and 2623 utterances by
their caregivers; this is manifested by residents utter-
ing 1012 words and caregivers uttering 12166 words
in total, using 747 unique terms. The toothbrushing
task consists of 9 subtasks, namely: 1) get brush and
paste, 2) put paste on brush, 3) turn on water, 4) wet
tooth brush, 5) brush teeth, 6) rinse mouth, 7) rinse
brush, 8) turn off water, 9) dry mouth.
These data were recorded as part of a large
project to study communication strategies of care-
givers rather than to study the acoustics of their
transactions with residents. As a result, the record-
50
ings were not of the highest acoustic quality; for
example, although the sampling rate and bit rate
were high (48 kHz and 384 kbps respectively), the
video camera used was placed relatively far from the
speakers, who generally faced away from the mi-
crophone towards the sink and running water. The
distribution of strategies employed by caregivers for
this task is the subject of ongoing work.
4 Experiments in speech recognition
Our first component of an automated caregiver
is the speech recognition subsystem. We test
two alternative systems, namely Carnegie Mellon?s
Sphinx framework and Microsoft?s Speech Plat-
form. Carnegie Mellon?s Sphinx framework (pock-
etsphinx, specifically) is an open-source speech
recognition system that uses traditional N -gram
language modeling, sub-phonetic acoustic hidden
Markov models (HMMs), Viterbi decoding and
lexical-tree structures (Lamere et al, 2003). Sphinx
includes tools to perform traditional Baum-Welch
estimation of acoustic models, but there were not
enough data for this purpose. The second ASR sys-
tem, Microsoft?s Speech Platform (version 11) is
less open but exposes the ability to vary the lexicon,
grammar, and semantics. Traditionally, Microsoft
has used continuous-density HMMs with 6000 tied
HMM states (senones), 20 Gaussians per state, and
Mel-cepstrum features (with delta and delta-delta).
Given the toothbrushing data described in section
3, two sets of experiments were devised to config-
ure these systems to the task. Specifically, we per-
form preprocessing of the acoustics to remove envi-
ronmental noise associated with toothbrushing and
adapt the lexica of the two systems, as described in
the following subsections.
4.1 Noise reduction
An emergent feature of the toothbrushing data is
very high levels of acoustic noise caused by the
running of water. In fact, the estimated signal-to-
noise ratio across utterances range from ?2.103 dB
to 7.63 dB, which is extremely low; for comparison
clean speech typically has an SNR of approximately
40 dB. Since the resident is likely to be situated close
to this source of the acoustic noise, it becomes im-
portant to isolate their speech in the incoming signal.
Speech enhancement involves the removal of
acoustic noise d(t) in a signal y(t), including am-
bient noise (e.g., running water, wind) and signal
degradation giving the clean ?source? signal x(t).
This involves an assumption that noise is strictly ad-
ditive, as in the formula:
y(t) = x(t) + d(t). (1)
Here, Yk, Xk, and Dk are the kth spectra of the
noisy observation y(t), source signal x(t), and un-
correlated noise signal d(t), respectively. Generally,
the spectral magnitude of a signal is more important
than its phase when assessing signal quality and per-
forming speech enhancement. Spectral subtraction
(SS), as the name suggests, subtracts an estimate of
the noisy spectrum from the measured signal (Boll,
1979; Martin, 2001), where the estimate of the noisy
signal is estimated from samples of the noise source
exclusively. That is, one has to learn estimates based
on pre-selected recordings of noise. We apply SS
speech enhancement given sample recordings of wa-
ter running. The second method of enhancement
we consider is the log-spectral amplitude estimator
(LSAE) which minimizes the mean squared error
(MMSE) of the log spectra given a model for the
source speech Xk = Ak exp(j?k), where Ak is the
spectral amplitude. The LSAE method is a modifi-
cation to the short-time spectral amplitude estima-
tor that attempts to find some estimate A?k that min-
imizes the distortion
E
[(
logAk ? log A?k
)2
]
, (2)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
=
?k
1 + ?k
exp
(
1
2
? ?
vk
e?t
t
dt
)
Rk,
(3)
where ?k is the a priori SNR,Rk is the noisy spectral
amplitude, vk =
?k
1+?k
?k, and ?k is the a posteriori
SNR (Erkelens, Jensen, and Heusdens, 2007). Of-
ten this is based on a Gaussian model of noise, as
it is here (Ephraim and Malah, 1985). We enhance
our recordings by both the SS and LSAE methods.
Archetypal instances of typical, low, and (relatively)
high SNR waveform recordings and their enhanced
versions are shown in 4.1.
51
(a) Dyad1.1
(b) Dyad4.2
(c) Dyad11.1
Figure 2: Representative samples of toothbrushing data
audio. Figures show normalized amplitude over time for
signals cleaned by the LSAE method overlaid over the
larger-amplitude original signals.
We compare the effects of this enhanced audio
across two ASR systems. For the Sphinx system,
we use a continuous tristate HMM for each of the 40
phones from the CMU dictionary trained with audio
from the complete Wall Street Journal corpus and
the independent variable we changed was the num-
ber of Gaussians per state (n. ?). These parame-
ters are not exposed by the Microsoft speech system,
so we instead vary the minimum threshold of confi-
dence C ? [0..1] required to accept a word; in theory
lower values of C would result in more insertion er-
rors and higher values would result in more deletion
errors. For each system, we used a common dic-
tionary of 123, 611 unique words derived from the
Carnegie Mellon phonemic dictionary.
Table 2 shows the word error rate for each of
the two systems. Both the SS and LSAE methods
of speech enhancement result in significantly better
word error rates than with the original recordings at
the 99.9% level of confidence according to the one-
tailed paired t-test across both systems. The LSAE
method has significantly better word error rates than
the SS method at the 99% level of confidence with
this test. Although these high WERs are impractical
for a typical system, they are comparable to other re-
sults for speech recognition in very low-SNR envi-
ronments (Kim and Rose, 2003). Deng et al (2000),
for example, describe an ASR system trained with
clean speech that has a WER of 87.11% given addi-
tive white noise for a resulting 5 dB SNR signal for
a comparable vocabulary of 5000 words. An inter-
esting observation is that even at the low confidence
threshold of C = 0.2, the number of insertion er-
rors did not increase dramatically relative to for the
higher values in the Microsoft system; only 4.0% of
all word errors were insertion errors at C = 0.2, and
2.7% of all word errors at C = 0.8.
Given Levenshtein alignments between annotated
target (reference) and hypothesis word sequences,
we separate word errors across residents and across
caregivers. Specifically, table 3 shows the propor-
tion of deletion and substitution word errors (relative
to totals for each system separately) across residents
and caregivers. This analysis aims to uncover dif-
ferences in rates of recognition between those with
AD and the more general population. For exam-
ple, 12.6% of deletion errors made by Sphinx were
words spoken by residents. It is not possible to at-
52
Word error rate %
Parameters Original SS LSAE
Sphinx
n. ? = 4 98.13 75.31 70.61
n. ? = 8 98.13 74.95 69.66
n. ? = 16 97.82 75.09 69.78
n. ? = 32 97.13 74.88 67.22
Microsoft
C = 0.8 97.67 73.59 67.11
C = 0.6 97.44 72.57 67.08
C = 0.4 96.85 71.78 66.54
C = 0.2 94.30 71.36 64.32
Table 2: Word error rates for the Sphinx and Microsoft
ASR systems according to their respective adjusted pa-
rameters, i.e., number of Gaussians per HMM state (n. ?)
and minimum confidence threshold (C). Results are given
on original recordings and waveforms enhanced by spec-
tral subraction (SS) and MMSE with log-spectral ampli-
tude estimates (LSAE).
tribute word insertion errors to either the resident or
caregiver, in general. If we assume that errors should
be distributed across residents and caregivers in the
same proportion as their respective total number of
words uttered, then we can compute the Pearson ?2
statistic of significance. Given that 7.68% of all
words were uttered by residents, the observed num-
ber of substitutions was significantly different than
the expected value at the 99% level of confidence
for both the Sphinx and Microsoft systems, but the
number of deletions was not significantly different
even at the 95% level of confidence. In either case,
however, substantially more errors are made propor-
tionally by residents than we might expect; this may
in part be caused by their relatively soft speech.
Proportion of errors
Sphinx Microsoft
Res. Careg. Res. Careg.
deletion 13.9 86.1 12.6 87.4
substitution 23.2 76.8 18.4 81.6
Table 3: Proportion of deletion and substitution errors
made by both (Res)idents and (Careg)ivers. Proportions
are relative to totals within each system.
4.2 Task-specific vocabulary
We limit the common vocabulary used in each
speech recognizer in order to be more specific to the
task. Specifically, we begin with the 747 words ut-
tered in the data as our most restricted vocabulary.
Then, we expand this vocabulary according to two
methods. The first method adds words that are se-
mantically similar to those already present. This
is performed by taking the most common sense for
each noun, verb, adjective, and adverb, then adding
each entry in the respective synonym sets accord-
ing to WordNet 3.0 (Miller, 1995). This results in
a vocabulary of 2890 words. At this point, we it-
eratively add increments of words at intervals of
10, 000 (up to 120, 000) by selecting random words
in the vocabulary and adding synonym sets for all
senses as well as antonyms, hypernyms, hyponyms,
meronyms, and holonyms. The result is a vocabu-
lary whose semantic domain becomes increasingly
generic. The second approach to adjusting the vo-
cabulary size is to add phonemic foils to more re-
stricted vocabularies. Specifically, as before, we be-
gin with the restricted 747 words observed in the
data but then add increments of new words that
are phonemically similar to existing words. This
is done exhaustively by selecting a random word
and searching for minimal phonemic misalignments
(i.e., edit distance) among out-of-vocabulary words
in the Carnegie Mellon phonemic dictionary. This
approach of adding decoy words is an attempt to
model increasing generalization of the systems. Ev-
ery vocabulary is translated into the format expected
by each recognizer so that each test involves a com-
mon set of words.
Word error rates are measured for each vocabu-
lary size across each ASR system and the manner in
which those vocabularies were constructed (seman-
tic or phonemic expansion). The results are shown
in figure 4.2 and are based on acoustics enhanced
by the LSAE method. Somewhat surprisingly, the
method used to alter the vocabulary did appear to
have a very large effect. Indeed, the WER across
the semantic and phonemic methods were correlated
at ? >= 0.99 across both ASR systems; there was
no significant difference between traces (within sys-
tem) even at the 60% level of confidence using the
two-tailed heteroscedastic t-test.
5 Ongoing work
This work represents the first phase of development
towards a complete communicative artificial care-
giver for the home. Here, we are focusing on the
53
102 103 104 105 10635
40
45
50
55
60
65
70
Vocabulary size
Wo
rd E
rror
 Ra
te (%
)
 
 
Sphinx ? Phonemic
Microsoft ? PhonemicSphinx ? Semantic
Microsoft ? Semantic
Figure 3: Word error rate versus size of vocabulary (log
scale) for each of the Sphinx and Microsoft ASR systems
according to whether the vocabularies were expanded by
semantic or phonemic similarity.
speech recognition component and have shown re-
ductions in error of up to 72% (Sphinx ASR with
n.? = 4) and 63.1% (Sphinx ASR), relative to base-
line rates of error. While significant, baseline er-
rors were so severe that other techniques will need
to be explored. We are now collecting additional
data by fixing the Microsoft Kinect sensor in the
environment, facing the resident; this is the default
configuration and may overcome some of the ob-
stacles present in our data. Specifically, the beam-
forming capabilities in the Kinect (generalizable to
other multi-microphone arrays) can isolate speech
events from ambient environmental noise (Balan and
Rosca, 2002). We are also collecting speech data for
a separate study in which individuals with AD are
placed before directional microphones and complete
tasks related to the perception of emotion.
As tasks can be broken down into non-linear (par-
tially ordered) sets of subtasks (e.g., replacing the
toothbrush is a subtask of toothbrushing), we are
specifying grammars ?by hand? specific to those sub-
tasks. Only some subset of all subtasks are possible
at any given time; e.g., one can only place tooth-
paste on the brush once both items have been re-
trieved. The possibility of these subtasks depend on
the state of the world which can only be estimated
through imperfect techniques ? typically computer
vision. Given the uncertainty of the state of the
world, we are integrating subtask-specific grammars
into a partially-observable Markov decision process
(POMDP). These grammars include the semantic
state variables of the world and break each task
down into a graph-structure of interdependent ac-
tions. Each ?action? is associated with its own gram-
mar subset of words and phrases that are likely to
be uttered during its performance, as well as a set
of prompts to be spoken by the system to aid the
user. Along these lines, we we will attempt to gen-
eralize the approach taken in section 4.2 to gener-
ate specific sub-vocabularies automatically for each
subtask. The relative weighting of words will be
modeled based on ongoing data collection.
Acknowledgments
This research was partially funded by Mitacs and
an operating grant from the Canadian Institutes of
Health Research and the American Alzheimer As-
sociation (ETAC program). The authors acknowl-
edge and thank the administrative staff, caregivers,
and residents at the Harold and Grace Baker Centre
and the Lakeside Long-Term Care Centre.
References
Balan, Radu and Justinian Rosca. 2002. Microphone
Array Speech Enhancement by Bayesian Estimation
of Spectral Amplitude and Phase. In Proceedings of
IEEE Sensor Array and Multichannel Signal Process-
ing Workshop.
Bharucha, Ashok J., Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent assis-
tive technology applications to dementia care: Current
capabilities, limitations, and future challenges. Amer-
ican Journal of Geriatric Psychiatry, 17(2):88?104,
February.
Boll, S.F. 1979. Suppression of acoustic noise in speech
using spectral subtraction. IEEE Transactions on
Acoustics, Speech, and Signal Processing, 27(2):113?
120, April.
Cummings, Jeffrey L. 2004. Alzheimer?s disease. New
England Journal of Medicine, 351(1):56?67.
Deng, Li, Alex Acero, M. Plumpe, and Xuedong Huang.
2000. Large-vocabulary speech recognition under ad-
verse acoustic environments. In Proceedings of the In-
ternational Conference on Spoken Language Process-
ing, October.
54
Ephraim, Y. and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral am-
plitude estimator. Acoustics, Speech and Signal Pro-
cessing, IEEE Transactions on, 33(2):443 ? 445, apr.
Erkelens, Jan, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spectral
speech enhancement methods for various error crite-
ria. Speech Communication, 49:530?541.
Ernst, Richard L., Joel W. Hay, Catharine Fenn, Jared
Tinklenberg, and Jerome A. Yesavage. 1997. Cog-
nitive function and the costs of alzheimer disease ?
an exploratory study. Archives of Neurology, 54:687?
693.
Folstein, Marshal F., Susan E. Folstein, and Paul R.
McHugh. 1975. Mini-mental state: A practical
method for grading the cognitive state of patients
for the clinician. Journal of Psychiatric Research,
12(3):189?198, November.
Gaugler, J. E., F. Yu, K. Krichbaum, and J.F. Wyman.
2009. Predictors of nursing home admission for per-
sons with dementia. Medical Care, 47(2):191?198.
Goldfarb, R. and M.J.S. Pietro. 2004. Support sys-
tems: Older adults with neurogenic communication
disorders. Journal of Ambulatory Care Management,
27(4):356?365.
Hopper, T. 2001. Indirect interventions to facilitate com-
munication in Alzheimers disease. Seminars in Speech
and Language, 22(4):305?315.
Kim, Hong Kook and Richard C. Rose. 2003. Cepstrum-
Domain Acoustic Feature Compensation Based on De-
composition of Speech and Noise for ASR in Noisy
Environments. IEEE Transactions on Speech and Au-
dio Processing, 11(5), September.
Lamere, Paul, Philip Kwok, Evandro Gouvea, Bhiksha
Raj, Rita Singh, William Walker, M. Warmuth, and
Peter Wolf. 2003. The CMU Sphinx-4 speech recog-
nition system. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP
2003), Hong Kong, April.
Martin, Rainer. 2001. Noise power spectral density es-
timation based on optimal smoothing and minimum
statistics. IEEE Transactions of Speech and Audio
Processing, 9(5):504?512, July.
Mihailidis, Alex, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system to
assist older adults with dementia through handwash-
ing: An efficacy study. BMC Geriatrics, 8(28).
Miller, George A. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39?
41.
Orange, J.B., Rosemary B. Lubinsky, and D. Jeffery Hig-
ginbotham. 1996. Conversational repair by individu-
als with dementia of the alzheimer?s type. Journal of
Speech and Hearing Research, 39:881?895, August.
Rochon, Elizabeth, Gloria S. Waters, and David Caplan.
2000. The Relationship Between Measures of Work-
ing Memory and Sentence Comprehension in Patients
With Alzheimer?s Disease. Journal of Speech, Lan-
guage, and Hearing Research, 43:395?413.
Saini, Privender, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social in-
telligence in home dialogue systems. In Proceedings
of INTERACT 2005, pages 510?521.
Schegloff, Emanuel A., Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. 1977,
53(2):361?382.
Small, Jeff A., Elaine S. Andersen, and Daniel Kempler.
1997. Effects of working memory capacity on under-
standing rate-altered speech. Aging, Neuropsychology,
and Cognition, 4(2):126?139.
Small, Jeff A., Gloria Gutman, Saskia Makela, and
Beth Hillhouse. 2003. Effectiveness of communi-
cation strategies used by caregivers of persons with
alzheimer?s disease during activities of daily living.
Journal of Speech, Language, and Hearing Research,
46(2):353?367.
Spanoudakis, Nikolaos, Boris Grabner, Christina Kot-
siopoulou, Olga Lymperopoulou, Verena Moser-
Siegmeth, Stylianos Pantelopoulos, Paraskevi Sakka,
and Pavlos Moraitis. 2010. A novel architecture and
process for ambient assisted living - the hera approach.
In Proceedings of the 10th IEEE International Confer-
ence on Information Technology and Applications in
Biomedicine (ITAB), pages 1?4.
Tomoeda, Cheryl K., Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slauson.
1990. Speech rate and syntactic complexity effects
on the auditory comprehension of alzheimer patients.
Journal of Communication Disorders, 23(2):151 ?
161.
Watson, Caroline M. 1999. An analysis of trou-
ble and repair in the natural conversations of people
with dementia of the Alzheimer?s type. Aphasiology,
13(3):195 ? 218.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. 2012. Examining success of com-
munication strategies used by formal caregivers assist-
ing individuals with alzheimer?s disease during an ac-
tivity of daily living. Journal of Speech, Language,
and Hearing Research, 55:328?341.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. submitted. Quantitative analy-
sis of formal caregivers? use of communication strate-
gies while assisting individuals with moderate and se-
vere alzheimer?s disease during oral care. Journal of
Speech, Language, and Hearing Research.
55
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17?26,
Baltimore, Maryland USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
Comparison of different feature sets for identification of variants in
progressive aphasia
Kathleen C. Fraser
1
, Graeme Hirst
1
, Naida L. Graham
2
, Jed A. Meltzer
3
,
Sandra E. Black
4
, and Elizabeth Rochon
2
1
Dept. of Computer Science, University of Toronto
2
Dept. of Speech-Language Pathology, University of Toronto, & Toronto Rehabilitation Institute
3
Rotman Research Institute, Baycrest Centre, Toronto
4
LC Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre, Toronto
{kfraser,gh}@cs.toronto.edu, {naida.graham,elizabeth.rochon}@utoronto.ca
jmeltzer@research.baycrest.org, sandra.black@sunnybrook.ca
Abstract
We use computational techniques to ex-
tract a large number of different features
from the narrative speech of individuals
with primary progressive aphasia (PPA).
We examine several different types of fea-
tures, including part-of-speech, complex-
ity, context-free grammar, fluency, psy-
cholinguistic, vocabulary richness, and
acoustic, and discuss the circumstances
under which they can be extracted. We
consider the task of training a machine
learning classifier to determine whether a
participant is a control, or has the fluent or
nonfluent variant of PPA. We first evaluate
the individual feature sets on their classifi-
cation accuracy, then perform an ablation
study to determine the optimal combina-
tion of feature sets. Finally, we rank the
features in four practical scenarios: given
audio data only, given unsegmented tran-
scripts only, given segmented transcripts
only, and given both audio and segmented
transcripts. We find that psycholinguis-
tic features are highly discriminative in
most cases, and that acoustic, context-free
grammar, and part-of-speech features can
also be important in some circumstances.
1 Introduction
In some types of dementia, such as primary pro-
gressive aphasia, language deficit is a core symp-
tom, and the analysis of narrative or conversa-
tional speech is important for assessing the extent
of an individual?s language impairment. Analy-
sis of connected speech has been limited in the
past because it is time-consuming and requires ex-
pert annotation. However, studies have shown that
it is possible for machine learning classifiers to
achieve high accuracy on some diagnostic tasks
when trained on features which were automati-
cally extracted from speech transcripts.
In this paper, we summarize previous research
on the automatic analysis of speech samples from
individuals with dementia, focusing in particular
on primary progressive aphasia. We discuss in de-
tail different types of features and compare their
effectiveness in the classification task. We sug-
gest some benefits and drawbacks of these differ-
ent features. We also examine the interactions be-
tween different feature sets, and discuss the rela-
tive importance of individual features across fea-
ture sets. Because we examine a large number
of features on a relatively small data set, we em-
phasize that this work is exploratory in nature;
nonetheless, our results are consistent with, and
extend, previous work in the field.
2 Background
In recent years, there has been growing interest in
using computer techniques to automatically detect
dementia from speech and language features de-
rived from a sample of narrative speech. Some re-
searchers have explored ways to use methods such
as part-of-speech tagging, statistical parsing, and
speech signal analysis to detect disorders such as
dementia of the Alzheimer?s type (DAT) (Bucks et
al., 2000; Singh et al., 2001; Thomas et al., 2005;
Jarrold et al., 2010) and mild cognitive impairment
(MCI) (Roark et al., 2011).
Here, we focus on a type of dementia called
primary progressive aphasia (PPA). PPA is a sub-
type of frontotemporal dementia (FTD) which is
characterized by progressive language impairment
without other notable cognitive impairment. There
are three subtypes of PPA: semantic dementia
(SD), progressive nonfluent aphasia (PNFA), and
logopenic progressive aphasia (LPA). SD, some-
times called ?fluent? progressive aphasia, is typi-
cally marked by fluent but empty speech, anomia,
17
deficits in comprehension, and spared grammar
and syntax (Gorno-Tempini et al., 2011). In
contrast, PNFA is characterized by halting and
sometimes agrammatic speech, reduced syntac-
tic complexity, word-finding difficulties, and rela-
tively spared single-word comprehension (Gorno-
Tempini et al., 2011). The third subtype, LPA, is
characterized by slow speech and frequent word
finding difficulties; this subtype is not included in
the current analysis.
Although clear diagnostic criteria for PPA have
been established (Gorno-Tempini et al., 2011),
there is no one test which can provide a diagno-
sis. Classification of PPA into subtypes requires
evaluation of spoken output, as well as neuropsy-
chological assessment and brain imaging. Quali-
tative evaluation of speech often can be done accu-
rately by clinicians or researchers, but the ability
to do this evaluation can require years of training
and experience. Some researchers have performed
detailed quantitative characterization of speech in
PPA, but the precise characteristics of speech are
not yet fully understood and this process is too
time-consuming for most clinicians.
Peintner et al. (2008) conducted one of the earli-
est automatic analyses of speech from individuals
with FTD, including SD and PNFA as well as a
behavioural variant. They considered psycholin-
guistic features as well as phoneme duration fea-
tures extracted from the audio samples. Although
they were fairly successful in classifying partici-
pants according to their subtype, they did not re-
port many details regarding the specific features
which were useful or how those features might re-
flect the underlying impairment of the speakers.
Pakhomov et al. (2010a) examined FTD speech
from an information-theoretic approach. They
constructed a language model of healthy control
speech, and then calculated the perplexity and out-
of-vocabulary rate for each of the patient groups
relative to that model. In another study, Pakhomov
et al. (2010b) extracted speech and language fea-
tures from samples of FTD speech. In a principal
components analysis, they discovered four com-
ponents which accounted for most of the variance
in their data: speech length, hesitancy, empty con-
tent, and grammaticality. However, they did not
perform any classification experiments.
Fraser et al. (2013a) attempted to classify par-
ticipants as either SD patients, PNFA patients, or
healthy controls using a large number of language
SD
(N = 11)
PNFA
(N = 13)
Control
(N = 16)
Male/Female 8/3 7/6 9/7
Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2)
Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3)
Table 1: Demographic information. Numbers are
given in the form: mean (standard deviation).
features extracted from manually-transcribed tran-
scripts. They distinguished between SD and con-
trol participants with very high accuracy, and were
also successful at distinguishing between PNFA
and control participants. However, their method
did not perform as well on the task of classify-
ing SD vs. PNFA speakers. In subsequent work
(Fraser et al., 2013b), they expanded their feature
set to include acoustic features extracted directly
from the audio file.
3 Methods
3.1 Data
Twenty-four patients with PPA were recruited
through three Toronto memory clinics, and 16 age-
and education-matched healthy controls were re-
cruited through a volunteer pool. All participants
were native speakers of English, or had completed
some of their education in English. Exclusion cri-
teria included a known history of drug or alcohol
abuse and a history of neurological or major psy-
chiatric illness. Each patient was diagnosed by a
behavioural neurologist and all met current crite-
ria for PPA (Gorno-Tempini et al., 2011). Table 1
shows demographic information for each group.
To elicit a sample of narrative speech, partici-
pants were asked to tell the well-known story of
Cinderella. They were given a wordless picture
book to remind them of the story; then the book
was removed and they were asked to tell the story
in their own words. This procedure, described in
full by Saffran et al. (1989), is commonly used in
studies of connected speech in aphasia.
The narrative samples were transcribed by
trained research assistants. The transcriptions in-
clude filled pauses, repetitions, and false starts,
and were annotated with the total speech time.
Sentence boundaries were marked according to se-
mantic, syntactic, and prosodic cues.
3.2 Classification framework
Given the audio files and transcripts, we can then
calculate our features (described in detail below)
18
and use them to train a support vector machine
(SVM) classifier. We use a leave-one-out cross-
validation framework and report the average ac-
curacy (i.e. proportion of correctly classified in-
stances) across folds. We optimize the complexity
parameter and the kernel type in a nested cross-
validation loop over the training set. For compar-
ison, we also tested a na??ve Bayes classifier; how-
ever we found that the results were consistently
poorer and we do not report them here.
3.3 Features
In the following sections we will describe each of
the feature sets that we use and explain how the
features are computed, and we will discuss some
of the potential advantages and disadvantages as-
sociated with each set. In particular, we discuss
what types of data are necessary for the extraction
of these features. The data types are: unsegmented
transcripts, segmented transcripts, and audio.
3.3.1 Part-of-speech features
Different categories of words may be selectively
impaired in different types of dementia. In PPA,
individuals with SD tend to be more impaired
with respect to nouns than verbs, and may replace
nouns with pronouns or circumlocutory phrases.
In contrast, individuals with PNFA may have more
difficulty with verbs and may even demonstrate
agrammatism, which can result in the omission
of grammatical morphemes and function words.
Thus, it is often useful to compare the relative fre-
quencies with which words representing the differ-
ent parts-of-speech (POS) are produced in a sam-
ple, as in Table 2. Similar features have been re-
ported in computational studies of MCI (Roark et
al., 2011), FTD (Pakhomov et al., 2010b), and
DAT (Bucks et al., 2000). Numerous POS taggers
exist, although we use the Stanford tagger here
(Toutanova et al., 2003).
3.3.2 Complexity features
Changes in linguistic complexity may accompany
the onset of dementia, although some studies have
found a decrease in complexity (e.g. Kemper et al.
(2001)) while others have found an increase (e.g.
Le et al. (2011)).
The features in Table 3 vary in their ease of
computation. Mean word length can be calculated
from an unsegmented transcript, while mean sen-
tence length requires only sentence boundary seg-
mentation. Other measures, such as Yngve depth
Nouns # nouns / # words
Verbs # verbs / # words
Noun-verb ratio # nouns / # verbs
Noun ratio # nouns / (# nouns + # verbs)
Inflected verbs # inflected verbs / # verbs
Determiners # determiners / # words
Demonstratives # demonstratives / # words
Prepositions # prepositions / # words
Adjectives # adjectives / # words
Adverbs # adverbs / # words
Pronoun ratio # pronouns / (# nouns + # pronouns)
Function words # function words / # words
Interjections # interjections / # words
Table 2: Part-of-speech features.
Max depth maximum Yngve depth of each parse tree,
averaged over all sentences
Mean depth mean Yngve depth of each node in the
parse tree, averaged over all sentences
Total depth total sum of the Yngve depths of each node
in the parse tree, averaged over all sentences
Tree height height of each parse tree, averaged over
all sentences
MLS mean length of sentence
MLC mean length of clause
MLT mean length of T-unit
Subordinate conjunctions number of subordinate
conjunctions
Coordinate conjunctions number of coordinate con-
junctions
Subordinate:coordinate ratio ratio of number of sub-
ordinate conjunctions to number of coordinate
conjunctions
Mean word length mean length, in letters, of each
word in the sample
Table 3: Complexity features.
(Yngve, 1960), require full parses of the sentences
(we use the Stanford parser (Klein and Manning,
2003) and Lu?s Syntactic Complexity Analyzer
(Lu, 2010)).
3.3.3 CFG features
Although many of the complexity features above
are derived from parse trees, in this section we
present a set of features that take into account
the context-free grammar (CFG) labels on each
of the nodes. CFG features have been previously
used to assess the grammaticality of sentences in
an artificial error corpus (Wong and Dras, 2010)
and to distinguish human from machine transla-
tions (Chae and Nenkova, 2009). However, this
is the first time such features have been applied to
speech from participants with dementia.
In Table 4 we list a few examples of our 134
CFG features, as well as the three phrase-level fea-
tures (calculated for noun phrases, verb phrases,
and prepositional phrases).
19
NP ? NNS Noun phrases consisting of only a plural
noun
VP ? VBN PP Verb phrases consisting of a past-
participle verb and a prepositional phrase
ROOT? INTJ Trees consisting of only an interjec-
tion
Phrase type proportion Length of each phrase type
(noun phrase, verb phrase, or prepositional
phrase), divided by total narrative length
Average phrase type length Total number of words in
a phrase type, divided by the number of phrases
of that type
Phrase type rate Number of phrases of a given type,
divided by total narrative length
Table 4: CFG features.
Um Frequency of filled pause um
Uh Frequency of filled pause uh
NID Frequency of words Not In Dictionary (e.g. para-
phasias, neologisms)
Verbal rate Number of words per minute
Total words Total number of words produced
Table 5: Fluency features.
3.3.4 Fluency features
Park et al. (2011) found that listeners? judgements
of fluency were affected by a number of different
variables, and the three most discriminative fea-
tures were ?speech rate, speech productivity, and
audible struggle.? For our list of fluency features
(Table 5), we include only those features which
could be extracted from the transcripts alone (as-
suming the total speech time is given). We count
pauses filled by um and uh separately, as research
has suggested that they may indicate different cog-
nitive processes (Clark and Fox Tree, 2002).
The number of words in a sample could be eas-
ily generated using the word count feature in most
text-editing software (although we first exclude
filled pauses and NID tokens), and the verbal rate
can subsequently be calculated directly. The other
three features are easily calculated using string
matching and an electronic dictionary.
3.3.5 Psycholinguistic features
Some types of dementia are characterized by im-
pairments in semantic access. Such impairments
may be sensitive to psycholinguistic features such
as lexical frequency, familiarity, imageability, and
age of acquisition (Table 6). We use the SUBTL
frequency norms (Brysbaert and New, 2009) and
the combined Bristol and Gilhooly-Logie norms
(Stadthagen-Gonzalez and Davis, 2006; Gilhooly
and Logie, 1980) for familiarity, imageability, and
Frequency Frequency with which a word occurs in
some corpus of natural language
Familiarity Subjective rating of how familiar a word
seems
Imageability Subjective rating of how easily a word
generates an image in the mind
Age of acquisition Subjective rating of how old a per-
son is when they first learn that word
Light verbs Number of occurrences of be, have, come,
go, give, take, make, do, get, move, and put,
normalized by total number of verbs
Table 6: Psycholinguistic features.
age of acquisition (see Table 6). We compute the
average of each of these measures for all content
words, as well as for nouns and verbs separately.
Another measure that fits into this category is
the frequency of occurrence of light verbs, which
an impaired speaker may use to replace a more
specific verb. We use the same list of light verbs
as Breedin et al. (1998), given in Table 6.
One challenge associated with psycholinguis-
tic features is finding norms which provide ade-
quate coverage for the given data. Fraser et al.
(2013a) reported that the SUBTL frequency norms
had a coverage of above 90% on their data, but the
Bristol-Gilhooly-Logie norms had a coverage of
only around 30%.
3.3.6 Vocabulary richness features
Individuals experiencing semantic difficulty may
use a limited range of vocabulary. We can mea-
sure the vocabulary richness or lexical diversity
of a narrative sample using a number of different
metrics (see Table 7). Type-token ratio has been
a popular choice, perhaps due to its simplicity;
however it is sensitive to the length of the sample.
Bucks et al. (2000) were the first to apply Honor?e?s
statistic and Brun?et?s index to the study of demen-
tia, and found significant differences between in-
dividuals with DAT and healthy controls. Cov-
ington and McFall (2010) proposed a new mea-
sure called the moving-average type-token ratio
(MATTR), which is independent of text length.
This feature was later applied to aphasic speech in
a study by Fergadiotis and Wright (2011), and was
found to be one of the most unbiased indicators of
lexical diversity in impaired speakers.
The measures given in Table 7 are easily com-
puted from their respective formulae. In this work,
we lemmatize each word using NLTK (Bird et
al., 2009) before calculating the features. For
MATTR, we consider w = 10,20,30,40,50.
20
Honor
?
e?s statistic N
V
?0.165
/ where V is the number of
word types and N is the number of word tokens.
Brun
?
et?s index 100logN/(1?V
1
/V ) where V
1
is the
number of words used only once, V is the total
number of word types, and N is the number of
word tokens.
Type-token ratio (TTR) V/N where V is the num-
ber of word types and N is the number of word
tokens.
Moving-average type-token ratio (MATTR
w
) TTR
calculated over a moving window of size w,
and averaged over all windows.
Table 7: Vocabulary richness features.
3.3.7 Acoustic features
What we call acoustic features are extracted di-
rectly from the audio file. We consider the fea-
tures given in Table 8. Acoustic features have been
shown to be useful when automatically detecting
conditions such as Parkinson?s disease, in which
changes in speech are common (Little et al., 2009;
Tsanas et al., 2012). Acoustic features have also
been examined in studies of DAT (Meil?an et al.,
2014), FTD (Pakhomov et al., 2010b), and PPA
(Fraser et al., 2013b, whose software we use here).
An obvious benefit to acoustic features is that
they do not require a transcription, and can be cal-
culated immediately given an audio sample. The
corresponding drawback is that they tell us noth-
ing about the linguistic content of the narrative.
4 Experiments
We report the results of three experiments explor-
ing the discriminative power of the different fea-
tures. We first compare the classification accura-
cies using each individual feature set. We then per-
form an ablation study to determine which com-
bination of feature sets leads to the highest clas-
sification accuracy. We also look at individual
features across sets and discuss which ones are
the most discriminative, particularly in situations
where data might be limited.
4.1 Individual comparison of accuracy by set
The accuracies which result from using each fea-
ture set individually are given in Table 9. The
highest accuracy across the three tasks is achieved
in distinguishing SD participants from controls.
An accuracy of .963 can be achieved using all
the features together, or using the psycholinguis-
tic or POS features alone. This is consistent with
the semantic impairments that are observed in SD.
Total duration of speech Total length of all non-silent
segments
Phonation rate Total duration of speech / total dura-
tion of the sample (including pauses)
Mean pause duration Mean length of pauses > 0.15
ms
Short pause count # Pauses > 0.15 ms and < 0.4 ms
Long pause count # Pauses > 0.4 ms
Pause:word ratio Ratio of silent segments longer than
150 ms to non-silent segments
F
0:3
mean Mean of the fundamental frequency and the
first three formant frequencies
F
0:3
variance Variance of the fundamental frequency
and the first three formant frequencies
Mean instantaneous power Measure related to the
loudness of the signal
Mean 1st ACF Mean first autocorrelation function
Max 1st ACF Maximum first autocorrelation function
Skewness Measure of lack of symmetry, associated
with tense or creaky voice
Kurtosis Measure of the peakedness of the signal
ZCR Zero-crossing rate, can be used to distinguish
between voiced and unvoiced regions
MRPDE Mean recurrence period density entropy, a
measure of periodicity
Jitter Measure of the short-term variation in the pitch
(frequency) of a voice
Shimmer Measure of the short-term variation in the
loudness (amplitude) of a voice
Table 8: Acoustic features.
The measures of vocabulary richness do not distin-
guish between the SD and control groups, suggest-
ing it is the words themselves, and not the number
of different words being used, that is important.
In the case of PNFA participants vs. controls,
we find that the highest accuracy is achieved us-
ing all the features, and the second highest by us-
ing only acoustic features. This is not surprising,
considering that the acoustic features include mea-
sures of pausing and phonation rate, which can
detect the characteristic halting speech of PNFA.
The third best accuracy is achieved using the flu-
ency features, which also fits with this explana-
tion. However, we might have expected that the
complexity and CFG features would be more sen-
sitive to the grammatical impairments of PNFA.
Finally, the best accuracy for SD vs. PNFA
is lower than in the previous two cases, and is
achieved using only CFG features. This sug-
gests that there are some grammatical construc-
tions which occur with different frequencies in
the two groups. These differences do not appear
to be captured by the complexity features, which
could explain why Fraser et al. (2013a) did not find
syntactic differences between the SD and PNFA
groups. Interestingly, the results using CFG fea-
21
Feature set
SD vs.
controls
PNFA vs.
controls
SD vs.
PNFA
All .963 .931 .708
Acoustic .778 .862 .167
Psycholinguistic .963 .724 .708
POS .963 .690 .375
Complexity .852 .621 .667
Fluency .667 .828 .500
Vocab. richness .481 .586 .583
CFG .630 .690 .792
Table 9: Classification accuracies for each feature
set individually using a SVM classifier. Bold indi-
cates the highest accuracy for each task.
tures are actually higher than the results using all
features. This demonstrates that classifier perfor-
mance can be adversely affected by the presence
of irrelevant features, especially in small data sets.
4.2 Combining feature sets
In the previous section we examined the feature
sets individually; however, one type of feature
may complement the information contained in an-
other feature set, or it may contain redundant in-
formation. To examine the interactions between
the feature sets, we perform an ablation study.
Starting with all the features, we remove each fea-
ture set one at a time and measure the accuracy
of the classifier. The feature set whose removal
causes the smallest decrease in accuracy is then re-
moved permanently from the experiment, the rea-
soning being that the most important feature sets
will cause the greatest decrease in accuracy when
removed. In some cases, we observe that the clas-
sification accuracy actually increases when a set
is removed, which suggests that those features are
not relevant to the classification (at least in combi-
nation with the other sets). In the case of a tie, we
remove the feature set whose individual classifica-
tion accuracy on that task is lowest. The procedure
is then repeated on the remaining feature sets, con-
tinuing until only one set remains.
The results for SD vs. controls are given in Ta-
ble 10a. The best result, 1.00, is achieved by
combining the psycholinguistic and POS features.
This is unsurprising, since each of those feature
sets perform well individually. Curiously, the
same result can also be achieved by also including
the complexity, vocabulary richness, and CFG fea-
tures, but not in the intermediate stages between
those two optimal sets. We attribute this to the in-
teractions between features and the small data set.
For PNFA vs. controls, shown in Table 10b, the
(a) SD vs. controls.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .963
F A+P+POS+C+VR+CFG .963
A P+POS+C+VR+CFG 1.00
VR P+POS+C+CFG .926
CFG P+POS+C .926
C P+POS 1.00
POS P .963
(b) PNFA vs. controls.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .931
VR A+P+POS+C+F+CFG .931
C A+P+POS+F+CFG .931
POS A+P+F+CFG .931
CFG A+P+F .966
F A+P .966
P A .862
(c) SD vs. PNFA.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .708
POS A+P+C+F+VR+CFG .750
VR A+P+C+F+CFG .833
F A+P+C+CFG .833
A P+C+CFG .792
C P+CFG .917
P CFG .792
Table 10: A=acoustic, P=psycholinguistic,
POS=part-of-speech, C=complexity, F=fluency,
VR=vocabulary richness, CFG=CFG production
rule features. Bold indicates the highest accuracy
with the fewest feature sets.
best result of .966 is achieved using a combina-
tion of acoustic and psycholinguistic features. In
this case the removal of the fluency features, which
gave the second highest individual accuracy, does
not make a difference to the accuracy. This sug-
gests that the fluency features contain similar in-
formation to one of the remaining sets, presum-
ably the acoustic set.
In the case of SD vs. PNFA, we again see that
the best accuracy can be achieved by combining
two feature sets, as shown in Table 10c. Us-
ing psycholinguistic and CFG features, we can
achieve an accuracy of .917, a substantial im-
provement over the best accuracy for this task in
Table 9. In fact, in all three cases we see that us-
ing a carefully selected combination of feature sets
can result in better accuracy than using all the fea-
ture sets together or using any one set individually.
4.3 Best features for incomplete data
Up to this point, we have examined complete fea-
ture sets. We now briefly explore which individual
22
features are the most discriminative across feature
sets. We approach this as a practical consideration:
given the data that a researcher has, and limited re-
sources, what are the best features to measure? We
consider the following four scenarios:
1. Given audio files only. This scenario often
arises because it is relatively easy to record
speech, but difficult to have it transcribed.
Only acoustic features can be extracted.
2. Given basic transcriptions only (no audio).
We assume there is no sentence segmentation
and the time is not marked (e.g. as in the out-
put of automatic speech recognition). Thus,
we can measure psycholinguistic, POS, and
vocabulary measures. We can also measure
the fluency features except for verbal rate,
as well as mean word length and subordi-
nate/coordinate conjunctions from the com-
plexity set. Without sentence boundaries, we
cannot parse the transcripts.
3. Given fully segmented transcripts (no audio).
We can measure all features except for acous-
tic features.
4. Given audio and fully segmented transcripts.
We can measure all features.
For each scenario, we rank the available fea-
tures by their ?
2
value and choose the top 10 only
as input to the SVM classifier (see Manning et al.
(2008) for a complete explanation of ?
2
feature se-
lection). We only include features if ?
2
> 0, so in
cases where there are very few relevant features,
fewer than 10 features may be selected. Because
we perform cross-validation, the selected features
may vary across different folds. In the tables that
follow, we present the features ranked by the num-
ber of folds in which they appear (i.e. a feature
with the value 1.00 was selected in every fold).
Due to space constraints, only the top 10 ranked
features are shown.
The results for Scenario 1 are given in Ta-
ble 11a. For the SD vs. controls and PNFA vs.
controls, the most highly ranked features tend to
be related to fluency and rate of speech, as well
as voice quality (skewness and MRPDE). How-
ever, when distinguishing between the two patient
groups, the acoustic features are essentially use-
less. In most cases, we see that none of the acous-
tic features had a non-zero ?
2
value, and thus the
classifier could not be properly trained.
For Scenario 2 (Table 11b), the results for SD
vs. controls show that within the psycholinguistic
and POS feature sets, features relating to familiar-
ity and frequency are very important, as well as
nouns and demonstratives. In the PNFA vs. con-
trols case, we see that a number of the vocabulary
richness features are selected, which is in contrast
to the previous two experiments. However, it ap-
pears that only the MATTR feature is important
(with varying window lengths), so when we con-
sidered only full feature sets, that information was
obscured by the other, irrelevant features in that
set. The SD vs. PNFA case shows a mix of fea-
tures from the previous two cases.
For Scenario 3 (Table 11c), we add the com-
plexity and CFG features. These features do not
have a large effect in the SD vs. controls case, but
a few CFG features are selected in the PNFA vs.
controls and SD vs. PNFA cases.
In Scenario 4 (Table 11d), we consider all fea-
tures. In the SD vs. controls case this increases
the accuracy. However, for PNFA vs. controls and
SD vs. PNFA, the classification accuracy actually
decreases, relative to Scenario 3. When the num-
ber of features increases, the potential to overfit to
the training data fold also increases, and it seems
likely that that is occurring here. Nonetheless, we
expect that the features which are selected in every
fold are still highly relevant. These features are
unchanged between Scenarios 3 and 4 in the SD
vs. controls and SD vs. PNFA case, however in the
PNFA vs. controls case, the acoustic features are
now ranked more highly than some of the vocabu-
lary richness and CFG features from Scenario 3.
5 Discussion
While it may be tempting to calculate as many
features as possible and use them all in a classi-
fier, we have shown here that better results can be
achieved by choosing a small, relevant subset of
features. In particular, psycholinguistic features
such as frequency and familiarity were useful in all
three classification tasks. Acoustic features were
useful in discriminating patients from controls, but
not for discriminating between the two PPA sub-
types. We also found that MATTR was relevant
in some cases, although the other vocabulary rich-
ness features were not, and that the CFG features
were more useful than traditional measures of syn-
tactic complexity. POS features were useful only
in distinguishing between SD and controls.
One of the biggest challenges in this type
of work is the small amount of data available.
23
(a) Scenario 1: audio only.
SD vs. control, Acc: .852 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .500
1.00 skewness 1.00 long pause count .083 max 1st ACF
1.00 phonation rate 1.00 phonation rate .042 mean F3
1.00 MRPDE 1.00 short pause count
1.00 mean duration of pauses 1.00 MRPDE
.037 long pause count 1.00 mean duration of pauses
.037 mean 1st ACF .966 pause:word ratio
.037 kurtosis .793 skewness
.793 ZCR
.345 mean inst. power
.035 jitter
(b) Scenario 2: unsegmented transcripts.
SD vs. control, Acc: .926 PNFA vs. control, Acc: .621 SD vs. PNFA, Acc: .792
1.00 familiarity 1.00 MATTR 50 1.00 familiarity
1.00 noun frequency 1.00 MATTR 40 1.00 noun frequency
1.00 noun familiarity 1.00 MATTR 30 1.00 noun familiarity
1.00 frequency 1.00 frequency 1.00 MATTR 20
1.00 verb frequency 1.00 MATTR 20 .708 MATTR 10
1.00 nouns .931 total words .208 MATTR 30
1.00 demonstratives .759 light verbs .042 MATTR 50
.778 pronoun ratio .690 adjectives .042 MATTR 40
.667 noun imageability .241 age of acquisition .042 light verbs
.630 Honor?e?s statistic .241 MATTR 10 .042 verbs
(c) Scenario 3: segmented transcripts.
SD vs. control, Acc: .926 PNFA vs. control, Acc: .897 SD vs. PNFA, Acc: .792
1.00 word length 1.00 MATTR 50 1.00 WHADVP?WRB
1.00 familiarity 1.00 MATTR 40 1.00 familiarity
1.00 noun frequency 1.00 WHNP?WP 1.00 noun familiarity
1.00 noun familiarity 1.00 frequency 1.00 noun frequency
1.00 frequency 1.00 MATTR 20 1.00 MATTR 20
1.00 demonstratives 1.00 verbal rate 1.00 NP? NNS
.889 nouns .966 MATTR 30 1.00 SBAR?WHADVP S
.852 verb frequency .827 S1? INTJ .667 MATTR 10
.630 MLS .483 total words .500 NP? DT JJ NNS
.630 total Yngve depth .414 word length .458 SQ? AUX NP VP
(d) Scenario 4: segmented transcripts + audio.
SD vs. control, Acc: .963 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .750
1.00 word length 1.00 frequency 1.00 WHADVP?WRB
1.00 familiarity 1.00 phonation rate 1.00 familiarity
1.00 noun frequency 1.00 MRPDE 1.00 noun familiarity
1.00 noun familiarity 1.00 verbal rate 1.00 noun frequency
1.00 frequency 1.00 mean duration of pauses 1.00 MATTR 20
1.00 demonstratives .897 MATTR 50 1.00 NP? NNS
.963 phonation rate .897 WHNP?WP 1.00 SBAR?WHADVP S
.741 verb frequency .897 MATTR 20 .625 MATTR 10
.593 nouns .690 MATTR 40 .500 NP? DT JJ NNS
.333 MLS .690 MATTR 30 .458 SQ? AUX NP VP
Table 11: Classification accuracies and top 10 features for four different data scenarios.
Psychological studies are typically on the or-
der of only tens to possibly hundreds of partic-
ipants, while machine learning researchers often
tackle problems with thousands to millions of data
points. We have chosen techniques appropriate for
small data sets, but acknowledging the potential
weaknesses of machine learning methods when
training data are limited, these findings must be
considered preliminary. However, we also believe
that this is a promising approach for future ap-
plications, including automated screening for lan-
guage impairment, support for clinical diagnosis,
tracking severity of symptoms over time, and eval-
uating therapeutic interventions.
Acknowledgments
This research was supported by the Natural Sciences and En-
gineering Research Council of Canada and the Canadian In-
stitutes of Health Research (grant #MOP-8277). Thanks to
Frank Rudzicz for the acoustic features software.
24
References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
language processing with Python. O?Reilly Media, Inc.
Sarah D. Breedin, Eleanor M. Saffran, and Myrna F.
Schwartz. 1998. Semantic factors in verb retrieval: An
effect of complexity. Brain and Language, 63:1?31.
Marc Brysbaert and Boris New. 2009. Moving beyond
Ku?cera and Francis: A critical evaluation of current word
frequency norms and the introduction of a new and im-
proved word frequency measure for American English.
Behavior Research Methods, 41(4):977?990.
R.S. Bucks, S. Singh, J.M. Cuerden, and G.K. Wilcock.
2000. Analysis of spontaneous, conversational speech in
dementia of Alzheimer type: Evaluation of an objective
technique for analysing lexical performance. Aphasiol-
ogy, 14(1):71?91.
Jieun Chae and Ani Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies of ma-
chine translation and human-written text. In Proceedings
of the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 139?147.
Association for Computational Linguistics.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh and
um in spontaneous speaking. Cognition, 84(1):73?111.
Michael A. Covington and Joe D. McFall. 2010. Cutting
the Gordian knot: The moving-average type?token ratio
(MATTR). Journal of Quantitative Linguistics, 17(2):94?
100.
Gerasimos Fergadiotis and Heather Harris Wright. 2011.
Lexical diversity for adults with and without apha-
sia across discourse elicitation tasks. Aphasiology,
25(11):1414?1430.
Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, Carol
Leonard, Graeme Hirst, Sandra E. Black, and Elizabeth
Rochon. 2013a. Automated classification of primary
progressive aphasia subtypes from narrative speech tran-
scripts. Cortex.
Kathleen C. Fraser, Frank Rudzicz, and Elizabeth Rochon.
2013b. Using text and acoustic features to diagnose pro-
gressive aphasia and its subtypes. In Proceedings of Inter-
speech.
K.J. Gilhooly and R.H. Logie. 1980. Age-of-acquisition, im-
agery, concreteness, familiarity, and ambiguity measures
for 1,944 words. Behavior Research Methods, 12:395?
427.
M.L. Gorno-Tempini, A.E. Hillis, S. Weintraub, A. Kertesz,
M. Mendez, S.F. Cappa, J.M. Ogar, J.D. Rohrer, S. Black,
B.F. Boeve, F. Manes, N.F. Dronkers, R. Vandenberghe,
K. Rascovsky, K. Patterson, B.L. Miller, D.S. Knopman,
J.R. Hodges, M.M. Mesulam, and M. Grossman. 2011.
Classification of primary progressive aphasia and its vari-
ants. Neurology, 76:1006?1014.
William Jarrold, Bart Peintner, Eric Yeh, Ruth Krasnow,
Harold Javitz, and Gary Swan. 2010. Language ana-
lytics for assessing brain health: Cognitive impairment,
depression and pre-symptomatic Alzheimer?s disease. In
Yiyu Yao, Ron Sun, Tomaso Poggio, Jiming Liu, Ning
Zhong, and Jimmy Huang, editors, Brain Informatics, vol-
ume 6334 of Lecture Notes in Computer Science, pages
299?307. Springer Berlin / Heidelberg.
Susan Kemper, Marilyn Thompson, and Janet Marquis.
2001. Longitudinal change in language production: Ef-
fects of aging and dementia on grammatical complex-
ity and propositional content. Psychology and Aging,
16(4):600?614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st Meeting
of the Association for Computational Linguistics, pages
423?430.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina Jokel.
2011. Longitudinal detection of dementia through lex-
ical and syntactic changes in writing: a case study of
three British novelists. Literary and Linguistic Comput-
ing, 26(4):435?461.
Max A. Little, Patrick E. McSharry, Eric J. Hunter, Jennifer
Spielman, and Lorraine O. Ramig. 2009. Suitability
of dysphonia measurements for telemonitoring of Parkin-
son?s disease. Biomedical Engineering, IEEE Transac-
tions on, 56(4):1015?1022.
Xiaofei Lu. 2010. Automatic analysis of syntactic complex-
ity in second language writing. International Journal of
Corpus Linguistics, 15(4):474?496.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich
Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Juan Jos?e G. Meil?an, Francisco Mart??nez-S?anchez, Juan
Carro, Dolores E. L?opez, Lymarie Millian-Morell, and
Jos?e M. Arana. 2014. Speech in Alzheimer?s disease:
Can temporal and acoustic parameters discriminate de-
mentia? Dementia and Geriatric Cognitive Disorders,
37(5-6):327?334.
Serguei V.S. Pakhomov, Glen E. Smith, Susan Marino, An-
gela Birnbaum, Neill Graff-Radford, Richard Caselli,
Bradley Boeve, and David D. Knopman. 2010a. A com-
puterized technique to asses language use patterns in pa-
tients with frontotemporal dementia. Journal of Neurolin-
guistics, 23:127?144.
S.V. Pakhomov, G.E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman. 2010b.
Computerized analysis of speech and language to identify
psycholinguistic correlates of frontotemporal lobar degen-
eration. Cognitive and Behavioral Neurology, 23:165?
177.
Hyejin Park, Yvonne Rogalski, Amy D. Rodriguez, Zvinka
Zlatar, Michelle Benjamin, Stacy Harnish, Jeffrey Ben-
nett, John C. Rosenbek, Bruce Crosson, and Jamie Reilly.
2011. Perceptual cues used by listeners to discriminate
fluent from nonfluent narrative discourse. Aphasiology,
25(9):998?1015.
Bart Peintner, William Jarrold, Dimitra Vergyri, Colleen
Richey, Maria Luisa Gorno Tempini, and Jennifer Ogar.
2008. Learning diagnostic models using speech and lan-
guage measures. In Engineering in Medicine and Biol-
ogy Society, 2008. EMBS 2008. 30th Annual International
Conference of the IEEE, pages 4648?4651.
Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy
Hollingshead, and Jeffery Kaye. 2011. Spoken language
derived measures for detecting mild cognitive impairment.
IEEE Transactions on Audio, Speech, and Language Pro-
cessing, 19(7):2081?2090.
25
Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.
Schwartz. 1989. The quantitative analysis of agrammatic
production: procedure and data. Brain and Language,
37:440?479.
Sameer Singh, Romola S. Bucks, and Joanne M. Cuerden.
2001. Evaluation of an objective technique for analysing
temporal variables in DAT spontaneous speech. Aphasiol-
ogy, 15(6):571?583.
Hans Stadthagen-Gonzalez and Colin J. Davis. 2006. The
Bristol norms for age of acquisition, imageability, and fa-
miliarity. Behavior Research Methods, 38(4):598?605.
Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth Rock-
wood, and Elissa Asp. 2005. Automatic detection and
rating of dementia of Alzheimer type through lexical anal-
ysis of spontaneous speech. In Proceedings of the IEEE
International Conference on Mechatronics and Automa-
tion, pages 1569?1574.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies, pages 252?259.
Athanasios Tsanas, Max A. Little, Patrick E. McSharry, Jen-
nifer Spielman, and Lorraine O. Ramig. 2012. Novel
speech signal processing algorithms for high-accuracy
classification of Parkinson?s disease. IEEE Transactions
on Biomedical Engineering, 59(5):1264?1271.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features
for sentence grammaticality classification. In Proceed-
ings of the Australasian Language Technology Association
Workshop, pages 67?75.
Victor Yngve. 1960. A model and hypothesis for language
structure. Proceedings of the American Physical Society,
104:444?466.
26
