Proceedings of ACL-08: HLT, pages 139?147,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Re-examination of Query Expansion Using Lexical Resources
Hui Fang
Department of Computer Science and Engineering
The Ohio State University
Columbus, OH, 43210
hfang@cse.ohio-state.edu
Abstract
Query expansion is an effective technique to
improve the performance of information re-
trieval systems. Although hand-crafted lexi-
cal resources, such as WordNet, could provide
more reliable related terms, previous stud-
ies showed that query expansion using only
WordNet leads to very limited performance
improvement. One of the main challenges is
how to assign appropriate weights to expanded
terms. In this paper, we re-examine this prob-
lem using recently proposed axiomatic ap-
proaches and find that, with appropriate term
weighting strategy, we are able to exploit
the information from lexical resources to sig-
nificantly improve the retrieval performance.
Our empirical results on six TREC collec-
tions show that query expansion using only
hand-crafted lexical resources leads to signif-
icant performance improvement. The perfor-
mance can be further improved if the proposed
method is combined with query expansion us-
ing co-occurrence-based resources.
1 Introduction
Most information retrieval models (Salton et al,
1975; Fuhr, 1992; Ponte and Croft, 1998; Fang
and Zhai, 2005) compute relevance scores based on
matching of terms in queries and documents. Since
various terms can be used to describe a same con-
cept, it is unlikely for a user to use a query term that
is exactly the same term as used in relevant docu-
ments. Clearly, such vocabulary gaps make the re-
trieval performance non-optimal. Query expansion
(Voorhees, 1994; Mandala et al, 1999a; Fang and
Zhai, 2006; Qiu and Frei, 1993; Bai et al, 2005;
Cao et al, 2005) is a commonly used strategy to
bridge the vocabulary gaps by expanding original
queries with related terms. Expanded terms are of-
ten selected from either co-occurrence-based the-
sauri (Qiu and Frei, 1993; Bai et al, 2005; Jing and
Croft, 1994; Peat and Willett, 1991; Smeaton and
van Rijsbergen, 1983; Fang and Zhai, 2006) or hand-
crafted thesauri (Voorhees, 1994; Liu et al, 2004) or
both (Cao et al, 2005; Mandala et al, 1999b).
Intuitively, compared with co-occurrence-based
thesauri, hand-crafted thesauri, such as WordNet,
could provide more reliable terms for query ex-
pansion. However, previous studies failed to show
any significant gain in retrieval performance when
queries are expanded with terms selected from
WordNet (Voorhees, 1994; Stairmand, 1997). Al-
though some researchers have shown that combin-
ing terms from both types of resources is effective,
the benefit of query expansion using only manually
created lexical resources remains unclear. The main
challenge is how to assign appropriate weights to the
expanded terms.
In this paper, we re-examine the problem of
query expansion using lexical resources with the
recently proposed axiomatic approaches (Fang and
Zhai, 2006). The major advantage of axiomatic ap-
proaches in query expansion is to provide guidance
on how to weight related terms based on a given
term similarity function. In our previous study, a co-
occurrence-based term similarity function was pro-
posed and studied. In this paper, we study several
term similarity functions that exploit various infor-
mation from two lexical resources, i.e., WordNet
139
and dependency-thesaurus constructed by Lin (Lin,
1998), and then incorporate these similarity func-
tions into the axiomatic retrieval framework. We
conduct empirical experiments over several TREC
standard collections to systematically evaluate the
effectiveness of query expansion based on these sim-
ilarity functions. Experiment results show that all
the similarity functions improve the retrieval per-
formance, although the performance improvement
varies for different functions. We find that the most
effective way to utilize the information from Word-
Net is to compute the term similarity based on the
overlap of synset definitions. Using this similarity
function in query expansion can significantly im-
prove the retrieval performance. According to the
retrieval performance, the proposed similarity func-
tion is significantly better than simple mutual infor-
mation based similarity function, while it is compa-
rable to the function proposed in (Fang and Zhai,
2006). Furthermore, we show that the retrieval per-
formance can be further improved if the proposed
similarity function is combined with the similar-
ity function derived from co-occurrence-based re-
sources.
The main contribution of this paper is to re-
examine the problem of query expansion using lexi-
cal resources with a new approach. Unlike previous
studies, we are able to show that query expansion us-
ing only manually created lexical resources can sig-
nificantly improve the retrieval performance.
The rest of the paper is organized as follows. We
discuss the related work in Section 2, and briefly re-
view the studies of query expansion using axiomatic
approaches in Section 3. We then present our study
of using lexical resources, such as WordNet, for
query expansion in Section 4, and discuss experi-
ment results in Section 5. Finally, we conclude in
Section 6.
2 Related Work
Although the use of WordNet in query expansion
has been studied by various researchers, the im-
provement of retrieval performance is often lim-
ited. Voorhees (Voorhees, 1994) expanded queries
using a combination of synonyms, hypernyms and
hyponyms manually selected from WordNet, and
achieved limited improvement (i.e., around ?2% to
+2%) on short verbose queries. Stairmand (Stair-
mand, 1997) used WordNet for query expansion, but
they concluded that the improvement was restricted
by the coverage of the WordNet and no empirical
results were reported.
More recent studies focused on combining the in-
formation from both co-occurrence-based and hand-
crafted thesauri. Mandala et. al. (Mandala et al,
1999a; Mandala et al, 1999b) studied the problem
in vector space model, and Cao et. al. (Cao et al,
2005) focused on extending language models. Al-
though they were able to improve the performance,
it remains unclear whether using only information
from hand-crafted thesauri would help to improve
the retrieval performance.
Another way to improve retrieval performance
using WordNet is to disambiguate word senses.
Voorhees (Voorhees, 1993) showed that using Word-
Net for word sense disambiguation degrade the re-
trieval performance. Liu et. al. (Liu et al, 2004)
used WordNet for both sense disambiugation and
query expansion and achieved reasonable perfor-
mance improvement. However, the computational
cost is high and the benefit of query expansion using
only WordNet is unclear. Ruch et. al. (Ruch et al,
2006) studied the problem in the domain of biology
literature and proposed an argumentative feedback
approach, where expanded terms are selected from
only sentences classified into one of four disjunct
argumentative categories.
The goal of this paper is to study whether query
expansion using only manually created lexical re-
sources could lead to the performance improve-
ment. The main contribution of our work is to
show query expansion using only hand-crafted lex-
ical resources is effective in the recently proposed
axiomatic framework, which has not been shown in
the previous studies.
3 Query Expansion in Axiomatic Retrieval
Model
Axiomatic approaches have recently been proposed
and studied to develop retrieval functions (Fang and
Zhai, 2005; Fang and Zhai, 2006). The main idea is
to search for a retrieval function that satisfies all the
desirable retrieval constraints, i.e., axioms. The un-
derlying assumption is that a retrieval function sat-
140
isfying all the constraints would perform well em-
pirically. Unlike other retrieval models, axiomatic
retrieval models directly model the relevance with
term level retrieval constraints.
In (Fang and Zhai, 2005), several axiomatic re-
trieval functions have been derived based on a set of
basic formalized retrieval constraints and an induc-
tive definition of the retrieval function space. The
derived retrieval functions are shown to perform as
well as the existing retrieval functions with less pa-
rameter sensitivity. One of the components in the
inductive definition is primitive weighting function,
which assigns the retrieval score to a single term
document {d} for a single term query {q} based on
S({q}, {d}) =
{
?(q) q = d
0 q 6= d (1)
where ?(q) is a term weighting function of q. A lim-
itation of the primitive weighting function described
in Equation 1 is that it can not bridge vocabulary
gaps between documents and queries.
To overcome this limitation, in (Fang and Zhai,
2006), we proposed a set of semantic term match-
ing constraints and modified the previously derived
axiomatic functions to make them satisfy these ad-
ditional constraints. In particular, the primitive
weighting function is generalized as
S({q}, {d}) = ?(q) ? f(s(q, d)),
where s(q, d) is a semantic similarity function be-
tween two terms q and d, and f is a monotonically
increasing function defined as
f(s(q, d)) =
{
1 q = d
s(q,d)
s(q,q) ? ? q 6= d
(2)
where ? is a parameter that regulates the weighting
of the original query terms and the semantically sim-
ilar terms. We have shown that the proposed gen-
eralization can be implemented as a query expan-
sion method. Specifically, the expanded terms are
selected based on a term similarity function s and
the weight of an expanded term t is determined by
its term similarity with a query term q, i.e., s(q, t),
as well as the weight of the query term, i.e., ?(q).
Note that the weight of an expanded term t is ?(t)
in traditional query expansion methods.
In our previous study (Fang and Zhai, 2006), term
similarity function s is derived based on the mutual
information of terms over collections that are con-
structed under the guidance of a set of term semantic
similarity constraints. The focus of this paper is to
study and compare several term similarity functions
exploiting the information from lexical resources,
and evaluate their effectiveness in the axiomatic re-
trieval models.
4 Term Similarity based on Lexical
Resources
In this section, we discuss a set of term similar-
ity functions that exploit the information stored in
two lexical resources: WordNet (Miller, 1990) and
dependency-based thesaurus (Lin, 1998).
The most commonly used lexical resource is
WordNet (Miller, 1990), which is a hand-crafted
lexical system developed at Princeton University.
Words are organized into four taxonomies based on
different parts of speech. Every node in the WordNet
is a synset, i.e., a set of synonyms. The definition of
a synset, which is referred to as gloss, is also pro-
vided. For a query term, all the synsets in which the
term appears can be returned, along with the defi-
nition of the synsets. We now discuss six possible
term similarity functions based on the information
provided by WordNet.
Since the definition provides valuable information
about the semantic meaning of a term, we can use
the definitions of the terms to measure their semantic
similarity. The more common words the definitions
of two terms have, the more similar these terms are
(Banerjee and Pedersen, 2005). Thus, we can com-
pute the term semantic similarity based on synset
definitions in the following way:
sdef (t1, t2) =
|D(t1) ? D(t2)|
|D(t1) ? D(t2)|
,
where D(t) is the concatenation of the definitions
for all the synsets containing term t and |D| is the
number of words of the set D.
Within a taxonomy, synsets are organized by their
lexical relations. Thus, given a term, related terms
can be found in the synsets related to the synsets
containing the term. In this paper, we consider the
following five word relations.
141
? Synonym(Syn): X and Y are synonyms if they
are interchangeable in some context.
? Hypernym(Hyper): Y is a hypernym of X if X
is a (kind of) Y.
? Hyponym(Hypo): X is a hyponym of Y if X is
a (kind of) Y.
? Holonym(Holo): Y is a holonym of Y if X is a
part of Y.
? Meronym(Mero): X is a meronym of Y if X is
a part of Y.
Since these relations are binary, we define the term
similarity functions based on these relations in the
following way.
sR(t1, t2) =
{
?R t1 ? TR(t2)
0 t1 /? TR(t2)
where R ? {syn, hyper, hypo, holo,mero}, TR(t)
is a set of words that are related to term t based on
the relation R, and ?s are non-zero parameters to
control the similarity between terms based on differ-
ent relations. However, since the similarity values
for all term pairs are same, the values of these pa-
rameters can be ignored when we use Equation 2 in
query expansion.
Another lexical resource we study in the paper is
the dependency-based thesaurus provided by Lin 1
(Lin, 1998). The thesaurus provides term similar-
ities that are automatically computed based on de-
pendency relationships extracted from a parsed cor-
pus. We define a similarity function that can utilize
this thesaurus as follows:
sLin(t1, t2) =
{
L(t1, t2) (t1, t2) ? TPLin
0 (t1, t2) /? TPLin
where L(t1, t2) is the similarity of terms stored in
the dependency-based thesaurus and TPLin is a set
of all the term pairs stored in the thesaurus. The
similarity of two terms would be assigned to zero if
we can not find the term pair in the thesaurus.
Since all the similarity functions discussed above
capture different perspectives of term relations, we
1Available at http://www.cs.ualberta.ca/?lindek/downloads.htm
propose a simple strategy to combine these similar-
ity functions so that the similarity of a term pair is
the highest similarity value of these two terms of
all the above similarity functions, which is shown
as follows.
scombined(t1, t2) = maxR?Rset(sR(t1, t2)),
where
Rset = {def, syn, hyper, hypo, holo,mero, Lin}.
In summary, we have discussed eight possible
similarity functions that exploit the information
from the lexical resources. We then incorporate
these similarity functions into the axiomatic retrieval
models based on Equation 2, and perform query ex-
pansion based on the procedure described in Section
3. The empirical results are reported in Section 5.
5 Experiments
In this section, we experimentally evaluate the effec-
tiveness of query expansion with the term similar-
ity functions discussed in Section 4 in the axiomatic
framework. Experiment results show that the sim-
ilarity function based on synset definitions is most
effective. By incorporating this similarity function
into the axiomatic retrieval models, we show that
query expansion using the information from only
WordNet can lead to significant improvement of re-
trieval performance, which has not been shown in
the previous studies (Voorhees, 1994; Stairmand,
1997).
5.1 Experiment Design
We conduct three sets of experiments. First, we
compare the effectiveness of term similarity func-
tions discussed in Section 4 in the context of
query expansion. Second, we compare the best
one with the term similarity functions derived from
co-occurrence-based resources. Finally, we study
whether the combination of term similarity func-
tions from different resources can further improve
the performance.
All experiments are conducted over six TREC
collections: ap88-89, doe, fr88-89, wt2g, trec7 and
trec8. Table 1 shows some statistics of the collec-
tions, including the description, the collection size,
142
Table 1: Statistics of Test Collections
Collection Description Size # Voc. # Doc. #query
ap88-89 news articles 491MB 361K 165K 150
doe technical reports 184MB 163K 226K 35
fr88-89 government documents 469MB 204K 204K 42
trec7 ad hoc data 2GB 908K 528K 50
trec8 ad hoc data 2GB 908K 528K 50
wt2g web collections 2GB 1968K 247K 50
the vocabulary size, the number of documents and
the number of queries. The preprocessing only in-
volves stemming with Porter?s stemmer.
We use WordNet 3.0 2, Lemur Toolkit 3 and
TrecWN library 4 in experiments. The results are
evaluated with both MAP (mean average preci-
sion) and gMAP (geometric mean average preci-
sion) (Voorhees, 2005), which emphasizes the per-
formance of difficulty queries.
There is one parameter ? in the query expansion
method presented in Section 3. We tune the value of
? and report the best performance. The parameter
sensitivity is similar to the observations described in
(Fang and Zhai, 2006) and will not be discussed in
this paper. In all the result tables, ? and ? indicate
that the performance difference is statistically sig-
nificant according to Wilcoxon signed rank test at
the level of 0.05 and 0.1 respectively.
We now explain the notations of different meth-
ods. BL is the baseline method without query ex-
pansion. In this paper, we use the best performing
function derived in axiomatic retrieval models, i.e,
F2-EXP in (Fang and Zhai, 2005) with a fixed pa-
rameter value (b = 0.5). QEX is the query expan-
sion method with term similarity function sX , where
X could be Def., Syn., Hyper., Hypo., Mero., Holo.,
Lin and Combined.
Furthermore, we examine the query expansion
method using co-occurrence-based resources. In
particular, we evaluate the retrieval performance us-
ing the following two similarity functions: sMIBL
and sMIImp. Both functions are based on the mutual
information of terms in a set of documents. sMIBL
uses the collection itself to compute the mutual in-
formation, while sMIImp uses the working sets con-
2http://wordnet.princeton.edu/
3http://www.lemurproject.org/
4http://l2r.cs.uiuc.edu/ cogcomp/software.php
structed based on several constraints (Fang and Zhai,
2006). The mutual information of two terms t1 and
t2 in collection C is computed as follow (van Rijs-
bergen, 1979):
I(Xt1 ,Xt2) =
?
p(Xt1 ,Xt2)log
p(Xt1 ,Xt2)
p(Xt1)p(Xt2)
Xti is a binary random variable corresponding to the
presence/absence of term ti in each document of col-
lection C .
5.2 Effectiveness of Lexical Resources
We first compare the retrieval performance of query
expansion with different similarity functions us-
ing short keyword (i.e., title-only) queries, because
query expansion techniques are often more effective
for shorter queries (Voorhees, 1994; Fang and Zhai,
2006). The results are presented in Table 2. It is
clear that query expansion with these functions can
improve the retrieval performance, although the per-
formance gains achieved by different functions vary
a lot. In particular, we make the following observa-
tions.
First, the similarity function based on synset def-
initions is the most effective one. QEdef signifi-
cantly improves the retrieval performance for all the
data sets. For example, in trec7, it improves the per-
formance from 0.186 to 0.216. As far as we know,
none of the previous studies showed such significant
performance improvement by using only WordNet
as query expansion resource.
Second, the similarity functions based on term re-
lations are less effective compared with definition-
based similarity function. We think that the worse
performance is related to the following two reasons:
(1) The similarity functions based on relations are
binary, which is not a good way to model term sim-
ilarities. (2) The relations are limited by the part
143
Table 2: Performance of query expansion using lexical resources (short keyword queries)
trec7 trec8 wt2g
MAP gMAP MAP gMAP MAP gMAP
BL 0.186 0.083 0.250 0.147 0.282 0.188
QEdef 0.216? 0.105? 0.266? 0.164? 0.301? 0.210?
(+16%) (+27%) (+6.4%) (+12%) (+6.7%) (+12%)
QEsyn 0.194 0.085? 0.252? 0.150? 0.287? 0.194?
(+4.3%) (+2.4%) (+0.8%) (+2.0%) (+1.8%) (+3.2%)
QEhyper 0.186 0.086 0.250 0.152 0.286? 0.192?
(0%) (+3.6%) (0%) (+3.4%) (+1.4%) (+2.1%)
QEhypo 0.186? 0.085? 0.250 0.147 0.282? 0.190
(0%) (+2.4%) (0%) (0%) (0%) (+1.1%)
QEmero 0.187? 0.084? 0.250 0.147 0.282 0.189
(+0.5%) (+1.2%) (0%) (0%) (0%) (+0.5%)
QEholo 0.191? 0.085? 0.250 0.147 0.282 0.188
(+2.7%) (+2.4%) (0%) (0%) (0%) (0%)
QELin 0.193? 0.092? 0.256? 0.156? 0.290? 0.200?
(+3.7%) (+11%) (+2.4%) (+6.1%) (+2.8%) (+6.4%)
QECombined 0.214? 0.104? 0.267? 0.165? 0.300? 0.208?
(+15%) (+25%) (+6.8%) (+12%) (+6.4%) (+10.5%)
ap88-89 doe fr88-89
MAP gMAP MAP gMAP MAP gMAP
BL 0.220 0.074 0.174 0.069 0.222 0.062
QEdef 0.254? 0.088? 0.181? 0.075? 0.225? 0.067?
(+15%) (+19%) (+4%) (+10%) (+1.4%) (+8.1%)
QEsyn 0.222? 0.077? 0.174 0.074 0.222 0.065
(+0.9%) (+4.1%) (0%) (+7.3%) (0%) (+4.8%)
QEhyper 0.222? 0.074 0.175 0.070 0.222 0.062
(+0.9%) (0%) (+0.5%) (+1.5%) (0%) (0%)
QEhypo 0.222? 0.076? 0.176? 0.073? 0.222 0.062
(+0.9%) (+2.7%) (+1.1%) (+5.8%) (0%) (0%)
QEmero 0.221 0.074? 0.174? 0.070? 0.222 0.062
(+0.45%) (0%) (0%) (+1.5%) (0%) (0%)
QEholo 0.221 0.076 0.177? 0.073 0.222 0.062
(+0.45%) (+2.7%) (+1.7%) (+5.8%) (0%) (0%)
QELin 0.245? 0.082? 0.178 0.073 0.222 0.067?
(+11%) (+11%) (+2.3%) (+5.8%) (0%) (+8.1%)
QECombined 0.254? 0.085? 0.179? 0.074? 0.223? 0.065
(+15%) (+12%) (+2.9%) (+7.3%) (+0.5%) (+4.3%)
144
Table 3: Performance comparison of hand-crafted and co-occurrence-based thesauri (short keyword queries)
Data MAP gMAP
QEdef QEMIBL QEMIImp QEdef QEMIBL QEMIImp
ap88-89 0.254 0.233? 0.265? 0.088 0.081? 0.089?
doe 0.181 0.175? 0.183 0.075 0.071? 0.078
fr88-89 0.225 0.222? 0.227? 0.067 0.063 0.071?
trec7 0.216 0.195? 0.236? 0.105 0.089? 0.097
trec8 0.266 0.250? 0.278 0.164 0.148? 0.172
wt2g 0.301 0.311 0.320? 0.210 0.218 0.219?
of speech of the terms, because two terms in Word-
Net are related only when they have the same part
of speech tags. However, definition-based similarity
function does not have such a limitation.
Third, the similarity function based on Lin?s the-
saurus is more effective than those based on term
relations from the WordNet, while it is less effective
compared with the definition-based similarity func-
tion, which might be caused by its smaller coverage.
Finally, combining different WordNet-based sim-
ilarity functions does not help, which may indicate
that the expanded terms selected by different func-
tions are overlapped.
5.3 Comparison with Co-occurrence-based
Resources
As shown in Table 2, the similarity function based
on synset definitions, i.e., sdef , is most effective. We
now compare the retrieval performance of using this
similarity function with that of using the mutual in-
formation based functions, i.e., sMIBL and sMIImp.
The experiments are conducted over two types of
queries, i.e. short keyword (keyword title) and short
verbose (one sentence description) queries.
The results for short keyword queries are shown
in Table 3. The retrieval performance of query ex-
pansion based on sdef is significantly better than
that based on sMIBL on almost all the data sets,
while it is slightly worse than that based on sMIImp
on some data sets. We can make the similar ob-
servation from the results for short verbose queries
as shown in Table 4. One advantage of sdef over
sMIImp is the computational cost, because sdef can
be computed offline in advance while sMIImp has to
be computed online from query-dependent working
sets which takes much more time. The low computa-
tional cost and high retrieval performance make sdef
more attractive in the real world applications.
5.4 Additive Effect
Since both types of similarity functions are able
to improve retrieval performance, we now study
whether combining them could lead to better per-
formance. Table 5 shows the retrieval performance
of combining both types of similarity functions for
short keyword queries. The results for short verbose
queries are similar. Clearly, combining the similar-
ity functions from different resources could further
improve the performance.
6 Conclusions
Query expansion is an effective technique in in-
formation retrieval to improve the retrieval perfor-
mance, because it often can bridge the vocabulary
gaps between queries and documents. Intuitively,
hand-crafted thesaurus could provide reliable related
terms, which would help improve the performance.
However, none of the previous studies is able to
show significant performance improvement through
query expansion using information only from man-
ually created lexical resources.
In this paper, we re-examine the problem of query
expansion using lexical resources in recently pro-
posed axiomatic framework and find that we are
able to significantly improve retrieval performance
through query expansion using only hand-crafted
lexical resources. In particular, we first study a
few term similarity functions exploiting the infor-
mation from two lexical resources: WordNet and
dependency-based thesaurus created by Lin. We
then incorporate the similarity functions with the
query expansion method in the axiomatic retrieval
145
Table 4: Performance Comparison (MAP, short verbose queries)
Data BL QEdef QEMIBL QEMIImp
ap88-89 0.181 0.220? (21.5%) 0.205? (13.3%) 0.230? (27.1%)
doe 0.109 0.121? (11%) 0.119 (9.17%) 0.117 (7.34%)
fr88-89 0.146 0.164? (12.3%) 0.162? (11%) 0.164? (12.3%)
trec7 0.184 0.209? (13.6%) 0.196 (6.52%) 0.224?(21.7%)
trec8 0.234 0.238?(1.71%) 0.235 (0.4%) 0.243? (3.85%)
wt2g 0.266 0.276 (3.76%) 0.276? (3.76%) 0.282? (6.02%)
Table 5: Additive Effect (MAP, short keyword queries)
ap88-89 doe fr88-89 trec7 trec8 wt2g
QEMIBL 0.233 0.175 0.222 0.195 0.250 0.311
QEdef+MIBL 0.257? 0.183? 0.225? 0.217? 0.267? 0.320?
QEMIImp 0.265 0.183 0.227 0.236 0.278 0.320
QEdef+MIImp 0.269? 0.187 0.232? 0.237? 0.280? 0.322?
models. Systematical experiments have been con-
ducted over six standard TREC collections and show
promising results. All the proposed similarity func-
tions improve the retrieval performance, although
the degree of improvement varies for different sim-
ilarity functions. Among all the functions, the one
based on synset definition is most effective and is
able to significantly and consistently improve re-
trieval performance for all the data sets. This simi-
larity function is also compared with some similarity
functions using mutual information. Furthermore,
experiment results show that combining similarity
functions from different resources could further im-
prove the performance.
Unlike previous studies, we are able to show that
query expansion using only manually created the-
sauri can lead to significant performance improve-
ment. The main reason is that the axiomatic ap-
proach provides guidance on how to appropriately
assign weights to expanded terms.
There are many interesting future research direc-
tions based on this work. First, we will study the
same problem in some specialized domain, such as
biology literature, to see whether the proposed ap-
proach could be generalized to the new domain.
Second, the fact that using axiomatic approaches to
incorporate linguistic information can improve re-
trieval performance is encouraging. We plan to ex-
tend the axiomatic approach to incorporate more
linguistic information, such as phrases and word
senses, into retrieval models to further improve the
performance.
Acknowledgments
We thank ChengXiang Zhai, Dan Roth, Rodrigo de
Salvo Braz for valuable discussions. We also thank
three anonymous reviewers for their useful com-
ments.
References
J. Bai, D. Song, P. Bruza, J. Nie, and G. Cao. 2005.
Query expansion using term relationships in language
models for information retrieval. In Fourteenth Inter-
national Conference on Information and Knowledge
Management (CIKM 2005).
S. Banerjee and T. Pedersen. 2005. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the 18th International Joint Conference on Ar-
tificial Intelligence.
G. Cao, J. Nie, and J. Bai. 2005. Integrating word rela-
tionships into language models. In Proceedings of the
2005 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
H. Fang and C. Zhai. 2005. An exploration of axiomatic
approaches to information retrieval. In Proceedings
of the 2005 ACM SIGIR Conference on Research and
Development in Information Retrieval.
H. Fang and C. Zhai. 2006. Semantic term matching
in axiomatic approaches to information retrieval. In
Proceedings of the 2006 ACM SIGIR Conference on
Research and Development in Information Retrieval.
146
N. Fuhr. 1992. Probabilistic models in information re-
trieval. The Computer Journal, 35(3):243?255.
Y. Jing and W. Bruce Croft. 1994. An association the-
saurus for information retreival. In Proceedings of
RIAO.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of International Conference
on Machine Learning (ICML).
S. Liu, F. Liu, C. Yu, and W. Meng. 2004. An effec-
tive approach to document retrieval via utilizing word-
net and recognizing phrases. In Proceedings of the
2004 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999a. Ad
hoc retrieval experiments using wornet and automati-
cally constructed theasuri. In Proceedings of the sev-
enth Text REtrieval Conference (TREC7).
R. Mandala, T. Tokunaga, and H. Tanaka. 1999b. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of the
1999 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4).
H. J. Peat and P. Willett. 1991. The limitations of term
co-occurence data for query expansion in document re-
trieval systems. Journal of the american society for
information science, 42(5):378?383.
J. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
the ACM SIGIR?98, pages 275?281.
Y. Qiu and H.P. Frei. 1993. Concept based query ex-
pansion. In Proceedings of the 1993 ACM SIGIR Con-
ference on Research and Development in Information
Retrieval.
P. Ruch, I. Tbahriti, J. Gobeill, and A. R. Aronson. 2006.
Argumentative feedback: A linguistically-motivated
term expansion for information retrieval. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 675?682.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A theory
of term importance in automatic text analysis. Jour-
nal of the American Society for Information Science,
26(1):33?44, Jan-Feb.
A. F. Smeaton and C. J. van Rijsbergen. 1983. The
retrieval effects of query expansion on a feedback
document retrieval system. The Computer Journal,
26(3):239?246.
M. A. Stairmand. 1997. Textual context analysis for in-
formation retrieval. In Proceedings of the 1997 ACM
SIGIR Conference on Research and Development in
Information Retrieval.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworths.
E. M. Voorhees. 1993. Using wordnet to disambiguate
word sense for text retrieval. In Proceedings of the
1993 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
E. M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 1994 ACM
SIGIR Conference on Research and Development in
Information Retrieval.
E. M. Voorhees. 2005. Overview of the trec 2005 ro-
bust retrieval track. In Notebook of the Thirteenth Text
REtrieval Conference (TREC2005).
147
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 600?609, Dublin, Ireland, August 23-29 2014.
Identifying Important Features for Graph Retrieval
Zhuo Li and Sandra Carberry and Hui Fang* and Kathleen F. McCoy
ivanka@udel.edu carberry@udel.edu hui@udel.edu mccoy@udel.edu
Department Computer and Information Science,
*Department of Electrical and Computer Engineering
University of Delaware
Abstract
Infographics, such as bar charts and line graphs, occur often in popular media and are a rich
knowledge source that should be accessible to users. Unfortunately, information retrieval re-
search has focused on the retrieval of text documents and images, with almost no attention specif-
ically directed toward the retrieval of information graphics. Our work is the first to directly tackle
the retrieval of infographics and to design a system that takes into account their unique charac-
teristics. Learning-to-rank algorithms are applied on a large set of features to develop several
models for infographics retrieval. Evaluation of the models shows that features pertaining to the
structure and the content of graphics should be taken into account when retrieving graphics and
that doing so results in a model with better performance than a baseline model that relies on
matching query words with words in the graphic.
1 Introduction
Infographics are non-pictorial graphics such as bar charts and line graphs. When such graphics appear in
popular media, they generally have a high-level message that they are intended to convey. For example,
the graphic in Figure 1 ostensibly conveys the message that Toyota has the highest profit among the
automobile companies listed. Thus infographics are a form of language since, according to Clark (Clark
and Curran, 2007), language is any deliberate signal that is intended to convey a message.
Although much research has addressed the retrieval of documents, very little attention has been given
to the retrieval of infographics. Yet research has shown that the content of an infographic is often not
included in the article?s text (Carberry et al., 2006). Thus infographics are an important knowledge
source that should be accessible to users of a digital library.
Techniques that have been effective for document or image retrieval are inadequate for the retrieval
of infographics. Current search engines employ strategies similar to those used in document retrieval,
relying primarily on the text surrounding a graphic and web link structures. But the text in the surround-
ing document generally does not refer explicitly to the infographic or even describe its content (Carberry
et al., 2006). An obvious extension to using the article text would be to collect all the words in an
infographic and use it as a bag of words. However, infographics have structure and often a high-level
message, and bag of words approaches ignore this structure and message content.
This paper explores the features that should be taken into account when ranking graphics for retrieval
in response to a user query. Using a learning-to-rank algorithm on a wide range of features (including
structural and content features), we produce a model that performs significantly better than a model that
ignores graph structure and content. Analysis of the model shows that features based on the structure
and content of graphs are very important and should not be ignored. To our knowledge, our research is
the first to take graph structure and content into account when retrieving infographics.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
600
Figure 1: An Example Infographic
2 Related Work
Information retrieval research has focused on the retrieval of text documents and images. Two popular
approaches to text retrieval are the vector space method and probabilistic methods. The vector space
method (Dubin, 2004) represents the document and the query each as a vector of weighted words and
then uses a similarity function to measure the similarity of each document to the query. Most weighting
mechanisms reward words that occur frequently in both the document and query but infrequently in
the overall collection of documents. Probabilistic retrieval models instead estimate the probability that
a document is relevant to a user query. In recent years, the language modeling approach has shown
promise as a retrieval strategy with sound statistical underpinnings (Lv and Zhai, 2009; Manning et al.,
2008). In all of the above approaches, query expansion techniques have been used to expand the query
with synonyms and related words before ranking documents for retrieval. Work on short document and
query expansion have shown improvements in retrieval performance (Arguello et al., 2008; Escalante et
al., 2008; Metzler and Cai, 2011).
Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that
retrieved images based solely on visual similarity, relying on low-level features such as color, texture and
shape ( (Flickner et al., 1995; Swain and Ballard, 1991; Smith and Chang, 1997; Gupta and Jain, 1997),
among others), to systems which attempt to classify and reason about the semantics of the images being
processed (Bradshaw, 2000; Smeulders et al., 2000; Datta et al., 2008). However, images are free-form
with relatively little inherent structure; thus it is extremely difficult to determine what is conveyed by an
image, other than to list the image?s constituent pieces. Most systems that retrieve infographics, such
as SpringerImages (http://www.springerimages.com) and Zanran (http://www.zanran.com), are based on
textual annotations of the graphics as in image retrieval (Gao et al., 2011) or on matching the user?s query
against the text surrounding the graphic. However, the structure and content of the graph are not taken
into consideration.
In this paper, we focus on natural language queries given that such queries allow users to express their
specific information need more clearly than keywords (Phan et al., 2007; Bendersky and Croft, 2009).
Previous work on verbose and natural language queries (Bendersky and Croft, 2008; Liu et al., 2013)
used probabilistic models and natural language processing techniques to identify the key contents in
such queries. Our query processing method not only extracts key entities but also further classifies the
extracted key entities into different components using a learned decision tree model.
3 Problem Formulation
Our research is currently limited to two kinds of infographics: simple bar charts and single line graphs.
We assume that our digital library contains an XML representation of each graphic that includes 1) the
graphic?s image, 2) its structural components: the set of independent axis (x-axis) labels
1
, the entity being
measured on the dependent axis (y-axis), and the text that appears in the graphic?s caption, referred to
as G
x
, G
y
, and G
c
respectively, and 3) the graphic?s intended message G
m
and any entities G
f
that the
1
We will refer to the independent axis as the x-axis and the dependent axis as the y-axis throughout this paper.
601
message focuses on. This paper is not concerned with the computer vision problem of recognizing the
bars, labels, colors, etc. in a graphic; other research efforts, such as the work in (Chester and Elzer, 2005;
Futrelle and Nikolakis, 1995) are addressing the parsing of electronic images such as bar charts and line
graphs.
Prior research on our project has addressed issues that arise in recognizing G
y
, G
m
, and G
f
. The
dependent axis of an infographic often does not explicitly label what is being measured, such as net
profit in Figure 1, and these must be inferred from other text in the graphic. Our prior work (Demir et
al., 2007) identified a hierarchy of graphic components in which pieces of the entity being measured
might appear; a set of heuristics were constructed that extracted these pieces and melded them together
to form what we refer to as a measurement axis descriptor and which is G
y
. The project?s prior work
also identified a set of 17 categories of intended message, such as Rank, Relative-difference, Maximum,
and Rising-trend, that might be conveyed by simple bar charts and line graphs; a Bayesian system (Elzer
et al., 2011; Wu et al., 2010) was developed that utilizes communicative signals in a graphic (such as the
coloring of one bar differently from the other bars) in order to recognize a graphic?s intended message,
including both the message category and the parameters of the message such as any focused entity. For
example, the intended message of the bar chart in Figure 1 is ostensibly that Toyota has the highest net
profit of any of the automobile manufacturers listed; thus its message falls into the Maximum message
category and its focused entity is Toyota.
Our vision is that since graphics have structure and content, the users whose particular information
needs could be satisfied by an infographic will formulate their queries to indicate the requisite structure
of the desired graphics. Thus we assume the use of full-sentence queries so that the semantics of the query
can be analyzed to identify characteristics of relevant graphics. For example, consider the following two
queries that contain similar keywords but represent different information needs:
Q
1
: Which countries have the highest occurrence of rare diseases?
Q
2
: Which rare diseases occur in the most countries?
These two queries contain almost identical words but are asking for completely different graphics. Query
Q
1
is asking for a comparison of countries (independent axis) according to their occurrence of rare dis-
eases (dependent axis) while query Q
2
is asking for a comparison of different rare diseases (independent
axis) according to the number of countries in which they occur (dependent axis). In addition, both queries
are asking for a graphic with a Rank message that ranks countries (query Q
1
) or rare diseases (query Q
2
)
as opposed to a graphic that shows the trend in rare diseases throughout the world.
4 Methodology
To retrieve relevant graphics in response to a user query, the query will first be analyzed to identify requi-
site characteristics of relevant infographics. We have developed learned decision trees (Li et al., 2013a;
Li et al., 2013b) for analyzing a query and identifying the requisite structure of relevant infographics (the
content of the independent axis or x-axis and dependent axis or y-axis, referred to as Q
x
and Q
y
), and
the category of intended message and focused entity, if any, (referred to as Q
m
and Q
f
) that will best
satisfy the user?s information need.
Given a new user query, it is parsed and noun phrases are extracted. Each query-phrase pair, consisting
of a query and an extracted noun phrase, is processed by a decision tree that determines whether the noun
phrase represents x-axis content, y-axis content, or neither. Attributes used by this decision tree include
whether the main verb of the query is a comparison verb (such as ?differ? and ?compare?) or a trend
verb (such as ?change? and ?decrease?), whether the noun phrase is preceded by a quantity phrase such
as ?the number of? suggesting that the noun phrase specifies y-axis content of relevant infographics, and
whether the noun phrase describes a period of time.
Similarly, another decision tree is constructed to identify the category of graph intended message
(such as Trend or Rank) that the query desires, using a subset of the attributes from the axes decision tree
combined with the classification results of the axes decision tree. An example of the reused attributes is
the class of the main verb in the user query; for example, a comparison main verb suggests that relevant
infographics will convey a comparison-based intended message, such as a Relative-difference or Rank
602
intended message. Other attributes include the presence of a superlative or comparative in the query
and attributes depending on the identified content of the x and y axes by the axes decision tree, such
as the number of x-axis entities, their plurality, and whether an x-axis entity describes a time interval.
A third decision tree is constructed for identifying whether a noun phrase describes a specific focused
x-axis entity. Then the infographics in the digital library must be rank-ordered according to how well
they satisfy the requirements of the user query.
This paper is concerned with identifying the most important features in a metric for rank-ordering
the graphics in response to a user query. We experiment with two learning-to-rank algorithms and 56
features that include both general features such as bag of words comparisons and structural and content
features. Our hypothesis is that structural and content-based features play an important role in graph
retrieval and cannot be ignored. Section 5 discusses the features used in our experiments, Section 6
discusses the learning algorithms, Section 6.1 compares the resultant models with a baseline that uses
just general features treating query and graphic each as one bag of words, and Section 6.2 discusses the
features that appear most influential in the models.
5 Features
We consider three kinds of features: 1) general features that compare words in the query with words in
the graphic, 2) structural features that compare the requisite structure hypothesized from the query with
the structure of candidate infographics, and 3) content-based features that compare the requisite message
hypothesized from the user query with the intended message of candidate graphics.
Query expansion is a commonly used strategy in information retrieval to bridge the vocabulary gap
between terms in a query and those in documents. The basic idea is to expand the original query with
terms that are semantically similar to the ones in the query. This addresses the problem encountered when
the query uses the word car but the document uses the term automobile. But retrieval of information
graphics presents an additional problem. Consider a query such as ?Which car manufacturer has the
highest net profit?? A graphic such as the one in Figure 1 displays a set of car manufacturers on the x-
axis (Toyota, Nissan, etc.) but nowhere in the graphic does the word car or a synonym appear. Identifying
the ontological category, such as car or automobile, of these labels is crucial since the user?s query often
generalizes the entities on the independent axis of relevant graphs rather than listing them.
To expand a given text string s, we use Wikimantic (Boston et al., 2013), a term expansion method
that uses Wikipedia articles as topic concepts. A topic concept is a unigram distribution built from words
in the Wikipedia article for that topic. A string s is interpreted by Wikimantic into a mixture concept that
is a weighted vector of topic concepts that capture the semantic meaning of the words in s. Each topic
concept is weighted by the likelihood that the concept (Wikipedia article) generates the text string s. The
weighted concepts are then used to produce a unigram distribution of words that serve as the expansion of
the terms in the string s. One issue in graph retrieval is correlating the requisite x-axis content specified
in the user query with the x-axis labels in graphs. A query such as ?Which car manufacturer has ... ??
is requesting a graph where ?car manufacturers? are listed on the x-axis. Thus we need to recognize
individual x-axis words which are often proper nouns (e.g., ?Ford?, ?Nissan?, ?Honda?) as instances of
car manufacturers. In the case of labels on the independent axis (such as Toyota, Nissan, Honda, etc.),
words such as car or automobile are part of the produced unigram distribution ? that is, as a side effect,
the ontological category of the individual entities becomes part of the term expansion.
We use Wikimantic to interpret and expand each of the graph components G
x
, G
y
, G
f
, and G
c
. The
expansion of the graph components (as opposed to the typical expansion of the query) accomplishes two
objectives: 1) it addresses the problem of sparse graphic text by adding semantically similar words and
2) it addresses the problem of terms in the query capturing general classes (such as car or automobile)
when the graphic instead contains an enumeration of members of the general class. Expansion of the
words in the graphics, unlike query expansion, has the added advantage that it is completed in advance
and off-line.
603
5.1 General Features
Our general feature set includes 17 general features capturing a variety of different kinds of relevance
scorings between two bags of words consisting respectively of words from the user query and words
from the candidate infographic:
? GF
1
: A modified version of Okapi-BM25 (Fang et al., 2004) calculated as:
Okapi-BM25 Score =
?
w?Q
log
|D|+1
df
w
+1
?
tf
w
?(1+k
1
)
tf
w
+k
1
where Q is a query, |D| is the number of graphs in the digital library, w is a query word in Q,
df
w
is the frequency of graphs containing word w in the digital library, tf
w
is the frequency of
word w in the text expansion of the given graphic, and k
1
is a parameter that is typically set to 1.2.
Okapi-BM25 is a bag-of-words ranking function used in many information retrieval systems. Our
modified version of Okapi-BM25 addresses the problem of negative values that can occur with the
original Okapi formula. In addition, our formula does not take text length or query term frequency
into account since graphics have relatively similar amounts of text and most terms in a query occur
only once.
? GF
2
: The term frequency-inverse document frequency (tf-idf) value of query words that appear in
the expanded graphic.
? GF
3
: The maximum, minimum, and arithmetic mean of the term frequency (tf) of query words that
appear in the expanded graphic.
? GF
4
: The maximum, minimum, and arithmetic mean of inverse document (graphic) frequency (idf)
of query words that appear in the expanded graphic.
5.2 Structural Features
Our structural feature set includes 35 features: 17 that address how well a graphic?s x-axis (independent
axis) relates to the requisite x-axis content hypothesized from the user?s query and 18 that address how
well a graphic?s y-axis (dependent axis) content captures the requisite dependent axis content hypothe-
sized from the query. The following are a few of the x-axis features:
? SFX
1
: The Okapi-BM25 value using the same modified formula as for general features, given the
query x-axis words and the text expansion of the x-axis labels in the graphic.
? SFX
2
: The tf-idf of x-axis words hypothesized from the query that appear in the expansion of the
x-axis labels in the graphic.
? SFX
3
: The maximum, minimum, and arithmetic mean of tf of x-axis words hypothesized from the
query that appear in the expansion of the x-axis labels in the graphic.
? SFX
4
: The maximum, minimum, and arithmetic mean of idf of x-axis words hypothesized from
the query that appear in the expansion of the x-axis labels in the graphic.
The y-axis features (SFY
1
, SFY
2
, SFY
3
, and SFY
4
) include the same relevance measurements as
used for the x-axis features; for example, feature SFY
1
captures the Okapi-BM25 score for the y-axis
content hypothesized from the query and the text expansion of the graphic y-axis words, and feature
SFY
2
is the tf-idf score for the y-axis content hypothesized from the query and the expansion of the
graphic y-axis words. One additional feature that is specific to the y-axis is:
? SFY
5
: The posterior probability of the Wikimantic (Boston et al., 2013) mixture concept
2
for the
y-axis words hypothesized from the query, given the Wikimantic mixture concept representing the
y-axis words in the graph, referred to as p(Q
y
|G
y
). Both query y-axis words and the graphic y-axis
2
A Wikimantic mixture concept is a set of weighted concepts (Boston et al., 2013).
604
descriptor are each interpreted by Wikimantic into a mixture concept, M
qy
and M
gy
respectively.
Recall from the introduction to Section 5 that a mixture concept is a weighted vector of topic con-
cepts that defines the semantic meaning of a term or set of terms. For example, the mixture concept
for the country China is represented by a vector of topic concepts such as ?China?, ?People?s Re-
public of China?, ?Mainland China?, and so on. Wikimantic estimates the probability of a concept
given another concept by the amount of overlapping words between the two concepts. For example,
the topic concept for the country ?United States? is likely to contain similar words to the concept
for ?China?, such as the words ?country?, ?nation?, ?region?, ?capital?, ?GDP?, etc. Therefore
the probability of United States given China is likely to be higher than that of United States given
the topic ?rugby?.
5.3 Content Features
Our content feature set contains four features that address how well the intended message of a graphic
captures the requisite message content hypothesized from the user?s query. Ideally, a relevant graphic?s
intended message G
m
will match the message category Q
m
hypothesized from the user?s query. When
the two do not match exactly, we use a hierarchy of message categories and the concept of relaxation
as the paradigm for estimating how much perceptual effort would be required to extract the message
specified by the query from the graphic. For example, suppose that the query requests a Rank message;
graphics with Rank messages will convey the rank of a specific entity by arranging the entities in order of
value and highlighting in some way the entity whose rank is being conveyed. Graphics with a Rank-all
intended message will convey the rank of a set of entities without highlighting any specific entity; the
Rank-all message category appears as a parent of Rank in the message hierarchy since it is less specific
than Rank. Although one can identify the rank of a specific entity from a graphic whose intended message
is a Rank-all message, it is perceptually more difficult since one must search through the graph for the
entity whose rank is desired. By moving up or down the message hierarchy from Q
m
to G
m
, Q
m
is
relaxed to match different G
m
. The greater the degree of relaxation involved, the less message-relevant
the infographic is to the user query. The four content-based features are:
? CF
1
: Whether the message category Q
m
hypothesized from the user?s query matches exactly the
intended message category G
m
of the graphic.
? CF
2
: The amount of relaxation needed to relax the message category Q
m
hypothesized from the
user?s query so that it matches the intended message category G
m
of the graphic.
? CF
3
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the focused entity G
f
in the graphic, if any.
? CF
4
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the non-focused x-axis entities G
nf
in the graphic.
6 Constructing a Ranking Model for Graph Retrieval
Learning-to-rank algorithms (Liu, 2009) construct a learned model that ranks objects based on partially
ordered training data. Tree-based ensemble methods have been shown to be very effective (Chapelle
and Chang, 2011). We experimented with two state-of-the-art tree-based learning-to-rank algorithms as
implemented in the RankLib library (http://people.cs.umass.edu/vdang/ranklib.html): Multiple Additive
Regression Trees abbreviated as MART (Friedman, 2001) and Random Forest (Breiman, 2001).
A human subject experiment was performed to collect a set of 152 full sentence user queries from
five topics. The queries were collected from 5 different tasks and covered a variety of topics involving
companies. Two sample queries are ?What credit card company made the most money in 2008?? and
?How does Avis rank compared to other car rental companies in revenue??. We used the collected
queries to search on popular commercial image search engines to get more infographics from the same
topics. These commercial search engines include Google Image, Microsoft Bing Image Search, and
Picsearch. This produced a set of 257 infographics that are in the topics of the collected queries. Each
605
query-infographic pair was assigned a relevance score on a scale of 0-3 by an undergraduate researcher.
A query-infographic pair was assigned 3 points if the infographic was considered highly relevant to the
query and 0 points if it was irrelevant. Query-infographic pairs where the graphic was somewhat relevant
to the query were assigned 1 or 2 points, depending on the judged degree of relevance of the graphic to
the query. This produced a corpus for training and testing.
Using MART and Random Forest, we developed four models from all 56 features, including the
structural and content features. Two of the models were built using our learned decision trees (Li et
al., 2013b; Li et al., 2013a) to analyze the queries and hypothesize the requisite x-axis content, y-axis
content, message category, and focused entity (if any); see the second row of Table 1. Since the learned
decision trees are not perfect, the other two models were built from hand-labelled data; see the last row of
Table 1. In addition, two baseline models were constructed using only the general features and omitting
the structural and content-based features.
6.1 Evaluating the Models
Normalized Discounted Cumulative Gain (NDCG) (Ja?rvelin and Keka?la?inen, 2002) is used to evaluate
the retrieval result. Table 1 displays the NDCG@10 results. In each case, we averaged together the
NDCG results of 10 runs using the Bootstrapping Method (Tan et al., 2006) in which the query data set
is sampled with replacement to select 152 queries; these 152 queries, and for each query the relevance
judgements assigned to each of the graphics, comprised the training set, with the unselected queries
and their relevance judgements comprising the testing set. The Bootstrapping method is a widely used
evaluation method for small datasets. Typically, approximately 63% of the dataset is selected for the
training set (with some items appearing more than once in the training set) and 37% for the testing set.
The second row of Table 1 provides results when each query is processed by our learned decision trees to
extract the structural content and message category that the query specifies. However, the decision trees
are imperfect. To determine whether our system could do even better if the decision trees were improved,
the third row of Table 1 reports results when each query was hand-labelled with the correctly extracted
structural and message content.
The models using all 56 features produced significantly better results than the baseline model that
used just the general features, indicating that structural and content-based features are very important
and must be taken into account in graph retrieval. In addition, the models built from the hand-labelled
data produced better results than the models where the structural and content features were automatically
extracted from the queries using the learned decision trees; this suggests that improving the decision trees
that process the queries would improve the accuracy of the learned graph retrieval models. In some cases,
the Random Forest learned model performed better than the MART model, but the improvement was not
significant. The experimental results show that both MART and Random Forest using all 56 features,
either using the hand-labelled query data or decision tree query data, provide significantly better results
than the baseline approach (p<0.0005).
Algorithm MART Random Forest
Baseline 0.4943 0.4935
Decision Tree Query Data 0.6239 0.6258
Hand-labelled Query Data 0.6723 0.6758
Table 1: NDCG@10 Results
Figure 2 displays the NDCG@k results for different values of k. The bottom solid line and the line
composed of triangles depict the baseline results, the middle dashed line and the line composed of circles
depict the results using the decision tree query data, and the top solid line and the line composed of
triangles depict the results using the hand-labelled data. All of the models improve as k increases. Most
important, both our MART and Random Forest models constructed from all 56 features perform much
better than the baseline models for all values of k. Thus we conclude that the use of structural and content
features helps in selecting the most relevant graphic as well as the most relevant sets of graphics.
606
Figure 2: NDCG@k for Various Values of n
6.2 Analysis of Influential Features
In both MART and Random Forest, features that are used at the top levels of each tree are more important
in ranking a graphic than features that appear lower in the tree. We analyzed the importance of each of
the 56 features based on the level in each tree where the feature is first used. 70% of the top ten most
important features in the trees produced by both MART and Random Forest were structural or content
features. The most influential two features in trees produced by MART were SFY
5
which captures
p(Q
y
| G
y
) and SFX
2
which captures the tf-idf of x-axis words hypothesized from the query that appear
in the expansion of the x-axis labels in the graphic. Although these two features were not the two
most influential features in the trees produced by Random Forest, they did appear among the top 5
features. Two content-based features appeared among the top ten most important features: CF
3
which
captures the relevance of the focused entity Q
f
(if any) hypothesized from the query to the focused
entity G
f
(if any) in the graphic and CF
4
which captures the relevance of the focused entity Q
f
(if any)
hypothesized from the query to the non-focused entities G
fx
in the graphic. The content features CF
1
and CF
2
that measure relevance of the message category hypothesized from the query to the intended
message category in a candidate graphic appeared among the top 20 features but not among the top 10
features. Further inspection of the trees and analysis of the queries and graphics leads us to believe
that message category relevance is influential in refining the ranking of graphics once graphics with
appropriate structural content have been identified. Our future work will examine these two features
more closely and determine whether modifications of them, or changes in how they are used, will improve
results.
Based on these results, we conclude that structural and content-based features are important when
ranking infographics for retrieval and must be taken into account in an effective graph retrieval system.
7 Conclusion and Future Work
To our knowledge, no other research effort has considered the use of structural and content-based fea-
tures when ranking graphics for retrieval from a digital library. We developed learned models that take
into account how well the structure and content of an infographic matches the requisite structure and con-
tent hypothesized from the user query, and showed that these models perform significantly better than
baseline models that ignore graph structure and message content. In addition, an analysis of the learned
models showed which structural and content features were most influential. In our future work, we will
improve our methods for hypothesizing requisite features of relevant graphics and will analyze our re-
laxation metric to determine whether an improved metric will play a more influential role in ranking
graphics for retrieval.
Acknowledgements
This work was supported by the National Science Foundation under grant III-1016916 and IIS-1017026.
607
References
Jaime Arguello, Jonathan L Elsas, Jamie Callan, and Jaime G Carbonell. 2008. Document representation and
query expansion models for blog recommendation. ICWSM, 2008(0):1.
Michael Bendersky and W Bruce Croft. 2008. Discovering key concepts in verbose queries. In Proceedings of the
31st annual international ACM SIGIR conference on Research and development in information retrieval, pages
491?498. ACM.
Michael Bendersky and W Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proceedings
of the 2009 workshop on Web Search Click Data, pages 8?14. ACM.
Christopher Boston, Hui Fang, Sandra Carberry, Hao Wu, and Xitong Liu. 2013. Wikimantic: Toward effective
disambiguation and expansion of queries. Data & Knowledge Engineering.
Ben Bradshaw. 2000. Semantic based image retrieval: a probabilistic approach. In Proceedings of the eighth ACM
international conference on Multimedia, pages 167?176. ACM.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Sandra Carberry, Stephanie Elzer, and Seniz Demir. 2006. Information graphics: an untapped resource for digital
libraries. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development
in information retrieval, pages 581?588. ACM.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank
Challenge, pages 1?24.
Daniel Chester and Stephanie Elzer. 2005. Getting computers to see information graphics so users do not have to.
In Foundations of Intelligent Systems, pages 660?668. Springer.
S. Clark and J.R. Curran. 2007. Wide-coverage efficient statistical parsing with ccg and log-linear models. Com-
putational Linguistics, 33(4):493?552.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image retrieval: Ideas, influences, and trends of the
new age. ACM Computing Surveys (CSUR), 40(2):5.
Seniz Demir, Sandra Carberry, and Stephanie Elzer. 2007. Effectively realizing the inferred message of an in-
formation graphic. In Proceedings of the International Conference on Recent Advances in Natural Language
Processing (RANLP), pages 150?156.
David Dubin. 2004. The most influential paper gerard salton never wrote.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman. 2011. The automated understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Hugo Jair Escalante, Carlos Herna?ndez, Aurelio Lo?pez, Heidy Mar??n, Manuel Montes, Eduardo Morales, Enrique
Sucar, and Luis Villasen?or. 2008. Towards annotation-based query and document expansion for image retrieval.
In Advances in Multilingual and Multimodal Information Retrieval, pages 546?553. Springer.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceed-
ings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ?04, pages 49?56, New York, NY, USA. ACM.
Myron Flickner, Harpreet Sawhney, Wayne Niblack, Jonathan Ashley, Qian Huang, Byron Dom, Monika Gorkani,
Jim Hafner, Denis Lee, Dragutin Petkovic, et al. 1995. Query by image and video content: The qbic system.
Computer, 28(9):23?32.
Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. The Annals of Statis-
tics, 29(5):1189?1232, 10.
Robert P Futrelle and Nikos Nikolakis. 1995. Efficient analysis of complex diagrams using constraint-based
parsing. In Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on,
volume 2, pages 782?790. IEEE.
Y. Gao, M. Wang, H. Luan, J. Shen, S. Yan, and D. Tao. 2011. Tag-based social image search with visual-text
joint hypergraph learning. In Proceedings of the 19th ACM international conference on Multimedia, pages
1517?1520. ACM.
608
Amarnath Gupta and Ramesh Jain. 1997. Visual information retrieval. Communications of the ACM, 40(5):70?79.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422?446, October.
Zhuo Li, Matthew Stagitis, Sandra Carberry, and Kathleen F. McCoy. 2013a. Towards retrieving relevant informa-
tion graphics. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?13, pages 789?792, New York, NY, USA. ACM.
Zhuo Li, Matthew Stagitis, Kathleen McCoy, and Sandra Carberry. 2013b. Towards finding relevant information
graphics: Identifying the independent and dependent axis from user-written queries.
Jingjing Liu, Panupong Pasupat, Yining Wang, Scott Cyphers, and Jim Glass. 2013. Query understanding en-
hanced by hierarchical parsing structures. In Automatic Speech Recognition and Understanding (ASRU), 2013
IEEE Workshop on, pages 72?77. IEEE.
Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval,
3(3):225?331.
Yuanhua Lv and ChengXiang Zhai. 2009. Positional language models for information retrieval. In Proceedings
of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages
299?306. ACM.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to information retrieval,
volume 1. Cambridge university press Cambridge.
Donald Metzler and Congxing Cai. 2011. Usc/isi at trec 2011: Microblog track. In TREC.
Nina Phan, Peter Bailey, and RossWilkinson. 2007. Understanding the relationship of information need specificity
to search query length. In Proceedings of the 30th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 709?710. ACM.
Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-
based image retrieval at the end of the early years. Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on, 22(12):1349?1380.
John R Smith and Shih-fu Chang. 1997. Querying by color regions using the visualseek content-based visual
query system. Intelligent multimedia information retrieval, 7(3):23?41.
Michael J Swain and Dana H Ballard. 1991. Color indexing. International journal of computer vision, 7(1):11?32.
Pang-Ning Tan, Michael Steinbach, Vipin Kumar, et al. 2006. Introduction to data mining. WP Co.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel Chester. 2010. Recognizing the intended message of line
graphs. In Diagrammatic Representation and Inference, pages 220?234. Springer.
609
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 603?612,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Study of Concept-based Weighting Regularization
for Medical Records Search
Yue Wang, Xitong Liu, Hui Fang
Department of Electrical & Computer Engineering,
University of Delaware, USA
{wangyue,xtliu,hfang}@udel.edu
Abstract
An important search task in the biomedical
domain is to find medical records of pa-
tients who are qualified for a clinical trial.
One commonly used approach is to apply
NLP tools to map terms from queries and
documents to concepts and then compute
the relevance scores based on the concept-
based representation. However, the map-
ping results are not perfect, and none of
previous work studied how to deal with
them in the retrieval process. In this pa-
per, we focus on addressing the limitations
caused by the imperfect mapping results
and study how to further improve the re-
trieval performance of the concept-based
ranking methods. In particular, we ap-
ply axiomatic approaches and propose two
weighting regularization methods that ad-
just the weighting based on the relations
among the concepts. Experimental results
show that the proposed methods are effec-
tive to improve the retrieval performance,
and their performances are comparable to
other top-performing systems in the TREC
Medical Records Track.
1 Introduction
With the increasing use of electronic health
records, it becomes urgent to leverage this rich
information resource about patients? health condi-
tions to transform research in health and medicine.
As an example, when developing a cohort for
a clinical trial, researchers need to identify pa-
tients matching a set of clinical criteria based on
their medical records during their hospital visits
(Safran et al, 2007; Friedman et al, 2010). This
selection process is clearly a domain-specific re-
trieval problem, which searches for relevant medi-
cal records that contain useful information about
their corresponding patients? qualification to the
criteria specified in a query, e.g., ?female patient
with breast cancer with mastectomies during ad-
mission?.
Intuitively, to better solve this domain-specific
retrieval problem, we need to understand the re-
quirements specified in a query and identify the
documents satisfying these requirements based on
their semantic meanings. In the past decades,
significant efforts have been put on constructing
biomedical knowledge bases (Aronson and Lang,
2010; Lipscomb, 2000; Corporation, 1999) and
developing natural language processing (NLP)
tools, such as MetaMap, to utilize the informa-
tion from the knowledge bases (Aronson, 2001;
McInnes et al, 2009). These efforts make it pos-
sible to map free text to concepts and use these
concepts to represent queries and documents.
Indeed, concept-based representation is one
of the commonly used approaches that leverage
knowledge bases to improve the retrieval perfor-
mance (Limsopatham et al, 2013d; Limsopatham
et al, 2013b). The basic idea is to represent
both queries and documents as ?bags of concepts?,
where the concepts are identified based on the in-
formation from the knowledge bases. This method
has been shown to be more effective than tra-
ditional term-based representation in the medical
record retrieval because of its ability to handle the
ambiguity in the medical terminology. However,
this method also suffers the limitation that its ef-
fectiveness depends on the accuracy of the concept
mapping results. As a result, directly applying
existing weighting strategies might lead to non-
optimal retrieval performance.
In this paper, to address the limitation caused by
the inaccurate concept mapping results, we pro-
pose to regularize the weighting strategies in the
concept-based representation methods. Specifi-
cally, by applying the axiomatic approaches (Fang
and Zhai, 2005), we analyze the retrieval func-
603
tions with concept-based representation and find
that they may violate some reasonable retrieval
constraints. We then propose two concept-based
weighting regularization methods so that the reg-
ularized retrieval functions would satisfy the re-
trieval constraints and achieve better retrieval per-
formance. Experimental results over two TREC
collections show that both proposed concept-
based weighting regularization methods can im-
prove the retrieval performance, and their perfor-
mance is comparable with the best systems of
the TREC Medical Records tracks (Voorhees and
Tong, 2011; Voorhees and Hersh, 2012).
Many NLP techniques have been developed to
understand the semantic meaning of textual in-
formation, and are often applied to improve the
search accuracy. However, due to the inherent am-
biguity of natural languages, the results of NLP
tools are not perfect. One of our contributions is
to present a general methodology that can be used
to adjust existing IR techniques based on the inac-
curate NLP results.
2 Related Work
The Medical Records track of the Text REtrieval
Conference (TREC) provides a common platform
to study the medical records retrieval problem
and evaluate the proposed methods (Voorhees and
Tong, 2011; Voorhees and Hersh, 2012).
Concept-based representation has been studied
for the medical record retrieval problem (Lim-
sopatham et al, 2013d; Limsopatham et al,
2013b; Limsopatham et al, 2013a; Qi and La-
querre, 2012; Koopman et al, 2011; Koopman et
al., 2012). For example, Qi and Laquerre used
MetaMap to generate the concept-based repre-
sentation and then apply a vector space retrieval
model for ranking, and their results are one of
the top ranked runs in the TREC 2012 Medi-
cal Records track (Qi and Laquerre, 2012). To
further improve the performance, Limsopatham
et al proposed a task-specific representation,
i.e., using only four types of concepts (symp-
tom, diagnostic test, diagnosis and treatment) in
the concept-based representation and a query ex-
pansion method based on the relationships among
the medical concepts (Limsopatham et al, 2013d;
Limsopatham et al, 2013a). Moreover, they also
proposed a learning approach to combine both
term-based and concept-based representation to
further improve the performance (Limsopatham et
Figure 1: Example of MetaMap result for a query.
al., 2013b).
Our work is also related to domain-specific
IR (Yan et al, 2011; Lin and Demner-Fushman,
2006; Zhou et al, 2007). For example, Yan et
al. proposed a granularity-based document rank-
ing model that utilizes ontologies to identify doc-
ument concepts. However, none of the previous
work has studied how to regularize the weight of
concepts based on their relations.
It is well known that the effectiveness of a re-
trieval function is closely related to the weight-
ing strategies (Fang and Zhai, 2005; Singhal et
al., 1996). Various term weighting strategies have
been proposed and studied for the term-based
representation (Amati and Van Rijsbergen, 2002;
Singhal et al, 1996; Robertson et al, 1996).
However, existing studies on concept-based rep-
resentation still used weighting strategies devel-
oped for term-based representation such as vector
space models (Qi and Laquerre, 2012) and diver-
gence from randomness (DFR) (Limsopatham et
al., 2013a) and did not take the inaccurate con-
cept mapping results into consideration. Com-
pared with previous work, we focus on address-
ing the limitation caused by the inaccurate con-
cept mapping. Note that our efforts are orthogonal
to existing work, and it is expected to bring addi-
tional improvement to the retrieval performance.
3 Concept-based Representation for
Medical Records Retrieval
3.1 Problem Formulation
We follow the problem setup used in the TREC
medical record track (Voorhees and Tong, 2011;
Voorhees and Hersh, 2012). The task is to retrieve
relevant patient visits with respect to a query.
Since each visit can be associated with multiple
medical records, the relevance of a visit is related
to the relevance of individual associated medical
records. Existing studies computed the relevance
604
scores at either visit-level, where all the medical
records of a visit are merged into a visit document
(Demner-Fushman et al, 2012; Limsopatham et
al., 2013c), or record-level, where we can first
compute the relevance score of individual records
and then aggregate their scores as the relevance
score of a visit (Limsopatham et al, 2013c; Zhu
and Carterette, 2012; Limsopatham et al, 2013d).
In this paper, we focus on the visit-level relevance
because of its simplicity. In particular, given a pa-
tient?s visit, all the medical records generated from
this visit are merged as a document. Note that our
proposed concept-weighting strategies can also be
easily applied to record-level relevance modeling.
Since the goal is to retrieve medical records of
patients that satisfying requirements specified in a
query, the relevance of medical records should be
modeled based on how well they match all the re-
quirements (i.e., aspects) specified in the queries.
3.2 Background: UMLS and MetaMap
Unified Medical Language System (UMLS) is a
metathesaurus containing information from more
than 100 controlled medical terminologies such as
the Systematized Nomenclature of Medicine Clin-
ical Terms (SNOMED-CT) and Medical Subject
Headings (MeSH). Specifically, it contains the in-
formation about over 2.8 million biomedical con-
cepts. Each concept is labeled with a Concept
Unique Identifier (CUI) and has a preferred name
and a semantic type.
Moreover, NLP tools for utilizing the informa-
tion from UMLS have been developed. In partic-
ular, MetaMap (Aronson, 2001) can take a text
string as the input, segment it into phrases, and
then map each phrase to multiple UMLS CUIs
with confidence scores. The confidence score is
an indicator of the quality of the phrase-to-concept
mapping by MetaMap. It is computed by four met-
rics: centrality, variation, coverage and cohesive-
ness (Aronson, 2001). These four measures try to
evaluate the mapping from different angles, such
as the involvement of the central part, the distance
of the concept to the original phrase, and how well
the concept matches the phrase. The maximum
confidence in MetaMap is 1000.
Figure 1 shows the MetaMap results for an ex-
ample query ?children with dental caries?. Two
query aspects, i.e., ?children? and ?dental caries?,
are identified. Each of them is mapped to multiple
concepts, and each concept is associated with the
confidence score as well as more detailed informa-
tion about this concept.
3.3 Concept-based Representation
Traditional retrieval models are based on ?bag of
terms? representation. One limitation of this rep-
resentation is that relevance scores are computed
based on the matching of terms rather than the
meanings. As a result, the system may fail to re-
trieve the relevant documents that do not contain
any query terms.
To overcome this limitation, concept-based rep-
resentation has been proposed to bridge the vo-
cabulary gap between documents and queries
(Qi and Laquerre, 2012; Limsopatham et al,
2013b; Koopman et al, 2012). In particular,
MetaMap is used to map terms from queries
and documents (e.g., medical records) to the
semantic concepts from biomedical knowledge
bases such as UMLS. Within the concept-based
representation, the query can then be repre-
sented as a bag of all the generated CUIs
in the MetaMap results. For example, the
query from Figure 1 can be represented as
{C0008059, C0680063, C0011334, C0333519,
C0226984}. Documents can be represented in a
similar way.
After converting both queries and documents
to concept-based representations using MetaMap,
previous work applied existing retrieval functions
such as vector space models (Singhal et al, 1996)
to rank the documents. Note that when referring
to existing retrieval functions in the paper, they
include traditional keyword matching based func-
tions such as pivoted normalization (Singhal et
al., 1996), Okapi (Robertson et al, 1996), Dirich-
let prior (Zhai and Lafferty, 2001) and basic ax-
iomatic functions (Fang and Zhai, 2005).
4 Weighting Strategies for
Concept-based Representation
4.1 Motivation
Although existing retrieval functions can be di-
rectly applied to concept-based representation,
they may lead to non-optimal performance. This
is mainly caused by the fact that MetaMap may
generate more than one mapped concepts for an
aspect, i.e., a semantic unit in the text.
Ideally, an aspect will be mapped to only one
concept, and different concepts would represent
different semantic meanings. Under such a situ-
605
                                                                                                                                             
Figure 2: Exploratory data analysis (From left to right are choosing minimum, average and maximum
IDF concepts as the representing concepts, respectively. The removed concepts are highlighted in the
figures.).
ation, traditional retrieval functions would likely
work well and generate satisfying retrieval per-
formance since the relations among concepts are
independent which is consistent with the assump-
tions made in traditional IR (Manning et al, 2008).
However, the mapping results generated by
MetaMap are not perfect. Although MetaMap is
able to rank all the candidate concepts with the
confidence score and pick the most likely one,
the accuracy is not very high. In particular, our
preliminary results show that turning on the dis-
ambiguation functionality provided by MetaMap
(i.e., returning only the most likely concept for
each query) could lead to worse retrieval per-
formance than using all the candidate mappings.
Thus, we use the one-to-many mapping results
generated by MetaMap, in which each aspect can
be mapped to multiple concepts.
Unfortunately, such one-to-many concept map-
pings could hinder the retrieval performance in the
following two ways.
? The multiple concepts generated from the
same aspect are related, which is inconsis-
tent with the independence assumption made
in the existing retrieval functions (Manning
et al, 2008). For example, as shown in Fig-
ure 1, ?dental caries? is mapped to three con-
cepts. It is clear that the concepts are related,
but existing retrieval functions are unable to
capture their relations and would compute the
weight of each concept independently.
? The one-to-many mapping results generated
by MetaMap could arbitrarily inflate the
weights of some query aspects. For exam-
ple, as shown in Figure 1, query aspect ?chil-
dren? is mapped to 2 concepts while ?den-
tal caries? is mapped to 3 concepts. In the
existing retrieval functions, term occurrences
are important relevance signals. However,
when converting the text to concepts repre-
sentation using MetaMap, the occurrences of
the concepts are determined by not only the
original term occurrences, a good indicator
of relevance, but also the number of mapped
concepts, which is determined by MetaMap
and has nothing to do with the relevance sta-
tus. As a result, the occurrences of concepts
might not be a very accurate indicator of im-
portance of the corresponding query aspect.
To address the limitations caused by the inac-
curate mapping results, we propose to apply ax-
iomatic approaches (Fang and Zhai, 2005) to reg-
ularize the weighting strategies for concept-based
representation methods. In particular, we first
formalize retrieval constraints that any reasonable
concept-based representation methods should sat-
isfy and then discuss how to regularize the existing
weighting strategies to satisfy the constraints and
improve the retrieval performance.
We first explain the notations used in this sec-
tion. Q and D denote a query and a document
with the concept-based representation. S(Q,D)
is the relevance score of D with respect to Q. e
i
denotes a concept, and A(e) denotes the query
aspect associated with e, i.e., a set of concepts
that are mapped to the same phrases as e by us-
ing MetaMap. i(e) is the normalized confidence
score of the mapping for concept e generated by
MetaMap. c(e,D) denotes the occurrences of
concept e in document D, df(e) denotes the num-
ber of documents containing e. |D| is the docu-
ment length of D. Imp
c
(e) is the importance of
the concept such as the concept IDF value, and
Imp
A
(A) is the importance of the aspect.
606
4.2 Unified concept weighting regularization
We now discuss how to address the first challenge,
i.e,. how to regularize the weighting strategy so
that we can take into consideration the fact that
concepts associated with the same query aspect are
not independent. We call a concept is a variant of
another one if both of them are associated with the
same aspect.
Intuitively, given a query with two aspects, a
document covering both aspects should be ranked
higher than those covering only one aspect. We
can formalize the intuition in the concept-based
representation as the following constraint.
Unified Constraint: Let query be Q =
{e
1
, e
2
, e
3
}, and we know that e
2
is a variant of
e
3
. Assume we have two documents D
1
and D
2
with the same document length, i.e., |D
1
| = |D
2
|.
If we know that c(e
1
, D
1
) = c(e
3
, D
2
) > 0,
c(e
1
, D
2
) = c(e
3
, D
1
) = 0 and c(e
2
, D
1
) =
c(e
2
, D
2
) > 0, then S(Q,D
1
) > S(Q,D
2
).
It is clear that existing retrieval functions would
violate this constraint since they ignore the rela-
tions among concepts.
One simple strategy to fix this problem is to
merge all the concept variants as a single concept
and select one representative concept to replace all
occurrences of other variants in both queries and
documents. By merging the concepts together, we
are aiming to purify the concepts and make the
similar concepts centralized so that the assumption
that all the concepts are independent would hold.
Formally, the adjusted occurrences of a concept
e in a document D is shown as follows:
c
mod
(e,D)=
{
?
e
?
?EC(e)
c(e
?
, D) e=Rep(EC(e))
0 e 6=Rep(EC(e))
(1)
where c(e,D) is the original occurrence of con-
cept e in document D, EC(e) denotes a set of
all the variants of e including itself (i.e., all the
concepts with the same preferred name as e), and
Rep(EC(e)) denotes the representative concept
from EC(e).
It is trivial to prove that, with such changes, ex-
isting retrieval functions would satisfy the above
constraint since the constraint implies TFC2 con-
straint defined in the previous study (Fang et al,
2004).
Now the remaining question is how to select the
representative concept from all the variants. There
are three options: select the concept with the maxi-
mum IDF, average IDF, or minimum IDF. We con-
duct exploratory data analysis on these three op-
tions. In particular, for each option, we generate
a plot indicating the correlation between the IDF
value of a concept and the relevance probability of
the concept (i.e., the probability that a document
containing the concept is relevant). Note that both
original and replaced IDF values are shown in the
plot for each option. Figure 2 shows the results. It
is clear that the right plot (i.e., selecting the con-
cept with the maximum IDF as the representative
concept) is the best choice since the changes make
the points less scattered. In fact, this can also be
confirmed by experimental results as reported in
Table 5. Thus, we use the concept with the max-
imum IDF value as the representative concept of
all the variants.
4.3 Balanced concept weighting
regularization
We now discuss how to address the second chal-
lenge, i.e., how to regularize the weighting strat-
egy to deal with the arbitrarily inflated statistics
caused by the one-to-many mappings.
The arbitrary inflation could impact the impor-
tance of the query aspects. For example, as shown
in Figure 1, one aspect is mapped to two con-
cepts while the other is mapped to three. More-
over, it could also impact the accuracy of the con-
cept IDF values. Consider ?colonoscopies? and
?adult?, it is clear that the first term is more im-
portant than the second one, which is consistent
with their term IDF values, i.e., 7.52 and 2.92, re-
spectively. However, with the concept-based rep-
resentation, the IDF value of the concept ?colono-
scopies?(C0009378) is 2.72, which is even smaller
than that of concept ?adult? (C1706450), i.e., 2.92.
To fix the negative impact on query aspects, we
could leverage the findings in the previous study
(Zheng and Fang, 2010) and regularize the weight-
ing strategy based on the length of query aspects
to favor documents covering more query aspects.
Since each concept mapping is associated with a
confidence score, we can incorporate them into the
regularization function as follows:
f(e,Q) = (1? ?) + ? ?
(
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
)
, (2)
where i(e) is the normalized confidence score of
concept e generated by MetaMap, and ? is a pa-
rameter between 0 and 1 to control the effect of the
regularization. When ? is set to 0, there is no reg-
ularization. This regularization function aims to
607
penalize the weight of concept e based on its vari-
ants as well as the concepts from other aspects. In
particular, a concept would receive more penalty
(i.e., its weight will be decreased more) when it
has more variants and the mappings of these vari-
ants are more accurate.
To fix the negative impact on the concept IDF
values, we propose to regularize the weighting
based on the importance of the query aspect. This
regularization can be formalized as the following
constraint.
Balanced Constraint: Let Q be a query
with two concepts and the concepts are associ-
ated with different aspects, i.e., Q = {e
1
, e
2
},
and A(e
1
) 6= A(e
2
). Assume D
1
and D
2
are two documents with the same length, i.e.,
|D
1
| = |D
2
|, and they cover different concepts
with the same occurrences, i.e., c(e
1
, D
1
) =
c(e
2
, D
2
) > 0 and c(e
2
, D
1
) = c(e
1
, D
2
) =
0. If we know Imp
c
(e
1
) = Imp
c
(e
2
) and
Imp
A
(A(e
1
)) < Imp
A
(A(e
2
)), then we have
S(Q,D
1
) < S(Q,D
2
).
This constraint requires that the relevance score
of a document should be affected by not only the
importance of the concepts but also the importance
of the associated query aspect. In a way, the con-
straint aims to counteract the arbitrary statistics in-
flation caused by MetaMap results and balance the
weight among concepts based on the importance
of the associated query aspects. And it is not dif-
ficult to show that existing retrieval functions vio-
late this constraint.
Now the question is how to revise the retrieval
functions to make them satisfy this constraint. We
propose to incorporate the importance of query as-
pect into the previous regularization function in
Equation (2) as follows:
f(e,Q) = (1??)+? ?
(
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
)
?Imp
A
(A(e)).
(3)
Note that Imp
A
(A(e)) is the importance of a
query aspect and can be estimated based on the
terms from the query aspect. In this paper, we
use the maximum term IDF value from the aspect
to estimate the importance, which performs better
than using minimum and average IDF values as
shown in the experiments (i.e., Table 6). We plan
to study other options in the future work.
4.4 Discussions
Both proposed regularization methods can be
combined with any existing retrieval functions. In
this paper, we focus on one of the state of the
art weighting strategies, i.e., F2-EXP function de-
rived from axiomatic retrieval model (Fang and
Zhai, 2005), and explain how to incorporate the
regularization methods into the function.
The original F2-EXP retrieval function is shown
as follows:
S(Q,D) =
?
e?Q?D
c(e,Q) ? (
N
df(e)
)
0.35
?
c(e,D)
c(e,D) + b +
b?|D|
avdl
(4)
where b is a parameter control the weight of the
document length normalization.
With the unified concept weighting regulariza-
tion, the revised function based on F2-EXP func-
tion, i.e., Unified, is shown as follows:
S(Q,D)=
?
e?Q?D
c
mod
(e,Q)?(
N
df(t)
)
0.35
?
c
mod
(e,D)
c
mod
(e,D)+b+
b?|D|
avdl
(5)
where c
mod
(e,D) and c
mod
(e,Q) denote the
modified occurrences as shown in Equation (1). It
can be shown that this function satisfies the unified
constraint but violates the balanced constraint.
Following the similar strategy used in the previ-
ous study (Zheng and Fang, 2010), we can further
incorporate the regularization function proposed
in Equation (3) to the above function to make it
satisfy the balanced constraint as follows:
S(Q,D) =
?
e?Q?D
c
mod
(e,Q)?(
N
df(t)
)
0.35
?f(e,Q) (6)
?
c
mod
(e,D)
c
mod
(e,D)+b+
b?|D|
avdl
where f(e,Q) is the newly proposed regular-
ization function as shown in Equation (3). This
method is denoted as Balanced, and can be shown
that it satisfies both constraints.
Table 1: Statistics of collections.
# of unique tokens AvgDL AvgQL11 AvgQL12
Term 263,356 2,659 10.23 8.82
Concept 58,192 2,673 8.79 7.81
5 Experiments
5.1 Experiment Setup
We conduct experiments using two data sets from
the TREC Medical Records track 2011 and 2012.
608
Table 2: Description of Methods
Name Representation Ranking strategies
Term-BL Term F2-EXP (i.e., Equation (4))
Concept-BL Concept (i.e., Section 3.3) F2-EXP (i.e., Equation (4))
TSConcept-BL Task specific concept ((Limsopatham et al, 2013d)) F2-EXP (i.e., Equation (4))
Unified Concept (i.e., Section 4.2) F2-EXP + Unified (i.e., Equation (5))
Balanced Concept (i.e., Section 4.3) F2-EXP + Balanced (i.e., Equation (6))
Table 3: Performance under optimized parameter settings
Med11 Med12
MAP bpref infNDCG infAP
Term-BL 0.3474 0.4727 0.4695 0.2106
Concept-BL 0.3967 0.5476 0.5243 0.2497
TSConcept-BL 0.3964 0.5329 0.5283 0.2694
Unified 0.4235
T
0.5443
T
0.5416
T
0.2586
T
Balanced 0.4561
T ,C ,TS
0.5697
T ,C ,TS
0.5767
T ,C ,TS
0.2859
T ,C ,TS
The data sets are denoted as Med11 and Med12.
Both data sets used the same document collection
with 100,866 medical records, each of which is as-
sociated with a unique patient visit to the hospi-
tal or emergency department. Since the task is to
retrieve relevant visits, we merged all the records
from a visit to form a single document for the visit,
which leads to 17,198 documents in the collection.
There are 34 queries in Med11 and 47 in Med12.
These queries were developed by domain experts
based on the ?inclusion criteria? of a clinical study
(Voorhees and Tong, 2011; Voorhees and Hersh,
2012).
After applying MetaMap to both documents and
queries, we can construct a concept-based collec-
tion. Since documents are often much longer, we
can first segment them into sentences, get the map-
ping results for each sentence, and then merge
them together to generate the concept-based rep-
resentation for the documents.
Table 1 compares the statistics of the term-
based and the concept-based collections, including
the number of unique tokens in the collection (i.e.,
the number of terms for term-based representa-
tion and the number of concepts for concept-based
representation), the average number of tokens in
the documents (AvgDL) and the average number
of tokens in the queries for these two collections
(AvgQL11 and AvgQL12). It is interesting to see
that the number of unique tokens is much smaller
when using the concept-based indexing. This is
expected since terms are semantically related and
a group of related terms would be mapped to one
semantic concept. Moreover, we observe that the
document length and query length are similar for
both collections. This is caused by the fact that
concepts are related and the MetaMap would map
an aspect to multiple related concepts.
Table 2 summarizes the methods that we com-
pare in the experiments. Following the evalua-
tion methodology used in the medical record track,
we use MAP@1000 as the primary measure for
Med11 and also report bpref. For Med12, we take
infNDCG@100 as the primary measure and also
report infAP@100. Different measures were cho-
sen for these two sets mainly because different
pooling strategies were used to create the judg-
ment pools (Voorhees and Hersh, 2012).
5.2 Performance Comparison
Table 3 shows the performance under optimized
parameter settings for all the methods over both
data sets. The performance is optimized in terms
of MAP in Med11, and infNDCG in Med12, re-
spectively. ? and b are tuned from 0 to 1 with the
step 0.1. Note that
T
,
C
and
TS
indicate improve-
ment over Term-BL, Concept-BL and TSConcept-
BL is statistically significant at 0.05 level based on
Wilcoxon signed-rank test, respectively.
Results show that Balanced method can signifi-
cantly improve the retrieval performance over both
collections. Unified method outperforms the base-
line methods in terms of the primary measure on
both collections, although it fails to improve the
infAP on Med12 for one baseline method. It is not
surprising to see that Balanced method is more ef-
fective than Unified since the former satisfies both
of the proposed retrieval constraints while the lat-
609
Table 4: Testing Performance
Trained on Med12 Med11
Tested on Med11 Med12
Measures MAP bpref infNDCG infAP
Term-BL 0.3451 0.4682 0.4640 0.2040
Concept-BL 0.3895 0.5394 0.5194 0.2441
TSConcept-BL 0.3901 0.5286 0.5208 0.2662
Unified 0.4176
T,C
0.5391
T
0.5346
T
0.2514
T
Balanced 0.4497
T ,C ,TS
0.5627
T ,C ,TS
0.5736
T ,C ,TS
0.2811
T ,C ,TS
ter satisfies only one. Finally, we noticed that
the performance difference between TSConcept-
BL and Concept-BL is not as significant as the
ones reported in the previous study (Limsopatham
et al, 2013d), which is probably caused by the
difference of problem set up (i.e., record-level vs.
visit-level as discussed in Section 3.1).
We also conduct experiments to train parame-
ters on one collection and compare the testing per-
formance on the other collection. The results are
summarized in Table 4. Clearly, Balanced is still
the most effective regularization method. The test-
ing performance is very close to the optimal per-
formance, which indicates that the proposed meth-
ods are robust with respect to the parameter set-
ting.
Moreover, we would like to point out that the
testing performance of Balanced is comparable
to the top ranked runs from the TREC Medical
records track. For example, the performance of
the best automatic system in Med11 (e.g., Cen-
gageM11R3) is 0.552 in terms of bpref, while
the performance of the best automatic system
in Med12 (e.g., udelSUM) is 0.578 in terms of
infNDCG. Note that the top system of Med12 used
multiple external resources such as Wikipedia and
Web, while we did not use such resources. More-
over, our performance might be further improved
if we apply the result filtering methods used by
many TREC participants (Leveling et al, 2012).
Table 5: Selecting representative concepts
MAP bpref
Unified (i.e., Unified-max) 0.4235 0.5443
Unified-min 0.3894 0.5202
Unified-avg 0.4164 0.5303
5.3 More Analysis
In the Unified method, we chose the concept with
the maximum IDF as the representative concept
Table 6: Estimating query aspect importance
MAP bpref
Balanced (i.e., Balanced-max) 0.4561 0.5697
Balanced-min 0.4216 0.5484
Balanced-avg 0.4397 0.5581
Table 7: Regularization components in Balanced
MAP bpref
Balanced 0.4561 0.5697
Confidence only 0.4294 0.5507
Importance only 0.4373 0.5598
among all the variants. We now conduct experi-
ments on Med11 to compare its performance with
those of using average IDF and minimum IDF
ones as the representative concept. The results are
shown in Table 5. It is clear that using maximum
IDF is the best choice, which is consistent with
our observation from the data exploratory analysis
shown in Figure 2.
In the Balanced method, we used the maximum
IDF value to estimate the query importance. We
also conduct experiments to compare its perfor-
mance with those using the minimum and aver-
age IDF values. Table 6 summarizes the results,
and shows that using the maximum IDF value per-
forms better than the other choices.
As shown in Equation (3), the Balanced method
regularizes the weights through two components:
(1) normalized confidence score of each aspect,
i.e.,
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
; and (2) the importance of the
query aspect, i.e., Imp
A
(A(e)). To examine the
effectiveness of each component, we conduct ex-
periments using the modified Balanced method
with only one of the components. The results are
shown in Table 7. It is clear that both components
are essential to improve the retrieval performance.
Finally, we report the performance improve-
ment of the proposed methods over the Concept-
BL for each query in Figure 3. Clearly, both of the
610
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
101 106 111 116 121 126 131 136 141 146 151 156 161 165 171 176 181 185
Pe
rfo
rm
an
ce
 D
iff
er
en
ce
Query ID
Improvement(Unified)
Improvement(Balanced)
Figure 3: Improvement of proposed methods (Compared with the Concept-BL method).
proposed methods can improve the effectiveness
of most queries, and the Balanced method is more
robust than the Unified method.
6 Conclusions and Future Work
Medical record retrieval is an important domain-
specific IR problem. Concept-based representa-
tion is an effective approach to dealing with am-
biguity terminology in medical domain. How-
ever, the results of the NLP tools used to gen-
erate the concept-based representation are often
not perfect. In this paper, we present a general
methodology that can use axiomatic approaches
as guidance to regularize the concept weighting
strategies to address the limitations caused by the
inaccurate concept mapping and improve the re-
trieval performance. In particular, we proposed
two weighting regularization methods based on
the relations among concepts. Experimental re-
sults show that the proposed methods can signif-
icantly outperform existing retrieval functions.
There are many interesting directions for our fu-
ture work. First, we plan to study how to automat-
ically predict whether to use concept-based index-
ing based on the quality of MetaMap results, and
explore whether the proposed methods are appli-
cable for other entity linking methods. Second,
we will study how to leverage other information
from knowledge bases to further improve the per-
formance. Third, more experiments could be con-
ducted to examine the effectiveness of the pro-
posed methods when using other ranking strate-
gies. Finally, it would be interesting to study how
to follow the proposed methodology to study other
domain-specific IR problems.
References
Gianni Amati and Cornelis Joost Van Rijsbergen.
2002. Probabilistic models of information retrieval
based on measuring the divergence from random-
ness. ACM TOIS.
Alan R. Aronson and Franc?ois-Michel Lang. 2010. An
overview of metamap: historical perspective and re-
cent advances. JAMIA, 17(3):229?236.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of AMIA Symposium.
Practice Management Information Corporation. 1999.
ICD-9-CM: International Classification of Dis-
eases, 9th Revision, Clinical Modification, 5th Edi-
tion. Practice Management Information Corpora-
tion.
Dina Demner-Fushman, Swapna Abhyankar, Anto-
nio Jimeno-Yepes, Russell Loane, Francois Lang,
James G. Mork, Nicholas Ide, and Alan R. Aron-
son. 2012. NLM at TREC 2012 Medical Records
Track. In Proceedings of TREC 2012.
Hui Fang and ChengXiang Zhai. 2005. An exploration
of axiomatic approaches to information retrieval. In
Proceedings of SIGIR?05.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A
formal study of information retrieval heuristics. In
Proceedings of SIGIR?04.
Charles P. Friedman, Adam K. Wong, and David Blu-
menthal. 2010. Achieving a nationwide learning
health system. Science Translational Medicine.
Beval Koopman, Michael Lawley, and Peter Bruza.
2011. AEHRC & QUT at TREC 2011 Medical
Track : a concept-based information retrieval ap-
proach. In Proceedings of TREC?11.
Bevan Koopman, Guido Zuccon, Anthony Nguyen,
Deanne Vickers, Luke Butt, and Peter D. Bruza.
611
2012. Exploiting SNOMED CT Concepts & Re-
lationships for Clinical Information Retrieval: Aus-
tralian e-Health Research Centre and Queensland
University of Technology at the TREC 2012 Med-
ical Track. In Proceedings of TREC?12.
Johannes Leveling, Lorraine Goeuriot, Liadh Kelly,
and Gareth J. F. Jones. 2012. DCU@TRECMed
2012: Using adhoc Baselines for Domain-Specific
Retrieval. In Proceedings of TREC 2012.
Nut Limsopatham, Craig Macdonald, and Iadh Ou-
nis. 2013a. Inferring conceptual relationships to
improve medical records search. In Proceedings of
OAIR?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ou-
nis. 2013b. Learning to combine representations
for medical records search. In Proceedings of SI-
GIR?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ounis.
2013c. Learning to selectively rank patients? medi-
cal history. In Proceedings of CIKM?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ounis.
2013d. A task-specific query and document repre-
sentation for medical records search. In Proceedings
of ECIR?13.
Jimmy Lin and Dina Demner-Fushman. 2006. The
role of knowledge in conceptual retrieval: a study
in the domain of clinical medicine. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 99?106, New York, NY,
USA. ACM.
Carolyn E Lipscomb. 2000. Medical Subject Headings
(MeSH). The Medical Library Association.
Christopher D. Manning, P. Raghavan, and H. Schutze.
2008. Introduction to Information Retrieval. Cam-
bridge University Press.
Bridget T. McInnes, Ted Pedersen, and Serguei V. S.
Pakhomov. 2009. UMLS-Interface and UMLS-
Similarity : Open Source Software for Measuring
Paths and Semantic Similarity. In Proceedings of
AMIA Symposium.
Yanjun Qi and Pierre-Francois Laquerre. 2012. Re-
trieving Medical Records: NEC Labs America at
TREC 2012 Medical Record Track. In Proceedings
of TREC 2012.
S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-
Beaulieu, and M. Gatford. 1996. Okapi at TREC-3.
pages 109?126.
Charles Safran, Meryl Bloomrosen, W. Edward
Hammond, Steven Labkoff, Suzanne Markel-Fox,
Paul C. Tang, and Don E. Detmer. 2007. White pa-
per: Toward a national framework for the secondary
use of health data: An american medical informatics
association white paper. JAMIA, 14(1):1?9.
Amit Singhal, Chris Buckley, and Mandar Mitra. 1996.
Pivoted document length normalization. In Pro-
ceedings of SIGIR?96.
Ellen M. Voorhees and William Hersh. 2012.
Overview of the TREC 2012 Medical Records
Track. In Proceedings of TREC 2012.
Ellen M. Voorhees and Richard M. Tong. 2011.
Overview of the TREC 2011 Medical Records
Track. In Proceedings of TREC 2011.
Xin Yan, Raymond Y.K. Lau, Dawei Song, Xue Li,
and Jian Ma. 2011. Toward a semantic granular-
ity model for domain-specific information retrieval.
ACM TOIS.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to Ad Hoc information retrieval. In Proceedings of
SIGIR?01.
Wei Zheng and Hui Fang. 2010. Query aspect
based term weighting regularization in information
retrieval. In Proceedings of ECIR?10.
Wei Zhou, Clement Yu, Neil Smalheiser, Vetle Torvik,
and Jie Hong. 2007. Knowledge-intensive concep-
tual retrieval and passage extraction of biomedical
literature. In Proceedings of SIGIR?07.
Dongqing Zhu and Ben Carterette. 2012. Combining
multi-level evidence for medical record retrieval. In
Proceedings of SHB?12.
612
