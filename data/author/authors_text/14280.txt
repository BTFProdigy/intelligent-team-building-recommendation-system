An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrated Shallow and Deep Parsing: TopP meets HPSG
Anette Frank, Markus Beckerz, Berthold Crysmann, Bernd Kiefer and Ulrich Scha?fer
DFKI GmbH School of Informaticsz
66123 Saarbru?cken, Germany University of Edinburgh, UK
firstname.lastname@dfki.de M.Becker@ed.ac.uk
Abstract
We present a novel, data-driven method
for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer
annotation architecture, we interleave a
robust, but accurate stochastic topological
field parser of German with a constraint-
based HPSG parser. Our annotation-based
method for dovetailing shallow and deep
phrasal constraints is highly flexible, al-
lowing targeted and fine-grained guidance
of constraint-based parsing. We conduct
systematic experiments that demonstrate
substantial performance gains.1
1 Introduction
One of the strong points of deep processing (DNLP)
technology such as HPSG or LFG parsers certainly
lies with the high degree of precision as well as
detailed linguistic analysis these systems are able
to deliver. Although considerable progress has been
made in the area of processing speed, DNLP systems
still cannot rival shallow and medium depth tech-
nologies in terms of throughput and robustness. As
a net effect, the impact of deep parsing technology
on application-oriented NLP is still fairly limited.
With the advent of XML-based hybrid shallow-
deep architectures as presented in (Grover and Las-
carides, 2001; Crysmann et al, 2002; Uszkoreit,
2002) it has become possible to integrate the added
value of deep processing with the performance and
robustness of shallow processing. So far, integration
has largely focused on the lexical level, to improve
upon the most urgent needs in increasing the robust-
ness and coverage of deep parsing systems, namely
1This work was in part supported by a BMBF grant to the
DFKI project WHITEBOARD (FKZ 01 IW 002).
lexical coverage. While integration in (Grover and
Lascarides, 2001) was still restricted to morphologi-
cal and PoS information, (Crysmann et al, 2002) ex-
tended shallow-deep integration at the lexical level
to lexico-semantic information, and named entity
expressions, including multiword expressions.
(Crysmann et al, 2002) assume a vertical,
?pipeline? scenario where shallow NLP tools provide
XML annotations that are used by the DNLP system
as a preprocessing and lexical interface. The per-
spective opened up by a multi-layered, data-centric
architecture is, however, much broader, in that it en-
courages horizontal cross-fertilisation effects among
complementary and/or competing components.
One of the culprits for the relative inefficiency of
DNLP parsers is the high degree of ambiguity found
in large-scale grammars, which can often only be re-
solved within a larger syntactic domain. Within a hy-
brid shallow-deep platform one can take advantage
of partial knowledge provided by shallow parsers to
pre-structure the search space of the deep parser. In
this paper, we will thus complement the efforts made
on the lexical side by integration at the phrasal level.
We will show that this may lead to considerable per-
formance increase for the DNLP component. More
specifically, we combine a probabilistic topological
field parser for German (Becker and Frank, 2002)
with the HPSG parser of (Callmeier, 2000). The
HPSG grammar used is the one originally developed
by (Mu?ller and Kasper, 2000), with significant per-
formance enhancements by B. Crysmann.
In Section 2 we discuss the mapping problem
involved with syntactic integration of shallow and
deep analyses and motivate our choice to combine
the HPSG system with a topological parser. Sec-
tion 3 outlines our basic approach towards syntactic
shallow-deep integration. Section 4 introduces vari-
ous confidence measures, to be used for fine-tuning
of phrasal integration. Sections 5 and 6 report on
experiments and results of integrated shallow-deep
parsing, measuring the effect of various integra-
tion parameters on performance gains for the DNLP
component. Section 7 concludes and discusses pos-
sible extensions, to address robustness issues.
2 Integrated Shallow and Deep Processing
The prime motivation for integrated shallow-deep
processing is to combine the robustness and effi-
ciency of shallow processing with the accuracy and
fine-grainedness of deep processing. Shallow analy-
ses could be used to pre-structure the search space of
a deep parser, enhancing its efficiency. Even if deep
analysis fails, shallow analysis could act as a guide
to select partial analyses from the deep parser?s chart
? enhancing the robustness of deep analysis, and the
informativeness of the combined system.
In this paper, we concentrate on the usage of shal-
low information to increase the efficiency, and po-
tentially the quality, of HPSG parsing. In particu-
lar, we want to use analyses delivered by an effi-
cient shallow parser to pre-structure the search space
of HPSG parsing, thereby enhancing its efficiency,
and guiding deep parsing towards a best-first analy-
sis suggested by shallow analysis constraints.
The search space of an HPSG chart parser can
be effectively constrained by external knowledge
sources if these deliver compatible partial subtrees,
which would then only need to be checked for com-
patibility with constituents derived in deep pars-
ing. Raw constituent span information can be used
to guide the parsing process by penalizing con-
stituents which are incompatible with the precom-
puted ?shape?. Additional information about pro-
posed constituents, such as categorial or featural
constraints, provide further criteria for prioritis-
ing compatible, and penalising incompatible con-
stituents in the deep parser?s chart.
An obvious challenge for our approach is thus to
identify suitable shallow knowledge sources that can
deliver compatible constraints for HPSG parsing.
2.1 The Shallow-Deep Mapping Problem
However, chunks delivered by state-of-the-art shal-
low parsers are not isomorphic to deep syntactic
analyses that explicitly encode phrasal embedding
structures. As a consequence, the boundaries of
deep grammar constituents in (1.a) cannot be pre-
determined on the basis of a shallow chunk analy-
sis (1.b). Moreover, the prevailing greedy bottom-up
processing strategies applied in chunk parsing do not
take into account the macro-structure of sentences.
They are thus easily trapped in cases such as (2).
(1) a. [
CL
There was [
NP
a rumor [
CL
it was going
to be bought by [
NP
a French company [
CL
that
competes in supercomputers]]]]].
b. [
CL
There was [
NP
a rumor]] [
CL
it was going
to be bought by [
NP
a French company]] [
CL
that competes in supercomputers].
(2) Fred eats [
NP
pizza and Mary] drinks wine.
In sum, state-of-the-art chunk parsing does nei-
ther provide sufficient detail, nor the required accu-
racy to act as a ?guide? for deep syntactic analysis.
2.2 Stochastic Topological Parsing
Recently, there is revived interest in shallow anal-
yses that determine the clausal macro-structure of
sentences. The topological field model of (German)
syntax (Ho?hle, 1983) divides basic clauses into dis-
tinct fields ? pre-, middle-, and post-fields ? delim-
ited by verbal or sentential markers, which consti-
tute the left/right sentence brackets. This model of
clause structure is underspecified, or partial as to
non-sentential constituent structure, but provides a
theory-neutral model of sentence macro-structure.
Due to its linguistic underpinning, the topologi-
cal field model provides a pre-partitioning of com-
plex sentences that is (i) highly compatible with
deep syntactic analysis, and thus (ii) maximally ef-
fective to increase parsing efficiency if interleaved
with deep syntactic analysis; (iii) partiality regarding
the constituency of non-sentential material ensures
robustness, coverage, and processing efficiency.
(Becker and Frank, 2002) explored a corpus-
based stochastic approach to topological field pars-
ing, by training a non-lexicalised PCFG on a topo-
logical corpus derived from the NEGRA treebank of
German. Measured on the basis of hand-corrected
PoS-tagged input as provided by the NEGRA tree-
bank, the parser achieves 100% coverage for length
 40 (99.8% for all). Labelled precision and recall
are around 93%. Perfect match (full tree identity) is
about 80% (cf. Table 1, disamb +).
In this paper, the topological parser was provided
a tagger front-end for free text processing, using the
TnT tagger (Brants, 2000). The grammar was ported
to the efficient LoPar parser of (Schmid, 2000). Tag-
ging inaccuracies lead to a drop of 5.1/4.7 percent-
CL-V2
VF-TOPIC LK-VFIN MF RK-VPART NF
ART NN VAFIN ART ADJA NN VAPP CL-SUBCL
Der,1 Zehnkampf,2 ha?tte,3 eine,4 andere,5 Dimension,6 gehabt,7 ,
The decathlon would have a other dimension had LK-COMPL MF RK-VFIN
KOUS PPER PROAV VAPP VAFIN
wenn,9 er,10 dabei,11 gewesen,12 wa?re,13 .
if he there been had .
<TOPO2HPSG type=?root? id=?5608?>
<MAP CONSTR id=?T1? constr=?v2 cp? conf
ent
=?0.87? left=?W1? right=?W13?/>
<MAP CONSTR id=?T2? constr=?v2 vf? conf
ent
=?0.87? left=?W1? right=?W2?/>
<MAP CONSTR id=?T3? constr=?vfronted vfin+rk? conf
ent
=?0.87? left=?W3? right=?W3?/>
<MAP CONSTR id=?T6? constr=?vfronted rk-complex? conf
ent
=?0.87? left=?W7? right=?W7?/>
<MAP CONSTR id=?T4? constr=?vfronted vfin+vp+rk? conf
ent
=?0.87? left=?W3? right=?W13?/>
<MAP CONSTR id=?T5? constr=?vfronted vp+rk? conf
ent
=?0.87? left=?W4? right=?W13?/>
<MAP CONSTR id=?T10? constr=?extrapos rk+nf? conf
ent
=?0.87? left=?W7? right=?W13?/>
<MAP CONSTR id=?T7? constr=?vl cpfin compl? conf
ent
=?0.87? left=?W9? right=?W13?/>
<MAP CONSTR id=?T8? constr=?vl compl vp? conf
ent
=?0.87? left=?W10? right=?W13?/>
<MAP CONSTR id=?T9? constr=?vl rk fin+complex+finlast? conf
ent
=?0.87? left=?W12? right=?W13?/>
</TOPO2HPSG>
Der
D
Zehnkampf
N?
NP-NOM-SG
haette
V
eine
D
andere
AP-ATT
Dimension
N?
N?
NP-ACC-SG
gehabt
V
EPS
wenn
C
er
NP-NOM-SG
dabei
PP
gewesen
V
waere
V-LE
V
V
S
CP-MOD
EPS
EPS
EPS/NP-NOM-SG
S/NP-NOM-SG
S
Figure 1: Topological tree w/param. cat., TOPO2HPSG map-constraints, tree skeleton of HPSG analysis
dis- cove- perfect LP LR 0CB 2CB
amb rage match in % in % in % in %
+ 100.0 80.4 93.4 92.9 92.1 98.9
  99.8 72.1 88.3 88.2 87.8 97.9
Table 1: Disamb: correct (+) / tagger ( ) PoS input.
Eval. on atomic (vs. parameterised) category labels.
age points in LP/LR, and 8.3 percentage points in
perfect match rate (Table 1, disamb  ).
As seen in Figure 1, the topological trees abstract
away from non-sentential constituency ? phrasal
fields MF (middle-field) and VF (pre-field) directly
expand to PoS tags. By contrast, they perfectly ren-
der the clausal skeleton and embedding structure of
complex sentences. In addition, parameterised cate-
gory labels encode larger syntactic contexts, or ?con-
structions?, such as clause type (CL-V2, -SUBCL,
-REL), or inflectional patterns of verbal clusters (RK-
VFIN,-VPART). These properties, along with their
high accuracy rate, make them perfect candidates for
tight integration with deep syntactic analysis.
Moreover, due to the combination of scrambling
and discontinuous verb clusters in German syntax, a
deep parser is confronted with a high degree of local
ambiguity that can only be resolved at the clausal
level. Highly lexicalised frameworks such as HPSG,
however, do not lend themselves naturally to a top-
down parsing strategy. Using topological analyses to
guide the HPSG will thus provide external top-down
information for bottom-up parsing.
3 TopP meets HPSG
Our work aims at integration of topological and
HPSG parsing in a data-centric architecture, where
each component acts independently2 ? in contrast
to the combination of different syntactic formalisms
within a unified parsing process.3 Data-based inte-
gration not only favours modularity, but facilitates
flexible and targeted dovetailing of structures.
3.1 Mapping Topological to HPSG Structures
While structurally similar, topological trees are not
fully isomorphic to HPSG structures. In Figure 1,
e.g., the span from the verb ?ha?tte? to the end of the
sentence forms a constituent in the HPSG analysis,
while in the topological tree the same span is domi-
nated by a sequence of categories: LK, MF, RK, NF.
Yet, due to its linguistic underpinning, the topo-
logical tree can be used to systematically predict
key constituents in the corresponding ?target? HPSG
2See Section 6 for comparison to recent work on integrated
chunk-based and dependency parsing in (Daum et al, 2003).
3As, for example, in (Duchier and Debusmann, 2001).
analysis. We know, for example, that the span from
the fronted verb (LK-VFIN) till the end of its clause
CL-V2 corresponds to an HPSG phrase. Also, the
first position that follows this verb, here the leftmost
daughter of MF, demarcates the left edge of the tra-
ditional VP. Spans of the vorfeld VF and clause cat-
egories CL exactly match HPSG constituents. Cate-
gory CL-V2 tells us that we need to reckon with a
fronted verb in position of its LK daughter, here 3,
while in CL-SUBCL we expect a complementiser in
the position of LK, and a finite verb within the right
verbal complex RK, which spans positions 12 to 13.
In order to communicate such structural con-
straints to the deep parser, we scan the topological
tree for relevant configurations, and extract the span
information for the target HPSG constituents. The
resulting ?map constraints? (Fig. 1) encode a bracket
type name4 that identifies the target constituent and
its left and right boundary, i.e. the concrete span in
the sentence under consideration. The span is en-
coded by the word position index in the input, which
is identical for the two parsing processes.5
In addition to pure constituency constraints, a
skilled grammar writer will be able to associate spe-
cific HPSG grammar constraints ? positive or neg-
ative ? with these bracket types. These additional
constraints will be globally defined, to permit fine-
grained guidance of the parsing process. This and
further information (cf. Section 4) is communicated
to the deep parser by way of an XML interface.
3.2 Annotation-based Integration
In the annotation-based architecture of (Crysmann
et al, 2002), XML-encoded analysis results of all
components are stored in a multi-layer XML chart.
The architecture employed in this paper improves
on (Crysmann et al, 2002) by providing a central
Whiteboard Annotation Transformer (WHAT) that
supports flexible and powerful access to and trans-
formation of XML annotation based on standard
XSLT engines6 (see (Scha?fer, 2003) for more de-
tails on WHAT). Shallow-deep integration is thus
fully annotation driven. Complex XSLT transforma-
tions are applied to the various analyses, in order to
4We currently extract 34 different bracket types.
5We currently assume identical tokenisation, but could ac-
commodate for distinct tokenisation regimes, using map tables.
6Advantages we see in the XSLT approach are (i) minimised
programming effort in the target implementation language for
XML access, (ii) reuse of transformation rules in multiple mod-
ules, (iii) fast integration of new XML-producing components.
extract or combine independent knowledge sources,
including XPath access to information stored in
shallow annotation, complex XSLT transformations
to the output of the topological parser, and extraction
of bracket constraints.
3.3 Shaping the Deep Parser?s Search Space
The HPSG parser is an active bidirectional chart
parser which allows flexible parsing strategies by us-
ing an agenda for the parsing tasks.7 To compute pri-
orities for the tasks, several information sources can
be consulted, e.g. the estimated quality of the parti-
cipating edges or external resources like PoS tagger
results. Object-oriented implementation of the prior-
ity computation facilitates exchange and, moreover,
combination of different ranking strategies. Extend-
ing our current regime that uses PoS tagging for pri-
oritisation,8 we are now utilising phrasal constraints
(brackets) from topological analysis to enhance the
hand-crafted parsing heuristic employed so far.
Conditions for changing default priorities Ev-
ery bracket pair br
x
computed from the topological
analysis comes with a bracket type x that defines its
behaviour in the priority computation. Each bracket
type can be associated with a set of positive and neg-
ative constraints that state a set of permissible or for-
bidden rules and/or feature structure configurations
for the HPSG analysis.
The bracket types fall into three main categories:
left-, right-, and fully matching brackets. A right-
matching bracket may affect the priority of tasks
whose resulting edge will end at the right bracket
of a pair, like, for example, a task that would
combine edges C and F or C and D in Fig. 2.
Left-matching brackets work analogously. For fully
matching brackets, only tasks that produce an edge
that matches the span of the bracket pair can be af-
fected, like, e.g., a task that combines edges B and C
in Fig. 2. If, in addition, specified rule as well as fea-
ture structure constraints hold, the task is rewarded
if they are positive constraints, and penalised if they
are negative ones. All tasks that produce crossing
edges, i.e. where one endpoint lies strictly inside the
bracket pair and the other lies strictly outside, are
penalised, e.g., a task that combines edges A and B.
This behaviour can be implemented efficiently
when we assume that the computation of a task pri-
7A parsing task encodes the possible combination of a pas-
sive and an active chart edge.
8See e.g. (Prins and van Noord, 2001) for related work.
brxbrx
A
B C
D E
F
Figure 2: An example chart with a bracket pair of
type x. The dashed edges are active.
ority takes into account the priorities of the tasks it
builds upon. This guarantees that the effect of chang-
ing one task in the parsing process will propagate
to all depending tasks without having to check the
bracket conditions repeatedly.
For each task, it is sufficient to examine the start-
and endpoints of the building edges to determine if
its priority is affected by some bracket. Only four
cases can occur:
1. The new edge spans a pair of brackets: a match
2. The new edge starts or ends at one of the brack-
ets, but does not match: left or right hit
3. One bracket of a pair is at the joint of the build-
ing edges and a start- or endpoint lies strictly
inside the brackets: a crossing (edges A and B
in Fig. 2)
4. No bracket at the endpoints of both edges: use
the default priority
For left-/right-matching brackets, a match behaves
exactly like the corresponding left or right hit.
Computing the new priority If the priority of a
task is changed, the change is computed relative to
the default priority. We use two alternative confi-
dence values, and a hand-coded parameter (x), to
adjust the impact on the default priority heuristics.
conf
ent
(br
x
) specifies the confidence for a concrete
bracket pair br
x
of type x in a given sentence, based
on the tree entropy of the topological parse. conf
pr
specifies a measure of ?expected accuracy? for each
bracket type. Sec. 4 will introduce these measures.
The priority p(t) of a task t involving a bracket
br
x
is computed from the default priority ~p(t) by:
p(t) = ~p(t)  (1 conf
ent
(br
x
)  conf
pr
(x) (x))
4 Confidence Measures
This way of calculating priorities allows flexible pa-
rameterisation for the integration of bracket con-
straints. While the topological parser?s accuracy is
high, we need to reckon with (partially) wrong anal-
yses that could counter the expected performance
gains. An important factor is therefore the confi-
dence we can have, for any new sentence, into the
best parse delivered by the topological parser: If
confidence is high, we want it to be fully considered
for prioritisation ? if it is low, we want to lower its
impact, or completely ignore the proposed brackets.
We will experiment with two alternative confi-
dence measures: (i) expected accuracy of particular
bracket types extracted from the best parse deliv-
ered, and (ii) tree entropy based on the probability
distribution encountered in a topological parse, as
a measure of the overall accuracy of the best parse
proposed ? and thus the extracted brackets.9
4.1 Conf
pr
: Accuracy of map-constraints
To determine a measure of ?expected accuracy? for
the map constraints, we computed precision and re-
call for the 34 bracket types by comparing the ex-
tracted brackets from the suite of best delivered
topological parses against the brackets we extracted
from the trees in the manually annotated evalua-
tion corpus in (Becker and Frank, 2002). We obtain
88.3% precision, 87.8% recall for brackets extracted
from the best topological parse, run with TnT front
end. We chose precision of extracted bracket types
as a static confidence weight for prioritisation.
Precision figures are distributed as follows: 26.5%
of the bracket types have precision  90% (93.1%
in avg, 53.5% of bracket mass), 50% have pre-
cision  80% (88.9% avg, 77.7% bracket mass).
20.6% have precision  50% (41.26% in avg, 2.7%
bracket mass). For experiments using a threshold
on conf
pr
(x) for bracket type x, we set a threshold
value of 0.7, which excludes 32.35% of the low-
confidence bracket types (and 22.1% bracket mass),
and includes chunk-based brackets (see Section 5).
4.2 Conf
ent
: Entropy of Parse Distribution
While precision over bracket types is a static mea-
sure that is independent from the structural complex-
ity of a particular sentence, tree entropy is defined as
the entropy over the probability distribution of the
set of parsed trees for a given sentence. It is a use-
ful measure to assess how certain the parser is about
the best analysis, e.g. to measure the training utility
value of a data point in the context of sample selec-
tion (Hwa, 2000). We thus employ tree entropy as a
9Further measures are conceivable: We could extract brack-
ets from some n-best topological parses, associating them with
weights, using methods similar to (Carroll and Briscoe, 2002).
10
20
30
40
50
60
70
80
90
00.20.40.60.81
in
 %
Normalized entropy
precision
recall
coverage
Figure 3: Effect of different thresholds of normal-
ized entropy on precision, recall, and coverage
confidence measure for the quality of the best topo-
logical parse, and the extracted bracket constraints.
We carry out an experiment to assess the effect
of varying entropy thresholds  on precision and re-
call of topological parsing, in terms of perfect match
rate, and show a way to determine an optimal value
for . We compute tree entropy over the full prob-
ability distribution, and normalise the values to be
distributed in a range between 0 and 1. The normali-
sation factor is empirically determined as the highest
entropy over all sentences of the training set.10
Experimental setup We randomly split the man-
ually corrected evaluation corpus of (Becker and
Frank, 2002) (for sentence length  40) into a train-
ing set of 600 sentences and a test set of 408 sen-
tences. This yields the following values for the train-
ing set (test set in brackets): initial perfect match
rate is 73.5% (70.0%), LP 88.8% (87.6%), and LR
88.5% (87.8%).11 Coverage is 99.8% for both.
Evaluation measures For the task of identifying
the perfect matches from a set of parses we give the
following standard definitions: precision is the pro-
portion of selected parses that have a perfect match
? thus being the perfect match rate, and recall is the
proportion of perfect matches that the system se-
lected. Coverage is usually defined as the proportion
of attempted analyses with at least one parse. We ex-
tend this definition to treat successful analyses with
a high tree entropy as being out of coverage. Fig. 3
shows the effect of decreasing entropy thresholds
 on precision, recall and coverage. The unfiltered
set of all sentences is found at =1. Lowering  in-
10Possibly higher values in the test set will be clipped to 1.
11Evaluation figures for this experiment are given disregard-
ing parameterisation (and punctuation), corresponding to the
first row of figures in table 1.
82
84
86
88
90
92
94
96
0.160.180.20.220.240.260.280.3
in
 %
Normalized entropy
precision
recall
f-measure
Figure 4: Maximise f-measure on the training set to
determine best entropy threshold
creases precision, and decreases recall and coverage.
We determine f-measure as composite measure of
precision and recall with equal weighting (=0.5).
Results We use f-measure as a target function on
the training set to determine a plausible . F-measure
is maximal at =0.236 with 88.9%, see Figure 4.
Precision and recall are 83.7% and 94.8% resp.
while coverage goes down to 83.0%. Applying the
same  on the test set, we get the following results:
80.5% precision, 93.0% recall. Coverage goes down
to 80.6%. LP is 93.3%, LR is 91.2%.
Confidence Measure We distribute the comple-
ment of the associated tree entropy of a parse tree tr
as a global confidence measure over all brackets br
extracted from that parse: conf
ent
(br) = 1 ent(tr).
For the thresholded version of conf
ent
(br), we set
the threshold to 1   = 1  0:236 = 0:764.
5 Experiments
Experimental Setup In the experiments we use
the subset of the NEGRA corpus (5060 sents,
24.57%) that is currently parsed by the HPSG gram-
mar.12 Average sentence length is 8.94, ignoring
punctuation; average lexical ambiguity is 3.05 en-
tries/word. As baseline, we performed a run with-
out topological information, yet including PoS pri-
oritisation from tagging.13 A series of tests explores
the effects of alternative parameter settings. We fur-
ther test the impact of chunk information. To this
12This test set is different from the corpus used in Section 4.
13In a comparative run without PoS-priorisation, we estab-
lished a speed-up factor of 1.13 towards the baseline used in
our experiment, with a slight increase in coverage (1%). This
compares to a speed-up factor of 2.26 reported in (Daum et al,
2003), by integration of PoS guidance into a dependency parser.
end, phrasal fields determined by topological pars-
ing were fed to the chunk parser of (Skut and Brants,
1998). Extracted NP and PP bracket constraints are
defined as left-matching bracket types, to compen-
sate for the non-embedding structure of chunks.
Chunk brackets are tested in conjunction with topo-
logical brackets, and in isolation, using the labelled
precision value of 71.1% in (Skut and Brants, 1998)
as a uniform confidence weight.14
Measures For all runs we measure the absolute
time and the number of parsing tasks needed to com-
pute the first reading. The times in the individual
runs were normalised according to the number of
executed tasks per second. We noticed that the cov-
erage of some integrated runs decreased by up to
1% of the 5060 test items, with a typical loss of
around 0.5%. To warrant that we are not just trading
coverage for speed, we derived two measures from
the primary data: an upper bound, where we asso-
ciated every unsuccessful parse with the time and
number of tasks used when the limit of 70000 pas-
sive edges was hit, and a lower bound, where we
removed the most expensive parses from each run,
until we reached the same coverage. Whereas the
upper bound is certainly more realistic in an applica-
tion context, the lower bound gives us a worst case
estimate of expectable speed-up.
Integration Parameters We explored the follow-
ing range of weighting parameters for prioritisation
(see Section 3.3 and Table 2).
We use two global settings for the heuristic pa-
rameter . Setting  to 1
2
without using any confi-
dence measure causes the priority of every affected
parsing task to be in- or decreased by half its value.
Setting  to 1 drastically increases the influence of
topological information, the priority for rewarded
tasks is doubled and set to zero for penalized ones.
The first two runs (rows with  P  E) ignore
both confidence parameters (conf
pr=ent
=1), measur-
ing only the effect of higher or lower influence of
topological information. In the remaining six runs,
the impact of the confidence measures conf
pr=ent
is
tested individually, namely +P  E and  P +E, by
setting the resp. alternative value to 1. For two runs,
we set the resp. confidence values that drop below
a certain threshold to zero (PT, ET) to exclude un-
14The experiments were run on a 700 MHz Pentium III ma-
chine. For all runs, the maximum number of passive edges was
set to the comparatively high value of 70000.
factor msec (1st) tasks
low-b up-b low-b up-b low-b up-b
Baseline     524 675 3813 4749
Integration of topological brackets w/ parameters
 P  E  1
2
2.21 2.17 237 310 1851 2353
 P  E 1 2.04 2.10 257 320 2037 2377
+P  E  1
2
2.15 2.21 243 306 1877 2288
PT  E  1
2
2.20 2.30 238 294 1890 2268
 P +E  1
2
2.27 2.23 230 302 1811 2330
 P ET  1
2
2.10 2.00 250 337 1896 2503
+P  E 1 2.06 2.12 255 318 2021 2360
PT  E 1 2.08 2.10 252 321 1941 2346
PT with chunk and topological brackets
PT  E  1
2
2.13 2.16 246 312 1929 2379
PT with chunk brackets only
PT  E  1
2
0.89 1.10 589 611 4102 4234
Table 2: Priority weight parameters and results
certain candidate brackets or bracket types. For runs
including chunk bracketing constraints, we chose
thresholded precision (PT) as confidence weights
for topological and/or chunk brackets.
6 Discussion of Results
Table 2 summarises the results. A high impact on
bracket constraints (1) results in lower perfor-
mance gains than using a moderate impact ( 1
2
)
(rows 2,4,5 vs. 3,8,9). A possible interpretation is
that for high , wrong topological constraints and
strong negative priorities can mislead the parser.
Use of confidence weights yields the best per-
formance gains (with  1
2
), in particular, thresholded
precision of bracket types PT, and tree entropy
+E, with comparable speed-up of factor 2.2/2.3 and
2.27/2.23 (2.25 if averaged). Thresholded entropy
ET yields slightly lower gains. This could be due to
a non-optimal threshold, or the fact that ? while pre-
cision differentiates bracket types in terms of their
confidence, such that only a small number of brack-
ets are weakened ? tree entropy as a global measure
penalizes all brackets for a sentence on an equal ba-
sis, neutralizing positive effects which ? as seen in
+/ P ? may still contribute useful information.
Additional use of chunk brackets (row 10) leads
to a slight decrease, probably due to lower preci-
sion of chunk brackets. Even more, isolated use of
chunk information (row 11) does not yield signifi-
01000
2000
3000
4000
5000
6000
7000
0 5 10 15 20 25 30 35
baseline
+PT ?(0.5)
12867 12520 11620 9290
0
100
200
300
400
500
600
#sentences
msec
Figure 5: Performance gain/loss per sentence length
cant gains over the baseline (0.89/1.1). Similar re-
sults were reported in (Daum et al, 2003) for inte-
gration of chunk- and dependency parsing.15
For PT -E  1
2
, Figure 5 shows substantial per-
formance gains, with some outliers in the range of
length 25?36. 962 sentences (length >3, avg. 11.09)
took longer parse time as compared to the baseline
(with 5% variance margin). For coverage losses, we
isolated two factors: while erroneous topological in-
formation could lead the parser astray, we also found
cases where topological information prevented spu-
rious HPSG parses to surface. This suggests that
the integrated system bears the potential of cross-
validation of different components.
7 Conclusion
We demonstrated that integration of shallow topo-
logical and deep HPSG processing results in signif-
icant performance gains, of factor 2.25?at a high
level of deep parser efficiency. We show that macro-
structural constraints derived from topological pars-
ing improve significantly over chunk-based con-
straints. Fine-grained prioritisation in terms of con-
fidence weights could further improve the results.
Our annotation-based architecture is now easily
extended to address robustness issues beyond lexical
matters. By extracting spans for clausal fragments
from topological parses, in case of deep parsing fail-
15(Daum et al, 2003) report a gain of factor 2.76 relative to a
non-PoS-guided baseline, which reduces to factor 1.21 relative
to a PoS-prioritised baseline, as in our scenario.
ure the chart can be inspected for spanning anal-
yses for sub-sentential fragments. Further, we can
simplify the input sentence, by pruning adjunct sub-
clauses, and trigger reparsing on the pruned input.
References
M. Becker and A. Frank. 2002. A Stochastic Topological
Parser of German. In Proceedings of COLING 2002,
pages 71?77, Taipei, Taiwan.
T. Brants. 2000. Tnt - A Statistical Part-of-Speech Tag-
ger. In Proceedings of Eurospeech, Rhodes, Greece.
U. Callmeier. 2000. PET ? A platform for experimenta-
tion with efficient HPSG processing techniques. Nat-
ural Language Engineering, 6 (1):99 ? 108.
C. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of COL-
ING 2002, pages 134?140.
B. Crysmann, A. Frank, B. Kiefer, St. Mu?ller, J. Pisko-
rski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu,
M. Becker, and H.-U. Krieger. 2002. An Integrated
Architecture for Deep and Shallow Processing. In
Proceedings of ACL 2002, Pittsburgh.
M. Daum, K.A. Foth, and W. Menzel. 2003. Constraint
Based Integration of Deep and Shallow Parsing Tech-
niques. In Proceedings of EACL 2003, Budapest.
D. Duchier and R. Debusmann. 2001. Topological De-
pendency Trees: A Constraint-based Account of Lin-
ear Precedence. In Proceedings of ACL 2001.
C. Grover and A. Lascarides. 2001. XML-based data
preparation for robust deep parsing. In Proceedings of
ACL/EACL 2001, pages 252?259, Toulouse, France.
T. Ho?hle. 1983. Topologische Felder. Unpublished
manuscript, University of Cologne.
R. Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proceedings of EMNLP/VLC-2000,
pages 45?52, Hong Kong.
S. Mu?ller and W. Kasper. 2000. HPSG analysis of
German. In W. Wahlster, editor, Verbmobil: Founda-
tions of Speech-to-Speech Translation, Artificial Intel-
ligence, pages 238?253. Springer, Berlin.
R. Prins and G. van Noord. 2001. Unsupervised pos-
tagging improves parsing accuracy and parsing effi-
ciency. In Proceedings of IWPT, Beijing.
U. Scha?fer. 2003. WHAT: An XSLT-based Infrastruc-
ture for the Integration of Natural Language Process-
ing Components. In Proceedings of the SEALTS Work-
shop, HLT-NAACL03, Edmonton, Canada.
H. Schmid, 2000. LoPar: Design and Implementation.
IMS, Stuttgart. Arbeitspapiere des SFB 340, Nr. 149.
W. Skut and T. Brants. 1998. Chunk tagger: statistical
recognition of noun phrases. In ESSLLI-1998 Work-
shop on Automated Acquisition of Syntax and Parsing.
H. Uszkoreit. 2002. New Chances for Deep Linguistic
Processing. In Proceedings of COLING 2002, pages
xiv?xxvii, Taipei, Taiwan.
Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
WHAT: An XSLT-based Infrastructure for the Integration of Natural
Language Processing Components
Ulrich Sch?fer
Language Technology Lab, German Research Center for Artificial Intelligence (DFKI)
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany
Ulrich.Schaefer@dfki.de
Abstract
The idea of the Whiteboard project is to integrate
deep and shallow natural language processing
components in order to benefit from their synergy.
The project came up with the first fully integrated
hybrid system consisting of a fast HPSG parser that
utilizes tokenization, PoS, morphology, lexical,
named entity, phrase chunk and (for German)
topological sentence field analyses from shallow
components. This integration increases robustness,
directs the search space and hence reduces
processing time of the deep parser. In this paper, we
focus on one of the central integration facilities, the
XSLT-based Whiteboard Annotation Transformer
(WHAT), report on the benefits of XSLT-based
NLP component integration, and present examples
of XSL transformation of shallow and deep
annotations used in the integrated architecture. The
infrastructure is open, portable and well suited for,
but not restricted to the development of hybrid NLP
architectures as well as NLP applications.
1 Introduction
During the last decade, SGML and XML have become
an important interchange format for linguistic data, be
they created manually by linguists, or automatically by
natural language processing (NLP) components.
LT-XML (Brew et al 2000), XCES (Ide and Romary
2001) and many other are examples for XML-based or
XML-supporting software architectures for natural
language processing.
The main focus of the Whiteboard project (2000-2002)
was to integrate shallow and deep natural language
processing components. The idea was to combine both
in order to benefit from their advantages. Successful and
beneficial integration included tokenization, PoS,
morphology, lexical, named entity, phrase chunk and
(for German) topological sentence field levels in a fully
automated XML-based system. Crysmann et al (2002)
and Frank et al (2003) show that this close deep-
shallow combination significantly increases robustness
and performance compared to the (already fast)
standalone deep HPSG parser by Callmeier (2000). The
only comparable architecture so far was described by
Grover et al (2002), but their integration was limited to
tokenization and PoS tagging (the shallow chunker did
not guide or contribute to deep analysis).
In this paper, we will focus on one of the central
integration facilities, the XSLT-based Whiteboard
Annotation Transformer (WHAT), report on the
benefits of XSLT-based NLP component integration,
and present examples of XSL transformation of shallow
and deep annotations used in the integrated architecture.
Because the infrastructure is in general independent of
deep or shallow paradigms, it can also be applied to
purely shallow or deep systems.
2 Whiteboard: Deep-Shallow Integration
Deep processing (DNLP) systems1 try to apply as much
linguistic knowledge as possible during the analysis of
sentences and result in a uniformly represented
collection of the knowledge that contributed to the
analysis. The result often consists of many possible
analyses per sentence reflecting the uncertainty which
of the possible readings was intended ? or no answer at
all if the linguistic knowledge was contradictory or
insufficient with respect to the input sentence.
Shallow processing (SNLP) systems do not attempt to
achieve such an exhaustive linguistic analysis. They are
desigend for specific tasks ignoring many details in
input and linguistic framework. Utilizing rule-based
(e.g., finite-state) or statistics-based approaches, they
are in general much faster than DNLP. Due to the lack
of efficiency and robustness of DNLP systems, the trend
in application-oriented language processing system
development in the last years was to improve SNLP
systems. They are now capable of analyzing Megabytes
of texts within seconds, but precision and quality
barriers are so obvious (especially on domains the
systems where not designed for or trained on) that a
need for 'deeper' systems re-emerged. Moreover,
1 In this paper, 'deep' is nearly synonymous to typed
unification-based grammar formalisms, e.g. HPSG
(Pollard and Sag 1994), although the infrastructure may
also apply to other deep linguistic frameworks.
semantics construction from an input sentence is quite
poor and erroneous in typical shallow systems.
But also development of DNLP made advances during
the last few years, especially in the field of efficiency
(Callmeier 2000).
A promising solution to improve quality of natural
language processing is the combination of deep and
shallow technologies. Deep processing benefits from
specialized and fast shallow analysis results, shallow
processing becomes 'deeper' using at least partial results
from DNLP.
Many natural language processing applications could
benefit from the synergy of the combination of deep and
shallow, e.g. advanced information extraction, question
answering, or grammar checking systems.
The Whiteboard architecture aims at integrating
different language technology components. Both online
and offline coupling of existing software modules is
supported, i.e., the architecture provides direct access to
standoff XML annotation as well as programming
interfaces. Applications communicate with the
components through programming interfaces. A multi-
layer chart holds the linguistic processing results in the
online system memory while XML annotations can be
accessed online as well as offline. Figure 1 gives an
overview of the general architecture called WHAM
(WHiteboard Annotation Machine).
There are two main points of the architecture that are
important to stress. First, the different paradigms of
DNLP and SNLP are preserved throughout the
architecture, e.g. there is a shallow and a deep
programming interface.
The second point is that the WHAM offers
programming interfaces which are not simply DOM
interfaces isomorphic to the XML markup they are
based on, but hierarchically defined classes. E.g., a fast
index-sequential storage and retrieval mechanism based
on XML is encapsulated through the shallow
programming interface. However, while the typed
feature structure-based programming interface to deep
components is stable, it turned out that the XML-based
interface was too inflexible when new, mainly shallow,
components with new DTDs had to be integrated.
Therefore, a more flexible approach had to be devised.
3 WHAT: The Whiteboard Annotation
Transformer
The main motivation for developing an XSLT-based
infrastructure for NLP components was to provide
flexible access to standoff XML annotations produced
by the components.
XSLT (Clark 1999) is a W3C standard language for the
transformation of XML documents. Input of an XSL
transformation must be XML, while output can be any
syntax (e.g., XML, text, HTML, RTF, or even
programming language source code, etc.). The power of
XSLT mainly comes from its sublanguage XPath (Clark
and DeRose 1999), which supports access to XML
structure, elements, attributes and text through concise
path expressions. An XSL stylesheet consists of
templates with XPath expressions that must match the
input document in order to be executed. The order in
which templates are called is by default top-down, left
to right, but can be modified, augmented, or suppressed
through loops, conditionals, and recursive call of
(named) templates.
WHAT, the WHiteboard Annotation Transformer, is
built on top of a standard XSL transformation engine. It
provides uniform access to standoff annotation through
queries that can either be used from non-XML aware
components to get access to information stored in the
annotation (V and N queries), or to transform (modify,
enrich, merge) XML annotation documents (D queries).
While the WHAT is written in a programming language
such as Java or C, the XSL query templates that are
specific for a standoff DTD of a component's XML
output are independent of that programming language,
i.e., they must only be written once for a new
XML
standoff
markup
component-
specific XSLT
template library
constructed
XSLT
stylesheet
query
result
XSLT
processor
WHAT
Figure 1: Whiteboard Architecture: WHAM
Figure 2: WHAT Architecture
deep NLP
components
program-
ming
interface
shallow
NLP
components XML
standoff
annotation
multilayer
chart
NLP-based
application
WHAT
WHAM
component and are collected in a so-called template
library.
3.1 WHAT Queries
Based on an input XML document (or DOM object), a
WHAT query that consists of
? component name,
? query name, and
? query-specific parameters such as an index or
identifier
is looked up in the XSLT template library for the
specified component, an XSLT stylesheet is returned
and applied to the XML document by the XSLT
processor. The result of stylesheet application is then
returned as the answer to the WHAT query. There are
basically three kinds of results:
? strings (including non-XML output, e.g. RTF or
even programming language source code)
? lists of unique identifiers denoting references to
nodes in the XML input document
? XML documents
In other words, if we formulate queries as functions, we
get the following query signatures:
? getValue: C ? D ? P* ? S*
? getNodes: C ? D ? P* ? N*
? getDocument: C ? D ? P* ? D
where C is the component, D an XML document, P* a
(possibly empty) sequence of parameters, S* a sequence
of strings, and N* a sequence of node identifiers.
We now give examples for each of the query types.
3.1.1 V-queries (getValue)
V-queries return string values from XML attribute
values or text. The simplest case is a single XPath
lookup. As an example, we determine the type of named
entity 23 in a shallow XML annotation produced by the
SPPC system (Piskorski and Neumann 2000).
The WHAT query
getValue("NE.type", "de.dfki.lt.sppc", 23)
would lead to the lookup of the following query in the
XSLT template library for SPPC
<query name="getValue.NE.type" component="de.dfki.lt.sppc">
<!-- returns the type of named entity as number -->
<xsl:param name="index"/>
<xsl:template match="/WHITEBOARD/SPPC_XML//NE[@id=$index]">
<xsl:value-of select="@type"/>
</xsl:template>
</query>
On appropriate SPPC XML annotation, containing the
named entity tag e.g. <NE id="23"
type="location"?> somewhere below the root tag,
this query would return the String "location".
By adding a subsequent lookup to a translation table
(through XML entity definitions or as part of the input
document or of the component-specific template
library), it would also be possible to translate namings,
e.g. in order to map SPPC-annotation-specific namings
to HPSG type names.
We see from this example how the WHAT helps to
abstract from component-specific DTD structure and
namings. However, queries need not be that simple.
Complex computations can be performed and the return
value can also be numbers, e.g., for queries that count
elements, words, etc.
3.1.2 N-queries (getNodes)
An important feature of WHAT is navigation in the
annotation. N-queries compute and return lists of node
identifiers that can again be used as parameters for
subsequent (e.g. V-)queries.
The sample query returns the node identifiers of all
named entities (NE tags) that are in the given range of
tokens (W tags). The template calls a recursive auxiliary
template that seeks the next named entity until the end
of the range is reached. The WHAT query
getNodes("W.NEinRange", "de.dfki.lt.sppc",3,19)
would lead to the lookup of the following query in the
XSLT template library for SPPC.
<query name="getNodes.W.NEinRange" compon.="de.dfki.lt.sppc">
<!-- returns NE nodes starting exactly at token $index to
(at most) token $index2 -->
<xsl:param name="index"/> <xsl:param name="index2"/>
<xsl:template match="/">
<xsl:variable name="startX"
select="/WHITEBOARD/SPPC_XML//W[@id=$index]/ancestor::NE"/>
<xsl:if test="$startX//W[1]/@id = $index">
<xsl:call-template name="checknextX">
<xsl:with-param name="nextX" select="$startX"/>
<xsl:with-param name="lastW" select="$index2"/>
</xsl:call-template>
</xsl:if>
</xsl:template>
<xsl:template name="checknextX">
<!-- auxiliary template (recursive) -->
<xsl:param name="nextX"/>
<xsl:param name="lastW"/>
<xsl:variable name="Xtokens" select="$nextX//W"/>
<xsl:if test="number(substring($Xtokens[last()]/@id, 2))
&lt;= number(substring($lastW, 2))">
<xsl:value-of select="$nextX/@id"/>
<xsl:text> </xsl:text>
<xsl:call-template name="checknextX">
<xsl:with-param name="nextX"
select="/WHITEBOARD/SPPC_XML//NE[@id=concat('N', string(1 +
number(substring($nextX/@id,2))))]"/>
<xsl:with-param name="lastW" select="$lastW"/>
</xsl:call-template>
</xsl:if>
</xsl:template>
</query>
Again, the query forms an abstraction from DTD
structure. E.g., in SPPC XML output, named entity
elements enclose token elements. This need not be the
case for another shallow component; its template would
be defined differently, but the query call syntax would
be the same.
3.1.3 D-queries (getDocument)
D-queries return transformed XML documents - this is
the classical, general use of XSLT. Complex
transformations that modify, enrich or produce
(standoff) annotation can be used for many purposes.
Examples are
? conversion from a different XML format
? merging of several XML documents into one
? auxiliary document modifications, e.g. to add
unique identifiers to elements, sort elements etc.
? providing interface to NLP applications (up to
code generation for a programming language
compiler?)
? visualization and formatting (Thistle, HTML,
PDF, ?)
? perhaps the most important is to do (linguistic)
computation and transformation in order to turn a
WHAT query into a kind of NLP component
itself. This is e.g. intensively used in the shallow
topological field parser integration we describe
below. Multiple queries are applied in a sequence
to transform a topological field tree into a list of
constraints over syntactic spans that are used for
initialization of the deep parser's chart. One of
these WHAT queries has more than 900 lines of
XSLT code.
We can show only a short example here, a query that
inserts unique identifier attributes into an arbitrary XML
document without id attributes.
<query name="getDocument.generateIDs">
<!-- generate unique id for each element -->
<xsl:template match="*">
<xsl:copy select=".">
<xsl:attribute name="id">
<xsl:value-of select="generate-id()"/>
</xsl:attribute>
<xsl:for-each select="@*">
<xsl:copy-of select="."/>
</xsl:for-each>
<xsl:apply-templates/>
</xsl:copy>
/xsl:template>
</query>
Note that this is an example for a stylesheet that is
completely independent of a DTD, it just works on any
XML document ? and thus shows how generic XSL
transformation rules can be.
Another example is transformation of XML tree
representations into Thistle trees (arbora DTD; see
Calder 2000). While the output DTD is fixed, this is
again not true for the input document which can contain
arbitrary element names and branches. Thistle
visualizations generated through WHAT are shown in
Fig. 4, 5 and 6 below.
3.2 Components of the Hybrid System
The WHAT has been successfully used in the
Whiteboard architecture for online analysis of German
newspaper sentences. For more details on motivation
and evaluation cf. Frank et al (2003) and Becker and
Frank (2002). The simplified diagram in Figure 3
depicts the components and places where WHAT comes
into play in the hybrid integration of deep and shallow
processing components (V, N, D denote the WHAT
query types). The system takes an input sentence, and
runs three shallow systems on it:
? the rule-based shallow SPPC (Piskorski and
Neumann 2000) for named entity recognition,
? TnT/Chunkie, a statistics-based shallow PoS
tagger and chunker by (Skut and Brants 1998),
? LoPar, a probabilistic context-free parser (Schmid
2000), which takes PoS-tagged tokens as input,
and produces binary tree representations of
sentence fields, e.g., topo.bin in Fig. 4. For a
justification for binary vs. flat trees cf. Becker and
Frank (2002).
The results of the components are three standoff
annotations of the input sentence. Then, a sequence of
D-queries is applied to flatten the binary topological
field trees (result is topo.flat, Fig. 5), merge with
shallow chunk information from Chunkie (topo.chunks,
Fig. 6), and apply the main D-query computing bracket
information for the deep parser from the merged topo
tree (topo.brackets, Fig. 7).
Finally, the deep parser PET (Callmeier 2000), modified
as described in Frank et al (2003), is started with a
chart initialized using the shallow bracket information
(topo.brackets) through WHAT V and N queries. PET
also accesses lexical and named entity information from
SPPC through V queries.
input sentence
SPPC TnT
LoPar Chunkie
topo.bin
topo.flat
topo.brackets
topo.chunks
PET
D
D
D
D
V, N
WHAT-based application
D,V,N
Figure 3: WHAT in the hybrid parser
Again, WHAT abstraction facilitates exchange of the
shallow input components of PET without needing to
rewrite the parser's code.
The dashed lines in Figure 3 indicate that a WHAT-
based application can have access to the standoff
annotation through D, V or N queries.
The Thistle diagrams below are created via D queries
out of the intermediate topo.* trees.
Figure 4. A binary tree as result of the topoparser
(topo.bin).
Figure 5. The same tree after flattening (topo.flat).
Figure 6. The topo tree merged with chunks
(topo.chunks).
<TOPO2HPSG type="root" id="5608">
<MAP_CONSTR id="T1" constr="v2_cp" left="W1" right="W13"/>
<MAP_CONSTR id="T2" constr="v2_vf" left="W1" right="W2"/>
<MAP_CONSTR id="T3" constr="vfronted_vfin+rk" left="W3" right="W3"/>
<MAP_CONSTR id="T4" constr="vfronted_vfin+vp+rk" left="W3" right="W13"/>
<MAP_CONSTR id="T5" constr="vfronted_vp+rk" left="W4" right="W13"/>
<MAP_CONSTR id="T6" constr="vfronted_rk-complex" left="W7" right="W7"/>
<MAP_CONSTR id="T7" constr="vl_cpfin_compl" left="W9" right="W13"/>
<MAP_CONSTR id="T8" constr="vl_compl_vp" left="W10" right="W13"/>
<MAP_CONSTR id="T9" constr="vl_rk_fin+complex+f" left="W12" right="W13"/>
<MAP_CONSTR id="T10" constr="extrapos_rk+nf" left="W7" right="W13"/>
</TOPO2HPSG>
Figure 7. The extracted brackets (topo.brackets)
3.3 Accessing and Transforming Deep
Annotation
In the sections so far, we showed examples for shallow
XML annotation. But annotation access should not stop
before deep analysis results. In this section, we turn to
deep XML annotation. Typed feature structures provide
a powerful, universal representation for deep linguistic
knowledge.
While it is in general inefficient to use XML markup to
represent typed feature structures during processing
(e.g. for unification, subsumption operations), there are
several applications that may benefit from a
standardized system-independent XML markup of typed
feature structures, e.g., as exchange format for
? deep NLP component results (e.g. parser chart)
? grammar definitions
? feature structure visualization or editing tools
? feature structure 'tree banks' of analysed texts
Sailer and Richter (2001) propose an XML markup
where the recursive embedding of attribute-value pairs
is decomposed into a kind of definite equivalences or
non-recursive node lists (triples of node ID, type name
and embedded lists of attribute-node pairs). The only
advantage we see for this kind of representation is its
proximity to a particular kind of feature structure
implementation.
We adopt an SGML markup for typed feature
structures originally developed by the Text Encoding
Initiative (TEI) which is very compact and seems to be
widely accepted, e.g. also in the Tree Adjoining
Grammar community (Issac 1998). Langendoen and
Simons (1995) give an in-depth justification for the
naming and structure of a feature structure DTD. We
will focus here on the feature structure DTD subset that
is able to encode the basic data structures of deep
systems such as LKB (Copestake 1999), PET
(Callmeier 2000), PAGE, or the shallow system
SProUT (Becker et al 2002) which have a subset of
TDL (Krieger and Sch?fer 1994) as their common basic
formalism2:
<?xml version="1.0" ?>
<!-- minimal typed feature structure DTD -->
<!ELEMENT FS ( F* ) >
<!ATTLIST FS type NMTOKEN #IMPLIED
coref NMTOKEN #IMPLIED >
<!ELEMENT F ( FS ) >
<!ATTLIST F name NMTOKEN #REQUIRED >
The FS tag encodes typed Feature Structure nodes, F
encodes Features. Atoms are encoded as typed Feature
structure nodes with empty feature list. An important
2 Encoding of type hierarchies or other possibly system
or formalism-specific definitions are of course not
covered by this minimal DTD.
point is the encoding of coreferences (reentrancies)
between feature structure nodes which denote structure
sharing. For the sake of symmetry in the
representation/DTD, we do not declare the coref
attribute as ID/IDREF, but as NMTOKEN.
An application of WHAT access or transformation of
deep annotation would be to specifiy a feature path
under which a value (type, atom, or complex FS) is to
be returned. The problem here are the coreferences
which must be dereferenced at every feature level of the
path. A general solution is to recursively dereference all
nodes in the path.
We give only a limited example here, a query to access
output of the SProUT system. It returns the value (type)
of a feature somewhere under the specified attribute in a
disjunction of typed feature structures, assuming that we
are not interested here in structure sharing between
complex values.
<query name="getValue.fs.attr" component="de.dfki.lt.sprout">
<xsl:param name="disj"/>
<xsl:param name="attr"/>
<xsl:template match='DISJ[$disj]'>
<xsl:variable name="node" select='.//F[@name=$attr]/FS'/>
<xsl:choose>
<xsl:when test="$node/@type">
<xsl:value-of select="$node/@type"/>
</xsl:when>
<xsl:otherwise>
<xsl:if test="$node/@coref">
<xsl:call-template name="deref">
<xsl:with-param name="coref"
select="$node/@coref"/>
</xsl:call-template>
</xsl:if>
</xsl:otherwise>
</xsl:choose>
<xsl:apply-templates/>
</xsl:template>
<xsl:template name="deref">
<xsl:param name="coref"/>
<xsl:for-each select=".//FS[@coref=$coref]">
<xsl:if test='@type'>
<xsl:value-of select="@type"/>
</xsl:if>
</xsl:for-each>
</xsl:template>
</query>
To complete the picture of abstraction through WHAT
queries, we can imagine that the same types of query are
possible to access e.g. the same morphology
information in both shallow and in deep annotation,
although their representation within the annotation
might be totally different.
3.4 Efficiency of XSLT Processors
Processing speed of current XSLT engines on large
input documents is a problem. Many XSLT
implementations lack efficiency (for an overview cf.
xmlbench.sourceforge.net). Although optimization is
possible (e.g. through DTD specification, indexing etc.),
this is not done seriously in many implementations.
However, there are several WHAT-specific solutions
that can help making queries faster. A pragmatic one is
pre-editing of large annotation files. An HPSG parser
e.g. focuses on one sentence at a time and does not
exceed the sentence boundaries (which can be
determined reliably by shallow methods) so that it
suffices to split shallow XML input into per-sentence
annotations in order to reduce processing time to a
reasonable amount.
Another solution could be packing several independent
queries into a 'prepared statement' in one stylesheet.
However, as processing speed is mainly determined by
the size of the input document, this does not speed up
processing time substantially.
WHAT implementations are free to be based on DOM
trees or plain XML text input (strings or streams). DOM
tree representations are used by XSLT implementations
such als libxml/libslt for C/Perl/Python/TCL or Xalan
for Java. Hence, DOM implementations of WHAT are
preferable in order to avoid unnecessary XML parsing
when processing multiple WHAT transformations on
the same input and thus help to improve processing
speed.
As in all programming language, there a multiple
solutions for a problem. An XSL profiling tool (e.g.
xsltprofiler.org) can help to locate inefficient XSLT
code.
3.5 Related Work
As argued in Thompson and McKelvie (1997), standoff
annotation is a viable solution in order to cope with the
combination of multiple overlapping hierarchies and the
efficiency problem of XML tree modification for large
annotations.
Ide (2000) gives an overview of NLP-related XML core
technologies that also strives XSLT.
We adopt the pragmatic view of Carletta et al (2002),
who see that computational linguistics greatly benefits
from general XMLification, namely by getting for free
standards and advanced technologies for storing and
manipulating XML annotation, mainly through W3C
and various open source projects. The trade-off for this
benefit is a representation language somewhat limited
with respect to linguistic expressivity.
NiteQL (Evert and Voormann 2002) can be seen as an
extension to XPath within XSLT, has a more concise
syntax especially for document structure-related
expressions and a focus on timeline support with
specialized queries (for speech annotation). The query
language in general does not add expressive power to
XSLT and the implementation currently only supports
Java XSLT engines.
Because of unstable standardization and implementation
status, we did not yet make use of XQuery (Boag et al
2002). XQuery is a powerful, SQL-like query language
on XML documents where XPath is a subset rather than
a sublanguage as in XSLT.
3.6 Advantages of WHAT
WHAT is
? based on standard W3C technology (XSLT)
? portable. As the programming language-specific
wrapper code is relatively small, WHAT can
easily be ported to any programming language for
which an XSLT engine exists. Currently, WHAT
is implemented in Java (JAXP/Xalan) and C/C++
(Gnome libxml/libxslt). Through libxml/libxslt, it
can also easily be ported to Perl, Python, Tcl and
other languages
? easy to extend to new components/DTDs. This
has to be done only once for a component through
XSLT query library definitions, and access will be
available immediately in all programming
languages for which a WHAT implementation
exists
? powerful (mainly through XPath which is part of
XSLT).
WHAT can be used
? to perform computations and complex
transformations on XML annotation,
? as uniform access to abstract from component-
specific namings and DTD structure, and
? to exchange results between components (e.g., to
give non-XML-aware components access to
information encoded XML annotation),
? to define application-specific architectures for
online and offline processing of NLP XML
annotation.
4 Conclusion and Future Work
We have presented an open, flexible and powerful
infrastructure based on standard W3C technology for
the online and offline combination of natural language
processing components, with a focus on, but not limited
to, hybrid deep and shallow architectures.
The infrastructure is part of the Whiteboard architecture
and is employed and will be continued in several
successor projects. The infrastructure is well suited for
rapid prototyping of hybrid NLP architectures as well as
for developing NLP applications, and can be used to
both access NLP XML markup from programming
languages and to compute or transform it.
Because WHAT is an open framework, it is worth
considering XQuery as a future extension to WHAT.
Which engine to ask, an XSLT or an XQuery processor,
could be coded in each <query> element of the WHAT
template library.
WHAT can be used to translate to the ingenious Thistle
tool (Calder 2000) for visualization of linguistic
analyses and back from Thistle in editor mode, e.g. for
manual, graphical correction of automatically annotated
texts for training etc.
A proximate approach is to combine WHAT with SDL
(Krieger 2003) to declaratively specify WHAT-based
NLP architectures (pipelines, loops, parallel
transformation) that can be compiled to Java code.
The proximity to W3C standards suggests using WHAT
directly for transformation of NLP results into
application-oriented (W3C) markup, or to use W3C
markup (e.g. RDF) for semantic web integration in
NLP, VoiceXML, etc.
5 Acknowledgements
I would like to thank my collegues, especially Anette
Frank, Bernd Kiefer, Hans-Ulrich Krieger and G?nter
Neumann, for cooperation and many discussions.
This work has been supported by a grant from the
German Federal Ministry of Education and Research
(FKZ 01IW002).
This document was generated partly in the context of
the DeepThought project, funded under the Thematic
Programme User-friendly Information Society of the 5th
Framework Programme of the European Community ?
(Contract N? IST-2001-37836). The author is solely
responsible for its content, it does not represent the
opinion of the European Community and the
Community is not responsible for any use that might be
made of data appearing therein.
6 References
Markus Becker and Anette Frank. 2002. A Stochastic
Topological Parser of German. Proceedings of
COLING-2002, pp 71-77, Taipei.
Markus Becker, Witold Dro?d?y  ski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Sch?fer, Feiyu Xu.
2002. SProUT - Shallow Processing with Typed
Feature Structures and Unification. Proceedings of
the International Conference on NLP (ICON 2002).
Mumbai, India.
Scott Boag, Don Chamberlin, Mary F. Fernandez,
Daniela Florescu, Jonathan Robie and J?r?me
Sim?on. 2002. XQuery 1.0: An XML Query
Language. http://www.w3c.org/TR/xquery
Chris Brew, David McKelvie, Richard Tobin, Henry
Thompson and Andrei Mikheev. 2000. The XML
Library LT XML. User documentation and reference
guide. LTG. University of Edinburgh.
Joe Calder. 2000. Thistle: Diagram Display Engines
and Editors. Technical report. HCRC. University of
Edinburgh.
Ulrich Callmeier. 2000. PET - A platform for
experimentation with efficient HPSG processing
techniques. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG:
Methods, systems, evaluation). Editors: D. Flickinger,
S.Oepen, H. Uszkoreit, J. Tsujii, pp. 99 ? 108.
Cambridge, UK: Cambridge University Press.
Jean Carletta, David McKelvie, Amy Isard, Andreas
Mengel, Marion Klein, Morten Baun M?ller. 2002. A
generic approach to software support for linguistic
annotation using XML. Readings in Corpus
Linguistics, ed. G. Sampson and D. McCarthy,
London and NY: Continuum International.
James Clark (ed.). 1999. XSL Transformations (XSLT)
http://www.w3c.org/TR/xslt
James Clark and Steve DeRose (eds.). 1999. XML Path
Language (XPath) http://www.w3c.org/TR/xpath
Anne Copestake. 1999. The (new) LKB system.
ftp://www-csli.stanford.edu/~aac/newdoc.pdf
Berthold Crysmann, Anette Frank, Bernd Kiefer, Hans-
Ulrich Krieger, Stefan M?ller, G?nter Neumann,
Jakub Piskorski, Ulrich Sch?fer, Melanie Siegel, Hans
Uszkoreit, Feiyu Xu. 2002. An Integrated
Architecture for Shallow and Deep Processing.
Proceedings of ACL-2002, Philadelphia, PA.
Stefan Evert with Holger Voormann. 2002. NITE Query
Language. NITE Project Document. Stuttgart.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer and Ulrich Sch?fer. 2003. Integrated
Shallow and Deep Parsing. Submitted manuscript.
Claire Grover, Ewan Klein, Alex Lascarides and Maria
Lapata. 2002. XML-based NLP Tools for Analysing
and Annotating Medical Language. Proceedings of
the Second International Workshop on NLP and XML
(NLPXML-2002). Taipei.
Nancy Ide. 2000. The XML Framework and its
Implications for the Development of Natural
Language Processing Tools. Proceedings of the
COLING Workshop on Using Toolsets and
Architectures to Build NLP Systems, Luxembourg.
Nancy Ide and Laurent Romary. 2001. A Common
Framework for Syntactic Annotation. Proceedings of
ACL-2001. pp. 298-305. Toulouse.
Fabrice Issac. 1998. A Standard Representation
Framework for TAG. In Fourth International
Workshop on Tree Adjoining Grammars and Related
Frameworks (TAG+4), Philadelphia, PA.
Hans-Ulrich Krieger. 2003. SDL ? A Description
Language for Specifying NLP Systems. DFKI
Technical Report. Saarbr?cken.
Hans-Ulrich Krieger and Ulrich Sch?fer. 1994. TDL - A
Type Description Language for Constraint-Based
Grammars. Proceedings of COLING-94. Vol. 2 pp.
893-899, Kyoto.
D. Terence Langendoen and Gary F. Simons. 1995. A
rationale for the TEI recommendations for feature-
structure markup. Computers and the Humanities
29(3). Reprinted in Nancy Ide and Jean Veronis, eds.
The Text Encoding Initiative: Background and
Context, pp. 191-209. Dordrecht: Kluwer Acad. Publ.
Jakub Piskorski and G?nter Neumann. 2000. An
Intelligent Text Extraction and Navigation System. In
proceedings of 6th RIAO-2000, Paris.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago: University of
Chicago Press.
Manfred Sailer and Frank Richter. 2001. Eine XML-
Kodierung f?r AVM-Beschreibungen (in German). In
Lobin H. (ed.) Proceedings of the Annual Meeting of
the Gesellschaft f?r linguistische Datenverarbeitung,
Giessen. pp. 161 ? 168.
Helmut Schmid. 2000. LoPar: Design and
Implementation. Arbeitspapiere des
Sonderforschungsbereiches 340, No. 149. University
of Stuttgart.
Wojciech Skut and Thorsten Brants. 1998. Chunk
tagger ? statistical recognition of noun phrases. In
Proceedings of the ESSLLI Workshop on Automated
Acquisition of Syntax and Parsing. Saarbr?cken.
Henry S. Thompson and David McKelvie. 1997.
Hyperlink Semantics for standoff markup of read-only
documents. In Proc SGML EU 1997.
Middleware for Creating and Combining Multi-dimensional NLP Markup
Ulrich Scha?fer
German Research Center for Artificial Intelligence (DFKI)
Language Technology Lab
Stuhlsatzenhausweg 3, D-66123 Saarbru?cken, Germany
email: ulrich.schaefer@dfki.de
Abstract
We present the Heart of Gold mid-
dleware by demonstrating three XML-
based integration scenarios where multi-
dimensional markup produced online by
multilingual natural language processing
(NLP) components is combined to deliver
rich, robust linguistic markup for use in
NLP-based applications like information
extraction, question answering and seman-
tic web. The scenarios include (1) robust
deep-shallow integration, (2) shallow pro-
cessing cascades, and (3) treebank storage
of multi-dimensionally annotated texts.
1 Introduction and Motivation
Heart of Gold is a middleware architecture for cre-
ating and combining markup produced by mul-
tiple natural language processing components in
multilingual environments. It was initially devel-
oped for a special sort of multi-dimensional an-
notation, namely application-oriented, XML- and
XSLT-based online integration of various shallow
NLP components with a deep HPSG parser for in-
creased robustness in the hybrid natural language
processing paradigm (Callmeier et al, 2004).
The middleware, however, can also be used for
various other online and offline tasks related to
multi-dimensional markup creation and integra-
tion. These comprise automatic corpus annotation,
incorporation of multi-dimensional markup into a
single XML representation, and NLP component
cascades interleaved with XSL annotation trans-
formation. The middleware provides XML-RPC
interfaces for simple, networking-enabled and
programming language-independent application
and component integration. Heart of Gold is avail-
able as one of the DELPH-IN open source tools
available from http://www.delph-in.net1.
1I would like to thank Robert Barbey, ?Ozgu?r Demir
2 Middleware Architecture
Fig. 1 gives a schematic overview of the middle-
ware server in between applications (above) and
external NLP components (below). When a new
application session in Heart of Gold is started, it
takes a configuration specifying NLP components
to start for the session. Each component is started
according to its own parameterized configuration.
The client can send texts to the middleware and the

Computed
annotations
XML,RMRS
Application
Module Communication Manager
Re
su
lts




Qu
er
ies

External,
persistent
annotation
databaseModules
External NLP
components
XSLT service
Figure 1: Middleware architecture
NLP components are then queried in a numerically
defined processing order (?Depth? in Fig. 4). The
shallowest components (e.g. tokenizer) are as-
signed a low number and are started first etc. The
output of each component must be XML markup.
Each component gets the output of the previous
component as input by default, but can also re-
quest (via configuration) other annotations as in-
put. Components may produce multiple output
annotations (e.g. in different formats). Thus, the
and Thomas Klo?cker for their implementation work, Bernd
Kiefer and the co-authors in the cited papers for fruitful co-
operation, and the reviewers for valuable comments. This
work has been supported by a grant from the German Federal
Ministry of Education and Research (FKZ 01IWC02).
81
component dependency structure in general forms
a graph.
2.1 Session and multi-dimensional
annotation management
The resulting multi-dimensional annotations are
stored in a per-session markup storage (Fig. 2)
that groups all annotations for an input query (a
sentence or text) in annotation collections. The
markup storage can also be made persistent by
saving it to XML files or to an XML database.
Annotations can be accessed uniquely via a URI of
 
 Session Annotation 
collection (1 
per input text) 
Standoff annotations (computed by modules/components) 
Figure 2: Session and multi-dimensional markup
storage
the form hog://sid/acid/aid in XPath expres-
sions where sid is a session ID, acid is an anno-
tation collection ID and aid is an annotation iden-
tifier typically signifying the name of the produc-
ing component. Structured metadata like configu-
ration and processing parameters (e.g. processing
time and date, language ID etc.) are always stored
within the annotation markup as first root daughter
element.
2.2 XML standoff markup as first-class citizen
Unlike other NLP architectures (e.g. GATE (Cun-
ningham et al, 2002) etc.), Heart of Gold treats
XML standoff annotations (Thompson and McK-
elvie, 1997) as first class citizens and natively sup-
ports XML (and only XML) markup of any kind.
Moreover, Heart of Gold does not prescribe spe-
cific DTDs or Schemata for annotations, provided
that the markup is well-formed. In this sense, it
is a completely open framework that may how-
ever be constrained by requirements of the actu-
ally configured components. The advantage of
this openness is easy integration of new compo-
nents. Mappings need only be defined for the
immediately depending annotations (see next sec-
tion) which is by far not an n-to-n mapping in prac-
tical applications.
However, the fact that a specific DTD or
Schema is not imposed by the middleware does
not mean that there are no minimal requirements.
Linking between different standoff annotations is
only possible on the basis of a least common en-
tity, which we propose to be the character spans in
the original text2. Moreover, we additionally pro-
pose the use of the XML ID/IDREF mechanism to
facilitate efficient integration and combination of
multi-dimensional markup.
Finally, depending on the scenario, specific
common, standardized markup formats are appro-
priate, an example is RMRS (Copestake, 2003) for
deep-shallow integration in Section 3 or the XML-
encoded typed feature structure markup generated
by SProUT (Droz?dz?yn?ski et al, 2004).
2.3 XSLT as ?glue? and query language
We propose and Heart of Gold heavily relies on the
use of XSLT for combining and integrating multi-
dimensional XML markup. The general idea has
already been presented in (Scha?fer, 2003), but the
developments and experiences since then have en-
couraged us to proceed in that direction and Heart
of Gold can be considered as a successful, more
elaborated proof of concept. The idea is related
to the open markup format framework presented
above: XSLT can be used to transform XML to
other XML formats, or to combine and query an-
notations. In particular, XSLT stylesheets may re-
solve conflicts resulting from multi-dimensional
markup, choose among alternative readings, fol-
low standoff links, or decide which markup source
to give higher preference.
(Carletta et al, 2003), e.g. propose the NXT
Search query language that extends XPath by
adding query variables, regular expressions, quan-
tification and special support for querying tem-
poral and structural relations. Their main argu-
ment against standard XPath is that it is impossi-
ble to constrain both structural and temporal re-
lations within a single XPath query. Our argu-
ment is that XSLT can complement XPath where
XPath alone is not powerful enough, yet providing
a standardized language. Further advantages we
see in the XSLT approach are portability and effi-
ciency (in contrast to ?proprietary? and slow XPath
extensions like NXT), while it has a quite sim-
ple syntax in its (currently employed) 1.0 version.
XSLT can be conceived as a declarative specifi-
cation language as long as an XML tree structure
2Our experience is that a common tokenization is not
realistic?too many existing NLP components have differing
concepts of what constitutes a token.
82
Figure 3: Heart of Gold analysis results in GUI with specialized XML visualizations
is preserved (not necessarily fully isomorphic to
the input structure). However, XSLT is Turing-
capable and therefore suited to solve in princi-
ple any markup integration or query problem. Fi-
nally, extensions like the upcoming XSLT/XPath
2.0 version or efficiency gains through XSLTC
(translet compilation) can be taken on-the-fly and
for free without giving up compatibility. Tech-
nically, the built-in Heart of Gold XSLT proces-
sor could easily replaced or complemented by an
XQuery processor. However, for the combination
and transformation of NLP markup, we see no ad-
vantage of XQuery over XSLT.
Heart of Gold comes with a built-in XSL trans-
formation service, and module adapters (Sec-
tion 2.4) can easily implement transformation sup-
port by including a few lines of code. Stylesheets
can also be generated automatically in Heart of
Gold, provided a systematic description of the
transformation input format is available. An
example is mapping from named entity gram-
mar output type definitions in scenario 1 below.
Stylesheets are also employed to visualize the lin-
guistic markup, e.g. by transforming RMRS to
HTML (Fig. 3) or LATEX.
2.4 Integrated NLP components
NLP components are integrated through adapters
called modules (either Java-based, subprocesses or
via XML-RPC) that are also responsible for gener-
ating XML standoff output if this is not supported
natively by the components (e.g., TnT, Chunkie).
Various shallow and deep NLP components have
already been integrated, cf. Fig. 4.
Component Type Depth Languages
JTok tokenizer 10 de, en, it,. . .
ChaSen Jap. tagger 10 ja
TnT stat. tagger 20 de, en
Chunkie stat. chunker 30 de, en
ChunkieRmrs chunk RMRS 35 de, en
LingPipe stat. NER 40 en, es,. . .
SDL subarchitect.
Sleepy shallow parser 40 de
SProUT shallow NLP 40 de, el, en, ja,. . .
RASP shallow NLP 50 en
PET HPSG parser 100 de, el, en, ja,. . .
Figure 4: Integrated components. References for
components and resources not cited are available
on http://heartofgold.dfki.de/Publications Components.html
3 Scenario 1: Deep-Shallow Integration
The idea of hybrid deep-shallow integration is to
provide robust linguistic analyses through multi-
dimensional NLP markup created by shallow and
deep components, e.g. those listed in Fig. 4. Ro-
bustness is achieved in two ways: (1) various
shallow components perform preprocessing and
partial statistical disambiguation (e.g. PoS tag-
ging of unknown words, named entity recognition)
that can be used by a deep parser by means of
a so-called XML input chart (multi-dimensional
markup combined through XSLT in a single XML
83
document in a format convenient for the parser).
(2) shallow component?s output is transformed
through XSLT to partial semantic representations
in RMRS syntax (Copestake, 2003) that is poten-
tially more fine-grained and structured than what
is digestible by the deep parser as preprocessing
input (mainly PoS/NE type and span information
via the XML input chart). This allows for (a) a
fallback to the shallow representation in case deep
parsing fails (e.g. due to ungrammatical input), (b)
combination with the RMRS generated by deep
parsing or fragments of it in case deep parsing
fails.
First application scenarios have been investi-
gated successfully in the DEEPTHOUGHT project
(Uszkoreit et al, 2004). A further application (hy-
brid question analysis) is presented in (Frank et al,
2006). Recently, linking to ontology instances and
concepts has been added (Scha?fer, 2006).
4 Scenario 2: Shallow Cascades
The second scenario is described in (Frank et
al., 2004) in detail. A robust, partial semantics
representation is generated from a shallow chun-
ker?s output and morphological analysis (English
and German) by means of a processing cascade
consisting of four SProUT grammar instances
with four interleaved XSLT transformations. The
cascade is defined using the declarative system
description language SDL (Krieger, 2003). An
SDL architecture description is compiled into a
Java class which is integrated in Heart of Gold
as a sub-architecture module (Fig. 5). The sce-
nario is equally a good example for XSLT-based
annotation integration. Chunker analysis results
are included in the RMRS to be built through an
XSLT stylesheet using the XPath expression
document($uri)/chunkie/chunks/chunk[
@cstart=$beginspan and @cend=$endspan]
where $uri is a variable containing an annotation
identifier of the form hog://sid/acid/aid as
explained in Section 2.1.
5 Scenario 3: Corpus Annotation
Given the powerful online middleware archi-
tecture described above, automatic, multi-
dimensional corpus annotation can then be
regarded as a simple by-product. Heart of Gold
supports persistent storage of XML markup either
on the file system or to XML databases through
the built-in XML:DB interface. Through XSLT, it
is possible to combine multi-dimensional markup
(that would straightforwardly be stored in multiple
XML documents) into a single XML document.
Heart of Gold NLP architecture instance
input sentence Chunkie
nodeid_cat SProUT SProUTrmrs_finalXSLT SProUT XSLT XSLT XSLTrmrs_phrase reorderfs2rmrsxmlrmrs_lex
RMRS result
pos_filterSProUTrmrs_morph
 . . . other NLP components . . .
SDL?defined SProUT?XSLT cascade sub?architectufe
Figure 5: SProUT XSLT cascade in a Heart of
Gold architecture instance.
References
U. Callmeier, A. Eisele, U. Scha?fer, and M. Siegel. 2004.
The DeepThought core architecture framework. In Proc.
of LREC-2004, pages 1205?1208, Lisbon, Portugal.
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robertson, and
H. Voormann. 2003. The NITE XML toolkit: flexible
annotation for multi-modal language data. Behavior Re-
search Methods, Instruments, and Computers, special is-
sue on Measuring Behavior, pages 353?363.
A. Copestake. 2003. Report on the design of RMRS. Tech-
nical Report D1.1b, University of Cambridge, UK.
H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan.
2002. GATE: A framework and graphical development
environment for robust NLP tools and applications. In
Proceedings of ACL-2002.
W. Droz?dz?yn?ski, H.-U. Krieger, J. Piskorski, U. Scha?fer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures ? foundations and applications.
Ku?nstliche Intelligenz, 2004(1):17?23.
A. Frank, K. Spreyer, W. Droz?dz?yn?ski, H.-U. Krieger, and
U. Scha?fer. 2004. Constraint-based RMRS construction
from shallow grammars. In Proceedings of HPSG-2004,
pages 393?413. CSLI Publications, Stanford.
A. Frank, H.-U. Krieger, F. Xu, H. Uszkoreit, B. Crysmann,
and U. Scha?fer. 2006. Question answering from struc-
tured knowledge sources. Journal of Applied Logics, Spe-
cial Issue on Questions and Answers. To appear.
H.-U. Krieger. 2003. SDL?a description language for
building NLP systems. In Proceedings of the HLT-NAACL
Workshop on the Software Engineering and Architecture
of Language Technology Systems, pages 84?91.
U. Scha?fer. 2003. WHAT: An XSLT-based infrastructure
for the integration of natural language processing compo-
nents. In Proceedings of the HLT-NAACL Workshop on
the Software Engineering and Architecture of Language
Technology Systems, pages 9?16, Edmonton, Canada.
U. Scha?fer. 2006. OntoNERdIE?mapping and linking on-
tologies to named entity recognition and information ex-
traction resources. In Proc. of LREC-2006, Genoa, Italy.
H. S. Thompson and D. McKelvie. 1997. Hyperlink seman-
tics for standoff markup of read-only documents. In Pro-
ceedings of SGML-EU-1997.
H. Uszkoreit, U. Callmeier, A. Eisele, U. Scha?fer, M. Siegel,
and J. Uszkoreit. 2004. Hybrid robust deep and shallow
semantic processing for creativity support in document
production. In Proc. of KONVENS-2004, pages 209?216.
84
Coling 2010: Poster Volume, pages 588?596,
Beijing, August 2010
DL Meet FL: A Bidirectional Mapping between Ontologies and
Linguistic Knowledge?
Hans-Ulrich Krieger and Ulrich Scha?fer
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
{krieger|ulrich.schaefer}@dfki.de
Abstract
We present a transformation scheme that me-
diates between description logics (DL) or
RDF-encoded ontologies and type hierar-
chies in feature logics (FL). The DL-to-FL
direction is illustrated by an implemented
offline procedure that maps ontologies with
large, dynamically maintained instance data
to named entity (NE) and information ex-
traction (IE) resources encoded in typed fea-
ture structures. The FL-to-DL translation is
exemplified by a (currently manual) trans-
lation of so-called MRS (Minimal Recur-
sion Semantics) representations into OWL
instances that are based on OWL classes,
generated from the the type hierarchy of a
deep linguistic grammar. The paper will
identify parts of knowledge which can be
translated from one formalism into the other
without loosing information and parts which
can only be approximated. The work de-
scribed here is important for the Seman-
tic Web to become a reality, since semantic
annotations of natural language documents
(DL) can be automatically generated by shal-
low and deep natural language parsing sys-
tems (FL).
1 Introduction and motivation
Ontologies on the one hand and resources for natu-
ral language processing (lingware) on the other hand,
though closely related, are often maintained indepen-
dently, thus constituting a duplication of work.
In the first part of this paper, we describe an im-
plemented offline procedure that can be used to map
concepts and instance information from ontologies to
lingware resources for named entity recognition and
information extraction systems. The approach (i) im-
proves NE/IE precision and recall in closed domains,
?The work described in this paper has been carried out
in the TAKE project (Technologies for Advanced Knowl-
edge Extraction), funded by the German Federal Min-
istry of Education and Research under contract number
01IW08003.
(ii) exploits linguistic knowledge for identifying on-
tology instances in texts more robustly, (iii) gives full
access to ontology instances and concepts in natu-
ral language processing results, and (iv) avoids du-
plication of work in development and maintenance of
ontologies and lingware. The advantages of this ap-
proach for Semantic Web and natural language (NL)
processing-based applications come from a cross-
fertilization effect. While ontology instance data can
improve precision and recall of, e.g., named entity
recognition (NER) and information extraction (IE)
in closed domains, linguistic knowledge contained in
NER and IE components can help to recognize ontol-
ogy instances (or concepts) occurring in text, e.g., by
taking into account inflection, anaphora, and context.
Furthermore, (Haghighi and Klein, 2009) and others
have shown that incorporating finer-grained seman-
tic information on entities occurring in text (e.g., for
antecedent filtering) helps to improve performance of
coreference resolution systems.
If both resources would be managed jointly at a
single place (in the ontology), they could be eas-
ily kept up-to-date and in sync, and their mainte-
nance would be less time-consuming. When ontol-
ogy concepts and instances are recognized in text,
their name or ID can be used by applications to
support subsequent queries, navigation, or inference
in the ontology using an ontology query language
(e.g., SPARQL). The procedure we describe here,
preserves hierarchical concept information and links
to ontology concepts and instances. Applications are,
e.g., hybrid deep-shallow question answering (Frank
et al, 2007), automatic typed hyperlinking (Buse-
mann et al, 2003) of instances and concepts occur-
ring in documents, or other innovative applications
that combine Semantic Web and NL processing tech-
nologies, e.g., for semantic search (Scha?fer et al,
2008).
The second part of this paper outlines the inverse
transformation from feature logics (FL) into descrip-
tion logics (DL). Walking along this direction has
the big advantage of potentially applying subsequent
description logic reasoners to the lexical semantics
of natural language input text in order to infer new
knowledge, e.g., in interactive natural language ques-
588
tion answering. As an example, we will carefully de-
velop the (approximate) translation of so-called ro-
bust minimal recursion semantic (RMRS) structures
(Copestake, 2003) into OWL descriptions (McGuin-
ness and van Harmelen, 2004). RMRS structures
are the semantic output of various NL processing
engines, encoded in typed feature structures (TFS).
Since NL processors (e.g., taggers, chunkers, deep
parsers) only build up structure, subsequent process-
ing steps are either not realized or implemented in ad
hoc way,
? dealing with merging & normalization of
RMRS,
? infering new knowledge (e.g., w.r.t. the forego-
ing dialog),
? taking into account extralinguistic knowledge
for reasoning.
Now, by moving from a specialized ?designer lan-
guage? (RMRS) to OWL, we can take advantage of
years of solid theoretical and practical work in logic,
especially in description logics. Since OWL is an
instance of the description logics family and the de-
facto language for the Semantic Web, we can utilize
the built-in reasoning capabilities of OWL and (rule-
based) description logic reasoners.
The structure of this paper is as follows. In the
next section, we outline the relationship between de-
scription logics and feature logics, trying to make
clear what they have in common, but at the same
time explaining their differences. Section 3 describes
the syntactic mapping process from the ontology
to feature structure descriptions. In Section 4, we
present an example where recognized named entities
enriched with ontology information are used in hy-
brid NL processing and subsequent applications. Af-
ter that, Section 5 explains the mapping of RMRS
structures into OWL descriptions. Finally, Section 6
shows that a subsequent description logic reasoner
can utilize these descriptions to infer new knowledge.
2 The relationship between description
and feature logics
Description logics (DL) (Baader et al, 2003) and fea-
ture logics (FL) (Carpenter, 1992) have been pursued
independently for quite a while. Their close relation-
ship was recognized by (Nebel and Smolka, 1990).
Instances of both families of knowledge representa-
tion formalisms are usually decidable two-variable
fragments of first-order predicate logic. Even though
DL dialects usually have an intractable worst-case
complexity, average-case reasoning is usually fast,
due to the availability of highly-optimized tableaux
reasoners. When adding seemingly easy constructs
such as ?role-value maps? (the analog to reentran-
cies), the underlying logical calculus becomes unde-
cidable.
From an abstract viewpoint, both DL and FL em-
ploy unary and binary predicates for which the two
communities invented different names (we only list
some of them):
arity description logic feature logic
unary concept, class type, category
binary role, property feature, attribute
Though these names are different, both represen-
tation families (usually) vary in further, not so subtle
details:
description logic feature logic
open world assumption closed world assumption
full Boolean concept logic only conjunctions
relational properties functional properties
role-value maps forbidden reentrancies allowed
Let us be more verbose here to see the descrip-
tional consequences of both approaches in terms of
a mutual translation. We note here that we take
OWL (McGuinness and van Harmelen, 2004) as an
instance of DL and TDL (type description language)
(Krieger and Scha?fer, 1994) as an example of FL.
OWL, the outcome of the DAML+OIL standard-
ization, is regarded to be the de-facto language for
the Semantic Web. OWL still makes use of con-
structs from RDF and RDFS, but restricts the ex-
pressive power of RDFS, thereby ensuring decidabil-
ity of the standard inference problems. Compared to
RDF(S), OWL provides more fine-grained modelling
constructs, such as intersectionOf or unionOf.
Within the Head-Driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994) paradigm in
modern computational linguistics (CL), TDL is a
language that has been employed in various imple-
mented systems, such as PAGE, LKB, PET, or
SProUT .
Before going into the details of our approximate
transformation schema, let us quickly explain how
to atomize a typed feature structure (TFS) in terms
of description logic primitives, using OWL. Consider
the following TFS which is a gross simplification of
the Head-Feature Principle in HPSG. In terms of
the ?one-dimensional? line-based TDL notation, we
write
phrase1 := phrase &
[HEAD #h1, HEAD-DTR|HEAD #h1],
or as a two-dimensional AVM (attribute-value ma-
trix) notation, we have
589
phrase1 ?
[ phrase
HEAD h1
HEAD-DTR|HEAD h1
]
Assuming that this is an individual of class phrase,
we can obtain a meaning-preserving OWL represen-
tation (we assume that HEAD and HEAD-DTR are
functional OWL object properties):
<owl:Thing rdf:ID="h1"/>
<rdf:Description rdf:about="hdtr1">
<rdf:type rdf:resource="owl:Thing"/>
<HEAD rdf:resource="h1"/>
</rdf:Description>
<rdf:Description rdf:about="phr1">
<rdf:type rdf:resource="phrase"/>
<HEAD rdf:resource="h1"/>
<HEAD-DTR rdf:resource="hdtr1"/>
</rdf:Description>
Note that only the top-level structure is explic-
itly typed (phrase); every other substructure thus
is assigned the most general type, which trans-
lates into the OWL class owl:Thing. Note also
the sharing of information under paths HEAD and
HEAD-DTR|HEAD?this is realized by refering to the
the name h1 in the above RDF/OWL description for
phr1 and hdtr1.
Given a set of OWL descriptions, obtaining the in-
verse direction from DL to FL should now be clear. It
is important here to group statements that are related
to a specific class, viz., inheritance information (e.g.,
intersectionOf) together with property informa-
tion about roles that are ?introduced? on a given class
(as given by the value of rdfs:domain ). In Sec-
tion 3, we focus on this inverse direction (DL-to-FL),
whereas Section 5 exemplifies the FL-to-DL direc-
tion.
Let us finally elaborate fundamental differences
between the DL and FL families that can only be ap-
proximated in terms of ?less expressive? constructs.
Open vs. closed world assumption. Typed feature
logics usually ?live? in a closed world, meaning that
if two types t1 and t2 do not share a common sub-
type (having a greatest lower bound), the unifica-
tion (conjuntion) is assumed to be the bottom type
(OWL: owl:Nothing), meaning that no individual
exists which is of both t1 and t2 at the same time.
This is totally different to the DL point of view: what
can not proven to be true (whether the conjunction
of t1 and t2 denotes the empty set) is not believed
to be false. Thus we either have to introduce a new
type t on the FL side, abbreviating the conjunction of
t1 and t2 (TDL: t := t1 & t2.), or to close the sub-
class hierarchy on the DL side: ? ? t1 u t2 (OWL:
disjointWith). This decision clearly depends on
the direction of the transformation.
Boolean vs. conjunctive description logic. Typed
feature logics rarely provide more than conjunc-
tions of feature-value constraints. This is due to the
fact that disjunctive descriptions render almost linear
(conjunctive) unification exponential. A full Boolean
calculus, such as OWL DL, even has an NEXPTIME
complexity. Thus it is clear that the direction from
DL to FL can only be approximated. The inverse di-
rection is clearly trivial with the notable exception of
reentrancies (see below).
To flesh out our point, consider the DL axiom
human ? man unionsq woman that fully determines (?)
human in terms of the union of the concepts man
and woman. Given the syntax of TDL, we can ap-
proximate parts of the intended meaning of the de-
scription by man :< human and woman :< human,
since the above DL axiom entails that man v human
and woman v human is the case. This is ex-
actly specified by the above two TDL type defini-
tions. Further, not so trivial approximations can be
found in (Flickinger, 2002). The idea here is that
foreseeable disjunctions of DL concepts can be emu-
lated by introducing additional FL types (in the worst
case, exponentially-many new types, however). Even
negated concepts can be simulated this way, since FL
lives in a closed world (see above).
Relational vs. functional properties. By default,
roles in DL are relational properties, meaning that
for a fixed individual in the domain of a given role,
the number of individuals in the range needs not to
be 0 or 1. DL further allows to impose cardinality
(or number) restrictions on roles, so that we might
write ? 0 livingParents u ? 2 livingParents which
says that one can have at least 0 and at most 2 liv-
ing parents. This is in sharp contrast to FL which
usually assume functional roles (so-called features),
making such roles essentially partial functions. A
partial workaround has been proposed in CL systems
by using (ordered) difference lists to collect informa-
tion. Other systems, such as SProUT (Krieger et al,
2004), come up with bags (or multisets) that even vi-
olate the foundational axiom (a set must not contain
itself) in order to achieve runtime efficiency.
Summarizing, the FL-to-DL direction of translat-
ing features into roles is easy, since features in FL
can be easily defined as functional roles in DL (OWL
even provides the owl:FunctionalProperty char-
acteristics). The inverse direction is only a gross ap-
proximation in that cardinality constraints can not be
590
stated on the FL side.
Role-value maps & reentrancies. The above Head-
Feature Principle example seems to indicate that role-
value maps can be easily represented in DL, simply
by using the name of an individual to specify iden-
tity. In fact, this is true, but only for the ABox of
a knowledge base, i.e., only for the set of individu-
als (or instances). However, the notion of role-value
maps in DL or reentrencies in FL refers to the TBox
and the set of concept definitions, resp. Thus, one
can not intensionally specify identity of information
for a potentially infinite number of individuals via a
class axiom in DL, but needs to extensionally spec-
ify identity of information for each individual in the
ABox.
3 OntoNERdIE: from OWL to TDL
In this section, we describe an instantiation of the DL-
to-FL mapping. OntoNERdIE is an offline procedure
that maps ontology concept and instance information
to lingware resources (Scha?fer, 2006). The approach
has been implemented for the language technology
ontology that backs up the LT World web portal
(http://www.lt-world.org), but can be easily adapted
to other domains and ontologies, since it is fully au-
tomated, except for the choice of relevant main con-
cepts and properties that are going to be mapped
which is a matter of configuration.
The target named entity recognition and infor-
mation extraction tool we employ here is SProUT
(Droz?dz?yn?ski et al, 2004), a shallow multilingual,
multi-purpose NL processor. The advantage of
SProUT in the described approach for named en-
tity recognition and information extraction is that
it comes with (1) a type system and typed feature
structures as the basic data type, (2) a powerful,
declarative rule mechanism with regular expressions
over typed feature structures, and (3) a highly effi-
cient gazetteer module with fine-grained, customiz-
able classification of recognized entities.
SProUT provides additional modules such as mor-
phology or a reference resolver that can be exploited
in the rule system, e.g., to use context or morpholog-
ical variation for improved NER. Through automat-
ically generated mappings, SProUT output enriched
with ontology information can be used for robust, hy-
brid deep-shallow parsing, and semantic analysis.
In this section, we describe the offline process-
ing steps of the OntoNERdIE approach. The on-
line part in applications is described in Section 4.
The approach heavily relies on XSLT transformations
(Clark, 1999) of the XML representation formats,
both in the offline mapping and in the online appli-
cation.
3.1 RDF preprocessing
Input to the mapping procedure is an OWL on-
tology file, containing both concept and instance
descriptions. The RDF file is pre-processed with
a generic XSLT stylesheet sorting and merging
rdf:Descriptions that are distributed over the file
but which belong together. We use XSLT?s key and
generate-id functions. Depending on the appli-
cation, the next two processing stages take a list of
concepts as filter because it will typically not be de-
sirable to extract all concepts or instances available
in the ontology. In both cases, resource files are gen-
erated as output that can be used to extend existing
named entity recognition resources. E.g., while gen-
eral rules can recognize domain-independent named
entities (e.g., any person name), the extended re-
source contains specific, and potentially more de-
tailed information for domain-specific entities.
3.2 Extracting inheritance
The second stylesheet converts RDFS subClassOf
statements from output step 1 (Section 3.1) into a
set of TDL type definitions that can be immediately
imported by the SProUT named entity recognition
grammar. Currently 1,260 type definitions for the
same number of subClassOf statements in the LT
World ontology are generated, e.g.,
NL_Parsing := Written_Language &
Language_Analysis.
This is of course a lossy conversion because not
all relations supported in an OWL ontology (such as
unionOf, disjointWith, intersectionOf) are
mapped. However, we think that for NE classifi-
cations, the subClassOf taxonomy mappings will
be sufficient. Other relations could be formulated
as direct (though slower) ontology queries using the
OBJID mechanism described in the next step. If
the target of OntoNERdIE is a NER system differ-
ent from SProUT and without a type hierarchy, this
step can be omitted. The subClassOf information
can always be gained by querying the ontology ap-
propriately on the basis of the concept name.
3.3 Generating gazetteer entries
The next stylesheet selects statements about instances
of relevant concepts via the rdf:type information
and converts them to structured gazetteer source files
for the SProUT gazetteer compiler (or into a differ-
ent format in case of another NER system). In the
following example, one of the approximately 20,000
converted entries for LT World is shown.
591
Bernd Kiefer | GTYPE: lt_person |
SNAME: "Kiefer" | GNAME: "Bernd" |
CONCEPT: Active_Person |
OBJID: "obj_62893"
The attribute CONCEPT contains a TDL type gener-
ated in step 2 (described in Section 3.2). For con-
venience, several ontology concepts are mapped (de-
fined manually as part of the configuration of the
stylesheet) to only a few named entity classes (under
attribute GTYPE). For the LT World ontology, these
classes are person, organization, event, project, prod-
uct, and technology. The advantage of this simplifica-
tion is that NER context rules from existing SProUT
named entity grammars can be re-used for improved
robustness and disambiguation.
The rules, e.g., recognize name variants with title
like Prof. Kiefer, Dr. Kiefer, or Mr. Kiefer with or
without a first name. Moreover, context (e.g., prepo-
sitions with location names, verbs), morphology and
reference resolution information can be exploited in
these rules.
The following SProUT rule lt-event (extended
TDL syntax) simply copies the slots of a matched
gazetteer entry for events (e.g., a conference) to the
output as a recognized named entity.
lt-event :> gazetteer &
[GTYPE lt_event, SURFACE #name,
CONCEPT #concept, OBJID #objid,
GABBID #abbrev]
->
ne-event & [EVENTNAME #name,
CONCEPT #concept, OBJID #objid,
GABBID #abbrev].
OBJID contains the object identifier of the instance
in the ontology. It can be used as a link back to the
full knowledge stored in the ontology, e.g., for sub-
sequent queries, like Who else participated in project
[with OBJID obj 4789]?.
In case multiple instances with same names but dif-
ferent object IDs occur in the ontology (which actu-
ally happens to be the case in LT World), multiple al-
ternatives are generated as output which is probably
the expected and desired behavior (e.g., for frequent
names such as John Smith). On the other hand, if
product or event names with an abbreviated variant
exist in the ontology, they both point to the same ob-
ject ID (provided they are stored appropriately in the
ontology).
4 Application to hybrid deep-shallow
parsing
We now describe and exemplify how the named en-
tities enriched with ontology information are em-
ployed in a robust, hybrid deep-shallow architec-
ture, combining domain-specific shallow named en-
tity recognition with deep, broad-coverage, domain-
independent, unification-based parsing for generating
a semantic representation of the meaning of parsed
sentences. An application of this scenario is deep
question analysis for question answering of struc-
tured knowledge sources, encoded as an OWL ontol-
ogy (Frank et al, 2007).
The output of SProUT for a recognized named en-
tity is a typed feature structure in XML containing
the instantiated RHS of the recognition rule as shown
in step 3 (Section 3.3) with the copied structured
gazetteer data, plus some additional information like
character span, named entity type, etc. The mapping
of recognized named entities to generic lexicon en-
tries of the deep grammar, in this case the English Re-
source Grammar (Flickinger, 2002), for hybrid pro-
cessing are performed through an XSLT stylesheet,
automatically generated from the SProUT type hi-
erarchy. Analogous mappings are supported for
other grammars available in the DELPH-IN reposi-
tory (see http://www.delph-in.net). The mapping ba-
sically transports the surface string, a character span,
and a generic lexicon type of the deep grammar for a
chart item to be generated in an XML format, read-
able by the deep parser. A sample output of the se-
mantic representation generated by the deep parser is
shown in Figure 1. The semantic representation for-
mat, called RMRS, is described in (Copestake, 2003)
and in Section 5.3 below.
In addition to the basic named entity type mapping
for default lexicon entries, the recognized concepts
are also useful for constraining the semantic sort in
the deep grammar in a more fine-grained way (e.g.,
for disambiguation). The deep parser?s XML input
chart format foresees ?injection? of such types into
deep structures. Here, OBJID and other structured in-
formation, like given name and surname, can be pre-
served in the representation. The advantage of the
RMRS format is that it can also be combined ex post
with analyses from other deep or shallow NLP com-
ponents, e.g., with partial analyses when a full parse
fails.
5 (R)MRS2OWL: from TDL to OWL
This section is devoted to the translation of MRSs
which are encoded as TFSs into a set of OWL expres-
sions. An example of a variant of MRS, a so-called
robust MRS (RMRS) has already been depicted in
Figure 1. RMRS will be explained in more detail in
Section 5.3.
592
?
??????????????????????????
TEXT ?Did Bernd Kiefer present a paper at IJCAI 2005??
TOP h1
RELS
?
??????????????????
??????????????????
?
??
int m rel
LBL h1
ARG0 e2
MARG h1
?
??
?
??
prpstn m rel
LBL h1001
ARG0 e2
MARG h5
?
??
?
????
proper q rel
LBL h6
ARG0 x8
RSTR h7
BODY h9
?
????
?
????
named rel
LBL h10
ARG0 x8
CARG BerndKiefer
?
????
?
????????
present v
LBL h11
ARG0 e2 tense=past
ARG1 x8 num=sgpers=3
ARG2 x12 num=sgpers=3
ARG3 u13
?
????????
?
????
a q
LBL h14
ARG0 x12
RSTR h15
BODY h16
?
????
?
?
paper n
LBL h17
ARG0 x12
?
?
?
?????
at p
LBL h1002
ARG0 e19 tense=u
ARG1 e2
ARG2 x18 num=sgpers=3
?
?????
?
????
proper q rel
LBL h20
ARG0 x18
RSTR h21
BODY h22
?
????
?
??
named rel
LBL h23
ARG0 x18
CARG IJCAI 2005
?
??
?
??????????????????
??????????????????
HCONS {h5 qeq h11, h7 qeq h10, h15 qeq h17, h21 qeq h23}
ING {h1 ing h1001, h11 ing h1002}
?
??????????????????????????
Figure 1: RMRS generated through hybrid parsing.
5.1 Some words on MRSs
There exist good linguistic reasons for assuming
that the semantics of a sentence like Kim ate a
cookie is not past(eat(kim?, cookie?), but instead
something like ?e . eating(e) ? subject(e, kim?) ?
object(e, cookie?)?before(e,now). This approach to
NL semantics is often called Event or Davidsonian
semantics (named after the American philosopher
Donald Davidson). HPSG has incorporated ideas
from event semantics by defining so-called Minimal
Recursion Semantics (MRS) structures (Copestake et
al., 2005) that are constructed in parallel with the syn-
tactic structure. MRS as such provides a flat com-
positional semantics and maximizes splitting using
equality constraints. Structural ambiguities, as can
be found in the famous sentence Every farmer who
owns a donkey beats it, are not spelled out, but in-
stead quantifier scope is underspecified. By impos-
ing constraints on the scope, specific analysis trees
can be reconstructed. Robust MRS (RMRS) (Copes-
take, 2003), derived from MRS, was designed as an
abstract language that supports the integration of par-
tial and total analysis results from deep and shallow
processors and provides a good tradeoff between ro-
bustness and accuracy (see (Frank et al, 2004) for an
example).
5.2 Why the translation is useful
NL processors (e.g., tokenizer, POS tagger, shallow
chunk parser, deep parser, etc.) that are geared to-
wards (R)MRSs (or another common language) have
the potential of combining their output on the level
of semantics. However, these engines do not provide
any form of reasoning, i.e., they only build up struc-
ture.
Consider, for instance, a deep unification-based
parser that might return analyses represented as typed
feature structures, where both syntax and semantics
(the MRS) has been constructed with the help of uni-
fication. Now, to bring structures together and to
perform deductive and abductive forms of reason-
ing, subsequent computational steps are necessary,
but these steps strictly go beyond the power of or-
dinary parsing.
In order to perform these subsequent steps, we
need a concrete implemented (and hopefully stan-
dardized) representation language for which editing,
displaying, and reasoning tools are available. Ex-
actly OWL accomplishes these requirements. Hence
we think that the described below translation process
from (R)MRSs into OWL is worthwhile, especially
when one is interested in interfacing linguistic knowl-
edge (the (R)MRSs) with extralinguistic ontologies
for specific domains.
5.3 The translation process
In order to explain the translation process, we will
analyze the RMRS depicted in Figure 1. The RMRS
was derived from the MRS of the deep unification-
based parser. We see that an RMRS contains four dis-
tinguished attributes (the TEXT attribute is only added
for illustration):
1. TOP: a handle (pointer) to the top-level structure.
2. RELS (relations): a set of so-called elementary
predications (EP), encoded as TFSs, each ex-
pressing an atomic semantic unit that can not be
593
further decomposed; due to the lack of sets, TFS
grammars use a list here.
3. HCONS (handle constraints): a set of so-called
qeq constraints (equality modulo quantifiers);
the left side of a qeq constraints (a handle h in
an argument position) is always related to a label
l of an EP, (i) either directly (h = l) or (ii) indi-
rectly, in case h dominates a quantifier q, such
that BODY(q) = l or again another quantifier,
where condition (ii) is recursively applied again.
4. ING (in group): a set of relations used to express
a conjunction of EPs from the set RELS.
Giving this information, it should now be clear that
the TFS from Figure 1 must be realized as an instance
of the OWL class RMRS and that the features TOP and
RELS must be implemented as roles in OWL, all de-
fined on RMRS through the use of rdfs:domain:
<owl:Class rdf:ID="RMRS"/>
<owl:ObjectProperty rdf:ID="TOP">
<rdf:type rdf:resource=
"&owl;FunctionalProperty"/>
<rdfs:domain rdf:resource="#RMRS"/>
<rdfs:range rdf:resource=
"#HandleVar"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:ID="RELS">
<rdfs:domain rdf:resource="#RMRS"/>
<rdfs:range rdf:resource="#EP"/>
</owl:ObjectProperty>
TOP takes exactly one argument, hence we use OWL?s
FunctionalProperty characteristics mechanism
here. Since RELS (as well as HCONS and ING, see
below) might take more than one argument, we do
not impose a property restriction here, so they are re-
lational by default. TOP maps to a special variable
class (see below), and RELS to EPs.
TOP. The TOP property always takes a handle vari-
able; other variable classes, such as label vars are
used for restricting properties:
<owl:Class rdf:ID="Var"/>
<owl:Class rdf:ID="HandleVar">
<rdfs:subClassOf rdf:resource=
"#Var"/>
</owl:Class>
<owl:Class rdf:ID="LabelVar">
<rdfs:subClassOf rdf:resource=
"#Var"/>
</owl:Class>
Actually, this modelling is mere window-dressing
and clearly verbose, since an OWL instance of
class RMRS is always assigned a name (<RMRS
rdf:ID="...">), and in fact, this name can be
taken to be the TOP handle. This means that we can
in principle forbear from the TOP property. However,
if we want to utilize morpho-syntactical information
in subsequent inference steps, we have to enrich the
above variable classes with further properties/roles,
such as tense, pers, or num (see, e.g., the ?struc-
tured? variables in the structure for present v in
Figure 1).
RELS. Elements of RELS, i.e., concrete EPs are
essentially ?slimed? instances of feature structure
types. Overall, this means that we have to represent
the relevant types of the linguistic type hierarchy and
their subsumption relationship as OWL classes. As
shown in Section 3, this process can be automated
and only some guidance from a knowledge engineer
is necessary to mark the features that should not be
taken over to the DL side.
HCONS and ING. HCONS essentially specifies a
ternary relation, but since OWL (and DL in general)
are restricted to unary and binary relations, one way
to model a qeq constraint is to define a binary prop-
erty, consisting of a left-hand and a right-hand side.
From what has been said above, the left-hand side is a
handle and the right-hand side a label, hence we have
the following declaration for qeq:
<owl:ObjectProperty rdf:ID="qeq">
<rdfs:domain rdf:resource=
"#HandleVar"/>
<rdfs:range rdf:resource=
"#LabelVar"/>
</owl:ObjectProperty>
Given this way of modelling, it is now impossible to
define a property HCONS (as well as ING) on class
RMRS, since properties can only take instances of
classes, but not instances of other properties. How-
ever, since we assume that our variables (instances
of class Var) are always unique at runtime, it is in
principle not necessary to group the qeq constraint
inside an (R)MRS?note that there is still a connec-
tion between EPs and qeq constraints through the use
of variables. However, if we want to talk about/want
to access the qeq constraints of a specific (R)MRS
instance directly, this kind of modelling is somewhat
unhandy.
To overcome this seemingly wrong representation
(we are neutral about this), we have to ?reify? or
?wrap? qeq property instances. This would mean that
qeq would no longer be a property, but instead be-
comes a class, say QEQ, consisting of a right-hand
and a left-hand side. With this in mind, we can eas-
ily model, e.g., the first qeq constraint qeq1 from the
above figure:
<RMRS rdf:ID="rmrs1">
<TOP rdf:resource="#h1"/>
<RELS rdf:resource="#ep1"/>
<HCONS rdf:resource="#qeq1"/>
594
...
</RMRS>
<QEQ rdf:ID="qeq1">
<LHS rdf:resource="#h5"/>
<RHS rdf:resource="#h11"/>
</QEQ>
<HandleVar rdf:ID="h1"/>
<HandleVar rdf:ID="h5"/>
<LabelVar rdf:ID="h11"/>
<int_m_rel rdf:ID="ep1">
<LBL rdf:resource="#h1"/>
<ARG0 rdf:resource="#e2"/>
<MARG rdf:resource="#h1"/>
</int_m_rel>
What we have said about qeq constraints so far do
hold for in-group constraints as well.
6 DL reasoning: a small example
We have already said that the OWL representation of
RMRS structures are a good starting point to imple-
ment some useful forms of reasoning. Consider the
sentence Did Bernd Kiefer present a paper at IJCAI
2005? from Figure 1. From the resulting EPs and
with the help of an in-group constraint, we can infer
the fact that Bernd Kiefer was (physically) at IJCAI
2005, assuming he has presented a paper (which he
did). The inference rule achieving this can be stated
informally as presenting a paper at a conference en-
tails being at the conference. A more formal rep-
resentation in terms of feature structures is given in
Figure 2.
Clearly this rule can be rewritten to operatate on
OWL expressions (as is proposed in SWRL (Hor-
rocks et al, 2004)) or on the underlying RDF triple
notation (which, for instance, OWLIM (Kiryakov,
2006) assumes). Note the use of logical variables
in the above rule in order to formulate the transport
of information from the LHS to the RHS. The above
rule abstract away from concrete persons and loca-
tions through the use of logic variables ?p and ?l.
Note further that the resulting RHS output structure
is no longer a RMRS but a domain-specific represen-
tation (somewhat simplified in this example) that can
be queried for or can be employed in subsequent rea-
soning tasks.
In (Frank et al, 2007), an implemented approach is
described that utilizes an additional frame represen-
tation layer (Ruppenhofer et al, 2006) in which rules
of the above kind are applied, using the term rewrit-
ing system of (Crouch, 2005).
7 Summary
Our paper returned to mind that there exists a close
relationship between feature logics as used in compu-
tational linguistics and description logics employed
in the Semantic Web community. This relation-
ship can be utilized to obtain more and better se-
mantic annotations through information extraction
and deep parsing of text documents. We have in-
dicated that specific language constructs in FL and
DL can be mutally transformed without losing any
meaning, whereas others can only be approximated
(esp., role-value maps/reentrencies and functional
features/relational roles).
We have described an implemented procedure that
maps ontology instances and concepts to named en-
tity recognition and information extraction resources.
As argued in the paper, the benefits for minimized
domain-specific and linguistic knowledge engineer-
ing are manifold. An application using hybrid shal-
low and deep NL processing on the basis of the
mapped ontology data has been successfully imple-
mented for question answering. This application
(Frank et al, 2007) employs an additional frame se-
mantics layer (cf. Section 6) on which light forms
of reasoning take place. In order to make this addi-
tional layer superflous, we have described a transfor-
mation scheme that maps (R)MRS into OWL descrip-
tions. Given these descriptions, rules of the above
kind (Section 6) can directly operate on OWL, and
no additional translation is necessary to query the in-
stance data, encoded in RDF/OWL.
References
Baader, Franz, Diego Calvanese, Deborah McGuinness,
Daniele Nardi, and Peter Patel-Schneider. 2003. The
Description Logic Handbook. Cambridge University
Press, Cambridge.
Busemann, Stephan, Witold Droz?dz?yn?ski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Scha?fer, Hans Uszko-
reit, and Feiyu Xu. 2003. Integrating Information Ex-
traction and Automatic Hyperlinking. In Proceedings of
the Interactive Posters/Demonstration at ACL-03, pages
117?120.
Carpenter, Bob. 1992. The Logic of Typed Feature Struc-
tures. Tracts in Theoretical Computer Science. Cam-
bridge University Press, Cambridge.
Clark, James, 1999. XSL Transformations (XSLT). W3C,
http://w3c.org/TR/xslt.
Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
3(4):281?332, 12. DOI 10.1007/s11168-006-6327-9.
Copestake, Ann. 2003. Report on the Design of RMRS.
Technical Report D1.1b, University of Cambridge, Cam-
bridge, UK.
595
?
??
present v
LBL ?h1
ARG1 ?s
ARG2 ?o
?
?? &
[
paper n
ARG0 ?o
]
&
[ named rel
ARG0 ?s
CARG ?p
]
&
[ at p
LBL ?h2
ARG2 ?x
]
&
[ named rel
ARG0 ?x
CARG ?l
]
& (?h1 ing ?h2) =?
[
PERSON ?p
LOCATION ?l
]
Figure 2: RMRS rule over EPs and in-group constraint.
Crouch, Richard. 2005. Packed rewriting for map-
ping semantics to KR. In Proceedings of the Interna-
tional Workshop on Computational Semantics (IWCS) 6,
Tilburg.
Droz?dz?yn?ski, Witold, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow Pro-
cessing with Unification and Typed Feature Structures?
Foundations and Applications. KI, 04(1):17?23.
Flickinger, Dan. 2002. On building a more efficient gram-
mar by exploiting types. In Oepen, S. D. Flickinger,
J. Tsuji, and H. Uszkoreit, editors, Collaborative Lan-
guage Engineering. A Case Study in Efficient Grammar-
based Processing, pages 1?17. CSLI Publications.
Frank, Anette, Kathrin Spreyer, Witold Droz?dz?yn?ski, Hans-
Ulrich Krieger, and Ulrich Scha?fer. 2004. Constraint-
Based RMRS Construction from Shallow Grammars. In
Mu?ller, Stefan, editor, Proceedings of the HPSG04 Con-
ference Workshop on Semantics in Grammar Engineer-
ing, pages 393?413. CSLI Publications, Stanford, CA.
Frank, Anette, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, and Ulrich Scha?fer.
2007. Question answering from structured knowledge
sources. Journal of Applied Logics, Special Issue on
Questions and Answers: Theoretical and Applied Per-
spectives, 5(1):20?48.
Haghighi, Aria and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1152?1161.
Horrocks, Ian, Peter F. Patel-Schneider, Harold Boley, Said
Tabet, Benjamin Grosof, and Mike Dean. 2004. SWRL:
A semantic web rule language combining OWL and
RuleML. W3C Member Submission.
Kiryakov, Atanas. 2006. OWLIM: balancing between
scalable repository and light-weight reasoner. Presen-
tation of the Developer?s Track of WWW2006.
Krieger, Hans-Ulrich and Ulrich Scha?fer. 1994. TDL?A
Type Description Language for Constraint-Based Gram-
mars. In Proceedings of the 15th International Confer-
ence on Computational Linguistics, COLING-94, pages
893?899.
Krieger, Hans-Ulrich, Witold Droz?dz?yn?ski, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. A Bag of Use-
ful Techniques for Unification-Based Finite-State Trans-
ducers. In Proceedings of KONVENS 2004, pages 105?
112.
McGuinness, Deborah L. and Frank van Harmelen. 2004.
OWL Web Ontology Language Overview. Technical re-
port, W3C. 10 February.
Nebel, Bernhard and Gert Smolka. 1990. Represen-
tation and reasoning with attributive descriptions. In
Bla?sius, K.-H., U. Hedtstu?ck, and C.-R. Rollinger, ed-
itors, Sorts and Types in Artificial Intelligence, pages
112?139. Springer, Berlin. Also available as IWBS Re-
port 81, IBM Germany, September 1989.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Linguis-
tics. University of Chicago Press, Chicago.
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: extended theory and prac-
tice. Technical report, International Computer Sci-
ence Institute (ICSI), University of California, Berkley.
http://framenet.icsi.berkley.edu/book/book.pdf.
Scha?fer, Ulrich, Hans Uszkoreit, Christian Federmann,
Torsten Marek, and Yajing Zhang. 2008. Extracting
and querying relations in scientific papers on language
technology. In Proceedings of LREC-2008.
Scha?fer, Ulrich. 2006. OntoNERdIE ? mapping and link-
ing ontologies to named entity recognition and infor-
mation extraction resources. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation LREC-2006, pages 1756?1761, Genoa, Italy.
596
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 7?13,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
The ACL Anthology Searchbench
Ulrich Scha?fer Bernd Kiefer Christian Spurk Jo?rg Steffen Rui Wang
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
D-66123 Saarbru?cken, Germany
{ulrich.schaefer,kiefer,cspurk,steffen,wang.rui}@dfki.de
http://www.dfki.de/lt
Abstract
We describe a novel application for structured
search in scientific digital libraries. The ACL
Anthology Searchbench is meant to become a
publicly available research tool to query the
content of the ACL Anthology. The applica-
tion provides search in both its bibliographic
metadata and semantically analyzed full tex-
tual content. By combining these two features,
very efficient and focused queries are possi-
ble. At the same time, the application serves
as a showcase for the recent progress in nat-
ural language processing (NLP) research and
language technology. The system currently
indexes the textual content of 7,500 anthol-
ogy papers from 2002?2009 with predicate-
argument-like semantic structures. It also
provides useful search filters based on bib-
liographic metadata. It will be extended to
provide the full anthology content and en-
hanced functionality based on further NLP
techniques.
1 Introduction and Motivation
Scientists in all disciplines nowadays are faced with
a flood of new publications every day. In addi-
tion, more and more publications from the past be-
come digitally available and thus even increase the
amount. Finding relevant information and avoiding
duplication of work have become urgent issues to be
addressed by the scientific community.
The organization and preservation of scientific
knowledge in scientific publications, vulgo text doc-
uments, thwarts these efforts. From a viewpoint of
a computer scientist, scientific papers are just ?un-
structured information?. At least in our own sci-
entific community, Computational Linguistics, it is
generally assumed that NLP could help to support
search in such document collections.
The ACL Anthology1 is a comprehensive elec-
tronic collection of scientific papers in our own field
(Bird et al, 2008). It is updated regularly with
new publications, but also older papers have been
scanned and are made available electronically.
We have implemented the ACL Anthology
Searchbench2 for two reasons: Our first aim is to
provide a more targeted search facility in this col-
lection than standard web search on the anthology
website. In this sense, the Searchbench is meant to
become a service to our own community.
Our second motivation is to use the developed
system as a showcase for the progress that has been
made over the last years in precision-oriented deep
linguistic parsing in terms of both efficiency and
coverage, specifically in the context of the DELPH-
IN community3. Our system also uses further NLP
techniques such as unsupervised term extraction,
named entity recognition and part-of-speech (PoS)
tagging.
By automatically precomputing normalized se-
mantic representations (predicate-argument struc-
ture) of each sentence in the anthology, the search
space is structured and allows to find equivalent or
related predicates even if they are expressed differ-
1http://www.aclweb.org/anthology
2http://aclasb.dfki.de
3http://www.delph-in.net ? DELPH-IN stands for
DEep Linguistic Processing with HPSG INitiative.
7
ently, e.g. in passive constructions, using synonyms,
etc. By storing the semantic sentence structure along
with the original text in a structured full-text search
engine, it can be guaranteed that recall cannot fall
behind the baseline of a fulltext search.
In addition, the Searchbench also provides de-
tailed bibliographic metadata for filtering as well as
autosuggest texts for input fields computed from the
corpus ? two further key features one can expect
from such systems today, nevertheless very impor-
tant for efficient search in digital libraries.
We describe the offline preprocessing and deep
parsing approach in Section 2. Section 3 concen-
trates on the generation of the semantic search in-
dex. In Section 4, we describe the search interface.
We conclude in Section 5 and present an outlook to
future extensions.
2 Parsing the ACL Anthology
The basis of the search index for the ACL Anthol-
ogy are its original PDF documents, currently 8,200
from the years 2002 through 2009. To overcome
quality problems in text extraction from PDF, we
use a commercial PDF extractor based on OCR tech-
niques. This approach guarantees uniform and high-
quality textual representations even from older pa-
pers in the anthology (before 2000) which mostly
were scanned from printed paper versions.
The general idea of the semantics-oriented ac-
cess to scholarly paper content is to parse each sen-
tence they contain with the open-source HPSG (Pol-
lard and Sag, 1994) grammar for English (ERG;
Flickinger (2002)) and then distill and index seman-
tically structured representations for search.
To make the deep parser robust, it is embedded
in a NLP workflow. The coverage (percentage of
full deeply parsed sentences) on the anthology cor-
pus could be increased from 65 % to now more
than 85 % through careful combination of several
robustness techniques; for example: (1) chart prun-
ing, directed search during parsing to increase per-
formance, and also coverage for longer sentences
(Cramer and Zhang, 2010); (2) chart mapping, a
novel method for integrating preprocessing informa-
tion in exactly the way the deep grammar expects
it (Adolphs et al, 2008); (3) new version of the
ERG with better handling of open word classes; (4)
more fine-grained named entity recognition, includ-
ing recognition of citation patterns; (5) new, better
suited parse ranking model (WeScience; Flickinger
et al (2010)). Because of limited space, we will fo-
cus on (1) and (2) below. A more detailed descrip-
tion and further results are available in (Scha?fer and
Kiefer, 2011).
Except for a small part of the named entity recog-
nition components (citations, some terminology)
and the parse ranking model, there are no further
adaptations to genre or domain of the text corpus.
This implies that the NLP workflow could be easily
and modularly adapted to other (scientific or non-
scientific) domains?mainly thanks to the generic
and comprehensive language modelling in the ERG.
The NLP preprocessing component workflow is
implemented using the Heart of Gold NLP mid-
dleware architecture (Scha?fer, 2006). It starts
with sentence boundary detection (SBR) and regu-
lar expression-based tokenization using its built-in
component JTok, followed by the trigram-based PoS
tagger TnT (Brants, 2000) trained on the Penn Tree-
bank (Marcus et al, 1993) and the named entity rec-
ognizer SProUT (Droz?dz?yn?ski et al, 2004).
2.1 Precise Preprocessing Integration with
Chart Mapping
Tagger output is combined with information from
the named entity recognizer, e.g. delivering hypo-
thetical information on citation expressions. The
combined result is delivered as input to the deep
parser PET (Callmeier, 2000) running the ERG.
Here, citations, for example, can be treated as either
persons, locations or appositions.
Concerning punctuation, the ERG can make use
of information on opening and closing quotation
marks. Such information is often not explicit in the
input text, e.g. when, as in our setup, gained through
OCR which does not distinguish between ? and ? or ?
and ?. However, a tokenizer can often guess (recon-
struct) leftness and rightness correctly. This infor-
mation, passed to the deep parser via chart mapping,
helps it to disambiguate.
2.2 Increased Processing Speed and Coverage
through Chart Pruning
In addition to a well-established discriminative max-
imum entropy model for post-analysis parse selec-
8
tion, we use an additional generative model as de-
scribed in Cramer and Zhang (2010) to restrict the
search space during parsing. This restriction in-
creases efficiency, but also coverage, because the
parse time was restricted to at most 60 CPU seconds
on a standard PC, and more sentences could now be
parsed within these bounds. A 4 GB limit for main
memory consumption was far beyond what was ever
needed. We saw a small but negligible decrease in
parsing accuracy, 5.4 % best parses were not found
due to the pruning of important chart edges.
Ninomiya et al (2006) did a very thorough com-
parison of different performance optimization strate-
gies, and among those also a local pruning strategy
similar to the one used here. There is an important
difference between the systems, in that theirs works
on a reduced context-free backbone first and recon-
structs the results with the full grammar, while PET
uses the HPSG grammar directly, with subsumption
packing and partial unpacking to achieve a similar
effect as the packed chart of a context-free parser.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  20  40  60  80  100
sentences x 1000mean parse time (CPU s)
sentence length ??
Figure 1: Distribution of sentence length and mean parse
times for mild pruning
In total, we parsed 1,537,801 sentences, of which
57,832 (3.8 %) could not be parsed because of lexi-
con errors. Most of them were caused by OCR ar-
tifacts resulting in unexpected punctuation character
combinations. These can be identified and will be
deleted in the future.
Figure 1 displays the average parse time of pro-
cessing with a mild chart pruning setting, together
with the mean quadratic error. In addition, it con-
tains the distribution of input sentences over sen-
tence length. Obviously, the vast majority of sen-
tences has a length of at most 60 words4. The parse
times only grow mildly due to the many optimiza-
tion techniques in the original system, and also the
new chart pruning method. The sentence length dis-
tribution has been integrated into Figure 1 to show
that the predominant part of our real-world corpus
can be processed using this information-rich method
with very low parse times (overall average parse
time < 2 s per sentence).
The large amount of short inputs is at first surpris-
ing, even more so that most of these inputs can not
be parsed. Most of these inputs are non-sentences
such as headings, enumerations, footnotes, table cell
content. There are several alternatives to deal with
such input, one to identify and handle them in a pre-
processing step, another to use a special root con-
dition in the deep analysis component that is able
to combine phrases with well-defined properties for
inputs where no spanning result could be found.
We employed the second method, which has the
advantage that it handles a larger range of phenom-
ena in a homogeneous way. Figure 2 shows the
change in percentage of unparsed and timed out in-
puts for the mild pruning method with and without
the root condition combining fragments.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
strictstrict timeoutstrict+fragmentsstrict+fragments timeout
sentence length ??
Figure 2: Unparsed and timed out sentences with and
without fragment combination
Figure 2 shows that this changes the curve for un-
parsed sentences towards more expected character-
istics and removes the uncommonly high percent-
age of short sentences for which no parse can be
computed. Together with the parses for fragmented
4It has to be pointed out that extremely long sentences also
may be non-sentences resulting from PDF extraction errors,
missing punctuation etc. No manual correction took place.
9
Figure 3: Multiple semantic tuples may be generated for a sentence
input, we get a recall (sentences with at least one
parse) over the whole corpus of 85.9 % (1,321,336
sentences), without a significant change for any of
the other measures, and with potential for further im-
provement.
3 Semantic Tuple Extraction with DMRS
In contrast to shallow parsers, the ERG not only
handles detailed syntactic analyses of phrases, com-
pounds, coordination, negation and other linguistic
phenomena that are important for extracting seman-
tic relations, but also generates a formal semantic
representation of the meaning of the input sentence
in the Minimal Recursion Semantics (MRS) repre-
sentation format (Copestake et al, 2005). It consists
of elementary predications for each word and larger
constituents, connected via argument positions and
variables, from which predicate-argument structure
can be extracted.
MRS representations resulting from deep parsing
are still relatively close to linguistic structures and
contain more detailed information than a user would
like to query and search for. Therefore, an additional
extraction and abstraction step is performed before
storing semantic structures in the search index.
Firstly, MRS is converted to DMRS (Copes-
take, 2009), a dependency-style version of MRS
that eases extraction of predicate-argument struc-
ture using the implementation in LKB (Copestake,
2002). The representation format we devised for the
search index we call semantic tuples, in fact quintu-
ples <subject, predicate, first object, second object,
adjuncts>; example in Figure 3. The basic extrac-
tion algorithm consists of the following three steps:
(1) calculate the closure for each elementary pred-
ication based on the EQ (variable equivalence) re-
lation, and group the predicates and entities in each
closure respectively; (2) extract the relations of the
groups, which results in a graph as a whole; (3) re-
cursively traverse the graph, form one semantic tu-
ple for each predicate, and fill in the corresponding
information under its scope, i.e. subject, object, etc.
In the example shown in Figure 3, entity groups
like ?our systems?, ?the baseline?, and ?good perfor-
mance on the SRL task?, as well as predicate groups
?beating? and ?achieved? are formed at the first step.
In the second step, the graph structure is extracted,
i.e., the relation between the groups. Finally, two
semantic tuples are filled in with both the predicates
and the corresponding information. Notice that the
modifier(s) of the entity belong to the same entity
group, but the modifier(s) of the predicate will be
put into the Adjuncts slot. Similarly, the coordina-
tion of the entities will be put into one entity group,
while the coordination of predicates will form mul-
tiple semantic tuples.
Since we are extracting predicate-argument struc-
ture, syntactic variations such as passive construc-
tions and relative clauses will be all ?normalized?
into the same form. Consequently, ?the book which
I read?, ?I read the book?, and ?the book was read
by me? will form the exact same semantic tuple <I,
read, the book, N/A, N/A>. The resulting tuple
structures along with their associated text are stored
in an Apache Solr/Lucene5 server which receives
queries from the Searchbench user interface.
4 Searchbench User Interface
The Searchbench user interface (UI) is a web appli-
cation running in every modern, JavaScript-enabled
web browser. As can be seen in Figure 4, the UI
is divided into three parts: (1) a sidebar on the left
(Filters View), where different filters can be set that
constrain the list of found documents; (2) a list of
found documents matching the currently set filters
in the upper right part of the UI (Results View); (3)
5http://lucene.apache.org/solr
10
Figure 4: Searchbench user interface with different filters set and currently looking at the debug menu for a sentence.
the Document View in the lower right part of the UI
with different views of the current document.
A focus in the design of the UI has been to al-
low the user to very quickly browse the papers of the
ACL Anthology and then to find small sets of rele-
vant documents based on metadata and content. This
is mainly achieved by these techniques: (i) changes
in the collection of filters automatically update the
Results View; (ii) metadata and searchable content
from both the Results View and the Document View
can easily be used with a single click as new filters;
(iii) filters can easily be removed with a single click;
(iv) manually entering filter items is assisted by sen-
sible autosuggestions computed from the corpus; (v)
accidental filter changes can easily be corrected by
going back in the browser history.
The following kinds of filters are supported:
Statements (filter by semantic statements, i.e., the
actual content of sentences, see Section 4.1), Key-
words (filter by simple keywords with a full-text
search), Topics (filter by topics of the articles that
were extracted with an extended approach of the un-
supervised term extractor of Frantzi et al (1998)),
Publication (filter by publication title/event), Au-
thors (filter by author names), Year (filter by pub-
lication year), Affiliations (filter by affiliation or-
ganizations), Affiliation Sites (filter by affiliation
cities and countries)6. Found papers always match
all currently set filters. For each filter type multi-
ple different filter items can be set; one could search
for papers written jointly by people from different
research institutes on a certain topic, for example.
Matches of the statements filter and the keywords
filter are highlighted in document snippets for each
paper in the Results View and in the currently se-
lected paper of the Document View.
Besides a header displaying the metadata of the
currently selected paper (including the automatically
extracted topics on the right), the Document View
provides three subviews of the selected paper: (1)
the Document Content View is a raw list of the sen-
tences of the paper and provides different kinds of
interaction with these sentences; (2) the PDF View
shows the original PDF version of the paper; (3) the
Citations View provides citation information includ-
6Affiliations have been added using the ACL Anthology
Network data (Radev et al, 2009).
11
ing link to the ACL Anthology Network (Radev et
al., 2009).
Figure 4 shows the search result for a query com-
bining a statement (?obtain improvements?), a topic
?dependency parsing? and the publication year 2008.
As can be seen in the Results View, six papers
match these filters; sentences with semantically sim-
ilar predicates and passive voice are found, too.
4.1 Semantic Search
The main feature which distinguishes the ACL An-
thology Searchbench from other search applications
for scientific papers is the semantic search in paper
content. This enables the search for (semantic) state-
ments in the paper content as opposed to searching
for keywords in the plain text. Our use of the term
?statement? is loosely along the lines of the same
term used in logic. Very simple sentences often
bear a single statement only, while more complex
sentences (especially when having multiple clauses)
contain multiple statements. Each of the semantic
tuples extracted from the papers of the ACL Anthol-
ogy (cf. Section 3) corresponds to a statement.
The Statements filter is responsible for the seman-
tic search. Statements used in filters may be under-
specified, e.g., one may search for statements with a
certain semantic subject but with arbitrary semantic
predicates and objects. There are two ways in which
a new statement filter can be set: (1) entering a state-
ment manually; (2) clicking a sentence in the Doc-
ument Content View and choosing the statements of
this sentence that shall be set as new statement fil-
ters (cf. Figure 5), i.e. it is possible to formulate and
refine queries ?by example?.
Figure 5: Dialog for choosing statements to be used as
new filters (for sentence ?Our systems achieved good per-
formance on the SRL task, easily beating the baseline.?).
Throughout the user interface, no distinction is
made between the different kinds of semantic ob-
jects and adjuncts so as to make it easy also for
non-linguists to use the search and to be more ro-
bust against bad analyses of the parser. Therefore,
the different semantic parts of a statement are high-
lighted in three different colors only, depending on
whether a part is the semantic subject, the semantic
predicate or anything else (object/adjunct).
In order to disengage even further from the con-
crete wording and make the semantic search even
more ?meaning-based?, we additionally search for
synonyms of the semantic predicates in statement
filters. These synonyms have been computed as an
intersection of the most frequent verbs (semantic
predicates) in the anthology corpus with WordNet
synsets (Fellbaum, 1998), the main reason being re-
duction of the number of meanings irrelevant for the
domain. This relatively simple approach could of
course be improved, e.g. by active learning from
user clicks in search results etc.
5 Summary and Outlook
We have described the ACL Anthology Search-
bench, a novel search application for scientific dig-
ital libraries. The system is fully implemented and
indexes 7,500 papers of the 8,200 parsed ones. For
the other 700, bibliographic metadata was missing.
These and the remaining 10,000 papers are currently
being processed and will be added to the search in-
dex. The goal of the Searchbench is both to serve
as a showcase for benefits and improvement of NLP
for text search and at the same time provide a use-
ful tool for researchers in Computational Linguis-
tics. We believe that the tool by now already sup-
ports targeted search in a large collection of digital
research papers better than standard web search en-
gines. An evaluation comparing Searchbench query
results with web search is in progress.
Optionally, the Searchbench runs in a linguistic
debug mode providing NLP output a typical user
would not need. These analyses are accessible from
a context menu on each sentence (cf. Figure 4). Both
a tabular view of the semantic tuples of a sentence
(cf. Figure 3) and different kinds of information re-
lated to the parsing of the sentence (including the
MRS and a parse tree) can be displayed.
Future work, for which we are urgently seek-
ing funding, could include integration of further
12
NLP-based features such as coreference resolution
or question answering, as well as citation classifi-
cation and graphical navigation along the ideas in
Scha?fer and Kasterka (2010).
Acknowledgments
We are indebted to Peter Adolphs, Bart Cramer, Dan
Flickinger, Stephan Oepen, Yi Zhang for their sup-
port with ERG and PET extensions such as chart
mapping and chart pruning. Melanie Reiplinger,
Benjamin Weitz and Leonie Gro?n helped with pre-
processing. We also thank the anonymous review-
ers for their encouraging comments. The work de-
scribed in this paper has been carried out in the
context of the project TAKE (Technologies for Ad-
vanced Knowledge Extraction), funded under con-
tract 01IW08003 by the German Federal Ministry
of Education and Research, and in the context of the
world-wide DELPH-IN consortium.
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Daniel Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural lan-
guage parsing. In Proceedings of LREC-2008, pages
1380?1387, Marrakesh, Morocco.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
ACL anthology reference corpus: A reference dataset
for bibliographic research. In Proceedings of LREC-
2008, pages 1755?1759, Marrakesh, Morocco.
Torsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proc. of ANLP, pages 224?231, Seattle, WA.
Ulrich Callmeier. 2000. PET ? A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: an in-
troduction. Research on Language and Computation,
3(2?3):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI publications, Stanford.
Ann Copestake. 2009. Slacker semantics: why superfi-
ciality, dependency and avoidance of commitment can
be the right way to go. In Proc. of EACL, pages 1?9.
Bart Cramer and Yi Zhang. 2010. Constraining robust
constructions for broad-coverage parsing with preci-
sion grammars. In Proceedings of COLING-2010,
pages 223?231, Beijing, China.
Witold Droz?dz?yn?ski, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow
processing with unification and typed feature struc-
tures ? foundations and applications. Ku?nstliche In-
telligenz, 2004(1):17?23.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods: Syntacto-semantic annotation for
English Wikipedia. In Proceedings of LREC-2010,
pages 1665?1671.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Dan Flickinger,
Stephan Oepen, Hans Uszkoreit, and Jun?ichi Tsujii,
editors, Collaborative Language Engineering. A Case
Study in Efficient Grammar-based Processing, pages
1?17. CSLI Publications, Stanford, CA.
Katerina T. Frantzi, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 1998. The C-value/NC-value method of automatic
recognition for multi-word terms. In Proceedings of
ECDL, pages 585?604.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English. The Penn Treebank. Computational
Linguistics, 19:313?330.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
Kenjiro Taura, and Jun?ichi Tsujii. 2006. Fast and
scalable HPSG parsing. Traitement automatique des
langues (TAL), 46(2).
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In Proceedings of the ACL-2009 Workshop
on Natural Language Processing and Information Re-
trieval for Digital Libraries, Singapore.
Ulrich Scha?fer and Uwe Kasterka. 2010. Scientific
authoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL-HLT 2010
Workshop on Computational Linguistics and Writing,
pages 7?14, Los Angeles, CA.
Ulrich Scha?fer and Bernd Kiefer. 2011. Advances in
deep parsing of scholarly paper content. In Raffaella
Bernardi, Sally Chambers, Bjo?rn Gottfried, Fre?de?rique
Segond, and Ilya Zaihrayeu, editors, Advanced Lan-
guage Technologies for Digital Libraries, LNCS Hot
Topics Series. Springer. to appear.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional NLP markup. In Pro-
ceedings of the EACL-2006 Workshop on Multi-
dimensional Markup in Natural Language Processing,
pages 81?84, Trento, Italy.
13
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 7?14,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Scientific Authoring Support: A Tool to Navigate in Typed Citation Graphs
Ulrich Scha?fer
Language Technology Lab
German Research Center for
Artificial Intelligence (DFKI)
D-66123 Saarbru?cken, Germany
ulrich.schaefer@dfki.de
Uwe Kasterka
Computer Science Department
Saarland University
Campus
D-66123 Saarbru?cken, Germany
uwe.kasterka@dfki.de
Abstract
Scientific authors urgently need help in man-
aging the fast increasing number of publica-
tions. We describe and demonstrate a tool
that supports authors in browsing graphically
through electronically available publications,
thus allowing them to quickly adapt to new
domains and publish faster. Navigation is as-
sisted by means of typed citation graphs, i.e.
we use methods and resources from compu-
tational linguistics to compute the kind of ci-
tation that is made from one paper to another
(refutation, use, confirmation etc.). To verify
the computed citation type, the user can in-
spect the highlighted citation sentence in the
original PDF document. While our classi-
fication methods used to generate a realistic
test data set are relatively simple and could
be combined with other proposed approaches,
we put a strong focus on usability and quick
navigation in the potentially huge graphs. In
the outlook, we argue that our tool could be
made part of a community approach to over-
come the sparseness and correctness dilemma
in citation classification.
1 Introduction and Motivation
According to different studies, the number of scien-
tific works is doubled every 5-10 years. Important
issues to be addressed by the scientific community
are finding relevant information and avoiding redun-
dancy and duplication of work. The organization
and preservation of scientific knowledge in scientific
publications, vulgo text documents, thwarts these ef-
forts. From a viewpoint of a computer scientist, sci-
entific papers are just ?unstructured information?.
One specific, but very important aspect of the con-
tent of scientific papers is their relation to previous
work and, once published, their impact to subse-
quent or derived research. While it is still hard if
not impossible to capture and formalize the semantic
content of a scientific publication automatically, at
least citation properties and derived scientific impact
can be and usually are measured automatically on
the basis of simple citation graphs. In other words,
these graphs can be used to describe I/O behavior of
publications in a very simple way.
However, just counting citations is a very coarse
approach and does not tell much about the reasons
for citing one?s work in a specific situation. More-
over, once such measure is formalized and standard-
ized e.g. for science evaluation, it can be exploited
to tune up statistics. Since the first proposal of the
Science Citation Index (Garfield, 1955), it has also
provoked criticism.
In the bibliometrics and computational linguistics
literature, many proposals are available on how ci-
tations could be further classified by careful analy-
sis of citation sentences and context (Garfield, 1965;
Garzone, 1996; Mercer and Di Marco, 2004; Teufel
et al, 2006; Bornmann and Daniel, 2008).
The number of different classes proposed varies
from 3 to 35. Different authors try to identify di-
mensions and mutually exclusive classes, but the
more classes a schema contains, the more difficult
becomes the automatic classification.
The focus of our paper is to combine automatic
classification approaches with a tool that supports
scientists in graphically navigating through typed ci-
tation graphs (TCG). Such TCGs can be generated
7
by augmenting a simple citation graph with informa-
tion synonymously called citation function (Teufel
et al, 2006), citation relation (Mercer and Di Marco,
2004) or citation sentiment, forming the labels of the
graph?s edges. In the following, we use the more
neutral and general term citation type.
The idea is to help scientists, especially those not
so familiar with an area, understanding the relations
between publications and quickly get an overview of
the field. Moreover, the goal is to embed this tool in
a larger framework for scientists that also supports
semantic search assisted by domain ontologies and
further tools for authoring support (Scha?fer et al,
2008).
Our paper is structured as follows. In Section 2,
we describe how we automatically compute the
typed citation graph from the raw text content of a
scientific paper corpus to generate realistic data for
testing the visualization and navigation tool. Sec-
tion 3 contains an evaluation of the quality of the
extracted unlabeled graphs and of the citation classi-
fication step. We then describe in Section 4 the ideas
of efficient and at the same time well-arranged visu-
alization and navigation in the typed citation graph.
We compare with related work in Section 5. Finally,
we conclude and give an outlook to future work in
Section 6.
2 Data Preparation and Automatic
Citation Type Classification
Our corpus is based on 6300 electronically-available
papers, a subset (published 2002-2008) of the ACL
Anthology (Bird et al, 2008), a comprehensive col-
lection of scientific conference and workshop papers
in the area of computational linguistics and language
technology.
The overall workflow of the employed tools and
data is shown in Fig. 1.
We ran the open source tool ParsCit (Councill et
al., 2008) to extract references lists and correspond-
ing citation sentences from raw paper texts. To build
the citation graph, we used the Levenshtein distance
(Levenshtein, 1966) to find and match titles and au-
thors of identical papers yet tolerating spelling and
PDF extraction errors.
To increase robustness, publication years were
not considered as they would hinder matches for
Figure 1: Workflow from ACL Anthology data (top)
to citation graph navigation applet and citation sentence
viewer (bottom)
delayed journal publications. Generation of the
graph edges, i.e. matching of papers and reference
strings, is performed by means of the ACL ID, a
unique identifier for each paper, available for the
PDF (source nodes of references) and BibTeX files
(targets of references).
We evaluated the generated graph against the one
that was corrected manually by the ACL Anthol-
ogy Network (AAN) group (Radev et al, 2009) and
found that 10821 citation links were shared between
both and can be considered correct1.
3883 additional ones were in the AAN but not rec-
ognized by us, the other way round, 1021 discovered
by us were not in the AAN. In addition, the publica-
tion bases were not identical. The anthology net-
work data ends in February 2007 but covers years
before 2002, while our data covers 2002-2008 in-
clusively. Given the fact that our graph is computed
fully automatically, the result can be considered very
good.
In the next step, we augmented the citation graph
by types for each edge. In contrast to other ap-
proaches, we currently only consider the citation
sentence itself to determine the citation type, neither
a wider context, its position nor the abstract, title or
content of the cited paper. A reference (from the
references section at the end of a paper) may be as-
sociated with several citation sentences mentioning
the paper referenced at the end.
1We only consider intra-network links here, not those point-
ing to books or other publications outside the corpus.
8
In only considering the citation sentence itself, we
may lose some citation type information, as it may
be (also) contained in follow-up sentences referring
to the citation using a pronoun (?they?, ?their ap-
proach? etc.). Considering follow-up or even pre-
ceding sentences is planned to be addressed in future
work.
After consulting the rich literature on citation
classification (Bornmann and Daniel, 2008; Gar-
zone, 1996; Teufel et al, 2006), we derived a simpli-
fied classification schema consisting of the follow-
ing five classes.
? Agree: The citing paper agrees with the cited
paper
? PRecycle: The citing paper uses an algorithm,
data, method or tool from the cited paper
? Negative: The paper is cited nega-
tively/contrastively
? Neutral: The paper is cited neutrally
? Undef: impossible determine the sentiment of
the citation (fallback)
Then, we used a blend of methods to collect ver-
bal and non-verbal patterns (cue words) and asso-
ciated each with a class from the aforementioned
schema.
? A list from (Garzone, 1996) devised for
biomedical texts; it is largely applicable to the
computational linguistics domain as well.
? Simple negation of positive cue words to obtain
negative patterns.
? A list of automatically extracted synonyms and
antonyms (the latter for increasing number of
patterns for negative citations) from WordNet
(Miller et al, 1993).
? Automatically computed most frequent cooc-
currences from all extracted citation sentences
of the corpus using an open source cooccur-
rence tool (Banerjee and Pedersen, 2003).
? Inspection: browse and filter cue words manu-
ally, remove redundancies.
3 Results: Distribution and Evaluation
These pattern where then used for the classification
algorithm and applied to the extracted citation sen-
tences. In case of multiple citations with different
classes, a voting mechanism was applied were the
?stronger? classes (Agree, Negative, PRecycle) won
in standoff cases. For the total of 91419 citations we
obtained the results shown in Table 1.
Classes Citations Percent
Agree 3513 3.8%
Agree, Neutral 2020 2.2%
Negative 1147 1.2%
PRecycle 10609 11.6%
PRecycle, Agree 1419 1.6%
PRecycle, Agree, Neutral 922 1.0%
PRecycle, Neutral 3882 4.2%
Neutral 13430 14.7%
Undef 54837 60.0%
Table 1: Citation classification result
The numbers reflect a careful classification ap-
proach where uncertain citations are classified as
Undef. In case of multiple matches, the first (left-
most) was taken to achieve a unique result.
The results also confirm obervations made in
other works: (1) citation classification is a hard task,
(2) there are only a few strongly negative citations
which coincides with observations made by (Teufel
et al, 2006), (Pendlebury, 2009) and others, (3) the
majority of citations is neutral or of unknown type.
An evaluation on a test set of 100 citations spread
across all the types of papers with a manual check
of the accuracy of the computed labels showed an
overall accuracy of 30% mainly caused by the fact
that 90% of undefined hits were in fact neutral
(i.e., labeling all undefs neutral would increase ac-
curacy). Negative citations are sparse and unreliable
(33%), neutral ones are about 60% accurate, PRecy-
cle: 33%, Agree: 25%.
To sum up, our automatic classification approach
based on only local citation information could surely
be improved by applying methods described in the
literature, but it helped us to quickly (without an-
notation effort) generate a plausible data set for the
main task, visualization and navigation in the typed
citation graphs.
9
Figure 2: Typed citation graph navigator applet
4 Visualization Algorithm and Navigation
User Interface
The overall idea of the citation visualization and
navigation tool is simple and intuitive. Each paper is
represented by a node, all citations between papers
are drawn as edges between nodes where the color
of the edge indicates the computed (overall) citation
type, e.g. green for agree, red for negative, blue for
recycle and black for neutral or undefined.
To cope with flexible layouts and scalability of
the graph, we decided to use the open source tool
Java Universal Network/Graph Framework (JUNG,
http://jung.sourceforge.net). Its main advantages
over similar tools are that it supports user interaction
(clicking on nodes and edges, tool tips) and user-
implemented graph layout algorithms. A screenshot
of the final user interface is presented in Figure 2.
The decision for and development of the visual-
ization and navigation tool was mainly driven by the
fact that citation graphs quickly grow and become
unmanagable by humans when extended to the tran-
sitive closures of citing or cited papers of a given
publication. The sheer number of crossing edges
would make the display unreadable.
Figure 3: Focused paper in the center
The main design goal therefore was reducing the
number of nodes and edges where possible and (by
default) have only one paper in focus (Fig. 3), with
10
all cited papers on the left side (Fig. 4), and all citing
papers on the right (Fig. 5).
This also reflects a time line order where the ori-
gin (oldest papers) is on the left. In the graphical
user interface, the citation depth (default 1) is ad-
justable by a slider to higher numbers. The graph
display is updated upon change of the configured
depth.
Figure 4: Papers cited by the focused paper
At level 1, papers citing the citing papers (anal-
ogously for cited papers), are not fully drawn as
nodes, but only adumbrated by short ingoing or out-
going edges (arrows). However, the color of these
short edges still signifies the citation type and may
attract interest which can easily be satisfied by click-
ing on the edge?s remaining node (cf. screenshot in
Figure 2). When the mouse is moved over a node,
a tooltip text display pops up displaying full author
list and paper title.
Figure 5: Papers citing the focused paper
To avoid crossing edges caused by citations at
the same level (citing or cited papers also cite
each other), we devised a fan-out layout generation
(Fig. 6). It increases the width of the displayed
graph, but leads to better readability. Fan-out lay-
out can also be switched off in the user interface.
Figure 6: Fan-out layout: avoid crossing edges caused by
citations on the same level
In addition, the graph layout algorithm orders pa-
pers chronologically in the vertical direction. Here,
we have implemented another technique that helps
to avoid crossing edges. As shown in Fig. 7, we
sort papers vertically by also taking into account the
position of its predecessor, the cited paper. It often
leads to less crossing edges.
Figure 7: Order: avoid crossing edges by ordering
chronologically (strict, simple variant on the left for com-
parison), taking into account the position of the cited pa-
per on the previous level (right)
11
By double-clicking on any node representing a
paper (cited or citing), this node can be made the
new center and the graph is re-arranged accordingly.
Zooming in and out is possible via mouse wheel
or shortcut buttons (?overview?, ?center?).
Using the right mouse button context menu on a
node, it is possible to open a details page for the
selected paper with bibliographic metadata and all
citations and types. All references in the document
with their citation sentences identified are displayed
in a structured list.
The citation context around a citation sentence
is shown as well, while the citation sentence itself
is colored according to the citation type color and
clickable. If clicked, the original PDF document
opens with the citation sentence highlighted (Fig. 8;
currently only possible in Acrobat Reader).
By clicking on an edge instead of a node, only the
citations between the two papers at both ends are
displayed, in the same way as described above for
all citations of a document.
5 Related Work
Our paper touches and combines results of three
disciplines, (1) bibliometrics, (2) computational lin-
guistics, and (3) information visualization. We
briefly discuss related and mostly recent literature,
being aware of the fact that this list is necessarily
incomplete.
(Garfield, 1965) is probably the first to discuss
an automatic computation of citation types. He is
also the founder of citation indexing and the Insti-
tute of Scientific Information (ISI). His first publica-
tion on science citation indexing appeared in 1955
(Garfield, 1955) and he remained the probably most
influential scientist in this field for decades. (Born-
mann and Daniel, 2008) is a comprehensive recent
metastudy on citing behavior.
Investigating citation classification has a long tra-
dition in bibliometrics and information science and
in the last 20 years also attracted computational
linguistics researchers trying to automate the task
based on rhetorics of science, statistical methods and
sentence parsing.
There is much more work than we can cite here
on citation function computation worth combination
with our approach (Bornmann and Daniel, 2008;
Garzone, 1996; Teufel et al, 2006) ? using our tool
one can easily browse to further publications!
There is little work on innovative layout tech-
niques for displaying and navigating citation graphs.
We found three independent approaches to citation
graph visualization: CiteViz (Elmqvist and Tsigas,
2004), CircleView (Bergstro?m and Jr., 2006), and
(Nguyen et al, 2007). They share a disadvantageous
property in that they try to visualize too much infor-
mation at the same time. In our opinion, this con-
tradicts the need to navigate and keep control over
displayable parts of large paper collections.
Moreover, these approaches do not provide infor-
mation on citation types derived from text as our
system does. Further ideas on visualizing science-
related information such as author co-citation net-
works are also discussed and summarized in (Chen,
2006).
6 Summary and Outlook
We have presented an innovative tool to support sci-
entific authors in browsing graphically through large
collections of publications by means of typed cita-
tion graphs. To quickly generate a realistic data set,
we devised a classification approach avoiding man-
ual annotation and intervention.
Our classification results cannot compete with ap-
proaches such as (Teufel et al, 2006) based on con-
siderable manual annotation for machine learning.
However, we think that our application could be
combined with this or other approaches described
for classifying citations between scientific papers.
We envisage to integrate the navigation tool in
a larger framework supporting scientific authoring
(Scha?fer et al, 2008). When publishing a service of
this kind on the Web, one would be faced with ethi-
cal issues such as the problem that authors could feel
offended by wrongly classified citations.
The reason is that citation type classification is
potentially even more subjective than a bare citation
index?which itself is already highly controversal,
as discussed in the introduction. Moreover, there is
not always a single, unique citation type, but often
vagueness and room for interpretation.
Therefore, we suggest to augment such a service
by a Web 2.0 application that would allow regis-
tered users to confirm, alter and annotate precom-
12
Figure 8: Citation sentence viewer; citation sentence in context on the left, highlighted in PDF on the right when
selected on the left
puted citation classifications. In this community ap-
plication, all citation links in the automatically gen-
erated graph could be represented by dashed arrows
initially, and users could turn them solid by confirm-
ing or correcting the citation type and also adding a
comment text.
Line thickness could be increased (up to an appro-
priate maximum) each time another user confirms a
classified citation type. The results could then also
be employed for active learning and help to improve
the automatic classification procedure.
Acknowledgments
First of all, we are indebted to the anonymous re-
viewers for their useful, encouraging and detailed
comments. Many thanks also to Donia Scott for her
feedback on an earlier version of the tool and helpful
comments on terminology. We would like to thank
Madeline Maher and Boris Fersing for generating
and evaluating the citation type data on a subcorpus
of the ACL Anthology.
The work described in this paper has been carried
out in the context of the project TAKE (Technolo-
gies for Advanced Knowledge Extraction), funded
under contract 01IW08003 by the German Federal
Ministry of Education and Research.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the ngram statistics
package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 370?381, Mexico City.
Peter Bergstro?m and E. James Whitehead Jr. 2006. Cir-
cleView: Scalable visualization and navigation of cita-
tion networks. In Proceedings of the 2006 Symposium
on Interactive Visual Information Collections and Ac-
tivity IVICA, College Station, Texas.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
ACL anthology reference corpus: A reference dataset
for bibliographic research. In Proceedings of the Lan-
guage Resources and Evaluation Conference (LREC-
2008), Marrakesh, Morocco.
Lutz Bornmann and Hans-Dieter Daniel. 2008. What
do citation counts measure? A review of studies on
13
citing behavior. Journal of Documentation, 64(1):45?
80. DOI 10.1108/00220410810844150.
Chaomei Chen. 2006. Information Visualization: Be-
yond the Horizon. Springer. 2nd Edition, Chapter 5.
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
ParsCit: An open-source CRF reference string parsing
package. In Proceedings of the Language Resources
and Evaluation Conference (LREC-2008), Marrakesh,
Morocco.
Niklas Elmqvist and Philippas Tsigas. 2004. CiteWiz:
A tool for the visualization of scientific citation net-
works. Technical Report CS:2004-05, Department of
Computing Science, Chalmers University of Technol-
ogy and Go?teborg University, Go?teborg, Sweden.
Eugene Garfield. 1955. Citation indexes for science: A
new dimension in documentation through association
of ideas. Science, 123:108?111.
Eugene Garfield. 1965. Can citation indexing be auto-
mated? In Mary Elizabeth Stevens, Vincent E. Giu-
liano, and Laurence B. Heilprin, editors, Statistical
Association Methods for Mechanical Documentation.
National Bureau of Standards, Washington, DC. NBS
Misc. Pub. 269.
Mark Garzone. 1996. Automated classification of ci-
tations using linguistic semantic grammars. Master?s
thesis, Dept. of Computer Science, The University of
Western Ontario, Canada.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Robert. E Mercer and Chrysanne Di Marco. 2004. A
design methodology for a biomedical literature index-
ing tool using the rhetoric of science. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking Bi-
ological Literature, Ontologies and Databases, pages
77?84, Boston, Massachusetts, USA.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1993.
Five papers on WordNet. Technical report, Cognitive
Science Laboratory, Princeton University.
Quang Vinh Nguyen, Mao Lin Huang, and Simeon
Simoff. 2007. Visualization of relational structure
among scientific articles. Advances in Visual Informa-
tion Systems, pages 415?425. Springer LNCS 4781,
DOI 10.1007/978-3-540-76414-4 40.
David A. Pendlebury. 2009. The use and misuse of jour-
nal metrics and other citation indicators. Archivum Im-
munologiae et Therapiae Experimentalis, 57(1):1?11.
DOI 10.1007/s00005-009-0008-y.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL anthology network cor-
pus. In Proceedings of the ACL Workshop on Nat-
ural Language Processing and Information Retrieval
for Digital Libraries, Singapore.
Ulrich Scha?fer, Hans Uszkoreit, Christian Federmann,
Torsten Marek, and Yajing Zhang. 2008. Extract-
ing and querying relations in scientific papers. In
Proceedings of the 31st Annual German Conference
on Artificial Intelligence, KI 2008, pages 127?134,
Kaiserslautern, Germany. Springer LNAI 5243. DOI
10.1007/978-3-540-85845-4 16.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 103?
110, Sydney, Australia.
14
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 55?65,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Extracting glossary sentences from scholarly articles:
A comparative evaluation of pattern bootstrapping and deep analysis
Melanie Reiplinger1 Ulrich Scha?fer2 Magdalena Wolska1?
1Computational Linguistics, Saarland University, D-66041 Saarbru?cken, Germany
2DFKI Language Technology Lab, Campus D 3 1, D-66123 Saarbru?cken, Germany
{mreiplin,magda}@coli.uni-saarland.de, ulrich.schaefer@dfki.de
Abstract
The paper reports on a comparative study of
two approaches to extracting definitional sen-
tences from a corpus of scholarly discourse:
one based on bootstrapping lexico-syntactic
patterns and another based on deep analysis.
Computational Linguistics was used as the tar-
get domain and the ACL Anthology as the
corpus. Definitional sentences extracted for a
set of well-defined concepts were rated by do-
main experts. Results show that both meth-
ods extract high-quality definition sentences
intended for automated glossary construction.
1 Introduction
Specialized glossaries serve two functions: Firstly,
they are linguistic resources summarizing the ter-
minological basis of a specialized domain. Sec-
ondly, they are knowledge resources, in that they
provide definitions of concepts which the terms de-
note. Glossaries find obvious use as sources of ref-
erence. A survey on the use of lexicographical aids
in specialized translation showed that glossaries are
among the top five resources used (Dura?n-Mun?oz,
2010). Glossaries have also been shown to facil-
itate reception of texts and acquisition of knowl-
edge during study (Weiten et al, 1999), while self-
explanation of reasoning by referring to definitions
has been shown to promote understanding (Aleven
et al, 1999). From a machine-processing point of
view, glossaries may be used as input for domain
ontology induction; see, e.g. (Bozzato et al, 2008).
?Corresponding author
The process of glossary creation is inherently de-
pendent on expert knowledge of the given domain,
its concepts and language. In case of scientific do-
mains, which constantly evolve, glossaries need to
be regularly maintained: updated and continually
extended. Manual creation of specialized glossaries
is therefore costly. As an alternative, fully- and
semi-automatic methods of glossary creation have
been proposed (see Section 2).
This paper compares two approaches to corpus-
based extraction of definitional sentences intended
to serve as input for a specialized glossary of a scien-
tific domain. The bootstrapping approach acquires
lexico-syntactic patterns characteristic of definitions
from a corpus of scholarly discourse. The deep ap-
proach uses syntactic and semantic processing to
build structured representations of sentences based
on which ?is-a?-type definitions are extracted. In
the present study we used Computational Linguis-
tics (CL) as the target domain of interest and the
ACL Anthology as the corpus.
Computational Linguistics, as a specialized do-
main, is rich in technical terminology. As a cross-
disciplinary domain at the intersection of linguistics,
computer science, artificial intelligence, and mathe-
matics, it is interesting as far as glossary creation
is concerned in that its scholarly discourse ranges
from descriptive informal to formal, including math-
ematical notation. Consider the following two de-
scriptions of Probabilistic Context-Free Grammar
(PCFG):
(1) A PCFG is a CFG in which each production
A ? ? in the grammar?s set of productions
R is associated with an emission probabil-
55
ity P (A ? ?) that satisfies a normalization
constraint
?
?:A???R
P (A? ?) = 1
and a consistency or tightness constraint [...]
(2) A PCFG defines the probability of a string
of words as the sum of the probabilities of
all admissible phrase structure parses (trees)
for that string.
While (1) is an example of a genus-differentia
definition, (2) is a valid description of PCFG which
neither has the typical copula structure of an ?is-a?-
type definition, nor does it contain the level of de-
tail of the former. (2) is, however, well-usable for a
glossary. The bootstrapping approach extracts defi-
nitions of both types. Thus, at the subsequent glos-
sary creation stage, alternative entries can be used to
generate glossaries of different granularity and for-
mal detail; e.g., targeting different user groups.
Outline. Section 2 gives an overview of related
work. Section 3 presents the corpora and the prepro-
cessing steps. The bootstrapping procedure is sum-
marized in Section 4 and deep analysis in Section 5.
Section 6 presents the evaluation methodology and
the results. Section 7 presents an outlook.
2 Related Work
Most of the existing definition extraction methods
? be it targeting definitional question answering or
glossary creation ? are based on mining part-of-
speech (POS) and/or lexical patterns typical of def-
initional contexts. Patterns are then filtered heuris-
tically or using machine learning based on features
which refer to the contexts? syntax, lexical content,
punctuation, layout, position in discourse, etc.
DEFINDER (Muresan and Klavans, 2002), a rule-
based system, mines definitions from online medical
articles in lay language by extracting sentences us-
ing cue-phrases, such as ?x is the term for y?, ?x
is defined as y?, and punctuation, e.g., hyphens and
brackets. The results are analyzed with a statistical
parser. Fahmi and Bouma (2006) train supervised
learners to classify concept definitions from medi-
cal pages of the Dutch Wikipedia using the ?is a?
pattern and apply a lexical filter (stopwords) to the
classifier?s output. Besides other features, the posi-
tion of a sentence within a document is used, which,
due to the encyclopaedic text character of the cor-
pus, allows to set the baseline precision at above
75% by classifying the first sentences as definitions.
Westerhout and Monachesi (2008) use a complex set
of grammar rules over POS, syntactic chunks, and
entire definitory contexts to extract definition sen-
tences from an eLearning corpus. Machine learn-
ing is used to filter out incorrect candidates. Gaudio
and Branco (2009) use only POS information in a
supervised-learning approach, pointing out that lex-
ical and syntactic features are domain and language
dependent. Borg et al (2009) use genetic program-
ming to learn rules for typical linguistic forms of
definition sentences in an eLearning corpus and ge-
netic algorithms to assign weights to the rules. Ve-
lardi et al (2008) present a fully-automatic end-to-
end methodology of glossary creation. First, Term-
Extractor acquires domain terminology and Gloss-
Extractor searches for definitions on the web using
google queries constructed from a set of manually
lexical definitional patterns. Then, the search results
are filtered using POS and chunk information as well
as term distribution properties of the domain of in-
terest. Filtered results are presented to humans for
manual validation upon which the system updates
the glossary. The entire process is automated.
Bootstrapping as a method of linguistic pattern
induction was used for learning hyponymy/is-a re-
lations already in the early 90s by Hearst (1992).
Various variants of the procedure ? for instance, ex-
ploiting POS information, double pattern-anchors,
semantic information ? have been recently pro-
posed (Etzioni et al, 2005; Pantel and Pennacchiotti,
2006; Girju et al, 2006; Walter, 2008; Kozareva et
al., 2008; Wolska et al, 2011). The method pre-
sented here is most similar to Hearst?s, however, we
acquire a large set of general patterns over POS tags
alone which we subsequently optimize on a small
manually annotated corpus subset by lexicalizing the
verb classes.
3 The Corpora and Preprocesssing
The corpora. Three corpora were used in this
study. At the initial stage two development corpora
were used: a digitalized early draft of the Jurafsky-
56
Martin textbook (Jurafsky and Martin, 2000) and the
WeScience Corpus, a set of Wikipedia articles in the
domain of Natural Language Processing (Ytrest?l et
al., 2009).1 The former served as a source of seed
domain terms with definitions, while the latter was
used for seed pattern creation.
For acquisition of definitional patterns and pat-
tern refinement we used the ACL Anthology, a dig-
ital archive of scientific papers from conferences,
workshops, and journals on Computational Linguis-
tics and Language Technology (Bird et al, 2008).2
The corpus consisted of 18,653 papers published be-
tween 1965 and 2011, with a total of 66,789,624
tokens and 3,288,073 sentences. This corpus was
also used to extract sentences for the evaluation us-
ing both extraction methods.
Preprocessing. The corpora have been sentence
and word-tokenized using regular expression-based
sentence boundary detection and tokenization tools.
Sentences have been part-of-speech tagged using the
TnT tagger (Brants, 2000) trained on the Penn Tree-
bank (Marcus et al, 1993).3
Next, domain terms were identified using the C-
Value approach (Frantzi et al, 1998). C-Value is
a domain-independent method of automatic multi-
word term recognition that rewards high frequency
and high-order n-gram candidates, but penalizes
those which frequently occur as sub-strings of an-
other candidate. 10,000 top-ranking multi-word to-
ken sequences, according to C-Value, were used.
Domain terms. The set of domain terms was com-
piled from the following sub-sets: 1) the 10,000 au-
tomatically identified multi-word terms, 2) the set
of terms appearing on the margins of the Jurafsky-
Martin textbook; the intuition being that these are
domain-specific terms which are likely to be defined
or explained in the text along which they appear,
3) a set of 5,000 terms obtained by expanding fre-
quent abbreviations and acronyms retrieved from the
ACL Anthology corpus using simple pattern match-
ing. The token spans of domain terms have been
marked in the corpora as these are used in the course
of definition pattern acquisition (Section 4.2).
1http://moin.delph-in.net/WeScience
2http://aclweb.org/anthology/
3The accuracy of tokenization and tagging was not verified.
S
ee
d
te
rm
s
machine translation language model
neural network reference resolution
finite(-| )state automaton hidden markov model
speech synthesis semantic role label(l)?ing
context(-| )free grammar ontology
generative grammar dynamic programming
mutual information
S
ee
d
pa
tt
er
ns
T .* (is|are|can be) used
T .* called
T .* (is|are) composed
T .* involv(es|ed|e|ing)
T .* perform(s|ed|ing)?
T \( or .*? \)
task of .* T .*? is
term T .*? refer(s|red|ring)?
Table 1: Seed domain terms (top) and seed patterns (bot-
tom) used for bootstrapping; T stands for a domain term.
4 Bootstrapping Definition Patterns
Bootstrapping-based extraction of definitional sen-
tences proceeds in two stages: First, aiming at recall,
a large set of lexico-syntactic patterns is acquired:
Starting with a small set of seed terms and patterns,
term and pattern ?pools? are iteratively augmented
by searching for matching sentences from the ACL
Anthology while acquiring candidates for definition
terms and patterns. Second, aiming at precision,
general patterns acquired at the first stage are sys-
tematically optimized on set of annotated extracted
definitions.
4.1 Seed Terms and Seed Patterns
As seed terms to initialize pattern acquisition, we
chose terms which are likely to have definitions.
Specifically, from the top-ranked multi-word terms,
ordered by C-value, we selected those which were
also in either the Jurafsky-Martin term list or the list
of expanded frequent abbreviations. The resulting
13 seed terms are shown in the top part of Table 1.
Seed definition patterns were created by inspect-
ing definitional contexts in the Jurafsky-Martin and
WeScience corpora. First, 12 terms from Jurafsky-
Martin and WeScience were selected to find domain
terms with which they co-occurred in simple ?is-a?
patterns. Next, the corpora were searched again to
find sentences in which the term pairs in ?is-a? rela-
tion occur. Non-definition sentences were discarded.
57
Finally, based on the resulting definition sentences,
22 seed patterns were constructed by transforming
the definition phrasings into regular expressions. A
subset of the seed phrases extracted in this way is
shown in the bottom part of Table 1.4
4.2 Acquiring Patterns
Pattern acquisition proceeds in two stages: First,
based on seed sets, candidate defining terms are
found and ranked. Then, new patterns are acquired
by instantiating existing patterns with pairs of likely
co-occurring domain terms, searching for sentences
in which the term pairs co-occur, and creating POS-
based patterns. These steps are summarized below.
Finding definiens candidates. Starting with a set
of seed terms and a set of definition phrases, the first
stage finds sentences with the seed terms in the T-
placeholder position of the seed phrases. For each
term, the set of extracted sentences is searched for
candidate defining terms (other domain terms in the
sentence) to form term-term pairs which, at the sec-
ond stage, will be used to acquire new patterns.
Two situations can occur: a sentence may con-
tain more than one domain term (other than one of
the seed terms) or the same domain terms may be
found to co-occur with multiple seed terms. There-
fore, term-term pairs are ranked.
Ranking. Term-term pairs are ranked using four
standard measures of association strength: 1) co-
occurrence count, 2) pointwise mutual information
(PMI), 3) refined PMI; compensates bias toward
low-frequency events by multiplying PMI with co-
occurrence count (Manning and Schu?tze, 1999), and
4) mutual dependency (MD); compensates bias to-
ward rare events by subtracting co-occurrence?s self-
information (entropy) from its PMI (Thanopoulos et
al., 2002). The measures are calculated based on the
corpus for co-occurrences within a 15-word window.
Based on experimentation, mutual dependency
was found to produce the best results and therefore it
was used in ranking definiens candidates in the final
evaluation of patterns. The top-k candidates make
up the set of defining terms to be used in the pattern
acquisition stage. Table 2 shows the top-20 candi-
4Here and further in the paper, regular expressions are pre-
sented in Perl notation.
Domain term Candidate defining terms
lexical functional phrase structure grammar
grammar (LFG) formal system
functional unification grammar
grammatical representation
phrase structure
generalized phrase
functional unification
binding theory
syntactic theories
functional structure
grammar formalism(s)
grammars
linguistic theor(y|ies)
Table 2: Candidate defining phrases of the term ?Lexical
Functional Grammar (LFG)?.
date defining terms for the term ?Lexical Functional
Grammar?, according to mutual dependency.
Pattern and domain term acquisition. At the
pattern acquisition stage, definition patterns are re-
trieved by 1) coupling terms with their definiens can-
didates, 2) extracting sentences that contain the pair
within the specified window of words, and finally
3) creating POS-based patterns corresponding to the
extracted sentences. All co-occurrences of each
term together with each of its defining terms within
the fixed window size are extracted from the POS-
tagged corpus. At each iteration also new definien-
dum and definiens terms are found by applying the
new abstracted patterns to the corpus and retrieving
the matching domain terms.
The newly extracted sentences and terms are fil-
tered (see ?Filtering? below). The remaining data
constitute new instances for further iterations. The
linguistic material between the two terms in the ex-
tracted sentences is taken to be an instantiation of a
potential definition pattern for which its POS pattern
is created as follows:
? The defined and defining terms are replaced by
placeholders, T and DEF,
? All the material outside the T and DEF anchors
is removed; i.e. the resulting patterns have the
form ?T ... DEF? or ?DEF ... T?
? Assuming that the fundamental elements of a
definition pattern, are verbs and noun phrases,
58
all tags except verb, noun, modal and the in-
finitive marker ?to? are replaced with by place-
holders denoting any string; punctuation is pre-
served, as it has been observed to be infor-
mative in detecting definitions (Westerhout and
Monachesi, 2008; Fahmi and Bouma, 2006),
? Sequences of singular and plural nouns and
proper nouns are replaced with noun phrase
placeholder, NP; it is expanded to match com-
plex noun phrases when applying the patterns
to extract definition sentences.
The new patterns and terms are then fed as input
to the acquisition process to extract more sentences
and again abstract new patterns.
Filtering. In the course of pattern acquisition in-
formation on term-pattern co-occurrence frequen-
cies is stored and relative frequencies are calculated:
1) for each term, the percentage of seed patterns it
occurs with, and 2) for each pattern, the percentage
of seed terms it occurs with. These are used in the
bootstrapping cycles to filter out terms which do not
occur as part of a sufficient number of patterns (pos-
sibly false positive definiendum candidates) and pat-
terns which do not occur with sufficient number of
terms (insufficient generalizing behavior).
Moreover, the following filtering rules are ap-
plied: Abstracted POS-pattern sequences of the
form ?T .+ DEF?5 and ?DEF T? are discarded;
the former because it is not informative, the latter
because it is rather an indicator of compound nouns
than of definitions. From the extracted sentences,
those containing negation are filtered out; negation
is contra-indicative of definition (Pearson, 1996).
For the same reason, auxiliary constructions with
?do? and ?have? are excluded unless, in case of the
latter, ?have? is followed by a two past participle
tags as in, e.g., ?has been/VBN defined/VBN (as).?
4.3 Manual Refinement
While the goal of the bootstrapping stage was to find
as many candidate patterns for good definition terms
as possible, the purpose of the refinement stage is to
aim at precision. Since the automatically extracted
patterns consist only of verb and noun phrase tags
5?.+? stands for at least one arbitrary character.
# Definitions # Non-definitions
25 is/VBZ 24 is/VBZ
8 represents/VBZ 14 contains/VBZ
6 provides/VBZ 9 employed/VBD
6 contains/VBZ 6 includes/VBZ
6 consists/VBZ 4 reflects/VBZ
3 serves/VBZ 3 uses/VBZ
3 describes/VBZ 3 typed/VBN
3 constitutes/VBZ 3 provides/VBZ
3 are/VBP 3 learning/VBG
Table 3: Subset of verbs occurring in sentences matched
by the most frequently extracted patterns.
between the definiendum and its defining term an-
chors, they are too general.
In order to create more precise patterns, we tuned
the pattern sequences based on a small development
sub-corpus of the extracted sentences which we an-
notated. The development corpus was created by ex-
tracting sentences using the most frequent patterns
instantiated with the term which occurred with the
highest percentage of seed patterns. The term ?on-
tology? appeared with more than 80% of the patterns
and was used for this purpose. The sentences were
then manually annotated as to whether they are true-
positive or false examples of definitions (101 and
163 sentences, respectively).
Pattern tuning was done by investigating which
verbs are and which are not indicative of defini-
tions based on the positive and negative example
sentences. Table 3 shows the frequency distribu-
tion of verbs (or verb sequences) in the annotated
corpus which occurred more than twice. Abstract-
ing over POS sequences of the sentences contain-
ing definition-indicative verbs, we created 13 pat-
terns, extending the automatically found patterns,
that yielded 65% precision on the development set,
matching 51% of the definition sentences, and re-
ducing noise to 17% non-definitions. Patterns re-
sulting from verb tuning were used in the evaluation.
Examples of the tuned patterns are shown below:
T VBZ DT JJ? NP .* DEF
T , NP VBZ IN NP .* DEF
T , .+ VBZ DT .+ NP .* DEF
T VBZ DT JJ? NP .* DEF
The first pattern matches our both introductory
59
example definitions of the term ?PCFG? (cf. Sec-
tion 1) with ?T? as a placeholder for the term it-
self, ?NP? denoting a noun phrase, and ?DEF? one
of the term?s defining phrases, in the first case, (1),
?grammar?, in the second case, (2), ?probabilities?.
The examples annotated with matched pattern ele-
ments are shown below:6
[PCFG]T [is]VBZ [a]DT [CFG]NP [in which each
production A ? ? in the].? [grammar]DEF ?s
set of productionsR is associated with an emis-
sion probability . . .
A [PCFG]T [defines]VBZ [the]DT
[probability]DEF of a string of words as
the sum of the probabilities of all admissible
phrase structure parses (trees) for that string.
5 Deep Analysis for Definition Extraction
An alternative, largely domain-independent ap-
proach to the extraction of definition sentences is
based on the sentence-semantic index generation of
the ACL Anthology Searchbench (Scha?fer et al,
2011).
Deep syntactic parsing with semantic predicate-
argument structure extraction of each of the approx.
3.3 million sentences in the 18,653 papers ACL An-
thology corpus is used for our experiments. We
briefly describe how in this approach we get from
the sentence text to the semantic representation.
The preprocessing is shared with the
bootstrapping-based approach for definition
sentence extraction, namely PDF-to-text extraction,
sentence boundary detection (SBR), and trigram-
based POS tagging with TnT (Brants, 2000). The
tagger output is combined with information from
a named entity recognizer that in addition delivers
hypothetical information on citation expressions.
The combined result is delivered as input to the
deep parser PET (Callmeier, 2000) running the open
source HPSG grammar (Pollard and Sag, 1994)
grammar for English (ERG; Flickinger (2002)).
The deep parser is made robust and fast through
a careful combination of several techniques; e.g.:
(1) chart pruning: directed search during parsing to
6Matching pattern elements in square brackets; tags from
the pattern subscripted.
increase performance and coverage for longer sen-
tences (Cramer and Zhang, 2010); (2) chart map-
ping: a framework for integrating preprocessing in-
formation from PoS tagger and named entity recog-
nizer in exactly the way the deep grammar expects it
(Adolphs et al, 2008)7; (3) a statistical parse rank-
ing model (WeScience; (Flickinger et al, 2010)).
The parser outputs sentence-semantic represen-
tation in the MRS format (Copestake et al, 2005)
that is transformed into a dependency-like vari-
ant (Copestake, 2009). From these DMRS represen-
tations, predicate-argument structures are derived.
These are indexed with structure (semantic subject,
predicate, direct object, indirect object, adjuncts) us-
ing a customized Apache Solr8 server. Matching
of arguments is left to Solr?s standard analyzer for
English with stemming; exact matches are ranked
higher than partial matches.
The basic semantics extraction algorithm consists
of the following steps: 1) calculate the closure for
each (D)MRS elementary predication based on the
EQ (variable equivalence) relation and group the
predicates and entities in each closure respectively;
2) extract the relations of the groups, which results in
a graph as a whole; 3) recursively traverse the graph,
form one semantic tuple for each predicate, and fill
information under its scope, i.e. subject, object, etc.
The semantic structure extraction algorithm gen-
erates multiple predicate-argument structures for
coordinated sentence (sub-)structures in the in-
dex. Moreover, explicit negation is recognized and
negated sentences are excluded for the task for the
same reasons as in the bootstrapping approach above
(see Section 4.2, ?Filtering?).
Further details of the deep parsing approach are
described in (Scha?fer and Kiefer, 2011). In the
Searchbench online system9, the definition extrac-
tion can by tested with any domain term T by using
statement queries of the form ?s:T p:is?.
6 Evaluation
For evaluation, we selected 20 terms, shown in Ta-
ble 4, which can be considered domain terms in the
7PoS tagging, e.g., helps the deep parser to cope with words
unknown to the deep lexicon, for which default entries based on
the PoS information are generated on the fly.
8http://lucene.apache.org/solr
9http://aclasb.dfki.de
60
integer linear programming (ILP)
conditional random field (CRF)
support vector machine (SVM)
latent semantic analysis (LSA)
combinatory categorial grammar (CCG)
lexical-functional grammar (LFG)
probabilistic context-free grammar (PCFG)
discourse representation theory (DRT)
discourse representation structure (DRS)
phrase-based machine translation (PSMT;PBSMT)
statistical machine translation (SMT)
multi-document summarization (MDS)
word sense disambiguation (WSD)
semantic role labeling (SRL)
coreference resolution
conditional entropy
cosine similarity
mutual information (MI)
default unification (DU)
computational linguistics (CL)
Table 4: Domain-terms used in the rating experiment
domain of computational linguistics. Five general
terms, such as ?English text? or ?web page?, were
also included in the evaluation as a control sample;
since general terms of this kind are not likely to be
defined in scientific papers in CL, their definition
sentences were of low quality (false positives). We
do not include them in the summary of the evalua-
tion results for space reasons. ?Computational lin-
guistics?, while certainly a domain term in the do-
main, is not likely to be defined in the articles in the
ACL Anthology, however, the term as such should
rather be included in a glossary of computational lin-
guistics, therefore, we included it in the evaluation.
Due to the lack of a gold-standard glossary defi-
nitions in the domain, we performed a rating exper-
iment in which we asked domain experts to judge
top-ranked definitional sentences extracted using the
two approaches. Below we briefly outline the evalu-
ation setup and the procedure.
6.1 Evaluation Data
A set of definitional sentences for the 20 domain
terms was extracted as follows:
Lexico-syntactic patterns (LSP). For the lexico-
syntactic patterns approach, sentences extracted by
the set of refined patterns (see Section 4.3) were
considered for evaluation only if they contained at
least one of the term?s potential defining phrases as
identified by the first stage of the glossary extraction
(Section 4.2). Acronyms were allowed as fillers of
the domain term placeholders.
The candidate evaluation sentences were ranked
using single linkage clustering in order to find sub-
sets of similar sentences. tf.idf-based cosine be-
tween vectors of lemmatized words was used as a
similarity function. As in (Shen et al, 2006), the
longest sentence was chosen from each of the clus-
ters. Results were ranked by considering the size of
the clusters as a measure of how likely it represents
a definition. The larger the cluster, the higher it was
ranked. Five top-ranked sentences for each of the 20
terms were used for the evaluation.
Deep analysis (DA). The only pattern used for
deep analysis extraction was ?subject:T predi-
cate:is?, with ?is? restricted by the HPSG grammar
to be the copula relation and not an auxiliary such as
in passive constructions, etc. Five top-ranked sen-
tences ? as per the Solr?s matching algorithm ? ex-
tracted with this pattern were used for the evaluation.
In total, 200 candidate definition sentences for
20 domain terms were evaluated, 100 per extraction
methods. Examples of candidate glossary sentences
extracted using both methods, along with their rat-
ings, are shown in the appendix.
6.2 Evaluation Method
Candidate definition sentences were presented to 6
human domain experts by a web interface display-
ing one sentence at a time in random order. Judges
were asked to rate sentences on a 5-point ordinal
scale with the following descriptors:10
5: The passage provides a precise and concise de-
scription of the concept
4: The passage provides a good description of the
concept
3: The passage provides useful information about
the concept, which could enhance a definition
10Example definitions at each scale point selected by the au-
thors were shown for the concept ?hidden markov model?.
61
DALSP
100,0%
80,0%
60,0%
40,0%
20,0%
0,0%
21,6727,33
18,1715,33
32,5026,17
16,3316,00
11,3315,17
1
2
3
4
5
Rating
Figure 1: Distribution of ratings across the 5 scale points;
LSP: lexico-syntactic patterns, DA: deep analysis
2: The passage is not a good enough description
of the concept to serve as a definition; for in-
stance, it?s too general, unfocused, or a subcon-
cept/superconcept of the target concept is de-
fined instead
1: The passage does not describe the concept at all
The judges participating in the rating experiment
were PhD students, postdoctoral researchers, or re-
searchers of comparable expertise, active in the ar-
eas of computational linguistics/natural language
processing/language technology. One of the raters
was one of the authors of this paper. The raters were
explicitly instructed to think along the lines of ?what
they would like to see in a glossary of computational
linguistics terms?.
6.3 Results
Figure 1 shows the distribution of ratings across
the five scale points for the two systems. Around
57% of the LSP ratings and 60% of DA ratings fall
within the top three scale-points (positive ratings)
and 43% and 40%, respectively, within the bottom
two scale-points (low ratings). Krippendorff?s or-
dinal ? (Hayes and Krippendorff, 2007) was 0.66
(1,000 bootstrapped samples) indicating a modest
degree of agreement, at which, however, tentative
conclusions can be drawn.
ILP
CRF
SVM
LSA
CCG
LFG
PCFG
DRT
DRS
PSMT;PBSMT
SMT
MDS
WSD
SRL
coref. resolution
cond. entropy
cos similarity
MI
DU
CL
Mode ratings
54321
DALSP
Method
Figure 2: Mode values of ratings per method for the indi-
vidual domain terms; see Table 4
Figure 2 shows the distribution of mode ratings
of the individual domain terms used in the evalua-
tion. Definitions of 6 terms extracted using the LSP
method were rated most frequently at 4 or 5 as op-
posed to the majority of ratings at 3 for most terms
in case of the DA method.
A Wilcoxon signed-rank test was conducted to
evaluate whether domain experts favored defini-
tional sentences extracted by one the two methods.11
The results indicated no significant difference be-
tween ratings of definitions extracted using LSP and
DA (Z = 0.43, p = 0.68).
Now, considering that the ultimate purpose of the
sentence extraction is glossary creation, we were
also interested in how the top-ranked sentences were
rated; that is, assuming we were to create a glossary
using only the highest ranked sentences (according
to the methods? ranking schemes; see Section 6.1)
we wanted to know whether one of the methods pro-
poses rank-1 candidates with higher ratings, inde-
pendently of the magnitude of the difference. A sign
test indicated no statistical difference in ratings of
the rank-1 candidates between the two methods.
11Definition sentences for each domain term were paired by
their rank assigned by the extraction methods: rank-1 DA sen-
tence with rank-1 LSP, etc.; see Section 6.1.
62
7 Conclusions and Future Work
The results show that both methods have the poten-
tial of extracting good quality glossary sentences:
the majority of the extracted sentences provide at
least useful information about the domain concepts.
However, both methods need improvement.
The rating experiment suggests that the concept of
definition quality in a specialized domain is largely
subjective (borderline acceptable agreement overall
and ? = 0.65 for rank-1 sentences). This calls for
a modification of the evaluation methodology and
for additional tests of consistency of ratings. The
low agreement might be remedied by introducing
a blocked design in which groups of judges would
evaluate definitions of a small set of concepts with
which they are most familiar, rather than a large set
of concepts from various CL sub-areas.
An analysis of the extracted sentences and their
ratings12 revealed that deep analysis reduces noise in
sentence extraction. Bootstrapping, however, yields
more candidate sentences with good or very good
ratings. While in the present work pattern refine-
ment was based only on verbs, we observed that also
the presence and position of (wh-)determiners and
prepositions might be informative. Further exper-
iments are needed 1) to find out how much speci-
ficity can be allowed without blocking the patterns?
productivity and 2) to exploit the complementary
strengths of the methods by combining them.
Since both approaches use generic linguistic re-
sources and preprocessing (POS-tagging, named-
entity extraction, etc.) they can be considered
domain-independent. To our knowledge, this is,
however, the first work that attempts to identify
definitions of Computational Linguistics concepts.
Thus, it contributes to evaluating pattern bootstrap-
ping and deep analysis in the context of the defini-
tion extraction task in our own domain.
Acknowledgments
The C-Value algorithm was implemented by Mi-
hai Grigore. We are indebted to our colleagues
from the Computational Linguistics department and
DFKI in Saarbru?cken who kindly agreed to partic-
ipate in the rating experiment as domain experts.
12Not included in this paper for space reasons
We are also grateful to the reviewers for their feed-
back. The work described in this paper has been
partially funded by the German Federal Ministry
of Education and Research, projects TAKE (FKZ
01IW08003) and Deependance (FKZ 01IW11003).
References
P. Adolphs, S. Oepen, U. Callmeier, B. Crysmann,
D. Flickinger, and B. Kiefer. 2008. Some Fine Points
of Hybrid Natural Language Parsing. In Proceedings
of the 6th LREC, pages 1380?1387.
V. Aleven, K. R. Koedinger, and K. Cross. 1999. Tutor-
ing Answer Explanation Fosters Learning with Under-
standing. In Artificial Intelligence in Education, pages
199?206. IOS Press.
S. Bird, R. Dale, B. Dorr, B. Gibson, M. Joseph, M.-
Y. Kan, D. Lee, B. Powley, D. Radev, and Y. F. Tan.
2008. The ACL Anthology Reference Corpus: A Ref-
erence Dataset for Bibliographic Research in Compu-
tational Linguistics. In Proceedings of the 6th LREC,
pages 1755?1759.
C. Borg, M. Rosner, and G. Pace. 2009. Evolutionary
Algorithms for Definition Extraction. In Proceedings
of the 1st Workshop on Definition Extraction, pages
26?32.
L. Bozzato, M. Ferrari, and A. Trombetta. 2008. Build-
ing a Domain Ontology from Glossaries: A General
Methodology. In Proceedings of the 5th Workshop on
Semantic Web Applications and Perspectives, pages 1?
10.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of ANLP, pages 224?231.
U. Callmeier. 2000. PET ? A Platform for Experimenta-
tion with Efficient HPSG Processing Techniques. Nat-
ural Language Engineering, 6(1):99?108.
A. Copestake, D. Flickinger, I. A. Sag, and C. Pollard.
2005. Minimal Recursion Semantics: an Introduction.
Research on Language and Computation, 3(2?3):281?
332.
A. Copestake. 2009. Slacker semantics: why superficial-
ity, dependency and avoidance of commitment can be
the right way to go. In Proceedings of the 12th EACL
Conference, pages 1?9.
B. Cramer and Y. Zhang. 2010. Constraining robust
constructions for broad-coverage parsing with preci-
sion grammars. In Proceedings of the 23rd COLING
Conference, pages 223?231.
I. Dura?n-Mun?oz, 2010. eLexicography in the 21st cen-
tury: New challenges, new applications, volume 7,
chapter Specialised lexicographical resources: a sur-
vey of translators? needs, pages 55?66. Presses Uni-
versitaires de Louvain.
63
O. Etzioni, M. Cafarella, D. Downey, A-M. Popescu,
T. Shaked, S. Soderland, D.S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165:91?134.
I. Fahmi and G. Bouma. 2006. Learning to identify defi-
nitions using syntactic features. In Proceedings of the
EACL Workshop on Learning Structured Information
in Natural Language Applications, pages 64?71.
D. Flickinger, S. Oepen, and G. Ytrest?l. 2010. Wiki-
Woods: Syntacto-semantic annotation for English
Wikipedia. In Proceedings of the 7th LREC, pages
1665?1671.
D. Flickinger. 2002. On building a more efficient gram-
mar by exploiting types. In Collaborative Language
Engineering. A Case Study in Efficient Grammar-
based Processing, pages 1?17. CSLI Publications,
Stanford, CA.
K. Frantzi, S. Ananiadou, and H. Mima. 1998. Au-
tomatic recognition of multi-word terms: the C-
value/NC-value method. In Proceedings of the 2nd
European Conference on Research and Advanced
Technology for Digital Libraries, pages 585?604.
R. Del Gaudio and A. Branco. 2009. Language inde-
pendent system for definition extraction: First results
using learning algorithms. In Proceedings of the 1st
Workshop on Definition Extraction, pages 33?39.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1):83?135.
A. F. Hayes and K. Krippendorff. 2007. Answering the
call for a standard reliability measure for coding data.
Communication Methods and Measures, 1(1):77?89.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
COLING Conference, pages 539?545.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics and
Speech Recognition. Prentice Hall Series in Artificial
Intelligence. 2nd Ed. Online draft (June 25, 2007).
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic Class Learning from the Web with Hyponym Pat-
tern Linkage Graphs. In Proceedings of the 46th ACL
Meeting, pages 1048?1056.
C. D. Manning and H. Schu?tze. 1999. Foundations of
statistical natural language processing. MIT Press,
Cambridge, MA, USA.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish. The Penn Treebank. Computational Linguistics,
19:313?330.
S. Muresan and J. Klavans. 2002. A method for automat-
ically building and evaluating dictionary resources. In
Proceedings of the 3rd LREC, pages 231?234.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging Generic Patterns for Automatically Harvesting
Semantic Relations. In Proceedings of the 21st COL-
ING and the 44th ACL Meeting, pages 113?120.
J. Pearson. 1996. The expression of definitions in spe-
cialised texts: A corpus-based analysis. In Proceed-
ings of Euralex-96, pages 817?824.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago.
U. Scha?fer and B. Kiefer. 2011. Advances in deep
parsing of scholarly paper content. In R. Bernardi,
S. Chambers, B. Gottfried, F. Segond, and I. Za-
ihrayeu, editors, Advanced Language Technologies for
Digital Libraries, number 6699 in LNCS, pages 135?
153. Springer.
U. Scha?fer, B. Kiefer, C. Spurk, J. Steffen, and R. Wang.
2011. The ACL Anthology Searchbench. In Proceed-
ings of ACL-HLT 2011, System Demonstrations, pages
7?13, Portland, Oregon, June.
Y. Shen, G. Zaccak, B. Katz, Y. Luo, and O. Uzuner.
2006. Duplicate Removal for Candidate Answer Sen-
tences. In Proceedings of the 1st CSAIL Student Work-
shop.
A. Thanopoulos, N. Fakotakis, and G. Kokkinakis. 2002.
Comparative evaluation of collocation extraction met-
rics. In Proceedings of the 3rd Language Resources
Evaluation Conference, pages 620?625.
P. Velardi, R. Navigli, and P. D?Amadio. 2008. Mining
the Web to Create Specialized Glossaries. IEEE Intel-
ligent Systems, pages 18?25.
S. Walter. 2008. Linguistic description and automatic
extraction of definitions from german court decisions.
In Proceedings of the 6th LREC, pages 2926?2932.
W. Weiten, D. Deguara, E. Rehmke, and L. Sewell.
1999. University, Community College, and High
School Students? Evaluations of Textbook Pedagogi-
cal Aids. Teaching of Psychology, 26(1):19?21.
E. Westerhout and P. Monachesi. 2008. Creating glos-
saries using pattern-based and machine learning tech-
niques. In Proceedings of the 6th LREC, pages 3074?
3081.
M. Wolska, U. Scha?fer, and The Nghia Pham. 2011.
Bootstrapping a domain-specific terminological taxon-
omy from scientific text. In Proceedings of the 9th In-
ternational Conference on Terminology and Artificial
Intelligence (TIA-11), pages 17?23. INALCO, Paris.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Extract-
ing and annotating wikipedia sub-domains. In Pro-
ceedings of the 7th Workshop on Treebanks and Lin-
guistic Theories, pages 185?197.
64
Appendix
Rated glossary sentences for ?word sense disambiguation (WSD)? and ?mutual information (MI)?. As shown
in Figure 2, for WSD, mode ratings of LSP sentences were higher, while for MI it was the other way round.
word sense disambiguation (WSD)
mode ratings of LSP sentences:
WSD is the task of determining the sense of a polysemous word within a specific context (Wang et al, 2006). 5
Word sense disambiguation or WSD, the task of identifying the correct sense of a word in context, is a central problem
for all natural language processing applications, and in particular machine translation: different senses of a word translate
differently in other languages, and resolving sense ambiguity is needed to identify the right translation of a word.
4
Unlike previous applications of co-training and self-training to natural languagelearning, where one general classifier is
build to cover the entire problem space, supervised word sense disambiguation implies a different classifier for each in-
dividual word, resulting eventually in thousands of different classifiers, each with its own characteristics (learning rate,
sensitivity to new examples, etc.).
3
NER identifies different kinds of names such as ?person?, ?location? or ?date?, while WSD distinguishes the senses of
ambiguous words.
3
This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian
classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context.
1
DA sentences:
Word Sense Disambiguation (WSD) is the task of formalizing the intended meaning of a word in context by selecting an
appropriate sense from a computational lexicon in an automatic manner.
5
Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs. {4,5}
Word sense disambiguation (WSD) is a difficult problem in natural language processing. 2
word sense disambiguation, Hownet, sememe, co-occurrence Word sense disambiguation (WSD) is one of the most difficult
problems in NLP.
{1,2}
There is a general concern within the field of word sense disambiguation about the inter-annotator agreement between
human annotators.
1
mutual information (MI)
mode ratings of LSP sentences:
According to Fano (1961), if two points (words), x and y, have probabilities P (x) and P (y), then their mutual information,
I(x, y), is defined to be I(x, y) = log2
P (x,y)
P (x)P (y) ); informally, mutual information compares the probability of observing x
and y together (the joint probability) with the probabilities of observing x and y independently (chance).
5
Mutual information, I(v; c/s), measures the strength of the statistical association between the given verb v and the candi-
date class c in the given syntactic position s.
3
In this equation, pmi(i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g.
consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples.
{1,3}
Note that while differential entropies can be negative and not invariant under change of variables, other properties of entropy
are retained (Huber et al, 2008), such as the chain rule for conditional entropy which describes the uncertainty in Y given
knowledge of X , and the chain rule for mutual information which describes the mutual dependence between X and Y .
2
The first term of the conditional probability measures the generality of the association, while the second term of the mutual
information measures the co-occurrence of the association.
2
DA sentences:
Mutual information (Shannon and Weaver, 1949) is a measure of mutual dependence between two random variables. 4
3 Theory Mutual information is a measure of the amount of information that one random variable contains about another
random variable.
4
Conditional mutual information is the mutual information of two random variables conditioned on a third one. {1,3}
Thus, the mutual information is log25 or 2.32 bits, meaning that the joint probability is 5 times more likely than chance. 1
Thus, the mutual information is log20, meaning that the joint is infinitely less likely than chance. 1
65
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88?97,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards an ACL Anthology Corpus with Logical Document Structure
An Overview of the ACL 2012 Contributed Task
Ulrich Sch?fer
DFKI Language Technology Lab
Campus D 3 1
D-66123 Saarbr?cken, Germany
ulrich.schaefer@dfki.de
Jonathon Read, Stephan Oepen
Department of Informatics
Universitetet i Oslo
0316 Oslo, Norway
{jread |oe}@ifi.uio.no
Abstract
The ACL 2012 Contributed Task is a com-
munity effort aiming to provide the full ACL
Anthology as a high-quality corpus with rich
markup, following the TEI P5 guidelines?
a new resource dubbed the ACL Anthology
Corpus (AAC). The goal of the task is three-
fold: (a) to provide a shared resource for ex-
perimentation on scientific text; (b) to serve
as a basis for advanced search over the ACL
Anthology, based on textual content and cita-
tions; and, by combining the aforementioned
goals, (c) to present a showcase of the benefits
of natural language processing to a broader au-
dience. The Contributed Task extends the cur-
rent Anthology Reference Corpus (ARC) both
in size, quality, and by aiming to provide tools
that allow the corpus to be automatically ex-
tended with new content?be they scanned or
born-digital.
1 Introduction?Motivation
The collection of the Association for Computational
Linguistics (ACL) Anthology began in 2002, with
3,100 scanned and born-digital1 PDF papers. Since
then, the ACL Anthology has become the open ac-
cess collection2 of scientific papers in the area of
Computational Linguistics and Language Technol-
ogy. It contains conference and workshop proceed-
ings and the journal Computational Linguistics (for-
merly the American Journal of Computational Lin-
guistics). As of Spring 2012, the ACL Anthol-
1The term born-digital means natively digital, i.e. prepared
electronically using typesetting systems like LATEX, OpenOffice,
and the like?as opposed to digitized (or scanned) documents.
2
http://aclweb.org/anthology
ogy comprises approximately 23,000 papers from 46
years.
Bird et al (2008) started collecting not only the
PDF documents, but also providing the textual con-
tent of the Anthology as a corpus, the ACL Anthol-
ogy Reference Corpus3 (ACL-ARC). This text ver-
sion was generated fully automatically and in differ-
ent formats (see Section 2.2 below), using off-the-
shelf tools and yielding somewhat variable quality.
The main goal was to provide a reference cor-
pus with fixed releases that researchers could use
and refer to for comparison. In addition, the vision
was formulated that manually corrected ground-
truth subsets could be compiled. This is accom-
plished so far for citation links from paper to paper
inside the Anthology for a controlled subset. The
focus thus was laid on bibliographic and bibliomet-
ric research and resulted in the ACL Anthology Net-
work (Radev et al, 2009) as a public, manually cor-
rected citation database.
What is currently missing is an easy-to-process
XML variant that contains high-quality running text
and logical markup from the layout, such as section
headings, captions, footnotes, italics etc. In prin-
ciple this could be derived from LATEX source files,
but unfortunately, these are not available, and fur-
thermore a considerable amount of papers have been
typeset with various other word processing software.
Here is where the ACL 2012 Contributed Task
starts: The idea is to combine OCR and PDFBox-
like born-digital text extraction methods and re-
assign font and logical structure information as part
of a rich XML format. The method would rely on
OCR exclusively only in cases where no born-digital
3
http://acl-arc.comp.nus.edu.sg
88
PDFs are available?in case of the ACL Anthology
mostly papers published before the year 2000. Cur-
rent results and status updates will always be acces-
sible through the following address:




	
http://www.delph-in.net/aac/
We note that manually annotating the ACL An-
thology is not viable. In a feasibility study we took
a set of five eight-page papers. After extracting
the text using PDFBox4 we manually corrected the
output and annotated it with basic document struc-
ture and cross-references; this took 16 person-hours,
which would suggest a rough estimate of some 25
person-years to manually correct and annotate the
current ACL Anthology. Furthermore, the ACL An-
thology grows substantially every year, requiring a
sustained effort.
2 State of Affairs to Date
In the following, we briefly review the current status
of the ACL Anthology and some of its derivatives.
2.1 ACL Anthology
Papers in the current Anthology are in PDF format,
either as scanned bitmaps or digitally typeset with
LATEX or word processing software. Older scanned
papers were often created using type writers, and
sometimes even contained hand-drawn graphics.
2.2 Anthology Reference Corpus (ACL-ARC)
In addition to the PDF documents, the ACL-ARC
also contains (per page and per paper)
? bitmap files (in the PNG file format)
? plain text in ?normal? reading order
? formatted text (in two columns for most of the
papers)
? XML raw layout format containing position in-
formation for each word, grouped in lines, with
font information, but no running text variant.
The latter three have been generated using OCR
software (OmniPage) operating on the bitmap files.
4
http://pdfbox.apache.org
However, OCR methods tend to introduce charac-
ter and layout recognition errors, from both scanned
and born-digital documents.
The born-digital subset of the ACL-ARC (mostly
papers that appeared in 2000 or later) also contains
PDFBox plain text output. However, this is not
available for approximately 4% of the born-digital
PDFs due to unusual font encodings. Note though,
that extracting text from PDFs in normal reading
order is not a trivial task (Berg et al, 2012), and
many errors exist. Furthermore, the plain text is
not dehyphenated, necessitating a language model
or lexicon-based lookup for post-processing.
2.3 ACL Anthology Network
The ACL Anthology Network (Radev et al, 2009)
is based on the ACL-ARC text outputs. It addition-
ally contains manually-corrected citation graphs, au-
thor and affiliation data for most of the Anthology
(papers until 2009).
2.4 Publications with the ACL Anthology as a
Corpus
We did a little survey in the ACL Anthology of pa-
pers reporting on having used the ACL Anthology as
corpus/dataset. The aim here is to get an overview
and distribution of the different NLP research tasks
that have been pursued using the ACL Anthology as
dataset. There are probably other papers outside the
Anthology itself, but these have not been looked at.
The pioneers working with the Anthology as cor-
pus are Ritchie et al (2006a, 2006b). They did work
related to citations which also forms the largest topic
cluster of papers applying or using Anthology data.
Later papers on citation analysis, summarization,
classification, etc. are Qazvinian et al (2010), Abu-
Jbara & Radev (2011), Qazvinian & Radev (2010),
Qazvinian & Radev (2008), Mohammad et al
(2009), Athar (2011), Sch?fer & Kasterka (2010),
and Dong & Sch?fer (2011).
Text summarization research is performed in
Qazvinian & Radev (2011) and Agarwal et al
(2011a, 2011b).
The HOO (?Help our own?) text correction shared
task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-
zovskaya et al, 2011; Dahlmeier et al, 2011) aims
at developing automated tools and techniques that
89
assist authors, e.g. non-native speakers of English,
in writing (better) scientific publications.
Classification/Clustering related publications are
Muthukrishnan et al (2011) and Mao et al (2010).
Keyword extraction and topic models based on
Anthology data are addressed in Johri et al (2011),
Johri et al (2010), Gupta & Manning (2011), Hall
et al (2008), Tu et al (2010) and Daudaravic?ius
(2012). Reiplinger et al (2012) use the ACL An-
thology to acquire and refine extraction patterns for
the identification of glossary sentences.
In this workshop several authors have used the
ACL Anthology to analyze the history of compu-
tational linguistics. Radev & Abu-Jbara (2012) ex-
amine research trends through the citing sentences
in the ACL Anthology Network. Anderson et al
(2012) use the ACL Anthology to perform a people-
centered analysis of the history of computational
linguistics, tracking authors over topical subfields,
identifying epochs and analyzing the evolution of
subfields. Sim et al (2012) use a citation analysis to
identify the changing factions within the field. Vo-
gel & Jurafsky (2012) use topic models to explore
the research topics of men and women in the ACL
Anthology Network. Gupta & Rosso (2012) look
for evidence of text reuse in the ACL Anthology.
Most of these and related works would benefit
from section (heading) information, and partly the
approaches already used ad hoc solutions to gather
this information from the existing plain text ver-
sions. Rich text markup (e.g. italics, tables) could
also be used for linguistic, multilingual example ex-
traction in the spirit of the ODIN project (Xia &
Lewis, 2008; Xia et al, 2009).
3 Target Text Encoding
To select encoding elements we adopt the TEI P5
Guidelines (TEI Consortium, 2012). The TEI en-
coding scheme was developed with the intention of
being applicable to all types of natural language, and
facilitating the exchange of textual data among re-
searchers across discipline. The guidelines are im-
plemented in XML; we currently use inline markup,
but stand-off annotations have also been applied
(Ban?ski & Przepi?rkowski, 2009).
We use a subset of the TEI P5 Guidelines as
not all elements were deemed necessary. This pro-
cess was made easier through Roma5, an online
tool that assists in the development of TEI valida-
tors. We note that, while we initially use a simpli-
fied version, the schemas are readily extensible. For
instance, Przepi?rkowski (2009) demonstrates how
constituent and dependency information can be en-
coded following the guidelines, in a manner which
is similar to other prominent standards.
A TEI corpus is typically encoded as a sin-
gle XML document, with several text elements,
which in turn contain front (for abstracts), body
and back elements (for acknowledgements and bib-
liographies). Then, sections are encoded using div
elements (with xml:ids), which contain a heading
(head) and are divided into paragraphs (p). We
aim for accountability when translating between for-
mats; for example, the del element records deletions
(such as dehyphenation at line breaks).
An example of a TEI version of an ACL Anthol-
ogy paper is depicted in Figure 1 on the next page.
4 An Overview of the Contributed Task
The goal of the ACL 2012 Contributed Task is to
provide a high-quality version of the textual content
of the ACL Anthology as a corpus. Its rich text
XML markup will contain information on logical
document structure such as section headings, foot-
notes, table and figure captions, bibliographic ref-
erences, italics/emphasized text portions, non-latin
scripts, etc.
The initial source are the PDF documents of the
Anthology, processed with different text extraction
methods and tools that output XML/HTML. The in-
put to the task itself then consists of two XML for-
mats:
? PaperXML from the ACL Anthology Search-
bench6 (Sch?fer et al, 2011) provided
by DFKI Saarbr?cken, of all approximately
22,500 papers currently in the Anthology (ex-
cept ROCLING which are mostly in Chi-
nese). These were obtained by running a com-
mercial OCR program and applying logical
markup postprocessing and conversion to XML
(Sch?fer & Weitz, 2012).
5
http://www.tei-c.org/Roma/
6
http://aclasb.dfki.de
90
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<author>
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii??*
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
</author>
</titleStmt>
<publicationStmt>
<publisher>Association for Computational Linguistics</publisher>
<pubPlace> Columbus, Ohio, USA</pubPlace>
<date>June 2008</date>
</publicationStmt>
<sourceDesc> [. . . ] </sourceDesc>
</fileDesc>
<encodingDesc> [. . . ] </encodingDesc>
</teiHeader>
<text>
<front>
<div type="abs">
<head>Abstract</head>
<p> [. . . ] </p>
</div>
</front>
<body>
<div xml:id="SE1">
<head>Introduction</head>
<p>
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame<del type="lb">-</del>
works (<ref target="#BI6">Charniak, 2000</ref>;
[. . . ]
</p>
</div>
</body>
<back>
<div type="ack">
<head>Acknowledgements</head>
<p> [. . . ] </p>
</div>
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI1">
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
<hi rend="italic">Computational Linguistics</hi>, 30(4):479?511.
</bibl>
[. . . ]
</listBibl>
<pb n="54"/>
</div>
</back>
</text>
</TEI>
Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006. Some elements are
truncated ([. . . ]) for brevity.
91
? TEI P5 XML generated by PDFExtract. For pa-
pers from after 1999, an additional high-quality
extraction step took place, applying state-of
the art word boundary and layout recognition
methods directly to the native, logical PDF
structure (Berg et al, 2012). As no charac-
ter recognition errors occur, this will form the
master format for textual content if available.
Because both versions are not perfect, a large, ini-
tial part of the Contributed Task requires automat-
ically adding missing or correcting markup, using
information from OCR where necessary (e.g. for ta-
bles). Hence, for most papers from after 1999 (cur-
rently approx. 70% of the papers), the Contributed
Task can make use of both representations simulta-
neously.
The role of paperXML in the Contributed Task is
to serve as fall-back source (1) for older, scanned
papers (mostly published before the year 2000), for
which born-digital PDF sources are not available,
or (2) for born-digital PDF papers on which the
PDFExtract method failed, or (3) for document parts
where PDFExtract does not output useful markup
such as currently for tables, cf. Section 4.2 below.
A big advantage of PDFExtract is its ability to ex-
tract the full Unicode character range without char-
acter recognition errors, while the OCR-based ex-
traction methods in our setup are basically limited
to Latin1 characters to avoid higher recognition er-
ror rates.
We proposed the following eight areas as possible
subtasks towards our goal.
4.1 Subtask 1: Footnotes
The first task addresses identification of footnotes,
assigning footnote numbers and text, and generating
markup for them in TEI P5 style. For example:
We first determine lexical heads of nonterminal
nodes by using Bikel's implementation of
Collins' head detection algorithm
<note place="foot" n="9">
<hi rend="monospace">http://www.cis.upenn.edu/
~dbikel/software.html</hi>
</note>
(<ref target="#BI1">Bikel, 2004</ref>;
<ref target="#BI11">Collins, 1997</ref>).
Footnotes are handled to some extent in PDFEx-
tract and paperXML, but the results require refine-
ment.
4.2 Subtask 2: Tables
Task 2 identifies figure/table references in running
text and links them to their captions. The latter
will also have to be distinguished from running text.
Furthermore, tables will have to be identified and
transformed into HTML style table markup. This
is currently not generated by PDFExtract, but the
OCR tool used for paperXML generation quite re-
liably recognizes tables and transforms tables into
HTML. Thus, a preliminary solution would be to in-
sert missing table content in PDFExtract output from
the OCR results. In the long run, implementing table
handling in PDFExtract would be desirable.
<ref target="#TA3">Table 3</ref> shows the
time for parsing the entire AImed corpus,...
<figure xml:id="TA3">
<head>Table 3: Parsing time (sec.)</head>
<!-- TEI table content markup here -->
</figure>
4.3 Subtask 3: Bibliographic Markup
The purpose of this task is to identify citations in
text and link them to the bibliographic references
listed at the end of each paper. In TEI markup, bibli-
ographies are contained in listBibl elements. The
contents of listBibl can range from formatted text
to moderately-structured entries (biblStruct) and
fully-structured entries (biblFull). For example:
We follow the PPI extraction method of
<ref target="#BI39">S?tre et al (2007)</ref>,
which is based on SVMs ...
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI39">
R. S?tre, K. Sagae, and J. Tsujii. 2007.
Syntactic features for protein-protein
interaction extraction. In
<hi rend="italic">LBM 2007 short papers</hi>.
</bibl>
</listBibl>
</div>
A citation extraction and linking tool that is
known to deliver good results on ACL Anthology
papers (and even comes with CRF models trained
on this corpus) is ParsCit (Councill et al, 2008). In
this volume, Nhat & Bysani (2012) provide an im-
plementation for this task using ParsCit and discuss
possible further improvements.
92
4.4 Subtask 4: De-hyphenation
Both paperXML and PDFExtract output contain soft
hyphenation indicators at places where the original
paper contained a line break with hyphenation. In
paperXML, they are represented by the Unicode soft
hyphen character (in contrast to normal dashes that
also occur). PDFExtract marks hyphenation from
the original text using a special element. How-
ever, both tools make errors: In some cases, the hy-
phens are in fact hard hyphens. The idea of this
task is to combine both sources and possibly ad-
ditional information, as in general the OCR pro-
gram used for paperXML more aggressively pro-
poses de-hyphenation than PDFExtract. Hyphen-
ation in names often persists in paperXML and
therefore remains a problem that will have to be ad-
dressed as well. For example:
In this paper, we present a comparative
eval<del type="lb">-</del>uation of syntactic
parsers and their output
represen<del type="lb">-</del>tations based on
different frameworks:
4.5 Subtask 5: Remove Garbage such as
Leftovers from Figures
In both paperXML and PDFExtract output, text
remains from figures, illustrations and diagrams.
This occurs more frequently in paperXML than in
PDFExtract output because text in bitmap figures
undergoes OCR as well. The goal of this subtask
is to recognize and remove such text.
Bitmaps in born-digital PDFs are embedded ob-
jects for PDFExtract and thus can be detected and
encoded within TEI P5 markup and ignored in the
text extraction process:
<figure xml:id="FI3">
<graphic url="P08-1006/FI3.png" />
<head>
Figure 3: Predicate argument structure
</head>
</figure>
4.6 Subtask 6: Generate TEI P5 Markup for
Scanned Papers from paperXML
Due to the nature of the extraction process, PDFEx-
tract output is not available for older, scanned pa-
pers. These are mostly papers from before 2000, but
also e.g. EACL 2003 papers. On the other hand, pa-
perXML versions exist for almost all papers of the
ACL Anthology, generated from OCR output. They
still need to be transformed to TEI P5, e.g. using
XSLT. The paperXML format and transformation to
TEI P5 is discussed in Sch?fer & Weitz (2012) in
this volume.
4.7 Subtask 7: Add Sentence Splitting Markup
Having a standard for sentence splitting with unique
sentence IDs per paper to which everyone can refer
to later could be important. The aim of this task is to
add sentence segmentation to the target markup. It
should be based on an open source tokenizer such as
JTok, a customizable open source tool7 that was also
used for the ACL Anthology Searchbench semantic
index pre-processing, or the Stanford Tokenizer8.
<p><s>PPI extraction is an NLP task to identify
protein pairs that are mentioned as interacting
in biomedical papers.</s> <s>Because the number
of biomedical papers is growing rapidly, it is
impossible for biomedical researchers to read
all papers relevant to their research; thus,
there is an emerging need for reliable IE
technologies, such as PPI identification.
</s></p>
4.8 Subtask 8: Math Formulae
Many papers in the Computational Linguistics area,
especially those dealing with statistical natural lan-
guage processing, contain mathematical formulae.
Neither paperXML nor PDFExtract currently pro-
vide a means to deal with these.
A math formula recognition is a complex task, in-
serting MathML9 formula markup from an external
tool (formula OCR, e.g. from InftyReader10) could
be a viable solution.
For example, the following could become the tar-
get format of MathML embedded in TEI P5, for
?? > 0 3 f (x) < 1:
<mrow>
<mo> there exists </mo>
<mrow>
<mrow>
<mi> &#916; <!--GREEK SMALL DELTA--></mi>
<mo> &gt; </mo>
<mn> 0 </mn>
7
http://heartofgold.opendfki.de/repos/trunk/
jtok; LPGL license
8
http://nlp.stanford.edu/software/tokenizer.
shtml; GPL V2 license
9
http://www.w3.org/TR/MathML/
10
http://sciaccess.net/en/InftyReader/
93
</mrow>
<mo> such that </mo>
<mrow>
<mrow>
<mi> f </mi>
<mo> &#2061; <!--FUNCTION APPL.--></mo>
<mrow>
<mo> ( </mo>
<mi> x </mi>
<mo> ) </mo>
</mrow>
</mrow>
<mo> &lt; </mo>
<mn> 1 </mn>
</mrow>
</mrow>
</mrow>
An alternative way would be to implement math
formula recognition directly in PDFExtract using
methods known from math OCR, similar to the page
layout recognition approach.
5 Discussion?Outlook
Through the ACL 2012 Contributed Task, we have
taken a (small, some might say) step further towards
the goal of a high-quality, rich-text version of the
ACL Anthology as a corpus?making available both
the original text and logical document structure.
Although many of the subtasks sketched above
did not find volunteers in this round, the Contributed
Task, in our view, is an on-going, long-term com-
munity endeavor. Results to date, if nothing else,
confirm the general suitability of (a) using TEI P5
markup as a shared target representation and (b) ex-
ploiting the complementarity of OCR-based tech-
niques (Sch?fer & Weitz, 2012), on the one hand,
and direct interpretation of born-digital PDF files
(Berg et al, 2012), on the other hand. Combin-
ing these approaches has the potential to solve the
venerable challenges that stem from inhomogeneous
sources in the ACL Anthology?e.g. scanned, older
papers and digital newer papers, generated from a
broad variety of typesetting tools.
However, as of mid-2012 there still is no ready-to-
use, high-quality corpus that could serve as a shared
starting point for the range of Anthology-based NLP
activities sketched in Section 1 above. In fact, we
remain slightly ambivalent about our recommenda-
tions for utilizing the current state of affairs and ex-
pected next steps?as we would like to avoid much
work getting underway with a version of the corpus
that we know is unsatisfactory. Further, obviously,
versioning and well-defined release cycles will be a
prerequisite to making the corpus useful for compa-
rable research, as discussed by Bird et al (2008).
In a nutshell, we see two possible avenues for-
ward. For the ACL 2012 Contributed Task, we col-
lected various views on the corpus data (as well as
some of the source code used in its production) in a
unified SVN repository. Following the open-source,
crowd-sourcing philosophy, one option would be to
make this repository openly available to all inter-
ested parties for future development, possibly aug-
menting it with support infrastructure like, for ex-
ample, a mailing list and shared wiki.
At the same time, our experience from the past
months suggests that it is hard to reach sufficient
momentum and critical mass to make substantial
progress towards our long-term goals, while con-
tributions are limited to loosely organized volun-
teer work. A possibility we believe might overcome
these limitations would be an attempt at formaliz-
ing work in this spirit further, for example through a
funded project (with endorsement and maybe finan-
cial support from organizations like the ACL, ICCL,
AFNLP, ELRA, or LDC).
A potential, but not seriously contemplated ?busi-
ness model? for the ACL Anthology Corpus could be
that only groups providing also improved versions
of the corpus would get access to it. This would
contradict the community spirit and other demands,
viz. that all code should be made publicly available
(as open source) that is used to produce the rich-text
XML for new papers added to the Anthology. To de-
cide on the way forward, we will solicit comments
and expressions of interest during ACL 2012, in-
cluding of course from the R50 workshop audience
and participants in the Contributed Task. Current
results and status updates will always be accessible
through the following address:




	
http://www.delph-in.net/aac/
The ACL publication process for conferences and
workshops already today supports automated collec-
tion of metadata and uniform layout/branding. For
future high-quality collections of papers in the area
of Computational Linguistics, the ACL could think
94
about providing extended macro packages for con-
ferences and journals that generate rich text and doc-
ument structure preserving (TEI P5) XML versions
as a side effect, in addition to PDF generation. Tech-
nically, it should be possible in both LATEX and (for
sure) in word processors such as OpenOffice or MS
Word. It would help reducing errors induced by
the tedious PDF-to-XML extraction this Contributed
Task dealt with.
Finally, we do think that it will well be possible to
apply the Contributed Task ideas and machinery to
scientific publications in other areas, including the
envisaged NLP research and existing NLP applica-
tions for search, terminology extraction, summariza-
tion, citation analysis, and more.
6 Acknowledgments
The authors would like to thank the ACL, the work-
shop organizer Rafael Banchs, the task contributors
for their pioneering work, and the NUS group for
their support. We are indebted to Rebecca Dridan
for helpful feedback on this work.
The work of the first author has been funded
by the German Federal Ministry of Education and
Research, projects TAKE (FKZ 01IW08003) and
Deependance (FKZ 01IW11003). The second and
third authors are supported by the Norwegian Re-
search Council through the VerdIKT programme.
References
Abu-Jbara, A., & Radev, D. (2011). Coherent
citation-based summarization of scientific papers.
In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human
language techologies (pp. 500?509). Portland,
OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011a). Scisumm: A multi-document summa-
rization system for scientific articles. In Proceed-
ings of the ACL-HLT 2011 system demonstrations
(pp. 115?120). Portland, OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011b). Towards multi-document summarization
of scientific articles: Making interesting compar-
isons with SciSumm. In Proceedings of the work-
shop on automatic summarization for different
genres, media, and languages (pp. 8?15). Port-
land, OR.
Anderson, A., McFarland, D., & Jurafsky, D.
(2012). Towards a computational history of the
ACL:1980?2008. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
Athar, A. (2011). Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of the ACL 2011 student session (pp. 81?87).
Portland, OR.
Ban?ski, P., & Przepi?rkowski, A. (2009). Stand-off
TEI annotation: the case of the National Corpus
of Polish. In Proceedings of the third linguistic
annotation workshop (pp. 64?67). Suntec, Singa-
pore.
Berg, ?. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.
Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
Corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proceed-
ings of the sixth international conference on lan-
guage resources and evaluation (LREC-08). Mar-
rakech, Morocco.
Councill, I. G., Giles, C. L., & Kan, M.-Y. (2008).
ParsCit: An open-source CRF reference string
parsing package. In Proceedings of LREC-2008
(pp. 661?667). Marrakesh, Morocco.
Dahlmeier, D., Ng, H. T., & Tran, T. P. (2011). NUS
at the HOO 2011 pilot shared task. In Proceedings
of the generation challenges session at the 13th
european workshop on natural language genera-
tion (pp. 257?259). Nancy, France.
Dale, R., & Kilgarriff, A. (2010). Helping Our Own:
Text massaging for computational linguistics as a
new shared task. In Proceedings of the 6th inter-
national natural language generation conference.
Trim, Co. Meath, Ireland.
95
Daudaravic?ius, V. (2012). Applying collocation seg-
mentation to the ACL Anthology Reference Cor-
pus. In Proceedings of the ACL-2012 main con-
ference workshop: Rediscovering 50 years of dis-
coveries. Jeju, Republic of Korea.
Dong, C., & Sch?fer, U. (2011). Ensemble-style
self-training on citation classification. In Pro-
ceedings of 5th international joint conference on
natural language processing (pp. 623?631). Chi-
ang Mai, Thailand.
Gupta, P., & Rosso, P. (2012). Text reuse with
ACL: (upward) trends. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Gupta, S., & Manning, C. (2011). Analyzing the
dynamics of research by extracting key aspects of
scientific papers. In Proceedings of 5th interna-
tional joint conference on natural language pro-
cessing (pp. 1?9). Chiang Mai, Thailand.
Hall, D., Jurafsky, D., & Manning, C. D. (2008).
Studying the history of ideas using topic models.
In Proceedings of the 2008 conference on empir-
ical methods in natural language processing (pp.
363?371). Honolulu, Hawaii.
Johri, N., Ramage, D., McFarland, D., & Jurafsky,
D. (2011). A study of academic collaborations
in computational linguistics using a latent mix-
ture of authors model. In Proceedings of the 5th
ACL-HLT workshop on language technology for
cultural heritage, social sciences, and humanities
(pp. 124?132). Portland, OR.
Johri, N., Roth, D., & Tu, Y. (2010). Experts?
retrieval with multiword-enhanced author topic
model. In Proceedings of the NAACL HLT 2010
workshop on semantic search (pp. 10?18). Los
Angeles, California.
Mao, Y., Balasubramanian, K., & Lebanon, G.
(2010). Dimensionality reduction for text using
domain knowledge. In COLING 2010: Posters
(pp. 801?809). Beijing, China.
Mohammad, S., Dorr, B., Egan, M., Hassan, A.,
Muthukrishan, P., Qazvinian, V., Radev, D., & Za-
jic, D. (2009). Using citations to generate surveys
of scientific paradigms. In Proceedings of human
language technologies: The 2009 annual confer-
ence of the north american chapter of the associa-
tion for computational linguistics (pp. 584?592).
Boulder, Colorado.
Muthukrishnan, P., Radev, D., & Mei, Q. (2011). Si-
multaneous similarity learning and feature-weight
learning for document clustering. In Proceedings
of textgraphs-6: Graph-based methods for natu-
ral language processing (pp. 42?50). Portland,
OR.
Nhat, H. D. H., & Bysani, P. (2012). Linking ci-
tations to their bibliographic references. In Pro-
ceedings of the ACL-2012 main conference work-
shop: Rediscovering 50 years of discoveries. Jeju,
Republic of Korea.
Przepi?rkowski, A. (2009). TEI P5 as an XML stan-
dard for treebank encoding. In Proceedings of the
eighth international workshop on treebanks and
linguistic theories (pp. 149?160). Milano, Italy.
Qazvinian, V., & Radev, D. R. (2008). Scientific
paper summarization using citation summary net-
works. In Proceedings of the 22nd international
conference on computational linguistics (COL-
ING 2008) (pp. 689?696). Manchester, UK.
Qazvinian, V., & Radev, D. R. (2010). Identi-
fying non-explicit citing sentences for citation-
based summarization. In Proceedings of the 48th
annual meeting of the association for computa-
tional linguistics (pp. 555?564). Uppsala, Swe-
den.
Qazvinian, V., & Radev, D. R. (2011). Learning
from collective human behavior to introduce di-
versity in lexical choice. In Proceedings of the
49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies (pp. 1098?1108). Portland, OR.
Qazvinian, V., Radev, D. R., & Ozgur, A. (2010).
Citation summarization through keyphrase ex-
traction. In Proceedings of the 23rd international
conference on computational linguistics (COL-
ING 2010) (pp. 895?903). Beijing, China.
Radev, D., & Abu-Jbara, A. (2012). Rediscovering
ACL discoveries through the lens of ACL Anthol-
ogy Network citing sentences. In Proceedings of
96
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Radev, D., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network corpus. In
Proceedings of the 2009 workshop on text and
citation analysis for scholarly digital libraries.
Morristown, NJ, USA.
Radev, D. R., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network. In Pro-
ceedings of the 2009 workshop on text and cita-
tion analysis for scholarly digital libraries (pp.
54?61). Suntec City, Singapore.
Reiplinger, M., Sch?fer, U., & Wolska, M. (2012).
Extracting glossary sentences from scholarly ar-
ticles: A comparative evaluation of pattern boot-
strapping and deep analysis. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Ritchie, A., Teufel, S., & Robertson, S. (2006a).
Creating a test collection for citation-based IR ex-
periments. In Proceedings of the human language
technology conference of the NAACL, main con-
ference (pp. 391?398). New York City.
Ritchie, A., Teufel, S., & Robertson, S. (2006b).
How to find better index terms through cita-
tions. In Proceedings of the workshop on how can
computational linguistics improve information re-
trieval? (pp. 25?32). Sydney, Australia.
Rozovskaya, A., Sammons, M., Gioja, J., & Roth,
D. (2011). University of illinois system in HOO
text correction shared task. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 263?266). Nancy, France.
Sch?fer, U., & Kasterka, U. (2010). Scientific au-
thoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL HLT
2010 workshop on computational linguistics and
writing: Writing processes and authoring aids
(pp. 7?14). Los Angeles, CA.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, OR.
Sch?fer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Sim, Y., Smith, N. A., & Smith, D. A. (2012).
Discovering factions in the computational linguis-
tics community. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)
Tu, Y., Johri, N., Roth, D., & Hockenmaier, J.
(2010). Citation author topic model in expert
search. In COLING 2010: Posters (pp. 1265?
1273). Beijing, China.
Vogel, A., & Jurafsky, D. (2012). He said, she said:
Gender in the ACL anthology. In Proceedings of
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Xia, F., Lewis, W., & Poon, H. (2009). Language
ID in the context of harvesting language data off
the web. In Proceedings of the 12th conference
of the european chapter of the ACL (EACL 2009)
(pp. 870?878). Athens, Greece.
Xia, F., & Lewis, W. D. (2008). Repurposing the-
oretical linguistic data for tool development and
search. In Proceedings of the third international
joint conference on natural language processing:
Volume-i (pp. 529?536). Hyderabad, India.
Zesch, T. (2011). Helping Our Own 2011: UKP
lab system description. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 260?262). Nancy, France.
97
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 104?109,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Combining OCR Outputs for Logical Document Structure Markup
Technical Background to the ACL 2012 Contributed Task
Ulrich Sch?fer Benjamin Weitz
DFKI Language Technology Lab
Campus D 3 1, D-66123 Saarbr?cken, Germany
{ulrich.schaefer|benjamin.weitz}@dfki.de
Abstract
We describe how paperXML, a logical docu-
ment structure markup for scholarly articles,
is generated on the basis of OCR tool out-
puts. PaperXML has been initially developed
for the ACL Anthology Searchbench. The
main purpose was to robustly provide uni-
form access to sentences in ACL Anthology
papers from the past 46 years, ranging from
scanned, typewriter-written conference and
workshop proceedings papers, up to recent
high-quality typeset, born-digital journal arti-
cles, with varying layouts. PaperXML markup
includes information on page and paragraph
breaks, section headings, footnotes, tables,
captions, boldface and italics character styles
as well as bibliographic and publication meta-
data. The role of paperXML in the ACL Con-
tributed Task Rediscovering 50 Years of Dis-
coveries is to serve as fall-back source (1) for
older, scanned papers (mostly published be-
fore the year 2000), for which born-digital
PDF sources are not available, (2) for born-
digital PDF papers on which the PDFExtract
method failed, (3) for document parts where
PDFExtract does not output useful markup
such as currently for tables. We sketch trans-
formation of paperXML into the ACL Con-
tributed Task?s TEI P5 XML.
1 Introduction
Work on the ACL Anthology Searchbench started in
2009. The goal was to provide combined sentence-
semantic, full-text and bibliographic search in the
complete ACL Anthology (Sch?fer et al, 2011), and
a graphical citation browser with citation sentence
context information (Weitz & Sch?fer, 2012). Since
the ACL-HLT 2011 conference, the Searchbench is
available as a free, public service1.
A fixed subset of the Anthology, the ACL An-
thology Reference Corpus2 (ACL-ARC), contains
various representations of the papers such as PDF,
bitmap and text files. The latter were generated
with PDFBox3 and OCR (Omnipage4), applied to
the PDF files or bitmap versions thereof. Its static
nature as infrequently released reference corpus and
low character recognition quality especially of older,
badly scanned papers, made us to look for alterna-
tives. For quick, automatic updates of the Search-
bench index, a robust method for getting the text
from old and new incoming PDF files was needed.
After a thorough comparison of different PDF-to-
text extraction tools, a decision was made to process
every PDF paper in the Anthology with ABBYY
PDF Transformer5, for various reasons. It ran stably
and delivered good character recognition rates on
both scanned, typewriter-typeset proceeding papers
as well as on born-digital PDF of various sources,
even on papers where PDFbox failed to extract (us-
able) text. Reading order recovery, table recognition
and output rendering (HTML) was impressive and
de-hyphenation for English text worked reasonably
well. All in all, ABBYY did not deliver perfect re-
sults, but at that time was the best and quickest so-
lution to get most of the millions of sentences from
the papers of 46 years.
The role of this OCR-based approach in the ACL
1
http://aclasb.dfki.de
2
http://acl-arc.comp.nus.edu.sg
3
http://pdfbox.apache.org
4
http://www.nuance.com/omnipage
5
http://www.abbyy.com
104
Contributed Task Rediscovering 50 Years of Discov-
eries (Sch?fer et al, 2012) is to serve as fall-back
source when the more precise PDFExtract method
(Berg et al, 2012) is not applicable.
2 Target Format
The focus of the Searchbench text extraction process
was to retrieve NLP-parsable sentences from scien-
tific papers. Hence distinguishing running text from
section headings, figure and table captions or foot-
notes was an important intermediate task.
PaperXML is a simple logical document markup
structure we specifically designed for scientific pa-
pers. It features tags for section headings (with spe-
cial treatment of abstract and references), footnotes,
figure and table captions. The full DTD is listed
in the Appendix. A sample document automatically
generated by our extraction tool is displayed in Fig-
ure 2 on the next page. In paperXML, figures are
ignored, but table layouts and character style infor-
mation such as boldface or italics are preserved.
3 Algorithm
Volk et al (2010) used two different OCR prod-
ucts (the above mentioned Omnipage and ABBYY)
and tried to improve the overall recognition accuracy
on scanned text by merging their outputs. This ap-
proach adds the challenge of having to decide which
version to trust in case of discrepancy. Unlike them,
we use a single OCR tool, ABBYY, but with two dif-
ferent output variants, layout and float, that in parts
contain complementary information. As no direct
XML output mode exists, we rely on HTML output
that can also be used to render PDF text extraction
results in a Web browser.
3.1 Core rich text and document structure
extraction
Our algorithm uses the layout variant as primary
source. Layout tries to render the extracted text as
closely as possible to the original layout. It pre-
serves page breaks and the two-column formatting
that most ACL Anthology papers (except the CL
Journal and some older proceedings) share.
In the float variant, page and line breaks as well
as multiple column layout are removed in favour of a
running text in reading order which is indispensable
for our purposes. However, some important layout-
specific information such as page breaks is not avail-
able in the float format. Both variants preserve table
layouts and character style information such as bold-
face or italics. Reading order in both variants may
differ. A special code part ensures that nothing is
lost when aligning the variants.
We implemented a Python6 module that reads
both HTML variants and generates a consolidated
XML condensate, paperXML. It interprets textual
content, font and position information to identify the
logical structure of a scientific paper.
Figure 1: PDF-to-paperXML workflow
Figure 1 depicts the overall workflow. In addi-
tion to the two HTML variants, the code also reads
BIBTEX metadata in XML format of each paper.
A rather large part in the document header of the
generated paperXML addresses frontpage and bib-
liographic metadata. Section 3.2 explains why and
how this information is extracted and processed.
Using XSLT7, paperXML is then transformed into
a tab-separated text file that basically contains one
sentence per line plus additional sentence-related
6
http://www.python.org
7
http://www.w3.org/TR/xslt
105
<?xml version="1.0" encoding="UTF-8"?>
<article>
<header>
<firstpageheader>
<page local="1" global="46"/>
<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<pubinfo>Proceedings ofACL-08: HLT,pages 46-54, Columbus, Ohio, USA, June 2008. ?2008 Association [...]</pubinfo>
<author surname="Miyao" givenname="Yusuke">
<org name="University of Tokyo" country="Japan" city="Tokyo"/>
</author>
[...]
</firstpageheader>
<frontmatter>
<p><b>Task-oriented Evaluation of Syntactic Parsers and Their Representations</b></p>
<p><b>Yusuke Miyao<footnote anchor="1"/>" Rune Saetre<footnote anchor="1"/>" Kenji Sagae
<footnote anchor="1"/>" Takuya Matsuzaki<footnote anchor="1"/>" Jun?ichi Tsujii<footnote anchor="1"/>"** </b>
^Department of Computer Science, University of Tokyo, Japan * School of Computer Science, University of Manchester,
UK *National Center for Text Mining, UK</p>
<p>{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp</p>
</frontmatter>
<abstract>This paper presents a comparative evaluation of several state-of-the-art English parsers [...]</abstract>
</header>
<body>
<section number="1" title="Introduction">
<p>Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are
no longer limited to PCFG-based frameworks (Charniak, 2000; [...]</p>
</section>
<section number="2" title="Syntactic Parsers and Their Representations">
<p>This paper focuses on eight representative parsers that are classified into three parsing frameworks:
<i>dependency parsing, phrase structure parsing, </i>and <i>deep parsing.</i> [...] </p>
<subsection number="2.1" title="Dependency parsing">
<p>Because the shared tasks of CoNLL-2006 and CoNLL-2007 focused [...] </p>
<p><b>mst </b>McDonald and Pereira (2006)?s dependency parser,<footnote anchor="1"/> based on the Eisner
algorithm for projective dependency parsing (Eisner, 1996) with the second-order factorization.</p>
<footnote label="1">http://sourceforge.net/projects/mstparser</footnote>
<figure caption="Figure 1: CoNLL-X dependency tree"/>
</subsection>
[...]
<subsection number="4.2" title="Comparison of accuracy improvements">
<p>Tables 1 and 2 show the accuracy [...] </p>
[...]
<p>While the accuracy level of PPI extraction is the similar for the different parsers, parsing speed differs significantly.
<page local="7" global="52"/> The dependency parsers are much faster than the other parsers, [...] </p>
<table caption="Table 1: Accuracy on the PPI task with WSJ-trained
parsers (precision/recall/f-score)" class="main" frame="box" rules="all" border="1" regular="False">
<tr class="row"> [...]
</table>
<section title="Acknowledgments">
<p>This work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan) [...]</p>
</section>
<references>
<p>D. M. Bikel. 2004. Intricacies of Collins? parsing model. <i>Computational Linguistics, </i>30(4):479-511.</p>
<p>T. Briscoe and J. Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC [...]</p>
[...]
</references>
</body>
</article>
Figure 2: An example of an automatically generated paperXML version of the ACL Anthology document P08-1006.
Parts are truncated ([. . . ]) and some elements are imbalanced for brevity.
106
characteristics such as type (paragraph text, head-
ing, footnote, caption etc.) page and offset. This
output format is used to feed NLP components such
as taggers, parsers or term extraction for the Search-
bench?s index generation. On the right hand side of
the diagram, we sketch a potentional transformation
of paperXML into TEI P5 for the Constributed Task.
It will be discussed in Section 4.
The extraction algorithm initially computes the
main font of a paper based on the number of char-
acters with the same style. Based on this, heuris-
tics allow to infer styles for headings, footnotes etc.
While headings typically are typeset in boldface in
recent publications, old publications styles e.g. use
uppercase letters with or without boldface.
On the basis of this information, special section
headings such as abstract, and references are
inferred. Similarly, formatting properties in com-
bination with regular expressions and Levenshtein
distance (Levenshtein, 1966) are used to identify
footnotes, figure and table captions etc. and
generate corresponding markup.
A special doubt element is inserted for text frag-
ments that do not look like normal, running text.
3.2 Bibliographic metadata and author
affiliations
Conference or publication information can often be
found on the first page footer or header or (in case of
the CL journal) on every page. Our code recognizes
and moves it to dedicated XML elements. The aim
is not to interrupt running text by such ?noise?.
Publication authors, title and conference informa-
tion as well as page number and PDF URL is com-
monly named bibliographic metadata. Because this
information was partly missing in the ACL Anthol-
ogy, special care was taken to extract it from the
papers. In the paperXML generation code, author
affiliations from the title page are mapped to au-
thor names using gazetteers, position information,
heuristics etc. as part of the paperXML generation
process. This experimental approach is imperfect,
leads to errors and would definitely require man-
ual correction. A solution would be to use man-
ually corrected author affiliation information from
the ACL Anthology Reference Corpus (Bird et al,
2008). This information, however, is not immedi-
ately available for recent proceedings or journal ar-
ticles. Therefore, we developed a tool with a graph-
ical user interface that assists quick, manual correc-
tion of author affiliation information inferred from
previous publications of the same author in the An-
thology by means of the ACL ARC data.
Independently from the paperXML extraction
process, bibliographic metadata for each paper in the
ACL Anthology has been extracted from BIBTEX
files and, where BIBTEX was missing, the An-
thology index web pages. We semi-automatically
corrected encoding errors and generated easy-to-
convert BIBTEXML
8 files for each paper. Using
the page number information extracted during the
paperXML generation process, our code enriches
BIBTEXML files with page number ranges where
missing in the ACL Anthology?s metadata. This
is of course only possible for papers that contain
page numbers in the header or footer. The resulting
BIBTEXML metadata are available at DFKI?s pub-
lic SubVersioN repository9 along with the affiliation
correction tool.
4 Transformation to TEI P5
The ACL Contributed Task Rediscovering 50 Years
of Discoveries (Sch?fer et al, 2012) proposes to use
TEI P510 as an open standard for document struc-
ture markup. The overall structure of paperXML
is largely isomorphic to TEI P5, with minor differ-
ences such as in the position of page break markup.
In paperXML, page break markup is inserted after
the sentence that starts before the page break, while
in TEI P5, it appears exactly where it was in the orig-
inal text, even within a hyphenated word.
The Python code that generates paperXML could
be modified to make its output conforming to TEI.
Alternatively, transformation of paperXML into the
TEI format could be performed using XSLT. Ta-
ble 1 summarizes mapping of important markup ele-
ments. Details of the element and attribute structure
differ, which makes a real mapping more compli-
cated than it may seem from the table.
8
http://bibtexml.sourceforge.net
9
http://aclbib.opendfki.de
10
http://www.tei-c.org/Guidelines/P5
107
TEI element paperXML element
TEI article
teiHeader header
author (unstructured) author (structured)
title title
div type="abs" abstract
front header/abstract
body body
back (no correspondance)
div type="ack" section title=
"Acknowledgments"
div type="bib" references
p p
head section title="..."
hi rend="italic" i
hi rend="bold" b
hi rend="underline" u
del type="lb" - (Unicode soft hyphen)
pb n="52" page local="7"
global="52"
table table
row tr
cell td
Table 1: Element and attribute mapping (incomplete) be-
tween paperXML and TEI P5
5 Summary and Outlook
We have described a pragmatic and robust solu-
tion for generating logical document markup from
scholarly papers in PDF format. It is meant as
an OCR-based fall-back solution in the ACL Con-
tributed Task Rediscovering 50 Years of Discoveries
(Sch?fer et al, 2012) when the more precise PDFEx-
tract method (Berg et al, 2012) is not applicable
because it can only handle born-digital PDF docu-
ments. Moreover, the approach can serve as fall-
back solution where PDFExtract fails or does not
produce markup (e.g. currently tables). Our solution
has been shown to work even on typewriter-typeset,
scanned papers from the 60ies. Correctness of the
produced markup is limited by heuristics that are
necessary to select at markup and layout borders, re-
construct reading order, etc. Levenshtein distance is
used at several places in order to cope with variants
such as those induced by character recognition er-
rors. The approach is implemented to produce XML
documents conforming to the paperXML DTD that
in turn could be transformed to TEI P5 using XSLT.
Acknowledgments
This work has been funded by the German Fed-
eral Ministry of Education and Research, projects
TAKE (FKZ 01IW08003) and Deependance (FKZ
01IW11003).
References
Berg, ?. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.
Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
Corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proceed-
ings of the sixth international conference on lan-
guage resources and evaluation (LREC-08). Mar-
rakech, Morocco.
Levenshtein, V. I. (1966). Binary codes capable
of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8), 707?710.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, OR.
Sch?fer, U., Read, J., & Oepen, S. (2012). Towards
an ACL Anthology corpus with logical document
structure. An overview of the ACL 2012 con-
tributed task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Volk, M., Marek, T., & Sennrich, R. (2010). Reduc-
ing OCR errors by combining two OCR systems.
In ECAI-2010 workshop on language technology
for cultural heritage, social sciences, and human-
ities (pp. 61?65). Lisbon, Portugal.
Weitz, B., & Sch?fer, U. (2012). A graphical cita-
tion browser for the ACL Anthology. In Proceed-
ings of the eighth international conference on lan-
guage resources and evaluation LREC-2012 (pp.
1718?1722). Istanbul, Turkey: ELRA.
108
Appendix: paperXML DTD
<!-- paperXML DTD second version as of
2009-10-16 Ulrich.Schaefer@dfki.de -->
<!ELEMENT article (header, body) >
<!ELEMENT header (file?, pdfmetadata?,
ocrmetadata?, firstpageheader,
frontmatter?, abstract) >
<!ELEMENT pdfmetadata (meta)* >
<!ELEMENT ocrmetadata (meta)* >
<!ELEMENT meta EMPTY >
<!ATTLIST meta name CDATA #REQUIRED
content CDATA #REQUIRED >
<!ELEMENT firstpageheader (page, title,
subtitle?, pubinfo?, author*) >
<!ELEMENT frontmatter (p)* >
<!ELEMENT title (#PCDATA) >
<!ELEMENT subtitle (#PCDATA) >
<!ELEMENT pubinfo (#PCDATA) >
<!ELEMENT author (#PCDATA | org)* >
<!ATTLIST author surname CDATA #IMPLIED
middlename CDATA #IMPLIED
givenname CDATA #IMPLIED
address CDATA #IMPLIED
email CDATA #IMPLIED
homepage CDATA #IMPLIED >
<!ELEMENT org EMPTY >
<!ATTLIST org name CDATA #IMPLIED
country CDATA #IMPLIED
city CDATA #IMPLIED >
<!ELEMENT abstract (#PCDATA | b | i | u |
footnote)* >
<!ELEMENT body (section*, references?,
appendix*) >
<!ELEMENT section (subsection | p | footnote |
table | figure | page | doubt)* >
<!ATTLIST section number CDATA #IMPLIED
title CDATA #REQUIRED >
<!ELEMENT subsection (subsubsection | p | table|
footnote | table | figure | doubt)* >
<!ATTLIST subsection number CDATA #IMPLIED
title CDATA #REQUIRED >
<!ELEMENT subsubsection (p | footnote | table |
figure | page | doubt)* >
<!ATTLIST subsubsection number CDATA #IMPLIED
title CDATA #REQUIRED >
<!ELEMENT references (p | footnote | page |
doubt)* >
<!ELEMENT appendix (p | footnote | table |
figure | page | doubt)* >
<!ATTLIST appendix number CDATA #IMPLIED
title CDATA #REQUIRED >
<!ELEMENT p (#PCDATA | page | b | i | u |
footnote)* >
<!ELEMENT page EMPTY >
<!ATTLIST page local CDATA #REQUIRED
global CDATA #IMPLIED >
<!-- boldface -->
<!ELEMENT b (#PCDATA | i | u | footnote)* >
<!-- italics -->
<!ELEMENT i (#PCDATA | b | u | footnote)* >
<!-- underlined -->
<!ELEMENT u (#PCDATA | i | b | footnote)* >
<!ELEMENT footnote (#PCDATA) >
<!ATTLIST footnote label NMTOKEN #IMPLIED
anchor NMTOKEN #IMPLIED >
<!-- text that is probably not sentential -->
<!ELEMENT doubt (#PCDATA) >
<!ATTLIST doubt alpha CDATA #REQUIRED
length CDATA #REQUIRED
tooSmall CDATA #REQUIRED
monospace CDATA #REQUIRED >
<!ELEMENT figure (#PCDATA | p)* >
<!ATTLIST figure caption CDATA #IMPLIED >
<!-- rest is HTML-like table markup -->
<!ELEMENT table (tr)* >
<!ATTLIST table caption CDATA #IMPLIED
class CDATA #IMPLIED
frame CDATA #IMPLIED
rules CDATA #IMPLIED
border CDATA #IMPLIED
regular CDATA #IMPLIED >
<!ELEMENT tr (td)* >
<!ATTLIST tr class CDATA #IMPLIED >
<!ELEMENT td (p)* >
<!ATTLIST td class CDATA #IMPLIED
rowspan CDATA #IMPLIED
colspan CDATA #IMPLIED >
109
