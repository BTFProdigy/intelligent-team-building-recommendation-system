95
96
97
98
Evaluation and Improvement
of Cross-Lingual Question Answering Strategies
Anne-Laure Ligozat and Brigitte Grau and Isabelle Robba and Anne Vilnat
LIMSI-CNRS
91403 Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
This article presents a bilingual question
answering system, which is able to process
questions and documents both in French
and in English. Two cross-lingual strate-
gies are described and evaluated. First, we
study the contribution of biterms trans-
lation, and the influence of the comple-
tion of the translation dictionaries. Then,
we propose a strategy for transferring the
question analysis from one language to the
other, and we study its influence on the
performance of our system.
1 Introduction
When a question is asked in a certain language
on the Web, it can be interesting to look for the
answer to the question in documents written in
other languages in order to increase the number of
documents returned. The CLEF evaluation cam-
paign for cross-language question answering sys-
tems addresses this issue by encouraging the deve-
lopment of such systems.
The objective of question answering systems
is to return precise answers to natural-language
questions, instead of the list of documents usually
returned by a search engine. The opening to mul-
tilingualism of question answering systems raises
issues both for the Information Retrieval and the
Information Extraction points of view.
This article presents a cross-language question
answering system able to treat questions and docu-
ments either in French or in English. Two different
strategies for shifting language are evaluated, and
several possibilities of evolution are presented.
2 Presentation of our question answering
system
Our bilingual question answering system has
participated in the CLEF 2005 evaluation cam-
paign 1. The CLEF QA task aims at evaluating dif-
ferent question answering systems on a given set
of questions, and a given corpus of documents, the
questions and the documents being either in the
same language (except English) or in two diffe-
rents languages. Last year, our system participated
in the French to English task, for which the ques-
tions are in French and the documents to search in
English.
This system is composed of several modules
that are presented Figure 1. The first module ana-
lyses the questions, and tries to detect a few of
their characteristics, that will enable us to find the
answers in the documents. Then the collection is
processed thanks to MG search engine 2. The do-
cuments returned are reindexed according to the
presence of the question terms, and more preci-
sely to the number and type of these terms ; next,
a module recognizes the named entities, and the
sentences from the documents are weighted accor-
ding to the information on the question. Finally,
different processes are applied depending on the
expected answer type, in order to extract answers
from the sentences.
3 Cross-language strategies for question
answering systems
Two main approaches are possible to deal with
multilingualism in question answering systems :
1Multilingual Question Answering task at the Cross Lan-
guage Evaluation Forum, http ://clef-qa.itc.it/
2MG for Managing Gigabytes
http ://www.cs.mu.oz.au/mg/
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
23
English
terms
Fusion
English
answers
English
questions
   
Collection
French
questions
   Selection
   Named entity tagging
Answer extraction
   Reindexing and ranking
   Sentence weighting
Document processing
   Answer extraction
English
   Focus 
   Answer type
   Semantically linked words
   Main verb
   Terms
   Syntactic relations
Question analysis
translation
answers
2 lists of ranked
                  (a)
                 (b)
Search
engine
FIG. 1 ? Architecture of our cross-language question answering system
question translation and term-by-term translation.
These approaches have been implemented and
evaluated by many systems in the CLEF evalua-
tions, which gives a wide state-of-the-art of this
domain and of the possible cross-language strate-
gies.
The first approach consists in translating the
whole question into the target language, and then
processing the question analysis in this target lan-
guage. This approach is the most widely used, and
has for example been chosen by the following sys-
tems : (Perret, 2004), (Jijkoun et al, 2004), (Neu-
mann and Sacaleanu, 2005), (de Pablo-Sa?nchez et
al., 2005), (Tanev et al, 2005). Among these sys-
tems, several have measured the performance loss
between their monolingual and their bilingual sys-
tems. Thus, the English-French version of (Perret,
2004) has a 11 % performance loss (in terms of ab-
solute loss), dropping from 24.5% to 13.5% of cor-
rect answers. The English-Dutch version of (Jij-
koun et al, 2004)?s system has an approximative
10% performance loss of correct answers : the per-
centage of correct answers drops from 45.5% to
35%. As for (de Pablo-Sa?nchez et al, 2005), they
lose 6% of correct answers between their Spanish
monolingual system and their English-Spanish bi-
lingual system. (Hartrumpf, 2005) also conducted
an experiment by translating the questions from
English to German, and reports a drop from about
50% of performance.
For their cross-language system, (Neumann and
Sacaleanu, 2004) chose to use several machine
translation tools, and to gather the different trans-
lations into a ?bag of words? that is used to ex-
pand queries. Synonyms are also added to the
?bag of words? and EuroWordNet 3 is used to
3Multilingual database with wordnets for several Euro-
disambiguate. They lose quite few correct ans-
wers between their German monolingual system
and their German-English bilingual system, with
which they obtain respectively 25 and 23.5% of
correct answers.
Translating the question raises two main pro-
blems : syntactically incorrect questions may be
produced, and the resolution of translation am-
biguities may be wrong. Moreover, the unknown
words such as some proper names are not or in-
correctly translated. We will describe later several
possibilities to deal with these problems, as well
as our own solution.
Other systems such as (Sutcliffe et al, 2005) or
(Tanev et al, 2004) use a term-by-term translation.
In this approach, the question is analyzed in the
source language and then the information retur-
ned by the question analysis is translated into the
target language. (Tanev et al, 2004), who partici-
pated in the Bulgarian-English and Italian-English
tasks in 2004, translate the question keywords by
using bilingual dictionaries and MultiWordNet 4.
In order to limit the noise stemming from the dif-
ferent translations and to have a better cohesion,
they validate the translations in two large cor-
pora, AQUAINT and TIPSTER. This system got a
score of 22.5% of correct answers in the bilingual
task, and 28% in the monolingual task in 2004.
(Sutcliffe et al, 2005) combine two translation
tools and a dictionary to translate phrases. Even-
tually, (Laurent et al, 2005) also translate words
or idioms, by using English as a pivot language.
The performance of this system is of 64% of cor-
rect answers for the French monolingual task, and
pean languages, http ://www.illc.uva.nl/EuroWordNet/
4Multilingual lexical database in which the Italian Word-
Net is strictly aligned with Princeton WordNet, http ://multi-
wordnet.itc.it
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
24
39.5% for the English-French bilingual task.
4 Adopted approach
In order to deal with the conversion from French
to English in our system, two strategies are ap-
plied in parallel. They differ on what is translated
to treat the question asked in French. The first sub-
system called MUSQAT proceeds to the question
analysis in French, and then translates the ques-
tion terms extracted by this question analysis mo-
dule, following the - - - arrows in Figure 1. The
second sub-system makes use of a machine trans-
lation tool (Reverso 5) to obtain translations of the
questions and then our English monolingual sys-
tem called QALC is applied, following the ..-.. ar-
rows in Figure 1 . These strategies will be detailed
later in the article.
If they represent the most common strategies for
this kind of task, an original feature of our system
is the implementation of both strategies, which en-
ables us to merge the results obtained by following
these strategies, in order to improve the global per-
formance of our system.
In Table 1, we present an analysis of the results
we obtained for the CLEF evaluation campaign.
We evaluate the results obtained at two different
points of the question-answering process, i.e. af-
ter the sentence selection (point (a) in Figure 1),
and after the answer extraction (point (b) in Fi-
gure 1). At point (a), we count how many ques-
tions (among the global evaluation set of 200 ques-
tions) have an appropriate answer in the first five
sentences. At point (b), we distinguish the answers
the analysis process labels as named entities (NE),
from the others, since the corresponding answe-
ring processes are different. We also detail how
many answers are ranked first, or in the first five
ranks, as we take into account the first five ans-
wers.
As illustrated in Table 1, the two strategies for
dealing with multilingualism give quite different
results, which can be explained by each strategy
characteristics.
MUSQAT proceeds to the question analysis
with French questions correctly expressed, and
which analysis is therefore more reliable. Yet, the
terms translations are then obtained from every
possible translation of each term, and thus without
taking account any context ; moreover, they de-
pend on the quality of the dictionaries used, and
5http ://www.reverso.net/
MUSQAT Reverso
+QALC
% %
(a) : Sentences first 5 41 46
with an answer ranks
(b) : Correct rank 1 18 14
NE answers
first 5 26 17
ranks
(b) : Correct rank 1 16 13
other answers
first 5 23 20
ranks
(b) : Total rank 1 17 13
(NE + non NE)
first 5 24 19
ranks
Final result 19
(fusion of both strategies)
TAB. 1 ? Performance of our system in CLEF
2005
introduce noise because of the erroneous transla-
tions.
In MUSQAT, we do not only translate mono-
terms (i.e. terms composed of single word) : the
biterms (composed of two words) of the French
questions are also extracted by the question analy-
sis. Every sequence of two terms which are tagged
as adjective/common noun or proper noun/proper
noun... constitutes a biterm. Each word of the bi-
term is translated, and then the existence of the
corresponding biterm built in English is checked
in the corpus. The biterms thus obtained are then
used by the further modules of the system. Taking
biterms into account is useful since they provide
a minimal context to the words forming them, as
well for the translation as for the re-indexing and
re-ranking of the documents (see Figure 1), as ex-
plained in (Ferret et al, 2002). Moreover, the pre-
sence of the biterm translations in the corpus is a
kind of validation of the monoterms translations.
As for translating the question, which is imple-
mented by Reverso+QALC, it presents the advan-
tage of giving a unique translation of the question
terms, which is quite reliable. But the grammati-
cality or realism of the question are not assured,
and thus the question analysis, based on regular
expression patterns, can be disturbed.
In this work, we tried to evaluate each strategy
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
25
and to bypass their drawbacks : on the one hand
(Section 5), by examining how the biterm transla-
tion in MUSQAT could be more reliable, and on
the other hand (Section 6) by improving the ques-
tion analysis, by relying on the French questions,
for QALC.
5 Biterm translation
The translation of terms and biterms present in
the question is achieved using two dictionaries.
The first of them, which was used last year for
our participation to CLEF is Magic-Dic 6. It is a
dictionary under GPL licence, which was retained
for its capacity to evolve. Indeed users can sub-
mit new translations which are controlled before
being integrated. Yet, it is quite incomplete. This
year we used FreeDict as well (FreeDict is also un-
der GPL licence), to fill in the gaps of Magic-Dic.
FreeDict added 424 translations to the 690 terms
already obtained. By mixing both sets of transla-
tions we obtained 463 additional biterms, making
a total of 777 biterms.
Nevertheless, whatever the quality and the size
of the dictionaries are, the problem of biterm trans-
lation remains the same : since biterms are not in
the dictionaries, the only way for us to get their
translation is to combine all the different term
translations. The main drawback of this approach
is the generated noise, for none of the terms consti-
tuting the biterm is disambiguated. For example,
three different translations are found for the bi-
term Conseil de de?fense : defense council, defense
advice and defense counsel ; but only the first of
those should be finally retained by our system.
To reduce this noise, an interesting possibility is
to validate the obtained biterms by searching them
or their variants in the complete collection of do-
cuments. (Grefenstette, 1999) reports a quite simi-
lar experiment in the context of a machine trans-
lation task : he uses the Web in order to order the
possible translations of noun phrases, and in par-
ticular noun biterms. Fastr (Jacquemin, 1996) is
a parser which takes as input a corpus and a list
of terms (multi or monoterms) and outputs the in-
dexed corpus in which terms and their variants are
recognized. Hence, Fastr is quite adequate for bi-
terms validation : it tags all the biterms present in
the collection, whether in their original form or in
a variant that can be semantic or syntactic.
In order to validate the biterms, the complete
6http ://magic-dic.homeunix.net
collection of the CLEF campaign (500 Mbyte) was
first tagged using the TreeTagger, then Fastr was
applied. The results are presented Table 2 : 39.5%
of the 777 biterms were found in the collection, in
a total of 63,404 occurrences. Thus there is an ave-
rage of 206 occurrences for each biterm. If we do
not take into account the biterm which is the most
represented (last year with 30,981 occurrences),
this average falls to 105. The 52 biterms which are
found in their original form only are most of the
time names of persons. Lastly, biterms that are ne-
ver found in their original form, are often consti-
tuted of one term badly translated, for example the
biterm oil importation is not present in the collec-
tion but its variant import of oil is found 28 times.
Then, it may be interesting to replace these biterms
by the most represented of their variants.
Whenever a biterm is thus validated (found in
the collection beyond a chosen threshold), the
translation of its terms is itself validated, other
translations being discarded. Thus, biterm valida-
tion enables us to validate monoterm translations.
Then, the following step will be to evaluate how
this new set of terms and biterms improves the re-
sults of MUSQAT.
After CLEF 2005 evaluation, we had at our dis-
posal the set of questions in their English original
version (this set was provided by the organizers).
We had also the English translation (far less cor-
rect) provided by the automatic translator Reverso.
As we can see it Table 3, for each set of ques-
tions the number of terms and biterms is nearly
the same. In the set of translations given by Re-
verso, we manually examined how many biterms
were false and found that here again the figures
were close to those of the original version. There
are two main reasons for which a biterm may be
false :
? in two thirds of cases, the association itself is
false : the two terms should not have been as-
sociated ; it is the case for example of many
country from the question How many coun-
tries joined the international coalition to res-
tore the democratic government in Haiti ? 7
? in one third of cases, one of the terms is
not translated or translated with an erroneous
term, like movement zapatiste coming from
the question What carry the courtiers of the
movement zapatiste in Mexico ? 8
7This sentence is an example of very good translation gi-
ven by Reverso
8This sentence is an example of bad translation given by
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
26
Total Number of biterms 777
Number of biterms found in the collection 307 - 39.5%
Number of biterms found in their original form only 52 - 17%
Number of biterms found with semantic variations only 150 - 54%
TAB. 2 ? Magic-Dic and FreeDict biterms validated by Fastr
Questions Questions Questions
in French translated in English in English
by Reverso (original version)
Terms 1180 1122 1163
Biterms 272 204 261
False Biterms 33 38 27
Common Biterms - 106
TAB. 3 ? Biterms in the different sets of questions
However, we calculated that among the 204 bi-
terms given by Reverso, 106 are also present in the
original set of questions in English. Among the 98
remaining biterms, 38 are false (for the reasons gi-
ven above). Then, there are 60 biterms which are
neither erroneous nor present in the original ver-
sion. Some of them contain a term which has been
translated using a different word, but that is never-
theless correct ; yet, most of these 60 biterms have
a different syntax from those constructed from the
original version, which is due to the syntax of the
questions translated by Reverso.
This leads us to conclude that even if Reverso
produces syntactically erroneous questions, the
vocabulary it chooses is most of the time adequate.
Yet, it is still interesting to use also the biterms
constructed from the dictionaries since they are
much more numerous and provide variants of the
biterms returned by Reverso.
6 Multilingual question analysis
We have developed for the evaluations a ques-
tion analysis in both languages. It is based on the
morpho-syntactic tagging and the syntactic analy-
sis of the questions. Then different elements are
detected from both analyses : recognition of the
expected answer type, of the question category, of
the temporal context...
There are of course lexicons and patterns which
are specific to each language, but the core of the
module is independent from the language. This
Reverso, which should have produced What do supporters of
the Zapatistas in Mexico wear ?
module was evaluated on corpora of similar ques-
tions in French and in English, and its results on
both languages are quite close (around 90% of re-
call and precision for the expected answer type
for example ; for more details, see (Ligozat et al,
2006)).
As presented above, our system relies on two
distinct strategies to answer to a cross-language
question :
? Either the question is analyzed in the ori-
ginal language, and next translated term-by-
term. The question analysis is then more re-
liable since it processes a grammatically cor-
rect question ; yet, the translation of terms has
no context to rely on.
? Or the question is first translated into the
target language before being analyzed. Al-
though this strategy improves the translation,
its main inconvenient is that each translation
error has strong consequences on the ques-
tion analysis. We will now try to evaluate to
which extent the translation errors actually
influence our question analysis and to find so-
lutions to avoid minimize this influence in the
Reverso+QALC system.
An error in the question translation can lead to
wrong terms or an incorrect English construction.
Thus, the translation of the question ?Combien y
a-t-il d?habitants en France ?? (?How many inhabi-
tants are there in France ??) is ?How much is there
of inhabitants in France ??.
In order to evaluate our second strategy, Re-
verso+QALC, using question translation and then
a monolingual system, it is interesting to estimate
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
27
the influence of a such a coarse translation on the
results of our system.
In order to avoid these translating problems, it
is possible to adapt either the input or the out-
put of the translating module. (Ahn et al, 2004)
present an example of a system processing pre-
and post-corrections thanks to surface reformu-
lation rules. However, this type of correction is
highly dependent on the kind of questions to pro-
cess, as well as on the errors of the translation tool
that is used.
We suggest to use another kind of processing,
which makes the most of the cross-lingual charac-
ter of the task, in order to improve the analysis of
the translated questions and to take into account
the possibilities of errors in these questions.
Our present system already takes into account
some of the most frequent translation errors, by
allowing the question analysis module to loosen
some of its rules in case the question be transla-
ted. Thus, a definition question such as ?Qu?est-
ce que l?UNITA ??, translated ?What UNITA ??
by our translating tool, instead of ?What is the
UNITA ??, will nevertheless be correctly analyzed
by our rules : indeed, the pattern WhatGN will be
considered as corresponding to a definition ques-
tion, while on a non-translated question, only the
pattern WhatBeGN will be allowed.
In order to try and improve our processing of
approximations in the translated questions, the so-
lution we suggest here consists in making the
question analysis in both the source and the target
languages, and in reporting the information (or at
least part of it) returned by the source analysis into
the target analysis. This is possible first because
our system treats both the languages in a parallel
way, and second, some of the information retur-
ned by the question analysis module use the same
terms in English and in French, like for example
the question category or the expected Named En-
tity type.
More precisely, we propose, in the task with
French questions and English documents, to ana-
lyse the French questions, and their English trans-
lations, and then to report the question category
and the expected answer type of the French ques-
tions into the English question analysis. The in-
formation found in the source language should be
more reliable since obtained on a real question.
For example, for the question ?Combien de
communaute?s Di Mambro a-t-il cre?e ?? (?How
many communities has Di Mambro created ??),
Reverso?s translation is ?How many Di Mambro
communities has he create ?? which prevents the
question analysis module to analyze it correctly.
The French analysis is thus used, which provides
the question category combien (how many) and the
expected named entity type NUMBER. This infor-
mation is reported in the English analysis file.
These characteristics of the question are used at
two different steps of the question answering pro-
cess : when selecting the candidate sentences and
when extracting the answers. Improving their re-
liability should then enable us to increase the num-
ber of correct answers after these two steps.
In order to test this strategy, we conducted an
experiment based on the CLEF 2005 FR-EN task,
and the 200 corresponding French questions. We
launched the question answering system on three
question files :
? The first question file (here called English
file) contained the original English questions
(provided by the CLEF organizers). This file
will be considered as a test file, since the re-
sults of our system on this file represent those
that would be reached without translation er-
rors.
? The second file (called Translated file) contai-
ned the translated questions analysis.
? The last file (called Improved file) contained
the same analysis, but for which the question
category and the expected answer type were
replaced by those of the French analysis.
Then we searched for the number of correct ans-
wers for each input question file after the sentence
selection and after the answer extraction. The re-
sults obtained by our system on each file are pre-
sented on Figure 2, Figure 3 and Figure 4. These
figures present the number of questions expecting
a named entity answer, expecting another kind of
answer, and the total number of questions, as well
as the results of our system on each type of ques-
tion : the number of correct questions are given at
the first five ranks, and at the first rank, first for the
sentences (?long answers?) and then for the short
answers.
These results show that the information trans-
fer from the source language to the target lan-
guage significantly improves the system?s results ;
the number of correct answers increases in every
case. It increases from 34 on the translated ques-
tions file to 36 on the improved file, and from 52
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
28
FIG. 2 ? QALC?s results (i.e. number of correct
answers) on the 200 questions
FIG. 3 ? Results on the named entities questions
FIG. 4 ? Results on the non named entities ques-
tions
to 55 for the first 5 ranks. These results are closer
to those of the monolingual system, which returns
41 correct answers at the first rank, and 59 on the
first 5 ranks.
It is interesting to see that the difference bet-
ween the monolingual and the bilingual systems
is less noticeable after the sentence selection step
than after the answer extraction step, which tends
to prove that the last step of our process is more
sensitive to translation errors. Moreover, this expe-
riment shows that this step can be improved thanks
to an information transfer between the source and
the target languages. In order to extend this stra-
tegy, we could also match each French question
term to its English equivalent, in order to trans-
late all the information given by the French analy-
sis into English. Thus, the question analysis errors
would be minimized.
7 Conclusion
The originality of our cross-language question
answering system is to use in parallel the two
most widely used strategies for shifting language,
which enables us to benefit from the advantages
of each strategy. Yet, each method presents draw-
backs, that we tried to evaluate in this article, and
to bypass.
For the term-by-term translation, we make the
most of the question biterms in order to restrict the
possible translation ambiguities. By validating the
biterms in the document collection, we have im-
proved the quality of both the biterms and the mo-
noterms translations. We hope this improvement
will lead to a better selection of the candidate sen-
tences from the documents.
For the question translation, we use the infor-
mation deduced from the source language to avoid
the problems coming from a bad or approximative
translation. This strategy enables us to solve some
of the problems coming from non-grammatical
translations ; matching each term of the French
question with its English equivalent would enable
us to transfer all the information of the French ana-
lysis. But the disambiguation errors of the transla-
tion remain.
References
Kisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Del-
mas, Jochen L. Leidner, and Matthew B. Smillie.
2004. Cross-lingual question answering with QED.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
29
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 335?342, Bath, UK.
Ce?sar de Pablo-Sa?nchez, Ana Gonza?lez-Ledesma,
Jose? Luis Mart??nez-Ferna?ndez, Jose? Maria Guirao,
Paloma Martinez, and Antonio Moreno. 2005.
MIRACLE?s 2005 approach to cross-lingual ques-
tion answering. In Working Notes, CLEF Cross-
Language Evaluation Forum, Vienna, Austria.
Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet,
Gabriel Illouz, Christian Jacquemin, Laura Mon-
ceaux, Isabelle Robba, and Anne Vilnat. 2002. How
NLP can improve question answering. Knowledge
Organization, 29(3-4).
Gregory Grefenstette. 1999. The world wide web as
a resource for example-based machine translation
tasks. In ASLIB Conference on Translating and the
Computer, volume 21, London, UK.
Sven Hartrumpf. 2005. University of Hagen at
QA@CLEF 2005 : Extending knowledge and dee-
pening linguistic processing for question answering.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Christian Jacquemin. 1996. A symbolic and surgical
acquisition of terms through variation. Connectio-
nist, Statistical and Symbolic Approaches to Lear-
ning for Natural Language Processing, pages 425?
438.
Valentin Jijkoun, Gilad Mishne, Maarten de Rijke, Ste-
fan Schlobach, David Ahn, and Karin Muller. 2004.
The University of Amsterdam at QA@CLEF2004.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 321?325, Bath, UK.
Dominique Laurent, Patrick Se?gue?la, and Sophie
Ne`gre. 2005. Cross lingual question answering
using QRISTAL for CLEF 2005. In Working Notes,
CLEF Cross-Language Evaluation Forum, Vienna,
Austria.
Anne-Laure Ligozat, Brigitte Grau, Isabelle Robba,
and Anne Vilnat. 2006. L?extraction des re?ponses
dans un syste`me de question-re?ponse. In Traitement
Automatique des Langues Naturelles (TALN 2006),
Leuven, Belgium.
Gu?nter Neumann and Bogdan Sacaleanu. 2004.
Experiments on robust NL question interpretation
and multi-layered doument annotation for a cross-
language question / answering system. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 311?320, Bath, UK.
Gu?nter Neumann and Bogdan Sacaleanu. 2005. DF-
KI?s LT-lab at the CLEF 2005 multiple language
question answering track. In Working Notes, CLEF
Cross-Language Evaluation Forum, Vienna, Aus-
tria.
Laura Perret. 2004. Question answering system for the
French language. In Working Notes, CLEF Cross-
Language Evaluation Forum, pages 295?305, Bath,
UK.
Richard F.E. Sutcliffe, Michael Mulcahy, Igal Gabbay,
Aoife O?Gorman, Kieran White, and Darina Slat-
tery. 2005. Cross-language French-English ques-
tion answering using the DLT system at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Hristo Tanev, Matteo Negri, Bernardo Magnini, and
Milen Kouylekov. 2004. The DIOGENE ques-
tion answering system at CLEF-2004. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 325?333, Bath UK.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2005. Exploiting
linguistic indices and syntactic structures for multi-
lingual question answering : ITC-irst at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
30
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 721?731, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generalizing Sub-sentential Paraphrase Acquisition
across Original Signal Type of Text Pairs
Aure?lien Max Houda Bouamor
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
This paper describes a study on the impact of
the original signal (text, speech, visual scene,
event) of a text pair on the task of both man-
ual and automatic sub-sentential paraphrase
acquisition. A corpus of 2,500 annotated sen-
tences in English and French is described, and
performance on this corpus is reported for
an efficient system combination exploiting a
large set of features for paraphrase recogni-
tion. A detailed quantified typology of sub-
sentential paraphrases found in our corpus
types is given.
1 Introduction
Sub-sentential paraphrases can be acquired from text
pairs expressing the same meaning (Madnani and
Dorr, 2010). If the semantic similarity of a text
pair has a direct impact on the quality of the ac-
quired paraphrases, it has, to our knowledge, never
been shown what impact the type of original sig-
nal has on paraphrase acquisition. In this work,
we consider four types of corpora, which we think
are representative of the main types of original
semantic signals: text pairs (roughly, sentences)
originating a) from independent translations of a
text (TEXT), b) from independent translations of a
speech (SPEECH), c) from independent descriptions
of a visual scene (SCENE), and d) from independent
descriptions of some event (EVENT). We will report
the results of experiments on sub-sentential para-
phrase acquisition on all these corpus types in two
languages, English and French, and provide some
answers to the following questions: What types of
paraphrases can be found by human annotators, with
what confidence and in which quantities? How well
can representative paraphrase acquisition systems
perform on each corpus type, and how performance
can be improved through combination? On what
corpus types can performance be improved by using
training material from other corpus types? Our ex-
perimental results will provide several indications of
the differences and complementarities of the corpus
types under study, and will notably show that perfor-
mance on the most readily available corpus type can
be improved by using training data from the set of
all other corpus types.
We will first describe the building procedures
and characteristics of our corpora (section 2), and
then describe our experimental settings for evalu-
ating paraphrase acquisition (section 3.1). Our ex-
periments will first consist of the description (sec-
tion 3.2) and evaluation (section 3.3) of a system
combination on each corpus type and then of our
system provided with additional training data from
the other corpus types (section 3.4). We will finally
briefly review related work (section 4) and discuss
our main findings and future work (section 5).
2 Collection of sentence pair corpora
In this study, we will focus on paraphrase acquisition
from related sentence pairs characteristic of 4 corpus
types, which correspond to different original signal
types of text pairs illustrated by the word alignment
matrices on Figure 1. A corpus for each type has
been collected for 2 languages, English and French,
and comprises 625 sentence pairs per language. We
now briefly describe how each corpus was built.
721
It
is
estimated
that
the
total
annual
volume
of
import
and
export
will
exceed
9
billion
US
dollars.
It is an
tic
ipa
ted
tha
t
the an
nu
al
tot
al
fo
rei
gn
tra
de
vo
lum
e
wi
ll
ex
ce
ed
US
$9
bil
lio
n
.
So
he
uses
the
photo
booths
to
remind
people
what
he
looks
like
.
He us
es
th
os
e
m
ac
hi
ne
s
to re
m
in
d
th
e
liv
in
g
of hi
s
fa
ce
.
a
boy
is
riding
on
a
bicycle
fast
.
a bo
y
rid
es
a bi
ke
on a di
rt
ro
ad
.
Pigeons
have
an
understanding
of
numbers
on
par
with
primates
Pi
ge
on
s
ha
ve
nu
m
er
ica
l
ab
ili
tie
s
ju
st
lik
e
pr
im
ate
s
Figure 1: Example reference alignment matrices for
(from top to bottom) TEXT, SPEECH, SCENE and
EVENT. Sure alignments appear in green or gray (identi-
ties) and possible alignments in yellow.
TEXT For English, we used the MTC corpus1 (de-
scribed in (Cohn et al2008)) consisting of sets
of news article translations from Chinese, and for
French the CESTA corpus2 consisting of sets of
news article translations from English. For each
sentence cluster, we selected sentence pairs with
minimal edit distance above an empirically-selected
threshold, covering all clusters first and then select-
ing from already used clusters to reach the target
number of sentence pairs.
e.g. It is estimated that the total annual volume of import
and export will exceed 9 billion US dollars. ? It is an-
ticipated that the annual total foreign trade volume will
exceed US$9 billion.
SPEECH For English, we used two freely avail-
able subtitle files3 of the French movies Le Fabuleux
Destin d?Ame?lie Poulain and Les Choristes, and for
French we used two subtitle files from the Desperate
Housewives TV series. We first aligned each paral-
lel corpus using the algorithm described in (Tiede-
mann, 2007), based on time frames and developed
for bilingual subtitles, we then filtered out sentence
pairs below a minimal edit distance threshold, and
manually removed obvious errors made by the algo-
rithm.
e.g. So he uses the photo booths to remind people what
he looks like. ? He uses those machines to remind the
living of his face.
SCENE We used the Multiple Video Description
Corpus (Chen and Dolan, 2011) obtained from mul-
tiple descriptions of short videos. Similarly to what
we did for TEXT, we selected sentence pairs from
clusters by minimal edit distance above a threshold.
An important fact is that for English we were able
to use what is described as ?verified? descriptions.
There were, however, far fewer descriptions avail-
able for French, and none had the ?verified? status.
We decided to use this corpus nonetheless, but with
the knowledge that this source for French is of a sub-
stantially lower quality (this corpus type will there-
fore appear as ?(SCENE)? in all tables to reflect this).
e.g. a boy is riding on a bicycle fast. ? a boy rides a bike
on a dirt road.
1http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T01
2http://www.elda.org/article125.html
3http://www.opensubtitles.org
722
Corpus statistics Annotator agreements Tokens in paraphrase statistics
500 sentence pairs 50 sentence pairs not considering identity paraphrases
sure para. possible para.
# tokens # tokens per sent. sure para. possible para. % tokens # tokens % tokens # tokens
ENGLISH
TEXT 21,473 21.0 66.1 20.4 18.6 4004 12.3 2651
SPEECH 11,049 10.5 79.1 10.9 17.5 1942 31.6 3500
SCENE 7,783 7.5 80.5 35.2 10.9 851 14.0 1094
EVENT 8,609 8.0 65.3 20.5 17.5 1506 14.5 1251
FRENCH
TEXT 24,641 24.0 64.6 16.6 29.2 7218 6.2 1527
SPEECH 11,850 11.5 82.7 20.8 22.5 2667 16.7 1981
(SCENE) 7,012 6.5 42.8 9.3 3.9 275 9.4 664
EVENT 9,121 9.1 67.8 3.8 19.6 1793 9.6 876
Table 1: Description of all corpora and paraphrase reference sets for English (top) and French (bottom). Note that
SCENE for French appears within parentheses as we do not consider it of the same quality as the other corpora.
EVENT We used titles of news article clusters
from the Google News4 news aggregation service.
We further refined the clustering algorithm by filter-
ing out article pairs whose publication dates differed
from more than one day. We repeated the same se-
lection procedure as for TEXT and SCENE to have
a maximal cluster coverage and select more similar
pairs first.
e.g. Pigeons Have an Understanding of Numbers on Par
With Primates ? Pigeons Have Numerical Abilities Just
Like Primates
Table 1 provides various statistics for these cor-
pora. The first observation is that TEXT contains sig-
nificantly larger sentences than the other types, more
than twice as long as those of SPEECH. Annotation
was performed following the guidelines proposed by
Cohn et al2008)5 using the YAWAT tool (Germann,
2008), except that alignments where not initially ob-
tained automatically so as not to bias our annota-
tors? work (there were two annotators per language).
The main guidelines that they had to follow were
that sure and possible paraphrases must be distin-
guished, smaller alignments were to be prefered but
any-to-any alignments may be used, and sentences
should be aligned as much as possible. Henceforth,
we will only consider for all reported statistics and
experiments those paraphrases that are not identity
pairs (e.g. (a nice day ? a nice day)), as they are
4http://news.google.com
5See http://staffwww.dcs.shef.ac.uk/
people/T.Cohn/paraphrase_guidelines.pdf
considered trivial as far as acquisition is concerned.
Table 1 also reports inter-annotator agreement6
values computed on sets of 50 sentence pairs. We
find that acceptable values are obtained for sure
paraphrases, but that low values are obtained for
possible paraphrases. This was somehow expected,
given the many possible interpretations of possible
paraphrases, but was not a problem for our experi-
ments: as we will describe in section 3.1, the evalua-
tion metrics we use will not count them as expected
solutions, but will simply not count them as false
when proposed as candidates.
Table 1 finally shows proportions and absolute
numbers of paraphrases of each type for all corpora.
We find that there are approximately the same to-
tal number of paraphrases for English (16,799) and
French (17,001), but that English corpora collec-
tively have an equivalent number of sure and pos-
sible paraphrases (8,303 vs. 8,496) and French have
more sure paraphrases (11,953 vs. 5,048). This may
be explained by the fact that our annotators worked
independently and that the corpora used have dif-
ferences by nature, as our experiments will show.
Other salient results include the fact that TEXT con-
tains more sure paraphrases in number than the other
corpora, that SPEECH contains relatively more pos-
sible paraphrases than the other corpora, and that
SCENE has significantly fewer paraphrases, both in
proportion and number. In Figure 2 various mea-
6For each paraphrase type, we used the average of recall
values obtained for each annotator set as the reference .
723
synonymy typography tense inclusion pragmatics syntax morphology number
ENGLISH
TEXT 51.2 7.6 5.1 12.1 0.6 4.4 12.1 6.4
SPEECH 39.8 25.6 3.5 12.3 1.7 3.5 3.5 9.7
SCENE 50.0 1.3 13.5 21.6 0.0 1.3 5.4 6.7
EVENT 36.9 15.0 8.2 19.1 1.3 6.8 6.8 5.4
FRENCH
TEXT 46.9 9.0 8.7 2.1 3.6 6.6 3.0 19.8
SPEECH 45.5 14.2 8.0 8.0 2.6 11.6 3.5 6.2
(SCENE) 46.4 5.3 3.5 8.9 0.0 5.3 0.0 30.3
EVENT 28.3 19.7 6.1 16.0 7.4 8.6 7.4 6.1
Table 2: Percentages of paraphrase classes in 50 randomly selected sentence pairs for reference paraphrases for English
(top) and French (bottom). Classes are illustrated by the following examples: (mutual understanding ? consensus)
(synonymy), (California ? CA) (typography), (letting ? having let) (tense), (Asian Development Bank ? Asian
Bank) (inclusion), (police dispatcher ? woman) (pragmatics), (grief-stricken ? struck with grief ) (syntactic), (Viet-
name ? Vietnam) (morphology), (mortgage ? mortgages) (number).
sures of sentence pair similarities are given. TEXT
contains the most similar sentence pairs according to
all metrics, with EVENT at a similar level on French.
SCENE has sentence pairs that are more similar than
those in SPEECH for English, but this is not the case
for French. While the metrics used can only provide
a crude account of semantic equivalence at the sen-
tence level, these results clearly indicate that trans-
lating from text yields more similar sentences than
translating from speech.
Table 2 provides a typology of paraphrases found
in all our corpora and two languages, where each
class has been quantified with respect to the refer-
ence alignments.7 The main observation here is that
phrasal synonymy (e.g. mutual understanding ?
consensus) is the most present phenomenon. It is
also interesting to note that the EVENT corpus type,
which is easy to collect on a daily basis, contains ref-
erence paraphrases spread over all classes. Lastly, it
is expected that paraphrases in the pragmatics class
(e.g. police dispatcher ? woman) would be diffi-
cult to acquire, as this would often rely on document
context and costly world knowledge.8
7Note that typologies of paraphrases have already been pro-
posed in the literature (e.g. (Culicover, 1968; Vila et al2011)),
but that the choice of our classes has been primarily moti-
vated by potential subsequent uses of the acquired paraphrases
(paraphrases could be annotated as belonging to more than one
class). Note also that our experiments will also include results
focused on the synonymy class only (cf. Table 5).
8Reusing such types of paraphrases into applications would
however often be too strongly context-dependent.
  COSINE*100 BLEU 1-TER METEOR010
203040
506070 TEXT SPEECH SCENE EVENT
  COSINE*100 BLEU 1-TER METEOR010
203040
506070
Figure 2: Sentence pair average similarities for all cor-
pora for English (left) and French (right) using the co-
sine of token vectors, BLEU (Papineni et al2002),
TER (Snover et al2006) and METEOR (Lavie and
Agarwal, 2007).
3 Bilingual experiments across corpus
types
3.1 Evaluation of paraphrase acquisition
We followed the PARAMETRIC methodology de-
scribed in (Callison-Burch et al2008) for assess-
ing the performance of systems on the task of sub-
sentential paraphrase acquisition. In this methodol-
ogy, a set of paraphrase candidates extracted from
a sentence pair is compared with a set of reference
paraphrases, obtained through human annotation, by
computing usual measures of precision (P ) and re-
call (R). The first value corresponds to the propor-
tion of paraphrase candidates, denoted H, produced
by a system and that are correct relative to the ref-
erence set containing sure and possible paraphrases,
denoted Rall. Recall is obtained by measuring the
proportion of the reference set of sure paraphrases,
724
  TEXT      SPEECH     SCENE       EVENT
GIZA
FASTR
TERp
PIVOT
biphrases
biphrases
biphrases
biphrases
combinationsystem
paraphrases
training and test instances of sentencepairs
union of biphraseswith features
Original signal type of text pairs
Figure 3: Architecture of our combination system for
paraphrase identification.
denoted Rsure, that are found by a system. We also
computed an F-measure value (F1), which consid-
ers recall and precision as equally important. These
values are thus given by the following formulae:
P =
|H ? Rall|
|H|
R =
|H ? Rsure|
|Rsure|
F1 =
2PR
P +R
Note that the way the sets Rall and Rsure of refer-
ence paraphrase pairs are defined ensures that para-
phrase pair candidates that include possible refer-
ence paraphrases will not penalize precision while
not increasing recall.
All performance values reported in the follow-
ing sections will be obtained using 10-fold cross-
validation and averaging the results on each sub-test.
All data sets of cross-validation contain 500 sen-
tence pairs per corpus type, and 125 pairs are kept
for development.
3.2 A framework for sub-sentential paraphrase
identification
We now describe the systems that will be tested
on the various corpora described in section 2 using
the methodology described in section 3.1. Follow-
ing (Bouamor et al2012), a combination system
is used to automatically weight paraphrase pair can-
didates produced by individual systems using a set
of features aiming at recognizing paraphrases, as il-
lustrated on Figure 3. Four individual systems have
been used and are described below: the reasons for
considering those systems include their free avail-
ability, the possibility of using comparable resources
when relevant for our two languages, and the spe-
cific characteristics of the techniques used.
Statistical learning of word alignments (GIZA)
The GIZA++ tool (Och and Ney, 2004) com-
putes statistical word alignment models of increas-
ing complexity from parallel corpora. It was run
on each monolingual corpus of sentence pairs in
both directions, symmetrized alignments were kept
and classical phrase extraction heuristics were ap-
plied (Koehn et al2003), without growing phrases
with unaligned tokens.
Linguistic knowledge on term variation (FASTR)
The FASTR tool (Jacquemin, 1999) spots term vari-
ants in large corpora, where variants are described
through metarules expressing how the morphosyn-
tactic structure of a term variant can be derived
from a given term by means of regular expressions
on morphosyntactic categories. Paradigmatic varia-
tion can also be expressed with constraints between
words, imposing that they be of the same morpho-
logical or semantic family using existing resources
available in our two languages. Variants for all
phrases from one sentence of a pair are extracted
from the other sentence, and the intersection of the
sets for both directions is kept.
Edit rate on word sequences (TERp) The TERp
tool (Snover et al2010) can be used to compute an
optimal set of word and phrase edits that can trans-
form one sentence into another one.9 Edit types are
parameterized by one or more weights which were
optimized towards F-measure by hill climbing with
100 random restarts using the held-out data set con-
sisting of 125 sentence pairs for each corpus type.
Translational equivalence (PIVOT) We exploited
the paraphrase probability defined by Bannard and
Callison-Burch (2005) on bilingual parallel corpora.
We used the Europarl corpus10 of parliamentary de-
bates in English and French, consisting of approx-
imately 1.7 million parallel sentences, using each
language as source and pivot in turn. GIZA++
9Note that contrarily to what TERp allows, we did not used
the possibility of using word or phrase equivalents as those are
only made available for English. This type of knowledge is
however captured in part by the FASTR and PIVOT systems.
10http://statmt.org/europarl
725
Phrase pair features ? edit distance between paraphrases, stem identity, bag-of-tokens similarity, phrase
length ratio
Sentence pair features ? sentence pair similarity (cosine, BLEU, TER, METEOR), relative position of
paraphrases, presence of common tokens at paraphrase boundaries, presence of another paraphrase pair
from each system at paraphrase boundaries, presence of a paraphrase at a different position in the other
sentence
Distributional features ? similarity of token context vectors for each phrase of a paraphrase (derived
from counts in the large English-French parallel corpus from WMT?11 (http://www.statmt.org/
wmt11/translation-task.html) (approx. 30 million parallel sentences)
System features ? combination of the individual systems that proposed the paraphrase pair
Table 3: Features used by our classifiers. Discretized intervals based on median values are used for real values, and
binarized values are used for combinations.
was used for word alignment and phrase transla-
tion probabilities were estimated from them by the
MOSES system (Koehn et al2007). For each
phrase of a sentence pair, we built its set of para-
phrases, and extracted its paraphrase from the other
sentence with highest probability. We repeated this
process in both directions, and finally kept for each
phrase its paraphrase pair from any direction with
highest probability.
Automatic validation of candidate paraphrases
Taking the union of all paraphrase pair candidates
from all the above systems for each sentence pair, we
perform a Maximum Entropy two-class classifica-
tion11, which allows us to include features that were
not necessarily exploited or straightforward to ex-
ploit by individual systems to determine the proba-
bility that each candidate is a good paraphrase. More
generally, this allows us to attempt to learn a more
generic characterization of paraphrases, which could
trivially accept any number of systems as inputs.
Positive examples for the classifier are those from
the union of candidates that are also in the reference
set Rsure, while negative examples are the remaining
ones from the union. The features that we used are
summarized in Table 3.
3.3 Experimental results
Results for individual systems, their union and our
validation system trained on each corpus type are
given on Table 4. First, we find that all individual
systems fare better on TEXT, for which more train-
ing data were available and where semantic equiv-
11Using the implementation at: http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.html
alence of sentence pairs is most likely. EVENT ap-
pears to be the most difficult corpus type, whereas
one could say that being the most readily data source
this is a disapointing result: we will return to this in
section 3.4. In terms of performance on F-measure
per corpus type, GIZA performs best for TEXT and
SPEECH, containing long sentences with possible
repetitions, while TERp performs on par with GIZA
for SCENE and best for EVENT, where equivalences
that are rare at the corpus level are more present.
FASTR achieves a very low recall, showing that the
encoded definitions of term variants do not cover all
types of paraphrases, and also possibly that the lex-
ical resource that it uses has incomplete coverage.
It nonetheless obtains high precision values, most
notably on TEXT. One last comment regarding in-
dividual systems is that PIVOT is by far the most
precise of all the techniques used, but with a recall
much lower than those of GIZA and TERp: as is
the case for FASTR, which makes use of manually-
encoded lexical resources, PIVOT encodes in some
sense some kind of semantic knowledge.12
In all cases, our combination system manages
to increase F-measure substantially over the best
individual system for a corpus type and the sim-
ple union. Improvements are strong on TEXT
(resp. +12.5 and +11.6 on English and French)
and on SPEECH (+11.7 and +11.1) and quite good
on SCENE (+3.2 and +6.4) and on EVENT (+5.4
12Note that the fact that English and French were used as the
pivot for one another may have had some positive effect here,
but, incidentally, the two corpora obtained by translating from
the other language (TEXT and SPEECH) are not those where
PIVOT fares better. The difference observed may however lie in
the higher complexity of the sentences in these corpus types.
726
Individual systems Combination systems
GIZA FASTR TERp?F PIVOT union validation
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
ENGLISH
TEXT 48.2 58.9 53.0 63.1 5.9 10.7 41.2 66.4 50.9 73.4 25.8 38.2 20.8 80.8 33.1 68.4 62.8 65.5
SPEECH 39.7 44.2 41.8 27.1 3.5 6.3 25.0 50.3 33.4 79.2 15.3 25.7 25.5 71.4 37.6 51.0 56.3 53.5
SCENE 44.8 57.7 50.5 47.4 5.2 9.5 40.1 67.9 50.4 84.6 14.6 25.0 36.2 83.4 50.5 44.9 66.8 53.7
EVENT 19.0 33.9 24.3 62.9 3.1 6.0 28.8 68.7 40.6 97.4 11.2 20.1 20.8 75.5 32.7 35.0 67.1 46.0
FRENCH
TEXT 52.5 58.9 55.5 56.9 4.9 9.1 46.4 61.4 52.8 64.5 30.3 41.2 41.5 77.9 54.1 74.7 61.0 67.1
SPEECH 44.0 54.9 48.9 30.7 4.3 7.6 34.8 60.2 44.1 75.5 19.0 30.4 31.4 76.2 44.5 60.2 59.7 60.0
(SCENE) 14.4 43.6 21.7 53.0 4.0 7.4 13.8 75.3 23.4 94.6 5.21 9.8 12.7 86.4 22.2 19.9 59.8 29.8
EVENT 28.7 44.2 34.8 34.4 2.3 4.3 29.9 58.9 39.7 79.5 15.0 25.2 25.2 72.5 37.4 40.0 56.3 46.8
Table 4: Evaluation results for individual systems (left) and combination systems (right) on all corpus types for English
(top) and French (bottom). Values in bold are for highest values for a given metric for each corpus type and language.
and +6.1). Recall from Table 1 that TEXT and
SPEECH were the two corpus types with the highest
number of sure paraphrase examples for both lan-
guages: results show that our classifier was able to
efficiently use them.
Recall values for the union are quite strong for
all corpus types, ranging from 71.4 (SPEECH in En-
glish) to 83.4 (SCENE in English). There is, how-
ever, a substantial decrease between the unions and
the results of our combination systems, although
recall values for our systems are roughly between
56 and 67, which may be considered an acceptable
range on such a task. Further study of false neg-
atives should help with engineering new features to
improve paraphrase recognition. Lastly, we note that
precision is in general highest for a specific system
(PIVOT), and reaches high values for our validation
system on TEXT, where we have the most examples
(resp. 68.4 and 74.7 for English and French).
As seen in Table 2, synonymy is the most present
phenomenon in all our corpora; it is also proba-
bly one of the most useful type of knowledge for
many applications. We now therefore focus on this
class, for which all the sure paraphrases in our cor-
pora falling in this class have been annotated. Ta-
ble 5 shows F-measure values for the individual
techniques and our combination systems on all cor-
pus types. We first observe that our combination sys-
tem also always improves here over the best individ-
ual system, albeit not by a large margin on EVENT.
GIZA FASTR TERp PIVOT validation
ENGLISH
TEXT 52.2 6.1 47.3 47.1 68.1
SPEECH 42.6 5.0 30.3 39.5 54.9
SCENE 51.8 6.0 48.0 26.0 56.3
EVENT 22.5 2.1 34.8 24.7 35.5
FRENCH
TEXT 55.3 3.9 50.7 50.5 70.3
SPEECH 49.8 1.6 40.9 36.2 57.2
(SCENE) 19.6 4.2 23.1 0.0 24.7
EVENT 36.8 3.5 35.3 25.6 39.9
Table 5: F-measure values for test instances in the syn-
onymy class (see Table 2) for all individual systems and
our validation system for English (top) and French (bot-
tom).
Also, we find that PIVOT performs relatively closer
to GIZA and TERp on TEXT and SPEECH than for
the full set of classes, confirming the intuition that
translational equivalence may be appropriate to rec-
ognize synonymy.
3.4 Experiments across corpus types
To test how different the corpora under study are as
regards paraphrase identification, we now consider
using as additional training data for our classifiers
corpora of the other types, both individually and col-
lectively. Results are given on Table 6.13
13Note that our results are still given by performing cross-
validation averaging over 10 test sets for each tested corpus
type.
727
+TEXT +SPEECH +SCENE +EVENT +All
ENGLISH
# ex+ 7,342 2,296 1,784 1,171 12,593
TEXT 65.5 66.2 65.1 66.2 65.1
SPEECH 56.0 53.5 52.8 54.8 56.6
SCENE 49.7 54.3 53.7 53.8 42.7
EVENT 51.1 45.3 42.5 46.0 56.2
FRENCH
# ex+ 12,961 3,340 966 2,160 19,427
TEXT 67.1 67.2 66.7 67.0 66.6
SPEECH 57.6 60.0 56.4 59.6 57.9
(SCENE) 23.7 22.0 29.8 23.9 21.1
EVENT 45.2 45.6 44.3 46.8 49.3
Table 6: Evaluation results (F1 scores) for all corpus
types for English (top) and French (bottom) when adding
training material from other corpus types (values with
gray background on the diagonal are when no additional
training data are used). ?#ex+? rows indicate numbers of
positive paraphrase examples for each additional corpus
type.
The most notable observation is that EVENT is
substantially improved by using all available addi-
tional training data for English (+10.2), and to a
lesser extent for French (+2.5) . It should be noted
that no individual corpus type, save TEXT, individu-
ally improves results on EVENT, and that results are
yet substantially improved over the use of training
data from TEXT when using all available data, re-
vealing a collective contribution of all corpus types.
The second major observation is that all other cor-
pus types seem to be quite specific in nature, as no
addition of training data from other types yields any
improvement (with the exception of SPEECH on En-
glish), but they often in fact decrease performance.
For instance, SCENE in English is substantially neg-
atively impacted by the use of the numerous exam-
ples of TEXT (-4 in F-measure) and even more when
using all other training data (-9). This underlines
the specific nature of this corpus type: independent
descriptions of the same scene in a video may be
worded with much variation that mostly differ from
that present in other corpus types.
Our main conclusion here is therefore that all our
corpora under study are quite specific in nature, but
that EVENT can benefit from all training data from
the other corpus types. We can further note that the
fact that TEXT is almost not impacted by additional
data may also be explained by the fact that this cor-
pus type contains more than half of the total number
of examples for the two languages. Finally, there are
substantially more positive paraphrase examples for
French (19,427) than for English (12,593).
4 Related work
Over the years, paraphrase acquisition and genera-
tion have attracted a wealth of research works that
are too many to adequatly summarize here: (Mad-
nani and Dorr, 2010) presents a complete and up-
to-date review of the main approaches. Sentential
paraphrase collection has been tackled from specific
resources increasing the probability of sentences be-
ing paraphrases (Dolan et al2004; Bernhard and
Gurevych, 2008; Wubben et al2009), from com-
parable monolingual corpora (Barzilay and Elhadad,
2003; Fung and Cheung, 2004; Nelken and Shieber,
2006), and even at web scale (Pasc?a and Dienes,
2005; Bhagat and Ravichandran, 2008).
Various techniques have been proposed for para-
phrase acquisition from related sentence pairs
(Barzilay and McKeown, 2001; Pang et al2003)
and from bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Kok and Brockett, 2010).
The issue of corpus construction for developing and
evaluating paraphrase acquisition techniques are ad-
dressed in (Cohn et al2008; Callison-Burch et al
2008). To the best of our knowledge, this is the first
time that a study in paraphrase acquisition is con-
ducted on several corpus types and for 2 languages.
Faruqui and Pado? (2011) study the acquisition of en-
tailment pairs (premise and hypothesis), with ex-
periments in 3 languages and various domains of
newspaper corpora for one language. Although their
work is not directly comparable to ours, they report
that robustness across domains is difficult to achieve.
Laslty, the evaluation of automatically generated
paraphrases has recently received some attention
(Liu et al2010; Chen and Dolan, 2011; Met-
zler et al2011) although it remains a difficult is-
sue. Application-driven paraphrase generation pro-
vides indirect means of evaluating paraphrase gen-
eration (Zhao et al2009). For instance, the field of
Statistical Machine Translation has produced works
showing both the usefulness of human-produced
728
(Schroeder et al2009; Resnik et al2010) and au-
tomatically produced paraphrases (Madnani et al
2008; Marton et al2009; Max, 2010; He et al
2011) for improving translation performance.
5 Discussion and future work
This work has addressed the issue of sub-sentential
paraphrase acquisition from text pairs. Analogu-
ously to bilingual parallel corpora, which are still
to date the most reliable resources for automatic ac-
quisition of sub-sentential translations, monolingual
parallel corpora are generally regarded as very ap-
propriate for paraphrase acquisition. However, their
low availability makes searching for less parallel
corpora a necessity. In this study, we have attempted
to identify corpora of various degrees of semantic
textual similarity by considering text pairs originat-
ing from various signal types. These signal types
allow various degrees of freedom as to how to for-
mulate a text: a text is read and translated into a dif-
ferent language (TEXT); some speech is listened to
in the context of a visual story and translated into a
different language (SPEECH); some action is looked
at and described (SCENE); and some event that took
place is concisely reported (EVENT).
The results presented in this paper have shown
how these corpora differed in various aspects. First,
they contain varying quantities of paraphrases that
are differently distributed into paraphrase classes.
Individual acquisition techniques, based on statis-
tical learning of word alignments (GIZA), linguis-
tic knowledge on term variation (FASTR), edit rate
on word sequence (TERp), and translational equiv-
alence (PIVOT), for which different performances
were observed among them on the same corpus
type, were shown to achieve different performances
across corpus types. An efficient combination of
candidate paraphrases from these individual tech-
niques exploiting additional features to character-
ize paraphrases has yielded substantial increases in
performance on all corpus types; however, it is in-
teresting to note that the highest amplitude in per-
formance across corpus types was not so much on
recall (amplitude of 10.5 on English and 4.7 on
French) than on precision (amplitude of 33.4 on En-
glish and 34.714 on French). This, some other fac-
14Not considering (SCENE) for French.
tors aside, emphasizes the fact that the correct idenfi-
cation of paraphrases is facilitated when equivalence
of semantic content is more probable. Many works
have accordingly attempted to identify text units that
are as parallel as possible from large corpora, and
the task of measuring semantic textual similarity,
which can find many uses, has received some atten-
tion lately (Agirre et al2012). However, it itself
relies on some knowledge on paraphrasing.
Our avenues for future work lie in three main ar-
eas. The first one is to continue our current line of
work and study the impact of additional individual
acquisition techniques and better characterizations
of paraphrases in context, in tandem with working
on identifying parallel text pairs in large corpora.
Another avenue is to start from the output of high
recall techniques and to attempt to characterize the
contexts of possible substitution for candidate para-
phrases from large corpora as a means to acquire
precise paraphrases. As the examples from Table 7
show, some classes of paraphrases, and in particular
in the continuum from our synonymy to pragmat-
ics classes, require the joint acquisition of contextual
information that license substitution. Lastly, we plan
to apply such knowledge in text-to-text applications.
synonymy
TEXT take part in ? participate in
great assistance ? enormous help
SPEECH make a deal ? come to an agreement
I don?t care ? I don?t give a damn
SCENE riding a bicycle ? cycling
lady ? woman
EVENT jail escapee ? prison fugitive
apologizes ? expresses regret
pragmatics
TEXT flew in ? arrived in
flood-control materials ? needed supplies
SPEECH face ? picture
want to sleep ? dream about sleeping
SCENE a man ? someone
bento ? food
EVENT violence ? bloodshed
anger ? emotion
Table 7: Examples in English for the synonymy and
pragmatics classes.
729
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions. This work was
partly funded by ANR project Edylex (ANR-09-
CORD-008).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
SemEval, Montre?al, Canada.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP, Sapporo, Japan.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Delphine Bernhard and Iryna Gurevych. 2008. Answer-
ing Learners? Questions by Retrieving Question Para-
phrases from Social Q&A Sites. In Proceedings of the
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, Columbus, USA.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2012.
Validation of sub-sentential paraphrases acquired from
parallel monolingual corpora. In EACL, Avignon,
France.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
David Chen and William Dolan. 2011. Collecting highly
parallel data for paraphrase evaluation. In Proceedings
of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
P. W. Culicover. 1968. Paraphrase Generation and Infor-
mation Retrieval from Stored Text. Mechanical Trans-
lation and Computational Linguistics, 11:78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING, Geneva, Switzerland.
Manaal Faruqui and Sebastian Pado?. 2011. Acquiring
entailment pairs across languages and domains: A data
analysis. In Proceedings of the International Con-
ference on Computational Semantics (IWCS), Oxford,
UK.
Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proceedings of COLING,
Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Wei He, Shiqi Zhao, Haifeng Wang, and Ting Liu. 2011.
Enriching SMT Training Data via Paraphrasing. In
Proceedings of IJCNLP, Chiang Mai, Thailand.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL-HLT, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, demo session, Prague, Czech Repub-
lic.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of NAACL,
Los Angeles, USA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the ACL Workshop on Statistical Machine Translation,
Prague, Czech Republic.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
PEM: A paraphrase evaluation metric exploiting par-
allel texts. In Proceedings of EMNLP, Cambridge,
USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceedings
of EMNLP, Singapore.
730
Aure?lien Max. 2010. Example-Based Paraphrasing for
Improved Phrase-Based Statistical Machine Transla-
tion. In Proceedings of EMNLP, Cambridge, USA.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of ACL-
HLT, Portland, USA.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, Trento, Italy.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
Philadelphia, USA.
Marius Pasc?a and Peter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, Jeju Island, South
Korea.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, USA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of EACL, Athens, Greece.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of AMTA, Boston, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Conference
on Computational Linguistics in the Netherlands, Leu-
ven, Belgium.
Marta Vila, M. Anto`nia Mart??, and Horacio Rodr??guez.
2011. Paraphrase Concept and Typology. A Linguisti-
cally Based and Computationally Oriented Approach.
Procesamiento del Lenguaje Natural, (462-3).
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and maching
headlines for automatic paraphrase acquisition. In
Proceedings of the European Workshop on Natural
Language Generation, Athens, Greece.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Generation.
In Proceedings of ACL-IJCNLP, Singapore.
731
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 716?725,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Validation of sub-sentential paraphrases acquired
from parallel monolingual corpora
Houda Bouamor Aure?lien Max
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
The task of paraphrase acquisition from re-
lated sentences can be tackled by a variety
of techniques making use of various types
of knowledge. In this work, we make the
hypothesis that their performance can be
increased if candidate paraphrases can be
validated using information that character-
izes paraphrases independently of the set of
techniques that proposed them. We imple-
ment this as a bi-class classification prob-
lem (i.e. paraphrase vs. not paraphrase),
allowing any paraphrase acquisition tech-
nique to be easily integrated into the com-
bination system. We report experiments on
two languages, English and French, with
5 individual techniques on parallel mono-
lingual parallel corpora obtained via multi-
ple translation, and a large set of classifi-
cation features including surface to contex-
tual similarity measures. Relative improve-
ments in F-measure close to 18% are ob-
tained on both languages over the best per-
forming techniques.
1 Introduction
The fact that natural language allows messages
to be conveyed in a great variety of ways consti-
tutes an important difficulty for NLP, with appli-
cations in both text analysis and generation. The
term paraphrase is now commonly used in the
NLP litterature to refer to textual units of equiva-
lent meaning at the phrasal level (including single
words). For instance, the phrases six months and
half a year form a paraphrase pair applicable in
many different contexts, as they would appropri-
ately denote the same concept. Although one can
envisage to manually build high-coverage lists of
synonyms, enumerating meaning equivalences at
the level of phrases is too daunting a task for hu-
mans. Because this type of knowledge can how-
ever greatly benefit many NLP applications, au-
tomatic acquisition of such paraphrases has at-
tracted a lot of attention (Androutsopoulos and
Malakasiotis, 2010; Madnani and Dorr, 2010),
and significant research efforts have been devoted
to this objective (Callison-Burch, 2007; Bhagat,
2009; Madnani, 2010).
Central to acquiring paraphrases is the need of
assessing the quality of the candidate paraphrases
produced by a given technique. Most works to
date have resorted to human evaluation of para-
phrases on the levels of grammaticality and mean-
ing equivalence. Human evaluation is however
often criticized as being both costly and non re-
producible, and the situation is even more compli-
cated by the inherent complexity of the task that
can produce low inter-judge agreement. Task-
based evaluation involving the use of paraphras-
ing into some application thus seem an acceptable
solution, provided the evaluation methodologies
for the given task are deemed acceptable. This,
in turn, puts the emphasis on observing the im-
pact of paraphrasing on the targeted application
and is rarely accompanied by a study of the intrin-
sic limitations of the paraphrase acquisition tech-
nique used.
The present work is concerned with the task of
sub-sentential paraphrase acquisition from pairs
of related sentences. A large variety of tech-
niques have been proposed that can be applied
to this task. They typically make use of differ-
ent kinds of automatically or manually acquired
knowledge. We make the hypothesis that their
performance can be increased if candidate para-
716
phrases can be validated using information that
characterize paraphrases in complement to the set
of techniques that proposed them. We propose to
implement this as a bi-class classification problem
(i.e. paraphrase vs. not paraphrase), allowing
any paraphrase acquisition technique to be easily
integrated into the combination system. In this
article, we report experiments on two languages,
English and French, with 5 individual techniques
based on a) statistical word alignment models,
b) translational equivalence, c) handcoded rules of
term variation, d) syntactic similarity, and e) edit
distance on word sequences. We used parallel
monolingual parallel corpora obtained via mul-
tiple translation from a single language as our
sources of related sentences, and a large set of
features including surface to contextual similarity
measures. Relative improvements in F-measure
close to 18% are obtained on both languages over
the best performing techniques.
The remainder of this article is organized as
follows. We first briefly review previous work
on sub-sentential paraphrase acquisition in sec-
tion 2. We then describe our experimental setting
in section 3 and the individual techniques that we
have studied in section 4. Section 5 is devoted to
our approach for validating paraphrases proposed
by individual techniques. Finally, section 6 con-
cludes the article and presents some of our future
work in the area of paraphrase acquisition.
2 Related work
The hypothesis that if two words or, by exten-
sion, two phrases, occur in similar contexts then
they may be interchangeable has been extensively
tested. The distributional hypothesis, attributed to
Zellig Harris, was for example applied to syntac-
tic dependency paths in the work of Lin and Pan-
tel (2001). Their results take the form of equiva-
lence patterns with two arguments such as {X asks
for Y, X requests Y, X?s request for Y, X wants Y,
Y is requested by X, . . .}.
Using comparable corpora, where the same in-
formation probably exists under various linguis-
tic forms, increases the likelihood of finding very
close contexts for sub-sentential units. Barzilay
and Lee (2003) proposed a multi-sequence align-
ment algorithm that takes structurally similar sen-
tences and builds a compact lattice representation
that encodes local variations. The work by Bhagat
and Ravichandran (2008) describes an application
of a similar technique on a very large scale.
The hypothesis that two words or phrases are
interchangeable if they share a common trans-
lation into one or more other languages has
also been extensively studied in works on sub-
sentential paraphrase acquisition. Bannard and
Callison-Burch (2005) described a pivoting ap-
proach that can exploit bilingual parallel corpora
in several languages. The same technique has
been applied to the acquisition of local paraphras-
ing patterns in Zhao et al(2008). The work of
Callison-Burch (2008) has shown how the mono-
lingual context of a sentence to paraphrase can be
used to improve the quality of the acquired para-
phrases.
Another approach consists in modelling local
paraphrasing identification rules. The work of
Jacquemin (1999) on the identification of term
variants, which exploits rewriting morphosyntac-
tic rules and descriptions of morphological and
semantic lexical families, can be extended to ex-
tract the various forms corresponding to input pat-
terns from large monolingual corpora.
When parallel monolingual corpora aligned at
the sentence level are available (e.g. multiple
translations into the same language), the task of
sub-sentential paraphrase acquisition can be cast
as one of word alignment between two aligned
sentences (Cohn et al 2008). Barzilay and
McKeown (2001) applied the distributionality hy-
pothesis on such parallel sentences, and Pang et
al. (2003) proposed an algorithm to align sen-
tences by recursive fusion of their common syn-
tactic constituants.
Finally, they has been a recent interest in auto-
matic evaluation of paraphrases (Callison-Burch
et al 2008; Liu et al 2010; Chen and Dolan,
2011; Metzler et al 2011).
3 Experimental setting
We used the main aspects of the methodology
described by Cohn et al(2008) for constructing
evaluation corpora and assessing the performance
of techniques on the task of sub-sentential para-
phrase acquisition. Pairs of related sentences are
hand-aligned to define a set of reference atomic
paraphrase pairs at the level of words or phrases,
denoted asRatom1.
1Note that in this study we do not distinguish between
?Sure? and ?Possible? alignments, and when reusing anno-
717
single language multiple language video descriptions multiply-translated news headlines
translation translation subtitles
# tokens 4,476 4,630 1,452 2,721 1,908
# unique tokens 656 795 357 830 716
% aligned tokens (excluding identities) 60.58 48.80 23.82 29.76 14.46
lexical overlap (tokens) 77.21 61.03 59.50 32.51 39.63
lexical overlap (lemmas content words) 83.77 71.04 64.83 39.54 45.31
translation edit rate (TER) 0.32 0.55 0.76 0.68 0.62
penalized n-gram prec. (BLEU) 0.33 0.15 0.13 0.14 0.39
Table 1: Various indicators of sentence pair comparability for different corpus types. Statistics are reported for
French on sets of 100 sentence pairs.
We conducted a small-scale study to assess dif-
ferent types of corpora of related sentences:
1. single language translation Corpora ob-
tained by several independent human trans-
lation of the same sentences (e.g. (Barzilay
and McKeown, 2001)).
2. multiple language translation Same as
above, but where a sentence is translated
from 4 different languages into the same lan-
guage (Bouamor et al 2010).
3. video descriptions Descriptions of short
YouTube videos obtained via Mechanical
Turk (Chen and Dolan, 2011).
4. multiply-translated subtitles Aligned mul-
tiple translations of contributed movie subti-
tles (Tiedemann, 2007).
5. comparable news headlines News head-
lines collected from Google News clusters
(e.g. (Dolan et al 2004)).
We collected 100 sentence pairs of each type
in French, for which various comparability mea-
sures are reported on Table 1. In particular, the
?% aligned tokens? row indicates the propor-
tion of tokens from the sentence pairs that could
be manually aligned by a native-speaker annota-
tor.2 Obviously, the more common tokens two
sentences from a pair contain, the fewer sub-
sentential paraphrases may be extracted from that
pair. However, high lexical overlap increases the
probability that two sentences be indeed para-
phrases, and in turn the probability that some of
their phrases be paraphrases. Furthermore, the
tated corpora using them we considered all alignments as be-
ing correct.
2The same annotator hand-aligned the 5*100=500 para-
phrase pairs using the YAWAT (Germann, 2008) manual
alignment tool.
presence of common token may serve as useful
clues to guide paraphrase extraction.
For our experiments, we chose to use parallel
monolingual corpora obtained by single language
translation, the most direct resource type for ac-
quiring sub-sentential paraphrase pairs. This al-
lows us to define acceptable references for the
task and resort to the most consensual evaluation
technique for paraphrase acquisition to date. Us-
ing such corpora, we expect to be able to extract
precise paraphrases (see Table 1), which will be
natural candidates for further validation, which
will be addressed in section 5.3.
Figure 1 illustrates a reference alignment ob-
tained on a pair of English sentential paraphrases
and the list of atomic paraphrase pairs that can be
extracted from it, against which acquisition tech-
niques will be evaluated. Note that we do not con-
sider pairs of identical units during evaluation, so
we filter them out from the list of reference para-
phrase pairs.
The example in Figure 1 shows different cases
that point to the inherent complexity of this task,
even for human annotators: it could be argued,
for instance, that a correct atomic paraphrase
pair should be reached ? amounted to rather
than reached ? amounted. Also, aligning in-
dependently 260 ? 0.26 and million ? billion
is assuredly an error, while the pair 260 mil-
lion? 0.26 billion would have been appropriate.
A case of alignment that seems non trivial can be
observed in the provided example (during the en-
tire year ? annual). The abovementioned rea-
sons will explain in part the difficulties in reach-
ing high performance values using such gold stan-
dards.
Reference composite paraphrase pairs (denoted
as R), obtained by joining adjacent atomic para-
phrase pairs from Ratom up to 6 tokens3, will
3We used standard biphrase extraction heuristics (Koehn
718
the
amount
of
foreign
capital
actually
utilized
during
the
entire
year
reached
260
million
us
dollars
.
th
e
an
nu
al
fo
re
ig
n
in
ve
stm
en
t
ac
tu
al
ly
us
ed
am
ou
nt
ed
to us
$
0.
26
bi
lli
on
capital ? investment
utilized ? used
during the entire year ? annual
reached ? amounted
260 ? 0.26
million ? billion
us dollars ? us$
Figure 1: Reference alignments for a pair of English
sentential paraphrases from the annotation corpus of
Cohn et al(2008) (note that possible and sure align-
ments are not distinguished here) and the list of atomic
paraphrase pairs extracted from these alignments.
also be considered when measuring performance.
Evaluated techniques have to output atomic can-
didate paraphrase pairs (denoted as Hatom) from
which composite paraphrase pairs (denoted as
H) are computed. The usual measures of pre-
cision (P ), recall (R) and F-measure (F1) can
then be defined in the following way (Cohn et al
2008):
P =
|Hatom ?R|
|Hatom|
R =
|H ? Ratom|
|Ratom|
F1 =
2pr
p+ r
We conducted experiments using two different
corpora in English and French. In each case,
a held-out development corpus of 150 sentential
paraphrase pairs was used for development and
tuning, and all techniques were evaluated on the
same test set consisting of 375 sentential para-
phrase pairs. For English, we used the MTC
et al 2007) : all words from a phrase must be aligned to at
least one word from the other and not to words outside, but
unaligned words at phrase boundaries are not used.
corpus described in (Cohn et al 2008), consist-
ing of multiply-translated Chinese sentences into
English, and used as our gold standard both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news ar-
ticles4 obtained by translating into French from
English.
We used the YAWAT (Germann, 2008) manual
alignment tool. Inter-annotator agreement val-
ues (averaging with each annotation set as the
gold standard) are 66.1 for English and 64.6 for
French, which we interpret as acceptable val-
ues. Manual inspection of the two corpora reveals
that the French corpus tends to contain more lit-
eral translations, possibly due to the original lan-
guages of the sentences, which are closer to the
target language than Chinese is to English.
4 Individual techniques for paraphrase
acquisition
As discussed in section 2, the acquisition of sub-
sentential paraphrases is a challenging task that
has previously attracted a lot of work. In this
work, we consider the scenario where sentential
paraphrases are available and words and phrases
from one sentence can be aligned to words and
phrases from the other sentence to form atomic
paraphrase pairs. We now describe several tech-
niques that perform the task of sub-sentential unit
alignment. We have selected and implemented
five techniques which we believe are representa-
tive of the type of knowledge that these techniques
use, and have reused existing tools, initially devel-
oped for other tasks, when possible.
4.1 Statistical learning of word alignments
(Giza)
The GIZA++ tool (Och and Ney, 2004) computes
statistical word alignment models of increasing
complexity from parallel corpora. While origi-
nally developed in the bilingual context of Statis-
tical Machine Translation, nothing prevents build-
ing such models on monolingual corpora. How-
ever, in order to build reliable models, it is nec-
essary to use enough training material includ-
ing minimal redundancy of words. To this end,
we provided GIZA++ with all possible sentence
pairs from our mutiply-translated corpus to im-
prove the quality of its word alignments (note that
4http://www.elda.org/article125.html
719
we used symmetrized alignments from the align-
ments in both directions). This constitutes a sig-
nificant advantage for this technique that tech-
niques working on each sentence pair indepen-
dently do not have.
4.2 Translational equivalence (Pivot)
Translational equivalence can be exploited to de-
termine that two phrases may be paraphrases.
Bannard and Callison-Burch (2005) defined a
paraphrasing probability between two phrases
based on their translation probability through all
possible pivot phrases as:
Ppara(p1, p2) =
?
piv
Pt(piv|p1)Pt(p2|piv)
where Pt denotes translation probabilies. We used
the Europarl corpus5 of parliamentary debates in
English and French, consisting of approximately
1.7 million parallel sentences : this allowed us
to use the same resource to build paraphrases for
English, using French as the pivot language, and
for French, using English as the pivot language.
The GIZA++ tool was used for word alignment
and the MOSES Statistical Machine Translation
toolkit (Koehn et al 2007) was used to com-
pute phrase translation probabilities from these
word alignments. For each sentential paraphrase
pair, we applied the following algorithm: for each
phrase, we build the entire set of paraphrases us-
ing the previous definition. We then extract its
best paraphrase as the one exactly appearing in the
other sentence with maximum paraphrase proba-
bility, using a minimal threshold value of 10?4.
4.3 Linguistic knowledge on term variation
(Fastr)
The FASTR tool (Jacquemin, 1999) was designed
to spot term/phrase variants in large corpora.
Variants are described through metarules express-
ing how the morphosyntactic structure of a term
variant can be derived from a given term by means
of regular expressions on word morphosyntactic
categories. Paradigmatic variation can also be ex-
pressed by expressing constraints between words,
imposing that they be of the same morphologi-
cal or semantic family. Both constraints rely on
preexisting repertoires available for English and
French. To compute candidate paraphrase pairs
using FASTR, we first consider all phrases from
5http://statmt.org/europarl
the first sentence and search for variants in the
other sentence, then do the reverse process and
finally take the intersection of the two sets.
4.4 Syntactic similarity (Synt)
The algorithm introduced by Pang et al(2003)
takes two sentences as input and merges them by
top-down syntactic fusion guided by compatible
syntactic substructure. A lexical blocking mecha-
nism prevents constituents from fusionning when
there is evidence of the presence of a word in an-
other constituent of one of the sentence. We use
the Berkeley Probabilistic parser (Klein and Man-
ning, 2003) to obtain syntactic trees for English
and its adapted version for French (Candito et al
2010). Because this process is highly sensitive to
syntactic parse errors, we use in our implemen-
tation k-best parses and retain the most compact
fusion from any pair of candidate parses.
4.5 Edit rate on word sequences (TERp)
TERp (Translation Edit Rate Plus) (Snover et al
2010) is a score designed for the evaluation of
Machine Translation output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some exist-
ing reference translation. Edit types include ex-
act word matching, word insertion and deletion,
block movement of contiguous words (computed
as an approximation), as well as optionally vari-
ants substitution through stemming, synonym or
paraphrase matching.6 Each edit type is parame-
terized by at least one weight which can be opti-
mized using e.g. hill climbing. TERp being a tun-
able metric, our experiments will include tuning
TERp systems towards either precision (? P ),
recall (? R), or F-measure (? F1).7
4.6 Evaluation of individual techniques
Results for the 5 individual techniques are given
on the left part of Table 2. It is first apparent
that all techniques but TERp fared better on the
French corpus than on the English corpus. This
can certainly be explained by the fact that the for-
mer results from more literal translations (from
6Note that for these experiments we did not use the stem-
ming module, the interface to WordNet for synonym match-
ing and the provided paraphrase table for English, due to the
fact that these resources were available for English only.
7Hill climbing was used for all tunings as done by Snover
et al(2010), and we used one iteration starting with uniform
weights and 100 random restarts.
720
Individual techniques Combinations
GIZA PIVOT FASTR SYNT TERp union validation? P ? R ? F1
English
P 31.01 31.78 37.38 52.17 50.00 29.15 33.37 21.44 50.51
R 38.30 18.50 6.71 2.53 5.83 45.19 45.37 60.87 41.19
F1 34.27 23.39 11.38 4.83 10.44 35.44 38.46 31.71 45.37
French
P 28.99 29.53 52.48 62.50 31.35 30.26 31.43 17.58 40.77
R 45.98 26.66 8.59 8.65 44.22 44.60 44.10 63.36 45.85
F1 35.56 28.02 14.77 15.20 36.69 36.05 36.70 27.53 43.16
Table 2: Results on the test set on English and French for the 5 individual paraphrase acquisition techniques (left
part) and for the 2 combination techniques (right part).
English to French, compared with from Chinese
to English), which should be consequently eas-
ier to word-align. This is for example clearly
shown by the results of the statistical aligner
GIZA, which obtains a 7.68 advantage on recall
for French over English.
The two linguistically-aware techniques,
FASTR and SYNT, have a very strong precision
on the more parallel French corpus, but fail to
achieve an acceptable recall on their own. This
is not surprising : FASTR metarules are focussed
on term variant extraction, and SYNT requires
two syntactic trees to be highly comparable
to extract sub-sentential paraphrases. When
these constrained conditions are met, these two
techniques appear to perform quite well in terms
of precision.
GIZA and TERp perform roughly in the same
range on French, with acceptable precision and
recall, TERp performing overall better, with e.g.
a 1.14 advantage on F-measure on French and
4.19 on English. The fact that TERp performs
comparatively better on English than on French8,
with a 1.76 advantage on F-measure, is not con-
tradictory: the implemented edit distance makes
it possible to align reasonably distant words and
phrases independently from syntax, and to find
alignments for close remaining words, so the dif-
ferences of performance between the two lan-
guages are not necessarily expected to be com-
parable with the results of a statistical alignment
technique. English being a poorly-inflected lan-
guage, alignment clues between two sentential
paraphrases are expected to be more numerous
8Recall that all specific linguistic modules for English
only from TERp had been disabled, so the better perfor-
mance on English cannot be explained by a difference in
terms of resources used.
than for highly-inflected French.
PIVOT is on par with GIZA as regards preci-
sion, but obtains a comparatively much lower re-
call (differences of 19.32 and 19.80 on recall on
French and English respectively). This may first
be due in part to the paraphrasing score threshold
used for PIVOT, but most certainly to the use of
a bilingual corpus from the domain of parliamen-
tary debates to extract paraphrases when our test
sets are from the news domain: we may be ob-
serving differences inherent to the domain, and
possibly facing the issue of numerous ?out-of-
vocabulary? phrases, in particular for named en-
tities which frequently occur in the news domain.
Importantly, we can note that we obtain at best
a recall of 45.98 on French (GIZA) and of 45.37
on English (TERp). This may come as a disap-
pointment but, given the broad set of techniques
evaluated, this should rather underline the inher-
ent complexity of the task. Also, recall that the
metrics used do not consider identity paraphrases
(e.g. at the same time ? at the same time), as
well as the fact that gold standard alignment is
a very difficult process as shown by interjudge
agreement values and our example from section 3.
This, again, confirms that the task that is ad-
dressed is indeed a difficult one, and provides fur-
ther justification for initially focussing on parallel
monolingual corpora, albeit scarce, for conduct-
ing fine-grained studies on sub-sentential para-
phrasing.
Lastly, we can also note that precision is not
very high, with (at best, using TERp?P ) average
values for all techniques of 40.97 and 40.46 on
French and English, respectively. Several facts
may provide explanations for this observation.
First, it should be noted that none of those tech-
niques, except SYNT, was originally developed
721
for the task of sub-sentential paraphrase acqui-
sition from monolingual parallel corpora. This
results in definitions that are at best closely re-
lated to this task.9 Designing new techniques
was not one of the objectives of our study, so we
have reused existing techniques, originally devel-
oped with different aims (bilingual parallel cor-
pora word alignment (GIZA), term variant recog-
nition (FASTR), Machine Translation evaluation
(TERp)). Also, techniques such as GIZA and
TERp attempt to align as many words as possi-
ble in a sentence pair, when gold standard align-
ments sometimes contain gaps.10 Finally, the met-
rics used will count as false small variations of
gold standard paraphrases (e.g. missing function
word): the acceptability or not of such candi-
dates could be either evaluated in a scenario where
such ?acceptable? variants would be taken into
account, and could be considered in the context
of some actual use of the acquired paraphrases
in some application. Nonetheless, on average the
techniques in our study produce more candidates
that are not in the gold standard: this will be an
important fact to keep in mind when tackling the
task of combining their outputs. In particular, we
will investigate the use of features indicating the
combination of techniques that predicted a given
paraphrase pair, aiming to capture consensus in-
formation.
5 Paraphrase validation
5.1 Technique complementarity
Before considering combining and validating the
outputs of individual techniques, it is informative
to look at some notion of ?complementarity? be-
tween techniques, in terms of how many correct
paraphrases a technique would add to a combined
set. The following formula was used to account
for the complementarity between the set of can-
didates from some technique i, ti, and the set for
some technique j, tj :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj))
9Recall, however, that our best performing technique on
F-measure, TERp, was optimized to our task using a held
out development set.
10It is arguable whether such cases should happen in sen-
tence pairs obtained by translating the same original sentence
into the same language, but this clearly depends on the inter-
pretation of the expected level of annotation by the annota-
tors.
Results on the test set for the two languages
are given in Table 3. A number of pairs of tech-
niques have strong complementarity values, the
strongest one being for GIZA and TERp for both
languages. According to these figures, PIVOT
identify paraphrases which are slightly more sim-
ilar to those of TERp than those of GIZA. Inter-
estingly, FASTR and SYNT exhibit a strong com-
plementarity, where in French, for instance, they
only have a very small proportion of paraphrases
in common. Considering the set of all other tech-
niques, GIZA provides the more new paraphrases
on French and TERp on English.
GIZA PIVOT FASTR SYNT TERp?R all others
English
GIZA - 4.65 2.83 0.59 10.31 8.31
PIVOT 4.65 - 2.30 1.88 3.12 3.72
FASTR 2.83 2.30 - 2.42 1.71 0.53
SYNT 0.59 1.88 2.42 - 0.59 0.00
TERp?R 10.31 3.12 1.71 0.59 - 12.20
French
GIZA - 9.79 3.64 2.20 10.73 8.91
PIVOT 9.79 - 2.26 5.22 7.84 3.39
FASTR 3.64 2.26 - 7.28 3.01 0.19
SYNT 2.20 5.22 7.28 - 1.76 0.44
TERp?R 10.73 7.84 3.01 1.76 - 5.65
Table 3: Values of complementarity on the test set for
both languages, where the following formula was used
for the set of technique outputs T = {t1, t2, ..., tn} :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj)).
Complementarity values are computed between all
pairs of individual techniques, and each individual
technique and the set of all other techniques. Values in
bold indicate highest values for the technique of each
row.
5.2 Naive combination by union
We first implemented a naive combination ob-
tained by taking the union of all techniques. Re-
sults are given in the first column of the right part
of Table 2. The first result is quite encouraging:
in both languages, more than 6 paraphrases from
the gold standard out of 10 are found by at least
one of the techniques, which, given our previous
discussion, constitutes a good result and provide
a clear justification for combining different tech-
niques for improving performance on this task.
Precision is mechanically lowered to account for
roughly 1 correct paraphrase over 5 candidates
for both languages. F-measure values are much
lower than those of TERp and GIZA, showing
that the union of all techniques is only interest-
ing for recall-oriented paraphrase acquisition. In
722
the next section, we will show how the results of
the union can be validated using machine learning
to improve these figures.
5.3 Paraphrase validation via automatic
classification
A natural improvement to the naive combination
of paraphrase candidates from all techniques can
consist in validating candidate paraphrases by us-
ing several models that may be good indicators of
their paraphrasing status. We can therefore cast
our problem as one of biclass classification (i.e.
?paraphrase? vs. ?not paraphrase?).
We have used a maximum entropy classifier11
with the following features, aiming at capturing
information on the paraphrase status of a candi-
date pair:
Morphosyntactic equivalence (POS) It may
be the case that some sequences of part-of-speech
can be rewritten as different sequences, e.g. as
a result of verb nominalization. We therefore
use features to indicate the sequences of part-of-
speech for a pair of candidate paraphrases. We
used the preterminal symbols of the syntactic
trees of the parser used for SYNT.
Character-based distance (CAR) Morpholog-
ical variants often have close word forms, and
more generally close word forms in sentential
paraphase pairs may indicate related words. We
used features for discretized values of the edit
distance between the two phrases of a candidate
paraphrase pair as measured by the Levenshtein
distance.
Stem similarity (STEM) Inflectional morphol-
ogy, which is quite productive in languages such
as French, can increase vocabulary size signifi-
cantly, while in sentential paraphrases common
stems may indicate related words. We used a
binary feature indicating whether the stemmed
phrases of a candidate paraphrase pair match.12
Token set identity (BOW) Syntactic rearrange-
ments may involve the same sets of words in var-
ious orders. We used discretized features indicat-
ing the proportion of common tokens in the set
11We used the implementation available at:
http://homepages.inf.ed.ac.uk/lzhang10/
maxent_toolkit.html
12We use the implementations of the Snowball stem-
mer from English and French available from: http://
snowball.tartarus.org
of tokens for the two phrases of a candidate para-
phrase pair.
Context similarity (CTXT) It can be derived
from the distributionality hypothesis that the more
two phrases will be seen in similar contexts, the
more they are likely to be paraphrases. We used
discretized features indicating how similar the
contexts of occurrences of two paraphrases are.
For this, we used the full set of bilingual English-
French data available for the translation task of
the Workshop on Statistical Machine Transla-
tion13, totalling roughly 30 million parallel sen-
tences: this again ensures that the same resources
are used for experiments in the two languages. We
collect all occurrences for the phrases in a pair,
and build a vector of content words cooccurring
within a distance of 10 words from each phrase.
We finally compute the cosine between the vec-
tors of the two phrases of a candidate paraphrase
pair.
Relative position in a sentence (REL) De-
pending on the language in which parallel sen-
tences are analyzed, it may be the case that sub-
sentential paraphrases occur at close locations in
their respective sentence. We used a discretized
feature indicating the relative position of the two
phrases in their original sentence.
Identity check (COOC) We used a binary fea-
ture indicating whether one of the two phrases
from a candidate pair, or the two, occurred at
some other location in the other sentence.
Phrase length ratio (LEN) We used a dis-
cretized feature indicating phrase length ratio.
Source techniques (SRC) Finally, as our set-
ting validates paraphrase candidates produced by
a set of techniques, we used features indicat-
ing which combination of techniques predicted a
paraphrase candidate. This can allow learning that
paraphrases in the intersection of the predicted
sets for some techniques may produce good re-
sults.
We used a held out training set consisting of
150 sentential paraphrase pairs from the same cor-
pora as our previous developement and test sets
for both languages. Positive examples were taken
from the candidate paraphrase pairs from any of
13http://www.statmt.org/wmt11/
translation-task.html
723
the 5 techniques in our study which belong to
the gold standard, and we used a corresponding
number of negative examples (randomly selected)
from candidate pairs not in the gold standard. The
right part of Table 2 provides the results for our
validation experiments of the union set for all pre-
vious techniques.
We obtain our best results for this study using
the output of our validation classifier over the set
of all candidate paraphrase pairs. On French, it
yields an improvement in F-measure (43.16) of
+6.46 over the best individual technique (TERp)
and of +15.63 over the naive union from all indi-
vidual techniques. On English, the improvement
in F-measure (45.37) is for the same conditions of
respectively +6.91 (over TERp) and +13.66. We
unfortunately observe an important decrease in re-
call over the naive union, of respectively -17.54
and -19.68 for French and English. Increasing our
amount of training data to better represent the full
range of paraphrase types may certainly overcome
this in part. This would indeed be sensible, as bet-
ter covering the variety of paraphrase types as a
one-time effort would help all subsequent valida-
tions. Figure 2 shows how performance varies on
French with number of training examples for var-
ious feature configurations. However, some para-
phrase types will require integration of more com-
plex knowledge, as is the case, for instance, for
paraphrase pairs involving some anaphora and its
antecedent (e.g. China? it).
While these results, which are very comparable
for the two languages studied, are already satisfy-
ing given the complexity of our task, further in-
spection of false positives and negatives may help
us to develop additional models that will help us
obtain a better classification performance.
6 Conclusions and future work
In this article, we have addressed the task of com-
bining the results of sub-sentential paraphrase ac-
quition from parallel monolingual corpora using a
large variety of techniques. We have provided jus-
tifications for using highly parallel corpora con-
sisting of multiply translated sentences from a
single language. All our experiments were con-
ducted on both English and French using com-
parable resources, so although the results cannot
be directly compared they give some acceptable
comparison points. The best recall of any indi-
vidual technique is around 45 for both language,
10 20 30 40 50 60 70 80 90 10031
33
35
37
39
41
43
All\POS\SRC\CTXT\STEM\LEN\COOC
F-mea
sure
% of examples from training corpus
Figure 2: Learning curves obtained on French by re-
moving features individually.
and F-measure in the range 36-38, indicating that
the task under study is a very challenging one.
Our validation strategy based on bi-class classi-
fication using a broad set of features applicable to
all candidate paraphrase pairs allowed us to obtain
a 18% relative improvement in F-measure over
the best individual technique for both languages.
Our future work include performing a deeper
error analysis of our current results, to better com-
prehend what characteristics of paraphrase still
defy current validation. Also, we want to inves-
tigate adding new individual techniques to pro-
vide so far unseen candidates. Another possible
approach would be to submit all pairs of sub-
sentential paraphrase pairs from a sentence pair
to our validation process, which would obviously
require some optimization and devising sensible
heuristics to limit time complexity. We also in-
tend to collect larger corpora for all other corpus
types appearing in Table 1 and conducting anew
our acquisition and validation tasks.
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions, as well as Guil-
laume Wisniewski for helpful discussions. This
work was partly funded by ANR project Edylex
(ANR-09-CORD-008).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
724
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of NAACL-HLT, Edmonton, Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL, Toulouse, France.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Rahul Bhagat. 2009. Learning Paraphrases from Text.
Ph.D. thesis, University of Southern California.
Houda Bouamor, Aure?lien Max, and Anne Vilnat.
2010. Comparison of Paraphrase Acquisition Tech-
niques on Sentential Paraphrases. In Proceedings of
IceTAL, Rejkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella La-
pata. 2008. Parametric: An automatic evaluation
metric for paraphrasing. In Proceedings of COL-
ING, Manchester, UK.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis.
2010. Statistical French dependency parsing: tree-
bank conversion and first results. In Proceedings of
LREC, Valletta, Malta.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of COLING, Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL, demo session,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: A paraphrase evaluation metric ex-
ploiting parallel texts. In Proceedings of EMNLP,
Cambridge, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing Phrasal and Sentential Paraphrases: A Survey
of Data-Driven Methods . Computational Linguis-
tics, 36(3).
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, University of Maryland College Park.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of
ACL-HLT, Portland, USA.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of NAACL-HLT, Edmonton,
Canada.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase,
semantic, and alignment enhancements to Transla-
tion Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Confer-
ence on Computational Linguistics in the Nether-
lands, Leuven, Belgium.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings
of ACL-HLT, Columbus, USA.
725
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 395?400,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Monolingual Alignment by Edit Rate Computation
on Sentential Paraphrase Pairs
Houda Bouamor Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Anne Vilnat
Abstract
In this paper, we present a novel way of tack-
ling the monolingual alignment problem on
pairs of sentential paraphrases by means of
edit rate computation. In order to inform the
edit rate, information in the form of subsenten-
tial paraphrases is provided by a range of tech-
niques built for different purposes. We show
that the tunable TER-PLUS metric from Ma-
chine Translation evaluation can achieve good
performance on this task and that it can effec-
tively exploit information coming from com-
plementary sources.
1 Introduction
The acquisition of subsentential paraphrases has at-
tracted a lot of attention recently (Madnani and Dorr,
2010). Techniques are usually developed for extract-
ing paraphrase candidates from specific types of cor-
pora, including monolingual parallel corpora (Barzi-
lay and McKeown, 2001), monolingual comparable
corpora (Dele?ger and Zweigenbaum, 2009), bilin-
gual parallel corpora (Bannard and Callison-Burch,
2005), and edit histories of multi-authored text (Max
and Wisniewski, 2010). These approaches face two
main issues, which correspond to the typical mea-
sures of precision, or how appropriate the extracted
paraphrases are, and of recall, or how many of the
paraphrases present in a given corpus can be found
effectively. To start with, both measures are often
hard to compute in practice, as 1) the definition of
what makes an acceptable paraphrase pair is still
a research question, and 2) it is often impractical
to extract a complete set of acceptable paraphrases
from most resources. Second, as regards the pre-
cision of paraphrase acquisition techniques in par-
ticular, it is notable that most works on paraphrase
acquisition are not based on direct observation of
larger paraphrase pairs. Even monolingual corpora
obtained by pairing very closely related texts such as
news headlines on the same topic and from the same
time frame (Dolan et al, 2004) often contain unre-
lated segments that should not be aligned to form a
subsentential paraphrase pair. Using bilingual cor-
pora to acquire paraphrases indirectly by pivoting
through other languages is faced, in particular, with
the issue of phrase polysemy, both in the source and
in the pivot languages.
It has previously been noted that highly parallel
monolingual corpora, typically obtained via mul-
tiple translation into the same language, consti-
tute the most appropriate type of corpus for ex-
tracting high quality paraphrases, in spite of their
rareness (Barzilay and McKeown, 2001; Cohn et
al., 2008; Bouamor et al, 2010). We build on this
claim here to propose an original approach for the
task of subsentential alignment based on the compu-
tation of a minimum edit rate between two sentential
paraphrases. More precisely, we concentrate on the
alignment of atomic paraphrase pairs (Cohn et al,
2008), where the words from both paraphrases are
aligned as a whole to the words of the other para-
phrase, as opposed to composite paraphrase pairs
obtained by joining together adjacent paraphrase
pairs or possibly adding unaligned words. Figure 1
provides examples of atomic paraphrase pairs de-
rived from a word alignment between two English
sentential paraphrases.
395
China
will
continue continue?carry on
implementing
the
financial financial opening
up?open financialopening
up
policy
Ch
ina
wi
ll
ca
rry
on op
en
fin
an
cia
l
po
lic
y
Figure 1: Reference alignments for a pair of English
sentential paraphrases and their associated list of atomic
paraphrase pairs extracted from them. Note that identity
pairs (e.g. China ? China) will never be considered in
this work and will not be taken into account for evalua-
tion.
The remainder of this paper is organized as fol-
lows. We first briefly describe in section 2 how we
apply edit rate computation to the task of atomic
paraphrase alignment, and we explain in section 3
how we can inform such a technique with paraphrase
candidates extracted by additional techniques. We
present our experiments and discuss their results in
section 4 and conclude in section 5.
2 Edit rate for paraphrase alignment
TER-PLUS (Translation Edit Rate Plus) (Snover et
al., 2010) is a score designed for evaluation of Ma-
chine Translation (MT) output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some existing
reference translation. Edit types include exact word
matching, word insertion and deletion, block move-
ment of contiguous words (computed as an approx-
imation), as well as variants substitution through
stemming, synonym or paraphrase matching. Each
edit type is parameterized by at least one weight
which can be optimized using e.g. hill climbing.
TER-PLUS is therefore a tunable metric. We will
henceforth design as TERMT the TER metric (basi-
cally, without variants matching) optimized for cor-
relation with human judgment of accuracy in MT
evaluation, which is to date one of the most used
metrics for this task.
While this metric was not designed explicitely for
the acquisition of word alignments, it produces as a
by-product of its approximate search a list of align-
ments involving either individual words or phrases,
potentially fitting with the previous definition of
atomic paraphrase pairs. When applying it on a
MT system hypothesis and a reference translation,
it computes how much effort would be needed to
obtain the reference from the hypothesis, possibly
independently of the appropriateness of the align-
ments produced. However, if we consider instead
a pair of sentential paraphrases, it can be used to
reveal what subsentential units can be aligned. Of
course, this relies on information that will often go
beyond simple exact word matching. This is where
the capability of exploiting paraphrase matching can
come into play: TER-PLUS can exploit a table of
paraphrase pairs, and defines the cost of a phrase
substitution as ?a function of the probability of the
paraphrase and the number of edits needed to align
the two phrases without the use of phrase substitu-
tions?. Intuitively, the more parallel two sentential
paraphrases are, the more atomic paraphrase pairs
will be reliably found, and the easier it will be for
TER-PLUS to correctly identify the remaining pairs.
But in the general case, and considering less appar-
ently parallel sentence pairs, its work can be facil-
itated by the incorporation of candidate paraphrase
pairs in its paraphrase table. We consider this possi-
ble type of hybridation in the next section.
3 Informing edit rate computation with
other techniques
In this article, we use three baseline techniques
for paraphrase pair acquisition, which we will only
briefly introduce (see (Bouamor et al, 2010) for
more details). As explained previously, we want to
evaluate whether and how their candidate paraphrase
pairs can be used to improve paraphrase acquisition
on sentential paraphrases using TER-PLUS. We se-
lected these three techniques for the complementar-
ity of types of information that they use: statistical
word alignment without a priori linguistic knowl-
edge, symbolic expression of linguistic variation ex-
ploiting a priori linguistic knowledge, and syntactic
similarity.
396
Statistical Word Alignment The GIZA++
tool (Och and Ney, 2004) computes statistical word
alignment models of increasing complexity from
parallel corpora. While originally developped in the
bilingual context of Machine Translation, nothing
prevents building such models on monolingual
corpora. However, in order to build reliable models
it is necessary to use enough training material
including minimal redundancy of words. To this
end, we will be using monolingual corpora made
up of multiply-translated sentences, allowing us to
provide GIZA++ with all possible sentence pairs
to improve the quality of its word alignments (note
that following common practice we used symetrized
alignments from the alignments in both directions).
This constitutes an advantage for this technique that
the following techniques working on each sentence
pair independently do not have.
Symbolic expression of linguistic variation The
FASTR tool (Jacquemin, 1999) was designed to spot
term variants in large corpora. Variants are de-
scribed through metarules expressing how the mor-
phosyntactic structure of a term variant can be de-
rived from a given term by means of regular ex-
pressions on word categories. Paradigmatic varia-
tion can also be expressed by defining constraints
between words to force them to belong to the same
morphological or semantic family, both constraints
relying on preexisting repertoires available for En-
glish and French. To compute candidate paraphrase
pairs using FASTR, we first consider all the phrases
from the first sentence and search for variants in the
other sentence, do the reverse process and take the
intersection of the two sets.
Syntactic similarity The algorithm introduced
by Pang et al (2003) takes two sentences as in-
put and merges them by top-down syntactic fusion
guided by compatible syntactic substructure. A
lexical blocking mechanism prevents sentence con-
stituents from fusionning when there is evidence of
the presence of a word in another constituent of one
of the sentence. We use the Berkeley Probabilistic
parser (Petrov and Klein, 2007) to obtain syntac-
tic trees for English and its Bonsai adaptation for
French (Candito et al, 2010). Because this process
is highly sensitive to syntactic parse errors, we use
k-best parses (with k = 3 in our experiments) and
retain the most compact fusion from any pair of can-
didate parses.
4 Experiments and discussion
We used the methodology described by Cohn et al
(2008) for constructing evaluation corpora and as-
sessing the performance of various techniques on the
task of paraphrase acquisition. In a nutshell, pairs of
sentential paraphrases are hand-aligned and define a
set of reference atomic paraphrase pairs at the level
of words or blocks or words, denoted as Ratom, and
also a set of reference composite paraphrase pairs
obtained by joining adjacent atomic paraphrase pairs
(up to a given length), denoted as R. Techniques
output word alignments from which atomic candi-
date paraphrase pairs, denoted as Hatom, as well as
composite paraphrase pairs, denoted as H, can be
extracted. The usual measures of precision, recall
and f-measure can then be defined in the following
way:
p =
|Hatom ?R|
|Hatom|
r =
|H ? Ratom|
|Ratom|
f1 =
2pr
p + r
To evaluate our individual techniques and their
use by the tunable TER-PLUS technique (hence-
forth TERP), we measured results on two different
corpora in French and English. In each case, a held-
out development corpus of 150 paraphrase pairs was
used for tuning the TERP hybrid systems towards
precision (? p), recall (? r), or F-measure (?
f1).1 All techniques were evaluated on the same test
set consisting of 375 paraphrase pairs. For English,
we used the MTC corpus described in (Cohn et al,
2008), which consists of multiply-translated Chi-
nese sentences into English, with an average lexical
overlap2 of 65.91% (all tokens) and 63.95% (content
words only). We used as our reference set both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news articles3
obtained by translating into French from various lan-
guages with an average lexical overlap of 79.63%
(all tokens) and 78.19% (content words only). These
1Hill climbing was used for tuning as in (Snover et al,
2010), with uniform weights and 100 random restarts.
2We compute the percentage of lexical overlap be-
tween the vocabularies of two sentences S1 and S2 as :
|S1 ? S2|/min(|S1|, |S2|)
3http://www.elda.org/article125.html
397
Individual techniques Hybrid systems (TERPpara+X)
Giza++ Fastr Pang TMT TERPpara +G +F +P +G + F + P
G F P ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1
French French
p 28.99 52.48 62.50 25.66 31.35 30.26 31.43 41.99 30.55 41.14 36.74 29.65 34.84 54.49 20.94 33.89 42.27 27.06 42.80
r 45.98 8.59 8.65 41.15 44.22 44.60 44.10 35.88 45.67 35.25 40.96 43.85 44.41 13.61 40.40 40.46 31.36 44.10 31.61
f1 35.56 14.77 15.20 25.66 36.69 36.05 36.70 38.70 36.61 37.97 38.74 35.38 39.05 21.78 27.58 36.88 36.01 33.54 36.37
English English
p 18.28 33.02 36.66 20.41 31.19 19.14 19.35 26.89 19.85 21.25 41.57 20.81 22.51 31.32 18.02 18.92 29.45 16.81 29.42
r 14.63 5.41 2.23 17.37 2.31 19.38 19.69 11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35
f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02
Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems.
Column headers of the form ?? c? indicate that TERP was tuned on criterion c.
figures reveal that the French corpus tends to contain
more literal translations, possibly due to the original
languages of the sentences, which are closer to the
target language than Chinese is to English. We used
the YAWAT (Germann, 2008) interactive alignment
tool and measure inter-annotator agreement over a
subset and found it to be similar to the value reported
by Cohn et al (2008) for English.
Results for all individual techniques in the two
languages are given on Figure 2. We first note that
all techniques fared better on the French corpus than
on the English corpus. This can certainly be ex-
plained by the fact that the former results from more
literal translations, which are consequently easier to
word-align.
TERMT (i.e. TER tuned for Machine Transla-
tion evaluation) performs significantly worse on all
metrics for both languages than our tuned TERP ex-
periments, revealing that the two tasks have differ-
ent objectives. The two linguistically-aware tech-
niques, FASTR and PANG, have a very strong pre-
cision on the more parallel French corpus, and also
on the English corpus to a lesser extent, but fail to
achieve a high recall (note, in particular, that they
do not attempt to report preferentially atomic para-
phrase pairs). GIZA++ and TERPpara perform in
the same range, with acceptable precision and re-
call, TERPpara performing overall better, with e.g. a
1.14 advantage on f-measure on French and 3.27 on
English. Recall that TERP works independently on
each paraphrase pair, while GIZA++ makes use of
artificial repetitions of paraphrases of the same sen-
tence.
Figure 3 gives an indication of how well each
technique performs depending on the difficulty of
the task, which we estimate here as the value
(1? TER(para1, para2)), whose low values cor-
respond to sentences which are costly to trans-
form into the other using TER. Not surprisingly,
TERPpara and GIZA++, and PANG to a lesser ex-
tent, perform better on ?more parallel? sentential
paraphrase pairs. Conversely, FASTR is not affected
by the degree of parallelism between sentences, and
manages to extract synonyms and more generally
term variants, at any level of difficulty.
We have further tested 4 hybrid configurations
by providing TERPpara with the output of the other
individual techniques and of their union, the latter
simply obtained by taking paraphrase pairs output
by at least one of these techniques. On French,
where individual techniques achieve good perfor-
mance, any hybridation improves the F-measure
over both TERPpara and the technique used, the best
performance, using FASTR, corresponding to an im-
provement of respectively +2.35 and +24.28 over
TERPpara and FASTR. Taking the union of all tech-
niques does not yield additional gains: this might
be explained by the fact that incorrect predictions
are proportionnally more present and consequently
have a greater impact when combining techniques
without weighting them, possibly at the level of each
398
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
(a) French (b) English
Figure 3: F-measure values for our 4 individual techniques on French and English depending on the complexity of
paraphrase pairs measured with the (1-TER) formula. Note that each value corresponds to the average of F-measure
values for test examples falling in a given difficulty range, and that all ranges do not necessarily contain the same
number of examples.
prediction.4 Successful hybridation on English seem
harder to obtain, which may be partly attributed to
the poor quality of the individual techniques relative
to TERPpara. We however note anew an improve-
ment over TERPpara of +1.81 when using FASTR.
This confirms that some types of linguistic equiva-
lences cannot be captured using edit rate computa-
tion alone, even on this type of corpus.
5 Conclusion and future work
In this article, we have described the use of edit rate
computation for paraphrase alignment at the sub-
sentential level from sentential paraphrases and the
possibility of informing this search with paraphrase
candidates coming from other techniques. Our ex-
periments have shown that in some circumstances
some techniques have a good complementarity and
manage to improve results significantly. We are
currently studying hard-to-align subsentential para-
phrases from the type of corpora we used in order to
get a better understanding of the types of knowledge
required to improve automatic acquisition of these
units.
4Indeed, measuring the precision on the union yields a poor
performance of 23.96, but with the highest achievable value of
50.56 for recall. Similarly, the maximum value for precision
with a good recall can be obtained by taking the intersection of
the results of TERPpara and GIZA++, which yields a value of
60.39.
Our future work also includes the acquisition of
paraphrase patterns (e.g. (Zhao et al, 2008)) to gen-
eralize the acquired equivalence units to more con-
texts, which could be both used in applications and
to attempt improving further paraphrase acquisition
techniques. Integrating the use of patterns within an
edit rate computation technique will however raise
new difficulties.
We are finally also in the process of conducting
a careful study of the characteristics of the para-
phrase pairs that each technique can extract with
high confidence, so that we can improve our hybri-
dation experiments by considering confidence val-
ues at the paraphrase level using Machine Learning.
This way, we may be able to use an edit rate com-
putation algorithm such as TER-PLUS as a more
efficient system combiner for paraphrase extraction
methods than what was proposed here. A poten-
tial application of this would be an alternative pro-
posal to the paraphrase evaluation metric PARAMET-
RIC (Callison-Burch et al, 2008), where individual
techniques, outputing word alignments or not, could
be evaluated from the ability of the informated edit
rate technique to use correct equivalence units.
Acknowledgments
This work was partly funded by a grant from LIMSI.
The authors wish to thank the anonymous reviewers
for their useful comments and suggestions.
399
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2010.
Comparison of Paraphrase Acquisition Techniques on
Sentential Paraphrases. In Proceedings of IceTAL, Re-
jkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical French dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350?356, Geneva,
Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-08: HLT
Demo Session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, pages 341?348, College Park, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC, Valletta, Malta.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings of ACL-
HLT, Columbus, USA.
400
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 36?43
Manchester, August 2008
Large Scale Production of Syntactic Annotations to Move Forward
Patrick Paroubek, Anne Vilnat, Sylvain Loiseau
LIMSI-CNRS
BP 133 91403 Orsay Cedex
France
prenom.nom@limsi.fr
Gil Francopoulo
Tagmatica
126 rue de Picpus 75012 Paris
France
gil.francopoulo@tagmatica.com
Olivier Hamon
ELDA and LIPN-P13
55-57 rue Brillat-Savarin 75013 Paris,
France
hamon@elda.org
Eric Villemonte de la Clergerie
Alpage-INRIA
Dom. de Voluceau Rocquencourt,
B.P. 105, 78153 Le Chesnay, France
Eric.De La Clergerie@inria.fr
Abstract
This article presents the methodology of
the PASSAGE project, aiming at syntacti-
cally annotating large corpora by compos-
ing annotations. It introduces the anno-
tation format and the syntactic annotation
specifications. It describes an important
component of the methodolgy, namely an
WEB-based evaluation service, deployed
in the context of the first PASSAGE parser
evaluation campaign.
1 Introduction
The last decade has seen, at the international level,
the emergence of a very strong trend of researches
on statistical methods in Natural Language Pro-
cessing. In our opinion, one of its origins, in
particular for English, is the availability of large
annotated corpora, such as the Penn Treebank
(1M words extracted from the Wall Street journal,
with syntactic annotations; 2
nd
release in 1995
1
,
the British National Corpus (100M words cover-
ing various styles annotated with parts of speech
2
),
or the Brown Corpus (1M words with morpho-
syntactic annotations). Such annotated corpora
were very valuable to extract stochastic grammars
or to parametrize disambiguation algorithms. For
instance (Miyao et al, 2004) report an experiment
where an HPSG grammar is semi-automatically
aquired from the Penn Treebank, by first annotat-
ing the treebank with partially specified derivation
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.cis.upenn.edu/
?
treebank/
2
http://www.natcorp.ox.ac.uk/
trees using heuristic rules , then by extracting lex-
ical entries with the application of inverse gram-
mar rules. (Cahill et al, 2004) managed to ex-
tract LFG subcategorisation frames and paths link-
ing long distance dependencies reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and used them in an long distance
dependency resolution algorithm to parse new text.
They achieved around 80% f-score for fstructures
parsing on the WSJ part of the Penn-II treebank,
a score comparable to the ones of the state-of-
the-art hand-crafted grammars. With similar re-
sults, (Hockenmaier and Steedman, 2007) trans-
lated the Penn Treebank into a corpus of Combina-
tory Categorial Grammar (CCG) derivations aug-
mented with local and long-range word to word
dependencies and used it to train wide-coverage
statistical parsers. The development of the Penn
Treebank have led to many similar proposals of
corpus annotations
3
. However, the development of
such treebanks is very costly from an human point
of view and represents a long standing effort, in
particular for getting of rid of the annotation errors
or inconsistencies, unavoidable for any kind of hu-
man annotation. Despite the growing number of
annotated corpora, the volume of data that can be
manually annotated remains limited thus restrict-
ing the experiments that can be tried on automatic
grammar acquisition. Furthermore, designing an
annotated corpus involves choices that may block
future experiments from acquiring new kinds of
linguistic knowledge because they necessitate an-
notation incompatible or difficult to produce from
the existing ones.
With PASSAGE (de la Clergerie et al, 2008b),
we believe that a new option becomes possible.
3
http://www.ims.uni-stuttgart.de/
projekte/TIGER/related/links.shtml
36
Funded by the French ANR program on Data
Warehouses and Knowledge, PASSAGE is a 3-
year project (2007?2009), coordinated by INRIA
project-team Alpage. It builds up on the re-
sults of the EASy French parsing evaluation cam-
paign, funded by the French Technolangue pro-
gram, which has shown that French parsing sys-
tems are now available, ranging from shallow to
deep parsing. Some of these systems were nei-
ther based on statistics, nor extracted from a tree-
bank. While needing to be improved in robustness,
coverage, and accuracy, these systems has nev-
ertheless proved the feasibility to parse medium
amount of data (1M words). Preliminary experi-
ments made by some of the participants with deep
parsers (Sagot and Boullier, 2006) indicate that
processing more than 10 M words is not a prob-
lem, especially by relying on clusters of machines.
These figures can even be increased for shallow
parsers. In other words, there now exists sev-
eral French parsing systems that could parse (and
re-parse if needed) large corpora between 10 to
100 M words.
Passage aims at pursuing and extending the
line of research initiated by the EASy campaign
by using jointly 10 of the parsing systems that
have participated to EASy. They will be used to
parse and re-parse a French corpus of more than
100 M words along the following feedback loop
between parsing and resource creation as follows
(de la Clergerie et al, 2008a):
1. Parsing creates syntactic annotations;
2. Syntactic annotations create or enrich linguis-
tic resources such as lexicons, grammars or
annotated corpora;
3. Linguistic resources created or enriched on
the basis of the syntactic annotations are then
integrated into the existing parsers;
4. The enriched parsers are used to create richer
(e.g., syntactico-semantic) annotations;
5. etc. going back to step 1
In order to improve the set of parameters of
the parse combination algorithm (inspired from
the Recognizer Output Voting Error Reduction,
i.e. ROVER, experiments), two parsing evalu-
ation campaigns are planned during PASSAGE,
the first of these already took place at the end of
2007 (de la Clergerie et al, 2008b). In the follow-
ing, we present the annotation format specification
and the syntactic annotation specifications of PAS-
SAGE, then give an account of how the syntactic
annotations were compared in the first evaluation
campaign, by first describing the evaluation met-
rics and the web server infrastructure that was de-
ployed to process them. We conclude by showing
how the results so far achieved in PASSAGE will
contribute to the second part of the project, extract-
ing and refining enriched linguistic annotations.
2 PASSAGE Annotation Format
The aim is to allow an explicit representation of
syntactic annotations for French, whether such an-
notations come from human annotators or parsers.
The representation format is intended to be used
both in the evaluation of different parsers, so the
parses? representations should be easily compara-
ble, and in the construction of a large scale anno-
tation treebank which requires that all French con-
structions can be represented with enough details.
The format is based on three distinct specifica-
tions and requirements:
1. MAF (ISO 24611)
4
and SynAF (ISO 24615)
5
which are the ISO TC37 specifications for
morpho-syntactic and syntactic annotation
(Ide and Romary, 2002) (Declerck, 2006)
(Francopoulo, 2008). Let us note that these
specifications cannot be called ?standards?
because they are work in progress and these
documents do not yet have the status Pub-
lished Standard. Currently, their official sta-
tus is only Committee Draft.
2. The format used during the previous TECH-
NOLANGUE/EASY evaluation campaign
in order to minimize porting effort for the ex-
isting tools and corpora.
3. The degree of legibility of the XML tagging.
From a technical point of view, the format is a
compromise between ?standoff? and ?embedded?
notation. The fine grain level of tokens and words
is standoff (wrt the primary document) but higher
levels use embedded annotations. A standoff nota-
tion is usually considered more powerful but less
4
http://lirics.loria.fr/doc pub/maf.pdf
5
http://lirics.loria.fr/doc pub/
N421 SynAF CD ISO 24615.pdf
37
Figure 1: UML diagram of the structure of an an-
notated document
readable and not needed when the annotations fol-
low a (unambiguous) tree-like structure. Let us
add that, at all levels, great care has been taken to
ensure that the format is mappable onto MAF and
SynAF, which are basically standoff notations.
The structure of a PASSAGE annotated docu-
ment may be summarized with the UML diagram
in Figure1. The document begins by the declara-
tion of all the morpho-syntactic tagsets (MSTAG)
that will be used within the document. These dec-
larations respect the ISO Standard Feature Struc-
ture Representation (ISO 24610-1). Then, tokens
are declared. They are the smallest unit address-
able by other annotations. A token is unsplittable
and holds an identifier, a character range, and a
content made of the original character string. A
word form is an element referencing one or sev-
eral tokens. It has has two mandatory attributes:
an identifier and a list of tokens. Some optional at-
tributes are allowed like a part of speech, a lemma,
an inflected form (possibly after spelling correc-
tion or case normalization) and morpho-syntactic
tags. The following XML fragment shows how
the original fragment ?Les chaises? can be repre-
sented with all the optional attributes offered by
the PASSAGE annotation format :
<T id="t0" start="0" end="3">
Les
</T>
<W id="w0" tokens="t0"
pos="definiteArticle"
lemma="le"
form="les"
mstag="nP"/>
<T id="t1" start="4" end="11">
chaises
</T>
<W id="w1" tokens="t1"
pos="commonNoun"
lemma="chaise"
form="chaises"
mstag="nP gF"/>
Note that all parts of speech are taken from the
ISO registry
6
(Francopoulo et al, 2008). As in
MAF, a word may refer to several tokens in or-
der to represent multi-word units like ?pomme de
terre?. Conversely, a unique token may be refered
by two different words in order to represent results
of split based spelling correction like when ?un-
etable? is smartly separated into the words ?une?
and ?table?. The same configuration is required to
represent correctly agglutination in fused preposi-
tions like the token ?au? that may be rewritten into
the sequence of two words ?`a? ?le?. On the con-
trary of MAF, cross-reference in token-word links
for discontiguous spans is not allowed for the sake
of simplicity. Let us add that one of our require-
ment is to have PASSAGE annotations mappable
onto the MAF model and not to map all MAF an-
notations onto PASSAGE model. A G element de-
notes a syntactic group or a constituent (see details
in section 3). It may be recursive or non-recursive
and has an identifier, a type, and a content made of
word forms or groups, if recursive. All group type
values are taken from the ISO registry. Here is an
example :
<T id="t0" start="0" end="3">
Les
</T>
<T id="t1" start="4" end="11">
chaises
</T>
<G id="g0" type="GN">
<W id="w0" tokens="t0"/>
<W id="w1" tokens="t1"/>
</G>
A group may also hold optional attributes like syn-
tactic tagsets of MSTAG type. The syntactic re-
lations are represented with a standoff annotations
which refer to groups and word forms. A relation
is defined by an identifier, a type, a source, and a
target (see details in section 3. All relation types,
like ?subject? or ?direct object? are mappable onto
the ISO registry. An unrestricted number of com-
ments may be added to any element by means of
the mark element (i.e. M). Finally, a ?Sentence?
6
Data Category Registry, see http://syntax.
inist.fr
38
element gathers tokens, word forms, groups, rela-
tions and marks and all sentences are included in-
side a ?Document? element.
3 PASSAGE Syntactic Annotation
Specification
3.1 Introduction
The annotation formalism used in PASSAGE
7
is
based on the EASY one(Vilnat et al, 2004) which
whose first version was crafted in an experimental
project PEAS (Gendner et al, 2003), with inspira-
tion taken from the propositions of (Carroll et al,
2002). The definition has been completed with the
input of all the actors involved in the EASY evalu-
ation campaign (both parsers? developers and cor-
pus providers) and refined with the input of PAS-
SAGE participants. This formalism aims at mak-
ing possible the comparison of all kinds of syn-
tactic annotation (shallow or deep parsing, com-
plete or partial analysis), without giving any ad-
vantage to any particular approach. It has six
kinds of syntactic ?chunks?, we call constituents
and 14 kinds of relations The annotation formal-
ism allows the annotation of minimal, continuous
and non recursive constituents, as well as the en-
coding of relations wich represent syntactic func-
tions. These relations (all of them being binary, ex-
cept for the ternary coordination) have sources and
targets which may be either forms or constituents
(grouping several forms). Note that the PASSAGE
annotation formalism does not postulate any ex-
plicit lexical head.
3.2 Constituent annotations
For the PASSAGE campaigns, 6 kinds of con-
stituents (syntactic ?chunks?) have been consid-
ered and are illustrated in Table 3.2:
? the Noun Phrase (GN for Groupe Nominal)
may be made of a noun preceded by a de-
terminer and/or by an adjective with its own
modifiers, a proper noun or a pronoun;
? the prepositional phrase (GP, for groupe
pr?epositionnel ) may be made of a preposi-
tion and the GN it introduces, a contracted
determiner and preposition, followed by the
introduced GN, a preposition followed by an
adverb or a relative pronoun replacing a GP;
7
Annotation guide: http://www.limsi.fr/
Recherche/CORVAL/PASSAGE/eval 1/2007 10
05PEAS reference annotations v11.12.html
? the verb kernel (NV for noyau verbal ) in-
cludes a verb, the clitic pronouns and possible
particles attached to it. Verb kernels may have
different forms: conjugated tense, present or
past participle, or infinitive. When the con-
jugation produces compound forms, distinct
NVs are identified;
? the adjective phrase (GA for groupe adjec-
tival) contains an adjective when it is not
placed before the noun, or past or present par-
ticiples when they are used as adjectives;
? the adverb phrase (GR for groupe adverbial )
contains an adverb;
? the verb phrase introduced by a preposition
(PV) is a verb kernel with a verb not inflected
(infinitive, present participle,...), introduced
by a preposition. Some modifiers or adverbs
may also be included in PVs.
GN - la tr`es grande porte
8
(the very big door);
- Rouletabille
- eux (they), qui (who)
GP - de la chambre (from the bedroom),
- du pavillon (from the lodge)
- de l`a (from there), dont (whose)
NV - j?entendais (I heared)
- [on ne l?entendait]
9
plus
(we could no more hear her)
- Jean [viendra] (Jean will come)
- [d?esob?eissant] `a leurs parents
(disobeying their parents),
- [ferm?ee] `a clef (key closed)
- Il [ne veut] pas [venir]
(He doesn?t want to come),
- [ils n??etaient] pas [ferm?es]
(they were not closed),
GA - les barreaux [intacts] (the intact bars)
- la solution [retenue] fut...
(the chosen solution has been...),
- les enfants [d?esob?eissants]
(the disobeying children)
GR - aussi (also)
- vous n?auriez [pas] (you would not)
PV - [pour aller] `a Paris (for going to Paris),
- de vraiment bouger (to really move)
Table 1: Constituent examples
39
3.2.1 Syntactic Relation annotations
The dependencies establish all the links between
the minimal constituents described above. All par-
ticipants, corpus providers and campaign organiz-
ers agreed on a list of 14 kinds of dependencies
listed below:
1. subject-verb (SUJ V): may be inside the
same NV as between elle and ?etait in elle
?etait (she was), or between a GN and a NV as
between mademoiselle and appelait in Made-
moiselle appelait (Miss was calling);
2. auxiliary-verb (AUX V), between two NVs
as between a and construit in: on a construit
une maison (we have built a house);
3. direct object-verb (COD V): the relation is
annotated between a main verb (NV) and a
noun phrase (GN), as between construit and
la premi`ere automobile in: on a construit la
premi`ere automobile (we have built the first
car);
4. complement-verb (CPL V): to link to the
verb the complements expressed as GP or PV
which may be adjuncts or indirect objects, as
between en quelle ann?ee and construit in en
quelle ann?ee a-t on construit la premi`ere au-
tomobile (In which year did we build the first
car);
5. modifier-verb (MOD V): concerns the con-
stituants which certainly modify the verb,
and are not mandatory, as adverbs or adjunct
clauses, as between profond?ement or quand
la nuit tombe and dort in Jean dort pro-
fond?ement quand la nuit tombe (Jean deeply
sleeps when the night falls);
6. complementor (COMP): to link the intro-
ducer and the verb kernel of a subordinate
clause, as between qu? and viendra in Je
pense qu?il viendra (I think that he will
come); it is also used to link a preposition and
a noun phrase when they are not contiguous,
preventing us to annotate them as GP;
7. attribute-subject/object (ATB SO): between
the attribute and the verb kernel, and precis-
ing that the attribute is relative to (a) the sub-
ject as between grand and est in il est grand
), or (b) the object as between ?etrange and
trouve in il trouve cette explication ?etrange;
8. modifier-noun (MOD N): to link to the noun
all the constituents which modify it, as the ad-
jective, the genitive, the relative clause... This
dependency is annotated between unique and
fen?etre in l?unique fen?etre (the unique win-
dow) or between de la chambre and la porte
in la porte de la chambre (the bedroom door);
9. modifier-adjective (MOD A): to relate to the
adjective the constituents which modify it, as
between tr`es et belle in ?la tr`es belle collec-
tion (the very impressive collection) or be-
tween de son fils and fi`ere in elle est fi`ere de
son fils (she is proud of her son);
10. modifier-adverb (MOD R): the same kind of
dependency than MOD A for the adverbs, as
between tr`es and gentiment in elle vient tr`es
gentiment (she comes very kindly);
11. modifier-preposition (MOD P): to relate to
a preposition what modifies it, as between
peu and avant in elle vient peu avant lui (she
comes just before him);
12. coordination (COORD): to relate the coor-
dinate and the coordinated elements, as be-
tween Pierre, Paul and et in Pierre et Paul
arrivent (Paul and Pierre are arriving);
13. apposition (APP): to link the elements which
are placed side by side, when they refer to the
same object, as between le d?eput?e and Yves
Tavernier in Le d?eput?e Yves Tavernier ... (the
Deputy Yves Tavernier...);
14. juxtaposition (JUXT): to link constituents
which are neither coordinate nor in an appo-
sition relation, as in enumeration. It also links
clauses as on ne l?entendait et elle ?etait in
on ne l? entendait plus ... elle ?etait peut-?etre
morte (we did not hear her any more... per-
haps she was dead).
Some dependencies are illustrated in the two an-
notated sentences illutrated in figure . These anno-
tations have been made using EasyRef, a specific
Web annotation tool developed by INRIA.
4 PASSAGE First Evaluation Campaign
4.1 Evalution Service
The first PASSAGE evaluation campaign was
carried out in two steps. During the ini-
tial one-month development phase, a develop-
ment corpus was used to improve the quality of
40
Figure 2: Example of two sentences annotations
parsers. This development corpus from the TECH-
NOLANGUE/EASY is composed of 40,000 sen-
tences, out of which 4,000 sentences have been
manually annotated for the gold standard. Based
on these annotated sentences, an automatic WEB-
based evaluation server provides fast performance
feedback to the parsers? developers. At the end
of this first phase, each participant indicated what
he thought was his best parser run and got evalu-
ated on a new set of 400 sentences selected from
another part of the developement corpus which
meanwhile had been manually annotated for the
purpose and kept undisclosed.
The two phases represent a strong effort for the
evaluators. To avoid adding the cost of managing
the distribution and installation of the evaluation
package at each developer?s site, the solution of the
WEB evaluation service was chosen. A few infras-
tructures have been already experimented in NLP,
like GATE (Cunningham et al, 2002) infrastruc-
tures, but to our knowledge none has been used to
provide an WEB-based evaluation service as PAS-
SAGE did. The server was designed to manage
two categories of users: parser developers and or-
ganizers. To the developers, it provides, almost in
real time, confidential and secure access to the au-
tomatic evaluation of their submitted parses. To
the organizers, it give access to statistics enabling
them to follow the progress made by the develop-
ers, and easy management of the test phase. The
evaluation server provides, through a simple WEB
browser, access to both coarse and fine grain statis-
tics to a developer?s performance evaluation, glob-
ally for the whole corpus, at the level of a partic-
ular syntactic annotation or of a particular genre
specific subcorpus, and also at the level of a single
annotation for a particular word form.
Figure 3: Overall functional relations results
4.2 Performance Results
Ten systems participated to the constituents anno-
tation task. For most of the systems, F-measure is
up to 90% and only three systems are between 80%
and 90%. The trend is quite the same for Recall
and Precision. Around 96.5% of the constituents
returned by the best system are correct and it found
95.5% of the constituents present in gold standard.
Figure 3 shows the results of the seven systems that
participated to the functional relations annotation
task. Performance is lower than for constituents
and differences between systems are larger, an evi-
dence that the task remains more difficult. No sys-
tems gets a performance above 70% in F-measure,
three are above 60% and two above 50%. The last
two systems are above 40%.
4.3 Systems Improvements
The higher system gets increasing results from the
beginning of the development phase to the test
phase for both constituents and relations. How-
ever, although the increase for relations is rather
continuous, constituents results grow during the
first few development evaluations, then reach a
threshold from which results do not vary. This
can be explained by the fact that the constituent
scores are rather high, while for relations, scores
are lower and starting from low scores.
Using the evaluation server, system improves
its performance by 50% for the constituents and
600% for the relations, although performance vary
according to the type of relation or constituent.
Moreover, in repeating development evaluations,
another consequence was the convergence of pre-
cision and recall.
41
5 Parser?s outputs combination
The idea to combine the output of systems partic-
ipating to an evalauation campaign in order to ob-
tain a combination with better performance than
the best one was invented to our knowledge by J.
Fiscus (Fiscus, 1997) in a DARPA/NIST speech
recognition evaluation (ROVER/Reduced Output
Voting Error Reduction). By aligning the out-
put of the participating speech transcription sys-
tems and by selecting the hypothesis which was
proposed by the majority of the systems, he ob-
tained better performances than these of the best
system. The idea gained support in the speech pro-
cessing community(L?o?of et al, 2007) and in gen-
eral better results are obtained with keeping only
the output of the two or three best performing sys-
tems, in which case the relative improvement can
go up to 20% with respect to the best performance
(Schwenk and Gauvain, 2000). For text process-
ing, the ROVER procedure was applied to POS
tagging (Paroubek, 2000) and machine translation
(Matusov et al, 2006).
In our case, we will use the text itself to realign
the annotations provided by the various parser be-
fore computing their combination, as we did for
our first experiments with the EASY evaluation
campaign data (Paroubek et al, 2008). Since it
is very likely taht the different parsers do not use
the same word and sentence segmentation, we will
realign all the data along a common word and sen-
tence segmentation obtained by majority vote from
the different outputs.
But our motivation for using such procedure
is not only concerned with performance improve-
ment but also with the obtention of a confidence
measure for the annotation since if all systems
agree on a particular annotation, then it is very
likely to be true.
At this stage many options are open for the way
we want to apply the ROVER algorithm, since we
have both constituents and relations in our anno-
tations. We could vary the selection order (be-
tween constituents and relations), or use differ-
ent comparison functions for the sources/targets of
constituents/relations(Patrick Paroubek, 2006), or
perform incremental/global merging of the annoa-
tions, or explore different weightings/thresholding
strategies etc. In passage, ROVER experiments
are only beginning and we have yet to determine
which is the best strategy before applying it to
word and sentence free segmentation data. In the
early experiment we did with the ?EASy classic?
PASSAGE track which uses a fixed word and sen-
tence segmentation, we measured an improvement
in precision for some specific subcorpora and an-
notations but improvement in recall was harder to
get.
6 Conclusion
The definition of a common interchange syntactic
annotation format is an essential element of any
methodology aiming at the creation of large an-
notated corpora from the cooperation of parsing
systems to acquire new linguistic knowledge. But
the formalism aquires all of its value when backed-
up by the deployment of a WEB-based evaluation
service as the PASSAGE examples shows. 167
experiments were carried out during the develop-
ment phase (around 17 experiments per participant
in one month). The results of the test phase were
available less than one hour after the end of the de-
velopment phase. The service proved so success-
ful that the participants asked after the evaluation,
that the evaluation service be extended to support
evaluation as a perennial service
References
Cahill, Aoife, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 319?326, Barcelona, Spain, July.
Carroll, J., D. Lin, D. Prescher, and H. Uszkoreit.
2002. Proceedings of the workshop beyond parse-
val - toward improved evaluation measures for pars-
ing systems. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC), Las Palmas, Spain.
Cunningham, Hamish, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. Gate:
an architecture for development of robust hlt ap-
plications. In ACL ?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 168?175, Morristown, NJ, USA.
Association for Computational Linguistics.
Declerck, T. 2006. Synaf: towards a standard for syn-
tactic annotation. In In proceedings of the fifth in-
ternational conference on Language Resources and
Evaluation (LREC 2006), Genoa, Italy, May. ELRA.
Fiscus, Jonathan G. 1997. A post-processing system
to yield reduced word error rates: recognizer output
voting error reduction (rover). In In proceedings of
42
the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, pages 347?357, Santa Bar-
bara, CA.
Francopoulo, G., T. Declerck, V. Sornlertlamvanich,
E. de la Clergerie, and M. Monachini. 2008. Data
category registry: Morpho-syntactic and syntactic
profiles. Marrakech. LREC.
Francopoulo, Gil. 2008. Tagparser: Well on the way
to iso-tc37 conformance. In In proceedings of the
International Conference on Global Interoperability
for Language Resources (ICGL), pages 82?88, Hong
Kong, January.
Gendner, V?eronique, Gabriel Illouz, Mich`ele Jardino,
Laura Monceaux, Patrick Paroubek, Isabelle Robba,
and Anne Vilnat. 2003. Peas the first instanciation
of a comparative framework for evaluating parsers of
french. In Proceedings of the 10
th
Conference of the
European Chapter fo the Association for Computa-
tional Linguistics, pages 95?98, Budapest, Hungary,
April. ACL. Companion Volume.
Hockenmaier, Julia and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355?396.
Ide, N. and L. Romary. 2002. Standards for language
ressources. Las Palmas. LREC.
L?o?of, J., C. Gollan, S. Hahn, G. Heigold, B. Hoffmeis-
ter, C. Plahl, D. Rybach, R. Schl?uter, , and H. Ney.
2007. The rwth 2007 tc-star evaluation system for
european english and spanish. In In proceedings of
the Interspeech Conference, pages 2145?2148.
Matusov, Evgeny, N. Ueffing, and Herman Ney. 2006.
Automatic sentence segmentation and punctuation
prediction for spoken language translation. In Pro-
ceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 158?165,
Trento, Italy.
de la Clergerie, Eric, Christelle Ayache, Ga?el de Chal-
endar, Gil Francopoulo, Claire Gardent, and Patrick
Paroubek. 2008a. Large scale production of syntac-
tic annotations for french. In In proceedings of the
First Workshop on Automated Syntactic Annotations
for Interoperable Language Resources at IGCL?08,
pages 45?52, Hong Kong, January.
de la Clergerie, Eric, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek, and Anne Vil-
nat. 2008b. Passage: from french parser evalua-
tion to large sized treebank. In ELRA, editor, In
proceedings of the sixth international conference on
Language Resources and Evaluation (LREC), Mar-
rakech, Morroco, May. ELRA.
Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Paroubek, Patrick, Isabelle Robba, Anne Vilnat, and
Christelle Ayache. 2008. Easy, evaluation of parsers
of french: what are the results? In Proceedings of
the 6
th
International Conference on Language Re-
sources and Evaluation (LREC), Marrakech, Mor-
roco.
Paroubek, Patrick. 2000. Language resources as by-
product of evaluation: the multitag example. In
In proceedings of the Second International Con-
ference on Language Resources and Evaluation
(LREC2000), volume 1, pages 151?154.
Patrick Paroubek, Isabelle Robba, Anne Vilnat Chris-
telle Ayache. 2006. Data, annotations and mea-
sures in easy - the evaluation campaign for parsers
of french. In ELRA, editor, In proceedings of
the fifth international conference on Language Re-
sources and Evaluation (LREC 2006), pages 315?
320, Genoa, Italy, May. ELRA.
Sagot, Beno??t and Pierre Boullier. 2006. Efficient
parsing of large corpora with a deep lfg parser. In
In proceedings of the sixth international conference
on Language Resources and Evaluation (LREC),
Genoa, Italy, May. ELDA.
Schwenk, Holger and Jean-Luc Gauvain. 2000. Im-
proved rover using language model information. In
In proceedings of the ISCA ITRW Workshop on Au-
tomatic Speech Recognition: Challenges for the new
Millenium, pages 47?52, Paris, September.
Vilnat, A., P. Paroubek, L. Monceaux, I. Robba,
V. Gendner, G. Illouz, and M. Jardino. 2004. The
ongoing evaluation campaign of syntactic parsing of
french: Easy. In Proceedings of the 4
th
International
Conference on Language Resources and Evaluation
(LREC), pages 2023?2026, Lisbonne, Portugal.
43
Workshop on Monolingual Text-To-Text Generation, pages 10?19,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Web-based validation for contextual targeted paraphrasing
Houda Bouamor
LIMSI-CNRS
Univ. Paris Sud
hbouamor@limsi.fr
Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Gabriel Illouz
LIMSI-CNRS
Univ. Paris Sud
gabrieli@limsi.fr
Anne Vilnat
LIMSI-CNRS
Univ. Paris Sud
anne@limsi.fr
Abstract
In this work, we present a scenario where con-
textual targeted paraphrasing of sub-sentential
phrases is performed automatically to support
the task of text revision. Candidate para-
phrases are obtained from a preexisting reper-
toire and validated in the context of the orig-
inal sentence using information derived from
the Web. We report on experiments on French,
where the original sentences to be rewrit-
ten are taken from a rewriting memory au-
tomatically extracted from the edit history of
Wikipedia.
1 Introduction
There are many instances where it is reasonable to
expect machines to produce text automatically. Tra-
ditionally, this was tackled as a concept-to-text real-
ization problem. However, such needs apply some-
times to cases where a new text should be derived
from some existing texts, an instance of text-to-text
generation. The general idea is not anymore to pro-
duce a text from data, but to transform a text so as to
ensure that it has desirable properties appropriate for
some intended application (Zhao et al, 2009). For
example, one may want a text to be shorter (Cohn
and Lapata, 2008), tailored to some reader pro-
file (Zhu et al, 2010), compliant with some spe-
cific norms (Max, 2004), or more adapted for sub-
sequent machine processing tasks (Chandrasekar et
al., 1996). The generation process must produce
a text having a meaning which is compatible with
the definition of the task at hand (e.g. strict para-
phrasing for document normalization, relaxed para-
phrasing for text simplification), while ensuring that
it remains grammatically correct. Its complexity,
compared with concept-to-text generation, mostly
stems from the fact that the semantic relationship
between the original text and the new one is more
difficult to control, as the mapping from one text to
another is very dependent on the rewriting context.
The wide variety of techniques for acquiring phrasal
paraphrases, which can subsequently be used by text
paraphrasing techniques (Madnani and Dorr, 2010),
the inherent polysemy of such linguistic units and
the pragmatic constraints on their uses make it im-
possible to ensure that potential paraphrase pairs
will be substitutable in any context, an observation
which was already made at a lexical level (Zhao et
al., 2007). Hence, automatic contextual validation of
candidate rewritings is a fundamental issue for text
paraphrasing with phrasal units.
In this article, we tackle the problem of what we
call targeted paraphrasing, defined as the rewriting
of a subpart of a sentence, as in e.g. (Resnik et al,
2010) where it is applied to making parts of sen-
tences easier to translate automatically. While this
problem is simpler than full sentence rewriting, its
study is justified as it should be handled correctly
for the more complex task to be successful. More-
over, being simpler, it offers evaluation scenarios
which make the performance on the task easier to
assess. Our particular experiments here aim to as-
sist a Wikipedia contributor in revising a text to im-
prove its quality. For this, we use a collection of
phrases that have been rewritten in Wikipedia, and
test the substitutability of paraphrases coming from
a repertoire of sub-sentential paraphrases acquired
10
from different sources. We thus consider that preex-
isting repertoires of sub-sentential paraphrase pairs
are available, and that each potential candidate has to
be tested in the specific context of the desired rewrit-
ing. Due to the large variety of potential phrases
and their associated known paraphrases, we do not
rely on precomputed models of substitutability, but
rather build them on-the-fly using information de-
rived from web queries.1
This article is organized as follows. In section 2,
we first describe the task of text revision, where a
subpart of a sentence is rewritten, as an instance
of targeted paraphrasing. Section 3 presents previ-
ous works on the acquisition of sub-sentential para-
phrases and describes the knowledge sources that we
have used in this work. We then describe in section 4
how we estimate models of phrase substitution in
context by exploiting information coming from the
web. We present our experiments and their results in
section 5, and finally discuss our current results and
future work in section 6.
2 Targeted paraphrasing for text revision
One of the important processes of text revision is
the rewording of parts of sentences. Some reword-
ings are not intended to alter meaning significantly,
but rather to make text more coherent and easier to
comprehend. Those instances which express close
meanings are sub-sentential paraphrases: in their
simpler form, they can involve synonym substitu-
tion, but they can involve more complex deeper
lexical-syntactic transformations.
Such rephrasings are commonly found in record-
ings of text revisions, which now exist in large
quantities in the collaborative editing model of
Wikipedia. In fact, revision histories of the encyclo-
pedia contain a significant amount of sub-sentential
paraphrases, as shown by the study of (Dutrey et al,
2011). This study also reports that there is an impor-
tant variety of rephrasing phenomena, as illustrated
by the difficulty of reaching a good identification
coverage using a rule-based term variant identifica-
tion engine.
1Note that using the web may not always be appropriate, or
that at least it should be used in a different way than what we
propose in this article, in particular in cases where the desired
properties of the rewritten text are better described in controlled
corpora.
The use of automatic targeted paraphrasing as an
authoring aid has been illustrated by the work of
Max and Zock (2008), in which writers are pre-
sented with potential paraphrases of sub-sentential
fragments that they wish to reword. The automatic
paraphrasing technique used is a contextual vari-
ant of bilingual translation pivoting (Bannard and
Callison-Burch, 2005). It has also been proposed
to externalize various text editing tasks, including
proofreading, by having crowdsourcing functions on
text directly from word processors (Bernstein et al,
2010).
Text improvements may also be more specifi-
cally targeted for automatic applications. In the
work by Resnik et al (2010), rephrasings for spe-
cific phrases are acquired through crowdsourcing.
Difficult-to-translate phrases in the source text are
first identified, and monolingual contributors are
asked to provide rephrasings in context. Collected
rephrasings can then be used as input for a Ma-
chine Translation system, which can positively ex-
ploit the increased variety in expression to pro-
duce more confident translations for better estimated
source units (Schroeder et al, 2009).2 For instance,
the phrase in bold in the sentence The number of
people known to have died has now reached 358
can be rewritten as 1) who died, 2) identified to
have died and 3) known to have passed away. All
such rephrasings are grammatically correct, the first
one being significantly shorter, and they all convey
a meaning which is reasonably close to the original
wording.
The task of rewriting complete sentences has also
been addressed in various works (e.g. (Barzilay and
Lee, 2003; Quirk et al, 2004; Zhao et al, 2010)). It
poses, however, numerous other challenges, in par-
ticular regarding how it could be correctly evalu-
ated. Human judgments of whole sentence trans-
formations are complex and intra- and inter-judge
coherence is difficult to attain with hypotheses of
comparable quality. Using sentential paraphrases
to support a given task (e.g. providing alternative
reference translations for optimizing Statistical Ma-
chine Translation systems (Madnani et al, 2008))
2It is to be noted that, in the scenario presented in (Resnik et
al., 2010), monolingual contributors cannot predict how useful
their rewritings will be to the underlying Machine Translation
engine used.
11
can be seen as a proxy for extrinsic evaluation of
the quality of paraphrases, but it is not clear from
published results that improvements on the task are
clearly correlated with the quality of the produced
paraphrases. Lastly, automatic metrics have been
proposed for evaluating the grammaticality of sen-
tences (e.g. (Mutton et al, 2007)). Automatic evalu-
ation of sentential paraphrases has not produced any
consensual results so far, as they do not integrate
task-specific considerations and can be strongly bi-
ased towards some paraphrasing techniques.
In this work, we tackle the comparatively more
modest task of sub-sentential paraphrasing applied
to text revision. In order to use an unbiased
task, we use a corpus of naturally-occurring rewrit-
ings from an authoring memory of Wikipedia ar-
ticles. We use the WICOPACO corpus (Max and
Wisniewski, 2010), a collection of local rephras-
ings from the edit history of Wikipedia which con-
tains instances of lexical, syntactical and semantic
rephrasings (Dutrey et al, 2011), the latter type be-
ing illustrated by the following example:
Ce vers de Nuit rhe?nane d?Apollinaire [qui para??t
presque sans structure rythmique? dont la ce?sure
est comme masque?e]. . . 3
The appropriateness of this corpus for our work
is twofold: first, the fact that it contains naturally-
occurring rewritings provides us with an interest-
ing source of text spans in context which have been
rewritten. Moreover, for those instances where the
meaning after rewriting was not significantly al-
tered, it provides us with at least one candidate
rewriting that should be considered as a correct para-
phrase, which can be useful for training validation
algorithms.
3 Automatic sub-sentential paraphrase
acquisition and generation
The acquisition of paraphrases, and in particular
of sub-sentential paraphrases and paraphrase pat-
terns, has attracted a lot of works with the advent of
data-intensive Natural Language Processing (Mad-
nani and Dorr, 2010). The techniques proposed have
a strong relationship to the type of text corpus used
3This verse from Apollinaire?s Nuit Rhe?nane [which seems
almost without rhythmic structure ? whose cesura is as if
hidden]. . .
for acquisition, mainly:
? pairs of sentential paraphrases (monolingual
parallel corpora) allow for a good precision
but evidently a low recall (e.g. (Barzilay and
McKeown, 2001; Pang et al, 2003; Cohn et
al., 2008; Bouamor et al, 2011))
? pairs of bilingual sentences (bilingual parallel
corpora) allow for a comparatively better re-
call (e.g. (Bannard and Callison-Burch, 2005;
Kok and Brockett, 2010))
? pairs of related sentences (monolingual com-
parable corpora) allow for even higher recall
but possibly lower precision (e.g. (Barzilay
and Lee, 2003; Li et al, 2005; Bhagat and
Ravichandran, 2008; Dele?ger and Zweigen-
baum, 2009)
Although the precision of such techniques can in
some cases be formulated with regards to a prede-
fined reference set (Cohn et al, 2008), it should
more generally be assessed in the specific context
of some use of the paraphrase pair. This refers to
the problem of substituability in context (e.g. (Con-
nor and Roth, 2007; Zhao et al, 2007)), which is a
well studied field at the lexical level and the object of
evaluation campains (McCarthy and Navigli, 2009).
Contextual phrase substitution poses the additional
challenge that phrases are rarer than words, so that
building contextual and grammatical models to en-
sure that the generated rephrasings are both seman-
tically compatible and grammatical is more compli-
cated (e.g. (Callison-Burch, 2008)).
The present work does not aim to present any
original technique for paraphrase acquisition, but
rather focusses on the task of sub-sentential para-
phrase validation in context. We thus resort to some
existing repertoire of phrasal paraphrase pairs. As
explained in section 2, we use the WICOPACO cor-
pus as a source of sub-sentential paraphrases: the
phrase after rewriting can thus be used as a potential
paraphrase in context.4 To obtain other candidates
of various quality, we used two knowledge sources.
The first uses automatic pivot translation (Bannard
and Callison-Burch, 2005), where a state-of-the-art
4Note, however, that in our experiments we will ask our hu-
man judges to assess anew its paraphrasing status in context.
12
general-purpose Statistical Machine Translation sys-
tem is used in a two-way translation. The second
uses manual acquisition of paraphrase candidates.
Web-based acquisition of this type of knowledge has
already been done before (Chklovski, 2005; Espan?a
Bonet et al, 2009), and could be done by crowd-
sourcing, a technique growing in popularity in recent
years. We have instead formulated manual acquisi-
tion as a web-based game. Players can take parts in
two parts of the game, illustrated on Figure 3.
First, players propose sub-sentential paraphrases
in context for selected text spans in web documents
(top of Figure 3), and then raters can take part in as-
sessing paraphrases proposed by other players (bot-
tom of Figure 3). In order to avoid any bias, players
cannot evaluate games in which they played. Eval-
uation is sped up by using a compact word lattice
view for eliciting human judgments, built using the
syntactic fusion algorithm of (Pang et al, 2003).
Data acquisition was done in French to remain co-
herent with our experiments on the French corpus
of WICOPACO, and both players and raters were
native speakers. An important point is that in our
experiments the context of acquisition and of evalu-
ation were different: players were asked to generate
paraphrases in contexts that are different from those
of the WICOPACO corpus used for evaluation. To
this end, web snippets were automatically retrieved
for the various phrases of our dataset without con-
texts, so that sentences from the Web (but not from
Wikipedia) were used for manual paraphrase acqui-
sition. This allows us to simulate the availability of a
preexisting repertoire of (contextless) sub-sentential
paraphrases, and to assess the performance of our
contextual validation techniques on a possibly in-
compatible context.
4 Web-based contextual validation
Given a repertoire of potential phrasal paraphrases
and a context for a naturally-occurring rewriting, our
task consists in deciding automatically which poten-
tial paraphrases can be substituted with good confi-
dence for the original phrase. A concrete instantia-
tion of it could correspond to the proposal of Max
and Zock (2008), where such candidate rephrasings
could be presented in order of decreasing suitability
to a word processor user, possibly during the revi-
sion of a Wikipedia article.
The specific nature of the text units that we are
dealing with calls for a careful treatment: in the
general scenario, it is unlikely that any supervised
corpus would contain enough information for ap-
propriate modeling of the substituability in context
decision. It is therefore tempting to consider using
the Web as the largest available information source,
in spite of several of its known limitations, includ-
ing that data can be of varying quality. It has how-
ever been shown that a large range of NLP applica-
tions can be improved by exploiting n-gram counts
from the Web (using Web document counts as a
proxy) (Lapata and Keller, 2005).
Paraphrase identification has been addressed pre-
viously, both using features computed from an of-
fline corpus (Brockett and Dolan, 2005) and fea-
tures computed from Web queries (Zhao et al,
2007). However, to our knowledge previous work
exploiting information from the Web was limited to
the identification of lexical paraphrases. Although
the probability of finding phrase occurrences sig-
nificantly increases by considering the Web, some
phrases are still very rare or not present in search
engine indexes.
As in (Brockett and Dolan, 2005), we tackle our
paraphrase identification task as one of monolingual
classification. More precisely, considering an orig-
inal phrase p within the context of sentence s, we
seek to determine whether a candidate paraphrase p?
would be a grammatical paraphrase of p within the
context of s. We make use of a Support Vector Ma-
chine (SVM) classifier which exploits the features
described in the remainder of this section.
Edit distance model score Surface similarity on
phrase pairs can be a good indicator that they share
semantic content. In order to account for the cost
of transforming one string into the other, rather
than simply counting common words, we use the
score produced by the Translation Edit Rate met-
ric (Snover et al, 2010). Furthermore, we perform
this computation on strings of lemmas rather than
surface forms:5
5Note that because we computed the TER metric on French
strings, stemming and semantic matching through WordNet
were not activated.
13
Figure 1: Interface of our web-based game for paraphrase acquisition and evaluation. On the top, players reformulate
all text spans highlighted by the game creator on any webpage (a Wikipedia article on the example). On the bottom,
raters evaluate paraphrases proposed by sets of players using a compact word-lattice view. Note that in its standard
definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer.
hedit = TER(Lemorig, Lempara) (1)
Note that this model is not derived from informa-
tion from the Web, in contrast to all the models de-
scribed next.
Language model score The likelihood of a sen-
tence can be a good indicator of its grammatical-
ity (Mutton, 2006). Language model probabilities
can now be obtained from Web counts. In our ex-
periments, we used the Microsoft Web N-gram Ser-
vice6 for research (Wang et al, 2010) to obtain log
likelihood scores for text units.7 However, this score
is certainly not sufficient as it does not take the orig-
inal wording into account. We therefore used a ratio
of the language model score of the paraphrased sen-
tence with the language model score of the original
6http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
7Note that in order to query on French text, we had to re-
move all diacritics for the service to behave correctly, indepen-
dently of encodings: careful examination of ranked hypotheses
showed that this trick allowed us to obtain results coherent with
expectations.
sentence, after normalization by sentence length of
the language model scores (Onishi et al, 2010):
hLM ratio =
LM(para)
LM(orig)
=
lm(para)1/length(para)
lm(orig)1/length(orig)
(2)
Contextless thematic model scores Cooccurring
words are used in distributional semantics to account
for common meanings of words. We build vector
representations of cooccurrences for both the origi-
nal phrase p and its paraphrase p?. Our contextless
thematic model is built in the following fashion: we
query a search engine to retrieve the top N docu-
ment snippets for phrase p. We then count frequen-
cies for all content words in these snippets, and keep
the set W of words appearing more than a fraction
of N . We then build a vector T (thematic profile)
of dimension |W | where values are computed by the
following formula:
Tnocontorig [w] =
count(p, w)
count(p)
(3)
14
where count(x) correspond to the number of docu-
ments containing a given exact phrase or word ac-
cording to the search engine used and count(x, y)
correspond to the number of documents containing
simultaneously both. We then compute the same
thematic profile for the paraphrase p?, using only the
subset of words W :
Tnocontpara [w] =
count(p?, w)
count(p)
(4)
Finally, we compute a similarity between the two
profiles by taking the cosinus between their two vec-
tors:
hnocontthem =
Tnocontorig ? T
nocont
para
||Tnocontorig || ? ||T
nocont
para ||
(5)
In all our experiments, we used the Yahoo! Search
BOSS8 Web service for obtaining Web counts and
retrieving snippets. Assuming that the distribution
of words in W is not biased by the result ordering
of the search engine, our model measures some sim-
ilarity between the most cooccurring content words
with p and the same words with p?.
Context-aware thematic model scores Our
context-aware thematic model takes into account
the words of sentence s in which the substitution
of p with p? is attempted. We now consider the set
of content words from s (s being the part of the
sentence without phrase p) in lieu of the previous
set of cooccurring words W , and compute the
same profile vectors and similarity between that of
the original sentence and that of the paraphrased
sentence:
hcontthem =
T contorig ? T
cont
para
||T contorig || ? ||T
cont
para||
(6)
However, words from s might not be strongly
cooccurring with p. In order to increase the likeli-
hood of finding thematically related words, we also
build an extended context model, hextcontthem where
content words from s are supplemented with their
most cooccurring words. This is done using the
same procedure as that previously used for finding
content words cooccurring with p.
8http://developer.yahoo.com/search/boss/
5 Experiments
In this section we report on experiments conducted
to assess the performance of our proposed approach
for validating candidate sub-sentential paraphrases
using information from the Web.
5.1 Data used
We randomly extracted 150 original sentences in
French and their rewritings from the WICOPACO
corpus which were marked as paraphrases. Of those,
we kept 100 for our training corpus and the remain-
ing 50 for testing. The number of original phrases of
each length is reported on Figure 2.
phrase length 1 2 3 4 5 6 7 8
original phrases 0 3 29 8 6 2 2 0
paraphrases 39 64 74 36 21 10 5 1
Figure 2: Distribution of number of phrases per phrase
length in tokens for the test corpus
For each original sentence, we collected 5 candi-
date paraphrases to simulate the fact that we had a
repertoire of paraphrases with the required entries:9
? WICOPACO: the original paraphrase from the
WICOPACO corpus;
? GAME: two candidate paraphrases from users
of our Web-based game;
? PIVOTES and PIVOTZH: two candidate para-
phrases obtained by translation by pivot, using
the Google Translate10 online SMT system and
one language close to French as pivot (Span-
ish), and another one more distant (Chinese).
We then presented the original sentence and its 5
paraphrases (in random order) to two judges. Four
native speakers took part in our experiments: they
all took part in the data collection for one half of
the sentences of the training and test corpora and to
the evaluation of paraphrases for the other half. For
the annotation with two classes (paraphrase vs. not
paraphrase), we obtain as inter-judge agreement11 a
9Note that, as a consequence, we did not carry any experi-
ment related to the recall of any technique here.
10http://translate.google.com
11We used R (http://www.r-project.org) to com-
pute this Cohen?s ? value.
15
Figure 3: Example of an original sentence and its 5 associated candidate paraphrases. The phrase in bold from the
original sentence (The brand is at the origin of many concepts that have revolutionized computing.) is paraphrased
as est le promoteur (is the promoter), a popularise? (popularized), origine (origin), est a` la source (is the source), and
l?origine (the origin).
value of ? = 0.65, corresponding to a substantial
agreement according to the literature. An example
of the interface used is provided in Figure 3.
We considered that our technique could not pro-
pose reliable results when web phrase counts were
too low. From the distribution of counts of phrases
and paraphrases from our training set (see Figure 4),
we empirically chose a threshold of 10 for the min-
imum count of any phrase. Our corpus was conse-
quently reduced from 750=150*5 to 434 examples
for the training corpus, and from 250=50*5 to 215
for the test corpus.
  <10 <100 <1000 <10000 <100000 <1000000 >10000000
1020
3040
5060
7080
90100 # of original phrases# of paraphrases
Range of number of counts
Figure 4: Number of phrases and paraphrases per web
count range
Results will be reported for three conditions:
? Possible: the gold standard for instances where
at least one of the judges indicated ?para-
phrases? records the pair as a paraphrase. In
this condition, the test set has 116 instances that
are paraphrases and 99 that are not.
? Sure: the gold standard for instances where not
all judges indicated ?paraphrases? records the
pair as not paraphrase. In this condition, the
test set has 76 instances that are paraphrases
and 139 that are not.
? Surer: only those instances where both judges
agree are recorded. This reduces our training
and test set to respectively 287 and 175 exam-
ples. Thus, results on this subcorpora will not
be directly comparable with the other results.
In this condition, the test set has 76 instances
that are paraphrases and 99 that are not.
5.2 Baseline techniques
Web-count based baselines We used two base-
lines based on simple Web counts. The first one,
WEBLM, considers a candidate sentence a para-
phrase of the original sentence whenever its Web
language model score is higher than that of the orig-
inal phrase. The second one, BOUNDLM, considers
a sentence as a paraphrase whenever the counts for
the bigrams crossing the left and right boundary of
the sub-sentential paraphrase is higher than 10.
Syntactic dependency baseline When rewriting a
subpart of a sentence, the fact that syntactic depen-
dencies between the rewritten phrase and its con-
text are the same than those of the original phrase
and the same context can provide some information
16
about the grammatical and semantic substituability
of the two phrases (Zhao et al, 2007; Max and Zock,
2008). We thus build syntactic dependencies for
both the original and rewritten sentence, using the
French version (Candito et al, 2010) of the Berkeley
probabilistic parser (Petrov and Klein, 2007), and
consider the subset of dependencies for the two sen-
tences that exist between a word inside the phrase
under focus and a word outside it (Deporig and
Deppara). Our CONTDEP baseline considers a sen-
tence as a paraphrase iff Deppara = Deporig.
5.3 Evaluation results
We used the models described in Section 4 to build
a SVM classifier using the LIBSVM package (Chang
and Lin, 2001). Accuracy results are reported on
Figure 5.
WEBLM BOUNDLM CONTDEP CLASSIFIER
POSSIBLE 62.79 54.88 48.53 57.67
SURE 68.37 36.27 51.90 70.69
SURER 56.79 51.41 42.69 62.85
Figure 5: Accuracy results for the three baselines and our
classifier on the test set for the three conditions. Note that
the SURER condition cannot be directly compared with
the other two as the number of training and test examples
are not the same.
The first notable observation is that our task is not
surprisingly a difficult one. The best performance
achieved is an accuracy of 70.69 with our system in
the SURE condition. There are, however, some im-
portant variations across conditions, with a result as
low as 57.67 for our system in the POSSIBLE condi-
tion (recall that in this condition candidates are con-
sidered paraphrases when only one of the two judges
considered it a paraphrase, i.e. when the two judges
disagreed).
Overall, the WEBLM baseline and our system ap-
pear as stronger than the two other baselines. The
two lower baselines, BOUNDLM and CONTDEP, at-
tempt to model local grammatical constraints, which
are not surprisingly not sufficient for paraphrase
identification. WEBLM is comparatively a much
more competitive baseline, but its accuracy in the
SURER condition is not very strong. As this latter
condition considers only consensual judgements for
the two judges, we can hypothesize that the interpre-
tation of its results is more reliable. In this condi-
WICOPACO GAMERS PIVOTES PIVOTZH
POSSIBLE 89.33 67.00 47.33 20.66
SURE 64.00 44.50 31.33 10.66
SURER 86.03 57.34 37.71 12.60
Figure 6: Paraphrase accuracy of our different paraphrase
acquisition methods for the three conditions.
tion, our system obtains the best performance, with
a +6.06 advantage over WEBLM. As found in other
works (e.g. (Bannard and Callison-Burch, 2005)),
using language models for paraphrase validation is
not sufficient as it cannot model meaning preserva-
tion, and our results show that this is also true even
when counts are estimated from the Web. Using a
ratio of normalized LM scores may have improved
the situation a bit.12
Lastly, we report in Figure 6 the paraphrase
accuracy of each individual acquisition technique
(i.e. source of paraphrases from the preexisting
repertoire). The original rewritting from WICO-
PACO obtains not surprisingly a very high para-
phrase accuracy, in particular in the POSSIBLE and
SURER conditions. Paraphrases obtained through
our Web-based game have an acceptable accuracy:
the numbers confirm that paraphrase pairs are highly
context-dependent, because the pairs which were
likely to be paraphrases in the context of the game
are not necessarily so in a different context. This,
of course, may be due to a number of reasons that
we will have to investigate. Lastly, there is a signif-
icant drop in accuracy for the automatic pivot para-
phrasers, but pivoting through Spanish obtained, not
suprisingly again, a much better performance than
pivoting through Chinese.
6 Discussion and future work
We have presented an approach to the task of
targeted paraphrasing in the context of text revi-
sion, a scenario which was supported by naturally-
occurring data from the rephrasing memory of
Wikipedia. Our framework takes a repertoire of ex-
isting sub-sentential paraphrases, coming from pos-
12A possible explanation for the relative good performance of
WEBLM may lie in the fact that our two automatic paraphrasers
using Google Translate as a pivot translation engine tend to pro-
duce strings that are very likely according to the language mod-
els used by the translation system, which we assume to be very
comparable to those that were used in our experiments.
17
sibly any source including manual acquisition, and
validates all candidate paraphrases using informa-
tion from the Web. Our experiments have shown
that the current version of our classifier outperforms
several baselines when considering paraphrases with
consensual judgements in the gold standard refer-
ence.
Although our initial experiments are positive, we
believe that they can be improved in a number of
ways. We intend to broaden our exploration of the
various characteristics at play. We will try more fea-
tures, including e.g. a model of syntactic depen-
dencies derived from the Web, and extend our work
to new languages. We will also attempt to analyze
more precisely our results to identify problematic
cases, some of which could turn to be almost im-
possible to model without resorting to world knowl-
edge, which was beyond our attempted modeling.
Finally, we will also be interested in considering the
applicability of this approach as a framework for the
evaluation of paraphrase acquisition techniques.
Acknowledgments
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The authors would
like to thank the anonymous reviewers for their help-
ful questions and comments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT,
Edmonton, Canada.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, Toulouse, France.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of the ACM symposium on User interface software and
technology.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2011.
Monolingual alignment by edit rate computation on
sentential paraphrase pairs. In Proceedings of ACL,
Short Papers session, Portland, USA.
Chris Brockett and William B. Dolan. 2005. Support
vector machines for paraphrase identification and cor-
pus construction. In Proceedings of The 3rd Inter-
national Workshop on Paraphrasing IWP, Jeju Island,
South Korea.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of COLING, Copenhagen, Denmark.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Timothy Chklovski. 2005. Collecting paraphrase cor-
pora from volunteer contributors. In Proceedings of
KCAP 2005, Banff, Canada.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, Manchester, UK.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Comput. Linguist.,
34(4):597?614.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of ECML, Warsaw, Poland.
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in wikipedia?s revision history. SEPLN
journal, 46:51?58.
Cristina Espan?a Bonet, Marta Vila, M. Anto`nia Mart??,
and Horacio Rodr??guez. 2009. Coco, a web interface
for corpora compilation. SEPLN journal, 43.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of NAACL-
HLT, Los Angeles, USA.
18
Mirella Lapata and Frank Keller. 2005. Web-based Mod-
els for Natural Language Processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Weigang Li, Ting Liu, Yu Zhang, Sheng Li, and Wei
He. 2005. Automated generalization of phrasal para-
phrases from the web. In Proceedings of the IJCNLP
Workshop on Paraphrasing, Jeju Island, South Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC 2010, Valletta, Malta.
Aure?lien Max and Michael Zock. 2008. Looking up
phrase rephrasings via a pivot language. In Proceed-
ings of the COLING Workshop on Cognitive Aspects
of the Lexicon, Manchester, United Kingdom.
Aure?lien Max. 2004. From controlled document au-
thoring to interactive document normalization. In Pro-
ceedings of COLING, Geneva, Switzerland.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2).
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proceedings of ACL, Prague, Czech
Republic.
Andrew Mutton. 2006. Evaluation of sentence grammat-
icality using Parsers and a Support Vector Machine.
Ph.D. thesis, Macquarie University.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, Short Papers ses-
sion, Uppsala, Sweden.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, Barcelona,
Spain.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, MA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of EACL, Athens, Greece.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Kuansan Wang, Chris Thrasher, Evelyne Viegas, Xiao-
long Li, and Bo-june (Paul) Hsu. 2010. An Overview
of Microsoft Web N-gram Corpus and Applications.
In Proceedings of the NAACL-HLT Demonstration
Session, Los Angeles, USA.
Shiqi Zhao, Ting Liu, Xincheng Yuan, Sheng Li, and
Yu Zhang. 2007. Automatic acquisition of context-
specific lexical paraphrases. In Proceedings of IJCAI
2007, Hyderabad, India.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint ACL-IJCNLP, Singapore.
Shiqi Zhao, Haifeng Wang, Ting Liu, , and Sheng Li.
2010. Leveraging multiple mt engines for paraphrase
generation. In Proceedings of COLING, Beijing,
China.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of COLING,
Beijing, China.
19
