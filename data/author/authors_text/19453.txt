Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 579?589, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Statistical Relational Learning Approach to Identifying
Evidence Based Medicine Categories
Mathias Verbeke? Vincent Van Asch? Roser Morante?
Paolo Frasconi? Walter Daelemans? Luc De Raedt?
? Department of Computer Science, Katholieke Universiteit Leuven, Belgium
{mathias.verbeke, luc.deraedt}@cs.kuleuven.be
? Department of Linguistics, Universiteit Antwerpen, Belgium
{roser.morante, vincent.vanasch, walter.daelemans}@ua.ac.be
? Dipartimento di Sistemi e Informatica, Universita` degli Studi di Firenze, Italy
p-f@dsi.unifi.it
Abstract
Evidence-based medicine is an approach
whereby clinical decisions are supported by
the best available findings gained from scien-
tific research. This requires efficient access
to such evidence. To this end, abstracts in
evidence-based medicine can be labeled using
a set of predefined medical categories, the so-
called PICO criteria. This paper presents an
approach to automatically annotate sentences
in medical abstracts with these labels. Since
both structural and sequential information are
important for this classification task, we use
kLog, a new language for statistical relational
learning with kernels. Our results show a clear
improvement with respect to state-of-the-art
systems.
1 Introduction
Evidence-based medicine (EBM) or evidence-based
practice (EBP) combines clinical expertise, the pref-
erences and values of the patient and the best
available evidence to make good patient care deci-
sions. Clinical research findings are systematically
reviewed, appraised and used to improve the patient
care, for which efficient access to such evidence is
required. In order to facilitate the search process,
medical documents are labeled using a set of prede-
fined medical categories, the PICO criteria. PICO is
an acronym for the mnemonic concepts that are used
to construct queries when searching for scientific ev-
idence in the EBM process. The need to automatize
the annotation process has initiated research into au-
tomatic approaches to annotate sentences in medical
documents with the PICO labels.
As indicated by Kim et al2011), both the struc-
tural information of the words in the sentence, and
that of the sentences in the document are important
features for this task. Furthermore, sequential infor-
mation can leverage the dependencies between dif-
ferent sentences in the text. Therefore we propose an
approach using kLog (Frasconi et al2012) to tackle
this problem. kLog is a new language for statistical
relational learning with kernels, that is embedded in
Prolog, and builds upon and links together concepts
from database theory, logic programming and learn-
ing from interpretations. Learning from interpreta-
tions is a logical and relational learning setting (De
Raedt et al2008) in which the examples are inter-
pretations, that is, sets of tuples that are true in the
examples. In a sense, each example can be viewed
as a small relational database. kLog is able to trans-
form relational into graph-based representations and
apply kernel methods to extract an extended high-
dimensional feature space.
The choice for kLog was motivated by previous
results (Verbeke et al2012), where we showed that
a statistical relational learning approach using kLog
is able to process the contextual aspects of language
improving on state-of-the-art results for hedge cue
detection. However, the current task adds two levels
of complexity. First, next to the relations between
the words in the sentence, now also the relations be-
tween the sentences in the document become impor-
tant. In the proposed approach, we first generate a
feature space with kLog that captures the intrasen-
tential properties and relations. Hereafter, these fea-
tures serve as input for a structured output support
vector machine that can handle sequence tagging
579
(Tsochantaridis et al2004), in order to take the
intersentential features into account. Second, since
there are more than two categories, and each sen-
tence can have multiple labels, the problem is now a
multiclass multilabel classification task.
The main contribution of this paper is that we
show that kLog?s relational nature and its ability
to declaratively specify and use background knowl-
edge is beneficial for natural language learning prob-
lems. This is shown on the NICTA-PIBOSO corpus,
for which we present results that indicate a clear im-
provement on the state-of-the-art.
The remainder of this paper is organized as fol-
lows. In Section 2, we outline earlier work that is
related to the research presented here. Section 3 de-
scribes the methodology of our method. We present
a thorough evaluation of our method in Section
4. The last section draws conclusions and presents
some ideas for future work.
2 Related Work
EBM is an approach to clinical problem-solving
based on ?systematically finding, appraising, and us-
ing contemporaneous research findings as the ba-
sis for clinical decisions? (Rosenberg and Donald,
1995). The evidence-based process consists of four
steps: (1) Formulating a question from a patient?s
problem; (2) Searching the literature for relevant
clinical articles; (3) Evaluating the evidence; And
(4) implementing useful findings in clinical prac-
tice. Given the amounts of medical publications
available in databases such as PubMed, automating
step 2 is crucial to help doctors in their practice.
Efforts in this direction from the NLP community
have so far focused on corpus annotation (Demner-
Fushman and Lin, 2007; Kim et al2011), text cate-
gorization (Davis-Desmond and Molla?, 2012), sum-
marization (Molla? and Santiago-Mart??nez, 2011),
and question-anwering (Niuet al2003; Demner-
Fushman and Lin, 2007).
The existing corpora are usually annotated with
the PICO mnemonic (Armstrong, 1999) concepts,
that are used to build queries when searching for
literature for EBM purposes. The PICO concepts
are: primary Problem (P) or population, main Inter-
vention (I), main intervention Comparison (C), and
Outcome of intervention (O). PICO helps determin-
ing what terms are important in a query and there-
fore it helps building the query, which is sent to the
search repositories. Once the documents are found,
they need to be read by a person who eliminates ir-
relevant documents.
The first attempt to classify PICO concepts is pre-
sented in Demner-Fushman and Lin (2007), who
apply a rule-based approach to identify sentences
where PICO concepts occur and a supervised ap-
proach to classify sentences that contain an Out-
come. The features used by this classifier are n-
grams, position, and semantic information from the
parser used to process the data. The system is trained
on 275 abstracts manually annotated. The accura-
cies reported range from 80% for Population, 86%
for Problem, 80% for Intervention, and, from 64%
to 95% for Outcome depending on the test set of ab-
stracts.
Kim et al2011) perform a similar classification
task in two steps. First a classifier identifies the sen-
tences that contain PICO concepts, and then another
classifier assigns PICO tags to the sentences found
to be relevant by the previous classifier. The sys-
tem is based on a CRF algorithm and is trained on
the NICTA-PIBOSO corpus. This dataset contains
1,000 medical abstracts manually annotated with an
extension of the PICO tagset, for which the defini-
tions are listed in Table 1. The annotation is per-
formed at sentence level and one sentence may have
more than one tag. An example of an annotated
abstract from the corpus can be found in the sup-
plementary material. The features used by the al-
gorithm include features derived from the context,
semantic relations, structure and sequencing of the
text. The system is evaluated for 5-way and 6-way
classification and results are provided apart from
structured and unstructured abstracts. The F-scores
for structured abstracts is 89.32% for 5-way classifi-
cation and 80.88% for 6-way classification, whereas
for unstructured abstracts it is 71.54% for 5-way
classification and 64.66% for 6-way classification.
Chung (2009) uses CRF to classify PICO con-
cepts by combining them with general categories as-
sociated with rhetorical roles: Aim, Method, Results
and Conclusion. Her system is tested on corpora of
abstracts of randomized control trials. First struc-
tured abstracts with headings labeled with PICO
580
Background Material that informs and may place the current study in perspective, e.g. work that preceded the
current; information about disease prevalence; etc.
Population The group of individual persons, objects or items comprising the study?s sample, or from which
the sample was taken for statistical measurement
Intervention The act of interfering with a condition to modify it or with a process to change its course (includes
prevention)
Outcome The sentence(s) that best summarizes the consequences of an intervention
Study Design The type of study that is described in the abstract
Other Any sentence not falling into one of the other categories and presumed to provide little help with
clinical decision making, i.e. non-key or irrelevant sentences
Table 1: Definitions of the semantic tags used as annotation categories (taken from Kim et al2011)).
concepts are used. A sentence level classification
task is performed, assigning only one rhetorical role
per sentence. The F-scores obtained range from 0.93
to 0.98. Then another sentence level classification
task is performed to automatically assign the labels
Intervention, Participant and Outcome Measures to
sentences in unstructured and structured abstracts
without headings. F-scores of up to 0.83 and 0.84
are obtained for Intervention and Outcome Measure
sentences.
Other work aimed at identifying rhetorical zones
in biomedical articles. In this case areas of text are
classified in terms of the rhetorical categories In-
troduction, Methods, Results and Discussion (IM-
RAD) (Agarwal and Yu, 2009) or richer categories,
such as problem-setting or insight (Mizuta et al
2006).
There exists a wide range of statistical relational
learning systems (Getoor and Taskar, 2007; De
Raedt et al2008), and many of these systems
are in principle useful for natural language process-
ing. The most popular formalism today is Markov
Logic, which has already been used for natural lan-
guage processing tasks such as semantic role label-
ing (Riedel and Meza-Ruiz, 2008) and coreference
resolution (Poon and Domingos, 2008). With re-
spect to Markov Logic, two distinguishing features
of kLog are that 1) it employs kernel based meth-
ods grounded in statistical learning theory, and 2) it
employs a Prolog like language for defining and us-
ing background knowledge. As Prolog is a program-
ming language, this is more flexible that the formal-
ism used by Markov Logic.
3 Methodology
In learning from examples, or interpretations (De
Raedt et al2008), the instances are sampled iden-
tically and independently from some unknown but
fixed distribution. They can be represented as pairs
z = (x, y), in which x represents the inputs and y
the outputs. An example interpretation can be found
in Figure 3, where the hasCategory relation repre-
sents y in this case, since it is the target relation we
want to predict. The inputs x are formed by all other
facts. The task is now to learn a function h : X ? Y
that maps the inputs to the outputs. Sentences may
have multiple labels. Hence this is a structured out-
put task where the output is a sequence of sets of
labels attached to the sentences in a given document.
kLog is the new statistical relational language for
learning with kernels that we use to tackle the PICO
categories classification task. The novelty of kLog
is that, based on the regular, linguistic features, it
allows to define an extended high-dimensional fea-
ture space that is also able to take relational features
into account in a principled manner. Furthermore,
its declarative approach offers a flexible and inter-
pretable way to construct features.
The choice of kLog is motivated by our previous
results (Verbeke et al2012), where we showed that
the relational representation of the domain as used
by kLog is able to take the contextual aspects of lan-
guage into account. Whereas there we only used
the relations at the sentence level, the current task
adds a new level of complexity, since the identifica-
tion of PICO categories in abstracts also requires to
take into account various relations between the sen-
tences of an abstract. The general workflow of our
approach is depicted in Figure 1, which will be de-
581
Database
(Fig. 3)
Extensionalized 
database
Graph
(Fig. 4)
Kernel matrix/
feature vectors
Statistical 
learner
Raw data
(sentence)
Feature extraction
(lemma, POS,?)
Declarative feature 
construction
Graphicalization
Feature 
generation
Graph kernel 
(NSPDK)
kLog
Figure 1: General kLog workflow.
scribed step by step in the following paragraphs.
Preprocessing The sentences have been prepro-
cessed with a named entity tagger and a dependency
parser.
Named entity tagging has been performed with
the BiogaphTA named entity module, which
matches token sequences with entries in the UMLS
database1. UMLS integrates over 2 million names
for some 900,000 concepts from more than 60 fami-
lies of biomedical vocabularies (Bodenreider, 2004).
The tagger matches sequences with a length of max-
imum 4 tokens. This covers 66.2% of the UMLS
entries. By using UMLS, different token sequences
referring to the same concept can be mapped to
the same concept identifier (CID). The BiographTA
named entity tagger has been evaluated on the
BioInfer corpus (Pyysalo et al2007) obtaining a
72.02 F1 score.
Dependency parsing has been performed with the
GENIA dependency parser GDep (Sagae and Tsu-
jii, 2007), which uses a best-first probabilistic shift-
reduce algorithm based on the LR algorithm (Knuth,
1965) and extended by the pseudo-projective pars-
ing technique. This parser is a version of the KSDep
dependency parser trained on the GENIA Treebank
for parsing biomedical text. KSDep was evaluated
in the CoNLL Shared Task 2007 obtaining a La-
beled Attachment Score of 89.01% for the English
dataset. GDEP outputs the lemmas, chunks, Genia
named entities and dependency relations of the to-
kens in a sentence.
This information can be represented as an
Entity/Relationship (E/R) diagram, a modeling
paradigm that is frequently used in database theory
(Garcia-Molina et al2008). The E/R-model for the
1From UMLS only the MRCONSO.RRF and MRSTY.RRF
files are used.
problem under consideration is shown in Figure 2,
which provides an abstract representation of the ex-
amples, i.e. medical abstracts in this case. We will
show later how this abstract representation can be
unrolled for each example, resulting in a graph; cf.
also Figure 4 for our example sentence. This rela-
tional database representation will serve as the input
for kLog.
w
depHead
next
wordID
depRel
lemma
POS-tag
chunktag
wordString
NEGenia
NEUMLS
sentence
hasWord
class
sentID
hasCategory
nextS
Figure 2: E/R-diagram modeling the sentence identifica-
tion task.
The entities are the words and sentences in the
abstract. They are represented by the rectangles in
the E/R-model. Each entity can have a number of
properties attached to it, depicted by the ovals and
has a unique identifier (underlined properties). As
in database theory, each entity corresponds with a
tuple, or fact, in the database.
Figure 3 shows a part of an example interpretation
z. For example, w(w4 1,?Surgical?,?Surgical?,b-
np,jj,?O?,?O?) specifies a word entity, with w4 1 as
identifier and the other arguments as properties. As
indicated before, as lexical information we take the
token string itself, its lemma, the part-of-speech tag
and the chunk tag into account. We also include
some semantic information, namely two binary val-
ues indicating if the word is a (biological) named
entity. sentence(s4,4) represents a sentence entity,
with its index in the abstract as a property.
Furthermore, the E/R-diagram also contains a
number of relationships, which are represented by
582
sentence(s4,4)
hasCategory(s4,?background?)
w(w4_1,?Surgical?,?Surgical?,b-np,
jj,?O?,?O?) hasWord(s4,w4_1)
dh(w4_1,w4_2,nmod)
nextW(w4_2,w4_1)
w(w4_2,?excision?,?excision?,i-np,
nn,?O?,?O?) hasWord(s4,w4_2)
dh(w4_2,w4_5,sub)
nextW(w4_3,w4_2)
w(w4_3,?of?,?of?,b-pp,in,?O?,?O?)
hasWord(s4,w4_3)
dh(w4_3,w4_2,nmod)
nextW(w4_4,w4_3)
w(w4_4,?CNV?,?CNV?,b-np,nn,
?B-protein?,?O?) hasWord(s4,w4_4)
dh(w4_4,w4_3,pmod)
nextW(w4_5,w4_4)
...
Figure 3: Part of an example interpretation z, represent-
ing the example sentence in Figure 4.
the diamonds. They are linked to the entities that
participate in the relationship, or stand alone if they
characterize general properties of the interpretation.
An example relation is nextW(w4 2,w4 1), which
indicates the sequence of the words in the sentence.
dh(w4 1,w4 2,nmod) specifies that word w4 1 is
a noun modifier of word w4 2, and thus serves to
incorporate the dependency relationships between
the words. hasCategory(s4,?background?) signi-
fies that sentence s4 is a sentence describing back-
ground information. This relation is the target re-
lation that we want to predict for this task and will
not be taken into account as a feature, but is listed in
the database and only used during the training of the
model.
Since the previously described entities and rela-
tionships are listed explicitly in the database, these
are called extensional relations, in contrast to the in-
tensional relations, as we will describe next.
Declarative feature construction A strength of
kLog is that it is also capable of constructing fea-
tures declaratively, by using intensional relations.
This enables one to encode additional background
knowledge based on a small set of preprocessed fea-
tures, which renders experimentation very flexible
and makes the results more interpretable. It further-
more allows one to limit the required features to the
core discriminative ones. These intensional features
are defined through definite clauses, and is done us-
ing an extension of the declarative programming lan-
guage Prolog. The following features were used.
We make a distinction between the features used for
structured and unstructured abstracts.
For structured abstracts, four intensional relations
were defined. The relation lemmaRoot(S,L) is
specified as:
lemmaRoot(S,L) ?
hasWord(S, I),
w(I,_,L,_,_,_,_),
dh(I,_,root).
For each sentence, it only selects the lemmas
of the root word in the dependency tree, which
markedly limits the number of word features used.
The following relations are related to, and try to
capture the document structure imposed by the sec-
tion headers present in the structured abstracts.
hasHeaderWord(S,X) identifies whether a sen-
tence is a header of a section. In order to realize this,
it selects the words of a sentence that count more
than four characters (to discard short names of bio-
logical entities), which all need to be uppercase.
hasHeaderWord(S,X) ?
w(W,X,_,_,_,_,_),
hasWord(S,W),
(atom(X) -> name(X,C) ; C = X),
length(C,Len),
Len > 4,
all_upper(C).
Also the sentences below a certain section header
need to be marked as belonging to this sec-
tion, which is done by the relation hasSection-
Header(S,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
hasHeaderWord(S1,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
not isHeaderSentence(S),
once(hasSectionHeader(S1,X)).
583
For the unstructured abstracts, also the lemma-
Root relation is used, but next to the lemma, now
also the part-of-speech tag of the root word is taken
into account. Since the unstructured abstracts lack
section headers, other features were needed to dis-
tinguish between the different sections, for which
the relation prevLemmaRoot proved to be very in-
formative. It adds the lemma of the root word in the
previous sentence as a property to the current sen-
tence under consideration.
prevLemmaRoot(S,L) ?
nextS(S1,S),
lemmaRoot(S1,L,_).
The intensional predicates are grounded. This is
a proces similar to materialization in databases, that
is, the atoms implied by the background knowledge
and the facts in the example are all computed using
Prolog?s deduction mechanism. This leads to the
extensionalized database, in which both the exten-
sional as well as the grounded intensional predicates
are listed.
Graphicalization and feature generation In the
third step, the interpretations are graphicalized, i.e.
transformed into graphs. Since the facts that form
the interpretation still conform to the E/R-diagram,
this can be interpreted as unfolding the E/R-diagram
over the data. An example illustrating this process
is given in Figure 4. Each interpretation is converted
into a bipartite graph, for which there is a vertex for
every ground atom of every E-relation, one for every
ground atom of every R-relation, and an undirected
edge {e, r} if an entity e participates in relationship
r.
The obtained graphs can then be used in the next
step for feature generation. This is done by means
of a graph kernel ?, which calculates the similar-
ity between two graphicalized interpretations. Any
graph kernel that allows fast computations on large
graphs and has a flexible bias to enable heteroge-
neous features can in theory be applied. In the cur-
rent implementation, an extension of the Neighbor-
hood Subgraph Pairwise Distance Kernel (NSPDK)
(Costa and De Grave, 2010) is used.
NSPDK is a decomposition kernel (Haussler,
1999), in which pairs of subgraphs are compared
                                         
      
       
s0
s1
s2
s3
nextS
nextS
next
s4
s5
s6
 
 
 
s7
 
s8
 
s9
 
title
title
Surgical  excision  of  CNV  may  allow  stabilisation  or  improvement  of  vision.
background
next next                         
dh(nmod)
dh(sub)
dh(pmod)
  
hasWord
  
  
  
  
  
Figure 4: Graphicalization Gz of interpretation z.
to each other in order to calculate the similarity be-
tween two graphs. These subgraphs can be seen as
circles in the graph, and are defined by three hyper-
parameters. First of all, there is the center of the
subgraph, the kernel point, which can be any entity
or relation in the graph. The entities and relations
to be taken into account as kernel points are marked
beforehand as a subset of the intensional and exten-
sional domain relations. The radius r determines
the size of the subgraphs and defines which entities
or relations around the kernel point are taken into ac-
count. Each entity or relation that is within a number
of r edges away from the kernel point is considered
to be part of the subgraph. The third hyperparam-
eter, the distance d, determines how far apart from
each other the kernel points can be. Each subgraph
around a kernel point that is within a distance d or
less from the current kernel point will be considered.
This is captured by the relation Rr,d(Av, Bu, G) be-
tween two rooted subgraphs Av, Bu and a graph G,
which selects all pairs of neighborhood graphs of ra-
dius r whose roots are at distance d in a given graph
G.
The kernel ?r,d(G,G?) between graphs G and G?
on the relation Rr,d is then defined as:
?r,d(G,G
?) =
?
Av , Bu ? R
?1
r,d(G)
A?v? , B
?
u? ? R
?1
r,d(G
?)
?(Av, A
?
v?)?(Bu, B
?
u?)
(1)
584
For efficiency reasons, an upper bound is imposed
on the radius and distance parameters, which leads
to the following kernel definition:
Kr?,d?(G,G
?) =
r??
r=0
d??
d=0
?r,d(G,G
?) (2)
We hereby limit the sum of the ?r,d kernels for all
increasing values of the radius and distance parame-
ter up to a maximum given value of r?, respectively
d?.
The result of this graphicalization and feature
generation process is an extended, high-dimensional
feature space, which serves as input for the statisti-
cal learner in the next step.
Learning The constructed feature space contains
one feature vector per sentence. This implies that
the sequence information of the sentences at the doc-
ument level is not taken into account yet. Since the
order of the sentences in the abstract is a valuable
feature for this prediction problem, a learner that
reflects this in the learning process is needed, al-
though in principle any statistical learner can be used
on the feature space constructed by kLog. There-
fore we opted for SVM-HMM2 (Tsochantaridis et
al., 2004), which is an implementation of structural
support vector machines for sequence tagging. In
contrast to a conventional Hidden Markov Model,
SVM-HMM is able to take these entire feature vec-
tors as observations, and not just atomic tokens.
In our case, the instances to be tagged are formed
by the sentences for which feature vectors were cre-
ated in the previous step. The qid is a special fea-
ture that is used in the structured SVM to restrict
the generation of constraints. Since every document
needs to be represented as a sequence of sentences,
in SVM-HMM, the qid?s are used to obtain the doc-
ument structure. The order of the HMM was set
to 2, which means that the two previous sentences
were considered for collective classification. The
cost value was set to 500, and was determined via
cross-validation. For epsilon, the default value, 0.5,
was kept, since this mainly only influences the run-
ning time and memory consumption during training.
2http://www.cs.cornell.edu/people/tj/
svm_light/svm_hmm.html
All S U
Nb. Abstracts 1000 376 624
Nb. Sentences 10379 4774 5605
- Background 2557 669 1888
- Intervention 690 313 377
- Outcome 4523 2240 2283
- Population 812 369 443
- Study Design 233 149 84
- Other 1564 1034 530
Table 2: Number of abstracts and sentences for Struc-
tured (S) and Unstructured (U) abstract sets, including
number of sentences per class (taken from (Kim et al
2011)).
4 Evaluation
We evaluate the performance of kLog against a base-
line system and a memory-based tagger (Daelemans
and van den Bosch, 2005). The results are also com-
pared against those from Kim et al2011), which is
the state-of-the-art system for this task.
4.1 Datasets
We perform our experiments on the NICTA-
PIBOSO dataset from Kim et al2011) (kindly pro-
vided by the authors). It contains 1,000 abstracts of
which 500 were retrieved from MEDLINE by query-
ing for diverse aspects in the traumatic brain injury
and spinal cord injury domain. The dataset consists
of two types of abstracts. If the abstract contains
section headings (e.g. Background, Methodology,
Results, etc.), it is considered to be structured. This
information can be used as a feature in the model.
The other abstracts are regarded unstructured.
The definitions of the semantic tags used as an-
notations categories are a variation on the PICO tag
set, with the addition of two additional categories
(see Table 1 in Section 2). Each sentence can be an-
notated with multiple classes. This renders the task
a multiclass multilabel classification problem. The
statistics on this dataset can be found in Table 2.
In order to apply the same evaluation setting as
Kim et al2011), we used the dataset from Demner-
Fushman et al2005) as external dataset. It con-
sists of 100 sentences of which 51 are structured.
Because the semantic tag set used for annotation
slightly differs from the one presented in Table 1,
and to make our results comparable, we will use the
same mapping as used in Kim et al2011).
585
4.2 Baseline and benchmarks
We compare the kLog system to three other systems:
a baseline system, a memory-based system, and the
scores reported by Kim et al2011).
The memory-based system that we use is based
on the memory-based tagger MBT3 (Daelemans and
van den Bosch, 2005). This machine learner is orig-
inally designed for part-of-speech tagging. It pro-
cesses data on a sentence basis by carrying out se-
quential tagging, viz. the class label or other features
from previously tagged tokens can be used when
classifying a new token. In our setup, the sentences
of an abstract are taken as the processing unit and
the collection of all sentences in an abstract is taken
as one sequence.
The features that are used to label a sentence are
the class labels of four previous sentences, the am-
bitags of the following two sentences, the lemma of
the dependency root of the sentence, the position of
the sentence in the abstract, the lemma of the root
of the previous sentence, and section information.
For each root lemma, all possible class labels, as ob-
served in the training data, are concatenated into one
ambitag. These tags are stored in a list. An am-
bitag for a sentence is retrieved by looking up the
root lemma in this list. The position of the sentence
is expressed by a number. Section information is ob-
tained by looking for a previous sentence that con-
sists of only one token in uppercase. Finally, basic
lemmatization is carried out by removing a final S.
All other settings of MBT are the default settings
and no feature optimization nor feature selection has
been carried out to prevent overfitting.
When a class label contains multiple labels, like
e.g. population and study design, these labels are
concatenated in an alphabetically sorted manner.
This method of working reduces the multilabel prob-
lem to a problem with many different labels, i.e. the
label powerset method of Tsoumakas et al2010).
The baseline system is exactly the same as
the memory-based system except that no machine
learner is included. The most frequent class label in
the training data, i.e. Outcome, is assigned to each
instance. The memory-based system enables us to
compare kLog against a basic machine learning ap-
proach, using few features. The majority baseline
3http://ilk.uvt.nl/mbt [16 March 2012]
system enables us to compare the memory-based
system and kLog against a baseline in which no in-
formation about the observations is used.
4.3 Parametrization
From the kernel definition it might be clear that
the kLog hyperparameters, namely the distance d
and radius r, can have a strong influence on the
results. This requires a deliberate choice during
parametrization. From a linguistic perspective, the
use of unigrams and bigrams is justifiable, since
most phrases that reveal clues on the structure of the
abstract (e.g. evaluation measures, methodolody, fu-
ture work) can be expressed with single or pairs of
words. This is reflected by a distance and radius both
set to 1, which enables to take all possible combina-
tions of consecutive words into account and captures
the relational information attached to the word in fo-
cus, i.e. the current kernel point. This is confirmed
by cross-validation on other settings for the hyper-
parameters.
Since kLog generates a feature vector, only the
sequence information at word level is taken into ac-
count by kLog. Since we use a sequence labeling
approach as statistical learner, i.e. SVM-HMM, at
the level of the abstract this information is however
implicitly taken into account during learning. For
SVM-HMM, only the cost parameter C, which reg-
ulates the trade-off between the slack and the mag-
nitude of the weight-vector, and , that specifies the
precision to which constraints are required to be sat-
isfied by the solution, were optimized by means of
cross-validation. For the other parameters, the de-
fault values were used.
4.4 Results
Experiments are run on structured and unstructured
abstracts separately. On the NICTA-PIBOSO cor-
pus, we performed 10-fold cross-validation. Over
all folds, all labels, i.e. the parts of the multilabels,
are compared in a binary way between gold standard
and prediction. Summing all true positives, false
positives, and false negatives over all folds leads to
micro-averaged F-scores. This was done for two dif-
ferent settings. In one setting, CV/6-way, we com-
bined the labeling of the sentences with the identifi-
cation of irrelevant information, by adding the Other
586
label as an extra class in the classification. The re-
sults are listed in Table 3.
CV/6-way MBT Kim et alLog
Label S U S U S U
Background 71.0 61.3 81.84 68.46 86.19 76.90
Intervention 24.3 6.4 20.25 12.68 26.05 16.14
Outcome 87.9 70.4 92.32 72.94 92.99 77.69
Population 50.6 15.9 56.25 39.80 35.62 21.58
Study Design 45.9 13.10 43.95 4.40 45.5 6.67
Other 86.1 20.9 69.98 24.28 87.98 24.42
Table 3: F-scores per class for structured (S) and unstruc-
tured (U) abstracts.
For this setting, kLog is able to outperform both
MBT and the system of Kim et al2011), for both
structured and unstructured abstracts on all classes
except Population. From Table 4, where the micro-
average F-scores over all classes and for all settings
are listed, it can be observed that kLog performs up
to 3.73% better than MBT over structured abstracts,
and 9.67% better over unstructured ones.
Although to a lesser extent for the structured ab-
stracts, the same pattern can be observed for the
CV/5-way setting, where we tried to classify the sen-
tences only, without considering the irrelevant ones.
The per-class results for this setting are shown in Ta-
ble 5. Now the scores for Population are comparable
to the other systems, due to which we assume these
sentences are similar in structure to the ones labeled
with Other.
For the external corpus, the results are listed in Ta-
ble 6. Although kLog performs comparably for the
individual classes Background and Intervention, its
overall performance is worse on the structured ab-
stracts. In case of the unstructured abstracts, kLog
performs better on the majority of the individual
classes and in overall performance for the 5-way set-
ting, and comparable for the 4-way setting.
Baseline MBT kLog
Method S U S U S U
CV/6-way 43.90 41.87 80.56 57.47 84.29 67.14
CV/5-way 61.79 46.66 86.96 64.37 87.67 72.95
Ext/5-way 66.18 6.76 36.34 11.56 20.50 14.00
Ext/4-way 30.11 27.23 67.29 55.96 50.40 50.50
Table 4: Micro-averaged F1-score obtained for structured
(S) and unstructured (U) abstracts, both for 10-fold cross-
validation (CV) and on the external corpus (Ext).
CV/5-way MBT Kim et alLog
Label S U S U S U
Background 87.1 64.9 87.92 70.67 91.45 80.06
Intervention 48.0 6.9 48.08 21.39 45.58 22.65
Outcome 95.8 75.9 96.03 80.51 96.21 83.04
Population 70.9 21.4 63.88 43.15 63.96 23.32
Study Design 50.0 7.4 47.44 8.6 48.08 4.50
Table 5: F-scores per class for 5-way classification over
structured (S) and unstructured (U) abstracts.
MBT Kim et alLog
Label S U S U S U
Ext/5-way
Background 58.9 15.7 56.18 15.67 58.30 29.10
Intervention 21.5 13.8 15.38 28.57 40.00 34.30
Outcome 29.3 17.8 81.34 60.45 27.80 24.10
Population 10.7 17.8 35.62 28.07 5.60 28.60
Other 40.7 3.5 46.32 15.77 11.40 8.50
Ext/4-way
Background 90.4 67.5 77.27 37.5 65 68.6
Intervention 29 23.1 28.17 8.33 28.1 32.3
Outcome 74.1 74.6 90.5 78.77 72.4 72.7
Population 48.7 23.8 42.86 28.57 11.8 15.4
Table 6: F-scores per class for 5-way and 4-way classifi-
cation over structured (S) and unstructured (U) abstracts
on the external corpus.
As a general observation, it is important to note
that there is a high variability between the different
labels. Due to kLog?s ability to take the structured
input into account, we assume a correlation between
the sentence structure of the label and the predic-
tion quality. We intend to perform an extensive error
analysis, in order to detect patterns which may allow
us to incorporate additional declarative background
knowledge into our model.
5 Conclusions
We presented a statistical relational learning ap-
proach for the automatic identification of PICO cat-
egories in medical abstracts. To this extent, we used
kLog, a new framework for logical and relational
learning with kernels. Due to its graphical approach,
it is able to exploit the full relational representation,
that is often inherent in language structure. Since
contextual features are often essential and relations
are prevalent, the aim of this paper was to show
that statistical relational learning in general, and the
graph kernel-based approach of kLog in particular,
is specifically suited for problems in natural lan-
587
guage learning.
In future work, we intend to explore additional
ways to incorporate background knowledge in a
declarative way, since it renders the language learn-
ing problem more intuitive and gives a better under-
standing of feature contribution. Furthermore, we
also want to investigate the use of SRL approaches
for high-relational domains, and make a clear com-
parison with related techniques.
6 Acknowledgements
This research is funded by the Research Foundation
Flanders (FWO project G.0478.10 - Statistical Re-
lational Learning of Natural Language), and made
possible through financial support from the KU Leu-
ven Research Fund (GOA project 2008/08 Proba-
bilistic Logic Learning), the University of Antwerp
(GOA project BIOGRAPH) and the Italian Min-
istry of Education, University, and Research (PRIN
project 2009LNP494 - Statistical Relational Learn-
ing: Algorithms and Applications). The authors
would like to thank Fabrizio Costa, Kurt De Grave
and the anonymous reviewers for their valuable
feedback.
References
Shashank Agarwal and Hong Yu. 2009. Automatically
Classifying Sentences in Full-text Biomedical Articles
into Introduction, Methods, Results and Discussion.
Bioinformatics, 25(23):3174?3180.
E. C. Armstrong. 1999. The Well-built Clinical Ques-
tion: the Key to Finding the Best Evidence Efficiently.
WMJ, 98(2):25?28.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): Integrating Biomed-
ical Terminology. Nucleic Acids Research,
32(Suppl.1):D267?D270.
Grace Y Chung. 2009. Sentence Retrieval for Abstracts
of Randomized Controlled Trials. BMC Medical In-
formatics and Decision Making, 9(10).
Fabrizio Costa and Kurt De Grave. 2010. Fast Neighbor-
hood Subgraph Pairwise Distance Kernel. Proceed-
ings of the 26th International Conference on Machine
Learning, 255?262, Haifa, Israel. Omnipress.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing.. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
P. Davis-Desmond and Diego Molla?. 2012. Detection of
Evidence in Clinical Research Papers. Proceedings of
the Australasian Workshop On Health Informatics and
Knowledge Management (HIKM 2012), Melbourne,
Australia, 129:13?20. Australian Computer Society,
Inc.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2005. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion (JAMIA), 13:52?60.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge Based
and Statistical Techniques. Computational Linguis-
tics, 33(1):63?103.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic In-
ductive Logic Programming. In: Lecture Notes in
Computer Science (LNCS), 4911. Springer-Verlag,
Heidelberg, Germany.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. kLog - a Language
for Logical and Relational Learning with Kernels.
arXiv:1205.3981v2.
Hector Garcia-Molina, Jeff Ullman, and Jennifer Widom.
2008. Database Systems: The Complete Book. Pren-
tice Hall Press, Englewood Cliffs, NJ, USA.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press, Cambridge,
MA, USA.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report (UCSC-CRL-99-10), Uni-
versity of California at Santa Cruz.
Su Nam Kim, David Martinez, Lawrence Cavedon, and
Lars Yencken. 2011. Automatic Classification of Sen-
tences to Support Evidence Based Medicine. BMC
Bioinformatics, 12(2):S5.
Donald E. Knuth. 1965. On the Translation of Lan-
guages from Left to Right. Information and Control,
8: 607?639.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2006. Zone Analysis in Biology Articles as a Basis
for Information Extraction. International Journal of
Medical Informatics, 75(6):468?487.
Diego Molla? and Mara Elena Santiago-Mart??nez. 2011.
Development of a Corpus for Evidence Medicine
Summarisation. Proceedings of the 2011 Australasian
Language Technology Workshop (ALTA 2011), Can-
berra, Australia, 86?94. Association for Computa-
tional Linguistics.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patri-
cia Rodriguez-Gianolli. 2003. Answering Clinical
588
Questions with Role Identification. Proceedings of the
ACL, Workshop on Natural Language Processing in
Biomedicine. Sapporo, Japan, 73?80. Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov logic.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2008. Hon-
olulu, Hawaii, 650?659. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: a Corpus for Information
Extraction in the Biomedical Domain. BMC Bioinfor-
matics, 8:50.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with Markov logic. Proceed-
ings of the Twelfth Conference on Computational Nat-
ural Language Learning (CoNLL 2008. Manchester,
United Kingdom, 193?197. Association for Computa-
tional Linguistics.
William Rosenberg and Anna Donald. 1995. Evidence
Based Medicine: an Approach to Clinical Problem
Solving. British Medical Journal, 310(6987):1122?
1126.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensembles. Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, Prague, Czech
Republic, 1044?1050. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support Vector
Machine Learning for Interdependent and Structured
Output Spaces. Proceedings of the twenty-first inter-
national conference on Machine learning (ICML), Al-
berta, Canada, 104?111. ACM.
Grigorios Tsoumakas and Ioannis Katakis and Ioannis P.
Vlahavas. Oded Maimon and Lior Rokach, editors.
2010. Mining Multi-label Data. In: Data Mining and
Knowledge Discovery Handbook, 2nd ed., 667?685.
Springer-Verlag, Heidelberg, Germany.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc De Raedt.
2012. Kernel-based Logical and Relational Learning
with kLog for Hedge Cue Detection. Proceedings of
the 21th International Conference on Inductive Logic
Programming, in press.
589
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 85?90,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
kLogNLP: Graph Kernel?based Relational Learning of Natural Language
Mathias Verbeke
?
Paolo Frasconi
?
Kurt De Grave
?
Fabrizio Costa
?
Luc De Raedt
?
?
Department of Computer Science, KU Leuven, Belgium
{mathias.verbeke, kurt.degrave, luc.deraedt}@cs.kuleuven.be
?
Dipartimento di Sistemi e Informatica, Universit`a degli Studi di Firenze, Italy,
p-f@dsi.unifi.it
?
Institut f?ur Informatik, Albert-Ludwigs-Universit?at, Germany,
costa@informatik.uni-freiburg.de
Abstract
kLog is a framework for kernel-based
learning that has already proven success-
ful in solving a number of relational tasks
in natural language processing. In this pa-
per, we present kLogNLP, a natural lan-
guage processing module for kLog. This
module enriches kLog with NLP-specific
preprocessors, enabling the use of exist-
ing libraries and toolkits within an elegant
and powerful declarative machine learn-
ing framework. The resulting relational
model of the domain can be extended by
specifying additional relational features in
a declarative way using a logic program-
ming language. This declarative approach
offers a flexible way of experimentation
and a way to insert domain knowledge.
1 Introduction
kLog (Frasconi et al., 2012) is a logical and re-
lational language for kernel-based learning. It has
already proven successful for several tasks in com-
puter vision (Antanas et al., 2012; Antanas et al.,
2013) and natural language processing. For ex-
ample, in the case of binary sentence classifica-
tion, we have shown an increase of 1.2 percent
in F1-score on the best performing system in the
CoNLL 2010 Shared Task on hedge cue detec-
tion (Wikipedia dataset) (Verbeke et al., 2012a).
On a sentence labeling task for evidence-based
medicine, a multi-class multi-label classification
problem, kLog showed improved results over both
the state-of-the-art CRF-based system of Kim et
al. (2011) and a memory-based benchmark (Ver-
beke et al., 2012b). Also for spatial relation ex-
traction from natural language, kLog has shown
to provide a flexible relational representation to
model the task domain (Kordjamshidi et al., 2012).
kLog has two distinguishing features. First, it is
able to transform relational into graph-based rep-
resentations, which allows to incorporate struc-
tural features into the learning process. Subse-
quently, kernel methods are used to work in an ex-
tended high-dimensional feature space, which is
much richer than most of the direct proposition-
alisation approaches. Second, it uses the logic
programming language Prolog for defining and
using (additional) background knowledge, which
renders the model very interpretable and provides
more insights into the importance of individual
(structural) features.
These properties prove especially advantageous
in the case of NLP. The graphical approach of
kLog is able to exploit the full relational represen-
tation that is often a natural way to express lan-
guage structures, and in this way allows to fully
exploit contextual features. On top of this rela-
tional learning approach, the declarative feature
specification allows to include additional back-
ground knowledge, which is often essential for
solving NLP problems.
In this paper, we present kLogNLP
1
, an NLP
module for kLog. Starting from a dataset and a
declaratively specified model of the domain (based
on entity-relationship modeling from database the-
ory), it transforms the dataset into a graph-based
relational format. We propose a general model
that fits most tasks in NLP, which can be extended
by specifying additional relational features in a
declarative way. The resulting relational represen-
tation then serves as input for kLog, and thus re-
sults in a full relational learning pipeline for NLP.
kLogNLP is most related to Learning-Based
Java (LBJ) (Rizzolo and Roth, 2010) in that it of-
fers a declarative pipeline for modeling and learn-
ing tasks in NLP. The aims are similar, namely ab-
stracting away the technical details from the pro-
grammer, and leaving him to reason about the
modeling. However, whereas LBJ focuses more
on the learning side (by the specification of con-
straints on features which are reconciled at in-
ference time, using the constrained conditional
1
Software available at http://dtai.cs.
kuleuven.be/klognlp
85
Interpretations
(small relational DBs)
Extensionalized 
database
Graph
Kernel matrix/
feature vectors
Statistical 
learner
Raw dataset
(sentence)
Feature extraction
based on model
Declarative feature 
construction
Graphicalization
Feature 
generation
Graph kernel 
(NSPDK)
kLog
kLogNLP
kLogNLP
(E/R-)model
Figure 1: General kLog workflow extended with the kLogNLP module
model framework), due to its embedding in kLog,
kLogNLP focuses on the relational modeling, in
addition to declarative feature construction and
feature generation using graph kernels. kLog in it-
self is related to several frameworks for relational
learning, for which we refer the reader to (Fras-
coni et al., 2012).
The remainder of this paper is organized ac-
cording to the general kLog workflow, preceded
with the kLogNLP module, as outlined in Fig-
ure 1. In Section 2, we discuss the modeling of the
data, and present a general relational data model
for NLP tasks. Also the option to declaratively
construct new features using logic programming is
outlined. In the subsequent parts, we will illustrate
the remaining steps in the kLog pipeline, namely
graphicalization and feature generation (Section
3), and learning (Section 4) in an NLP setting. The
last section draws conclusions and presents ideas
for future work.
2 Data Modeling
kLog employs a learning from interpretations set-
ting (De Raedt et al., 2008). In learning from
interpretations, each interpretation is a set of tu-
ples that are true in the example, and can be
seen as a small relational database. Listing 3, to
be discussed later, shows a concise example. In
the NLP setting, an interpretation most commonly
corresponds to a document or a sentence. The
scope of an interpretation is either determined by
the task (e.g., for document classification, the in-
terpretations will at least need to comprise a sin-
gle document), or by the amount of context that
is taken into account (e.g., in case the task is sen-
tence classification, the interpretation can either be
a single sentence, or a full document, depending
on the scope of the context that you want to take
into account).
Since kLog is rooted in database theory, the
modeling of the problem domain is done using an
entity-relationship (E/R) model (Chen, 1976). It
gives an abstract representation of the interpreta-
tions. E/R models can be seen as a tool that is tai-
word
depRel
nextW
wordID
depType
lemma
POS-tag
wordString
namedEntity
hasWord
sentID
nextS
coref
synonymous
sentence
Figure 2: Entity-relationship diagram of the
kLogNLP model
lored to model the domain at hand. As the name
indicates, E/R models consist of entities, which we
will represent as purple rectangles, and relations,
represented as orange diamonds. Both entities and
relations can have several attributes (yellow ovals).
Key attributes (green ovals) uniquely identify an
instance of an entity. We will now discuss the
E/R model we propose as a starting point in the
kLogNLP pipeline.
2.1 kLogNLP model
Since in NLP, most tasks are situated at either
the document, sentence, or token level, we pro-
pose the E/R model in Figure 2 as a general do-
main model suitable for most settings. It is able
to represent interpretations of documents as a se-
quence (nextS) of sentence entities, which
are composed of a sequence (nextW) of word
entities. Next to the sequence relations, also the
dependency relations between words (depRel)
are taken into account, where each relation has
its type (depType) as a property. Furthermore,
also the coreference relationship between words
or phrases (coref) and possibly synonymy re-
lations (synonymous) are taken into account.
The entities in our model also have a primary key,
namely wordID and sentID for words and sen-
tences respectively. Additional properties can be
attached to words such as the wordString it-
self, its lemma and POS-tag, and an indication
whether the word is a namedEntity.
This E/R model of Figure 2 is coded declara-
tively in kLog as shown in Listing 1. The kLog
syntax is an extension of the logical programming
language Prolog. In the next step this script will
be used for feature extraction and generation. Ev-
86
ery entity or relationship is declared with the key-
word signature. Each signature is of a certain
type; either extensional or intensional.
kLogNLP only acts at the extensional level. Each
signature is characterized by a name and a list
of typed arguments. There are three possible ar-
gument types. First of all, the type can be the
name of an entity set which has been declared
in another signature (e.g., line 4 in Listing 1; the
nextS signature represents the sequence relation
between two entities of type sentence, namely
sent 1 and sent 2). The type self is used to
denote the primary key of an entity. An example is
word id (line 6), which denotes the unique iden-
tifier of a certain word in the interpretation. The
last possible type is property, in case the argu-
ment is neither a reference to another entity nor a
primary key (e.g., postag, line 9).
We will first discuss extensional signatures, and
the automated extensional feature extraction pro-
vided by kLogNLP, before illustrating how the
user can further enrich the model with intensional
predicates.
1 begin_domain.
2 signature sentence(sent_id::self)::
extensional.
3
4 signature nextS(sent_1::sentence, sent_2
::sentence)::extensional.
5
6 signature word(word_id::self,
7 word_string::property,
8 lemma::property,
9 postag::property,
10 namedentity::property
11 )::extensional.
12
13 signature nextW(word_1::word, word_2::
word)::extensional.
14
15 signature corefPhrase(coref_id::self)::
extensional.
16 signature isPartOfCorefPhrase(
coref_phrase::corefPhrase, word::
word)::extensional.
17 signature coref(coref_phrase_1::
corefPhrase, coref_phrase_2::
corefPhrase)::extensional.
18
19 signature synonymous(word_1::word,
word_2::word)::extensional.
20
21 signature dependency(word_1::word,
22 word_2::word,
23 dep_rel::property
24 )::extensional.
25
26 kernel_points([word]).
27 end_domain.
Listing 1: Declarative representation of the
kLogNLP model
2.2 Extensional Feature Extraction
kLog assumes a closed-world, which means that
atoms that are not known to be true, are assumed
to be false. For extensional signatures, this en-
tails that all ground atoms need to be listed ex-
plicitly in the relational database of interpreta-
tions. These atoms are generated automatically
by the kLogNLP module based on the kLog script
and the input dataset. Considering the defined at-
tributes and relations in the model presented in
Listing 1, the module interfaces with NLP toolk-
its to preprocess the data to the relational format.
The user can remove unnecessary extensional sig-
natures or modify the number of attributes given in
the standard kLogNLP script as given in Listing 1
according to the needs of the task under consider-
ation.
An important choice is the inclusion of the
sentence signature. By inclusion, the gran-
ularity of the interpretation is set to the docu-
ment level, which implies that more context can
be taken into account. By excluding this signa-
ture, the granularity of the interpretation is set to
the sentence level.
Currently, kLogNLP interfaces with the follow-
ing NLP toolkits:
NLTK The Python Natural Language Toolkit
(NLTK) (Bird et al., 2009) offers a suite
of text processing libraries for tokenization,
stemming, tagging and parsing, and offers an
interface to WordNet.
Stanford CoreNLP Stanford CoreNLP
2
pro-
vides POS tagging, NER, parsing and
coreference resolution functionality.
The preprocessing toolkit to be used can be
set using the kLogNLP flags mechanism, as il-
lustrated by line 3 of Listing 2. Subsequently,
the dataset predicate (illustrated in line 4 of
Listing 2) calls kLogNLP to preprocess a given
dataset
3
. This is done according to the speci-
fied kLogNLP model, i.e., the necessary prepro-
cessing modules to be called in the preprocess-
ing toolkit are determined based on the presence
of the entities, relationships, and their attributes in
the kLogNLP script. For example, the presence
2
http://nlp.stanford.edu/software/
corenlp.shtml
3
Currently supported dataset formats are directories con-
sisting of (one or more) plain text files or XML files consist-
ing of sentence and/or document elements.
87
of namedentity as a property of word results
in the addition of a named entity recognizer in the
preprocessing toolkit. The resulting set of inter-
pretations is output to a given file. In case sev-
eral instantiations of a preprocessing module are
available in the toolkit, the preferred one can be
chosen by setting the name of the property accord-
ingly. The names as given in Listing 1 outline the
standard settings for each module. For instance, in
case the Snowball stemmer is preferred above the
standard (Wordnet) lemmatizer in NLTK, it can be
selected by changing lemma into snowball as
name for the word lemma property (line 8).
1 experiment :-
2 % kLogNLP
3 klognlp_flag(preprocessor,
stanfordnlp),
4 dataset(?/home/hedgecuedetection/
train/?,?trainingset.pl?),
5 attach(?trainingset.pl?),
6 % Kernel parametrization
7 new_feature_generator(my_fg,nspdk),
8 klog_flag(my_fg,radius,1),
9 klog_flag(my_fg,distance,1),
10 klog_flag(my_fg,match_type, hard),
11 % Learner parametrization
12 new_model(my_model,libsvm_c_svc),
13 klog_flag(my_model,c,0.1),
14 kfold(target, 10, my_model, my_fg).
Listing 2: Full predicate for 10-fold classification
experiment
Each interpretation can be regarded as a small
relational database. We will illustrate the exten-
sional feature extraction step on the CoNLL-2010
dataset on hedge cue detection, a binary classifi-
cation task where the goal is to detect uncertainty
in sentences. This task is situated at the sentence
level, so we left out the sentence and nextS
signatures, as no context from other sentences was
taken into account. A part of a resulting interpre-
tation is shown in Listing 3.
1 word(w1,often,often,rb,0,1).
2 depRel(w1,w5,adv).
3 nextW(w1,w2).
4 word(w2,the,the,dt,0,2).
5 depRel(w2,w4,nmod).
6 nextW(w2,w3).
7 word(w3,response,response,nn,0,3).
8 nextW(w3,w4).
9 depRel(w3,w4,nmod).
10 word(w4,may,may,md,0,5).
11 nextW(w4,w5).
Listing 3: Part of an interpretation
Optionally, additional extensional signatures
can easily be added to the knowledge base by the
user, as deemed suitable for the task under consid-
eration. At each level of granularity (document,
sentence, or word level), the user is given the
corresponding interpretation and entity IDs, with
which additional extensional facts can be added
using the dedicated Python classes. We will now
turn to declarative feature construction. The fol-
lowing steps are inherently part of the kLog frame-
work. We will briefly illustrate their use in the
context of NLP.
2.3 Declarative Feature Construction
The kLog script presented in Listing 1 can now
be extended using declarative feature construction
with intensional signatures. In contrast to ex-
tensional signatures, intensional signatures intro-
duce novel relations using a mechanism resem-
bling deductive databases. This type of signatures
is mostly used to add domain knowledge about the
task at hand. The ground atoms are defined implic-
itly using Prolog definite clauses.
For example, in case of sentence labeling for
evidence-based medicine, the lemma of the root
word proved to be a distinguishing feature (Ver-
beke et al., 2012b), which can be expressed as
1 signature lemmaRoot(sent_id::sentence,
lemmaOfRoot::property)::intensional.
2 lemmaRoot(S,L) :-
3 hasWord(S, I),
4 word(I,_,L,_,_,_),
5 depRel(I,_,root).
Also more complex features can be constructed.
For example, section headers in documents (again
in the case of sentence labeling using document
context) can be identified as follows:
1 hasHeaderWord(S,X) :-
2 word(W,X,_,_,_,_),
3 hasWord(S,W),
4 (atom(X) -> name(X,C) ; C = X),
5 length(C,Len),
6 Len > 4,
7 all_upper(C).
8
9 signature isHeaderSentence(sent_id::
sentence)::intensional.
10 isHeaderSentence(S) :-
11 hasHeaderWord(S,_).
12
13 signature hasSectionHeader(sent_id::
sentence, header::property)::
intensional.
14 hasSectionHeader(S,X) :-
15 nextS(S1,S),
16 hasHeaderWord(S1,X).
17 hasSectionHeader(S,X) :-
18 nextS(S1,S),
19 not isHeaderSentence(S),
20 once(hasSectionHeader(S1,X)).
In this case, first the sentences that contain a
header word are identified using the helper pred-
88
word(often,often,rb,0,1) word(the,the,dt,0,2) word(response,response,nn,0,3) word(variable,variable,nn,0,4) word(may,may,md,0,5)
nextW
depRel(adv)
nextW
depRel(nmod)
nextW
depRel(nmod)
nextW
depRel(sbj)
Figure 3: Graphicalization of the (partial) interpretation in Listing 3. For the sake of clarity, attributes of
entities and relationships are depicted inside the respective entity or relationship.r=0 d=2
r=1 d=2v u
INSTANCE G
FEATURESA
Figure 4: Illustration of the NSPDK feature concept. Left: instance G with 2 vertices v, u as roots for
neighborhood subgraphs (A, B) at distance 2. Right: some of the neighborhood pairs, which form the
NSPDK features, at distance d = 2 and radius r = 0 and 1 respectively. Note that neighborhood subgraphs
can overlap.
icate hasHeaderWord, where a header word is
defined as an upper case string that has more than
four letters (lines 1-7). Next, all sentences that rep-
resent a section header are identified using the in-
tensional signature isHeaderSentence (lines
9-11), and each sentence in the paragraphs follow-
ing a particular section header is labeled with this
header, using the hasSectionHeader predi-
cate (lines 13-20).
Due to the relational approach, the span can be
very large. Furthermore, since these features are
defined declaratively, there is no need to reprocess
the dataset each time a new feature is introduced,
which renders experimentation very flexible
4
.
3 Graphicalization and Feature
Generation
In this step, a technique called graphicalization
transforms the relational representations from the
previous step into graph-based ones and derives
features from a grounded entity/relationship dia-
gram using graph kernels. This can be interpreted
as unfolding the E/R diagram over the data. An ex-
ample of the graphicalization of the interpretation
part in Listing 3 can be found in Figure 3.
From the resulting graphs, features can be ex-
tracted using a feature generation technique that is
based on Neighborhood Subgraph Pairwise Dis-
4
Note that changes in the extensional signatures do re-
quire reprocessing the dataset. However, for different runs of
an experiment with varying parameters for the feature gener-
ator or the learner, kLogNLP uses a caching mechanism to
check if the extensional signatures have changed, when call-
ing the dataset predicate.
tance Kernel (NSPDK) (Costa and De Grave,
2010), a particular type of graph kernel. Infor-
mally the idea of this kernel is to decompose a
graph into small neighborhood subgraphs of in-
creasing radii r ? r
max
. Then, all pairs of such
subgraphs whose roots are at a distance not greater
than d ? d
max
are considered as individual fea-
tures. The kernel notion is finally given as the frac-
tion of features in common between two graphs.
Formally, the kernel is defined as:
?
r,d
(G,G
?
) =
?
A,B?R
?1
r,d
(G)
A
?
,B
?
?R
?1
r,d
(G
?
)
1
A
?
=
A
?
? 1
B
?
=
B
?
(1)
whereR
?1
r,d
(G) indicates the multiset of all pairs
of neighborhoods of radius r with roots at distance
d that exist inG, and where 1 denotes the indicator
function and
?
=
the isomorphism between graphs.
For the full details, we refer the reader to (Costa
and De Grave, 2010). The neighborhood pairs are
illustrated in Figure 4 for a distance of 2 between
two arbitrary roots (v and u).
In kLog, the feature set is generated in a combi-
natorial fashion by explicitly enumerating all pairs
of neighborhood subgraphs; this yields a high-
dimensional feature space that is much richer than
most of the direct propositionalization approaches.
The result is an extended high-dimensional fea-
ture space on which a statistical learning algorithm
can be applied. The feature generator is initialized
using the new feature generator predicate
and hyperparameters (e.g., maximum distance and
radius, and match type) can be set using the kLog
flags mechanism (Listing 2, lines 6-10).
89
4 Learning
In the last step, different learning tasks can be per-
formed on the resulting extended feature space. To
this end, kLog interfaces with several solvers, in-
cluding LibSVM (Chang and Lin, 2011) and SVM
SGD (Bottou, 2010). Lines 11-15 (Listing 2) illus-
trate the initialization of LibSVM and its use for
10-fold cross-validation.
5 Conclusions and Future Work
In this paper, we presented kLogNLP, a natu-
ral language processing module for kLog. Based
on an entity-relationship representation of the do-
main, it transforms a dataset into the graph-based
relational format of kLog. The basic kLogNLP
model can be easily extended with additional
background knowledge by adding relations us-
ing the declarative programming language Prolog.
This offers a more flexible way of experimenta-
tion, as new features can be constructed on top
of existing ones without the need to reprocess the
dataset. In future work, interfaces will be added
to other (domain-specific) NLP frameworks (e.g.,
the BLLIP parser with the self-trained biomedical
parsing model (McClosky, 2010)) and additional
dataset formats will be supported.
Acknowledgments
This research is funded by the Research Founda-
tion Flanders (FWO project G.0478.10 - Statistical
Relational Learning of Natural Language). KDG
was supported by ERC StG 240186 ?MiGraNT?.
References
Laura Antanas, Paolo Frasconi, Fabrizio Costa, Tinne
Tuytelaars, and Luc De Raedt. 2012. A relational
kernel-based framework for hierarchical image un-
derstanding. In Lecture Notes in Computer Science,,
pages 171?180. Springer, November.
Laura Antanas, McElory Hoffmann, Paolo Frasconi,
Tinne Tuytelaars, and Luc De Raedt. 2013. A re-
lational kernel-based approach to scene classifica-
tion. IEEE Workshop on Applications of Computer
Vision, 0:133?139.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc., 1st edition.
L?eon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In Proc. of the 19th In-
ternational Conference on Computational Statistics
(COMPSTAT?2010), pages 177?187. Springer.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Peter Pin-Shan Chen. 1976. The entity-relationship
model - toward a unified view of data. ACM Trans.
Database Syst., 1(1):9?36, March.
Fabrizio Costa and Kurt De Grave. 2010. Fast neigh-
borhood subgraph pairwise distance kernel. In Proc.
of the 26th International Conference on Machine
Learning,, pages 255?262. Omnipress.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic
Inductive Logic Programming - Theory and Appli-
cations, volume 4911 of Lecture Notes in Computer
Science. Springer.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. klog: A language for log-
ical and relational learning with kernels. CoRR,
abs/1205.3981.
Su Kim, David Martinez, Lawrence Cavedon, and Lars
Yencken. 2011. Automatic classification of sen-
tences to support evidence based medicine. BMC
Bioinformatics, 12(Suppl 2):S5.
Parisa Kordjamshidi, Paolo Frasconi, Martijn van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
2012. Relational learning for spatial relation extrac-
tion from natural language. In Inductive Logic Pro-
gramming, pages 204?220. Springer.
David McClosky. 2010. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Brown University, Provi-
dence, RI, USA. AAI3430199.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC, Val-
letta, Malta, 5.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc
De Raedt. 2012a. Kernel-based logical and re-
lational learning with kLog for hedge cue detec-
tion. In Proc. of the 21st International Conference
on Inductive Logic Programming, pages 347?357.
Springer, March.
Mathias Verbeke, Vincent Van Asch, Roser Morante,
Paolo Frasconi, Walter Daelemans, and Luc
De Raedt. 2012b. A statistical relational learn-
ing approach to identifying evidence based medicine
categories. In Proc. of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 579?589. ACL.
90
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385?390,
Dublin, Ireland, August 23-24, 2014.
KUL-Eval: A Combinatory Categorial Grammar Approach for
Improving Semantic Parsing of Robot Commands using Spatial Context
Willem Mattelaer, Mathias Verbeke and Davide Nitti
Department of Computer Science, KU Leuven, Belgium
willem.mattelaer@gmail.com
mathias.verbeke@cs.kuleuven.be
davide.nitti@cs.kuleuven.be
Abstract
When executing commands, a robot has
a certain level of contextual knowledge
about the environment in which it oper-
ates. Taking this knowledge into account
can be beneficial to disambiguate com-
mands with multiple interpretations. We
present an approach that uses combina-
tory categorial grammars for improving
the semantic parsing of robot commands
that takes into account the spatial context
of the robot. The results indicate a clear
improvement over non-contextual seman-
tic parsing. This work was done in the
context of the SemEval-2014 task on su-
pervised semantic parsing of spatial robot
commands.
1 Introduction
One of the long-standing goals of robotics is to
build autonomous robots that are able to perform
everyday tasks. Two important requirements to
achieve this are an efficient way of communicating
with the robot, and transforming these commands
such that the robot is able to capture their mean-
ing. Furthermore, this needs to be consistent with
the context in which the robot is operating, i.e., the
robot?s belief.
Semantic parsing focuses on translating natural
language (NL) into a formal representation that
captures the meaning of the sentence. Most of
the current semantic parsing approaches are non-
contextual, i.e., they do not take into account the
context in which the command sentence should be
executed. This can lead to erroneous parses, most
often due to ambiguity in the original sentence.
Consider the following example sentence ?Move
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
the pyramid on the blue cube on the gray cube?.
This sentence has two valid interpretations. Either
the robot needs to move the pyramid that is cur-
rently standing on the blue cube and put it on the
gray cube, or move the pyramid and place it on the
blue cube that is standing on the gray cube.
Humans will decide on the correct interpreta-
tion by taking into account the context. For in-
stance, by looking at Figure 1, it is clear that the
second interpretation is not possible, because there
is no blue cube on top of a gray cube. However,
there is a pyramid on top of a blue cube, making
the first interpretation possible. The goal of this
paper is to improve on non-contextual semantic
parsing by tailoring the context to guide the parser.
In this way, part of the ambiguity that causes mul-
tiple interpretations can be resolved.
Figure 1: Possible situation (taken from (Dukes,
2013b)).
Our approach consists of two steps. First, non-
contextual semantic parsing using combinatory
categorial grammars (CCG) (Steedman, 1996;
Steedman, 2000) is performed on the sentence.
This returns multiple possible parses, each with an
attached likelihood of correctness. Subsequently,
each parse is checked against the current context.
The parse with the highest score that is possible
given the current context is returned.
385
This paper is organized as follows. In Section 2
we discuss related work, followed by a detailed
description of our approach in Section 3. In Sec-
tion 4, the approach is evaluated and compared to
non-contextual parsing. Finally, in Section 5 we
conclude and outline directions for future work.
The software is available from https://
github.com/wmattelaer/Thesis.
2 Related Work
There is a significant body of previous work on
learning semantic parsers. We will first review
approaches that translate NL sentences into a for-
mal representation without taking context into ac-
count, followed by related techniques that use the
context to improve the parsing.
Our approach is inspired by the work of
Kwiatkowski et al. (2010). The authors present
a supervised CCG approach to parse queries to
a geographical information database and a flight-
booking system. This differs from the current set-
ting in that the database querying does not require
to take the context of the environment into ac-
count, as is the case when executing robot com-
mands. SILT (Kate et al., 2005) uses transfor-
mation rules to translate the NL sentence to a
query for the robot. This approach was extended
to tailor support vector machines with string ker-
nels (KRISP) (Kate and Mooney, 2006) and statis-
tical machine learning (WASP) (Mooney, 2007).
Also unsupervised approaches exist. Poon (2013)
solves this lack of supervision by 1) inferring su-
pervision using the target database, which con-
strains the search space, and 2) by using aug-
mented dependency trees.
Artzi and Zettlemoyer (2013) study the use of
grounded CCG semantic parsing using weak su-
pervision for interpreting navigational robot com-
mands. Their approach is similar to ours, but in-
stead of postprocessing the results in a verification
step, the context (or state) is added to the training
data. Krishnamurthy and Kollar (2013) use CCGs
as a foundation, but match it to the context using
an evaluation function. This evaluation function
scores a denotation, i.e., the set of entity referents
for the entire sentence, given a logical form and a
knowledge base, which is considered as the con-
text.
3 Methodology
Our approach consists of two steps: a parse step
and a verification step. Before these steps can
be executed, a Combinatory Categorial Grammar
needs to be trained. The training data for this
grammar consists of typed ?-expressions (Car-
penter, 1997) that are annotated with their cor-
responding NL sentences. As the input data for
the SemEval-2014 task consists of Robot Control
Language (RCL) expressions (Dukes, 2013a)
1
, the
data needs to be preprocessed first.
3.1 Preprocessing
During preprocessing, the RCL expressions are
transformed into equivalent ?-expressions. In the
?-expressions, each entity is represented by a
lambda term where the variable is a reference to
the object. The properties of an entity are defined
by a conjunction of literals with two arguments.
The predicate details the property that is being de-
fined. An example entity, a blue cube, can be rep-
resented as ?x.color(x, blue), type(x, cube). A
spatial relation between two entities is a literal
with three arguments: the variable of the first en-
tity, the type of relation and the second entity. The
latter is given by its lambda term. This lambda
term has to be wrapped in a definite determiner,
det, that selects a single element from the set cre-
ated by the lambda term (Artzi and Zettlemoyer,
2013). For example: the RCL expression
(entity:
(type: prism)
(spatial-relation:
(relation: above)
(entity:
(color: blue)
(type: cube))))
is transformed to the ?-calculus expression
?x.type(x,prism), relation(x, above,
det(?y.color(y, blue), type(y, cube)))
Events are contained in one lambda term with
one variable per event. There are three possible
event predicates. The action predicate defines the
action by detailing the action type and the object
entity. The destination predicate will set the des-
tination of the object
2
. Finally, the sequence pred-
icate is necessary to detail the order of the events.
1
RCL is a linguistically-oriented formal language for
controlling a robot arm, that represents entities, attributes,
anaphora, ellipsis and qualitative spatial relations.
2
Note that this event is not always necessary, e.g., in the
case of a take action, the robot will not release the object.
386
An example of this can be seen at the bottom of
Figure 2.
Besides transforming the RCL expressions to ?-
calculus, also the action types of the events are
checked. If an event has an action of type move
or drop, it is changed to the combined move &
drop action type. This change was introduced be-
cause the actual verbs that are used to instruct the
robot to perform one of these two actions are often
the same. To illustrate this, consider the following
two sentences taken from the training data: ?place
blue block on top of single red block? and ?place
green block on top of blue block?. In the former,
the intended action is a drop action, while in the
latter the action should be a move action. During
parsing, the correct action can be selected by look-
ing at the context it has to be executed in. If the
robot is currently grasping an object, the intended
action is a drop action, otherwise it is a move ac-
tion.
Furthermore, the anaphoric references are
resolved in the natural language sentences.
Anaphoric references are words that refer to one or
more words mentioned earlier in the sentence. The
sentences of the dataset are annotated with mark-
ers that capture the references in the sentence. The
markers that are used are [1], (1) and {1} and
are placed right after the word that is used for the
reference. [1] is used to mark a word that is re-
ferred to by another word, whereas (1) is used
to detail a word that refers to another word, e.g.,
it. Finally, {1} marks a word that refers to the
type of an earlier entity, e.g., one. The numbers
in these markers can increase if there are differ-
ent references in one sentence, but the sentences
of this dataset do not contain different references.
For instance, the sentence Pick the blue block and
place it above the gray one is transformed to the
sentence Pick the blue block [1] and place it (1)
above the gray one {1}.
The anaphoric references are found using
the coreference resolution system of Stanford
CoreNLP (Recasens et al., 2013; Lee et al., 2013;
Lee et al., 2011; Raghunathan et al., 2010). How-
ever, it is not capable of finding references that use
one. This can be solved by letting the one always
refer to the first entity of the sentence, because of
the simplicity of the sentences.
3.2 Parsing
To parse the robot commands, a Probabilis-
tic Combinatory Categorial Grammar (PCCG)
(Kwiatkowski et al., 2010) is used. Regular CCGs
consist out of two sets: a lexicon of lexical items
and a set of operations. A lexical entry combines
a word or phrase with its meaning. This meaning
is represented by a category. A category captures
the syntactic as well as the the semantic informa-
tion of a word. A number of primitive symbols, a
subset of the part-of-speech tags, are used to rep-
resent the syntax. These primitive symbols can be
combined using specific operator symbols (/, \).
The semantics are represented by a ?-expression.
Some example lexical entries are:
blue ` ADJ : ?x.color(x, blue)
pyramid ` N : ?x.type(x, prism)
pick up ` S/NP : ?y?x.action(x, take, y)
The operator symbols can now be used to de-
termine how the categories can be combined using
operations. The operations that are used by the
CCG take one or two categories as input and re-
turn one category as output. These operations will
simultaneously address syntax and semantics. The
two most frequently used operations are the appli-
cation operations, i.e., forward (>) and backward
(<):
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
The forward application takes as input a CCG
category with syntax X/Y and ?-expression f
followed by a category with syntax Y and ?-
expression g and returns a CGG category with syn-
tax X and ?-expression f(g).
The operations will derive syntactic and seman-
tic information, while keeping track of the word
order that is encoded using the slash direction.
Another important operation deals with the def-
inite determiner in the ?-expressions:
N : f ? NP : det(f)
This operation takes a single noun (N) category
as input and returns an noun phrase (NP) category
where the original ?-expression is wrapped in a
determiner. A complete parsing example is shown
in Figure 2.
CCGs will usually have multiple possible parses
for a sentence given a certain lexicon for which it
387
Take
S/NP
?z?y.action(y, take, z)
the
N/N
?x.x
pyramid
N
?x.type(x, prism)
>
N
?x.type(x, prism)
NP
det(?x.type(x, prism))
>
S
?y.action(y, take, det(?x.type(x, prism)))
Figure 2: A possible parse for the sentence ?Take
the pyramid?.
is not possible to determine which of these is best.
To alleviate this problem, PCCGs have been intro-
duced (Kwiatkowski et al., 2010). PCCGs will re-
turn the most likely parse using a log-linear model
that contains a parameter vector ?, estimated us-
ing stochastic gradient updates. The joint proba-
bility of a ?-calculus expression z and a parse y is
given by P (y, z|x; ?,?), with ? being the entire
lexicon. The most likely ?-calculus expression z
given a sentence x can then be found by:
f(x) = arg max
z
P (z|x; ?,?)
where the probability of z is equal to the sum of
the probabilities of all parses that produce z:
P (z|x; ?,?) =
?
y
P (y, z|x; ?,?)
For training the PCCGs, the algorithm as de-
scribed by Kwiatkowski et al. (2010) was used. It
consists of two steps. In the first step the lexicon is
expanded with new lexical items. The second step
will update the parameters of the grammar using
stochastic gradient updates (LeCun et al., 1998).
All parameters are associated with a feature. The
system uses lexical features: for each item in the
lexicon a feature is added that fires when the item
is used.
3.3 Verification
The parser will return multiple ?-expressions,
each with an attached likelihood score. In the
verification step, these resulting expressions are
checked against the context. These ?-expressions
are first transformed to RCL expressions
3
. Next,
the entities are extracted from the RCL expres-
sions and for each entity a corresponding object
is searched using a spatial planner, provided by
the task organizer. This spatial planner will, given
3
Note that during pre- and postprocessing no information
is lost, as the mapping between ?-calculus and RCL is a one-
to-one function.
Complete Partial Without context
Correct 71.29% 78.58% 57.76%
Wrong 11.66% 4.37% 27.72%
No result 17.05% 17.05% 14.52%
Table 1: Results.
an entity description in RCL, return the objects in
the context that satisfy that description. RCL ex-
pressions with entities that have no corresponding
object in the context are discarded. From the re-
maining RCL expressions the one with the highest
likelihood is returned.
4 Evaluation
The provided dataset for the task was crowd-
sourced using Train Robots, an online game in
which players were given before and after im-
ages of a scene and were asked to give the NL
command that the robot had executed (Dukes,
2013a). Each scene is a formal description of a
discrete 8x8x8 3D game board consisting of col-
ored blocks. The entire dataset consists of 3409
annotated examples, and was split in a training and
test set of 2500 and 909 sentences respectively.
The results are listed in Table 1. The first col-
umn (?Complete?) contains the results when the
resulting RCL expression is exactly the same as
the ground-truth RCL expression. Next to the full
matching scores, we also provide the scores for
partial matching of the RCL expressions (?Par-
tial?), based on the Parseval metric (Black et al.,
1991). Each RCL expression is scored between 0
and 1 according to the resemblance with the ex-
pected expression. The tree representations of the
RCL expressions are compared and the number of
correct nodes in the actual expression are divided
by the number of nodes in the tree of the expected
expression to calculate the score. A node is correct
if it is present at the same position in both trees and
if all children are correct.
The last column (?Without context?) contains
the results when using the parser without the veri-
fication step. This can be considered a baseline.
It may be clear that the use of contextual parsing
is advantageous when comparing the contextual
with the non-contextual setting, with an increase
of 13% in the number of correct results.
Error Analysis
When inspecting the wrong parses, it could be ob-
served that the wrong results were usually mini-
mally wrong. Either the value of a certain element
388
Expected Actual Occurrences
edge region 17
above within 15
right left 8
left front 7
within above 6
Table 2: Wrong values.
was wrong, an unnecessary element was added
to the expression or a required element was not
present in the resulting expression. This is also
clear when comparing the complete with the par-
tial match results, from which it can be seen that
66 sentences were only partially incorrect. Some
of the most commonly wrong values are listed in
Table 2. A final common reason for a wrong parse
was that a sequence of a take and a drop action
is considered as a single move action. There are
6 occurrences of this final case of which 5 would
result in the same end state.
One of the most common reasons that the parser
returned no result for a sentence, is because one
type of sentences was not present in the training
set. Sentences of the form ?pick up red block. put
it on grey block? were completely absent from the
training data, but did appear 34 times in the test
set. Their structure is quite simple and should not
present a problem, but the parser was only trained
on sentences that combined the two actions with
an ?and? connective. This is a problem because
the trained grammar is very dependent on the pro-
vided training data. Another difficult type of sen-
tences are the ones that contain measures. Only
17 of these were parsed correctly, while 70 had no
result and 3 were wrong.
Without considering the context, the combined
move& drop action is not possible, since the con-
text is required to decide afterwards which specific
action has to be executed. 59 sentences (6.5%)
were wrong because a wrong action was selected.
5 Conclusions and Future Work
In this paper we have presented an improved se-
mantic parsing approach for robot commands by
integrating spatial context. It consists of two steps.
First, the sentence is parsed using a Probabilis-
tic Combinatory Categorial Grammar. Next, the
parses are checked against the context. The re-
sulting parse is the one with the highest likeli-
hood that is valid given the context. This ap-
proach was evaluated on the SemEval-2014 Task
6 dataset. The results indicate that integrating
contextual knowledge is advantageous for parsing
spatial robot commands.
In future work, we will perform an in-depth
analysis of our system in comparison with the
other participating systems. Furthermore, we will
extend our approach to contexts that also contain
probabilistic facts, in order to be able to handle
noisy sensor data.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the ACL,
1:49?62.
Ezra W. Black, Steven P. Abney, Daniel P. Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip Harri-
son, Donald Hindle, Robert J. P. Ingria, Freder-
ick Jelinek, Judith L. Klavans, Mark Y. Liberman,
Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In HLT. Morgan Kaufmann.
Bob Carpenter. 1997. Type-Logical Semantics. The
MIT Press.
Kais Dukes. 2013a. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference.
Kais Dukes. 2013b. Supervised semantic parsing of
robotic spatial commands. http://alt.qcri.
org/semeval2014/task6/.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th An-
nual Meeting of the ACL, ACL-44, pages 913?920,
Stroudsburg, PA, USA. ACL.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence - Volume
3, AAAI?05, pages 1062?1068. AAAI Press.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly Learning to Parse and Perceive : Connecting
Natural Language to the Physical World. In Trans-
actions of ACL, volume 1, pages 193?206.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1512?1523, Stroudsburg, PA, USA. ACL.
389
Yann LeCun, L?eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the CoNLL-11 Shared Task.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311?324. Springer Berlin Heidelberg.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL (1), pages 933?943. ACL.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. EMNLP-
2010, Boston, USA.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of NAACL 2013, pages 627?633. ACL.
Mark Steedman. 1996. Surface structure and inter-
pretation. Linguistic inquiry monographs. The MIT
Press, Cambridge, MA, USA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA, USA.
390
