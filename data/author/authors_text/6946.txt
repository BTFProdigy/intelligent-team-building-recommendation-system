Using a Probabil istic Class-Based Lexicon 
for Lexical Ambiguity Resolution 
Det le f  P rescher  and Ste fan  R iez le r  and Mats  Rooth  
Inst i tut  fiir Maschinelle S1)rachvcrarbeitung 
UniversitSt Stuttgart ,  Germany 
Abst ract  
This paper presents the use of prot)abilistie 
class-based lexica tbr dismnbiguati(m in target- 
woxd selection. Our method emlfloys nfinimal 
1)llt; precise contextual information for disam- 
biguation. That is, only information provided 
by the target-verb, enriched by the condensed 
information of a probabilistic class-based lexi- 
con, is used. Induction of classes and fine-tuning 
to verbal arguments i done in an unsupervised 
manner by EM-lmsed clustering techniques. The 
method shows pronlising results in an evaluation 
on real-world translations. 
1 I n t roduct ion  
Disambiguation of lexical ambiguities in nat- 
urally oceuring free text is considered a hard 
task for computational linguistics. For instance, 
word sense disa.inbiguatiol~ is concerned with the 
protflem of assigning sense labels to occurrences 
of an ambiguous word. Resolving such ambi- 
guil;ies is useful in constraining semantic inter- 
pretation. A related task is target-word isam- 
biguation in machine translation. Here a deci- 
sion has to be made which of a set of alterna- 
tive target-language words is the most appro- 
priate translation of a source-language word. A 
sohltion to this disambiguation problem is di- 
rectly applicable in a machine translation sys- 
tem which is able to propose the translation al- 
ternatives. A further problem is the resolution 
of attachment ambiguities in syntactic parsing. 
Here the decision of verb versus argunlent at- 
ta&ment of noun phrases, or the choice for verb 
phrase versus noun phrase attachment of prepo- 
sitional phrases Call build upon a resolution of 
the related lexical mnbiguities. 
Statistical approaches have been applied suc- 
cessfully to these 1)roblems. The great advantage 
of statistical methods over symbolic-linguistic 
methods has been deemed to be their effec- 
tive exploitation of minimal linguist;it knowl- 
edge. However, the best performing statisti- 
cal approaches to lexical ambiguity resolution 
l;lmmselves rely on complex infornmtion sources 
such as "lemmas, inflected forms, parts of speech 
and arbitrary word classes If-.. \] local and dis- 
tant collocations, trigram sequences, a.nd predi- 
cate m'gument association" (Yarowsky (1995), p. 
190) or large context-windows up to 1000 neigh- 
boring words (Sch/itze, 1992). Unfortmmtely, in
many applications uch information is not read- 
ily available. For instance, in incremental ma- 
chine translation, it may be desirable to decide 
for the most probable translation of the argu- 
ments of a verb with only the translation of the 
verb as information source lint no large window 
of sun'ounding translations available. In parsing, 
the attachment of a nolninal head nlay haa~e to 
be resolved with only information al)out the se- 
mmltic roles of the verb but no other predi('ate 
argument associations at; hand. 
The aim of this paper is to use only nfinimal, 
but yet precise information fbr lexical ambiguity 
resolution. We will show that good results are 
obtainable by employing a simple and natural 
look-up in a probabilistic lass-labeled lexicon 
for disambiguation. The lexicon provides a prob- 
ability distribution on semantic selection-classes 
labeling the slots of verbal subcategorization 
frames. Induction of distributions on frames and 
class-labels is accomplished in an unsupervised 
manner by applying the EM Mgorittnn. Disam- 
biguation then is done by a simple look-up in the 
probabilistie lexicon. We restrict our attention 
to a definition of senses as alternative transla- 
tions of source-words. Our approach provides a 
very natural solution for such a target-language 
disambiguation task--look for the most fl'equent 
target-noun whose semantics fits best with the 
649 
Class  19 
PROB 0 .0235 
0 .0629 
0 .0386 
0 .0321 
0 .0236 
0 .0226 
0 .0214 
0 .0173 
0 .0136 
0 .0132 
0 .0126 
0 .0124 
0 .0115 
0 .0113 
0 .0108 
0 .0009 
0 .0086 
0 .0085 
0 .0082 
0 .0082 
0 .0082 
enter .aso :o  
cover .aso :o  
ca l l .aso :o  
i nc lude .aso :o  
l 't l lh &SO ; O 
a t tend .aso :o  
cross .aso :o  
dominate .aso :o  
have .aso :s  
at t rac t .aso :s  
occupy .aso :o  
inc lude .aso :s  
COllt aJ n,  ~-'~s o :S 
beCOlne,g8:S 
fo rn l .aso :o  
co l lapse .as :s  
cre l~te.aso:o  
prov ide .aso :s  
o rga l l i ze .aso :o  
o f fe r .aso :s  
d c:; c5 d d d d d d d d d d d d ~ d c; d c:; d d d c5 d d d d c:; 6 
? ? ? 
? ? ? 
? ? 
? ? ? 
? ? ? 
o 
? ? ? 
? ? ? ? ? ? * ? ? 
? ? ? ? ? ? ? ? ? el ? ? ? 
? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
Figure 1: Class 19: "locative action". At the top are listed the 20 most t:)robable nouns in the 
pLc(n119 ) distribution and their probabilities, and at leg are tile 30 most probable verbs in the 
pLC(V 119) distribution. 19 is the class index. Those verb-noun pairs which were seen in the training 
data appear with a dot in the class matrix. Verbs with suffix .as : s indicate the subject slot of an 
active intransitive. Similarily .aso : s denotes the subject slot of an active transitive, and .aso : o 
denotes the object slot of an active transitive. 
semantics required by the target-verb. We eval- 
uated this simple method on a large number of 
real-world translations and got results compara- 
ble to related approaches such as that of Dagan 
and Itai (1994) where much more selectional in- 
t!ormation is used. 
2 Lex icon  Induct ion  v ia EM-Based  
C lus ter ing  
2.1 EM-Based Clustering 
For clustering, we used the method described 
in Rooth et al (1999). There classes are de- 
rived from distributional data a sample of 
pairs of verbs and nouns, gathered by parsing 
an unannotated corpus and extracting tile fillers 
of grammatical relations. The semantically 
smoothed probability of a pair (v,n) is calcu- 
lated in a latent class (LC) model as pLC(V, n) = 
~<cPLC(C, v,'n). The joint distribution is de- 
fined by PLC(C, v, n) = PLC(C)PLc(V\[C)PLC(nIC ). 
By construction, conditioning of v and n on 
each other is solely made through the classes 
c. The parameters PLC(C), PLC(V\[C), PLC(n\[c) 
are estilnated by a particularily silnple version 
of tile EM algorithm for context-free models. 
Input to our clustering algorithm was a train- 
ing corpus of 1,178,698 tokens (608,850 types) 
of verb-noun pairs participating in the gram- 
matical relations of intransitive and transitive 
verbs and their subject- and object-fillers. Fig. 
1 shows an induced class froln a model with 35 
classes. Induced classes often have a basis in lex- 
ical semantics; class 19 can be interpreted as 
locative, involving location nouns "room", "are?', 
and "world" and verbs as "enter" and "cross". 
2.2 Probabil ist ic Labeling with Latent 
Classes using EM-est imat ion 
To induce latent classes tbr the object slot; of a 
fixed transitive verb v, another statistical infer- 
ence step was performed. Given a latent class 
modal PLC(') Ibr verb-noun pairs, and a sam- 
ple n l , . . . ,  nM of objects for a fixed transitive 
verb, we calculate tile probability of ml arbitrary 
object noun ,I, I~ N by p(n) = ~<cP(C,  ~;,) = 
~<c P(c)pLc(n'Ic)" This fine-tuning of the class 
parameters p(c) to tile sample of objects for a 
fixed verb is formalized again as a simple in- 
stance of the EM algorithm. In an experiment 
with English data, we used a clustering model 
with 35 classes. From the maximum probabil- 
650 
ity pm:ses derived fl)r the British National Cor- 
pus with the head-lexicalized parser of Carroll 
and Rooth (1.998), we extracted frequency ta- 
bles tbr transitive verb-noun pairs. These tables 
were used to induce a small class-labeled lexicon 
(336 verbs). 
cross.aso:o 19 0.692 
mind 74.2 
road 30.3 
line 28.1 
1)ridge 27.5 
room 20.5 
t)order 17.8 
l)oundary 16.2 
river 14.6 
street 11.5 
atlantic 9.9 
mobilize.aso:o 6 0.386 
h)rce 2.00 
t)eoi)le 1.95 
army 1.46 
sector 0.90 
society 0.90 
worker 0.90 
meinber 0.88 
company 0.86 
majority 0.85 
party 0.80 
lID 160867) Es gibt einigc alte Passvorschriften, die. be- 
sagen, dass man cinch Pass habcn muss, wcnn man dic 
Grenze iiberschreitct. There are some old provisions re- 
ga.rding passports which state that people crossing the 
{border/ frontier/ boundary/ limit/ periphery/ 
edge} shoukI have their 1)assl)ort on them. 
lID 201946) Es 9ibt sehlie.sslich keinc L5sung ohne 
die Mobilisierung der bii~yerlichen Gesellschaft und 
die Solidaritiit dcr Dcmok,nten in der 9anzcn Welt. 
Ttmrc can be no solution, tinally, mflcss civilian {com- 
pany/  society/companionship/party/associate} 
is mobilized and solidarity demonstrated bydemocrats 
throughout the world. 
Figure 3: Exami)les for target-word ambiguities 
Figure 2: Estinmted fl:equencies of the objects 
of' the transitive verbs cross and mobi l ize 
Fig. 2 shows the topmost parts of the lexical 
entries for the transitive verbs cross and mo-  
bilize. Class 19 is the most prol)abh ~,class-label 
for the ol)jeet-slot of cross (prol)al)ility 0.692); 
tl~e objects of mobi l ize belong with prol)ability 
0.386 to class 16, which is the most probable 
(:lass for this slot. Fig. 2 shows for each verb the 
te l l  llOllllS 'It with highest estimated frequencies 
.l',,('n,) = f (n)p(cln), where . f lu) is the fre(\]ll(~.ll(:y 
of n in the sample v,l, ? ? ? , 'n,M. For example, the 
Dequency of seeing mind as object of c,ro.ss is 
estimated as 74.2 times, and the most fl'equent 
object of mobi l ize  is estimated to be force. 
3 Disambiguat ion  w i th  P robab i l i s t i c  
C lus ter -Based  Lex icons  
Ii:t the following, we will des(:ril)e the simt}le 
and natural lexicon look-up mechanism which 
is eml)loyed in our disambiguation at)t)roach. 
Consider Fig. 3 which shows two bilingual sen- 
tences taken from our evaluation corlms (see 
Sect. 4). The source-words and their correspond- 
ing target-words are highlighted in bo ld  thee. 
The correct translation of the source-noun (e.g. 
Gre.nzc) as deternfined by the actual trmlslators 
is replaced by the set of alterlmtive translations 
(e.g. { border, frontier, b(mndary, limit, peril)h- 
cry, edge }) as proposed by the word-to-word 
dictionary of Fig. 5 (see Sect. LI). 
The prol)lem to be solved is to lind a correct 
l;ranslation of the source-word using only min- 
imal contextual intbrmation. In our apt)roach , 
the decision between alternative target-nouns i
done by llSillg only int'ormal,ion provided by the 
governing target-verb. The key idea is to back 
up this nfinimal information with the condensed 
and precise information of a probabilistic lass- 
based lexicon. The criterion for choosing an al- 
terlmtive target-noun is thus the best fit of the 
lexical and semantic information of the target:- 
noun to the semantics of the argument-slot of 
the target-verb. This criterion is checked by a 
silnple lexicon look-up where the target-noun 
with highest estinmted class-based fl'equeney is 
determined. Fornmlly, choose l;11(; tm'get-nom~ gt,
(and a class ~?) such that 
j& , , )=   .ax 
'nC N~c~C 
where L-(-.) = f ( - , )v(d- . )  is the estimated fre- 
quency of 'n, in tile sample of objects of a 
fixed target-verb, p(cl,n ) is the class-melnbershi t)
probability of'n in c as determined by the proba- 
bilistic lexicon, and f (n)  is the frequency of n in 
the combined sample of objects and trmlslation 
alternatives1. 
Consider example ID 160867 fron, Fig. 3. The 
mnbiguity to be resolved concerns the direct ob- 
jects of the verb cross  whose lexical entry is 
partly shown in Fig. 2. Class 19 and the noun 
border is the pair yielding a higher estimated 
trequency than any other combination of a class 
and an alternative translation such as boundary. 
Similarly, for example ID 301946, the pair of the 
1Note that p(8) = max p(c) in most, but l lOt all cases. 
c E C  - - 
651 
target-noun society and class 6 gives highest es- 
timated frequency of the objects of mobilize. 
4 Evaluat ion 
We evaluated our resolution methods on a 
pseudo-disambiguation ask sinlilar to that used 
in Rooth et al (1999) for evaluating clustering 
models. We used a test set of 298 (v, n, n ~) triples 
where (v, n) is chosen randomly from a test cor- 
pus of pairs, and n ~ is chosen randomly accord- 
ing to the marginal noun distribution for the test 
corpus. Precision was calculated as the nmnber 
of times the disambiguation method decided for 
the non-random target noun (f~. = n). 
As shown in Fig. 4, we obtained 88 % pre- 
cision for the class-based lexicon (ProbLex), 
which is a gain of 9 % over the best cluster- 
ing model and a gain of 15 % over the hmnan 
baseline 2 .
human clustering ProbLex mnt)iguity baseline 
2 73.5 % 79.0 % 88.3 % I 
Figure 4: Evaluation on pseudo-disambiguation 
task for noun-ambiguity 
The results of the pseudo-disambiguation 
could be confirmed in a fllrther evaluation on a 
large number of randonfly selected examples of 
a real-world bilingual corpus. The corpus con- 
sists of sentence-aligned debates of the Euro- 
pean parliament (mlcc = multilingual corpus 
for cooperation) with ca. 9 million tokens for 
German and English. From this corpus we pre- 
pared a gold standard as follows. We gathered 
word-to-word translations from online-available 
dictionaries and eliminated German nouns fbr 
which we could not find at least two English 
translations in the mice-corpus. The resulting 
35 word dictionary is shown in Fig. 5. Based on 
this dictionary, we extracted all bilingual sen- 
tence pairs from the corpus which included both 
the source-noun and the target-noun. We re- 
stricted the resulting ca. 10,000 sentence pairs 
to those which included a source-noun from this 
2Similar esults for pseudo-dismnbiguation were ob- 
tained for a simpler approach which avoids an- 
other EM application for probabilistic lass labeling. 
Here ~ (and ~) was chosen such that f~(v,~) = 
max((fLc (v, n) + 1)pcc (el v, n)). However, the sensitivity 
to class-parmnetcrs was lost in this approach. 
dictionary in the object position of a verb. Fm'- 
therniore, the target-object was required to be 
included in our dictionary mid had to appear 
in a similar verb-object position as the source- 
object fbr an acceptable English translation of 
the German verb. We marked the German noun 
n q in the source-sentence, its English translation 
ne as appearing in the corpus, and the English 
lexical verb re. For the 35 word dictionary of 
Fig. 5 this senti-automatic procedure resulted 
ill a test corpus of 1,340 examples. The aver- 
age ambiguity in this test corpus is 8.63 trans- 
lations per source-word. Furthermore, we took 
the semantically most distant ranslations for 25 
words which occured with a certain fi'equency 
in the ew~luation corpus. This gave a corpus of 
814 examples with an average ambiguity of 2.83 
translations. The entries belonging to this dic- 
tionary are highlighted in bo ld  face in Fig. 5. 
The dictionaries and the related test corpora are 
available on the web 3. 
We believe that an evaluation on these test 
corpora is a realistic simulation of the hard task 
of target-language disambiguation i  real-word 
machine translation. The translation alterna- 
tives are selected fl'om online dictionaries, cor- 
rect translations are deternfined as the actual 
translations found in the bilingual corpus, no 
examples are omitted, the average ambiguity is 
high, and the translations are often very close 
to each other. In constrast o this, most other 
evaluations are based on frequent uses of only 
two clearly distant senses that were deternfined 
as interesting by the experimenters. 
Fig. 6 shows the results of lexical ambigu- 
ity resolution with probabilistic lcxica in com- 
parison to simpler methods. The rows show 
the results tbr evaluations on the two corpora 
with average ambiguity of 8.63 and 2.83 respec- 
tively. Colunm 2 shows the percentage of cor- 
rect translations found by disambiguation by 
random choice. Column 3 presents as another 
baseline disambiguation with the major sense, 
i.e., always choose the most frequent target- 
noun as translation of the source-noun. In col- 
unto 4, the empirical distribution of (v, n) pairs 
in the training corpus extracted from the BNC 
is used as disambiguator. Note that this method 
yields good results in terms of precision (P - 
#correct / $correct + $incorrect), but is much 
3http ://www. ims .uni-stuttgart. de/proj ekt e/gramot ron/ 
652 
Angrif \[  
A r t  
Au fgabe  
Auswahl 
Begriff 
Boden 
E in r i cht  ung  
Erwe i t  e rung  
Feh ler  
Gcnehmigung 
C leseh ichte  
Gesd lschaf t  
O| -e l |ze  
Grund  
Kar te  
Lage 
Mangel 
Menge 
Pr f i fung  
Schwler igke l t  
Se i te  
S icherhe i t  
S thnme 
' rer l l l in  
"Vet'blnd ung  
Verbot 
Verp f l i ch t  u ng  
"~'et't ra l len  
"Wahl 
"lgVeg 
Widers tand 
Zeiehen 
Ziel  
Z \[isaln 111 e llll alia 
Zust lmmung 
aggression, assault,  oll)2nce, onset, onsbmght,  attack , charge, raid, whammy, inroad 
form~ type ,  way ,  fashion, lit, kind, wise, lllallller, species, mode, sort, wtriety 
abandonment~ otIieo~ task ,  exercise, lesson, giveup, jot) , 1)roblcm, tax 
eligibility, selection, choice, wwlty, assortment,  extract,  range, sample 
concept, item, notion, idea 
ground,  land,  soi l ,  floor, bottom 
ar rangement ,  ins t i tu t ion ,  const itut ion,  cstablishlnellt,  feature, instal lation, construction, setup, adjustment,  composit ion, 
organizat ion 
ampl i f i ca t ion ,  extens ion ,  enhancement,  expansion, di latat ion,  upgr~ding, add-on, increment 
error~ shor tcoming ,  blemish, I)lunder, bug, defect, demeri t ,  failure, fault, flaw, mistake,  trouble, slip, blooper, lapsus 
pernlission, approval, COllSellt, acceptance, al)l)robation , author izat ion 
hlstory~ s tory ,  tale, saga, str ip 
company~ soc ie ty ,  COmlmnionshil), party, associate 
border ,  f ront ie r ,  boundary, Ihnl t ,  periphery, edge 
nlaster~ n lat ter~ reasoll~ base, catlse, grOlllld~ bottoli i  root 
card ,  map,  ticket, chart 
site, s i tuat ion,  position, bearing, layer, tier 
deficiency, lack, pr ivation, want, shortage, shortcoming,  absence, dearth,  demerit ,  des ideratum, insufticimlcy, paucity, scarceness 
alnol lnt~ deal ,  lot,  Illass I mtlltitttde, l)lenty, qtlalltity, quiverful~ vOhlllle 1 abull(latlce, aplellty 1 
assemblage , crowd, batch, crop, heal), lashings, scores, set, loads, I)ulk 
examinat lon ,  sc rut iny ,  ver i f i ca t ion ,  ordeal, test, trial,  inspection, tryout,  
assay, canvass, check, inquiry~ perusal, reconsideration, scrut ing 
difficttlty~ trol l l l le  1 problenl, severity, ar(lotlSlleSS 1heaviness 
page~ party~ s lde,  point, aspect 
cer ta in ty ,  guarantee ,  sa fe ty ,  immunity,  security , collateral , doubtlessness, ureness, deposit  
voice~ vote ,  tones 
elate, deadl ine~ meethtg ,  appointment,  t ime, term 
assoc la t ion ,  contact ,  link~ cha\[ll, ColIjtlnCtlOll~ COlll/ectioll~ fllSiOll, joint , conlpOtlll(l~ all iance, cl~tenation, tie, lllllOIl I t)Olld~ 
interface, liaison, touch, relation, incorporat ion 
ban, interdiction, I)rohibition, forbiddance 
eomin i tment :  ob l igat ion ,  under tak ing ,  duty, indebtedness , onus, debt, engagement,  liability, bond 
COllfidence~ re l lance ,  trl lst~ faith, asstlrance~ dependence,  pr ivate, secret 
e lec t ion ,  opt ion ,  choice , ballot, alternagive, poll , list 
path~ road ,  way ,  alley, route, lane 
resistance, opposit ion, drag 
character,  icon, Sigll I sigllal, Syllll)ol, lllark, tokell~ figure, olneil 
ahn ,  des t inat ion ,  end ,  designation, target,  goal, object, objective, sightings, intent imb prompt  
coherence, context~ COlltlgtllty, connectloli 
agree inent~ approvaI~ assont ,  accordance, approbat ion,  consent, af I innation, allowance, compliance, comi)Iiancy, acclamation 
Figure 5: Dictionaries extracted from online resources 
ambiguity random major emlfirical sense distrib, clusl;ering ProbLex 
P: 46.1% 
8.63 14.2 % 31.9 % E: 36.2 % 43.3 % 49.4 % 
P: 60.8 % 
2.83 35.9 % 45.5 % E: 49.4 % 61.5 % 68.2 % 
Figure 6: Disambig, mtion results for clustering versus probabilistic lexicon methods 
worse in terms of effectiveness (E //corre(-t 
/ \]/-correct q #:incorrect \]/:don't know). The 
reason for this is that even if the distribution 
(ff (v,n) pairs is estimated quite precisely for 
the pairs in the large training corpus, there are 
still many pairs which receive the same or no 
positive probability at all. These effects can'be 
overcome by a clustering approach to disam- 
biguation (column 5). Here the class-smoothed 
probability of a (v, n) pair is used to decide be- 
tween alternative target-nouns. Since the clus- 
tering model assigns a more fine-grained prob- 
ability to nearly every pair in its domain, there 
are no don't know cases for comparable preci- 
sion values. However, the senmntically smoothed 
probability of the clustering models is still too 
coarse-grained when compared to a disambigua- 
tion with a prot)abilistic lexicon. Here ~ fllrther 
gain in precision and equally effectiveness of ca. 
7 % is obtained on both corpora (column 6). 
We conjecture that this gain (:an be attrilmted 
to the combination of Dequency iilformation of 
the nouns and the fine-tuned istribution on the 
selection classes of the the nominal arguments 
of the verbs. We believe that including the set 
of translation alternatives in the ProbLex dis- 
tribution is important for increasing efficiency, 
because it gives the dismnbiguation model the 
opportunity to choose among unseen alterna- 
tives. Furthermore, it seems that the higher pre- 
cision of ProbLex can not be attributed to filling 
in zeroes in the empirical distribution. Rather, 
we speculate that ProbLex intelligently filters 
the empirical distribution by reducing maximal 
653 
counts for observations which do not fit into 
classes. This might help in cases where the em- 
pirical distribution has equal values for two al- 
ternatives. 
source target 
Seite 
Sicherheit 
Verbindung 
Verpflichtung 
Ziel 
overall precision 
page 
side 
guarantee 
safety 
commction 
link 
commitment 
obligation 
objective 
target 
Figure 7: Precision for finding correct and ac- 
ceptable translations by lexicon look-up 
Fig. 7 shows the results for disambiguation 
with probabilistic lexica for five sample words 
with two translations each. For this dictionary, 
a test corpus of 219 sentences was extracted, 200 
of which were additionally labeled with accept- 
able translations. Precision is 78 % for finding 
correct translations and 90 % for finding accept- 
able translations. 
Furthermore, in a subset of 100 test items 
with average ambiguity 8.6, a lmnlan judge hav- 
ing access only to the English verb and the set of 
candidates for the targel,-lloun, i.e. the informa- 
tion used by the model, selected anlong transla- 
tions. On this set;, human precision was 39 %. 
5 D iscuss ion  
Fig. 8 shows a comparison of our approadl 
to state-of-the-art unsupervised algorithlns for 
word sense disambiguation. Column 2 shows the 
number of test examples used to evaluate the 
various approaches. The range is from ca. 100 
examples to ca. 37,000 examples. Our method 
was evaluated on test corpora of sizes 219, 814, 
and 1,340. Column 3 gives the average number 
of senses/eranslations for the different disam- 
biguation methods. Here the range of the ambi- 
guity rate is from 2 to about 9 senses 4. Column 4 
4The mnbiguity factor 2.27 attributed to Dagan and 
Itai's (1994) experiment is calculated by dividing their 
average of 3.27 alternative translations by their average 
of 1.44 correct translations. Furthermore, we calculated 
the ambiguity factor 3.51 for Resnik's (1997) experiment 
shows the rmldom baselines cited for the respec- 
tive experiments, ranging t'rom ca. 11% to 50 %. 
Precision values are given in column 5. In order 
to compare these results which were computed 
for different ambiguity factors, we standardized 
the measures to an evaluation for binary ambi- 
guity. This is achieved by calculal;ing pl/log2 arab 
for precision p and ambiguity factor arab. The 
consistency of this "binarization" can be seen by 
a standardization of the different random base- 
lines which yields a value of ca. 50 % for all 
approaches 5. The standardized precision of our 
approach is ca. 79 % on all test corpora. The 
most direct point of comparison is the method 
of Dagan and Itai (1994) whirl1 gives 91.4 % pre- 
cision (92.7 % standardized) and 62.1% effec- 
tiveness (66.8 % standardized) on 103 test; exam- 
ples for target word selection in the transfer of 
Hebrew to English. However, colnpensating this 
high precision measure for the low effectiveness 
gives values comparable to our results. Dagan 
and Itai's (1994) method is based on a large vari- 
ety of gramnmtieal relations tbr verbal, nominal, 
and adjectival predicates, but no class-based in- 
fornmtion or slot-labeling is used. I{esnik (1997) 
presented a disambiguation method which yields 
44.3 % precision (63.8 % standardized) tbr a 
test set of 88 verb-object tokens. His approach is 
coral)arable to ours in terlns of infbrmedness of 
the (tisambiguator. Hc also uses a class-based se- 
lection measure, but based on WordNet classes. 
However, the task of his evaluation was to se- 
lect WordNet-senses tbr the objects rather than 
the objects themselves, so the results cannot 
be compared directly. The stone is true for the 
SENSEVAL evaluation exelcise (Kilgarriff and 
Rosenzweig, 2000)--there word senses from the 
HECTOl~-dictionary had to be disambiguated. 
The precision results for the ten unsupervised 
systems taking part in the comt)etitive valu- 
ation ranged Kern 20-65% at efficiency values 
from 3-54%. The SENSEVAL '~tan(lard is clearly 
beaten by the earlier results of Yarowsky (1995) 
(96.5 % precision) and Schiitze (1992) (92 % 
precision). However, a comparison to these re- 
from his random baseline 28.5 % by taking 100/28.5; re- 
versely, Dagan and Itai's (1994) random baseline can be 
calculated as 100/2.27 = 44.05. Tile ambiguity t;'~ctor for 
SENSEVAL is calculated for tile llOUll task in the English 
SENSEVAL test set. 
5Note that  we are guaranteed to get exactly 50 % 
standardized random 1)aseline if random, arab = 100 %. 
654 
disambiguation corlms random precision 
method size aml)iguity random 1)recision (standardized) (standardized) 
)robLex 1 340 8.63 14.2 % 49.4 % 53.4 % 79.7 % 
814 2.83 35.9 % 68.2 % 50.5 % 77.5 % 
219 2 50.0 % 78.0 % 50.0 % 78.0 % 
)agan, Itai 94 
{esnik 97 
;ENSEVAL 00 
(m'owsky 95 
',chiitze 92 
103 
88 
2 756 
37 000 
3 000 
2.27 
3.51 
9.17 
2 
2 
44.1% 
28.5 % 
10.9 % 
50.0 % 
50.0 % 
P: 91.4 % 
E: 62.1% 
44.3 % 
P: 20-65 % 
E: 3-54 % 
96.5 % 
92.0 % 
50.0 % 
50.0 % 
50.0 % 
50.0 % 
50.0 % 
P: 92.7 % 
E: 66.8 % 
63.8 % 
P: 60-87 % 
E: 33-83 % 
96.5 % 
92.0 % 
Figure 8: Comparison of unsupervised lexical disambiguation methods. 
sults is again somewhat difficult. Firstly, these 
at)proaches were ewfluated on words with two 
clearly (tistmlt senses which were de/;el'nfined by 
the experimenters. In contrast, our method was 
evalutated on randonfly selected actual transla- 
tions of a large t)ilingual cortms. Furthermore, 
these apl)roaches use large amounts of infbrma- 
tion in terms of linguistic ca.tegorizations, large 
context windows, or even 1111nual intervention 
such as initial sense seeding (hqtrowsky, 1995). 
Such information is easily obtainabh;, e.g., in I1\]. 
at)tflications , but often burdensome to gather or 
sim.i)ly uslavail~bh'~ in situations such as incre- 
mental parsing O1' translation. 
6 Conc lus ion  
The disanfl3iguation method presented in this 
pa.per delibera.tely is restricted to the limited 
mnomlt of information provided by a proba- 
bilistic class-based lexicon. This intbrmation yet 
proves itself accurate nough to yield good em- 
pirical results, e.g., in target-language disam- 
biguation. The t)rol)al)ilistic class-based lexica 
are induced in an unsupervised manner fl'om 
large mmnnotated corpora. Once the lexica are 
constructed, lexical mnbiguity resolution can be 
done by a simple lexicon look-up. I51 target- 
word selection, the nlOSt fl'equent target-noun 
whose semantics fits best to tit(; semantics of the 
argument-slot of the target-verb is chosen. We 
evaluated our method on randomly selected ex- 
amities Dora real-world bilingual corpora which 
constitutes a realistic hard task. Dismnbiguation 
based on probabilistie lexica perfornmd satisfim-' 
tory for this |;ask. The lesson lem'ned tYom our 
experimental results is that hybrid models con> 
bining fi:equency information and class-based 
t)robabilities outlmrtbnn both pure fl'equency- 
based models and pure clustering models. 1'511"- 
ther improvements are to be expected from 
extended lexica including, e.g., adjectival and 
prepositional predicates. 
References  
Gleml Carroll mid Mats F\[ooth. 1998. Valence 
induction with a head-lexicalized PCFG. In 
P'roceediugs o.f EMNLP-,7, Granada. 
Ido l)agan and Ahm Itai. 1994:. Word sense dis- 
ambiguation using a second language 1110510- 
linguaJ corlms. Computational Linguistics, 
20:563 596. 
Adam Kilgarriff and Joseph lq.osenzweig. 2000. 
English SENSEVAIA I-{.el)ol't and results. In 
Proceedings of LR\]';C 2000. 
Philip l{csnik. 1997. Selectional preference and 
sense dis~mfl)iguation, l\[ll Proceedings of the 
ANLP'97 Workshop: Tagging Tc:ct 'with Lezi- 
cal Semantics: Why, What, and How?, V~:ash- 
ington, D.C. 
Mats l\].ooth, Stefan I{iezler, Detlef Prescher, 
Glenn Carroll, and Franz Bell. 1999. Induc- 
ing a semantically annotated lexicon via EM- 
based clustering. In Proceedings of the 37th 
Annual Meeting of th, c Assoc.iation .for Com,- 
putational Linguistics (A CL '99), Maryland. 
Ilinrieh Schfitze. 1992. Dimensions of meaning. 
151 Proceedings of S'upercomlnd.ing '92. 
David Yarowsky. 1995. Unsupervised word 
sense dismnbiguation rivaling supervised 
methods. In Proceedings of the 33rd Annual 
Meeting of th, c Association for Compv, tational 
Linguistics (ACL'95), Cambridge, MA. 
655 
Parse Forest Computation of Expected Governors
Helmut Schmid
Institute for Computational Linguistics
University of Stuttgart
Azenbergstr. 12
70174 Stuttgart, Germany
schmid@ims.uni-stuttgart.de
Mats Rooth
Department of Linguistics
Cornell University
Morrill Hall
Ithaca, NY 14853, USA
mats@cs.cornell.edu
Abstract
In a headed tree, each terminal word
can be uniquely labeled with a gov-
erning word and grammatical relation.
This labeling is a summary of a syn-
tactic analysis which eliminates detail,
reflects aspects of semantics, and for
some grammatical relations (such as
subject of finite verb) is nearly un-
controversial. We define a notion
of expected governor markup, which
sums vectors indexed by governors and
scaled by probabilistic tree weights.
The quantity is computed in a parse for-
est representation of the set of tree anal-
yses for a given sentence, using vector
sums and scaling by inside probability
and flow.
1 Introduction
A labeled headed tree is one in which each non-
terminal vertex has a distinguished head child,
and in the usual way non-terminal nodes are la-
beled with non-terminal symbols (syntactic cat-
egories such as NP) and terminal vertices are
labeled with terminal symbols (words such as
The governor algorithm was designed and implemented
in the Reading Comprehension research group in the 2000
Workshop on Language Engineering at Johns Hopkins Uni-
versity. Thanks to Marc Light, Ellen Riloff, Pranav Anand,
Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen
for discussion and assistance. Oral presentations were made
at that workshop in August 2000, and at the University of
Sussex in January 2001. Thanks to Fred Jelinek, John Car-
roll, and other members of the audiences for their comments.
S  
NP  	
 
Peter
VP  
V  
reads
NP   	 
NP    
D 	 
every
N   	 
paper
PP:on    
P:on 
on
NP   

N   

markup
Figure 1: A tree with percolated lexical heads.
reads).1 We work with syntactic trees in which
terminals are in addition labeled with uninflected
word forms (lemmas) derived from the lexicon.
By percolating lemmas up the chains of heads,
each node in a headed tree may be labeled with
a lexical head. Figure 1 is an example, where lex-
ical heads are written as subscripts. We use the
notation
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214?217,
Paris, October 2009. c?2009 Association for Computational Linguistics
Smoothing fine-grained PCFG lexicons
Tejaswini Deoskar
ILLC
University of Amsterdam
t.deoskar@uva.nl
Mats Rooth
Dept. of Linguistics and CIS
Cornell University
mr249@cornell.edu
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
We present an approach for smoothing
treebank-PCFG lexicons by interpolating
treebank lexical parameter estimates with
estimates obtained from unannotated data
via the Inside-outside algorithm. The
PCFG has complex lexical categories,
making relative-frequency estimates from
a treebank very sparse. This kind of
smoothing for complex lexical categories
results in improved parsing performance,
with a particular advantage in identify-
ing obligatory arguments subcategorized
by verbs unseen in the treebank.
1 Introduction
Lexical scarcity is a problem faced by all sta-
tistical NLP applications that depend on anno-
tated training data, including parsing. One way
of alleviating this problem is to supplement super-
vised models with lexical information from unla-
beled data. In this paper, we present an approach
for smoothing the lexicon of a treebank PCFG
with frequencies estimated from unannotated data
with Inside-outside estimation (Lari and Young,
1990). The PCFG is an unlexicalised PCFG, but
contains complex lexical categories (akin to su-
pertags in LTAG (Bangalore and Joshi, 1999) or
CCG (Clark and Curran, 2004)) encoding struc-
tural preferences of words, like subcategorization.
The idea behind unlexicalised parsing is that the
syntax and lexicon of a language are largely inde-
pendent, being mediated by ?selectional? proper-
ties of open-class words. This is the intuition be-
hind lexicalised formalisms like CCG: here lexical
categories are fine-grained and syntactic in nature.
Once a word is assigned a lexical category, the
word itself is not taken into consideration further
in the syntactic analysis. Fine-grained categories
imply that lexicons estimated from treebanks will
be extremely sparse, even for a language like En-
glish with a large treebank resource like the Penn
Treebank (PTB) (Marcus et al, 1993). Smoothing
a treebank lexicon with an external wide-coverage
lexicon is problematic due to their respective rep-
resentations being incompatible and without an
obvious mapping, assuming that the external lexi-
con is probabilistic to begin with. In this paper, we
start with a treebank PCFG with fine-grained lex-
ical categories and re-estimate its parameters on a
large corpus of unlabeled data. We then use re-
estimates of lexical parameters (i.e. pre-terminal
to terminal rule probabilities) to smooth the orig-
inal treebank lexical parameters by interpolation
between the two. Since the treebank PCFG itself is
used to propose analyses of new data, the mapping
problem is inherently taken care of. The smooth-
ing procedure takes into account the fact that unsu-
pervised estimation has benefits for unseen or low-
frequency lexical items, but the treebank relative-
frequency estimates are more reliable in the case
of high-frequency items.
2 Treebank PCFG
In order to have fine-grained and linguistic lexi-
cal categories (like CCG) within a simple formal-
ism with well-understood estimation methods, we
first build a PCFG containing such categories from
the PTB. The PCFG is unlexicalised (with lim-
ited lexicalization of certain function words, like
in Klein and Manning (2003)). It is created by
first transforming the PTB (Johnson, 1998) in an
appropriate way and then extracting a PCFG from
the transformed trees (Deoskar and Rooth, 2008).
All functional tags in the PTB (such as NP-SBJ,
PP-TMP, etc.) are maintained, as are all empty
categories, making long-distance dependencies re-
coverable. The PCFG is trained on the standard
training sections of the PTB and performs at the
state-of-the-art level for unlexicalised PCFGs, giv-
ing 86.6% f-score on Sec. 23.
214
VP
VB.np
add
NP
four more
Boeings
PP-TMP
by 1994
PP-CLR
to the
two units.
(a) An NP PP subcategorization frame marked on the
verb ?add? as np. Note that the arguments NP and PP-
CLR are part of the subcategorization frame and are
represented locally on the verb but the adjunct PP-
TMP is not.
VP
VBG.s.e.to
seeking
S.e.to
+E-NP+ VP.to
TO
to
VP
avoid..
(b) An S frame on the verb ?seeking?: +E-
NP+ represents the empty subject of the
S. Note that structure internal to S is also
marked on the verb.
VP
Vb.sb
think
SBAR
+C+ S
the consumer
is right
(c) An SBAR frame: +C+ is the
empty complementizer.
Figure 1: Subcategorized structures are marked as features on the verbal POS category.
An important feature of our PCFG is that pre-
terminal categories for open-class items like verbs,
nouns and adverbs are more complex than PTB
POS tags. They encode information about the
structure selected by the lexical item, in effect,
its subcategorization frame. A pre-terminal in our
PCFG consists of the standard PTB POS tag, fol-
lowed by a sequence of features incorporated into
it. Thus, each PTB POS tag can be considered to
be divided into multiple finer-grained ?supertags?
by the incorporated features. These features en-
code the structure selected by the words. We fo-
cus on verbs in this paper, as they are important
structural determiners. A sequence of one or more
features forms the ?subcategorization frame? of a
verb: three examples are shown in Figure 1. The
features are determined by a fully automated pro-
cess based on PTB tree structure and node labels.
There are 81 distinct subcategorization frames for
verbal categories. The process can be repeated for
other languages with a treebank annotated in the
PTB style which marks arguments like the PTB.
3 Unsupervised Re-estimation
Inside-outside (henceforth I-O) (Lari and Young,
1990), an instance of EM, is an iterative estima-
tion method for PCFGs that, given an initial model
and a corpus of unannotated data, produces mod-
els that assign increasingly higher likelihood to
the corpus at each iteration. I-O often leads to
sub-optimal grammars, being subject to the well-
known problem of local maxima, and dependence
on initial conditions (de Marcken, 1995) (although
there have been positive results using I-O as well,
for e.g. Beil et al (1999)). More recently, Deoskar
(2008) re-estimated an unlexicalised PTB PCFG
using unlabeled Wall Street Journal data. They
compared models for which all PCFG parameters
were re-estimated from raw data to models for
which only lexical parameters were re-estimated,
and found that the latter had better parsing results.
While it is common to constrain EM either by
good initial conditions or by heuristic constraints,
their approach used syntactic parameters from a
treebank model to constrain re-estimation of lex-
ical parameters. Syntactic parameters are rela-
tively well-estimated from a treebank, not being as
sparse as lexical parameters. At each iteration, the
re-estimated lexicon was interpolated with a tree-
bank lexicon, ensuring that re-estimated lexicons
did not drift away from the treebank lexicon.
We follow their methodology of constrained
EM re-estimation. Using the PCFG with fine
lexical categories (as described in ?2) as the ini-
tial model, we re-estimate its parameters from an
unannotated corpus. The lexical parameters of
the re-estimated PCFG form its probabilistic ?lex-
icon?, containing the same fine-grained categories
as the original treebank PCFG. We use this re-
estimated ?lexicon? to smooth the lexical proba-
bilities in the treebank PCFG.
4 Smoothing based on a POS tagger : the
initial model.
In order to use the treebank PCFG as an initial
model for unsupervised estimation, new words
from the unannotated training corpus must be in-
cluded in it ? if not, parameter values for new
words will never be induced. Since the treebank
model contains no information regarding correct
feature sequences for unseen words, we assign all
possible sequences that have occurred in the tree-
bank model with the POS tag of the word. We
assign all possible sequences to seen words as
215
well ? although the word is seen, the correct fea-
ture sequence for a structure in a training sentence
might still be unseen with that word. This is done
as follows: a standard POS-tagger (TreeTagger,
(Schmid, 1994)) is used to tag the unlabeled cor-
pus. A frequency table cpos(w, ?) consisting of
words and POS-tags is extracted from the result-
ing corpus, where w is the word and ? its POS
tag. The frequency cpos(w, ?) is split amongst all
possible feature sequences ? for that POS tag in
proportion to treebank marginals t(?, ?) and t(?)
cpos(w, ?, ?) = t(?, ?)t(?) cpos(w, ?) (1)
Then the treebank frequency t(w, ?, ?) and the
scaled corpus frequency are interpolated to get a
smoothed model tpos. We use ?=0.001, giving a
small weight initially to the unlabeled corpus.
tpos(w, ?, ?) = (1? ?)t(w, ?, ?) + ?cpos(w, ?, ?)
(2)
The first term will be zero for words unseen in the
treebank: their distribution in the smoothed model
will be the average treebank distribution over all
possible feature sequences for a POS tag. For
seen words, the treebank distribution over feature
sequence is largely maintained, but a small fre-
quency is assigned to unseen sequences.
5 Smoothing based on EM re-estimation
After each iteration i of I-O, the expected counts
cemi(w, ?, ?) under the model instance at itera-
tion (i ? 1) are obtained. A smoothed treebank
lexicon temi is obtained by linearly interpolating
the smoothed treebank lexicon tpos(w, ?, ?) and a
scaled re-estimated lexicon c?emi(w, ?, ?).
temi(w, ?, ?) = (1??)tpos(w, ?, ?)+?c?emi (w, ?, ?)
(3)
where 0 < ? < 1. The term c?emi(w, ?, ?) is ob-
tained by scaling the frequencies cemi(w, ?, ?) ob-
tained by I-O, ensuring that the treebank lexicon is
not swamped with the large training corpus1.
c?emi(w, ?, ?) = t(?, ?)?
w cemi(w, ?, ?)
cemi(w, ?, ?)
(4)
? determines the relative weights given to the
treebank and re-estimated model for a word. Since
parameters of high-frequency words are likely
to be more accurate in the treebank model, we
parametrize ? as ?f according to the treebank fre-
quency f = t(w, ?).
1Note that in Eq. 4, the ratio of the two terms involving
cemi is the conditional, lexical probability Pemi(w|?, ?).
6 Experiments
The treebank PCFG is trained on sections 0-22 of
the PTB, with 5000 sentences held-out for evalu-
ation. We conducted unsupervised estimation us-
ing Bitpar (Schmid, 2004) with unannotated Wall
Street Journal data of 4, 8 and 12 million words,
with sentence length <25 words. The treebank
and re-estimated models are interpolated with ? =
0.5 (in Eq. 3). We also parametrize ? for treebank
frequency of words ? optimizing over a develop-
ment set gives us the following values of ?f for
different ranges of treebank word frequencies.
if t(w, ?) <= 5 , ?f = 0.5
if 5 < t(w, ?) <= 15 , ?f = 0.25
if 15 < t(w, ?) <= 50 , ?f = 0.05
if t(w, ?) > 50 , ?f = 0.005
(5)
Evaluations are on held-out data from the PTB
by stripping all PTB annotation and obtaining
Viterbi parses with the parser Bitpar. In addition
to standard PARSEVAL measures, we also eval-
uate parses by another measure specific to sub-
categorization2 : the POS-tag+feature sequence on
verbs in the Viterbi parse is compared against the
corresponding tag+feature sequence on the trans-
formed PTB gold tree, and errors are counted. The
tag-feature sequence correlates to the structure se-
lected by the verb, as exemplified in Fig. 1.
7 Results
There is a statistically significant improvement3
in labeled bracketing f-score on Sec. 23 when
the treebank lexicon is smoothed with an EM-re-
estimated lexicon. In Table 1, tt refers to the base-
line treebank model, smoothed using the POS-
tag smoothing method (from ?4) on the test data
(Sec. 23) in order to incorporate new words from
the test data4. tpos refers to the initial model for
re-estimation, obtained by smoothed the treebank
model with the POS-tag smoothing method with
the large unannotated corpus (4 million words).
This model understandably does not improve over
tt for parsing Sec. 23. tem1,?=0.5 is the model
obtained by smoothing with an EM-re-estimated
model with a constant interpolation factor ? =
0.5. This model gives a statistically significant im-
provement in f-score over both tt and tpos. The
last model tem1,?f is obtained by smoothing with
2PARSEVAL measures are known to be insensitive to sub-
categorization (Carroll et al, 1998).
3A randomized version of a paired-sample t-test is used.
4This is always done before parsing test data.
216
tt tpos tem1,?=0.5 tem1,?f
Recall 86.48 86.48 86.72 87.44
Precision 86.61 86.63 86.95 87.15
f-score 86.55 86.56 *86.83 *87.29
Table 1: Labeled bracketing F-score on section 23.
an interpolation factor as in Eq. 5 : this is the best
model with a statistically significant improvement
in f-score over tt, tpos and tem1,?=0.5.
Since we expect that smoothing will be advanta-
geous for unseen or low-frequency words, we per-
form an evaluation targeted at identifying struc-
tures subcategorized by unseen verbs. Table 2
shows the error reduction in identifying subcat.
frames in Viterbi parses, of unseen verbs and also
of all verbs (seen and unseen) in the testset. A
breakup of error by frame type for unseen verbs is
also shown (here, only frames with >10 token oc-
currences in the test data are shown). In all cases
(unseen verbs and all verbs) we see a substantial
error reduction. The error reduction improves with
larger amounts of unannotated training data.
8 Discussion and Conclusions
We have shown that lexicons re-estimated with I-
O can be used to smooth unlexicalised treebank
PCFGs, with a significant increase in f-score even
in the case of English with a large treebank re-
source. We expect this method to have more
impact for languages with a smaller treebank or
richer tag-set. An interesting aspect is the substan-
tial reduction in subcategorization error for un-
seen verbs for which no word-specific information
about subcategorization exists in the unsmoothed
or POS-tag-smoothed lexicon. The error reduction
in identifying subcat. frames implies that some
constituents (such as PPs) are not only attached
correctly but also identified correctly as arguments
(such as PP-CLR) rather than as adjuncts.
There have been previous attempts to use POS-
tagging technologies (such as HMM or maximum-
entropy based taggers) to enhance treebank-
trained grammars (Goldberg et al (2009) for He-
brew, (Clark and Curran, 2004) for CCG). The re-
estimation method we use builds full parse-trees,
rather than use local features like taggers do, and
hence might have a benefit over such methods. An
interesting option would be to train a ?supertag-
ger? on fine-grained tags from the PTB and to su-
pertag a large corpus to harvest lexical frequen-
Frame # tokens %Error %Error %Error
(test) tpos tem1 Reduc.
All unseen (4M words) 1258 33.47 22.81 31.84
All unseen (8M words) 1258 33.47 22.26 33.49
All unseen (12M words) 1258 33.47 21.86 34.68
transitive 662 23.87 18.73 21.52
intransitive 115 38.26 33.91 11.36
NP PP-CLR 121 34.71 32.23 7.14
PP-CLR 73 27.4 20.55 25
SBAR 124 12.1 12.1 0
S 12 83.33 58.33 30
NP NP 10 90 80 11.11
PRT NP 21 38.1 33.33 12.5
s.e.to (see Fig.1b) 50 16 12 25
NP PP-DIR 11 63.64 54.55 14.28
All verbs (4M) 11710 18.5 16.84 8.97
Table 2: Subcat. error for verbs in Viterbi parses.
cies. This would form another (possibly higher)
baseline for the I-O re-estimation approach pre-
sented here and is the focus of our future work.
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An Ap-
proach to Almost Parsing. Computational Linguistics,
25:237?265.
F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.
1999. Inside-outside estimation of a lexicalized PCFG for
German. In ACL 37.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcate-
gorization probabilities help parsing. In 6th ACL/SIGDAT
Workshop on Very Large Corpora.
S. Clark and J. R. Curran. 2004. The Importance of Supertag-
ging for Wide-Coverage CCG Parsing. In 22nd COLING.
Carl de Marcken. 1995. On the unsupervised induction of
Phrase Structure grammars. In Proceedings of the 3rd
Workshop on Very Large Corpora.
T. Deoskar. 2008. Re-estimation of Lexical Parameters for
Treebank PCFGs. In 22nd COLING.
Tejaswini Deoskar and Mats Rooth. 2008. Induction of
Treebank-Aligned Lexical Resources. In 6th LREC.
Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009.
Enhancing Unlexicalized Parsing Performance using a
Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and
EM-HMM-based Lexical Probabilities. In EACL-09.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL 41.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4:35?56.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In International Conference on New
Methods in Language Processing.
H. Schmid. 2004. Efficient Parsing of Highly Ambiguous
CFGs with Bit Vectors. In 20th COLING.
217
