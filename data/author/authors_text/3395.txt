Proceedings of the Linguistic Annotation Workshop, pages 93?100,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Expressions of Appraisal in English
Jonathon Read, David Hope and John Carroll
Department of Informatics
University of Sussex
United Kingdom
{j.l.read,drh21,j.a.carroll}@sussex.ac.uk
Abstract
The Appraisal framework is a theory of the
language of evaluation, developed within the
tradition of systemic functional linguistics.
The framework describes a taxonomy of the
types of language used to convey evaluation
and position oneself with respect to the eval-
uations of other people. Accurate automatic
recognition of these types of language can
inform an analysis of document sentiment.
This paper describes the preparation of test
data for algorithms for automatic Appraisal
analysis. The difficulty of the task is as-
sessed by way of an inter-annotator agree-
ment study, based on measures analogous to
those used in the MUC-7 evaluation.
1 Introduction
The Appraisal framework (Martin and White, 2005)
describes a taxonomy of the language employed in
communicating evaluation, explaining how users of
English convey attitude (emotion, judgement of peo-
ple and appreciation of objects), engagement (as-
sessment of the evaluations of other people) and
how writers may modify the strength of their atti-
tude/engagement. Accurate automatic analysis of
these aspects of language will augment existing re-
search in the fields of sentiment (Pang et al, 2002)
and subjectivity analysis (Wiebe et al, 2004), but as-
sessing the usefulness of analysis algorithms lever-
aging the Appraisal framework will require test data.
At present there are no machine-readable
Appraisal-annotated texts publicly available. Real-
world instances of Appraisal in use are limited
to example extracts that demonstrate the theory,
coming from a wide variety of genres as disparate
as news reporting (White, 2002; Martin, 2004) and
poetry (Martin and White, 2005). These examples,
while useful in demonstrating the various aspects
of Appraisal, can only be employed in a qualitative
analysis and would bring about inconsistencies
if analysed collectively ? one can expect the
writing style to depend upon the genre, resulting in
significantly different syntactic constructions and
lexical choices.
We therefore need to examine Appraisal across
documents in the same genre and investigate pat-
terns within that particular register. This paper dis-
cusses the methodology of an Appraisal annotation
study and an analysis of the inter-annotator agree-
ment exhibited by two human judges. The output
of this study has the additional benefit of bringing
a set of machine-readable annotations of Appraisal
into the public domain for further research.
This paper is structured as follows. The next sec-
tion offers an overview of the Appraisal framework.
Section 3 discusses the methodology adopted for
the annotation study. Section 4 discusses the mea-
sures employed to assess inter-annotator agreement
and reports the results of these measures. Section
5 offers an analysis of cases of systematic disagree-
ment. Other computational work utilising the Ap-
praisal framework is reviewed in Section 6. Section
7 summarises the paper and outlines future work.
2 The linguistic framework of Appraisal
The Appraisal framework (Martin and White, 2005)
is a development of work in Systemic Functional
93
appraisal
attitude
engagement
graduation
affect
judgement
appreciation
inclination
happiness
security
satisfaction
esteem
sanction
normality
capacity
tenacity
veracity
propriety
reaction
composition
valuation
impact
quality
balance
complexity
contract
expand
disclaim
proclaim
deny
counter
pronounce
endorse
concur
affirm
concede
entertain
attribute acknowledge
distance
force
focus
quantification
intensification
number
mass
extent
proximity (space)
proximity (time)
distribution (space)
distribution (time)
degree
vigour
Figure 1: The Appraisal framework.
Linguistics (Halliday, 1994) and is concerned with
interpersonal meaning in text?the negotiation of
social relationships by communicating emotion,
judgement and appreciation. The taxonomy de-
scribed by the Appraisal framework is depicted in
Figure 1.
Appraisal consists of three subsystems that oper-
ate in parallel: attitude looks at how one expresses
private state (Quirk et al, 1985) (one?s emotion and
opinions); engagement considers the positioning of
oneself with respect to the opinions of others and
graduation investigates how the use of language
functions to amplify or diminish the attitude and en-
gagement conveyed by a text.
2.1 Attitude: emotion, ethics and aesthetics
The Attitude sub-system describes three areas of pri-
vate state: emotion, ethics and aesthetics. An atti-
tude is further qualified by its polarity (positive or
negative). Affect identifies feelings?author?s emo-
tions as represented by their text. Judgement deals
with authors? attitude towards the behaviour of peo-
ple; how authors applaud or reproach the actions
of others. Appreciation considers the evaluation of
things?both man-made and natural phenomena.
2.2 Engagement: appraisals of appraisals
Through engagement, Martin and White (2005) deal
with the linguistic constructions by which authors
construe their point of view and the resources used
to adopt stances towards the opinions of other peo-
ple. The theory of engagement follows Stubbs
(1996) in that it assumes that all utterances convey
point of view and Bakhtin (1981) in supposing that
all utterances occur in a miscellany of other utter-
ances on the same motif, and that they carry both
implicit and explicit responses to one another. In
other words, all text is inherently dialogistic as it en-
codes authors? reactions to their experiences (includ-
ing previous interaction with other writers). Engage-
ment can be both retrospective (that is, an author will
acknowledge and agree or disagree with the stances
of others who have previously appraised a subject),
and prospective (one may anticipate the responses of
an intended audience and include counter-responses
in the original text).
2.3 Graduation: strength of evaluations
Martin and White (2005) consider the resources by
which writers alter the strength of their evaluation
as a system of graduation. Graduation is a general
property of both attitude and engagement. In atti-
tude it enables authors to convey greater or lesser
degrees of positivity or negativity, while graduation
of engagements scales authors? conviction in their
utterance.
Graduation is divided into two subsystems. Force
alters appraisal propositions in terms of its inten-
94
sity, quantity or temporality, or by means of spatial
metaphor. Focus considers the resolution of seman-
tic categories, for example:
They play real jazz.
They play jazz, sort of.
In real terms a musician either plays jazz or they
do not, but these examples demonstrate how authors
blur the lines of semantic sets and how binary rela-
tionships can be turned into scalar ones.
3 Annotation methodology
The corpus used in this study consists of unedited
book reviews. Book reviews are good candidates for
this study as, while they are likely to contain similar
language by virtue of being from the same genre of
writing, we can also expect examples of Appraisal?s
many classes (for example, the emotion attributed
to the characters in reviews of novels, judgements
of authors? competence and character, appreciation
of the qualities of books and engagement with the
propositions put forth by the authors under review).
The articles were taken from the web sites of
four British newspapers (The Guardian, The Inde-
pendent, The Telegraph and The Times) on two dif-
ferent dates?31 July 2006 and 11 September 2006.
Each review is attributed to a unique author. The
corpus is comprised of 38 documents, containing a
total of 36,997 tokens in 1,245 sentences.
Two human annotators, d and j, participated in
this study, assigning tags independently. The anno-
tators were well-versed in the Appraisal framework,
having studied the latest literature. The judges were
asked to annotate appraisal-bearing terms with the
appraisal type presumed to be intended by the au-
thor of the text. They were asked to highlight each
example of appraisal and specify the type of atti-
tude, engagement or graduation present. They also
assigned a polarity (positive or negative) to attitudi-
nal items and a scaling (up or down) to graduating
items, employing a custom-developed software tool
to annotate the documents.
Four alternative annotation strategies were con-
sidered. One approach is to allow only a single token
per annotation. However, this is too simplistic for
an Appraisal annotation study?a unit of Appraisal
is frequently larger than a single token. Consider the
following examples:
(1)
The design was deceptively?VERACITY simple?
COMPLEXITY. (?)
(2)
The design was deceptively simple?COMPLEXITY.
Example 1 demonstrates that a single-token ap-
proach is inappropriate as it ascribes a judgement
of someone?s honesty, whereas Example 2 indicates
the correct analysis?the sentence is an apprecia-
tion of the simplicity of the ?design?. This example
shows how it is necessary to annotate larger units of
appraisal-bearing language.
Including more tokens, however, increases the
complexity of the annotation task, and reduces the
likelihood of agreement between the judges, as the
annotated tokens of one judge may be a subset of,
or overlap with, those of another. We therefore ex-
perimented with tagging entire sentences in order to
constrain the annotators? range of choices. This re-
sulted in its own problems as there is often more than
one appraisal in a sentence, for example:
(3)
The design was deceptively simple?COMPLEXITY
and belied his ingenuity?CAPACITY.
An alternative approach is to permit annotators
to tag an arbitrary number of contiguous tokens.
Arbitrary-length tagging is disadvantageous as the
judges will frequently tag units of differing length,
but this can be compensated for by relaxing the rules
for agreement?for example, by allowing intersect-
ing annotations to match successfully (Wiebe et al,
2005). Bruce and Wiebe (1999) employ another
approach, creating units from every non-compound
sentence and each conjunct of every compound sen-
tence. This side-steps the problem of ambiguity in
appraisal unit length, but will still fail to capture both
appraisals demonstrated in the second conjunct of
Example 4.
(4)
The design was deceptively simple?COMPLEXITY
and belied his remarkable?NORMALITY
ingenuity?CAPACITY.
Ultimately in this study, we permitted judges to
annotate any number of tokens in order to allow
for multiple Appraisal units of differing sizes within
sentences. Annotation was carried out over two
rounds, punctuated by an intermediary analysis of
95
d j d j d j
Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59
Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63
Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63
Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14
Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55
Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39
Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56
Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72
Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45
Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29
Quality 2.55 3.40 Acknowledge 2.42 3.33
Table 1: The distribution of the Appraisal types selected by each annotator (%).
d j
Documents 115.74 77.21
Sentences 3.65 2.43
Words 0.12 0.08
Table 2: The density of annotations relative to the
number of documents, sentences and words.
agreement and disagreement between the two anno-
tators. The judges discussed examples of the most
common types of disagreement in an attempt to ac-
quire a common understanding for the second round,
but annotations from the first round were left unal-
tered.
Following the methodology described above, d
made 3,176 annotations whilst j made 2,886 anno-
tations. The distribution of the Appraisal types as-
cribed is shown in Table 1, while Table 2 details the
density of annotations in documents, sentences and
words.
4 Measuring inter-annotator agreement
The study of inter-annotator agreement begins by
considering the level of agreement exhibited by the
annotators in deciding which tokens are representa-
tive of Appraisal, irrespective of the type. As dis-
cussed, this is problematic as judges are liable to
choose different length token spans when marking
up what is essentially the same appraisal, as demon-
strated by Example 5.
(5)
[d] It is tempting to point to the bombs in Lon-
don and elsewhere, to the hideous mess?QUALITY
in Iraq, to recent victories of the Islamists, to
the violent and polarised rhetoric?PROPRIETY and
answer yes.
[j] It is tempting to point to the bombs in
London and elsewhere, to the hideous?QUALITY
mess?BALANCE in Iraq, to recent victories of Is-
lamists, to the violent?PROPRIETY and polarised?
PROPRIETY rhetoric and answer yes.
Wiebe et al (2005), who faced this problem when
annotating expressions of opinion under their own
framework, accept that it is necessary to consider the
validity of all judges? interpretations and therefore
consider intersecting annotations (such as ?hideous?
and ?hideous mess?) to be matches. The same relax-
ation of constraints is employed in this study.
Tasks with a known number of annotative units
can be analysed with measures of agreement such as
Cohen?s ? Coefficient (1960), but the judges? free-
dom in this task prohibits meaningful application of
this measure. For example, consider howword sense
annotators are obliged to choose from a limited fixed
set of senses for each token, whereas judges anno-
tating Appraisal are free to select one of thirty-two
classes for any contiguous substring of any length
within each document; there are 16
(
n2 ? n
)
pos-
sible choices in a document of n tokens (approxi-
mately 6.5 ? 108 possibilities in this corpus).
A wide range of evaluation metrics have been em-
ployed by the Message Understanding Conferences
(MUCs). The MUC-7 tasks included extraction of
named entities, equivalence classes, attributes, facts
and events (Chinchor, 1998). The participating sys-
tems were evaluated using a variety of related mea-
sures, defined in Table 3. These tasks are similar to
Appraisal annotation in that the units are formed of
an arbitrary number of contiguous tokens.
In this study the agreement exhibited by an an-
notator a is evaluated as a pair-wise comparison
against the other annotator b. Annotator b provides
96
COR Number correct
INC Number incorrect
MIS Number missing
SPU Number spurious
POS Number possible = COR + INC + MIS
ACT Number actual = COR + INC + SPU
FSC F-score = (2 ? REC ? PRE)
/ (REC + PRE)
REC Precision = COR/POS
PRE Recall = COR/ACT
SUB Substitution = INC/ (COR + INC)
ERR Error per response = (INC + SPU + MIS)
/ (COR + INC + SPU + MIS)
UND Under-generation = MIS/POS
OVG Over-generation = SPU/ACT
Table 3: MUC-7 score definitions (Chinchor 1998).
FSC REC PRE ERR UND OVG
d 0.682 0.706 0.660 0.482 0.294 0.340
j 0.715 0.667 0.770 0.444 0.333 0.230
x? 0.698 0.686 0.711 0.462 0.312 0.274
Table 4: MUC-7 test scores, evaluating the agree-
ment in text anchors selected by the annotators. x?
denotes the average value, calculated using the har-
monic mean.
a presumed gold standard for the purposes of evalu-
ating agreement. Note, however, that in this case it
does not necessarily follow that REC (a w.r.t. b) =
PRE (b w.r.t. a). Consider that a may tend to make
one-word annotations whilst b prefers to annotate
phrases; the set of a?s annotations will contain mul-
tiple matches for some of the phrases annotated by b
(refer to Example 5, for instance). The ?number cor-
rect? will differ for each annotator in the pair under
evaluation.
Table 4 lists the values for the MUC-7 measures
applied to the text spans selected by the annota-
tors. Annotator d is inclined to identify text as Ap-
praisal more frequently than annotator j. This re-
sults in higher recall for d, but with lower preci-
sion. Naturally, the opposite observation can be
made about annotator j. Both annotators exhibit a
high error rate at 48.2% and 44.4% for d and j re-
spectively. The substitution rate is not listed as there
are no classes to substitute when considering only
text anchor agreement. The second round of anno-
tation achieved slightly higher agreement (the mean
F-score increased by 0.033).
FSC REC PRE SUB ERR
0 0.698 0.686 0.711 0.000 0.462
1 0.635 0.624 0.647 0.090 0.511
2 0.528 0.518 0.538 0.244 0.594
3 0.448 0.441 0.457 0.357 0.655
4 0.396 0.388 0.403 0.433 0.696
5 0.395 0.388 0.403 0.433 0.696
Table 5: Harmonic means of the MUC-7 test scores
evaluating the agreement in text anchors and Ap-
praisal classes selected by the annotators, at each
level of hierarchical abstraction.
Having considered the annotators? agreement
with respect to text anchors, we go on to analyse
the agreement exhibited by the annotators with re-
spect to the types of Appraisal assigned to the text
anchors. The Appraisal framework is a hierarchi-
cal system?a tree with leaves corresponding to the
annotation types chosen by the judges. When in-
vestigating agreement in Appraisal type, the follow-
ing measures include not just the leaf nodes but also
their parent types, collapsing the nodes into increas-
ingly abstract representations. For example happi-
ness is a kind of affect, which is a kind of attitude,
which is a kind of appraisal. These relationships are
depicted in full in Figure 2. Note that in the follow-
ing measurements of inter-annotator agreement leaf
nodes are included in subsequent levels (for exam-
ple, focus is a leaf node at level 2, but is also consid-
ered to be a member of levels 3, 4 and 5).
Table 5 shows the harmonic means of the MUC-
7 measures of the annotators? agreement at each of
the levels depicted in Figure 2. As one might ex-
pect, the agreement steadily drops as the classes be-
come more concrete?classes become more specific
and more numerous so the complexity of the task
increases.
Table 5 also lists the average rate of substitutions
as the annotation task?s complexity increases, show-
ing that the annotators were able to fairly easily
distinguish between instances of the three subsys-
tems of Appraisal (Attitude, Engagement and Grad-
uation) as the substitution rate at level 1 is low (only
9%). As the number of possible classes increases an-
notators are more likely to confuse appraisal types,
with disagreement occurring on approximately 44%
of annotations at level 5. The second round of an-
notations resulted in slightly improved agreement at
97
Level 0: .698
Level 1: .635
Level 2: .528
Level 3: .448
Level 4: .396
Level 5: .395
appraisal
attitude: .701
engagement: .507
graduation: .479
affect: .519
judgement: .586
appreciation: .567
contract: .502
expand: .445
force: .420
focus: .287
inclination: .249
happiness: .448
security: .335
satisfaction: .374
esteem: .489
sanction: .575
reaction: .510
composition: .432
valuation: .299
disclaim: .555
proclaim: .336
entertain: .459
attribute: .427
quantification: .233
intensification: .513
normality: .289
capacity: .431
tenacity: .395
veracity: .519
propriety: .540
impact: .462
quality: .336
balance: .300
complexity: .314
deny: .451
counter: .603
pronounce: .195
endorse: .331
concur: .297
acknowledge: .390
distance: .415
number: .191
mass: .104
extent: .242
degree: .510
vigour: .117
affirm: .325
concede: .000
proximity (space): .000
proximity (time): .000
distribution (space): .110
distribution (time): .352
Figure 2: The Appraisal framework with hierarchical levels highlighted. Appraisal classes and levels are
accompanied by the harmonic mean of the F-scores of the annotators for that class/level.
each level of abstraction (the mean F-score increased
by 0.051 at the most abstract level).
Of course, some Appraisal classes are easier to
identify than others. Figure 2 summarises the agree-
ment for each node in the Appraisal hierarchy with
the harmonic mean of the F-scores of the annotators
for each class. Typically, the attitude annotations are
easiest to identify, whereas the other subsystems of
engagement and graduation tend to be more difficult.
The Proximity children of Extent exhibited no
agreement whatsoever. This seems to have arisen
from the differences in the judges? interpretations of
proximity. In the case of Proximity (Space), for ex-
ample, one judge annotated words that function to
modify the spatial distance of other concepts (e.g.
near), whereas the other selected words placing con-
cepts at a specific location (e.g. homegrown, local).
This confusion between modifying words and spe-
98
cific locations also accounts for the low agreement
in the Distribution (Space) type.
The measures show that it is also difficult to
achieve a consensus on what qualifies as engage-
ments of the Pronounce type. Both annotators select
expressions that assert the irrefutability of a propo-
sition (e.g. certainly or in fact or it has to be said).
Judge d, however, tends to perceive pronouncement
as occurring wherever the author makes an assertion
(e.g. this is or there will be). Judge j seems to re-
quire that the assertion carry a degree of emphasis to
include a term in the Pronounce class.
The low agreement of the Mass graduations can
also be explained in this way, as both d and j se-
lect strong expressions relating to size (e.g. massive
or scant). Annotator j found additional but weaker
terms like largely or slightly.
The Pronounce and Mass classes provide typical
examples of the disagreement exhibited by the an-
notators. It is not that the judges have wildly differ-
ent understandings of the system, but rather they dis-
agree in the bounds of a class?one annotator may
require a greater degree of strength of a term to war-
rant its inclusion in a class.
Contingency tables (not depicted due to space
constraints) reveal some interesting tendencies for
confusion between the two annotators. Approxi-
mately 33% of d?s annotations of Proximity (Space)
were ascribed as Capacity by j. The high percent-
age is due to the rarity of annotations of Proxim-
ity (Space), but the confusion comes from differing
units of Appraisal, as shown in Example 6.
(6)
[d] But at key points in this story, one gets
the feeling that the essential factors are op-
erating just outside?PROXIMITY (SPACE)
James?s field of vision?CAPACITY.
[j] But at key points in this story, one gets the
feeling that the essential factors are operating just
outside James?s field of vision?CAPACITY.
Another interesting case of frequent confusion is
the pair of Satisfaction and Propriety. Though not
closely related in the Attitude subsystem, j chooses
Propriety for 21% of d?s annotations of Satisfaction.
The confusion is typified by Example 7, where it is
apparent that there is disagreement in terms of who
is being appraised.
(7)
[d] Like him, Vermeer ? or so he chose to be-
lieve ? was an artist neglected?SATISFACTION and
wronged?SATISFACTION by critics and who had
died an almost unknown.
[j] Like him, Vermeer ? or so he chose to believe
? was an artist neglected and wronged?PROPRIETY
by critics and who had died an almost unknown.
Annotator d believes that the author is communi-
cating the artist?s dissatisfaction with the way he is
treated by critics, whereas j believes that the critics
are being reproached for their treatment of the artist.
This highlights a problem with the coding scheme,
which simplifies the task by assuming only one type
of Appraisal is conveyed by each unit.
5 Related work
Taboada and Grieve (2004) initiated computational
experimentation with the Appraisal framework, as-
signing adjectives into one of the three broad atti-
tude classes. The authors apply SO-PMI-IR (Turney,
2002) to extract and determine the polarity of adjec-
tives. They then use a variant of SO-PMI-IR to de-
termine a ?potential? value for affect, judgement and
appreciation, calculating the mutual information be-
tween the adjective and three pronoun-copular pairs:
I was (affect); he was (judgement) and it was (ap-
preciation). While the pairs seem compelling mark-
ers of the respective attitude types, they incorrectly
assume that appraisals of affect are limited to the
first person whilst judgements are made only of the
third person. We can expect a high degree of overlap
between the sets of documents retrieved by queries
formed using these pairs (e.g. I was a happy ?X?;
he was a happy ?X?; It was a happy ?X?).
Whitelaw et al (2005) use the Appraisal frame-
work to specify frames of sentiment. These ?Ap-
praisal Groups? are derived from aspects of Attitude
and Graduation:
Attitude: affect | judgement | appreciation
Orientation positive | negative
Force: low | neutral | high
Focus: low | neutral | high
Polarity: marked | unmarked
Their process begins with a semi-automatically con-
structed lexicon of these Appraisal groups, built us-
ing example terms from Martin and White (2005) as
seeds into WordNet synsets. The frames supplement
bag of words-based machine learning techniques for
99
sentiment analysis and they achieve minor improve-
ments over unigram features.
6 Summary
This paper has discussed the methodology of an ex-
ercise annotating book reviews according to the Ap-
praisal framework, a functional linguistic theory of
evaluation in English. The agreement exhibited by
two human judges was measured by analogy with
the evaluation employed for the MUC-7 shared tasks
(Chinchor, 1998).
The agreement varied greatly depending on the
level of abstraction in the Appraisal hierarchy
(a mean F-score of 0.698 at the most abstract
level through to 0.395 at the most concrete level).
The agreement also depended on the type being
annotated?there was more agreement evident for
types of attitude compared to types of engagement
or graduation.
The exercise is the first step in an ongoing study
of approaches for the automatic analysis of expres-
sions of Appraisal. The primary output of this work
is a corpus of book reviews independently annotated
with Appraisal types by two coders. Agreement was
in general low, but if one assumes that the intersec-
tion of both sets of annotations contains reliable ex-
amples, this leaves 2,223 usable annotations.
Future work will employ these annotations to
evaluate algorithms for the analysis of Appraisal,
and investigate the usefulness of the Appraisal
framework when in the computational analysis of
document sentiment and subjectivity.
Acknowledgments
We would like to thank Bill Keller for advice when
designing the annotation methodology. The work of
the first author is supported by a UK EPSRC stu-
dentship.
References
M. M. Bakhtin. 1981. The Dialogic Imagination. Uni-
versity of Texas Press, Austin. Translated by C. Emer-
son & M. Holquist.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: a case study in manual tagging. Natural
Language Engineering, 5(1):1?16.
N. Chinchor. 1998. MUC-7 test scores introduction.
In Proceedings of the Seventh Message Understanding
Conference.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measures,
20:37?46.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
J. R. Martin. 2004. Mourning: how we get algned. Dis-
course & Society, 15(2-3):321?344.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman.
M. Stubbs. 1996. Towards a modal grammar of English:
a matter of prolonged fieldwork. In Text and Corpus
Analysis. Blackwell, Oxford.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal automatically. In Spring Symposium on Ex-
ploring Attitude and Affect in Text. American Associa-
tion for Artificial Intelligence, Stanford. AAAI Tech-
nical Report SS-04-07.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, USA.
P. R. R.White. 2002. Appraisal? the language of evalu-
ation and stance. In Jef Verschueren, Jan-Ola O?stman,
Jan Blommaert, and Chris Bulcaen, editors, Handbook
of Pragmatics, pages 1?27. John Benjamins, Amster-
dam.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international confer-
ence on Information and knowledge management.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational linguistics, 30(3):277?308.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
100
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 689?694, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UoS: A Graph-Based System for Graded Word Sense Induction
David Hope, Bill Keller
University of Sussex
Cognitive and Language Processing Systems Group
Brighton, Sussex, UK
davehope@gmail.com, billk@sussex.ac.uk
Abstract
This paper presents UoS, a graph-based Word
Sense Induction system which attempts to
find all applicable senses of a target word
given its context, grading each sense accord-
ing to its suitability to the context. Senses
of a target word are induced through use of
a non-parameterised, linear-time clustering al-
gorithm that returns maximal quasi-strongly
connected components of a target word graph
in which vertex pairs are assigned to the same
cluster if either vertex has the highest edge
weight to the other. UoS participated in
SemEval-2013 Task 13: Word Sense Induc-
tion for Graded and Non-Graded Senses. Two
system were submitted; both systems returned
results comparable with those of the best per-
forming systems.
1 Introduction
Word Sense Induction (WSI) is the task of automat-
ically discovering word senses from text. In princi-
ple, WSI avoids reliance on a pre-defined sense in-
ventory.1 Whereas the related task of Word Sense
Disambiguation (WSD) can only assign pre-defined
senses to words on the basis of context, WSI fol-
lows the dictum that ?The meaning of a word is its
use in the language.? (Wittgenstein, 1953) to dis-
cover senses through examination of context of use
in large text corpora. WSI, therefore, may be applied
1In practice, evaluation of a WSI system requires the use of
a gold standard sense inventory such as WordNet (Miller et al,
1990) or OntoNotes (Hovy et al, 2006).
to discover new, rare, or domain specific senses;
senses undefined in existing sense inventories.2
Previous WSI evaluations (Agirre and Soroa,
2007; Manandhar et al, 2010) have approached
sense induction in terms of finding the single most
salient sense of a target word given its context.
However, as shown in Erk and McCarthy (2009), a
graded notion of sense may be more applicable, as
multiple senses of the target word may be perceived
by readers. The SemEval-2013 WSI evaluation de-
scribed in this paper is designed to explore the possi-
bility of finding all perceived senses of a target word
in a single contextual instance. The aim for partici-
pants in the task is therefore to design a system that
will induce a set of graded (weighted) senses of a
target word in a particular context.
The paper is organised as follows: Section 2 in-
troduces SemEval-2013 Task 13: Word Sense In-
duction for Graded and Non-Graded Senses; Sec-
tion 3 presents UoS, the system that participated in
the task; Section 4 reports evaluation results, show-
ing that UoS returns scores comparable with those
of the best performing systems.
2 SemEval-2013 Task 13
2.1 Aim
The aim for participants in SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded
Senses is to construct a system that will: (1) induce
the senses of a given set of target words and (2), label
each test set context (instance) of a target word with
2Surveys of WSI and WSD approaches are found in Navigli
(2009) and Navigli (2012).
689
all applicable target word senses. Candidate senses
are drawn from the WordNet 3.1 sense inventory.
Systems must therefore return a set of graded senses
for each target word in a particular context, where a
numeric weight signifies (grades) each sense?s appli-
cability to the context. A non-graded sense is simply
the highest graded (weighted) sense out of all graded
senses.
2.2 Test Set
The test set consists of 4806 instances of 50 target
words: 20 verbs (1901 instances), 20 nouns (1908),
and 10 adjectives (997).3 Instances are extracted
from the Open American National Corpus, being a
mix of both written and spoken contexts of target
words.4 Only 542 instances are assigned more than
one sense by annotators, thus have graded senses.
This figure somewhat detracts from the task?s aim as
just 11.62% of the test set can be assigned graded
senses.
2.3 Evaluation Measures
Systems are evaluated in two ways: (1) in a WSD
task and (2), a clustering task. In the first evalu-
ation, systems are assessed by their ability to cor-
rectly identify which WordNet 3.1 senses of the tar-
get word are applicable in a given instance, and to
quantify, and so, rank, senses according to their level
of applicability. The supervised evaluation method
of previous SemEval WSI tasks (Agirre and Soroa,
2007; Manandhar et al, 2010) is applied to map in-
duced senses to WordNet 3.1 senses, with the map-
ping function of Jurgens (2012) used to account for
the applicability weights. Three evaluation metrics
are used -
? Jaccard Index: measures the overlap between
gold standard senses and those returned by a
WSI system.
? Positionally-Weighted Kendall?s Tau: measures
the ability of a system to rank senses by their
applicability.
3Stated as 4664 instances on the task website. Note that the
figure of 4806 is for the revised test set.
4http://www.americannationalcorpus.org/
OANC/index.html.
? Weighted Normalized Discounted Cumulative
Gain (NDCG): measures the agreement in ap-
plicability ratings, accounting for both the
ranking and difference in weights assigned to
senses.
In the second evaluation, similarity between a partic-
ipant?s clustering solution and that of the gold stan-
dard set of senses is measured using two metrics -
? Fuzzy Normalised Mutual Information (NMI):
extends the method of Lancichinetti et al
(2009) to compute NMI between overlapping
(fuzzy) clusters. Fuzzy NMI measures the
alignment of system and gold standard senses
independently of the cluster sizes, so returns a
measure of how well a WSI system would per-
form regardless of the sense distribution in a
corpus.
? Fuzzy B-Cubed: adapts the overlapping B-
Cubed measure defined in Amigo? et al (2009)
to the fuzzy clustering setting. As an item-
based, rather than cluster-based, measure,
Fuzzy B-Cubed is sensitive to cluster size skew,
thus captures the expected performance of a
WSI system on a new corpus where the sense
distribution is the same.
3 The UoS System
The UoS system uses a graph-based model of word
co-occurrence to induce target word senses as fol-
lows:
3.1 Constructing a Target Word Graph
A graph G = (V,E) is constructed for each tar-
get word. V is a set of vertices and E ? V ? V
a set of edges. Each vertex v ? V represents a
word found in a dependency relation with the tar-
get word. Words are extracted from the dependency-
parsed version of ukWaC (Ferraresi et al, 2008). In
this evaluation V consists of the 300 highest ranked
dependency relation words.5 Words are ranked us-
ing the Normalised Pointwise Mutual Information
5|V | = 300 was found to return the best results on the trial
set over the range |V | = [100, 200, 300, ..., 1000].
690
(NPMI) measure (Bouma, 2009)6, defined for two
words w1, w2 as:
NPMI(w1, w2) =
(
log p(w1,w2)p(w1) p(w2)
)
?log p(w1, w2)
. (1)
An edge (vi, vj) ? E is a pair of vertices. An edge
represents a symmetrical relationship between ver-
tices vi and vj ; here, that words wi and wj co-occur
in ukWaC contexts. Each edge (vi, vj) is assigned
a weight w(vi, vj) to quantify the significance of
wi, wj co-occurrence, the weight being the value re-
turned by NPMI(wi, wj).
3.2 Clustering the Target Word Graph
A clustering algorithm is applied to the target word
graph, partitioning it to a set of clusters. Each
set of words in a cluster is taken to represent a
sense of the target word. The clustering algorithm
applied is MaxMax, a non-parameterised, linear-
time algorithm shown to return good results in pre-
vious WSI evaluations (Hope and Keller, 2013).
MaxMax transforms the weighted, undirected target
word graph G into an unweighted, directed graph
G?, where edge direction in G? indicates a maximal
affinity relationship between two vertices. A ver-
tex vi is said to have maximal affinity to a vertex
vj if the edge weight w(vi, vj) is maximal amongst
the weights of all edges incident on vi. Clusters are
identified by finding root vertices of quasi-strongly
connected (QSC) subgraphs in G? (Thulasiraman
and Swamy, 1992). A directed subgraph is said to
be QSC if, for any vertices vi and vj , there is a root
vertex vk (not necessarily distinct from vi and vj)
with a directed path from vk to vi and a directed path
from vk to vj .7
3.3 Merging Clusters
MaxMax tends to generate many fine-grained sense
clusters. Clusters are therefore merged using two
measures: cohesion and separation (Tan et al,
6Application of the Log Likelihood Ratio measure (Dun-
ning, 1993) returned the same set of words. Though not re-
quired here, NPMI has the useful properties that: if w1 and w2
always co-occur NPMI = 1; if w1 and w2 are distributed as ex-
pected under independence NPMI = 0, and if w1 and w2 never
occur together, NPMI = ?1.
7MaxMax is described in detail in Hope and Keller (2013).
2006). The cohesion of a cluster Ci is defined as:
cohesion(Ci) =
?
x?Ci,
y?Ci
w(x, y)
|Ci|
. (2)
Separation between two clusters Ci, Cj is defined
as:
separation(Ci, Cj) = 1?
?
?
?
?
x?Ci,
y?Cj
w(x, y)
|Ci| ? |Cj |
?
?
? .
(3)
Cluster pairs with high cohesion and low separation
are merged, the intuition being that words in such
pairs will retain a relatively high degree of semantic
similarity. High cohesion is defined as greater than
average cohesion. Low separation is defined as a re-
ciprocal relationship between two clusters: if a clus-
ter Ci has the lowest separation to a cluster Cj (out
of all clusters) and Cj the lowest separation to Ci,
then the two (high cohesion) clusters are merged.8
3.4 Assigning Graded Word Senses to Target
Words
Each test instance is labelled with graded senses of
the target word. A score is computed for the test in-
stance and each target word cluster as the reciprocal
of the separation measure, where Ci is the set of con-
tent words in the instance (nouns, verbs, adjectives,
and adverbs, minus the target word itself) and Cj ,
the words in the cluster. The cluster with the lowest
separation score is taken to be the most salient sense
of the target word, with all other positive separation
scores taken to be perceived, graded senses of the
target word in that particular instance.
4 Evaluation Results
Two sets of results were submitted. The first, UoS
(top 3), returns the three highest scoring senses for
each instance; the second, UoS (# WN senses), re-
turns the n = number of target word senses in Word-
Net 3.1 most cohesive clusters, as defined by Equa-
tion (2).
Results for the seven participating WSI systems
are reported in Tables 1 and 2. The ten baselines,
provided by the organisers of the task, are -
8The average number of WordNet 3.1 senses for target
words is 8.58. MaxMax returns an average of 59.54 clusters for
target words; merging results in an average of 21.86 clusters.
691
System/Baseline Jaccard Index
F-Score
Positionally Weighted Tau
F-Score
Weighted NDCG
F-Score
UoS (top 3) 0.232 0.625 0.374
AI-KU (r5-a1000) 0.244 0.642 0.332
AI-KU 0.197 0.620 0.387
Unimelb (50k) 0.213 0.620 0.371
Unimelb (5p) 0.218 0.614 0.365
UoS (# WN senses) 0.192 0.596 0.315
AI-KU (a1000) 0.197 0.606 0.215
Most Frequent Sense 0.552 0.560 0.718
Senses Eq. Weighted 0.149 0.787 0.436
Senses, Avg. Weight 0.187 0.613 0.499
One sense 0.192 0.609 0.288
1 of 2 random senses 0.220 0.627 0.287
1 of 3 random senses 0.244 0.633 0.287
1 of n random senses 0.290 0.638 0.286
1 sense per instance 0.000 0.945 0.000
SemCor, MFS 0.455 0.465 0.339
SemCor, All Senses 0.149 0.559 0.489
Table 1: Results for the WSD evaluation: all instances.
? SemCor, Most Frequent Sense (MFS): labels
each instance with the MFS in SemCor.9
? SemCor, All Senses: labels each instance with
all SemCor senses, weighting each according
to its frequency in SemCor.
? 1 sense per instance: labels each instance with
a unique induced sense, equivalent to the 1
cluster per instance baseline of the SemEval-
2010 WSI task (Manandhar et al, 2010).
? One sense: labels each instance with the same
induced sense, equivalent to the MFS baseline
of the SemEval-2010 WSI task.
? Most Frequent Sense: labels each instance with
the sense that is most frequently selected by an-
notators for all target word instances.
? Senses Avg.Weighted: labels each instance with
all senses. Each sense is scored according to its
average applicability rating from the gold stan-
dard labelling.
? Senses Eq. Weighted: labels each instance with
all senses, equally weighted.
9http://www.cse.unt.edu/?rada/downloads.
html#semcor.
? 1 of 2 random senses: labels each instance with
one of two randomly selected induced senses.
? 1 of 3 random senses: labels each instance with
one of three randomly selected induced senses.
? 1 of n random senses: labels each instance with
one of n randomly selected induced senses,
where n is the number of senses for the target
word in WordNet 3.1.10
As noted by the task?s organisers11, the SemCor
scores are the fairest baselines for participating sys-
tems to compare against as they have no knowledge
of the test set sense distribution; the other baselines
are more challenging as they have knowledge of the
test set sense distribution and annotator grading.
4.1 Summary Analysis of Evaluation Results
Given the number of evaluation metrics (16 in total
on the task website), individual analysis of system
results per metric is beyond the scope of this paper.
However, a ranking of systems may be obtained by
taking a summed ranked score; that is, by adding
10For the random senses baselines, induced senses are
mapped to WordNet 3.1 senses using the mapping procedure
described in Agirre and Soroa (2007). The mapping is provided
by the task organisers.
11http://www.cs.york.ac.uk/semeval-2013/
task13/index.php?id=results
692
System/Baseline Fuzzy NMI Fuzzy B-Cubed
Precision
Fuzzy B-Cubed
Recall
Fuzzy B-Cubed
F-Score
Unimelb (50k) 0.060 0.524 0.447 0.483
Unimelb (5p) 0.056 0.470 0.449 0.459
AI-KU 0.065 0.838 0.254 0.390
AI-KU (r5-a1000) 0.039 0.502 0.409 0.451
UoS (top 3) 0.045 0.479 0.420 0.448
UoS (# WN senses) 0.047 0.988 0.112 0.201
AI-KU (a1000) 0.035 0.905 0.194 0.320
One sense 0.000 0.989 0.455 0.623
1 of 2 random senses 0.028 0.495 0.456 0.474
1 of 3 random senses 0.018 0.329 0.455 0.382
1 of n random senses 0.016 0.168 0.451 0.245
1 sense per instance 0.071 0.000 0.000 0.000
Table 2: Results for the cluster-based evaluation: all instances.
up each system?s rankings over all evaluation met-
rics. The summed ranking finds that UoS (top 3)
is placed first. If the WSD and cluster-based eval-
uations are considered separately, then UoS (top 3)
is ranked, respectively, first and fourth. However,
this result is countered by the relatively poor per-
formance of UoS (# WN senses), being ranked fifth
overall. Considering baselines, UoS (top 3) equals
or surpasses the SemCor baseline scores 67% of the
time, and 54% for the more challenging baselines;
UoS (# WN senses) scores, respectively, 50% and
44%.
All instances results were supplemented with
single-sense (non-graded) and multi-sense (graded)
splits at a later date.12 These results show (again,
using a ranked score) that for single-sense instances,
AI-KU is the best performing system, with UoS (top
3) placed fifth, and UoS (# WN senses) last. Both
UoS (top 3) and UoS (# WN senses) surpass the
SemCor MFS baseline, with UoS (top 3) surpassing
or equalling the harder baselines 79% of the time,
and UoS (# WN senses) 68% of the time. For multi-
sense instances, AI-KU is, again, the best perform-
ing system, with UoS (# WN senses) placed sec-
ond and UoS (top 3) sixth. UoS (top 3) surpasses
or equals the SemCor baseline scores 67% of the
time; UoS (# WN senses) 83% of the time. UoS
(top3) passes/equals, the harder baselines 63% of the
time, with UoS (# WN senses) doing so 67% of the
time. These results are somewhat confounding as
12http://www.cs.york.ac.uk/semeval-2013/
task13/index.php?id=results (4/4/2013)
one would expect a system that performs well in the
main set of results (all instances), as UoS (top 3)
does, to do so in at least one of the single-sense /
multi-sense splits: this is clearly not the case. In-
deed, the results suggest that UoS (# WN senses),
found to perform poorly over all instances, is better
suited to the task?s aim of finding graded senses.
5 Conclusion
This paper presented UoS, a graph-based WSI sys-
tem that participated in SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded
Senses. UoS applied the MaxMax clustering algo-
rithm to find a set of sense clusters in a target word
graph. The number of clusters was found automati-
cally through identification of root vertices of max-
imal quasi-strongly connected subgraphs. Evalua-
tion results showed the UoS (top 3) system to be
the best performing system (all instances), if a sim-
ple ranking over all evaluation measures is applied.
The second system, UoS (# WN senses), performed
poorly, being ranked fifth out of the seven participat-
ing WSI systems. Note, however, that the number of
evaluation metrics applied, and the wide variability
in each system?s performances over different met-
rics and different splits of instance types, make it
difficult to judge exactly which system is the best
performing. Future research therefore aims to carry
out a detailed analysis of the results and to assess
whether the measures applied in the evaluation ade-
quately reflect the performance of WSI systems.
693
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-
2007 Task 02: Evaluating Word Sense Induction
and Discrimination Systems. In Proceedings of the
4th International Workshop on Semantic Evaluations,
pages 7?12. Association for Computational Linguis-
tics. Prague, Czech Republic.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A Comparison of Extrinsic Cluster-
ing Evaluation Metrics Based on Formal Constraints.
Information Retrieval, 12(4):461?486.
Gerlof Bouma. 2009. Normalized (Pointwise) Mutual
Information in Collocation Extraction. Proceedings of
GSCL, pages 31?40.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
19(1):61?74.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1, pages 440?449, Singapore. As-
sociation for Computational Linguistics.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54. Mar-
rakech, Morocco.
David Hope and Bill Keller. 2013. MaxMax: A Graph-
Based Soft Clustering Algorithm Applied to Word
Sense Induction. In A. Gelbukh, editor, CICLing
2013, Part I, LNCS 7816, pages 368?381. Springer-
Verlag Berlin Heidelberg. to appear.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60. Association for Computational Linguistics.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation Using Word Sense Induction. Pro-
ceedings of *SEM First Joint Conference on Lexi-
cal and Computational Semantics, 2012. Association
for Computational Linguistics, pages 189?198. Mon-
treal,Canada.
Andrea Lancichinetti, Santo Fortunato, and Ja?nos
Kerte?sz. 2009. Detecting the Overlapping and Hier-
archical Community Structure in Complex Networks.
New Journal of Physics, 11(3):033015.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
Task 14: Word Sense Induction and Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68. Association for
Computational Linguistics. Uppsala, Sweden.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys (CSUR), 41(2):10.
Roberto Navigli. 2012. A Quick Tour of Word Sense
Disambiguation, Induction and Related Approaches.
In SOFSEM 2012: Theory and Practice of Computer
Science, volume 7147 of Lecture Notes in Computer
Science, pages 115?129. Springer Berlin / Heidelberg.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2006. Introduction to Data Mining. Pearson Addison
Wesley.
K. Thulasiraman and N.S. Swamy. 1992. Graphs: The-
ory and Algorithms. Wiley.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwell.
694
