Proceedings of the SIGDIAL 2014 Conference, pages 133?140,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Extractive Summarization and Dialogue Act Modeling on Email 
Threads: An Integrated Probabilistic Approach 
 
 
 
Tatsuro Oya and Giuseppe Carenini 
Department of Computer Science 
University of British Columbia 
Vancouver, B.C. Canada 
{toya, carenini}@cs.ubc.ca 
 
 
Abstract 
In this paper, we present a novel supervised 
approach to the problem of summarizing 
email conversations and modeling dialogue 
acts. We assume that there is a relationship 
between dialogue acts and important sen-
tences. Based on this assumption, we intro-
duce a sequential graphical model approach 
which simultaneously summarizes email 
conversation and models dialogue acts. We 
compare our model with sequential and 
non-sequential models, which independent-
ly conduct the tasks of extractive summari-
zation and dialogue act modeling. An 
empirical evaluation shows that our ap-
proach significantly outperforms all base-
lines in classifying correct summary 
sentences without losing performance on 
dialogue act modeling task.  
1 Introduction 
Nowadays, an overwhelming amount of text in-
formation can be found on the web. Most of this 
information is redundant and thus the task of 
document summarization has attracted much at-
tention. Since emails in particular are used for a 
wide variety of purposes, the process of automat-
ically summarizing emails might be of great 
benefit in dealing with this excessive amount of 
information. Much work has already been con-
ducted on email summarization. The first re-
search on this topic was conducted by Rambow 
et al. (2004), who took a supervised learning ap-
proach to extracting important sentences. A 
study on the supervised summarization of email 
threads was also performed by Ulrich et al. 
(2009). This study used the regression-based 
method for classification. There have been stud-
ies on unsupervised summarization of email 
threads as well. Zhou et al. (2007, 2008) pro-
posed a graph-based unsupervised approach to 
email conversation summarization using clue 
words, i.e., recurring words contained in replies. 
In addition, the task of labeling sentences 
with dialogue acts has become important and has 
been employed in many conversation analysis 
systems. For example, applications such as meet-
ing summarization and collaborative task learn-
ing agents use dialogue acts as their underlying 
structure (Allen et al., 2007; Murray et al., 
2010). In a previous work, Cohen et al. (2004) 
defined a set of ?email acts? and employed text 
classification methods to detect these acts in 
emails. Later, Carvalho et al. (2006) employed a 
combination of n-gram sequences as features and 
then used a supervised machine learning method 
to improve the accuracy of this email act classifi-
cation. In addition, Shafiq et al. (2011) presented 
unsupervised dialogue act labeling methods. In 
their work, they introduced a graph-based meth-
od and two probabilistic sequence-labeling 
methods for modeling dialogue acts. 
However, little work has been done on dis-
covering the relationship between dialogue acts 
and extractive summaries. If there is a relation-
ship between them, combining these approaches 
so as to model both simultaneously will yield 
better results. In this paper, we investigate this 
hypothesis by introducing a new sequential 
graphical model approach that performs dialogue 
act modeling and extractive summarization joint-
ly on email threads.   
2 Related Work 
While email summarization and dialogue act 
modeling have been effectively studied, in most 
previous work, these tasks were studied inde-
pendently. This section provides related work for 
each task separately. 
133
2.1 Extractive Summarization 
Rambow et al. (2004) introduced sentence ex-
traction techniques that work for email threads. 
In their work, they introduced email-specific fea-
tures and used a machine learning method to 
classify whether or not a sentence should be in-
corporated into a summary. Their experiments 
demonstrated that their features were highly ef-
fective for email summarization. 
Ulrich et al. (2009) proposed a regression-
based machine learning approaches to email 
thread summarization. They compared regres-
sion-based classifiers to binary classifiers and 
showed that their approach significantly im-
proves the summarization accuracy. They em-
ployed the feature set introduced by Rambow et 
al. (2004) as their baseline and introduced new 
features that are also effective for email summa-
rization. Some of their features refer to dialogue 
acts but the assumption is that they are computed 
before the summarization task is performed. Our 
work is aimed at a much closer integration of the 
two tasks by modeling them simultaneously. 
Carenini et al. (2007) developed a fragment 
quotation graph that can capture a fine-grain 
conversation structure in email threads, which 
we will describe in detail in Section 3. They then 
introduced a ClueWordSummarizer (CWS), a 
graph-based unsupervised summarization ap-
proach based on the concept of clue words, 
which are recurring words found in email replies. 
Their experiment showed that the CWS performs 
better than the email summarization approach in 
Rambow et al. (2004).  
Extractive summarization using a sequential 
labeling technique has also been studied. While 
this is not an email summarization, Shen et al. 
(2007) proposed a linear-chain Conditional Ran-
dom Field (CRF) based approach for extractive 
document summarization. In their work, they 
treated the summarization task as a sequence la-
beling problem to take advantage of interaction 
relationships between sentences; their approach 
showed significant improvement when compared 
with non-sequential classifiers. 
2.2 Dialogue Act Modeling 
The first studies on the dialogue act modeling in 
emails were performed by Cohen et al. (2004). 
They defined ?email speech acts? (e.g., Request, 
Deliver, Propose, and Commit) and used ma-
chine learning methods to classify emails accord-
ing to the intent of the sender.  
Carvalho et al. (2006) further developed this 
initial proposal by using contextual information 
such as combinations of n-gram sequences in 
emails as their features for a supervised learning 
approach. The experiment showed that their ap-
proach reduced classification error rates by 
26.4%. Shafiq et al. (2011) proposed unsuper-
vised dialogue act modeling in email threads and 
on forums.  They introduced a graph-based and 
two probabilistic unsupervised approaches for 
modeling dialogue acts. By comparing those ap-
proaches, they demonstrated that the probabilis-
tic approaches were quite effective and 
performed better than the graph-based one. 
While the following work is not done on the 
email domain, Kim et al. (2010) introduced a 
dialogue act classification on one-on-one online 
chat forums. To be able to capture sequential 
dialogue act dependency on chats, they applied a 
CRF model. They demonstrated that, compared 
with other classifiers, their CRF model per-
formed the best. In their later work (Kim et al., 
2012), they extended the domain to multi-party 
live chats and proposed new features for that 
domain. 
3 Capturing Conversation Structure in 
Email Threads  
In this section, we describe how to build a frag-
ment quotation graph which captures the conver-
sation structure of any email thread at finer 
granularity. This graph was developed and 
shown to be effective by Carenini et al. (2011). 
A key assumption of this approach is that in or-
der to effectively perform summarization and 
dialogue act modeling, a fine graph representa-
tion of the underlying conversation structure is 
needed. 
Here, we start with the sample email conver-
sation shown in Figure 1 (a).  For convenience, 
the content of the emails is represented as a se-
quence of fragments.  
First, we identify all new and quoted frag-
ments. For example, email E1 is composed of 
one new fragment, ?b?, and one quoted fragment, 
?a?.  As for email E3, since we do not yet know 
whether or not ?d? and ?e? are different frag-
ments, we consider E3 as being composed of one 
new fragment, ?de? and one quoted fragment, ?b?.  
Second, we identify distinct fragments. To do 
this, we first identify overlaps by comparing 
fragments with each other. If necessary, we split 
the fragments and remove any duplicates from 
them.  For example, a fragment, ?de?, in E3 is 
134
split into ?d? and ?e? after being compared with 
fragments in E4 and the duplicates are removed. 
By applying this process to all of the emails, 
seven distinct fragments, a, b ..., and, g remain in 
this example. 
In the third step, edges which represent the 
replying relationships among the fragments are 
created. These edges are determined based on the 
assumption that any fragment is a reply to neigh-
boring quotations (the quoted fragments immedi-
ately preceding or following the current one). For 
example, the neighboring nodes of ?f? in E4 are 
?d? and ?e?. Thus, we create two edges from node 
?f? in E4 to node ?d? and ?e? in E3.  In the same 
way, we see that the neighboring node of ?g? in 
E4 is ?e?. Hence, there is one edge from node ?g? 
to ?e?.  If no quotation is contained in a reply 
email, we connect the fragments in the email to 
fragments in emails to which it reply.   
In email threads, there are cases in which the 
original email with its quotations is missing from 
the user?s folder, as in the case of ?a? in Figure 1 
(a). These types of emails are called hidden 
emails. Carenini et al. (2005) studied in detail 
how these email types might be treated and their 
influence on email summarization. 
Figure 1 (b) shows the completed fragment 
quotation graph of the email thread shown in 
Figure 1 (a). In the fragment quotation graph 
structure, all paths (e.g., a-b-c, a-b-d-f, a-b-e-f, 
and a-b-e-g in Figure 1 (b)) capture the adjacent 
relationships between email fragments. Hence, 
we use every path that can be derived from the 
graph as our dataset. However, in this case, when 
we run the labeling task on these paths, we ob-
tain multiple labels for some of the sentences 
because the sentences in fragments such as ?a?, 
?b?, and ?f? in Figure 1 (b) are shared among 
multiple paths. Therefore, to assign a label to one 
of these sentences, we take the label more fre-
quently assigned to that sentence when all its 
paths are considered (i.e., the majority vote). 
4 Features 
For both dialogue act modeling and extractive 
summarization, many effective sentence features 
have been discovered so far. Interestingly, some 
common features are shown to be effective in 
both tasks. This section explains the features 
used in our model. We begin with the features 
for extractive summarization and then describe 
how we derive the features for dialogue act mod-
eling. All the features explained in this section, 
whether they belong to extractive summarization 
or dialogue act modeling, are included in our 
model. 
 
(a) A possible configuration of an email conversation 
(E2 and E3 reply to E1, and E4 replies to E3) 
 
(b) An example of a fragment quotation graph 
Figure 1: A fragment quotation graph derived from a 
possible configuration of an email conversation 
4.1 Extractive Summarization Features  
The features we use for extractive summarization 
are mostly from Carenini et al. (2008) and Ram-
bow et al. (2004) and have proven to be effective 
on conversational data. Details of these features 
are described below. Note that all sentences in an 
email thread are ordered based on paths derived 
from a fragment quotation graph. 
 
Length Feature: The number of words in 
each sentence. 
Relative Position Feature: The number of 
sentences preceding the current divided by 
the total number of sentences in one path. 
Thread Name Overlaps Feature: The num-
ber of overlaps of the content words between 
the email thread title and a sentence. 
Subject Name Overlaps Feature: The num-
ber of overlaps of the content words between 
the subject of the email and a sentence. 
Question Feature: A binary feature that in-
dicates whether or not a sentence has a ques-
tion mark. 
CC Feature: A binary feature that indicates 
whether or not an email contains CC. 
135
Participation Dominance Feature: The 
number of utterances each person makes in 
one path. 
 
Finally, we also include a simplified version of 
the ClueWordScore (CWS) developed by 
Carenini et al. (2007), which is listed below.   
 
Simplified CWS Feature: The number of 
overlaps of the content words that occur in 
both the current and adjacent sentences in the 
path, ignoring stopwords. 
4.2 Dialogue Act Features  
The relative positions and length features have 
proven to be beneficial to both tasks (Jeong et al., 
2009; Carenini et al., 2008). Hence, these are 
categorized as both dialogue acts and extractive 
summarization features. In addition, we use word 
and POS n-grams as our features for dialogue act 
modeling. These features are extracted by the 
following process explained in Carvalho et al. 
(2006).  However, we extend the original ap-
proach in order to further abstract n-gram fea-
tures to avoid making them too sparse to be 
effective. In this section, we describe the deriva-
tion process in detail. 
A multi-step approach is used to generate 
word n-gram features. First, all words are tagged 
with the named entity using the Stanford Named 
Entity Recognizer (Finkel et al., 2005), and are 
then replaced with these tags. Second, a se-
quence of word-replacement tasks is applied to 
all email messages. Initially, some types of punc-
tuation marks (e.g., <>()[];:. and ,) and extra 
spaces are removed. Then, shortened phrases 
such as ?I?m? and ?We?ll? are substituted for 
more formal versions such as ?I am? and ?We 
will?. Next, other replacement tasks are per-
formed. Some of them are described in Table1. 
In the third step, unigrams and bigrams are ex-
tracted. In this paper, unigrams and bigrams refer 
to all possible sequences of length one and two 
terms. After extracting all unigrams and bigrams 
for each dialogue act, we then compute Infor-
mation Gain Score (Forman, 2003) and select the 
n-grams whose scores are in the top five greatest 
on the training set. In this way, we can automati-
cally detect features that represent the character-
istics of each dialogue act. In addition to word n-
grams, we also include POS n-grams in our fea-
tures. In a similar way, we first tag each word in 
sentences with POS using the Stanford POS tag-
ger (Toutanova et al., 2003). Then, for each dia-
logue act, we extract bigrams and trigrams, all of 
which are scored by the Information Gain. Based 
on their scores, we select the POS bigram and 
trigram features whose scores are within the top 
five greatest. One example of word n-gram fea-
tures for a Question dialogue act selected by this 
derivation method is shown in Table 2. 
 
Pattern Replacement 
?why?,  ?where?,  ?who?,  ?what? ?when? [WWHH] 
nominative pronouns [I] 
objective pronouns [ME] 
'it',  'those',  'these',  'this',  'that' [IT] 
'will',  ?would',  'shall',  'should', 'must' [MODAL_STRONG] 
?can',  'could',  'may',  'might' [MODAL_WEAK] 
'do',  'does',  'did',  ?done' [DO] 
'is',  'was',  'were',  'are',  'been' 'be',  'am' [BE] 
 'after' , 'before',  'during' [AAAFTER] 
?Jack?, ?Wendy? [Personal_PRONOUN] 
?New York? [LOCATION] 
?Acme Corp.? [ORGANIZATION] 
Table 1: Some Preprocessing Replacement Pattern 
Word Unigram Word Bigram 
? [MODAL_STRONG] [I] 
anyone [IT] ? 
WWHH [DO] anyone 
deny [WWHH] [BE] 
[Personal _PRONOUN] [BE] [IT] 
Table 2: Sample word n-grams selected as the fea-
tures for Question dialogue act 
5 The Sequential Labeling Task 
We use a Dynamic Conditional Random Field 
(DCRF) (Sutton et al., 2004) for labeling tasks. 
A DCRF is a generalization of a linear-chain 
CRF which allows us to represent complex inter-
action between labels. To be more precise, it is a 
conditionally-trained undirected graphical model 
whose structure and parameters are repeated over 
a sequence. Hence, it is the most appropriate 
method for performing multiple labeling tasks on 
the same sequence. 
136
Our DCRF uses the graph structure shown in 
Figure 2 with one chain (the top X nodes) model-
ing extractive summary and the other (the middle 
Y nodes) modeling dialogue acts.  Each node in 
the observation sequence (the bottom Z nodes) 
corresponds to each sentence in a path of the 
fragment quotation graph of the email thread. As 
shown in Figure 2, the graph structure captures 
the relationship between extractive summaries 
and dialogue acts by connecting their nodes.  
We use Mallet1 (McCallum, 2002) to implement 
our DCRF model.  It uses l2-based regularization 
to avoid overfitting, and a limited BFGS fitting 
algorithm to learn the DCRF model parameters. 
Also, it uses tree-based reparameterization 
(Wainwright et al., 2002) to compute the poste-
rior marginal, or inference. 
 
Figure 2: The DCRF model used to create extractive 
summaries and model dialogue acts 
6 Empirical Evaluations 
6.1 Dataset Setup  
In our experiment, the publically available BC3 
corpus2 (Ulrich et al., 2008) is used for training 
and evaluation purposes. The corpus contains 
email threads from the World Wide Web Con-
sortium (W3C) mailing list.  It consists of 40 
threads with an average of five emails per thread. 
The corpus provides extractive summaries of 
each email thread, all of which were annotated 
by three annotators. Hence, we use sentences that 
are selected by more than one annotator as the 
gold standard summary for each conversation. 
In addition, all sentences in the 39 out of 40 
threads are annotated for dialogue act tags. The 
tagset consists of five general and 12 specific 
tags. All of these tags are based on Jeong et al. 
(2009). For our experiment, considering that our 
data is relatively small, we decide to use the 
coarser five tag set. The details are shown in Ta-
ble 3. 
                                                          
1 http://mallet.cs.umass.edu 
2 http://www.cs.ubc.ca/nest/lci/bc3.html 
Tag Description Relative Frequency (%) 
S Statement 73.8 
Q Question 7.92 
R Reply 5.23 
Su Suggestion 5.62 
M Miscellaneous 7.46 
Table 3: Dialogue act tag categories and their relative 
frequency in the BC3 corpus 
After removing quoted sentences and redundant 
information such as senders and addresses, 1300 
distinct sentences remain in the 39 email threads. 
The detailed content of the corpus is summarized 
in Table 4. 
 
 Total 
Dataset 
No. of Threads 39 
No. of Sentences 1300 
No. of Extractive Summary Sentences 521 
No. of S Sentences 959 
No. of Q Sentences 103 
No. of R Sentences 68 
No. of Su Sentences 73 
No. of M Sentences  97 
Table 4: Detailed content of the BC3 corpus 
6.2 Evaluation Metrics  
Here, we introduce evaluation metrics for our 
joint model of extractive summarization and dia-
logue act recognition.  
The CRF model has been shown to be the ef-
fective one in both dialogue act modeling and 
extractive summarization (Shen et al., 2007; Kim 
et al., 2010; Kim et al., 2012). Hence, for com-
parison, we implement two different CRFs, one 
for extractive summarization and the other for 
dialogue act modeling. When classifying extrac-
tive summaries using the CRF, we only use its 
extractive summarization features. Similarly, 
when modeling dialogue acts, we only use its 
dialogue act features. In addition, we also com-
137
pare our system with a non-sequential classifier, 
a support vector machine (SVM), with the same 
settings as those described above. For these im-
plementations, we use Mallet and SVM-light 
package3 (Joachims, 1999).  
In our experiment, we first measure separate-
ly the performance of extractive summarization 
and dialogue act modeling. The performance of 
extractive summarization is measured by its av-
eraged precision, recall, and F-measure. For dia-
logue acts, we report the averaged-micro and 
macro accuracies as well as the averaged accura-
cies of each dialogue act. 
Second, we evaluate the combined perfor-
mance of extractive summarization and dialogue 
act modeling tasks. In general, we are interested 
in the dialogue acts in summary sentences be-
cause they can be later used as input for other 
natural language processing applications such as 
automatic abstractive summarization (Murray et 
al., 2010). Therefore, we measure the perfor-
mance of our model with the following modified 
precision (Pre?), recall (Rec?), and F-measure 
(F?): 
 
     
{                                         }
{                                              }
 (1) 
 
     
{                                        }
{                            }
                       (2)  
 
   
           
         
                                                               (3) 
 
where a correctly classified sentence refers to a 
true summary sentence that is classified as such 
and whose dialogue acts are also correctly classi-
fied. 
6.3 Experiment Procedure  
For all cases, we run five sets of 10-fold cross 
validation to train and test the classifiers on a 
shuffled dataset and calculate the average of the 
results. For each cross validation run, we extract 
all features following the process described in 
Section 4 on the training set. When comparing 
these two baselines with our model, we report p-
values obtained from a student paired t-test on 
the results to determine their significance.   
 
 
 
 
                                                          
3 http://www.cs.cornell.edu/people/tj/svm_light 
6.4 Results 
The performances of extractive summarization 
and dialogue act modeling using the three meth-
ods are summarized in Table 5 and 6, respective-
ly. 
 
 DCRF CRF SVM 
F-measure 0.485 0.428 0.397 
t-test?s  p-value   0.00046 2.5E-07 
Precision 0.562 0.591 0.675 
Recall 0.457 0.370 0.308 
Table 5: A comparison of the extractive summariza-
tion performance of our DCRF model and the two 
baselines based on precision, recall, and F-measure 
 DCRF CRF SVM 
Micro Accuracy 0.785 0.779 0.775 
t-test?s p-value   0.116 0.036 
Macro Accuracy 0.516 0.516 0.304 
t-test?s p-value   0.950 5.2E-32 
S Accuracy 0.901 0.892 0.999 
Q Accuracy 0.832 0.809 0.465 
R Accuracy 0.580 0.575 0.05 
Su Accuracy 0.139 0.108 0.00 
M Accuracy 0.126 0.198 0.00 
Table 6: A comparison of the dialogue act modeling 
performance of our DCRF model and the two base-
lines based on averaged accuracies 
From Table 5, we observe that, in terms of 
extractive summarization results, our DCRF 
model significantly outperforms the two base-
lines. Noticeable improvements can be seen for 
the recall and F-measure. In terms of F-measure, 
compared with the CRF and SVM, our model 
improves by 5.7% and 8.8% respectively. The p-
values obtained from the t-test indicate that our 
results are statistically significantly different (p < 
0.05) from those of the two baselines.  
Regarding dialogue act modeling, the results 
are summarized in Table 6. While no improve-
ment is shown for the micro-averaged accuracy, 
our model and the CRF significantly outperform 
the SVM in terms of the macro-averaged accura-
138
cy. Both our model and the CRF consider the 
sequential structure of the conversation, which is 
not captured in the SVM model. Clearly, this 
indicates that the sequential models are effective 
in modeling dialogue acts due to their ability to 
capture the inter-utterance relations of conversa-
tions.  
Compared with the CRF, our DCRF model 
outperforms it in most cases except in classifying 
the ?M? dialogue act. However these improve-
ments are not significant as t-test of both macro 
and micro-averaged accuracies indicate that the 
differences are not statistically significant (p > 
0.05).   
Another item to be mentioned here is that the 
accuracies of classifying ?R?, ?Su? and ?M? dia-
logue acts are relatively low. This issue applies 
to all classifiers and is plausibly due to the small 
dataset. There are only 68, 73 and 97 sentences, 
respectively, out of 1300 that are labeled as ?R?, 
?Su? and ?M? in the BC3 corpus. Since our dia-
logue act classifiers rely heavily on n-gram fea-
tures, were the data small, these features would 
be too sparse to effectively represent the charac-
teristics of the dialogue acts. However, compared 
with the SVM results, our joint model and the 
CRF perform significantly better in classifying 
these dialogue acts. This also explains why the 
sequential model is preferable in dialogue act 
modeling. 
Note that despite the small dataset, all the 
classifiers are relatively accurate in classifying 
?Q?. This is because n-gram features selected for 
?Q? such as ??? and ?WWHH? are very specific to 
this dialogue act, which makes the task of ?Q? 
classification easier compared to those of others.   
Next, we discuss the result of the com-
bined performance. The performances of our 
model and the two baselines are summarized in 
Table 7. 
 
 DCRF CRF SVM 
F-measure? 0.352 0.324 0.292 
t-test?s  p-value  0.015 3.3E-05 
Precision? 0.407 0.450 0.501 
Recall? 0.335 0.280 0.227 
Table 7: A comparison of the overall performance of 
our DCRF model and the two baselines based on 
modified precision, recall and F-measure 
 
We see that our DCRF model significantly 
outperforms the two baselines. While our model 
yields the lowest Pre? of all, its Rec? is much 
greater than the other two baselines and this 
leads to its achieving the highest F?. Compared 
with the CRF and SVM, the F? obtained from 
our system improves by 2.8% and 6% respec-
tively. In addition, the p-values show that the 
results of our model are statistically significant 
(p < 0.05) compared with those of the two base-
lines. 
Overall, these experiments clearly indicate 
that our model is effective in classifying both 
dialogue acts and summary sentences. 
7 Conclusions and Future Work 
In this work, we have explored a new automated 
approach for extractive summarization and dia-
logue act modeling on email threads. In particu-
lar, we have presented a statistical approach for 
jointly modeling dialogue acts and extractive 
summarization in a single DCRF. The empirical 
results demonstrate that our approach outper-
forms the two baselines on the summarization 
task without loss of performance on the dialogue 
act modeling one. In the future, we would like to 
extend our approach by exploiting more effective 
features. We also plan to apply our approach to 
different domains possessing large dataset. 
Acknowledgements 
We are grateful to Yashar Mehdad, Raimond Ng, 
Maryam Tavafi and Shafiq Joty for their com-
ments and UBC LCI group and ICICS for finan-
cial support. 
References  
J. Allen, N. Chambers, G. Ferguson, L. Galescu, H. 
Jung, and W. Taysom. Plow: A collaborative task 
learning agent. In AAAI-07, pages 22?26, 2007. 
Giuseppe Carenini, Gabriel Murray, and Raymond 
Ng. 2011. Methods for Mining and Summarizing 
Text Conversations. Morgan Claypool. 
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2005. Scalable discovery of hidden emails 
from large folders. In ACM SIGKDD?05, pages 
544?549. 
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2008. Summarizing Emails with Conversa-
tional Cohesion and Subjectivity In proceeding 
46th Annual Meetint Assoc.for Computational Lin-
guistics, page 353-361. 
139
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2007. Summarizing email conversations 
with clue words. 16th International World Wide 
Web Conference (ACM WWW?07). 
Vitor R. Carvalho and William W. Cohen. 2006. Im-
proving ?email speech acts? analysis via n-gram 
selection. In Proceedings of the HLT-NAACL 2006 
Workshop on Analyzing Conversations in Text and 
Speech, ACTS ?09, pages 35?41, Stroudsburg, PA, 
USA. Association for Computational Linguistics. 
William W. Cohen, Vitor R. Carvalho, and Tom M. 
Mitchell. 2004. Learning to classify email into 
?speech acts?. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 309?
316, Barcelona, Spain, July. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Infor-
mation into Information Extraction Systems by 
Gibbs Sampling. In Proceedings of the 43nd An-
nual Meeting of the Association for Computational 
Linguistics (ACL 2005), pp. 363-370. 
George Forman. 2003. An extensive empirical study 
of feature selection metrics for text classification. 
The Journal of Machine Learning Research, 
3:1289?1305. 
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae 
Lee. 2009. Semi-supervised speech act recognition 
in emails and forums. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing. 
Thorsten Joachims. 1999 Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press, 1999.  
Shafiq Joty, Giuseppe Carenini, and Lin, Chin-Yew 
Lin. 2011. Unsupervised Modeling of Dialog Acts 
in Asynchronous Conversations. In Proceedings of 
the twenty second International Joint Conference 
on Artificial Intelligence (IJCAI) 2011. Barcelona, 
Spain. 
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and 
Raymond Ng. 2009 Finding Topics in Emails: Is 
LDA enough? NIPS-2009 workshop on applica-
tions for topic models: text and beyond. Whistler, 
Canada. 
McCallum, A. Kachites, 2002. MALLET: A Machine 
Learning for Language Toolkit. 
http://mallet.cs.umass.edu. 
Su Nam Kim, Lawrence Cavedon, and Timothy 
Baldwin. 2010a. Classifying dialogue acts in 1-to-1 
live chats. In Proceedings of the 2010 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2010), pages 862?871, Boston, 
USA. 
Su Nam Kim, Lawrence Cavedon and Timothy Bald-
win (2012) Classifying Dialogue Acts in Multi-
party Live Chats, In Proceedings of the 26th Pacif-
ic Asia Conference on Language, Information and 
Computation (PACLIC 26), Bali, Indonesia, pp. 
463?472. 
Gabriel Murray and Giuseppe Carenini. 2008. Sum-
marizing Spoken and Written Conversations. Em-
pirical Methods in NLP (EMNLP 2008), Waikiki, 
Hawaii, 2008. 
Gabriel Murray and Giuseppe Carenini. 2010. Sum-
marizing Spoken and Written Conversations. Gen-
erating and Validating Abstracts of Meeting 
Conversations: a User study (INLG 2010), Dublin, 
Ireland, 2010. 
Gabriel Murray, Renals Steve, and Carletta Jean. 
2005a. Extrative summarization of meeting record-
ings. In Proceeding of Interspeech 2005, Lisbon, 
Portugal, pages 593-596. 
Owen Rambow, Lokesh Shrestha, John Chen, and 
Chirsty Lauridsen. 2004. Summarizing email 
threads. In Proceedings of HLTNAACL 2004. 
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and 
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proc. of IJCAI, 
volume 7, 2862?2867. 
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004.  Dynamic conditional 
random fields: Factorized probabilistic models for 
labeling and segmenting sequence data. In Proc. 
ICML. 
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, 
Giuseppe Carenini and Raymond Ng. 2013. Dia-
logue Act Recognition in Synchronous and Asyn-
chronous Conversations. In Proceedings of the 
SIGDIAL 2013 Conference, pages 117?121, Metz, 
France. Association for Computational Linguistics. 
Kristina Toutanova, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL 2003, pp. 
252-259.  
Jan Ulrich, Giuseppe Carenini, Gabriel Murray, 
and Raymond T. Ng: Regression-Based Summari-
zation of Email Conversations. ICWSM 2009 
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 
2008. A publicly available annotated corpus for 
supervised email summarization. AAAI-2008 
EMAIL Workshop. 
Martin J. Wainwright, Tommi Jaakkola, and Alan S. 
Willsky. 2002. Treebased Reparameterization for 
Approximate Inference on Loopy Graphs. In Ad-
vances in Neural Information Processing Systems 
14, pages 1001 1008. MIT Press. 
140
Proceedings of the 8th International Natural Language Generation Conference, pages 45?53,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Template-based Abstractive Meeting Summarization: Leveraging 
Summary and Source Text Relationships 
 
Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond Ng 
Department of Computer Science 
University of British Columbia, Vancouver, Canada 
{toya, mehdad, carenini, rng}@cs.ubc.ca 
 
 
 
Abstract 
In this paper, we present an automatic 
abstractive summarization system of 
meeting conversations. Our system ex-
tends a novel multi-sentence fusion algo-
rithm in order to generate abstract tem-
plates. It also leverages the relationship 
between summaries and their source 
meeting transcripts to select the best 
templates for generating abstractive 
summaries of meetings. Our manual and 
automatic evaluation results demonstrate 
the success of our system in achieving 
higher scores both in readability and in-
formativeness. 
1. Introduction 
People spend a vast amount of time in meetings 
and these meetings play a prominent role in their 
lives. Consequently, study of automatic meeting 
summarization has been attracting peoples? atten-
tion as it can save a great deal of their time and 
increase their productivity. 
The most common approaches to automatic 
meeting summarization have been extractive. 
Since extractive approaches do not require natu-
ral language generation techniques, they are ar-
guably simpler to apply and have been extensive-
ly investigated. However, a user study conducted 
by Murray et al. (2010) indicates that users pre-
fer abstractive summaries to extractive ones. 
Thereafter, more attention has been paid to ab-
stractive meeting summarization systems (Me-
hdad et al.  2013; Murray et al. 2010; Wang and 
Cardie 2013). However, the approaches intro-
duced in previous studies create summaries by 
either heavily relying on annotated data or by 
fusing human utterances which may contain 
grammatical mistakes. In this paper, we address 
these issues by introducing a novel summariza-
tion approach that can create readable summaries 
with less need for annotated data. Our system 
first acquires templates from human-authored 
summaries using a clustering and multi-sentence 
fusion algorithm. It then takes a meeting tran-
script to be summarized, segments the transcript 
based on topics, and extracts important phrases 
from it. Finally, our system selects templates by 
referring to the relationship between human-
authored summaries and their sources and fills 
the templates with the phrases to create summar-
ies. 
The main contributions of this paper are: 1) 
The successful adaptation of a word graph algo-
rithm to generate templates from human-
authored summaries; 2) The implementation of a 
novel template selection algorithm that effective-
ly leverages the relationship between human-
authored summary sentences and their source 
transcripts; and 3) A comprehensive testing of 
our approach, comprising both automatic and 
manual evaluations. 
 We instantiate our framework on the AMI 
corpus (Carletta et al., 2005) and compare our 
summaries with those created from a state-of-
the-art systems. The evaluation results demon-
strate that our system successfully creates in-
formative and readable summaries. 
2. Related Work 
Several studies have been conducted on creating 
automatic abstractive meeting summarization 
systems. One of them includes the system pro-
posed by Mehdad et al., (2013). Their approach 
first clusters human utterances into communities 
(Murray et al., 2012) and then builds an entail-
ment graph over each of the latter in order to se-
lect the salient utterances. It then applies a se-
mantic word graph algorithm to them and creates 
abstractive summaries. Their results show some 
improvement in creating informative summaries. 
45
However, since they create these summaries by 
merging human utterances, their summaries are 
still partially extractive.  
Recently, there have been some studies on 
creating abstract summaries of specific aspects of 
meetings such as decisions, actions and problems 
(Murray et al. 2010; Wang and Cardie, 2013). 
These summaries are called the Focused Meeting 
Summaries (Carenini et al., 2011). 
The system introduced by Murray et al. first 
classifies human utterances into specific aspects 
of meetings, e.g. decisions, problem, and action, 
and then maps them onto ontologies. It then se-
lects the most informative subsets from these on-
tologies and finally generates abstractive sum-
maries of them, utilizing a natural language gen-
eration tool, simpleNLG (Gatt and Reiter, 2009). 
Although their approach is essentially focused 
meeting summarization, after creating summaries 
of specific aspects, they aggregate them into one 
single summary covering the whole meeting. 
Wang and Cardie introduced a template-based 
focused abstractive meeting summarization sys-
tem. Their system first clusters human-authored 
summary sentences and applies a Multiple-
Sequence Alignment algorithm to them to gener-
ate templates. Then, given a meeting transcript to 
be summarized, it identifies a human utterance 
cluster describing a specific aspect and extracts 
all summary-worthy relation instances, i.e. indi-
cator-argument pairs, from it. Finally, the tem-
plates are filled with these relation instances and 
ranked accordingly, to generate summaries of a 
specific aspect of the meeting.  
Although the two approaches above are both 
successful in creating readable summaries, they 
rely on much annotated information, such as dia-
log act and sentiment types, and also require the 
accurate classification of human utterances that 
contain much noise and much ill-structured 
grammar. 
Our approach is inspired by the works intro-
duced here but improves on their shortcomings. 
Unlike those of Murray et al. (2010) and Wang 
and Cardie (2013), our system relies less on an-
notated training data and does not require a clas-
sifier. In addition, our evaluation indicates that 
our system can create summaries of the entire 
conversations that are more informative and 
readable than those of Mehdad et al.(2013). 
3. Framework 
In order for summaries to be readable and in-
formative, they should be grammatically correct 
and contain important information in meetings. 
To this end, we have created our framework con-
sisting of the following two components: 1) An 
off-line template generation module, which gen-
eralizes collected human-authored summaries 
and creates templates from them; and 2) An on-
line summary generation module, which seg-
ments meeting transcripts based on the topics 
discussed, extracts the important phrases from 
these segments, and generate abstractive sum-
maries of them by filling the phrases into the ap-
propriate templates. Figure 1 depicts our frame-
work. In the following sections, we describe each 
of the two components in detail. 
 
Figure 1: Our meeting summarization framework. Top: off-line Template generation module. Bottom: on-line 
Summary Generation module. 
46
3.1 Template Generation Module 
Our template generation module attempts to sat-
isfy two possibly conflicting objectives. First, 
templates should be quite specific such that they 
accept only the relevant fillers. Second, our 
module should generate generalized templates 
that can be used in many situations. We assume 
that the former is achieved by labeling phrases 
with their hypernyms that are not too general and 
the latter by merging related templates. Based on 
these assumptions, we divide our module into the 
three tasks: 1) Hypernym labeling; 2) Clustering; 
and 3) Template fusion. 
3.1.1 Hypernym Labeling 
Templates are derived from human-authored 
meeting summaries in the training data. We first 
collect sentences whose subjects are meeting par-
ticipant(s) and that contain active root verbs, 
from the summaries. This is achieved by utilizing 
meeting participant information provided in the 
corpus and parsing sentences with the Stanford 
Parser (Marneffe et al., 2006). The motivation 
behind this process is to collect sentences that are 
syntactically similar. We then identify all noun 
phrases in these sentences using the Illinois 
Chunker (Punyakanok and Roth, 2001). This 
chunker extracts all noun phrases as well as part 
of speech (POS) for all words. To add further in-
formation on each noun phrase, we label the right 
most nouns (the head nouns) in each phrase with 
their hypernyms using WordNet (Fellbaum, 
1998). In WordNet, hypernyms are organized in-
to hierarchies ranging from the most abstract to 
the most specific. For our work, we utilize the 
fourth most abstract hypernyms in light of the 
first goal discussed at the beginning of Section 
3.1, i.e. not too general. For disambiguating the 
sense of the nouns, we simply select the sense 
that has the highest frequency in WordNet.  
At this stage, all noun phrases in sentences 
are tagged with their hypernyms defined in 
WordNet, such as ?artifact.n.01?, and ?act.n.02?, 
where n?s stands for nouns and the two digit 
numbers represent their sense numbers. We treat 
these hypernym-labeled sentences as templates 
and the phrases as blanks. 
In addition, we also create two additional 
rules for tagging noun phrases: 1) Since the sub-
jects of all collected sentences are meeting par-
ticipant(s), we label all subject noun phrases as 
?speaker?; and 2) If the noun phrases consist of 
meeting specific terms such as ?the meeting? or 
?the group?, we do not convert them into blanks. 
These two rules guarantee the creation of tem-
plates suitable for meetings. 
 
Figure 2: Some examples of hypernym labeling task 
3.1.2 Clustering 
Next, we cluster the templates into similar 
groups. We utilize root verb information for this 
process assuming that these verbs such as ?dis-
cuss? and ?suggest? that appear in summaries are 
the most informative factors in describing meet-
ings. Therefore, after extracting root verbs in 
summary sentences, we create fully connected 
graphs where each node represents the root verbs 
and each edge represents a score denoting how 
similar the two word senses are. To measure the 
similarity of two verbs, we first identify the verb 
senses based on their frequency in WordNet and 
compute the similarity score based on the short-
est path that connects the senses in the hypernym 
taxonomy. We then convert the graph into a 
similarity matrix and apply a Normalized Cuts 
method (Shi and Malik, 2000) to cluster the root 
verbs. Finally, all templates are organized into 
the groups created by their root verbs. 
 
Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 
47
3.1.3 Template Fusion 
We further generalize the clustered templates by 
applying a word graph algorithm. The algorithm 
was originally proven to be effective in summa-
rizing a cluster of related sentences (Boudin and 
Morin, 2013; Filippova, 2010; Mehdad et al., 
2013). We extend it so that it can be applied to 
templates. 
Word Graph Construction  
In our system, a word graph is a directed graph 
with words or blanks serving as nodes and edges 
representing adjacency relations.  
Given a set of related templates in a group, 
the graph is constructed by first creating a start 
and end node, and then iteratively adding tem-
plates to it. When adding a new template, the al-
gorithm first checks each word in the template to 
see if it can be mapped onto existing nodes in the 
graph. The word is mapped onto a node if the 
node consists of the same word and the same 
POS tag, and no word from this template has 
been mapped onto this node yet. Then, it checks 
each blank in the template and maps it onto a 
node if the node consists of the same hypernym-
labeled blank and no blank from this template 
has been mapped onto this node yet.  
When more than one node refer to the same 
word or blank in the template, or when more than 
one word or blank in the template can be mapped 
to the same node in the graph, the algorithm 
checks the neighboring nodes in the current 
graph as well as the preceding and the subse-
quent words or blanks in the template. Then, 
those word-node or blank-node pairs with higher 
overlap in the context are selected for mapping. 
Otherwise, a new node is created and added to 
the graph. As a simplified illustration, we show a 
word graph in Figure 3 obtained from the follow-
ing four templates. 
  
? After introducing [situation.n.01], [speaker] then dis-
cussed [content.n.05] . 
? Before beginning [act.n.02] of [artifact.n.01], [speaker] 
discussed [act.n.02] and [content.n.05] for [arti-
fact.n.01] . 
? [speaker] discussed [content.n.05] of [artifact.n.01] and 
[material.n.01] . 
? [speaker] discussed [act.n.02] and [asset.n.01] in attract-
ing [living_thing.n.01] . 
Path Selection  
The word graph generates many paths connect-
ing its start and end nodes, not all of which are 
readable and cannot be used as templates. Our 
aim is to create concise and generalized tem-
plates. Therefore, we create the following rank-
ing strategy to be able to select the ideal paths. 
First, to filter ungrammatical or complex tem-
plates, the algorithm prunes away the paths hav-
ing more than three blanks; having subordinate 
clauses; containing no verb; having two consecu-
tive blanks; containing blanks which are not la-
beled by any hypernym; or whose length are 
shorter than three words. Note that these rules, 
which were defined based on close observation 
of the results obtained from our development set, 
greatly reduce the chance of selecting ill-
structured templates. Second, the remaining 
paths are reranked by 1) A normalized path 
weight and 2) A language model learned from 
hypernym-labeled human-authored summaries in 
our training data, each of which is described be-
low. 
1) Normalized Path Weight 
We adapt Filippova (2010)?s approach to com-
pute the edge weight. The formula is shown as: 
         
                  
?                 
                
    
where ei,j  is an edge that connects the nodes i 
and j in a graph, freq(i) is the number of words 
and blanks in the templates that are mapped to 
node i and diff(p,i,j) is the distance between the 
offset positions of nodes i and j in path p. This 
weight is defined so that the paths that are in-
formative and that contain salient (frequent) 
words are selected. To calculate a path score, 
W(p), all the edge weights on the path are 
summed and normalized by its length. 
2) Language Model 
Although the goal is to create concise templates, 
these templates must be grammatically correct. 
Hence, we train an n-gram language model using 
all templates generated from the training data in 
the hypernym labeling stage. Then for each path, 
we compute a sum of negative log probabilities 
of n-gram occurrences and normalize the score 
by its length, which is represented as H(p).  
The final score of each path is calculated as 
follows: 
                                   
where ? and ? are the coefficient factors which 
are tuned using our development set. For each 
48
group of clusters, the top ten best scored paths 
are selected as templates and added to its group. 
As an illustration, the path shown in bold in 
Figure 3 is the highest scored path obtained from 
this path ranking strategy.  
3.2 Summary Generation Module 
This section explains our summary generation 
module consisting of four tasks: 1) Topic seg-
mentation; 2) Phrase and speaker extraction; 3) 
Template selection and filling; and 4) Sentence 
ranking.  
3.2.1 Topic Segmentation 
It is important for a summary to cover all topics 
discussed in the meeting. Therefore, given a 
meeting transcript to be summarized, after re-
moving speech disfluencies such as ?uh?, and 
?ah?, we employ a topic segmenter, LCSeg (Gal-
ley et al., 2003) which create topic segments by 
observing word repetitions.  
One shortcoming of LCSeg is that it ignores 
speaker information when segmenting transcripts. 
Important topics are often discussed by one or 
two speakers. Therefore, in order to take ad-
vantage of the speaker information, we extend 
LCSeg by adding the following post-process 
step: If a topic segment contains more than 25 ut-
terances, we subdivide the segment based on the 
speakers. These subsegments are then compared 
with one another using cosine similarity, and if 
the similarity score is greater than that of the 
threshold (0.05), they are merged. The two num-
bers, i.e. 25 and 0.05, were selected based on the 
development set so that, when segmenting a tran-
script, the system can effectively take into ac-
count speaker information without creating too 
many segments. 
3.2.2 Phrase And Speaker Extraction 
All salient phrases are then extracted from each 
topic segment in the same manner as performed 
in the template generation module in Section 3.1, 
by: 1) Extracting all noun phrases; and 2) Label-
ing each phrase with the hypernym of its head 
noun. Furthermore, to be able to select salient 
phrases, these phrases are subsequently scored 
and ranked based on the sum of the frequency of 
each word in the segment. Finally, to handle re-
dundancy, we remove phrases that are subsets of 
others. 
In addition, for each utterance in the meeting, 
the transcript contains its speaker?s name. There-
fore, we extract the most dominant speakers? 
name(s) for each topic segment and label them as 
?speaker?. These phrases and this speaker infor-
mation will later be used in the template filling 
process. Table 1 below shows an example of 
dominant speakers and high scored phrases ex-
tracted from a topic segment. 
Dominant speakers 
Project Manager (speaker) 
Industrial Designer (speaker) 
High scored phrases (hypernyms) 
the whole look (appearance.n.01) 
the company logo (symbol.n.01) 
the product (artifact.n.01) 
the outside (region.n.01) 
electronics (content.n.05) 
the fashion (manner.n.01) 
Table 1: Dominant speakers and high scored 
phrases extracted from a topic segment 
3.2.3 Template Selection and Filling 
In terms of our training data, all human-authored 
abstractive summary sentences have links to the 
subsets of their source transcripts which support 
and convey the information in the abstractive 
sentences as illustrated in Figure 4. These subsets 
are called communities. Since each community is 
used to create one summary sentence, we hy-
pothesize that each community covers one spe-
cific topic.  
Thus, to find the best templates for each topic 
segment, we refer to our training data. In particu-
lar, we first find communities in the training set 
that are similar to the topic segment and identify 
the templates derived from the summary sen-
tences linked to these communities.   
 
Figure 4: A link from an abstractive summary sentence to a subset of a meeting transcript that conveys or sup-
ports the information in the abstractive sentence 
49
This process is done in two steps, by: 1) As-
sociating the communities in the training data 
with the groups containing templates that were 
created in our template generation module; and 
2) Finding templates for each topic segment by 
comparing the similarities between the segments 
and all sets of communities associated with the 
template groups. Below, we describe the two 
steps in detail. 
1) Recall that in the template generation 
module in Section 3.1, we label human-authored 
summary sentences in training data with hyper-
nyms and cluster them into similar groups. Thus, 
as shown in Figure 5, we first associate all sets of 
communities in the training data into these 
groups by determining to which groups the 
summary sentences linked by these communities 
belong. 
 
Figure 5: An example demonstrating how each com-
munity in training data is associated with a group con-
taining templates  
2) Next, for each topic segment, we compute 
average cosine similarity between the segment 
and all communities in all of the groups.  
 
Figure 6: Computing the average cosine similarities 
between a topic segment and all sets of com munities 
in each group 
At this stage, each community is already as-
sociated with a group that contains ranked tem-
plates. In addition, each segment has a list of av-
erage-scores that measures how similar the seg-
ment is to the communities in each group. Hence, 
the templates used for each segment are decided 
by selecting the ones from the groups with higher 
scores.  
Our system now contains for each segment a 
set of phrases and ideal templates, both of which 
are scored, as well as the most dominant speakers? 
name(s). Thus, candidate sentences are generated 
for each segment by: first, selecting speakers? 
name(s), then selecting phrases and templates 
based on their scores; and finally filling the tem-
plates with matching labels. Here, we limit the 
maximum number of sentences created for each 
topic segment to 30. This number is defined so 
that the system can avoid generating sentences 
consisting of low scored phrases and templates. 
Finally, these candidate sentences are passed to 
our sentence ranking module. 
3.2.4 Sentence Ranking 
Our system will create many candidate sentenc-
es, and most of them will be redundant. Hence, 
to be able to select the most fluent, informative 
and appropriate sentences, we create a sentence 
ranking model considering 1) Fluency, 2) Cover-
age, and 3) The characteristics of the meeting, 
each of which are summarized below: 
1) Fluency 
We estimate the fluency of the generated sen-
tences in the same manner as in Section 3.1.3. 
That is, we train a language model on human-
authored abstract summaries from the training 
portions of meeting data and then compute a 
normalized sum of negative log probabilities of 
n-gram occurrences in the sentence. The fluency 
score is represented as H(s) in the equation be-
low. 
2) Coverage 
To select sentences that cover important topics, 
we give special rewards to the sentences that 
contain the top five ranked phrases.   
3) The Characteristics of the Meeting 
We also add three additional scoring rules that 
are specific to the meeting summaries. In particu-
lar, these three rules are created based on phrases 
often used in the opening and closing of meet-
ings in a development set: 1) If sentences derived 
50
from the first segment contain the words ?open? 
or ?meeting?, they will be rewarded; 2) If sen-
tences derived from the last segment contain the 
words ?close? or ?meeting?, the sentences will 
again be rewarded; and 3)  If sentences not de-
rived  from the first or last segment contains the 
words ?open? or ?close?,  they will be penalized. 
The final ranking score of the candidate sen-
tences is computed using the follow formula: 
      s  ?  s  ? ?
i
 
i 1  i s  ?  i
 
i 1  i s     
where, Ri (s) is a binary that indicates whether 
the top i ranked phrase exists in sentence s; Mi (s) 
is also a binary that indicates whether the i th 
meeting specific rule can be met for sentence s; 
and ?, ? i and ? i are the coefficient factors to tune 
the ranking score, all of which are tuned using 
our development set. 
Finally, the sentence ranked the highest in 
each segment is selected as the summary sen-
tence, and the entire meeting summary is created 
by collecting these sentences and sorting them by 
the chronological order of the topic segments. 
4. Evaluation 
In this section, we describe an evaluation of our 
system. First, we describe the corpus data. Next, 
the results of the automatic and manual evalua-
tions of our system against various baseline ap-
proaches are discussed. 
4.1 Data 
For our meeting summarization experiments, we 
use manually transcripted meeting records and 
their human-authored summaries in the AMI 
corpus. The corpus contains 139 meeting records 
in which groups of four people play different 
roles in a fictitious team. We reserved 20 meet-
ings for development and implemented a three-
fold cross-validation using the remaining data.   
4.2 Automatic Evaluation 
We report the F1-measure of ROUGE-1, 
ROUGE-2 and ROUGE-SU4 (Lin and Hovy, 
2003) to assess the performance of our system. 
The scores of automatically generated summaries 
are calculated by comparing them with human-
authored ones.  
For our baselines, we use the system intro-
duced by Mehdad et al. (2013) (FUSION), which 
creates abstractive summaries from extracted 
sentences and was proven to be effective in cre-
ating abstractive meeting summaries; and Tex-
tRank (Mihalcea and Tarau, 2004), a graph based 
sentence ranker that is suitable for creating ex-
tractive summaries. Our system can create sum-
maries of any length by adjusting the number of 
segments to be created by LCSeg. Thus, we cre-
ate summaries of three different lengths (10, 15, 
and 20 topic segments) with the average number 
of words being 100, 137, and 173, respectively. 
These numbers generally corresponds to human-
authored summary length in the corpus which 
varies from 82 to 200 words.  
Table 2 shows the results of our system in 
comparison with those of the two baselines. The 
results show that our model significantly outper-
forms the two baselines. Compared with FU-
SION, our system with 20 segments achieves 
about 3 % of improvement in all ROUGE scores. 
This indicates that our system creates summaries 
that are more lexically similar to human-authored 
ones. Surprisingly, there was not a significant 
change in our ROUGE scores over the three dif-
ferent summary lengths. This indicates that our 
system can create summaries of any length with-
out losing its content. 
Models Rouge-1 Rouge-2 Rouge-SU4 
TextRank 21.7 2.5 6.5 
FUSION 27.9 4.0 8.1 
Our System 10 Seg. 28.4 6.7 10.1 
Our System 15 Seg. 30.6 6.8 10.9 
Our System 20 Seg. 31.5 6.7 11.4 
Table 2: An evaluation of summarization performance 
using the F1 measure of ROUGE-1 2, and SU4 
4.3 Manual Evaluation 
We also conduct manual evaluations utilizing a 
crowdsourcing tool1. In this experiment, our sys-
tem with 15 segments is compared with FUSION, 
human-authored summaries (ABS) and, human-
annotated extractive summaries (EXT).  
After randomly selecting 10 meetings, 10 par-
ticipants were selected for each meeting and giv-
en instructions to browse the transcription of the 
meeting so as to understand its gist. They were 
then asked to read all different types of summar-
ies described above and rate each of them on a 1-
5 scale for the following three items: 1) The 
summary?s overall quality, with ?5? being the 
best and ?1? being the worst possible quality; 2) 
The summary?s fluency, ignoring the capitaliza-
tion or punctuation, with ?5? indicating no 
grammatical mistakes and ?1? indicating too 
many; and 3) The summary?s informativeness, 
with ?5? indicating that the summary covers all 
meeting content and ?1? indicating that the 
                                                 
1 http://www.crowdflower.com/ 
51
summary does not cover the content at all.  
The results are described in Table 3. Overall, 
58 people worldwide, who are among the most 
reliable contributors accounting for 7 % of over-
all members and who maintain the highest levels 
of accuracy on test questions provided in pervi-
ous crowd sourcing jobs, participated in this rat-
ing task.  As to statistical significance, we use the 
2-tail pairwise t-test to compare our system with 
the other three approaches. The results are sum-
marized in Table 4. 
Models Quality Fluency Informativeness 
Our System  3.52 3.69 3.54 
ABS 3.96 4.03 3.87 
EXT 3.02 3.16 3.30 
FUSION 3.16 3.14 3.05 
Table 3: Average rating scores. 
Models  
Compared 
Quality 
(P-value) 
Fluency 
(P-value) 
Informativeness 
(P-value) 
Our System 
vs. ABS 
0.000162 0.000437 0.00211 
Our System 
vs. FUSION 
0.00142 0.0000135 0.000151 
Our System 
vs. EXT. 
0.000124 0.0000509 0.0621 
Table 4: T-test results of manual evaluation 
As expected, for all of the three items, ABS 
received the highest of all ratings, while our sys-
tem received the second highest. The t-test re-
sults indicate that the difference in the rating data 
is statistically significant for all cases except that 
of informativeness between ours and the extrac-
tive summaries. This can be understood because 
the extractive summaries were manually created 
by an annotator and contain all of the important 
information in the meetings. 
From this observation, we can conclude that 
users prefer our template-based summaries over 
human-annotated extractive summaries and ab-
stractive summaries created from extracted sali-
ent sentences. Furthermore, it demonstrates that 
our summaries are as informative as human-
annotated extractive ones. 
Finally, we show in Figure 7 one of the sum-
maries created by our system in line-with a hu-
man-authored one.  
5. Conclusion and Future Work 
In this paper, we have demonstrated a robust ab-
stractive meeting summarization system. Our ap-
proach makes three main contributions. First, we 
have proposed a novel approach for generating 
templates leveraging a multi-sentence fusion al-
gorithm and lexico-semantic information. Sec-
ond, we have introduced an effective template 
selection method, which utilize the relationship 
between human-authored summaries and their 
source transcripts. Finally, comprehensive evalu-
ation demonstrated that summaries created by 
our system are preferred over human-annotated 
extractive ones as well as those created from a 
state-of-the-art meeting summarization system.  
The current version of our system uses only 
hypernym information in WordNet to label 
phrases. Considering limited coverage in Word-
Net, future work includes extending our frame-
work by applying a more sophisticated labeling 
task utilizing a richer knowledge base (e.g., YA-
GO). Also, we plan to apply our framework to 
different multi-party conversational domains 
such as chat logs and forum discussions.  
Human-Authored Summary 
The project manager opened the meeting and had the 
team members introduce themselves and describe their 
roles in the upcoming project. The project manager then 
described the upcoming project. The team then discussed 
their experiences with remote controls. They also 
discussed the project budget and which features they 
would like to see in the remote control they are to create. 
The team discussed universal usage, how to find remotes 
when misplaced, shapes and colors, ball shaped remotes, 
marketing strategies, keyboards on remotes, and remote 
sizes. team then discussed various features to consider in 
making the remote. 
 
Summary Created by Our System with 15 Segment 
project manager summarized their role of the meeting .  
user interface expert and project manager talks about a 
universal remote . the group recommended using the 
International Remote Control Association rather than a 
remote control . project manager offered the ball 
idea .user interface expert suggested few buttons . user 
interface expert and industrial designer then asked a 
member about a nice idea for The idea . project manager 
went over a weak point . the group announced the one-
handed design . project manager and industrial designer 
went over their remote control idea . project manager 
instructed a member to research the ball function .  
industrial designer went over stability point .industrial 
designer went over definite points . 
Figure 7: A comparison between a human-authored 
summary and a summary created by our system 
Acknowledgements 
We would like to thank all members in UBC 
NLP group for their comments and UBC LCI 
group and ICICS for financial support. 
 References 
Florian Boudin and Emmanuel Morin. 2013. 
Keyphrase Extraction for N-best Reranking in Mul-
ti-Sentence Compression. In  Proceedings of the 
52
2013 Conference of the North American Chapter of 
the Association for Computational Linguistics: 
Human Language Technologies (NAACL-HLT 
2013), 2013. 
Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 
2011. Methods for Mining and Summarizing Text 
Conversations. Morgan Claypool. 
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guil-
lemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, 
M. Kronenthal, G. Lathoud, M. Lincoln, A. 
Lisowska, I. McCowan, W. Post, D. Reidsma, and 
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proceeding of MLMI 2005, Ed-
inburgh, UK, pages 28?39. 
Christiane Fellbaum 1998. WordNet, An Electronic 
Lexical Database. The MIT Press. Cambridge, MA. 
Katja Filippova. 2010. Multi-sentence compression: 
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistic 
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL 2003). 562?
569. Sapporo, Japan. 
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a 
Realisation Engine for Practical Applications. In 
ENLG?09: Proceedings of the 12th European 
Workshop on Natural Language Generation, pages 
90?93, Morristown, NJ, USA. Association for 
Computational Linguistics. 
Ravi Kondadadi, Blake Howald and Frank Schilder. 
2013. A Statistical NLG Framework for Aggregat-
ed Planning and Realization. In Proceeding of the 
Annual Conferene for the Association of Computa-
tional Linguistic (ACL 2013).  
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation 
(LREC'06). 
Yashar Mehdad, Giuseppe Carenini, Frank Tompa. 
2013. Abstractive Meeting Summarization with En-
tailment and Fusion. In Proceedings of the 14th Eu-
ropean Natural Language Generation (ENLG - 
SIGGEN 2013), Sofia, Bulgaria. 
Rata Mihalcea and Paul Tarau 2004. TextRank: 
Bringing order into texts. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing, July. 
Gabriel Murray, Giuseppe Carenini, and Raymond T. 
Ng. 2010. Generating and validating abstracts of 
meeting conversations: a user study. In INLG 2010. 
Gabriel Murray, Giuseppe Carenini and Raymond Ng. 
2012. Using the Omega Index for Evaluating Ab-
stractive Community Detection, NAACL 2012, 
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, Montreal, 
Canada. 
Vasin Punyakanok and Dan Roth. 2001. The Use of 
Classifiers in Sequential Inference. NIPS (2001) pp. 
995-1001. 
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts 
& Image Segmentation. IEEE Trans. of PAMI, Aug 
2000. 
David C. Uthus and David W. Aha. 2011. Plans to-
ward automated chat summarization. In Proceed-
ings of the Workshop on Automatic Summarization 
for Different Genres, Media, and Languages, 
WASDGML?11, pages 1-7, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Lu Wang and Claire Cardie. 2013. Domain-
Independent Abstract Generation for Focused 
Meeting Summarization.  In ACL 2013. 
Liang Zhou and Eduard Hovy. 2005. Digesting virtual 
?geek? culture: The summarization of technical in-
ternet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational 
Linguistics (ACL?05), pages 298-305, Ann Arbor, 
Michigan, June. Association for Computational 
Linguistics. 
53
