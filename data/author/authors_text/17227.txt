Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 109?118, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Textual Similarity based on Lexical-Semantic features 
 
 
Alexander Ch?vez, Antonio Fern?ndez Orqu?n, 
H?ctor D?vila, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ?  
Matanzas, Cuba.  
{alexander.chavez, tony, 
hector.davila, yoan.gutierrez, 
armando.collazo, jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera de 
San Vicente S/N Alicante, Spain. 
{montoyo,rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the Semantic Textual 
Similarity task (STS) of SemEval-2013. Our 
supervised system uses different types of 
lexical and semantic features to train a 
Bagging classifier used to decide the correct 
option. Related to the different features we 
can highlight the resource ISR-WN used to 
extract semantic relations among words and 
the use of different algorithms to establish 
semantic and lexical similarities. In order to 
establish which features are the most 
appropriate to improve STS results we 
participated with three runs using different 
set of features. Our best run reached the 
position 44 in the official ranking, obtaining 
a general correlation coefficient of 0.61. 
1 Introduction 
SemEval-2013 (Agirre et al, 2013) presents the 
task Semantic Textual Similarity (STS) again. In 
STS, the participating systems must examine the 
degree of semantic equivalence between two 
sentences. The goal of this task is to create a 
unified framework for the evaluation of semantic 
textual similarity modules and to characterize 
their impact on NLP applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that 
STS assumes bidirectional graded equivalence 
between the pair of textual snippets. 
In case of TE, the equivalence is directional 
(e.g. a student is a person, but a person is not 
necessarily a student). In addition, STS differs 
from TE and Paraphrase in that, rather than 
being a binary yes/no decision, STS is a 
similarity-graded notion (e.g. a student is more 
similar to a person than a dog to a person).  
This graded bidirectional is useful for NLP 
tasks such as Machine Translation (MT), 
Information Extraction (IE), Question 
Answering (QA), and Summarization. Several 
semantic tasks could be added as modules in the 
STS framework, ?such as Word Sense 
Disambiguation and Induction, Lexical 
Substitution, Semantic Role Labeling, Multiword 
Expression detection and handling, Anaphora 
and Co-reference resolution, Time and Date 
resolution and Named Entity, among others?1  
1.1 Description of 2013 pilot task 
This edition of SemEval-2013 remain with the 
same classification approaches that in their first 
version in 2012. The output of different systems 
was compared to the reference scores provided 
by SemEval-2013 gold standard file, which 
range from five to zero according to the next 
criterions2: (5) ?The two sentences are 
equivalent, as they mean the same thing?. (4) 
?The two sentences are mostly equivalent, but 
some unimportant details differ?. (3) ?The two 
sentences are roughly equivalent, but some 
important information differs/missing?. (2) ?The 
two sentences are not equivalent, but share some 
details?. (1) ?The two sentences are not 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
2 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
109
equivalent, but are on the same topic?. (0) ?The 
two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 3 shows the 
Related Works. Section 4 presents our system 
architecture and description of the different runs. 
In section 4 we describe the different features 
used in our system. Results and a discussion are 
provided in Section 5 and finally we conclude in 
Section 6. 
2 Related Works 
There are more extensive literature on measuring 
the similarity between documents than to 
between sentences. Perhaps the most recently 
scenario is constituted by the competition of 
SemEval-2012 task 6: A Pilot on Semantic 
Textual Similarity (Aguirre and Cerd, 2012). In 
SemEval-2012, there were used different tools 
and resources like stop word list, multilingual 
corpora, dictionaries, acronyms, and tables of 
paraphrases, ?but WordNet was the most used 
resource, followed by monolingual corpora and 
Wikipedia? (Aguirre and Cerd, 2012). 
According to Aguirre, Generic NLP tools were 
widely used. Among those that stand out were 
tools for lemmatization and POS-tagging 
(Aguirre and Cerd, 2012). On a smaller scale 
word sense disambiguation, semantic role 
labeling and time and date resolution. In 
addition, Knowledge-based and distributional 
methods were highly used. Aguirre and Cerd 
remarked on (Aguirre and Cerd, 2012) that 
alignment and/or statistical machine translation 
software, lexical substitution, string similarity, 
textual entailment and machine translation 
evaluation software were used to a lesser extent. 
It can be noted that machine learning was widely 
used to combine and tune components. 
Most of the knowledge-based methods ?obtain 
a measure of relatedness by utilizing lexical 
resources and ontologies such as WordNet 
(Miller et al, 1990b) to measure definitional 
overlap, term distance within a graphical 
taxonomy, or term depth in the taxonomy as a 
measure of specificity? (Banea et al, 2012). 
Some scholars as in (Corley and Mihalcea, 
June 2005) have argue ?the fact that a 
comprehensive metric of text semantic similarity 
should take into account the relations between 
words, as well as the role played by the various 
entities involved in the interactions described by 
each of the two sentences?. This idea is resumed 
in the Principle of Compositionality, this 
principle posits that the meaning of a complex 
expression is determined by the meanings of its 
constituent expressions and the rules used to 
combine them (Werning et al, 2005). Corley 
and Mihalcea in this article combined metrics of 
word-to-word similarity, and language models 
into a formula and they pose that this is a 
potentially good indicator of the semantic 
similarity of the two input texts sentences. They 
modeled the semantic similarity of a sentence as 
a function of the semantic similarity of the 
component words (Corley and Mihalcea, June 
2005). 
One of the top scoring systems at SemEval-
2012 (?ari? et al, 2012) tended to use most of 
the aforementioned resources and tools. They 
predict the human ratings of sentence similarity 
using a support-vector regression model with 
multiple features measuring word-overlap 
similarity and syntax similarity. They also 
compute the similarity between sentences using 
the semantic alignment of lemmas. First, they 
compute the word similarity between all pairs of 
lemmas from first to second sentence, using 
either the knowledge-based or the corpus-based 
semantic similarity. They named this method 
Greedy Lemma Aligning Overlap. 
Daniel B?r presented the UKP system, which 
performed best in the Semantic Textual 
Similarity (STS) task at SemEval-2012 in two 
out of three metrics. It uses a simple log-linear 
regression model, trained on the training data, to 
combine multiple text similarity measures of 
varying complexity. 
3 System architecture and description 
of the runs 
As we can see in Figure 1, our three runs begin 
with the pre-processing of SemEval-2013?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS-tagged using Freeling 2.2 
tool (Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group 
of features. 
110
Figure 1. System Architecture. 
The Run 1 (named MultiSemLex) is our main 
run. This takes into account all extracted features 
and trains a model with a Bagging classifier 
(Breiman, 1996) (using REPTree). The training 
corpus has been provided by SemEval-2013 
competition, in concrete by the Semantic Textual 
Similarity task.  
The Run 2 (named MultiLex) and Run 3 
(named MultiSem) use the same classifier, but 
including different features. Run 2 uses (see 
Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 
4.1, and Lexical-Semantic Alignment (LS-A) 
described in section 4.2. 
On the other hand, Run 3 uses features 
extracted only from Semantic Alignment (SA) 
described in section 4.3. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two phrases. 
Finally, we test our system with the SemEval-
2013 test set (see Table 14 with the results of our 
three runs). The following section describes the 
features extraction process. 
4 Description of the features used in the 
Machine Learning System 
Many times when two phrases are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired in this fact we 
developed various algorithms, which measure 
the level of overlapping by computing a quantity 
of matching words in a pair of phrases. In our 
system, we used as features for a MLS lexical 
and semantic similarity measures. Other features 
were extracted from a lexical-semantic sentences 
alignment and a variant using only a semantic 
alignment. 
4.1 Similarity measures 
We have used well-known string based 
similarity measures like: Needleman-Wunch 
(sequence alignment), Smith-Waterman 
(sequence alignment), Smith-Waterman-Gotoh, 
Smith-Waterman-Gotoh-Windowed-Affine, 
Jaro, Jaro-Winkler, Chapman-Length-Deviation, 
Chapman-Mean-Length, QGram-Distance, 
Block-Distance, Cosine Similarity, Dice 
Similarity, Euclidean Distance, Jaccard 
Similarity, Matching Coefficient, Monge-Elkan 
and Overlap-Coefficient. These algorithms have 
been obtained from an API (Application 
Program Interface) SimMetrics library v1.5 for 
.NET 2.03. We obtained 17 features for our MLS 
from these similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order 
to obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as 
the LED between two sentences. Contrary to 
(Tatu et al, 2006), we do not remove the 
punctuation or stop words from the sentences, 
                                                          
3 Copyright (c) 2006 by Chris Parkinson, available in 
http://sourceforge.net/projects/simmetrics/ 
Run1.Bagging Classifier 
Training set from 
SemEval 2013 
Pre-Processing (using Freeling) 
 
Run 3 Bagging  
classifier 
Run 2 Bagging 
classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
 
Jaro QGra
m 
Rel. 
Concept 
. . . 
Tokenizing Lemmatizing POS tagging 
SemEval 
2013 Test 
set 
     Training Process (using Weka) 
111
neither consider different cost for transformation 
operation, and we used all the operations 
(deletion, insertion and substitution).  
The second one is a variant that we named 
Double Levenshtein?s Edit Distance (DLED) 
(see Table 9 for detail). For this algorithm, we 
used LED to measure the distance between the 
phrases, but in order to compare the words, we 
used LED again (Fern?ndez et al, 2012; 
Fern?ndez Orqu?n et al, 2009). 
Another distance we used is an extension of 
LED named Extended Distance (in spanish 
distancia extendida (DEx)) (see (Fern?ndez et 
al., 2012; Fern?ndez Orqu?n et al, 2009) for 
details). This algorithm is an extension of the 
Levenshtein?s algorithm, with which penalties 
are applied by considering what kind of 
transformation (insertion, deletion, substitution, 
or non-operation) and the position it was carried 
out, along with the character involved in the 
operation. In addition to the cost matrixes used 
by Levenshtein?s algorithm, DEx also obtains 
the Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes 
for determining similarity between strings in a 
single iteration. It is worth noting that the 
inclusion of all these penalizations makes the 
DEx algorithm a good candidate for our 
approach.  
In our previous work (Fern?ndez Orqu?n et al, 
2009), DEx demonstrated excellent results when 
it was compared with other distances as 
(Levenshtein, 1965), (Neeedleman and Wunsch, 
1970), (Winkler, 1999). We also used as a 
feature the Minimal Semantic Distances 
(Breadth First Search (BFS)) obtained between 
the most relevant concepts of both sentences. 
The relevant concepts pertain to semantic 
resources ISR-WN (Guti?rrez et al, 2011; 
2010a), as WordNet (Miller et al, 1990a), 
WordNet Affect (Strapparava and Valitutti, 
2004), SUMO (Niles and Pease, 2001) and 
Semantic Classes (Izquierdo et al, 2007). Those 
concepts were obtained after having applied the 
Association Ratio (AR) measure between 
concepts and words over each sentence. (We 
refer reader to (Guti?rrez et al, 2010b) for a 
further description). 
Another attribute obtained by the system was a 
value corresponding with the sum of the smaller 
distances (using QGram-Distance) between the 
words or the lemmas of the phrase one with each 
words of the phrase two. 
As part of the attributes extracted by the 
system, was also the value of the sum of the 
smaller distances (using Levenshtein) among 
stems, chunks and entities of both phrases. 
4.2 Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried 
to align the phrases by its lemmas. If the lemmas 
coincide we look for coincidences among parts-
of-speech4 (POS), and then the phrase is 
realigned using both. If the words do not share 
the same POS, they will not be aligned. To this 
point, we only have taken into account a lexical 
alignment. From now on, we are going to apply 
a semantic variant. After all the process, the non-
aligned words will be analyzed taking into 
account its WordNet?s relations (synonymy, 
hyponymy, hyperonymy, derivationally-related-
form, similar-to, verbal group, entailment and 
cause-to relation); and a set of equivalences like 
abbreviations of months, countries, capitals, days 
and currency. In case of hyperonymy and 
hyponymy relation, words are going to be 
aligned if there is a word in the first sentence 
that is in the same relation (hyperonymy or 
hyponymy) with another one in the second 
sentence. For the relations ?cause-to? and 
?implication? the words will be aligned if there 
is a word in the first sentence that causes or 
implicates another one in the second sentence. 
All the other types of relations will be carried 
out in bidirectional way, that is, there is an 
alignment if a word of the first sentence is a 
synonymous of another one belonging to the 
second one or vice versa. 
Finally, we obtain a value we called alignment 
relation. This value is calculated as ??? =
 ??? / ????. Where ??? is the final 
alignment value, ??? is the number of aligned 
words, and ???? is the number of words of the 
shorter phrase. The  ??? value is also another 
feature for our system. Other extracted attributes 
they are the quantity of aligned words and the 
quantity of not aligned words. The core of the 
alignment is carried out in different ways, which 
                                                          
4 (noun, verb, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, etc.) 
112
are obtained from several attributes.  Each way 
can be compared by: 
? the part-of-speech. 
? the morphology and the part-of-speech. 
? the lemma and the part-of-speech. 
? the morphology, part-of-speech, and 
relationships of WordNet. 
? the lemma, part-of-speech, and 
relationships of WordNet. 
4.3 Semantic Alignment 
This alignment method depends on calculating 
the semantic similarity between sentences based 
on an analysis of the relations, in ISR-WN, of 
the words that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according 
to their POS, creating different groups. 
The distance between two words will be the 
distance, based on WordNet, of the most 
probable sense of each word in the pair, on the 
contrary of our previously system in SemEval 
2012. In that version, we assumed the selected 
sense after apply a double Hungarian Algorithm 
(Kuhn, 1955), for more details  please refer to 
(Fern?ndez et al, 2012). The distance is 
computed according to the equation (1): 
?(?, ?) = ? ? ? ?(?[?], ?[? + 1])?=??=0 ; (1) 
Where ? is the collection of synsets 
corresponding to the minimum path between 
nodes ? and ?, ? is the length of ? subtracting 
one, ? is a function that search the relation 
connecting ? and ? nodes, ? is a weight 
associated to the relation searched by ? (see 
Table 1). 
Relation Weight 
Hyponym, Hypernym 2 
Member_Holonym, Member_Meronym, 
Cause, Entailment 
5 
Similar_To 10 
Antonym 200 
Other relation different to Synonymy 60 
Table 1. Weights applied to WordNet relations. 
Table 1 shows the weights associated to 
WordNet relations between two synsets. 
Let us see the following example: 
? We could take the pair 99 of corpus 
MSRvid (from training set of SemEval-
2013) with a littler transformation in 
order to a better explanation of our 
method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using equation (1), a matrix with the 
distances between all groups of both phrases is 
created (see Table 2). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 2. Distances between groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of 
the first sentence is checked with each element 
of the second sentence, and the rest is marked as 
words that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, 
so the number of non-aligned words is two. 
There is only one perfect match: ?group-group? 
(match with cost=0). The length of the shortest 
sentence is four. The Table 3 shows the results 
of this analysis. 
Number of exact 
coincidence 
Total Distances of 
optimal Matching 
Number of 
non-aligned 
Words 
1 5 2 
Table 3. Features from the analyzed sentences. 
This process has to be repeated for nouns (see 
Table 4), verbs, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, 
digits and date times. On the contrary, the tables 
have to be created only with the similar groups 
of the sentences. Table 4 shows features 
extracted from the analysis of nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 4. Distances between groups of nouns. 
113
Number of 
exact 
coincidence 
Total Distances 
of optimal 
Matching 
Number of non-aligned 
Words 
1 3 0 
Table 5. Feature extracted from analysis of nouns. 
Several attributes are extracted from the pair of 
sentences (see Table 3 and Table 5). Three 
attributes considering only verbs, only nouns, 
only adjectives, only adverbs, only prepositions, 
only conjunctions, only pronouns, only 
determinants, only modifiers, only digits, and 
only date times. These attributes are:  
? Number of exact coincidences 
? Total distance of matching 
? Number of words that do not match 
Many groups have particular features 
according to their parts-of-speech. The group of 
the nouns has one more feature that indicates if 
the two phrases have the same number (plural or 
singular). For this feature, we take the average of 
the number of each noun in the phrase like a 
number of the phrase.  
For the group of adjectives we added a feature 
indicating the distance between the nouns that 
modify it from the aligned adjectives, 
respectively.  
For the verbs, we search the nouns that precede 
it, and the nouns that are next of the verb, and 
we define two groups. We calculated the 
distance to align each group with every pair of 
aligned verbs. The verbs have other feature that 
specifies if all verbs are in the same verbal time.  
With the adverbs, we search the verb that is 
modified by it, and we calculate their distance 
from all alignment pairs.  
With the determinants and the adverbs we 
detect if any of the alignment pairs are 
expressing negations (like don?t, or do not) in 
both cases or not. Finally, we determine if the 
two phrases have the same principal action. For 
all this new features, we aid with Freeling tool. 
As a result, we finally obtain 42 attributes from 
this alignment method. It is important to remark 
that this alignment process searches to solve, for 
each word from the rows (see Table 4) it has a 
respectively word from the columns. 
4.4 Description of the alignment feature 
From the alignment process, we extract different 
features that help us a better result of our MLS. 
Table 6 shows the group of features with lexical 
and semantic support, based on WordNet 
relation (named F1). Each of they were named 
with a prefix, a hyphen and a suffix. Table 7 
describes the meaning of every prefix, and Table 
8 shows the meaning of the suffixes. 
Features 
CPA_FCG, CPNA_FCG, SIM_FCG, CPA_LCG, 
CPNA_LCG, SIM_LCG, CPA_FCGR, 
CPNA_FCGR, SIM_FCGR, CPA_LCGR, 
CPNA_LCGR, SIM_LCGR 
Table 6. F1. Semantic feature group. 
Prefixes Descriptions 
CPA Number of aligned words. 
CPNA Number of non-aligned words. 
SIM Similarity 
Table 7. Meaning of each prefixes. 
Prefixes Compared words for? 
FCG Morphology and POS 
LCG Lemma and POS 
FCGR Morphology, POS and WordNet relation. 
LCGR Lemma, POS and WordNet relation. 
Table 8. Suffixes for describe each type of alignment. 
Features Descriptions 
LevForma Levenshtein Distance between two 
phrases comparing words by 
morphology 
LevLema The same as above, but now 
comparing by lemma. 
LevDoble Idem, but comparing again by 
Levenshtein and accepting words 
match if the distance is ? 2. 
DEx Extended Distance 
NormLevF, 
NormLevL 
Normalized forms of LevForma and 
LevLema. 
Table 9. F2. Lexical alignment measures. 
Features 
NWunch, SWaterman, SWGotoh, SWGAffine, Jaro, 
JaroW, CLDeviation, CMLength, QGramD, BlockD, 
CosineS, DiceS, EuclideanD, JaccardS, MaCoef, 
MongeElkan, OverlapCoef. 
Table 10. Lexical Measure from SimMetrics library. 
Features Descriptions 
AxAQGD_L All against all applying QGramD 
and comparing by lemmas of the 
words. 
AxAQGD_F Same as above, but applying 
QGramD and comparing by 
morphology. 
AxAQGD_LF Idem, not only comparing by lemma 
but also by morphology. 
AxALev_LF All against all applying Levenhstein 
114
comparing by morphology and 
lemmas. 
AxA_Stems Idem, but applying Levenhstein 
comparing by the stems of the 
words. 
Table 11. Aligning all against all. 
Other features we extracted were obtained 
from the following similarity measures (named 
F2) (see Table 9 for detail). 
We used another group named F3, with lexical 
measure extracted from SimMetric library (see 
Table 10 for detail). 
Finally we used a group of five feature (named 
F4), extracted from all against all alignment (see 
Table 11 for detail). 
4.5 Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
as a training corpus. Using ten-fold cross 
validation with the classifier mentioned in 
section 3 (experimentally selected). 
As we can see in Table 12, the attributes 
corresponding with the Test 1 (only lexical 
attributes) obtain 0.7534 of correlation. On the 
other side, the attributes of the Test 2 (lexical 
features with semantic support) obtain 0.7549 of 
correlation, and all features obtain 0.7987. Being 
demonstrated the necessity to tackle the problem 
of the similarity from a multidimensional point 
of view (see Test 3 in the Table 12). 
Features 
Correlation on the training data of SemEval-
2013 
Test 1 Test 2 Test 3 
F1 
 0.7549 
0.7987 
F2 
F3 0.7534  
F4   
Table 12. Features influence. Gray cells mean 
features are not taking into account. 
5 Result and discussion 
Semantic Textual Similarity task of SemEval-
2013 offered two official measures to rank the 
systems5: Mean- the main evaluation value, 
Rank- gives the rank of the submission as 
ordered by the "mean" result. 
                                                          
5http://ixa2.si.ehu.es/sts/index.php?option=com_content&vi
ew=article&id=53&Itemid=61 
Test data for the core test datasets, coming 
from the following: 
Corpus Description 
Headlineas: news headlines mined from several news 
sources by European Media Monitor 
using the RSS feed. 
OnWN: mapping of lexical resources OnWN. The 
sentences are sense definitions from 
WordNet and OntoNotes. 
FNWN: the sentences are sense definitions from 
WordNet and FrameNet. 
SMT: SMT dataset comes from DARPA GALE 
HTER and HyTER. One sentence is a 
MT output and the other is a reference 
translation where a reference is generated 
based on human post editing. 
Table 13. Test Core Datasets. 
Using these measures, our second run (Run 2) 
obtained the best results (see Table 14). As we 
can see in Table 14, our lexical run has obtained 
our best result, given at the same time worth 
result in our other runs. This demonstrates that 
tackling this problem with combining multiple 
lexical similarity measure produce better results 
in concordance to this specific test corpora. 
To explain Table 14 we present following 
descriptions: caption in top row mean: 1- 
Headlines, 2- OnWN, 3- FNWN, 4- SMT and 5- 
mean. 
Run 1 R 2 R 3 R 4 R 5 R 
1 0.5841 60 0.4847 54 0.2917 52 0.2855 66 0.4352 58 
2 0.6168 55 0.5557 39 0.3045 50 0.3407 28 0.4833 44 
3 0.3846 85 0.1342 88 -0.0065 85 0.2736 72 0.2523 87 
Table 14. Official SemEval-2013 results over test 
datasets. Ranking (R). 
The Run 1 is our main run, which contains the 
junction of all attributes (lexical and semantic 
attributes).  Table 14 shows the results of all the 
runs for a different corpus from test phase. As 
we can see, Run 1 did not obtain the best results 
among our runs. 
Otherwise, Run 3 uses more semantic analysis 
than Run 2, from this; Run 3 should get better 
results than reached over the corpus of FNWN, 
because this corpus is extracted from FrameNet 
corpus (Baker et al, 1998) (a semantic network). 
FNWN provides examples with high semantic 
content than lexical. 
Run 3 obtained a correlation coefficient of 
0.8137 for all training corpus of SemEval 2013, 
115
while Run 2 and Run 1 obtained 0.7976 and 
0.8345 respectively with the same classifier 
(Bagging using REPTree, and cross validation 
with ten-folds). These results present a 
contradiction between test and train evaluation. 
We think it is consequence of some obstacles 
present in test corpora, for example:  
In headlines corpus there are great quantity of 
entities, acronyms and gentilics that we not take 
into account in our system. 
The corpus FNWN presents a non-balance 
according to the length of the phrases. 
In OnWN -test corpus-, we believe that some 
evaluations are not adequate in correspondence 
with the training corpus. For example, in line 7 
the goal proposed was 0.6, however both phrases 
are semantically similar. The phrases are: 
? the act of lifting something 
? the act of climbing something. 
We think that 0.6 are not a correct evaluation 
for this example. Our system result, for this 
particular case, was 4.794 for Run 3, and 3.814 
for Run 2, finally 3.695 for Run 1. 
6 Conclusion and future works 
This paper have introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional 
interpretation of a text. 
As mentioned in section 3 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 14 
that all runs obtained encouraging results. Our 
best run was situated at 44th position of 90 runs 
of the ranking of SemEval-2013.  Table 12 and 
Table 14 show the reached positions for the three 
different runs and the ranking according to the 
rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment and Semantic Alignment. 
We have conducted the semantic features 
extraction in a multidimensional context using 
the resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWordNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we 
show that this approach can be used to correctly 
classify several examples from the STS task of 
SemEval-2013. Compared with the best run of 
the ranking (UMBC_EBIQUITY- ParingWords) 
(see Table 15) our main run has very close 
results in headlines (1), and SMT (4) core test 
datasets. 
Run 1 2 3 4 5 6 
(First) 0.7642 0.7529 0.5818 0.3804 0.6181 1 
(Our) 
RUN 2 
0.6168 0.5557 0.3045 0.3407 0.4833 44 
Table 15. Comparison with best run (SemEval 2013). 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think 
that with this improvement we can increase the 
results obtained with texts like those in OnWN 
test set. 
6.1 Team Collaboration 
Is important to remark that our team has been 
working up in collaboration with INAOE 
(Instituto Nacional de Astrof?sica, ?ptica y 
Electr?nica) and LIPN (Laboratoire 
d'Informatique de Paris-Nord), Universit? Paris 
13 universities, in order to encourage the 
knowledge interchange and open shared 
technology. Supporting this collaboration, 
INAOE-UPV (Instituto Nacional de Astrof?sica, 
?ptica y Electr?nica and Universitat Polit?cnica 
de Val?ncia) team, in concrete in INAOE-UPV-
run 3 has used our semantic distances for nouns, 
adjectives, verbs and adverbs, as well as lexical 
attributes like LevDoble, NormLevF, NormLevL 
and Ext (see influence of these attributes in 
Table 12). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
116
Reference 
Agirre, E.; D. Cer; M. Diab and W. Guo. *SEM 2013 
Shared Task: Semantic Textual Similarity 
including a Pilot on Typed-Similarity. *SEM 
2013: The Second Joint Conference on Lexical and 
Computational Semantics, Association for 
Computational Linguistics, 2013.  
Aguirre, E. and D. Cerd. SemEval 2012 Task 6:A 
Pilot on Semantic Textual Similarity. First Join 
Conference on Lexical and Computational 
Semantic (*SEM), Montr?al, Canada, Association 
for Computational Linguistics., 2012. 385-393 p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. 
Baker, C. F.; C. J. Fillmore and J. B. Lowe. The 
berkeley framenet project. Proceedings of the 17th 
international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, 1998. 86-90 p.  
Banea, C.; S. Hassan; M. Mohler and R. Mihalcea. 
UNT:A Supervised Synergistic Approach to 
SemanticText Similarity. First Joint Conference on 
Lexical and Computational Semantics (*SEM), 
Montr?al. Canada, Association for Computational 
Linguistics, 2012. 635?642 p.  
Breiman, L. Bagging predictors Machine learning, 
1996, 24(2): 123-140. 
Corley, C. and R. Mihalcea. Measuring the Semantic 
Similarity of Texts, Association for Computational 
Linguistic. Proceedings of the ACL Work shop on 
Empirical Modeling of Semantic Equivalence and 
Entailment, pages 13?18, June 2005. 
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; 
A. Gonz?lez; R. Estrada; Y. Casta?eda; S. 
V?zquez; A. Montoyo and R. Mu?oz. 
UMCC_DLSI: Multidimensional Lexical-
Semantic Textual Similarity. {*SEM 2012}: The 
First Joint Conference on Lexical and 
Computational Semantics -- Volume 1: 
Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Fern?ndez Orqu?n, A. C.; J. D?az Blanco; A. Fundora 
Rolo and R. Mu?oz Guillena. Un algoritmo para la 
extracci?n de caracter?sticas lexicogr?ficas en la 
comparaci?n de palabras. IV Convenci?n 
Cient?fica Internacional CIUM, Matanzas, Cuba, 
2009.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010a. 161-168 p. 1135-
5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschberg, D. S. Algorithms for the longest common 
subsequence problem J. ACM, 1977, 24: 664?675. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Kuhn, H. W. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly, 1955, 2: 83?97. 
Levenshtein, V. I. Binary codes capable of correcting 
spurious insertions and deletions of ones. Problems 
of information Transmission. 1965. pp. 8-17 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller. Five papers on WordNet. 
Princenton University, Cognositive Science 
Laboratory, 1990a. 
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-
line Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990b. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Neeedleman, S. and C. Wunsch A general method 
applicable to the search for similarities in the 
amino acid sequence of two proteins Mol. Biol, 
1970, 48(443): 453. 
Niles, I. and A. Pease. Origins of the IEEE Standard 
Upper Ontology. Working Notes of the IJCAI-
2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA., 2001. 
117
?ari?, F.; G. Glava?; Mladenkaran; J. ?najder and B. 
D. Basi?. TakeLab: Systems for Measuring 
Semantic Text Similarity.  Montr?al, Canada, First 
Join Conference on Lexical and Computational 
Semantic (*SEM), pages 385-393. Association for 
Computational Linguistics., 2012.  
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of 
the 4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p. 
Tatu, M.; B. Iles; J. Slavick; N. Adrian and D. 
Moldovan. COGEX at the Second Recognizing 
Textual Entailment Challenge. Proceedings of the 
Second PASCAL Recognising Textual Entailment 
Challenge Workshop, Venice, Italy, 2006. 104-109 
p.  
Werning, M.; E. Machery and G. Schurz. The 
Compositionality of Meaning and Content, 
Volume 1: Foundational issues. ontos verlag 
[Distributed in] North and South America by 
Transaction Books, 2005. p. Linguistics & 
philosophy, Bd. 1. 3-937202-52-8. 
Winkler, W. The state of record linkage and current 
research problems. Technical Report, Statistical 
Research Division, U.S, Census Bureau, 1999. 
 
 
118
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 93?97, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(EPS): Paraphrases Detection Based on Semantic 
Distance 
 
H?ctor D?vila, Antonio Fern?ndez Orqu?n, 
Alexander Ch?vez, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba. 
{hector.davila, tony, 
alexander.chavez, yoan.gutierrez, 
armando.collazo, 
jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera 
de San Vicente S/N Alicante, Spain. 
{montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI-(EPS) system, which 
participated in the first Evaluating Phrasal 
Semantics of SemEval-2013. Our supervised 
system uses different kinds of semantic 
features to train a bagging classifier used to 
select the correct similarity option. Related to 
the different features we can highlight the 
resource WordNet used to extract semantic 
relations among words and the use of different 
algorithms to establish semantic similarities. 
Our system obtains promising results with a 
precision value around 78% for the English 
corpus and 71.84% for the Italian corpus. 
1 Introduction 
It is well known finding words similarity, even 
when it is lexical or semantic can improve 
entailment recognition and paraphrase 
identification; and ultimately lead to improvements 
in a wide range of applications in Natural 
Language Processing (NLP). Several areas like 
question answering, query expansion, information 
retrieval, and many others, depend on phrasal 
semantics (PS). PS, is concerned with how the 
meaning of a sentence is composed both from the 
meaning of the constituent words, and from extra 
meaning contained within the structural 
organization of the sentence itself (Dominey, 
2005). 
The aim of SemEval 2013 competition is also 
discovering similarity, specifically in Evaluating 
Phrasal Semantics (EPS). The goal of this task is to 
evaluate how well systems can judge the semantic 
similarity of a word and a short sequence of words. 
That is, given a set of pairs of this type; classify it 
on negative (if the meaning of the word is 
semantically different to the meaning of the 
sequence) or positive (if the meaning of the 
sequence, as a whole, is semantically close to the 
meaning of the word).  
Based on this, we developed a system capable to 
detect if two phrases are semantically close. 
The rest of this paper, specifically section 2 is a 
brief Related Work. Section 3 describes the system 
architecture and our run. Continuing with section 4 
we describe the training phase. Following that, 
section 5 presents the results and discussion for our 
Machine Learning System. Finally we conclude 
and propose our future works (Section 6). 
2 Related Work 
There have been many WordNet-based similarity 
measures, among other highlights the work of 
researchers like (Budanitsky and Hirst, 2006; 
Leacock and Chodorow, 1998; Mihalcea et al, 
2006; Richardson et al, 1994). 
On the other hand, WordNet::Similarity1 
(Pedersen et al, 2004) has been used by other 
researchers in an interesting array of domains. 
WordNet::Similarity implements measures of 
similarity and relatedness between a pair of 
concepts (or synsets2) based on the structure and 
content of WordNet. According to (Pedersen et al, 
2004), three of the six measures of similarity are 
based on the information content of the least 
                                                     
1http://sourceforge.net/projects/wn-similarity/ 
2 A group of English words into sets of synonyms. 
93
common subsumer (LCS). These measures include 
res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang 
and Conrath, 1997). 
Pursuant to Pedersen, there are three other 
similarity measures based on path lengths between 
a pair of concepts: lch (Leacock and Chodorow, 
1998), wup (Wu and Palmer, 1994), and path. 
Our proposal differs from those of 
WordNet::Similarity and other measures of 
similarity in the way we selected the relevant 
WordNet relations (see section 3.2 for detail). 
Unlike others, our measure assign weight to 
WordNet relations (any we consider relevant) 
depending to the place they occupy in the 
minimum path and the previously visited relations. 
Besides these, the novelty of our approach is 
using the weights as a function of semantic 
relations in a minimal distance path and also the 
method we used to arrive to those weight functions 
or rules. 
3 System Architecture and description of 
the run 
As we can see in Figure 1 our run begin with the 
pre-processing of SemEval 2013?s training set. 
Every sentence pair is tokenized, lemmatized and 
POS-tagged using Freeling 2.2 tool (Atserias et al, 
2006). Afterwards, several methods and algorithms 
are applied in order to extract all features for our 
Machine Learning System (MLS). The system 
trains the classifier using a model based on 
bagging (using JRip3). The training corpus has 
been provided by SemEval-2013 competition, in 
concrete by the EPS task. As a result, we obtain a 
trained model capable to detect if one phrase 
implies other. Finally, we test our system with the 
SemEval 2013 test set (see Table 2 with the results 
of our run). The following section describes the 
features extraction process. 
3.1 Description of the features used in the 
Machine Learning System 
In order to detect entailment between a pair of 
phrases, we developed an algorithm that searches a 
semantic distance, according to WordNet (Miller et 
al., 1990), between each word in the first phrase 
with each one in the second phrase. 
We used four features which intend to measure 
the level of proximity between both sentences: 
                                                     
3 JRip is an inference and rules-based learner. 
? The minimum distance to align the first 
phrase with the second (MinDist). See section 
3.2 for details. 
? The maximal distance to align the first phrase 
with the second (MaxDist). 
? The average of all distances results to align 
the first phrase with the second one. 
(AverageDistance). 
? The absolute relative error of all distances 
results to align the first phrase with the 
second respect to the average of them. 
 
Figure 1. System Architecture. 
Other features included are the most frequent 
relations contained in the shorted path of the 
minimum distance; result to align the first phrase 
with the second one. Following table shows the 
relations selected as most frequent. 
A weight was added to each of them, according 
to the place it occupy in the shortest path between 
two synsets. The shortest path was calculated using 
Breadth -First-Search algorithm (BFS) (Cormen et 
al., 2001). 
In addition, there is one feature that takes into 
account any other relationship that is not 
previously considered. 
Finally, as a result we obtain 22 features from 
this alignment method. 
Semeval 2013 test 
set
?
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Run 1
Bagging Classifier (JRip)
Feature Extraction
MinDistance MaxDistance error ?
Training set from 
Semeval 2013
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Feature Extraction
MinDistance MaxDistance error
Supervised Model
Training process (using Weka)
Bagging Classifier (JRip)
Paraphrases Detection
94
Relation Weight (? function) 
Antonym 1000 
Synonym 0 
Hyponym/ Hypernym 
100 if exist an antonym 
before, 30 if exist other 
relation before (except 
synonym, hyponym, 
hypernym), 5 otherwise. 
Meber_Holonym/ 
PartHolonym 
100 if exist an antonym 
before, 20 if exist a 
hyponym or a hypernym,10 
otherwise. 
Cause/ Entailment 
100 if exist an antonym 
before, 2 otherwise. 
Similar_To 
100 if exist an antonym 
before, 3 otherwise. 
Attribute 
100 if exist an antonym 
before, 8 otherwise. 
Also_See 
100 if exist an antonym 
before, 10 otherwise. 
Derivationaly_Related_Form 
100 if exist an antonym 
before, 5 otherwise. 
Domain_Of_Synset_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Domain_Of_Synset_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Member_Of_Domain_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Member_Of_Domain_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Other 100 
Table 1. Most frequents relations with their weight. 
3.2 Semantic Distance 
As aforementioned, our distance depends on 
calculating the similarity between sentences, based 
on the analysis of WordNet relations, and we only 
took into account the most frequent ones. When 
searching the shortest path between two WordNet 
synsets, frequents relations were considered the 
ones extracted according to the analysis made in 
the training corpus, provided by SemEval-2013. 
The distance between two synsets is calculated 
with the relations found; and simply it is the sum 
of the weights assigned to each connection. 
????????(?, ?) =  ????????(??, ??), ? (?, ?) (1) 
????????(?, ?) = ???(?? , ??), ?(?, ?) (2) 
???(??; ??) = ? ?(???(?[?], ?[? + 1]))
?=?
?=0
 (3) 
? = ???(??; ??) (4) 
Where ? and ? represents the i-th and j-th sense of 
the word; P and Q represents words collections; ?? 
is the X-th word of ?; ?? is the Y-th word of ?; 
???????? obtains a value that represents a 
minimal semantic distance across WordNet (Miller 
et al, 2006) resource (this resource is involved into 
the integrator resource, ISR-WN (Guti?rrez et al, 
2011a; 2010a); ????????  the minimal semantic 
distance between two words; ??? represents the 
minimal semantic distance between two senses 
collections; ? is a collection of synsets that 
represents the minimal path between two synsets 
using BFS; ??? obtains semantic relation types 
between two synsets; W is a functions that apply 
the rules described in Table 1. The maximum and 
average distance is calculated in a similar fashion 
but using the maximum and average instead of the 
minimum. 
3.3 Semantic Alignment 
First, the two sentences are pre-processed with 
Freeling 2.2 and the words are classified according 
to their parts-of-speech. Then, all senses of every 
word are taken and treated as a group. Distance 
between two groups will be the minimal distance 
(described in 3.1) between senses of any pair of 
words belonging to the group. 
In the example of Figure 2, Dist=280 is selected 
for the pair ?Balance-Culture? (minimal cost).  
Following the explanation on section 3.1 we 
extract the features guided to measure the level of 
proximity between both sentences. 
 
Figure 2. Distance between ?Balance? and ?Culture?. 
A maximum and average distance is calculated in a 
similar fashion, but using the maximum and 
average instead of the minimum. 
4 Description of the training phase 
For the training process, we used a supervised 
learning framework (based on Weka4), including 
all the training set (positive and negative instances) 
as a training corpus. We conduct several 
experiments in order to select the correct classifier, 
the best result being obtained with a model based 
on bagging (using JRip algorithm). Finally, we 
used 10-fold cross validation technique with the 
selected classifier, obtaining a classification value 
of 73.21%. 
                                                     
4 http://prdownloads.sourceforge.net/weka/ 
L mma: Balance
Sense 1
Sense 2
Lemma: Culture
Sense 1
Sense 2
3350
1030 280
880
Dist=280
95
5 Results and discussion 
EPS task of SemEval-2013 offered many official 
measures to rank the systems. Some of them are 
the following: 
o F-Measure (FM): Correct Response (CR), 
Instances correctly classified, True positives 
(TP), Instances correctly classified as 
positive. False Positives (FP), Instances 
incorrectly classified as positive, True 
Negatives (TN), Instances correctly 
classified as negative, False Negatives (FN), 
Instances incorrectly classified as negative. 
Corpus FM CR TP FP TN FN 
English 0.6892 2826 1198 325 1628 755 
Italian 0.6396 574 245 96 329 180 
Table 2. Official SemEval 2013 results. 
The behavior of our system, for English and 
Italian corpus is shown in Table 2. 
The only thing that changes to process the 
Italian corpus is that Freeling is used as input to 
identify Italian words and it returns the English 
WN synsets. The process continues in the same 
way as English. 
Figure 3: Semantic Distance distribution between 
negative and positive instances.  
As shown in Table 2, our main drawback is to 
classify positive instances. Sometimes, the distance 
between positive phrases is very far. This is due to 
the relations found in the minimum path are very 
similar to the one found in other pairs of negatives 
instances; this can be the cause of our MLS 
classifies them as negatives (see Figure 3). 
Figure 3 shows a distributional graphics that 
take a sample of 200 negative and positive 
instances. The graphics illustrate how close to zero 
value the positive instances are, while the 
negatives are far away from this value. However, 
in the approximate range between 80 and 200, we 
can see values of positive and negative instances 
positioning together. This can be the cause that our 
MLS misclassified some positive instances as 
negative. 
6 Conclusion and future work 
This paper introduced a new framework for EPS, 
which depends on the extraction of several features 
from WordNet relations. We have conducted the 
semantic features extraction in a multidimensional 
context using the resource ISR-WN(Guti?rrez et 
al., 2010a). 
Our semantic distance provides an appealing 
approach for dealing with phrasal detection based 
on WordNet relation. Our team reached the sixth 
position of ten runs for English corpus, with a 
small difference of 0.07 points compared to the 
best results with respect to accuracy parameter. 
Despite the problems caused by poorly selected 
positive instances, our distance (labeled as Our) 
obtained very similar results to those obtained by 
the best team (labeled as First5), which indicates 
that our work is well underway (see Table 3 for 
details). 
Team accuracy recall precision 
First 0.802611 0.751664 0.836944128 
Our 0.723502 0.613415 0.786605384 
Table 3. Comparative results (English corpus). 
It is important to remark that our system has 
been the only competitor to evaluate Italian texts. 
It has been possible due to our system include 
Freeling in the preprocessing stage. 
Our future work will aim to resolve instances 
misclassified by our algorithm. In addition, we will 
introduce lexical substitutions (synonyms) to 
expand the corpus, we will also apply conceptual 
semantic similarity using relevant semantic trees 
(Guti?rrez et al, 2010b; Guti?rrez et al, 2011b). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
                                                     
5 christian_wartena. Team HsH. 
0
500
1000
Semantic Distance Distribution
Positive Instances Negative Instances
96
semantic services in an open-source NLP library. 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC?06), 
2006. 48-55 p.  
Budanitsky, A. and G. Hirst Evaluating wordnet-based 
measures of lexical semantic relatedness 
Computational Linguistics, 2006, 32(1): 13-47. 
Cormen, T. H.; C. E. Leiserson; R. L. Rivest and C. 
Stein. Introduction to algorithms. MIT press, 2001. 
0262032937. 
Dominey, P. F. Aspects of descriptive, referential, and 
information structure in phrasal semantics: A 
construction-based model Interaction Studies, 2005, 
6(2): 287-310. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, 
RANLP 2011 Organising Committee, 2011b. 233--
239 p.  
Jiang, J. J. and D. W. Conrath Semantic similarity based 
on corpus statistics and lexical taxonomy arXiv 
preprint cmp-lg/9709008, 1997. 
Leacock, C. and M. Chodorow Combining local context 
and WordNet similarity for word sense identification 
WordNet: An electronic lexical database, 1998, 
49(2): 265-283. 
Lin, D. An information-theoretic definition of 
similarity. Proceedings of the 15th international 
conference on Machine Learning, San Francisco, 
1998. 296-304 p.  
Mihalcea, R.; C. Corley and C. Strapparava. Corpus-
based and knowledge-based measures of text 
semantic similarity. Proceedings of the national 
conference on artificial intelligence, Menlo Park, 
CA; Cambridge, MA; London; AAAI Press; MIT 
Press; 1999, 2006. 775 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Miller, G. A.; C. Fellbaum; R. Tengi; P. Wakefield; H. 
Langone and B. R. Haskell. WordNet a lexical 
database for the English language. Cognitive Science 
Laboratory Princeton University 2006. 
Pedersen, T.; S. Patwardhan and J. Michelizzi. 
WordNet:: Similarity: measuring the relatedness of 
concepts. Demonstration Papers at HLT-NAACL 
2004, Association for Computational Linguistics, 
2004. 38-41 p.  
Resnik, P. Using information content to evaluate 
semantic similarity in a taxonomy arXiv preprint 
cmp-lg/9511007, 1995. 
Richardson, R.; A. F. Smeaton and J. Murphy. Using 
WordNet as a knowledge base for measuring 
semantic similarity between words, Technical Report 
Working Paper CA-1294, School of Computer 
Applications, Dublin City University, 1994. 
Wu, Z. and M. Palmer. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting on 
Association for Computational Linguistics, 
Association for Computational Linguistics, 1994. 
133-138 p.  
 
 
97
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 636?643, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Semantic and Lexical features for detection and 
classification Drugs in biomedical texts 
 
 
Armando Collazo, Alberto 
Ceballo, Dennys D. Puig, Yoan 
Guti?rrez, Jos? I. Abreu, Roger 
P?rez 
Antonio Fern?ndez 
Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
{armando.collazo, dennys.puig, 
yoan.gutierrez, jose.abreu, 
roger.perez}@umcc.cu, 
alberto.ceballo@infonet.umcc.cu 
DLSI, University of Alicante 
Carretera de San Vicente 
S/N Alicante, Spain 
antonybr@yahoo.com, 
{montoyo, 
rafael}@dlsi.ua.es 
Independent Consultant 
USA 
info@franccamara.com 
 
Abstract 
In this paper we describe UMCC_DLSI-
(DDI) system which attempts to detect and 
classify drug entities in biomedical texts. 
We discuss the use of semantic class and 
words relevant domain, extracted with ISR-
WN (Integration of Semantic Resources 
based on WordNet) resource to obtain our 
goal. Following this approach our system 
obtained an F-Measure of 27.5% in the 
DDIExtraction 2013 (SemEval 2013 task 
9). 
1. Introduction 
To understand biological processes, we must 
clarify how some substances interact with our 
body and one to each other. One of these 
important relations is the drug-drug interactions 
(DDIs). They occur when one drug interacts 
with another or when it affects the level, or 
activity of another drug. DDIs can change the 
way medications act in the body, they can cause 
powerful, dangerous and unexpected side 
effects, and also they can make the medications 
less effective. 
As suggested by (Segura-Bedmar et al, 2011), 
?...the detection of DDI is an important research 
area in patient safety since these interactions 
can become very dangerous and increase health 
care costs?. More recent studies (Percha and 
Altman, 2013) reports that ??Recent estimates 
indicate that DDIs cause nearly 74000 
emergency room visits and 195000 
hospitalizations each year in the USA?.  
But, on the other hand, there is an expansion in 
the volume of published biomedical research, 
and therefore the underlying biomedical 
knowledge base (Cohen and Hersh, 2005). 
Unfortunately, as often happens, this 
information is unstructured or in the best case 
scenario semi-structured. 
As we can see in (Tari et al, 2010), ?Clinical 
support tools often provide comprehensive lists 
of DDIs, but they usually lack the supporting 
scientific evidences and different tools can 
return inconsistent results?.  
Although, as mentioned (Segura-Bedmar et al, 
2011) ?there are different databases supporting 
healthcare professionals in the detection of DDI, 
these databases are rarely complete, since their 
update periods can reach up to three years?. In 
addition to these and other difficulties, the great 
amount of drug interactions are frequently 
reported in journals of clinical pharmacology 
and technical reports, due to this fact, medical 
literature becomes most effective source for 
detection of DDI. Thereby, the management of 
DDI is a critical issue due to the overwhelming 
amount of information available on them 
(Segura-Bedmar et al, 2011). 
636
1.1. Task Description 
With the aim of reducing the time the health care 
professionals invest on reviewing the literature, 
we present a feature-based system for drug 
detection and classification in biomedical texts. 
The DDIExtraction2013 task was divided into 
two subtasks: Recognition and classification of 
drug names (Task 9.1) and Extraction of drug-
drug interactions (Task 9.2). Our system was 
developed to be presented in the Task 9.1. In this 
case, participants were to detect and classify the 
drugs that were present in the test data set which 
was a set of sentences related to the biomedical 
domain obtained from a segmented corpus. The 
output consisted of a list mentioning all the 
detected drugs with information concerning the 
sentence it was detected from as well as its 
offset in that sentence (the position of the first 
and the last character of the drug in the sentence, 
0 being the first character of a sentence). Also 
the type of the drug should have been provided. 
As to the type, participants had to classify 
entities in one of these four groups1: 
? Drug: any chemical agent used for 
treatment, cure, prevention or diagnose of 
diseases, which have been approved for 
human usage. 
? Brand: any drug which firstly have been 
developed by a pharmaceutical company. 
? Group: any term in the text designating a 
relation among pharmaceutical substances. 
? No-Human: any chemical agent which 
affects the human organism. An active 
substance non-approved for human usage 
as medication. 
In the next section of the paper, we present 
related works (Section 2). In Section 3, we 
discuss the feature-based system we propose. 
Evaluation results are discussed in Section 4. 
Finally, we conclude and propose future work 
(Section 5). 
2. Related Work 
One of the most important workshops on the 
domain of Bioinformatics has been BioCreAtIve 
(Critical Assessment of Information Extraction 
                                                     
1 http://www.cs.york.ac.uk/semeval-2013/task9 
in Biology) (Hirschman et al, 2005). This 
workshop has improved greatly the Information 
Extraction techniques applied to the biological 
domain. The goal of the first BioCreAtIvE 
challenge was to provide a set of common 
evaluation tasks to assess the state-of-the-art for 
text mining applied to biological problems. The 
workshop was held in Granada, Spain on March 
28-31, 2004. 
According to Hirschman, the first 
BioCreAtIvE assessment achieved a high level 
of international participation (27 groups from 10 
countries). The best system results for a basic 
task (gene name finding and normalization), 
where a balanced 80% precision/recall or better, 
which potentially makes them suitable for real 
applications in biology. The results for the 
advanced task (functional annotation from free 
text) were significantly lower, demonstrating the 
current limitations of text-mining approaches. 
The greatest contribution of BioCreAtIve was 
the creation and release of training and test data 
sets for both tasks (Hirschman et al, 2005). 
One of the seminal works where the issue of 
drug detection was mentioned was (Gr?nroos et 
al., 1995). Authors argue the problem can be 
solved by using a computerized information 
system, which includes medication data of 
individual patients as well as information about 
non-therapeutic drug-effects. Also, they suggest 
a computerized information system to build 
decision support modules that, automatically 
give alarms or alerts of important drug effects 
other than therapeutic effects. If these warnings 
concern laboratory tests, they would be checked 
by a laboratory physician and only those with 
clinical significance would be sent to clinicians. 
Here, it is important to note the appearance of 
the knowledgebase DrugBank 2 . Since its first 
release in 2006 (Wishart et al, 2008) it has been 
widely used to facilitate in silico drug target 
discovery, drug design, drug docking or 
screening, drug metabolism prediction, drug 
interaction prediction and general 
pharmaceutical education. DrugBank has also 
significantly improved the power and simplicity 
of its structure query and text query searches. 
                                                     
2 http://redpoll.pharmacy.ualberta.ca/drugbank/ 
637
Later on, in 2010 Tari propose an approach 
that integrates text mining and automated 
reasoning to derive DDIs (Tari et al, 2010). 
Through the extraction of various facts of drug 
metabolism, they extract, not only the explicitly 
DDIs mentioned in text, but also the potential 
interactions that can be inferred by reasoning. 
This approach was able to find several potential 
DDIs that are not present in DrugBank. This 
analysis revealed that 81.3% of these 
interactions are determined to be correct. 
On the DDIExtraction 2011 (Segura-Bedmar et 
al., 2011) workshop (First Challenge Task on 
Drug-Drug Interaction Extraction) the best 
performance was achieved by the team WBI 
from Humboldt-Universitat, Berlin. This team 
combined several kernels and a case-based 
reasoning (CBR) system, using a voting 
approach. 
In this workshop relation extraction was 
frequently and successfully addressed by 
machine learning methods. Some of the more 
common used features were co-occurrences, 
character n-grams, Maximal Frequent 
Sequences, bag-of-words, keywords, etc. 
Another used technique is distant supervision. 
The first system evaluating distant supervision 
for drug-drug interaction was presented in 
(Bobi? et al, 2012), they have proposed a 
constraint to increase the quality of data used for 
training based on the assumption that no self-
interaction of real-world objects are described in 
sentences. In addition, they merge information 
from IntAct and the University of Kansas 
Proteomics Service (KUPS) database in order to 
detect frequent exceptions from the distant 
supervision assumption and make use of more 
data sources. 
Another important work related to Biomedical 
Natural Language Processing was BioNLP 
(Bj?rne et al, 2011) it is an application of 
natural language processing methods to analyze 
textual data on biology and medicine, often 
research articles. They argue that information 
extraction techniques can be used to mine large 
text datasets for relevant information, such as 
relations between specific types of entities. 
Inspired in the previews works the system we 
propose makes use of machine learning methods 
too, using some of the common features 
described above, such as the n-grams and 
keywords and co-occurrences, but we also add 
some semantic information to enrich those 
features. 
3. System Description  
As it has been mentioned before, the system was 
developed to detect and classify drugs in 
biomedical texts, so the process is performed in 
two main phases:  
? drug detection. 
? drug classification. 
Both phases are determined by the following 
stages, described in Figure 1: 
I. Preprocessing 
II. Feature extraction 
III. Classification 
 
 
 
 
 
 
 
Figure 1. Walkthrough system process. 
Given a biomedical sentence, the system 
obtains the lemmas and POS-tag of every token 
Classification 
 
Pre-Processing (using Freeling 2.2) 
Training set from Semeval 2013 
DDIExtraction2013 task 
Run(3) 
MFSC?s 
CSC MF 2-grams, 3-grams 
UC 
UCA 
MWord 
N 
 
   
I 
II 
III 
Tokenizing 
Run(1) Run(2) 
CSC - All 
EMFS
C 
DRD 
CD 
GC 
InRe 
WNum 
Feature Extraction 
Lemmatizing 
 
POS tagging 
 
 
   
 
   
638
of the sentence, by means of Freeling tool 3 . 
After that, it is able to generate candidates 
according to certain parameters (see section 3.3). 
Then, all the generated candidates are 
processed to extract the features needed for the 
learning methods, in order to determine which 
candidates are drugs. 
After the drugs are detected, the system 
generates a tagged corpus, following the 
provided training corpus structure, containing 
the detected entities, and then it proceeds to 
classify each one of them. To do so, another 
supervised learning algorithm was used (see 
section 3.3). 
3.1. Candidates generation 
Drugs and drug groups, as every entity in 
Natural Language, follow certain grammatical 
patterns. For instance, a drug is usually a noun 
or a set of nouns, or even a combination of verbs 
and nouns, especially verbs in the past participle 
tense and gerunds. But, one thing we noticed is 
that both drugs and drug groups end with a noun 
and as to drug groups that noun is often in the 
plural. 
Based on that idea, we decided to generate 
candidates starting from the end of each 
sentence and going forward. 
Generation starts with the search of a pivot 
word, which in this case is a noun. When the 
pivot is found, it is added to the candidates list, 
and then the algorithm takes the word before the 
pivot to see if it complies with one of the 
patterns i.e. if the word is a noun, an adjective, a 
gerund or past participle verb. If it does, then it 
and the pivot form another candidate.  
After that, the algorithm continues until it finds 
a word that does not comply with a pattern. In 
this case, it goes to the next pivot and stops 
when all the nouns in the sentence have been 
processed, or the first word of the sentence is 
reached. 
3.2. Feature Description 
For the DDIExtraction20134 task 9 three runs of 
the same system were performed with different 
                                                     
3 http://nlp.lsi.upc.edu/freeling/ 
features each time. The next sections describes 
the features we used. 
3.2.1. Most Frequent Semantic Classes 
(MFSC) 
Given a word, its semantic class label (Izquierdo 
et al, 2007) is obtained from WordNet using the 
ISR-WN resource (Guti?rrez et al, 2011; 2010). 
The semantic class is that associated to the most 
probable sense of the word. For each entity in 
the training set we take the words in the same 
sentence and for each word its semantic class is 
determined. This way, we identify the 4005 most 
frequent semantic classes associated to words 
surrounding the entities in the training set.  
For a candidate entity we use 400 features to 
encode information with regard to whether or 
not in its same sentence a word can be found 
belonging to one of the most frequent semantic 
classes. 
Each one of these features takes a value 
representing the distance (measured in words) a 
candidate is from the nearest word with same 
semantic class which represents the attribute.  
If the word is to the left of the candidate, the 
attribute takes a negative value, if it is to the 
right, the value is positive, and zero if no word 
with that semantic class is present in the 
sentence the candidate belongs to. 
To better understand that, consider A1 is the 
attribute which indicates if in the sentence of the 
candidate a word can be found belonging to the 
semantic class 1. Thus, the value of A1 is the 
distance the candidate is from the closest word 
with semantic class 1 in the sentence that is 
being analyzed. 
3.2.2. Candidate Semantic Class (CSC) 
The semantic class of candidates is also included 
in the feature set, if the candidate is a multi-
word, then the semantic class of the last word 
(the pivot word) is taken.  
 
                                                                               
4 http://www.cs.york.ac.uk/semeval-2013/task9/ 
5 This value was extracted from our previous experiment. 
639
3.2.3. Most Frequent Semantic Classes 
from Entities (EMFSC) 
In order to add more semantic information, we 
decided to find the most frequent semantic 
classes among all the entities that were tagged in 
the training data set. We included, in the feature 
set, all the semantic classes with a frequency of 
eight or more, because all the classes we wanted 
to identify were represented in that threshold. In 
total, they make 29 more features. The values of 
every one of them, is the sum of the number of 
times it appears in the candidate.  
3.2.4. Candidate Semantic Class All 
Words (CSC-All) 
This feature is similar to CSC, but in this case 
the candidate is a multi-word, we not only look 
for the semantic class of the pivot, but also the 
whole candidate as one. 
3.2.5. Drug-related domains (DRD) 
Another group of eight attributes describes how 
many times each one of the candidates belongs 
to one of the following drug-related domains 
(DRD) (medicine, anatomy, biology, chemistry, 
physiology, pharmacy, biochemistry, genetics).  
These domains where extracted from WordNet 
Domains. In order to determine the domain that 
a word belongs to, the proposal of DRelevant 
(V?zquez et al, 2007; V?zquez et al, 2004) was 
used. 
To illustrate how the DRD features take their 
values, consider the following sentence: 
??until the lipid response to Accutane is 
established.? 
One of the candidates the system generates 
would be ?lipid response?. It is a two-word 
candidate, so we take the first word and see if it 
belongs to one of the above domains. If it does, 
then we add one to that feature. If the word does 
not belong to any of the domains, then its value 
will be zero. We do the same with the other 
word. In the end, we have a collection where 
every value corresponds to each one of the 
domains. For the example in question the 
collection would be:  
 
 
medicine 1 
anatomy 0 
biology 0 
chemistry 0 
physiology 1 
pharmacy 0 
biochemistry 0 
genetics 0 
Table 1. DRD value assignment example. 
3.2.6. Candidate word number (WNum) 
Because there are candidates that are a multi-
word and others that are not, it may be the case 
that a candidate, which is a multi-word, has an 
EMFSC bigger than others which are not a 
multi-word, just because more than one of the 
words that conform it, have a frequent semantic 
class.  
We decided to add a feature, called WNum, 
which would help us normalize the values of the 
EMFSC. The value of the feature would be the 
number of words the candidate has. Same thing 
happens with DRD. 
3.2.7. Candidate Domain (CD) 
The value of this nominal feature is the domain 
associated to the candidate. If the candidate is a 
multi-word; we get the domain of all the words 
as a whole. In both cases the domain for a single 
word as well as for a multi-word is determined 
using the relevant domains obtained by 
(V?zquez et al, 2007; V?zquez et al, 2004). 
3.2.8. Maximum Frequent 2-grams, 3-
grams 
Drugs usually contain sequences of characters 
that are very frequent in biomedical domain 
texts. These character sequences are called n-
grams, where n is the number of characters in 
the sequence. Because of that, we decided to add 
the ten most frequent n-grams with n between 
two and three. The selected n-grams are the 
following: ?in? (frequency: 8170), ?ne? (4789), 
?ine? (3485), ?ti? (3234), ?id? (2768), ?an? 
(2704), ?ro? (2688), ?nt? (2593), ?et? (2423), 
?en? (2414). 
These features take a value of one if the 
candidate has the corresponding character 
sequence and zero if it does not. For instance: if 
640
we had the candidate ?panobinostat? it will 
generate the following collection:  
?in? 1 
?ne? 0 
?ine? 0 
?ti? 0 
?id? 0 
?an? 1 
?ro? 0 
?nt? 0 
?et? 0 
?en? 0 
Table 2. MF 2-gram, 3-gram. 
3.2.9. Uppercase (UC), Uppercase All 
(UCA). Multi-word (MWord) and 
Number (N) 
Other features say if the first letter of the 
candidate is an uppercase; if all of the letters are 
uppercase (UCA); if it is a multi-word (MWord) 
and also if it is in the singular or in the plural 
(N).  
3.2.10. L1, L2, L3 and R1, R2, R3 
The Part-of-Speech tags of the closest three 
surrounding words of the candidates are also 
included. We named those features L1, L2, and 
L3 for POS tags to the left of the candidate, and 
R1, R2, and R3 for those to the right. 
3.2.11. POS-tagging combination (GC) 
Different values are assigned to candidates, in 
order to identify its POS-tagging combination. 
For instance: to the following entity ?combined 
oral contraceptives? taken from DDI13-train-
TEES-analyses-130304.xml6 training file, which 
was provided for task 9.1, corresponds 5120. 
This number is the result of combining the four 
grammatical categories that really matter to us: 
R for adverb, V for verb, J for adjective, N for 
noun.  
A unique number was given to each 
combination of those four letters. We named this 
feature  GC. 
 
                                                     
6 http://www.cs.york.ac.uk/semeval-2013/task9 
3.2.12. In resource feature (InRe) 
A resource was created which contains all the 
drug entities that were annotated in the training 
corpus, so another attribute tells the system if the 
candidate is in the resource.  
Since all of the entities in the training data set 
were in the resource this attribute could take a 
value of one for all instances. Thus the classifier 
could classify correctly all instances in the 
training data set just looking to this attribute, 
which is not desirable. To avoid that problem, 
we randomly set its value to zero every 9/10 of 
the training instances. 
3.3. Classification 
All the features extracted in the previous stages 
are used in this stage to obtain the two models, 
one for drug detection phase, and the other for 
drug classification phase.  
We accomplished an extensive set of 
experiments in order to select the best classifier. 
All algorithms implemented in WEKA, except 
those that were designed specifically for a 
regression task, were tried. In each case we 
perform a 10-fold cross-validation. In all 
experiments the classifiers were settled with the 
default configuration. From those tests we select 
a decision tree, the C4.5 algorithm (Guti?rrez et 
al., 2011; 2010) implemented as the J48 
classifier in WEKA.  This classifier yields the 
better results for both drug detection and drug 
classification. 
The classifier was trained using a set of 463 
features, extracted from the corpus provided by 
SemEval 2013, the task 9 in question. 
As it was mentioned before, three runs were 
performed for the competition. Run (1) used the 
following features for drug detection: MFSC 
(only 200 frequent semantic classes), MF 2-
grams, 3-grams, UC, UCA, MWord, N, L1, L2, 
L3, R1, R2, R3, CSC, CD, WNum, GC and 
InRe.  
Drug classification in this run used the same 
features except for CD, WNum, and GC. Run 
(2) has all the above features, but we added the 
remaining 200 sematic classes that we left out in 
Run (1) to the detection and the classification 
models. In Run (3), we added EMFSC feature to 
the detection and the classification models. 
641
4. Results 
In the task, the results of the participants were 
compared to a gold-standard and evaluated 
according to various evaluation criteria:  
? Exact evaluation, which demands not only 
boundary match, but also the type of the 
detected drug has to be the same as that of 
the gold-standard. 
? Exact boundary matching (regardless of 
the type). 
? Partial boundary matching (regardless of 
the type) 
? Type matching. 
Precision and recall were calculated using the 
scoring categories proposed by MUC 7: 
? COR: the output of the system and the 
gold-standard annotation agree. 
? INC: the output of the system and the 
gold-standard annotation disagree. 
? PAR: the output of the system and the 
gold-standard annotation are not identical 
but has some overlapping text. 
? MIS: the number of gold-standard entities 
that were not identify by the system. 
? SPU: the number of entities labeled by the 
system that are not in the gold-standard. 
Table 3 , Table 4 and Table 5 show the system 
results in the DDIExtraction2013 competition 
for Run (1).  
Run (2) and Run (3) results are almost the 
same as Run (1). It is an interesting result since 
in those runs 200 additional features were 
supplied to the classifier.  In feature evaluation, 
using CfsSubsetEval and GeneticSearch with 
WEKA we found that all these new features 
were ranked as worthless for the classification. 
On the other hand, the following features were 
the ones that really influenced the classifiers: 
MFSC (215 features only), MF 2-grams, 3-
grams (?ne?, ?ine?, ?ti?, ?ro?, ?et?, ?en?), 
WNum, UC, UCA, L1, R1, CSC, CSC-All, CD, 
DRD (anatomy, physiology, pharmacy, 
biochemistry), InRe, GC and EMFS, specifically 
music.n.01, substance.n.01, herb.n.01, 
artifact.n.01, nutriment.n.01, nonsteroidal_anti-
inflammatory.n.01, causal_agent.n.01 have a 
                                                     
7http://www.itl.nist.gov/iaui/894.02/related_projects/muc/m
uc_sw/muc_sw_manual.html 
frequency of 8, 19, 35, 575, 52, 80, 63 
respectively.  
Measure Strict 
Exact 
Matching 
Partial 
Matching 
Type 
COR 319 354 354 388 
INC 180 145 0 111 
PAR 0 0 145 0 
MIS 187 187 187 187 
SPU 1137 1137 1137 1137 
Precision 0.19 0.22 0.22 0.24 
Recall 0.47 0.52 0.62 0.57 
Table 3. Run (1), all scores. 
Measure Drug Brand Group Drug_n 
COR 197 20 93 9 
INC 23 2 43 1 
PAR 0 0 0 0 
MIS 131 37 19 111 
SPU 754 47 433 14 
Precision 0.2 0.29 016 0.38 
Recall 0.56 0.34 0.6 0.07 
F1 0.3 0.31 0.26 0.12 
Table 4. Scores for entity types, exact matching in 
Run (1). 
 Precision Recall F1 
Macro average 0.26 0.39 0.31 
Strict matching 0.19 0.46 0.27 
Table 5. Macro average and Strict matching measures 
in Run (1). 
5. Conclusion and future works 
In this paper we show the description of 
UMCC_DLSI-(DDI) system, which is able to 
detect and classify drugs in biomedical texts 
with acceptable efficacy. It introduces in this 
thematic the use of semantic information such as 
semantic classes and the relevant domain of the 
words, extracted with ISR-WN resource. With 
this approach we obtained an F-Measure of 
27.5% in the Semeval DDI Extraction2013 task 
9.  
As further work we propose to eliminate some 
detected bugs (i.e. repeated instances, 
multiwords missed) and enrich our knowledge 
base (ISR-WN), using biomedical sources as 
UMLS8, SNOMED9 and OntoFis10. 
                                                     
8 http://www.nlm.nih.gov/research/umls 
9 http://www.ihtsdo.org/snomed-ct/ 
10 http://rua.ua.es/dspace/handle/10045/14216 
642
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
References 
Bj?rne, J.; A. Airola; T. Pahikkala and T. Salakoski 
Drug-Drug Interaction Extraction from 
Biomedical Texts with SVM and RLS Classifiers 
Proceedings of the 1st Challenge Task on Drug-
Drug Interaction Extraction, 2011, 761: 35-42. 
Bobi?, T.; R. Klinger; P. Thomas and M. Hofmann-
Apitius Improving Distantly Supervised Extraction 
of Drug-Drug and Protein-Protein Interactions 
EACL 2012, 2012: 35. 
Cohen, A. M. and W. R. Hersh A survey of current 
work in biomedical text mining Briefings in 
bioinformatics, 2005, 6(1): 57-71. 
Gr?nroos, P.; K. Irjala; J. Heiskanen; K. Torniainen 
and J. Forsstr?m Using computerized individual 
medication data to detect drug effects on clinical 
laboratory tests Scandinavian Journal of Clinical 
& Laboratory Investigation, 1995, 55(S222): 31-
36. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010. 161-168 p. 1135-
5948 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschman, L.; A. Yeh; C. Blaschke and A. Valencia 
Overview of BioCreAtIvE: critical assessment of 
information extraction for biology BMC 
bioinformatics, 2005, 6(Suppl 1): S1. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Percha, B. and R. B. Altman Informatics confronts 
drug?drug interactions Trends in pharmacological 
sciences, 2013. 
Segura-Bedmar, I.; P. Mart?nez and D. S?nchez-
Cisneros The 1st DDIExtraction-2011 challenge 
task: Extraction of Drug-Drug Interactions from 
biomedical texts Challenge Task on Drug-Drug 
Interaction Extraction, 2011, 2011: 1-9. 
Tari, L.; S. Anwar; S. Liang; J. Cai and C. Baral 
Discovering drug?drug interactions: a text-mining 
and reasoning approach based on properties of 
drug metabolism Bioinformatics, 2010, 26(18): 
i547-i553. 
V?zquez, S.; A. Montoyo and Z. Kozareva. 
Extending Relevant Domains for Word Sense 
Disambiguation. IC-AI?07. Proceedings of the 
International Conference on Artificial Intelligence 
USA, 2007.  
V?zquez, S.; A. Montoyo and G. Rigau. Using 
Relevant Domains Resource for Word Sense 
Disambiguation. IC-AI?04. Proceedings of the 
International Conference on Artificial Intelligence, 
Ed: CSREA Press. Las Vegas, E.E.U.U., 2004. 
Wishart, D. S.; C. Knox; A. C. Guo; D. Cheng; S. 
Shrivastava; D. Tzur; B. Gautam and M. Hassanali 
DrugBank: a knowledgebase for drugs, drug 
actions and drug targets Nucleic acids research, 
2008, 36(suppl 1): D901-D906. 
643
