Arabic Morphology Generation Using a Concatenative Strategy 
Violetta Cavalli-Sforza 
Carnegie Technology 
Education 
4615 Forbes Avenue 
Pittsburgh, PA, 15213 
violetta@cs.cmu.edu 
Abdelhadi Soudi 
Computer Science Department 
Ecole Nationale de L'Industrie 
Minerale 
Rabat, Morocco 
asoudi@enim.ac.ma 
Teruko Mitamura 
Language Technologies 
Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
teruko @cs.cmu.edu 
Abstract 
Arabic inflectional morphology requires 
infixation, prefixation and suffixation, 
giving rise to a large space of morphological 
variation. In this paper we describe an 
approach to reducing the complexity of 
Arabic morphology generation using 
discrimination trees and transformational 
rules. By decoupling the problem of stem 
changes from that of prefixes and suffixes, 
we gain a significant reduction in the 
number of rules required, as much as a 
factor of three for certain verb types. We 
focus on hollow verbs but discuss the wider 
applicability of the approach. 
Introduction 
Morphologically, Arabic is a non-concatenative 
language. The basic problem with generating 
Arabic verbal morphology is the large number of 
variants that must be generated. Verbal stems 
are based on triliteral or quadriliteral roots (3- or 
4-radicals). Stems are formed by a derivational 
combination of a root morpheme and a vowel 
melody; the two are arranged according to 
canonical patterns. Roots are said to 
interdigitate with patterns to form stems. For 
example, the Arabic stem katab (he wrote) is 
composed of the morpheme ktb (notion of 
writing) and the vowel melody morpheme 'a-a'. 
The two are coordinated according to the pattern 
CVCVC (C=consonant, V=vowel). 
There are 15 triliteral patterns, of which at least 
9 are in common use, and 4 much rarer 
quadriliteral patterns. All these patterns undergo 
some stem changes with respect o voweling in 
the 2 tenses (perfect and imperfect), the 2 voices 
(active and passive), and the 5 moods 
(indicative, subjunctive, jussive, imperative and 
energetic). ~ The stem used in the conjugation of 
the verb may differ depending on the person, 
number, gender, tense, mood, and the presence 
of certain root consonants. Stem changes 
combine with suffixes in the perfect indicative 
(e.g., katab-naa 'we wrote', kutib-a 'it was 
written') and the imperative (e.g. uktub-uu 
'write', plural), and with both prefixes and 
suffixes for the imperfect tense in the indicative, 
subjunctive, and jussive moods (e.g. ya-ktub-na 
'they write, feminine plural') and in the 
energetic mood (e.g. ya-ktub-unna orya-ktub-un 
'he certainly writes'). There are a total of 13 
person-number-gender combinations. Distinct 
prefixes are used in the active and passive voices 
in the imperfect, although in most cases this 
results in a change in the written form only if 
diacritic marks are used. 2
Most previous computational treatments of 
Arabic morphology are based on linguistic 
models that describe Arabic in a non- 
concatenative way and focus primarily on 
analysis. Beesley (1991) describes a system that 
analyzes Arabic words based on Koskenniemi's 
1 The jussive is used in specific constructions, for 
example, negation in the past with the negative 
particle tam (e.g., tam aktub 'I didn't write'). The 
energetic expresses corroboration of an action taking 
place. The indicative is common to both perfect and 
imperfect tenses, but the subjunctive and the jussive 
are restricted to the imperfect tense. The imperative 
has a special form, and the energetic can be derived 
from either the imperfect or the imperative. 
z Diacritic marks are used in Arabic language 
textbooks and occasionally in regular texts to resolve 
ambiguous words (e.g. to mark a passive verb use). 
86 
(1983) two-level morphology. In Beesley 
(1996) the system is reworked into a finite-state 
lexical transducer to perform analysis and 
generation. In two-level systems, the lexical 
level includes hort vowels that are typically not 
realized on the the surface level. Kiraz (1994) 
presents an analysis of Arabic morphology 
based on the CV-, moraic-, and affixational 
models. He introduces a multi-tape two-level 
model and a formalism where three tapes are 
used for the lexical level (root, pattern, and 
vocalization) and one tape for the surface level. 
In this paper, we propose a computational 
approach that applies a concatenative treatment 
to Arabic morphology generation by separating 
the issue of infixation from other inflectional 
variations. We are developing an Arabic 
morphological generator using MORPHE 
(Leavitt, 1994), a tool for modeling morphology 
based on discrimination trees and regular 
expressions. MORPHE is part of a suite of tools 
developed at the Language Technologies 
Institute, Carnegie Mellon University, for 
knowledge-based machine translation. Large 
systems for MT from English to Spanish, 
French, German, Portuguese and a prototype for 
Italian have already been developed. Within this 
framework, we are exploring English to Arabic 
translation and Arabic generation for 
pedagogical purposes. We generate Arabic 
words including short vowels and diacritic 
marks, since they are pedagogically useful and 
can always be stripped before display. 
Our approach seeks to reduce the number of 
rules for generating morphological variants of 
Arabic verbs by breaking the problem into two 
parts. We observe that, with the exception of a 
few verb types, there is very little interaction 
between stem changes and the processes of 
prefixation and suffixation. It is therefore 
possible to decouple, in large part, the problem 
of stem changes from that of prefixes and 
suffixes. The gain is a significant reduction in 
the size number of transformational rules, as 
much as a factor of three for certain verb classes. 
This improves the space efficiency of the system 
and its maintainability by reducing duplication 
of rules, and simplifies the rules by isolating 
different types of changes. 
To illustrate our approach, we focus on a 
particular type of verbs, termed hollow verbs, 
and show how we integrate their treatment with 
that of more regular verbs. We also discuss how 
the approach can be extended to other classes of 
verbs and other parts of speech. 
1 Arabic Verbal Morphology 
Verb roots in Arabic can be classified as shown 
in Figure 1. 3 A primary distinction is made 
between weak and strong verbs. Weak verbs 
have a weak consonant ('w' or 'y') as one or 
more of their radicals; strong verbs do not have 
any weak radicals. 
Strong verbs undergo systematic hanges in 
stem voweling from the perfect o the imperfect. 
The first radical vowel disappears in the 
imperfect. Verbs whose middle radical vowel in 
the perfect is 'a' can change it to 'a' (e.g., 
qaTa'a 'he cut' -> yaqTa'u 'he cuts'), 4 'i' (e.g., 
Daraba 'he hit' -> yaDribu 'he hits'), or 'u' (e.g., 
kataba 'he wrote' -> yaktubu 'he writes') in the 
imperfect. Verbs whose middle radical vowel in 
the perfect is 'i' can only change it to 'a' (e.g., 
shariba 'he drank' -> yashrabu 'he drinks') or 'i' 
(e.g., Hasiba 'he supposed' -> yaHsibu 'he 
supposes'). Verbs with middle radical vowel 'u' 
in the perfect do not change it in the imperfect 
(e.g., Hasuna 'he was beautiful' -> yaHsunu 'he 
is beautiful'). For strong verbs, neither perfect 
nor imperfect stems change with person, gender, 
or number. 
Hollow verbs are those with a weak middle 
radical. In both perfect and imperfect tenses, the 
underlying stem is realized by two characteristic 
allomorphs, one short and one long, whose use 
depends on the person, number and gender. 
3 Grammars of Arabic are not uniform in their 
classification of "hamzated" verbs, verbs containing 
the glottal stop as one of the radicals (e.g. \[sa?a\[\] 'to 
ask'). Wright (1968) includes them as weak verbs, 
but Cowan (1964) doesn't. Hamzated verbs change 
the written 'seat' of the hamza from 'alif' to 'waaw' 
or 'yaa?', depending on the phonetic ontext. 
4 In the Arabic transcription capital letters indicate 
emphatic onsonants; 'H' is the voiceless pharyngeal 
fricative ; " '  the voiced pharyngeal fricative ; '?' is 
the glottal stop 'hamza'. 
87 
I 
strong 
, I 
regular hamzated 
I 
reterit present 
ffect) (imperfect) 
' I 
I I 
active passive 
triliteral 
I 
weak 
I 
I I \[ I 
doubled weak initial weak middle weak final 
radical radical radical radical 
(assimilated) (hollow) (defective) 
I I I 
I 
I I 
tense mood 
I , , I 
participle indicative 
I I I 
imperative subjunctive jussive energetic 
Figure 1: Classification of Arabic Verbal Roots and Mood Tense System 
Hollow verbs fall into four classes: 
. Verbs of the pattern CawaC or  CawuC 
(e.g. \[Tawut\] 'to be long'), where the 
middle radical is 'w'. Their characteristic 
is a long 'uu' between the first and last 
radical in the imperfect. E.g., 
From the underlying root \[zawar\]: 
zaara 'he visited' and yazuuru 'he visits' 
Stem allomorphs: 
Perfect: -zur- and -zaar- 
Imperfect:-zur- and-zuur- 
. Verbs of the pattern CawiC, where the 
middle radical is 'w'. Their characteristic 
is a long 'aa' between the first and last 
radical in the imperfect. E.g., 
From the underlying root \[nawim\]: 
naama 'he slept and yanaamu 'he sleeps' 
Stem aUomorphs : 
Perfect: -nirn- and -naam- 
Imperfect:-ham- and-naam- 
. Verbs of the pattern CayaC, where the 
middle radical is 'y'. Their characteristic 
is a long 'ii' before the first and last radical 
in the imperfect. E.g., 
From the underlying root \[baya" \]: 
baa" a 'he sold' and yabii" u 'he sells' 
Stem allomorphs : 
Perfect: -bi'- and -baa'- 
Imperfect: and -bi'- and -bii'- 
. Verbs of the pattern CayiC, where middle 
radical is 'y'. E.g., 
From the underlying root \[hayib\]: 
haaba 'he feared' and yahaabu 'he fears' 
Stem allomorphs : 
Perfect: -bib- and-haab- 
Imperfect: -hab- and-haab- 
In the relevant literature (e.g., Beesley, 1998; 
Kiraz, 1994), verbs belonging to the above 
classes are all assumed to have the pattern 
CVCVC. The pattern does not show the verb 
conjugation class and makes it difficult to 
predict he type of stem allomorph to use. To 
avoid these problems, we keep information on 
the middle radical and vowel in the base form 
of the verb. In generation, classes 2 and 4 of 
the verb can be handled as one because they 
have the same perfect and imperfect stemsP 
5 The only exception is the passive participle. Verbs 
of classes 1 and 2 behave the same (e.g. Class 1: 
\[zawar\]: mazuwr 'visited'; Class 2 \[nawil\] --) 
manuwt 'obtained'), as do verbs of classes 3 and 4 
(e.g. Class 3: \[baya'\]  --) mabii" 'sold', Class 4: 
\[hayib\] --) mahiib 'feared'). 
88 
We describe our approach to modeling strong 
and hollow verbs below, following a 
description of the implementation framework. 
2 The MORPHE System 
MORPHE (Leavitt, 1994) is a tool that 
compiles morphological transformation rules 
into either a word parsing program or a word 
generation program. 6 In this paper we will 
focus on the use of MORPHE in generation. 
Input and Output. MORPHE's output is 
simply a string. Input is a feature structure 
(FS) which describes the item that MORPHE 
must transform. A FS is implemented as a 
recursive Lisp list. Each element of the FS is a 
feature-value pair (FVP), where the value can 
be atomic or complex. A complex value is 
itself a FS. For example, the FS for generating 
the Arabic zurtu 'I visited' would be: 
((ROOT "zawar") 
(CAT V) (PAT CVCVC) (VOW HOL) 
(TENSE PERF) (MOOD IND) 
(VOICE ACT) 
(NI/MBER SG) (PERSON i)) 
The choice of feature names and values, other 
than ROOT, which identifies the lexical item to 
be transformed, is entirely up to the user. The 
FVPs in a FS come from one of two sources. 
Static features, such as CAT (part of speech) 
and ROOT, come from the syntactic lexicon, 
which, in addition to the base form of words, 
can contain morphological and syntactic 
features. Dynamic features, such as TENSE and 
NUMBER, are set by MORPHE's caller. 
The Morphological Form Hierarchy. 
MORPHE is based on the notion of a 
morphological form hierarchy (MFH) or tree. 
Each internal node of the tree specifies a piece 
of the FS that is common to that entire 
subtree. The root of the tree is a special node 
that simply binds all subtrees together. The 
leaf nodes of the tree correspond to distinct 
morphological forms in the language. Each 
node in the tree below the root is built by 
specifying the parent of the node and the 
conjunction or disjunction of FVPs that define 
the node. Portions of the Arabic MFH are 
shown in Figures 2-4. 
Transformational Rules. A rule attached to 
each leaf node of the MFH effects the desired 
morphological transformations for that node. 
A rule consists of one or more mutually 
exclusive clauses. The 'if' part of a clause is a 
regular expression pattern, which is matched 
against he value of the feature ROOT (a string). 
The 'then' part includes one or more operators, 
applied in the given order. Operators include 
addition, deletion, and replacement of prefixes, 
infixes, and suffixes. The output of the 
transformation is the transformed ROOT string. 
An example of a rule attached to a node in the 
MFH is given in Section 3.1 below. 
Process Logic. In generation, the MFH acts as 
a discrimination etwork. The specified FS is 
matched against the features defining each 
subtree until a leaf is reached. At that point, 
MORPHE first checks in the irregular forms 
lexicon for an entry indexed by the name of the 
leaf node (i.e., the MF) and the value of the 
ROOT feature in the FS. If an irregular form is 
not found, the transformation rule attached to 
the leaf node is tried. If no rule is found or 
none of the clauses of the applicable rule 
match, MORPHE returns the value of ROOT 
unchanged. 
3 Handling Arabic Verbal 
Morphology in MORPHE 
Figure 2 sketches the basic MFH and the 
division of the verb subtree into stem changes 
and prefix/suffix additions. 7 The inflected verb 
is generated in two steps. MORPHE is first 
called with the feature CHG set to STEM. The 
required stem is returned and temporarily 
substituted for the value of the ROOT feature. 
6 MORPHE is written in Common Lisp and the 
compiled MFH and transformation rules are 
themselves a set of Common Lisp functions. 
7 The use of two parts of the same tree for the two 
problems is a constraint of MORPHE's 
implementation, which does not permit multiple 
trees with separate roots. 
89 
The second call to MORPHE, with feature CHG 
set to PSFIX, adds the necessary prefix and/or 
suffix and returns the fully inflected verb. 
N) (CAT ADJ) 
A 
(CHG STEM) (CHG PSFIX) 
(PA~T~CCVC)  other forms 
( V O ~ I C E  PAS) 
(TENSE PERF) (TENSE IMPERF) /x /X 
Figure 2 : The Basic Verb Hierarchy 
Figure 2 also shows some of the features used 
to traverse the discrimination tree. The feature 
PAT is used in conjunction with the ROOT 
feature to select the appropriate affixes. 
Knowing the underlying root and its voweling 
is crucial for the determination f hollow verb 
stems, as described in Section 1. Knowing the 
pattern is also important in cases where it is 
unclear. For example, verbs of pattern 
CtVCVC insert a 't' after the first radical (e.g. 
ntaqat 'to move, change location', 
intransitive). With some consonants as first 
radicals, in order to facilitate pronunciation, 
the 't' undergoes a process of assimilation 
whose effects differ depending on the 
preceding consonant. For example, the pattern 
CtVCVC verb from zaHam 'to shove' instead 
of *ztaHarn is zdaHam 'to team'. It is also 
difficult to determine from just the string 
ntaqat whether this is pattern CVCVC of the 
verb *taqat (if it existed) or pattern CtVCVC 
of naqat 'to transport, move', transitive). 
3.1 Handling Strong and Hollow Verb 
Morphology in MORPHE 
As a demonstration of our approach, we 
discuss the case of hollow verbs, whose 
characteristics were described in Section 1. 
Figure 3 shows the MFH for strong and hollow 
verbs of pattern CVCVC in the perfect ense, 
active voice. We use the feature vow to carry 
information about the voweling of the verb in 
the imperfect (discussed below) and overload it 
to distinguish ollow and other kinds of verbs. 
(TENSE PERF) / ' , ,  
~ (VOW (*or* a i u)) A 
(PERS (*or* 1 2)) (PERS 3) 
short s t e m ~  
(NUM (*or* sl~ dl)) (NUM PL) 
long stem 
(GENDER M) (GENDER F) 
long stem short stem 
Figure 3: The Perfect Stem Change Subtree for 
Strong and Hollow Verbs of Pattern CVCVC 
In the perfect active voice, regular strong verbs 
do not undergo any stem changes, but doubled 
radical verbs do. Rules effecting these changes 
are attached to the node labeled with the FVP 
(vow (*or* a i u)). 8 The hollow verbs, on the 
other hand, use a long stem with a middle 'alif' 
(e.g. \[daam\] 'to last') for third person singular 
and dual (masculine and feminine) and for 
third person plural masculine. The remaining 
person-number-gender combinations take a 
short stem whose voweling depends on the 
underlying root of the verb, as specified earlier. 
Transformation rules attached to the leaf nodes 
perform the conversion of the ROOT feature 
value to the short and long stem. 
Inside the stem change rules, the four different 
classes of hollow verbs are treated as three 
separate conditions (classes 2 and 4 can be 
merged, as described in Section 1) by matching 
on the middle radical and the adjacent vowels 
and replacing them with the appropriate vowel. 
8 Hamzated verbs changes are due to interactions 
with specific suffixes and are best dealt with in the 
prefixation and suffixation subtree. 
90 
An example of such a rule, which changes the 
perfect stem to a short one for persons 1 and 2 
both singular and plural, follows. 
(morph-rule v-stem-fl-act-perf-12 
("^%{cons}(awa)%{cons}$" 
(ri *i* "u")) 
("^%{cons}(a\[wy\]i)%{cons}$" 
(ri *i* "i")) 
("^%{cons}(aya)%{cons)$" 
(ri *i* "i"))) 
The syntax %{var} is used to indicate 
variables with a given set of values. Enclosing 
a string in parenthesis associates it with a 
numbered register, so the replace infix (ri) 
operator can access it for substitution. 
Figure 4 shows the imperfect subtree for strong 
and hollow verbs. Strong verbs are treated 
efficiently by three rules branching on the 
middle radical vowel, given as the value of 
vow. The consonant-vowel pattern of the 
computed stem is shown (e.g. for kataba 'he 
wrote', the imperfect stem would be -ktub- in 
the pattern CCuC). As described in Section 1, 
the possible vowel in the imperfect is restricted 
but not always determined by the perfect 
vowel and so must be stored in the syntactic 
lexicon. 9 Separating stem changes from the 
addition of prefixes and suffixes significantly 
reduces the number of transformation rules that 
must be written by eliminating much repetition 
of prefix and suffix addition for different stem 
changes. For strong verbs of pattern CVCVC, 
there is at least a three-fold reduction in the 
number of rules for active voice (recall the 
different kinds of vowel changes for these 
verbs from perfect to imperfect described in 
Section 1). Other patterns and the passive of 
pattern CVCVC verbs show less variation in 
stem voweling but more variation in prefix and 
suffix voweling. Since some of the patterns 
share the same prefix and suffix voweling, 
once the stem has been determined, the 
prefixation and suffixation rules can be shared 
by pattern groups. 
The hollow verb subtree is not as small for the 
imperfect as it is for the perfect, since the stem 
depends not only on the mood but also on the 
person, gender, and number. It is still 
advantageous to decouple stem changes from 
prefixation and suffixation. Suffixes differ in 
the indicative and subjunctive moods; if the 
two types of changes were merged, the stem 
transformations would have to be repeated in 
each of the two moods and for each person- 
number-gender combination. The same 
observation applies to stem changes in the 
passive voice as well. Significant replication 
of transformational rules that include stem 
changes makes the system bigger and harder to 
maintain in case of changes, particularly 
because each transformational rule needs to 
take into consideration the four different 
classes of hollow verbs. 
3.2 An Example of Generation 
Consider again the example verb form zurtu 'I 
visited' and the feature structure (FS) given in 
Section 2. During generation, the feature- 
value pair (CHG STEM) is added to the FS 
before the first call to MORPHE. Traversing 
the MFH shown in Figure 2, MORPHE finds 
the rule v-stem-fl-act-perf-12 given in 
Section 3.1 above. The first clause fires, 
replacing the 'awa' with 'u' and MORPHE 
returns the stem -zur-. This stem is substituted 
as the value of the ROOT feature in the FS and 
the feature-value pair (CHG STEM) is changed 
to (CHG PSFIX) before the second call to 
MORPHE. This time MORPHE traverses a
different subtree and reaches the rule: 
(morph-rule v-psfix-perf-l-sg 
It II 
(+s "otu") ) ) 
This rule, currently simply appends "otu" to 
the string, and MORPHE returns the string 
"zurotu", where the 'o' denotes the diacritic 
"sukuun" or absence of vowel. This is the 
desired form for zurtu 'I visited'. 
9 In the presence of certain second and third 
radicals, the middle radical vowel is more precisely 
determined. This information can be incorporated 
into the syntactic lexicon as it is being built. 
91 
(TENSE IMPERF) 
(VOW HOL) (VOW a) (VOW i) (VOW u) 
(MOOD (*or* IND SUB)) 
(NUM (*or* sg dl)) (NUM PL) 
l o n ~ ~ ' ~  
(PERS l) ~ 
(PERS (*or* 2 3)) (PERS (*or* 2 3)) 
(GENDER M) (GENDER F) 
long stem short stem 
CCaC CCiC CCuC 
(MOOD JUS) 
(NUM SG) 
(PERS (*or* 1 3)) (PERS 2) 
short s t e ~  
(GENDER M) 
short stem 
(NUM DL) (NUM PL) 
long s t e m ~  
(PERS I) / 
(GENDER F) (PERS (*or* 2 3)) (PERS (*or* 2 3)) 
long stem (GENDER M) (GENDER F) 
long stem short stem 
Figure 4: The Imperfect Stem Change Subtree for Strong and Hollow Verbs of Pattern CvCvC 
4 Extensions 
In this paper so far we have focused on regular 
and hollow verbs of the pattern CVCVC. Here 
we examine how our approach applies to other 
verb types and other parts of speech. 
4.1 Extending the Approach to Other 
Verb Types 
The two-step treatment of verbal inflection 
described in this paper is easily extended to the 
passive, to doubled radical and hamzated 
verbs, and to different patterns of strong and 
hollow verbs. In fact, since not all higher 
patterns are affected by the presence of a 
middle or weak radical (e.g. patterns CVCCV, 
CaaCVC, taCVCCVC and others), the subtrees 
for these patterns will be significantly less 
bushy than for pattern CVCVC. The two-step 
treatment also covers verbs with a weak first 
radical, especially the radical 'w', which is 
normally dropped in the active imperfect (e.g. 
perfect stem warad 'to come', imperfect stem -
rid-). ~? Alternatively, it can be placed in the 
10 Exceptions to this rule exist (e.g. the verb waji\[ 
'to be afraid'), with imperfect stem - wjat-) but are 
rare and can be handled in MORPHE by placing the 
irregular stem in the syntactic lexicon and checking 
for it prior to calling MORPHE for stem changes. 
irregular lexicon, which MORPHE consults 
when it reaches a leaf node, prior to applying 
any of the transformational ru es. 
Verbs with a weak third radical, including 
doubly or trebly weak verbs, are the most 
problematic since the stem changes interact 
heavily with the inflectional suffixes, and less 
is gained by trying to modify the stem 
separately. We are currently investigating this 
issue and the best way to treat it in MORPHE. 
4.2 Extending the Approach to Other 
Parts of Speech 
The two-step approach to generating verbal 
morphology also presents advantages for the 
inflectional morphology of nouns and 
adjectives. In Arabic, the plural of many 
nouns, especially masculine nouns, is not 
formed regularly by suffixation. Instead, the 
stem itself undergoes changes according to a 
complex set of patterns (e.g. rajut 'man' 
pluralizes as rijaat 'men'), giving rise to so- 
called "broken plurals". The inflection of 
broken plurals according to case (nominative, 
genitive, accusative) and definiteness, 
however, is basically the same as the inflection 
The radical 'y' is largely not dropped or changed. 
92 
of most masculine or feminine singular nouns. 
The same holds true for adjectives. 
Finally we note that our two-step approach can 
also be used to combine derivational and 
inflectional morphology for nouns and 
adjectives. Deverbal nouns and present and 
past participles can be derived regularly from 
each verb pattern (with the exception of 
deverbal nouns from pattern CVCVC). 
Relational or "nisba" adjectives are derived, 
with small variations, from nouns. Since these 
parts of speech are inflected as normal nouns 
and adjectives, we can perform derivational 
and inflectional morphology in two calls to 
MORPHE, much as we do stem change and 
prefix/suffix addition. 
Conclusion 
We have presented a computational model that 
handles Arabic morphology generation 
concatenatively by separating the infixation 
changes undergone by an Arabic stem from the 
processes of prefixation and suffixation. Our 
approach was motivated by practical concerns. 
We sought to make efficient use of a 
morphological generation tool that is part of 
our standard environment for developing 
machine translation systems. The two-step 
approach significantly reduces the number of 
morphological transformation rules that must 
be written, allowing the Arabic generator to be 
smaller, simpler, and easier to maintain. 
The current implementation has been tested on 
a subset of verbal morphology including 
hollow verbs and various types of strong verbs. 
We are currently working on the other kinds of 
weak verbs: defective and assimilated verbs. 
Other categories of words can be handled in a 
similar manner, and we will turn our attention 
to them next. 
References 
K. Beesley. 1990. Finite-State Description of 
Arabic Morphology. In Proceedings of the 
Second Cambridge Conference: Bilingual 
Computing in Arabic and English. 
K. Beesley. 1991. Computer Analysis of Arabic: A 
Two-Level Approach with Detours. In B. Comrie 
and M. Eid, editors, Perspectives on Arabic 
Linguistics III: Papers from the Third Annual 
Symposium on Arabic Linguistics. Benjamins, 
Amsterdam, pages 155-172. 
K. Beesley. 1996. Arabic Finite-State 
Morphological Analysis and Generation. In 
Proceedings COLING'96, Vol. 1, pages 89-94. 
K. Beesley. 1998. Consonant Spreading in Arabic 
Stems. In Proceedings of COLING'98. 
D. Cowan. 1964. An introduction to modern 
literary Arabic. Cambridge University Press, 
Cambridge. 
G. Hudson. 1986. Arabic Root and Pattern 
Morphology without Tiers. Journal of 
Linguistics, 22:85-122. 
G. Kiraz. 1994. Multi-tape Two-level Morphology: 
A Case study in Semitic Non-Linear 
Morphology. In Proceedings of COLING-94, 
Vol. 1, pages 180-186. 
K. Koskenniemi. 1983. Two-level morphology: A
General Computational Model for Word-Form 
Recognition and Production. PhD thesis, 
University of Helsinki. 
A. Lavie, A. Itai, U. Ornan, and M. Rimon. 1988. 
On the Applicability of Two Level Morphology 
to the Inflection of Hebrew Verbs. In 
Proceedings of the Association of Literary and 
Linguistic Computing Conference. 
J.R. Leavitt. 1994. MORPHE: A Morphological 
Rule Compiler. Technical Report, CMU-CMT- 
94-MEMO. 
J. McCarthy and A. Prince. 1990. Foot and Word 
in Prosodic Morphology: The Arabic Broken 
Plural. Natural Language and Linguistics 
Theory, 8: 209-283. 
J. McCarthy and A. Prince. 1993. Template in 
Prosodic Morphology. In Stvan, L. et al, editors, 
Papers from the Third Annual Formal Linguistics 
Society of Midamerica Conference,. 
Bloomington, Indiana. Indiana University 
Linguistics Club, pages 187-218. 
G. Ritchie. 1992. Languages Generated by Two- 
Level Morphological Rules. Computational 
Linguistics, 18(1), pages 41-59. 
R. Sproat. 1992. Morphology and Computation. 
MIT Press, Cambridge, Mass. 
H. Wehr. 1971. A Dictionary of Modern Written 
Arabic, J.M. Cowan, editor. Spoken Language 
Services, Ithaca, NY, fourth edition. 
W. Wright. 1988. A Grammar of the Arabic 
Language. Cambridge University Press, 
Cambridge, third edition. 
93 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 22?32, Prague, June 2007. c?2007 Association for Computational Linguistics
What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
Mengqiu Wang and Noah A. Smith and Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mengqiu,nasmith,teruko}@cs.cmu.edu
Abstract
This paper presents a syntax-driven ap-
proach to question answering, specifically
the answer-sentence selection problem for
short-answer questions. Rather than us-
ing syntactic features to augment exist-
ing statistical classifiers (as in previous
work), we build on the idea that ques-
tions and their (correct) answers relate to
each other via loose but predictable syntac-
tic transformations. We propose a prob-
abilistic quasi-synchronous grammar, in-
spired by one proposed for machine trans-
lation (D. Smith and Eisner, 2006), and pa-
rameterized by mixtures of a robust non-
lexical syntax/alignment model with a(n
optional) lexical-semantics-driven log-linear
model. Our model learns soft alignments as
a hidden variable in discriminative training.
Experimental results using the TREC dataset
are shown to significantly outperform strong
state-of-the-art baselines.
1 Introduction and Motivation
Open-domain question answering (QA) is a widely-
studied and fast-growing research problem. State-
of-the-art QA systems are extremely complex. They
usually take the form of a pipeline architecture,
chaining together modules that perform tasks such
as answer type analysis (identifying whether the
correct answer will be a person, location, date,
etc.), document retrieval, answer candidate extrac-
tion, and answer reranking. This architecture is so
predominant that each task listed above has evolved
into its own sub-field and is often studied and evalu-
ated independently (Shima et al, 2006).
At a high level, the QA task boils down to only
two essential steps (Echihabi andMarcu, 2003). The
first step, retrieval, narrows down the search space
from a corpus of millions of documents to a fo-
cused set of maybe a few hundred using an IR en-
gine, where efficiency and recall are the main fo-
cus. The second step, selection, assesses each can-
didate answer string proposed by the first step, and
finds the one that is most likely to be an answer
to the given question. The granularity of the tar-
get answer string varies depending on the type of
the question. For example, answers to factoid ques-
tions (e.g., Who, When, Where) are usually single
words or short phrases, while definitional questions
and other more complex question types (e.g., How,
Why) look for sentences or short passages. In this
work, we fix the granularity of an answer to a single
sentence.
Earlier work on answer selection relies only on
the surface-level text information. Two approaches
are most common: surface pattern matching, and
similarity measures on the question and answer, rep-
resented as bags of words. In the former, pat-
terns for a certain answer type are either crafted
manually (Soubbotin and Soubbotin, 2001) or ac-
quired from training examples automatically (Itty-
cheriah et al, 2001; Ravichandran et al, 2003;
Licuanan and Weischedel, 2003). In the latter,
measures like cosine-similarity are applied to (usu-
ally) bag-of-words representations of the question
and answer. Although many of these systems have
achieved very good results in TREC-style evalua-
tions, shallow methods using the bag-of-word repre-
sentation clearly have their limitations. Examples of
22
cases where the bag-of-words approach fails abound
in QA literature; here we borrow an example used by
Echihabi and Marcu (2003). The question is ?Who
is the leader of France??, and the sentence ?Henri
Hadjenberg, who is the leader of France ?s Jewish
community, endorsed ...? (note tokenization), which
is not the correct answer, matches all keywords in
the question in exactly the same order. (The cor-
rect answer is found in ?Bush later met with French
President Jacques Chirac.?)
This example illustrates two types of variation
that need to be recognized in order to connect this
question-answer pair. The first variation is the
change of the word ?leader? to its semantically re-
lated term ?president?. The second variation is the
syntactic shift from ?leader of France? to ?French
president.? It is also important to recognize that
?France? in the first sentence is modifying ?com-
munity?, and therefore ?Henri Hadjenberg? is the
?leader of ... community? rather than the ?leader of
France.? These syntactic and semantic variations oc-
cur in almost every question-answer pair, and typi-
cally they cannot be easily captured using shallow
representations. It is also worth noting that such
syntactic and semantic variations are not unique to
QA; they can be found in many other closely related
NLP tasks, motivating extensive community efforts
in syntactic and semantic processing.
Indeed, in this work, we imagine a generative
story for QA in which the question is generated
from the answer sentence through a series of syn-
tactic and semantic transformations. The same story
has been told for machine translation (Yamada and
Knight, 2001, inter alia), in which a target language
sentence (the desired output) has undergone seman-
tic transformation (word to word translation) and
syntactic transformation (syntax divergence across
languages) to generate the source language sen-
tence (noisy-channel model). Similar stories can
also be found in paraphrasing (Quirk et al, 2004;
Wu, 2005) and textual entailment (Harabagiu and
Hickl, 2006; Wu, 2005).
Our story makes use of a weighted formalism
known as quasi-synchronous grammar (hereafter,
QG), originally developed by D. Smith and Eisner
(2006) for machine translation. Unlike most syn-
chronous formalisms, QG does not posit a strict iso-
morphism between the two trees, and it provides
an elegant description for the set of local configura-
tions. In Section 2 we situate our contribution in the
context of earlier work, and we give a brief discus-
sion of quasi-synchronous grammars in Section 3.
Our version of QG, called the Jeopardy model, and
our parameter estimation method are described in
Section 4. Experimental results comparing our ap-
proach to two state-of-the-art baselines are presented
in Section 5. We discuss portability to cross-lingual
QA and other applied semantic processing tasks in
Section 6.
2 Related Work
To model the syntactic transformation process, re-
searchers in these fields?especially in machine
translation?have developed powerful grammatical
formalisms and statistical models for representing
and learning these tree-to-tree relations (Wu and
Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed,
2004; Ding and Palmer, 2005; Quirk et al, 2005;
Galley et al, 2006; Smith and Eisner, 2006, in-
ter alia). We can also observe a trend in recent work
in textual entailment that more emphasis is put on
explicit learning of the syntactic graph mapping be-
tween the entailed and entailed-by sentences (Mac-
Cartney et al, 2006).
However, relatively fewer attempts have been
made in the QA community. As pointed out by
Katz and Lin (2003), most early experiments in
QA that tried to bring in syntactic or semantic
features showed little or no improvement, and it
was often the case that performance actually de-
graded (Litkowski, 1999; Attardi et al, 2001). More
recent attempts have tried to augment the bag-of-
words representation?which, after all, is simply a
real-valued feature vector?with syntactic features.
The usual similarity measures can then be used on
the new feature representation. For example, Pun-
yakanok et al (2004) used approximate tree match-
ing and tree-edit-distance to compute a similarity
score between the question and answer parse trees.
Similarly, Shen et al (2005) experimented with de-
pendency tree kernels to compute similarity between
parse trees. Cui et al (2005) measured sentence
similarity based on similarity measures between de-
pendency paths among aligned words. They used
heuristic functions similar to mutual information to
23
assign scores to matched pairs of dependency links.
Shen and Klakow (2006) extend the idea further
through the use of log-linear models to learn a scor-
ing function for relation pairs.
Echihabi and Marcu (2003) presented a noisy-
channel approach in which they adapted the IBM
model 4 from statistical machine translation (Brown
et al, 1990; Brown et al, 1993) and applied it to QA.
Similarly, Murdock and Croft (2005) adopted a sim-
ple translation model from IBM model 1 (Brown et
al., 1990; Brown et al, 1993) and applied it to QA.
Porting the translation model to QA is not straight-
forward; it involves parse-tree pruning heuristics
(the first two deterministic steps in Echihabi and
Marcu, 2003) and also replacing the lexical trans-
lation table with a monolingual ?dictionary? which
simply encodes the identity relation. This brings us
to the question that drives this work: is there a statis-
tical translation-like model that is natural and accu-
rate for question answering? We propose Smith and
Eisner?s (2006) quasi-synchronous grammar (Sec-
tion 3) as a general solution and the Jeopardy model
(Section 4) as a specific instance.
3 Quasi-Synchronous Grammar
For a formal description of QG, we recommend
Smith and Eisner (2006). We briefly review the cen-
tral idea here. QG arose out of the empirical obser-
vation that translated sentences often have some iso-
morphic syntactic structure, but not usually in en-
tirety, and the strictness of the isomorphism may
vary across words or syntactic rules. The idea is that,
rather than a synchronous structure over the source
and target sentences, a tree over the target sentence
is modeled by a source-sentence-specific grammar
that is inspired by the source sentence?s tree.1 This
is implemented by a ?sense??really just a subset
of nodes in the source tree?attached to each gram-
mar node in the target tree. The senses define an
alignment between the trees. Because it only loosely
links the two sentences? syntactic structure, QG is
particularly well-suited for QA insofar as QA is like
?free? translation.
A concrete example that is easy to understand
is a binary quasi-synchronous context-free grammar
1Smith and Eisner also show how QG formalisms generalize
synchronous grammar formalisms.
(denoted QCFG). Let VS be the set of constituent to-
kens in the source tree. QCFG rules would take the
augmented form
?X, S1? ? ?Y, S2??Z, S3?
?X, S1? ? w
where X,Y, and Z are ordinary CFG nonterminals,
each Si ? 2VS (subsets of nodes in the source tree
to which the nonterminals align), and w is a target-
language word. QG can be made more or less ?lib-
eral? by constraining the cardinality of the Si (we
force all |Si| = 1), and by constraining the relation-
ships among the Si mentioned in a single rule. These
are called permissible ?configurations.? An example
of a strict configuration is that a target parent-child
pair must align (respectively) to a source parent-
child pair. Configurations are shown in Table 1.
Here, following Smith and Eisner (2006), we use
a weighted, quasi-synchronous dependency gram-
mar. Apart from the obvious difference in appli-
cation task, there are a few important differences
with their model. First, we are not interested in the
alignments per se; we will sum them out as a hid-
den variable when scoring a question-answer pair.
Second, our probability model includes an optional
mixture component that permits arbitrary features?
we experiment with a small set of WordNet lexical-
semantics features (see Section 4.4). Third, we ap-
ply a more discriminative training method (condi-
tional maximum likelihood estimation, Section 4.5).
4 The Jeopardy Model
Our model, informally speaking, aims to follow the
process a player of the television game show Jeop-
ardy! might follow. The player knows the answer
(or at least thinks he knows the answer) and must
quickly turn it into a question.2 The question-answer
pairs used on Jeopardy! are not precisely what we
have in mind for the real task (the questions are not
specific enough), but the syntactic transformation in-
spires our model. In this section we formally define
2A round of Jeopardy! involves a somewhat involved and
specific ?answer? presented to the competitors, and the first
competitor to hit a buzzer proposes the ?question? that leads to
the answer. For example, an answer might be, This Eastern Eu-
ropean capital is famous for defenestrations. In Jeopardy! the
players must respond with a queston: What is Prague?
24
this probability model and present the necessary al-
gorithms for parameter estimation.
4.1 Probabilistic Model
The Jeopardy model is a QG designed for QA. Let
q = ?q1, ..., qn? be a question sentence (each qi is a
word), and let a = ?a1, ..., am? be a candidate an-
swer sentence. (We will use w to denote an abstract
sequence that could be a question or an answer.) In
practice, these sequences may include other infor-
mation, such as POS, but for clarity we assume just
words in the exposition. Let A be the set of can-
didate answers under consideration. Our aim is to
choose:
a? = argmax
a?A
p(a | q) (1)
At a high level, we make three adjustments. The
first is to apply Bayes? rule, p(a | q) ? p(q |
a) ? p(a). Because A is known and is assumed to
be generated by an external extraction system, we
could use that extraction system to assign scores
(and hence, probabilities p(a)) to the candidate an-
swers. Other scores could also be used, such as
reputability of the document the answer came from,
grammaticality, etc. Here, aiming for simplicity, we
do not aim to use such information. Hence we treat
p(a) as uniform over A.3
The second adjustment adds a labeled, directed
dependency tree to the question and the answer.
The tree is produced by a state-of-the-art depen-
dency parser (McDonald et al, 2005) trained on
the Wall Street Journal Penn Treebank (Marcus et
al., 1993). A dependency tree on a sequence w =
?w1, ..., wk? is a mapping of indices of words to in-
dices of their syntactic parents and a label for the
syntactic relation, ? : {1, ..., k} ? {0, ..., k} ? L.
Each word wi has a single parent, denoted w?(i).par .
Cycles are not permitted. w0 is taken to be the invis-
ible ?wall? symbol at the left edge of the sentence; it
has a single child (|{i : ?(i) = 0}| = 1). The label
for wi is denoted ?(i).lab.
The third adjustment involves a hidden variable
X , the alignment between question and answer
3The main motivation for modeling p(q | a) is that it is eas-
ier to model deletion of information (such as the part of the sen-
tence that answers the question) than insertion. Our QG does
not model the real-world knowledge required to fill in an an-
swer; its job is to know what answers are likely to look like,
syntactically.
words. In our model, each question-word maps to
exactly one answer-word. Let x : {1, ..., n} ?
{1, ...,m} be a mapping from indices of words in q
to indices of words in a. (It is for computational rea-
sons that we assume |x(i)| = 1; in general x could
range over subsets of {1, ...,m}.) Because we de-
fine the correspondence in this direction, note that it
is possible for multple question words to map to the
same answer word.
Why do we treat the alignmentX as a hidden vari-
able? In prior work, the alignment is assumed to be
known given the sentences, but we aim to discover
it from data. Our guide in this learning is the struc-
ture inherent in the QG: the configurations between
parent-child pairs in the question and their corre-
sponding, aligned words in the answer. The hidden
variable treatment lets us avoid commitment to any
one x mapping, making the method more robust to
noisy parses (after all, the parser is not 100% ac-
curate) and any wrong assumptions imposed by the
model (that |x(i)| = 1, for example, or that syntactic
transformations can explain the connection between
q and a at all).4
Our model, then, defines
p(q, ?q | a, ?a) =
?
x
p(q, ?q, x | a, ?a) (2)
where ?q and ?a are the question tree and answer
tree, respectively. The stochastic process defined by
our model factors cleanly into recursive steps that
derive the question from the top down. The QG de-
fines a grammar for this derivation; the grammar de-
pends on the specific answer.
Let ? iw refer to the subtree of ?w rooted at wi. The
model is defined by:
p(? iq | qi, ?q(i), x(i), ?a) = (3)
p#kids(|{j : ?q(j) = i, j < i}| | qi, left)
?p#kids(|{j : ?q(j) = i, j > i}| | qi, right)
?
?
j:?q(j)=i
m?
x(j)=0
pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a)
?p(? jq | qj , ?q(j), x(j), ?a)
4If parsing performance is a concern, we might also treat the
question and/or answer parse trees as hidden variables, though
that makes training and testing more computationally expen-
sive.
25
Note the recursion in the last line. While the above
may be daunting, in practice it boils down only to
defining the conditional distribution pkid , since the
number of left and right children of each node need
not be modeled (the trees are assumed known)?
p#kids is included above for completeness, but in the
model applied here we do not condition it on qi and
therefore do not need to estimate it (since the trees
are fixed).
pkid defines a distribution over syntactic children
of qi and their labels, given (1) the word qi, (2) the
parent of qi, (3) the dependency relation between
qi and its parent, (4) the answer-word qi is aligned
to, (5) the answer-word the child being predicted is
aligned to, and (6) the remainder of the answer tree.
4.2 Dynamic Programming
Given q, the score for an answer is simply p(q, ?q |
a, ?a). Computing the score requires summing over
alignments and can be done efficiently by bottom-up
dynamic programming. Let S(j, `) refer to the score
of ? jq, assuming that the parent of qj , ?q(j).par , is
aligned to a`. The base case, for leaves of ?q, is:
S(j, `) = (4)
p#kids(0 | qj , left) ? p#kids(0 | qj , right)
?
m?
k=0
pkid (qj , ?q(j).lab | q?q(j) , `, k, ?a)
Note that k ranges over indices of answer-words to
be aligned to qj . The recursive case is
S(i, `) = (5)
p#kids(|{j : ?q(j) = i, j < i}| | qj , left)
?p#kids(|{j : ?q(j) = i, j > i}| | qj , right)
?
m?
k=0
pkid (qi, ?q(i).lab | q?q(i), `, k, ?a)
?
?
j:?q(j)=i
S(j, k)
Solving these equations bottom-up can be done
in O(nm2) time and O(nm) space; in practice this
is very efficient. In our experiments, computing the
value of a question-answer pair took two seconds on
average.5 We turn next to the details of pkid , the core
of the model.
4.3 Base Model
Our base model factors pkid into three conditional
multinomial distributions.
pbasekid (qi, ?q(i).lab | q?q(i), `, k, ?a) =
p(qi.pos | ak.pos) ? p(qi.ne | ak.ne)
?p(?q(i).lab | config(?q, ?a, i)) (6)
where qi.pos is question-word i?s POS label and
qi.ne is its named-entity label. config maps
question-word i, its parent, and their alignees to
a QG configuration as described in Table 1; note
that some configurations are extended with addi-
tional tree information. The base model does not
directly predict the specific words in the question?
only their parts-of-speech, named-entity labels, and
dependency relation labels. This model is very sim-
ilar to Smith and Eisner (2006).
Because we are interested in augmenting the QG
with additional lexical-semantic knowledge, we also
estimate pkid by mixing the base model with a
model that exploits WordNet (Miller et al, 1990)
lexical-semantic relations. The mixture is given by:
pkid (? | ?) = ?p
base
kid (? | ?)+(1??)p
ls
kid (? | ?) (7)
4.4 Lexical-Semantics Log-Linear Model
The lexical-semantics model plskid is defined by pre-
dicting a (nonempty) subset of the thirteen classes
for the question-side word given the identity of
its aligned answer-side word. These classes in-
clude WordNet relations: identical-word, synonym,
antonym (also extended and indirect antonym), hy-
pernym, hyponym, derived form, morphological
variation (e.g., plural form), verb group, entailment,
entailed-by, see-also, and causal relation. In ad-
dition, to capture the special importance of Wh-
words in questions, we add a special semantic re-
lation called ?q-word? between any word and any
Wh-word. This is done through a log-linear model
with one feature per relation. Multiple relations may
fire, motivating the log-linear model, which permits
?overlapping? features, and, therefore prediction of
5Experiments were run on a 64-bit machine with 2? 2.2GHz
dual-core CPUs and 4GB of memory.
26
any of the possible 213 ? 1 nonempty subsets. It
is important to note that this model assigns zero
probability to alignment of an answer-word with
any question-word that is not directly related to it
through any relation. Such words may be linked in
the mixture model, however, via pbasekid .
6
(It is worth pointing out that log-linear models
provide great flexibility in defining new features. It
is straightforward to extend the feature set to include
more domain-specific knowledge or other kinds of
morphological, syntactic, or semantic information.
Indeed, we explored some additional syntactic fea-
tures, fleshing out the configurations in Table 1 in
more detail, but did not see any interesting improve-
ments.)
parent-child Question parent-child pair align respec-
tively to answer parent-child pair. Aug-
mented with the q.-side dependency la-
bel.
child-parent Question parent-child pair align respec-
tively to answer child-parent pair. Aug-
mented with the q.-side dependency la-
bel.
grandparent-child Question parent-child pair align respec-
tively to answer grandparent-child pair.
Augmented with the q.-side dependency
label.
same node Question parent-child pair align to the
same answer-word.
siblings Question parent-child pair align to sib-
lings in the answer. Augmented with
the tree-distance between the a.-side sib-
lings.
c-command The parent of one answer-side word is
an ancestor of the other answer-side
word.
other A catch-all for all other types of config-
urations, which are permitted.
Table 1: Syntactic alignment configurations are par-
titioned into these sets for prediction under the Jeop-
ardy model.
4.5 Parameter Estimation
The parameters to be estimated for the Jeopardy
model boil down to the conditional multinomial
distributions in pbasekid , the log-linear weights in-
side of plskid , and the mixture coefficient ?.
7 Stan-
6It is to preserve that robustness property that the models are
mixed, and not combined some other way.
7In our experiments, all log-linear weights are initialized to
be 1; all multinomial distributions are initialized as uniform dis-
dard applications of log-linear models apply con-
ditional maximum likelihood estimation, which for
our case involves using an empirical distribution p?
over question-answer pairs (and their trees) to opti-
mize as follows:
max
?
?
q,?q,a,?a
p?(q, ?q,a, ?a) log p?(q, ?q | a, ?a)
? ?? ?
P
x p?(q,?q,x|a,?a)
(8)
Note the hidden variable x being summed out; that
makes the optimization problem non-convex. This
sort of problem can be solved in principle by condi-
tional variants of the Expectation-Maximization al-
gorithm (Baum et al, 1970; Dempster et al, 1977;
Meng and Rubin, 1993; Jebara and Pentland, 1999).
We use a quasi-Newton method known as L-BFGS
(Liu and Nocedal, 1989) that makes use of the gra-
dient of the above function (straightforward to com-
pute, but omitted for space).
5 Experiments
To evaluate our model, we conducted experiments
using Text REtrieval Conference (TREC) 8?13 QA
dataset.8
5.1 Experimental Setup
The TREC dataset contains questions and answer
patterns, as well as a pool of documents returned by
participating teams. Our task is the same as Pun-
yakanok et al (2004) and Cui et al (2005), where
we search for single-sentence answers to factoid
questions. We follow a similar setup to Shen and
Klakow (2006) by automatically selecting answer
candidate sentences and then comparing against a
human-judged gold standard.
We used the questions in TREC 8?12 for training
and set aside TREC 13 questions for development
(84 questions) and testing (100 questions). To gen-
erate the candidate answer set for development and
testing, we automatically selected sentences from
each question?s document pool that contains one or
more non-stopwords from the question. For gen-
erating the training candidate set, in addtion to the
sentences that contain non-stopwords from the ques-
tion, we also added sentences that contain correct
tributions; ? is initialized to be 0.1.
8We thank the organizers and NIST for making the dataset
publicly available.
27
answer pattern. Manual judgement was produced
for the entire TREC 13 set, and also for the first 100
questions from the training set TREC 8?12.9 On av-
erage, each question in the development set has 3.1
positive and 17.1 negative answers. There are 3.6
positive and 20.0 negative answers per question in
the test set.
We tokenized sentences using the standard tree-
bank tokenization script, and then we performed
part-of-speech tagging using MXPOST tagger (Rat-
naparkhi, 1996). The resulting POS-tagged sen-
tences were then parsed using MSTParser (McDon-
ald et al, 2005), trained on the entire Penn Treebank
to produce labeled dependency parse trees (we used
a coarse dependency label set that includes twelve
label types). We used BBN Identifinder (Bikel et al,
1999) for named-entity tagging.
As answers in our task are considered to be sin-
gle sentences, our evaluation differs slightly from
TREC, where an answer string (a word or phrase
like 1977 or George Bush) has to be accompanied
by a supporting document ID. As discussed by Pun-
yakanok et al (2004), the single-sentence assump-
tion does not simplify the task, since the hardest part
of answer finding is to locate the correct sentence.
From an end-user?s point of view, presenting the
sentence that contains the answer is often more in-
formative and evidential. Furthermore, although the
judgement data in our case are more labor-intensive
to obtain, we believe our evaluation method is a bet-
ter indicator than the TREC evaluation for the qual-
ity of an answer selection algorithm.
To illustrate the point, consider the example ques-
tion, ?When did James Dean die?? The correct an-
9More human-judged data are desirable, though we will ad-
dress training from noisy, automatically judged data in Sec-
tion 5.4. It is important to note that human judgement of an-
swer sentence correctness was carried out prior to any experi-
ments, and therefore is unbiased. The total number of questions
in TREC 13 is 230. We exclude from the TREC 13 set questions
that either have no correct answer candidates (27 questions), or
no incorrect answer candidates (19 questions). Any algorithm
will get the same performance on these questions, and therefore
obscures the evaluation results. 6 such questions were also ex-
cluded from the 100 manually-judged training questions, result-
ing in 94 questions for training. For computational reasons (the
cost of parsing), we also eliminated answer candidate sentences
that are longer than 40 words from the training and evaluation
set. After these data preparation steps, we have 348 positive
Q-A pairs for training, 1,415 Q-A pairs in the development set,
and 1,703 Q-A pairs in the test set.
swer as appeared in the sentence ?In 1955, actor
James Dean was killed in a two-car collision near
Cholame, Calif.? is 1955. But from the same docu-
ment, there is another sentence which also contains
1955: ?In 1955, the studio asked him to become a
technical adviser on Elia Kazan?s ?East of Eden,?
starring James Dean.? If a system missed the first
sentence but happened to have extracted 1955 from
the second one, the TREC evaluation grants it a ?cor-
rect and well-supported? point, since the document
ID matches the correct document ID?even though
the latter answer does not entail the true answer. Our
evaluation does not suffer from this problem.
We report two standard evaluation measures com-
monly used in IR and QA research: mean av-
erage precision (MAP) and mean reciprocal rank
(MRR). All results are produced using the standard
trec eval program.
5.2 Baseline Systems
We implemented two state-of-the-art answer-finding
algorithms (Cui et al, 2005; Punyakanok et al,
2004) as strong baselines for comparison. Cui et
al. (2005) is the answer-finding algorithm behind
one of the best performing systems in TREC eval-
uations. It uses a mutual information-inspired score
computed over dependency trees and a single align-
ment between them. We found the method to be brit-
tle, often not finding a score for a testing instance
because alignment was not possible. We extended
the original algorithm, allowing fuzzy word align-
ments through WordNet expansion; both results are
reported.
The second baseline is the approximate tree-
matching work by Punyakanok et al (2004). Their
algorithm measures the similarity between ?q and ?a
by computing tree edit distance. Our replication is
close to the algorithm they describe, with one subtle
difference. Punyakanok et al used answer-typing in
computing edit distance; this is not available in our
dataset (and our method does not explicitly carry out
answer-typing). Their heuristics for reformulating
questions into statements were not replicated. We
did, however, apply WordNet type-checking and ap-
proximate, penalized lexical matching. Both results
are reported.
28
development set test set
training dataset model MAP MRR MAP MRR
100 manually-judged TreeMatch 0.4074 0.4458 0.3814 0.4462
+WN 0.4328 0.4961 0.4189 0.4939
Cui et al 0.4715 0.6059 0.4350 0.5569
+WN 0.5311 0.6162 0.4271 0.5259
Jeopardy (base only) 0.5189 0.5788 0.4828 0.5571
Jeopardy 0.6812 0.7636 0.6029 0.6852
+2,293 noisy Cui et al 0.2165 0.3690 0.2833 0.4248
+WN 0.4333 0.5363 0.3811 0.4964
Jeopardy (base only) 0.5174 0.5570 0.4922 0.5732
Jeopardy 0.6683 0.7443 0.5655 0.6687
Table 2: Results on development and test sets. TreeMatch is our implementation of Punyakanok et al
(2004); +WN modifies their edit distance function using WordNet. We also report our implementation of
Cui et al (2005), along with our WordNet expansion (+WN). The Jeopardy base model and mixture with
the lexical-semantics log-linear model perform best; both are trained using conditional maximum likelihood
estimation. The top part of the table shows performance using 100 manually-annotated question examples
(questions 1?100 in TREC 8?12), and the bottom part adds noisily, automatically annotated questions 101?
2,393. Boldface marks the best score in a column and any scores in that column not significantly worse
under a a two-tailed paired t-test (p < 0.03).
5.3 Results
Evaluation results on the development and test sets
of our model in comparison with the baseline algo-
rithms are shown in Table 2. Both our model and
the model in Cui et al (2005) are trained on the
manually-judged training set (questions 1-100 from
TREC 8?12). The approximate tree matching algo-
rithm in Punyakanok et al (2004) uses fixed edit dis-
tance functions and therefore does not require train-
ing. From the table we can see that our model signif-
icantly outperforms the two baseline algorithms?
even when they are given the benefit of WordNet?
on both development and test set, and on both MRR
and MAP.
5.4 Experiments with Noisy Training Data
Although manual annotation of the remaining 2,293
training sentences? answers in TREC 8?12 was too
labor-intensive, we did experiment with a simple,
noisy automatic labeling technique. Any answer
that had at least three non-stop word types seen in
the question and contains the answer pattern defined
in the dataset was labeled as ?correct? and used in
training. The bottom part of Table 2 shows the re-
sults. Adding the noisy data hurts all methods, but
the Jeopardy model maintains its lead and consis-
tently suffers less damage than Cui et al (2005).
(The TreeMatch method of Punyakanok et al (2004)
does not use training examples.)
5.5 Summing vs. Maximizing
Unlike most previous work, our model does not try
to find a single correspondence between words in the
question and words in the answer, during training or
during testing. An alternative method might choose
the best (most probable) alignment, rather than the
sum of all alignment scores. This involves a slight
change to Equation 3, replacing the summation with
a maximization. The change could be made during
training, during testing, or both. Table 3 shows that
summing is preferable, especially during training.
6 Discussion
The key experimental result of this work is that
loose syntactic transformations are an effective way
to carry out statistical question answering.
One unique advantage of our model is the mix-
ture of a factored, multinomial-based base model
and a potentially very rich log-linear model. The
base model gives our model robustness, and the log-
29
test set
training decoding MAP MRR
? ? 0.6029 0.6852
? max 0.5822 0.6489
max ? 0.5559 0.6250
max max 0.5571 0.6365
Table 3: Experimental results on comparing sum-
ming over alignments (?) with maximizing (max)
over alignments on the test set. Boldface marks the
best score in a column and any scores in that column
not significantly worse under a a two-tailed paired t-
test (p < 0.03).
linear model allows us to throw in task- or domain-
specific features. Using a mixture gives the advan-
tage of smoothing (in the base model) without hav-
ing to normalize the log-linear model by summing
over large sets. This powerful combination leads
us to believe that our model can be easily ported
to other semantic processing tasks where modeling
syntactic and semantic transformations is the key,
such as textual entailment, paraphrasing, and cross-
lingual QA.
The traditional approach to cross-lingual QA is
that translation is either a pre-processing or post-
processing step done independently from the main
QA task. Notice that the QG formalism that we have
employed in this work was originally proposed for
machine translation. We might envision transfor-
mations that are performed together to form ques-
tions from answers (or vice versa) and to translate?
a Jeopardy! game in which bilingual players must
ask a question in a different language than that in
which the answer is posed.
7 Conclusion
We described a statistical syntax-based model that
softly aligns a question sentence with a candidate
answer sentence and returns a score. Discrimina-
tive training and a relatively straightforward, barely-
engineered feature set were used in the implementa-
tion. Our scoring model was found to greatly out-
perform two state-of-the-art baselines on an answer
selection task using the TREC dataset.
Acknowledgments
The authors acknowledge helpful input from three
anonymous reviewers, Kevin Gimpel, and David
Smith. This work is supported in part by
ARDA/DTO Advanced Question Answering for
Intelligence (AQUAINT) program award number
NBCHC040164.
References
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, Alessandro Tommasi, Ellen M.
Voorhees, and D. K. Harman. 2001. Selectively using
relations to improve precision in question answering.
In Proceedings of the 10th Text REtrieval Conference
(TREC-10), Gaithersburg, MD, USA.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic functions
of Markov chains. The Annals of Mathematical Statis-
tics, 41(1):164?171.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns whats? in
a name. Machine Learning, 34(1-3):211?231.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
the 28th ACM-SIGIR International Conference on Re-
search and Development in Information Retrieval, Sal-
vador, Brazil.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Ann Arbor, MI, USA.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Sapporo,
Japan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
30
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and the 44st Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL), Sydney, Australia.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics (ACL), Sapporo, Japan.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL), Sydney, Australia.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system?
TREC-10. In Proceedings of the 10th Text REtrieval
Conference (TREC-10), Gaithersburg, MD, USA.
Tony Jebara and Alex Pentland. 1999. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Proceedings of the 1998 Confer-
ence on Advances in Neural Information Processing
Systems II (NIPS), pages 494?500, Denver, CO, USA.
Boris Katz and Jimmy Lin. 2003. Selectively using
relations to improve precision in question answering.
In Proceedings of the EACL-2003 Workshop on Nat-
ural Language Processing for Question Answering,
Gaithersburg, MD, USA.
Jinxi Xu Ana Licuanan and Ralph Weischedel. 2003.
Trec2003 qa at bbn: Answering definitional questions.
In Proceedings of the 12th Text REtrieval Conference
(TREC-12), Gaithersburg, MD, USA.
Kenneth C. Litkowski. 1999. Question-answering us-
ing semantic relation triples. In Proceedings of the
8th Text REtrieval Conference (TREC-8), Gaithers-
burg, MD, USA.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45:503?528.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), New York, NY, USA.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernado Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43st Annual Meeting of
the Association for Computational Linguistics (ACL),
Ann Arbor, MI, USA.
I. Dan Melamed. 2004. Algorithms for syntax-aware
statistical machine translation. In Proceedings of the
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI), Baltimore, MD, USA.
Xiao-Li Meng and Donald B. Rubin. 1993. Maximum
likelihood estimation via the ECM algorithm: A gen-
eral framework. Biometrika, 80:267?278.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4).
Vanessa Murdock and W. Bruce Croft. 2005. A trans-
lation model for sentence retrieval. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP), Vancouver, BC, USA.
Vasin Punyakanok, Dan Roth, and Wen-Tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the 8th Interna-
tional Symposium on Artificial Intelligence and Math-
ematics, Fort Lauderdale, FL, USA.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics
(ACL), Ann Arbor, MI, USA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Philadelphia, PA, USA.
Deepak Ravichandran, Abharam Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of the Human Lan-
guage Technology Conference and North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), Edmonton, Canada.
Dan Shen and Dietrich Klakow. 2006. Exploring corre-
lation of dependency relation paths for answer extrac-
tion. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics (COLING-ACL), Sydney, Australia.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), Jeju Island, Republic of Korea.
Hideki Shima, Mengqiu Wang, Frank Lin, and Teruko
Mitamura. 2006. Modular approach to error analysis
and evaluation for multilingual question answering. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation (LREC), Genoa,
Italy.
31
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
New York, NY, USA.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns for potential answer expressions as clues to
the right answers. In Proceedings of the 10th Text
REtrieval Conference (TREC-10), Gaithersburg, MD,
USA.
Dekai Wu and Hongsing Wong. 1998. Machine
translation with a stochastic grammatical channel.
In Proceedings of the 17th International Conference
on Computational Linguistics (COLING), Montreal,
Canada.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, MI, USA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Toulouse, France.
32
Correction Grammars for Error Handling in a Speech Dialog System 
 
Hirohiko Sagawa Teruko Mitamura Eric Nyberg 
Language Technologies Institute, Carnegie Mellon University 
Pittsburgh, PA 15213, U.S.A. 
{hsagawa, teruko, ehn}@cs.cmu.edu 
 
 
 
Abstract 
Speech recognition errors are inevitable in a 
speech dialog system. This paper presents an 
error handling method based on correction 
grammars which recognize the correction 
utterances which follow a recognition error. 
Correction grammars are dynamically created 
from existing grammars and a set of 
correction templates. We also describe a 
prototype dialog system which incorporates 
this error handling method, and provide 
empirical evidence that this method can 
improve dialog success rate and reduce the 
number of dialog turns required for error 
recovery. 
1 Introduction 
In a dialog system, speech recognition errors are 
inevitable and often make smooth communication 
between a user and a system difficult. Figure 1 shows an 
example of a dialog between a user and a system which 
illustrates a system error. The system misrecognized 
?Tokyo? in the user?s utterance (U1) as ?Kyoto? (S3). If 
the system correctly recognized the user?s utterance, the 
user could answer ?yes? at U3 and the weather is 
reported (S6). However, in this case, the user must 
correct the error at U3 and the turns from S4 to U5 are 
required to recover from the error. The dialog system 
must recognize the user?s response to the system error 
(correction utterance). Otherwise, more turns (or a 
complete restart) will be required to correct the error. 
Therefore, an error handling method which corrects a 
system error and returns to the normal dialog flow 
smoothly is an important requirement for practical 
dialog systems.  
Recent work related to error handling in speech 
dialog systems has mainly focused on error detection. 
Walker et al (2000), Bosch et al (2001) and 
Kazemzadeh et al (2003) extracted several parameters 
(e.g., acoustic, lexical and semantic) from a dialog 
corpus, and analyzed the differences between correction 
utterances and the other utterances in a dialog. They 
also tried to detect system errors by using these 
parameters as input to machine learning methods. 
However, the issue of error recovery is not addressed. 
Danieli (1996) and LuperFoy & Duff (1996) 
proposed error detection methods based on plan 
matching. An error is detected when the intention or the 
parameter expressed in the user?s utterance is not 
consistent with the system?s assumptions and/or 
limitations. In these studies, the correction utterances 
are assumed to be recognized correctly. 
Kitaoka et al (2003) proposed a method to detect 
system errors based on the similarity of speech patterns 
and hypotheses overlapping in the recognition result. 
They also proposed a method to improve the recognition 
accuracy for correction utterances by selecting a speech 
recognition grammar according to the results of the 
error detection. 
The previous studies assumed that the rules for 
speech recognition or natural language processing of 
correction utterances were prepared in advance (Danieli , 
1996; LuperFoy & Duff, 1996). These rules are 
indispensable because the correction utterance often 
includes the information required to correct the error. 
The correction utterance depends on the dialog context, 
especially on the user?s utterances prior to the system 
error. Therefore it is difficult for the system designer to 
prepare these rules in advance when the dialog flow 
becomes complex. To solve this problem, a method that 
can automatically create the rules to interpret correction 
utterances is desirable. 
In this paper, we will propose a method to 
dynamically create the rules to recognize correction 
utterances and repair recognition errors based on the 
dialog context. A prototype dialog system which 
incorporates the proposed method has been developed, 
S1:  Please tell me the area. 
U1: Tokyo. 
S2: Please tell me the date. 
U2: Tomorrow. 
S3: Would you like to know the weather for Kyoto 
tomorrow? 
U3: No, Tokyo. 
S4: Did you say Tokyo? 
U4: Yes. 
S5:  Would you like to know the weather for Tokyo 
tomorrow? 
U5: Yes. 
S6: The weather for Tokyo tomorrow is fine. 
Correction utterance 
System error 
 
Figure 1. Example of a dialog with a system error 
and we present experimental results which show the 
effectiveness of the approach. 
2 CAMMIA Dialog System 
Our current approach focuses on dialog systems which 
incorporate speech recognition modules utilizing regular 
grammars. The CAMMIA system is an example of such 
a dialog system (Nyberg et al, 2002). 
The CAMMIA system is a client-server dialog 
management system based on VoiceXML. Each dialog 
scenario in this system is described in the format of 
DialogXML. The system has the initiative in the dialog, 
and dialogs are oriented around slot-filling for particular 
queries or requests. The server sends a VoiceXML data 
file to the client VoiceXML interpreter for a particular 
dialog turn, compiled from the DialogXML scenario 
according to the current dialog context. The VoiceXML 
data includes system prompts, names of grammar files 
and valid transitions to subsequent dialog states. The 
client interacts with the user according to the 
VoiceXML data.  
Figure 2 shows an example of a grammar rule used 
in the CAMMIA system. The regular grammar rule can 
be represented as a transition network. The following 
sentences are recognized by the rule in Figure 2: 
? I would like to know the weather for Tokyo. 
? I would like to know the weather for Tokyo tomorrow. 
3 Error Handling Based on Correction 
Grammars 
To recognize the user?s utterances in a dialog system, a 
grammar for potential user utterances must be prepared 
in advance for each dialog context. For error handling, it 
is also necessary to anticipate correction utterances and 
prepare a correction grammar. We propose a method to 
automatically create the correction grammar based on 
the current dialog context; error detection and repair is 
implemented using the correction grammar. 
To create the correction grammar, the system must 
know the user?s utterances prior to the error, because 
correction utterances typically depend on them. If the 
user?s utterances are consistent with what the system is 
expecting, the correction grammar can be generated 
based on the grammar previously in use by the speech 
recognizer. Therefore, the sequence of grammars used 
in the dialog so far is stored in the grammar history as 
the dialog context, and the correction grammar is 
created using the grammars in this history. 
Most of the forms of correction utterances can be 
expected in advance because correction utterances 
include many repetitions of words or phrases from 
previous turns (Kazemzadeh et al, 2003). We assume 
that the rules to generate the correction grammar can be 
prepared as templates; the correction grammar is created 
by inserting information extracted from the grammar 
history into a template. 
Figure 3 shows an example of a process flow in a 
dialog system which performs error handling based on a 
correction grammar. The ?system prompt n? is the 
process to output the n-th prompt to the user. The 
correction grammar is created based on the grammar 
used in the ?user response n-1?, which is the process to 
recognize the (n-1)-th user utterance, and it is used in 
the ?user response n? together with the ?grammar n? 
which is used to recognize the n-th normal user?s 
utterance. The system detects the error when the user?s 
utterance is recognized using the correction grammar, 
and then transits into the ?correction of errors? to 
modify the error. The grammar history in Figure 3 
stores only the grammar used in the last recognition 
process. The number of grammars stored in the history 
can be changed depending on the dialog management 
strategy and error handling requirements. 
4 Generation of Correction Grammar 
The correction grammar is created as follows. 
(1) Copying the grammar rules in the history 
The user often repeats the same utterance when the 
system misunderstood what s/he spoke. To detect 
when the user repeats exactly the same utterance, the 
grammar rules in the grammar history are copied into 
the correction grammar. 
(2) Inserting the rules in the history into the template 
When the user tries to correct the system error, some 
 
1 
2 
4 
?I would like to know 
the weather for? 
?Tokyo? 
?tomorrow? 
3 
?Tokyo? 
 
Figure 2. Example of the grammar rule used in the 
CAMMIA system 
 
System prompt n-1
Grammar n-1
Generation of 
correction grammar 
Correction 
grammar n-1 
Grammar n 
. 
. 
. 
. 
. 
. 
Template 
Recognized by 
correction grammar ? 
Yes 
No 
User response n-1
System prompt n 
User response n 
Correction of errors 
System prompt n+1 
 
Figure 3. Process flow: Error handling based on a 
correction grammar 
phrases are often added to the original utterance 
(Kitaoka, 2003). The template mentioned above is 
used to support this type of correction utterance. An 
example of the correction grammar rule generated by 
this method is shown in Figure 4. The ?null? in Figure 
4 implies a transition with no condition, and the ?X? 
shows where the original rule is embedded. In this 
example, the created grammar rule in Figure 4(c) 
corresponds to the following sentences: 
? No, I?d like to know the weather for Tokyo. 
? I said I?d like to know the weather for Tokyo. 
(3) Inserting slot-values into the template 
The user often repeats only words or phrases which 
the system is focusing on (Kazemzadeh et al, 2003). 
In a slot-filling dialog, these correspond to the slot 
values. Therefore, correction grammar rules are also 
created by extracting the slot values from the grammar 
in the history and inserting them into the template. If 
there are several slot values that can be corrected at 
the same time, all of their possible combinations and 
permutations are also generated. An example is shown 
in Figure 5. In Figure 5(b), the slot-values are 
?Tokyo? and ?tomorrow?. The grammar rule in Figure 
5(c) includes each slot value plus their combination(s), 
and represents the following sentences: 
? I said Tokyo. 
? I said tomorrow. 
? I said Tokyo tomorrow. 
5 Prototype System with Error Handling 
We have implemented the proposed error handling 
method for a set of Japanese dialog scenarios in the 
CAMMIA system. We added to this system: a) a 
process to create a correction grammar file when the 
system sends a grammar file to the client, b) a process to 
repair errors based on the recognition result, and c) 
transitions to the repair action when the user?s utterance 
is recognized by the correction grammar. 
There are two types of errors: task transition errors 
and slot value errors. If the error is a task transition error, 
the modification process cancels the current task and 
transits to the new task as specified by the correction 
utterance. When the error is a slot value error, the slot 
value is replaced by the value given in the correction 
utterance. However, if the new value is identical to the 
old one, we assume a recognition error and the second 
candidate in the recognition result is used. This 
technique requires a speech recognizer that can output 
N-best results; we used Julius for SAPI (Kyoto Univ., 
2002) for this experiment. 
6 Experiments 
We carried out an experiment to verify whether the 
proposed method works properly in a dialog system. In 
this experiment, dialog systems with and without the 
error handling method were compared. In this 
experiment, a weather information dialog was selected 
as the task for the subjects and about 1200 dialog 
instances were analyzed (both with and without error 
handling). The dialog flow was the same as shown in 
Figure 1. The grammar included 500 words for place 
names, and 69 words for the date. The subjects were 
instructed in advance on the utterance patterns allowed 
by the system, and used only those patterns during the 
experiment. A summary of the collected data is shown 
in Table 1. When error handling is disabled, the system 
returns to the place name query when the user denies the 
system?s confirmation, e.g. it returns from U3 to S1 in 
Figure 1. A comparison of the number of turns in these 
two systems is shown in Table 2. ?1 error? in Table 2 
means that the dialog included one error and ?2 errors? 
means that the same error was repeated. 
The success rate for the task and the average number 
of turns in the dialog (including errors) are tabulated.  
Dialogs including more than 3 errors were regarded as 
incomplete tasks in the calculation of the success rate. 
The results are shown in Table 3. 
 
1 
X 2 
?no? 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like
 
to know 
the weather for? 
2 
?Tokyo? 
 
(b) Grammar rule in the history 
 
3 4 
?I would like
 
to 
know the  
weather for? 
?Tokyo? 
1 
2 5 
?no? 
null 
?I said? 
 
(c) Created correction grammar rule 
Figure 4. Correction grammar created by inserting 
the original rule into a template 
  
1 
X 2 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like to know 
the weather for? 
2 
?Tokyo? 
?tomorrow? 
 
(b) Grammar rule in the history 
 
3 
4 
?Tokyo? 
1 
2 5 
null 
?I said? 
?tomorrow? 
?Tokyo? 
?tomorrow? 
 
(c) Created correction grammar rule 
Figure 5. Correction grammar rules created by 
inserting slot values into a template 
7 Discussion 
The task completion rate was improved from 86.4% to 
93.4% when the proposed error handling method was 
used. The average number of turns was reduced by 3 
turns as shown in Table 3. This result shows that the 
proposed error handling method was working properly 
and effectively. 
One reason that the success rate was improved is 
that the proposed method prevents the repetition of 
errors. When the error handling method is not 
implemented, errors can be easily repeated. The error 
handling method can avoid repeated errors by selecting 
the second candidate in the recognition result even when 
the correction utterance is also misrecognized. There 
were 7 dialogs in which the correction utterance was 
correctly recognized by selecting the second candidate. 
However, there were 13 dialogs in which the error 
was not repaired by one correction utterance. There are 
two explanations. One is that there are insertion errors 
in speech recognition which causes words not spoken to 
appear in the recognition result. For example, the 
system prompt S4 for U3 in Figure 1 becomes as 
follows: 
S4: Did you say Tokyo yesterday? 
In this case, the user has to speak more correction 
utterances. The second explanation is that the 
recognition result did not always include the correct 
result within the first two candidates. It is not clear that 
extending the repair mechanism to always consider 
additional recognition candidates (3rd, 4th, etc.) is a 
viable technique, given the drop off in recognition 
accuracy; more study is required. 
8 Conclusions 
In this paper, we proposed an error handling method 
based on dynamic generation of correction grammars to 
recognize the user corrections which follow system 
errors. The correction grammars detect system errors 
and also repair the dialog flow, improving task 
completion rates and reducing the average number of 
dialog turns. We developed a prototype dialog system 
using the proposed method, and demonstrated 
empirically that the success rate improved by 7.0%, and 
the number of turns was reduced by 3. 
The creation of rules for correction utterances based 
on the dialog history could be applicable to dialog 
systems which use speech recognition or natural 
language processing and other kinds of rules beyond 
regular grammars; we plan to study this in future work. 
We are also planning to develop an algorithm to 
improve the precision of corrections that are based on 
the set of recognition candidates for the correction 
utterance and an error recovery strategy. We also plan to 
apply the proposed method to other types of dialogs, 
such as user-initiative dialogs and mixed-initiative 
dialogs. 
References 
Abe Kazemzadeh, Sungbok Lee and Shrikanth 
Narayanan. 2003. Acoustic Correlates of User 
Response to Error in Human-Computer Dialogues, 
Proceedings of ASRU 2003: 215-220. 
Antal van den Bosch, Emiel Krahmer and Marc 
Swerts. 2001. Detecting problematic turns in 
human-machine interactions: Rule-Induction 
Versus Memory-Based Learning Approaches, 
Proceedings of ACL 2001: 499-507. 
Eric Nyberg, Teruko Mitamura, Paul Placeway, 
Michael Duggan, Nobuo Hataoka. 2002. Dynamic 
Dialog Management with VoiceXML, Proceedings 
of HLT-2002. 
Kyoto University, 2002, Julius Open-Source Real-
Time Large Vocabulary Speech Recognition 
Engine, http://julius.sourceforge.jp. 
Marilyn Walker, Jerry Wright and Irene Langkilde. 
2000. Using Natural Language Processing and 
Discourse Features to Identify Understanding 
Errors in a Spoken Dialogue System, Proceedings 
of ICML-2000: 1111-1118. 
Morena Danieli. 1996. On the Use of Expectations for 
Detecting and Repairing Human-Machine 
Miscommunication, Proceedings of AAAI 
Workshop on Detection, Repair and Prevention of 
Human-Machine Miscommunication: 87-93. 
Norihide Kitaoka, Kaoko Kakutani and Seiichi 
Nakagawa. 2003. Detection and Recognition of 
Correction Utterance in Spontaneously Spoken 
Dialog, Proceedings of EUROSPEECH 2003: 625-
628. 
Susann LuperFoy and David Duff. 1996. Disco: A 
Four-Step Dialog Recovery Program, The 
Proceedings of the AAAI Workshop on Detection, 
Repair and Prevention of Human-Machine 
Miscommunication: 73-76. 
Table 1. Summary of the collected data 
 w/o error handling w/ error handling 
# of users 2 male, 1 female 2 male, 1 female 
# of dialog 603 596 
# of error dialog 66 61 
 
Table 2. Number of turns in the dialog 
 No error 1 error 2 errors 
w/o error handling 13 19 
w/ error handling 7 11 13 
 
Table 3. Success rate and average number of turns 
 Success rate Ave. # turns 
w/o error handling 86.4% 14.6 
w/ error handling 93.4% 11.6 
 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 425?432,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Fast, Accurate Deterministic Parser for Chinese
Mengqiu Wang Kenji Sagae Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{mengqiu,sagae,teruko}@cs.cmu.edu
Abstract
We present a novel classifier-based deter-
ministic parser for Chinese constituency
parsing. Our parser computes parse trees
from bottom up in one pass, and uses
classifiers to make shift-reduce decisions.
Trained and evaluated on the standard
training and test sets, our best model (us-
ing stacked classifiers) runs in linear time
and has labeled precision and recall above
88% using gold-standard part-of-speech
tags, surpassing the best published re-
sults. Our SVM parser is 2-13 times faster
than state-of-the-art parsers, while produc-
ing more accurate results. Our Maxent
and DTree parsers run at speeds 40-270
times faster than state-of-the-art parsers,
but with 5-6% losses in accuracy.
1 Introduction and Background
Syntactic parsing is one of the most fundamental
tasks in Natural Language Processing (NLP). In
recent years, Chinese syntactic parsing has also
received a lot of attention in the NLP commu-
nity, especially since the release of large collec-
tions of annotated data such as the Penn Chi-
nese Treebank (Xue et al, 2005). Corpus-based
parsing techniques that are successful for English
have been applied extensively to Chinese. Tradi-
tional statistical approaches build models which
assign probabilities to every possible parse tree
for a sentence. Techniques such as dynamic pro-
gramming, beam-search, and best-first-search are
then employed to find the parse tree with the high-
est probability. The massively ambiguous nature
of wide-coverage statistical parsing,coupled with
cubic-time (or worse) algorithms makes this ap-
proach too slow for many practical applications.
Deterministic parsing has emerged as an attrac-
tive alternative to probabilistic parsing, offering
accuracy just below the state-of-the-art in syn-
tactic analysis of English, but running in linear
time (Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004). Encour-
aging results have also been shown recently by
Cheng et al (2004; 2005) in applying determin-
istic models to Chinese dependency parsing.
We present a novel classifier-based determin-
istic parser for Chinese constituency parsing. In
our approach, which is based on the shift-reduce
parser for English reported in (Sagae and Lavie,
2005), the parsing task is transformed into a suc-
cession of classification tasks. The parser makes
one pass through the input sentence. At each parse
state, it consults a classifier to make shift/reduce
decisions. The parser then commits to a decision
and enters the next parse state. Shift/reduce deci-
sions are made deterministically based on the lo-
cal context of each parse state, and no backtrack-
ing is involved. This process can be viewed as a
greedy search where only one path in the whole
search space is considered. Our parser produces
both dependency and constituent structures, but in
this paper we will focus on constituent parsing.
By separating the classification task from the
parsing process, we can take advantage of many
machine learning techniques such as classifier en-
semble. We conducted experiments with four
different classifiers: support vector machines
(SVM), Maximum-Entropy (Maxent), Decision
Tree (DTree) and memory-based learning (MBL).
We also compared the performance of three differ-
ent classifier ensemble approaches (simple voting,
classifier stacking and meta-classifier).
Our best model (using stacked classifiers) runs
in linear time and has labeled precision and
recall above 88% using gold-standard part-of-
speech tags, surpassing the best published results
(see Section 5). Our SVM parser is 2-13 times
faster than state-of-the-art parsers, while produc-
425
ing more accurate results. Our Maxent and DTree
parsers are 40-270 times faster than state-of-the-
art parsers, but with 5-6% losses in accuracy.
2 Deterministic parsing model
Like other deterministic parsers, our parser as-
sumes input has already been segmented and
tagged with part-of-speech (POS) information
during a preprocessing step1. The main data struc-
tures used in the parsing algorithm are a queue and
a stack. The input word-POS pairs to be processed
are stored in the queue. The stack holds the partial
parse trees that are built during parsing. A parse
state is represented by the content of the stack and
queue.
The classifier makes shift/reduce decisions
based on contextual features that represent the
parse state. A shift action removes the first item
on the queue and puts it onto the stack. A reduce
action is in the form of Reduce-{Binary|Unary}-
X, where {Binary|Unary} denotes whether one or
two items are to be removed from the stack, and X
is the label of a new tree node that will be domi-
nating the removed items. Because a reduction is
either unary or binary, the resulting parse tree will
only have binary and/or unary branching nodes.
Parse trees are also lexicalized to produce de-
pendency structures. For lexicalization, we used
the same head-finding rules reported in (Bikel,
2004). With this additional information, reduce
actions are now in the form of Reduce-{Binary
|Unary}-X-Direction. The ?Direction? tag gives
information about whether to take the head-node
of the left subtree or the right subtree to be the
head of the new tree, in the case of binary reduc-
tion. A simple transformation process as described
in (Sagae and Lavie, 2005) is employed to con-
vert between arbitrary branching trees and binary
trees. This transformation breaks multi-branching
nodes down into binary-branching nodes by in-
serting temporary nodes; temporary nodes are col-
lapsed and removed when we transform a binary
tree back into a multi-branching tree.
The parsing process succeeds when all the items
in the queue have been processed and there is only
one item (the final parse tree) left on the stack.
If the classifier returns a shift action when there
are no items left on the queue, or a reduce ac-
tion when there are no items on the stack, the
1We constructed our own POS tagger based on SVM; see
Section 3.3.
parser fails. In this case, the parser simply com-
bines all the items on the stack into one IP node,
and outputs this as a partial parse. Sagae and
Lavie (2005) have shown that this algorithm has
linear time complexity, assuming that classifica-
tion takes constant time. The next example il-
lustrates the process for the input ?Y? (Brown)
6? (visits)?0 (Shanghai)? that is tagged with
the POS sequence ?NR (Proper Noun) VV (Verb)
NR (Proper Noun)?.
1. In the initial parsing state, the stack (S) is
empty, and the queue (Q) holds word and
POS tag pairs for the input sentence.
(S): Empty
(Q): NR
Y?
VV
6?
NR
?0
2. The first action item that the classifier gives
is a shift action.
(S): NR
Y?
(Q): VV
6?
NR
?0
3. The next action is a reduce-Unary-NP, which
means reducing the first item on the stack to a
NP node. Node (NRY?) becomes the head
of the new NP node and this information is
marked by brackets. The new parse state is:
(S): NP (NRY?)
NR
Y?
(Q): VV
6?
NR
?0
4. The next action is shift.
(S): NP (NRY?)
NR
Y?
VV
6?
(Q): NR
?0
5. The next action is again shift.
(S): NP (NRY?)
NR
Y?
VV
6?
NR
?0
(Q): Empty
6. The next action is reduce-Unary-NP.
(S): NP (NRY?)
NR
Y?
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
7. The next action is reduce-Binary-VP-Left.
The node (VV6?) will be the head of the
426
new VP node.
(S): NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
8. The next action is reduce-Binary-IP-Right.
Since after the action is performed, there will
be only one tree node(IP) left on the stack and
no items on the queue, this is the final action.
The final state is:
(S): IP (VV6?)
NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
3 Classifiers and Feature Selection
Classification is the key component of our parsing
model. We conducted experiments with four dif-
ferent types of classifiers.
3.1 Classifiers
Support Vector Machine: Support Vector Ma-
chine is a discriminative classification technique
which solves the binary classification problem by
finding a hyperplane in a high dimensional space
that gives the maximum soft margin, based on
the Structural Risk Minimization Principle. We
used the TinySVM toolkit (Kudo and Matsumoto,
2000), with a degree 2 polynomial kernel. To train
a multi-class classifier, we used the one-against-all
scheme.
Maximum-Entropy Classifier: In a
Maximum-entropy model, the goal is to esti-
mate a set of parameters that would maximize
the entropy over distributions that satisfy certain
constraints. These constraints will force the model
to best account for the training data (Ratnaparkhi,
1999). Maximum-entropy models have been used
for Chinese character-based parsing (Fung et al,
2004; Luo, 2003) and POS tagging (Ng and Low,
2004). In our experiments, we used Le?s Maxent
toolkit (Zhang, 2004). This implementation uses
the Limited-Memory Variable Metric method for
parameter estimation. We trained all our models
using 300 iterations with no event cut-off, and
a Gaussian prior smoothing value of 2. Maxent
classifiers output not only a single class label, but
also a number of possible class labels and their
associated probability estimate.
Decision Tree Classifier: Statistical decision
tree is a classic machine learning technique that
has been extensively applied to NLP. For exam-
ple, decision trees were used in the SPATTER sys-
tem (Magerman, 1994) to assign probability dis-
tribution over the space of possible parse trees.
In our experiment, we used the C4.5 decision
tree classifier, and ignored lexical features whose
counts were less than 7.
Memory-Based Learning: Memory-Based
Learning approaches the classification problem
by storing training examples explicitly in mem-
ory, and classifying the current case by finding
the most similar stored cases (using k-nearest-
neighbors). We used the TiMBL toolkit (Daele-
mans et al, 2004) in our experiment, with k = 5.
3.2 Feature selection
For each parse state, a set of features are
extracted and fed to each classifier. Fea-
tures are distributionally-derived or linguistically-
based, and carry the context of a particular parse
state. When input to the classifier, each feature is
treated as a contextual predicate which maps an
outcome and a context to true, false value.
The specific features used with the classifiers
are listed in Table 1.
Sun and Jurafsky (2003) studied the distribu-
tional property of rhythm in Chinese, and used the
rhythmic feature to augment a PCFG model for
a practical shallow parsing task. This feature has
the value 1, 2 or 3 for monosyllabic, bi-syllabic or
multi-syllabic nouns or verbs. For noun and verb
phrases, the feature is defined as the number of
words in the phrase. Sun and Jurafsky found that
in NP and VP constructions there are strong con-
straints on the word length for verbs and nouns
(a kind of rhythm), and on the number of words
in a constituent. We employed these same rhyth-
mic features to see whether this property holds for
the Penn Chinese Treebank data, and if it helps in
the disambiguation of phrase types. Experiments
show that this feature does increase classification
accuracy of the SVM model by about 1%.
In both Chinese and English, there are punctu-
ation characters that come in pairs (e.g., parenthe-
ses). In Chinese, such pairs are more frequent
(quotes, single quotes, and book-name marks).
During parsing, we note how many opening punc-
427
1 A Boolean feature indicates if a closing punctuation is expected or not.
2 A Boolean value indicates if the queue is empty or not.
3 A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not.
4 Last action given by the classifier, and number of words in S(1) and S(2).
5 Headword and its POS of S(1), S(2), S(3) and S(4), and word and POS of Q(1), Q(2), Q(3) and Q(4).
6 Nonterminal label of the root of S(1) and S(2), and number of punctuations in S(1) and S(2).
7 Rhythmic features and the linear distance between the head-words of the S(1) and S(2).
8 Number of words found so far to be dependents of the head-words of S(1) and S(2).
9 Nonterminal label, POS and headword of the immediate left and right child of the root of S(1) and S(2).
10 Most recently found word and POS pair that is to the left of the head-word of S(1) and S(2).
11 Most recently found word and POS pair that is to the right of the head-word of S(1) and S(2).
Table 1: Features for classification
tuations we have seen on the stack. If the number
is odd, then feature 2 will have value 1, otherwise
0. A boolean feature is used to indicate whether or
not an odd number of opening punctuations have
been seen and a closing punctuation is expected;
in this case the feature gives a strong hint to the
parser that all the items in the queue before the
closing punctuation, and the items on the stack
after the opening punctuation should be under a
common constituent node which begins and ends
with the two punctuations.
3.3 POS tagging
In our parsing model, POS tagging is treated as
a separate problem and it is assumed that the in-
put has already been tagged with POS. To com-
pare with previously published work, we evaluated
the parser performance on automatically tagged
data. We constructed a simple POS tagger using
an SVM classifier. The tagger makes two passes
over the input sentence. The first pass extracts fea-
tures from the two words and POS tags that came
before the current word, the two words follow-
ing the current word, and the current word itself
(the length of the word, whether the word con-
tains numbers, special symbols that separates for-
eign first and last names, common Chinese family
names, western alphabets or dates). Then the tag
is assigned to the word according to SVM classi-
fier?s output. In the second pass, additional fea-
tures such as the POS tags of the two words fol-
lowing the current word, and the POS tag of the
current word (assigned in the first pass) are used.
This tagger had a measured precision of 92.5% for
sentences ? 40 words.
4 Experiments
We performed experiments using the Penn Chi-
nese Treebank. Sections 001-270 (3484 sentences,
84,873 words) were used for training, 271-300
(348 sentences, 7980 words) for development, and
271-300 (348 sentences, 7980 words) for testing.
The whole dataset contains 99629 words, which is
about 1/10 of the size of the English Penn Tree-
bank. Standard corpus preparation steps were
done prior to parsing, so that empty nodes were
removed, and the resulting A over A unary rewrite
nodes are collapsed. Functional labels of the non-
terminal nodes are also removed, but we did not
relabel the punctuations, unlike in (Jiang, 2004).
Bracket scoring was done by the EVALB pro-
gram2, and preterminals were not counted as con-
stituents. In all our experiments, we used labeled
recall (LR), labeled precision (LP) and F1 score
(harmonic mean of LR and LP) as our evaluation
metrics.
4.1 Results of different classifiers
Table 2 shows the classification accuracy and pars-
ing accuracy of the four different classifiers on the
development set for sentences ? 40 words, with
gold-standard POS tagging. The runtime (Time)
of each model and number of failed parses (Fail)
are also shown.
Classification Parsing Accuracy
Model Accuracy LR LP F1 Fail Time
SVM 94.3% 86.9% 87.9% 87.4% 0 3m 19s
Maxent 92.6% 84.1% 85.2% 84.6% 5 0m 21s
DTree1 92.0% 78.8% 80.3% 79.5% 42 0m 12s
DTree2 N/A 81.6% 83.6% 82.6% 30 0m 18s
MBL 90.6% 74.3% 75.2% 74.7% 2 16m 11s
Table 2: Comparison of different classifier mod-
els? parsing accuracies on development set for sen-
tences ? 40 words, with gold-standard POS
For the DTree learner, we experimented with
two different classification strategies. In our first
approach, the classification is done in a single
stage (DTree1). The learner is trained for a multi-
2http://nlp.cs.nyu.edu/evalb/
428
class classification problem where the class labels
include shift and all possible reduce actions. But
this approach yielded a lot of parse failures (42 out
of 350 sentences failed during parsing, and par-
tial parse tree was returned). These failures were
mostly due to false shift actions in cases where
the queue is empty. To alleviate this problem, we
broke the classification process down to two stages
(DTree2). A first stage classifier makes a binary
decision on whether the action is shift or reduce.
If the output is reduce, a second-stage classifier de-
cides which reduce action to take. Results showed
that breaking down the classification task into two
stages increased overall accuracy, and the number
of failures was reduced to 30.
The SVM model achieved the highest classifi-
cation accuracy and the best parsing results. It
also successfully parsed all sentences. The Max-
ent model?s classification error rate (7.4%) was
30% higher than the error rate of the SVM model
(5.7%), and its F1 (84.6%) was 3.2% lower than
SVM model?s F1 (87.4%). But Maxent model was
about 9.5 times faster than the SVM model. The
DTree classifier achieved 81.6% LR and 83.6%
LP. The MBL model did not perform well; al-
though MBL and SVM differed in accuracy by
only about 3 percent, the parsing results showed
a difference of more than 10 percent. One pos-
sible explanation for the poor performance of
the MBL model is that all the features we used
were binary features, and memory-based learner
is known to work better with multivalue features
than binary features in natural language learning
tasks (van den Bosch and Zavrel, 2000).
In terms of speed and accuracy trade-off, there
is a 5.5% trade-off in F1 (relative to SVM?s F1)
for a roughly 14 times speed-up between SVM
and two-stage DTree. Maxent is more balanced
in the sense that its accuracy was slightly lower
(3.2%) than SVM, and was just about as fast as the
two-stage DTree on the development set. The high
speed of the DTree and Maxent models make them
very attractive in applications where speed is more
critical than accuracy. While the SVM model
takes more CPU time, we show in Section 5 that
when compared to existing parsers, SVM achieves
about the same or higher accuracy but is at least
twice as fast.
Using gold-standard POS tagging, the best clas-
sifier model (SVM) achieved LR of 87.2% and LP
of 88.3%, as shown in Table 4. Both measures sur-
pass the previously known best results on parsing
using gold-standard tagging. We also tested the
SVM model using data automatically tagged by
our POS tagger, and it achieved LR of 78.1% and
LP of 81.1% for sentences ? 40 words, as shown
in Table 3.
4.2 Classifier Ensemble Experiments
Classifier ensemble by itself has been a fruitful
research direction in machine learning in recent
years. The basic idea in classifier ensemble is
that combining multiple classifiers can often give
significantly better results than any single classi-
fier alone. We experimented with three different
classifier ensemble strategies: classifier stacking,
meta-classifier, and simple voting.
Using the SVM classifier?s results as a baseline,
we tested these approaches on the development
set. In classifier stacking, we collect the outputs
from Maxent, DTree and TiMBL, which are all
trained on a separate dataset from the training set
(section 400-650 of the Penn Chinese Treebank,
smaller than the original training set). We use their
classification output as features, in addition to the
original feature set, to train a new SVM model
on the original training set. We achieved LR of
90.3% and LP of 90.5% on the development set,
a 3.4% and 2.6% improvement in LR and LP, re-
spectively. When tested on the test set, we gained
1% improvement in F1 when gold-standard POS
tagging is used. When tested with automatic tag-
ging, we achieved a 0.5% improvement in F1. Us-
ing Bikel?s significant tester with 10000 times ran-
dom shuffle, the p-value for LR and LP are 0.008
and 0.457, respectively. The increase in recall
is statistically significant, and it shows classifier
stacking can improve performance.
On the other hand, we did not find meta-
classification and simple voting very effective. In
simple voting, we make the classifiers to vote in
each step for every parse action. The F1 of sim-
ple voting method is downgraded by 5.9% rela-
tive to SVM model?s F1. By analyzing the inter-
agreement among classifiers, we found that there
were no cases where Maxent?s top output and
DTree?s output were both correct and SVM?s out-
put was wrong. Using the top output from Maxent
and DTree directly does not seem to be comple-
mentary to SVM.
In the meta-classifier approach, we first col-
lect the output from each classifier trained on sec-
429
MODEL ? 40 words ? 100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel & Chiang 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy & Manning 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong et al 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel?s Thesis 2004 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang & Bikel 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Jiang?s Thesis 2004 80.1% 82.0% 81.1% 92.4% - - - - - - - -
Sun & Jurafsky 2004 85.5% 86.4% 85.9% - - - - - 83.3% 82.2% 82.7% -
DTree model 71.8% 76.9% 74.4% 92.5% 69.2% 74.5% 71.9% 92.2% 68.7% 74.2% 71.5% 92.1%
SVM model 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Stacked classifier model 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Table 3: Comparison with related work on the test set using automatically generated POS
tion 1-210 (roughly 3/4 of the entire training set).
Then specifically for Maxent, we collected the top
output as well as its associated probability esti-
mate. Then we used the outputs and probabil-
ity estimate as features to train an SVM classifier
that makes a decision on which classifier to pick.
Meta-classifier results did not change at all from
our baseline. In fact, the meta-classifier always
picked SVM as its output. This agrees with our
observation for the simple voting case.
5 Comparison with Related Work
Bikel and Chiang (2000) constructed two parsers
using a lexicalized PCFG model that is based on
Collins? model 2 (Collins, 1999), and a statisti-
cal Tree-adjoining Grammar(TAG) model. They
used the same train/development/test split, and
achieved LR/LP of 76.8%/77.8%. In Bikel?s the-
sis (2004), the same Collins emulation model
was used, but with tweaked head-finding rules.
Also a POS tagger was used for assigning tags
for unseen words. The refined model achieved
LR/LP of 78.0%/81.2%. Chiang and Bikel (2002)
used inside-outside unsupervised learning algo-
rithm to augment the rules for finding heads, and
achieved an improved LR/LP of 78.8%/81.1%.
Levy and Manning (2003) used a factored model
that combines an unlexicalized PCFG model with
a dependency model. They achieved LR/LP
of 79.2%/78.4% on a different test/development
split. Xiong et al (2005) used a similar model to
the BBN?s model in (Bikel and Chiang, 2000),
and augmented the model by semantic categori-
cal information and heuristic rules. They achieved
LR/LP of 78.7%/80.1%. Hearne and Way (2004)
used a Data-Oriented Parsing (DOP) approach
that was optimized for top-down computation.
They achieved F1 of 71.3 on a different test and
training set. Jiang (2004) reported LR/LP of
80.1%/82.0% on sentences ? 40 words (results
not available for sentences ? 100 words) by ap-
plying Collins? parser to Chinese. In Sun and
Jurafsky (2004)?s work on Chinese shallow se-
mantic parsing, they also applied Collin?s parser
to Chinese. They reported up-to-date the best
parsing performance on Chinese Treebank. They
achieved LR/LP of 85.5%/86.4% on sentences ?
40 words, and LR/LP of 83.3%/82.2% on sen-
tences ? 100 words, far surpassing all other pre-
viously reported results. Luo (2003) and Fung et
al. (2004) addressed the issue of Chinese text seg-
mentation in their work by constructing character-
based parsers. Luo integrated segmentation, POS
tagging and parsing into one maximum-entropy
framework. He achieved a F1 score of 81.4% in
parsing. But the score was achieved using 90% of
the 250K-CTB (roughly 2.5 times bigger than our
training set) for training and 10% for testing. Fung
et al(2004) also took the maximum-entropy mod-
eling approach, but augmented by transformation-
based learning. They used the standard training
and testing split. When tested with gold-standard
segmentation, they achieved a F1 score of 79.56%,
but POS-tagged words were treated as constituents
in their evaluation.
In comparison with previous work, our parser?s
accuracy is very competitive. Compared to Jiang?s
work and Sun and Jurafsky?s work, the classifier
ensemble model of our parser is lagging behind by
1% and 5.8% in F1, respectively. But compared
to all other works, our classifier stacking model
gave better or equal results for all three measures.
In particular, the classifier ensemble model and
SVM model of our parser achieved second and
third highest LP, LR and F1 for sentences ? 100
words as shown in Table 3. (Sun and Jurafsky did
not report results on sentences ? 100 words, but
it is worth noting that out of all the test sentences,
430
only 2 sentences have length > 100).
Jiang (2004) and Bikel (2004)3 also evaluated
their parsers on the test set for sentences ? 40
words, using gold-standard POS tagged input. Our
parser gives significantly better results as shown
in Table 4. The implication of this result is two-
fold. On one hand, it shows that if POS tagging
accuracy can be increased, our parser is likely to
benefit more than the other two models; on the
other hand, it also indicates that our deterministic
model is less resilient to POS errors. Further de-
tailed analysis is called for, to study the extent to
which POS tagging errors affects the deterministic
parsing model.
Model LR LP F1
Bikel?s Thesis 2004 80.9% 84.5% 82.7%
Jiang?s Thesis 2004 84.5% 88.0% 86.2%
DTree model 80.5% 83.9% 82.2%
Maxent model 81.4% 82.8% 82.1%
SVM model 87.2% 88.3% 87.8%
Stacked classifier model 88.3% 88.1% 88.2%
Table 4: Comparison with related work on the test
set for sentence ? 40 words, using gold-standard
POS
To measure efficiency, we ran two publicly
available parsers (Levy and Manning?s PCFG
parser (2003) and Bikel?s parser (2004)) on
the standard test set and compared the run-
time4. The runtime of these parsers are shown
in minute:second format in Table 5. Our SVM
model is more than 2 times faster than Levy and
Manning?s parser, and more than 13 times faster
than Bikel?s parser. Our DTree model is 40 times
faster than Levy and Manning?s parser, and 270
times faster than Bikel?s parser. Another advan-
tage of our parser is that it does not take as much
memory as these other parsers do. In fact, none
of the models except MBL takes more than 60
megabytes of memory at runtime. In compari-
son, Levy and Manning?s PCFG parser requires
more than 400 mega-bytes of memory when pars-
ing long sentences (70 words or longer).
6 Discussion and future work
One unique attraction of this deterministic pars-
ing framework is that advances in machine learn-
ing field can be directly applied to parsing, which
3Bikel?s parser used gold-standard POS tags for unseen
words only. Also, the results are obtained from a parser
trained on 250K-CTB, about 2.5 times bigger than CTB 1.0.
4All the experiments were conducted on a Pentium IV
2.4GHz machine with 2GB of RAM.
Model runtime
Bikel 54m 6s
Levy & Manning 8m 12s
Our DTree model 0m 14s
Our Maxent model 0m 24s
Our SVM model 3m 50s
Table 5: Comparison of parsing speed
opens up lots of possibilities for continuous im-
provements, both in terms of accuracy and effi-
ciency. For example, in this paper we experi-
mented with one method of simple voting. An al-
ternative way of doing simple voting is to let the
parsers vote on membership of constituents after
each parser has produced its own parse tree (Hen-
derson and Brill, 1999), instead of voting at each
step during parsing.
Our initial attempt to increase the accuracy of
the DTree model by applying boosting techniques
did not yield satisfactory results. In our exper-
iment, we implemented the AdaBoost.M1 (Fre-
und and Schapire, 1996) algorithm using re-
sampling to vary the training set distribution.
Results showed AdaBoost suffered severe over-
fitting problems and hurts accuracy greatly, even
with a small number of samples. One possible
reason for this is that our sample space is very
unbalanced across the different classes. A few
classes have lots of training examples while a large
number of classes are rare, which could raise the
chance of overfitting.
In our experiments, SVM model gave better re-
sults than the Maxent model. But it is important
to note that although the same set of features were
used in both models, a degree 2 polynomial ker-
nel was used in the SVM classifier while Maxent
only has degree 1 features. In our future work, we
will experiment with degree 2 features and L1 reg-
ularization in the Maxent model, which may give
us closer performance to the SVM model with a
much faster speed.
7 Conclusion
In this paper, we presented a novel determinis-
tic parser for Chinese constituent parsing. Us-
ing gold-standard POS tags, our best model (us-
ing stacked classifiers) runs in linear time and has
labeled recall and precision of 88.3% and 88.1%,
respectively, surpassing the best published results.
And with a trade-off of 5-6% in accuracy, our
DTree and Maxent parsers run at speeds 40-270
times faster than state-of-the-art parsers. Our re-
431
sults have shown that the deterministic parsing
framework is a viable and effective approach to
Chinese parsing. For future work, we will fur-
ther improve the speed and accuracy of our mod-
els, and apply them to more Chinese and multi-
lingual natural language applications that require
high speed and accurate parsing.
Acknowledgment
This work was supported in part by ARDA?s
AQUAINT Program. We thank Eric Nyberg for
his help during the final preparation of this paper.
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop, ACL ?00.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2004. Deterministic dependency structure
analyzer for Chinese. In Proceedings of IJCNLP
?04.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Machine learning-based dependency
analyzer for Chinese. In Proceedings of ICCC ?05.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING ?02.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl version 5.1 ref-
erence guide. Technical report, Tilburg University.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML ?96.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy Chinese
parser augmented by transformation-based learning.
ACM Transactions on Asian Language Information
Processing, 3(2):159?168.
Mary Hearne and Andy Way. 2004. Data-oriented
parsing and the Penn Chinese Treebank. In Proceed-
ings of IJCNLP ?04.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combining
parsers. In Proceedings of EMNLP ?99.
Zhengping Jiang. 2004. Statistical Chinese parsing.
Honours thesis, National University of Singapore.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL and LLL ?00.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL ?03.
Xiaoqiang Luo. 2003. A maximum entropy Chinese
character-based parser. In Proceedings of EMNLP
?03.
David M. Magerman. 1994. Natural Language Pars-
ing as Statistical Pattern Recognition. Ph.D. thesis,
Stanford University.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP ?04.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING ?04.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the IWPT ?05.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop ?03.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantic parsing of Chinese. In Proceedings of the
HLT/NAACL ?04.
Antal van den Bosch and Jakub Zavrel. 2000. Un-
packing multi-valued symbolic features and classes
in memory-based language learning. In Proceedings
of ICML ?00.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
of IJCNLP ?05.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT ?03.
Le Zhang, 2004. Maximum Entropy Modeling Toolkit
for Python and C++. Reference Manual.
432
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784?791,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Language-independent Probabilistic Answer Ranking
for Question Answering
Jeongwoo Ko, Teruko Mitamura, Eric Nyberg
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{jko, teruko, ehn}@cs.cmu.edu
Abstract
This paper presents a language-independent
probabilistic answer ranking framework for
question answering. The framework esti-
mates the probability of an individual an-
swer candidate given the degree of answer
relevance and the amount of supporting evi-
dence provided in the set of answer candi-
dates for the question. Our approach was
evaluated by comparing the candidate an-
swer sets generated by Chinese and Japanese
answer extractors with the re-ranked answer
sets produced by the answer ranking frame-
work. Empirical results from testing on NT-
CIR factoid questions show a 40% perfor-
mance improvement in Chinese answer se-
lection and a 45% improvement in Japanese
answer selection.
1 Introduction
Question answering (QA) systems aim at find-
ing precise answers to natural language questions
from large document collections. Typical QA sys-
tems (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2000) adopt a pipeline architec-
ture that incorporates four major steps: (1) question
analysis, (2) document retrieval, (3) answer extrac-
tion and (4) answer selection. Question analysis is
a process which analyzes a question and produces a
list of keywords. Document retrieval is a step that
searches for relevant documents or passages. An-
swer extraction extracts a list of answer candidates
from the retrieved documents. Answer selection is a
process which pinpoints correct answer(s) from the
extracted candidate answers.
Since the first three steps in the QA pipeline may
produce erroneous outputs, the final answer selec-
tion step often entails identifying correct answer(s)
amongst many incorrect ones. For example, given
the question ?Which Chinese city has the largest
number of foreign financial companies??, the an-
swer extraction component produces a ranked list of
five answer candidates: Beijing (AP880603-0268)1,
Hong Kong (WSJ920110-0013), Shanghai (FBIS3-
58), Taiwan (FT942-2016) and Shanghai (FBIS3-
45320). Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) can be ranked in
the first position, and the correct answer (?Shang-
hai?) was extracted from two different documents
and ranked in the third and the fifth positions. In or-
der to rank ?Shanghai? in the top position, we have
to address two interesting challenges:
? Answer Similarity. How do we exploit simi-
larity among answer candidates? For example,
when the candidates list contains redundant an-
swers (e.g., ?Shanghai? as above) or several an-
swers which represent a single instance (e.g.
?U.S.A.? and ?the United States?), how much
should we boost the rank of the redundant an-
swers?
? Answer Relevance. How do we identify
relevant answer(s) amongst irrelevant ones?
This task may involve searching for evi-
dence of a relationship between the answer
1Answer candidates are shown with the identifier of the
TREC document where they were found.
784
and the answer type or a question key-
word. For example, we might wish to query
a knowledge base to determine if ?Shang-
hai? is a city (IS-A(Shanghai, city)),
or to determine if Shanghai is in China
(IS-IN(Shanghai, China)).
The first challenge is to exploit redundancy in the
set of answer candidates. As answer candidates are
extracted from different documents, they may con-
tain identical, similar or complementary text snip-
pets. For example, ?U.S.? can appear as ?United
States? or ?USA? in different documents. It is im-
portant to detect redundant information and boost
answer confidence, especially for list questions that
require a set of unique answers. One approach is
to perform answer clustering (Nyberg et al, 2002;
Jijkoun et al, 2006). However, the use of cluster-
ing raises additional questions: how to calculate the
score of the clustered answers, and how to select the
cluster label.
To address the second question, several answer
selection approaches have used external knowledge
resources such as WordNet, CYC and gazetteers for
answer validation or answer reranking. Answer can-
didates are either removed or discounted if they are
not of the expected answer type (Xu et al, 2002;
Moldovan et al, 2003; Chu-Carroll et al, 2003;
Echihabi et al, 2004). The Web also has been used
for answer reranking by exploiting search engine re-
sults produced by queries containing the answer can-
didate and question keywords (Magnini et al, 2002).
This approach has been used in various languages
for answer validation. Wikipedia?s structured in-
formation was used for Spanish answer type check-
ing (Buscaldi and Rosso, 2006).
Although many QA systems have incorporated in-
dividual features and/or resources for answer selec-
tion in a single language, there has been little re-
search on a generalized probabilistic framework that
supports answer ranking in multiple languages using
any answer relevance and answer similarity features
that are appropriate for the language in question.
In this paper, we describe a probabilistic answer
ranking framework for multiple languages. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer relevance features and answer sim-
ilarity features. An existing framework which was
originally developed for English (Ko et al, 2007)
was extended for Chinese and Japanese answer
ranking by incorporating language-specific features.
Empirical results on NTCIR Chinese and Japanese
factoid questions show that the framework signifi-
cantly improved answer selection performance; Chi-
nese performance improved by 40% over the base-
line, and Japanese performance improved by 45%
over the baseline.
The remainder of this paper is organized as fol-
lows: Section 2 contains an overview of the answer
ranking task. Section 3 summarizes the answer rank-
ing framework. In Section 4, we explain how we
extended the framework by incorporating language-
specific features. Section 5 describes the experimen-
tal methodology and results. Finally, Section 6 con-
cludes with suggestions for future research.
2 Answer Ranking Task
The relevance of an answer to a question can be es-
timated by the probability P(correct(Ai) |Ai, Q),
where Q is a question and Ai is an answer can-
didate. To exploit answer similarity, we estimate
the probability P (correct(Ai) |Ai, Aj), where Aj
is similar to Ai. Since both probabilities influence
overall answer ranking performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
The estimated probability is used to rank answer
candidates and select final answers from the list. For
factoid questions, the top answer is selected as a fi-
nal answer to the question. In addition, we can use
the estimated probability to classify incorrect an-
swers: if the probability of an answer candidate is
lower than 0.5, it is considered to be a wrong answer
and is filtered out of the answer list. This is useful
in deciding whether or not a valid answer to a ques-
tion exists in a given corpus (Voorhees, 2002). The
estimated probability can also be used in conjunc-
tion with a cutoff threshold when selecting multiple
answers to list questions.
3 Answer Ranking Framework
This section summarizes our answer ranking frame-
work, originally developed for English answers (Ko
785
P (correct(Ai)|Q,A1, ..., An)
? P (correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
Figure 1: Estimating correctness of an answer candidate given a question and a set of answer candidates
et al, 2007). The model uses logistic regression
to estimate the probability of an answer candidate
(Figure 1). Each relk(Ai) is a feature function used
to produce an answer relevance score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer relevance and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric, sim?k(Ai, Aj). For example, if Levenshtein
distance is used as one similarity metric, simk(Ai)
is calculated by summing N-1 Levenshtein distances
between one answer candidate and all other candi-
dates.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood. We used
the Quasi-Newton algorithm (Minka, 2003) for pa-
rameter estimation.
Multiple features were used to generate answer
relevance scores and answer similarity scores; these
are discussed below.
3.1 Answer Relevance Features
Answer relevance features can be classified into
knowledge-based features or data-driven features.
1) Knowledge-based features
Gazetteers: Gazetteers provide geographic infor-
mation, which allows us to identify strings as in-
stances of countries, their cities, continents, capitals,
etc. For answer ranking, three gazetteer resources
were used: the Tipster Gazetteer, the CIA World
Factbook and information about the US states pro-
vided by 50states.com. These resources were used
to assign an answer relevance score between -1 and
1 to each candidate. For example, given the question
?Which city in China has the largest number of for-
eign financial companies??, the candidate ?Shang-
hai? receives a score of 0.5 because it is a city in the
gazetteers. But ?Taiwan? receives a score of -1.0 be-
cause it is not a city in the gazetteers. A score of 0
means the gazetteers did not contribute to the answer
selection process for that candidate.
Ontology: Ontologies such as WordNet contain
information about relationships between words and
general meaning types (synsets, semantic categories,
etc.). WordNet was used to identify answer rele-
vance in a manner analogous to the use of gazetteers.
For example, given the question ?Who wrote the
book ?Song of Solomon???, the candidate ?Mark
Twain? receives a score of 0.5 because its hyper-
nyms include ?writer?.
2) Data-driven features
Wikipedia: Wikipedia was used to generate an an-
swer relevance score. If there is a Wikipedia docu-
ment whose title matches an answer candidate, the
document is analyzed to obtain the term frequency
(tf) and the inverse term frequency (idf) of the can-
didate, from which a tf.idf score is calculated. When
there is no matched document, each question key-
word is also processed as a back-off strategy, and the
answer relevance score is calculated by summing the
tf.idf scores obtained from individual keywords.
Google: Following Magnini et al (2002), a query
consisting of an answer candidate and question key-
786
words was sent to the Google search engine. Then
the top 10 text snippets returned by Google were
analyzed to generate an answer relevance score by
computing the minimum number of words between
a keyword and the answer candidate.
3.2 Answer Similarity Features
Answer similarity is calculated using multiple string
distance metrics and a list of synonyms.
String Distance Metrics: String distance metrics
such as Levenshtein, Jaro-Winkler, and Cosine sim-
ilarity were used to calculate the similarity between
two English answer candidates.
Synonyms: Synonyms can be used as another
metric to calculate answer similarity. If one answer
is synonym of another answer, the score is 1. Other-
wise the score is 0. To get a list of synonyms, three
knowledge bases were used: WordNet, Wikipedia
and the CIA World Factbook. In addition, manually
generated rules were used to obtain synonyms for
different types of answer candidates. For example,
?April 12 1914? and ?12th Apr. 1914? are converted
into ?1914-04-12? and treated as synonyms.
4 Extensions for Multiple Languages
We extended the framework for Chinese and
Japanese QA. This section details how we incor-
porated language-specific resources into the frame-
work. As logistic regression is based on a proba-
bilistic framework, the model does not need to be
changed to support other languages. We only re-
trained the model for individual languages. To sup-
port Chinese and Japanese QA, we incorporated new
features for individual languages.
4.1 Answer Relevance Features
We replaced the English gazetteers and WordNet
with language-specific resources for Japanese and
Chinese. As Wikipedia and the Web support mul-
tiple languages, the same algorithm was used in
searching language-specific corpora for the two lan-
guages.
1) Knowledge-based features
The knowledge-based features involve searching for
facts in a knowledge base such as gazetteers and
WordNet. We utilized comparable resources for
Chinese and Japanese. Using language-specific re-
#Articles
Language Nov. 2005 Aug. 2006
English 1,811,554 3,583,699
Japanese 201,703 446,122
Chinese 69,936 197,447
Table 1: Articles in Wikipedia for different lan-
guages
sources, the same algorithms were applied to gener-
ate an answer relevance score between -1 and 1.
Gazetteers: There are few available gazetteers
for Chinese and Japanese. Therefore, we extracted
location data from language-specific resources. For
Japanese, we extracted Japanese location informa-
tion from Yahoo2, which contains many location
names in Japan and the relationships among them.
For Chinese, we extracted location names from the
Web. In addition, we translated country names pro-
vided by the CIA World Factbook and the Tipster
gazetteers into Chinese and Japanese names. As
there is more than one translation, top 3 translations
were used.
Ontology: For Chinese, we used HowNet (Dong,
2000) which is a Chinese version of WordNet.
It contains 65,000 Chinese concepts and 75,000
corresponding English equivalents. For Japanese,
we used semantic classes provided by Gengo
GoiTaikei3. Gengo GoiTaikei is a Japanese lexicon
containing 300,000 Japanese words with their asso-
ciated 3,000 semantic classes. The semantic infor-
mation provided by HowNet and Gengo GoiTaikei
was used to assign an answer relevance score be-
tween -1 and 1.
2) Data-driven features
Wikipedia: As Wikipedia supports more than 200
language editions, the approach used in English can
be used for different languages without any modifi-
cation. Table 1 shows the number of text articles in
three different languages. Wikipedia?s current cov-
erage in Japanese and Chinese does not match its
coverage in English, but coverage in these languages
continues to improve.
To supplement the small corpus of Chi-
nese documents available, we used Baidu
2http://map.yahoo.co.jp/
3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei
787
(http://baike.baidu.com), which is similar to
Wikipedia but contains more articles written in
Chinese. We first search for Chinese Wikipedia.
When there is no matching document in Wikipedia,
each answer candidate is sent to Baidu and the
retrieved document is analyzed in the same way to
analyze Wikipedia documents.
The idf score was calculated using word statis-
tics from Japanese Yomiuri newspaper corpus and
the NTCIR Chinese corpus.
Google: The same algorithm was applied to ana-
lyze Japanese and Chinese snippets returned from
Google. But we restricted the language to Chi-
nese or Japanese so that Google returned only Chi-
nese or Japanese documents. To calculate the dis-
tance between an answer candidate and question
keywords, segmentation was done with linguistic
tools. For Japanese, Chasen4 was used. For Chinese
segmentation, a maximum-entropy based parser was
used (Wang et al, 2006).
3) Manual Filtering
Other than the features mentioned above, we man-
ually created many rules for numeric and temporal
questions to filter out invalid answers. For example,
when the question is looking for a year as an answer,
an answer candidate which contains only the month
receives a score of -1. Otherwise, the score is 0.
4.2 Answer Similarity Features
The same features used for English were applied
to calculate the similarity of Chinese/Japanese an-
swer candidates. To identify synonyms, Wikipedia
were used for both Chinese and Japanese. EIJIRO
dictionary was used to obtain Japanese synonyms.
EIJIRO is a English-Japanese dictionary contain-
ing 1,576,138 words and provides synonyms for
Japanese words.
As there are several different ways to represent
temporal and numeric expressions (Nyberg et al,
2002; Greenwood, 2006), language-specific conver-
sion rules were applied to convert them into a canon-
ical format; for example, a rule to convert Japanese
Kanji characters to Arabic numbers is shown in Fig-
ure 2.
4http://chasen.aist-nara.ac.jp/hiki/ChaSen
0.25
??
??
1993-
07-04
1993 
? 7 ?
4 ?
50 %
??
1993-
07-04
??
??
?
??
??
3E+11
 ?
3,000
??
3E+11
 ?
??
 ? ?
Norm
alized
 answ
er stri
ng
Origin
al ans
wer st
ring
Figure 2: Example of normalized answer strings
5 Experiments
This section describes the experiments to evaluate
the extended answer ranking framework for Chinese
and Japanese QA.
5.1 Experimental Setup
We used Chinese and Japanese questions provided
by the NTCIR (NII Test Collection for IR Sys-
tems), which focuses on evaluating cross-lingual
and monolingual QA tasks for Chinese, Japanese
and English. For Chinese, a total of 550 fac-
toid questions from the NTCIR5-6 QA evaluations
served as the dataset. Among them, 200 questions
were used to train the Chinese answer extractor and
350 questions were used to evaluate our answer
ranking framework. For Japanese, 700 questions
from the NTCIR5-6 QA evaluations served as the
dataset. Among them, 300 questions were used to
train the Japanese answer extractor and 400 ques-
tions were used to evaluate our framework.
Both the Chinese and Japanese answer extractors
use maximum-entropy to extract answer candidates
based on multiple features such as named entity, de-
pendency structures and some language-dependent
features.
Performance of the answer ranking framework
was measured by average answer accuracy: the
number of correct top answers divided by the num-
ber of questions where at least one correct answer
exists in the candidate list provided by an extrac-
tor. Mean Reciprocal Rank (MRR5) was also used
to calculate the average reciprocal rank of the first
correct answer in the top 5 answers.
The baseline for average answer accuracy was
calculated using the answer candidate likelihood
scores provided by each individual extractor; the
788
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ja
pa
ne
se
 A
ns
we
r S
ele
cti
on
 
 B
as
eli
ne
 Fr
am
ew
ork
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ch
ine
se
 A
ns
we
r S
ele
cti
on
 
Avgerage Accuracy
 B
as
eli
ne
 Fr
am
ew
ork
Figure 3: Performance of the answer ranking framework for Chinese and Japanese answer selection (TOP1:
average accuracy of top answer, TOP3: average accuracy of top 3 answers, MRR5: average of mean recip-
rocal rank of top 5 answers)
answer with the best extractor score was chosen,
and no validation or similarity processing was per-
formed.
3-fold cross-validation was performed, and we
used a version of Wikipedia downloaded in Aug
2006.
5.2 Results and Analysis
We first analyzed the average accuracy of top 1, top3
and top 5 answers. Figure 3 compares the average
accuracy using the baseline and the answer selec-
tion framework. As can be seen, the answer rank-
ing framework significantly improved performance
on both Chinese and Japanese answer selection. As
for the average top answer accuracy, there were 40%
improvement over the baseline (Chinese) and 45%
improvement over the baseline (Japanese).
We also analyzed the degree to which the average
accuracy was affected by answer similarity and rel-
evance features. Table 2 compares the average top
answer accuracy using the baseline, the answer rel-
evance features, the answer similarity features and
all feature combinations. Both the similarity and the
relevance features significantly improved answer se-
lection performance compared to the baseline, and
combining both sets of features together produced
the best performance.
We further analyzed the utility of individual rele-
vance features (Figure 4). For both languages, filter-
ing was useful in ruling out wrong answers. The im-
Baseline Rel Sim All
Chinese 0.442 0.482 0.597 0.619
Japanese 0.367 0.463 0.502 0.532
Table 2: Average top answer accuracy of individ-
ual features (Rel: merging relevance features, Sim:
merging similarity features, ALL: merging all fea-
tures).
pact of the ontology was more positive for Japanese;
we assume that this is because the Chinese ontol-
ogy (HowNet) contains much less information over-
all than the Japanese ontology (Gengo GoiTaikei).
The comparative impact of Wikipedia was similar.
For Chinese, there were many fewer Wikipedia doc-
uments available. Even though we used Baidu as a
supplemental resource for Chinese, this did not im-
prove answer selection performance. On the other
hand, the use of Wikipedia was very helpful for
Japanese, improving performance by 26% over the
baseline. This shows that the quality of answer
relevance estimation is significantly affected by re-
source coverage.
When comparing the data-driven features with the
knowledge-based features, the data-driven features
(such as Wikipedia and Google) tended to increase
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet).
Table 3 shows the effect of individual similar-
ity features on Chinese and Japanese answer selec-
789
Ba
se
lin
eF
IL
ON
T
GA
Z
GL
W
IK
I
All
0.3
0
0.3
5
0.4
0
0.4
5
0.5
0
0.5
5
Avg. Top Answer Accuracy
 C
hin
es
e
 Ja
pa
ne
se
Figure 4: Average top answer accuracy of individ-
ual answer relevance features.(FIL: filtering, ONT,
ontology, GAZ: gazetteers, GL: Google, WIKI:
Wikipedia, ALL: combination of all relevance fea-
tures)
Chinese Japanese
0.3 0.5 0.3 0.5
Cosine 0.597 0.597 0.488 0.488
Jaro-Winkler 0.544 0.518 0.410 0.415
Levenshtein 0.558 0.544 0.434 0.449
Synonyms 0.527 0.527 0.493 0.493
All 0.588 0.580 0.502 0.488
Table 3: Average accuracy using individual similar-
ity features under different thresholds: 0.3 and 0.5
(?All?: combination of all similarity metrics)
tion. As some string similarity features (e.g., Lev-
enshtein distance) produce a number between 0 and
1 (where 1 means two strings are identical and 0
means they are different), similarity scores less than
a threshold can be ignored. We used two thresh-
olds: 0.3 and 0.5. In our experiments, using 0.3
as a threshold produced better results in Chinese.
In Japanese, 0,5 was a better threshold for individ-
ual features. Among three different string similar-
ity features (Levenshtein, Jaro-Winkler and Cosine
similarity), cosine similarity tended to perform bet-
ter than the others.
When comparing synonym features with string
similarity features, synonyms performed better than
string similarity in Japanese, but not in Chinese. We
had many more synonyms available for Japanese
Data-driven features All features
Chinese 0.606 0.619
Japanese 0.517 0.532
Table 4: Average top answer accuracy when using
data-driven features v.s. when using all features.
and they helped the system to better exploit answer
redundancy.
We also analyzed answer selection performance
when combining all four similarity features (?All?
in Table 3). Combining all similarity features im-
proved the performance in Japanese, but hurt the
performance in Chinese, because adding a small set
of synonyms to the string metrics worsened the per-
formance of logistic regression.
5.3 Utility of data-driven features
In our experiments we used data-driven fea-
tures as well as knowledge-based features. As
knowledge-based features need manual effort to ac-
cess language-specific resources for individual lan-
guages, we conducted an additional experiment only
with data-driven features in order to see how much
performance gain is available without the manual
work. As Google, Wikipedia and string similarity
metrics can be used without any additional manual
effort when extended to other languages, we used
these three features and compared the performance.
Table 4 shows the performance when using data-
driven features v.s. all features. It can be seen that
data-driven features alone achieved significant im-
provement over the baseline. This indicates that the
framework can easily be extended to any language
where appropriate data resources are available, even
if knowledge-based features and resources for the
language are still under development.
6 Conclusion
In this paper, we presented a generalized answer se-
lection framework which was applied to Chinese and
Japanese question answering. An empirical evalu-
ation using NTCIR test questions showed that the
framework significantly improves baseline answer
selection performance. For Chinese, the perfor-
mance improved by 40% over the baseline. For
Japanese, the performance improved by 45% over
790
the baseline. This shows that our probabilistic
framework can be easily extended for multiple lan-
guages by reusing data-driven features (with new
corpora) and adding language-specific resources
(ontologies, gazetteers) for knowledge-based fea-
tures.
In our previous work, we evaluated the perfor-
mance of the framework for English QA using ques-
tions from past TREC evaluations (Ko et al, 2007).
The experimental results showed that the combina-
tion of all answer ranking features improved per-
formance by an average of 102% over the baseline.
The relevance features improved performance by an
average of 99% over the baseline, and the similar-
ity features improved performance by an average of
46% over the baseline. Our hypothesis is that answer
relevance features had a greater impact for English
QA because the quality and coverage of the data re-
sources available for English answer validation is
much higher than the quality and coverage of ex-
isting resources for Japanese and Chinese. In future
work, we will continue to evaluate the robustness of
the framework. It is also clear from our comparison
with English QA that more work can and should be
done in acquiring data resources for answer valida-
tion in Chinese and Japanese.
Acknowledgments
We would like to thank Hideki Shima, Mengqiu
Wang, Frank Lin, Justin Betteridge, Matthew
Bilotti, Andrew Schlaikjer and Luo Si for their valu-
able support. This work was supported in part
by ARDA/DTO AQUAINT program award number
NBCHC040164.
References
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Fer-
rucci. 2003. A Multi-Strategy and Multi-Source Ap-
proach to Question Answering. In Proceedings of Text
REtrieval Conference.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
Zhendong Dong. 2000. Hownet:
http://www.keenage.com.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2004. How to select an answer
string? In T. Strzalkowski and S. Harabagiu, editors,
Advances in Textual Question Answering. Kluwer.
Mark A. Greenwood. 2006. Open-Domain Question An-
swering. Thesis.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunsecu, R. Girju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge for
answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Frame-
work for Answer Selection in Question Answering. In
Proceedings of NAACL/HLT.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of Text REtrieval Conference.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
E. Voorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of Text REtrieval
Conference.
M. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Ac-
curate Deterministic Parser for Chinese. In Proceed-
ings of COLING/ACL.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2002. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of Text RE-
trieval Conference.
791
 
		
Interlingual Annotation of Multilingual Text Corpora 
 
Stephen Helmreich 
David Farwell 
Computing Research Laboratory 
New Mexico State University 
david@crl.nmsu.edu
shelmrei@crl.nmsu.edu
Florence Reeder 
Keith Miller 
Information Discovery & Understanding 
MITRE Corporation 
freeder@mitre.org
keith@mitre.org   
Bonnie Dorr 
Nizar Habash 
Institute for Advanced Computer Studies 
University of Maryland 
bonnie@umiacs.umd.edu
habash@umiacs.umd.edu
 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
hovy@isi.edu 
Lori Levin 
Teruko Mitamura 
Language Technologies Institute 
Carnegie Mellon University 
lsl@cs.cmu.edu
teruko@cs.cmu.edu 
Owen Rambow 
Advaith Siddharthan 
Department of Computer Science 
Columbia University 
rambow@cs.columbia.edu
as372@cs.columbia.edu  
 
 
Abstract 
This paper describes a multi-site project to 
annotate six sizable bilingual parallel corpora 
for interlingual content. After presenting the 
background and objectives of the effort, we 
describe the data set that is being annotated, 
the interlingua representation language used, 
an interface environment that supports the an-
notation task and the annotation process itself. 
We will then present a preliminary version of 
our evaluation methodology and conclude 
with a summary of the current status of the 
project along with a number of issues which 
have arisen.  
1 Introduction 
This paper describes a multi-site National Science 
Foundation project focusing on the annotation of six 
sizable bilingual parallel corpora for interlingual content 
with the goal of providing a significant data set for im-
proving knowledge-based approaches to machine trans-
lation (MT) and a range of other Natural Language 
Processing (NLP) applications. The project participants 
include the Computing Research Laboratory at NMSU, 
the Language Technologies Institute at CMU, the In-
formation Science Institute at USC, UMIACS at the 
University of Maryland, the MITRE Corporation and 
Columbia University. In the remainder of the paper, we 
first present the background and objectives of the pro-
ject. We then describe the data set that is being anno-
tated, the interlingual representation language being 
used, an interface environment that is designed to sup-
port the annotation task, and the process of annotation 
itself. We will then outline a preliminary version of our 
evaluation methodology and conclude with a summary 
of the current status of the project along with a set of 
issues that have arisen since the project began.  
2 Project Goals and Expected Outcomes 
The central goals of the project are: 
? to produce a practical, commonly-shared system 
for representing the information conveyed by a 
text, or ?interlingua?, 
? to develop a methodology for accurately and 
consistently assigning such representations to 
texts across languages and across annotators, 
? to annotate a sizable multilingual parallel corpus 
of source language texts and translations for IL 
content. 
This corpus is expected to serve as a basis for improving 
meaning-based approaches to MT and a range of other 
natural language technologies.  The tools and annotation 
standards will serve to facilitate more rapid annotation 
of texts in the future. 
3 Corpus 
The target data set is modeled on and an extension of 
the DARPA MT Evaluation data set (White and 
O?Connell 1994) and includes data from the Linguistic 
Data Consortium (LDC) Multiple Translation Arabic, 
Part 1 (Walker et al, 2003). The data set consists of 6 
bilingual parallel corpora. Each corpus is made up of 
125 source language news articles along with three 
translations into English, each produced independently 
by different human translators. However, the source 
news articles for each individual language corpus are 
different from the source articles in the other language 
corpora.  Thus, the 6 corpora themselves are comparable 
to each other rather than parallel. The source languages 
are Japanese, Korean, Hindi, Arabic, French and Span-
ish.  Typically, each article is between 300 and 400 
words long (or the equivalent) and thus each corpus has 
between 150,00 and 200,000 words. Consequently, the 
size of the entire data set is around 1,000,000 words. 
Thus, for any given corpus, the annotation effort is 
to assign interlingual content to a set of 4 parallel texts, 
3 of which are in the same language, English, and all of 
which theoretically communicate the same information. 
The following is an example set from the Spanish cor-
pus: 
S: Atribuy? esto en gran parte a
una pol?tica que durante muchos a?os
tuvo un "sesgo concentrador" y repre-
sent? desventajas para las clases me-
nos favorecidas.
T1: He attributed this in great
part to a type of politics that
throughout many years possessed a
"concentrated bias" and represented
disadvantages for the less favored
classes.
T2: To a large extent, he attrib-
uted that fact to a policy which had
for many years had a "bias toward
concentration" and represented disad-
vantages for the less favored
classes.
T3: He attributed this in great
part to a policy that had a "centrist
slant" for many years and represented
disadvantages for the less-favored
classes.
 
The annotation process involves identifying the 
variations between the translations and then assessing 
whether these differences are significant. In this case, 
the translations are, for the most part, the same although 
there are a few interesting variations.  
For instance, where this appears as the translation 
of esto in the first and third translations, that fact 
appears in the second. The translator choice potentially 
represents an elaboration of the semantic content of the 
source expression and the question arises as to whether 
the annotation of the variation in expressions should be 
different or the same.  
More striking perhaps is the variation between 
concentrated bias, bias toward concen-
tration and centrist slant as the translation 
for sesgo concentrador. Here, the third transla-
tion offers a clear interpretation of the source text au-
thor?s intent. The first two attempt to carry over the 
vagueness of the source expression assuming that the 
target text reader will be able to figure it out. But even 
here, the two translators appear to differ as to what the 
source language text author?s intent actually was, the 
former referring to bias of a certain degree of strength 
and the second to a bias  in a certain direction. Seem-
ingly, then, the annotation of each of these expressions 
should differ. 
Furthermore, each source language has different 
methods of encoding meaning linguistically. The resul-
tant differing types of translation mismatch with English 
should provide insight into the appropriate structure and 
content for an interlingual representation. 
The point is that a multilingual parallel data set of 
source language texts and English translations offers a 
unique perspective and unique problem for annotating 
texts for meaning. 
4 Interlingua 
Due to the complexity of an interlingual annotation as 
indicated by the differences described in the previous 
section, the representation has developed through three 
levels and incorporates knowledge from sources such as 
the Omega ontology and theta grids.  Since this is an 
evolving standard, the three levels will be presented in 
order as building on one another. Then the additional 
data components will be described.  
4.1 Three Levels of Representation 
We now describe three levels of representation, referred 
to as IL0, IL1 and IL2. The aim is to perform the annota-
tion process incrementally, with each level of represen-
tation incorporating additional semantic features and 
removing existing syntactic ones. IL2 is intended as the 
interlingua, that abstracts away from (most) syntactic 
idiosyncrasies of the source language. IL0 and IL1 are 
intermediate representations that are useful starting 
points for annotating at the next level. 
4.1.1 IL0 
IL0 is a deep syntactic dependency representation. It 
includes part-of-speech tags for words and a parse tree 
that makes explicit the syntactic predicate-argument 
structure of verbs. The parse tree is labeled with syntac-
tic categories such as Subject or Object , which refer to 
deep-syntactic grammatical function (normalized for 
voice alternations).  IL0 does not contain function words 
(determiners, auxiliaries, and the like): their contribu-
tion is represented as features.  Furthermore, semanti-
cally void punctuation has been removed.  While this 
representation is purely syntactic, many disambiguation 
decisions, relative clause and PP attachment for exam-
ple, have been made, and the presentation abstracts as 
much as possible from surface-syntactic phenomena.  
Thus, our IL0 is intermediate between the analytical and 
tectogrammatical levels of the Prague School (Haji? et 
al 2001). IL0 is constructed by hand-correcting the out-
put of a dependency parser (details in section 6) and is a 
useful starting point for semantic annotation at  IL1, 
since it allows annotators to see how textual units relate 
syntactically when making semantic judgments.  
4.1.2 IL1 
IL1 is an intermediate semantic representation. It asso-
ciates semantic concepts with lexical units like nouns, 
adjectives,  adverbs and verbs (details of the ontology in 
section 4.2). It also replaces the syntactic relations in 
IL0, like subject and object, with thematic roles, like 
agent, theme and goal (details in section 4.3). Thus, like 
PropBank (Kingsbury et al2002), IL1 neutralizes dif-
ferent alternations for argument realization.  However, 
IL1 is not an interlingua; it does not normalize over all 
linguistic realizations of the same semantics. In particu-
lar, it does not address how the meanings of individual 
lexical units combine to form the meaning of a phrase or 
clause. It also does not address idioms, metaphors and 
other non-literal uses of language.  Further, IL1 does not 
assign semantic features to prepositions; these continue 
to be encoded as syntactic heads of their phrases, al-
though these might have been annotated with thematic 
roles such as location or time. 
4.1.3 IL2 
IL2 is intended to be an interlingua, a representation of 
meaning that is reasonably independent of language. IL2 
is intended to capture similarities in meaning across 
languages and across different lexical/syntactic realiza-
tions within a language. For example, IL2 is expected to 
normalize over conversives (e.g. X bought a book from 
Y vs. Y sold a book to X)  (as does FrameNet (Baker et 
al 1998)) and non-literal language usage (e.g. X started 
its business vs. X opened its doors to customers).  The 
exact definition of IL2 will be the major research con-
tribution of this project. 
4.2 The Omega Ontology 
In progressing from IL0 to IL1, annotators have to se-
lect semantic terms (concepts) to represent the nouns, 
verbs, adjectives, and adverbs present in each sentence.  
These terms are represented in the 110,000-node ontol-
ogy Omega (Philpot et al, 2003), under construction at 
ISI.  Omega has been built semi-automatically from a 
variety of sources, including Princeton's WordNet (Fell-
baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-
renburg, 1995), ISI's Upper Model (Bateman et al, 
1989) and ISI's SENSUS (Knight and Luk, 1994).  After 
the uppermost region of Omega was created by hand, 
these various resources? contents were incorporated and, 
to some extent, reconciled.  After that, several million 
instances of people, locations, and other facts were 
added (Fleischman et al, 2003).  The ontology, which 
has been used in several projects in recent years (Hovy 
et al, 2001), can be browsed using the DINO browser at 
http://blombos.isi.edu:8000/dino; this browser forms a 
part of the annotation environment.  Omega remains 
under continued development and extension.  
4.3 The Theta Grids 
Each verb in Omega is assigned one or more theta grids 
specifying the arguments associated with a verb and 
their theta roles (or thematic role).  Theta roles are ab-
stractions of deep semantic relations that generalize 
over verb classes.  They are by far the most common 
approach in the field to represent predicate-argument 
structure.  However, there are numerous variations with 
little agreement even on terminology (Fillmore, 1968; 
Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-
Hovav, 1998). 
The theta grids used in our project were extracted 
from the Lexical Conceptual Structure Verb Database 
(LVD) (Dorr, 2001).  The WordNet senses assigned to 
each entry in the LVD were then used to link the theta 
grids to the verbs in the Omega ontology.  In addition to 
the theta roles, the theta grids specify the mapping be-
tween theta roles and their syntactic realization in argu-
ments, such as Subject, Object or Prepositional Phrase, 
and the Obligatory/Optional nature of the argument, 
thus facilitating IL1 annotation.  For example, one of the 
theta grids for the verb ?load? is listed in Table 1 (at the 
end of the paper). 
Although based on research in LCS-based MT 
(Dorr, 1993; Habash et al 2002), the set of theta roles 
used has been simplified for this project.  This list (see 
Table 2 at the end of the paper), was used in the Inter-
lingua Annotation Experiment 2002 (Habash and 
Dorr).1  
4.4 Incremental Annotation 
As described earlier, the development and annota-
tion of the interlingual notation is incremental in nature.  
This necessitates constraining the types and categories 
of attributes included in the annotation during the be-
ginning phases.  Other topics not addressed here, but 
considered for future work include time, aspect, loca-
tion, modality, type of reference, types of speech act, 
causality, etc.  
Thus, IL2 itself is not a final interlingual representa-
tion, but one step along the way. IL0 and IL1 are also 
intermediate representations, and as such are an occa-
sionally awkward mixture of syntactic and semantic 
information. The decisions as to what to annotate, what 
to normalize, what to represent as features at each level 
are semantically and syntactically principled, but also 
governed by expectations about reasonable annotator 
tasks. What is important is that at each stage of trans-
formation, no information is lost, and the original lan-
guage recoverable in principle from the representation. 
5 Annotation Tool 
We have assembled a suite of tools to be used in the 
annotation process.  Some of these tools are previously 
existing resources that were gathered for use in the pro-
ject, and others have been developed specifically with 
the annotation goals of this project in mind.  Since we 
are gathering our corpora from disparate sources, we 
need to standardize the text before presenting it to 
automated procedures.  For English, this involves sen-
tence boundary detection, but for other languages, it 
may involve segmentation, chunking of text, or other 
?text ecology? operations.  The text is then processed 
with a dependency parser, the output of which is viewed 
and corrected in TrED (Haji?, et al, 2001), a graphi-
cally-based tree editing program, written in Perl/Tk2.  
The revised deep dependency structure produced by this 
process is the IL0 representation for that sentence. 
In order to derive IL1 from the IL0 representation, 
annotators use Tiamat, a tool developed specifically for 
                                                           
1 Other contributors to this list are Dan Gildea and Karin 
Kipper Schuler. 
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tre
d/ 
this project.  This tool enables viewing of the IL0 tree 
with easy reference to all of the IL resources described 
in section 4 (the current IL representation, the ontology, 
and the theta grids).  This tool provides the ability to 
annotate text via simple point-and-click selections of 
words, concepts, and theta-roles.  The IL0 is displayed 
in the top left pane, ontological concepts and their asso-
ciated theta grids, if applicable, are located in the top 
right, and the sentence itself is located in the bottom 
right pane.  An annotator may select a lexical item (leaf 
node) to be annotated in the sentence view; this word is 
highlighted, and the relevant portion of the Omega on-
tology is displayed in the pane on the left.  In addition, 
if this word has dependents, they are automatically un-
derlined in red in the sentence view.  Annotators can 
view all information pertinent to the process of deciding 
on appropriate ontological concepts in this view.  Fol-
lowing the procedures described in section 6, selection 
of concepts, theta grids and roles appropriate to that 
lexical item can then be made in the appropriate panes. 
Evaluation of the annotators? output would be daunt-
ing based solely on a visual inspection of the annotated 
IL1 files.  Thus, a tool was also developed to compare 
the output and to generate the evaluation measures that 
are described in section 7.  The reports generated by the 
evaluation tool allow the researchers to look at both 
gross-level phenomena, such as inter-annotator agree-
ment, and at more detailed points of interest, such as 
lexical items on which agreement was particularly low, 
possibly indicating gaps or other inconsistencies in the 
ontology being used. 
6 Annotation Task 
To describe the annotation task, we first present the 
annotation process and tools used with it as well as the 
annotation manuals.  Finally, setup issues relating to 
negotiating multi-site annotations are discussed. 
6.1 Annotation process 
The annotation process was identical for each text. For 
the initial testing period, only English texts were anno-
tated, and the process described here is for English text. 
The process for non-English texts will be, mutatis mu-
tandis, the same. 
Each sentence of the text is parsed into a depend-
ency tree structure. For English texts, these trees were 
first provided by the Connexor parser at UMIACS 
(Tapanainen and Jarvinen, 1997), and then corrected by 
one of the team PIs. For the initial testing period, anno-
tators were not permitted to alter these structures. Al-
ready at this stage, some of the lexical items are 
replaced by features (e.g., tense), morphological forms 
are replaced by features on the citation form, and certain 
constructions are regularized (e.g., passive) and empty 
arguments inserted.  It is this dependency structure that 
is loaded into the annotation tool and which each anno-
tator then marks up. 
The annotator was instructed to annotate all nouns, 
verbs, adjectives, and adverbs. This involves annotating 
each word twice ? once with a concept from Wordnet 
SYNSET and once with a Mikrokosmos concept; these 
two units of information are merged, or at least inter-
twined in Omega. One of the goals and results of this 
annotation process will be a simultaneous coding of 
concepts in both ontologies, facilitating a closer union 
between them.  
In addition, users were instructed to provide a se-
mantic case role for each dependent of a verb. In many 
cases this was ?NONE? since adverbs and conjunctions 
were dependents of verbs in the dependency tree. LCS 
verbs were identified with Wordnet classes and the LCS 
case frames supplied where possible. The user, how-
ever, was often required to determine the set of roles or 
alter them to suit the text. In both cases, the revised or 
new set of case roles was noted and sent to a guru for 
evaluation and possible permanent inclusion. Thus the 
set of event concepts in the ontology supplied with roles 
will grow through the course of the project. 
6.2 The annotation manuals 
Markup instructions are contained in three manuals: a 
users guide for Tiamat (including procedural instruc-
tions), a definitional guide to semantic roles, and a 
manual for creating a dependency structure (IL0). To-
gether these manuals allow the annotator to (1) under-
stand the intention behind aspects of the dependency 
structure; (2) how to use Tiamat to mark up texts; and 
(3) how to determine appropriate semantic roles and 
ontological concepts. In choosing a set of appropriate 
ontological concepts, annotators were encouraged to 
look at the name of the concept and its definition, the 
name and definition of the parent node, example sen-
tences, lexical synonyms attached to the same node, and 
sub- and super-classes of the node. All these manuals 
are available on the IAMTC website3. 
6.3 The multi-site set up 
For the initial testing phase of the project, all annotators 
at all sites worked on the same texts. Two texts were 
provided by each site as were two translations of the 
same source language (non-English) text. To test for the 
effects of coding two texts that are semantically close, 
since they are both translations of the same source 
document, the order in which the texts were annotated 
differed from site to site, with half the sites marking one 
translation first, and the other half of the sites marking 
the second translation first. Another variant tested was 
                                                           
3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation
_manual.wiki?cmd=get&anchor=Annotation+Manual 
to interleave the two translations, so that two similar 
sentences were coded consecutively. 
During the later production phase, a more complex 
schedule will be followed, making sure that many texts 
are annotated by two annotators, often from different 
sites, and that regularly all annotators will mark the 
same text. This will help ensure continued inter-coder 
reliability. 
In the period leading up to the initial test phase, 
weekly conversations were held at each site by the an-
notators, going over the texts coded. This was followed 
by a weekly conference call among all the annotators. 
During the test phase, no discussion was permitted. 
One of the issues that arose in discussion was how 
certain constructions should be displayed and whether 
each word should have a separate node or whether cer-
tain words should be combined into a single node. In 
view of the fact that the goal was not to tag individual 
words, but entities and relations, in many cases words 
were combined into single nodes to facilitate this proc-
ess. For instance, verb-particle constructions were com-
bined into a single node. In a sentence like ?He threw it 
up?, ?throw? and ?up? were combined into a single 
node ?throw up? since one action is described by the 
combined words. Similarly, proper nouns, compound 
nouns and copular constructions required specialized 
handling.    In addition, issues arose about whether an-
notators should change dependency trees; and in in-
structing the annotators on how best to determine an 
appropriate ontology node.    
7 Evaluation 
The evaluation criteria and metrics continue to evolve 
and are in the early stages of formation and implementa-
tion.  Several possible courses for evaluating the annota-
tions and resulting structures exist.  In the first of these, 
the annotations are measured according to inter-
annotator agreement.  For this purpose, data is collected 
reflecting the annotations selected, the Omega nodes 
selected and the theta roles assigned.  Then, inter-coder 
agreement is measured by a straightforward match, with 
agreement calculated by a Kappa measure (Carletta, 
1993) and a Wood standard similarity (Habash and 
Dorr, 2002).  This is done for three agreement points:  
annotations, Omega selection and theta role selection.  
At this time, the Kappa statistic?s expected agreement is 
defined as 1/(N+1) where N is the number of choices at 
a given data point.  In the case of Omega nodes, this 
means the number of matched Omega nodes (by string 
match) plus one for the possibility of the annotator trav-
ersing up or down the hierarchy. Multiple measures are 
used because it is important to have a mechanism for 
evaluating inter-coder consistency in the use of the IL 
representation language which does not depend on the 
assumption that there is a single correct annotation of a 
given text.  The tools for evaluation have been modified 
from pervious use (Habash and Dorr, 2002). 
Second, the accuracy of the annotation is measured.  
Here accuracy is defined as correspondence to a prede-
fined baseline.  In the initial development phase, all 
sites annotated the same texts and many of the varia-
tions were discussed at that time, permitting the devel-
opment of a baseline annotation.  While not a useful 
long-term strategy, this produced a consensus baseline 
for the purpose of measuring the annotators? task and 
the solidity of the annotation standard.  
The final measurement technique derives from the 
ultimate goal of using the IL representation for MT, 
therefore, we are measuring the ability to generate accu-
rate surface texts from the IL representation as anno-
tated.  At this stage, we are using an available generator, 
Halogen (Knight and Langkilde, 2000).  A tool to con-
vert the representation to meet Halogen requirements is 
being built.  Following the conversion, surface forms 
will be generated and then compared with the originals 
through a variety of standard MT metrics (ISLE, 2003).   
8 Accomplishments and Issues 
In a short amount of time, we have identified languages 
and collected corpora with translations.  We have se-
lected representation elements, from parser outputs to 
ontologies, and have developed an understanding of 
how their component elements fit together.  A core 
markup vocabulary (e.g., entity-types, event-types and 
participant relations) was selected.  An initial version of 
the annotator?s toolkit (Tiamat) has been developed and 
has gone through alpha testing.  The multi-layered ap-
proach to annotation  decided upon reduces the burden 
on the annotators for any given text as annotations build 
upon one another.  In addition to developing individual 
tools, an infrastructure exists for carrying out a multi-
site annotation project.   
In the coming months we will be fleshing out the 
current procedures for evaluating the accuracy of an 
annotation and measuring inter-coder consistency.  
From this, a multi-site evaluation will be produced and  
results reported.  Regression testing, from the interme-
diate stages and representations will be able to be car-
ried out.  Finally, a growing corpus of annotated texts 
will become available.   
In addition to the issues discussed throughout the 
paper, a few others have not yet been identified.  From a 
content standpoint, looking at IL systems for time and 
location should utilize work in personal name, temporal 
and spatial annotation (e.g., Ferro et al, 2001).  Also, an 
ideal IL representation would also account for causality, 
co-reference, aspectual content, modality, speech acts, 
etc.  At the same time, while incorporating these items, 
vagueness and redundancy must be eliminated from the 
annotation language.  Many inter-event relations would 
need to be captured such as entity reference, time refer-
ence, place reference, causal relationships, associative 
relationships, etc.  Finally, to incorporate these, cross-
sentence phenomena remain a challenge.     
From an MT perspective, issues include evaluating 
the consistency in the use of an annotation language 
given that any source text can result in multiple, differ-
ent, legitimate translations (see Farwell and Helmreich, 
2003) for discussion of evaluation in this light.  Along 
these lines, there is the problem of annotating texts for 
translation without including in the annotations infer-
ences from the source text.   
9 Conclusions  
This is a radically different annotation project from 
those that have focused on morphology, syntax or even 
certain types of semantic content (e.g., for word sense 
disambiguation competitions). It is most similar to 
PropBank (Kingsbury et al2002) and FrameNet (Baker 
et al1998).  However, it is novel in its emphasis on:  (1) 
a more abstract level of mark-up (interpretation); (2) the 
assignment of a well-defined meaning representation to 
concrete texts; and (3) issues of a community-wide con-
sistent and accurate annotation of meaning. 
By providing an essential, and heretofore non-
existent, data set for training and evaluating natural lan-
guage processing systems, the resultant annotated multi-
lingual corpus of translations is expected to lead to 
significant research and development opportunities for 
Machine Translation and a host of other Natural Lan-
guage Processing technologies including Question-
Answering and Information Extraction.  
References 
Baker, C., J. Fillmore and J B. Lowe, 1998.  The Berke-
ley FrameNet Project.  Proceedings of ACL. 
Bateman, J.A., R. Kasper, J. Moore, and R. Whitney. 
1989. A General Organization of Knowledge for 
Natural Language Processing: The Penman Upper 
Model. Unpublished research report, USC / Informa-
tion Sciences Institute, Marina del Rey, CA.  
Carletta, J. C. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2), 249-254 
Conceptual Structures and Documentation, UMCP. 
http://www.umiacs.umd.edu/~bonnie/LCS_Database
_Documentation.html  
Dorr, B. J. 2001.  LCS Verb Database, Online Software 
Database of Lexical  
Dorr, B. J., 1993. Machine Translation: A View from the 
Lexicon, MIT Press, Cambridge, MA. 
Farwell, D., and S. Helmreich.  2003.  Pragmatics-based 
Translation and MT Evaluation.  In Proceedings of 
Towards Systematizing MT Evaluation.  MT-Summit 
Workshop, New Orleans, LA. 
Fellbaum, C. (ed.). 1998. WordNet: An On-line Lexical 
Database and Some of its Applications. MIT Press, 
Cambridge, MA. 
Ferro, L., I. Mani, B. Sundheim and G. Wilson.  2001. 
TIDES Temporal Annotation Guidelines. Version 
1.0.2 MITRE Technical Report, MTR 01W0000041 
Fillmore, C..  1968. The Case for Case. In E. Bach and 
R. Harms, editors, Universals in Linguistic Theory, 
pages 1--88. Holt, Rinehart, and Winston.  
Fleischman, M., A. Echihabi, and E.H. Hovy. 2003. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked.  Pro-
ceedings of the ACL Conference. Sapporo, Japan. 
Habash, N. and B. Dorr. 2002. Interlingua Annotation 
Experiment Results. AMTA-2002 Interlingua Reli-
ability Workshop. Tiburon, California, USA. 
Habash, N., B. J. Dorr, and D. Traum, 2002. "Efficient 
Language Independent Generation from Lexical 
Conceptual Structures," Machine Translation, 17:4. 
Haji?, J.; B. Vidov?-Hladk?; P. Pajas.  2001: The Pra-
gue Dependency Treebank: Annotation Structure and 
Support. In Proceeding of the IRCS Workshop on 
Linguistic Databases, pp. . University of Pennsyl-
vania, Philadelphia, USA, pp. 105-114. 
Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans, 
W. Bourne, and D. Saroz.  2001. Data Acquisition 
and Integration in the DGRC's Energy Data Collec-
tion Project, in Proceedings of the NSF's dg.o 2001. 
Los Angeles, CA. 
ISLE 2003.  Framework for Evaluation of Machine 
Translation in ISLE.  
http://www.issco.unige.ch/projects/isle/femti/ 
Jackendoff, R. 1972. Grammatical Relations and Func-
tional Structure. Semantic Interpretation in Genera-
tive Grammar. The MIT Press, Cambridge, MA. 
Kingsbury, P and M Palmer and M Marcus , 2002.  
Adding Semantic Annotation to the Penn TreeBank. 
Proceedings of the Human Language Technology 
Conference (HLT 2002).  
Knight, K., and I. Langkilde. 2000.  Preserving Ambi-
guities in Generation via Automata Intersection. 
American Association for Artificial Intelligence con-
ference (AAAI). 
Knight, K, and S. K. Luk.  1994. Building a Large-Scale 
Knowledge Base for Machine Translation.  Proceed-
ings of AAAI. Seattle, WA. 
Levin, B. and M. Rappaport-Hovav. 1998. From Lexical 
Semantics to Argument Realization. Borer, H. (ed.) 
Handbook of Morphosyntax and Argument Structure. 
Dordrecht: Kluwer Academic Publishers. 
Mahesh, K., and Nirenberg, S.  1995. A Situated Ontol-
ogy for Practical NLP, in Proceedings on the Work-
shop on Basic Ontological Issues in Knowledge 
Sharing at IJCAI-95. Montreal, Canada. 
Philpot, A., M. Fleischman, E.H. Hovy. 2003. Semi-
Automatic Construction of a General Purpose Ontol-
ogy.  Proceedings of the International Lisp Confer-
ence.  New York, NY. Invited. 
Stowell, T. 1981. Origins of Phrase Structure. PhD the-
sis, MIT, Cambridge, MA.  
Tapanainen, P. and T Jarvinen.  1997.  A non-projective 
dependency parser.  In the 5th Conference on Applied 
Natural Language Processing / Association for Com-
putational Linguistics, Washington, DC. 
White, J., and T. O?Connell.  1994.  The ARPA MT 
evaluation methodologies: evolution, lessons, and fu-
ture approaches.  Proceedings of the 1994 Confer-
ence, Association for Machine Translation in the 
Americas 
Walker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, and 
G. Doddington 2003.  Multiple-Translation Arabic 
Corpus, Part 1. Linguistic Data Consortium (LDC) 
catalog num. LDC2003T18 & ISBN 1-58563-276-7. 
 
 
Role Description Grid Syntax Type 
Agent The entity that does the action Agent:  load 
Theme  with possessed 
SUBJ OBLIGATORY 
Theme The entity that is worked on Agent:  load 
Theme with possessed 
OBJ OBLIGATORY 
Possessed The entity controlled or owned Agent:  load 
Theme  with possessed 
PP OPTIONAL 
Table 1 :  A theta grid for the verb "load" 
 
 
Role and Definition Examples 
Agent:  Agents have the features of volition, sentience, causation and 
independent exist 
? Henry pushed/broke the vase. 
Instrument: An instrument should have causation but no volition. Its 
sentience and existence are not relevant. 
? The Hammer broke the vase. 
? She hit him with a baseball bat 
Experiencer: An experiencer has no causation but is sentient and 
exists independently. Typically an experiencer is the subject of verbs 
like feel, hear, see, sense, smell, notice, detect, etc. 
? John heard the vase shatter.   
? John shivered. 
Theme: The theme is typically causally affected or experiences a 
movement and/or change in state. The theme can appear as the infor-
mation in verbs like acquire, learn, memorize, read, study, etc. It can 
also be a thing, event or state (clausal complement). 
? John went to school.  
? John broke the vase.   
? John memorized his lines.  
? She buttered the bread with marga-
rine.   
Perceived: Refers to a perceived entity that isn't required by the verb 
but further characterizes the situation. The perceived is neither caus-
ally affected nor causative. It doesn't experience a movement or 
change in state. Its volition and sentience are irrelevant. Its existence 
is independent of an experiencer. 
? He saw the play.   
? He looked into the room.  
? The cat's fur feels good to John.   
? She imagined the movie to be loud.    
Predicate: Indicates new modifying information about other thematic 
roles. 
? We considered him a fool.   
? She acted happy.   
Source: Indicates where/when the theme started in its motion, or 
what its original state was, or where its original (possibly abstract) 
location/time was. 
? John left the house. 
Goal: Indicates where the theme ends up in its motion, or what its 
final state is, or where/when its final (possibly abstract) location/time 
is. It also can indicate the thing/event resulting from the verb's occur-
rence (the result). 
? John ran home.   
? John ran to the store.  
? John gave a book to Mary.   
? John gave Mary a book. 
Location: Indicates static locations---as opposed to a source or goal, 
i.e., the (possibly abstract) location of the theme or event. 
? He lived in France.   
? The water fills the box.   
? This cabin sleeps five people 
Time Indicates time. ? John sleeps for five hours.   
? Mary ate during the meeting. 
Beneficiary: Indicates the thing that receives the benefit/result of the 
event/state. 
? John baked the cake for Mary.   
? John baked Mary a cake.  
? An accident happened to him.   
Purpose: Indicates the purpose/reason behind an event/state ? He studied for the exam.  
? He searched for rabbits.  
Possessed: Indicates the possessed entity in verbs such as own, have, 
possess, fit, buy, and carry. 
? John has five bucks.  
? He loaded the cart with hay.   
? He bought it for five dollars 
Proposition: Indicates the secondary event/state ? He wanted to study for the exam. 
Modifier: Indicates a property of a thing such as color, taste, size, 
etc. 
? The red book sitting on the table is 
old.  
Null Indicates no thematic contribution. Typical examples are imper-
sonal it and there. 
? It was raining all morning in Miami. 
 
TABLE 2:  List of Theta Roles 
Keyword Translation Accuracy and Cross-Lingual Question  
Answering in Chinese and Japanese 
 
Teruko Mitamura 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
teruko@cs.cmu.edu 
 Mengqiu Wang 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
mengqiu@cs.cmu.edu
Hideki Shima 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
hideki@cs.cmu.edu
Frank Lin 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
frank+@cs.cmu.edu
 
 
  
 
Abstract 
In this paper, we describe the extension 
of an existing monolingual QA system 
for English-to-Chinese and English-to-
Japanese cross-lingual question answer-
ing (CLQA). We also attempt to charac-
terize the influence of translation on 
CLQA performance through experimen-
tal evaluation and analysis. The paper 
also describes some language-specific is-
sues for keyword translation in CLQA. 
1 Introduction 
The JAVELIN system is a modular, extensible 
architecture for building question-answering 
(QA) systems (Nyberg, et al, 2005).  Since the 
JAVELIN architecture is language-independent, 
we extended the original English version of 
JAVELIN for cross-language question answering 
(CLQA) in Chinese and Japanese.  The same 
overall architecture was used for both systems, 
allowing us to compare the performance of the 
two systems. In this paper, we describe how we 
extended the monolingual system for CLQA (see 
Section 3).  Keyword translation is a crucial ele-
ment of the system; we describe our translation 
module in Section 3.2.  In Section 4, we evaluate 
the end-to-end CLQA systems using three differ-
ent translation methods.  Language-specific 
translation issues are discussed in Section 5. 
2 Javelin Architecture 
The JAVELIN system is composed of four main 
modules: the Question Analyzer (QA), Retrieval 
Strategist (RS), Information eXtractor (IX) and 
Answer Generator (AG). Inputs to the system are 
processed by these modules in the order listed 
above. The QA module is responsible for parsing 
the input question, assigning the appropriate an-
swer type to the question, and producing a set of 
keywords. The RS module is responsible for 
finding documents containing answers to the 
question, using keywords produced by the QA 
module. The IX module finds and extracts an-
swers from the documents based on the answer 
type, and then produces a ranked list of answer 
candidates. The AG module normalizes and clus-
ters the answer candidates to rerank and generate 
a final ranked list. The overall monolingual ar-
chitecture is shown in Figure 1. 
3 Extension for Cross-Lingual QA 
Because of JAVELIN?s modular design, signifi-
cant changes to the monolingual architecture 
were not required. We customized the system in 
order to handle Unicode characters and ?plug in? 
cross-lingual components and resources. 
For the Question Analyzer, we created the 
Keyword Translator, a sub-module for translat-
ing keywords. The Retrieval Strategist was 
adapted to search in multilingual corpora. The 
Information Extractors use language-independent 
extraction algorithms. The Answer Generator 
uses language-specific sub-modules for normali-
zation, and a language-independent algorithm for 
answer ranking. The overall cross-lingual archi-
tecture is shown in Figure 2.  The rest of this sec-
tion explains the details of each module. 
3.1 Question Analyzer 
The Question Analyzer (QA) is responsible for 
extracting information from the input question in 
order to formulate a representation of the  
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
31
 Figure1: Javelin Monolingual Architecture Figure2: Javelin Architecture with Cross-Lingual 
Extension 
 
information required to answer the question.  
Input questions are processed using the RASP 
parser (Korhonen and Briscoe, 2004), and the 
module output contains three main components: 
a) selected keywords; b) the answer type (e.g. 
numeric-expression, person-name, location); and 
c) the answer subtype (e.g. author, river, city).  
The selected keywords are words or phrases 
which are expected to appear in documents with 
correct answers. In order to reduce noise in the 
document retrieval phase, we use stop-word lists 
to eliminate high-frequency terms; for example, 
the term ?old? is not included as a keyword for 
?how-old? questions. 
We extended the QA module with a keyword 
translation sub-module, so that translated key-
words can be used to retrieve documents from 
multilingual corpora. This straightforward ap-
proach has been used by many other CLQA sys-
tems. An alternative approach is to first translate 
the whole question sentence from English to the 
target language, and then analyze the translated 
question. Our reasons for favoring keyword 
translation are two-fold. First, to translate the 
question to the target language and analyze it, we 
would have to replace the English NLP compo-
nents in the Question Analyzer with their coun-
terparts for the target language. In contrast, key-
word translation decouples the question analysis 
from the translation, and requires no language 
specific resources during question analysis. The 
second reason is that machine translation is not 
perfect, and therefore the resulting translation(s) 
for the question may be incomplete or ungram-
matical, thus adding to the complexity of the 
analysis task. One could argue that when trans-
lating the full sentence instead of just the key-
words, we can better utilize state-of-art machine 
translation techniques because more context in-
formation is available. But for our application, an 
accurate translation of functional words (such as 
prepositions or conjunctions) is less important. 
We focus more on words that carry more content 
information, such as verbs and nouns. We will 
present more detail on the use of contextual in-
formation for disambiguation in the next section. 
In some recent work (Kwok, 2005, Mori and 
Kawagishi, 2005), researchers have combined 
these two approaches, but to date no studies have 
compared their effectiveness. 
3.2 Translation Module 
The Translation Module (TM) is used by the QA 
module to translate keywords into the language 
of the target corpus. Instead of combining multi-
ple translation candidates with a disjunctive 
query operator (Isozaki et al, 2005), the TM se-
lects the best combination of translated keywords 
from several sources: Machine Readable Dic-
tionaries (MRDs), Machine Translation systems 
(MTs) and Web-mining-Based Keyword Trans-
lators (WBMTs) (Nagata et al, 2001, Li et al, 
2003). For translation from English to Japanese, 
we used two MRDs, eight MTs and one WBMT. 
If none of them return a translation, the word is 
transliterated into kana for Japanese (for details 
on transliteration, see Section 5.2). For transla-
tion from English to Chinese, we used one MRD, 
three MTs and one WBMT. After gathering all 
possible translations for every keyword, the TM 
uses a noisy channel model to select the best 
combination of translated keywords. The TM 
estimates model statistics using the World Wide 
Web. Details of the translation selection method 
are described in the rest of this subsection. 
 
The Noisy Channel Model: In the noisy channel 
model, an undistorted signal passes through a 
noisy channel and becomes distorted.  Given the 
distorted signal, we are to find the original, un-
distorted signal. IBM applied the noisy channel 
model idea to translation of sentences from 
aligned parallel corpora, where the source lan-
guage sentence is the distorted signal, and the 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
32
target language sentence is the original signal 
(Brown et al, 1990). We adopt this model for 
disambiguating keyword translation, with the 
source language keyword terms as the distorted 
signal and the target language terms as the origi-
nal signal.  The TM's job is to find the target lan-
guage terms given the source language terms, by 
finding the probability of the target language 
terms given the source language terms  P(T|S).  
 
Using Bayes' Rule, we can break the equation 
down to several components: 
 
)(
)|()(
)|(
SP
TSPTP
STP
?=  
Because we are comparing probabilities of dif-
ferent translations of the same source keyword 
terms, we can simplify the problem to be: 
 
)|()()|( TSPTPSTP ?=  
 
We can now reduce the equation to two compo-
nents. P(T) is the language model and P(S|T) is 
the translation model. If we assume independ-
ence among the translations of individual terms, 
we can represent the translation probability of a 
keyword by the product of the probabilities of 
the individual term translations: 
 
?=
i
ii tsPTSP )|()|(  
 
Estimating Probabilities using the World 
Wide Web: For estimating the probabilities of 
the translation model and the language model, 
we chose to gather statistics from the World 
Wide Web. There are three advantages in utiliz-
ing the web for gathering translation statistics: 1) 
it contains documents written in many different 
languages, 2) it has high coverage of virtually all 
types of words and phrases, and 3) it is con-
stantly updated. However, we also note that the 
web contains a lot of noisy data, and building up 
web statistics is time-consuming unless one has 
direct access to a web search index. 
 
Estimating Translation Model Probabilities: 
We make an assumption that terms that are trans-
lations of each other co-occur more often in 
mixed-language web pages than terms that are 
not translations of each other. This assumption is 
analogous to Turney?s work on the co-
occurrence of synonyms (Turney, 2001). We 
then define the translation probability of each 
keyword translation as: 
?=
j
jii
jii
jii tsco
tsco
tsP
)),(log(
)),(log(
)|(
,
,
,
 
Where si is the i-th term in the source language 
and ti,j is the j-th translation candidate for si. Let 
hits be a number of web pages retrieved from a 
certain search engine. co(si, t i,j) is the hits given 
a query si and ti,j., where log is applied to adjust 
the count so that translation probabilities can still 
be comparable at higher counts. 
 
Estimating Language Model Probabilities: In 
estimating the language model, we simply obtain 
hits given a conjunction of all the candidate 
terms in the target language, and divide that 
count by the sum of the occurrences of the indi-
vidual terms: 
 
?=
i
i
n
to
tttco
TP
)(
),...,(
)( 21  
The final score of a translation candidate for a 
query is the product of the translation model 
score P(S|T) and the language model score P(T). 
 
Smoothing and Pruning: As with most statisti-
cal calculations in language technologies, there is 
a data sparseness problem when calculating the 
language model score. Also, because statistics 
are gathered real-time by accessing a remote 
search engine via internet, it can take a long time 
to process a single query when there is a large 
number of translation candidates. We describe 
methods for smoothing the language model and 
pruning the set of translation candidates below. 
The data sparseness problem occurs when 
there are many terms in the query, and the terms 
are relatively rare keywords. When calculating 
the language model score, it is possible that none 
of the translation candidates appear on any web 
page. To address this issue, we propose a "mov-
ing-window smoothing" algorithm: 
 
? When the target keyword co-occurrence 
count with n keywords is below a set 
threshold for all of the translation candi-
dates, we use a moving window of size 
n-1 that "moves" through the keywords 
in sequence, splitting the set of keywords 
into two sets, each with n-1 keywords. 
 
? If the co-occurrence count of all of these 
sets of keywords is above the threshold, 
return the product of the language model 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
33
score of these two sets as the language 
model score. 
 
? If not, decrease the window and repeat 
until either all of the split sets are above 
the threshold or n = 1. 
 
The moving window smoothing technique 
gradually relaxes the search constraint without 
losing the "connectivity" of keywords (there is 
always overlap in the split parts) before finally 
backing off to just the individual keywords. 
However, there are two issues worth noting with 
this approach: 
 
1. "Moving-window smoothing" assumes 
that keywords that are next to each other 
are also more semantically related, 
which may not always be the case. 
 
2. "Moving-window smoothing" tends to 
give the keywords near the middle of the 
question more weight, which may not be 
desirable. 
 
A better smoothing technique may be used 
with trying all possible "splits" at each stage, but 
this would greatly increase the time cost. There-
fore, we chose the moving-window smoothing as 
a trade-off between a more robust smoothing 
technique that tries all possible split combina-
tions and no smoothing at all. 
The set of possible translation candidates is 
produced by creating all possible combinations 
of the translations of individual keywords. For a 
question with n keywords and an average of m 
possible translations per keyword, the number of 
possible combinations is mn. This quickly be-
comes intractable as we have to access a search 
engine at least mn times just for the language 
model score. Therefore, pruning is needed to cut 
down the number of translation candidates. We 
prune possible translation candidates twice dur-
ing each run, using early and late pruning: 
 
1. Early Pruning: We prune possible trans-
lations of the individual keywords before 
combining them to make all possible 
translations of a query. We use a very 
simple pruning heuristic based on target 
word frequency using a word frequency 
list. Very rare translations produced by a 
resource are not considered. 
 
2. Late Pruning: We prune possible transla-
tion candidates of the entire set of key-
words after calculating translation prob-
abilities. Since the calculation of the 
translation probabilities requires little 
access to the web, we can calculate only 
the language model score for the top N 
candidates with the highest translation 
score and prune the rest. 
 
An Example of English to Chinese Keyword 
Translation Selection: Suppose we translate the 
following question from English to Chinese.    
 
"What if Bush leaves Iraq?" 
 
Three keywords are extracted: ?Bush?, 
?leaves?, and ?Iraq.? Using two MT systems and 
an MRD, we obtain the following translations: 
 
 i=1 i=2 i=3 
Source Bush leaves Iraq 
Target j=1 ?? ?? ??? 
Target j=2 ?? ??  
Table 1. E-C Keyword Translation 
 
"Bush" and "leaves" both have two transla-
tions because they are ambiguous keywords, 
while "Iraq" is unambiguous. Translation (1,1) 
means bush as in a shrub, and translation (1,2) 
refers to the person named Bush. Translation 
(2,1) is the verb "to go away", and translation 
(2,2) is the noun for leaf. Note that we would like 
translation (1,2) and translation (2,1) because 
they match the sense of the word intended by the 
user. Now we can create all possible combina-
tions of the keywords in the target language: 
 
"?? ?? ???" 
"?? ?? ???" 
"?? ?? ???" 
"?? ?? ???" 
 
Query "Bush" 
"??" 
"Bush" 
"??" 
"leaves" 
"??" 
"leaves" 
"??" 
"Iraq" 
"???" 
hits 3790 41100 5780 7240 24500 
Table 2. Translation Pair Page Counts 
 
 
Candidate Translation Score 
"?? ?? ???" 0.215615 
"?? ?? ???" 0.221219 
"?? ?? ???" 0.277970 
"?? ?? ???" 0.285195 
Table 3. Translation Scores 
 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
34
By calculating hits, we obtain the statistics and 
the translation scores shown in Table 2 and 3. 
Now we can proceed to use the search engine to 
obtain language model statistics, which we use to 
obtain the language model. Then, together with 
the translation model score, we calculate the 
overall score1. 
 
Query ?? ?? ?? ?? ??? 
hits 428K 459K 1490K 1100K 9590K 
Table 4. Individual Term Page Counts 
 
Query hits 
"?? ?? ???" 1200 
"?? ?? ???" 455 
"?? ?? ???" 17300 
"?? ?? ???" 2410 
Table 5. Target Language Query Page Counts 
 
Cand Translation Language Overall 
?? 
?? 
??? 
2.1562E-1 1.0428E-4 2.2483E-5 
?? 
?? 
??? 
2.2122E-1 4.0925E-5 9.0533E-6 
?? 
?? 
??? 
2.7797E-1 1.4993E-3 4.1675E-4 
?? 
?? 
??? 
2.8520E-1 2.1616E-4 6.1649E-5 
 
Table 6. Translation Score, Language Model 
Score, and Overall Score 
 
As shown in Table 6, we select the most prob-
able combination of translated keywords with the 
highest overall score (the third candidate), which 
is the correct translation of the English keywords. 
3.3 Retrieval Strategies 
The Retrieval Strategist (RS) module retrieves 
documents from a corpus in response to a query.  
For document retrieval, the RS uses the Lemur 
3.0 toolkit (Ogilvie and Callan, 2001). Lemur 
supports structured queries using operators such 
as Boolean AND, Synonym, Ordered/Un-
Ordered Window and NOT. An example of a 
structured query is shown below: 
 
                                                 
1  For simplicity, we don?t apply smoothing 
and pruning. 
 
#BAND( #OD4(???? ??) 
?? 
#SYN(*organization *person) ) 
 
In formulating a structured query, the RS uses an 
incremental relaxation technique, starting from 
an initial query that is highly constrained; the 
algorithm searches for all the keywords and data 
types in close proximity to each other. The prior-
ity is based on a function of the likely answer 
type, keyword type (word, proper name, or 
phrase) and the inverse document frequency of 
each keyword. The query is gradually relaxed 
until the desired number of relevant documents is 
retrieved.  
3.4 Information Extraction 
In the JAVELIN system, the Information Ex-
tractor (IX) is not a single module that uses one 
extraction algorithm; rather, it is an abstract in-
terface which allows different information ex-
tractor implementations to be plugged into 
JAVELIN. These different extractors can be used 
to produce different results for comparison, or 
the results of running them all in parallel can be 
merged. Here we will describe just one of the 
extractors, the one which is currently the best 
algorithm in our CLQA experiment: the Light IX. 
The Light IX module uses simple, distance-
based algorithms to find a named entity that 
matches the expected answer type and is ?clos-
est? to all the keywords according to some dis-
tance measure.  The algorithm considers as an-
swer candidates only those terms that are tagged 
as named entities which match the desired an-
swer type.  The score for an answer candidate a 
is calculated as follows: 
 
)()()( aDistScoreaOccScoreaScore ?+?= ??   
 
where ? + ? = 1, OccScore is the occurrence 
score and DistScore is the distance score. Both 
OccScore and DistScore return a number be-
tween zero and one, and likewise Score returns a 
number between zero and one. Usually, ? is 
much smaller than ?. The occurrence score for-
mula is: 
n
kExist
aOccScore
n
i i? == 1 )()(  
where a is the answer candidate and ki is the i-th 
keyword, and n is the number of keywords. Exist  
returns 1 if the i-th keyword exists in the docu-
ment, and 0 otherwise. The distance score for 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
35
each answer candidate is calculated according to 
the following formula: 
n
kaDist
aDistScore
n
i
i
? == 1 ),(
1
)(  
This formula produces a score between zero 
and one. If the i-th keyword does not exist in a 
document, the equation inside the summation 
will return zero. If the i-th keyword appears more 
than once in the document, the one closest to the 
answer candidate is considered. An additional 
restriction is that the answer candidate cannot be 
one of the keywords. The Dist function is the 
distance measure, which has two definitions: 
 
1. ),(),( batTokensAparbaDist =  
2. )),(log(),( batTokensAparbaDist =   
 
The first definition simply counts the number 
of tokens between two terms.  The second defini-
tion is a logarithmic measure. The function re-
turns the number of tokens from a to b; if a and b 
are adjacent, the count is 1; if a and b are sepa-
rated by one token, the count is 2, and so on. A 
token can either be a character or a word; for the 
E-C, we used character-based tokenization, 
whereas for the E-J, we use word-based tokeni-
zation. By heuristics obtained from training re-
sults, we used the linear Dist measure for E-C 
and logarithmic Dist measure for E-J in the 
evaluation. 
This algorithm is a simple statistical approach 
which requires no language-specific external 
tools beyond word segmentation and a named-
entity tagger. It is not as sophisticated as other 
approaches which perform deep linguistic analy-
sis, but one advantage is faster adaptation to mul-
tiple languages. In our experiments, this simple 
algorithm performs at the same level as a FST-
based approach (Nyberg, et al 2005). 
3.5 Answer Generator 
The task of the Answer Generator (AG) module 
is to produce a ranked list of answer candidates 
from the IX output. The AG is designed to nor-
malize answer candidates by resolving represen-
tational differences (e.g. in how numbers, dates, 
etc. are expressed in text). This canonicalization 
makes it possible to combine answer candidates 
that differ only in surface form. 
Even though the AG module plays an impor-
tant role in JAVELIN, we did not use its full po-
tential in our E-C and E-J systems, since we 
lacked some language-specific resources re-
quired for multilingual answer merging. 
4 Evaluation and Effect of Translation 
Accuracy 
To evaluate the effect of translation accuracy on 
the overall performance of the CLQA system, we 
conducted several experiments using different 
translation methods. Three different runs were 
carried out for both the E-C and E-J systems, 
using the same 200-question test set and the 
document corpora provided by the NTCIR 
CLQA task. The first run was a fully automatic 
run using the original translation module in the 
CLQA system; the result is exactly same as the 
one we submitted to NTCIR5 CLQA. For the 
second run, we manually translated the keywords 
that were selected by the Question Analyzer 
module. This translation was done by looking at 
only the selected keywords, but not the original 
question. For both E-C and E-J tasks, the NTCIR 
organizers provided the translations for the Eng-
lish questions, which we assume are the gold-
standard translations. Taking advantage of this 
resource, in the third run we simply looked up 
the corresponding term for each English keyword 
from the gold-standard translation of the ques-
tion. The results for these runs are shown in Ta-
ble 7 and 8 below. 
 
  
Translation 
Accuracy 
Top1 
  
Top1+U 
  
Run 1 69.3% 15 (7.5%) 23 (11.5%) 
Run 2 85.5% 16 (8.0%) 31 (15.5%) 
Run 3 100% 18 (9.0%) 38 (19.0%) 
Table 7.  Effect of Translation (E-C) 
 
 
  
Translation 
Accuracy 
Top1 
  
Top1+U 
  
Run 1 54.2% 20 (10.0%) 25 (12.5%) 
Run 2 81.2% 19 (9.5%) 30 (15.0%) 
Run 3 100% 18 (9.0%) 31 (15.5%) 
Table 8.  Effect of Translation (E-J) 
 
We found that in the NTCIR task, the sup-
ported/correct document set was not complete. 
Some answers judged as unsupported were in-
deed well supported, but the supporting docu-
ment did not appear in NTCIR's correct docu-
ment set. Therefore, we think the Top1+U col-
umn is more informative for this evaluation. 
From Table 7 and 8, it is obvious that the overall 
performance increases as translation accuracy 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
36
increases. From Run1 to Run2, we eliminated all 
the overt translation errors produced by the sys-
tem, and also corrected word-sense errors. Then 
from Run2 to Run3, we made different lexical 
choices among the seemingly all correct transla-
tions of a word. This type of inappropriateness 
cannot be classified as an error, but it makes a 
difference in QA systems, especially at the docu-
ment retrieval stage. For example, the phrase 
"Kyoto Protocol" can have two valid transla-
tions: ???? or ?????. Both translations 
would be understandable to a human, but the sec-
ond translation will appear much more frequently 
than the first one in the document set. This type 
of lexical choice is hard to make, because we 
would need either subtle domain-specific knowl-
edge, or knowledge about the target corpus; nei-
ther is easily obtainable.  
    Comparing Run 1 and 3 in Table 8, we see 
that improving keyword translation had less 
overall impact on the E-J system. Information 
extraction (including named entity identification) 
did not perform as well in E-J.  We also com-
pared the translation effect on cross-lingual 
document retrieval (Figure 3).  As we can see, 
Run 3 retrieved supporting documents more fre-
quently in rank 1 than in Run 1 or 2. From these 
preliminary investigations, it would seem that 
information extraction and/or answer generation 
must be improved for English-Japanese CLQA. 
 
Figure3: Comparison of three runs: Cross-lingual 
document retrieval performance in E-J 
5 Translation Issues 
In this section, we discuss language specific key-
word translation issues for Chinese and Japanese 
CLQA. 
5.1 Chinese 
One prominent problem in Chinese keyword 
translation is word sense disambiguation. In 
question answering systems, the translation re-
sults are used directly in information retrieval, 
which exhibits a high dependency on the lexical 
form of a word but not so much on the meaning. 
In other words, having a different lexical form 
from the corresponding term in corpora is the 
same as having a wrong translation. For exam-
ple, to translate the word ?bury? into Chinese, 
our system gives a translation of ? , which 
means ?bury? as the action of digging a hole, 
hiding some items in the hole and then covering 
it with earth. But the desired translation, as it 
appears in the document is ? , which means 
?bury? too, but specifically for burial in funerals. 
Even more challenging are regional language 
differences. In our system, for example, the cor-
pora are newswire articles written in Traditional 
Chinese from Taiwan, and if we use an MT sys-
tem that produces translations in Simplified Chi-
nese followed by conversion to Traditional Chi-
nese, we may run into problems. The MT system 
generates Simplified Chinese translations first, 
which may suggest that the translation resources 
it uses were written in Simplified Chinese and 
originate from mainland China. In mainland 
China and in Taiwan, people commonly use dif-
ferent words for describing the same thing, espe-
cially for proper nouns like foreign names. Table 
9 lists some examples. Therefore if the MT sys-
tem generates its output using text from 
mainland China, it may produce a different word 
than the one used in Taiwan, which may not ap-
pear in the corpora. This could lead to failure in 
document retrieval.  
 
English  Mainland China Taiwan 
Band ?? ?? 
Computer Game ???? ?? 
World Guinness 
Record 
??????? ?????? 
 
The Catcher in 
the Rye 
??????? ???? 
 
Nelson ??? ??? 
Salinger ??? ??? 
Creutzfeldt 
Jakob Disease 
????? ???? 
 
Luc Besson ?? ?? ??? 
Pavarotti ???? ???? 
Table 9. Different Translation in Chinese 
5.2 Japanese 
Representational Gaps: One of the advantages 
of using structured queries and automatic query 
formulation in the RS is that the system is able to 
handle slight representational gaps between a 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
37
translated query and corresponding target words 
in the corpus.  
For example, Werner Spies appears as ???
?? ? ????? in our Japanese preproc-
essed corpus and therefore ????? ????
?, which is missing a dot between last and first 
name, is a wrong translation if our retrieval 
module only allows exact match. Lemur supports 
an Ordered Distance Operator where the terms 
within a #ODN operator must be found within N 
words of each other in the text in order to con-
tribute to the document's belief value. This en-
ables us to bridge the representational gaps; such 
as when #OD1(????? ?????) does not 
match any words in the corpus, #OD2(????? 
?????) is formulated in the next step in or-
der to capture ????? ? ?????. 
 
Transliteration in WBMT: After detecting 
Japanese nouns written in romaji (e.g. Funaba-
shi), we transliterated them into hiragana for a 
better result in WBMT. This is because we are 
assuming higher positive co-occurrence between 
kana and kanji (i.e. ???? and ??) than be-
tween romaji and kanji (i.e. funabashi and??). 
When there are multiple transliteration candi-
dates, we iterate through each candidate. 
 
Document Retrieval in Kana: Suppose we are 
going to transliterate Yusuke. This romaji can be 
mapped to kana characters with relatively less 
ambiguity (i.e. ??? , ????), when com-
pared to their subsequent transliteration to kanji 
(i.e. ??, ??, ??, ??, ?? etc.).  Therefore, 
indexing kana readings in the corpus and query-
ing in kana is sometimes a useful technique for 
CLQA, given the difficulty in converting romaji 
to kana and romaji to kanji.  
To implement this approach, the Japanese cor-
pus was first preprocessed by annotating named 
entities and by chunking morphemes. Then, we 
annotated a kana reading for each named entity. 
At query time, if there is no translation found 
from other resources, the TM transliterates ro-
maji to kana as a back-off strategy. 
6 Conclusion 
We described how we extended an existing 
monolingual (English) system for CLQA (Eng-
lish to Chinese and English to Japanese), includ-
ing a translation disambiguation technique 
which uses a noisy channel model with probabil-
ity estimations using web as corpora. We dis-
cussed the influence of translation accuracy on 
CLQA by presenting experimental results and 
analysis. We concluded by introducing some 
language-specific issues for keyword translation 
from English to Chinese and Japanese which we 
hope to address in ongoing research. 
Acknowledgements 
This work is supported by the Advanced Re-
search and Development Activity (ARDA)?s 
Advanced Question Answering for Intelligent 
(AQUAINT) Program. 
References  
Brown, P., J. Cocke, S.D. Pietra, V.D. Pietra, F. 
Jelinek., J. Lafferty, R. Mercer, and P. Roossin. 
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):38?45.  
Isozaki, H., K. Sudoh and H. Tsukada. 2005. NTT?s 
Japanese-English Cross-Language Question An-
swering System. In Proceedings of the NTCIR 
Workshop 5 Meeting, pages 186-193. 
Korhonen, A. and E. Briscoe. 2004. Extended Lexi-
cal-Semantic Classification of English Verbs. Pro-
ceedings of the HLT/NAACL '04 Workshop on 
Computational Lexical Semantics, pages 38-45. 
Kwok, K., P. Deng, N. Dinstl and S. Choi. 2005. 
NTCIR-5 English-Chinese Cross Language Ques-
tion-Answering Experiments using PIRCS. In Pro-
ceedings of the NTCIR Workshop 5 Meeting. 
Li, Hang, Yunbo Cao, and Cong Li. 2003. Using Bi-
lingual Web Data To Mine and Rank Translations, 
IEEE Intelligent Systems 18(4), pages 54-59. 
Mori, T. and M. Kawagishi. 2005. A Method of Cross 
Language Question-Answering Based on Machine 
Translation and Transliteration. In Proceedings of 
the NTCIR Workshop 5 Meeting. 
Nagata,  N., T. Saito, and K. Suzuki. 2001. Using the 
Web as a Bilingual Dictionary, In Proceedings of 
ACL 2001 Workshop Data-Driven Methods in Ma-
chine Translation, pages 95-102 
Nyberg, E., R. Frederking, T. Mitamura, J. M. Bilotti, 
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita, 
V. Pedro, A. Schlaikjer. 2005. JAVELIN I and II in 
TREC2005. In Proceedings of TREC 2005. 
Ogilvie, P. and J. Callan. 2001. Experiments Using 
the Lemur Toolkit. In Proceedings of the 2001 Text 
REtrieval Conference (TREC 2001), pages 103-
108. 
Turney, P.D. 2001, Mining the Web for synonyms: 
PMI-IR versus LSA on  TOEFL, Proceedings of the 
Twelfth European Conference on Machine Learn-
ing, pages 491-502. 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
38
Committed Belief Annotation and Tagging
Mona T. Diab Lori Levin
CCLS LTI
Columbia U. CMU
mdiab@cs.columbia.edu lsl@cs.cmu.edu
Teruko Mitamura Owen Rambow
LTI CCLS
CMU Columbia U.
teruko+@cs.cmu.edu rambow@ccls.columbia.edu
Vinodkumar Prabhakaran Weiwei Guo
CS CS
Columbia U. Columbia U.
Abstract
We present a preliminary pilot study of
belief annotation and automatic tagging.
Our objective is to explore semantic mean-
ing beyond surface propositions. We aim
to model people?s cognitive states, namely
their beliefs as expressed through linguis-
tic means. We model the strength of their
beliefs and their (the human) degree of
commitment to their utterance. We ex-
plore only the perspective of the author of
a text. We classify predicates into one of
three possibilities: committed belief, non
committed belief, or not applicable. We
proceed to manually annotate data to that
end, then we build a supervised frame-
work to test the feasibility of automati-
cally predicting these belief states. Even
though the data is relatively small, we
show that automatic prediction of a belief
class is a feasible task. Using syntactic
features, we are able to obtain significant
improvements over a simple baseline of
23% F-measure absolute points. The best
performing automatic tagging condition is
where we use POS tag, word type fea-
ture AlphaNumeric, and shallow syntac-
tic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
1 Introduction
As access to large amounts of textual informa-
tion increases, there is a strong realization that
searches and processing purely based on surface
words is highly limiting. Researchers in infor-
mation retrieval and natural language processing
(NLP) have long used morphological and (in a
more limited way) syntactic analysis to improve
access and processing of text; recently, interest has
grown in relating text to more abstract representa-
tions of its propositional meaning, as witnessed by
work on semantic role labeling, word sense disam-
biguation, and textual entailment. However, there
are more levels to ?meaning? than just proposi-
tional content. Consider the following examples,
and suppose we find these sentences in the New
York Times:1
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
If we are searching text to find out whether GM
will lay off workers, all of the sentences in (1) con-
1In this paper, we concentrate on written communication,
and we use the terms reader and writer. However, nothing in
the approach precludes applying it to spoken communication.
tain the proposition LAYOFF(GM,WORKERS).
However, the six sentences clearly allow us very
different inferences about whether GM will lay off
workers or not. Supposing we consider the Times
a trustworthy news source, we would be fairly cer-
tain with (1a) and (1b). (1c) suggests the Times is
not certain about the layoffs, but considers them
possible. When reading (1d), we know that some-
one else thinks that GM will lay off workers, but
that the Times does not necessarily share this be-
lief. (1e), (1f), and (1g) do not tell us anything
about whether anyone believes whether GM will
lay off workers.
In order to tease apart what is happening, we
need to refine a simple IR-ish view of text as a
repository of propositions about the world. We use
two theories to aid us. The first theory is that in ad-
dition to facts about the world (GM will or will not
lay off workers), we have facts about people?s cog-
nitive states, and these cognitive states relate their
bearer to the facts in the world. (Though perhaps
there are only cognitive states, and no facts about
the world.) Following the literature in Artificial
Intelligence (Cohen and Levesque, 1990), we can
model cognitive state as beliefs, desires, and inten-
tions. In this paper, we are only interested in be-
liefs (and in distinguishing them from desires and
intentions). The second theory is that communi-
cation is intention-driven, and understanding text
actually means understanding the communicative
intention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader?s
cognitive state ? his or her beliefs, desires, and/or
intentions. This view has been worked out in the
text generation and dialog community more than
in the text understanding community (Mann and
Thompson, 1987; Hovy, 1993; Moore, 1994).
In this paper we are interested in exploring the
following: we would like to recognize what the
text wants to make us believe about various peo-
ple?s cognitive states, including the speaker?s. As
mentioned, we are only interested in people?s be-
lief. In this view, the result of text processing is
not a list of facts about the world, but a list of facts
about different people?s cognitive states.
This paper is part of an on-going research effort.
The goals of this paper are to summarize a pilot
annotation effort, and to present the results of ini-
tial experiments in automatically extracting facts
about people?s beliefs from open domain running
text.
2 Belief Annotation
We have developed a manual for annotating be-
lief, which we summarize here. For more de-
tailed information, we refer to the cited works. In
general, we are interested in the writer?s intention
as to making us believe that various people have
certain beliefs, desires, and intentions. We sim-
plify the annotation in two ways: we are only in-
teretsed in beliefs, and we are only interested in
the writer?s beliefs. This is not because we think
this is the only interesting information in text, but
we do this in order to obtain a manageable anno-
tation in our pilot study. Specifically, we annotate
whether the writer intends the reader to interpret
a stated proposition as the writer?s strongly held
belief, as a proposition which the writer does not
believe strongly (but could), or as a proposition
towards which the writer has an entirely differ-
ent cognitive attitude, such as desire or intention.
We do not annotate subjectivity (Janyce Wiebe and
Martin, 2004; Wilson and Wiebe, 2005), nor opin-
ion (for example: (Somasundaran et al, 2008)):
the nature of the proposition (opinion and type of
opinion, statement about interior world, external
world) is not of interest. Thus, this work is or-
thogonal to the extensive literature on opinion de-
tection. And we do not annotate truth: real-world
(encyclopedic) truth is not relevant.
We have three categories:
? Committed belief (CB): the writer indicates
in this utterance that he or she believes the
proposition. For example, GM has laid off
workers, or, even stronger, We know that GM
has laid off workers.
A subcase of committed belief concerns
propositions about the future, such as GM
will lay off workers. People can have equally
strong beliefs about the future as about the
past, though in practice probably we have
stronger beliefs about the past than about the
future.
? Non-committed belief (NCB): the writer
identifies the propositon as something which
he or she could believe, but he or she hap-
pens not to have a strong belief in. There are
two subcases. First, there are cases in which
the writer makes clear that the belief is not
strong, for example by using a modal auxil-
iary:2 GM may lay off workers. Second, in
reported speech, the writer is not signaling to
us what he or she believes about the reported
speech: The politician claimed that GM will
lay off workers. However, sometimes, we can
use the speech act verb to infer the writer?s
attitude,3 and we can use our own knowledge
2The annotators must distinguish epistemic and deontic
uses of modals.
3Some languages may also use grammatical devices; for
to infer the writer?s beliefs; for example, in
A GM spokesman said that GM will lay off
workers, we can assume that the writer be-
lieves that GM intends to lay off workers, not
just the spokesman. However, this is not part
of the annotation, and all reported speech is
annotated as NCB. Again, the issue of tense
is orthogonal.
? Not applicable (NA): for the writer, the
proposition is not of the type in which he or
she is expressing a belief, or could express a
belief. Usually, this is because the proposi-
tion does not have a truth value in this world
(be it in the past or in the future). This covers
expressions of desire (Some wish GM would
lay of workers), questions (Will GM lay off
workers? or Many wonder if GM will lay
off workers, and expressions of requirements
(GM is required to lay off workers or Lay off
workers!).
This sort of annotation is part of an annotation
of all ?modalities? that a text may express. We
only annotate belief. A further complication is
that these modalities can be nested: one can ex-
press a belief about someone else?s belief, and one
may be strong and the other weak (I believe John
may believe that GM will lay off workers). At this
phase, we only annotate from the perspective of
the writer, i.e. what the writer of the text that is
being annotated believes.
The annotation units (annotatables) are, con-
ceptually, propositions as defined by PropBank
(Kingsbury et al, 2002). In practice, annotators
are asked to identify full lexical verbs (whether
in main or embedded clauses, whether finite or
non-finite). In predicative constructions (John is a
doctor/in the kitchen/drunk), we ask them to iden-
tify the nominal, prepositional, or adjectival head
rather than the form of to be, in order to also han-
dle small clauses (I think [John an idiot]).
The interest of the annotation is clear: we want
to be able to determine automatically from a given
text what beliefs we can ascribe to the writer,
and with what strengths he or she holds them.
Across languages, many different linguistic means
are used to denote this attitude towards an uttered
proposition, including syntax, lexicon, and mor-
phology. To our knowledge, no systematic empir-
ical study exists for English, and this annotation is
a step towards that goal.
example, in German, the choice between indicative mood and
subjunctive mood in reported speech can signal the writer?s
attitude.
3 Related Work
The work of Roser et al (2006) is, in many re-
spects, very similar to ours. In particular, they are
concerned with extracting information about peo-
ple?s beliefs and the strength of these beliefs from
text. However, their annotation is very different
from ours. They extend the TimeML annotation
scheme to include annotation of markers of belief
and strength of belief. For example, in the sen-
tence The Human Rights Committee regretted that
discrimination against women persisted in prac-
tice, TimeML identifies the events associated with
the verbs regret and persist, and then the extension
to the annotation adds the mark that there is a ?fac-
tive? link between the regret event and the persist
event, i.e., if we regret something, then we assume
the truth of that something. In contrast, in our
annotation, we directly annotate events with their
level of belief. In this example, we would annotate
persist as being a committed belief of the Human
Rights Committee (though in this paper we only
report on beliefs attributed to the writer). This dif-
ference is important, as in the annotation of Roser
et al (2006), the annotator must analyze the situ-
ation and find evidence for the level of belief at-
tributed to an event. As a result, we cannot use
the annotation to discover how natural language
expresses level of belief. Our annotation is more
primitively semantic: we ask the annotators sim-
ply to annotate meaning (does X believe the event
takes place), as opposed to annotating the linguis-
tic structures which express meaning. As a conse-
quence of the difference in annotation, we cannot
compare our automatic prediction results to theirs.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Ralf Krestel and Bergler,
2007; Krestel et al, 2008), the authors explore
belief in the context of news media exploring re-
ported speech where they track newspaper text
looking for elements indicating evidentiality. The
notion of belief is more akin to finding statements
that support or negate specific events with differ-
ent degrees of support. This is different from our
notion of committed belief in this work, since we
seek to make explicit the intention of the author or
the speaker.
4 Our Approach
4.1 Data
We create a relatively small corpus of English
manually annotated for the three categories: CB,
NCB, NA. The data covers different domains and
genres from newswire, to blog data, to email cor-
respondence, to letter correspondence, to tran-
scribed dialogue data. The data comprises 10K
words of running text. 70% of the data was dou-
bly annotated comprising 6188 potentially anno-
tatable tokens. Hence we had a 4 way manual clas-
sification in essence between NONE, CB, NCB,
and NA. Most of the confusions between NONE
and CB from both annotators, for 103 tokens.
The next point of disagreement was on NCB and
NONE for 48 tokens.They disagreed on NCB and
CB for 32 of the tokens. In general the interanno-
tator agreements were high as they agreed 95.8%
of the time on the annotatable and the exact belief
classification.4 Here is an example of a disagree-
ment between the two annotators, The Iraqi gov-
ernment has agreed to let Rep Tony Hall visit the
country next week to assess a humanitarian cri-
sis that has festered since the Gulf War of 1991
Hall?s office said Monday. One annotator deemed
?agreed? a CB while the other considered it an
NCB.
4.2 Automatic approach
Once we had the data manually annotated and re-
vised, we wanted to explore the feasibility of au-
tomatically predicting belief states based on lin-
guistic features. We apply a supervised learning
framework to the problem of both identifying and
classifying a belief annotatable token in context.
This is a three way classification task where an
annotatable token is tagged as one of our three
classes: Committed Belief (CB), Non Committed
Belief (NCB), and Not Applicable (NA). We adopt
a chunking approach to the problem using an In-
side Outside Beginning (IOB) tagging framework
for performing the identification and classification
of belief tokens in context. For chunk tagging,
we use YamCha sequence labeling system.5 Yam-
Cha is based on SVM technology. We use the de-
fault parameter settings most importantly the ker-
nels are polynomial degree 2 with a c value of 0.5.
We label each sentence with standard IOB tags.
Since this is a ternary classification task, we have
7 different tags: B-CB (Beginning of a commit-
ted belief chunk), I-CB (Inside of a committed be-
lief chunk), B-NCB (Beginning of non commit-
ted belief chunk), I-NCB (Inside of a non com-
mitted belief chunk), B-NA (Beginning of a not
applicable chunk), I-NA (Inside a not applicable
chunk), and O (Outside a chunk) for the cases
that are not annotatable tokens. As an example
of the annotation, a sentence such as Hall said
he wanted to investigate reports from relief agen-
cies that a quarter of Iraqi children may be suffer-
4This interannotator agreement number includes the
NONE category.
5http://www.tado-chasen.com/yamcha
ing from chronic malnutrition. will be annotated
as follows: {Hall O said B-CB he O wanted B-
NCB to B-NA investigate I-NA reports O from O
relief O agencies O that O a O quarter O of O
Iraqi O children O may O be O suffering B-NCB
from O chronic O malnutrition O.}
We experiment with some basic features and
some more linguistically motivated ones.
CXT: Since we adopt a sequence labeling
paradigm, we experiment with different window
sizes for context ranging from ?/+2 tokens after
and before the token of interest to ?/+5.
NGRAM: This is a character n-gram feature,
explicity representing the first and last character
ngrams of a word. In this case we experiment with
up to ?/+4 characters of a token. This feature
allows us to capture implicitly the word inflection
morphology.
POS: An important feature is the Part-of-Speech
(POS) tag of the words. Most of the annotatables
are predicates but not all predicates in the text are
annotatables. We obtain the POS tags from the
TreeTagger POS tagger tool which is trained on
the Penn Treebank.6
ALPHANUM: This feature indicates whether
the word has a digit in it or not or if it is a non
alphanumeric token.
VerbType: We classify the verbs as to whether
they are modals (eg. may, might, shall, will,
should, can, etc.), auxilliaries (eg. do, be, have),7
or regular verbs. Many of our annotatables occur
in the vicinity of modals and auxilliaries. The list
of modals and auxilliaries is deterministic.
Syntactic Chunk (CHUNK): This feature ex-
plicitly models the syntactic phrases in which our
tokens occur. The possible phrases are shallow
syntactic representations that we obtain from the
TreeTagger chunker:8 ADJC (Adjective Chunk),
ADVC (Adverbial Chunk), CONJC (Conjunc-
tional Chunk), INTJ (Interjunctional Chunk), LST
(numbers 1, 2,3 etc), NC (Noun Chunk), PC
(Prepositional Chunk), PRT (off,out,up etc), VC
(Verb Chunk).
5 Experiments and Results
5.1 Conditions
Since the data is very small, we tested our au-
tomatic annotation using 5 fold cross validation
6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
7We realize in some of the grammar books auxilliaries
include modal verbs.
8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
where 10% of the data is set aside as development
data, then 70% is used for training and 20% for
testing. The reported results are averaged over the
5 folds for the Test data for each of our experimen-
tal conditions.
Our baseline condition is using the tokenized
words only with no other features (TOK). We em-
pirically establish that a context size of ?/+3
yields the best results in the baseline condition as
evaluated on the development data set. Hence all
the results are yielded from a CXT of size 3.
The next conditions present the impact of
adding a single feature at a time and then combin-
ing them. It is worth noting that the results reflect
the ability of the classifier to identify a token that
could be annotatable and also classify it correctly
as one of the possible classes.
5.2 Evaluation Metrics
We use F?=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall. All the pre-
sented results are the F-measure. We report the
results separately for the three classes CB, NCB,
and NA as well as the overall global F measure for
any one condition averaged over the 5 folds of the
TEST data set.
5.3 Results
In Table 1 we present the results yielded per con-
dition including the baseline TOK and presented
for the three different classes as well as the overall
F-measure.
All the results yielded by our experiments
outperform the baseline TOK. We highlight
the highest performing conditions in Ta-
ble 1: TOK+AlphaNum+POS +CHUNK,
TOK+AN+POS and TOK+POS. Even though
all the features independently outperform the
baseline TOK in isolation, POS is the single most
contributing feature. The least contributing factor
independently is the AlphaNumeric feature AN.
However combining AN with character Ngram
NG yields better results than using each of them
independently. We note that adding NG to any
other feature combination is not helpful, in fact
it seems to add noise rather than signal to the
learning process in the presence of more sophis-
ticated features such as POS or syntactic chunk
information. Adding the verbtype VT explicitly
as a feature is not helpful for all categories, it
seems most effective with CB. As mentioned
earlier we deterministically considered all modal
verbs to be modal. This might not be the case
for all modal auxilliaries since some of them
are used epistemically while others deontically,
hence our feature could be introducing an element
of noise. Adding syntactic chunk information
helps boost the results by a small margin from
53.5 to 53.97 F-measure. All the results seem to
suggest the domination of the POS feature and it?s
importance for such a tagging problem. In general
our performance on CB is the highest, followed
by NA then we note that NCB is the hardest
category to predict. Examining the data, NCB
has the lowest number of occurrence instances
in this data set across the board in the whole
data set and accordingly in the training data,
which might explain the very low performance.
Also in our annotation effort, it was the hardest
category to annotate since the annotation takes
more than the sentential context into account.
Hence a typical CB verb such as ?believe? in the
scope of a reporting predicate such as ?say? as
in the following example Mary said he believed
the suspect with no qualms. The verb believed
should be tagged NCB however in most cases it
is tagged as a CB. Our syntactic feature CHUNK
helps a little but it does not capture the overall
dependencies in the structure. We believe that
representing deeper syntactic structure should
help tremendously as it will model these relatively
longer dependencies.
We also calculated a confusion matrix for the
different classes. The majority of the errors are
identification errors where an annotatable is con-
sidered an O class as opposed to one of the 3 rel-
evant classes. This suggests that identifying the
annotatable words is a harder task than classifica-
tion into one of the three classes, which is consis-
tent with our observation from the interannotator
disagreements where most of their disagreements
were on the annotatable tokens, though a small
overall number of tokens, 103 tokens out of 6188,
it was the most significant disagreement category.
We find that for the TOK+POS condition, CBs are
mistagged as un-annotatable O 55% of the time.
We find most of the confusions between NA and
CB, and NCB and CB, both cases favoring a CB
tag.
6 Conclusion
We presented a preliminary pilot study of belief
annotation and automatic tagging. Even though
the data is relatively tiny, we show that automatic
prediction of a belief class is a feasible task. Us-
ing syntactic features, we are able to obtain signif-
icant improvements over a simple baseline of 23%
F-measure absolute points. The best performing
automatic tagging condition is where we use POS
tag, word type feature AlphaNumeric, and shallow
syntactic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
CB NA NCB Overall F
TOK 25.12 41.18 13.64 30.3
TOK+NG 33.18 42.29 5 34.25
TOK+AN 30.43 44.57 12.24 33.92
TOK+AN+NG 37.17 42.46 9.3 36.61
TOK+POS 54.8 59.23 13.95 53.5
TOK+NG+POS 43.15 50.5 22.73 44.35
TOK+AN+POS 54.79 58.97 22.64 53.54
TOK+NG+AN+POS 43.09 54.98 18.18 45.91
TOK+POS+CHUNK 55.45 57.5 15.38 52.77
TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43
TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97
TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89
Table 1: Final results averaged over 5 folds of test data using different features and their combinations:
NG is NGRAM, AN is AlphaNumeric, VT is verbtype
In the future we are looking at ways of adding
more sophisticated deep syntactic and semantic
features using lexical chains from discourse struc-
ture. We will also be exploring belief annotation in
Arabic and Urdu on a parallel data collection since
these languages express evidentiality in ways that
differ linguistically from English. Finally we will
explore ways of automatically augmenting the la-
beled data pool using active learning.
Acknowledgement
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the sponsor.
References
Philip R. Cohen and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Jerry Morgan Philip Cohen and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Rebecca Bruce Matthew Bell Janyce Wiebe,
Theresa Wilson and Melanie Martin. 2004.
Learning subjective language. In Computational
Linguistics, Volume 30 (3).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May 28?30.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Johanna Moore. 1994. Participating in Explanatory
Dialogues. MIT Press.
Rene? Witte Ralf Krestel and Sabine Bergler. 2007.
Processing of Beliefs extracted from Reported
Speech in Newspaper Articles. In International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bul-
garia, September 27?29.
Saur?? Roser, Marc Verhagen, and James Pustejovsky.
2006. Annotating and Recognizing Event Modality
in Text. In FLAIRS 2006, editor, In Proceedings
of the 19th International FLAIRS Conference, Mel-
bourne Beach, Florida, May 11-13.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53?60, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Proceedings of the ACL 2010 Conference Short Papers, pages 115?119,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Collocation Suggestion in Academic Writing 
 Jian-Cheng Wu1 Yu-Chia Chang1,* Teruko Mitamura2 Jason S. Chang1 1 National Tsing Hua University Hsinchu, Taiwan {wujc86, richtrf, jason.jschang} @gmail.com 
2 Carnegie Mellon University  Pittsburgh, United States            teruko@cs.cmu.edu 
 Abstract In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speak-ers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica-tion. The system generates and ranks suggestions to assist learners? collocation usages in their academic writing with sat-isfactory results. * 1 Introduction The notion of collocation has been widely dis-cussed in the field of language teaching for dec-ades. It has been shown that collocation, a suc-cessive common usage of words in a chain, is important in helping language learners achieve native-like fluency. In the field of English for Academic Purpose, more and more researchers are also recognizing this important feature in academic writing. It is often argued that colloca-tion can influence the effectiveness of a piece of writing and the lack of such knowledge might cause cumulative loss of precision (Howarth, 1998). Many researchers have discussed the function of collocations in the highly conventionalized and specialized writing used within academia. Research also identified noticeable increases in the quantity and quality of collocational usage by                                                            * Corresponding author: Yu-chia Chang (Email address: richtrf@gmail.com) 
native speakers (Howarth, 1998). Granger (1998) reported that learners underuse native-like collo-cations and overuse atypical word combinations. This disparity in collocation usage between na-tive and non-native speakers is clear and should receive more attention from the language tech-nology community. To tackle such word usage problems, tradi-tional language technology often employs a da-tabase of the learners' common errors that are manually tagged by teachers or specialists (e.g. Shei and Pain, 2000; Liu, 2002). Such system then identifies errors via string or pattern match-ing and offer only pre-stored suggestions. Com-piling the database is time-consuming and not easily maintainable, and the usefulness is limited by the manual collection of pre-stored sugges-tions. Therefore, it is beneficial if a system can mainly use untagged data from a corpus contain-ing correct language usages rather than the error-tagged data from a learner corpus. A large corpus of correct language usages is more readily avail-able and useful than a small labeled corpus of incorrect language usages. For this suggestion task, the large corpus not only provides us with a rich set of common col-locations but also provides the context within which these collocations appear. Intuitively, we can take account of such context of collocation to generate more suitable suggestions. Contextual information in this sense often entails more lin-guistic clues to provide suggestions within sen-tences or paragraph. However, the contextual information is messy and complex and thus has long been overlooked or ignored. To date, most fashionable suggestion methods still rely upon the linguistic components within collocations as well as the linguistic relationship between mis-used words and their correct counterparts (Chang et al, 2008; Liu, 2009).  In contrast to other research, we employ con-textual information to automate suggestions for verb-noun lexical collocation. Verb-noun collo-cations are recognized as presenting the most 
115
challenge to students (Howarth, 1996; Liu, 2002). More specifically, in this preliminary study we start by focusing on the word choice of verbs in collocations which are considered as the most difficult ones for learners to master (Liu, 2002; Chang, 2008). The experiment confirms that our collocation writing assistant proves the feasibility of using machine learning methods to automatically prompt learners with collocation suggestions in academic writing.  2 Collocation Checking and Suggestion This study aims to develop a web service, Collo-cation Inspector (shown in Figure 1) that accepts sentences as input and generates the related can-didates for learners. In this paper, we focus on automatically pro-viding academic collocation suggestions when users are writing up their abstracts. After an ab-stract is submitted, the system extracts linguistic features from the user?s text for machine learning model. By using a corpus of published academic texts, we hope to match contextual linguistic clues from users? text to help elicit the most rele-vant suggestions. We now formally state the problem that we are addressing: Problem Statement: Given a sentence S writ-ten by a learner and a reference corpus RC, our goal is to output a set of most probable sugges-tion candidates c1, c2, ... , cm. For this, we train a classifier MC to map the context (represented as feature set f1, f2, ..., fn) of each sentence in RC to the collocations. At run-time, we predict these collocations for S as suggestions. 2.1 Academic Collocation Checker Train-ing Procedures Sentence Parsing and Collocation Extraction: We start by collecting a large number of ab-stracts from the Web to develop a reference cor-pus for collocation suggestion. And we continue to identify collocations in each sentence for the subsequent processing. Collocation extraction is an essential step in preprocessing data. We only expect to extract the collocation which comprises components having a syntactic relationship with one another. How-ever, this extraction task can be complicated. Take the following scholarly sentence from the reference corpus as an example (example (1)): 
(1) We introduce a novel method for learning to find documents on the web. 
 Figure 1. The interface for the collocation suggestion  
nsubj (introduce-2, We-1) det (method-5, a-3) amod (method-5, novel-4) dobj (introduce-2, method-5) prepc_for (introduce-2, learning-7) aux (find-9, to-8) ? ? Figure 2. Dependency parsing of Example (1)  Traditionally, through part-of-speech tagging, we can obtain a tagged sentence as follows (ex-ample (2)). We can observe that the desired col-location ?introduce method?, conforming to ?VERB+NOUN? relationship, exists within the sentence. However, the distance between these two words is often flexible, not necessarily rigid. Heuristically writing patterns to extract such verb and noun might not be effective. The patterns between them can be tremendously varied. In addition, some verbs and nouns are adjacent, but they might be intervened by clause and thus have no syntactic relation with one another (e.g. ?pro-pose model? in example (3)). 
(2) We/PRP  introduce/VB  a/DT  novel/JJ  method/NN  for/IN  learning/VBG  to/TO  find/VB  documents/NNS  on/IN  the/DT  web/NN  ./.  
(3) We proposed that the web-based model would be more ef-fective than corpus-based one. A natural language parser can facilitate the ex-traction of the target type of collocations. Such parser is a program that works out the grammati-cal structure of sentences, for instance, by identi-fying which group of words go together or which 
116
word is the subject or object of a verb. In our study, we take advantage of a dependency parser, Stanford Parser, which extracts typed dependen-cies for certain grammatical relations (shown in Figure 2). Within the parsed sentence of example (1), we can notice that the extracted dependency ?dobj (introduce-2, method-4)? meets the crite-rion.  Using a Classifier for the Suggestion task: A classifier is a function generally to take a set of attributes as an input and to provide a tagged class as an output. The basic way to build a clas-sifier is to derive a regression formula from a set of tagged examples. And this trained classifier can thus make predication and assign a tag to any input data. The suggestion task in this study will be seen as a classification problem. We treat the colloca-tion extracted from each sentence as the class tag (see examples in Table 1). Hopefully, the system can learn the rules between tagged classes (i.e. collocations) and example sentences (i.e. schol-arly sentences) and can predict which collocation is the most appropriate one given attributes ex-tracted from the sentences. Another advantage of using a classifier to automate suggestion is to provide alternatives with regard to the similar attributes shared by sentences. In Table 1, we can observe that these collocations exhibit a similar discourse function and can thus become interchangeable in these sentences. Therefore, based on the outputs along with the probability from the classifier, we can provide more than one adequate suggestions.  Feature Selection for Machine Learning: In the final stage of training, we build a statistical machine-learning model. For our task, we can use a supervised method to automatically learn the relationship between collocations and exam-ple sentences. We choose Maximum Entropy (ME) as our train-ing algorithm to build a collocation suggestion classifier. One advantage of an ME classifier is that in addition to assigning a classification it can provide the probability of each assignment. The ME framework estimates probabilities based on the principle of making as few assumptions as possible. Such constraints are derived from the training data, expressing relationships between features and outcomes.  Moreover, an effective feature selection can increase the precision of machine learning. In our study, we employ the contextual features which  
Table 1. Example sentences and class tags (colloca-tions) Example Sentence  Class tag   We introduce a novel method for learning to find documents on the web.  introduce  We presented a method of improving Japa-nese dependency parsing by using large-scale statistical information.  present  In this paper, we will describe a method of identifying the syntactic role of antece-dents, which consists of two phases  describe  In this paper, we suggest a method that automatically constructs an NE tagged cor-pus from the web to be used for learning of NER systems.  suggest   consist of two elements, the head and the ngram of context words:  Head: Each collocation comprises two parts, collocate and head. For example, in a given verb-noun collocation, the verb is the collocate as well as the target for which we provide suggestions; the noun serves as the head of collocation and convey the essential meaning of the collocation. We use the head as a feature to condition the classifier to generate candidates relevant to a given head.  Ngram: We use the context words around the target collocation by considering the correspond-ing unigrams and bigrams words within the sen-tence. Moreover, to ensure the relevance, those context words, before and after the punctuation marks enclosing the collocation in question, will be excluded. We use the parsed sentence from previous step (example (2)) to show the extracted context features1 (example (4)): 
(4) CN=method UniV_L=we UniV_R=a UniV_R=novel UniN_L=a UniN_L=novel UniN_R=for UniN_R=learn BiV_R=a_novel BiN_L=a_novel BiN_R=for_learn BiV_I=we_a BiN_I=novel_for  
                                                           1 CN refers to the head within collocation. Uni and Bi indi-cate the unigram and bigram context words of window size two respectively. V and N differentiate the contexts related to verb or noun. The ending alphabets L, R, I show the posi-tion of the words in context, L = left, R = right, and I = in between. 
117
2.2 Automatic Collocation Suggestion at Run-time After the ME classifier is automatically trained, the model is used to find out the best collocation suggestion. Figure 3 shows the algorithm of pro-ducing suggestions for a given sentence. The input is a learner?s sentence in an abstract, along with an ME model trained from the reference corpus.  In Step (1) of the algorithm, we parse the sen-tence for data preprocessing. Based on the parser output, we extract the collocation from a given sentence as well as generate features sets in Step (2) and (3). After that in Step (4), with the trained machine-learning model, we obtain a set of likely collocates with probability as predicted by the ME model. In Step (5), SuggestionFilter singles out the valid collocation and returns the best collocation suggestion as output in Step (6). For example, if a learner inputs the sentence like Example (5), the features and output candidates are shown in Table 2. 
(5) There are many investiga-tions about wireless network communication, especially it is important to add Internet transfer calculation speeds.  3 Experiment From an online research database, CiteSeer, we have collected a corpus of 20,306 unique ab-stracts, which contained 95,650 sentences. To train a Maximum Entropy classifier, 46,255 col-locations are extracted and 790 verbal collocates are identified as tagged classes for collocation suggestions. We tested the classifier on scholarly sentences in place of authentic student writings which were not available at the time of this pilot study. We extracted 364 collocations among 600 randomly selected sentences as the held out test data not overlapping with the training set. To automate the evaluation, we blank out the verb collocates within these sentences and treat these verbs directly as the only correct suggestions in question, although two or more suggestions may be interchangeable or at least appropriate. In this sense, our evaluation is an underestimate of the performance of the proposed method.    While evaluating the quality of the suggestions provided by our system, we used the mean recip-rocal rank (MRR) of the first relevant sugges-tions returned so as to assess whether the sugges-tion list contains an answer and how far up the answer is in the list as a quality metric of the sys-  
Procedure CollocationSuggestion(sent, MEmodel)   (1)   parsedSen = Parsing(sent) (2)   extractedColl = CollocationExtraction(parsedSent) (3)   features = AssignFeature(ParsedSent)   (4)   probCollection = MEprob(features, MEmodel)    (5)   candidate = SuggestionFilter(probCollection) (6)   Return candidate  Figure 3. Collocation Suggestion at Run-time  Table 2. An example from learner?s sentence Extracted Collocation Features Ranked Candidates 
add speed 
CN=speed UniV_L=important UniV_L=to UniV_R=internet UniV_R=transfer UniN_L=transfer UniN_L=calculation BiV_L=important_to BiV_R=internet_transfer BiN_L=transfer_calcula-tion BiV_I=to_intenet 
improve increase determine maintain ? ? 
 Table 3. MRR for different feature sets Feature Sets Included In Classifier MRR  Features of HEAD 0.407 Features of CONTEXT 0.469 Features of HEAD+CONTEXT 0.518  tem output. Table 3 shows that the best MRR of our prototype system is 0.518. The results indi-cate that on average users could easily find an-swers (exactly reproduction of the blanked out collocates) in the first two to three ranking of suggestions. It is very likely that we get a much higher MMR value if we would go through the lists and evaluate each suggestion by hand. Moreover, in Table 3, we can further notice that contextual features are quite informative in com-parison with the baseline feature set containing merely the feature of HEAD. Also the integrated feature set of HEAD and CONTEXT together achieves a more satisfactory suggestion result. 4 Conclusion Many avenues exist for future research that are important for improving the proposed method. For example, we need to carry out the experi-ment on authentic learners? texts. We will con-duct a user study to investigate whether our sys-tem would improve a learner?s writing in a real setting. Additionally, adding classifier features based on the translation of misused words in learners? text could be beneficial  (Chang et al, 
118
2008). The translation can help to resolve preva-lent collocation misuses influenced by a learner's native language. Yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as AN and PN in addition to VN dealt with in this paper. In summary, we have presented an unsuper-vised method for suggesting collocations based on a corpus of abstracts collected from the Web. The method involves selecting features from the reference corpus of the scholarly texts. Then a classifier is automatically trained to determine the most probable collocation candidates with regard to the given context. The preliminary re-sults show that it is beneficial to use classifiers for identifying and ranking collocation sugges-tions based on the context features.  Reference Y. Chang, J. Chang, H. Chen, and H. Liou. 2008. An automatic collocation writing assistant for Taiwan-ese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learn-ing, 21(3), pages 283-299. S. Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In Cowie, A. (ed.) Phraseology: theory, analysis and applica-tions. Oxford University Press, Oxford, pages 145-160. P. Howarth. 1996. Phraseology in English Academic Writing. T?bingen: Max Niemeyer Verlag. P. Howarth. 1998. The phraseology of learner?s aca-demic writing. In Cowie, A. (ed.) Phraseology: theory, analysis and applications. Oxford Univer-sity Press, Oxford, pages 161-186. D. Hawking and N. Craswell. 2002. Overview of the TREC-2001 Web track. In Proceedings of the 10th Text Retrieval Conference (TREC 2001), pages 25-31. L. E. Liu. 2002. A corpus-based lexical semantic in-vestigation of verb-noun miscollocations in Taiwan learners? English. Unpublished master?s thesis, Tamkang University, Taipei, January. A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 47-50. C. C. Shei and H. Pain. 2000. An ESL writer?s collo-cational aid. Computer Assisted Language Learn-ing, 13, pages 167-182. 
119
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 35?39,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Diversity-aware Evaluation for Paraphrase Patterns 
 
 
Hideki Shima Teruko Mitamura 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA, USA Pittsburgh, PA, USA 
hideki@cs.cmu.edu teruko@cs.cmu.edu 
 
 
 
 
Abstract 
Common evaluation metrics for paraphrase 
patterns do not necessarily correlate with 
extrinsic recognition task performance. We 
propose a metric which gives weight to lex-
ical variety in paraphrase patterns; our pro-
posed metric has a positive correlation with 
paraphrase recognition task performance, 
with a Pearson correlation of 0.5~0.7 (k=10, 
with ?strict? judgment) in a statistically sig-
nificant level (p-value<0.01). 
1 Introduction 
We propose a diversity-aware paraphrase evaluation me-
tric called DIMPLE1, which boosts the scores of lexically 
diverse paraphrase pairs. Paraphrase pairs or patterns are 
useful in various NLP related research domains, since 
there is a common need to automatically identify meaning 
equivalence between two or more texts. 
Consider a paraphrase pair resource that links ?killed? 
to ?assassinated? (in the rest of this paper we denote such 
a rule as ??killed?2, ?assassinated?3?). In automatic evalu-
ation for Machine Translation (MT) (Zhou et al, 2006; 
Kauchak and Barzilay, 2006; Pad? et al, 2009), this rule 
may enable a metric to identify phrase-level semantic 
similarity between a system response containing ?killed?, 
and a reference translation containing ?assassinated?. 
Similarly in query expansion for information retrieval 
(IR) (Riezler et al, 2007), this rule may enable a system to 
                                                 
1
 DIversity-aware Metric for Pattern Learning Experiments 
2
 Source term/phrase that contains ?killed? 
3
 Paraphrase that contains ?assassinated? 
expand the query term ?killed? with the paraphrase ?as-
sassinated?, in order to match a potentially relevant doc-
ument containing the expanded term. 
To evaluate paraphrase patterns during pattern dis-
covery, ideally we should use an evaluation metric that 
strongly predicts performance on the extrinsic task (e.g. 
fluency and adequacy scores in MT, mean average 
precision in IR) where the paraphrase patterns are used.  
Many existing approaches use a paraphrase evaluation 
methodology where human assessors judge each paraph-
rase pair as to whether they have the same meaning. Over 
a set of paraphrase rules for one source term, Expected 
Precision (EP) is calculated by taking the mean of preci-
sion, or the ratio of positive labels annotated by assessors 
(Bannard and Callison-Burch, 2005; Callison-Burch, 
2008; Kok and Brockett, 2010; Metzler et al, 2011).  
The weakness of this approach is that EP is an in-
trinsic measure that does not necessarily predict how 
well a paraphrase-embedded system will perform in 
practice. For example, a set of paraphrase pairs 
??killed?, ?shot and killed??, ??killed?, ?reported 
killed?? ? ??killed?, ?killed in?? may receive a perfect 
score of 1.0 in EP; however, these patterns do not 
provide lexical diversity (e.g. ??killed?, ?assassi-
nated??) and therefore may not perform well in an 
application where lexical diversity is important.  
The goal of this paper is to provide empirical evidence 
to support the assumption that the proposed paraphrase 
evaluation metric DIMPLE correlates better with pa-
raphrase recognition task metric scores than previous 
metrics do, by rewarding lexical diverse patterns. 
2 DIMPLE Metric 
Patterns or rules for capturing equivalence in meaning 
are used in various NLP applications. In a broad sense, 
35
the terms ?paraphrase? will be used to denote pairs or 
a set of patterns that represent semantically equivalent 
or close texts with different surface forms.  
Given paraphrase patterns P, or the ranked list of dis-
tinct paraphrase pairs sorted by confidence in descending 
order, DIMPLEk evaluates the top k patterns, and pro-
duces a real number between 0 and 1 (higher the better). 
2.1 Cumulative Gain 
DIMPLE is inspired by the Cumulative Gain (CG) 
metric (J?rvelin and Kek?l?inen, 2002; Kek?l?inen, 
2005) used in IR. CG for the top k retrieved docu-
ments is calculated as ?
=
=
k
i ik
gain
1
CG
 
where the 
gain function is human-judged relevance grade of the 
i-th document with respect to information need (e.g. 0 
through 3 for irrelevant, marginally relevant, fairly 
relevant and highly relevant respectively). We take an 
alternative well-known formula for CG calculation, 
which puts stronger emphasis at higher gain: 
.)1^2(CG
1? = ?= ki ik gain  
2.2 DIMPLE Algorithm 
DIMPLE is a normalized CG calculated on each pa-
raphrase. The gain function of DIMPLE is 
represented as a product of pattern quality Q and lex-
ical diversity D: .iii DQgain ?=
 
DIMPLE at rank k is 
a normalized CGk which is defined as: 
Z
DQ
Z
k
i iik
k
?
=
??
==
1
}1)(^2{CGDIMPLE
 
where Z is a normalization factor such that the perfect 
CG score is given. Since Q takes a real value between 
0 and 1, and D takes an integer between 1 and 3, 
.}13^2{
1? = ?= kiZ
 Being able to design Q and D independently is one of 
characteristics in DIMPLE. In theory, Q can be any 
quality measure on paraphrase patterns, such as the in-
stance-based evaluation score (Szpektor et al, 2007), or 
alignment-based evaluation score (Callison-Burch et al, 
2008). Similarly, D can be implemented depending on 
the domain task; for example, if we are interested in 
learning paraphrases that are out-of-vocabulary or do-
main-specific, D could consult a dictionary, and return a 
high score if the lexical entry could not be found.  
The DIMPLE framework is implemented in the 
following way4. Let Q be the ratio of positive labels 
                                                 
4
 Implementation used for this experiment is available at 
http://code.google.com/p/dimple/ 
averaged over pairs by human assessors given pi as to 
whether a paraphrase has the same meaning as the 
source term or not. Let D be the degree of lexical di-
versity of a pattern calculated using Algorithm 1 below.  
Algorithm 1. D score calculation 
Input: paraphrases {w1, ?, wk} for a source term s 
1: Set history1 = extractContentWords(s) 
2: Set history2 = stemWords(history1) 
3: for i=1 to k do 
4:     Set W1 = extractContentWords(wi) 
5:     Set W2 = stemWords(W1) // Porter stemming 
6:     if W1==? || W1 ? history1 != ? 
7:         D[i] = 1 // word already seen 
8:     else 
9:         if W2 ? history2 != ? 
10:             D[i] = 2 // root already seen 
11:         else 
12:             D[i] = 3 // unseen word 
13:         end if 
14:         history1 = W1 ? history1 
15:         history2 = W2 ? history2 
16:     end if 
17: end for 
3 Experiment 
We use the Pearson product-moment correlation coeffi-
cient to measure correlation between two vectors con-
sisting of intrinsic and extrinsic scores on paraphrase 
patterns, following previous meta-evaluation research 
(Callison-Burch et al, 2007; Callison-Burch et al, 2008; 
Tratz and Hovy, 2009; Przybocki et al, 2009). By in-
trinsic score, we mean a theory-based direct assessment 
result on the paraphrase patterns. By extrinsic score, we 
mean to measure how much the paraphrase recognition 
component helps the entire system to achieve a task. The 
correlation score is 1 if there is a perfect positive corre-
lation, 0 if there is no correlation and -1 if there is a per-
fect negative correlation.  
Using a task performance score to evaluate a pa-
raphrase generation algorithm has been studied pre-
viously (Bhagat and Ravichandran, 2008; Szpektor 
and Dagan, 2007; Szpektor and Dagan, 2008). A 
common issue in extrinsic evaluations is that it is hard 
to separate out errors, or contributions from other 
possibly complex modules. This paper presents an 
approach which can predict task performance in more 
simple experimental settings. 
3.1 Annotated Paraphrase Resource 
We used the paraphrase pattern dataset ?paraph-
rase-eval? (Metzler et al, 2011; Metzler and Hovy, 
2011) which contains paraphrase patterns acquired by 
multiple algorithms: 1) PD (Pasca and Dienes, 2005), 
36
which is based on the left and right n-gram contexts of 
the source term, with scoring based on overlap; 2) BR 
(Bhagat and Ravichandran, 2008), based on Noun 
Phrase chunks as contexts; 3) BCB (Bannard and 
Callison-Burch, 2005) and 4) BCB-S (Callison-Burch, 
2008), which are based on monolingual phrase 
alignment from a bilingual corpus using a pivot. In the 
dataset, each paraphrase pair is assigned with an an-
notation as to whether a pair is a correct paraphrase or 
not by 2 or 3 human annotators. 
The source terms are 100 verbs extracted from 
newswire about terrorism and American football. We 
selected 10 verbs according to their frequency in ex-
trinsic task datasets (details follow in Section 3.3). 
Following the methodology used in previous pa-
raphrase evaluations (Bannard and Callison-Burch, 
2005; Callison-Burch, 2008; Kok and Brockett, 2010), 
the labels were annotated on a pair of two sentences: an 
original sentence containing the source term, and the 
same sentence with the source term replaced with the 
paraphrase pattern, so that contextual information 
could help annotators to make consistent judgments. 
The judgment is based on whether the ?same meaning? 
is present between the source term and its paraphrase. 
There is a lenient and a strict distinction on the ?same 
meaning? judgments. The strict label is given when the 
replaced sentence is grammatically correct whereas the 
lenient label is given even when the sentence is gram-
matically incorrect. 
In total, we have 10 (source terms listed in Table 1) 
?  4 (paraphrase generation algorithms introduced 
above) = 40 sets of paraphrase patterns. In each set of 
paraphrase patterns, there are up to 10 unique ?source 
term, paraphrase? pairs. 
3.2 Intrinsic Paraphrase Metrics 
We will discuss the common metric EP, and its variant 
EPR as baselines to be compared with DIMPLE. For 
each metric, we used a cutoff value of k=1, 5 and 10. 
EP: Our baseline is the Expected Precision at k, which is 
the expected number of correct paraphrases among the 
top k returned, and is computed as:
 
?
=
=
k
i ik
Q
k 1
1EP where 
Q is the ratio of positive labels. For instance, if 2 out of 3 
human annotators judged that pi = ??killed?, ?fatally 
shot?? has the same meaning, Qi = 2/3. 
EPR: Metzler et al, (2011) extended EP with a Re-
dundancy judgment, which we shall call EPR where 
lexically redundant paraphrases did not receive a cre-
dit. Unlike Metzler et al, (2011) where humans 
judged redundancies, we do the judgment automati-
cally with a Porter Stemmer (Porter, 1980) to extract 
and compare stemmed forms. In that way EPR?s 
output become comparable to DIMPLE?s, remaining 
redundancy scoring different (i.e. binary filtering in 
EPR and 3-level weighting in DIMPLE). 
3.3 Extrinsic Evaluation Datasets  
Ideally, paraphrase metric scores should correlate well 
with task performance metrics. To insulate the expe-
riment from external, uncontrollable factors (e.g. er-
rors from other task components), we created three 
datasets with slightly different characteristics, where 
the essential task of recognizing meaning equivalence 
between different surface texts can be conducted. 
The numbers of positive-labeled pairs that we ex-
tracted for the three corpus, MSRPC, RTE and CQAE 
are 3900, 2805 and 27397 respectively. Table 1 shows 
the number of text pairs selected in which at least one 
of each pair contains a frequently occurring verb.  
 Src verb MSRPC RTE CQAE 
found 89 62 319 
called 59 61 379 
told 125 34 189 
killed 48 109 277 
accused 30 44 143 
to take 21 23 63 
reached 22 18 107 
returned 14 20 57 
turned 22 10 94 
broke 10 10 35 
Table 1. 10 most frequently occurring source verbs 
in three datasets. Numbers are positive-labeled pairs 
where the verb appears in at least one side of a pair.  
MSRPC: The Microsoft Research Paraphrase Corpus 
(Dollan et al, 2005) contains 5800 pairs of sentences 
along with human annotations where positive labels 
mean semantic equivalence of pairs.  
RTE: (Quasi-)paraphrase patterns are useful for the 
closely related task, Recognizing Textual Entailment. 
This dataset has been taken from the 2-way/3-way track 
at PASCAL/TAC RTE1-4. Positive examples are pre-
mise-hypothesis pairs where human annotators assigned 
the entailment label. The original dataset has been gen-
erated from actual applications such as Text Summari-
zation, Information Extraction, IR, Question Answering. 
CQAE: Complex Question Answering Evaluation 
(CQAE) dataset has been built from 6 past TREC QA 
tracks, i.e., ?Other? QA data from TREC 2005 through 
2007,  relation QA data from TREC 2005 and ciQA 
from TREC 2006 and 2007 (Voorhees and Dang, 2005; 
Dang et al, 2006; Dang et al, 2007). We created unique 
pairs consisting of a system response (often sen-
37
tence-length) and an answer nugget as positive examples, 
where the system response is judged by human as con-
taining or expressing the meaning of the nugget.  
3.4 Extrinsic Performance Metric 
Using the dataset described in Section 3.3, perfor-
mance measures for each of the 40 paraphrase sets (10 
verbs times 4 generators) are calculated as the ratio of 
pairs correctly identified as paraphrases.  
In order to make the experimental settings close to an 
actual system with an embedded paraphrase engine, we 
first apply simple unigram matching with stemming 
enabled. At this stage, a text with the source verb ?killed? 
and another text with the inflectional variant ?killing? 
would match. As an alternative approach, we consult the 
paraphrase pattern set trying to find a match between the 
texts. This identification judgment is automated, where 
we assume a meaning equivalence is identified between 
texts when the source verb matches5 one text and one of 
up to 10 paraphrases in the set matches the other. Given 
these evaluation settings, a noisy paraphrase pair such as 
??killed?, ?to?? can easily match many pairs and falsely 
boost the performance score. We filter such exceptional 
cases when the paraphrase text contains only functional 
words.  
3.5 Results 
We conducted experiments to provide evidence that 
the Pearson correlation coefficient of DIMPLE is 
higher than that of the other two baselines. Table 2 
and 3 below present the result where each number is 
the correlation calculated on the 40 data points.  
  EPk EPRk DIMPLEk 
 
k=1 5 10 1 5 10 1 5 10 
MSRPC -0.02 -0.24 -0.11 0.33 0.27 -0.12 0.32 0.20 0.25 
RTE 0.13 -0.05 0.11 0.33 0.12 0.09 0.46 0.25 0.37 
CQAE 0.08 -0.09 0.00 -0.02 -0.08 -0.13 0.35 0.25 0.40 
Table 2. Correlation between intrinsic paraphrase 
metrics and extrinsic paraphrase recognition task me-
trics where DIMPLE?s Q score is based on lenient 
judgment. Bold figures indicate statistical significance 
of the correlation statistics (null-hypothesis tested: 
?there is no correlation?, p-value<0.01). 
  EPk EPRk DIMPLEk 
 
k=1 5 10 1 5 10 1 5 10 
MSRPC 0.12 0.13 0.19 0.26 0.36 0.37 0.26 0.35 0.52 
RTE 0.34 0.34 0.29 0.43 0.41 0.38 0.49 0.55 0.58 
CQAE 0.44 0.51 0.47 0.37 0.60 0.55 0.37 0.70 0.70 
Table 3. Same as the Table 2, except that the Q 
score is based on strict judgment. 
                                                 
5
 We consider word boundaries when matching texts, e.g. 
?skilled? and ?killed? do not match. 
Table 2 shows that correlations are almost always 
close to 0, indicating that EP does not correlate with 
the extrinsic measures when the Q score is calculated 
in lenient judgment mode. On the other hand, when 
the Q function is based on strict judgments, EP scores 
sometimes show a medium positive correlation with 
the extrinsic task performance, such as on the CQAE 
dataset. 
In both tables, there is a general trend where the 
correlation scores fall in the same relative order (given 
the same cut-off value): EP < EPR < DIMPLE. This 
suggests that DIMPLE has a higher correlation than the 
other two baselines, given the task performance meas-
ure we experimented with. As we can see from Table 2, 
DIMPLE correlates well with paraphrase task perfor-
mance, especially when the cutoff value k is 5 or 10. 
The higher values in Table 3 (compared to Table 2) 
show that the strict judgment used for intrinsic metric 
calculation is preferable over the lenient one. 
4 Conclusion and Future Works 
We proposed a novel paraphrase evaluation metric 
called DIMPLE, which gives weight to lexical variety. 
We built large scale datasets from three sources and 
conducted extrinsic evaluations where paraphrase 
recognition is involved. Experimental results showed 
that Pearson correlation statistics for DIMPLE are 
approximately 0.5 to 0.7 (when k=10 and ?strict? 
annotations are used to calculate the score), which is 
higher than scores for the commonly used EP and 
EPR metrics.  
Future works include applying DIMPLE on pat-
terns for other tasks where lexical diversity matters 
(e.g. Relation Extraction) with a customized Q and D 
functions. If Q function can be also calculated fully 
automatically, DIMPLE may be useful for learning 
lexically diverse pattern learning when it is incorpo-
rated into optimization criteria.  
Acknowledgments 
We gratefully acknowledges the support of Defense 
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research 
Laboratory (AFRL) prime contract no. 
FA8750-09-C-0172. Any opinions, findings, and 
conclusion or recommendations expressed in this 
material are those of the author(s) and do not neces-
sarily reflect the view of the DARPA, AFRL, or the 
US government. We also thank Donald Metzler et al 
for sharing their data, and Eric Nyberg and anonym-
ous reviewers for their helpful comments. 
38
References 
Bannard, Colin and Chris Callison-Burch. 2005. Pa-
raphrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL 2005. 
Bhagat, Rahul, Patrick Pantel, Eduard Hovy, and Marina 
Rey. 2007. LEDIR: An Unsupervised Algorithm for 
Learning Directionality of Inference Rules. In Pro-
ceedings of EMNLP-CoNLL 2007.  
Bhagat, Rahul and Deepak Ravichandran. 2008. Large 
Scale Acquisition of Paraphrases for Learning Sur-
face Patterns. In Proceedings of ACL-08: HLT. 
Callison-Burch, Chris. 2008. Syntactic Constraints on 
Paraphrases Extracted from Parallel Corpora. In 
Proceedings of EMNLP 2008. 
Callison-Burch, Chris, Cameron Fordyce, Philipp Koehn, 
Christof Monz, and Josh Schroeder. 2007. (Meta-) 
Evaluation of Machine Translation. In Proceedings of 
the Second Workshop on Statistical Machine Trans-
lation - StatMT ?07.  
Callison-Burch, Chris, Cameron Fordyce, Philipp Koehn, 
Christof Monz, and Josh Schroeder. 2008. Further 
Meta-Evaluation of Machine Translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation - StatMT ?08.   
Dang, Hoa Trang, Jimmy Lin, and Diane Kelly. 2006. 
Overview of the TREC 2006 Question Answering 
Track. In Proceedings of TREC 2006. 
Dang, Hoa Trang, Diane Kelly, and Jimmy Lin. 2007. 
Overview of the TREC 2007 Question Answering 
Track. In Proceedings of TREC 2007. 
Dolan, William B., and Chris Brockett. 2005. Automat-
ically Constructing a Corpus of Sentential Paraph-
rases. In Proceedings of the Third International 
Workshop on Paraphrasing (IWP2005). 
J?rvelin, Kalervo, Jaana Kek?l?inen. 2002. Cumulated 
Gain-based Evaluation of IR Techniques. ACM 
Trans. Inf. Syst., Vol. 20, No. 4. (October 2002), pp. 
422-446. 
Kauchak, David, and Regina Barzilay. 2006. Paraph-
rasing for Automatic Evaluation. In Proceedings of 
HLT-NAACL 2006.  
Kek?l?inen, Jaana. 2005. Binary and Graded Relevance 
in IR Evaluations ? Comparison of the Effects on 
Ranking of IR Systems. Information Processing & 
Management, 41, 1019-1033. 
Kok, Stanley and Chris Brockett. 2010. Hitting the Right 
Paraphrases in Good Time. In Proceedings of 
HLT-NAACL 2010. 
Lin, Dekang, and Patrick Pantel. 2001. DIRT - Discovery 
of Inference Rules from Text. In Proceedings of the 
seventh ACM SIGKDD international conference on 
Knowledge discovery and data mining - KDD ?01 
323-328.  
Metzler, Donald, Eduard Hovy, and Chunliang Zhang. 
2011. An Empirical Evaluation of Data-Driven Pa-
raphrase Generation Techniques. In Proceedings of 
ACL-HLT 2011.  
Metzler, Donald and Eduard Hovy. 2011. Mavuno: A 
Scalable and Effective Hadoop-Based Paraphrase 
Harvesting System. To appear in Proceedings of the 
KDD Workshop on Large-scale Data Mining: Theory 
and Applications (LDMTA 2011). 
Miller, Geroge A. 1995. Wordnet: A Lexical Database 
for English. CACM, 38(11):39-41.  
Pad?, Sebastian, Michel Galley, Dan Jurafsky, and 
Christopher D. Manning. 2009. Robust Machine 
Translation Evaluation with Entailment Features. In 
Proceedings of  ACL-IJCNLP ?09. 
Pasca, Marius and Pter Dienes. 2005. Aligning Needles 
in a Haystack: Paraphrase Acquisition Across the 
Web. In Processing of IJCNLP 2005. 
Porter, Martin F. 1980. An Algorithm for Suffix Strip-
ping, Program, 14(3): 130?137. 
Przybocki, Mark, Kay Peterson, S?bastien Bronsart, and 
Gregory Sanders. 2009. The NIST 2008 Metrics for 
Machine Translation Challenge?Overview, Me-
thodology, Metrics, and Results. Machine Translation, 
Volume 23 Issue 2-3. 
Riezler, Stefan, Alexander Vasserman, Ioannis Tso-
chantaridis, Vibhu Mittal, and Yi Liu. 2007. Statis-
tical Machine Translation for Query Expansion in 
Answer Retrieval. In Proceedings of ACL 2007. 
Szpektor, Idan and Ido Dagan. 2007. Learning Canonical 
Forms of Entailment Rules. In Proceedings of 
RANLP 2007. 
Szpektor, Idan, Eyal Shnarch and Ido Dagan. 2007. In-
stance-based Evaluation of Entailment Rule Acqui-
sition. In Proceedings of ACL 2007. 
Szpektor, Idan and Ido Dagan. 2008. Learning Entail-
ment Rules for Unary Templates. In Proceedings of 
COLING 2008. 
Tratz, Stephen and Eduard Hovy. 2009. BEwT-E for 
TAC 2009's AESOP Task. In Proceedings of TAC-09. 
Gaithersburg, Maryland. 
Voorhees, Ellen M., and Hoa Trang Dang. 2005. Over-
view of the TREC 2005 Question Answering Track. 
In Proceedings of TREC 2005. 
Zhou, Liang, Chin-Yew Lin, and Eduard Hovy. 2006. 
Re-evaluating Machine Translation Results with Pa-
raphrase Support. In Proceedings of EMNLP 2006. 
 
39
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Events are Not Simple: Identity, Non-Identity, and Quasi-Identity   Eduard Hovy Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA hovy@cmu.edu  
Teruko Mitamura Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA teruko@cs.cmu.edu  
Felisa Verdejo E.T.S.I. Inform?tica, UNED C/ Juan del Rosal, 16 (Ciudad Universitaria) 28040 Madrid, Spain felisa@lsi.uned.es                  Jun Araki Andrew Philpot                   Language Technology Institute Information Sciences Institute                   Carnegie Mellon University University of Southern California                 5000 Forbes Avenue                  Pittsburgh, PA 15213, USA 4676 Admiralty Way Marina del Rey, CA 90292, USA        junaraki@cs.cmu.edu philpot@isi.edu  Abstract1 Despite considerable theoretical and computa-tional work on coreference, deciding when two entities or events are identical is very difficult.  In a project to build corpora containing corefer-ence links between events, we have identified three levels of event identity (full, partial, and none). Event coreference annotation on two cor-pora was performed to validate the findings.    1 The Problem of Identity Last year we had HLT in Montreal, and this year we did it in Atlanta.   Does the ?did it? refer to the same conference or a different one?  The two conferences are not iden-tical, of course, but they are also not totally unre-lated?else the ?did it? would not be interpretable.   When creating text, we treat instances of entities and events as if they are fixed, well-described, and well-understood.  When we say ?that boat over there? or ?Mary?s wedding next month?, we as-sume the reader creates a mental representation of the referent, and we proceed to refer to it without further thought.   However, as has been often noted in theoretical studies of semantics, this assumption is very prob-lematic (Mill, 1872; Frege 1892; Guarino, 1999).  Entities and (even more so) events are complex composite phenomena in the world, and they un-dergo change.                                                              1 This work was supported by grants from DARPA and NSF, as well as by funding that supported Prof. M. Felisa Vedejo from UNED Madrid. 
Since nobody has complete knowledge, the au-thor?s mental image of the entity or event in ques-tion might differ from the reader?s, and from the truth.  Specifically, the properties the author as-sumes for the event or entity might not be the ones the reader assumes. This difference has deep con-sequences for the treatment of the semantic mean-ing of a text.  In particular, it fundamentally affects how one must perform coreference among entities or events.   As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al, 1997).  Determining when two event mentions in text corefer is, however, an un-solved problem2.  Past work in NLP has avoided some of the more complex problems by consider-ing only certain types of coreference, or by simply ignoring the major problems.  The results have been partial, or inconsistent, annotations.   In this paper we describe our approach to the problem of coreference among events.  In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary to create a model of event identi-ty that is more elaborate than is usually assumed in the NLP literature, and to formulate quite specific definitions for its central concepts.                                                               2 In this work, we mean both events and states when we say ?event?.  A state refers to a fixed, or regularly changing, con-figuration of entities in the world, such as ?it is hot? or ?he is running?.  An event occurs when there is a change of state in the world, such as ?he stops running? or ?the plane took off?. 
21
 Event coreference is the problem of determin-ing when two mentions in a text refer to the ?same? event. Whether or not the event actually occurred in reality is a separate issue; a text can describe people flying around on dragons or broomsticks.  While the events might be actual occurrences, hy-pothesized or desired ones, etc., they exist in the text as Discourse Elements (DEs), and this is what we consider in this work. Each DE is referred to (explicitly or implicitly) in the text by a mention, for example ?destroy?, ?the attack?, ?that event?, or ?it?. But it is often unclear whether two mentions refer to the same DE or to closely related ones, or to something alto-gether different. The following example illustrates two principal problems of event coreference:  While Turkish troops have been fighting_E.1 a Kurdish faction in northern Iraq, two other Kurdish groups have been battling_E.2 each other. A radio station operated_E.3 by the Kurdistan Democratic Party said_E.4 the party's forces attacked_E.5 positions of the Patriotic Union of Kurdistan on Monday in the Kurdish re-gion's capital Irbil. The Voice of Iraqi Kurdistan radio, moni-tored_E.6 by the British Broadcasting Corp., said_E.7 more than 80 Patriotic Union fight-ers were killed_E.8 and at least 150 wound-ed_E.9. The fighting_E.10 was also reported_E.11 by a senior Patriotic Union official, Kusret Rasul Ali, who said_E.12 PUK forces re-pelled_E.13 a large KDP attack_E.14. ? Ali claimed_E.16 that 300 KDP fighters were killed_E.17 or wounded_E.18 and only 11 Patriotic Union members died_E.19. Problem 1: Partial event overlap.  Event E.2, ?battling each other?, refers to an ongoing series of skirmishes between two Kurdish groups, the KDP and the PUK.  Since one of these battles, where the KDP attacked positions of the PUK, is E.5, it is natural to say that E.2 and E.5 corefer.  However, E.2 clearly denotes other battles as well, and there-fore E.5 and E.2 cannot fully corefer.  In another example, event E.8 refers to the killing of a num-ber of soldiers as part of this fight E.5, and event E.9 to the wounding of others.  Both events E.8 
and E.9 constitute an intrinsic part of the attack E.5, and hence corefer to it, but are each only part of E.5, and hence neither can fully corefer to it.   Problem 2: Inconsistent reporting.  This news fragment contains two reports of the fight: E.5 and E.10.  Since E.10 describes E.5 from the perspec-tive of a senior PUK official, it should corefer to E.5.  But where the KDP?s report claims more than 80 PUK fighters killed (event E.8, part of E.5), the PUK official said that only 11 PUK members died (event E.19, part of E.10).  Without taking into account the fact that the two killing events are re-ports made by different speakers, it would not be possible to recognize them as coreferent.   Examples of partial event overlap and incon-sistent reporting are common in text, and occur as various types.  In our work, we formally recognize partial event overlap, calling it partial event identi-ty, which permits different degrees and types of event coreference.  This approach simplifies the coreference problem and highlights various inter-event relationships that facilitates grouping events into ?families? that support further analysis and combination with other NLP system components.   In this paper, we introduce the idea that there are three degrees of event identity: fully identical, qua-si-identical, and fully independent (not identical).  Full identity reflects in full coreference and quasi-identity in partial coreference.  Fully independent events are singletons.  Our claims in this paper are:  ? Events, being complex phenomena, can corefer fully (identity) or partially (quasi-identity).  ? Event coreference annotation is considera-bly clarified when partial coreference is allowed.  ? A relatively small fixed set of types of quasi-identity suffices to describe most of them.  ? Different domains and genres highlight different subsets of these quasi-identity types.   ? Different auxiliary knowledge sources and texts are relevant for different types. 2 Types of Full and Partial Identity Def: Two mentions fully corefer if their activi-ty/event/state DE is identical in all respects, as far as one can tell from their occurrence in the text.  (In particular, their agents, location, and time are identical or compatible.)  One can distinguish sev-eral types of identity, as spelled out below.  
22
Def: Two mentions partially corefer if activi-ty/event/state DE is quasi-identical: most aspects are the same, but some additional information is provided for one or the other that is not shared. There are two principal types of quasi-identity, as defined below.  Otherwise, two mentions do not corefer.  2.1 Full Identity  Mention1 is identical to mention2 iff there is no semantic (meaning) difference between them. Just one DE, and exactly the same aspects of the DE, are understood from both mentions in their con-texts. It is possible to replace the one mention with the other without any semantic change (though some small syntactic changes might be required to ensure grammaticality). Note that mention2 may contain less detail than mention1 and remain iden-tical, if it carries over information from mention1 that is understood / inherited from the context.  However, when mention2 provides more or new information not contained in mention1 or naturally inferred for it, then the two are no longer identical. Usually, exact identity is rare within a single text, but may occur more often across texts.  We identi-fy the following types:  1. Lexical identity: The two mentions use exactly the same senses of the same word(s), in-cluding derivational words (e.g., ?destroy?, ?de-struction?). 2. Synonym: One mention?s word is a syno-nym of the other?s word.  3. Wide-reading: One mention is a synonym of the wide reading of the other (defined below, under Quasi-identity:Scriptal).  For example, in ?the attack(E1) took place yesterday.  The bomb-ing(E2) killed four people?, E1 and E2 are fully coreferent only when ?bombing? is read in its wide sense that denotes the whole attack, not the narrow sense that denotes just the actual exploding of the bomb.   4. Paraphrase: One mention is a paraphrase of the other.  Here some syntactic differences may occur.  Some examples are active/passive trans-formation (?she gave him the book? / ?he was giv-en the book by her?), shifts of perspective that do not add or lose information (?he went to Boston? / ?he came to Boston?), etc.  No extra semantic in-formation is provided in one mention or the other.    
5. Pronoun: One mention refers deictically to the DE, as in (?the party? / ?that event?), (?the election [went well]? / ?it [went well]?).   2.2  Quasi-identity  Mention1 is quasi- (partially) identical to mention2 iff they refer to the ?same? DE but one mention includes information that is not contained in the other, not counting information understood/inhe-rited from the context.  They are semantically not fully identical, though the core part of the two mentions is.  One mention can replace the other, but some information will be changed, added, or lost.  (This is the typical case between possible coreferent mentions within a document.)   We distinguish between two core types of partial identity: Membership and Subevent.  The essential difference between the two is which aspects of the two events in question differ.  Member-of obtains when we have two instances of the same event that differ in some particulars, such as time and loca-tion and [some] participants (agents, patients, etc).  In contrast, Subevent obtains when we have differ-ent events that occur at more or less the same place and time with the same cast of participants.   Membership: Mention1 is a set of similar DEs (multiple instances of the same kind of event), like several birthday parties, and mention2 is one or more of them.  More precisely, we say that an event B is a member of A if: (i) A is a set of mul-tiple instances of the same type of event (and hence its mention usually pluralized); (ii) B?s DE(s) is one or more (but not all) of them; (iii) ei-ther or both the time and the place of B?s DE(s) and (some of) A?s DEs are different.  For example, in ?I attended three parties(E1) last month.  The first one(E2) was the best?, E2 is a member of E1.  The relation that links the single instance to the set is member-of.  Subevent: The DE of mention1 is a script (a ste-reotypical sequence of events, performed by an agent in pursuit of a given goal, such as eating at a restaurant, executing a bombing, running for elec-tion), and mention2 is one of the actions/events executed as part of that script (say, paying the waiter, or detonating the bomb, or making a cam-paign speech).  More precisely, we say that an event B is a subevent of an event A if: (i) A is a complex sequence of activities, mostly performed by the same (or compatible) agent; (ii) B is one of 
23
these activities; and (iii) B occurs at the same time and place as A.  Here A acts as a kind of collector event.  Often, the whole script is named by the key event of the script (for example, in ?he planned the explosion?, the ?explosion? signifies the whole script, including planning, planting the bomb, the detonation, etc.; but the actual detonation event itself can also be called ?the explosion?).  We call the interpretation of the mention that refers to the whole script its wide reading, and the interpreta-tion that refers to just the key subevent the narrow reading.  It is important not to confuse the two; a wide reading and a narrow reading of a word can-not corefer3. The relation that links the narrow reading DE to the wide one is sub-to.   Several aspects of the events in question provide key information to differentiate between members and subevents:   1. Time: When the time of occurrence of mention1 is temporally ?close enough? to the time of occurrence of mention2, then it is likely that one is a Subevent of the other.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the time of occurrence of B is contained in the time of oc-currence of A. But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a mem-ber of A.  (In (Humphreys et al, 1997), any varia-tion in time automatically results in a decision of non-coreference.)   2. Space/location: The location of mention1 is spatially ?close enough? to the location of men-tion2.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the location of oc-currence of B is contained in, or overlaps with, or abuts the location of occurrence of A.  But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a member of A. 
                                                            3 For example, in ?James perpetrated the shooting. He was arrested for the attack?, ?shooting? is used in its wide sense and here is coreferent with ?attack?, since it applies to a whole sequence of events.  In contrast, ?James perpetrated the shoot-ing.  He is the one who actually pulled the trigger?, ?shooting? is used in its narrow sense to mean just the single act.  Typi-cally, a word with two readings can corefer (i.e., be lexically or synonymically identical to) another in the same reading only. 
3.  Event participants: Mention1 and men-tion2 refer to the same DE but differ in the overall cast of participants involved.  In these cases, the member relation obtains, and can be differentiated into subtypes, since participants of events can dif-fer in several ways.  For example, if: (i) the men-tions of events A and B refer to the same overall DE; and (ii) the participants (agents, patients, etc.) of mention2 are a subset of the participants of mention1, as in ?the crowd demonstrated on the square. Susan and Mary were in it?, then event B is a participant-member of event A.  In another ex-ample, event B is a participant-instance-member of event A if: (i) the mentions of events A and B refer to the same overall DE; and (ii) one or more of the participants (agents, patients, etc.) of men-tion2 is/are an instance of the participants of men-tion1, as in ?a firebrand addressed the crowd on the square. Joe spoke for an hour?, where Joe is the firebrand.  There are other ways in which two mentions may refer to the same DE but differ from one an-other.  Usually these differences are not semantic but reflect an orientation or perspective difference.  For example, one mention may include the speak-er?s evaluation/opinion, while the other is neutral, as in ?He sang the silly song.  He embarrassed himself?, or the spatial orientation of the speaker, as in ?she went to New York? / ?she came to New York?.  We treat these cases as fully coreferent.   Sometimes it is very difficult to know whether two mentions are bidirectionally implied, meaning that the two must corefer, or whether they are only quasi-identical (i.e., one entails the other but not vice versa).  For example, in ?he had a heart at-tack? / ?he died?, the two mentions are not identi-cal because one can have a heart attack and not die from it. In contrast, ?he had a fatal heart attack? / ?he died from a heart attack? are identical.  In ?she was elected President? / ?she took office as Presi-dent?, it is more difficult to decide. Does being elected automatically entail taking office?  In some political systems it may, and in others it may not.  When in doubt, we treat the case as only quasi-identical.  Thus, comparing to examples from Full-Identity: Paraphrase, the following are only quasi-identical because of additional information: ?she sold the book? / ?she sold Peter the book?; ?she sold Peter the book? / ?Peter got [not bought] the book from her?. 
24
Quasi-identity has been considered in corefer-ence before in (Hasler et al, 2006) but not as ex-tensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities.  When applied to events, the issue becomes more complex.  3 Two Problems  3.1 Domain and Reporting Events  As described above, inconsistent reporting occurs when a DE stated in reported text contains signifi-cant differences from the author?s description of the same DE.   To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation be-cause they provide a context in which a DE is stat-ed.  We identify two principal types of Reporting verbs: locutionary verbs ?say?, ?report?, ?an-nounce?, etc.) and Speech Acts (?condemn?, ?promise?, ?support?, ?blame?, etc.).  Where the former verbs signal merely a telling, the latter verbs both say and thereby do something.  For ex-ample in the following paragraph, ?admitted? and ?say? are communication events:  Memon admitted_R.7,in-sayR.3 his in-volvement in activities_E.8,in-sayR.3 in-volving an explosives-laden van near the president's motorcade, police said_R.3?.  Sometimes the same event can participate in-side two reporting events, as in   ?The LA Times lauded_R.1 the decision_E.2,in-sayR.1,in-sayR.3, which the NY Times lampooned_R.3. Though an added annotation burden, the link from a DE to a reporting event allows the analyst or learning system to discount apparent contradictory aspects of the DE and make more accurate identity decisions.     3.2 Unclear Semantics of Events  Sometimes it is difficult to determine the exact relationships between events since their semantics is unclear.  In the following, is E.45 coreferent to E.44, or only partially?  If so, how?  Amnesty International has accused both sides of violating_E.44 international humanitarian law by targeting_E.45 civilian areas, and ... 
We decided that E.44 is not fully coreferent with E.45, since violating is not the same as targeting.  Also, E.45 is not a subevent of E.44 since ?violat-ing? is not a script with a well-defined series of steps, does not trigger ?targeting?, and does not occur before ?targeting?.  Rather, targeting is a certain form or example of violation/violating. (It might be easier if the sentence were: ?... of violat-ing international humanitarian law by targeting civilian areas and the human rights group, by kill-ing civilians, and by....?.  As such E.45 could be interpreted as a member of E.44, interpreting the latter as a series of violations.)   4 Annotation  To validate these ideas we have been annotating newspaper texts within the context of a large pro-ject on automated deep reading of text. This pro-ject combines Information Extraction, parsing, and various forms of inference to analyze a small num-ber of texts and to then answer questions about them.  The inability of current text analysis engines to handle event coreference has been a stumbling block in the project.  By creating a corpus of texts annotated for coreference we are working to enable machine learning systems to learn which features are relevant for coreference and then ultimately to perform such coreference as well.  We are annotating two corpora: 1. The Intelligence Community (IC) Corpus contains texts in the Violent Events domain (bombings, killings, wars, etc.).  Given the relative scarcity of the partial coreference subtypes, we annotated only instances of full coreference, Subevent, and Member relations.  To handle Subevents one needs an unambiguous definition of the scripts in the domain.  Fortunately this domain offers a manageable set of events (our event ontol-ogy comprises approximately 50 terms) with a subevent structure that is not overly complex but still realistic.  We did not find the need to exceed three layers of scriptal granularity, as in  campaign > {bombing, attack} > {blast, kill, wound}.  2. The Biography (Bio) Corpus contains texts describing the lives of famous people. Typically, these texts are written when the person dies or has some notable achievement.  Given the complexi-ties of description of artistic and other creative achievements, we restrict our corpus to achieve-
25
ments in politics, science, sports, and other more factual endeavors.  More important than scriptal granularity in this domain is temporal sequencing.  We obtained and modified a version of the An-CoraPipe entity coreference annotation interface (Bertran et al, 2010) that was kindly given us by the AnCora team at the University of Barcelona.  We implemented criteria and an automated method for automatically identifying domain and reporting events.  We also created a tool to check and dis-play the results of annotation, and technology to deliver various agreement scores.  Using different sets of annotators (from 3 to 6 people per text), we have completed a corpus of 100 texts in the IC domain and are in process of annotating the Bio corpus. Our various types of full and partial coreference and the associated an-notation guidelines were developed and refined over the first third of these documents.   Table 1 shows statistics and inter-annotator agreement for the remaining 65 articles.  The aver-age number of domain and reporting events per article is 41.2.  We use Fleiss?s kappa since we have more than two annotators per article.  The (rather low) score for member coreference is not really reliable given the small number of instances.     Avg no per article Agreement (Fleiss?s kappa) Full coreference relations Member coreference relations Subevent coreference relations 19.5 0.620 2.7 0.213 7.2 0.467 Table 1: Annotation statistics and agreement. 5 Validation and Use To validate the conceptualization and definitions of full and partial identity relations, we report in (Araki et al, 2013) a study that determines correla-tions between the Member and Subevent relation instances and a variety of syntactic and lexico-semantic features.  The utility of these features to support automated event coreference is reported in the same paper.   We are now developing a flexible recursive pro-cedure that integrates coreference of events and of their pertinent participants (including locations and times).  This procedure employs inference in addi-tion to feature-based classification to compensate for the shortcomings of each method alone.   
6 Relevant Past Work The problem of identity has been addressed by scholars since antiquity.  In the intensional ap-proach (for example, De Saussure, 1896) a concept is defined as a set of attributes (differentiae), that serve to distinguish it from other concepts; two concepts are identical iff all their attributes and values are.  In the extensional approach (Frege, 1982) a concept can be defined as the set of all in-stances of that concept; two concepts are identical when their two extensional sets are.    Given the impossibility of either approach to support practical work, AI scholars have devoted some attention to so-called Identity Criteria.  Gua-rino (1999) outlines several ?dimensions? along which entities can remain identical or change un-der transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership.   There has not been much theoretical work on semantic identity in the NLP community.  But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Cu-lotta et al, 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009).  Focusing on event coreference are (Humphries et al, 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010).   Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al, 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Ku?ov? and Haji?ov?. 2004), the ACE corpus (Walker et al, 2006), and OntoNotes (Pra-dhan et al, 2007).  Most similar to our work is that of (Hasler et al, 2006). In that study, coreferential events and their arguments (also coreference between the argu-ments) were annotated for the terrorism/security domain, considering five event categories (attack, defend, injure, die, contact), and five event clusters (Bukavu bombing, Peru hostages, Tajikistan hos-tages, Israel suicide bombing and China-Taiwan 
26
hijacking). They also annotated information about the kind of coreferential link, such as identity / synonymy / generalization / specialization / other.   Our work takes further the ideas of (Hasler et al, 2006) and (Recasens et al, 2011) in elaborating the types of full and partial identity, as they are manifest in event coreference.   7 Conclusion The problem of entity and event identity, and hence coreference, is challenging.  We provide a definition of identity and two principal types of quasi-identity, with differentiation based on differ-ences in location, time, and participants.  We hope that these ideas help to clarify the problem and im-prove inter-annotator agreement. Acknowledgments Our grateful thanks goes to Prof. Antonia Mart? and her team for their extensive work on the modi-fications of the AnCoraPipe annotation interface. References  Araki, J., T, Mitamura, and E.H. Hovy. 2013. Identity and Quasi-Identity Relations for Event Coreference. Unpublished manuscript. Bejan, C.A. and S. Harabagiu. 2008. A Linguistic Re-source for Discovering Event Structures and Resolv-ing Event Coreference. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 08). Bejan, C.A. and S. Harabagiu. 2010. Unsupervised Event Coreference Resolution with Rich Linguistic Features. Proceedings of the 48th conference of the Association for Computational Linguistics (ACL 10). Bertran, M., O. Borrega, M.A. Mart?, and M. Taul?, 2010. AnCoraPipe: A New Tool for Corpora Annota-tion. Working paper 1: TEXT-MESS 2.0 (Text-Knowledge 2.0). Available at http://clic.ub.edu/files/AnCoraPipe_0.pdf  Chen, Z. and H. Ji. 2009. Graph-based Event Corefer-ence Resolution. Proceedings of the ACL-IJCNLP 09 workshop on TextGraphs-4: Graph-based Methods for Natural Language Processing. Culotta, A., M. Wick, and A. McCallum. 2007. First-order probabilistic models for coreference resolution. Proceedings of the HLT/NAACL conference.  
De Saussure, F. 1896. Course in General Linguistics. Open Court Classics. Finkel, J.R. and C.D. Manning. 2008. Enforcing transi-tivity in coreference resolution. Proceedings of the ACL-HLT conference, pp. 45?48.  Florian, R., J F Pitrelli, S Roukos, I Zitouni. 2010. Im-proving Mention Detection Robustness to Noisy In-put.  Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Frege, G. 1892. On Sense and Reference. Reprinted in P. Geach and M. Black (eds.) Translations from the Philosophical Writings of Gottlob Frege. Oxford: Blackwell, 1960. Guarino, N. 1999. The Role of Identity Conditions in Ontology Design. In C. Freksa and D.M. Mark (eds.), Spatial Information Theory: Cognitive and Computa-tional Foundations of Geographic Information Sci-ence. Proceedings of International Conference COSIT '99.  Springer Verlag. Hasler, L., C. Orasan, and K. Naumann. 2006. NPs for Events: Experiments in Coreference Annotation. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06), pp. 1167?1172.  Hasler, L. and C. Orasan. 2009. Do Coreferential Ar-guments make Event Mentions Coreferential? Pro-ceedings of the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 09), pp 151?163. Humphreys, K., R. Gaizauskas and S. Azzam. 1997.  Event Coreference for Information Extraction. Pro-ceedings of the ACL conference Workshop on Opera-tional Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts (ANARESOLU-TION 97).  Ku?ov?, L. and E. Haji?ov?. 2004. Coreferential rela-tions in the Prague Dependency Treebank. Proceed-ings of the DAARC workshop, pp. 97?102. Mani, I. and J. Pustejovsky. 2004. Temporal Discourse Models for Narrative Structure. Proceedings of the ACL 2004 Workshop on Discourse Annotation.  McCarthy, J.F. and W. Lehnert. 1995. Using Decision trees for Coreference Resolution. Proceedings of the IJCAI conference.  Mill, J.S. 1872. A System of Logic, definitive 8th edi-tion. 1949 reprint, London: Longmans, Green and Company. Ng, V. 2007. Shallow Semantics for Coreference Reso-lution. Proceedings of the IJCAI conference. 
27
Ng, V. 2009. Graph-cut-based Anaphoricity Determina- tion for Coreference Resolution. Proceedings of the NAACL-HLT conference, pp. 575?583. Poesio, M. and R. Artstein. 2005. The reliability of ana-phoric annotation, reconsidered: Taking ambiguity into account. Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II. Poesio, M. and R. Artstein. 2008. Anaphoric annotation in the ARRAU corpus. Proceedings of the LREC conference. Pradhan, S., E.H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel 2007. OntoNotes: A Unified Relational Semantic Representation. Interna-tional Journal of Semantic Computing 1(4), pp. 405?420.  Pustejovsky, J., J. Casta?o, R. Ingria, R. Saur?, R. Gai-zauskas, A. Setzer and G. Katz. 2003. TimeML: Ro-bust Specification of Event and Temporal Expressions in Text. Proceedings of IWCS-5, Fifth International Workshop on Computational Seman-tics. Recasens, M. and E.H. Hovy. 2010a. Coreference Reso-lution across Corpora: Languages, Coding Schemes, and Preprocessing Information. Proceedings of the Association of Computational Linguistics conference (ACL 10).  Recasens, M. and E.H. Hovy. 2010b. BLANC: Imple-menting the Rand Index for Coreference Evaluation.  Journal of Natural Language Engineering 16(5).  Recasens, M., E.H. Hovy, and M.A. Mart?. 2011. Identi-ty, Non-identity, and Near-identity: Addressing the Complexity of Coreference.  Lingua.   Taul?, M., M.A. Mart?. and M. Recasens. 2008. An-Cora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the LREC 08 conference, pp. 96?101.  Walker, C., S. Strassel, J. Medero 2006. The ACE 2005 multilingual training corpus.  Linguistic Data Con-sortium, University of Pennsylvania, Philadelphia. 
28
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 68?76,
Baltimore, Maryland, USA, June 22-27, 2014.
c
?2014 Association for Computational Linguistics
Evaluation for Partial Event Coreference
Jun Araki Eduard Hovy Teruko Mitamura
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
junaraki@cs.cmu.edu, hovy@cmu.edu, teruko@cs.cmu.edu
Abstract
This paper proposes an evaluation scheme
to measure the performance of a system
that detects hierarchical event structure for
event coreference resolution. We show
that each system output is represented as
a forest of unordered trees, and introduce
the notion of conceptual event hierarchy to
simplify the evaluation process. We enu-
merate the desiderata for a similarity met-
ric to measure the system performance.
We examine three metrics along with the
desiderata, and show that metrics extended
from MUC and BLANC are more ade-
quate than a metric based on Simple Tree
Matching.
1 Introduction
Event coreference resolution is the task to de-
termine whether two event mentions refer to the
same event. This task is important since resolved
event coreference is useful in various tasks such as
topic detection and tracking, information extrac-
tion, question answering, textual entailment, and
contradiction detection.
A key challenge for event coreference resolu-
tion is that one can define several relations be-
tween two events, where some of them exhibit
subtle deviation from perfect event identity. For
clarification, we refer to perfect event identity
as full (event) coreference in this paper. To ad-
dress the subtlety in event identity, Hovy et al.
(2013) focused on two types of partial event iden-
tity: subevent and membership. Subevent relations
form a stereotypical sequence of events, or a script
(Schank and Abelson, 1977; Chambers and Juraf-
sky, 2008). Membership relations represent in-
stances of an event collection. We refer to both
as partial (event) coreference in this paper. Fig-
ure 1 shows some examples of the subevent and
membership relations in the illustrative text be-
low, taken from the Intelligence Community do-
main of violent events. Unlike full coreference,
partial coreference is a directed relation, and forms
hierarchical event structure, as shown in Figure 1.
Detecting partial coreference itself is an important
task because the resulting event structures are ben-
eficial to text comprehension. In addition, such
structures are also useful as background knowl-
edge information to resolve event coreference.
A car bomb that police said was set by Shining Path
guerrillas ripped off(E4) the front of a Lima police
station before dawn Thursday, wounding(E5) 25 peo-
ple. The attack(E6) marked the return to the spotlight
of the feared Maoist group, recently overshadowed by
a smaller rival band of rebels. The pre-dawn bomb-
ing(E7) destroyed(E8) part of the police station and
a municipal office in Lima?s industrial suburb of Ate-
Vitarte, wounding(E9) 8 police officers, one seriously,
Interior Minister Cesar Saucedo told reporters. The
bomb collapsed(E11) the roof of a neighboring hospi-
tal, injuring(E12) 15, and blew out(E13) windows and
doors in a public market, wounding(E14) two guards.
Figure 1: Examples of subevent and member-
ship relations. Solid and dashed arrows represent
subevent and membership relations respectively,
with the direction from a parent to its subevent
or member. For example, we say that E4 is a
subevent of E6. Solid lines without any arrow
heads represent full coreference.
In this paper, we address the problem of evalu-
68
ating the performance of a system that detects par-
tial coreference in the context of event coreference
resolution. This problem is important because, as
with other tasks, a good evaluation method for par-
tial coreference will facilitate future research on
the task in a consistent and comparable manner.
When one introduces a certain evaluation metric
to such a new complex task as partial event coref-
erence, it is often unclear what metric is suitable
to what evaluation scheme for the task under what
assumptions. It is also obscure how effectively and
readily existing algorithms or tools, if any, can be
used in a practical setting of the evaluation. In or-
der to resolve these sub-problems for partial coref-
erence evaluation, we need to formulate an evalu-
ation scheme that defines assumptions to be made
regarding the evaluation, specifies some desider-
ata that an ideal metric should satisfy for the task,
and examines how adequately particular metrics
can satisfy them. For this purpose, we specifi-
cally investigate three existing algorithms MUC,
BLANC, and Simple Tree Matching (STM).
The contributions of this work are as follows:
? We introduce a conceptual tree hierarchy that
simplifies the evaluation process for partial
event coreference.
? We present a way to extend MUC, BLANC,
and STM for the case of unordered trees.
Those metrics are generic and flexible
enough to be used in evaluations involving
data structures based on unordered trees.
? Our experimental results indicate that the ex-
tended MUC and BLANC are better than
Simple Tree Matching for evaluating partial
coreference.
2 Related Work
Recent studies on both entity and event coref-
erence resolution use several metrics to evaluate
system performance (Bejan and Harabagiu, 2010;
Lee et al., 2012; Durrett et al., 2013; Lassalle and
Denis, 2013) since there is no agreement on a sin-
gle metric. Currently, five metrics are widely used:
MUC (Vilain et al., 1995), B-CUBED (Bagga and
Baldwin, 1998), two CEAF metrics CEAF-?
3
and
CEAF-?
4
(Luo, 2005), and BLANC (Recasens
and Hovy, 2011). We can divide these metrics
into two groups: cluster-based metrics, e.g., B-
CUBED and CEAF, and link-based metrics, e.g.,
MUC and BLANC. The former group is not ap-
plicable to evaluate partial coreference because it
is unclear how to define a cluster. The latter is
not readily applicable to the evaluation because it
is unclear how to penalize incorrect directions of
links. We discuss these aspects in Section 4.1 and
Section 4.2.
Tree Edit Distance (TED) is one of the tradi-
tional algorithms for measuring tree similarity. It
has a long history of theoretical studies (Tai, 1979;
Zhang and Shasha, 1989; Klein, 1998; Bille, 2005;
Demaine et al., 2009; Pawlik and Augsten, 2011).
It is also widely studied in many applications, in-
cluding Natural Language Processing (NLP) tasks
(Mehdad, 2009; Wang and Manning, 2010; Heil-
man and Smith, 2010; Yao et al., 2013). However,
TED has a disadvantage: we need to predefine ap-
propriate costs for basic tree-edit operations. In
addition, an implementation of TED for unordered
trees is fairly complex.
Another tree similarity metric is Simple Tree
Matching (STM) (Yang, 1991). STM measures
the similarity of two trees by counting the max-
imum match with dynamic programming. Al-
though this algorithm was also originally devel-
oped for ordered trees, the underlying idea of the
algorithm is simple, making it relatively easy to
extend the algorithm for unordered trees.
Tree kernels have been also widely studied and
applied to NLP tasks, more specifically, to cap-
ture the similarity between parse trees (Collins and
Duffy, 2001; Moschitti et al., 2008) or between
dependency trees (Croce et al., 2011; Srivastava
et al., 2013). This method is based on a super-
vised learning model with training data; hence we
need a number of pairs of trees and associated nu-
meric similarity values between these trees as in-
put. Thus, it is not appropriate for an evaluation
setting.
3 Evaluation Scheme
When one formulates an evaluation scheme for a
new task, it is important to define assumptions for
the evaluation and desiderata that an ideal metric
should satisfy. In this section, we first describe as-
sumptions for partial coreference evaluation, and
introduce the notion of conceptual event hierarchy
to address the challenge posed by one of the as-
sumptions. We then enumerate the desiderata for
a metric.
69
3.1 Assumptions on Partial Coreference
We make the following three assumptions to eval-
uate partial coreference.
Twinless mentions: Twinless mentions (Stoyanov
et al., 2009) are the mentions that exist in the gold
standard but do not in a system response, or vice
versa. In reality, twinless mentions often happen
since an end-to-end system might produce them in
the process of detecting mentions. The assump-
tion regarding twinless mentions has been inves-
tigated in research on entity coreference resolu-
tion. Cluster-based metrics such as B-CUBED and
CEAF assume that a system is given true men-
tions without any twinless mentions in the gold
standard, and then resolves full coreference on
them. Researchers have made different assump-
tions about this issue. Early work such as (Ji et
al., 2005) and (Bengtson and Roth, 2008) simply
ignored such mentions. Rahman and Ng (2009)
removed twinless mentions that are singletons in a
system response. Cai and Strube (2010) proposed
two variants of B-CUBED and CEAF that can deal
with twinless mentions in order to make the evalu-
ation of end-to-end coreference resolution system
consistent.
In evaluation of partial coreference where twin-
less mentions can also exist, we believe that the
value of making evaluation consistent and compa-
rable is the most important, and hypothesize that
it is possible to effectively create a metric to mea-
sure the performance of partial coreference while
dealing with twinless mentions. A potential prob-
lem of making a single metric handle twinless
mentions is that the metric would not be informa-
tive enough to show whether a system is good at
identifying coreference links but poor at identify-
ing mentions, or vice versa (Recasens and Hovy,
2011). However, our intuition is that the prob-
lem is avoidable by showing the performance of
mention identification with metrics such as pre-
cision, recall, and the F-measure simultaneously
with the performance of link identification. In this
work, therefore, we assume that a metric for par-
tial coreference should be able to handle twinless
mentions.
Intransitivity: As described earlier, partial coref-
erence is a directed relation. We assume that par-
tial coreference is not transitive. To illustrate the
intransitivity, let e
i
s
?? e
j
denote a subevent rela-
tion that e
j
is a subevent of e
i
. In Figure 1, we
have E7
s
?? E8 and E8
s
?? E9. In this case,
E9 is not a subevent of E7 due to the intransi-
tivity of subevent relations. One could argue that
the event ?wounding(E9)? is one of stereotypical
events triggered by the event ?bombing(E7)?, and
thus E7
s
?? E9. However, if we allow transitiv-
ity of partial coreference, then we have to measure
all implicit partial coreference links (e.g., the one
between E7 and E9) from hierarchical event struc-
tures. Consequently, this evaluation policy could
result in an unfair scoring scheme biased toward
large event hierarchy.
Link propagation: We assume that partial coref-
erence links can be propagated due to a combi-
nation of full coreference links with them. To il-
lustrate the phenomenon, let e
i
? e
j
denote full
coreference between e
i
and e
j
. In Figure 1, we
have E6 ? E7 and E7
s
?? E8. In this case, E8
is also a subevent of E6, i.e., E6
s
?? E8. The
rationale behind this assumption is that if a sys-
tem identifies E6
s
?? E8 instead of E7
s
?? E8,
then there is no reason to argue that the identified
subevent relation is incorrect given that E6? E7
and E7
s
?? E8. The discussion here also applies
to membership relations.
3.2 Conceptual Event Hierarchy
The assumption of link propagation poses a chal-
lenge in measuring the performance of partial
coreference. We illustrate the challenge with the
example in the discussion on link propagation
above. We focus only on subevent relations to de-
scribe our idea, but one can apply the same dis-
cussion to membership relations. Suppose that a
system detects a subevent link E7
s
?? E8, but not
E6
s
?? E8. Then, is it reasonable to give the
system a double reward for two links E7
s
?? E8
and E6
s
?? E8 due to link propagation, or should
one require a system to perform such link propa-
gation and detect E7
s
?? E8 as well for the system
to achieve the double reward? In the evaluation
scheme based on event trees whose nodes repre-
sent event mentions, we need to predefine how to
deal with link propagation of full and partial coref-
erence in evaluation. In particular, we must pay at-
tention to the potential risk of overcounting partial
corefrence links due to link propagation.
To address the complexity of link propagation,
we introduce a conceptual event tree where each
node represents a conceptual event rather than an
event mention. Figure 2 shows an example of
a conceptual subevent tree constructed from full
70
coreference and subevent relations in Figure 1.
Using set notation, each node of the tree represents
an abstract event. For instance, node {E6, E7}
represents an ?attacking? event which both event
mentions E6 and E7 refer to.
Figure 2: A conceptual subevent tree constructed
from the full coreference and subevent relations in
Figure 1.
The notion of a conceptual event tree obviates
the need to cope with link propagation, thereby
simplifying the evaluation for partial coreference.
Given a conceptual event tree, an evaluation met-
ric is basically just required to measure how many
links in the tree a system successfully detects.
When comparing two conceptual event trees, a
link in a tree is identical to one in the other tree
if there is at least one event mention shared in par-
ent nodes of those links and at least one shared
in child nodes of those links. For example, sup-
pose that system A identifies E6
s
?? E8, system
B E7
s
?? E8, system C both, and all the systems
identify E6 ? E7 in Figure 1. In this case, they
gain the same score since the subevent links that
they identify correspond to one correct subevent
link {E6, E7}
s
?? {E8} in Figure 2. It is pos-
sible to construct the conceptual event hierarchy
for membership relations in the same way as de-
scribed above. This means that the conceptual
event hierarchy allows us to show the performance
of a system on each type of partial coreference
separately, which leads to more informative evalu-
ation output.
One additional note is that the conceptual event
tree representing partial coreference is an un-
ordered tree, as illustrated in Figure 2. Although
we could represent a subevent tree with an or-
dered tree because of the stereotypical sequence of
subevents given by definition, partial coreference
is in general represented with a forest of unordered
trees
1
.
1
For example, it is impossible to intuitively define a se-
3.3 Desiderata for Metrics
In general, a system output of partial event coref-
erence in a document is represented not by a sin-
gle tree but by a forest, i.e., a set of disjoint trees
whose nodes are event mentions that appear in the
document. Let T be a tree, and let F be a forest
F = {T
i
}. Let sim(F
g
, F
r
) ? [0, 1] denote a sim-
ilarity score between the gold standard forest F
g
and a system response forest F
r
. We define the
following properties that an ideal evaluation met-
ric for partial event coreference should satisfy.
P1. Identity: sim(F
1
, F
1
) = 1.
P2. Symmetricity: sim(F
1
, F
2
) = sim(F
2
, F
1
).
P3. Zero: sim(F
1
, F
2
) = 0 if F
1
and F
2
are to-
tally different forests.
P4. Monotonicity: The metric score should in-
crease from 0 to 1 monotonically as two to-
tally different forests approach the identical
one.
P5. Linearity: The metric score should increase
linearly as each single individual correct
piece of information is added to a system re-
sponse.
The first three properties are relatively intuitive.
P4 is important because otherwise a higher score
by the metric does not necessarily mean higher
quality of partial event coreference output. In P5, a
correct piece of information is the addition of one
correct link or the deletion of one incorrect link.
This property is useful for tracking performance
progress over a certain period of time. If the met-
ric score increases nonlinearly, then it is difficult to
compare performance progress such as a 0.1 gain
last year and a 0.1 gain this year, for example.
In addition, one can think of another property
with respect to structural consistency. The moti-
vation for the property is that one might want to
give more reward to partial coreference links that
form hierarchical structures, since they implicitly
form sibling relations among child nodes. For in-
stance, suppose that system A detects two links
{E6, E7}
s
?? {E8} and {E6, E7}
s
?? {E11}, and
system B two links {E8}
s
?? {E9} and {E11}
s
??
{E12} in Figure 2. We can think that system A
performs better since the system successfully de-
tects an implicit subevent sibling relation between
{E8} and {E11} as well. Due to space limita-
tions, however, we do not explore the property in
this work, and leave it for future work.
quence of child nodes in a membership event tree in Figure 1.
71
4 Evaluation Metrics
In this section, we examine three evaluation met-
rics based on MUC, BLANC, and STM respec-
tively under the evaluation scheme described in
Section 3.
4.1 B-CUBED and CEAF
B-CUBED regards a coreference chain as a set of
mentions, and examines the presence and absence
of mentions in a system response that are relative
to each of their corresponding mentions in the gold
standard (Bagga and Baldwin, 1998). Let us call
such set a mention cluster. A problem in applying
B-CUBED to partial coreference is that it is diffi-
cult to properly form a mention cluster for partial
coreference. In Figure 2, for example, we could
form a gold standard cluster containing all nodes
in the tree. We could then form a system response
cluster, given a certain system output. The prob-
lem is that B-CUBED?s way of counting mentions
overlapped in those clusters cannot capture parent-
child relations between the mentions in a cluster.
It is also difficult to extend the counting algorithm
to incorporate such relations in an intuitive man-
ner. Therefore, we observe that B-CUBED is not
appropriate for evaluating partial coreference.
We see the basically same reason for the inade-
quacy of CEAF. It also regards a coreference chain
as a set of mentions, and measures how many men-
tions two clusters share using two similarity met-
rics ?
3
(R,S) = |R ? S| and ?
4
(R,S) =
2|R?S|
|R|+|S|
,
given two clustersR and S. One can extend CEAF
for partial coreference by selecting the most ap-
propriate tree similarity algorithm for ? that works
well with the algorithm to compute maximum bi-
partite matching in CEAF. However, that is an-
other line of work, and due to space limitations
we leave it for future work.
4.2 Extension to MUC and BLANC
MUC relies on the minimum number of links
needed when mapping a system response to the
gold standard (Vilain et al., 1995). Given a set of
key entitiesK and a set of response entitiesR, pre-
cision of MUC is defined as the number of com-
mon links between entities in K and R divided by
the number of links in R, whereas recall of MUC
is defined as the number of common links between
entities inK andR divided by the number of links
inK. After finding a set of mention clusters by re-
solving full coreference, we can compute the num-
ber of correct links by counting all links spanning
in those mention clusters that matched the gold
standard. It is possible to apply the idea of MUC
to the case of partial coreference simply by chang-
ing the definition of a correct link. In the partial
coreference case, we define a correct link as a link
matched with the gold standard including its di-
rection. Let MUC
p
denote such extension to MUC
for partial coreference.
Similarly, it is also possible to define an ex-
tension to BLANC. Let BLANC
p
denote the ex-
tension. BLANC computes precision, recall,
and F1 scores for both coreference and non-
coreference links, and average them for the final
score (Recasens and Hovy, 2011). As with MUC
p
,
BLANC
p
defines a correct link as a link matched
with the gold standard including its direction. An-
other difference between BLANC and BLANC
p
is
the total number of mention pairs, denoted asL. In
the original BLANC, L = N(N ? 1)/2 where N
is the total number of mentions in a document. We
use L
p
= N(N ? 1) instead for BLANC
p
since
we consider two directed links in partial corefer-
ence with respect to each undirected link in full
coreference.
4.3 Extension to Simple Tree Matching
The underlying idea of STM is that if two trees
have more node-matching, then they are more sim-
ilar. The original STM uses a dynamic program-
ming approach to perform recursive node-level
matching in a top-down fashion. In the case of
partial coreference, we cannot readily use the ap-
proach because partial coreference is represented
with unordered trees, and thus time complexity of
node-matching is the exponential order with re-
spect to the number of child nodes. However, par-
tial event coreference is normally given in a small
hierarchy with three levels or less. Taking ad-
vantage of this fact and assuming that each event
mention is uniquely identified in a tree, we ex-
tend STM for the case of unordered trees by using
greedy search. Algorithm 1 shows an extension to
the STM algorithm for unordered trees.
We can also naturally extend STM to take
forests as input. Figure 3 shows how one can con-
vert a forest into a single tree whose subtrees are
the trees in the forest by introducing an additional
dummy root node on top of those tree. The result-
ing tree is also an unordered tree, and thus we can
apply Algorithm 1 to that tree to measure the sim-
72
Algorithm 1 Extended simple tree matching for
unordered trees.
Input: two unordered trees A and B
Output: score
1: procedure SimpleTreeMatching(A, B)
2: if the roots of A and B have different elements then
3: return 0
4: else
5: s := 1 . The initial score for the root match.
6: m := the number of first-level sub-trees of A
7: n := the number of first-level sub-trees of B
8: for i = 1? m do
9: for j = 1? n do
10: if A
i
and B
j
have the same element then
11: s = s + SimpleTreeMatching(A
i
, B
j
)
Figure 3: Conversion from a forest to a single tree
with an additional dummy root.
ilarity of two forests comprising unordered trees.
Let STM
p
denote the extended STM. Finally, we
normalize STM
p
. Let NSTM
p
be a normalized
version of STM
p
as follows: NSTM
p
(F
1
, F
2
) =
STM
p
(F
1
, F
2
)/max(|F
1
|, |F
2
|) where |F | de-
notes the number of nodes in F .
4.4 Flexibility of Metrics
Making assumptions on evaluation for a particular
task and defining desiderata for a metric determine
what evaluation scheme we are going to formulate.
However, this kind of effort tends to make result-
ing evaluation metrics too restrictive to be reusable
in other tasks. Such metrics might be adequate
for that task, but we also value the flexibility of
a metric that can be directly used or be easily ex-
tended to other tasks. To investigate the flexibil-
ity of MUC
p
, BLANC
p
and STM
p
, we will exam-
ine these metrics without making the assumptions
of twinless mentions and intransitivity of partial
coreference against each metric. We consider that
the assumption of link propagation is more funda-
mental and regard it as a basic premise, and thus
we will continue to make that assumption.
MUC was originally designed to deal with re-
sponse links spanning mentions that even key links
do not reach. Thus, it is able to handle twinless
mentions. If we do not assume intransitivity of
partial coreference, we do not see any difficulty in
changing the definition of correct links in MUC
p
and making it capture transitive relations. There-
fore, MUC
p
does not require both assumptions of
twinless mentions and intransitivity.
In contrast, BLANC was originally designed to
handle true mentions in the gold standard. Since
BLANC
p
does not make any modifications on this
aspect, it cannot deal with twinless mentions ei-
ther. As for intransitivity, it is possible to easily
change the definition of correct and incorrect links
in BLANC
p
to detect transitive relations. Thus,
BLANC
p
does not require intransitivity but does
require the assumption of no twinless mentions.
Since STM
p
simply matches elements in nodes
as shown in Algorithm 1, it does not require the as-
sumption of twinless mentions. With respect to in-
transitivity, we can extend STM
p
by adding extra
edges from a parent to grandchild nodes or others
and applying Algorithm 1 to the modified trees.
Hence, it does not require the assumption of in-
transitivity.
5 Experiments
To empirically examine the three metrics de-
scribed in Section 4.2 and Section 4.3, we con-
ducted an experiment using the artificial data
shown in Table 1. Since BLANC
p
cannot han-
dle twinless mentions, we removed twinless men-
tions. We first created the gold standard shown in
the first row of the table. It contains fifty events,
twenty one singleton events, and seven event trees
with three levels or less. We believe this distri-
bution of partial coreference is representative of
that of real data. We then created several system
responses that are ordered toward two extremes.
One extreme is all singletons in which they do not
have correct links. The other is a single big tree
that merges all event trees including singletons in
the gold standard.
Figure 4 shows how the three metrics behave
in two cases: (a) we increase the number of cor-
rect links from all singletons to the perfect output
(equal to the gold standard), and (b) we increase
the incorrect links from the perfect output to a sin-
gle tree merging all trees in the gold standard. In
the former case, we started with System 3 in Ta-
ble 1. Next we added one correct link 28
s
?? 29
shown in System 2. This way, we added cor-
rect links up to the perfect output one by one in
a bottom-up fashion. In the latter case, we started
73
Response
Output
Gold standard
(1(2(6))(3(7))(4)(5)) (8(9(11)(12))(10)) (13(14)(15)(16)(17)(18)) (19(20(21))(22)) (23(24)(25))
(26(27)) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46)
(47) (48) (49) (50)
System 1
(1(4)(5)(2(6))(3(7))) (8(9(11)(12))(10)) (13(18)(14)(15)(16)(17)) (19(22)(20(21))) (23(24)(25))
(26(27)) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46)
(47) (48) (49(50))
System 2
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24)
(25) (26) (27) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45)
(46) (47) (48) (49) (50)
System 3
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24)
(25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45)
(46) (47) (48) (49) (50)
Table 1: Examples of a system response against a gold standard partial coreference. Each event tree is
shown in the bold font and in the Newick standard format with parentheses.
with the perfect output, and then added one incor-
rect link 49
s
?? 50 shown in System 1. In a manner
similar to case (a), we added incorrect links up to
the merged tree one by one in a bottom-up fashion.
The results indicate that MUC
p
and BLANC
p
meet the desiderata defined in Section 3.3 more
adequately than NSTM
p
. The curve of MUC
p
and
BLANC
p
in Figure 4 are close to the linearity,
which is practically useful as a metric. In contrast,
NSTM
p
fails to meet P4 and P5 in case (a), and
fails to meet P5 in case (b). This is because STM
first checks whether root nodes of two trees have
the same element, and if the root nodes have dif-
ferent elements, STM stops searching the rest of
nodes in the trees.
6 Discussion
In Section 4.4, we observed that MUC
p
and STM
p
are more flexible than BLANC
p
because they can
measure the performance coreference in the case
of twinless mentions as well. The experimental re-
sults in Section 5 show that MUC
p
and BLANC
p
more adequate in terms of the five properties de-
fined in Section 3.3. Putting these together, MUC
p
seems the best metric for partial event coreference.
However, MUC has two disadvantages that (1) it
prefers systems that have more mentions per en-
tity (event), and (2) it ignores recall for singletons
(Pradhan et al., 2011). MUC
p
also has these disad-
vantages. Thus, BLANC
p
might be the best choice
for partial coreference if we could assume that a
system is given true mentions in the gold standard.
Although STM
p
fails to satisfy P4 and P5, it
has potential power to capture structural proper-
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Sc
ore
Ratio of correct links [%]
MUCpBLANCpNSTMp
(a) The number of correct links increases from singletons to
the perfect output (the gold standard) one by one.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Sc
ore
Ratio of incorrect links [%]
MUCpBLANCpNSTMp
(b) The number of incorrect links increases from the perfect
output to a single tree merging all trees one by one.
Figure 4: Score comparison among MUC
p
,
BLANC
p
, and NSTM
p
.
74
ties of partial coreference described in Section 3.3.
This is because STM?s recursive fashion of node-
counting can be easily extend to counting the num-
ber of correct sibling relations.
7 Conclusion
We proposed an evaluation scheme for partial
event coreference with conceptual event hierar-
chy constructed from mention-based event trees.
We discussed possible assumptions that one can
make, and examined extensions to three existing
metrics. Our experimental results indicate that the
extensions to MUC and BLANC are more ade-
quate than the extension to STM. To our knowl-
edge, this is the first work to argue an evaluation
scheme for partial event coreference. Neverthe-
less, we believe that our scheme is generic and
flexible enough to be applicable to other directed
relations of events (e.g., causality and entailment)
or other related tasks to compare hierarchical data
based on unordered trees (e.g., ontology compari-
son). One future work is to improve the metrics
by incorporating structural consistency of event
trees as an additional property and implementing
the metrics from the perspective of broad contexts
beyond local evaluation by link-based counting.
8 Acknowledgements
This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of the DARPA or the US government. We
would like to thank anonymous reviewers for their
helpful comments.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for Scoring Coreference Chains. In Proceedings of
LREC 1998 Workshop on Linguistics Coreference,
pages 563?566.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution. In
Proceedings of EMNLP 2008, pages 294?303.
Philip Bille. 2005. A Survey on Tree Edit Distance and
Related Problems. Theoretical Computer Science,
337(1-3):217?239.
Jie Cai and Michael Strube. 2010. Evaluation Metrics
For End-to-End Coreference Resolution Systems. In
Proceedings of SIGDIAL 2010, pages 28?36.
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised Learning of Narrative Event Chains. In
Proceedings of ACL-HLT 2008, pages 789?797.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS 2001, pages 625?632.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured Lexical Similarity via Con-
volution Kernels on Dependency Trees. In Proceed-
ings of EMNLP 2011, pages 1034?1046.
Erik D. Demaine, Shay Mozes, Benjamin Rossman,
and Oren Weimann. 2009. An Optimal Decomposi-
tion Algorithm for Tree Edit Distance. ACM Trans-
actions on Algorithms (TALG), 6(1):2:1?2:19.
Greg Durrett, David Hall, and Dan Klein. 2013. De-
centralized Entity-Level Modeling for Coreference
Resolution. In Proceedings of ACL 2013, pages
114?124.
Michael Heilman and Noah A. Smith. 2010. Tree Edit
Models for Recognizing Textual Entailments, Para-
phrases, and Answers to Questions. In Proceedings
of NAACL-HLT 2013, pages 1011?1019.
Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun
Araki, and Andrew Philpot. 2013. Events are Not
Simple: Identity, Non-Identity, and Quasi-Identity.
In Proceedings of NAACL-HLT 2013 Workshop on
Events: Definition, Detection, Coreference, and
Representation, pages 21?28.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using Semantic Relations to Refine Coreference
Decisions. In Proceedings of EMNLP/HLT 2005,
pages 17?24.
Philip N. Klein. 1998. Computing the Edit-Distance
Between Unrooted Ordered Trees. In Proceed-
ings of the 6th European Symposium on Algorithms
(ESA), pages 91?102.
Emmanuel Lassalle and Pascal Denis. 2013. Im-
proving pairwise coreference models through fea-
ture space hierarchy learning. In Proceedings of
ACL 2013, pages 497?506.
Heeyoung Lee, Marta Recasens, Angel Chang, Mi-
hai Surdeanu, and Dan Jurafsky. 2012. Joint En-
tity and Event Coreference Resolution across Doc-
uments. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 489?500.
75
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of EMNLP 2005,
pages 25?32.
Yashar Mehdad. 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. In Proceedings of ACL-IJCNLP 2009, pages
289?292.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role La-
beling. Computational Linguistics, 34(2):193?224.
Mateusz Pawlik and Nikolaus Augsten. 2011. RTED:
A Robust Algorithm for the Tree Edit Distance.
Proceedings of the VLDB Endowment (PVLDB),
5(4):334?345.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In Proceed-
ings of CoNLL Shared Task 2011, pages 1?27.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceedings
of EMNLP 2009, pages 968?977.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry into
Human Knowledge Structures. Lawrence Erlbaum
Associates.
Shashank Srivastava, Dirk Hovy, and Eduard Hovy.
2013. A Walk-Based Semantically Enriched Tree
Kernel Over Distributed Word Representations. In
Proceedings of EMNLP 2013, pages 1411?1416.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in Noun Phrase
Coreference Resolution: Making Sense of the State-
of-the-Art. In Proceedings of ACL/IJCNLP 2009,
pages 656?664.
Kuo-Chung Tai. 1979. The Tree-to-Tree Correction
Problem. Journal of the ACM (JACM), 26(3):422?
433.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Pro-
ceedings of the 6th Message Understanding Confer-
ence (MUC), pages 45?52.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic Tree-Edit Models with Structured La-
tent Variables for Textual Entailment and Question
Answering. In Proceedings of COLING 2010, pages
1164?1172.
Wuu Yang. 1991. Identifying Syntactic Differences
Between Two Programs. Software: Practice and
Experience, 21(7):739?755.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
burch, and Peter Clark. 2013. Answer Extraction
as Sequence Tagging with Tree Edit Distance. In
Proceedings of NAACL-HLT 2013, pages 858?867.
Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance Between
Trees and Related Problems. SIAM J. Comput.,
18(6):1245?1262.
76
