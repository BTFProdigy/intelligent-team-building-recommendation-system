Two Step Chinese Named Entity Recognition Based on Conditional 
Random Fields Models 
Yuanyong Feng*          Ruihong Huang*         Le Sun? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{comerfeng,ruihong2}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, part-
of-speech and a small-vocabulary-
character-lists feature and heristic post-
process rules for MSRA NER open track 
under the framework of Conditional Ran-
dom Fields (CRFs) model. 
1  Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et al, 
2001) framework. It integrates multiple features 
based on single Chinese character or space sepa-
rated ASCII words. The early designed system 
(Feng et al, 2006) is used for the MSRA NER 
open track this year. The output of an external 
part-of-speech tagging tool and some carefully 
collected small-scale-character-lists are used as 
open knowledge. Some post process steps are also 
applied to complement the local limitation in 
model?s feature engineering. 
The remaining of this paper is organized as fol-
lows. Section 2 introduces Conditional Random 
Fields model. Section 3 presents the details of our 
system on Chinese NER integrating multiple fea-
tures. Section 4 describes the post-processings 
based on some heuristic rules. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2  Conditional Random Fields Model 
Conditional random fields are undirected graphical 
models for calculating the conditional probability 
for output vertices based on input ones. While 
sharing the same exponential form with maximum 
entropy models, they have more efficient proce-
dures for complete, non-greedy finite-state infer-
ence and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines the 
corresponding state sequence s? probability as fol-
lows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f
Z
?? ?
=
= ??
o
s o os s t
 
(1)
Where ? is the model parameter set, Zo is the nor-
malization factor over all state sequences, fk is an 
arbitrary feature function, and ?k is the learned fea-
ture weight. A feature function defines its value to 
be 0 in most cases, and to be 1 in some designated 
cases. For example, the value of a feature named 
?MAYBE-SURNAME? is 1 if and only if st-1 is 
OTHER, st is PER, and the t-th character in o is a 
common-surname. 
The inference and training procedures of CRFs 
can be derived directly from those equivalences in 
HMM. For instance, the forward variable ?t(si) 
defines the probability that state at time t being si 
at time t given the observation sequence o. As-
sumed that we know the probabilities of each 
possible value si for the beginning state  ?0(si), 
then we have 
1( ) ( )exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?=? ? o (2)
120
Sixth SIGHAN Workshop on Chinese Language Processing
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3  Chinese NER Using CRFs Model Inte-
grating Multiple Features 
Besides the text feature(TXT), simplified part-of-
speech (POS) feature, and small-vocabulary-
character-lists (SVCL) feature, which use in the 
early system (Feng et al, 2006), some new fea-
tures such as word boundary, adjoining state bi-
gram ? observation and early NE output are also 
combined under the unified CRFs framework. 
The text feature includes single Chinese charac-
ter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Unlike those in 
the earyly system, the POS tag set are merged into 
9 categories from the criterion of modern Chinese 
corpora construction (Yu, 1999), which contains 
39 tags.  
The third type of features are derived from the 
small-vocabulary-character lists which are essen-
tially same as the ones used in last year except 
with some additional items. Some examples of this 
list are given in Table 1. 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is 
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail charac-ter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail charac- ? ? ? ? ?, , , , , 
ter ?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
The fourth type of feature is word boundary. We 
use the B, I, E, U, and O to indicate Begining, In-
ner, Ending, and Uniq part of, or outside of a word 
given a word segmentation. The O case occurs 
when a token, for example the charactor ?&?, is 
ignored by the segmentator. We do not combine 
the boundary information with other features be-
cause we argue it is very limited and may cause 
errors. 
The last type of features is bigram state com-
bined with observations. We argue that observa-
toin (mainly is of named entity derived by early 
system or character text itself) and state transition 
are not conditionally independent and entails dedi-
cate considerings. 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an obser-
vation sequence is feed into the model. 
Each token state is a combination of the type of 
the named entity it belongs to and the boundary 
type it locates within. The entity types are person 
name (PER), location name (LOC), organization 
name (ORG), date expression (DAT), time expres-
sion (TIM), numeric expression (NUM), and not 
named entity (OTH). The boundary types are sim-
ply Beginning, Inside, and Outside (BIO).  
All above types of features are extracted from a 
varying length window. The main criteria is that 
wider window with smaller feature space and nar-
row window when the observation features are in a 
large range. 
The main feature set is shown the following. 
Character Texts(TXT): 
TXT-2, TXT-1, TXT0, TXT1, TXT2, 
TXT-1TXT0,  TXT1TXT0, TXT1TXT2 
simplified part-of-speech (POS): 
unigram: POS-4 ~ POS4 
121
Sixth SIGHAN Workshop on Chinese Language Processing
small-vocabulary-character-lists (SVCL): 
unigram: SVCL-2 ~ SVCL7 
bigram:SVCL0SVCL1, SVCL1SVCL2 
Word Boundary (WB): 
WB-1,WB0,WB1 
Named Entity (NE): 
unigram: NE-4 ~ NE4 
bigram:NE-2NE-1, NE-1NE0, NE0NE1, NE1NE2 
State Bigram (B) ? Observation: 
B, B-TXT0, B-NE-1, B-NE0, B-NE1 
Table 2.  The Main Feature Set 
4  Post Processing on Heuristic Rules 
Observing from the evaluation, our model has 
worse performance on ORG and PER than LOC. 
Furthermore, the analysis of the errors tells us that 
they are hard to be tackled with the improvement 
of the model itself. Therefore, we decided to do 
some post-process to correct certain types of tag-
ging errors of the unified model mainly concerning 
the two kinds of entities, ORG and PER.  
At the training phrase, we compare the tagging 
output of the model with the correct tags and col-
lect the falsely tagged instances. To identify the 
rules used in the post-process, we categorize the 
errors into several types, discriminate the types 
and encode them into the rules according to two 
principles: 
1) the rules are applied on the tagged sequences 
output by the unified model.  
2) The rules applied shouldn?t introduce more 
other errors. 
As a result, we have extracted eight rules, seven 
for ORG, one for PER. Generally, the rules work 
only on the local context of the examined tags, 
they correct some type of error by changing some 
tags when seeing certain pattern of context before 
or after the current tags in a limited distance. We 
want to give one rule as one example to explain 
the way they function. 
Example: {<LOC>}+<ORG> ==> <ORG>,  
After this rule is applied, one or more locations 
followed by a organization name will be tagged 
ORG. This is the case where there are a location 
name in a organization name. Besides, we can see 
since the location and latter part of the organiza-
tion name are tagged separately in the unified 
model, we may only resort to the post-process to 
get the right government boundary.  
5  Evaluation 
5.1  Results 
The evaluations in training phrase tell us the 
post-process can improve the performance by one 
percent. We are satisfied since we just applied 
eight rules. 
The formal eveluation results of our system are 
shown in Table 3. 
 R P F 
Overall 86.74 90.03 88.36 
PER 90.83 92.16 91.49 
LOC 89.89 91.66 90.77 
ORG 77.99 85.16 81.41 
Table 3. Formal Results on MSRA NER Open 
5.2  Errors  from NER Track 
The NER errors in our system are mainly of as 
follows: 
z Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding full 
names. Some common abbreviations has no corre-
sponding full names appeared in document. Here 
are some examples: 
R1: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
122
Sixth SIGHAN Workshop on Chinese Language Processing
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
z Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:?[???? LOC]?[???? PER]
?? 
K:?[???? PER]?[???? PER]
?? 
z Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For in-
stance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from failing 
to identify verbs and prepositions, such as: 
R:??????????? ????? 
K:[??????????? ORG]????
? 
6  Conclusions and Future Work 
We mainly described a Chinese named entity rec-
ognition system NER@ISCAS, which integrates 
text, part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER open track 
under the framework of Conditional Random 
Fields (CRFs) model. Although it provides a uni-
fied framework to integrate multiple flexible fea-
tures, and to achieve global optimization on input 
text sequence, the popular linear chained Condi-
tional Random Fields model often fails to catch 
semantic relations among reoccurred mentions and 
adjoining entities in a catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post process-
ing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108.  
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Human-
Machine Interface Technology Evaluation. 
Yuanyong Feng, Le Sun, Yuanhua Lv. 2006. 
Chinese Word Segmentation and Named Entity 
Recognition Based on Conditional Random 
Fields Models. Proceedings of SIGHAN-2006, 
Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia,. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Label-
ing Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
123
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704?714,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sarcasm as Contrast between a Positive Sentiment and Negative Situation
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,
Nathan Gilbert, Ruihong Huang
School Of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,asheq,alnds,ngilbert,huangrh}@cs.utah.edu, prafulla.surve@gmail.com
Abstract
A common form of sarcasm on Twitter con-
sists of a positive sentiment contrasted with a
negative situation. For example, many sarcas-
tic tweets include a positive sentiment, such as
?love? or ?enjoy?, followed by an expression
that describes an undesirable activity or state
(e.g., ?taking exams? or ?being ignored?). We
have developed a sarcasm recognizer to iden-
tify this type of sarcasm in tweets. We present
a novel bootstrapping algorithm that automati-
cally learns lists of positive sentiment phrases
and negative situation phrases from sarcastic
tweets. We show that identifying contrast-
ing contexts using the phrases learned through
bootstrapping yields improved recall for sar-
casm recognition.
1 Introduction
Sarcasm is generally characterized as ironic or satir-
ical wit that is intended to insult, mock, or amuse.
Sarcasm can be manifested in many different ways,
but recognizing sarcasm is important for natural lan-
guage processing to avoid misinterpreting sarcastic
statements as literal. For example, sentiment anal-
ysis can be easily misled by the presence of words
that have a strong polarity but are used sarcastically,
which means that the opposite polarity was intended.
Consider the following tweet on Twitter, which in-
cludes the words ?yay? and ?thrilled? but actually
expresses a negative sentiment: ?yay! it?s a holi-
day weekend and i?m on call for work! couldn?t be
more thrilled! #sarcasm.? In this case, the hashtag
#sarcasm reveals the intended sarcasm, but we don?t
always have the benefit of an explicit sarcasm label.
In the realm of Twitter, we observed that many
sarcastic tweets have a common structure that
creates a positive/negative contrast between a senti-
ment and a situation. Specifically, sarcastic tweets
often express a positive sentiment in reference to a
negative activity or state. For example, consider the
tweets below, where the positive sentiment terms
are underlined and the negative activity/state terms
are italicized.
(a) Oh how I love being ignored. #sarcasm
(b) Thoroughly enjoyed shoveling the driveway
today! :) #sarcasm
(c) Absolutely adore it when my bus is late
#sarcasm
(d) I?m so pleased mom woke me up with
vacuuming my room this morning. :) #sarcasm
The sarcasm in these tweets arises from the jux-
taposition of a positive sentiment word (e.g., love,
enjoyed, adore, pleased) with a negative activity or
state (e.g., being ignored, bus is late, shoveling, and
being woken up).
The goal of our research is to identify sarcasm
that arises from the contrast between a positive sen-
timent referring to a negative situation. A key chal-
lenge is to automatically recognize the stereotypi-
cally negative ?situations?, which are activities and
states that most people consider to be unenjoyable or
undesirable. For example, stereotypically unenjoy-
able activities include going to the dentist, taking an
exam, and having to work on holidays. Stereotypi-
cally undesirable states include being ignored, hav-
ing no friends, and feeling sick. People recognize
704
these situations as being negative through cultural
norms and stereotypes, so they are rarely accompa-
nied by an explicit negative sentiment. For example,
?I feel sick? is universally understood to be a nega-
tive situation, even without an explicit expression of
negative sentiment. Consequently, we must learn to
recognize phrases that correspond to stereotypically
negative situations.
We present a bootstrapping algorithm that auto-
matically learns phrases corresponding to positive
sentiments and phrases corresponding to negative
situations. We use tweets that contain a sarcasm
hashtag as positive instances for the learning pro-
cess. The bootstrapping algorithm begins with a sin-
gle seed word, ?love?, and a large set of sarcastic
tweets. First, we learn negative situation phrases
that follow a positive sentiment (initially, the seed
word ?love?). Second, we learn positive sentiment
phrases that occur near a negative situation phrase.
The bootstrapping process iterates, alternately learn-
ing new negative situations and new positive sen-
timent phrases. Finally, we use the learned lists
of sentiment and situation phrases to recognize sar-
casm in new tweets by identifying contexts that con-
tain a positive sentiment in close proximity to a neg-
ative situation phrase.
2 Related Work
Researchers have investigated the use of lexical
and syntactic features to recognize sarcasm in text.
Kreuz and Caucci (2007) studied the role that dif-
ferent lexical factors play, such as interjections (e.g.,
?gee? or ?gosh?) and punctuation symbols (e.g., ???)
in recognizing sarcasm in narratives. Lukin and
Walker (2013) explored the potential of a bootstrap-
ping method for sarcasm classification in social di-
alogue to learn lexical N-gram cues associated with
sarcasm (e.g., ?oh really?, ?I get it?, ?no way?, etc.)
as well as lexico-syntactic patterns.
In opinionated user posts, Carvalho et al (2009)
found oral or gestural expressions, represented us-
ing punctuation and other keyboard characters, to
be more predictive of irony1 in contrast to features
representing structured linguistic knowledge in Por-
1They adopted the term ?irony? instead of ?sarcasm? to re-
fer to the case when a word or expression with prior positive
polarity is figuratively used to express a negative opinion.
tuguese. Filatova (2012) presented a detailed de-
scription of sarcasm corpus creation with sarcasm
annotations of Amazon product reviews. Their an-
notations capture sarcasm both at the document level
and the text utterance level. Tsur et al (2010) pre-
sented a semi-supervised learning framework that
exploits syntactic and pattern based features in sar-
castic sentences of Amazon product reviews. They
observed correlated sentiment words such as ?yay!?
or ?great!? often occurring in their most useful pat-
terns.
Davidov et al (2010) used sarcastic tweets and
sarcastic Amazon product reviews to train a sarcasm
classifier with syntactic and pattern-based features.
They examined whether tweets with a sarcasm hash-
tag are reliable enough indicators of sarcasm to be
used as a gold standard for evaluation, but found that
sarcasm hashtags are noisy and possibly biased to-
wards the hardest form of sarcasm (where even hu-
mans have difficulty). Gonza?lez-Iba?n?ez et al (2011)
explored the usefulness of lexical and pragmatic fea-
tures for sarcasm detection in tweets. They used sar-
casm hashtags as gold labels. They found positive
and negative emotions in tweets, determined through
fixed word dictionaries, to have a strong correlation
with sarcasm. Liebrecht et al (2013) explored N-
gram features from 1 to 3-grams to build a classifier
to recognize sarcasm in Dutch tweets. They made an
interesting observation from their most effective N-
gram features that people tend to be more sarcastic
towards specific topics such as school, homework,
weather, returning from vacation, public transport,
the church, the dentist, etc. This observation has
some overlap with our observation that stereotypi-
cally negative situations often occur in sarcasm.
The cues for recognizing sarcasm may come from
a variety of sources. There exists a line of work
that tries to identify facial and vocal cues in speech
(e.g., (Gina M. Caucci, 2012; Rankin et al, 2009)).
Cheang and Pell (2009) and Cheang and Pell (2008)
performed studies to identify acoustic cues in sarcas-
tic utterances by analyzing speech features such as
speech rate, mean amplitude, amplitude range, etc.
Tepperman et al (2006) worked on sarcasm recog-
nition in spoken dialogue using prosodic and spec-
tral cues (e.g., average pitch, pitch slope, etc.) as
well as contextual cues (e.g., laughter or response to
questions) as features.
705
While some of the previous work has identi-
fied specific expressions that correlate with sarcasm,
none has tried to identify contrast between positive
sentiments and negative situations. The novel con-
tributions of our work include explicitly recogniz-
ing contexts that contrast a positive sentiment with a
negative activity or state, as well as a bootstrapped
learning framework to automatically acquire posi-
tive sentiment and negative situation phrases.
3 Bootstrapped Learning of Positive
Sentiments and Negative Situations
Sarcasm is often defined in terms of contrast or ?say-
ing the opposite of what you mean?. Our work fo-
cuses on one specific type of contrast that is common
on Twitter: the expression of a positive sentiment
(e.g., ?love? or ?enjoy?) in reference to a negative
activity or state (e.g., ?taking an exam? or ?being ig-
nored?). Our goal is to create a sarcasm classifier for
tweets that explicitly recognizes contexts that con-
tain a positive sentiment contrasted with a negative
situation.
Our approach learns rich phrasal lexicons of pos-
itive sentiments and negative situations using only
the seed word ?love? and a collection of sarcastic
tweets as input. A key factor that makes the algo-
rithm work is the presumption that if you find a pos-
itive sentiment or a negative situation in a sarcastic
tweet, then you have found the source of the sar-
casm. We further assume that the sarcasm probably
arises from positive/negative contrast and we exploit
syntactic structure to extract phrases that are likely
to have contrasting polarity. Another key factor is
that we focus specifically on tweets. The short na-
ture of tweets limits the search space for the source
of the sarcasm. The brevity of tweets also probably
contributes to the prevalence of this relatively com-
pact form of sarcasm.
3.1 Overview of the Learning Process
Our bootstrapping algorithm operates on the as-
sumption that many sarcastic tweets contain both a
positive sentiment and a negative situation in close
proximity, which is the source of the sarcasm.2 Al-
though sentiments and situations can be expressed
2Sarcasm can arise from a negative sentiment contrasted
with a positive situation too, but our observation is that this is
much less common, at least on Twitter.
Positive
Sentiment
Phrases
Negative
Situation
Phrases
Seed Word
"love"
Sarcastic Tweets
1 2
34
Figure 1: Bootstrapped Learning of Positive Sentiment
and Negative Situation Phrases
in numerous ways, we focus on positive sentiments
that are expressed as a verb phrase or as a predicative
expression (predicate adjective or predicate nomi-
nal), and negative activities or states that can be a
complement to a verb phrase. Ideally, we would
like to parse the text and extract verb complement
phrase structures, but tweets are often informally
written and ungrammatical. Therefore we try to rec-
ognize these syntactic structures heuristically using
only part-of-speech tags and proximity.
The learning process relies on an assumption that
a positive sentiment verb phrase usually appears to
the left of a negative situation phrase and in close
proximity (usually, but not always, adjacent). Picto-
rially, we assume that many sarcastic tweets contain
this structure:
[+ VERB PHRASE] [? SITUATION PHRASE]
This structural assumption drives our bootstrap-
ping algorithm, which is illustrated in Figure 1.
The bootstrapping process begins with a single seed
word, ?love?, which seems to be the most common
positive sentiment term in sarcastic tweets. Given
a sarcastic tweet containing the word ?love?, our
structural assumption infers that ?love? is probably
followed by an expression that refers to a negative
situation. So we harvest the n-grams that follow the
word ?love? as negative situation candidates. We se-
lect the best candidates using a scoring metric, and
add them to a list of negative situation phrases.
Next, we exploit the structural assumption in the
opposite direction. Given a sarcastic tweet that con-
tains a negative situation phrase, we infer that the
negative situation phrase is preceded by a positive
sentiment. We harvest the n-grams that precede the
negative situation phrases as positive sentiment can-
didates, score and select the best candidates, and
706
add them to a list of positive sentiment phrases.
The bootstrapping process then iterates, alternately
learning more positive sentiment phrases and more
negative situation phrases.
We also observed that positive sentiments are fre-
quently expressed as predicative phrases (i.e., pred-
icate adjectives and predicate nominals). For ex-
ample: ?I?m taking calculus. It is awesome. #sar-
casm?. Wiegand et al (2013) offered a related ob-
servation that adjectives occurring in predicate ad-
jective constructions are more likely to convey sub-
jectivity than adjectives occurring in non-predicative
structures. Therefore we also include a step in
the learning process to harvest predicative phrases
that occur in close proximity to a negative situation
phrase. In the following sections, we explain each
step of the bootstrapping process in more detail.
3.2 Bootstrapping Data
For the learning process, we used Twitter?s stream-
ing API to obtain a large set of tweets. We col-
lected 35,000 tweets that contain the hashtag #sar-
casm or #sarcastic to use as positive instances of sar-
casm. We also collected 140,000 additional tweets
from Twitter?s random daily stream. We removed
the tweets that contain a sarcasm hashtag, and con-
sidered the rest to be negative instances of sarcasm.
Of course, there will be some sarcastic tweets that do
not have a sarcasm hashtag, so the negative instances
will contain some noise. But we expect that a very
small percentage of these tweets will be sarcastic, so
the noise should not be a major issue. There will also
be noise in the positive instances because a sarcasm
hashtag does not guarantee that there is sarcasm in
the body of the tweet (e.g., the sarcastic content may
be in a linked url, or in a prior tweet). But again, we
expect the amount of noise to be relatively small.
Our tweet collection therefore contains a total of
175,000 tweets: 20% are labeled as sarcastic and
80% are labeled as not sarcastic. We applied CMU?s
part-of-speech tagger designed for tweets (Owoputi
et al, 2013) to this data set.
3.3 Seeding
The bootstrapping process begins by initializing the
positive sentiment lexicon with one seed word: love.
We chose this seed because it seems to be the most
common positive sentiment word in sarcastic tweets.
3.4 Learning Negative Situation Phrases
The first stage of bootstrapping learns new phrases
that correspond to negative situations. The learning
process consists of two steps: (1) harvesting candi-
date phrases, and (2) scoring and selecting the best
candidates.
To collect candidate phrases for negative situa-
tions, we extract n-grams that follow a positive senti-
ment phrase in a sarcastic tweet. We extract every 1-
gram, 2-gram, and 3-gram that occurs immediately
after (on the right-hand side) of a positive sentiment
phrase. As an example, consider the tweet in Figure
2, where ?love? is the positive sentiment:
I love waiting forever for the doctor #sarcasm
Figure 2: Example Sarcastic Tweet
We extract three n-grams as candidate negative situ-
ation phrases:
waiting, waiting forever, waiting forever for
Next, we apply the part-of-speech (POS) tagger
and filter the candidate list based on POS patterns so
we only keep n-grams that have a desired syntactic
structure. For negative situation phrases, our goal
is to learn possible verb phrase (VP) complements
that are themselves verb phrases because they should
represent activities and states. So we require a can-
didate phrase to be either a unigram tagged as a verb
(V) or the phrase must match one of 7 POS-based
bigram patterns or 20 POS-based trigram patterns
that we created to try to approximate the recogni-
tion of verbal complement structures. The 7 POS bi-
gram patterns are: V+V, V+ADV, ADV+V, ?to?+V,
V+NOUN, V+PRO, V+ADJ. Note that we used
a POS tagger designed for Twitter, which has a
smaller set of POS tags than more traditional POS
taggers. For example there is just a single V tag
that covers all types of verbs. The V+V pattern will
therefore capture negative situation phrases that con-
sist of a present participle verb followed by a past
participle verb, such as ?being ignored? or ?getting
hit?.3 We also allow verb particles to match a V tag
in our patterns. The remaining bigram patterns cap-
ture verb phrases that include a verb and adverb, an
3In some cases it may be more appropriate to consider the
second verb to be an adjective, but in practice they were usually
tagged as verbs.
707
infinitive form (e.g., ?to clean?), a verb and noun
phrase (e.g., ?shoveling snow?), or a verb and ad-
jective (e.g., ?being alone?). We use some simple
heuristics to try to ensure that we are at the end of an
adjective or noun phrase (e.g., if the following word
is tagged as an adjective or noun, then we assume
we are not at the end).
The 20 POS trigram patterns are similar in nature
and are designed to capture seven general types of
verb phrases: verb and adverb mixtures, an infini-
tive VP that includes an adverb, a verb phrase fol-
lowed by a noun phrase, a verb phrase followed by a
prepositional phrase, a verb followed by an adjective
phrase, or an infinitive VP followed by an adjective,
noun, or pronoun.
Returning to Figure 2, only two of the n-grams
match our POS patterns, so we are left with two can-
didate phrases for negative situations:
waiting, waiting forever
Next, we score each negative situation candidate
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase following
a positive sentiment phrase:
| follows(?candidate, +sentiment) & sarcastic |
| follows(?candidate, +sentiment) |
We compute the number of times that the negative
situation candidate immediately follows a positive
sentiment in sarcastic tweets divided by the number
of times that the candidate immediately follows a
positive sentiment in all tweets. We discard phrases
that have a frequency < 3 in the tweet collection
since they are too sparse.
Finally, we rank the candidate phrases based on
this probability, using their frequency as a secondary
key in case of ties. The top 20 phrases with a prob-
ability ? .80 are added to the negative situation
phrase list.4 When we add a phrase to the nega-
tive situation list, we immediately remove all other
candidates that are subsumed by the selected phrase.
For example, if we add the phrase ?waiting?, then
the phrase ?waiting forever? would be removed from
the candidate list because it is subsumed by ?wait-
ing?. This process reduces redundancy in the set of
4Fewer than 20 phrases will be learned if < 20 phrases pass
this threshold.
phrases that we add during each bootstrapping itera-
tion. The bootstrapping process stops when no more
candidate phrases pass the probability threshold.
3.5 Learning Positive Verb Phrases
The procedure for learning positive sentiment
phrases is analogous. First, we collect phrases that
potentially convey a positive sentiment by extract-
ing n-grams that precede a negative situation phrase
in a sarcastic tweet. To learn positive sentiment verb
phrases, we extract every 1-gram and 2-gram that
occurs immediately before (on the left-hand side of)
a negative situation phrase.
Next, we apply the POS tagger and filter the n-
grams using POS tag patterns so that we only keep
n-grams that have a desired syntactic structure. Here
our goal is to learn simple verb phrases (VPs) so we
only retain n-grams that contain at least one verb and
consist only of verbs and (optionally) adverbs. Fi-
nally, we score each candidate sentiment verb phrase
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase preceding
a negative situation phrase:
| precedes(+candidateVP,?situation) & sarcastic |
| precedes(+candidateVP,?situation) |
3.6 Learning Positive Predicative Phrases
We also use the negative situation phrases to harvest
predicative expressions (predicate adjective or pred-
icate nominal structures) that occur nearby. Based
on the same assumption that sarcasm often arises
from the contrast between a positive sentiment and
a negative situation, we identify tweets that contain
a negative situation and a predicative expression in
close proximity. We then assume that the predicative
expression is likely to convey a positive sentiment.
To learn predicative expressions, we use 24 copu-
lar verbs from Wikipedia5 and their inflections. We
extract positive sentiment candidates by extracting
1-grams, 2-grams, and 3-grams that appear immedi-
ately after a copular verb and occur within 5 words
of the negative situation phrase, on either side. This
constraint only enforces proximity because predica-
tive expressions often appear in a separate clause or
sentence (e.g., ?It is just great that my iphone was
stolen? or ?My iphone was stolen. This is great.?)
5http://en.wikipedia.org/wiki/List of English copulae
708
We then apply POS patterns to identify n-grams
that correspond to predicate adjective and predicate
nominal phrases. For predicate adjectives, we re-
tain ADJ and ADV+ADJ n-grams. We use a few
heuristics to check that the adjective is not part of a
noun phrase (e.g., we check that the following word
is not a noun). For predicate nominals, we retain
ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.
We excluded noun phrases consisting only of nouns
because they rarely seemed to represent a sentiment.
The sentiment in predicate nominals was usually
conveyed by the adjective. We discard all candidates
with frequency < 3 as being too sparse. Finally,
we score each remaining candidate by estimating the
probability that a tweet is sarcastic given that it con-
tains the predicative expression near (within 5 words
of) a negative situation phrase:
| near(+candidatePRED,?situation) & sarcastic |
| near(+candidatePRED,?situation) |
We found that the diversity of positive senti-
ment verb phrases and predicative expressions is
much lower than the diversity of negative situation
phrases. As a result, we sort the candidates by their
probability and conservatively add only the top 5
positive verb phrases and top 5 positive predicative
expressions in each bootstrapping iteration. Both
types of sentiment phrases must pass a probability
threshold of ? .70.
3.7 The Learned Phrase Lists
The bootstrapping process alternately learns pos-
itive sentiments and negative situations until no
more phrases can be learned. In our experiments,
we learned 26 positive sentiment verb phrases, 20
predicative expressions and 239 negative situation
phrases.
Table 1 shows the first 15 positive verb phrases,
the first 15 positive predicative expressions, and the
first 40 negative situation phrases learned by the
bootstrapping algorithm. Some of the negative sit-
uation phrases are not complete expressions, but it
is clear that they will often match negative activities
and states. For example, ?getting yelled? was gener-
ated from sarcastic comments such as ?I love getting
yelled at?, ?being home? occurred in tweets about
?being home alone?, and ?being told? is often be-
ing told what to do. Shorter phrases often outranked
longer phrases because they are more general, and
will therefore match more contexts. But an avenue
for future work is to learn linguistic expressions that
more precisely characterize specific negative situa-
tions.
Positive Verb Phrases (26): missed, loves,
enjoy, cant wait, excited, wanted, can?t wait,
get, appreciate, decided, loving, really like,
looooove, just keeps, loveee, ...
Positive Predicative Expressions (20): great,
so much fun, good, so happy, better, my
favorite thing, cool, funny, nice, always fun,
fun, awesome, the best feeling, amazing,
happy, ...
Negative Situations (239): being ignored, be-
ing sick, waiting, feeling, waking up early, be-
ing woken, fighting, staying, writing, being
home, cleaning, not getting, crying, sitting at
home, being stuck, starting, being told, be-
ing left, getting ignored, being treated, doing
homework, learning, getting up early, going to
bed, getting sick, riding, being ditched, get-
ting ditched, missing, not sleeping, not talking,
trying, falling, walking home, getting yelled,
being awake, being talked, taking care, doing
nothing, wasting, ...
Table 1: Examples of Learned Phrases
4 Evaluation
4.1 Data
For evaluation purposes, we created a gold stan-
dard data set of manually annotated tweets. Even
for people, it is not always easy to identify sarcasm
in tweets because sarcasm often depends on con-
versational context that spans more than a single
tweet. Extracting conversational threads from Twit-
ter, and analyzing conversational exchanges, has its
own challenges and is beyond the scope of this re-
search. We focus on identifying sarcasm that is self-
contained in one tweet and does not depend on prior
conversational context.
We defined annotation guidelines that instructed
human annotators to read isolated tweets and label
709
a tweet as sarcastic if it contains comments judged
to be sarcastic based solely on the content of that
tweet. Tweets that do not contain sarcasm, or where
potential sarcasm is unclear without seeing the prior
conversational context, were labeled as not sarcas-
tic. For example, a tweet such as ?Yes, I meant that
sarcastically.? should be labeled as not sarcastic be-
cause the sarcastic content was (presumably) in a
previous tweet. The guidelines did not contain any
instructions that required positive/negative contrast
to be present in the tweet, so all forms of sarcasm
were considered to be positive examples.
To ensure that our evaluation data had a healthy
mix of both sarcastic and non-sarcastic tweets, we
collected 1,600 tweets with a sarcasm hashtag (#sar-
casm or #sarcastic), and 1,600 tweets without these
sarcasm hashtags from Twitter?s random streaming
API. When presenting the tweets to the annotators,
the sarcasm hashtags were removed so the annota-
tors had to judge whether a tweet was sarcastic or
not without seeing those hashtags.
To ensure that we had high-quality annotations,
three annotators were asked to annotate the same set
of 200 tweets (100 sarcastic + 100 not sarcastic).
We computed inter-annotator agreement (IAA) be-
tween each pair of annotators using Cohen?s kappa
(?). The pairwise IAA scores were ?=0.80, ?=0.81,
and ?=0.82. We then gave each annotator an addi-
tional 1,000 tweets to annotate, yielding a total of
3,200 annotated tweets. We used the first 200 tweets
as our Tuning Set, and the remaining 3000 tweets as
our Test Set.
Our annotators judged 742 of the 3,200 tweets
(23%) to be sarcastic. Only 713 of the 1,600 tweets
with sarcasm hashtags (45%) were judged to be sar-
castic based on our annotation guidelines. There are
several reasons why a tweet with a sarcasm hash-
tag might not have been judged to be sarcastic. Sar-
casm may not be apparent without prior conversa-
tional context (i.e., multiple tweets), or the sarcastic
content may be in a URL and not the tweet itself, or
the tweet?s content may not obviously be sarcastic
without seeing the sarcasm hashtag (e.g., ?The most
boring hockey game ever #sarcasm?).
Of the 1,600 tweets in our data set that were ob-
tained from the random stream and did not have a
sarcasm hashtag, 29 (1.8%) were judged to be sar-
castic based on our annotation guidelines.
4.2 Baselines
Overall, 693 of the 3,000 tweets in our Test Set
were annotated as sarcastic, so a system that classi-
fies every tweet as sarcastic will have 23% precision.
To assess the difficulty of recognizing the sarcastic
tweets in our data set, we evaluated a variety of base-
line systems.
We created two baseline systems that use n-gram
features with supervised machine learning to create
a sarcasm classifier. We used the LIBSVM (Chang
and Lin, 2011) library to train two support vector
machine (SVM) classifiers: one with just unigram
features and one with both unigrams and bigrams.
The features had binary values indicating the pres-
ence or absence of each n-gram in a tweet. The clas-
sifiers were evaluated using 10-fold cross-validation.
We used the RBF kernel, and the cost and gamma
parameters were optimized for accuracy using un-
igram features and 10-fold cross-validation on our
Tuning Set. The first two rows of Table 2 show the
results for these SVM classifiers, which achieved F
scores of 46-48%.
We also conducted experiments with existing sen-
timent and subjectivity lexicons to see whether they
could be leveraged to recognize sarcasm. We exper-
imented with three resources:
Liu05 : A positive and negative opinion lexicon
from (Liu et al, 2005). This lexicon contains
2,007 positive sentiment words and 4,783 neg-
ative sentiment words.
MPQA05 : The MPQA Subjectivity Lexicon that
is part of the OpinionFinder system (Wilson et
al., 2005a; Wilson et al, 2005b). This lexicon
contains 2,718 subjective words with positive
polarity and 4,910 subjective words with nega-
tive polarity.
AFINN11 The AFINN sentiment lexicon designed
for microblogs (Nielsen, 2011; Hansen et al,
2011) contains 2,477 manually labeled words
and phrases with integer values ranging from -5
(negativity) to 5 (positivity). We considered all
words with negative values to have negative po-
larity (1598 words), and all words with positive
values to have positive polarity (879 words).
We performed four sets of experiments with each
resource to see how beneficial existing sentiment
710
System Recall Precision F score
Supervised SVM Classifiers
1grams .35 .64 .46
1+2grams .39 .64 .48
Positive Sentiment Only
Liu05 .77 .34 .47
MPQA05 .78 .30 .43
AFINN11 .75 .32 .44
Negative Sentiment Only
Liu05 .26 .23 .24
MPQA05 .34 .24 .28
AFINN11 .24 .22 .23
Positive and Negative Sentiment, Unordered
Liu05 .19 .37 .25
MPQA05 .27 .30 .29
AFINN11 .17 .30 .22
Positive and Negative Sentiment, Ordered
Liu05 .09 .40 .14
MPQA05 .13 .30 .18
AFINN11 .09 .35 .14
Our Bootstrapped Lexicons
Positive VPs .28 .45 .35
Negative Situations .29 .38 .33
Contrast(+VPs, ?Situations), Unordered .11 .56 .18
Contrast(+VPs, ?Situations), Ordered .09 .70 .15
& Contrast(+Preds, ?Situations) .13 .63 .22
Our Bootstrapped Lexicons ? SVM Classifier
Contrast(+VPs, ?Situations), Ordered .42 .63 .50
& Contrast(+Preds, ?Situations) .44 .62 .51
Table 2: Experimental results on the test set
lexicons could be for sarcasm recognition in tweets.
Since our hypothesis is that sarcasm often arises
from the contrast between something positive and
something negative, we systematically evaluated the
positive and negative phrases individually, jointly,
and jointly in a specific order (a positive phrase fol-
lowed by a negative phrase).
First, we labeled a tweet as sarcastic if it con-
tains any positive term in each resource. The Pos-
itive Sentiment Only section of Table 2 shows that
all three sentiment lexicons achieved high recall (75-
78%) but low precision (30-34%). Second, we la-
beled a tweet as sarcastic if it contains any negative
term from each resource. The Negative Sentiment
Only section of Table 2 shows that this approach
yields much lower recall and also lower precision
of 22-24%, which is what would be expected of a
random classifier since 23% of the tweets are sar-
castic. These results suggest that explicit negative
sentiments are not generally indicative of sarcasm.
Third, we labeled a tweet as sarcastic if it contains
both a positive sentiment term and a negative senti-
ment term, in any order. The Positive and Negative
Sentiment, Unordered section of Table 2 shows that
this approach yields low recall, indicating that rela-
tively few sarcastic tweets contain both positive and
negative sentiments, and low precision as well.
Fourth, we required the contrasting sentiments to
occur in a specific order (the positive term must pre-
cede the negative term) and near each other (no more
than 5 words apart). This criteria reflects our obser-
vation that positive sentiments often closely precede
negative situations in sarcastic tweets, so we wanted
to see if the same ordering tendency holds for neg-
ative sentiments. The Positive and Negative Senti-
ment, Ordered section of Table 2 shows that this or-
dering constraint further decreases recall and only
slightly improves precision, if at all. Our hypothe-
711
sis is that when positive and negative sentiments are
expressed in the same tweet, they are referring to
different things (e.g., different aspects of a product).
Expressing positive and negative sentiments about
the same thing would usually sound contradictory
rather than sarcastic.
4.3 Evaluation of Bootstrapped Phrase Lists
The next set of experiments evaluates the effective-
ness of the positive sentiment and negative situa-
tion phrases learned by our bootstrapping algorithm.
The results are shown in the Our Bootstrapped Lex-
icons section of Table 2. For the sake of compar-
ison with other sentiment resources, we first eval-
uated our positive sentiment verb phrases and neg-
ative situation phrases independently. Our positive
verb phrases achieved much lower recall than the
positive sentiment phrases in the other resources, but
they had higher precision (45%). The low recall
is undoubtedly because our bootstrapped lexicon is
small and contains only verb phrases, while the other
resources are much larger and contain terms with
additional parts-of-speech, such as adjectives and
nouns.
Despite its relatively small size, our list of neg-
ative situation phrases achieved 29% recall, which
is comparable to the negative sentiments, but higher
precision (38%).
Next, we classified a tweet as sarcastic if it con-
tains both a positive verb phrase and a negative sit-
uation phrase from our bootstrapped lists, in any
order. This approach produced low recall (11%)
but higher precision (56%) than the sentiment lex-
icons. Finally, we enforced an ordering constraint
so a tweet is labeled as sarcastic only if it contains
a positive verb phrase that precedes a negative situa-
tion in close proximity (no more than 5 words apart).
This ordering constraint further increased precision
from 56% to 70%, with a decrease of only 2 points
in recall. This precision gain supports our claim that
this particular structure (positive verb phrase fol-
lowed by a negative situation) is strongly indicative
of sarcasm. Note that the same ordering constraint
applied to a positive verb phrase followed by a neg-
ative sentiment produced much lower precision (at
best 40% precision using the Liu05 lexicon). Con-
trasting a positive sentiment with a negative situa-
tion seems to be a key element of sarcasm.
In the last experiment, we added the positive pred-
icative expressions and also labeled a tweet as sar-
castic if a positive predicative appeared in close
proximity to (within 5 words of) a negative situa-
tion. The positive predicatives improved recall to
13%, but decreased precision to 63%, which is com-
parable to the SVM classifiers.
4.4 A Hybrid Approach
Thus far, we have used the bootstrapped lexicons
to recognize sarcasm by looking for phrases in our
lists. We will refer to our approach as the Contrast
method, which labels a tweet as sarcastic if it con-
tains a positive sentiment phrase in close proximity
to a negative situation phrase.
The Contrast method achieved 63% precision but
with low recall (13%). The SVM classifier with un-
igram and bigram features achieved 64% precision
with 39% recall. Since neither approach has high
recall, we decided to see whether they are comple-
mentary and the Contrast method is finding sarcastic
tweets that the SVM classifier overlooks.
In this hybrid approach, a tweet is labeled as sar-
castic if either the SVM classifier or the Contrast
method identifies it as sarcastic. This approach im-
proves recall from 39% to 42% using the Contrast
method with only positive verb phrases. Recall im-
proves to 44% using the Contrast method with both
positive verb phrases and predicative phrases. This
hybrid approach has only a slight drop in precision,
yielding an F score of 51%. This result shows that
our bootstrapped phrase lists are recognizing sarcas-
tic tweets that the SVM classifier misses.
Finally, we ran tests to see if the performance of
the hybrid approach (Contrast ? SVM) is statisti-
cally significantly better than the performance of the
SVM classifier alone. We used paired bootstrap sig-
nificance testing as described in Berg-Kirkpatrick
et al (2012) by drawing 106 samples with repeti-
tion from the test set. These results showed that the
Contrast ? SVM system is statistically significantly
better than the SVM classifier at the p < .01 level
(i.e., the null hypothesis was rejected with 99% con-
fidence).
4.5 Analysis
To get a better sense of the strength and limitations
of our approach, we manually inspected some of the
712
tweets that were labeled as sarcastic using our boot-
strapped phrase lists. Table 3 shows some of the sar-
castic tweets found by the Contrast method but not
by the SVM classifier.
i love fighting with the one i love
love working on my last day of summer
i enjoy tweeting [user] and not getting a reply
working during vacation is awesome .
can?t wait to wake up early to babysit !
Table 3: Five sarcastic tweets found by the Contrast
method but not the SVM
These tweets are good examples of a positive sen-
timent (love, enjoy, awesome, can?t wait) contrast-
ing with a negative situation. However, the negative
situation phrases are not always as specific as they
should be. For example, ?working? was learned as
a negative situation phrase because it is often neg-
ative when it follows a positive sentiment (?I love
working...?). But the attached prepositional phrases
(?on my last day of summer? and ?during vacation?)
should ideally have been captured as well.
We also examined tweets that were incorrectly la-
beled as sarcastic by the Contrast method. Some
false hits come from situations that are frequently
negative but not always negative (e.g., some peo-
ple genuinely like waking up early). However, most
false hits were due to overly general negative situa-
tion phrases (e.g., ?I love working there? was labeled
as sarcastic). We believe that an important direction
for future work will be to learn longer phrases that
represent more specific situations.
5 Conclusions
Sarcasm is a complex and rich linguistic phe-
nomenon. Our work identifies just one type of sar-
casm that is common in tweets: contrast between a
positive sentiment and negative situation. We pre-
sented a bootstrapped learning method to acquire
lists of positive sentiment phrases and negative ac-
tivities and states, and show that these lists can be
used to recognize sarcastic tweets.
This work has only scratched the surface of pos-
sibilities for identifying sarcasm arising from posi-
tive/negative contrast. The phrases that we learned
were limited to specific syntactic structures and we
required the contrasting phrases to appear in a highly
constrained context. We plan to explore methods for
allowing more flexibility and for learning additional
types of phrases and contrasting structures.
We also would like to explore new ways to iden-
tify stereotypically negative activities and states be-
cause we believe this type of world knowledge is
essential to recognize many instances of sarcasm.
For example, sarcasm often arises from a descrip-
tion of a negative event followed by a positive emo-
tion but in a separate clause or sentence, such as:
?Going to the dentist for a root canal this after-
noon. Yay, I can?t wait.? Recognizing the intensity
of the negativity may also be useful to distinguish
strong contrast from weak contrast. Having knowl-
edge about stereotypically undesirable activities and
states could also be important for other natural lan-
guage understanding tasks, such as text summariza-
tion and narrative plot analysis.
6 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
References
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 995?1005.
Paula Carvalho, Lu??s Sarmento, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Clues for detecting irony
in user-generated contents: oh...!! it?s ?so easy? ;-). In
Proceedings of the 1st international CIKM workshop
on Topic-sentiment analysis for mass opinion, TSA
2009.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
713
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Henry S. Cheang and Marc D. Pell. 2008. The sound of
sarcasm. Speech Commun., 50(5):366?381, May.
Henry S. Cheang and Marc D. Pell. 2009. Acous-
tic markers of sarcasm in cantonese and english.
The Journal of the Acoustical Society of America,
126(3):1394?1405.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL 2010.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12).
Roger J. Kreuz Gina M. Caucci. 2012. Social and par-
alinguistic cues to sarcasm. online 08/02/2012, 25:1?
22, February.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Lars Kai Hansen, Adam Arvidsson, Finn Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news - affect and virality in twitter. In
The 2011 International Workshop on Social Comput-
ing, Network, and Services (SocialComNet 2011).
Roger Kreuz and Gina Caucci. 2007. Lexical influences
on the perception of sarcasm. In Proceedings of the
Workshop on Computational Approaches to Figurative
Language.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for detect-
ing sarcasm in tweets #not. In Proceedings of the 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, WASSA
2013.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005).
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media.
Finn Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages (http://arxiv.org/abs/1103.2903).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In The 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013).
Katherine P. Rankin, Andrea Salazar, Maria Luisa Gorno-
Tempini, Marc Sollberger, Stephen M. Wilson, Dani-
jela Pavlic, Christine M. Stanley, Shenly Glenn,
Michael W. Weiner, and Bruce L. Miller. 2009. De-
tecting sarcasm from paralinguistic cues: Anatomic
and cognitive correlates in neurodegenerative disease.
Neuroimage, 47:2005?2015.
Joseph Tepperman, David Traum, and Shrikanth
Narayanan. 2006. ?Yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings of
the INTERSPEECH 2006 - ICSLP, Ninth International
Conference on Spoken Language Processing.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Product
Reviews. In Proceedings of the Fourth International
Conference on Weblogs and Social Media (ICWSM-
2010), ICWSM 2010.
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative adjectives: An unsuper-
vised criterion to extract subjective adjectives. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 534?
539, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations, pages 34?35, Vancouver,
Canada, October.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the 2005
Human Language Technology Conference / Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
714
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1557?1562,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Classifying Message Board Posts with an Extracted Lexicon of Patient
Attributes
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh, riloff}@cs.utah.edu
Abstract
The goal of our research is to distinguish vet-
erinary message board posts that describe a
case involving a specific patient from posts
that ask a general question. We create a text
classifier that incorporates automatically gen-
erated attribute lists for veterinary patients to
tackle this problem. Using a small amount of
annotated data, we train an information extrac-
tion (IE) system to identify veterinary patient
attributes. We then apply the IE system to a
large collection of unannotated texts to pro-
duce a lexicon of veterinary patient attribute
terms. Our experimental results show that us-
ing the learned attribute lists to encode pa-
tient information in the text classifier yields
improved performance on this task.
1 Introduction
Our research focuses on the problem of classify-
ing message board posts in the domain of veterinary
medicine. Most of the posts in our corpus discuss a
case involving a specific patient, which we will call
patient-specific posts. But there are also posts that
ask a general question, for example to seek advice
about different medications, information about new
procedures, or how to perform a test. Our goal is
to distinguish the patient-specific posts from general
posts so that they can be automatically routed to dif-
ferent message board folders.
Distinguishing patient-specific posts from general
posts is a challenging problem for two reasons. First,
virtually any medical topic can appear in either type
of post, so the vocabulary is very similar. Second,
a highly skewed distribution exists between patient-
specific posts and general posts. Almost 90% of the
posts in our data are about specific patients.
With such a highly skewed distribution, it would
seem logical to focus on recognizing instances of the
minority class. But the distinguishing characteristic
of a general post is the absence of a patient. Two
nearly identical posts belong in different categories
if one mentions a patient and the other does not.
Consequently, our aim is to create features that iden-
tify references to a specific patient and use these to
more accurately distinguish the two types of posts.
Our research explores the use of information ex-
traction (IE) techniques to automatically identify
common attributes of veterinary patients, which we
use to encode patient information in a text classifier.
Our approach involves three phases. First, we train
a conditional random fields (CRF) tagger to iden-
tify seven common types of attributes that are of-
ten ascribed to veterinary patients: SPECIES/BREED,
NAME, AGE, GENDER, WEIGHT, POSSESSOR, and
DISEASE/SYMPTOM. Second, we apply the CRF
tagger to a large set of unannotated message board
posts, collect its extractions, and harvest the most
frequently extracted terms to create a Veterinary Pa-
tient Attribute (VPA) Lexicon.
Finally, we define three types of features that ex-
ploit the harvested VPA lexicon. These features rep-
resent the patient attribute terms, types, and com-
binations of them to help the classifier determine
whether a post is discussing a specific patient. We
conduct experiments which show that the extracted
patient attribute information improves text classifi-
cation performance on this task.
1557
2 Related Work
Our work demonstrates the use of information ex-
traction techniques to benefit a text classification ap-
plication. There has been a great deal of research on
text classification (e.g., (Borko and Bernick, 1963;
Hoyle, 1973; Joachims, 1998; Nigam et al, 2000;
Sebastiani, 2002)), which most commonly has used
bag-of-word features. Researchers have also inves-
tigated clustering (Baker and McCallum, 1998), La-
tent Semantic Indexing (LSI) (Zelikovitz and Hirsh,
2001), Latent Dirichlet Allocation (LDA) (Br et al,
2008) and string kernels (Lodhi et al, 2001). Infor-
mation extraction techniques have been used previ-
ously to create richer features for event-based text
classification (Riloff and Lehnert, 1994) and web
page classification (Furnkranz et al, 1998). Se-
mantic information has also been incorporated for
text classification. However, most previous work re-
lies on existing semantic resources, such as Wordnet
(Scott and Stan, 1998; Bloehdorn and Hotho, 2006)
or Wikipedia (Wang et al, 2009).
There is also a rich history of automatic lexicon
induction from text corpora (e.g., (Roark and Char-
niak, 1998; Riloff and Jones, 1999; McIntosh and
Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel,
2009)), and the Web (e.g., (Etzioni et al, 2005;
Kozareva et al, 2008; Carlson et al, 2010)). The
novel aspects of our work are in using an IE tagger
to harvest a domain-specific lexicon from unanno-
tated texts, and using the induced lexicon to encode
domain-specific features for text classification.
3 Text Classification with Extracted
Patient Attributes
This resesarch studies message board posts from the
Veterinary Information Network (VIN), which is a
web site (www.vin.com) for professionals in veteri-
nary medicine. VIN hosts forums where veterinar-
ians discuss medical issues, challenging cases, etc.
We observed that patient-specific veterinary posts
almost always include some basic facts about the
patient, such as the animal?s breed, age, or gender.
It is also common to mention the patient?s owner
(e.g., ?a new client?s cat?) or a disease or symptom
that the patient has (e.g., ?a diabetic cat?). General
posts almost never contain this information.
Although some of these terms can be found in
existing resources such as Wordnet (Miller, 1990),
our veterinary message board posts are filled with
informal and unconventional vocabulary. For ex-
ample, one might naively assume that ?male? and
?female? are sufficient to identify gender. But the
gender of animals is often revealed by describing
their spayed/neutered status, often indicated with
shorthand notations. For example, ?m/n? means
male and neutered, ?fs? means female spayed, ?cas-
trated? means neutered and implies male. Short-
hand terms and informal jargon are also frequently
used for breeds (e.g., ?doxy? for dachsund, ?labx?
for labrador cross, ?gshep? for German Shepherd)
and ages (e.g., ?3-yr-old?, ?3yo?, ?3mo?). A par-
ticularly creative age expression describes an animal
as (say) ?a 1999 model? (i.e., born in 1999). To rec-
ognize the idiosyncratic vocabulary in these texts,
we use information extraction techniques to identify
terms corresponding to seven attributes of veterinary
patients: SPECIES/BREED, NAME, AGE, WEIGHT,
GENDER, POSSESSOR, and DISEASE/SYMPTOM.
Figure 1 illustrates our overall approach, which
consists of three steps. First, we train a sequential
IE tagger to label veterinary patient attributes using
supervised learning. Second, we apply the tagger
to 10,000 unannotated message board posts to auto-
matically create a Veterinary Patient Attribute (VPA)
Lexicon. Third, we use the VPA Lexicon to encode
patient attribute features in a document classifier.
Unannotated
Texts
PI Sentence 
Classifier
VPA Tagger
(CRF)
VPA
Lexicon
Step 2
PI Sentence 
Classifier
VPA Tagger
(CRF)
Annotated
Texts
Step 1
Annotated
Texts
VPA
Lexicon
Document
Classifier
Step 3
Figure 1: Flowchart for Creating a Patient-Specific vs.
General Document Classifier
3.1 Patient Attribute Tagger
The first component of our system is a tagger that
labels veterinary patient attributes. To train the tag-
ger, we need texts labeled with patient attributes.
1558
The message board posts can be long and tedious
to read (i.e., they are often filled with medical his-
tory and test results), so manually annotating every
word would be arduous. However, the patient is usu-
ally described at the beginning of a post, most com-
monly in 1-2 ?introductory? sentences. Therefore
we adopted a two stage process, both for manual and
automatic tagging of patient attributes.
First, we created annotation guidelines to iden-
tify ?patient introductory? (PI) sentences, which we
defined as sentences that introduce a patient to the
reader by providing a general (non-medical) descrip-
tion of the animal (e.g., ?I was presented with a m/n
Siamese cat that is lethargic.?) We randomly se-
lected 300 posts from our text collection and asked
two human annotators to manually identify the PI
sentences. We measured their inter-annotator agree-
ment using Cohen?s kappa (?) and their agreement
was ?=.93. The two annotators then adjudicated
their differences to create our gold standard set of PI
sentence annotations. 269 of the 300 posts contained
at least one PI sentence , indicating that 89.7% of the
posts mention a specific patient. The remaining 31
posts (10.3%) are general in nature.
Second, the annotators manually labeled the
words in these PI sentences with respect to the 7 vet-
erinary patient attributes. On 50 randomly selected
texts, the annotators achieved an inter-annotator
agreement of ? = .89. The remaining 250 posts were
then annotated with patient attributes (in the PI sen-
tences), providing us with gold standard attribute an-
notations for all 300 posts. To illustrate, the sentence
below would have the following labels:
Daisyname is a 10yrage oldage labspecies
We used these 300 annotated posts to train both
a PI sentence classifier and a patient attribute tag-
ger. The PI sentence classifier is a support vector
machine (SVM) with a linear kernel (Keerthi and
DeCoste, 2005), unigram and bigram features, and
binary feature values. The PI sentences are the posi-
tive training instances, and the sentences in the gen-
eral posts are negative training instances.
For the tagger, we trained a single conditional ran-
dom fields (CRF) model to label all 7 types of pa-
tient attributes using the CRF++ package (Lafferty
et al, 2001). We defined features for the word string
and the part-of-speech tags of the targeted word, two
words on its left, and two words on its right.
Given new texts to process, we first apply the PI
sentence classifier to identify sentences that intro-
duce a patient. These sentences are given to the pa-
tient attribute tagger, which labels the words in those
sentences for the 7 patient attribute categories.
To evaluate the performance of the patient at-
tribute tagger, we randomly sampled 200 of the 300
annotated documents to use as training data and used
the remaining 100 documents for testing. For this
experiment, we only applied the CRF tagger to the
gold standard PI sentences, to eliminate any con-
founding factors from the PI sentence classifier. Ta-
ble 1 shows the performance of the CRF tagger in
terms of Recall (%), Precision (%), and F Score (%).
Its precision is consistently high, averaging 91%
across all seven attributes. But the average recall is
only 47%, with only one attribute (AGE) achieving
recall ? 80%. Nevertheless, the CRF?s high preci-
sion justifies our plan to use the CRF tagger to har-
vest additional attribute terms from a large collection
of unannotated texts. As we will see in Section 4,
the additional terms harvested from the unannotated
texts provide substantially more attribute informa-
tion for the document classifier to use.
Attribute Rec Prec F
SPECIES/BREED 59 93 72
NAME 62 100 76
POSSESSOR 12 100 21
AGE 80 91 85
GENDER 59 81 68
WEIGHT 19 100 32
DISEASE/SYMPTOM 35 73 47
Average 47 91 62
Table 1: Patient Attribute Tagger Evaluation
3.2 Creating a Veterinary Patient Attribute
(VPA) Lexicon
The patient attribute tagger was trained with super-
vised learning, so its ability to recognize important
words is limited by the scope of its training set.
Since we had an additional 10,000 unannotated vet-
erinary message board posts, we used the tagger to
acquire a large lexicon of patient attribute terms.
We applied the PI sentence classifier to all 10,000
texts and then applied the patient attribute tagger to
each PI sentence. The patient attribute tagger is not
1559
perfect, so we assumed that words tagged with the
same attribute value at least five times1 are most
likely to be correct and harvested them to create a
veterinary patient attribute (VPA) lexicon. This pro-
duced a VPA lexicon of 592 words. Table 2 shows
examples of learned terms for each attribute, with
the total number of learned words in parentheses.
Species/Breed (177): DSH, Schnauzer, kitty, Bengal,
pug, Labrador, siamese, Shep, miniature, golden, lab,
Spaniel, Westie, springer, Chow, cat, Beagle, Mix, ...
Name (53): Lucky, Shadow, Toby, Ginger, Boo, Max,
Baby, Buddy, Tucker, Gracie, Maggie, Willie, Tiger,
Sasha, Rusty, Beau, Kiki, Oscar, Harley, Scooter, ...
Age (59): #-year, adult, young, YO, y/o, model, wk,
y.o., yr-old, yrs, y, #-yr, #-month, #m, mo, mth, ...
Gender (39): F/s, speyed, neutered, spayed, N/M,
FN, CM, F, mc, mn, SF, male, fs, M/N, Female,
S, S/F, m/n, m/c, intact, M, NM, castrated, ...
Weight (5): lb, lbs, pound, pounds, kg
Possessor (7): my, owner, client, technician, ...
Disease/Symptom (252): abscess, fever, edema,
hepatic, inappetance, sneezing, blindness, pain,
persistent, mass, insufficiency, acute, poor, ...
Table 2: Examples from the Induced VPA Lexicon
3.3 Text Classification with Patient Attributes
Our ultimate goal is to incorporate patient attribute
information into a text classifier to help it distinguish
between patient-specific posts and general posts. We
designed three sets of features:
Attribute Types: We create one feature for each
attribute type, indicating whether a word of that at-
tribute type appeared or not.
Attribute Types with Neighbor: For each word la-
beled as a patient attribute, we create two features
by pairing its Attribute Type with a preceding or fol-
lowing word. For example, given the sentence: ?The
tiny Siamese kitten was lethargic.?, if ?Siamese? has
attribute type SPECIES then we create two features:
<tiny, SPECIES> and <SPECIES, kitten>.
Attribute Pairs: We create features for all pairs of
patient attribute words that occur in the same sen-
tence. For each pair, we create one feature repre-
1After our text classification experiments were done, we re-
ran the experiments with the unigrams+lexicon classifier using
thresholds ranging from 1 to 10 for lexicon creation, just to see
how much difference this threshold made. We found that values
? 5 produced nearly identical classification results.
senting the words themselves and one feature repre-
senting the attribute types of the words.
4 Evaluation
To create a blind test set for evaluation, our anno-
tators labeled an additional 500 posts as patient-
specific or general. Specifically, they labeled those
500 posts with PI sentences. The absence of a PI
sentence meant that the post was general. Of the 500
texts, 48 (9.6%) were labeled as general posts. We
evaluated the performance of the PI sentence classi-
fier on this test set and found that it achieved 88% ac-
curacy at identifying patient introductory sentences.
We then conducted a series of experiments for the
document classification task: distinguishing patient-
specific message board posts from general posts.
All of our experiments used support vector machine
(SVM) classifiers with a linear kernel, and ran 10-
fold cross validation on our blind test set of 500
posts. We report Recall (%), Precision (%), and F
score (%) results for the patient-specific posts and
general posts separately, and for the macro-averaged
score across both classes. For the sake of complete-
ness, we also show overall Accuracy (%) results.
However, we will focus attention on the results for
the general posts, since our main goal is to improve
performance at recognizing this minority class.
As a baseline, we created SVM classifiers using
unigram features.2 We tried binary, frequency, and
tf-idf feature values. The first three rows of Table 3
show that binary feature values performed the best,
yielding a macro-averaged F score of 81% but iden-
tifying only 54% of the general posts.
The middle section of Table 3 shows the perfor-
mance of SVM classifiers using our patient attribute
features. We conducted three experiments: apply-
ing the CRF tagger to PI sentences (per its design),
and labeling words with the VPA lexicon either on
all sentences or only on PI sentences (as identi-
fied by the PI sentence classifier). The CRF fea-
tures produced extremely low recall and precision
on the general posts. The VPA lexicon performed
best when applied only to PI sentences and pro-
duced much higher recall than all of the other clas-
sifiers, although with lower precision than the two
2We also tried unigrams + bigrams, but they did not perform
better.
1560
Patient-Specific Posts General Posts Macro Avg
Method Rec Prec F Rec Prec F Rec Prec F Acc
Unigram Features
Unigrams (freq) 96 96 96 58 60 59 77 76 77 92
Unigrams (tf-idf) 99 93 96 33 84 48 66 89 76 93
Unigrams (binary) 98 95 97 54 79 64 76 87 81 94
Patient Attribute Features
CRF Features (PI Sents) 99 91 95 02 25 04 51 58 54 90
VPA Lexicon Features (All Sents) 96 96 96 60 63 62 78 79 79 93
VPA Lexicon Features (PI Sents) 96 98 97 81 66 73 88 82 85 94
Unigram & Patient Attribute Features
CRF Features (PI Sents) 97 96 97 60 71 65 79 83 81 94
VPA Lexicon Features (PI Sents) 98 98 98 79 78 78 88 88 88 96
Table 3: Experimental Results
best unigram-based SVMs.
The bottom section of Table 3 shows results for
classifiers with both unigrams (binary) and patient
attribute features. Using the CRF features increases
recall on the general posts from 54 ? 60, but de-
creases precision from 79 ? 71. Using the patient
attribute features from the VPA lexicon yields a sub-
stantial improvement. Recall improves from 54 ?
79 and precision is just one point lower. Overall, the
macro-averaged F score across the two categories
jumps from 81% to 88%.
We performed paired bootstrap testing (Berg-
Kirkpatrick et al, 2012)) to determine whether the
SVM with unigrams and VPA lexicon features is
statistically significantly better than the best SVM
with only unigram features (binary). The SVM with
unigrams and VPA lexicon features produces sig-
nificantly better F scores at the p < 0.05 level for
general post classification as well as the macro av-
erage. The F score for patient-specific classification
and overall accuracy are statistically significant at
the p < 0.10 level.
Attribute CRF VPA
Tagger Lexicon
SPECIES/BREED 270 1045
NAME 36 43
POSSESSOR 12 233
AGE 545 1773
GENDER 153 338
WEIGHT 27 83
DISEASE/SYMPTOM 220 2673
Table 4: Number of Attributes Labeled in Test Set
Finally, we did an analysis to understand why the
VPA lexicon was so much more effective than the
CRF tagger when used to create features for text
classification. Table 4 shows the number of words
in PI sentences (identified by the classifier) of the
test set that were labeled as patient attributes by the
CRF tagger or the VPA lexicon. The VPA lexicon
clearly labeled many more terms, and the additional
coverage made a big difference for the text classifier.
5 Conclusions
This work demonstrated how annotated data can be
leveraged to automatically harvest a domain-specific
lexicon from a large collection of unannotated texts.
Our induced VPA lexicon was then used to create
patient attribute features that improved the ability of
a document classifier to distinguish between patient-
specific message board posts and general posts. We
believe that this approach could also be used to cre-
ate specialized lexicons for many other domains and
applications. A key benefit of inducing lexicons
from unannotated texts is that they provide addi-
tional vocabulary coverage beyond the terms found
in annotated data sets, which are usually small.
6 Acknowledgements
This material is based upon work supported by
the National Science Foundation under grant IIS-
1018314. We are very grateful to the Veterinary In-
formation Network for providing us with samples of
their data.
1561
References
D. Baker and A. McCallum. 1998. Distributional cluster-
ing of words for text classification. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval.
T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An
Empirical Investigation of Statistical Significance in
NLP. In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing.
S. Bloehdorn and A. Hotho. 2006. Boosting for text
classification with semantic features. In Advances in
Web mining and Web usage Analysis.
H. Borko and M. Bernick. 1963. Automatic Document
Classification. J. ACM, 10(2):151?162.
I. Br, J. Szab, and A. Benczr. 2008. Latent dirichlet alo-
cation in web spam filtering. In Proceedings of the 4th
international workshop on Adversarial information re-
trieval on the web.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, R. Es-
tevam, J. Hruschka, and T. Mitchell. 2010. Toward
an Architecture for Never-Ending Language Learning.
In Proceedings of the Twenty-Fourth National Confer-
ence on Artificial Intelligence.
O. Etzioni, M. Cafarella, A. Popescu, T. Shaked,
S. Soderland, D. Weld, and A. Yates. 2005. Unsuper-
vised Named-Entity Extraction from the Web: An Ex-
perimental Study. Artificial Intelligence, 165(1):91?
134.
J. Furnkranz, T. Mitchell, and E. Riloff. 1998. A Case
Study in Using Linguistic Phrases for Text Catego-
rization from the WWW. In Working Notes of the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
W. Hoyle. 1973. Automatic Indexing and Generation
of Classification Systems by Algorithm. Information
Storage and Retrieval, 9(4):233?242.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the European Conference on Ma-
chine Learning (ECML).
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning.
H. Lodhi, J. Shawe-Taylor, N. Christianini, and
C. Watkins. 2001. Text classification using string ker-
nels. In Advances in Neural Information Processing
Systems (NIPS).
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Unla-
beled Documents using EM. Machine Learning, 39(2-
3):103?134, May.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and W. Lehnert. 1994. Information Ex-
traction as a Basis for High-Precision Text Classifi-
cation. ACM Transactions on Information Systems,
12(3):296?333, July.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
S. Scott and M. Stan. 1998. Text classification using
WordNet hypernyms. In In Use of WordNet in Natu-
ral Language Processing Systems: Proceedings of the
Conference.
F. Sebastiani. 2002. Machine learning in automated text
categorization. In ACM computing surveys (CSUR).
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of North American Asso-
ciation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT-09).
P. Wang, J. Hu, H. Zeng, and Z. Chen. 2009. Using
Wikipedia knowledge to improve text classification.
In Knowledge and Information Systems.
S. Zelikovitz and H. Hirsh. 2001. Using LSI for text
classication in the presence of background text. In
Proceedings of the 10th International Conference on
Information and Knowledge Management (CIKM).
1562
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286?295,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapped Training of Event Extraction Classifiers
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
Most event extraction systems are trained
with supervised learning and rely on a col-
lection of annotated documents. Due to
the domain-specificity of this task, event
extraction systems must be retrained with
new annotated data for each domain. In
this paper, we propose a bootstrapping so-
lution for event role filler extraction that re-
quires minimal human supervision. We aim
to rapidly train a state-of-the-art event ex-
traction system using a small set of ?seed
nouns? for each event role, a collection
of relevant (in-domain) and irrelevant (out-
of-domain) texts, and a semantic dictio-
nary. The experimental results show that
the bootstrapped system outperforms previ-
ous weakly supervised event extraction sys-
tems on the MUC-4 data set, and achieves
performance levels comparable to super-
vised training with 700 manually annotated
documents.
1 Introduction
Event extraction systems process stories about
domain-relevant events and identify the role fillers
of each event. A key challenge for event extrac-
tion is that recognizing role fillers is inherently
contextual. For example, a PERSON can be a
perpetrator or a victim in different contexts (e.g.,
?John Smith assassinated the mayor? vs. ?John
Smith was assassinated?). Similarly, any COM-
PANY can be an acquirer or an acquiree depending
on the context.
Many supervised learning techniques have
been used to create event extraction systems us-
ing gold standard ?answer key? event templates
for training (e.g., (Freitag, 1998a; Chieu and Ng,
2002; Maslennikov and Chua, 2007)). How-
ever, manually generating answer keys for event
extraction is time-consuming and tedious. And
more importantly, event extraction annotations
are highly domain-specific, so new annotations
must be obtained for each domain.
The goal of our research is to use bootstrap-
ping techniques to automatically train a state-of-
the-art event extraction system without human-
generated answer key templates. The focus of our
work is the TIER event extraction model, which
is a multi-layered architecture for event extrac-
tion (Huang and Riloff, 2011). TIER?s innova-
tion over previous techniques is the use of four
different classifiers that analyze a document at in-
creasing levels of granularity. TIER progressively
zooms in on event information using a pipeline
of classifiers that perform document-level classi-
fication, sentence classification, and noun phrase
classification. TIER outperformed previous event
extraction systems on the MUC-4 data set, but re-
lied heavily on a large collection of 1,300 docu-
ments coupled with answer key templates to train
its four classifiers.
In this paper, we present a bootstrapping solu-
tion that exploits a large unannotated corpus for
training by using role-identifying nouns (Phillips
and Riloff, 2007) as seed terms. Phillips and
Riloff observed that some nouns, by definition,
refer to entities or objects that play a specific role
in an event. For example, ?assassin?, ?sniper?,
and ?hitman? refer to people who play the role
of PERPETRATOR in a criminal event. Similarly,
?victim?, ?casualty?, and ?fatality? refer to peo-
ple who play the role of VICTIM, by virtue of
their lexical semantics. Phillips and Riloff called
these words role-identifying nouns and used them
286
to learn extraction patterns. Our research also
uses role-identifying nouns to learn extraction pat-
terns, but the role-identifying nouns and patterns
are then used to create training data for event ex-
traction classifiers. Each classifier is then self-
trained in a bootstrapping loop.
Our weakly supervised training procedure re-
quires a small set of ?seed nouns? for each event
role, and a collection of relevant (in-domain) and
irrelevant (out-of-domain) texts. No answer key
templates or annotated texts are needed. The seed
nouns are used to automatically generate a set
of role-identifying patterns, and then the nouns,
patterns, and a semantic dictionary are used to
label training instances. We also propagate the
event role labels across coreferent noun phrases
within a document to produce additional train-
ing instances. The automatically labeled texts are
used to train three components of TIER: its two
types of sentence classifiers and its noun phrase
classifiers. To create TIER?s fourth component,
its document genre classifier, we apply heuristics
to the output of the sentence classifiers.
We present experimental results on the MUC-
4 data set, which is a standard benchmark for
event extraction research. Our results show that
the bootstrapped system, TIERlite, outperforms
previous weakly supervised event extraction sys-
tems and achieves performance levels comparable
to supervised training with 700 manually anno-
tated documents.
2 Related Work
Event extraction techniques have largely focused
on detecting event ?triggers? with their arguments
for extracting role fillers. Classical methods are
either pattern-based (Kim and Moldovan, 1993;
Riloff, 1993; Soderland et al 1995; Huffman,
1996; Freitag, 1998b; Ciravegna, 2001; Califf and
Mooney, 2003; Riloff, 1996; Riloff and Jones,
1999; Yangarber et al 2000; Sudo et al 2003;
Stevenson and Greenwood, 2005) or classifier-
based (e.g., (Freitag, 1998a; Chieu and Ng, 2002;
Finn and Kushmerick, 2004; Li et al 2005; Yu et
al., 2005)).
Recently, several approaches have been pro-
posed to address the insufficiency of using only
local context to identify role fillers. Some ap-
proaches look at the broader sentential context
around a potential role filler when making a de-
cision (e.g., (Gu and Cercone, 2006; Patwardhan
and Riloff, 2009)). Other systems take a more
global view and consider discourse properties of
the document as a whole to improve performance
(e.g., (Maslennikov and Chua, 2007; Ji and Gr-
ishman, 2008; Liao and Grishman, 2010; Huang
and Riloff, 2011)). Currently, the learning-based
event extraction systems that perform best all use
supervised learning techniques that require a large
number of texts coupled with manually-generated
annotations or answer key templates.
A variety of techniques have been explored
for weakly supervised training of event extrac-
tion systems, primarily in the realm of pattern or
rule-based approaches (e.g., (Riloff, 1996; Riloff
and Jones, 1999; Yangarber et al 2000; Sudo et
al., 2003; Stevenson and Greenwood, 2005)). In
some of these approaches, a human must man-
ually review and ?clean? the learned patterns to
obtain good performance. Research has also been
done to learn extraction patterns in an unsuper-
vised way (e.g., (Shinyama and Sekine, 2006;
Sekine, 2006)). But these efforts target open do-
main information extraction. To extract domain-
specific event information, domain experts are
needed to select the pattern subsets to use.
There have also been weakly supervised ap-
proaches that use more than just local context.
(Patwardhan and Riloff, 2007) uses a semantic
affinity measure to learn primary and secondary
patterns, and the secondary patterns are applied
only to event sentences. The event sentence clas-
sifier is self-trained using seed patterns. Most
recently, (Chambers and Jurafsky, 2011) acquire
event words from an external resource, group the
event words to form event scenarios, and group
extraction patterns for different event roles. How-
ever, these weakly supervised systems produce
substantially lower performance than the best su-
pervised systems.
3 Overview of TIER
The goal of our research is to develop a weakly
supervised training process that can successfully
train a state-of-the-art event extraction system for
a new domain with minimal human input. We de-
cided to focus our efforts on the TIER event ex-
traction model because it recently produced bet-
ter performance on the MUC-4 data set than prior
learning-based event extraction systems (Huang
and Riloff, 2011). In this section, we briefly give
an overview of TIER?s architecture and its com-
287
Figure 1: TIER Overview
ponents.
TIER is a multi-layered architecture for event
extraction, as shown in Figure 1. Documents pass
through a pipeline where they are analyzed at dif-
ferent levels of granularity, which enables the sys-
tem to gradually ?zoom in? on relevant facts. The
pipeline consists of a document genre classifier,
two types of sentence classifiers, and a set of noun
phrase (role filler) classifiers.
The lower pathway in Figure 1 shows that all
documents pass through an event sentence clas-
sifier. Sentences labeled as event descriptions
then proceed to the noun phrase classifiers, which
are responsible for identifying the role fillers in
each sentence. The upper pathway in Figure 1 in-
volves a document genre classifier to determine
whether a document is an ?event narrative? story
(i.e., an article that primarily discusses the details
of a domain-relevant event). Documents that are
classified as event narratives warrant additional
scrutiny because they most likely contain a lot of
event information. Event narrative stories are pro-
cessed by an additional set of role-specific sen-
tence classifiers that look for role-specific con-
texts that will not necessarily mention the event.
For example, a victim may be mentioned in a sen-
tence that describes the aftermath of a crime, such
as transportation to a hospital or the identifica-
tion of a body. Sentences that are determined to
have ?role-specific? contexts are passed along to
the noun phrase classifiers for role filler extrac-
tion. Consequently, event narrative documents
pass through both the lower pathway and the up-
per pathway. This approach creates an event ex-
traction system that can discover role fillers in a
variety of different contexts by considering the
type of document being processed.
TIER was originally trained with supervised
learning using 1,300 texts and their corresponding
answer key templates from the MUC-4 data set
(MUC-4 Proceedings, 1992). Human-generated
answer key templates are expensive to produce
because the annotation process is both difficult
and time-consuming. Furthermore, answer key
templates for one domain are virtually never
reusable for different domains, so a new set of
answer keys must be produced from scratch for
each domain. In the next section, we present our
weakly supervised approach for training TIER?s
event extraction classifiers.
4 Bootstrapped Training of Event
Extraction Classifiers
We adopt a two-phase approach to train TIER?s
event extraction modules using minimal human-
generated resources. The goal of the first phase
is to automatically generate positive training ex-
amples using role-identifying seed nouns as input.
The seed nouns are used to automatically gener-
ate a set of role-identifying patterns for each event
role. Each set of patterns is then assigned a set
of semantic constraints (selectional restrictions)
that are appropriate for that event role. The se-
mantic constraints consist of the role-identifying
seed nouns as well as general semantic classes
that constrain the event role (e.g., a victim must
be a HUMAN). A noun phrase will satisfy the se-
mantic constraints if its head noun is in the seed
noun list or if it has the appropriate semantic type
(based on dictionary lookup). Each pattern is then
matched against the unannotated texts, and if the
extracted noun phrase satisfies its semantic con-
straints, then the noun phrase is automatically la-
beled as a role filler.
The second phase involves bootstrapped train-
ing of TIER?s classifiers. Using the labeled in-
stances generated in the first phase, we iteratively
train three of TIER?s components: the two types
of sentential classifiers and the noun phrase clas-
sifiers. For the fourth component, the document
classifier, we apply heuristics to the output of the
sentence classifiers to assess the density of rel-
evant sentences in a document and label high-
density stories as event narratives. In the fol-
lowing sections, we present the details of each of
these steps.
4.1 Automatically Labeling Training Data
Finding seeding instances of high precision and
reasonable coverage is important in bootstrap-
ping. However, this is especially challenging
for event extraction task because identifying role
fillers is inherently contextual. Furthermore, role
288
Figure 2: Using Basilisk to Induce Role-Identifying
Patterns
fillers occur sparsely in text and in diverse con-
texts.
In this section, we explain how we gener-
ate role-identifying patterns automatically using
seed nouns, and we discuss why we add seman-
tic constraints to the patterns when producing la-
beled instances for training. Then, we discuss the
coreference-based label propagation that we used
to obtain additional training instances. Finally, we
give examples to illustrate how we create training
instances.
4.1.1 Inducing Role-Identifying Patterns
The input to our system is a small set of
manually-defined seed nouns for each event role.
Specifically, the user is required to provide
10 role-identifying nouns for each event role.
(Phillips and Riloff, 2007) defined a noun as be-
ing ?role-identifying? if its lexical semantics re-
veal the role of the entity/object in an event. For
example, the words ?assassin? and ?sniper? are
people who participate in a violent event as a PER-
PETRATOR. Therefore, the entities referred to by
role-identifying nouns are probable role fillers.
However, treating every context surrounding a
role-identifying noun as a role-identifying pattern
is risky. The reason is that many instances of role-
identifying nouns appear in contexts that do not
describe the event. But, if one pattern has been
seen to extract many role-identifying nouns and
seldomly seen to extract other nouns, then the pat-
tern likely represents an event context.
As (Phillips and Riloff, 2007) did, we use
Basilisk to learn patterns for each event role.
Basilisk was originally designed for semantic
class learning (e.g., to learn nouns belonging to
semantic categories, such as building or human).
As shown in Figure 2, beginning with a small set
of seed nouns for each semantic class, Basilisk
learns additional nouns belonging to the same se-
mantic class. Internally, Basilisk uses extraction
patterns automatically generated from unanno-
tated texts to assess the similarity of nouns. First,
Basilisk assigns a score to each pattern based on
the number of seed words that co-occur with it.
Basilisk then collects the noun phrases extracted
by the highest-scoring patterns. Next, the head
noun of each noun phrase is assigned a score
based on the set of patterns that it co-occurred
with. Finally, Basilisk selects the highest-scoring
nouns, automatically labels them with the seman-
tic class of the seeds, adds these nouns to the lex-
icon, and restarts the learning process in a boot-
strapping fashion.
For our work, we give Basilisk role-identifying
seed nouns for each event role. We run the boot-
strapping process for 20 iterations and then har-
vest the 40 best patterns that Basilisk identifies
for each event role. We also tried using the addi-
tional role-identifying nouns learned by Basilisk,
but found that these nouns were too noisy.
4.1.2 Using the Patterns to Label NPs
The induced role-identifying patterns can be
matched against the unannotated texts to produce
labeled instances. However, relying solely on the
pattern contexts can be misleading. For example,
the pattern context <subject> caused damage
will extract some noun phrases that are weapons
(e.g., the bomb) but some noun phrases that are
not (e.g., the tsunami).
Based on this observation, we add selectional
restrictions to each pattern that requires a noun
phrase to satisfy certain semantic constraints in
order to be extracted and labeled as a positive
instances for an event role. The selectional re-
strictions are satisfied if the head noun is among
the role-identifying seed nouns or if the semantic
class of the head noun is compatible with the cor-
responding event role. In the previous example,
tsunami will not be extracted as a weapon because
it has an incompatible semantic class (EVENT),
but bomb will be extracted because it has a com-
patible semantic class (WEAPON).
We use the semantic class labels assigned by
the Sundance parser (Riloff and Phillips, 2004) in
our experiments. Sundance looks up each noun
in a semantic dictionary to assign the semantic
class labels. As an alternative, general resources
(e.g., WordNet (Miller, 1990)) or a semantic tag-
ger (e.g., (Huang and Riloff, 2010)) could be
used.
289
John Smith was killed by 
. . . . . .
 
was killed by <np>
Role?Identifying
Patterns
two armed men
1
an hour later.
Police arrested the unidentified men  
3
 
in broad daylight this morning.  
left his house to go to work about 8:00 am.
The assassins
2
attacked the mayor as he 
<subject> fired shots
men = Human
Role?Identifying  Semantic
 Dictionary 
terrorists
snipers
assassins
. . .
building = Object <subject> attacked
Noun
Constraints Constraints
Figure 3: Automatic Training Data Creation
4.1.3 Propagating Labels with Coreference
To enrich the automatically labeled training in-
stances, we also propagate the event role labels
across coreferent noun phrases within a docu-
ment. The observation is that once a noun phrase
has been identified as a role filler, its corefer-
ent mentions in the same document likely fill the
same event role since they are referring to the
same real world entity.
To leverage these coreferential contexts, we
employ a simple head noun matching heuristic to
identify coreferent noun phrases. This heuristic
assumes that two noun phrases that have the same
head noun are coreferential. We considered us-
ing an off-the-shelf coreference resolver, but de-
cided that the head noun matching heuristic would
likely produce higher precision results, which is
important to produce high-quality labeled data.
4.1.4 Examples of Training Instance
Creation
Figure 3 illustrates how we label training in-
stances automatically. The text example shows
three noun phrases that are automatically labeled
as perpetrators. Noun phrases #1 and #2 oc-
cur in role-identifying pattern contexts (was killed
by <np> and <subject> attacked) and satisfy
the semantic constraints for perpetrators because
?men? has a compatible semantic type and ?assas-
sins? is a role-identifying noun for perpetrators.
Noun phrase #3 (?the unidentified men?) does
not occur in a pattern context, but it is deemed
to be coreferent with ?two armed men? because
they have the same head noun. Consequently, we
propagate the perpetrator label from noun phrase
#1 to noun phrase #3.
4.2 Creating TIERlite with Bootstrapping
In this section, we explain how the labeled in-
stances are used to train TIER?s classifiers with
bootstrapping. In addition to the automatically
labeled instances, the training process depends
on a text corpus that consists of both relevant
(in-domain) and irrelevant (out-of-domain) doc-
uments. Positive instances are generated from
the relevant documents and negative instances are
generated by randomly sampling from the irrele-
vant documents.
The classifiers are all support vector machines
(SVMs), implemented using the SVMlin software
(Keerthi and DeCoste, 2005). When applying the
classifiers during bootstrapping, we use a sliding
confidence threshold to determine which labels
are reliable based on the values produced by the
SVM. Initially, we set the threshold to be 2.0 to
identify highly confident predictions. But if fewer
than k instances pass the threshold, then we slide
the threshold down in decrements of 0.1 until we
obtain at least k labeled instances or the thresh-
old drops below 0, in which case bootstrapping
ends. We used k=10 for both sentence classifiers
and k=30 for the noun phrase classifiers.
The following sections present the details of the
bootstrapped training process for each of TIER?s
components.
Figure 4: The Bootstrapping Process
4.2.1 Noun Phrase Classifiers
The mission of the noun phrase classifiers is to
determine whether a noun phrase is a plausible
event role filler based on the local features sur-
rounding the noun phrase (NP). A set of classifiers
is needed, one for each event role.
As shown in Figure 4, to seed the classifier
training, the positive noun phrase instances are
290
generated from the relevant documents follow-
ing Section 4.1. The negative noun phrase in-
stances are drawn randomly from the irrelevant
documents. Considering the sparsity of role fillers
in texts, we set the negative:positive ratio to be
10:1. Once the classifier is trained, it is applied to
the unlabeled noun phrases in the relevant docu-
ments. Noun phrases that are assigned role filler
labels by the classifier with high confidence (us-
ing the sliding threshold) are added to the set of
positive instances. New negative instances are
drawn randomly from the irrelevant documents to
maintain the 10:1 (negative:positive) ratio.
We extract features from each noun phrase
(NP) and its surrounding context. The features
include the NP head noun and its premodifiers.
We also use the Stanford NER tagger (Finkel et
al., 2005) to identify Named Entities within the
NP. The context features include four words to the
left of the NP, four words to the right of the NP,
and the lexico-syntactic patterns generated by Au-
toSlog to capture expressions around the NP (see
(Riloff, 1993) for details).
4.2.2 Event Sentence Classifier
The event sentence classifier is responsible
for identifying sentences that describe a relevant
event. Similar to the noun phrase classifier train-
ing, positive training instances are selected from
the relevant documents and negative instances are
drawn from the irrelevant documents. All sen-
tences in the relevant documents that contain one
or more labeled noun phrases (belonging to any
event role) are labeled as positive training in-
stances. We randomly sample sentences from the
irrelevant documents to obtain a negative:positive
training instance ratio of 10:1. The bootstrapping
process is then identical to that of the noun phrase
classifiers. The feature set for this classifier con-
sists of unigrams, bigrams and AutoSlog?s lexico-
syntactic patterns surrounding all noun phrases in
the sentence.
4.2.3 Role-Specific Sentence Classifiers
The role-specific sentence classifiers are
trained to identify the contexts specific to each
event role. All sentences in the relevant doc-
uments that contain at least one labeled noun
phrase for the appropriate event role are used
as positive instances. Negative instances are
randomly sampled from the irrelevant documents
to maintain the negative:positive ratio of 10:1.
The bootstrapping process and feature set are the
same as for the event sentence classifier.
The difference between the two types of sen-
tence classifiers is that the event sentence classi-
fier uses positive instances from all event roles,
while each role-specific sentence classifiers only
uses the positive instances for one particular event
role. The rationale is similar as in the super-
vised setting (Huang and Riloff, 2011); the event
sentence classifier is expected to generalize over
all event roles to identify event mention contexts,
while the role-specific sentence classifiers are ex-
pected to learn to identify contexts specific to in-
dividual roles.
4.2.4 Event Narrative Document Classifier
TIER also uses an event narrative document
classifier and only extracts information from role-
specific sentences within event narrative docu-
ments. In the supervised setting, TIER uses
heuristic rules derived from answer key templates
to identify the event narrative documents in the
training set, which are used to train an event nar-
rative document classifier. The heuristic rules re-
quire that an event narrative should have a high
density of relevant information and tend to men-
tion the relevant information within the first sev-
eral sentences.
In our weakly supervised setting, we use the
information density heuristic directly instead of
training an event narrative classifier. We approxi-
mate the relevant information density heuristic by
computing the ratio of relevant sentences (both
event sentences and role-specific sentences) out of
all the sentences in a document. Thus, the event
narrative labeller only relies on the output of the
two sentence classifiers. Specifically, we label a
document as an event narrative if ? 50% of the
sentences in the document are relevant (i.e., la-
beled positively by either sentence classifier).
5 Evaluation
In this section, we evaluate our bootstrapped sys-
tem, TIERlite, on the MUC-4 event extraction
data set. First, we describe the IE task, the data
set, and the weakly supervised baseline systems
that we use for comparison. Then we present the
results of our fully bootstrapped system TIERlite,
the weakly supervised baseline systems, and two
fully supervised event extraction systems, TIER
291
and GLACIER. In addition, we analyze the per-
formance of TIERlite using different configura-
tions to assess the impact of its components.
5.1 IE Task and Data
We evaluated the performance of our systems on
the MUC-4 terrorism IE task (MUC-4 Proceed-
ings, 1992) about Latin American terrorist events.
We used 1,300 texts (DEV) as our training set and
200 texts (TST3+TST4) as the test set. All the
documents have answer key templates. For the
training set, we used the answer keys to separate
the documents into relevant and irrelevant sub-
sets. Any document containing at least one rel-
evant event was considered to be relevant.
PerpInd PerpOrg Target Victim Weapon
129 74 126 201 58
Table 1: # of Role Fillers in the MUC-4 Test Set
Following previous studies, we evaluate our
system on five MUC-4 string event roles: perpe-
trator individuals (PerpInd), perpetrator organi-
zations (PerpOrg), physical targets, victims, and
weapons. Table 1 shows the distribution of role
fillers in the MUC-4 test set. The complete IE task
involves the creation of answer key templates, one
template per event1. Our work focuses on extract-
ing individual role fillers and not template genera-
tion, so we evaluate the accuracy of the role fillers
irrespective of which template they occur in.
We used the same head noun scoring scheme
as previous systems, where an extraction is cor-
rect if its head noun matches the head noun in the
answer key2. Pronouns were discarded from both
the system responses and the answer keys since
no coreference resolution is done. Duplicate ex-
tractions were conflated before being scored, so
they count as just one hit or one miss.
5.2 Weakly Supervised Baselines
We compared the performance of our system with
three previous weakly supervised event extraction
systems.
AutoSlog-TS (Riloff, 1996) generates lexico-
syntactic patterns exhaustively from unannotated
texts and ranks them based on their frequency and
probability of occurring in relevant documents.
A human expert then examines the patterns and
1Documents may contain multiple events per article.
2For example, ?armed men? will match ?5 armed men?.
manually selects the best patterns for each event
role. During testing, the patterns are matched
against unseen texts to extract event role fillers.
PIPER (Patwardhan and Riloff, 2007; Patward-
han, 2010) learns extraction patterns using a se-
mantic affinity measure, and it distinguishes be-
tween primary and secondary patterns and ap-
plies them selectively. (Chambers and Jurafsky,
2011) (C+J) created an event extraction system
by acquiring event words from WordNet (Miller,
1990), clustering the event words into different
event scenarios, and grouping extraction patterns
for different event roles.
5.3 Performance of TIERlite
Table 2 shows the seed nouns that we used in our
experiments, which were generated by sorting the
nouns in the corpus by frequency and manually
identifying the first 10 role-identifying nouns for
each event role.3 Table 3 shows the number of
training instances (noun phrases) that were auto-
matically labeled for each event role using our
training data creation approach (Section 4.1).
Event Role Seed Nouns
Perpetrator terrorists assassins criminals rebels
Individual murderers death squads guerrillas
member members individuals
Perpetrator FMLN ELN FARC MRTA M-19 Front
Organization Shining Path Medellin Cartel
The Extraditables
Army of National Liberation
Target houses residence building home homes
offices pipeline hotel car vehicles
Victim victims civilians children jesuits Galan
priests students women peasants Romero
Weapon weapons bomb bombs explosives rifles
dynamite grenades device car bomb
Table 2: Role-Identifying Seed Nouns
PerpInd PerpOrg Target Victim Weapon
296 157 522 798 248
Table 3: # of Automatically Labeled NPs
Table 4 shows how our bootstrapped system
TIERlite compares with previous weakly super-
vised systems and two supervised systems, its su-
pervised counterpart TIER (Huang and Riloff,
2011) and a model that jointly considers local
and sentential contexts, GLACIER (Patwardhan
3We only found 9 weapon terms among the high-
frequency terms.
292
Weakly Supervised Baselines
PerpInd PerpOrg Target Victim Weapon Average
AUTOSLOG-TS (1996) 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
PIPERBest (2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/46/45
C+J (2011) - - - - - 44/36/40
Supervised Models
GLACIER (2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
TIER (2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
Weakly Supervised Models
TIERlite 47/51/49 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
Table 4: Performance of the Bootstrapped Event Extraction System (Precision/Recall/F-score)
0 200 400 600 800 1000 1200 1400
30
35
40
45
50
55
60
# of training documents
IE
 pe
rfo
rm
an
ce
(F1
)
Figure 5: The Learning Curve of Supervised TIER
and Riloff, 2009). We see that TIERlite outper-
forms all three weakly supervised systems, with
slightly higher precision and substantially more
recall. When compared to the supervised sys-
tems, the performance of TIERlite is similar to
GLACIER, with comparable precision but slightly
lower recall. But the supervised TIER system,
which was trained with 1,300 annotated docu-
ments, is still superior, especially in recall.
Figure 5 shows the learning curve for TIER
when it is trained with fewer documents, rang-
ing from 100 to 1,300 in increments of 100. Each
data point represents five experiments where we
randomly selected k documents from the train-
ing set and averaged the results. The bars show
the range of results across the five runs. Figure 5
shows that TIER?s performance increases from an
F score of 34 when trained on just 100 documents
up to an F score of 56 when training on 1,300 doc-
uments. The circle shows the performance of our
bootstrapped system, TIERlite, which achieves an
F score comparable to supervised training with
about 700 manually annotated documents.
5.4 Analysis
Table 6 shows the effect of the coreference prop-
agation step described in Section 4.1.3 as part of
training data creation. Without this step, the per-
formance of the bootstrapped system yields an F
score of 41. With the benefit of the additional
training instances produced by coreference prop-
agation, the system yields an F score of 53. The
new instances produced by coreference propaga-
tion seem to substantially enrich the diversity of
the set of labeled instances.
Seeding P/R/F
wo/Coref 45/38/41
w/Coref 47/53/50
Table 6: Effects of Coreference Propagation
In the evaluation section, we saw that the su-
pervised event extraction systems achieve higher
recall than the weakly supervised systems. Al-
though our bootstrapped event extraction sys-
tem TIERlite produces higher recall than previ-
ous weakly supervised systems, a substantial re-
call gap still exists.
Considering the pipeline structure of the event
extraction system, as shown in Figure 1, the noun
phrase extractors are responsible for identifying
all candidate role fillers. The sentential classifiers
and the document classifier effectively serve as
filters to rule out candidates from irrelevant con-
texts. Consequently, there is no way to recover
missing recall (role fillers) if the noun phrase ex-
tractors fail to identify them.
Since the noun phrase classifiers are so central
to the performance of the system, we compared
the performance of the bootstrapped noun phrase
classifiers directly with their supervised conter-
parts. The results are shown in Table 5. Both sets
of classifiers produce low precision when used in
isolation, but their precision levels are compara-
293
PerpInd PerpOrg Target Victim Weapon Average
Supervised Classifier 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
Bootstrapped Classifier 30/54/39 37/53/44 30/71/42 28/63/39 36/57/44 32/60/42
Table 5: Evaluation of Bootstrapped Noun Phrase Classifiers (Precision/Recall/F-score)
ble. The TIER pipeline architecture is successful
at eliminating many of the false hits. However,
the recall of the bootstrapped classifiers is consis-
tently lower than the recall of the supervised clas-
sifiers. Specifically, the recall is about 10 points
lower for three event roles (PerpInd, Target and
Victim) and 20 points lower for the other two event
roles (PerpOrg and Weapon). These results sug-
gest that our bootstrapping approach to training
instance creation does not fully capture the diver-
sity of role filler contexts that are available in the
supervised training set of 1,300 documents. This
issue is an interesting direction for future work.
6 Conclusions
We have presented a bootstrapping approach for
training a multi-layered event extraction model
using a small set of ?seed nouns? for each event
role, a collection of relevant (in-domain) and ir-
relevant (out-of-domain) texts and a semantic dic-
tionary. The experimental results show that the
bootstrapped system, TIERlite, outperforms pre-
vious weakly supervised event extraction sys-
tems on a standard event extraction data set, and
achieves performance levels comparable to super-
vised training with 700 manually annotated docu-
ments. The minimal supervision required to train
such a model increases the portability of event ex-
traction systems.
7 Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation under grant IIS-
1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading
Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0172.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the U.S. govern-
ment.
References
M.E. Califf and R. Mooney. 2003. Bottom-up Re-
lational Learning of Pattern Matching rules for In-
formation Extraction. Journal of Machine Learning
Research, 4:177?210.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-Based Information Extraction without the
Templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy
Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the
18th National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370,
Ann Arbor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level
Boundary Classification for Information Extraction.
In In Proceedings of the 15th European Conference
on Machine Learning, pages 111?122, Pisa, Italy,
September.
Dayne Freitag. 1998a. Multistrategy Learning for
Information Extraction. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing. Morgan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose
Learning for Information Extraction. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 481?488, Sydney, Australia, July.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (Al-
most) Nothing. In Proceedings of The 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2010).
Ruihong Huang and Ellen Riloff. 2011. Peeling Back
the Layers: Detecting Event Role Fillers in Sec-
ondary Contexts. In Proceedings of the 49th Annual
294
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
S. Huffman. 1996. Learning Information Extraction
Patterns from Examples. In Stefan Wermter, Ellen
Riloff, and Gabriele Scheler, editors, Connectionist,
Statistical, and Symbolic Approaches to Learning
for Natural Language Processing, pages 246?260.
Springer-Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings of ACL-08: HLT, pages 254?262, Colum-
bus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
J. Kim and D. Moldovan. 1993. Acquisition of
Semantic Patterns for Information Extraction from
Corpora. In Proceedings of the Ninth IEEE Con-
ference on Artificial Intelligence for Applications,
pages 171?176, Los Alamitos, CA. IEEE Computer
Society Press.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on Computational Natural Language Learn-
ing, pages 72?79, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event
Extraction. In Proceedings of the 48st Annual
Meeting on Association for Computational Linguis-
tics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-
Resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
MUC-4 Proceedings. 1992. Proceedings of the
Fourth Message Understanding Conference (MUC-
4). Morgan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of 2007 the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model
of Phrasal and Sentential Evidence for Information
Extraction. In Proceedings of 2009 the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2009).
S. Patwardhan. 2010. Widening the Field of View
of Information Extraction through Sentential Event
Recognition. Ph.D. thesis, University of Utah.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Information
Extraction. In Proceedings of the 2007 Interna-
tional Conference on Recent Advances in Natural
Language Processing (RANLP-07), pages 468?473.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceed-
ings of the 11th National Conference on Artificial
Intelligence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intel-
ligence, pages 1044?1049.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive In-
formation Extraction using Unrestricted Relation
Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304?311, New York City, NY,
June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictio-
nary. In Proc. of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314?
1319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 379?386, Ann
Arbor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resume? In-
formation Extraction with Cascaded Hybrid Model.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, pages
499?506, Ann Arbor, MI, June.
295
Proceedings of NAACL-HLT 2013, pages 41?51,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Multi-faceted Event Recognition with Bootstrapped Dictionaries
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh, riloff}@cs.utah.edu
Abstract
Identifying documents that describe a specific
type of event is challenging due to the high
complexity and variety of event descriptions.
We propose a multi-faceted event recognition
approach, which identifies documents about
an event using event phrases as well as defin-
ing characteristics of the event. Our research
focuses on civil unrest events and learns civil
unrest expressions as well as phrases cor-
responding to potential agents and reasons
for civil unrest. We present a bootstrapping
algorithm that automatically acquires event
phrases, agent terms, and purpose (reason)
phrases from unannotated texts. We use the
bootstrapped dictionaries to identify civil un-
rest documents and show that multi-faceted
event recognition can yield high accuracy.
1 Introduction
Many people are interested in following news re-
ports about events. Government agencies are keenly
interested in news about civil unrest, acts of terror-
ism, and disease outbreaks. Companies want to stay
on top of news about corporate acquisitions, high-
level management changes, and new joint ventures.
The general public is interested in articles about
crime, natural disasters, and plane crashes. We will
refer to the task of identifying documents that de-
scribe a specific type of event as event recognition.
It is tempting to assume that event keywords
are sufficient to identify documents that discuss in-
stances of an event. But event words are rarely reli-
able on their own. For example, consider the chal-
lenge of finding documents about civil unrest. The
words ?strike?, ?rally?, and ?riot? refer to com-
mon types of civil unrest, but they frequently refer to
other things as well. A strike can refer to a military
event or a sporting event (e.g., ?air strike?, ?bowl-
ing strike?), a rally can be a race or a spirited ex-
change (e.g.,?car rally?, ?tennis rally?), and a riot
can refer to something funny (e.g., ?she?s a riot?).
Event keywords also appear in general discussions
that do not mention a specific event (e.g., ?37 states
prohibit teacher strikes? or ?The fine for inciting a
riot is $1,000?). Furthermore, many relevant docu-
ments are not easy to recognize because events can
be described with complex expressions that do not
include event keywords. For example, ?took to the
streets?, ?walked off their jobs? and ?stormed par-
liament? often describe civil unrest.
The goal of our research is to recognize event de-
scriptions in text by identifying event expressions as
well as defining characteristics of the event. We pro-
pose that agents and purpose are characteristics of
an event that are essential to distinguish one type of
event from another. The agent responsible for an ac-
tion often determines how we categorize the action.
For example, natural disasters, military operations,
and terrorist attacks can all produce human casual-
ties and physical destruction. But the agent of a nat-
ural disaster must be a natural force, the agent of
a military incident must be military personnel, and
the agent of a terrorist attack is never a natural force
and rarely military personnel. There may be other
important factors as well, but the agent is often an
essential part of an event definition.
The purpose of an event is also a crucial factor
in distinguishing between event types. For exam-
41
ple, civil unrest events and sporting events both in-
volve large groups of people amassing at a specific
site. But the purpose of civil unrest gatherings is to
protest against socio-political problems, while sport-
ing events are intended as entertainment. As another
example, terrorist events and military incidents can
both cause casualties, but the purpose of terrorism is
to cause widespread fear, while the purpose of mili-
tary actions is to protect national security interests.
Our research explores the idea of multi-faceted
event recognition: using event expressions as well
as facets of the event (agents and purpose) to iden-
tify documents about a specific type of event. We
present a bootstrapping framework to automatically
create event phrase, agent, and purpose dictionaries.
The learning process uses unannotated texts, a few
event keywords, and seed terms for common agents
and purpose phrases associated with the event type.
Our bootstrapping algorithm exploits the obser-
vation that event expressions, agents, and purpose
phrases often appear together in sentences that in-
troduce an event. In the first step, we extract event
expressions based on dependency relations with an
agent and purpose phrase. The harvested event ex-
pressions are added to an event phrase dictionary. In
the second step, new agent terms are extracted from
sentences containing an event phrase and a purpose
phrase, and new purpose phrases are harvested from
sentences containing an event phrase and an agent.
These harvested terms are added to agent and pur-
pose dictionaries. The bootstrapping algorithm rico-
chets back and forth, alternately learning new event
phrases and learning new agent/purpose phrases, in
an iterative process.
We explore several ways of using these boot-
strapped dictionaries. We conclude that finding at
least two different types of event information pro-
duces high accuracy (88% precision) with good re-
call (71%) on documents that contain an event key-
word. We also present experiments with documents
that do not contain event keywords, and obtain 74%
accuracy when matching all three types of event in-
formation.
2 Related Work
Event recognition has been studied in several dif-
ferent contexts. There has been a lot of research
on event extraction, where the goal is to extract
facts about events from text (e.g., (ACE Evaluations,
2006; Appelt et al, 1993; Riloff, 1996; Yangar-
ber et al, 2000; Chieu and Ng, 2002; Califf and
Mooney, 2003; Sudo et al, 2003; Stevenson and
Greenwood, 2005; Sekine, 2006)). Although our re-
search does not involve extracting facts, event ex-
traction systems can also be used to identify sto-
ries about a specific type of event. For example, the
MUC-4 evaluation (MUC-4 Proceedings, 1992) in-
cluded ?text filtering? results that measured the per-
formance of event extraction systems at identifying
event-relevant documents. The best text filtering re-
sults were high (about 90% F score), but relied on
hand-built event extraction systems. More recently,
some research has incorporated event region detec-
tors into event extraction systems to improve extrac-
tion performance (Gu and Cercone, 2006; Patward-
han and Riloff, 2007; Huang and Riloff, 2011).
There has been recent work on event detection
from social media sources (Becker et al, 2011;
Popescu et al, 2011). Some research identifies spe-
cific types of events in tweets, such as earthquakes
(Sakaki et al, 2010) and entertainment events (Ben-
son et al, 2011). There has also been work on event
trend detection (Lampos et al, 2010; Mathioudakis
and Koudas, 2010) and event prediction through so-
cial media, such as predicting elections (Tumasjan
et al, 2010; Conover et al, 2011) or stock mar-
ket indicators (Zhang et al, 2010). (Ritter et al,
2012) generated a calendar of events mentioned on
twitter. (Metzler et al, 2012) proposed structured
retrieval of historical event information over mi-
croblog archives by distilling high quality event rep-
resentations using a novel temporal query expansion
technique.
Some text classification research has focused on
event categories. (Riloff and Lehnert, 1994) used
an information extraction system to generate rele-
vancy signatures that were indicative of different
event types. This work originally relied on man-
ually labeled patterns and a hand-crafted semantic
dictionary. Later work (Riloff and Lorenzen, 1999)
eliminated the need for the dictionary and labeled
patterns, but still assumed the availability of rele-
vant/irrelevant training texts.
Event recognition is also related to Topic Detec-
tion and Tracking (TDT) (Allan et al, 1998; Allan,
42
Figure 1: Bootstrapped Learning of Event Dictionaries
2002) which addresses event-based organization of a
stream of news stories. Event recognition is similar
to New Event Detection, also called First Story De-
tection, which is considered the most difficult TDT
task (Allan et al, 2000a). Typical approaches re-
duce documents to a set of features, either as a word
vector (Allan et al, 2000b) or a probability distri-
bution (Jin et al, 1999), and compare the incoming
stories to stories that appeared in the past by com-
puting similarities between their feature representa-
tions. Recently, event paraphrases (Petrovic et al,
2012) have been explored to deal with the diversity
of event descriptions. However, the New Event De-
tection task differs from our event recognition task
because we want to find all stories describing a cer-
tain type of event, not just new events.
3 Bootstrapped Learning of Event
Dictionaries
Our bootstrapping approach consists of two stages
of learning as shown in Figure 1. The process be-
gins with a few agent seeds, purpose phrase patterns,
and unannotated articles selected from a broad-
coverage corpus using event keywords. In the first
stage, event expressions are harvested from the sen-
tences that have both an agent and a purpose phrase
in specific syntactic positions. In the second stage,
new purpose phrases are harvested from sentences
that contain both an event phrase and an agent, while
new agent terms are harvested from sentences that
contain both an event phrase and a purpose phrase.
The new terms are added to growing event dictionar-
ies, and the bootstrapping process repeats. Our work
focuses on civil unrest events.
3.1 Stage 1: Event Phrase Learning
We first extract potential civil unrest stories from the
English Gigaword corpus (Parker et al, 2011) using
six civil unrest keywords. As explained in Section 1,
event keywords are not sufficient to obtain relevant
documents with high precision, so the extracted sto-
ries are a mix of relevant and irrelevant articles. Our
algorithm first selects sentences to use for learning,
and then harvests event expressions from them.
3.1.1 Event Sentence Identification
The input in stage 1 consists of a few agent terms
and purpose patterns for seeding. The agent seeds
are single nouns, while the purpose patterns are
verbs in infinitive or present participle forms. Table
1 shows the agent terms and purpose phrases used in
our experiments. The agent terms were manually se-
lected by inspecting the most frequent nouns in the
documents with civil unrest keywords. The purpose
patterns are the most common verbs that describe the
reason for a civil unrest event. We identify probable
event sentences by extracting all sentences that con-
tain at least one agent term and one purpose phrase.
Agents protesters, activists, demonstrators,
students, groups, crowd, workers,
palestinians, supporters, women
Purpose demanding, to demand,
Phrases protesting, to protest
Table 1: Agent and Purpose Phrases Used for Seeding
43
3.1.2 Harvesting Event Expressions
To constrain the learning process, we require
event expressions and purpose phrases to match cer-
tain syntactic structures. We apply the Stanford de-
pendency parser (Marneffe et al, 2006) to the prob-
able event sentences to identify verb phrase candi-
dates and to enforce syntactic constraints between
the different types of event information.
Figure 2: Phrasal Structure of Event & Purpose Phrases
Figure 2 shows the two types of verb phrases
that we learn. One type consists of a verb paired
with the head noun of its direct object. For exam-
ple, event phrases can be ?stopped work? or ?oc-
cupied offices?, and purpose phrases can be ?show
support? or ?condemn war?. The second type con-
sists of a verb and an attached prepositional phrase,
retaining only the head noun of the embedded noun
phrase. For example, ?took to street? and ?scuffled
with police? can be event phrases, while ?call for
resignation? and ?press for wages? can be purpose
phrases. In both types of verb phrases, a particle can
optionally follow the verb.
Event expressions, agents, and purpose phrases
must appear in specific dependency relations, as il-
lustrated in Figure 3. An agent must be the syn-
tactic subject of the event phrase. A purpose phrase
must be a complement of the event phrase, specif-
ically, we require a particular dependency relation,
?xcomp?1, between the two verb phrases. For ex-
ample, in the sentence ?Leftist activists took to
the streets in the Nepali capital Wednesday protest-
ing higher fuel prices.?, the dependency relation
1In the dependency parser, ?xcomp? denotes a general rela-
tion between a VP or an ADJP and its open clausal complement.
For example, in the sentence ?He says that you like to swim.?,
the ?xcomp? relation will link ?like? (head) and ?swim? (de-
pendent). With our constraints on the verb phrase forms, the
dependent verb phrase in this construction tends to describe the
purpose of the verb phrase.
?xcomp? links ?took to the streets? with ?protest-
ing higher fuel prices?.
Figure 3: Syntactic Dependencies between Agents, Event
Phrases, and Purpose Phrases
Given the syntactic construction shown in Figure
3, with a known agent and purpose phrase, we ex-
tract the head verb phrase of the ?xcomp? depen-
dency relation as an event phrase candidate. The
event phrases that co-occur with at least two unique
agent terms and two unique purposes phrases are
saved in our event phrase dictionary.
3.2 Stage 2: Learning Agent and Purpose
Phrases
In the second stage of bootstrapping, we learn new
agent terms and purpose phrases. Our rationale is
that if a sentence contains an event phrase and one
other important facet of the event (agent or pur-
pose), then the sentence probably describes a rele-
vant event. We can then look for additional facets
of the event in the same sentence. We learn both
agent and purpose phrases simultaneously in paral-
lel learning processes. As before, we first identify
probable event sentences and then harvest agent and
purpose phrases from these sentences.
3.2.1 Event Sentence Identification
We identify probable event sentences by extract-
ing sentences that contain at least one event phrase
(based on the dictionary produced in the first stage
of bootstrapping) and an agent term or a purpose
phrase. As before, the event information must oc-
cur in the sentential dependency structures shown in
Figure 3.
3.2.2 Harvesting Agent and Purpose Phrases
The sentences that contain an event phrase and
an agent are used to harvest more purpose phrases,
while the sentences that contain an event phrase
and a purpose phrase are used to harvest more
agent terms. Purpose phrases are extracted from the
phrasal structures shown in Figure 2. In the learn-
ing process for agents, if a sentence has an event
44
phrase as the head of the ?xcomp? dependency re-
lation and a purpose phrase as the dependent clause
of the ?xcomp? dependency relation, then the head
noun of the syntactic subject of the event phrase is
harvested as a candidate agent term. We also record
the modifiers appearing in all of the noun phrases
headed by an agent term. Agent candidates that co-
occur with at least two unique event phrases and at
least two different modifiers of known agent terms
are selected as new agent terms.
The learning process for purpose phrases is anal-
ogous. If the syntactic subject of an event phrase
is an agent and the event phrase is the head of
the ?xcomp? dependency relation, then the depen-
dent clause of the ?xcomp? dependency relation is
harvested as a candidate purpose phrase. Purpose
phrase candidates that co-occur with at least two dif-
ferent event phrases are selected as purpose phrases.
The bootstrapping process then repeats, ricochet-
ing back and forth between learning event phrases
and learning agent and purpose phrases.
3.3 Domain Relevance Criteria
To avoid domain drift during bootstrapping, we use
two additional criteria to discard phrases that are not
necessarily associated with the domain.
For each event phrase and purpose phrase, we es-
timate its domain-specificity as the ratio of its preva-
lence in domain-specific texts compared to broad-
coverage texts. The goal is to discard phrases that
are common across many types of documents, and
therefore not specific to the domain. We define the
domain-specificity of phrase p as:
domain-specificity(p) = frequency of p in domain-specific corpusfrequency of p in broad-coverage corpus
We randomly sampled 10% of the Gigaword texts
that contain a civil unrest event keyword to create
the ?domain-specific? corpus, and randomly sam-
pled 10% of the remaining Gigaword texts to cre-
ate the ?broad-coverage? corpus.2 Keyword-based
sampling is an approximation to domain-relevance,
but gives us a general idea about the prevalance of a
phrase in different types of texts.
For agent terms, our goal is to identify people who
participate as agents of civil unrest events. Other
types of people may be commonly mentioned in
civil unrest stories too, as peripheral characters. For
2The random sampling was simply for efficiency reasons.
example, police may provide security and reporters
may provide media coverage of an event, but they
are not the agents of the event. We estimate the
event-specificity of each agent term as the ratio of
the phrase?s prevalence in event sentences compared
to all the sentences in the domain-specific corpus.
We define an event sentence as one that contains
both a learned event phrase and a purpose phrase,
based on the dictionaries at that point in time. There-
fore, the number of event sentences increases as the
bootstrapped dictionaries grow. We define the event-
specificity of phrase p as:
event-specificity(p) = frequency of p in event sentencesfrequency of p in all sentences
In our experiments we required event and purpose
phrases to have domain-specificity ? .33 and agent
terms to have event-specificity ? .01.3
4 Evaluation
4.1 Data
We conducted experiments to evaluate the perfor-
mance of our bootstrapped event dictionaries for rec-
ognizing civil unrest events. Civil unrest is a broad
term typically used by the media or law enforce-
ment to describe a form of public disturbance that
involves a group of people, usually to protest or pro-
mote a cause. Civil unrest events include strikes,
protests, occupations, rallies, and similar forms of
obstructions or riots. We chose six event keywords to
identify potential civil unrest documents: ?protest?,
?strike?, ?march?, ?rally?, ?riot? and ?occupy?. We
extracted documents from the English Gigaword
corpus (Parker et al, 2011) that contain at least one
of these event keywords, or a morphological variant
of a keyword.4 This process extracted nearly one
million documents, which we will refer to as our
event-keyword corpus.
We randomly sampled 400 documents5 from the
event-keyword corpus and asked two annotators to
determine whether each document mentioned a civil
3This value is so small because we simply want to filter
phrases that virtually never occur in the event sentences, and
we can recognize very few event sentences in the early stages
of bootstrapping.
4We used ?marched? and ?marching? as keywords but did
not use ?march? because it often refers to a month.
5These 400 documents were excluded from the unannotated
data used for dictionary learning.
45
unrest event. We defined annotation guidelines and
conducted an inter-annotator agreement study on
100 of these documents. The annotators achieved a
? score of .82. We used these 100 documents as our
tuning set. Then each annotator annotated 150 more
documents to create our test set of 300 documents.
4.2 Baselines
The first row of Table 2 shows event recognition ac-
curacy when only the event keywords are used. All
of our documents were obtained by searching for a
keyword, but only 101 of the 300 documents in our
test set were labeled as relevant by the annotators
(i.e., 101 describe a civil unrest event). This means
that using only the event keywords to identify civil
unrest documents yields about 34% precision. In a
second experiment, KeywordTitle, we required the
event keyword to be in the title (headline) of the doc-
ument. The KeywordTitle approach produced better
precision (66%), but only 33% of the relevant docu-
ments had a keyword in the title.
Method Recall Precision F
Keyword Accuracy
Keyword - 34 -
KeywordTitle 33 66 44
Supervised Learning
Unigrams 62 66 64
Unigrams+Bigrams 55 71 62
Bootstrapped Dictionary Lookup
Event Phrases (EV) 60 79 69
Agent Phrases (AG) 98 42 59
Purpose Phrases (PU) 59 67 63
All Pairs 71 88 79
Table 2: Experimental Results
The second section of Table 2 shows the re-
sults of two supervised classifiers based on 10-fold
cross validation with our test set. Both classifiers
were trained using support vector machines (SVMs)
(Joachims, 1999) with a linear kernel (Keerthi and
DeCoste, 2005). The first classifier used unigrams
as features, while the second classifier used both un-
igrams and bigrams. All the features are binary. The
evaluation results show that the unigram classifier
has an F-score of .64. Using both unigram and bi-
gram features increased precision to 71% but recall
fell by 7%, yielding a slightly lower F-score of .62.
4.3 Event Recognition with Bootstrapped
Dictionaries
Next, we used our bootstrapped dictionaries for
event recognition. The bootstrapping process ran
for 8 iterations and then stopped because no more
phrases could be learned. The quality of boot-
strapped data often degrades as bootstrapping pro-
gresses, so we used the tuning set to evaluate the
dictionaries after each iteration. The best perfor-
mance6 on the tuning set resulted from the dictionar-
ies produced after four iterations, so we used these
dictionaries for our experiments. Table 3 shows the
Event Agent Purpose
Phrases Terms Phrases
Iter #1 145 67 124
Iter #2 410 106 356
Iter #3 504 130 402
Iter #4 623 139 569
Table 3: Dictionary Sizes after Several Iterations
number of event phrases, agents and purpose phrases
learned after each iteration. All three lexicons were
significantly enriched after each iteration. The final
bootstrapped dictionaries contain 623 event phrases,
569 purpose phrases and 139 agent terms. Table 4
shows samples from each event dictionary.
Event Phrases: went on strike, took to street,
chanted slogans, gathered in capital, formed chain,
clashed with police, staged rally, held protest,
walked off job, burned flags, set fire, hit streets,
marched in city, blocked roads, carried placards
Agent Terms: employees, miners, muslims, unions,
protestors, journalists, refugees, prisoners, immigrants,
inmates, pilots, farmers, followers, teachers, drivers
Purpose Phrases: accusing government, voice anger,
press for wages, oppose plans, urging end, defying ban,
show solidarity, mark anniversary, calling for right,
condemning act, pressure government, mark death,
push for hike, call attention, celebrating withdrawal
Table 4: Examples of Dictionary Entries
The third section of Table 2 shows the results
when using the bootstrapped dictionaries for event
recognition. We used a simple dictionary look-up
approach that searched for dictionary entries in each
document. Our phrases were generated based on
6Based on the performance for the All Pairs approach.
46
syntactic analysis and only head words were re-
tained for generality. But we wanted to match dic-
tionary entries without requiring syntactic analysis
of new documents. So we used an approximate
matching scheme that required each word to appear
within 5 words of the previous word. For example,
?held protest? would match ?held a large protest?
and ?held a very large political protest?. In this way,
we avoid the need for syntactic analysis when using
the dictionaries for event recognition.
First, we labeled a document as relevant if it con-
tained any Event Phrase (EV) in our dictionary. The
event phrases achieved better performance than all
of the baselines, yielding an F-score of 69%. The
best baseline was the unigram classifier, which was
trained with supervised learning. The bootstrapped
event phrase dictionary produced much higher pre-
cision (79% vs. 66%) with only slightly lower recall
(60% vs. 62%), and did not require annotated texts
for training. Statistical significance testing shows
that the Event Phrase lookup approach works signif-
icantly better than the unigram classifier (p < 0.05,
paired bootstrap (Berg-Kirkpatrick et al, 2012)).
For the sake of completeness, we also evaluated
the performance of dictionary look-up using our
bootstrapped Agent (AG) and Purpose (PU) dictio-
naries, individually. The agents terms produced 42%
precision with 98% recall, demonstrating that the
learned agent list has extremely high coverage but
(unsurprisingly) does not achieve high precision on
its own. The purpose phrases achieved a better bal-
ance of recall and precision, producing an F-score
of 63%, which is nearly the same as the supervised
unigram classifier.
Our original hypothesis was that a single type of
event information is not sufficient to accurately iden-
tify event descriptions. Our goal was high-accuracy
event recognition by requiring that a document con-
tain multiple clues pertaining to different facets of an
event (multi-faceted event recognition). The last row
of Table 2 (All Pairs) shows the results when requir-
ing matches from at least two different bootstrapped
dictionaries. Specifically, we labeled a document
as relevant if it contained at least one phrase from
each of two different dictionaries and these phrases
occurred in the same sentence. Table 2 shows that
multi-faceted event recognition achieves 88% preci-
sion with reasonably good recall of 71%, yielding an
F-score of 79%. This multi-faceted approach with
simple dictionary look-up outperformed all of the
baselines, and each dictionary used by itself. Sta-
tistical significance testing shows that the All Pairs
approach works significantly better than the unigram
classifier (p < 0.001, paired bootstrap). The All
Pairs approach is significantly better than the Event
Phrase (EV) lookup approach at the p < 0.1 level.
Method Recall Precision F-score
EV + PU 14 100 24
EV + AG 47 94 62
AG + PU 50 85 63
All Pairs 71 88 79
Table 5: Analysis of Dictionary Combinations
Table 5 takes a closer look at how each pair of
dictionaries performed. The first row shows that re-
quiring a document to have an event phrase and a
purpose phrase produces the best precision (100%)
but with low recall (14%). The second row reveals
that requiring a document to have an event phrase
and an agent term yields better recall (47%) and high
precision (94%). The third row shows that requiring
a document to have a purpose phrase and an agent
term produces the best recall (50%) but with slightly
lower precision (85%). Finally, the last row of Ta-
ble 5 shows that taking the union of these results
(i.e., any combination of dictionary pairs is suffi-
cient) yields the best recall (71%) with high preci-
sion (88%), demonstrating that we get the best cov-
erage by recognizing multiple combinations of event
information.
Lexicon Recall Precision F-score
Seeds 13 87 22
Iter #1 50 88 63
Iter #2 63 89 74
Iter #3 68 88 77
Iter #4 71 88 79
Table 6: All Pairs Lookup Results using only Seeds and
the Lexicons Learned after each Iteration, on the Test Set
Table 6 shows the performance of the lexicon
lookup approach using the All Pairs criteria dur-
ing the bootstrapping process. The first row shows
the results using only 10 agent seeds and 4 purpose
seeds as shown in Table 1. The following four rows
in the table show the performance of All Pairs using
47
the lexicons learned after each bootstrapping itera-
tion. We can see that the recall increases steadily and
that precision is maintained at a high level through-
out the bootstrapping process.
Event recognition can be formulated as an infor-
mation retrieval (IR) problem. As another point of
comparison, we ran an existing IR system, Terrier
(Ounis et al, 2007), on our test set. We used Ter-
rier to rank these 300 documents given our set of
event keywords as the query 7, and then generated a
recall/precision curve (Figure 4) by computing the
precisions at different levels of recall, ranging from
0 to 1 in increments of .10. Terrier was run with the
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Figure 4: Comparison with the Terrier IR system
parameter PL2 which refers to an advanced Diver-
gence From Randomness weighting model (Amati
and Van Rijsbergen, 2002). In addition, Terrier used
automatic query expansion. We can see that Terrier
identified the first 60 documents (20% recall) with
100% precision. But precision dropped sharply after
that. The circle in Figure 4 shows the performance
of our bootstrapped dictionaries using the All Pairs
approach. At comparable level of precision (88%),
Terrier achieved about 45% recall versus 71% recall
produced with the bootstrapped dictionaries.
4.4 Supervised Classifiers with Bootstrapped
Dictionaries
We also explored the idea of using the bootstrapped
dictionaries as features for a classifier to see if a su-
pervised learner could make better use of the dic-
7We gave Terrier one query with all of the event keywords.
tionaries. We created five SVM classifiers and per-
formed 10-fold cross validation on the test set.
Method Recall Precision F-score
TermLex 66 85 74
PairLex 10 91 18
TermSets 59 83 69
PairSets 68 84 75
AllSets 70 84 76
Table 7: Supervised classifiers using the dictionaries
Table 7 shows the results for the five classifiers.
TermLex encodes a binary feature for every phrase
in any of our dictionaries. PairLex encodes a binary
feature for each pair of phrases from two different
dictionaries and requires them to occur in the same
sentence. The TermLex classifier achieves good per-
formance (74% F-score), but is not as effective as
our All Pairs dictionary look-up approach (79% F-
score). The PairLex classifier yield higher precision
but very low recall, undoubtedly due to sparsity is-
sues in matching specific pairs of phrases.
One of the strengths of our bootstrapping method
is that it creates dictionaries from large volumes of
unannotated documents. A limitation of supervised
learning with lexical features is that the classifier can
not benefit from terms in the bootstrapped dictionar-
ies that do not appear in its training documents. To
address this issue, we also tried encoding the dic-
tionaries as set-based features. The TermSets clas-
sifier encodes three binary features, one for each
dictionary. A feature gets a value of 1 if a docu-
ment contains any word in the corresponding dictio-
nary. The PairSets classifier also encodes three bi-
nary features, but each feature represents a different
pair of dictionaries (EV+AG, EV+PU, or AG+PU).
A feature gets a value of 1 if a document contains at
least one term from each of the two dictionaries in
the same sentence. The AllSets classifier encodes 7
set-based features: the previous six features and one
additional feature that requires a sentence to contain
at least one entry from all three dictionaries.
The All Sets classifier yields the best performance
with an F-score of 76%. However, our straightfor-
ward dictionary look-up approach still performs bet-
ter (79% F-score), and does not require annotated
documents for training.
48
4.5 Finding Articles with no Event Keyword
The learned event dictionaries have the potential to
recognize event-relevant documents that do not con-
tain any human-selected event keywords. This can
happen in two ways. First, 378 of the 623 learned
event phrases do not contain any of the original event
keywords. Second, we expect that some event de-
scriptions will contain a known agent and purpose
phrase, even if the event phrase is unfamiliar.
We performed an additional set of experiments
with documents in the Gigaword corpus that contain
no human-selected civil unrest keyword. Following
our multi-faceted approach to event recognition, we
collected all documents that contain a sentence that
matches phrases in at least two of our bootstrapped
event dictionaries. This process retrieved 178,197
documents. The first column of Table 8 shows the
number of documents that had phrases found in two
different dictionaries (EV+AG, EV+PU, AG+PU) or
in all three dictionaries (EV+AG+PU).
Total Samples Accuracy
EV+AG 67,796 50 44%
EV+PU 2,375 50 54%
AG+PU 101,173 50 18%
EV+AG+PU 6,853 50 74%
Table 8: Evaluation of articles with no event keyword
We randomly sampled 50 documents from each
category and had them annotated. The accura-
cies are shown in the third column. Finding all
three types of phrases produced the best accuracy,
74%. Furthermore, we found over 6,800 documents
that had all three types of event information us-
ing our learned dictionaries. This result demon-
strates that the bootstrapped dictionaries can recog-
nize many event descriptions that would have been
missed by searching only with manually selected
keywords. This experiment also confirms that multi-
facted event recognition using all three learned dic-
tionaries achieves good accuracy even for docu-
ments that do not contain the civil unrest keywords.
5 Conclusions
We proposed a multi-faceted approach to event
recognition and presented a bootstrapping technique
to learn event phrases as well as agent terms and
purpose phrases associated with civil unrest events.
Our results showed that multi-faceted event recog-
nition using the learned dictionaries achieved high
accuracy and performed better than several other
methods. The bootstrapping approach can be eas-
ily trained for new domains since it requires only
a large collection of unannotated texts and a few
event keywords, agent terms, and purpose phrases
for the events of interest. Furthermore, although the
training phase requires syntactic parsing to learn the
event dictionaries, the dictionaries can then be used
for event recognition without needing to parse the
documents.
An open question for future work is to investigate
whether the same multi-faceted approach to event
recognition will work well for other types of events.
Our belief is that many different types of events have
characteristic agent terms, but additional types of
facets will need to be defined to cover a broad array
of event types. The syntactic constructions used to
harvest dictionary items may also vary depending on
the types of event information that must be learned.
In future research, we plan to explore these issues in
more depth to design a more general multi-faceted
event recognition system, and we plan to investigate
new ways to use these event dictionaries for event
extraction as well.
6 Acknowledgments
This research was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI /
NBC) contract number D12PC00285 and by the Na-
tional Science Foundation under grant IIS-1018314.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, NSF, or the U.S. Government.
49
References
ACE Evaluations. 2006.
http://www.itl.nist.gov/iad/mig/tests/ace/.
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic Detection and Tracking Pilot
Study: Final Report. In Proceedings of DARPA Broad-
cast News Transcription and Understanding Work-
shop.
J. Allan, V. Lavrenko, and H. Jin. 2000a. First Story
Detection in TDT is Hard. In Proceedings of the 2000
ACM CIKM International Conference on Information
and Knowledge Management.
J. Allan, Victor Lavrenko, Daniella Malin, and Russell
Swan. 2000b. Detections, Bounds, and Timelines:
UMass and TDT-3. In Proceedings of Topic Detection
and Tracking Workshop.
J. Allan, 2002. Topic Detection and Tracking: Event
Based Information Organization. Kluwer Academic
Publishers.
G. Amati and C. J. Van Rijsbergen. 2002. Probabilistic
Models of Information Retrieval based on Measuring
Divergence from Randomness. ACM Transactions on
Information Systems, 20(4):357?389.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
H. Becker, M. Naaman, and L. Gravano. 2011. Be-
yond trending topics: Real-world event identification
on twitter. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds.
T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An
Empirical Investigation of Statistical Significance in
NLP. In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177?210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
M. D. Conover, J. Ratkiewicz, M. Francisco,
B. Goncalves, A. Flammini, and F. Menczer. 2011.
Political Polarization on Twitter. In Proceedings of
the Fifth International AAAI Conference on Weblogs
and Social Media.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481?488, Sydney, Australia, July.
R. Huang and E. Riloff. 2011. Peeling Back the Layers:
Detecting Event Role Fillers in Secondary Contexts.
H. Jin, R. Schwartz, S. Sista, and F. Walls. 1999. Topic
Tracking for Radio, TV broadcast, and Newswire. In
EUROSPEECH.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
V. Lampos, T. D. Bie, and N. Cristianini. 2010. Flu
Detector - Tracking Epidemics on Twitter. In ECML
PKDD.
M. d. Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the Fifth
Conference on Language Resources and Evaluation
(LREC-2006).
M. Mathioudakis and N. Koudas. 2010. TwitterMonitor:
trend detection over the twitter stream. In Proceedings
of the 2010 international conference on Management
of data, page 11551158. ACM.
D. Metzler, C. Cai, and E. Hovy. 2012. Structured Event
Retrieval over Microblog Archives. In Proceedings of
The 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
I. Ounis, C. Lioma, C. Macdonald, and V. Plachouras.
2007. Research Directions in Terrier. Novat-
ica/UPGRADE Special Issue on Web Information Ac-
cess, Ricardo Baeza-Yates et al (Eds), Invited Paper.
R. Parker, D. Graff, J. Kong, K. Chen, and Kazuaki M.
2011. English Gigaword. In Linguistic Data Consor-
tium.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Petrovic, M. Osborne, and V. Lavrenko. 2012. Us-
ing Paraphrases for Improving First Story Detection in
50
News and Twitter. In Proceedings of The 2012 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.
A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe.
2011. Extracting events and event descriptions from
twitter.
E. Riloff and W. Lehnert. 1994. Information Ex-
traction as a Basis for High-Precision Text Classifi-
cation. ACM Transactions on Information Systems,
12(3):296?333, July.
E. Riloff and J. Lorenzen. 1999. Extraction-based text
categorization: Generating domain-specific role rela-
tionships automatically. In Tomek Strzalkowski, edi-
tor, Natural Language Information Retrieval. Kluwer
Academic Publishers.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open
domain event extraction from twitter. In The Proceed-
ings of The 18th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors.
S. Sekine. 2006. On-demand Information Extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL-06).
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting Elections with Twitter: What
140 Characters Reveal about Political Sentiment. In
Proceedings of the 4th International AAAI Conference
on Weblogs and Social Media.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
X. Zhang, H. Fuehres, and P. A. Gloor. 2010. Predicting
Stock Market Indicators Through Twitter ?I hope it is
not as bad as I fear?. In COINs.
51
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 275?285,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
This research explores the idea of inducing
domain-specific semantic class taggers us-
ing only a domain-specific text collection
and seed words. The learning process be-
gins by inducing a classifier that only has
access to contextual features, forcing it to
generalize beyond the seeds. The contex-
tual classifier then labels new instances,
to expand and diversify the training set.
Next, a cross-category bootstrapping pro-
cess simultaneously trains a suite of clas-
sifiers for multiple semantic classes. The
positive instances for one class are used as
negative instances for the others in an it-
erative bootstrapping cycle. We also ex-
plore a one-semantic-class-per-discourse
heuristic, and use the classifiers to dynam-
ically create semantic features. We eval-
uate our approach by inducing six seman-
tic taggers from a collection of veterinary
medicine message board posts.
1 Introduction
The goal of our research is to create semantic class
taggers that can assign a semantic class label to ev-
ery noun phrase in a sentence. For example, con-
sider the sentence: ?The lab mix was diagnosed
with parvo and given abx?. A semantic tagger
should identify the ?the lab mix? as an ANIMAL,
?parvo? as a DISEASE, and ?abx? (antibiotics)
as a DRUG. Accurate semantic tagging could be
beneficial for many NLP tasks, including coref-
erence resolution and word sense disambiguation,
and many NLP applications, such as event extrac-
tion systems and question answering technology.
Semantic class tagging has been the subject of
previous research, primarily under the guises of
named entity recognition (NER) and mention de-
tection. Named entity recognizers perform se-
mantic tagging on proper name noun phrases, and
sometimes temporal and numeric expressions as
well. The mention detection task was introduced
in recent ACE evaluations (e.g., (ACE, 2007;
ACE, 2008)) and requires systems to identify all
noun phrases (proper names, nominals, and pro-
nouns) that correspond to 5-7 semantic classes.
Despite widespread interest in semantic tag-
ging, nearly all semantic taggers for comprehen-
sive NP tagging still rely on supervised learn-
ing, which requires annotated data for training.
A few annotated corpora exist, but they are rela-
tively small and most were developed for broad-
coverage NLP. Many domains, however, are re-
plete with specialized terminology and jargon that
cannot be adequately handled by general-purpose
systems. Domains such as biology, medicine, and
law are teeming with specialized vocabulary that
necessitates training on domain-specific corpora.
Our research explores the idea of inducing
domain-specific semantic taggers using a small
set of seed words as the only form of human su-
pervision. Given an (unannotated) collection of
domain-specific text, we automatically generate
training instances by labelling every instance of a
seed word with its designated semantic class. We
then train a classifier to do semantic tagging using
these seed-based annotations, using bootstrapping
to iteratively improve performance.
On the surface, this approach appears to be a
contradiction. The classifier must learn how to as-
sign different semantic tags to different instances
of the same word based on context (e.g., ?lab?
may refer to an animal in one context but a labora-
tory in another). And yet, we plan to train the clas-
sifier using stand-alone seed words, making the as-
sumption that every instance of the seed belongs to
the same semantic class. We resolve this apparent
contradiction by using semantically unambiguous
seeds and by introducing an initial context-only
training phase before bootstrapping begins. First,
we train a strictly contextual classifier that only
275
has access to contextual features and cannot see
the seed. Then we apply the classifier to the corpus
to automatically label new instances, and combine
these new instances with the seed-based instances.
This process expands and diversifies the training
set to fuel subsequent bootstrapping.
Another challenge is that we want to use a small
set of seeds to minimize the amount of human ef-
fort, and then use bootstrapping to fully exploit
the domain-specific corpus. Iterative self-training,
however, often has difficulty sustaining momen-
tum or it succumbs to semantic drift (Komachi
et al, 2008; McIntosh and Curran, 2009). To
address these issues, we simultaneously induce
a suite of classifiers for multiple semantic cat-
egories, using the positive instances of one se-
mantic category as negative instances for the oth-
ers. As bootstrapping progresses, the classifiers
gradually improve themselves, and each other,
over many iterations. We also explore a one-
semantic-class-per-discourse (OSCPD) heuristic
that infuses the learning process with fresh train-
ing instances, which may be substantially differ-
ent from the ones seen previously, and we use the
labels produced by the classifiers to dynamically
create semantic features.
We evaluate our approach by creating six se-
mantic taggers using a collection of message board
posts in the domain of veterinary medicine. Our
results show this approach produces high-quality
semantic taggers after a sustained bootstrapping
cycle that maintains good precision while steadily
increasing recall over many iterations.
2 Related Work
Semantic class tagging is most closely related to
named entity recognition (NER), mention detec-
tion, and semantic lexicon induction. NER sys-
tems (e.g., (Bikel et al, 1997; Collins and Singer,
1999; Cucerzan and Yarowsky, 1999; Fleischman
and Hovy, 2002) identify proper named entities,
such as people, organizations, and locations. Sev-
eral bootstrapping methods for NER have been
previously developed (e.g., (Collins and Singer,
1999; Niu et al, 2003)). NER systems, how-
ever, do not identify nominal NP instances (e.g.,
?a software manufacturer? or ?the beach?), or han-
dle semantic classes that are not associated with
proper named entities (e.g., symptoms).1 ACE
1Some NER systems also handle specialized constructs
such as dates and monetary amounts.
mention detection systems (e.g., see (ACE, 2005;
ACE, 2007; ACE, 2008)) require tagging of NPs
that correspond to 5-7 general semantic classes.
These systems are typically trained with super-
vised learning using annotated corpora, although
techniques have been developed to use resources
for one language to train systems for different lan-
guages (e.g., (Zitouni and Florian, 2009)).
Another line of relevant work is semantic class
induction (e.g., (Riloff and Shepherd, 1997; Roark
and Charniak, 1998; Thelen and Riloff, 2002; Ng,
2007; McIntosh and Curran, 2009), where the goal
is to induce a stand-alone dictionary of words with
semantic class labels. These techniques are of-
ten designed to learn specialized terminology from
unannotated domain-specific texts via bootstrap-
ping. Our work, however, focuses on classifica-
tion of NP instances in context, so the same phrase
may be assigned to different semantic classes in
different contexts. Consequently, our classifier
can also assign semantic class labels to pronouns.
There has also been work on extracting seman-
tically related terms or category members from
the Web (e.g., (Pas?ca, 2004; Etzioni et al, 2005;
Kozareva et al, 2008; Carlson et al, 2009)). These
techniques harvest broad-coverage semantic infor-
mation from the Web using patterns and statistics,
typically for the purpose of knowledge acquisi-
tion. Importantly, our goal is to classify instances
in context, rather than generate lists of terms. In
addition, the goal of our research is to learn spe-
cialized terms and jargon that may not be common
on the Web, as well as domain-specific usages that
may differ from the norm (e.g., ?mix? and ?lab?
are usually ANIMALS in our domain).
The idea of simulataneously learning multiple
semantic categories to prevent semantic drift has
been explored for other tasks, such as semantic
lexicon induction (Thelen and Riloff, 2002; McIn-
tosh and Curran, 2009) and pattern learning (Yan-
garber, 2003). Our bootstrapping model can be
viewed as a form of self-training (e.g., (Ng and
Cardie, 2003; Mihalcea, 2004; McClosky et al,
2006)), and cross-category training is similar in
spirit to co-training (e.g., (Blum and Mitchell,
1998; Collins and Singer, 1999; Riloff and Jones,
1999; Mueller et al, 2002; Phillips and Riloff,
2002)). But, importantly, our classifiers all use the
same feature set so they do not represent indepen-
dent views of the data. They do, however, offer
slightly different perspectives because each is at-
276
tempting to recognize a different semantic class.
3 Bootstrapping an Instance-based
Semantic Class Tagger from Seeds
3.1 Motivation
Our goal is to create a bootstrapping model that
can rapidly create semantic class taggers using
just a small set of seed words and an unanno-
tated domain-specific corpus. Our motivation
comes from specialized domains that cannot be
adequately handled by general-purpose NLP sys-
tems. As an example of such a domain, we have
been working with a collection of message board
posts in the field of veterinary medicine. Given a
document, we want a semantic class tagger to label
every NP with a semantic category, for example:
[A 14yo doxy]ANIMAL owned by
[a reputable breeder]HUMAN is be-
ing treated for [IBD]DISEASE with
[pred]DRUG.
When we began working with these texts, we
were immediately confronted by a dizzying array
of non-standard words and word uses. In addition
to formal veterinary vocabulary (e.g., animal dis-
eases), veterinarians often use informal, shorthand
terms when posting on-line. For example, they
frequently refer to breeds using ?nicknames? or
shortened terms (e.g., gshep for German shepherd,
doxy for dachsund, bxr for boxer, labx for labrador
cross). Often, they refer to animals based solely on
their physical characteristics, for example ?a dlh?
(domestic long hair), ?a m/n? (male, neutered), or
?a 2yo? (2 year old). This phenomenon occurs
with other semantic categories as well, such as
drugs and medical tests (e.g., pred for prednisone,
and rads for radiographs).
Nearly all semantic class taggers are trained us-
ing supervised learning with manually annotated
data. However, annotated data is rarely available
for specialized domains, and it is expensive to ob-
tain because domain experts must do the annota-
tion work. So we set out to create a bootstrapping
model that can rapidly create domain-specific se-
mantic taggers using just a few seed words and a
domain-specific text collection.
Our bootstrapping model consists of two dis-
tinct phases. First, we train strictly contextual
classifiers from the seed annotations. We then ap-
ply the classifiers to the unlabeled data to gener-
ate new annotated instances that are added to the
training set. Second, we employ a cross-category
bootstrapping process that simultaneously trains
a suite of classifiers for multiple semantic cate-
gories, using the positive instances for one se-
mantic class as negative instances for the oth-
ers. This cross-category training process gives
the learner sustained momentum over many boot-
strapping iterations. Finally, we explore two ad-
ditional enhancements: (1) a one-semantic-class-
per-discourse heuristic to automatically generate
new training instances, and (2) dynamically cre-
ated semantic features produced by the classifiers
themselves. In the following sections, we explain
each of these steps in detail.
3.2 Phase 1: Inducing a Contextual Classifier
The main challenge that we faced was how to train
an instance-based classifier using seed words as
the only form of human supervision. First, the user
must provide a small set of seed words that are
relatively unambiguous (e.g., ?dog? will nearly
always refer to an animal in our domain). But
even so, training a traditional classifier from seed-
based instances would likely produce a classifier
that learns to recognize the seeds but is unable to
classify new examples. We needed to force the
classifier to generalize beyond the seed words.
Our solution was to introduce an initial train-
ing step that induces a strictly contextual classifier.
First, we generate training instances by automati-
cally labeling each instance of a seed word with
its designated semantic class. However, when we
create feature vectors for the classifier, the seeds
themselves are hidden and only contextual fea-
tures are used to represent each training instance.
By essentially ?masking? the seed words so the
classifier can only see the contexts around them,
we force the classifier to generalize.
We create a suite of strictly contextual classi-
fiers, one for each semantic category. Each classi-
fier makes a binary decision as to whether a noun
phrase belongs to its semantic category. We use
the seed words for category Ck to generate posi-
tive training instances for the Ck classifier, and the
seed words for all other categories to generate the
negative training instances for Ck.
We use an in-house sentence segmenter and NP
chunker to identify the base NPs in each sentence
and create feature vectors that represent each con-
stituent in the sentence as either an NP or an in-
dividual word. For each seed word, the feature
277
vector captures a context window of 3 constituents
(word or NP) to its left and 3 constituents (word
or NP) to its right. Each constituent is represented
with a lexical feature: for NPs, we use its head
noun; for individual words, we use the word itself.
The seed word, however, is discarded so that the
classifier is essentially blind-folded and cannot see
the seed that produced the training instance. We
also create a feature for every modifier that pre-
cedes the head noun in the target NP, except for
articles which are discarded. As an example, con-
sider the following sentence:
Fluffy was diagnosed with FELV after a
blood test showed that he tested positive.
Suppose that ?FELV? is a seed for the DISEASE
category and ?test? is a seed for the TEST cate-
gory. Two training instances would be created,
with feature vectors that look like this, where M
represents a modifier inside the target NP:
was?3 diagnosed?2 with?1 after1 test2
showed3 ? DISEASE
with?3 FELV?2 after?1 bloodM showed1
that2 he3 ? TEST
The contextual classifiers are then applied to the
corpus to automatically label new instances. We
use a confidence score to label only the instances
that the classifiers are most certain about. We com-
pute a confidence score for instance i with respect
to semantic class Ck by considering both the score
of the Ck classifier as well as the scores of the
competing classifiers. Intuitively, we have confi-
dence in labeling an instance as category Ck if the
Ck classifier gave it a positive score, and its score
is much higher than the score of any other classi-
fier. We use the following scoring function:
Confidence(i,Ck) =
score(i,Ck) - max(?j 6=k score(i,Cj ))
We employ support vector machines (SVMs)
(Joachims, 1999) with a linear kernel as our classi-
fiers, using the SVMlin software (Keerthi and De-
Coste, 2005). We use the value produced by the
decision function (essentially the distance from
the hyperplane) as the score for a classifier. We
specify a threshold ?cf and only assign a semantic
tag Ck to an instance i if Confidence(i,Ck) ? ?cf .
All instances that pass the confidence thresh-
old are labeled and added to the training set.
This process greatly enhances the diversity of
the training data. In this initial learning step,
the strictly contextual classifiers substantially in-
crease the number of training instances for each
semantic category, producing a more diverse mix
of seed-generated instances and context-generated
instances.
3.3 Phase 2: Cross-Category Bootstrapping
The next phase of the learning process is an iter-
ative bootstrapping procedure. The key challenge
was to design a bootstrapping model that would
not succumb to semantic drift and would have sus-
tained momentum to continue learning over many
iterations.
Figure 1 shows the design of our cross-category
bootstrapping model.2 We simultaneously train a
suite of binary classifiers, one for each semantic
category, C1 . . . Cn. After each training cycle,
all of the classifiers are applied to the remaining
unlabeled instances and each classifier labels the
(positive) instances that it is most confident about
(i.e., the instances that it classifies with a confi-
dence score ? ?cf ). The set of instances positively
labeled by classifier Ck are shown as C+k in Figure
1. All of the new instances produced by classifier
Ck are then added to the set of positive training
instances for Ck and to the set of negative training
instances for all of the other classifiers.
One potential problem with this scheme is that
some categories are more prolific than others, plus
we are collecting negative instances from a set
of competing classifiers. Consequently, this ap-
proach can produce highly imbalanced training
sets. Therefore we enforced a 3:1 ratio of nega-
tives to positives by randomly selecting a subset
of the possible negative instances. We discuss this
issue further in Section 4.4.
labeled
C+1
C2
C+2 C
+
n
C
n
unlabeled
seeds
C1
i=1C
+
_( )
C+1
(+) (+)
C+2 i=2C
+
_( ) _( )
i=nC
+C+
n
(+)
Figure 1: Cross-Category Bootstrapping
2For simplicity, this picture does not depict the initial con-
textual training step, but that can be viewed as the first itera-
tion in this general framework.
278
Cross-category training has two advantages
over independent self-training. First, as oth-
ers have shown for pattern learning and lexicon
induction (Thelen and Riloff, 2002; Yangarber,
2003; McIntosh and Curran, 2009), simultane-
ously training classifiers for multiple categories
reduces semantic drift because each classifier is
deterred from encroaching on another one?s terri-
tory (i.e., claiming the instances from a compet-
ing class as its own). Second, similar in spirit to
co-training3 , this approach allows each classifier
to obtain new training instances from an outside
source that has a slightly different perspective.
While independent self-training can quickly run
out of steam, cross-category training supplies each
classifier with a constant stream of new (negative)
instances produced by competing classifiers. In
Section 4, we will show that cross-category boot-
strapping performs substantially better than an in-
dependent self-training model, where each classi-
fier is bootstrapped separately.
The feature set for these classifiers is exactly the
same as described in Section 3.2, except that we
add a new lexical feature that represents the head
noun of the target NP (i.e., the NP that needs to be
tagged). This allows the classifiers to consider the
local context as well as the target word itself when
making decisions.
3.4 One Semantic Class Per Discourse
We also explored the idea of using a one semantic
class per discourse (OSCPD) heuristic to gener-
ate additional training instances during bootstrap-
ping. Inspired by Yarowsky?s one sense per dis-
course heuristic for word sense disambiguation
(Yarowsky, 1995), we make the assumption that
multiple instances of a word in the same discourse
will nearly always correspond to the same seman-
tic class. Since our data set consists of message
board posts organized as threads, we consider all
posts in the same thread to be a single discourse.
After each training step, we apply the classi-
fiers to the unlabeled data to label some new in-
stances. For each newly labeled instance, the OS-
CPD heuristic collects all instances with the same
head noun in the same discourse (thread) and uni-
laterally labels them with the same semantic class.
This heuristic serves as meta-knowledge to label
instances that (potentially) occur in very different
3But technically this is not co-training because our feature
sets are all the same.
contexts, thereby infusing the bootstrapping pro-
cess with ?fresh? training examples.
In early experiments, we found that OSCPD can
be aggressive, pulling in many new instances. If
the classifier labels a word incorrectly, however,
then the OSCPD heuristic will compound the er-
ror and mislabel even more instances incorrectly.
Therefore we only apply this heuristic to instances
that are labeled with extremely high confidence
(?cf ? 2.5) and that pass a global sanity check,
gsc(w) ? 0.2, which ensures that a relatively high
proportion of labeled instances with the same head
noun have been assigned to the same semantic
class. Specifically, gsc(w) = 0.1?wl/cwl +0.9?
wu/c
wu
where wl and wu are the # of labeled and unla-
beled instances, respectively, wl/c is the # of in-
stances labeled as c, and wu/c is the # of unlabeled
instances that receive a positive confidence score
for c when given to the classifier. The intuition
behind the second term is that most instances are
initially unlabeled and we want to make sure that
many of the unlabeled instances are likely to be-
long to the same semantic class (even though the
classifier isn?t ready to commit to them yet).
3.5 Dynamic Semantic Features
For many NLP tasks, classifiers use semantic fea-
tures to represent the semantic class of words.
These features are typically obtained from exter-
nal resources such as Wordnet (Miller, 1990). Our
bootstrapping model incrementally trains seman-
tic class taggers, so we explored the idea of using
the labels assigned by the classifiers to create en-
hanced feature vectors by dynamically adding se-
mantic features. This process allows later stages
of bootstrapping to directly benefit from earlier
stages. For example, consider the sentence:
He started the doxy on Vetsulin today.
If ?Vetsulin? was labeled as a DRUG in a previ-
ous bootstrapping iteration, then the feature vector
representing the context around ?doxy? can be en-
hanced to include an additional semantic feature
identifying Vetsulin as a DRUG, which would look
like this:
He?2 started?1 on1 V etsulin2 DRUG2 today3
Intuitively, the semantic features should help the
classifier identify more general contextual pat-
terns, such as ?started <X> on DRUG?. To create
semantic features, we use the semantic tags that
279
have been assigned to the current set of labeled in-
stances. When a feature vector is created for a tar-
get NP, we check every noun instance in its context
window to see if it has been assigned a semantic
tag, and if so, then we add a semantic feature. In
the early stages of bootstrapping, however, rela-
tively few nouns will be assigned semantic tags,
so these features are often missing.
3.6 Thresholds and Stopping Criterion
When new instances are automatically labeled
during bootstrapping, it is critically important that
most of the labels are correct or performance
rapidly deteriorates. This suggests that we should
only label instances in which the classifier has
high confidence. On the other hand, a high thresh-
old often yields few new instances, which can
cause the bootstrapping process to sputter and halt.
To balance these competing demands, we used
a sliding threshold that begins conservatively but
gradually loosens the reins as bootstrapping pro-
gresses. Initially, we set ?cf = 2.0, which only
labels instances that the classifier is highly confi-
dent about. When fewer than min new instances
can be labeled, we automatically decrease ?cf by
0.2, allowing another batch of new instances to be
labeled, albeit with slightly less confidence. We
continue decreasing the threshold, as needed, un-
til ?cf < 1.0, when we end the bootstrapping
process. In Section 4, we show that this sliding
threshold outperforms fixed threshold values.
4 Evaluation
4.1 Data
Our data set consists of message board posts from
the Veterinary Information Network (VIN), which
is a web site (www.vin.com) for professionals in
veterinary medicine. Among other things, VIN
hosts forums where veterinarians engage in dis-
cussions about medical issues, cases in their prac-
tices, etc. Over half of the small animal veterinar-
ians in the U.S. and Canada use VIN. Analysis of
veterinary data could not only improve pet health
care, but also provide early warning signs of in-
fectious disease outbreaks, emerging zoonotic dis-
eases, exposures to environmental toxins, and con-
tamination in the food chain.
We obtained over 15,000 VIN message board
threads representing three topics: cardiology, en-
docrinology, and feline internal medicine. We did
basic cleaning, removing html tags and tokeniz-
ing numbers. For training, we used 4,629 threads,
consisting of 25,944 individual posts. We devel-
oped classifiers to identify six semantic categories:
ANIMAL, DISEASE/SYMPTOM4 , DRUG, HUMAN,
TEST, and OTHER.
The message board posts contain an abundance
of veterinary terminology and jargon, so two do-
main experts5 from VIN created a test set (answer
key) for our evaluation. We defined annotation
guidelines6 for each semantic category and con-
ducted an inter-annotator agreement study to mea-
sure the consistency of the two domain experts on
30 message board posts, which contained 1,473
noun phrases. The annotators achieved a relatively
high ? score of .80. Each annotator then labeled an
additional 35 documents, which gave us a test set
containing 100 manually annotated message board
posts. The table below shows the distribution of
semantic classes in the test set.
Animal Dis/Sym Drug Test Human Other
612 900 369 404 818 1723
To select seed words, we used the procedure
proposed by Roark and Charniak (1998), ranking
all of the head nouns in the training corpus by fre-
quency and manually selecting the first 10 nouns
that unambiguously belong to each category.7 This
process is fast, relatively objective, and guaranteed
to yield high-frequency terms, which is important
for bootstrapping. We used the Stanford part-of-
speech tagger (Toutanova et al, 2003) to identify
nouns, and our own simple rule-based NP chunker.
4.2 Baselines
To assess the difficulty of our data set and task,
we evaluated several baselines. The first baseline
searches for each head noun in WordNet and la-
bels the noun as category Ck if it has a hypernym
synset corresponding to that category. We manu-
ally identified the WordNet synsets that, to the best
of our ability, seem to most closely correspond
4We used a single category for diseases and symptoms
because our domain experts had difficulty distinguishing be-
tween them. A veterinary consultant explained that the same
term (e.g., diabetes) may be considered a symptom in one
context if it is secondary to another condition (e.g., pancre-
atitis) and a disease in a different context if it is the primary
diagnosis.
5One annotator is a veterinarian and the other is a veteri-
nary technician.
6The annotators were also allowed to label an NP as
POS Error if it was clearly misparsed. These cases were not
used in the evaluation.
7We used 20 seeds for DIS/SYM because we merged two
categories and for OTHER because it is a broad catch-all class.
280
Method Animal Dis/Sym Drug Test Human Other Avg
BASELINES
WordNet 32/80/46 21/81/34 25/35/29 NA 62/66/64 NA 35/66/45.8
Seeds 38/100/55 14/99/25 21/97/35 29/94/45 80/99/88 18/93/30 37/98/53.1
Supervised 67/94/78 20/88/33 24/96/39 34/90/49 79/99/88 31/91/46 45/94/60.7
Ind. Self-Train I.13 61/84/71 39/80/52 53/77/62 55/70/61 81/96/88 30/82/44 58/81/67.4
CROSS-CATEGORY BOOTSTRAPPED CLASSIFIERS
Contextual I.1 59/77/67 33/84/47 42/80/55 49/77/59 82/93/87 33/80/47 53/82/64.3
XCategory I.45 86/71/78 57/82/67 70/78/74 73/65/69 85/92/89 46/82/59 75/78/76.1
XCat+OSCPD I.40 86/69/77 59/81/68 72/70/71 72/69/71 86/92/89 50/81/62 75/76/75.6
XCat+OSCPD+SF I.39 86/70/77 60/81/69 69/81/75 73/69/71 86/91/89 50/81/62 75/78/76.6
Table 1: Experimental results, reported as Recall/Precision/F score
to each semantic class. We do not report Word-
Net results for TEST because there did not seem
be an appropriate synset, or for the OTHER cate-
gory because that is a catch-all class. The first row
of Table 1 shows the results, which are reported
as Recall/Precision/F score8. The WordNet base-
line yields low recall (21-32%) for every category
except HUMAN, which confirms that many veteri-
nary terms are not present in WordNet. The sur-
prisingly low precision for some categories is due
to atypical word uses (e.g., patient, boy, and girl
are HUMAN in WordNet but nearly always ANI-
MALS in our domain), and overgeneralities (e.g.,
WordNet lists calcium as a DRUG).
The second baseline simply labels every in-
stance of a seed with its designated semantic class.
All non-seed instances remain unlabeled. As ex-
pected, the seeds produce high precision but low
recall. The exception is HUMAN, where 80% of
the instances match a seed word, undoubtedly be-
cause five of the ten HUMAN seeds are 1st and 2nd
person pronouns, which are extremely common.
A third baseline trains semantic classifiers using
supervised learning by performing 10-fold cross-
validation on the test set. The feature set and
classifier settings are exactly the same as with
our bootstrapped classifiers.9 Supervised learning
achieves good precision but low recall for all cate-
gories except ANIMAL and HUMAN. In the next
section, we present the experimental results for
our bootstrapped classifiers.
4.3 Results for Bootstrapped Classifiers
The bottom section of Table 1 displays the results
for our bootstrapped classifiers. The Contextual
I.1 row shows results after just the first iteration,
8We use an F(1) score, where recall and precision are
equally weighted.
9For all of our classifiers, supervised and bootstrapped,
we label all instances of the seed words first and then apply
the classifiers to the unlabeled (non-seed) instances.
which trains only the strictly contextual classi-
fiers. The average F score improved from 53.1 for
the seeds alone to 64.3 with the contextual classi-
fiers. The next row, XCategory I.45, shows the
results after cross-category bootstrapping, which
ended after 45 iterations. (We indicate the num-
ber of iterations until bootstrapping ended using
the notation I.#.) With cross-category bootstrap-
ping, the average F score increased from 64.3 to
76.1. A closer inspection reveals that all of the se-
mantic categories except HUMAN achieved large
recall gains. And importantly, these recall gains
were obtained with relatively little loss of preci-
sion, with the exception of TEST.
Next, we measured the impact of the one-
semantic-class-per-discourse heuristic, shown as
XCat+OSCPD I.40. From Table 1, it appears that
OSCPD produced mixed results: recall increased
by 1-4 points for DIS/SYM, DRUG, HUMAN, and
OTHER, but precision was inconsistent, improv-
ing by +4 for TEST but dropping by -8 for DRUG.
However, this single snapshot in time does not tell
the full story. Figure 2 shows the performance
of the classifiers during the course of bootstrap-
ping. The OSCPD heuristic produced a steeper
learning curve, and consistently improved perfor-
mance until the last few iterations when its perfor-
mance dipped. This is probably due to the fact that
noise gradually increases during bootstrapping, so
incorrect labels are more likely and OSCPD will
compound any mistakes by the classifier. A good
future strategy might be to use the OSCPD heuris-
tic only during the early stages of bootstrapping
when the classifier?s decisions are most reliable.
We also evaluated the effect of dynamically cre-
ated semantic features. When added to the ba-
sic XCategory system, they had almost no ef-
fect. We suspect this is because the semantic fea-
tures are sparse during most of the bootstrapping
process. However, the semantic features did im-
281
0 5 10 15 20 25 30 35 40 4564
66
68
70
72
74
76
78
F
 
m
e
a
s
u
re
 
(%
)
# of iterations
 
 
independent self?training
cross?category bootstrapping
+OSCPD
+OSCPD+SemFeat
Figure 2: Average F scores after each iteration
prove performance when coupled with the OSCPD
heuristic, presumably because the OSCPD heuris-
tic aggressively labels more instances in the earlier
stages of bootstrapping, increasing the prevalence
of semantic class tags. The XCat+OSCPD+SF
I.39 row in Table 1 shows that the semantic fea-
tures coupled with OSCPD dramatically increased
the precision for DRUG, yielding the best overall F
score of 76.6.
We conducted one additional experiment to as-
sess the benefits of cross-category bootstrapping.
We created an analogous suite of classifiers using
self-training, where each classifier independently
labels the instances that it is most confident about,
adds them only to its own training set, and then
retrains itself. The Ind. Self-Train I.13 row in
Table 1 shows that these classifiers achieved only
58% recall (compared to 75% for XCategory) and
an average F score of 67.4 (compared to 76.1 for
XCategory). One reason for the disparity is that
the self-training model ended after just 13 boot-
strapping cycles (I.13), given the same threshold
values. To see if we could push it further, we low-
ered the confidence threshold to 0 and it continued
learning through 35 iterations. Even so, its final
score was 65% recall with 79% precision, which is
still well below the 75% recall with 78% precision
produced by the XCategory model. These results
support our claim that cross-category bootstrap-
ping is more effective than independently self-
trained models.
Figure 3 tracks the recall and precision scores
of the XCat+OSCPD+SF system as bootstrap-
ping progresses. This graph shows the sustained
momentum of cross-category bootstrapping: re-
0 5 10 15 20 25 30 35 40
50
55
60
65
70
75
80
85
# of iterations
 
 
Precision
Recall
Figure 3: Recall and Precision scores during
cross-category bootstrapping
call steadily improves while precision stays con-
sistently high with only a slight dropoff at the end.
4.4 Analysis
To assess the impact of corpus size, we generated
a learning curve with randomly selected subsets
of the training texts. Figure 4 shows the average F
score of our best system using 116 ,
1
8 ,
1
4 ,
1
2 ,
3
4 , and
all of the data. With just 116 th of the training set,
the system has about 1,600 message board posts
to use for training, which yields a similar F score
(roughly 61%) as the supervised baseline that used
100 manually annotated posts via 10-fold cross-
validation. So with 16 times more text, seed-based
bootstrapping achieves roughly the same results as
supervised learning. This result reflects the natural
trade-off between supervised learning and seed-
based bootstrapping. Supervised learning exploits
manually annotated data, but must make do with
a relatively small amount of training text because
manual annotations are expensive. In contrast,
seed-based bootstrapping exploits a small number
of human-provided seeds, but needs a larger set of
(unannotated) texts for training because the seeds
produce relatively sparse annotations of the texts.
An additional advantage of seed-based boot-
strapping methods is that they can easily exploit
unlimited amounts of training text. For many do-
mains, large text collections are readily available.
Figure 4 shows a steady improvement in perfor-
mance as the amount of training text grows. Over-
all, the F score improves from roughly 61% to
nearly 77% simply by giving the system access to
more unannotated text during bootstrapping.
We also evaluated the effectiveness of our slid-
ing confidence threshold (Section 3.6). The ta-
ble below shows the results using fixed thresholds
282
0 1/16 1/8 1/4 1/2 3/4 10
20
40
60
65
70
75
80
ration of data
F 
m
e
a
su
re
 
(%
)
Figure 4: Learning Curve
of 1.0, 1.5, 2.0, as well as the sliding threshold
(which begins at 2.0 and ends at 1.0 decreasing by
0.2 when the number of newly labeled instances
falls below 3000 (i.e., < 500 per category, on av-
erage). This table depicts the expected trade-off
between recall and precision for the fixed thresh-
olds, with higher thresholds producing higher pre-
cision but lower recall. The sliding threshold pro-
duces the best F score, achieving the best balance
of high recall and precision.
?cf R/P/F
1.0 71/77/74.1
1.5 69/81/74.7
2.0 65/82/72.4
Sliding 75/78/76.6
As mentioned in Section 3.3, we used 3 times
as many negative instances as positive instances
for every semantic category during bootstrap-
ping. This ratio was based on early experiments
where we needed to limit the number of neg-
ative instances per category because the cross-
category framework naturally produces an ex-
tremely skewed negative/positive training set. We
revisited this issue to empirically assess the impact
of the negative/positive ratio on performance. The
table below shows recall, precision, and F score
results when we vary the ratio from 1:1 to 5:1. A
1:1 ratio seems to be too conservative, improving
precision a bit but lowering recall. However the
difference in performance between the other ra-
tios is small. Our conclusion is that a 1:1 ratio is
too restrictive but, in general, the cross-category
bootstrapping process is relatively insensitive to
the specific negative/positive ratio used. Our ob-
servation from preliminary experiments, however,
is that the negative/positive ratio does need to be
controlled, or else the dominant categories over-
whelm the less frequent categories with negative
instances.
Neg:Pos R/P/F
1:1 72/79/75.2
2:1 74/78/76.1
3:1 75/78/76.6
4:1 75/77/76.0
5:1 76/77/76.4
Finally, we examined performance on gendered
pronouns (he/she/him/her), which can refer to ei-
ther animals or people in the veterinary domain.
84% (220/261) of the gendered pronouns were an-
notated as ANIMAL in the test set. Our classi-
fier achieved 95% recall (209/220) and 90% preci-
sion (209/232) for ANIMAL and 15% recall (6/41)
and 100% precision (6/6) for HUMAN. So while
it failed to recognize most of the (relatively few)
gendered pronouns that refer to a person, it was
highly effective at identifying the ANIMAL refer-
ences and it was always correct when it did assign
a HUMAN tag to a pronoun.
5 Conclusions
We presented a novel technique for inducing
domain-specific semantic class taggers from a
handful of seed words and an unannotated text
collection. Our results showed that the induced
taggers achieve good performance on six seman-
tic categories associated with the domain of vet-
erinary medicine. Our technique allows seman-
tic class taggers to be rapidly created for special-
ized domains with minimal human effort. In future
work, we plan to investigate whether these seman-
tic taggers can be used to improve other tasks.
Acknowledgments
We are very grateful to the people at the Veterinary
Information Network for providing us access to
their resources. Special thanks to Paul Pion, DVM
and Nicky Mastin, DVM for making their data
available to us, and to Sherri Lofing and Becky
Lundgren, DVM for their time and expertise in
creating the gold standard annotations. This re-
search was supported in part by Department of
Homeland Security Grant N0014-07-1-0152 and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
References
ACE. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
283
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings
of ANLP-97, pages 194?201.
A. Blum and T. Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-Training. In Proceed-
ings of the 11th Annual Conference on Computa-
tional Learning Theory (COLT-98).
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language In-
dependent Named Entity Recognition Combining
Morphologi cal and Contextual Evidence. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-99).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: an experimental study. Artificial Intelli-
gence, 165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of
the COLING conference, August.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-08).
D. McClosky, E. Charniak, and M Johnson. 2006. Ef-
fective self-training for parsing. In HLT-NAACL-
2006.
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the As-
sociation for Computational Linguistics.
R. Mihalcea. 2004. Co-training and Self-training for
Word Sense Disambiguation. In CoNLL-2004.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
C. Mueller, S. Rapp, and M. Strube. 2002. Applying
co-training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL-2003.
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Sri-
hari. 2003. A bootstrapping approach to named
entity classification using successive learners. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics (ACL-03), pages
335?342.
M. Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the Thirteenth
ACM International Conference on Information and
Knowledge Management, pages 137?145.
W. Phillips and E. Riloff. 2002. Exploiting Strong
Syntactic Heuristics and Co-Training to Learn Se-
mantic Lexicons. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing, pages 125?132.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?
124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1110?1116.
284
M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons Using Ex-
traction Pa ttern Contexts. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 214?221.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with
a Cyclic Dependency Network. In Proceedings of
HLT-NAACL 2003.
R. Yangarber. 2003. Counter-training in the discovery
of semantic patterns. In Proceedings of the 41th An-
nual Meeting of the Association for Computational
Linguistics.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics.
Imed Zitouni and Radu Florian. 2009. Cross-language
information propagation for arabic mention detec-
tion. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 8(4):1?21.
285
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
The goal of our research is to improve
event extraction by learning to identify sec-
ondary role filler contexts in the absence
of event keywords. We propose a multi-
layered event extraction architecture that pro-
gressively ?zooms in? on relevant informa-
tion. Our extraction model includes a docu-
ment genre classifier to recognize event nar-
ratives, two types of sentence classifiers, and
noun phrase classifiers to extract role fillers.
These modules are organized as a pipeline to
gradually zero in on event-related information.
We present results on the MUC-4 event ex-
traction data set and show that this model per-
forms better than previous systems.
1 Introduction
Event extraction is an information extraction (IE)
task that involves identifying the role fillers for
events in a particular domain. For example, the
Message Understanding Conferences (MUCs) chal-
lenged NLP researchers to create event extraction
systems for domains such as terrorism (e.g., to iden-
tify the perpetrators, victims, and targets of terrorism
events) and management succession (e.g., to iden-
tify the people and companies involved in corporate
management changes).
Most event extraction systems use either a
learning-based classifier to label words as role
fillers, or lexico-syntactic patterns to extract role
fillers from pattern contexts. Both approaches, how-
ever, generally tackle event recognition and role
filler extraction at the same time. In other words,
most event extraction systems primarily recognize
contexts that explicitly refer to a relevant event. For
example, a system that extracts information about
murders will recognize expressions associated with
murder (e.g., ?killed?, ?assassinated?, or ?shot to
death?) and extract role fillers from the surround-
ing context. But many role fillers occur in contexts
that do not explicitly mention the event, and those
fillers are often overlooked. For example, the per-
petrator of a murder may be mentioned in the con-
text of an arrest, an eyewitness report, or specula-
tion about possible suspects. Victims may be named
in sentences that discuss the aftermath of the event,
such as the identification of bodies, transportation
of the injured to a hospital, or conclusions drawn
from an investigation. We will refer to these types of
sentences as ?secondary contexts? because they are
generally not part of the main event description. Dis-
course analysis is one option to explicitly link these
secondary contexts to the event, but discourse mod-
elling is itself a difficult problem.
The goal of our research is to improve event ex-
traction by learning to identify secondary role filler
contexts in the absence of event keywords. We cre-
ate a set of classifiers to recognize role-specific con-
texts that suggest the presence of a likely role filler
regardless of whether a relevant event is mentioned
or not. For example, our model should recognize
that a sentence describing an arrest probably in-
cludes a reference to a perpetrator, even though the
crime itself is reported elsewhere.
Extracting information from these secondary con-
texts can be risky, however, unless we know that
the larger context is discussing a relevant event. To
1137
address this, we adopt a two-pronged strategy for
event extraction that handles event narrative docu-
ments differently from other documents. We define
an event narrative as an article whose main purpose
is to report the details of an event. We apply the role-
specific sentence classifiers only to event narratives
to aggressively search for role fillers in these sto-
ries. However, other types of documents can men-
tion relevant events too. The MUC-4 corpus, for ex-
ample, includes interviews, speeches, and terrorist
propaganda that contain information about terrorist
events. We will refer to these documents as fleet-
ing reference texts because they mention a relevant
event somewhere in the document, albeit briefly. To
ensure that relevant information is extracted from all
documents, we also apply a conservative extraction
process to every document to extract facts from ex-
plicit event sentences.
Our complete event extraction model, called
TIER, incorporates both document genre and role-
specific context recognition into 3 layers of analy-
sis: document analysis, sentence analysis, and noun
phrase (NP) analysis. At the top level, we train a
text genre classifier to identify event narrative doc-
uments. At the middle level, we create two types
of sentence classifiers. Event sentence classifiers
identify sentences that are associated with relevant
events, and role-specific context classifiers identify
sentences that contain possible role fillers irrespec-
tive of whether an event is mentioned. At the low-
est level, we use role filler extractors to label indi-
vidual noun phrases as role fillers. As documents
pass through the pipeline, they are analyzed at dif-
ferent levels of granularity. All documents pass
through the event sentence classifier, and event sen-
tences are given to the role filler extractors. Docu-
ments identified as event narratives additionally pass
through role-specific sentence classifiers, and the
role-specific sentences are also given to the role filler
extractors. This multi-layered approach creates an
event extraction system that can discover role fillers
in a variety of different contexts, while maintaining
good precision.
In the following sections, we position our research
with respect to related work, present the details of
our multi-layered event extraction model, and show
experimental results for five event roles using the
MUC-4 data set.
2 Related Work
Some event extraction data sets only include doc-
uments that describe relevant events (e.g., well-
known data sets for the domains of corporate ac-
quisitions (Freitag, 1998b; Freitag and McCallum,
2000; Finn and Kushmerick, 2004), job postings
(Califf and Mooney, 2003; Freitag and McCallum,
2000), and seminar announcements (Freitag, 1998b;
Ciravegna, 2001; Chieu and Ng, 2002; Finn and
Kushmerick, 2004; Gu and Cercone, 2006). But
many IE data sets present a more realistic task where
the IE system must determine whether a relevant
event is present in the document, and if so, extract
its role fillers. Most of the Message Understand-
ing Conference data sets represent this type of event
extraction task, containing (roughly) a 50/50 mix
of relevant and irrelevant documents (e.g., MUC-3,
MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)).
Our research focuses on this setting where the event
extraction system is not assured of getting only rele-
vant documents to process.
Most event extraction models can be character-
ized as either pattern-based or classifier-based ap-
proaches. Early event extraction systems used hand-
crafted patterns (e.g., (Appelt et al, 1993; Lehn-
ert et al, 1991)), but more recent systems gener-
ate patterns or rules automatically using supervised
learning (e.g., (Kim and Moldovan, 1993; Riloff,
1993; Soderland et al, 1995; Huffman, 1996; Fre-
itag, 1998b; Ciravegna, 2001; Califf and Mooney,
2003)), weakly supervised learning (e.g., (Riloff,
1996; Riloff and Jones, 1999; Yangarber et al,
2000; Sudo et al, 2003; Stevenson and Greenwood,
2005)), or unsupervised learning (e.g., (Shinyama
and Sekine, 2006; Sekine, 2006)). In addition, many
classifiers have been created to sequentially label
event role fillers in a sentence (e.g., (Freitag, 1998a;
Chieu and Ng, 2002; Finn and Kushmerick, 2004;
Li et al, 2005; Yu et al, 2005)). Research has
also been done on relation extraction (e.g., (Roth
and Yih, 2001; Zelenko et al, 2003; Bunescu and
Mooney, 2007)), but that task is different from event
extraction because it focuses on isolated relations
rather than template-based event analysis.
Most event extraction systems scan a text and
search small context windows using patterns or a
classifier. However, recent work has begun to ex-
1138
Figure 1: TIER: A Multi-Layered Architecture for Event Extraction
plore more global approaches. (Maslennikov and
Chua, 2007) use discourse trees and local syntactic
dependencies in a pattern-based framework to incor-
porate wider context. Ji and Grishman (2008) en-
force event role consistency across different docu-
ments. (Liao and Grishman, 2010) use cross-event
inference to help with the extraction of role fillers
shared across events. And there have been several
recent IE models that explore the idea of identify-
ing relevant sentences to gain a wider contextual
view and then extracting role fillers. (Gu and Cer-
cone, 2006) created HMMs to first identify relevant
sentences, but their research focused on eliminating
redundant extractions and worked with seminar an-
nouncements, where the system was only given rel-
evant documents. (Patwardhan and Riloff, 2007) de-
veloped a system that learns to recognize event sen-
tences and uses patterns that have a semantic affinity
for an event role to extract role fillers. GLACIER
(Patwardhan and Riloff, 2009) jointly considers sen-
tential evidence and phrasal evidence in a unified
probabilistic framework. Our research follows in
the same spirit as these approaches by performing
multiple levels of text analysis. But our event ex-
traction model includes two novel contributions: (1)
we develop a set of role-specific sentence classifiers
to learn to recognize secondary contexts associated
with each type of event role , and (2) we exploit text
genre to incorporate a third level of analysis that en-
ables the system to aggressively hunt for role fillers
in documents that are event narratives. In Section 5,
we compare the performance of our model with both
the GLACIER system and Patwardhan & Riloff?s
semantic affinity model.
3 A Multi-Layered Approach to Event
Extraction
The main idea behind our approach is to analyze
documents at multiple levels of granularity in order
to identify role fillers that occur in different types of
contexts. Our event extraction model progressively
?zooms in? on relevant information by first identi-
fying the document type, then identifying sentences
that are likely to contain relevant information, and
finally analyzing individual noun phrases to identify
role fillers. The key advantage of this architecture is
that it allows us to search for information using two
different principles: (1) we look for contexts that di-
rectly refer to the event, as per most traditional event
extraction systems, and (2) we look for secondary
contexts that are often associated with a specific type
of role filler. Identifying these role-specific contexts
can root out important facts would have been oth-
erwise missed. Figure 1 shows the multi-layered
pipeline of our event extraction system.
An important aspect of our model is that two dif-
ferent strategies are employed to handle documents
of different types. The event extraction task is to
find any description of a relevant event, even if the
event is not the topic of the article.1 Consequently,
all documents are given to the event sentence recog-
nizers and their mission is to identify any sentence
that mentions a relevant event. This path through the
pipeline is conservative because information is ex-
tracted only from event sentences, but all documents
are processed, including stories that contain only a
fleeting reference to a relevant event.
1Per the MUC-4 task definition (MUC-4 Proceedings,
1992).
1139
The second path through the pipeline performs
additional processing for documents that belong to
the event narrative text genre. For event narratives,
we assume that most of the document discusses a
relevant event so we can more aggressively hunt for
event-related information in secondary contexts.
In this section, we explain how we create the two
types of sentence classifiers and the role filler extrac-
tors. We will return to the issue of document genre
and the event narrative classifier in Section 4.
3.1 Sentence Classification
We have argued that event role fillers commonly oc-
cur in two types of contexts: event contexts and
role-specific secondary contexts. For the purposes
of this research, we use sentences as our definition
of a ?context?, although there are obviously many
other possible definitions. An event context is a sen-
tence that describes the actual event. A secondary
context is a sentence that provides information re-
lated to an event but in the context of other activities
that precede or follow the event.
For both types of classifiers, we use exactly the
same feature set, but we train them in different ways.
The MUC-4 corpus used in our experiments in-
cludes a training set consisting of documents and an-
swer keys. Each document that describes a relevant
event has answer key templates with the role fillers
(answer key strings) for each event. To train the
event sentence recognizer, we consider a sentence
to be a positive training instance if it contains one or
more answer key strings from any of the event roles.
This produced 3,092 positive training sentences. All
remaining sentences that do not contain any answer
key strings are used as negative instances. This pro-
duced 19,313 negative training sentences, yielding a
roughly 6:1 ratio of negative to positive instances.
There is no guarantee that a classifier trained in
this way will identify event sentences, but our hy-
pothesis was that training across all of the event
roles together would produce a classifier that learns
to recognize general event contexts. This approach
was also used to train GLACIER?s sentential event
recognizer (Patwardhan and Riloff, 2009), and they
demonstrated that this approach worked reasonably
well when compared to training with event sentences
labelled by human judges.
The main contribution of our work is introducing
additional role-specific sentence classifiers to seek
out role fillers that appear in less obvious secondary
contexts. We train a set of role-specific sentence
classifiers, one for each type of event role. Every
sentence that contains a role filler of the appropri-
ate type is used as a positive training instance. Sen-
tences that do not contain any answer key strings are
negative instances.2 In this way, we force each clas-
sifier to focus on the contexts specific to its particu-
lar event role. We expect the role-specific sentence
classifiers to find some secondary contexts that the
event sentence classifier will miss, although some
sentences may be classified as both.
Using all possible negative instances would pro-
duce an extremely skewed ratio of negative to pos-
itive instances. To control the skew and keep the
training set-up consistent with the event sentence
classifier, we randomly choose from the negative in-
stances to produce a 6:1 ratio of negative to positive
instances.
Both types of classifiers use an SVM model cre-
ated with SVMlin (Keerthi and DeCoste, 2005), and
exactly the same features. The feature set consists
of the unigrams and bigrams that appear in the train-
ing texts, the semantic class of each noun phrase3,
plus a few additional features to represent the tense
of the main verb phrase in the sentence and whether
the document is long (> 35 words) or short (< 5
words). All of the feature values are binary.
3.2 Role Filler Extractors
Our extraction model also includes a set of role filler
extractors, one per event role. Each extractor re-
ceives a sentence as input and determines which
noun phrases (NPs) in the sentence are fillers for the
event role. To train an SVM classifier, noun phrases
corresponding to answer key strings for the event
role are positive instances. We randomly choose
among all noun phrases that are not in the answer
keys to create a 10:1 ratio of negative to positive in-
stances.
2We intentionally do not use sentences that contain fillers
for competing event roles as negative instances because sen-
tences often contain multiple role fillers of different types (e.g.,
a weapon may be found near a body). Sentences without any
role fillers are certain to be irrelevant contexts.
3We used the Sundance parser (Riloff and Phillips, 2004) to
identify noun phrases and assign semantic class labels.
1140
The feature set for the role filler extractors is
much richer than that of the sentence classifiers be-
cause they must carefully consider the local context
surrounding a noun phrase. We will refer to the noun
phrase being labelled as the targeted NP. The role
filler extractors use three types of features:
Lexical features: we represent four words to the
left and four words to the right of the targeted NP, as
well as the head noun and modifiers (adjectives and
noun modifiers) of the targeted NP itself.
Lexico-syntactic patterns: we use the AutoSlog
pattern generator (Riloff, 1993) to automatically
create lexico-syntactic patterns around each noun
phrase in the sentence. These patterns are similar
to dependency relations in that they typically repre-
sent the syntactic role of the NP with respect to other
constituents (e.g., subject-of, object-of, and noun ar-
guments).
Semantic features: we use the Stanford NER tag-
ger (Finkel et al, 2005) to determine if the targeted
NP is a named entity, and we use the Sundance
parser (Riloff and Phillips, 2004) to assign seman-
tic class labels to each NP?s head noun.
4 Event Narrative Document Classification
One of our goals was to explore the use of document
genre to permit more aggressive strategies for ex-
tracting role fillers. In this section, we first present
an analysis of the MUC-4 data set which reveals the
distribution of event narratives in the corpus, and
then explain how we train a classifier to automati-
cally identify event narrative stories.
4.1 Manual Analysis
We define an event narrative as an article whose
main focus is on reporting the details of an event.
For the purposes of this research, we are only con-
cerned with events that are relevant to the event ex-
traction task (i.e., terrorism). An irrelevant docu-
ment is an article that does not mention any rele-
vant events. In between these extremes is another
category of documents that briefly mention a rele-
vant event, but the event is not the focus of the ar-
ticle. We will refer to these documents as fleeting
reference documents. Many of the fleeting reference
documents in the MUC-4 corpus are transcripts of
interviews, speeches, or terrorist propaganda com-
muniques that refer to a terrorist event and mention
at least one role filler, but within a discussion about
a different topic (e.g., the political ramifications of a
terrorist incident).
To gain a better understanding of how we might
create a system to automatically distinguish event
narrative documents from fleeting reference docu-
ments, we manually labelled the 116 relevant docu-
ments in our tuning set. This was an informal study
solely to help us understand the nature of these texts.
# of Event # of Fleeting
Narratives Ref. Docs Acc
Gold Standard 54 62
Heuristics 40 55 .82
Table 1: Manual Analysis of Document Types
The first row of Table 1 shows the distribution of
event narratives and fleeting references based on our
?gold standard? manual annotations. We see that
more than half of the relevant documents (62/116)
are not focused on reporting a terrorist event, even
though they contain information about a terrorist
event somewhere in the document.
4.2 Heuristics for Event Narrative
Identification
Our goal is to train a document classifier to automat-
ically identify event narratives. The MUC-4 answer
keys reveal which documents are relevant and irrel-
evant with respect to the terrorism domain, but they
do not tell us which relevant documents are event
narratives and which are fleeting reference stories.
Based on our manual analysis of the tuning set, we
developed several heuristics to help separate them.
We observed two types of clues: the location of
the relevant information, and the density of rele-
vant information. First, we noticed that event nar-
ratives tend to mention relevant information within
the first several sentences, whereas fleeting refer-
ence texts usually mention relevant information only
in the middle or end of the document. Therefore our
first heuristic requires that an event narrative men-
tion a role filler within the first 7 sentences.
Second, event narratives generally have a higher
density of relevant information. We use several cri-
teria to estimate information density because a sin-
gle criterion was inadequate to cover different sce-
1141
narios. For example, some documents mention role
fillers throughout the document. Other documents
contain a high concentration of role fillers in some
parts of the document but no role fillers in other
parts. We developed three density heuristics to ac-
count for different situations. All of these heuristics
count distinct role fillers. The first density heuristic
requires that more than 50% of the sentences contain
at least one role filler ( |RelSents||AllSents| > 0.5) . Figure 2
shows histograms for different values of this ratio in
the event narrative (a) vs. the fleeting reference doc-
uments (b). The histograms clearly show that docu-
ments with a high (> 50%) ratio are almost always
event narratives.
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 10
5
10
15
Ratio of Relevant Sentences
# 
of
 D
oc
um
en
ts
(a)
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 10
5
10
15
Ratio of Relevant Sentences
# 
of
 D
oc
um
en
ts
(b)
Figure 2: Histograms of Density Heuristic #1 in Event
Narratives (a) vs. Fleeting References (b).
A second density heuristic requires that the ratio
of different types of roles filled to sentences be >
50% ( |Roles||AllSents| > 0.5). A third density heuristic
requires that the ratio of distinct role fillers to sen-
tences be > 70% ( |RoleF illers||AllSents| > 0.7). If any of
these three criteria are satisfied, then the document
is considered to have a high density of relevant in-
formation.4
We use these heuristics to label a document as an
event narrative if: (1) it has a high density of relevant
information, and (2) it mentions a role filler within
the first 7 sentences.
The second row of Table 1 shows the performance
of these heuristics on the tuning set. The heuristics
correctly identify 4054 event narratives and
55
62 fleeting
reference stories, to achieve an overall accuracy of
82%. These results are undoubtedly optimistic be-
cause the heuristics were derived from analysis of
the tuning set. But we felt confident enough to move
forward with using these heuristics to generate train-
4Heuristic #1 covers most of the event narratives.
ing data for an event narrative classifier.
4.3 Event Narrative Classifier
The heuristics above use the answer keys to help de-
termine whether a story belongs to the event narra-
tive genre, but our goal is to create a classifier that
can identify event narrative documents without the
benefit of answer keys. So we used the heuristics
to automatically create training data for a classifier
by labelling each relevant document in the training
set as an event narrative or a fleeting reference doc-
ument. Of the 700 relevant documents, 292 were
labeled as event narratives. We then trained a doc-
ument classifier using the 292 event narrative docu-
ments as positive instances and all irrelevent training
documents as negative instances. The 308 relevant
documents that were not identified as event narra-
tives were discarded to minimize noise (i.e., we es-
timate that our heuristics fail to identify 25% of the
event narratives). We then trained an SVM classifier
using bag-of-words (unigram) features.
Table 2 shows the performance of the event nar-
rative classifier on the manually labeled tuning set.
The classifier identified 69% of the event narratives
with 63% precision. Overall accuracy was 81%.
Recall Precision Accuracy
.69 .63 .81
Table 2: Event Narrative Classifier Results
At first glance, the performance of this classifier
is mediocre. However, these results should be inter-
preted loosely because there is not always a clear di-
viding line between event narratives and other doc-
uments. For example, some documents begin with
a specific event description in the first few para-
graphs but then digress to discuss other topics. For-
tunately, it is not essential for TIER to have a per-
fect event narrative classifier since all documents
will be processed by the event sentence recognizer
anyway. The recall of the event narrative classifier
means that nearly 70% of the event narratives will
get additional scrutiny, which should help to find ad-
ditional role fillers. Its precision of 63% means that
some documents that are not event narratives will
also get additional scrutiny, but information will be
extracted only if both the role-specific sentence rec-
ognizer and NP extractors believe they have found
1142
Method PerpInd PerpOrg Target Victim Weapon Average
Baselines
AutoSlog-TS 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
Semantic Affinity 48/39/43 36/58/45 56/46/50 46/44/45 53/46/50 48/47/47
GLACIER 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
New Results without document classification
AllSent 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
EventSent 52/54/53 50/44/47 52/67/59 55/51/53 56/57/56 53/54/54
RoleSent 37/54/44 37/58/45 49/75/59 52/60/55 38/66/48 43/63/51
EventSent+RoleSent 38/60/46 36/63/46 47/78/59 52/64/57 36/66/47 42/66/51
New Results with document classification
DomDoc/EventSent+DomDoc/RoleSent 45/54/49 42/51/46 51/68/58 54/56/55 46/63/53 48/58/52
EventSent+DomDoc/RoleSent 43/59/50 45/61/52 51/77/61 52/61/56 44/66/53 47/65/54
EventSent+ENarrDoc/RoleSent 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
Table 3: Experimental results, reported as Precision/Recall/F-score
something relevant.
4.4 Domain-relevant Document Classifier
For comparison?s sake, we also created a docu-
ment classifier to identify domain-relevant docu-
ments. That is, we trained a classifier to determine
whether a document is relevant to the domain of
terrorism, irrespective of the style of the document.
We trained an SVM classifier with the same bag-of-
words feature set, using all relevant documents in the
training set as positive instances and all irrelevant
documents as negative instances. We use this clas-
sifier for several experiments described in the next
section.
5 Evaluation
5.1 Data Set and Metrics
We evaluated our approach on a standard benchmark
collection for event extraction systems, the MUC-4
data set (MUC-4 Proceedings, 1992). The MUC-4
corpus consists of 1700 documents with associated
answer key templates. To be consistent with previ-
ously reported results on this data set, we use the
1300 DEV documents for training, 200 documents
(TST1+TST2) as a tuning set and 200 documents
(TST3+TST4) as the test set. Roughly half of the
documents are relevant (i.e., they mention at least 1
terrorist event) and the rest are irrelevant.
We evaluate our system on the five MUC-4
?string-fill? event roles: perpetrator individuals,
perpetrator organizations, physical targets, victims
and weapons. The complete IE task involves tem-
plate generation, which is complex because many
documents have multiple templates (i.e., they dis-
cuss multiple events). Our work focuses on extract-
ing individual facts and not on template generation
per se (e.g., we do not perform coreference resolu-
tion or event tracking). Consequently, our evalua-
tion follows that of other recent work and evaluates
the accuracy of the extractions themselves by match-
ing the head nouns of extracted NPs with the head
nouns of answer key strings (e.g., ?armed guerril-
las? is considered to match ?guerrillas?)5 . Our re-
sults are reported as Precision/Recall/F(1)-score for
each event role separately. We also show an overall
average for all event roles combined.6
5.2 Baselines
As baselines, we compare the performance of our
IE system with three other event extraction sys-
tems. The first baseline is AutoSlog-TS (Riloff,
1996), which uses domain-specific extraction pat-
terns. AutoSlog-TS applies its patterns to every sen-
tence in every document, so does not attempt to
explicitly identify relevant sentences or documents.
The next two baselines are more recent systems:
the (Patwardhan and Riloff, 2007) semantic affin-
ity model and the (Patwardhan and Riloff, 2009)
GLACIER system. The semantic affinity approach
5Pronouns were discarded since we do not perform corefer-
ence resolution. Duplicate extractions with the same head noun
were counted as one hit or one miss.
6We generated the Average scores ourselves by macro-
averaging over the scores reported for the individual event roles.
1143
explicitly identifies event sentences and uses pat-
terns that have a semantic affinity for an event role
to extract role fillers. GLACIER is a probabilistic
model that incorporates both phrasal and sentential
evidence jointly to label role fillers.
The first 3 rows in Table 3 show the results for
each of these systems on the MUC-4 data set. They
all used the same evaluation criteria as our results.
5.3 Experimental Results
The lower portion of Table 3 shows the results of
a variety of event extraction models that we cre-
ated using different components of our system. The
AllSent row shows the performance of our Role
Filler Extractors when applied to every sentence in
every document. This system produced high recall,
but precision was consistently low.
The EventSent row shows the performance of
our Role Filler Extractors applied only to the event
sentences identified by our event sentence classi-
fier. This boosts precision across all event roles, but
with a sharp reduction in recall. We see a roughly
20 point swing from recall to precision. These re-
sults are similar to GLACIER?s results on most event
roles, which isn?t surprising because GLACIER also
incorporates event sentence identification.
The RoleSent row shows the results of our Role
Filler Extractors applied only to the role-specific
sentences identified by our classifiers. We see a 12-
13 point swing from recall to precision compared
to the AllSent row. This result is consistent with
our hypothesis that many role fillers exist in role-
specific contexts that are not event sentences. As ex-
pected, extracting facts from role-specific contexts
that do not necessarily refer to an event is less reli-
able. The EventSent+RoleSent row shows the re-
sults when information is extracted from both types
of sentences. We see slightly higher recall, which
confirms that one set of extractions is not a strict
subset of the other, but precision is still relatively
low.
The next set of experiments incorporates docu-
ment classification as the third layer of text analy-
sis. The DomDoc/EventSent+DomDoc/RoleSent
row shows the results of applying both types of
sentence classifiers only to documents identified as
domain-relevant by the Domain-relevant Document
(DomDoc) Classifier described in Section 4.4. Ex-
tracting information only from domain-relevant doc-
uments improves precision by +6, but also sacrifices
8 points of recall.
The EventSent row reveals that information
found in event sentences has the highest precision,
even without relying on document classification. We
concluded that evidence of an event sentence is
probably sufficient to warrant role filler extraction
irrespective of the style of the document. As we dis-
cussed in Section 4, many documents contain only
a fleeting reference to an event, so it is important
to be able to extract information from those isolated
event descriptions as well. Consequently, we cre-
ated a system, EventSent+DomDoc/RoleSent, that
extracts information from event sentences in all doc-
uments, but extracts information from role-specific
sentences only if they appear in a domain-relevant
document. This architecture captured the best of
both worlds: recall improved from 58% to 65% with
only a one point drop in precision.
Finally, we evaluated the idea of using document
genre as a filter instead of domain relevance. The
last row, EventSent+ENarrDoc/RoleSent, shows
the results of our final architecture which extracts
information from event sentences in all documents,
but extracts information from role-specific sentences
only in Event Narrative documents. This architec-
ture produced the best F1 score of 56. This model in-
creases precision by an additional 4 points and pro-
duces the best balance of recall and precision.
Overall, TIER?s multi-layered extraction architec-
ture produced higher F1 scores than previous sys-
tems on four of the five event roles. The improved
recall is due to the additional extractions from sec-
ondary contexts. The improved precision comes
from our two-pronged strategy of treating event nar-
ratives differently from other documents. TIER ag-
gressively searches for extractions in event narrative
stories but is conservative and extracts information
only from event sentences in all other documents.
5.4 Analysis
We looked through some examples of TIER?s output
to try to gain insight about its strengths and limita-
tions. TIER?s role-specific sentence classifiers did
correctly identify some sentences containing role
fillers that were not classified as event sentences.
Several examples are shown below, with the role
1144
fillers in italics:
(1) ?The victims were identified as David Lecky, director
of the Columbus school, and James Arthur Donnelly.?
(2) ?There were seven children, including four of the
Vice President?s children, in the home at the time.?
(3) ?The woman fled and sought refuge inside the
facilities of the Salvadoran Alberto Masferrer University,
where she took a group of students as hostages, threaten-
ing them with hand grenades.?
(4) ?The FMLN stated that several homes were damaged
and that animals were killed in the surrounding hamlets
and villages.?
The first two sentences identify victims, but the
terrorist event itself was mentioned earlier in the
document. The third sentence contains a perpetrator
(the woman), victims (students), and weapons (hand
grenades) in the context of a hostage situation after
the main event (a bus attack), when the perpetrator
escaped. The fourth sentence describes incidental
damage to civilian homes following clashes between
government forces and guerrillas.
However there is substantial room for improve-
ment in each of TIER?s subcomponents, and many
role fillers are still overlooked. One reason is that it
can be difficult to recognize acts of terrorism. Many
sentences refer to a potentially relevant subevent
(e.g., injury or physical damage) but recognizing
that the event is part of a terrorist incident depends
on the larger discourse. For example, consider the
examples below that TIER did not recognize as
relevant sentences:
(5) ?Later, two individuals in a Chevrolet Opala automo-
bile pointed AK rifles at the students, fired some shots,
and quickly drove away.?
(6) ?Meanwhile, national police members who were
dressed in civilian clothes seized university students
Hugo Martinez and Raul Ramirez, who are still missing.?
(7) ?All labor union offices in San Salvador were looted.?
In the first sentence, the event is described as
someone pointing rifles at people and the perpetra-
tors are referred to simply as individuals. There are
no strong keywords in this sentence that reveal this
is a terrorist attack. In the second sentence, police
are being accused of state-sponsored terrorism when
they seize civilians. The verb ?seize? is common
in this corpus, but usually refers to the seizing of
weapons or drug stashes, not people. The third sen-
tence describes a looting subevent. Acts of looting
and vandalism are not usually considered to be ter-
rorism, but in this article it is in the context of accu-
sations of terrorist acts by government officials.
6 Conclusions
We have presented a new approach to event extrac-
tion that uses three levels of analysis: document
genre classification to identify event narrative sto-
ries, two types of sentence classifiers, and noun
phrase classifiers. A key contribution of our work is
the creation of role-specific sentence classifiers that
can detect role fillers in secondary contexts that do
not directly refer to the event. Another important as-
pect of our approach is a two-pronged strategy that
handles event narratives differently from other doc-
uments. TIER aggressively hunts for role fillers in
event narratives, but is conservative about extract-
ing information from other documents. This strategy
produced improvements in both recall and precision
over previous state-of-the-art systems.
This work just scratches the surface of using doc-
ument genre identification to improve information
extraction accuracy. In future work, we hope to
identify additional types of document genre styles
and incorporate genre directly into the extraction
model. Coreference resolution and discourse anal-
ysis will also be important to further improve event
extraction performance.
7 Acknowledgments
We gratefully acknowledge the support of the Na-
tional Science Foundation under grant IIS-1018314
and the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0172. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the U.S. government.
1145
References
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
R. Bunescu and R. Mooney. 2007. Learning to Extract
Relations from the Web using Minimal Supervision.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177?210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 363?370, Ann Ar-
bor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level Boundary
Classification for Information Extraction. In In Pro-
ceedings of the 15th European Conference on Machine
Learning, pages 111?122, Pisa, Italy, September.
D. Freitag and A. McCallum. 2000. Information Ex-
traction with HMM Structures Learned by Stochas-
tic Optimization. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence, pages
584?589, Austin, TX, August.
Dayne Freitag. 1998a. Multistrategy Learning for In-
formation Extraction. In Proceedings of the Fifteenth
International Conference on Machine Learning. Mor-
gan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose Learn-
ing for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481?488, Sydney, Australia, July.
L. Hirschman. 1998. ?The Evolution of Evaluation:
Lessons from the Message Understanding Confer-
ences. Computer Speech and Language, 12.
S. Huffman. 1996. Learning Information Extraction Pat-
terns from Examples. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statisti-
cal, and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 246?260. Springer-
Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extraction
through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254?262, Columbus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
J. Kim and D. Moldovan. 1993. Acquisition of Semantic
Patterns for Information Extraction from Corpora. In
Proceedings of the Ninth IEEE Conference on Artifi-
cial Intelligence for Applications, pages 171?176, Los
Alamitos, CA. IEEE Computer Society Press.
W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and
R. Williams. 1991. University of Massachusetts: De-
scription of the CIRCUS System as Used for MUC-
3. In Proceedings of the Third Message Understand-
ing Conference (MUC-3), pages 223?233, San Mateo,
CA. Morgan Kaufmann.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on Computational Natural Language Learning,
pages 72?79, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using docu-
ment level cross-event inference to improve event ex-
traction. In Proceedings of the 48st Annual Meeting on
Association for Computational Linguistics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-Resolution
Framework for Information Extraction from Free Text.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model of
Phrasal and Sentential Evidence for Information Ex-
traction. In Proceedings of 2009 the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009).
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
1146
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the 11th National Conference on Artificial Intelli-
gence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
D. Roth and W. Yih. 2001. Relational Learning via
Propositional Algorithms: An Information Extraction
Case Study. In Proceedings of the Seventeenth In-
ternational Joint Conference on Artificial Intelligence,
pages 1257?1263, Seattle, WA, August.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive Informa-
tion Extraction using Unrestricted Relation Discovery.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 304?
311, New York City, NY, June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictionary.
In Proc. of the Fourteenth International Joint Confer-
ence on Artificial Intelligence, pages 1314?1319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resume? Infor-
mation Extraction with Cascaded Hybrid Model. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 499?506,
Ann Arbor, MI, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research, 3.
1147
