Exploiting auxiliary distributions in stochastic unification-based 
grammars 
Mark  Johnson*  
Cognitive and Linguistic Sciences 
Brown University 
Mark_Johnson@Brown.edu 
Ste fan  R iez le r  
Inst i tut  fiir Maschinelle Sprachverarbeitung 
Universit?t Stut tgart  
riezler~ims.uni-stuttgart.de 
Abst rac t  
This paper describes a method for estimat- 
ing conditional probability distributions over 
the parses of "unification-based" grammars 
which can utilize auxiliary distributions that 
are estimated by other means. We show how 
this can be used to incorporate information 
about lexical selectional preferences gathered 
from other sources into Stochastic "Unification- 
based" Grammars (SUBGs). While we ap- 
ply this estimator to a Stochastic Lexical- 
Functional Grammar, the method is general, 
and should be applicable to stochastic versions 
of HPSGs, categorial grammars and transfor- 
mational grammars. 
1 In t roduct ion  
"Unification-based" Grammars (UBGs) can 
capture a wide variety of linguistically impor- 
tant syntactic and semantic onstraints. How- 
ever, because these constraints can be non-local 
or context-sensitive, developing stochastic ver- 
sions of UBGs and associated estimation pro- 
cedures is not as straight-forward as it is for, 
e.g., PCFGs. Recent work has shown how to 
define probability distributions over the parses 
of UBGs (Abney, 1997) and efficiently estimate 
and use conditional probabilities for parsing 
(Johnson et al, 1999). Like most other practical 
stochastic grammar estimation procedures, this 
latter estimation procedure requires a parsed 
training corpus. 
Unfortunately, large parsed UBG corpora are 
not yet available. This restricts the kinds of 
models one can realistically expect to be able 
to estimate. For example, a model incorporat- 
ing lexical selectional preferences of the kind 
* This research was supported by NSF awards 9720368, 
9870676 and 9812169. 
described below might have tens or hundreds 
of thousands of parameters, which one could 
not reasonably attempt o estimate from a cor- 
pus with on the order of a thousand clauses. 
However, statistical models of lexical selec- 
tional preferences can be estimated from very 
large corpora based on simpler syntactic struc- 
tures, e.g., those produced by a shallow parser. 
While there is undoubtedly disagreement be- 
tween these simple syntactic structures and the 
syntactic structures produced by the UBG, one 
might hope that they are close enough for lexical 
information gathered from the simpler syntactic 
structures to be of use in defining a probability 
distribution over the UBG's structures. 
In the estimation procedure described here, 
we call the probability distribution estimated 
from the larger, simpler corpus an auxiliary dis- 
tribution. Our treatment of auxiliary distribu- 
tions is inspired by the treatment of reference 
distributions in Jelinek's (1997) presentation of
Maximum Entropy estimation, but in our es- 
timation procedure we simply regard the loga- 
r ithm of each auxiliary distribution as another 
(real-valued) feature. Despite its simplicity, our 
approach seems to offer several advantages over 
the reference distribution approach. First, it 
is straight-forward to utilize several auxiliary 
distributions simultaneously: each is treated as 
a distinct feature. Second, each auxiliary dis- 
tribution is associated with a parameter which 
scales its contribution to the final distribution. 
In applications such as ours where the auxiliary 
distribution may be of questionable relevance 
to the distribution we are trying to estimate, it
seems reasonable to permit the estimation pro- 
cedure to discount or even ignore the auxiliary 
distribution. Finally, note that neither Jelinek's 
nor our estimation procedures require that an 
auxiliary or reference distribution Q be a prob- 
154 
ability distribution; i.e., it is not necessary that 
Q(i2) -- 1, where f~ is the set of well-formed 
linguistic structures. 
The rest of this paper is structured as fol- 
lows. Section 2 reviews how exponential mod- 
els can be defined over the parses of UBGs, 
gives a brief description of Stochastic Lexical- 
Functional Grammar, and reviews why maxi- 
mum pseudo-likelihood estimation is both feasi- 
ble and sufficient of parsing purposes. Section 3 
presents our new estimator, and shows how it 
is related to the minimization of the Kullback- 
Leibler divergence between the conditional es- 
t imated and auxiliary distributions. Section 4 
describes the auxiliary distribution used in our 
experiments, and section 5 presents the results 
of those experiments. 
2 S tochast i c  Un i f i ca t ion -based  
Grammars  
Most of the classes of probabilistic language 
models used in computational linguistic are ex- 
ponential families. That is, the probability P(w) 
of a well-formed syntactic structure w E ~ is de- 
fined by a function of the form 
PA(w) = Q(~v) eX.f(oj ) (1) 
where f (w) E R m is a vector of feature values, 
)~ E It m is a vector of adjustable feature param- 
eters, Q is a function of w (which Jelinek (1997) 
calls a reference distribution when it is not an in- 
dicator function), and ZA = fn Q(w) ex'f(~)dw is 
a normalization factor called the partition func- 
tion. (Note that a feature here is just a real- 
valued function of a syntactic structure w; to 
avoid confusion we use the term "attribute" to 
refer to a feature in a feature structure). If 
Q(w) = 1 then the class of exponential dis- 
tributions is precisely the class of distributions 
with maximum entropy satisfying the constraint 
that the expected values of the features is a cer- 
tain specified value (e.g., a value estimated from 
training data), so exponential models are some- 
times also called "Maximum Entropy" models. 
For example, the class of distributions ob- 
tained by varying the parameters of a PCFG 
is an exponential family. In a PCFG each rule 
or production is associated with a feature, so m 
is the number of rules and the j th  feature value 
f j  (o.,) is the number of times the j rule is used 
in the derivation of the tree w E ~. Simple ma- 
nipulations how that P,x (w) is equivalent to the 
PCFG distribution ifAj = logpj, where pj is the 
rule emission probability, and Q(w) = Z~ = 1. 
If the features atisfy suitable Markovian in- 
dependence constraints, estimation from fully 
observed training data is straight-forward. For 
example, because the rule features of a PCFG 
meet "context-free" Markovian independence 
conditions, the well-known "relative frequency" 
estimator for PCFGs both maximizes the likeli- 
hood of the training data (and hence is asymp- 
totically consistent and efficient) and minimizes 
the Kullback-Leibler divergence between train- 
ing and estimated istributions. 
However, the situation changes dramatically 
if we enforce non-local or context-sensitive con- 
straints on linguistic structures of the kind that 
can be expressed by a UBG. As Abney (1997) 
showed, under these circumstances the relative 
frequency estimator is in general inconsistent, 
even if one restricts attention to rule features. 
Consequently, maximum likelihood estimation 
is much more complicated, as discussed in sec- 
tion 2.2. Moreover, while rule features are natu- 
ral for PCFGs given their context-free indepen- 
dence properties, there is no particular eason 
to use only rule features in Stochastic UBGs 
(SUBGs). Thus an SUBG is a triple (G, f,  A), 
where G is a UBG which generates a set of well- 
formed linguistic structures i2, and f and A are 
vectors of feature functions and feature param- 
eters as above. The probability of a structure 
w E ~ is given by (1) with Q(w) = 1. Given a 
base UBG, there are usually infinitely many dif- 
ferent ways of selecting the features f to make 
a SUBG, and each of these makes an empirical 
claim about the class of possible distributions 
of structures. 
2.1 S tochast i c  Lexica l  Funct iona l  
Grammar  
Stochastic Lexical-Functional Grammar 
(SLFG) is a stochastic extension of Lexical- 
Functional Grammar (LFG), a UBG formalism 
developed by Kaplan and Bresnan (1982). 
Given a base LFG, an SLFG is constructed 
by defining features which identify salient 
constructions in a linguistic structure (in LFG 
this is a c-structure/f-structure pair and its 
associated mapping; see Kaplan (1995)). Apart 
from the auxiliary distributions, we based our 
155 
features on those used in Johnson et al (1999), 
which should be consulted for further details. 
Most of these feature values range over the 
natural numbers, counting the number of times 
that a particular construction appears in a 
linguistic structure. For example, adjunct and 
argument features count the number of adjunct 
and argument attachments, permitting SLFG 
to capture a general argument attachment pref- 
erence, while more specialized features count 
the number of attachments oeach grammatical 
function (e.g., SUB J, OBJ,  COMP, etc.). 
The flexibility of features in stochastic UBGs 
permits us to include features for relatively 
complex constructions, such as date expres- 
sions (it seems that date interpretations, if 
possible, are usually preferred), right-branching 
constituent structures (usually preferred) and 
non-parallel coordinate structures (usually 
dispreferred). Johnson et al remark that they 
would have liked to have included features for 
lexical selectional preferences. While such fea- 
tures are perfectly acceptable in a SLFG, they 
felt that their corpora were so small that the 
large number of lexical dependency parameters 
could not be accurately estimated. The present 
paper proposes a method to address this by 
using an auxiliary distribution estimated from 
a corpus large enough to (hopefully) provide 
reliable estimates for these parameters. 
2.2 Estimating stochastic 
unification-based grammars 
Suppose ~ = Wl,...,Wn is a corpus of n syn- 
tactic structures. Letting fj(fJ) = ~--~=1 fj(oJi) 
and assuming each wi E 12, the likelihood of the 
corpus L~(&) is: 
T~ 
L~(~) = 1-I Px(w,) 
i=1 
= e ~/(c~) Z-~ n (2) 
0 
logan(&) = fj(Co) - -  nEa(fj) (3) 0Aj 
where E~(fj) is the expected value of f~ un- 
der the distribution P~. The maximum likeli- 
hood estimates are the )~ which maximize (2), or 
equivalently, which make (3) zero, but as John- 
son et al (1999) explain, there seems to be no 
practical way of computing these for realistic 
SUBGs since evaluating (2) and its derivatives 
(3) involves integrating over all syntactic struc- 
tures ft. 
However, Johnson et al observe that parsing 
applications require only the conditional prob- 
ability distribution P~(wly), where y is the ter- 
minal string or yield being parsed, and that this 
can be estimated by maximizing the pseudo- 
likelihood of the corpus PL~(SJ): 
rz 
PLx(SJ) = I I  P~(wilyi) 
i=-I 
n 
= eA'f(w) ~I Z;  l(yi) 
i=1 
In (4), Yi is the yield of wi and 
Z~(yi) = f~(y,) e~I(~)dw, 
(4) 
where f~(Yi) is the set of all syntactic structures 
in f~ with yield yi (i.e., all parses of Yi gener- 
ated by the base UBG). It turns out that cal- 
culating the pseudo-likelihood f a corpus only 
involves integrations over the sets of parses of 
its yields f~(Yi), which is feasible for many inter- 
esting UBGs. Moreover, the maximum pseudo- 
likelihood estimator isasymptotically consistent 
for the conditional distribution P(w\]y). For the 
reasons explained in Johnson et al (1999) we ac- 
tually estimate )~ by maximizing a regularized 
version of the log pseudo-likelihood (5), where 
aj is 7 times the maximum value of fj found in 
the training corpus: 
m ~2 
logPL~(~) - ~ 2"~2 (5) 
j= l  v j  
See Johnson et al (1999) for details of the calcu- 
lation of this quantity and its derivatives, and 
the conjugate gradient routine used to calcu- 
late the )~ which maximize the regularized log 
pseudo-likelihood f the training corpus. 
3 Aux i l i a ry  d i s t r ibut ions  
We modify the estimation problem presented in 
section 2.2 by assuming that in addition to the 
corpus ~ and the m feature functions f we are 
given k auxiliary distributions Q1,. . . ,  Qk whose 
support includes f~ that we suspect may be re- 
lated to the joint distribution P(w) or condi- 
tional distribution P(wly ) that we wish to esti- 
156 
mate. We do not require that the Qj be proba- 
bility distributions, i.e., it is not necessary that 
f~ Qj(w)dw = 1, but we do require that they 
are strictly positive (i.e., Qj(w) > O, Vw E ~). 
We define k new features fro+l,..., fm+k where 
fm+j(w) = log Qj(w), which we call auxiliary 
features. The m + k parameters associated with 
the resulting m+k features can be estimated us- 
ing any method for estimating the parameters 
of an exponential family with real-valued fea- 
tures (in our experiments we used the pseudo- 
likelihood estimation procedure reviewed in sec- 
tion 2.2). Such a procedure stimates parame- 
ters )~m+l,.-., Am+k associated with the auxil- 
iary features, so the estimated istributions take 
the form (6) (for simplicity we only discuss joint 
distributions here, but the treatment of condi- 
tional distributions i parallel). 
P,(w) = I'Ik=l QJ(w)A~+J eZ_,~=lAjlj(~)(6 ) v - - ~  
Note that the auxiliary distributions Qj are 
treated as fixed distributions for the purposes 
of this estimation, even though each Qj may it- 
self be a complex model obtained via a previous 
estimation process. Comparing (6) with (1) on 
page 2, we see that the two equations become 
identical if the reference distribution Q in (1) is 
replaced by a geometric mixture of the auxiliary 
distributions Qj, i.e., if: 
k 
Q(w) = ~I Q~(w) xm+i- 
j= l  
The parameter associated with an auxiliary fea- 
ture represents he weight of that feature in the 
mixture. If a parameter ~m+j = 1 then the 
corresponding auxiliary feature Qj is equivalent 
to a reference distribution in Jelinek's sense, 
while if ~m+j = 0 then Qj is effectively ig- 
nored. Thus our approach can be regarded as 
a smoothed version Jelinek's reference distribu- 
tion approach, generalized to permit multiple 
auxiliary distributions. 
4 Lex ica l  se lec t iona l  p re ferences  
The auxiliary distribution we used here is based 
on the probabilistic model of lexical selectional 
preferences described in Rooth et al (1999). An 
existing broad-coverage parser was used to find 
shallow parses (compared to the LFG parses) 
for the 117 million word British National Cor- 
pus (Carroll and Rooth, 1998). We based our 
auxiliary distribution on 3.7 million (g, r, a) tu- 
ples (belonging to 600,000 types) we extracted 
these parses, where g is a lexical governor (for 
the shallow parses, g is either a verb or a prepo- 
sition), a is the head of one of its NP arguments 
and r is the the grammatical relationship be- 
tween the governor and argument (in the shal- 
low parses r is always OBJ for prepositional gov- 
ernors, and r is either SUBJ or OBJ for verbal 
governors). 
In order to avoid sparse data problems we 
smoothed this distribution over tuples as de- 
scribed in (Rooth et al, 1999). We assume that 
governor-relation pairs (g, r) and arguments a 
are independently generated from 25 hidden 
classes C, i.e.: 
P((g,r,a)) = ~'~ Pe((g,r)lc)~)e(alc)ee(c) 
cEC 
where the distributions Pe are estimated from 
the training tuples using the Expectation- 
Maximization algorithm. While the hidden 
classes axe not given any prior interpretation 
they often cluster semantically coherent pred- 
icates and arguments, as shown in Figure 1. 
The smoothing power of a clustering model such 
as this can be calculated explicitly as the per- 
centage of possible tuples which are assigned a
non-zero probability. For the 25-class model 
we get a smoothing power of 99%, compared 
to only 1.7% using the empirical distribution of 
the training data. 
5 Empi r i ca l  eva luat ion  
Hadar Shemtov and Ron Kaplan at Xerox PARC 
provided us with two LFG parsed corpora called 
the Verbmobil corpus and the Homecentre cor- 
pus. These contain parse forests for each sen- 
tence (packed according to scheme described in 
Maxwell and Kaplan (1995)), together with a 
manual annotation as to which parse is cor- 
rect. The Verbmobil corpus contains 540 sen- 
tences relating to appointment planning, while 
the Homecentre corpus contains 980 sentences 
from Xerox documentation their "homecen- 
tre" multifunction devices. Xerox did not pro- 
vide us with the base LFGs for intellectual prop- 
erty reasons, but from inspection of the parses 
157 
Class  16 
PROB 0.0340 o?5 d d ~ c ;d  ?~ d dd  ~ d d d?5?~ d ?5~ ~d~dd dd  ?~d ?~ 
I .  
0.3183 say :s  \] ? ? 
0 .0405 say :o  i ? ? 
0 .0345 ask:s  ? 
0.0276 te l l : s  ? ? 
0 .0214 be:s  ? 
0 .0193 know:s  ? 
0 .0147 h&ve:s  
0.0144 nod:s  ? 
0 .0137 th lnk :s  ? 
0 .0130 shake:s  ? 
0.0128 take :s  ? 
0 .0104 rep ly :s  ? 
0 .0096 smi le :s  ? 
10.0094 do:s  
0.0094 laugh:s  ? 
0.0089 te lho  
0.0084 saw:s  ? 
~ 0.0082 add:s  ? 
0.0078 feehs ? 
0.0071 make:s  ? 
0.0070 g ive:s  ? ? 
0 .0067 ask :o  ? 
0.0066 shrug:s  ? 
0 .0061 exp la in :s  ? ? 
0 .0051 l ike:s ? 
0 .0050 Iook:s 
0 .0050 s igh:s  ? 
0 .0049 watch :s  ? 
0 .0049 hear :s  
0.0047 answer :s  ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  
? ? Q ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? 
? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? 
? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? o ? o ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? o ? 
. . . .  . . . . . . . . . . . . . . .  : : : : : : : :  
? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  
? ? ? ? ? ? ? ? ? ? ? o o ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? Q t ? ? ? ? 
? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? Q ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
Figure 1: A depiction of the highest probability predicates and arguments in Class 16. The class 
matrix shows at the top the 30 most probable nouns in the Pe (a116) distribution and their probabil- 
ities, and at the left the 30 most probable verbs and prepositions listed according to Pre((g, r)116) 
and their probabilities. Dots in the matrix indicate that the respective pair was seen in the training 
data. Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; the 
suffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions. 
it seems that slightly different grammars were 
used with each corpus, so we did not merge the 
corpora. We chose the features of our SLFG 
based solely on the basis of the Verbmobil cor- 
pus, so the Homecentre corpus can be regarded 
as a held-out evaluation corpus. 
We discarded the unambiguous sentences in 
each corpus for both training and testing (as 
explained in Johnson et al (1999), pseudo- 
likelihood estimation ignores unambiguous en- 
tences), leaving us with a corpus of 324 am- 
biguous sentences in the Verbmobil corpus and 
481 sentences in the Homecentre corpus; these 
sentences had a total of 3,245 and 3,169 parses 
respectively. 
The (non-auxiliary) features used in were 
based on those described by Johnson et 
al. (1999). Different numbers of features 
were used with the two corpora because 
some of the features were generated semi- 
automatically (e.g., we introduced a feature for 
every attribute-value pair found in any feature 
structure), and "pseudo-constant" features (i.e., 
features whose values never differ on the parses 
of the same sentence) are discarded. We used 
172 features in the SLFG for the Verbmobil cor- 
pus and 186 features in the SLFG for the Home- 
centre corpus. 
We used three additional auxiliary features 
derived from the lexical selectional preference 
model described in section 4. These were de- 
fined in the following way. For each governing 
predicate g, grammatical relation r and argu- 
ment a, let n(g,r,a)(w) be the number of times 
that the f-structure: 
PRED ~ g \] 
r = \[PRED=a\] 
appears as a subgraph of the f-structure of 
w, i.e., the number of times that a fills the 
158 
grammatical role r of g. We used the lexical 
model described in the last section to estimate 
P(alg , r), and defined our first auxiliary feature 
as :  
ft(w) = logP(g0) + Z n(g,r,a)(w)l?gP(al g,r) 
(g,r,a) 
where g0 is the predicate of the root feature 
structure. The justification for this feature is 
that if f-structures were in fact a tree, ft(w) 
would be the (logarithm of) a probability dis- 
tribution over them. The auxiliary feature ft 
is defective in many ways. Because LFG f- 
structures are DAGs with reentrancies rather 
than trees we double count certain arguments, 
so ft is certainly not the logarithm of a prob- 
ability distribution (which is why we stressed 
that our approach does not require an auxiliary 
distribution to be a distribution). 
The number of governor-argument tuples 
found in different parses of the same sentence 
can vary markedly. Since the conditional prob- 
abilities P(alg, r) are usually very small, we 
found that ft(w) was strongly related to the 
number of tuples found in w, so the parse with 
the smaller number of tuples usually obtains the 
higher fl score. We tried to address this by 
adding two additional features. We set fc(w) to 
be the number of tuples in w, i.e.: 
fc(w) = Z n(g,r,a)(w) ? 
(g,r,a) 
Then we set .Q(w) = h(w)/L(w), i . e . , / , (w)  is 
the average log probability of a lexical depen- 
dency tuple under the auxiliary lexical distribu- 
tion. We performed our experiments with ft as 
the sole auxiliary distribution, and with ft, fe 
and fn as three auxiliary distributions. 
Because our corpora were so small, we trained 
and tested these models using a 10-fold cross- 
validation paradigm; the cumulative results are 
shown in Table 1. On each fold we evaluated 
each model in two ways. The correct parses 
measure simply counts the number of test sen- 
tences for which the estimated model assigns 
its maximum parse probability to the correct 
parse, with ties broken randomly. The pseudo- 
likelihood measure is the pseudo-likelihood f
test set parses; i.e., the conditional probability 
of the test parses given their yields. We actu- 
ally report he negative log of this measure, so a 
smaller score corresponds to better performance 
here. The correct parses measure is most closely 
related to parser performance, but the pseudo- 
likelihood measure is more closely related to the 
quantity we are optimizing and may be more 
relevant to applications where the parser has to 
return a certainty factor associated with each 
parse. 
Table 1 also provides the number of indistin- 
guishable sentences under each model. A sen- 
tence y is indistinguishable with respect o fea- 
tures f iff f(wc) : f(w'), where wc is the correct 
parse of y and wc ~ w I E ~(y), i.e., the feature 
values of correct parse of y are identical to the 
feature values of some other parse of y. If a 
sentence is indistinguishable it is not possible 
to assign its correct parse a (conditional) prob- 
ability higher than the (conditional) probability 
assigned to other parses, so all else being equal 
we would expect a SUBG with with fewer indis- 
tinguishable sentences to perform better than 
one with more. 
Adding auxiliary features reduced the already 
low number of indistinguishable sentences in the 
Verbmobil corpus by only 11%, while it reduced 
the number of indistinguishable sentences in the 
Homecentre corpus by 24%. This probably re- 
flects the fact that the feature set was designed 
by inspecting only the Verbmobil corpus. 
We must admit disappointment with these 
results. Adding auxiliary lexical features im- 
proves the correct parses measure only slightly, 
and degrades rather than improves performance 
on the pseudo-likelihood measure. Perhaps this 
is due to the fact that adding auxiliary features 
increases the dimensionality of the feature vec- 
tor f ,  so the pseudo-likelihood scores with dif- 
ferent numbers of features are not strictly com- 
parable. 
The small improvement in the correct parses 
measure is typical of the improvement we might 
expect to achieve by adding a "good" non- 
auxiliary feature, but given the importance usu- 
ally placed on lexical dependencies in statistical 
models one might have expected more improve- 
ment. Probably the poor performance is due 
in part to the fairly large differences between 
the parses from which the lexical dependencies 
were estimated and the parses produced by the 
LFG. LFG parses are very detailed, and many 
ambiguities depend on the precise grammatical 
159 
Verbmobi l  corpus  (324 sentences, 172 non-auxiliary features) 
Auxi l iary  features  used Ind is t ingu ishab le  Cor rect  - log PL  
(none) 9 180 401.3 
fl 8 183 401.6 
f,, fc, .f. 8 180.5 404.0 
Homecentre  corpus (481 sentences, 186 non-auxiliary features) 
Aux i l ia ry  features  used Ind is t ingu ishab le  Cor rect  - log PL  
(none) 45 283.25 580.6 
fl 34 284 580.6 
f l, f c, f n 34 285 582.2 
Table h The effect of adding auxiliary lexical dependency features to a SLFG. The auxiliary 
features are described in the text. The column labelled "indistinguishable" gives the number of 
indistinguishable s ntences with respect o each feature set, while "correct" and "- log PL" give 
the correct parses and pseudo-likelihood measures respectively. 
relationship holding between a predicate and its 
argument. It could also be that better perfor- 
mance could be achieved if the lexical dependen- 
cies were estimated from a corpus more closely 
related to the actual test corpus. For example, 
the verb feed in the Homecentre corpus is used in 
the sense of "insert (paper into printer)", which 
hardly seems to be a prototypical usage. 
Note that overall system performance is quite 
good; taking the unambiguous sentences into 
account he combined LFG parser and statisti- 
cal model finds the correct parse for 73% of the 
Verbmobil test sentences and 80% of the Home- 
centre test sentences. On just the ambiguous 
sentences, our system selects the correct parse 
for 56% of the Verbmobil test sentences and 59% 
of the Homecentre test sentences. 
6 Conc lus ion  
This paper has presented a method for incorpo- 
rating auxiliary distributional information gath- 
ered by other means possibly from other corpora 
into a Stochastic "Unification-based" Grammar 
(SUBG). This permits one to incorporate de- 
pendencies into a SUBG which probably can- 
not be estimated irectly from the small UBG 
parsed corpora vailable today. It has the virtue 
that it can incorporate several auxiliary dis- 
tributions imultaneously, and because it asso- 
ciates each auxiliary distribution with its own 
"weight" parameter, it can scale the contribu- 
tions of each auxiliary distribution toward the 
final estimated istribution, or even ignore it 
entirely. We have applied this to incorporate 
lexical selectional preference information into 
a Stochastic Lexical-Functional Grammar, but 
the technique generalizes to stochastic versions 
of HPSGs, categorial grammars and transfor- 
mational grammars. An obvious extension of 
this work, which we hope will be persued in the 
future, is to apply these techniques in broad- 
coverage feature-based TAG parsers. 
References  
Steven P. Abney. 1997. Stochastic Attribute- 
Value Grammars. Computational Linguis- 
tics, 23(4):597-617. 
Glenn Carroll and Mats Rooth. 1998. Valence 
induction with a head-lexicalized PCFG. In 
Proceedings of EMNLP-3, Granada. 
Frederick Jelinek. 1997. Statistical Methods for 
Speech Recognition. The MIT Press, Cam- 
bridge, Massachusetts. 
Mark Johnson, Stuart Geman, Stephen Canon, 
Zhiyi Chi, and Stefan Riezler. 1999. Estima- 
tors for stochastic "unification-based" gram- 
mars. In The Proceedings of the 37th Annual 
Conference of the Association for Computa- 
tional Linguistics, pages 535-541, San Fran- 
cisco. Morgan Kaufmann. 
Ronald M. Kaplan and Joan Bresnan. 1982. 
Lexical-Functional Grammar: A formal sys- 
tem for grammatical representation. In Joan 
Bresnan, editor, The Mental Representation 
of Grammatical Relations, chapter 4, pages 
173-281. The MIT Press. 
160 
Ronald M. Kaplan. 1995. The formal architec- 
ture of LFG. In Mary Dalrymple, Ronald M. 
Kaplan, John T. Maxwell III, and Annie 
Zaenen, editors, Formal Issues in Lexical- 
Functional Grammar, number 47 in CSLI 
Lecture Notes Series, chapter 1, pages 7-28. 
CSLI Publications. 
John T. Maxwell III and Ronald M. Kaplan. 
1995. A method for disjunctive constraint 
satisfaction. In Mary Dalrymple, Ronald M. 
Kaplan, John T. Maxwell III, and Annie 
Zaenen, editors, Formal Issues in Lexical- 
Functional Grammar, number 47 in CSLI 
Lecture Notes Series, chapter 14, pages 381- 
481. CSLI Publications. 
Mats Rooth, Stefan Riezler, Detlef Prescher, 
Glenn Carroll,, and Franz Beil. 1999. Induc- 
ing a semantically annotated lexicon via EM- 
based clustering. In Proceedings of the 37th 
Annual Meeting of the Association .for Com- 
putational Linguistics, San Francisco. Mor- 
gan Kaufmann. 
161 
Explaining away ambiguity: Learning verb selectional preference 
with Bayesian networks* 
Mass imi l iano  C iarami ta  and Mark  Johnson  
Cognit ive and l , inguist ic  Sci('.n(:es 
Box \]978, Brown Univers i ty 
Pr()vi(h;n(:e, l{,I 02912, USA 
mas s ?m?:l J_ano_ciaramita@broun. edu mj @cs. broun, edu 
Abst ract  
This t)at)er presents a Bayesian lnodel for unsu- 
t)(;rvised learning of v(;rb s(;le(-t;ional t)refer(;nc('s. 
For each vert) the model creates a 13~Wo.sian 
n(~twork whose archii;e(:lan'(~ is (l(fl;(wmin(;d t).5' 
the h',xical hicr~m:hy of W()r(hmt mtd whos('~ 
1)ar~mmt;(;rs are ('~sl;im;~l:(~d from a list; of v('d'l)- 
ol)je('t pairs \]'cram from a tort)us. "lgxl)laining 
away", t~ well-known t)rop('xi;y of Baycsi~m net- 
works, helps the moth;1 (teal in a natural  fash- 
ion with word sense aml)iguity in tlw, training 
(tat~L. ()n a, word sense disamt)igu;~tion Lest our 
model t)erformed l)ctl;c,r than ot;h(',r stal;(~ of tim 
art systems for unSUl)crvis(~d learning ()t7 seh'x:- 
tionM t)r(d'er(mces. Coml)utational (:Oml)lcxity 
l)rol)lems, wn.ys of improving tiffs ;tl)l)roa(:h mM 
methods for iml)h'menting "('xt)laining away" in 
oth(;r graphical frameworks are dis('ussed. 
1 Se lec t iona l  p re ference  and sense 
ambigu i ty  
R('.gularil;i('~s of avcrt) with rcsl)e(:t o t;lw. seman- 
tic class of its m:guments (sul)j('.cl:, ol)j('.(:l; mM 
indirect o\])je(:l;) arc called selectional prefer- 
enees (S1)) (Katz and Fodor, 1964; Chomsky, 
1965; Johnson-Laird, 1983). The verb pilot car- 
ries the information thal; its ol)jecl; will likely 1)e 
some kind of veh, icle; sut)jects of tim vert) th, int,: 
t(md to 1)e h,'uman; ;rod sul)jects of the verb bark 
l;end l;o l)c, dogs. For the sake of simt)licity we 
will locus on the verl)-ot)je(:t relation all;hough 
the techniques we will describe can be at)t)li(;d 
to other verb-argument pairs. 
* We wouhl like to ttmnk the Brown Lal)orat;ory for Lin- 
guistic Inibrmation Processing; Thomas IIoflnann; Elie 
Bienenstoek; 1)hilip Resnik, who provided us with train- 
ing and test data; and Daniel Oarcia for his hel ) with the 
SMILE lil)rary of (:lasses tbr Bayesian etworks that we 
used for our exl)eriments. This research was SUl)l)orted 
1)y NSF awards 9720368, 9870676 and 9812169. 
. EN77TY._ 
,vomet\]tiug - " ~ , " - . .  " ~ ,~ . 
. _ " " CtHHy 
FOOl )  I , IQUl l )  PIISI:h'ICAI. O l t J I iCT 
..... "? / '  i " \  , I , \  \ \  . . . .  ; -  " \ \ ,  ICK .lbod 
/ I I . IM l iNT III'VI;'RA(;I" WAT/ : 'R  liqltid lAND olejeet ": 
? / 
71<4 bel'i'rat~e ('OI"H'.'I'; dr ink carlh land ISLAND ~ro l l ( 'APE  
<'q~lFe I;'S1't?1:'S.';0 .IA VA- I ,I/'~ VA-2 IIAI,I i.~hmd 
t . . . . . . . . . . . . . . .   i 
e.v~rcs.~o .\]av~ bali 
Figm'e 1: A 1)ortion of Wordnet. 
Models of the acquisition of SP arc impor- 
|;ant in their own right and h;w(', at)plic~tt.ions in 
N~tural ,anguage l~ro(:essing (NIA'). The selcc- 
tional l)rclhr(;nc(;s of ~L verb can b(; used t;o inti;r 
I;he l)ossil)\]c meanings of an mlknown re'gum(mr 
of a known verb; e.g., it; might be possibh; to 
infer that  xzzz  is ~ kind ot! dog front the tbllow- 
ing sentence: "The :rzJ:z barked all night". In 
p~rsing ;~ sentence seh,ctional l)refe, rcn(:es can 1)(; 
used to rank competing parses, providing a par- 
tim nlt',asur(; of scmmlt;ic well-forlnedness, in- 
v('stigating SI ) might hel l) us to understand the 
structure of the mental lexicon. 
Systems for mlsupervised learning of SP usu- 
ally combine statistical aim knowledge-1)ased 
approaches. The knowledge-base component 
is typicMly a database that  groups words into 
classes. In the models w(' will see. the knowl- 
edge base is Wordnet (Miller, 1990). Word- 
net groups nouns into c, lasses of synonyms 
ret)resenting concel)ts , called synsets ,  e.g., 
{car,,,'ld, o,a, 'utomobilc,. . .}. A noun that lm- 
hmgs to sew:ral synsets is ambiguous .  A t ran-  
187 
sitive and asymmetrical relation, hyponymy,  
is defined between synsets. A synset is a hy- 
ponym of another synset if the tbrmer has the 
latter as a broader concept; for example, BEV- 
ERAGE is a hyponym of LIQ UID. Figure 1 de- 
picts a portion of the hierarchy. 
The statistical component consists of 
predicate-argument pairs extracted from a 
corpus in which the semantic lass of the words 
is not indicated. A trivial algorithm might 
get a list of words that occurred as objects 
of the verb and output the semantic classes 
the words belong to according to Wordnet. 
For example, if the verb drink occurred with 
'water and water E L IQUID,  the nlodel would 
learn that drink selects tbr LIQUID. As Resnik 
(1997) and abney and Light (1999) have found, 
the main problem these systems face is the 
presence of ambiguous words in the training 
data. If the word java also occurred as an 
object of drink, since java C BEVERAGE 
and java C ISLAND,  this model would learn 
that drink selects tbr both BEVERAGE and 
ISLAND.  
More complex models have been t)roposed. 
These models, though, deal with word sense 
ambiguity by applying an unselective strategy 
similar to the one above; i.e., they assmne that 
anfl)iguous words provide equal evidence tbr all 
their senses. These models choose as the con- 
eepts the verb selects tbr those that are in com- 
mon among several words (e.g., BEVERAGE 
above). This strategy works to the extent that 
these overlapping senses are also the concepts 
the verb selects tbr. 
2 Prev ious  approaches to learn ing  
se lect iona l  preference 
2.1 P~esnik's model 
Ours system is closely related to those proposed 
in (Resnik, 1997) and (Atmey and Light, 1999). 
The fact; that a predicate p selects for a class 
c, given a syntactic relation r, can be repre- 
sented as a relation, selects(p, r, c); e.g., that 
eat selects for FOOD in object position can 
be represented as selects(eat, object, FOOD). 
In (Resnik, 1997) selectional preference is quan- 
tiffed by comparing the prior distribution of 
a given class c appearing as an argument, 
P(c)~ and the conditional probability of |;lie 
same class given a predicate and a syntac- 
COGNI770N 1/4 FOOD 7/4 
ESSENCE 1/4 FLESH 1/4 FRUH" 1/2 BREAD I/2 DAIRY I/2 
l' J l l l 
idea(O) meal(l) apple(I) bagel(l) cheese(l) 
Figure 2: Simplified Wordnet. The numbers 
next to the synsets represent he values of 
freq(p, r, c) estimated using (3), the nmnbers in 
l)arentheses represent the values of frcq(p, r, w). 
tic relation P(c\[p,r),  e.g., P(FOOD) and 
J ( OODleat, object), The relative ntropy be- 
tween P(c) and P(clp, r) measures how nmch 
the predicate constrains its arguments: 
S(p,r) = D(P(clp, r) II P(c)) (1) 
Resnik defines the se lect ional  assoc iat ion of 
a predicate for a particular class c to be the por- 
tion of the selectional preference strength due to 
that class: 
P(clP, ~') 1 P(clp , r) log (2) A(p,, . ,  c) - S(v, , ' )  P(c)  
Here the main problem is the estimation of 
P(clp, r). Resnik suggests  as  a plausil)le esti- 
mator /5(clp, r) de=r freq(p, r, c)/freq(p, r). 13ut 
since the model is trained on data that are not 
sense-tagged, there is no obvious way to esti- 
mate freq(p, r, c). Resnik suggests considering 
each observation of a word as evidence tbr each 
of the classes |;lie word belongs to, 
count(p, r, w) 
c) aasses(w) (3) 
~uc:e 
where count(p, r, w) is the nmnber of times 
the word w occurred as an argument of p in 
relation r, and classes(w) is the number of 
classes w belongs to. For exmnple, suppose 
the system is trained on (eat, object) pairs and 
the verb occurred once each with meat, ap- 
ple, bagel, and cheese, and Wordnet is simpli- 
fied as in Figure 2. An ambiguous word like 
meat provides evidence also tbr classes that ap- 
pear unrelated to those selected by the verb. 
Resnik's assumption is that only the classes e- 
lected by the verb will be associated with each 
188 
( )  ,-<., 
/ / - "  1\8 \ \ - .~S  
COGNIllON ( ) FOOl) 
1\7 " - .  " - - -  
( - - .  1 1 ' I ' I ' 
idea meat apph' bagel cheeae 
Figure 3: The HMM version of the siml)le ex- 
ample. 
of the observed words, and hence will re(:eive 
the highest values for l'(clp, r). Using (3) we 
fill(t that the highesl; fre(luen(;y is in t'~t(;t as- 
social;ed with FOOD: .f'req(ea, I, objecl,, food) 
I + ~ + 7 _1_ 2 1  ~ ~ -- 717 an(t I'(leOODie.,l, ) = 0.44. 
H()wever, some eviden('e is tomM also for COG- 
NIT ION:  .fr(:q(cat,, obj,cl,, co9'~,,it, io'~,,) ~ ? and 
I ' (COGNIT ' IONieat)  = 0.06. 
2.2 Abney  and  L ight ' s  approach  
Atmey and Light (1999) pointed ouI; l;h~tt l;he 
distri l)ution of se.llSeS of all anfl)iguous word is 
no|; unifornl. '\['tiey not;iced also l:hai; it is nol; 
clear how the 1)rol)ability l'((:\[p, 7") is t;o 1)e ini;er- 
t)reted sin(:e there is no exl)lieit sto(:llasl, i(: geil- 
(',ration nlodel involved. 
They \])rol)ose(t a syst;enl l;hat; ass()(',ial;es 
a Hidden Markov Model (\[IMM) wii;h 0,a(:h 
l)re(lic~te-re, lal;ion 1)air (p,r). 'l~'ansil;ions be- 
tween synsel; states rel)resent he hyt)onynly re- 
lation, and c, the empty word, is emitted with 
probal)ility 1; transit ions to ~t tinal sta|;e enli|; a 
word w with 1)rot)al)ility 0 < P('m) < I. 35an- 
sit|on and emission t)rol)al)ilities are est imated 
using the \]'DM ;dgorittnn on i;raining data I;hat 
consist of the lieu|is that o(;clirrext wi th  the w;rb. 
Abney and Ligh|,'s mo(lel e, stintates i '(clp, r ) 
ti'om the model trained tbr (p,r); the disl, ri- 
l int|on P(c) (:ml 1)e calculatxxt from a model 
trained for all n(mns in the (:orpus. 
This model did not perti)rm as well as ex- 
pected. All amt)iguous word ill the nlodel call 
be generated 1)y more than one sl;;~te sequence. 
Atmey and Light dis(:overed that the EM al- 
goril;hm tinds t)arameter wflues that associate 
some t)rol)ability mass with all the trmlsitions 
in the lnultil)le paths that lead to an ambigu- 
ous word. In other words, when there are sev- 
eral state sequences fi)r the same word, \]DM does 
not select one of l;hen: over the others. I Figure 3 
shows the I)arameters esl;imated by EM for the 
same examt)le as above. The transit ion to the 
COGNITION sl;ate has \])een assigne(t a t)rol)a- 
1)|lily of 1/8 because it is part of ~t possible l)ath 
to meat. The I IMM nlodel does not solve t, he 
l)roblent of the unselective distr ibution of the 
frequen(:y of oceurren(:e of an aml)iguous word 
to all its senses. A1)ney and Light claimed that; 
this is a serious l)roblem, par|;i(:iilarly when l;he 
aml)iguous word is a ti'equent one, and cruised 
the model to learn the wrong seleel;ional pref- 
eren('es. To (:orre(:i; this undesiral)le outcome 
they introduced some smoothing and t)alam:ing 
te, chniques. Howe, ver, even with these modiliea- 
tions their sysl;em's l)erfornlance, was t)elow fllal; 
a(:hieved l)y Resnik's. 
3 Bayes ian  networks  
A Bayes ian  network  (Pearl, 1988), or 
Bayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol; 
of var iab les  and a sel; of d i rec ted  edges  (:on- 
neel;ing the w~riat)les. The variables and tile 
edges detine, a dire,(:te, d acyclic graph (DAG) 
where each wtrial)le is rei)resented 1)y ~ node. 
Ea(:h vmi~d)le is asso(:iated with a finite number 
of (nmi;u;dly ex('lusive) sl;ates. '1)) each wu:ial)le 
A with \]);n'eni;s \]31,..., I7~ is ;t|;l;;mll(',(l ;t condi- 
tio'n, al probability tabh', (CPT) l'(A\[131, ...,Hn). 
Given a BBN, Bayesiml int~rence (:~m 1)e used 
1;o esi;ilm~l;e marg ina l  and poster io r  p roba-  
b i l i t ies  given the evidence at hand ~md (;It(', in- 
fornlation six)red in the CPTs,  the pr io r  prob-  
ab i l i t ies ,  by means of B~yes' rule, P(H IE  ) = 
l'(H)P(s~ln) P(E) , where It stands fi)r hyt)othesis and 
E tbr eviden(:e. 
Baye, sian nel;works display ml exl;remely inter- 
eslfing t)roi)ert;y called exp la in ing  away.  Word 
sense mnbiguity in the 1)recess of learning SP de,- 
tines a 1)rot)lem that nlight, l)e solved by a model 
that imt)lements an explaining away strategy. 
Sul)t)ose we ;~re learning the, selectional 1)refer- 
en(:e of drink, and the network ill Figure 4 is the 
'As a nmtt;er of fi*cl;, for this HMM there are (in- 
finitely) many i )a ramel ;e r  vahles that nmxinfize the like- 
lihood of t;he training data; i.e., l;he i)arame, l;ers are, not; 
idenl;ifiable. The intuil;ively correct solution is one of 
l;helll, \])ILL SO are infinitely lilalty ()|;her, intuitively incor- 
re(:t; ones. Thus il, is no surprise l;hat the EM algorithm 
emmet; lind the intuitively correct sohlt;ion. 
189 
I ISIAND ?> BEVERAGE 
.\](11'(1 0 WllICF 
Figure 4: A Bayesian network for word ainbigu- 
ity. 
knowledge base. The verb occurred with java 
and water. This situation can be ret)resented 
as a Bayesian network. The variables ISLAND 
and BEVERAGE represent concepts in a se- 
mantic hierarchy. The wtriables java and water 
stand for I)ossible instantiations of the concet)ts. 
All the w,riables are Boolean; i.e., they are as- 
sociated with two states, true or false. Suppose 
the tbllowing CPTs define the priors associated 
with each node. 2 The unconditional probabili- 
ties are P ( I  = t,'~,,e) = P (B  = t,'~,,~0 = 0.01 and 
P( I  = false) = I"(13 = false) = 0.99, and the 
CPTs for the child nodes are 
I =>)  I 
1,13 1, ~13 ~I,13 -~I, ~f3 
j = true 0.99 (I.99 0.99 0.01 
j = false 0.01 0.01 0.01 0.99 
w = true 0.99 0.99 0.01 (I.01 
w false. 0.01 0.01 0.99 0.99 
These vahms mean that the occu,'rence of either 
concept is a priori unlikely. If either concept is 
true the word java is likely to occur. Similarly, 
if BE VERA GE occurs it; is likely to observe also 
the word water. As the posterior probabilities 
show, if java occurs, the belief~ in both concepts 
increase: P(II.j) = P(B I j  ) = 0.3355. However, 
'water provides evidence for BEVERAGE only. 
Overall there is more evidence for the hypoth- 
esis that the concept being expressed is BEV- 
ERAGE and not ISLAND. Bayesian networks 
implement his inference scheme; if we compute 
the conditional probabilities given that both 
words occurred, we obtain P(BI j  , w) = 0.98 and 
P(I\[ j ,  w) = 0.02. The new evidence caused the 
"island" hyt)othesis to be explained away! 
3.1 The relevance of priors 
Explaining away seems to depend on the spec- 
ification of the prior prolmt)ilities. The priors 
2I, 13, j and w abbrev ia te  ISLAND, 1lEVERAGE, 
java and water, respect ively.  
(-)coaNmo,v ) roe, \[0 ii~ 
\] L_ m 
Figm'e 5: A Bayesian network for the simple 
example. 
define the background knowledge awdlable to 
the model relative to the conditional probabili- 
ties of the events represented by the variables, 
but also about the joint distributions of several 
events. In the simple network above, we de- 
fined the probat)ility that either concept is se- 
lected (i.e., that the correst)onding variable is 
true) to be extremely small. Intuitively, there 
are many concepts and the probability of ob- 
serving any particular one is small. This means 
that the joint probability of the two events is 
much higher in the case in which only one of 
them is true (0.0099) than in the case in which 
they are both true (0.0001). Therefore, via the 
priors, we introduced a bias according to which 
the hypothesis that one concept is selected will 
be t/wored over two co-occurring ones. This is a 
general pattern of Bayesian networks; the prior 
causes impler explanations to be preferred over 
more complex ones, and thereby the explaining 
away effect. 
4 A Bayesian network approach to 
learning select ional  preference 
4.1 Structure and parameters of the 
model 
The hierarchy of nouns in Wordnet defines a 
DAG. Its mapping into a BBN is straightibr- 
ward. Each word or synset in Wordnet is a 
node in the network. If A is a hyponym of B 
there is an are in the network from B to A. All 
the variables are Boolean. A synset node is true 
if the verb selects for that class. A word node is 
true if the word can appear as an argument of 
the verb. The priors are defined tbllowing two 
intuitive principles. First, it is unlikely that a 
verb a priori selects for troy particular synset. 
Second, if a verb does select for a synset, say 
FOOD, then it; is likely that it also selects tbr 
190 
i(;s hyl)onyms, say FR.UFI'. The sam(', l>rin(:il)les 
;H)t)Iy (;o words: it is likely l;h~t a wor(t ;q)I)ears 
as an m:gmnent of t,h(; verl) if the vert) seh;(:l;s for 
any of il;s possible senses. On l;he other h~m(t, 
if (,he verb does nol; selecl; for a synsel;, it; is 
'u, nlikely that  the words insl;anl;b~l;ing (;he synse(; 
occur ~s its ;~rgumen~s. "Likely" and "unlikely" 
are given mml(;rical values l;h~l; Sllill 1l t) 1;o 1. 
The following l;at)le defines l;\]te scheme for the 
CPTs  associated wil;h each node in the nel;work; 
p i (X)  (lcnot;cs 1;12(: il.h, t2ar(:n(, of (;h(: no(t(: X. 
F__ \ ] .  P(X = xlI, I(X)V,... , VI,,(X) = t,",,(~) \] 
z .lalsc, unl ihc ly 
\[_ T l " (X = : , : lp , (X )a , . . . , /> , , (X )= .l',,.Z.~(:) \] 
:t; .fitls(: like.l?/ 
For (;h(; rool; nod(:s, the l;;d)le r(:(hl(:es 1;o (;h(: 
uncondil;ion~d t)rol)al/iliI;y of (;h(: node. Now 
WO, (;}UI I;(:Si; (;he mo(M on (;h(: siml)l(: ex~mq)\](: 
seen (:~rli(:r. W + is th(: set of wor(ls l;ha( o(:- 
(:m'rc(t with I;he v(:rl). 'l.'\]l(: \]2o(l(:s (:orr(:st)on(l- 
ing (;o |;he wor(ls in l/l/q ;w(: s(:l, 1;o lr,,,c ;rod 
(;h(: o(;hers l(:f(; mls(:l;. For 1;12(: l)revious ex- 
ample W -I = {mca/,~ apph'., bagel, ch, c~c'.sc'.}, mt(\] 
l;h(; (:orrest)on(ting no(t(:s m:c sol; (;o I/rue, as (t(:- 
t)i(:l;e(t in Figm'(; 5. Wil;h lil,:(dy ;m(t ',.nhT~:c'.ly 
r(:sl)(:(:l;iv(:ly equal Ix) 0.9.0 mM 0.01, tit(: i)osl;(:- 
rior l)rol);d)ilil;i(:s are a /)(/i'\[m, a., b, c.) = 0.9899 
mM .P(Cl,m,, a, b, c) = 0.0101. Expla.ining away 
works. Th(; t)osl;(;rior 1)rol);fl)ilil;y of COGNI -  
T ION g(;l;s as low as i(;s prior, whcr(',as l;h(; 
1)rol)al)ilil;y of FOOD goes u t) to almost; 1. A 
13~y(;sim~ n(;l;work ~q)t/roa(:\]~ seems 1;o ;l(:l;u~dl.y 
imt)leancn(; he. conse.rva/,'ive, stral;egy w(: l;hough(; 
(;o 1)e (;he corr(;(:I; one. for unsupervisc(t l(;~mfing 
of sehx-t, ional resi;ri(:tions. 
4.2 Computat iona l  i ssues  in bu i ld ing  
BBNs  based  on  Wordnet  
Th(: imt)l(:m(:ni;~d;ion f a BBN for (;h(; whole of 
W()r(lnel; fax:as (:oml)ul;al;ional (:oml)lexi|;y pro|)- 
l(:ms (;ypi(:al of graphi(:al too(Ms. A (l(:ns(:ly 
(:ommcI;ed \]IBN presents (;wo kinds of l)rol)l(:ms. 
The tirst is (;he storage of the CPTs.  The size 
of a CPT  grows extIonenl;ially with the nmnber 
of parents of the node. 4 This prol)lem can lie 
aF, C, m, a, b and c respec(;ively stand for FOOD,  
COGNITION, meat, apph'., bagd and chces(: 
4Some words in \Vordnet have mor(: than 20 senses. 
For (,Xaml)h: , line in \'Vordne(: is asso(:ia(,ed with 25 
EN77TY 
? j 
? "~.  
- / 
I"0()1) I,IQUID 
\- ..// 
B E VERA GE 
('OFI"EI? drink 
I 
,IA VA - I 
I>IISYSIC'AL OB,II'2CT 
1AND 
t 
ISIANI) 
>/ 
. /  
/ -  
JAVA -2 
X. / : /  
j<,ra 
Figure. 6: Th(; sulmel;work for d'riuJ<. 
solved 1)y ot)i;infizing the r(:l)r(:s(;nl;~ti;ion f thes(: 
(;;d)l(:s. In our case most of l,h(: (:ntri(:s h~w(: l;he 
s~tln('~ v;~luos~ ~ttl(l ~t COlll\])a,(:(; re l )resenl ;a l ; ion for 
l ; lmln ( ;&n l)(; fOllll(\] (ltlll(;h l ike l;h(', ( )he  llS(Xl in 
(;h(', no isy -OR too(tel (Pearl, 1{)88)). 
A h;~r(l(:r lirol)lem is lmrforming inf(;rc\]me. 
The gr;q)hi(:al sl;rlt('i;llr( ~,of & BBN r(;t)resenl;s 
l;h(: (l(:t)(:n(len(:y r(:lal;ions among th(: rml(lom 
wtrial)lcs of the, nel;work, r\]?he ~dgoril;hlns use(t 
wil;lt B\]INs usmdly l)(M'orm inference t)y (ty- 
mmfi(" t)rogrmmning on the tri~mgul~d;e(t lnor~d 
gr~q)h. A low(n" 1)(mn(t on l, he mmfl)er of (:om- 
l)ul;al;ions l;h;~(; are n(:(:(:ssa.ry I;() mo(t(:I l;h(; joint 
(lisl;ritmi;ion ov(:r l;h(: wn'bd)h:s using su(:h ~dgo- 
ril;hms is 21"1t I wh(:r(: 'r~, is t;\]m size of (:h(; ma.x- 
imal l)(mn(tary s(:l; a(:(:or(ling (;o t;hc visil;a,tion 
s(:h(:(lul(:. 
4.3 Subnetworks  and  ba lanc ing  
B(,(:mls(; of l;h(:s(, 1)rol)h:ms w(: (:(told not t)uihl a 
singl(: BBN for Wor(hmI;. Insl;e~M w(: simt/litie(t 
(;he sl;rll('l;ur(: of 1;12(: model by building a smaller 
sutmei;work for each 1)re(ticate-argumenl; pair. A 
sulm(:twork consis(;s of (;he mlion of the s(:ts of 
ml(:(;sl;ors of the words in W +. Figure. 6 pro- 
vid(:s ml example of the union of these :%nces- 
tral sul)grat)hs" of Wordne(; for (;he words java 
~m(l drink (COml)~We i(; wil;h Figure 1). 
This siml)liti(:ation (toes not atfe(:t the (:om- 
pul;~tion of the (tistril)ui;ions we are inl;(;resl;ed 
in; l;h;fl; is, the marginals of the synset nodes. 
A BBN provi(tes a coral)act representation tbr 
the joinI; disl;rit)ution over the set of variables 
senses. The size of its OPT is therefor('. 2 2(~. Six)ring a (:a- 
1)Ie of tloa(; numbers tbr l;his node alone requires around 
(2'-)~)8 = 537 MBytes of memory. 
191 
in the network. If N = X1, ..., Xn  i8 a Bayesian 
network with variables X1,..., Xn, its joint dis- 
tribution P(N) is the product of all the condi- 
tional probabilities pecified in the network, 
P (N)  = I I  P(XJl p,,(Xj)) (4) 
J 
where pa(X) is the set of parents of X. A BBN 
generates a factorization of the joint distribu- 
tion over its variables. Consider a network of 
three nodes A, B~ C with arcs fl'om A to \]5 and 
C. Its joint distribution can be characterized as 
P(A, B, C) = P(A)P(BIA)P(CIA ). If there is 
no evidence for C the joint distribution is 
P(A ,B ,C)  = P(A)P(BIA ) ~ P(CIA ) 
c 
= P(A)P(B IA  ) 
= P(A, B) 
The node C gets marginalized out. Marginaliz- 
ing over a childless node is equivalent o remov- 
ing it with its connections from the network. 
Therefore the subnetworks are equivalent to the 
whole network; i.e., they have the same joint 
distribution. 
Our model comtmtes the value of P(c\[p,r), 
lint we did not compute the prior P(c) for all 
n(mns in the cortms. We assumed this to be 
a constant, equal to the 'u'nlihcly wflue, for all 
classes. In a BBN the wdues of the marginals 
increase with their distance fl'om the root nodes. 
To avoid undesired bias (see table of results) we 
defined a balancing formula that adjusted the 
conditional probabilities of the CPTs in such a 
way that we got; all tim marginals to have ap- 
proximately the same wdue)  
5 Exper iments  and  resu l t s  a 
5.1 Learn ing of  se leet iona l  pre ferences  
When trained on t)redicate-argument pairs ex- 
tracted from a large corpus, the San .Jose Mer- 
cury Corpus, the model gave very good results. 
The corpus contains about 1.a million verb- 
object tokens. The obtained rankings of classes 
according to their posterior marginal probabili- 
ties were good. Table 1 shows the top and the 
'~More details can be found in an extended version of 
the paper: www.cog.brown.edu/~massi/. 
6For these experimc'nts we used values for the likely 
and unlikely 1)arameters of 0.9 and 0.1~ respectively. 
Ranking Synset P(clp, r) 
1 VEIIICLE 0.9995 
2 VESSEL 0.9893 
3 AIRCRAFT 0.9937 
4 AIRPLANE 0.9500 
5 SHIP 0.9114 
255 CONCEPT 0.1002 
256 LAW 0.1001 
257 PIIILOSOPIIY 0.1000 
258 ,IUI?,ISPRUDENCE 0.1000 
Table 1: Results tbr (maneuver, object). 
bottom of the list of synsets for the verb ma- 
neuver. Tile model learned that maneuver "se- 
lects" for melnbers of the class VEttlCLE and 
of other plausible classes, hyponynls of I/EHI - 
CLE. It also learned that the verb does not 
select for direct; objects that are inembers of 
(:lasses, like CONCEPT or PItILOSOPltY. 
5.2 Word  sense d i sambiguat ion  test  
A direct ewfluation measure for unsupervised 
learning of SP models does not exist. These 
lnodels are instead evaluated on a word-sense 
disambiguation test (WSD). The idea is that 
systems that learn SP produce word sense dis- 
amt)iguation as a side-effect. Java might be in- 
terl)reted as the island or the beverage, but in a 
context like "the tourists flew to Java" the for- 
mer seems more correct, because fly could select 
for geographic locations but not for beverages. 
A system trained on a predicate p should he 
able to disambiguate arguments of p if it has 
learned its selectional restrictions. 
We tested our model using the test and 
training data developed by Resnik (see Resnik, 
1997). The same test was used in (Almey 
and Light, 1999). The training data consists 
of predicate-object ounts extracted fl'oln 4/5 
of the Brown corpus (at)out 1M words). The 
test set consists of predicate-object pairs from 
the remaining 1/5 of the corpus, which has 
been manually sense-annotated by Wordnet re- 
searchers. The results are shown in Table 2. 
The baseline algorithm chooses at random one 
of the multiple senses of an ambiguous word. 
The "first sense" method always chooses the 
most frequent sense (such a system should be 
trained on sense-tagged data). Our model per- 
192 
Method Result 
Baseline 28.5% 
Abney mM Light (HMM smootlm(l) 35.6% 
Abney and Light (ItMM 1)alan(:ed) 42.3% 
Resnik 44.3% 
BBN (without bM~mcing) 45.6% 
BBN (with bMancing) 51.4(/(/ 
First Sense 82.5(/0 
Table 2: R,esults 
formed 1)etter th;m the state of tlm art mo(tels 
for mlSUl)ervised le~n'ning of SP. It seems to de- 
fine i~ l)ett(;r esi;ima, l;or for \])(ell), 'r). 
It is remnrkabh: fliat the model ~mhi(:ved this 
result making only a limi(;(:(t use of distribu- 
tional informal;ion. A n(mn is in 14 f+ if it oe- 
('urred at h;ast once in th(: tra,ining set, 1)ut the 
sysi;em does not know if i(; o(:(:urre(l once or sev- 
eral times; either it oc(:urred or it didn't. The 
nl()(te, l did not; suffer too mu(:h froxn this limi- 
(;ntion (htril~g (;his task. This is 1)rol)~d)ly (tuc 
to the Sl)arsencss of the (;rltining (t;tl;a for (;tie 
test. For each verb (;he average mmfl)er ()f el)- 
\]eel; tyl)es is 3.3, for each of them tim :werng(: 
lxumber of (;ok(ms is 1.3; i.e., most of the, words 
in the training data. only ()(:(:urred once. Pin" 
this training set we ~dso t(:sted a version of (;he 
m()del that |rail(; it word node ti)r each el)served 
ol)je(:l; token ;~n(| (;here, fore inl,(~gral;e(t the (listri- 
lmtional informntion. ()n (;\]m WSI) test it; per- 
ti)rmed exactly the stone its the simph~r version. 
When trained on the, San .lose Mercury Cort)us 
the model l)erfornle(t worse on the WSI) t:esl; 
(35.8%). This is not too surprising considering 
(;he diftbxelmes beA;ween tim SJM and the \]~rown 
(:ort)or~: the former, i~ re(:ent newswire cortms; 
the llg;ter, &Xl older, |)alml(:ed ('orl)uS. Allot;her 
ilnportmlt factor is the different relevance of dis- 
tributional informntion. The training (tatn fl'om 
the SJM Corlms are nnlch ri(:her and noisier 
than the Brown data. Here the, fl'equen(:y in- 
tbrm~tion is probably crucbfl; however, in this 
case we could not imt)lement the silnl)le s('hem(~ 
;tb ove. 
5.3 Conc lus ion  
Explaining away imt)lements ;~ (:ognitively ~tt- 
tractive and successful strategy. A straighttbr- 
ward lint)rove, men( would lm tbr the me(tel to 
make flfll use of the distrilmtional ildbrmtLtion 
present in the training data; we only partially 
achieved this. B~yesian networks are usually 
confronted with a single present~Ltion of evi- 
den('e. Their exi;ension to multil)le evidence is 
not triviM. We believe the model can be ex- 
tended in this direction. Possibly there m'e sev- 
erM ways to do so (muir(hernial Saml)ling , ded- 
i(:ated implementations, etc.). However, we be- 
lieve that the most relevmit finding of this re- 
search mighl; t)e (;h~t(; %xplnining itww" is not 
only a 1)roperl;y of Bayesian networks but of 
\]3ayesimx infe, rence in general and that it might 
tm imt)lemental)le in other kinds of graphical 
models. We, observed that (;he prol)erty seems to 
del)end on the specification of the prior proba- 
bilities. We t'omld (;lm(; (;he HMM model of (Ab- 
hey mid Ligh(;, 1999) was 'unidentifiable; that is, 
(;here are several sohli;iolxs tbr the \])~tra, xnel;els of 
the lno(te,1, including the desired Olle. OHr intu- 
ition is (;hat it shouht l)e possible to imt)lemelxt 
"exl)laining awlty" in a HMM with 1)tiers, so 
(;hit(; il; wouh\[ 1)rethr only ()lie or ~t ti~w solu(;ions 
()ver lmuiy. This model would have also the a(t- 
wmtttgc of t)eing (:Onll)U|;~d;ionally silnt)ler. 
References  
S. Almey told M. Light. 1999. Hiding a serum> 
tic hierarchy in ~ Mm'kov model. In P'm(:e,d- 
ings q/' the Workshop o'n Uns'~q)ervi,scd Le,,'r'n,- 
ing in Nat'wra,1 Lang,uagc Proeessin9, A CL. 
N. Chomsky. 1965. Aspcct.s of th, e Theory of 
Sy'nta,:r. MIT Press, Cambridge, MA. 
P. N. Johnson-Laird. 1983. Mental Models: 2b- 
'wards a Cognitive Sciev, ce of Lang'uage , Pn:fi',r- 
c'nce, a'n,d Con.sciousncss. ilm'w~rd University 
Press. 
J . . \] .  Kntz and .\]. A. Fodor. 1964. The. struc- 
ture of ;~ semmiti(: theory. In ,\]. ,J. KaI;z mM 
J. A. l/odor, editors, The St'ructure of Lan- 
g'uage. Prentice-Ilall. 
G. Miller. 299(I. Wordnet: An on-line lexical 
database, btternational Journal of Lexicog- 
raphy, 3(4). 
J. Pearl. \] 988. Probabilistic R, easo'n, ing in Intel- 
ligent, Systems: Net'worl?s of Plausible l~l:fer- 
e'n, ee. Morgml Kimthmn. 
P. Resnik. 2997. Sele(:tiona\] prefi:ren(:e and 
sens(; disamt)iguation. In Proceedings of th.e 
ANLP-97 Workshop: Taggin 9 Text with Lex- 
ical Semantics: Wll.y, What, and Itow? 
193 
Compact non-left-recursive grammars using the selective 
left-corner transform and factoring* 
Mark  Johnson  Br ian  Roark  
C()gnitiv(; and  L ingu is t ic  Sci('.n(:('.s, Box  1978 
Brown Un ivers i ty  
Mark_Johnson@\[3rown.ed u Bria n_Roark@grown.edu 
Abst ract  
Tim lcft-c.orner transibrm reiIloves left-r(;cursion 
fl'om (l)rol)al)ilisti(') (:ontext-free granunars and uni- 
t|cation grammars, i)ermitting siml)l(~ tol)-down 
parsing te(:hniques to l)e used. Unforl.unately the 
grammars l)roduced by the stal~dard \]etTt-('orner 
transform are usually much larger than |;he original. 
The select, lye left-corner i;ransform (lescril)ed in this 
l)aI)er 1)rodu(:es a transformed grammar which simu- 
lates left-corner ecognition of a user-st)coiffed set of 
tim original productions, and tOl)-down r(~cognition 
of the, others. C()mbined with tw() factorizations, it;
1)rOdll(;es l lOl l - lef l ; - reci l rs ive grallll l l~ll'S |;lilt|; ~/re 11o|; 
much larger than the original. 
1 In t roduct ion  
TOl)-down i)arsing techniques are al;tl'a(:tiv(! because 
of their simt)licity, and can often a(:hi(~ve good 1)er- 
formance in 1)racti(:e (l{oark and .\]()hns(m, 1999). 
However, with a left-re(:ursive grammar such l)ars('as 
tyl)i(:ally fail to termim~te. Tim left>corner gram- 
l t lar  l ; ra l l s for ln  eo l iver ts  a le f l ; - recurs ive  ~l'iillllll~lr 
into a non-lefl;-recursive one: a top-down t)arser 
using a left-corner transformed grammar simulates 
a lefl;-(:orner parser using the original granllnar 
(l/os(mkrantz and Lewis II, 1970; Aho and Ulhnan, 
1972). Ih)wever, the left-corner transformed gram- 
mar can 1)e significantly larger than the original 
grammar, ca.using mlmero~ls l)rol)lelns. For exam- 
ple, we show 1)clew that a probat)ilistic ontext-fr(.~e 
grammm: (PCFG) estimated froln left-corner trans- 
formed Petal WSJ tree-bank trees exhil)its consid- 
erably greater st)arse data prol)lems than a PCFG 
estimal, e(t in the usual manner, siint)ly because the 
left-corner transformed grammar contains approxi- 
mately 20 times more 1)reductions. The transform 
described in this paper t)roduees a grammar al)prox- 
imately the same size as the inlmt grmmnar, which 
is not as adversely at\[ected by sparse data. 
* This  research was slli)i)orl;ed t)y NSF awards !1720368, 
9870676 and 98121(19. We would like to t lmnk o1|1" (:olleagues 
in I~I,IAP (Brown l ,aboratory for Linguistic Information Pro- 
ccssing) and Bet) Moore tbr their hcll)ful comments  on this 
pal)Or. 
Left-corner transforms a.re particularly useflll be- 
cause they can i)reserve annotations on productions 
(n:ore on this 1)(flow) and are thereibre apt)lieable to 
more COml)Iex graminar formalisms as well its CFGs; 
a t)roI)erty which other al)l)roaehes to lefl;-recursion 
elimination tyl)ically lack. For examl)le , they al)l)ly 
to l(~ft-r(~cursive unification-based granmmrs (Mat;- 
sumoto et al, 1983; Pereira and Shieber, 1987; .h)hn- 
son, 1998a). Because the emission 1)robabilit;y of a 
PCFG 1)ro(hm(;ion ca15 be regarded as an anllotatioll 
on a CFG 1)reduction, the left-corner transform can 
t)rodue(', a CFG with weighted l)roductions which 
assigns the same l)robal)iliti(~s to strings an(l trans- 
tbrmed trees its the original grammar (Abney et al, 
11999). Ilowever, the transibrmed grammars (:an be 
much larger than the original, which is unac('el)table 
tbr many aI)t)lieations involving large grammars. 
The selective left-corner transform reduces the 
transl'ornm(l grammar size because only those l)ro- 
(lu(:tions which apt)ear in a left-recto'sire (:y(:le llee(l 
1)e recognized left-(:orner in order to remove left- 
recurs|on. A tOl)-down parser using a grammar pro- 
dueed by the selective left-(:orner |;ranst.'orm simu- 
lates a generalized left-corner parser (Demers, 1977; 
Nijholt, 1980) wlfich recognizes t user-specified sul)- 
set; of the original productions in a left-corner fash- 
ion, and the other productions tol)-down. 
Although we do not investigate it in this 1)al)er, 
the selective left-(:orner transform should usually 
lmve a slnaller sear(:h sl)ace relative, to tim standard 
left-corner transform, all else being equal. The par- 
tial l)arses t)roduced uring a tot)-down parse consist 
of a single connected tree fragment, while the par- 
tial parses l)rodueed produced during a let't-corner 
t)arse generally consist of several discommcted tree 
fragments. Since these fragments arc only weakly re- 
lated (via the "link" constraint descril)ed below), the 
search for each fragment ix relatively independent. 
This lllay l)e rest)onsil)le for the ol)servation that 
exhaustive left-corner 1)arsing is less efficient titan 
top-down l)arsing (Covington, 1994). Intbrmally, be- 
cause the selective left-corner transforln recognizes 
only a sul)set of 1)reductions in a lefl;-corner fashion, 
its partial parses contain fewer tree discontiguous 
355 
fl'agnlents and the search inay be more efficient. 
While this paper focuses oil reducing grammar 
size to nlinimize sparse data problems in PCFG 
estilnation, tile modified left-corner transforms de- 
scribed here are generally api)licable wherever the 
original left-conler transform is. For example, tile 
selective left-corner transform can be used in place 
of the standard left-comer transform in the con- 
struction of finite-state approximations (Johnson, 
1998a), often reducing the size of the intermedi- 
ate automata constructed. The selective left-corner 
transform can be generalized to head-corner parsing 
(vail Noord, 1997), yielding a selective head-corner 
parser. (This follows from generalizing the selective 
left-corner transform to Horn clauses). 
After this paI)er was accepted for publication we 
learnt of Moore (2000), which addresses the issue 
of grammar size using very similar techniques to 
those proposed here. The goals of tile two pat)ers 
are slightly different: Moore's approach is designed 
to reduce the total grammar size (i.e., the sunl of 
the lengths of the productions), wtfile our approach 
minimizes the number of productions. Moore (2000) 
does not address left-corner tree-transforms, or ques- 
tions of sparse data and parsing accuracy that are 
covered ill section 3. 
2 The  se lec t ive  le f t - corner  and  
re la ted  t rans forms 
This section introduces the selective left-corner 
transform and two additional factorization trans- 
forms which apply to its output. These transfbrnm 
are used ill tile experiInents described in tile follow- 
ing section. As Moore (2000) observes, in general 
the transforms produce a non-left-recursive output 
grammar only if tile input grammar G does not con- 
tain unary cycles, i.e., there is no nonterminal A 
such that A -~+ A. 
2.1 The selective left-corner t rans form 
The selective left-corner transform takes as input a 
CFG G = (V, T, P, S) and a set of left-corner produc- 
tions L C_ P, which contains no epsilon t)roductions; 
the non-left-corner prodnctions P - L are called top- 
down productions. The standard left-corner tr'ans- 
form is obtained by setting L to the set of all 
non-epsilon productions in P. The selective left- 
corner trnnsform of G with respect o L is the CFG 
?.Cn (G) = (V1, T, P1, S), where: 
V1 = VU{D X:DEV,  XcVUT} 
and P1 contains all instances of tile schemata 1. In 
these schemata, D E V, w E T, and lower case 
greek letters range over (V tO T)*. The D-X are 
new nont;ernlinals; informally they encode a parse 
state in which an D is predicted top-down and an X 
? - -D  . . . . . .  D . . -  
/31 f~o a D A 
: ~ ~ ~  
? " f in  D /3n  
CC '.. 
A ~,~ D-Bj 
a rio D-D 
Figure 1: Schematic parse trees generated by the origi- 
nal grmnmar G and the selective left-corner transformed 
grammar gCL(G). The shaded local trees in the orig- 
inal parse tree correspond to left-corner productions; 
the corresponding local trees (generated by instances of 
schema lc) in the selective leff-conler transfornled tree 
are also shown shaded. The local tree colored black is 
generated by an instance of schema lb. 
has been found left-corner, so D X ~cr . (c , )  7 only 
if D ~b XT. 
D ~ w D-w (la) 
D -~ (.~D-A whereA-+aEP-L  (lb) 
D-/3 -+ \[3 D C whereC+/3 /3EL  (lc) 
D-D ---> e (ld) 
Tile schemata flmction as follows. The productions 
introduced by schema 1~ start a left-corner parse of 
a predicted nonterminal D with its let'mlost ermi- 
nal w, while those introduced by schenla lb start; a 
left-corner parse of D with a left>corner A, which is 
itself found by the top-down recognition of produc- 
tion A -+ (t E P -  L. Scheina lc extends the current 
left-corner B tit) to a C with tile left>corner recogni- 
tion of production C ~ /3 ft. Finally, scheina ld 
inatches tile top-down prediction with tile recog- 
nized left-corner category. 
Figure 1 schematically depicts the relationship be- 
tween a chain of left-comer t)roductions in a parse 
tree generated by G and the chain of correst)onding 
instances of schema le. The left-comer ecognition 
of the chain starts with the recognition of (t, tile 
right-hand side of a top-down production A --+ ~, 
using an instance of schema lb. Tile left-branching 
chain of left-corner productions corresponds to a 
right-branching chain of instances of schema lc; the 
left-corner transforln in effect converts left recursion 
into right recursion. Notice that tile top-down pre- 
dicted category D is passed own this right-recursive 
chain, effectively multiplying each left-conler pro- 
ductions by the possible top-down predicted cate- 
gories. Tile right recursion terininates with an ill- 
stance of schema ld when tile left-comer and top- 
down categories match. 
Figure 2 shows how tot)-down productions from 
G are recognized using ?CL(G). When the se- 
356 
. . .A . . .  - . .A - - .  . . .A . . .  
(&, ?C O: A--A r:-re, nmval (t: 
.K2ZX LECx 
Figure 2: The recognition of a top-down produc, tion A --+ 
a: by ?CL(G) involves a left-corner category A-A, which 
immediately rewrites to e. One-step e-removal applied 
to ?CL(G) l)roduces a grmnmar in which each top-down 
production A -+ ct corresponds to a production A --+ tt 
in the transformed grammar. 
lective left-corner tra,nsform is tbllowed by a one- 
step c-renlowd transfornl (i.e., coml)osition or partial 
evaluation of schema 1t) with respect o schema ld 
(Johnson, 1998a; Abney and 3oMson, 1991; Resnik, 
1992)), each top-down production f'rolll G appears 
uilclmnged in tile tinal grammar. Full e-relnoval 
yields the grannnar giwm 1) 3, the schemata below. 
.D -~ w D-w 
D -~ 'w where. D ~ j  w 
D ~ ~DA whereA-+(~c l  ) -L  
.D -+ a where D =>* A P L A, -+ ~ G -- L 
D-B --+ fl D C whereC- ->Bf lcL  
D-B -} fl wl le reD~},C ,C~Bf l6L  
Moore (2000) introduces a version of the left- 
corner transform called LCLIt, which al)plies only to 
productions with left-recursive parent and left clfihl 
categories. \]n the~ (:ontext of the other transforms 
that Moore introduces, it seems to have the, sallle 
effect in his system as the s(Je(;tive lefl;-corll(W trails- 
form does lmre. 
2.2  Select ive le f t -corner  t ree  transfor l l lS 
There is a 1.-to-1 correspondence b tween the 1)arse 
trees generated by G and ?CL(G). A tree t is gener- 
ated by G iff there is a corresponding t' generated by 
?CL(G), where each occurrence of a top-down pro- 
duction in the derivation of t corresponds to exactly 
one local l, ree gelmrated by occurrence of the cor- 
responding instance of schema 11) ill the derivation 
of t', and each occurrence of a M't-corner produc- 
tion in 1 corresponds to exactly one occurrence of 
the corresponding instance of schema le in t'. It; is 
straightforward to detine a 14o-1 tree l;ransform TL 
mapping parse trees of G into parse trees of ?dL (G) 
(.Johnson, 1998a; Roark and Johnson, 1999). In the 
empirical evaluation below, we estinmte a PCFG 
Dora the trees obtained by applying 7}, to the trees 
in the Petal WSJ tree-lmnk, and compare it to tile 
PCFG estinmted from the original tree-bank trees. 
A stochastic top-down parser using the I 'CFG es- 
timated from the trees produced by ~,  simulates 
a stochastic generalized left-corner Imrser, wlfich is 
a generalization of a standard stochastic lefl;-corner 
1)arser that pernfits productions to t)e ret;ognize, d 
top-down as well as left-corner (Manning and Car- 
penter, 1997). Thus investigating the 1)roperties of 
PCFG estimated from trees transformed with "YL is 
an easy way of studying stochastic trash-down au- 
tomata performing eneralized lefi;-corner parses. 
2.3  Prun ing  useless product ions  
We turn now to the problmn of reducing the size of 
tile grmnmars produced by left-corner transforms. 
Many of the productions generated by schemata 1
art: useless, i.e., they never appear in any termi- 
nating deriw~tion. Wtfile they can be removed by 
standard methods for deleting useless productions 
(Ilopcroft and Ulhnan, 1979), the relationship be- 
tween the parse trees of G and ?CL(G) depicted in 
Figure 1 shows how to determine ahead of time the 
new nonterminals D X that can at)pear in useful 
productions of ECL (G). This is known as a link con- 
straint. 
D)r (P)CFGs there is a particularly simple link 
constrainl;: \]) X apt)ears in useflfl productions of 
?CL(G) only if ~7 < ( 17 U T)*.D =>* XT. If ? L 
epsilon removal is applied to the resulting gram- 
mar, D X appears in usefill productions only if 
H7 C (17 U T) + .D ~}, X7. Thus one only need 
ge.nerate instances of the left-corner schemata which 
satist~y the corresponding link constraints. 
Moore (2000) suggests all additional constraint on 
nonte.rminals D X that can al)l)ear in useflll 1)roduc - 
l;iolts of ?CL(G): D lllllsl; eitller be th(! start synJ)ol 
of G or else al)pear in a production A --+ o'D/3 of G, 
for .,,;, A c- V, c {Vu T}+ c Tp .  
It is easy to see that the l}roducl,ions that Moore's 
constraint prohibits are useless. There is one non- 
ternfinal in the tree-bank gramnmr investigated be- 
low that has this property, namely LST. However, 
ill the tree-lmnk granmmr none of the productions 
exlmnding LST are left-recursive (in fact, the first; 
dfild is ahvays a pretermiiml), so Moore's constraint 
does not atgect he size of the transformed grammars 
investigated below. 
While these constraints can dramatically reduce 
both the number of productions and the size of the 
1)arsing search space of the 1;ransformed grmnmar, 
in general the transfl)rmed grammar ?CL (G) can 1)e 
quadratically larger than G. There are two causes 
for the explosion ill grmnmar size. First, ?CL(G) 
contains an instance of sdmma lb tbr each top- 
down production A --+ a and each D such that 
37. D ~}, A 7. Second, ?CI,(G) contains an in- 
stance of schema lc for each left-corner production 
C -~ fi and each D such that BT.D ~,  C7. In 
etDct, ?CL(G) contains one copy of each production 
for each possible left-comer ancestor. Section 2.5 
describes filrther factorizations of the l)roductions 
of ?CL (G) which mitigate these causes. 
357 
2.4 Optimal choice of L 
Because ::>~, increases monotonically with =>L and 
hence L, we typically reduce the size of ?CL(G) by 
making the left-corner production set L as small as 
possit)le. This section shows how to find the unique 
minimal set of left-corner productions L such that 
?CL(G) is not left-recursive. 
Assume G = (V,T, P, S) is wuned (i.e., P con- 
tains no useless productions) and that there is no 
A 6 V such that A --++ A (i.e., G does not gen- 
erate recursive unary branching chains). For rea- 
sons of space we also assume that P contains no 
e-productions, but this approach can be extended to 
deal with them if desired. A production A -+/3fl C 
P is left-rccursive iff ~3' C (V U T)*. \]3 ~,  AT, i.e., 
P rewrites B into a string beginning with A. Let L0 
be the set of left-recursive prodtlctious in G. Then 
we claim (1) that ?CLo (G) is not left-recursive, and 
(2) that for all L C Lo, ?CL(G) is leff-recursive. 
Claim 1 follows t?om the fact, that if A ~s,0 B7 
then A =:>,, /37 and tile constraints ill section 2.3 
on useful productions of ?CLo(G). Claim 2 tbllows 
from the fact that if L C L0 then there is a chain of 
left-recursive productions that includes a top-down 
production; a simple induction on tile length of the 
chain shows that gCL (G) is left-recursive. 
This result justifies the common practice in natu- 
ral language lefl;-corner t)arsing of taking tile termi- 
nals to be the preterminal t)art-of-speech tags, rather 
than the lexical items themselves. (We did not at- 
tempt to calculate tile size of such a left-comer gram- 
mar in tilt empirical evaluation below, lint it would 
be much larger than any of the grammars described 
there). In fact, if the preterminals are distinct from 
the other nonterminals (as they are ill the tree-bank 
grammars investigated below) then L0 does not in- 
clude any productions beginning with a preterminal, 
and ?CLo (G) contains no instances of schema la at 
all. We now turn our attention to tlm other sclmmata 
of the selective left-corner grammar transform. 
2.5 Factoring the output of ?CL 
This section defines two factorizations of the outtmt 
of the selective left-corner grammar transform that 
can dramatically reduce its size. These factoriza- 
tions are most effective if the number of t)roductions 
is much larger than the number of nonterminals, as 
is usually the case with tree-bank grmnmars. 
Tilt top-down factorization decomposes 
schema lb by introducing new imnterminals 
D t, where D C V, that have the stone expansions 
that D does in G. Using the same interpretation for 
variables as in schemata 1, if G = (I~ T, P, S) then 
(? (a) = T, S), where: 
14a = Iq tO{D' :DEV} 
and Ptd contains all instances of the schemata la, 
3a, 3b, lc and 1(t. 
D --+ A'D-A whereA-+aEP-L  (3a) 
A' -+ a, whereA- ->creP-L  (3b) 
Notice that the number of instances of schema 3a is 
less than the square of tile number of nonterminals 
and that the number of instances of sdmma 31) is the 
number of top-down productions; the sum of these 
numbers is usually much less than tile mlmber of 
instances of schema lb. 
Top-down factoring p lws approximately tile same 
role as "non-left-recursion grouping" (NLRG) does 
in Moore's (2000) approach. The meier difl!erence 
is that NLRG applies to all productions A ~ /3/9 
in wtfich /3 is not left-recm'sive, i.e., ~7./7 =>~ /3% 
while in our system toll-down factorization applies to 
those productions tbr which ~7. B ~,  AO', i.e., the 
productions not directly involved in left recursion. 
Tim left-corner factorization decomposes 
schema lc in a similar way using new nonter- 
minals D\X, where D e V and X ~ V U T. 
c)(c) = T, S), where: 
I'}o = ~qU{D\X:D6V,  X6VUT} 
and Plc contains all instances of tile schemata la, 
ib, 4a., 4b and id. 
D /3 -+ C \BD C whereC-+B\ [9?L  (4a) 
CxB --+ fl whereC- -+Bf lEL  (4b) 
The number of instances of schema 4a is bounded 
by the numtmr of instances of schema lc and is typ- 
ically nmch smaller, while the number of instances 
of schema 41) is precisely the munber of left-corner 
productions L. 
Left-corner factoring seems to correspond to one 
step of Moore's (2000) "left factor" (LF) operation. 
Tile left; factor operation constructs new nontermi- 
nals corresponding to common prefixes of" arbitrary 
length, while left-corner factoring effectively only 
factors the f rst  nonterminal symbol on the right 
hand side of left-corner productions. While we have 
not done experiments, Moore's left factor operation 
would seem to reduce the total number of symbols 
in the transformed grammar at tile expense of pos- 
sibly introducing additional productions, while our 
left-corner factoring reduces the number of produc- 
tions. 
These two factorizations can be used together 
in the obvious way to define a grmnmar trans- ~__.C(ld,le) form "L , whose productions are defined by 
schemata la, 3a, 3b, 4a, 4b and ld. There are corre- 
spondiug tree transtbrms, which we refer to as TI! td) , 
etc., below. Of course, the pruning constraints de- 
scribed in section 2.3 are applicable with these fac- 
torizations, and corresponding invertible tree trans- 
forms can be constructed. 
358 
3 Empi r i ca l  Resu l t s  
To examine the effect of the tra.nsforms outlined 
above, we experimented with vm'ious PCFGs in- 
dueed from sections 2--21 of a modified Pcml WSJ 
tree-bank as described in Johnson (19981)) (i.e., 
labels simplifiecl to grammatical ca.tegorics, R.OOT 
lu)des added, empty nodes and vacuous unary 
bra.nehcs deleted, and auxiliaries retagged as AUX 
or AUX('). \~,Ze. ignored lexic.al items, and treated 
the part-of-speech tags as terminals. As Bob Moore 
pointed out Lo us, the left-corner transform may pro- 
duc.e left-recursive grmnmars if its inlmt grammar 
contains mmry cycles, so we removed them using the 
a transforln that Moore suggested. Given an iifitial 
set of (non-epsihm) productions P,  the transtbrmed 
grammar contains the following in:odu(:tions, wherc~ 
l;he A ~ are 1lew llOll-terlilillals: 
A "-~ (t where A--} (t G P, A 75~; A 
A~D~ whereA=>~,D~iA  
./1 h -~ ~: where A -~ (~ G P, A ~;  .,1, (t ~>~, A
This transform can be extended t,o one on PCFGs 
which preserves derivation probabilities. In this sec- 
tic)n, we fix P to) be the produeticms l;lmt re.sult; afl;er 
al)plying this unary t:yc:le removal transforma.tion to
the tree-l)ank 1)roductions, and G to \])e the ('orre- 
st)onding grammm'. 
Tables 1 and 2 give the sizes of selective left;- 
(:orner grmnlnar trmlsforms of G for various wthles 
of l;he left-et)rner set L and fa(:torizal;ions, without 
and with epsilon-remowfl respectively. In l;he ta- 
bles, L/j is the st'./; of hd't-rc.cm'siv(' 1)roductions in 
P,  as detined in set:lion 2.4. N is the sel of 1)roclu(: - 
l;ions in 1~ whose hfft-ha\]M sides do not begin with 
a part-ofspee(:h (P()S) tag; 1)ecause I 'OS tags are 
distinct front other nontermimtls in l;he tree-lmnk, 
N is an easily identified set of I)roductions guaran- 
teed to include L0. The tables also gives the sizes 
of maximum-likelihood PCFGs estimated from the 
tr(;es resulting fl:om applying the sele(:tive left-corner 
tree transforms 7- 1,(} the tree-bank, l)reaking mmry 
t:yeles as clescribed above. For the I)arsing exl)eri- 
ments below we always deleted empty nodes in the 
outl)ut of these tree transforms; this corresponds to 
el)silon removal in the grammar transform. 
First, note that/2Cv(G),  the result of al)plying the 
standard left-corner g lmmnar transform to G, has 
al)proximately 20 times the number of t)roductions 
?C (m't~)(G), the result of aI)- tha.t G has. Itowever "co 
plying the selective left-corner grammar transforma- 
tion with factorization, has approximately 11.4 times 
the munber of productions that G has. Thus the. 
methods described in this paper cml in fact dramati- 
cally reduce the. size of left-corner transformed gram- 
mars. Second, note that ?C(~t'I")(G) is not much 
th.,,  :his t,et:.,,se N larger is l lOt  IJO \ \] 
G 
?C 1' 
?CN 
?CLo 
T,\, 
,,o,~e (ta) (z~) (t(z, z(..) 
1.5,040 
346,344 30,716 
345,272 113,616 254,067 22,4:11 
314,555 103,504 232,41.5 21,364 
20,087 17,146 
19,61.9 16,349 19,002 15,732 
18,945 16,126 18,437 15,618 
Table \] : Sizes of PCFGs inferred using vm'ious grammar 
and tree transtbrms after pruning with link constraints 
without epsihm removal. Cohmms indicate thctorization. 
In the grammar and tree transfl)rms, P is the set, of pro- 
ductions in G (i.e., the standard M't-corner transform), 
N is the set of all productions in P which do not be- 
gin with a POS tag, mM L0 is the set of left-recursive 
t)roclu(:tions. 
?C1' 
?CN 
?CL, 
"I-N 
"~\])o 
,,o,,e (?,~z) (l~) (~< >) 
564,430 38,489 
563,295 1.76,644: 411,986 25,335 
505,435 157,899 371,102 23,566 
22,035 17,398 
2:1,58!) 16,688 20,696 15,795 
21,061 16,566 20,168 15,673 
'.l'alfle 2: Sizc's of PCFGs inferred using various grammm: 
and tree trmtsforms aftc.r pruning with link constraints 
with epsihm removM, using the same notation as Table 1. 
much larger than L0, which in turn is be(:ausc, most 
pairs of non-P()S nonternfinals A, B are nmt;ually 
left-recursive. 
'l)lrning now to the PCFGs estimated after at)- 
plying tree transtbrms, we notice that grammar size 
(Loes l l ( )t  Jll(;Fe}Lqe. Ile}llJ\]y St) dramatically. These 
PCFGs encode a. maximum-likelihood estimate of 
the state transiti(m probabilities for vmious stochas- 
tic generalized h;t't-(-orner t)m'sers, since a tol).-clt)wn 
parser using these, grammars simulates a general- 
ized left-corner 1)arser. The fact that ?Cp(G) is 
17 timc.s larger than the. PCFG infe.rred a.fter apply- 
ing "T}, to the tree-lmnk means that most of tile l}OS -
sible transitions of a standard stochastic left-corner 
parser are not observc.d in the tree-bank la"'ammg" 
data. The state of a left-corner parser does capture 
some linguistic generalizations (Mmming an<l Car- 
penter, 1997; Roark a.nd Johnson, 1999), but one 
might still expect sparse-data problems. Note that 
"Lo is only 1.4 times larger than T, (t~'z~) Lo , SO We 
expect less serious sp~rse data problems with the 
fat:toted selective left-corner transibrm. 
We quantii~ these sparse data prol)lems in two 
ways using a held-out test eorIms, viz., all sentences 
in section 23 of the trce-lmnk. First, table 3 lists the 
mmfl)er of sentences in the test corpus that fail to 
receive a parse with the wwious PCFGs  mentioned 
359 
TransfBrln 
I lone 
%, 
TN 
%.0 
none (t(0 (t~) (td, lc) 
0 
2 0 
2 0 2 
0 0 0 
Table 3: The number of sentences 
not receive a parse using various 
fl'om sections 2-21. 
in section 23 that do 
grammars estimated 
Transforin 
none 
7?, 
TN 
Tp~ 
TN~ 
7I~0 e 
nolle (td) (lc) (td, Ic) 
514 
665 535 
664 543 639 518 
640 547 615 522 
719 539 
718 554 685 521 
706 561 666 521 
Table 4: The lmml)er of productions found in the trans- 
formed trees of sentences in section 23 that do not appear 
in the corresponding transforined trccs f,'om sections 2 
21. (The subscript epsilon indicates epsilon remowfl was 
applied). 
above. This is a relatively crude lneasure, but cor- 
relates roughly with the ratios of gralnlnar sizes, as 
expected. 
Second, table 4 lists the number of productions 
found in the tree-transformed test cortms that (lo 
not at)pear in the correspondingly transformed trees 
of sections 2 2t. What is striking here is tlmt the 
number of missing I)roductions aft;er either of the 
l ;ransforlns , Lo or , N is apl)roxil l lal;ely the 
sa, ine as t im number  of in iss ing 1)reductions us ing 
the untransformed trees, indicating that the factored 
selective left-corner transfl)rms cause little or no ad- 
ditional sparse data problem. (The relationship be- 
tween local trees ill the parse trees of G and ?dc(G) 
mentioned earlier implies that left-corner tree trans- 
tbrmations wilt not decrease the number of missing 
productions). 
We also investigate the accuracy of the maximum- 
likelihood parses (MLPs) obtained using the PCFGs 
estimated from tile output of the various left-corner 
tree transforms. 1 We searched for these parses us- 
ing all exhaustive CKY parser. Because the parse 
trees of these PCFGs are isomorphic to the deriva- 
tions of the corresponding stochastic generalized 
left-corner parsers, we are in fact evaluating different 
kinds of stochastic generalized left-corner parsers in- 
ferred from sections 2-21 of the tree-bank. We used 
1\?e (lid not investigate the grammars produced by the 
various left-corner grammar transforms. Because a left-corner 
grammar transform ECL preserves production probal)ilities, 
the highest scoring parses obtained using the weighted CFG 
EeL(G) should be the highest scoring parses obtained using 
G transformed by TL. 
nolle 
%, 
TN 
7}:0 
,,one (t~Z) (Z~) (ta, to) 
70.8,75.3 
75.8,77.7 74.8,76.9 
75.8,77.6 73.8,75.8 75.5,77.8 72.8,75.4 
75.8,77.4 73.0,74.7 75.6,77.8 72.9,75.4 
Table 5: Labelled recall and precision scores of PCFGs 
estimated using various tree-transforms ill a transform- 
detransform framework using test data from section 23. 
tile transforn>detransfornl franmwork described in 
Johnson (1998b) to evaluate the parses, i.e., we ap- 
plied tile at)propriate inverse tree transfornl ,\]---1 
to detransform the parse trees produced using the 
PCFG estimated froul trees transtbrnmd by T. By 
calculating the labelled precision and recall scores 
tbr the detransformed trees in the usual rammer, we 
can systematically compare the parsing accuracy of 
diflbrent kinds of stochastic generalized left-corner 
parsers. 
Table 5 presents the results of this comparison. As 
reported previously, the standard left-corner grmn- 
inar embeds sufficient non-local infornlation in its 
productions to significantly improve the labelled pre- 
cision and recall of its MLPs with respect o MLPs of 
the PCFG estimated from the untransfornmd trees 
(Maiming and Carpenter, 1997; ll.oark and John- 
son, 1999). Parsing accuracy drops off as granunar 
size decreases, presuntably because smaller PCFGs 
have fewer adjustatfle parameters with which to de- 
scribe this non-local information. There are other 
kinds of non-local information which can be incor- 
porated into a PCFG using a transforln-detransform 
approacll that result in an eve.n greater improvement 
of lml'sing accuracy (3ohnson, 1998b). Ultinmtely, 
however, it seems that a more complex ai)t)roach in- 
corporating back-off and smoothing is necessary ill 
order to achieve the parsing accuracy achieved by 
Charniak (1997) and Collins (1997). 
4 Conc lus ion  
This paper presented factored selective left-corner 
grammar transtbrms. These transtbrlns preserve the 
priinary benefits of the left-conmr grammar trans- 
form (i.e., elimination of left-recursion and preserva- 
tion of annotations on tlroductions) while dranmti- 
tally ameliorating its 1)rincipal problems (gramnmr 
size and sparse data problelns). This should extend 
the applicability of left-conmr techniques to situa- 
tions involving large grammars. We showed how to 
identif~y the nfinimal set L0 of productions of a gram- 
mar that must be recognized left-corner ill order for 
the transformed grammar not to be left-recursive. 
We also proposed two factorizations of tile output of 
the selective left-corner grmnmar t, ransfbrm which 
fllrther reduce grammar size, and showed that there 
is only a nfinor increase in gralnmar size when the 
360 
B~ctored sele(:tive left-corner transform is apl)lied to 
a large tre('A)a.nk grmnmar.  Finally, we exploited 
the tree trm~sforms that, correspond to these gram- 
mar trmlsforms to formulate and study a class of 
sto(:hastie general ized left-corner t)arsers. 
This work could be extended in a. nmnber of ways. 
D)r examl)le, in this t)al)er we assumed that  one 
would always choose a left-corner l)ro(lut'tioll set 
that  inehtdes the nfinimal set L0 required to ensure 
that  the transfbrmed grammar  is not left-recursive. 
However, Roark mid Johnson (1999) report  good 
perR)rmance from a stochast ical ly-guided top-down 
parser, suggesting that lefl;-recm'sion is not; always 
fatal. It might be possible to judic iously choose 
a M't-cor,mr product ion set smaller than L0 which 
el imiimtes t)erni(:ious left-r(;cursion, so that  the re- 
maining lefl;-reeursive cycles llav(', su(:h low t)rol)a- 
1)ility that  tlmy will efl'(~(:t;ively never l)e used and 
a stochast ical ly-guided top-down l)arser will II(~,Vel ' 
sea.reh l;h(un. 
References 
Stephen Almey and Mark Johnson. 1991. Memory re- 
quirements and local mnbiguities of parsing strategies. 
Journal of 1Lsycholinflui.stic R.e.scavch, 20(3):233 250. 
Steven Abney, David MeAlles,x~r, and D;rnando Pereira. 
1999. Relating 1)r()|)al)ilisl;i(: grammars and automata. 
Ill Procccdinfls of tile ,?Tth Annual Mcefinfl of the Asso- 
ciation for Computational Linipdstics, pages 542 549, 
San Francisco. Morgan Kauflmmn. 
Alfred V. Aho mM .leth;ry D. 1Jllman. 1!172. The 5lTtc- 
ory oJ'Parsing, Translation and Compiling; Volume. I: 
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey. 
Eugene Charniak. 1997. St;atist:i(:al parsing wit;h a 
colL|;ex1;-ll'ee ~lalll l l lar and word sl;atisl;i(:s. In \])~Y)('ccd,- 
ings of the Fourteenth Nationo, l Covj):rcnce on Artifi- 
cial hLtcdliflcnce, Menlo Park. AAAI Press/MIT Press. 
Michael Collins. 1997. Three generative, lexicalised 
models for st;atistical parsing. In The Proceedings of 
the 35th Annual Meeting of tile: Association jbr Com- 
p'utational Linguistics, San Francisco. M~orgall Kallf- 
I l I~U l l l .  
Michael A. Covington. 1994. Natural Lanfluage Pro- 
cessin9 for Prolo 9 Programmers. Prentice Ilall, En- 
glewood Clitli% New Jersey. 
A. Demers. 1977. Generalized left>corner parsing. In 
Cot@fence R.ccord of the Fourth ACM Symposium 
on Principles of Programming Lanfluagcs, 1977 A C'M 
SIGA CT/SIGPLAN, pages 170 -182. 
,John E. Hopcroft ml.d JeIfrey D. Ulhmm. 1979. Intro- 
duction to Automata Theory, Languages and Compu- 
tation. Addison-\Vesley. 
Mark Johnson. 1998a. Finite state apl)roximation 
of unification grammars using lefl;-eorner grmmnar 
l\[,,rallsforlllS. In ~'hc \])rocccdiltga of tlt('. 3O'th dtrt- 
nual Gin@fence of the Association J'or Computational 
Linguistics (COLING-ACL), pages 619 623. Morgan 
Kaufmann. 
Marl{ Johnson. 19981). PCFG mode, Is of linguis- 
tic tree representations. Computational Linguistics, 
24(4):613 632. 
Christol)her D. Manning mM Bob Carl)enter. 1997. 
1)robal)ilistic parsing using left>corner models. In Pro- 
cecdings of flu: 5th hdcrnational Workshop on Parsing 
Technologies, 1)ages 147- 158, Massachusetts Institut, e 
of Technology. 
guji Matsumoto, IIozumi Tanaka, Ilideki Ilirakawa, 
IIideo Miyoshi, and Hideki Yasukawa. 1983. BUP: 
A 1)otl;oIn-ll I) t)arser embedded in Prolog. New Gen- 
eration Computing, 1(2):145 158. 
l{.obert C. Moore. 2000. Removing left reeursion from 
context-flee grmnmars. In Proceedings of 1st Annual 
Conference of tile North American Chapter of the As- 
sociation for Computational Linguistics, San ~'an- 
cisco. Morgan Kauflnann. 
Anton Nijholt. 1980. Context-free Grammars: Co'rots, 
Normal Forms, and Par,sing. Springer Verlag, Berlin. 
Fermmdo C.N. Pereira and Stuart M. Shieber. 1987. 
P'~wlofl and Natural Language Analysis. Numbex l 0 in 
CSLI lx~eture Notes Series. Chicago University Press, 
Chicago. 
Philip l{esnik. 1.992. l,eft-corner parsing and psychologi- 
cal plausibility. In The Proceedings of th(', JlJ'tec, nth h~- 
tcrnational Conference on Computational Linfluistics, 
COLING-92, vohmw, 1, pages 191 197. 
\]3rian l/.oark and Mark Johnson. 1999. Efficient proba- 
bilistic top-down and left-corner parsing. In P~wcccd- 
ings of the 37th Annual Mcctinfl of the ACL, pages 
421 428. 
Stanley J. Rosenkrantz and Philip M. Lewis II. 197{). 
Deterministic M't corner parser. In IEEE CmLfcrcnce 
Record of the l lth Annual Symposium on Switchinq 
and Automata, pages 139 152. 
Gertjan van Noord. 1997. An efficient implenmnl;al;ion 
of the head-(:orn(!r l)arser. Computational Linguistics, 
23(3):425 q56. 
361 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568
Manchester, August 2008
When is Self-Training Effective for Parsing?
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Self-training has been shown capable of
improving on state-of-the-art parser per-
formance (McClosky et al, 2006) despite
the conventional wisdom on the matter and
several studies to the contrary (Charniak,
1997; Steedman et al, 2003). However, it
has remained unclear when and why self-
training is helpful. In this paper, we test
four hypotheses (namely, presence of a
phase transition, impact of search errors,
value of non-generative reranker features,
and effects of unknown words). From
these experiments, we gain a better un-
derstanding of why self-training works for
parsing. Since improvements from self-
training are correlated with unknown bi-
grams and biheads but not unknown words,
the benefit of self-training appears most in-
fluenced by seeing known words in new
combinations.
1 Introduction
Supervised statistical parsers attempt to capture
patterns of syntactic structure from a labeled set of
examples for the purpose of annotating new sen-
tences with their structure (Bod, 2003; Charniak
and Johnson, 2005; Collins and Koo, 2005; Petrov
et al, 2006; Titov and Henderson, 2007). These
annotations can be used by various higher-level ap-
plications such as semantic role labeling (Pradhan
et al, 2007) and machine translation (Yamada and
Knight, 2001).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
However, labeled training data is expensive to
annotate. Given the large amount of unlabeled text
available for many domains and languages, tech-
niques which allow us to use both labeled and
unlabeled text to train our models are desirable.
These methods are called semi-supervised. Self-
training is a specific type of semi-supervised learn-
ing. In self-training, first we train a model on the
labeled data and use that model to label the unla-
beled data. From the combination of our original
labeled data and the newly labeled data, we train a
second model ? our self-trained model. The pro-
cess can be iterated, where the self-trained model
is used to label new data in the next iteration. One
can think of self-training as a simple case of co-
training (Blum and Mitchell, 1998) using a single
learner instead of several. Alternatively, one can
think of it as one step of the Viterbi EM algorithm.
Studies prior to McClosky et al (2006) failed to
show a benefit to parsing from self-training (Char-
niak, 1997; Steedman et al, 2003). While the re-
cent success of self-training has demonstrated its
merit, it remains unclear why self-training helps in
some cases but not others. Our goal is to better un-
derstand when and why self-training is beneficial.
In Section 2, we discuss the previous applica-
tions of self-training to parsing. Section 3 de-
scribes our experimental setup. We present and
test four hypotheses of why self-training helps in
Section 4 and conclude with discussion and future
work in Section 5.
2 Previous Work
To our knowledge, the first reported uses of self-
training for parsing are by Charniak (1997). He
used his parser trained on the Wall Street Journal
(WSJ, Mitch Marcus et al (1993)) to parse 30 mil-
lion words of unparsed WSJ text. He then trained
561
a self-trained model from the combination of the
newly parsed text with WSJ training data. How-
ever, the self-trained model did not improve on the
original model.
Self-training and co-training were subsequently
investigated in the 2002 CLSP Summer Work-
shop at Johns Hopkins University (Steedman et
al., 2003). They considered several different pa-
rameter settings, but in all cases, the number of
sentences parsed per iteration of self-training was
relatively small (30 sentences). They performed
many iterations of self-training. The largest seed
size (amount of labeled training data) they used
was 10,000 sentences from WSJ, though many ex-
periments used only 500 or 1,000 sentences. They
found that under these parameters, self-training did
not yield a significant gain.
Reichart and Rappoport (2007) showed that one
can self-train with only a generative parser if the
seed size is small. The conditions are similar to
Steedman et al (2003), but only one iteration of
self-training is performed (i.e. all unlabeled data is
labeled at once).1 In this scenario, unknown words
(words seen in the unlabeled data but not in train-
ing) were a useful predictor of when self-training
improves performance.
McClosky et al (2006) showed that self-training
improves parsing accuracy when the two-stage
Charniak and Johnson (2005) reranking parser is
used. Using both stages (a generative parser and
discriminative reranker) to label the unlabeled data
set is necessary to improve performance. Only re-
training the first stage had a positive effect. How-
ever, after retraining the first stage, both stages pro-
duced better parses. Unlike Steedman et al (2003),
the training seed size is large and only one itera-
tion of self-training is performed. Error analysis
revealed that most improvement comes from sen-
tences with lengths between 20 and 40 words. Sur-
prisingly, improvements were also correlated with
the number of conjunctions but not with the num-
ber of unknown words in the sentence.
To summarize, several factors have been iden-
tified as good predictors of when self-training im-
proves performance, but a full explanation of why
self-training works is lacking. Previous work es-
tablishes that parsing all unlabeled sentences at
once (rather than over many iterations) is impor-
tant for successful self-training. The full effect of
1Performing multiple iterations presumably fails because
the parsing models become increasingly biased. However,
this remains untested in the large seed case.
seed size and the reranker on self-training is not
well understood.
3 Experimental Setup
We use the Charniak and Johnson reranking parser
(outlined below), though we expect many of these
results to generalize to other generative parsers
and discriminative rerankers. Our corpora consist
of WSJ for labeled data and NANC (North Amer-
ican News Text Corpus, Graff (1995)) for unla-
beled data. We use the standard WSJ division for
parsing: sections 2-21 for training (39,382 sen-
tences) and section 24 for development (1,346 sen-
tences). Given self-training?s varied performance
in the past, many of our experiments use the con-
catenation of sections 1, 22, and 24 (5,039 sen-
tences) rather than the standard development set
for more robust testing.
A full description of the reranking parser can be
found in Charniak and Johnson (2005). Briefly
put, the reranking parser consists of two stages:
A generative lexicalized PCFG parser which pro-
poses a list of the n most probable parses (n-best
list) followed by a discriminative reranker which
reorders the n-best list. The reranker uses about
1.3 million features to help score the trees, the
most important of which is the first stage parser?s
probability. In Section 4.3, we mention two classes
of reranker features in more depth. Since some of
experiments rely on details of the first stage parser,
we present a summary of the parsing model.
3.1 The Parsing Model
The parsing model assigns a probability to a parse
? by a top-down process of considering each con-
stituent c in ? and, for each c, first guessing the
preterminal of c, t(c) then the lexical head of c,
h(c), and then the expansion of c into further con-
stituents e(c). Thus the probability of a parse is
given by the equation
p(?) =
?
c??
p(t(c) | l(c),R(c))
?p(h(c) | t(c), l(c),R(c))
?p(e(c) | l(c), t(c), h(c),R(c))
where l(c) is the label of c (e.g., whether it is a
noun phrase (np), verb phrase, etc.) and R(c) is
the relevant history of c ?- information outside c
that the probability model deems important in de-
termining the probability in question.
562
For each expansion e(c) we distinguish one of
the children as the ?middle? child M(c). M(c) is
the constituent from which the head lexical item
h is obtained according to deterministic rules that
pick the head of a constituent from among the
heads of its children. To the left of M is a sequence
of one or more left labels L
i
(c) including the spe-
cial termination symbol ? and similarly for the la-
bels to the right, R
i
(c). Thus an expansion e(c)
looks like:
l ? ?L
m
...L
1
MR
1
...R
n
?. (1)
The expansion is generated by guessing first M ,
then in order L
1
through L
m+1
(= ?), and simi-
larly for R
1
through R
n+1
.
So the parser assigns a probability to the parse
based upon five probability distributions, T (the
part of speech of the head), H (the head), M (the
child constituent which includes the head), L (chil-
dren to the left of M ), and R (children to the right
of M ).
4 Testing the Four Hypotheses
The question of why self-training helps in some
cases (McClosky et al, 2006; Reichart and Rap-
poport, 2007) but not others (Charniak, 1997;
Steedman et al, 2003) has inspired various the-
ories. We investigate four of these to better un-
derstand when and why self-training helps. At
a high level, the hypotheses are (1) self-training
helps after a phase transition, (2) self-training re-
duces search errors, (3) specific classes of reranker
features are needed for self-training, and (4) self-
training improves because we see new combina-
tions of words.
4.1 Phase Transition
The phase transition hypothesis is that once a
parser has achieved a certain threshold of perfor-
mance, it can label data sufficiently accurately.
Once this happens, the labels will be ?good
enough? for self-training.
To test the phase transition hypothesis, we use
the same parser as McClosky et al (2006) but train
on only a fraction of WSJ to see if self-training is
still helpful. This is similar to some of the ex-
periments by Reichart and Rappoport (2007) but
with the use of a reranker and slightly larger seed
sizes. The self-training protocol is the same as
in (Charniak, 1997; McClosky et al, 2006; Re-
ichart and Rappoport, 2007): we parse the entire
unlabeled corpus in one iteration. We start by tak-
ing a random subset of the WSJ training sections
(2-21), accepting each sentence with 10% proba-
bility. With the sampled training section and the
standard development data, we train a parser and a
reranker. In Table 1, we show the performance of
the parser with and without the reranker. For ref-
erence, we show the performance when using the
complete training division as well. Unsurprisingly,
both metrics drop as we decrease the amount of
training data. These scores represent our baselines
for this experiment.
Using this parser model, we parse one million
sentences from NANC, both with and without the
reranker. We combine these one million sentences
with the sampled subsets of WSJ training and train
new parser models from them.2
Finally, we evaluate these self-trained models
(Table 2). The numbers in parentheses indicate the
change from the corresponding non-self-trained
model. As in Reichart and Rappoport (2007), we
see large improvements when self-training on a
small seed size (10%) without using the reranker.
However, using the reranker to parse the self-
training and/or evaluation sentences further im-
proves results. From McClosky et al (2006), we
know that when 100% of the training data is used,
self-training does not improve performance with-
out a reranker.
From this we conclude that there is no such
threshold phase transition in this case. High per-
formance is not a requirement to successfully use
self-training for parsing, since there are lower per-
forming parsers which can self-train and higher
performing parsers which cannot. The higher per-
forming Charniak and Johnson (2005) parser with-
out reranker achieves an f -score of 89.0 on section
24 when trained on all of WSJ. This parser does
not benefit from self-training unless paired with a
reranker. Contrast this with the same parser trained
on only 10% of WSJ, where it gets an f -score of
85.8 (Table 2) or the small seed models of Reichart
and Rappoport (2007). Both of these lower per-
forming parsers can successfully self-train. Ad-
ditionally, we now know that while a reranker is
not required for self-training when the seed size is
small, it still helps performance considerably (f -
score improves from 87.7 to 89.0 in the 10% case).
2We do not weight the original WSJ data, though our ex-
pectation is that performance would improve if WSJ were
given a higher relative weight. This is left as future work.
563
% WSJ # sentences Parser f -score Reranking parser f -score
10 3,995 85.8 87.0
100 39,832 89.9 91.5
Table 1: Parser and reranking parser performance on sentences ? 100 words in sections 1, 22, and 24
when trained on different amounts of training data. % WSJ is the percentage of WSJ training data trained
on (sampled randomly). Note that the full amount of development data is still used as held out data.
Parsed NANC with reranker? Parser f -score Reranking parser f -score
No 87.7 (+1.9) 88.7 (+1.7)
Yes 88.4 (+2.6) 89.0 (+2.0)
Table 2: Effect of self-training using only 10% of WSJ as labeled data. The parser model is trained from
one million parsed sentences from NANC + WSJ training. The first column indicates whether the million
NANC sentences were parsed by the parser or reranking parser. The second and third columns differ in
whether the reranker is used to parse the test sentences (WSJ sections 1, 22, and 24, sentences 100 words
and shorter). Numbers in parentheses are the improvements over the corresponding non-self-trained
parser.
4.2 Search Errors
Another possible explanation of self-training?s im-
provements is that seeing newly labeled data re-
sults in fewer search errors (Daniel Marcu, per-
sonal communication). A search error would in-
dicate that the parsing model could have produced
better (more probable) parses if not for heuristics
in the search procedure. The additional parse trees
may help produce sharper distributions and reduce
data sparsity, making the search process easier. To
test this, first we present some statistics on the n-
best lists (n = 50) from the baseline WSJ trained
parser and self-trained model3 from McClosky et
al. (2006). We use each model to parse sentences
from held-out data (sections 1, 22, and 24) and ex-
amine the n-best lists.
We compute statistics of the WSJ and self-
trained n-best lists with the goal of understand-
ing how much they intersect and whether there are
search errors. On average, the n-best lists over-
lap by 66.0%. Put another way, this means that
about a third of the parses from each model are
unique, so the parsers do find a fair number of dif-
ferent parses in their search. The next question
is where the differences in the n-best lists lie ?
if all the differences were near the bottom, this
would be less meaningful. Let W and S repre-
sent the n-best lists from the baseline WSJ and self-
trained parsers, respectively. The top
m
(?) func-
tion returns the highest scoring parse in the n-best
list ? according to the reranker and parser model
3http://bllip.cs.brown.edu/selftraining/
m.
4 Almost 40% of the time, the top parse in
the self-trained model is not in the WSJ model?s
n-best list, (top
s
(S) /? W ) though the two mod-
els agree on the top parse roughly 42.4% of the
time (top
s
(S) = top
w
(W )). Search errors can
be formulated as top
s
(S) /? W ? top
s
(S) =
top
w
(W ? S). This captures sentences where the
parse that the reranker chose in the self-trained
model is not present in the WSJ model?s n-best list,
but if the parse were added to the WSJ model?s list,
the parse?s probability in the WSJ model and other
reranker features would have caused it to be cho-
sen. These search errors occur in only 2.5% of
the n-best lists. At first glance, one might think
that this could be enough to account for the differ-
ences, since the self-trained model is only several
tenths better in f -score. However, we know from
McClosky et al (2006) that on average, parses do
not change between the WSJ and self-trained mod-
els and when they do, they only improve slightly
more than half the time. For this reason, we run a
second test more focused on performance.
For our second test we help the WSJ trained
model find the parses that the self-trained model
found. For each sentence, we start with the n-best
list (n = 500 here) from the WSJ trained parser,
W . We then consider parses in the self-trained
parser?s n-best list, S, that are not present in W
(S ? W ). For each of these parses, we deter-
mine its probability under the WSJ trained parsing
4Recall that the parser?s probability is a reranker feature
so the parsing model influences the ranking.
564
Model f -score
WSJ 91.5
WSJ & search help 91.7
Self-trained 92.0
Table 3: Test of whether ?search help? from the
self-trained model impacts the WSJ trained model.
WSJ + search help is made by adding self-trained
parses not proposed by the WSJ trained parser but
to which the parser assigns a positive probability.
The WSJ reranker is used in all cases to select the
best parse for sections 1, 22, and 24.
model. If the probability is non-zero, we add the
parse to the n-best list W , otherwise we ignore the
parse. In other words, we find parses that the WSJ
trained model could have produced but didn?t due
to search heuristics. In Table 3, we show the per-
formance of the WSJ trained model, the model with
?search help? as described above, and the self-
trained model on WSJ sections 1, 22, and 24. The
WSJ reranker is used to pick the best parse from
each n-best list. WSJ with search help performs
slightly better than WSJ alone but does not reach
the level of the self-trained model. From these ex-
periments, we conclude that reduced search errors
can only explain a small amount of self-training?s
improvements.
4.3 Non-generative reranker features
We examine the role of specific reranker features
by training rerankers using only subsets of the fea-
tures. Our goal is to determine whether some
classes of reranker features benefit self-training
more than others. We hypothesize that features
which are not easily captured by the generative
first-stage parser are the most beneficial for self-
training. If we treat the parser and reranking parser
as different (but clearly dependent) views, this is a
bit like co-training. If the reranker uses features
which are captured by the first-stage, the views
may be too similar for there to be an improvement.
We consider two classes of features (GEN and
EDGE) and their complements (NON-GEN and
NON-EDGE).5 GEN consists of features that
are roughly captured by the first-stage generative
parser: rule rewrites, head-child dependencies, etc.
EDGE features describe items across constituent
boundaries. This includes the words and parts of
5A small number of features overlap hence these sizes do
not add up.
Feature set # features f -score
GEN 448,349 90.4
NON-GEN 885,492 91.1
EDGE 601,578 91.0
NON-EDGE 732,263 91.1
ALL 1,333,519 91.3
Table 4: Sizes and performance of reranker feature
subsets. Reranking parser f -scores are on all sen-
tences in section 24.
speech of the tokens on the edges between con-
stituents and the labels of these constituents. This
represents a specific class of features not captured
by the first-stage. These subsets and their sizes are
shown in Table 4. For comparison, we also include
the results of experiments using the full feature set,
as in McClosky et al (2006), labeled ALL. The
GEN features are roughly one third the size of the
full feature set.
We evaluate the effect of these new reranker
models on self-training (Table 4). For each fea-
ture set, we do the following: We parse one million
NANC sentences with the reranking parser. Com-
bining the parses with WSJ training data, we train
a new first-stage model. Using this new first-stage
model and the reranker subset, we evaluate on sec-
tion 24 of WSJ. GEN?s performance is weaker
while the other three subsets achieve almost the
same score as the full feature set. This confirms
our hypothesis that when the reranker helps in self-
training it is due to features which are not captured
by the generative first-stage model.
4.4 Unknown Words
Given the large size of the parsed self-training cor-
pus, it contains an immense number of parsing
events which never occur in the training corpus.
The most obvious of these events is words ? the
vocabulary grows from 39,548 to 265,926 words
as we transition from the WSJ trained model to
the self-trained model. Slightly less obvious is bi-
grams. There are roughly 330,000 bigrams in WSJ
training data and approximately 4.8 million new
bigrams in the self-training corpus.
One hypothesis (Mitch Marcus, personal com-
munication) is that the parser is able to learn a lot
of new bilexical head-to-head dependencies (bi-
heads) from self-training. The reasoning is as fol-
lows: Assume the self-training corpus is parsed in
a mostly correct manner. If there are not too many
565
new pairs of words in a sentence, there is a de-
cent chance that we can tag these words correctly
and bracket them in a reasonable fashion from con-
text. Thus, using these parses as part of the train-
ing data improves parsing because should we see
these pairs of words together in the future, we will
be more likely to connect them together properly.
We test this hypothesis in two ways. First, we
perform an extension of the factor analysis simi-
lar to that in McClosky et al (2006). This is done
via a generalized linear regression model intended
to determine which features of parse trees can pre-
dict when the self-training model will perform bet-
ter. We consider many of the same features (e.g.
bucketed sentence length, number of conjunctions,
and number of unknown words) but also consider
two new features: unknown bigrams and unknown
biheads. Unknown items (words, bigrams, bi-
heads) are calculated by counting the number of
items which have never been seen in WSJ train-
ing but have been seen in the parsed NANC data.
Given these features, we take the f -scores for each
sentence when parsed by the WSJ and self-trained
models and look at the differences. Our goal is to
find out which features, if any, can predict these f -
score differences. Specifically, we ask the question
of whether seeing more unknown items indicates
whether we are more likely to see improvements
when self-training.
The effect of unknown items on self-training?s
relative performance is summarized in Figure 1.
For each item, we show the total number of incor-
rect parse nodes in sentences that contain the item.
We also show the change in the number of correct
parse nodes in these sentences between the WSJ
and self-trained models. A positive change means
that performance improved under self-training. In
other words, looking at Figure 1a, the greatest per-
formance improvement occurs, perhaps surpris-
ingly, when we have seen no unknown words.
As we see more unknown words, the improve-
ment from self-training decreases. This is a pretty
clear indication that unknown words are not a good
predictor of when self-training improves perfor-
mance.
A possible objection that one might raise is that
using unknown biheads as a regression feature will
bias our results if they are counted from gold trees
instead of parsed trees. Seeing a bihead in train-
ing will cause the otherwise sparse biheads dis-
tribution to be extremely peaked around that bi-
f -score Model
89.8 ? WSJ (baseline)
89.8 ? WSJ+NANC M
89.9 ? WSJ+NANC T
89.9 ? WSJ+NANC L
90.0 ? WSJ+NANC R
90.0 WSJ+NANC MT
90.1 WSJ+NANC H
90.2 WSJ+NANC LR
90.3 WSJ+NANC LRT
90.4 WSJ+NANC LMRT
90.4 WSJ+NANC LMR
90.5 WSJ+NANC LRH
90.7 ? WSJ+NANC LMRH
90.8 ? WSJ+NANC (fully self-trained)
Table 5: Performance of the first-stage parser
on various combinations of distributions WSJ and
WSJ+NANC (self-trained) models on sections 1,
22, and 24. Distributions are L (left expansion), R
(right expansion), H (head word), M (head phrasal
category), and T (head POS tag). ? and ? indicate
the model is not significantly different from base-
line and self-trained model, respectively.
head. If we see the same pair of words in testing,
we are likely to connect them in the same fash-
ion. Thus, if we count unknown biheads from gold
trees, this feature may explain away other improve-
ments: When gold trees contain a bihead found in
our self-training data, we will almost always see an
improvement. However, given the similar trends in
Figures 1b and 1c, we propose that unknown bi-
grams can be thought of as a rough approximation
of unknown biheads.
The regression analysis reveals that unknown bi-
grams and unknown biheads are good predictors of
f -score improvements. The significant predictors
from McClosky et al (2006) such as the number
of conjunctions or sentence length continue to be
helpful whereas unknown words are a weak pre-
dictor at best. These results are apparent in Figure
1: as stated before, seeing more unknown words
does not correlate with improvements. However,
seeing more unknown bigrams and biheads does
predict these changes fairly well. When we have
seen zero or one new bigrams or biheads, self-
training negatively impacts performance. After
seeing two or more, we see positive effects until
about six to ten after which improvements taper
off.
566
To see the effect of biheads on performance
more directly, we also experiment by interpolat-
ing between the WSJ and self-trained models on a
distribution level. To do this, we take specific dis-
tributions (see Section 3.1) from the self-trained
model and have them override the corresponding
distributions in a compatible WSJ trained model.
From this we hope to show which distributions
self-training boosts. According to the biheads hy-
pothesis, the H distribution (which captures infor-
mation about head-to-head dependencies) should
account for most of the improvement.
The results of moving these distributions is
shown in Table 5. For each new model, we show
whether the model?s performance is not signifi-
cantly different than the baseline model (indicated
by ?) or not significantly different than the self-
trained model (?). H (biheads) is the strongest sin-
gle feature and the only one to be significantly bet-
ter than the baseline. Nevertheless, it is only 0.3%
higher, accounting for 30% of the full self-training
improvement. In general, the performance im-
provements from distributions are additive (+/?
0.1%). Self-training improves all distributions, so
biheads are not the full picture. Nevertheless, they
remain the strongest single feature.
5 Discussion
The experiments in this paper have clarified many
details about the nature of self-training for parsing.
We have ruled out the phase transition hypothe-
sis entirely. Reduced search errors are responsible
for some, but not all, of the improvements in self-
training. We have confirmed that non-generative
reranker features are more beneficial than genera-
tive reranker features since they make the rerank-
ing parser more different from the base parser. Fi-
nally, we have found that while unknown bigrams
and biheads are a significant source of improve-
ment, they are not the sole source of it. Since
unknown words do not correlate well with self-
training improvements, there must be something
about the unknown bigrams and biheads which are
aid the parser. Our belief is that new combinations
of words we have already seen guides the parser in
the right direction. Additionally, these new combi-
nations result in more peaked distributions which
decreases the number of search errors.
However, while these experiments and others
get us closer to understanding self-training, we still
lack a complete explanation. Naturally, the hy-
0 1 2 3 4 5 6 7 10 11 12
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
60
00
0 1 2 3 4 5 6 7 10 11 12
Number of unknown words in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
0
30
0
(a) Effect of unknown words on performance
0 2 4 6 8 10 12 14 16 18 20
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20
Number of unknown bigrams in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
10
0
10
0
(b) Effect of unknown bigrams on performance
0 2 4 6 8 10 12 14 16 18 20 25
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20 25
Number of unknown biheads in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
50
10
0
(c) Effect of unknown biheads on performance
Figure 1: Change in the number of incorrect parse
tree nodes between WSJ and self-trained models
as a function of number of unknown items. See-
ing any number of unknown words results in fewer
errors on average whereas seeing zero or one un-
known bigrams or biheads is likely to hurt perfor-
mance.
567
potheses tested are by no means exhaustive. Addi-
tionally, we have only considered generative con-
stituency parsers here and a good direction for fu-
ture research would be to see if self-training gener-
alizes to a broader class of parsers. We suspect that
using a generative parser/discriminative reranker
paradigm should allow self-training to extend to
other parsing formalisms.
Recall that in Reichart and Rappoport (2007)
where only a small amount of labeled data was
used, the number of unknown words in a sen-
tence was a strong predictor of self-training ben-
efits. When a large amount of labeled data is avail-
able, unknown words are no longer correlated with
these gains, but unknown bigrams and biheads are.
When using a small amount of training data, un-
known words are useful since we have not seen
very many words yet. As the amount of train-
ing data increases, we see fewer new words but
the number of new bigrams and biheads remains
high. We postulate that this difference may help
explain the shift from unknown words to unknown
bigrams and biheads. We hope to further inves-
tigate the role of these unknown items by seeing
how our analyses change under different amounts
of labeled data relative to unknown item rates.
Acknowledgments
This work was supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank Matt Lease, the
rest of the BLLIP team, and our anonymous reviewers for
their comments. Any opinions, findings, and conclusions or
recommendations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Blum, Avrim and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory (COLT-98).
Bod, Rens. 2003. An efficient implementation of a
new DOP model. In 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Budapest, Hungary.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the Assoc.
for Computational Linguistics (ACL), pages 173?
180.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proc.
AAAI, pages 598?603.
Collins, Michael and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?69.
Graff, David. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159.
Mitch Marcus et al 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Pradhan, Sameer, Wayne Ward, and James Martin.
2007. Towards robust semantic role labeling. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 556?563, Rochester, New York,
April. Association for Computational Linguistics.
Reichart, Roi and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 616?623.
Steedman, Mark, Steven Baker, Jeremiah Crim,
Stephen Clark, Julia Hockenmaier, Rebecca Hwa,
Miles Osborne, Paul Ruhlen, and Anoop Sarkar.
2003. CLSP WS-02 Final Report: Semi-Supervised
Training for Statistical Parsing. Technical report,
Johns Hopkins University.
Titov, Ivan and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 632?
639, Prague, Czech Republic, June. Association for
Computational Linguistics.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting of the Association for
Computational Linguistics, pages 523?529.
568
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344?352,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A comparison of Bayesian estimators for
unsupervised Hidden Markov Model POS taggers
Jianfeng Gao
Microsoft Research
Redmond, WA, USA
jfgao@microsoft.com
Mark Johnson
Brown Univeristy
Providence, RI, USA
Mark?Johnson@Brown.edu
Abstract
There is growing interest in applying Bayesian
techniques to NLP problems. There are a
number of different estimators for Bayesian
models, and it is useful to know what kinds of
tasks each does well on. This paper compares
a variety of different Bayesian estimators for
Hidden Markov Model POS taggers with var-
ious numbers of hidden states on data sets of
different sizes. Recent papers have given con-
tradictory results when comparing Bayesian
estimators to Expectation Maximization (EM)
for unsupervised HMM POS tagging, and we
show that the difference in reported results is
largely due to differences in the size of the
training data and the number of states in the
HMM. We invesigate a variety of samplers for
HMMs, including some that these earlier pa-
pers did not study. We find that all of Gibbs
samplers do well with small data sets and few
states, and that Variational Bayes does well
on large data sets and is competitive with the
Gibbs samplers. In terms of times of conver-
gence, we find that Variational Bayes was the
fastest of all the estimators, especially on large
data sets, and that explicit Gibbs sampler (both
pointwise and sentence-blocked) were gener-
ally faster than their collapsed counterparts on
large data sets.
1 Introduction
Probabilistic models now play a central role in com-
putational linguistics. These models define a prob-
ability distribution P(x) over structures or analyses
x. For example, in the part-of-speech (POS) tag-
ging application described in this paper, which in-
volves predicting the part-of-speech tag ti of each
word wi in the sentence w = (w1, . . . , wn), the
structure x = (w, t) consists of the words w in a
sentence together with their corresponding parts-of-
speech t = (t1, . . . , tn).
In general the probabilistic models used in com-
putational linguistics have adjustable parameters ?
which determine the distribution P(x | ?). In this
paper we focus on bitag Hidden Markov Models
(HMMs). Since our goal here is to compare algo-
rithms rather than achieve the best performance, we
keep the models simple by ignoring morphology and
capitalization (two very strong cues in English) and
treat each word as an atomic entity. This means that
the model parameters ? consist of the HMM state-
to-state transition probabilities and the state-to-word
emission probabilities.
In virtually all statistical approaches the parame-
ters ? are chosen or estimated on the basis of training
data d. This paper studies unsupervised estimation,
so d = w = (w1, . . . , wn) consists of a sequence
of words wi containing all of the words of training
corpus appended into a single string, as explained
below.
Maximum Likelihood (ML) is the most common
estimation method in computational linguistics. A
Maximum Likelihood estimator sets the parameters
to the value ?? that makes the likelihood Ld of the
data d as large as possible:
Ld(?) = P(d | ?)
?? = argmax
?
Ld(?)
In this paper we use the Inside-Outside algo-
rithm, which is a specialized form of Expectation-
344
Maximization, to find HMM parameters which (at
least locally) maximize the likelihood function Ld.
Recently there is increasing interest in Bayesian
methods in computational linguistics, and the pri-
mary goal of this paper is to compare the perfor-
mance of various Bayesian estimators with each
other and with EM.
A Bayesian approach uses Bayes theorem to fac-
torize the posterior distribution P(? | d) into the
likelihood P(d | ?) and the prior P(?).
P(? | d) ? P(d | ?) P(?)
Priors can be useful because they can express pref-
erences for certain types of models. To take an
example from our POS-tagging application, most
words belong to relatively few parts-of-speech (e.g.,
most words belong to a single POS, and while there
are some words which are both nouns and verbs,
very few are prepositions and adjectives as well).
One might express this using a prior which prefers
HMMs in which the state-to-word emissions are
sparse, i.e., each state emits few words. An appro-
priate Dirichlet prior can express this preference.
While it is possible to use Bayesian inference to
find a single model, such as the Maximum A Pos-
teriori or MAP value of ? which maximizes the
posterior P(? | d), this is not necessarily the best
approach (Bishop, 2006; MacKay, 2003). Instead,
rather than commiting to a single value for the pa-
rameters ? many Bayesians often prefer to work
with the full posterior distribution P(? | d), as this
naturally reflects the uncertainty in ??s value.
In all but the simplest models there is no known
closed form for the posterior distribution. However,
the Bayesian literature describes a number of meth-
ods for approximating the posterior P(? | d). Monte
Carlo sampling methods and Variational Bayes are
two kinds of approximate inference methods that
have been applied to Bayesian inference of unsu-
pervised HMM POS taggers (Goldwater and Grif-
fiths, 2007; Johnson, 2007). These methods can also
be used to approximate other distributions that are
important to us, such as the conditional distribution
P(t | w) of POS tags (i.e., HMM hidden states) t
given words w.
This recent literature reports contradictory results
about these Bayesian inference methods. John-
son (2007) compared two Bayesian inference algo-
rithms, Variational Bayes and what we call here a
point-wise collapsed Gibbs sampler, and found that
Variational Bayes produced the best solution, and
that the Gibbs sampler was extremely slow to con-
verge and produced a worse solution than EM. On
the other hand, Goldwater and Griffiths (2007) re-
ported that the same kind of Gibbs sampler produced
much better results than EM on their unsupervised
POS tagging task. One of the primary motivations
for this paper was to understand and resolve the dif-
ference in these results. We replicate the results of
both papers and show that the difference in their re-
sults stems from differences in the sizes of the train-
ing data and numbers of states in their models.
It turns out that the Gibbs sampler used in these
earlier papers is not the only kind of sampler for
HMMs. This paper compares the performance of
four different kinds of Gibbs samplers, Variational
Bayes and Expectation Maximization on unsuper-
vised POS tagging problems of various sizes. Our
goal here is to try to learn how the performance of
these different estimators varies as we change the
number of hidden states in the HMMs and the size
of the training data.
In theory, the Gibbs samplers produce streams
of samples that eventually converge on the true
posterior distribution, while the Variational Bayes
(VB) estimator only produces an approximation to
the posterior. However, as the size of the training
data distribution increases the likelihood function
and therefore the posterior distribution becomes in-
creasingly peaked, so one would expect this varia-
tional approximation to become increasingly accu-
rate. Further the Gibbs samplers used in this paper
should exhibit reduced mobility as the size of train-
ing data increases, so as the size of the training data
increases eventually the Variational Bayes estimator
should prove to be superior.
However the two point-wise Gibbs samplers in-
vestigated here, which resample the label of each
word conditioned on the labels of its neighbours
(amongst other things) only require O(m) steps per
sample (where m is the number of HMM states),
while EM, VB and the sentence-blocked Gibbs sam-
plers require O(m2) steps per sample. Thus for
HMMs with many states it is possible to perform one
or two orders of magnitude more iterations of the
345
point-wise Gibbs samplers in the same run-time as
the other samplers, so it is plausible that they would
yield better results.
2 Inference for HMMs
There are a number of excellent textbook presen-
tations of Hidden Markov Models (Jelinek, 1997;
Manning and Schu?tze, 1999), so we do not present
them in detail here. Conceptually, a Hidden Markov
Model uses a Markov model to generate the se-
quence of states t = (t1, . . . , tn) (which will be in-
terpreted as POS tags), and then generates each word
wi conditioned on the corresponding state ti.
We insert endmarkers at the beginning and end
of the corpus and between sentence boundaries,
and constrain the estimators to associate endmarkers
with a special HMM state that never appears else-
where in the corpus (we ignore these endmarkers
during evaluation). This means that we can formally
treat the training corpus as one long string, yet each
sentence can be processed independently by a first-
order HMM.
In more detail, the HMM is specified by a pair of
multinomials ?t and ?t associated with each state t,
where ?t specifies the distribution over states t? fol-
lowing t and ?t specifies the distribution over words
w given state t.
ti | ti?1 = t ? Multi(?t)
wi | ti = t ? Multi(?t)
(1)
The Bayesian model we consider here puts a fixed
uniform Dirichlet prior on these multinomials. Be-
cause Dirichlets are conjugate to multinomials, this
greatly simplifies inference.
?t | ? ? Dir(?)
?t | ?? ? Dir(??)
A multinomial ? is distributed according to the
Dirichlet distribution Dir(?) iff:
P(? | ?) ?
m
?
j=1
??j?1j
In our experiments we set ? and ?? to the uniform
values (i.e., all components have the same value ? or
??), but it is possible to estimate these as well (Gold-
water and Griffiths, 2007). Informally, ? controls
the sparsity of the state-to-state transition probabil-
ities while ?? controls the sparsity of the state-to-
word emission probabilities. As ?? approaches zero
the prior strongly prefers models in which each state
emits as few words as possible, capturing the intu-
ition that most word types only belong to one POS
mentioned earlier.
2.1 Expectation Maximization
Expectation-Maximization is a procedure that iter-
atively re-estimates the model parameters (?,?),
converging on a local maximum of the likelihood.
Specifically, if the parameter estimate at iteration `
is (?(`),?(`)), then the re-estimated parameters at it-
eration `+ 1 are:
?(`+1)t?|t = E[nt?,t]/E[nt] (2)
?(`+1)w|t = E[n
?
w,t]/E[nt]
where n?w,t is the number of times word w occurs
with state t, nt?,t is the number of times state t? fol-
lows t and nt is the number of occurences of state t;
all expectations are taken with respect to the model
(?(`),?(`)).
The experiments below used the Forward-
Backward algorithm (Jelinek, 1997), which is a dy-
namic programming algorithm for calculating the
likelihood and the expectations in (2) in O(nm2)
time, where n is the number of words in the train-
ing corpus and m is the number of HMM states.
2.2 Variational Bayes
Variational Bayesian inference attempts to find a
function Q(t,?,?) that minimizes an upper bound
(3) to the negative log likelihood.
? log P(w)
= ? log
?
Q(t,?,?)P(w, t,?,?)Q(t, ?, ?) dt d? d?
? ?
?
Q(t,?,?) log P(w, t,?,?)Q(t,?,?) dt d? d? (3)
The upper bound (3) is called the Variational Free
Energy. We make a ?mean-field? assumption that
the posterior can be well approximated by a factor-
ized model Q in which the state sequence t does not
covary with the model parameters ?,?:
P(t,?,? | w) ? Q(t,?,?) = Q1(t)Q2(?,?)
346
P(ti|w, t?i, ?, ??) ?
(n?wi,ti + ??
nti + m???
) (nti,ti?1 + ?
nti?1 + m?
) (nti+1,ti + I(ti?1 = ti = ti+1) + ?
nti + I(ti?1 = ti) + m?
)
Figure 1: The conditional distribution for state ti used in the pointwise collapsed Gibbs sampler, which conditions on
all states t?i except ti (i.e., the counts n do not include ti). Here m? is the size of the vocabulary, m is the number of
HMM states and I(?) is the indicator function (i.e., equal to one if its argument is true and zero otherwise),
The calculus of variations is used to minimize the
KL divergence between the desired posterior distri-
bution and the factorized approximation. It turns
out that if the likelihood and conjugate prior be-
long to exponential families then the optimal Q1 and
Q2 do too, and there is an EM-like iterative pro-
cedure that finds locally-optimal model parameters
(Bishop, 2006).
This procedure is especially attractive for HMM
inference, since it involves only a minor modifica-
tion to the M-step of the Forward-Backward algo-
rithm. MacKay (1997) and Beal (2003) describe
Variational Bayesian (VB) inference for HMMs. In
general, the E-step for VB inference for HMMs is
the same as in EM, while the M-step is as follows:
??(`+1)t?|t = f(E[nt?,t] + ?)/f(E[nt] +m?) (4)
??(`+1)w|t = f(E[n
?
w,t] + ??)/f(E[nt] + m???)
f(v) = exp(?(v))
where m? and m are the number of word types and
states respectively, ? is the digamma function and
the remaining quantities are as in (2). This means
that a single iteration can be performed in O(nm2)
time, just as for the EM algorithm.
2.3 MCMC sampling algorithms
The goal of Markov Chain Monte Carlo (MCMC)
algorithms is to produce a stream of samples from
the posterior distribution P(t | w,?). Besag (2004)
provides a tutorial on MCMC techniques for HMM
inference.
A Gibbs sampler is a simple kind of MCMC
algorithm that is well-suited to sampling high-
dimensional spaces. A Gibbs sampler for P(z)
where z = (z1, . . . , zn) proceeds by sampling and
updating each zi in turn from P(zi | z?i), where
z?i = (z1, . . . , zi?1, zi+1, . . . , zn), i.e., all of the
z except zi (Geman and Geman, 1984; Robert and
Casella, 2004).
We evaluate four different Gibbs samplers in this
paper, which vary along two dimensions. First, the
sampler can either be pointwise or blocked. A point-
wise sampler resamples a single state ti (labeling a
single word wi) at each step, while a blocked sam-
pler resamples the labels for all of the words in a
sentence at a single step using a dynamic program-
ming algorithm based on the Forward-Backward al-
gorithm. (In principle it is possible to use block
sizes other than the sentence, but we did not explore
this here). A pointwise sampler requires O(nm)
time per iteration, while a blocked sampler requires
O(nm2) time per iteration, where m is the number
of HMM states and n is the length of the training
corpus.
Second, the sampler can either be explicit or col-
lapsed. An explicit sampler represents and sam-
ples the HMM parameters ? and ? in addition to
the states t, while in a collapsed sampler the HMM
parameters are integrated out, and only the states t
are sampled. The difference between explicit and
collapsed samplers corresponds exactly to the dif-
ference between the two PCFG sampling algorithms
presented in Johnson et al (2007).
An iteration of the pointwise explicit Gibbs sam-
pler consists of resampling ? and ? given the state-
to-state transition counts n and state-to-word emis-
sion counts n? using (5), and then resampling each
state ti given the corresponding word wi and the
neighboring states ti?1 and ti+1 using (6).
?t | nt,? ? Dir(nt +?)
?t | n?t,?? ? Dir(n?t +??)
(5)
P(ti | wi, t?i,?,?) ? ?ti|ti?1?wi|ti?ti+1|ti (6)
The Dirichlet distributions in (5) are non-uniform;
nt is the vector of state-to-state transition counts in
t leaving state t in the current state vector t, while
347
n?t is the vector of state-to-word emission counts for
state t. See Johnson et al (2007) for a more detailed
explanation, as well as an algorithm for sampling
from the Dirichlet distributions in (5).
The samplers that Goldwater and Griffiths (2007)
and Johnson (2007) describe are pointwise collapsed
Gibbs samplers. Figure 1 gives the sampling distri-
bution for this sampler. As Johnson et al (2007)
explains, samples of the HMM parameters ? and ?
can be obtained using (5) if required.
The blocked Gibbs samplers differ from the point-
wise Gibbs samplers in that they resample the POS
tags for an entire sentence at a time. Besag (2004)
describes the well-known dynamic programming
algorithm (based on the Forward-Backward algo-
rithm) for sampling a state sequence t given the
words w and the transition and emission probabil-
ities ? and ?.
At each iteration the explicit blocked Gibbs sam-
pler resamples ? and ? using (5), just as the explicit
pointwise sampler does. Then it uses the new HMM
parameters to resample the states t for the training
corpus using the algorithm just mentioned. This can
be done in parallel for each sentence in the training
corpus.
The collapsed blocked Gibbs sampler is a
straight-forward application of the Metropolis-
within-Gibbs approach proposed by Johnson et al
(2007) for PCFGs, so we only sketch it here. We
iterate through the sentences of the training data, re-
sampling the states for each sentence conditioned
on the state-to-state transition counts n and state-
to-word emission counts n? for the other sentences
in the corpus. This is done by first computing the
parameters ?? and ?? of a proposal HMM using (7).
??t?|t =
nt?,t + ?
nt + m?
(7)
??w|t =
n?w,t + ??
nt + m??
Then we use the dynamic programming sampler de-
scribed above to produce a proposal state sequence
t? for the words in the sentence. Finally, we use
a Metropolis-Hastings accept-reject step to decide
whether to update the current state sequence for the
sentence with the proposal t?, or whether to keep the
current state sequence. In practice, with all but the
very smallest training corpora the acceptance rate is
very high; the acceptance rate for all of our collapsed
blocked Gibbs samplers was over 99%.
3 Evaluation
The previous section described six different unsu-
pervised estimators for HMMs. In this section
we compare their performance for English part-of-
speech tagging. One of the difficulties in evalu-
ating unsupervised taggers such as these is map-
ping the system?s states to the gold-standard parts-
of-speech. Goldwater and Griffiths (2007) proposed
an information-theoretic measure known as the Vari-
ation of Information (VI) described by Meila? (2003)
as an evaluation of an unsupervised tagging. How-
ever as Goldwater (p.c.) points out, this may not be
an ideal evaluation measure; e.g., a tagger which as-
signs all words the same single part-of-speech tag
does disturbingly well under Variation of Informa-
tion, suggesting that a poor tagger may score well
under VI.
In order to avoid this problem we focus here on
evaluation measures that construct an explicit map-
ping between the gold-standard part-of-speech tags
and the HMM?s states. Perhaps the most straight-
forward approach is to map each HMM state to the
part-of-speech tag it co-occurs with most frequently,
and use this mapping to map each HMM state se-
quence t to a sequence of part-of-speech tags. But as
Clark (2003) observes, this approach has several de-
fects. If a system is permitted to posit an unbounded
number of states (which is not the case here) it can
achieve a perfect score on by assigning each word
token its own unique state.
We can partially address this by cross-validation.
We divide the corpus into two equal parts, and from
the first part we extract a mapping from HMM states
to the parts-of-speech they co-occur with most fre-
quently, and use that mapping to map the states of
the second part of the corpus to parts-of-speech. We
call the accuracy of the resulting tagging the cross-
validation accuracy.
Finally, following Haghighi and Klein (2006) and
Johnson (2007) we can instead insist that at most
one HMM state can be mapped to any part-of-speech
tag. Following these authors, we used a greedy algo-
rithm to associate states with POS tags; the accuracy
of the resulting tagging is called the greedy 1-to-1
348
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165
VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599
GSe,p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811
GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032
GSc,p 0.49910? 0.45028 0.42785 0.43652 0.39182 0.39164
GSc,b 0.49486? 0.46193 0.41162 0.42278 0.38497 0.36793
Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators.
The column heading indicates the size of the corpus and the number of HMM states. In the Gibbs sampler (GS) results
the subscript ?e? indicates that the parameters ? and ? were explicitly sampled while the subscript ?c? indicates that
they were integrated out, and the subscript ?p? indicates pointwise sampling, while ?b? indicates sentence-blocked
sampling. Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but was
still slowly improving.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.62115 0.64651 0.44135 0.56215 0.28576 0.46669
VB 0.60484 0.63652 0.48427 0.36458 0.35946 0.36926
GSe,p 0.64190 0.63057 0.53571 0.46986 0.41620 0.37165
GSe,b 0.65953 0.65606 0.57918 0.48975 0.47228 0.37311
GSc,p 0.61391? 0.67414 0.65285 0.65012 0.58153 0.62254
GSc,b 0.60551? 0.65516 0.62167 0.58271 0.55006 0.58728
Figure 3: Average cross-validation accuracy of state sequences produced by HMMs estimated by the various estima-
tors. The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 4.47555 3.86326 6.16499 4.55681 7.72465 5.42815
VB 4.27911 3.44029 5.00509 3.19670 4.80778 3.14557
GSe,p 4.24919 3.53024 4.30457 3.23082 4.24368 3.17076
GSe,b 4.04123 3.46179 4.22590 3.20276 4.29474 3.10609
GSc,p 4.03886? 3.52185 4.21259 3.17586 4.30928 3.18273
GSc,b 4.11272? 3.61516 4.36595 3.23630 4.32096 3.17780
Figure 4: Average Variation of Information between the state sequences produced by HMMs estimated by the various
estimators and the gold tags (smaller is better). The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 558 346 648 351 142 125
VB 473 123 337 24 183 20
GSe,p 2863 382 3709 63 2500 177
GSe,b 3846 286 5169 154 4856 139
GSc,p ? 34325 44864 40088 45285 43208
GSc,b ? 6948 7502 7782 7342 7985
Figure 5: Average number of iterations until the negative logarithm of the posterior probability (or likelihood) changes
by less than 0.5% (smaller is better) per at least 2,000 iterations. No annealing was used.
349
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
?
lo
g
po
st
er
io
r
pr
o
ba
bi
lit
y
50000400003000020000100000
8.1e+06
8.05e+06
8e+06
7.95e+06
7.9e+06
7.85e+06
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
G
re
ed
y
1-
to
-
1
ac
cu
ra
cy
50000400003000020000100000
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42
0.4
Figure 6: Variation in (a) negative log likelihood and (b) 1-to-1 accuracy as a function of running time on a 3GHz
dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states. Each iteration took
approximately 96 sec. for the collapsed blocked sampler, 7.5 sec. for the collapsed pointwise sampler, 25 sec. for the
explicit blocked sampler and 4.4 sec. for the explicit pointwise sampler.
350
accuracy.
The studies presented by Goldwater and Griffiths
(2007) and Johnson (2007) differed in the number of
states that they used. Goldwater and Griffiths (2007)
evaluated against the reduced tag set of 17 tags de-
veloped by Smith and Eisner (2005), while Johnson
(2007) evaluated against the full Penn Treebank tag
set. We ran all our estimators in both conditions here
(thanks to Noah Smith for supplying us with his tag
set).
Also, the studies differed in the size of the corpora
used. The largest corpus that Goldwater and Grif-
fiths (2007) studied contained 96,000 words, while
Johnson (2007) used all of the 1,173,766 words
in the full Penn WSJ treebank. For that reason
we ran all our estimators on corpora containing
24,000 words and 120,000 words as well as the full
treebank.
We ran each estimator with the eight different
combinations of values for the hyperparameters ?
and ?? listed below, which include the optimal
values for the hyperparameters found by Johnson
(2007), and report results for the best combination
for each estimator below 1.
? ??
1 1
1 0.5
0.5 1
0.5 0.5
0.1 0.1
0.1 0.0001
0.0001 0.1
0.0001 0.0001
Further, we ran each setting of each estimator at
least 10 times (from randomly jittered initial start-
ing points) for at least 1,000 iterations, as Johnson
(2007) showed that some estimators require many it-
erations to converge. The results of our experiments
are summarized in Figures 2?5.
1We found that on some data sets the results are sensitive to
the values of the hyperparameters. So, there is a bit uncertainty
in our comparison results because it is possible that the values
we tried were good for one estimator and bad for others. Un-
fortunately, we do not know any efficient way of searching the
optimal hyperparameters in a much wider and more fine-grained
space. We leave it to future work.
4 Conclusion and future work
As might be expected, our evaluation measures dis-
agree somewhat, but the following broad tendancies
seem clear. On small data sets all of the Bayesian
estimators strongly outperform EM (and, to a lesser
extent, VB) with respect to all of our evaluation
measures, confirming the results reported in Gold-
water and Griffiths (2007). This is perhaps not too
surprising, as the Bayesian prior plays a compara-
tively stronger role with a smaller training corpus
(which makes the likelihood term smaller) and the
approximation used by Variational Bayes is likely to
be less accurate on smaller data sets.
But on larger data sets, which Goldwater et aldid
not study, the results are much less clear, and depend
on which evaluation measure is used. Expectation
Maximization does surprisingly well on larger data
sets and is competitive with the Bayesian estimators
at least in terms of cross-validation accuracy, con-
firming the results reported by Johnson (2007).
Variational Bayes converges faster than all of the
other estimators we examined here. We found that
the speed of convergence of our samplers depends
to a large degree upon the values of the hyperparam-
eters ? and ??, with larger values leading to much
faster convergence. This is not surprising, as the ?
and ?? specify how likely the samplers are to con-
sider novel tags, and therefore directly influence the
sampler?s mobility. However, in our experiments the
best results are obtained in most settings with small
values for ? and ??, usually between 0.1 and 0.0001.
In terms of time to convergence, on larger data
sets we found that the blocked samplers were gen-
erally faster than the pointwise samplers, and that
the explicit samplers (which represented and sam-
pled ? and ?) were faster than the collapsed sam-
plers, largely because the time saved in not com-
puting probabilities on the fly overwhelmed the time
spent resampling the parameters.
Of course these experiments only scratch the sur-
face of what is possible. Figure 6 shows that
pointwise-samplers initially converge faster, but are
overtaken later by the blocked samplers. Inspired
by this, one can devise hybrid strategies that inter-
leave blocked and pointwise sampling; these might
perform better than both the blocked and pointwise
samplers described here.
351
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience unit, University College
London.
Julian Besag. 2004. An introduction to Markov Chain
Monte Carlo methods. In Mark Johnson, Sanjeev P.
Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-
itors, Mathematical Foundations of Speech and Lan-
guage Processing, pages 247?270. Springer, New
York.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
59?66. Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
David J.C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labora-
tory, Cambridge.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, COLT 2003: The Sixteenth
Annual Conference on Learning Theory, volume 2777
of Lecture Notes in Computer Science, pages 173?187.
Springer.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
352
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 233?240, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Effective Use of Prosody in Parsing Conversational Speech
Jeremy G. Kahn? Matthew Lease?
Eugene Charniak? Mark Johnson? Mari Ostendorf?
University of Washington, SSLI? Brown University?
{jgk,mo}@ssli.ee.washington.edu {mlease,ec,mj}@cs.brown.edu
Abstract
We identify a set of prosodic cues for parsing con-
versational speech and show how such features can
be effectively incorporated into a statistical parsing
model. On the Switchboard corpus of conversa-
tional speech, the system achieves improved parse
accuracy over a state-of-the-art system which uses
only lexical and syntactic features. Since removal
of edit regions is known to improve downstream
parse accuracy, we explore alternatives for edit de-
tection and show that PCFGs are not competitive
with more specialized techniques.
1 Introduction
For more than a decade, the Penn Treebank?s Wall
Street Journal corpus has served as a benchmark for
developing and evaluating statistical parsing tech-
niques (Collins, 2000; Charniak and Johnson, 2005).
While this common benchmark has served as a valu-
able shared task for focusing community effort, it
has unfortunately led to the relative neglect of other
genres, particularly speech. Parsed speech stands to
benefit from practically every application envisioned
for parsed text, including machine translation, infor-
mation extraction, and language modeling. In con-
trast to text, however, speech (in particular, conver-
sational speech) presents a distinct set of opportu-
nities and challenges. While new obstacles arise
from the presence of speech repairs, the possibility
of word errors, and the absence of punctuation and
sentence boundaries, speech also presents a tremen-
dous opportunity to leverage multi-modal input, in
the form of acoustic or even visual cues.
As a step in this direction, this paper identifies a
set of useful prosodic features and describes how
they can be effectively incorporated into a statisti-
cal parsing model, ignoring for now the problem
of word errors. Evaluated on the Switchboard cor-
pus of conversational telephone speech (Graff and
Bird, 2000), our prosody-aware parser out-performs
a state-of-the-art system that uses lexical and syntac-
tic features only. While we are not the first to employ
prosodic cues in a statistical parsing model, previous
efforts (Gregory et al, 2004; Kahn et al, 2004) in-
corporated these features as word tokens and thereby
suffered from the side-effect of displacing words in
the n-gram models by the parser. To avoid this prob-
lem, we generate a set of candidate parses using an
off-the-shelf, k-best parser, and use prosodic (and
other) features to rescore the candidate parses.
Our system architecture combines earlier models
proposed for parse reranking (Collins, 2000) and
filtering out edit regions (Charniak and Johnson,
2001). Detecting and removing edits prior to parsing
is motivated by the claim that probabilistic context-
free grammars (PCFGs) perform poorly at detect-
ing edit regions. We validate this claim empirically:
two state-of-the-art PCFGs (Bikel, 2004; Charniak
and Johnson, 2005) are both shown to perform sig-
nificantly below a state-of-the-art edit detection sys-
tem (Johnson et al, 2004).
2 Previous Work
As mentioned earlier, conversational speech
presents a different set of challenges and opportu-
nities than encountered in parsing text. This paper
focuses on the challenges associated with disfluen-
cies (Sec. 2.1) and the opportunity of leveraging
acoustic-prosodic cues at the sub-sentence level
(Sec. 2.2). Here, sentence segmentation is assumed
to be known (though punctuation is not available);
233
. . . while I think,
? ?? ?
Reparandum
+ uh, I mean,
? ?? ?
Editing phrase
I know
? ?? ?
Repair
that. . .
Figure 1: The structure of a typical repair, with ?+? indicating the interruption point.
the impact of automatic segmentation is addressed
in other work (Kahn et al, 2004).
2.1 Speech Repairs and Parsing
Spontaneous speech abounds with disfluencies such
as partial words, filled pauses (e.g., ?uh?, ?um?),
conversational fillers (e.g., ?you know?), and par-
enthetical asides. One type of disfluency that has
proven particularly problematic for parsing is speech
repairs: when a speaker amends what he is saying
mid-sentence (see Figure 1). Following the analy-
sis of (Shriberg, 1994), a speech repair can be un-
derstood as consisting of three parts: the reparan-
dum (the material repaired), the editing phrase (that
is typically either empty or consists of a filler), and
the repair. The point between the reparandum and
the editing phrase is referred to as the interruption
point (IP), and it is the point that may be acousti-
cally marked. We refer to the reparandum and edit-
ing phrase together as an edit or edit region. Speech
repairs are difficult to model with HMM or PCFG
models, because these models can induce only linear
or tree-structured dependencies between words. The
relationship between reparandum and repair is quite
different: the repair is often a ?rough copy? of the
reparandum, using the same or very similar words
in roughly the same order. A language model char-
acterizing this dependency with hidden stack opera-
tions is proposed in (Heeman and Allen, 1999).
Several parsing models have been proposed which
accord special treatment to speech repairs. Most
prior work has focused on handling disfluencies
and continued to rely on hand-annotated transcripts
that include punctuation, case, and known sentence
boundaries (Hindle, 1983; Core and Schubert, 1999;
Charniak and Johnson, 2001; Engel et al, 2002).
Of particular mention is the analysis of the rela-
tionship between speech repairs and parsing accu-
racy presented by Charniak and Johnson (2001), as
this directly influenced our work. They presented
evidence that improved edit detection (i.e. detect-
ing the reparandum and edit phrase) leads to better
parsing accuracy, showing a relative reduction in F -
score error of 14% (2% absolute) between oracle and
automatic edit removal. Thus, this work adopts their
edit detection preprocessing approach. They have
subsequently presented an improved model for de-
tecting edits (Johnson et al, 2004), and our results
here complement their analysis of the edit detection
and parsing relationship, particularly with respect to
the limitations of PCFGs in edit detection.
2.2 Prosody and parsing
While spontaneous speech poses problems for pars-
ing due to the presence of disfluencies and lack of
punctuation, there is information in speech associ-
ated with prosodic cues that can be taken advantage
of in parsing. Certainly, prosodic cues are useful
for sentence segmentation (Liu et al, 2004), and
the quality of automatic segmentation can have a
significant impact on parser performance (Kahn et
al., 2004). There is also perceptual evidence that
prosody provides cues to human listeners that aid
in syntactic disambiguation (Price et al, 1991), and
the most important of these cues seems to be the
prosodic phrases (perceived groupings of words) or
the boundary events marking them. However, the
utility of sentence-internal prosody in parsing con-
versational speech is not well established.
Most early work on integrating prosody in parsing
was in the context of human-computer dialog sys-
tems, where parsers typically operated on isolated
utterances. The primary use of prosody was to rule
out candidate parses (Bear and Price, 1990; Batliner
et al, 1996). Since then, parsing has advanced con-
siderably, and the use of statistical parsers makes the
candidate pruning benefits of prosody less impor-
tant. This raises the question of whether prosody
is useful for improving parsing accuracy for con-
versational speech, apart from its use in sentence
234
Figure 2: System architecture
boundary detection. Extensions of Charniak and
Johnson (2001) look at using quantized combina-
tions of prosodic features as additional ?words?,
similar to the use of punctuation in parsing written
text (Gregory et al, 2004), but do not find that the
prosodic features are useful. It may be that with the
short ?sentences? in spontaneous speech, sentence-
internal prosody is rarely of use in parsing. How-
ever, in edit detection using a parsing model (John-
son et al, 2004), posterior probabilities of automati-
cally detected IPs based on prosodic cues (Liu et al,
2004) are found to be useful. The seeming discrep-
ancy between results could be explained if prosodic
cues to IPs are useful but not other sub-sentence
prosodic constituents. Alternatively, it could be that
including a representation of prosodic features as
terminals in (Gregory et al, 2004) displaces words
in the parser n-gram model history. Here, prosodic
event posteriors are used, with the goal of providing
a more effective way of incorporating prosody than
a word-like representation.
3 Approach
3.1 Overall architecture
Our architecture, shown in Figure 2, combines the
parse reranking framework of (Collins, 2000) with
the edit detection and parsing approach of (Charniak
and Johnson, 2001). The system operates as follows:
1. Edit words are identified and removed.
2. Each resulting string is parsed to produce a set
of k candidate parses.
3. Edit words reinserted into the candidates with
a new part-of-speech tag EW. Consecutive se-
quences of edit words are inserted as single, flat
EDITED constituents.
4. Features (syntactic and/or prosodic) are ex-
tracted for each candidate, i.e. candidates are
converted to feature vector representation.
5. The candidates are rescored by the reranker to
identify the best parse.
Use of Collins? parse reranking model has several
advantages for our work. In addition to allowing us
to incorporate prosody without blocking lexical de-
pendencies, the discriminative model makes it rela-
tively easy to experiment with a variety of prosodic
features, something which is considerably more dif-
ficult to do directly with a typical PCFG parser.
Our use of the Charniak-Johnson approach of sep-
arately detecting disfluencies is motivated by their
result that edit detection error degrades parser accu-
racy, but we also include experiments that omit this
step (forcing the PCFG to model the edits) and con-
firm the practical benefit of separating responsibili-
ties between the edit detection and parsing tasks.
3.2 Baseline system
We adopt an existing parser-reranker as our base-
line (Charniak and Johnson, 2005). The parser
component supports k-best parse generation, and
the reranker component is used to rescore candi-
date parses proposed by the parser. In detail, the
reranker selects from the set of k candidates T =
{t1, . . . tk} the parse t? ? T with the highest bracket
F -score (in comparison with a hand-annotated ref-
erence). To accomplish this, a feature-extractor con-
verts each candidate parse t ? T into a vector of
real-valued features f(t) = (f1(t), . . . , fm(t)) (e.g.,
the value fj(t) of the feature fj might be the num-
ber of times a certain syntactic structure appears in
t). The reranker training procedure associates each
feature fj with a real-valued weight ?j , and ??f(t)
(the dot product of the feature vector and the weight
vector ?) is a single scalar weight for each parse can-
didate. The reranker employs a maximum-entropy
estimator that selects the ? that minimizes the log
loss of the highest bracket F -score parse t? condi-
tioned on T (together with a Gaussian regularizer
to prevent overtraining). Informally, ? is chosen to
235
make high F -score parses as likely as possible un-
der the (conditional) distribution defined by f and ?.
As in (Collins, 2000), we generate training data for
the reranker by reparsing the training corpus, using
n ? 1 folds as training data to parse the n-th fold.
The existing system also includes a feature extrac-
tor that identifies interesting syntactic relationships
not included in the PCFG parsing model (but used
in the reranker). These features are primarily related
to non-local dependencies, including parallelism of
conjunctions, the number of terminals dominated by
coordinated structures, right-branching root-to-leaf
length, lexical/functional head pairs, n-gram style
sibling relationships, etc.
3.3 Prosodic Features
Most theories of prosody have a symbolic represen-
tation for prosodic phrasing, where different combi-
nations of acoustic cues (fundamental frequency, en-
ergy, timing) combine to give categorical perceptual
differences. Our approach to integrating prosody in
parsing is to use such symbolic boundary events, in-
cluding prosodic break labels that build on linguistic
notions of intonational phrases and hesitation phe-
nomena. These events are predicted from a com-
bination of continuous acoustic correlates, rather
than using the acoustic features directly, because
the intermediate representation simplifies training
with high-level (sparse) structures. Just as phone-
based acoustic models are useful in speech recogni-
tion systems as an intermediate level between words
and acoustic features (especially for characterizing
unseen words), the small set of prosodic boundary
events are used here to simplify modeling the inter-
dependent set of continuous-valued acoustic cues re-
lated to prosody. However, also as in speech recog-
nition, we use posterior probabilities of these events
as features rather than making hard decisions about
presence vs. absence of a constituent boundary.
In the past, the idea of using perceptual categories
has been dismissed as impractical due to the high
cost of hand annotation. However, with advances
in weakly supervised learning, it is possible to train
prosodic event classifiers with only a small amount
of hand-labeled data by leveraging information in
syntactic parses of unlabeled data. Our strategy is
similar to that proposed in (No?th et al, 2000), which
uses categorical labels defined in terms of syntactic
structure and pause duration. However, their sys-
tem?s category definitions are without reference to
human perception, while we leverage learned re-
lations between perceptual events and syntax with
other acoustic cues, without predetermining the re-
lation or requiring a direct coupling to syntax.
More specifically, we represent three classes of
prosodic boundaries (or, breaks): major intonational
phrase, hesitation, and all other word boundaries.1
A small set of hand-labeled data from the treebanked
portion of the Switchboard corpus (Ostendorf et al,
2001) was used to train initial break prediction mod-
els based on both parse and acoustic cues. Next, the
full set of treebanked Switchboard data is used with
an EM algorithm that iterates between: i) finding
probabilities of prosodic breaks in unlabeled data
based on the current model, again using parse and
acoustic features, and ii) re-estimating the model us-
ing the probabilities as weighted counts. Finally, a
new acoustic-only break prediction model was de-
signed from this larger data set for use in the parsing
experiments.
In each stage, we use decision trees for models, in
part because of an interest in analyzing the prosody-
syntax relationships learned. The baseline system
trained on hand-labeled data has error rates of 9.6%
when all available cues are used (both syntax and
prosody) and 16.7% when just acoustic and part-of-
speech cues are provided (our target environment).
Using weakly supervised (EM) training to incorpo-
rate unannotated data led to a 15% reduction in error
rate to 14.2% for the target trees. The final decision
tree was used to generate posteriors for each of the
three classes, one for each word in a sentence.
?From perceptual studies and decision tree analy-
ses, we know that major prosodic breaks tend to co-
occur with major clauses, and that hesitations often
occur in edit regions or at high perplexity points in
the word sequence. To represent the co-occurrence
as a feature for use in parse reranking, we treat
the prosodic break posteriors as weighted counts in
accumulating the number of constituents in parse
t of type i with break type j at their right edge,
which (with some normalization and binning) be-
comes feature fij . Note that the unweighted count
1The intonational phrase corresponds to a break of ?4? in the
ToBI labeling system (Pitrelli et al, 1994), and a hesitation is
marked with the ?p? diacritic.
236
for constituent i corresponds directly to a feature
in the baseline set, but the baseline set of features
also includes semantic information via association
with specific words. Here, we simply use syntactic
constituents. It is also known that major prosodic
breaks tend to be associated with longer syntactic
constituents, so we used the weighted count strategy
with length-related features as well. In all, the vari-
ous attributes associated with prosodic break counts
were the constituent label of the subtree, its length
(in words), its height (maximal distance from the
constituent root to any leaf), and the depth of the
rightmost word (distance from the right word to the
subtree root). For each type in each of these cate-
gories, there are three prosodic features, correspond-
ing to the three break types.
3.4 Edit detection
To provide a competitive baseline for our parsing
experiments, we used an off-the-shelf, state-of-the-
art TAG-based model as our primary edit detec-
tor (Johnson et al, 2004).2 This also provided us a
competitive benchmark for contrasting the accuracy
of PCFGs on the edit detection task (Section 4.2).
Whereas the crossing-dependencies inherent in
speech repairs makes them difficult to model us-
ing HMM or PCFG approaches (Section 2.1), Tree
Adjoining Grammars (TAGs) are capable of cap-
turing these dependencies. To model both the
crossed-dependencies of speech repairs and the lin-
ear or tree-structured dependencies of non-repaired
speech, Johnson et al?s system applies the noisy
channel paradigm: a PCFG language model defines
a probability distribution over non-repaired speech,
and a TAG is used to model the optional insertion of
edits. The output of this noisy channel model is a
set of candidate edits which are then reranked using
a max-ent model (similar to what is done here for
parse reranking). This reranking step enables incor-
poration of features based on an earlier word-based
classifier (Charniak and Johnson, 2001) in addition
to output features of the TAG model. Acoustic fea-
tures are not yet incorporated.
2We also evaluated another state-of-the-art edit detection
system (Liu et al, 2004) but found that it suffered from a mis-
match between the current LDC specification of edits (LDC,
2004) and that used in the treebank.
4 Experimental design
4.1 Corpus
Experiments were carried out on conversational
speech using the hand-annotated transcripts associ-
ated with the Switchboard treebank (Graff and Bird,
2000). As was done in (Kahn et al, 2004), we
resegmented the treebank?s sentences into V5-style
sentence-like units (SUs) (LDC, 2004), since our ul-
timate goal was to be able to parse speech given au-
tomatically detected boundaries. Unfortunately, the
original transcription effort did not provide punctu-
ation guidelines, and the Switchboard treebanking
was performed on the transcript unchanged, with-
out reference to the audio. As a result, the sentence
boundaries sometimes do not match human listener
decisions using SU annotation guidelines, with dif-
ferences mainly corresponding to treatment of dis-
course markers and backchannels. In the years since
the original Switchboard annotation was performed,
LDC has iteratively refined guidelines for annotating
SUs, and significant progress has been made in au-
tomatically recovering SU boundaries annotated ac-
cording to this standard (Liu et al, 2004). To even-
tually leverage this work, we have taken the Meteer-
annotated SUs (Meteer et al, 1995), for which there
exists treebanked training data, and automatically
adjusted them to be more like the V5 LDC stan-
dard, and resegmented the Switchboard treebank ac-
cordingly. In cases where the original syntactic con-
stituents span multiple SUs, we discard any con-
stituents violating the SU boundary, and in the event
that an SU spans a treebank sentence boundary, a
new top-level constituent SUGROUP is inserted to
produce a proper tree (and evaluated like any other
constituent in the gold tree).3 While this SU reseg-
mentation makes it difficult to compare our experi-
mental results to past work, we believe this is a nec-
essary step towards developing a more realistic base-
line for fully automated parsing of speech.
In addition to resegmention, we removed all punc-
tuation and case from the corpus to more closely
reflect the form of output available from a speech
recognizer. We retained partial words for consis-
3SU and treebank segments disagree at about 5% in each di-
rection, due mostly to the analysis of discourse markers as con-
junctions (sentences of >1 SU) and the separation of backchan-
nels into separate treebank sentences (SUs of >1 sentence).
237
Table 1: Statistics on the Switchboard division used.
Section Sides SUs Words
Train 1,031 87,599 659,437
Tune 126 13,147 103,500
Test 128 8,726 61,313
Total 1,285 109,472 824,250
tency with other work (Liu et al, 2004; Johnson et
al., 2004), although word fragments would not typ-
ically be available from ASR. Finally, of the 1300
total conversation sides, we discarded 15 for which
we did not have prosodic data. Our corpus division
statistics are given in Table 1. During development,
experiments were carried out on the tune section; the
test section was reserved for a final test run.
4.2 Experimental Variables
Our primary goal is to evaluate the extent to which
prosodic cues could augment and/or stand-in for lex-
ical and syntactic features. Correspondingly, we
report on using three flavors of feature extraction:
syntactic and lexical features (Section 3.2), prosodic
features (Section 3.3), and both sets of features com-
bined. For all three conditions, the first-stage score
for each parse (generated by the off-the-shelf k-best
parser) was always included as a feature.
A second parameter varied in the experiments was
the method of upstream edit detection employed
prior to parsing: PCFG, TAG-based, and oracle
knowledge of treebank edit annotations. While it
had been claimed that PCFGs perform poorly as edit
detectors (Charniak and Johnson, 2001), we could
not find empirical evidence in the literature quan-
tifying the severity of the problem. Therefore, we
evaluated two PCFGs (Bikel, 2004; Charniak and
Johnson, 2005) on edit detection and compared their
performance to a state-of-the-art TAG-based edit de-
tection system (Johnson et al, 2004). For this ex-
periment, we evaluated edit detection accuracy on a
per-word basis, where any tree terminal is consid-
ered an edit word if and only if it is dominated by
an EDITED constituent in the gold tree. The PCFGs
were trained on the train section of the treebank data
with the flattened edit regions included4 and then
4Training on flattened EDITED nodes improved detection ac-
curacy for both PCFGs: as much as 15% for Bikel-Collins.
Table 2: Edit word detection performance for two
word-based PCFGs and the TAG-based edit detec-
tor. F -score and error are word-based measures.
Edit Detector Edit F -score Edit Error
Bikel-Collins PCFG 65.3 62.1
Charniak PCFG 65.8 59.9
TAG-based 78.2 42.2
Table 3: Parsing F -score of various feature and edit-
detector combinations.
PCFG TAG Oracle
Edit F (Table 2) 65.8 78.2 100.0
Parser 1-best 84.4 85.0 86.9
Prosodic feats 85.0 85.6 87.6
Syntactic feats 85.9 86.4 88.4
Combined feats 86.0 86.6 88.6
Oracle-rate 92.6 93.2 95.2
used to parse the test data.5 The TAG-based de-
tector was trained on the same conversation sides,
with its channel model trained on the Penn Treebank
disfluency-annotated files and its language model
trained on trees with the EDITED nodes excised. As
shown in Table 2, we did find that both PCFGs per-
formed significantly below the TAG-based detector.
5 Results
In evaluating parse accuracy, we adopt the relaxed
edited revision (Charniak and Johnson, 2001) to the
standard PARSEVAL metric, which penalizes sys-
tems that get EDITED spans wrong, but does not pe-
nalize disagreements in the attachment or internal
structure of edit regions. This metric is based on the
assumption that there is little reason to recover syn-
tactic structure in regions of speech that have been
repaired or restarted by the speaker.
Table 3 shows the F -scores for the top-ranked
parses after reranking, where the first-stage PCFG
parser was run with a beam-size of 50. The first
and last rows show lower and upper bounds, respec-
tively, for reranked parsing accuracy on each edit
condition. As the oracle rate6 shows, there is con-
5For the Charniak parser, edits were detected using only its
PCFG component in 1-best mode, not its 2nd stage reranker.
6Oracle F uses the best parse in the 50-best list.
238
siderable room for improvement. Statistical signif-
icance was computed using a non-parametric shuf-
fle test similar to that in (Bikel, 2004). For TAG
and oracle edit detection conditions, the improve-
ment from using the combined features over either
prosodic or syntactic features in isolation was sig-
nificant (p < 0.005). (For PCFG edit detection,
p < 0.04.) Similarly, for all three feature extraction
conditions, the improvement from using the TAG-
based edit detector instead of the PCFG edit detector
was also significant (p < 0.001). Interestingly, the
TAG?s 34% reduction in edit detection error relative
to the PCFG yielded only about 23% of the parse
accuracy differential between the PCFG and oracle
conditions. Nevertheless, there remains a promising
2.0% difference in parse F -score between the TAG
and oracle detection conditions to be realized by fur-
ther improvements in edit detection. Training for
the syntactic feature condition resulted in a learned
weight ? with approximately 50K features, while
the prosodic features used only about 1300 features.
Despite this difference in the length of the ? vectors,
the prosodic feature condition achieved 40?50% of
the improvement of the syntactic features.
In some situations, e.g. for language modeling,
improving the ordering and weights of the entire
parse set (an not just the top ranked parse) is im-
portant. To illustrate the overall improvement of the
reranked order, in Table 4 we report the reranked-
oracle rate over the top s parses, varying the beam s.
The error for each feature condition, relative to using
the PCFG parser in isolation, is shown in Figure 3.
Both the table and figure show that the reranked
beam achieves a consistent trend in parse accuracy
improvement relative to the PCFG beam, similar to
what is demonstrated by the 1-best scores (Table 3).
Table 4: Reranked-oracle rate parse F -score for the
top s parses with reference edit detection.
s 1 2 3 5 10 25
PCFG 86.9 89.8 91.0 92.2 93.4 94.6
Pros. 87.6 90.3 91.5 92.7 93.9 94.8
Syn. 88.4 91.3 92.4 93.4 94.3 95.0
Comb. 88.6 91.5 92.5 93.5 94.4 95.0
Figure 3: Reduction in error (Error = 1?F ) for the
s-best reranked-oracle relative to the parser-only or-
acle, for different feature rerankings (reference edit
detection).
6 Conclusion
This study shows that incorporating prosodic infor-
mation into the parse selection process, along with
non-local syntactic information, leads to improved
parsing accuracy on accurate transcripts of conver-
sational speech. Gains are shown to be robust to dif-
ficulties introduced by automatic edit detection and,
in addition to improving the one-best performance,
the overall ordering of the parse candidates is im-
proved. While the gains from combining prosodic
and syntactic features are not additive, since the
prosodic features incorporates some constituent-
structure information, the combined gains are sig-
nificant. These results are consistent with related ex-
periments with a different type of prosodically cued
event, which showed that automatically detected IPs
based on prosodic cues (Liu et al, 2004) are useful
in the reranking stage of a TAG-based speech repair
detection system (Johnson et al, 2004).
The experiments described here used automat-
ically extracted prosodic features in combination
with human-produced transcripts. It is an open ques-
tion as to whether the conclusions will hold for er-
rorful ASR transcripts and automatically detected
SU boundaries. However, there is reason to believe
that relative gains from using prosody may be larger
than those observed here for reference transcripts
239
(though overall performance will degrade), based on
prior work combining prosody and lexical cues to
detect other language structures (Shriberg and Stol-
cke, 2004). While the prosody feature extraction de-
pends on timing of the hypothesized word sequence,
the acoustic cues are relatively robust to word errors
and the break model can be retrained on recognizer
output to automatically learn to discount the lexical
evidence. Furthermore, if parse reranking operates
on the top N ASR hypotheses, the reranking pro-
cedure can improve recognition outputs, as demon-
strated in (Kahn, 2005) for syntactic features alone.
Allowing for alternative SU hypotheses in reranking
may also lead to improved SU segmentation.
In addition to assessing the impact of prosody
in a fully automatic system, other avenues for fu-
ture work include improving feature extraction. One
could combine IP and prosodic break features (so
far explored separately), find new combinations of
prosody and syntactic structure, and/or incorporate
other prosodic events. Finally, it may also be use-
ful to integrate the prosodic events directly into the
PCFG, in addition to their use in reranking.
This work was supported by the NSF under grants DMS-
0074276, IIS-0085940, IIS-0112432, IIS-0326276, and LIS-
9721276. Conclusions are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
A. Batliner et al 1996. Prosody, empty categories and
parsing - a success story. Proc. ICSLP, pp. 1169-1172.
J. Bear and P. Price. 1990. Prosody, syntax and parsing.
Proc. ACL, pp. 17-22.
D. Bikel. 2004. On the Parameter Space of Lexicalized
Statistical Parsing Models. Ph.D. thesis, U. Penn.
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. NAACL, pp. 118-126.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking.
Proc. ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. Proc. ICML, pp. 175-182.
M. Core and L. Schubert. 1999. A syntactic framework
for speech repairs and other disruptions. Proc. ACL,
pp. 413-420.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. Proc. EMNLP, pp. 49-54.
D. Graff and S. Bird. 2000. Many uses, many annota-
tions for large speech corpora: Switchboard and TDT
as case studies. Proc. LREC, pp. 427-433.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. Proc. NAACL, pp. 81-88.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases, and discourse markers: Model-
ing speaker?s utterances in spoken dialogue. Compu-
tational Linguistics, 25(4):527-571.
D. Hindle. 1983. Deterministic parsing of syntactic non-
fluencies. Proc. ACL, pp. 123-128.
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. Proc. Rich Transcription Workshop.
J. G. Kahn, M. Ostendorf, and C. Chelba. 2004. Pars-
ing conversational speech using enhanced segmenta-
tion. Proc. HLT-NAACL 2004, pp. 125-128.
J. G. Kahn. 2005. Moving beyond the lexical layer in
parsing conversational speech. M.A. thesis, U. Wash.
LDC. 2004. Simple metadata annotation specification.
Tech. report, Linguistic Data Consortium. Available
at http://www.ldc.upenn.edu/Projects/MDE.
Y. Liu et al 2004. The ICSI-SRI-UW metadata extrac-
tion system. Proc. ICSLP, pp. 577-580.
M. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.
Dysfluency annotation stylebook for the switchboard
corpus. Tech. report, Linguistic Data Consortium.
E. No?th et al 2000. Verbmobil: The use of prosody in
the linguistic components of a speech understanding
system. IEEE Trans. SAP, 8(5):519-532.
M. Ostendorf et al 2001. A prosodically labeled
database of spontaneous speech. ISCA Workshop on
Prosody in Speech Recognition and Understanding,
pp. 119-121, 10.
J. Pitrelli, M. Beckman, and J. Hirschberg. 1994. Eval-
uation of prosodic transcription labeling reliability in
the ToBI framework. Proc. ICSLP, pp. 123-126.
P. J. Price et al 1991. The use of prosody in syntactic
disambiguation. JASA, 90(6):2956-2970, 12.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, U.C. Berkeley.
E. Shriberg and A. Stolcke. 2004. Prosody modeling
for automatic speech recognition and understanding.
Mathematical Foundations of Speech and Language
Processing. Springer-Verlag, pp. 105-114.
240
Squibs and Discussions
The DOP Estimation Method Is Biased and
Inconsistent
Mark Johnson
Brown University
A data-oriented parsing or DOP model for statistical parsing associates fragments of linguistic
representations with numerical weights, where these weights are estimated by normalizing the
empirical frequency of each fragment in a training corpus (see Bod [1998] and references cited
therein). This note observes that this estimation method is biased and inconsistent; that is, the
estimated distribution does not in general converge on the true distribution as the size of the
training corpus increases.
1. Introduction
The data-oriented parsing or DOP approach to statistical natural language analy-
sis has attracted considerable attention recently and has been used to produce sta-
tistical language models based on various kinds of linguistic representation, as
described in Bod (1998). These models are based on the intuition that statistical gen-
eralizations about natural languages should be stated in terms of ?chunks? or ?frag-
ments? of linguistic representations. Linguistic representations are produced by
combining these fragments, but unlike in stochastic models such as Probabilistic Con-
text-Free Grammars, a single linguistic representation may be generated by several
different combinations of fragments. These fragments may be large, permitting DOP
models to describe nonlocal dependencies. Usually the fragments used in a
DOP model are themselves obtained from a training corpus of linguistic represen-
tations. For example, in DOP1 or Tree-DOP the fragments are typically all the
connected multinode trees that appear as subgraphs of any tree in the training
corpus.
This note shows that the estimation procedure standardly used to set the parame-
ters or fragment weights of a DOP model (see, for example, Bod [1998]) is biased and
inconsistent. This means that as sample size increases, the corresponding sequence of
probability distributions estimated by this procedure does not converge to the true
distribution that generated the training data. Consistency is usually regarded as the
minimal requirement any estimation method must satisfy (Breiman 1973; Shao 1999),
and the inconsistency of the standard DOP estimation method suggests it may be
worth looking for other estimation methods. Note that while the bulk of DOP re-
search uses the estimation procedure studied here, recently there has been research
that has used other estimators for DOP models (Bonnema, Buying, and Scha 1999;
Bod 2000), and it would be interesting to investigate the statistical properties of these
estimators as well.
 Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912. E-mail:
Mark Johnson@Brown.edu.
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
Edit Detection and Parsing for Transcribed Speech
Eugene Charniak and Mark Johnson
Deparments of Computer Science and Cognitive and Linguistic Sciences
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
ec,mj@cs.brown.edu 
Abstract
We present a simple architecture for parsing
transcribed speech in which an edited-word de-
tector rst removes such words from the sen-
tence string, and then a standard statistical
parser trained on transcribed speech parses the
remaining words. The edit detector achieves a
misclassication rate on edited words of 2.2%.
(The NULL-model, which marks everything as
not edited, has an error rate of 5.9%.) To evalu-
ate our parsing results we introduce a new eval-
uation metric, the purpose of which is to make
evaluation of a parse tree relatively indierent
to the exact tree position of EDITED nodes. By
this metric the parser achieves 85.3% precision
and 86.5% recall.
1 Introduction
While signicant eort has been expended on
the parsing of written text, parsing speech
has received relatively little attention. The
comparative neglect of speech (or transcribed
speech) is understandable, since parsing tran-
scribed speech presents several problems absent
in regular text: \um"s and \ah"s (or more
formally, lled pauses), frequent use of par-
entheticals (e.g., \you know"), ungrammatical
constructions, and speech repairs (e.g., \Why
didn?t he, why didn?t she stay home?").
In this paper we present and evaluate a simple
two-pass architecture for handling the problems
of parsing transcribed speech. The rst pass
tries to identify which of the words in the string
are edited (\why didn?t he," in the above exam-
ple). These words are removed from the string
given to the second pass, an already existing sta-
tistical parser trained on a transcribed speech
? This research was supported in part by NSF grant LIS
SBR 9720368 and by NSF ITR grant 20100203.
corpus. (In particular, all of the research in this
paper was performed on the parsed \Switch-
board" corpus as provided by the Linguistic
Data Consortium.)
This architecture is based upon a fundamen-
tal assumption: that the semantic and prag-
matic content of an utterance is based solely
on the unedited words in the word sequence.
This assumption is not completely true. For
example, Core and Schubert [8] point to coun-
terexamples such as \have the engine take the
oranges to Elmira, um, I mean, take them to
Corning" where the antecedent of \them" is
found in the EDITED words. However, we be-
lieve that the assumption is so close to true that
the number of errors introduced by this assump-
tion is small compared to the total number of
errors made by the system.
In order to evaluate the parser?s output we
compare it with the gold-standard parse trees.
For this purpose a very simple third pass is
added to the architecture: the hypothesized
edited words are inserted into the parser output
(see Section 3 for details). To the degree that
our fundamental assumption holds, a \real" ap-
plication would ignore this last step.
This architecture has several things to recom-
mend it. First, it allows us to treat the editing
problem as a pre-process, keeping the parser un-
changed. Second, the major clues in detecting
edited words in transcribed speech seem to be
relatively shallow phenomena, such as repeated
word and part-of-speech sequences. The kind
of information that a parser would add, e.g.,
the node dominating the EDITED node, seems
much less critical.
Note that of the major problems associated
with transcribed speech, we choose to deal with
only one of them, speech repairs, in a special
fashion. Our reasoning here is based upon what
one might and might not expect from a second-
pass statistical parser. For example, ungram-
maticality in some sense is relative, so if the
training corpus contains the same kind of un-
grammatical examples as the testing corpus,
one would not expect ungrammaticality itself
to be a show stopper. Furthermore, the best
statistical parsers [3,5] do not use grammatical
rules, but rather dene probability distributions
over all possible rules.
Similarly, parentheticals and lled pauses ex-
ist in the newspaper text these parsers currently
handle, albeit at a much lower rate. Thus there
is no particular reason to expect these construc-
tions to have a major impact.1 This leaves
speech repairs as the one major phenomenon
not present in written text that might pose a
major problem for our parser. It is for that rea-
son that we have chosen to handle it separately.
The organization of this paper follows the ar-
chitecture just described. Section 2 describes
the rst pass. We present therein a boosting
model for learning to detect edited nodes (Sec-
tions 2.1 { 2.2) and an evaluation of the model
as a stand-alone edit detector (Section 2.3).
Section 3 describes the parser. Since the parser
is that already reported in [3], this section sim-
ply describes the parsing metrics used (Section
3.1), the details of the experimental setup (Sec-
tion 3.2), and the results (Section 3.3).
2 Identifying EDITED words
The Switchboard corpus annotates disfluencies
such as restarts and repairs using the terminol-
ogy of Shriberg [15]. The disfluencies include
repetitions and substitutions, italicized in (1a)
and (1b) respectively.
(1) a. I really, I really like pizza.
b. Why didn?t he, why didn?t she stay
home?
Restarts and repairs are indicated by disfluency
tags ?[?, ?+? and ?]? in the disfluency POS-tagged
Switchboard corpus, and by EDITED nodes in
the tree-tagged corpus. This section describes
a procedure for automatically identifying words
corrected by a restart or repair, i.e., words that
1Indeed, [17] suggests that filled pauses tend to indi-
cate clause boundaries, and thus may be a help in pars-
ing.
are dominated by an EDITED node in the tree-
tagged corpus.
This method treats the problem of identify-
ing EDITED nodes as a word-token classication
problem, where each word token is classied as
either edited or not. The classier applies to
words only; punctuation inherits the classica-
tion of the preceding word. A linear classier
trained by a greedy boosting algorithm [16] is
used to predict whether a word token is edited.
Our boosting classier is directly based on the
greedy boosting algorithm described by Collins
[7]. This paper contains important implemen-
tation details that are not repeated here. We
chose Collins? algorithm because it oers good
performance and scales to hundreds of thou-
sands of possible feature combinations.
2.1 Boosting estimates of linear
classifiers
This section describes the kinds of linear clas-
siers that the boosting algorithm infers. Ab-
stractly, we regard each word token as an event
characterized by a nite tuple of random vari-
ables
(Y;X1; : : : ;Xm):
Y is the the conditioned variable and ranges
over f?1;+1g, with Y = +1 indicating that
the word is not edited. X1; : : : ;Xm are the con-
ditioning variables; each Xj ranges over a nite
set Xj . For example, X1 is the orthographic
form of the word and X1 is the set of all words
observed in the training section of the corpus.
Our classiers use m = 18 conditioning vari-
ables. The following subsection describes the
conditioning variables in more detail; they in-
clude variables indicating the POS tag of the
preceding word, the tag of the following word,
whether or not the word token appears in a
\rough copy" as explained below, etc.
The goal of the classier is to predict the
value of Y given values for X1; : : : ;Xm. The
classier makes its predictions based on the oc-
curence of combinations of conditioning vari-
able/value pairs called features. A feature F
is a set of variable-value pairs hXj ; xji, with
xj 2 Xj. Our classier is dened in terms of
a nite number n of features F1; : : : ;Fn, where
n  106 in our classiers.2 Each feature Fi de-
2It turns out that many pairs of features are exten-
sionally equivalent, i.e., take the same values on each
nes an associated random boolean variable
Fi =
?
hX
j
,x
j
i2F
i
(Xj=xj);
where (X=x) takes the value 1 if X = x and 0
otherwise. That is, Fi = 1 i Xj = xj for all
hXj ; xji 2 Fi.
Our classier estimates a feature weight i for
each feature Fi, that is used to dene the pre-
diction variable Z:
Z =
n
?
i=1
iFi:
The prediction made by the classier is
sign(Z) = Z=jZj, i.e., ?1 or +1 depending on
the sign of Z.
Intuitively, our goal is to adjust the vector
of feature weights ~ = (1; : : : ; n) to minimize
the expected misclassification rate E[(sign(Z) 6=
Y )]. This function is dicult to minimize,
so our boosting classier minimizes the ex-
pected Boost loss E[exp(?Y Z)]. As Singer and
Schapire [16] point out, the misclassication
rate is bounded above by the Boost loss, so a
low value for the Boost loss implies a low mis-
classication rate.
Our classier estimates the Boost loss as
?Et[exp(?Y Z)], where ?Et[] is the expectation
on the empirical training corpus distribution.
The feature weights are adjusted iteratively;
one weight is changed per iteration. The fea-
ture whose weight is to be changed is selected
greedily to minimize the Boost loss using the
algorithm described in [7]. Training contin-
ues for 25,000 iterations. After each iteration
the misclassication rate on the development
corpus ?Ed[(sign(Z) 6= Y )] is estimated, where
?Ed[] is the expectation on empirical develop-
ment corpus distribution. While each iteration
lowers the Boost loss on the training corpus, a
graph of the misclassication rate on the de-
velopment corpus versus iteration number is a
noisy U-shaped curve, rising at later iterations
due to overlearning. The value of ~ returned
word token in our training data. We developed a method
for quickly identifying such extensionally equivalent fea-
ture pairs based on hashing XORed random bitmaps,
and deleted all but one of each set of extensionally equiv-
alent features (we kept a feature with the smallest num-
ber of conditioning variables).
by the estimator is the one that minimizes the
misclassciation rate on the development cor-
pus; typically the minimum is obtained after
about 12,000 iterations, and the feature weight
vector ~ contains around 8000 nonzero feature
weights (since some weights are adjusted more
than once).3
2.2 Conditioning variables and features
This subsection describes the conditioning vari-
ables used in the EDITED classier. Many of the
variables are dened in terms of what we call
a rough copy. Intuitively, a rough copy iden-
ties repeated sequences of words that might
be restarts or repairs. Punctuation is ignored
for the purposes of dening a rough copy, al-
though conditioning variables indicate whether
the rough copy includes punctuation. A rough
copy in a tagged string of words is a substring
of the form 1?2, where:
1. 1 (the source) and 2 (the copy) both be-
gin with non-punctuation,
2. the strings of non-punctuation POS tags of
1 and 2 are identical,
3.  (the free final) consists of zero or more
sequences of a free nal word (see below)
followed by optional punctuation, and
4. ? (the interregnum) consists of sequences of
an interregnum string (see below) followed
by optional punctuation.
The set of free-final words includes all partial
words (i.e., ending in a hyphen) and a small set
of conjunctions, adverbs and miscellanea, such
as and, or, actually, so, etc. The set of interreg-
num strings consists of a small set of expressions
such as uh, you know, I guess, I mean, etc. We
search for rough copies in each sentence start-
ing from left to right, searching for longer copies
rst. After we nd a rough copy, we restart
searching for additional rough copies following
the free nal string of the previous copy. We
say that a word token is in a rough copy i it
appears in either the source or the free nal.4
(2) is an example of a rough copy.
3We used a smoothing parameter  as described in
[7], which we estimate by using a line-minimization rou-
tine to minimize the classifier?s minimum misclassifica-
tion rate on the development corpus.
4In fact, our definition of rough copy is more complex.
For example, if a word token appears in an interregnum
(2) I thought I
????
1
cou-,
? ?? ?

I mean,
? ?? ?
?
I
????
2
would n-
ish the work
Table 1 lists the conditioning variables used
in our classier. In that table, subscript inte-
gers refer to the relative position of word to-
kens relative to the current word; e.g. T1 is
the POS tag of the following word. The sub-
script f refers to the tag of the rst word of the
free nal match. If a variable is not dened for
a particular word it is given the special value
?NULL?; e.g., if a word is not in a rough copy
then variables such as Nm, Nu, Ni, Nl, Nr and
Tf all take the value NULL. Flags are boolean-
valued variables, while numeric-valued variables
are bounded to a value between 0 and 4 (as well
as NULL, if appropriate). The three variables
Ct, Cw and Ti are intended to help the classier
capture very short restarts or repairs that may
not involve a rough copy. The flags Ct and Ci
indicate whether the orthographic form and/or
tag of the next word (ignoring punctuation) are
the same as those of the current word. Ti has
a non-NULL value only if the current word is
followed by an interregnum string; in that case
Ti is the POS tag of the word following that
interregnum.
As described above, the classier?s features
are sets of variable-value pairs. Given a tuple of
variables, we generate a feature for each tuple
of values that the variable tuple assumes in the
training data. In order to keep the feature set
managable, the tuples of variables we consider
are restricted in various ways. The most impor-
tant of these are constraints of the form ?if Xj
is included among feature?s variables, then so
is Xk?. For example, we require that if a fea-
ture contains Pi+1 then it also contains Pi for
i  0, and we impose a similiar constraint on
POS tags.
2.3 Empirical evaluation
For the purposes of this research the Switch-
board corpus, as distributed by the Linguistic
Data Consortium, was divided into four sections
and the word immediately following the interregnum also
appears in a (different) rough copy, then we say that the
interregnum word token appears in a rough copy. This
permits us to approximate the Switchboard annotation
convention of annotating interregna as EDITED if they
appear in iterated edits.
(or subcorpora). The training subcorpus con-
sists of all les in the directories 2 and 3 of the
parsed/merged Switchboard corpus. Directory
4 is split into three approximately equal-size sec-
tions. (Note that the les are not consecutively
numbered.) The rst of these (les sw4004.mrg
to sw4153.mrg) is the testing corpus. All edit
detection and parsing results reported herein
are from this subcorpus. The les sw4154.mrg
to sw4483.mrg are reserved for future use. The
les sw4519.mrg to sw4936.mrg are the devel-
opment corpus. In the complete corpus three
parse trees were suciently ill formed in that
our tree-reader failed to read them. These trees
received trivial modications to allow them to
be read, e.g., adding the missing extra set of
parentheses around the complete tree.
We trained our classier on the parsed data
les in the training and development sections,
and evaluated the classifer on the test section.
Section 3 evaluates the parser?s output in con-
junction with this classier; this section focuses
on the classier?s performance at the individual
word token level. In our complete application,
the classier uses a bitag tagger to assign each
word a POS tag. Like all such taggers, our tag-
ger has a nonnegligible error rate, and these tag-
ging could conceivably aect the performance of
the classier. To determine if this is the case,
we report classier performance when trained
both on \Gold Tags" (the tags assigned by the
human annotators of the Switchboard corpus)
and on \Machine Tags" (the tags assigned by
our bitag tagger). We compare these results to
a baseline \null" classier, which never identi-
es a word as EDITED. Our basic measure of
performance is the word misclassication rate
(see Section 2.1). However, we also report pre-
cision and recall scores for EDITED words alone.
All words are assigned one of the two possible
labels, EDITED or not. However, in our evalua-
tion we report the accuracy of only words other
than punctuation and lled pauses. Our logic
here is much the same as that in the statistical
parsing community which ignores the location
of punctuation for purposes of evaluation [3,5,
6] on the grounds that its placement is entirely
conventional. The same can be said for lled
pauses in the switchboard corpus.
Our results are given in Table 2. They show
that our classier makes only approximately 1/3
W0 Orthographic word
P0; P1; P2; Pf Partial word flags
T
?1; T0; T1; T2; Tf POS tags
Nm Number of words in common in source and copy
Nu Number of words in source that do not appear in copy
Ni Number of words in interregnum
Nl Number of words to left edge of source
Nr Number of words to right edge of source
Ct Followed by identical tag flag
Cw Followed by identical word flag
Ti Post-interregnum tag flag
Table 1: Conditioning variables used in the EDITED classier.
of the misclassication errors made by the null
classier (0.022 vs. 0.059), and that using the
POS tags produced by the bitag tagger does
not have much eect on the classier?s perfor-
mance (e.g., EDITED recall decreases from 0.678
to 0.668).
3 Parsing transcribed speech
We now turn to the second pass of our two-pass
architecture, using an \o-the-shelf" statistical
parser to parse the transcribed speech after hav-
ing removed the words identied as edited by
the rst pass. We rst dene the evaluation
metric we use and then describe the results of
our experiments.
3.1 Parsing metrics
In this section we describe the metric we use
to grade the parser output. As a rst desider-
atum we want a metric that is a logical exten-
sion of that used to grade previous statistical
parsing work. We have taken as our starting
point what we call the \relaxed labeled preci-
sion/recall" metric from previous research (e.g.
[3,5]). This metric is characterized as follows.
For a particular test corpus let N be the total
number of nonterminal (and non-preterminal)
constituents in the gold standard parses. Let
M be the number of such constituents returned
by the parser, and let C be the number of these
that are correct (as dened below). Then pre-
cision = C=M and recall = C=N .
A constituent c is correct if there exists a con-
stituent d in the gold standard such that:
1. label(c) = label(d)5
5For some reason, starting with [12] the labels ADVP
2. begin(c) r begin(d)
3. end(c) r end(d)
In 2 and 3 above we introduce an equivalence
relation r between string positions. We dene
r to be the smallest equivalence relation sat-
isfying a r b for all pairs of string positions a
and b separated solely by punctuation symbols.
The parsing literature uses r rather than =
because it is felt that two constituents should
be considered equal if they disagree only in the
placement of, say, a comma (or any other se-
quence of punctuation), where one constituent
includes the punctuation and the other excludes
it.
Our new metric, \relaxed edited labeled preci-
sion/recall" is identical to relaxed labeled preci-
sion/recall except for two modications. First,
in the gold standard all non-terminal subcon-
stituents of an EDITED node are removed and
the terminal constituents are made immediate
children of a single EDITED node. Furthermore,
two or more EDITED nodes with no separating
non-edited material between them are merged
into a single EDITED node. We call this version
a \simplied gold standard parse." All precision
recall measurements are taken with respected to
the simplied gold standard.
Second, we replace r with a new equiva-
lence relation e which we dene as the smallest
equivalence relation containing r and satisfy-
ing begin(c) e end(c) for each EDITED node c
in the gold standard parse.6
and PRT are considered to be identical as well.
6We considered but ultimately rejected defining ?
e
using the EDITED nodes in the returned parse rather
Classifer
Null Gold Tags Machine Tags
Misclassication rate 0.059 0.021 0.022
EDITED precision { 0.952 0.944
EDITED recall 0 0.678 0.668
Table 2: Performance of the \null" classier (which never marks a word as EDITED) and boosting
classiers trained on \Gold Tags" and \Machine Tags".
1 2 3 4 5 6 7 8
E E E E
the , bagel with uh , doughnut
1 2 2 4 5 2 2 8
Figure 1: Equivalent string positions as dened by e.
We give a concrete example in Figure 1. The
rst row indicates string position (as usual in
parsing work, position indicators are between
words). The second row gives the words of the
sentence. Words that are edited out have an
\E" above them. The third row indicates the
equivalence relation by labeling each string posi-
tion with the smallest such position with which
it is equivalent.
There are two basic ideas behind this deni-
tion. First, we do not care where the EDITED
nodes appear in the tree structure produced by
the parser. Second, we are not interested in the
ne structure of EDITED sections of the string,
just the fact that they are EDITED. That we
do care which words are EDITED comes into
our gure of merit in two ways. First, (non-
contiguous) EDITED nodes remain, even though
their substructure does not, and thus they are
counted in the precision and recall numbers.
Secondly (and probably more importantly), fail-
ure to decide on the correct positions of edited
nodes can cause collateral damage to neighbor-
ing constituents by causing them to start or stop
in the wrong place. This is particularly rele-
vant because according to our denition, while
the positions at the beginning and ending of an
edit node are equivalent, the interior positions
are not (unless related by the punctuation rule).
than the simplified gold standard. We rejected this be-
cause the ?
e
relation would then itself be dependent
on the parser?s output, a state of affairs that might al-
low complicated schemes to improve the parser?s perfor-
mance as measured by the metric.
See Figure 1.
3.2 Parsing experiments
The parser described in [3] was trained on the
Switchboard training corpus as specied in sec-
tion 2.1. The input to the training algorithm
was the gold standard parses minus all EDITED
nodes and their children.
We tested on the Switchboard testing sub-
corpus (again as specied in Section 2.1). All
parsing results reported herein are from all sen-
tences of length less than or equal to 100 words
and punctuation. When parsing the test corpus
we carried out the following operations:
1. create the simplied gold standard parse
by removing non-terminal children of an
EDITED node and merging consecutive
EDITED nodes.
2. remove from the sentence to be fed to the
parser all words marked as edited by an
edit detector (see below).
3. parse the resulting sentence.
4. add to the resulting parse EDITED nodes
containing the non-terminal symbols re-
moved in step 2. The nodes are added as
high as possible (though the denition of
equivalence from Section 3.1 should make
the placement of this node largely irrele-
vant).
5. evaluate the parse from step 4 against the
simplied gold standard parse from step 1.
We ran the parser in three experimental sit-
uations, each using a dierent edit detector in
step 2. In the rst of the experiments (labeled
\Gold Edits") the \edit detector" was simply
the simplied gold standard itself. This was to
see how well the parser would do it if had perfect
information about the edit locations.
In the second experiment (labeled \Gold
Tags"), the edit detector was the one described
in Section 2 trained and tested on the part-of-
speech tags as specied in the gold standard
trees. Note that the parser was not given the
gold standard part-of-speech tags. We were in-
terested in contrasting the results of this experi-
ment with that of the third experiment to gauge
what improvement one could expect from using
a more sophisticated tagger as input to the edit
detector.
In the third experiment (\Machine Tags") we
used the edit detector based upon the machine
generated tags.
The results of the experiments are given in
Table 3. The last line in the gure indicates
the performance of this parser when trained and
tested on Wall Street Journal text [3]. It is
the \Machine Tags" results that we consider the
\true" capability of the detector/parser combi-
nation: 85.3% precision and 86.5% recall.
3.3 Discussion
The general trends of Table 3 are much as one
might expect. Parsing the Switchboard data is
much easier given the correct positions of the
EDITED nodes than without this information.
The dierence between the Gold-tags and the
Machine-tags parses is small, as would be ex-
pected from the relatively small dierence in
the performance of the edit detector reported in
Section 2. This suggests that putting signicant
eort into a tagger for use by the edit detec-
tor is unlikely to produce much improvement.
Also, as one might expect, parsing conversa-
tional speech is harder than Wall Street Jour-
nal text, even given the gold-standard EDITED
nodes.
Probably the only aspect of the above num-
bers likely to raise any comment in the pars-
ing community is the degree to which pre-
cision numbers are lower than recall. With
the exception of the single pair reported in [3]
and repeated above, no precision values in the
recent statistical-parsing literature [2,3,4,5,14]
have ever been lower than recall values. Even
this one exception is by only 0.1% and not sta-
tistically signicant.
We attribute the dominance of recall over pre-
cision primarily to the influence of edit-detector
mistakes. First, note that when given the
gold standard edits the dierence is quite small
(0.3%). When using the edit detector edits the
dierence increases to 1.2%. Our best guess is
that because the edit detector has high preci-
sion, and lower recall, many more words are left
in the sentence to be parsed. Thus one nds
more nonterminal constituents in the machine
parses than in the gold parses and the precision
is lower than the recall.
4 Previous research
While there is a signicant body of work on nd-
ing edit positions [1,9,10,13,17,18], it is dicult
to make meaningful comparisons between the
various research eorts as they dier in (a) the
corpora used for training and testing, (b) the
information available to the edit detector, and
(c) the evaluation metrics used. For example,
[13] uses a subsection of the ATIS corpus, takes
as input the actual speech signal (and thus has
access to silence duration but not to words), and
uses as its evaluation metric the percentage of
time the program identies the start of the in-
terregnum (see Section 2.2). On the other hand,
[9,10] use an internally developed corpus of sen-
tences, work from a transcript enhanced with
information from the speech signal (and thus
use words), but do use a metric that seems to be
similar to ours. Undoubtedly the work closest
to ours is that of Stolcke et al [18], which also
uses the transcribed Switchboard corpus. (How-
ever, they use information on pause length, etc.,
that goes beyond the transcript.) They cate-
gorize the transitions between words into more
categories than we do. At rst glance there
might be a mapping between their six categories
and our two, with three of theirs corresponding
to EDITED words and three to not edited. If
one accepts this mapping they achieve an er-
ror rate of 2.6%, down from their NULL rate of
4.5%, as contrasted with our error rate of 2.2%
down from our NULL rate of 5.9%. The dier-
ence in NULL rates, however, raises some doubts
that the numbers are truly measuring the same
thing.
Experiment Labeled Precision Labeled Recall F-measure
Gold Edits 87.8 88.1 88.0
Gold Tags 85.4 86.6 86.0
Machine Tags 85.3 86.5 85.9
WSJ 89.5 89.6
Table 3: Results of Switchboard parsing, sentence length  100.
There is also a small body of work on parsing
disfluent sentences [8,11]. Hindle?s early work
[11] does not give a formal evaluation of the
parser?s accuracy. The recent work of Schubert
and Core [8] does give such an evaluation, but
on a dierent corpus (from Rochester Trains
project). Also, their parser is not statistical
and returns parses on only 62% of the strings,
and 32% of the strings that constitute sentences.
Our statistical parser naturally parses all of our
corpus. Thus it does not seem possible to make
a meaningful comparison between the two sys-
tems.
5 Conclusion
We have presented a simple architecture for
parsing transcribed speech in which an edited
word detector is rst used to remove such words
from the sentence string, and then a statistical
parser trained on edited speech (with the edited
nodes removed) is used to parse the text. The
edit detector reduces the misclassication rate
on edited words from the null-model (marking
everything as not edited) rate of 5.9% to 2.2%.
To evaluate our parsing results we have intro-
duced a new evaluation metric, relaxed edited
labeled precision/recall. The purpose of this
metric is to make evaluation of a parse tree
relatively indierent to the exact tree posi-
tion of EDITED nodes, in much the same way
that the previous metric, relaxed labeled pre-
cision/recall, make it indierent to the attach-
ment of punctuation. By this metric the parser
achieved 85.3% precision and 86.5% recall.
There is, of course, great room for improve-
ment, both in stand-alone edit detectors, and
their combination with parsers. Also of interest
are models that compute the joint probabilities
of the edit detection and parsing decisions |
that is, do both in a single integrated statistical
process.
References
1. Bear, J., Dowding, J. and Shriberg, E.
Integrating multiple knowledge sources for
detection and correction of repairs in human-
computer dialog. In Proceedings of the 30th
Annual Meeting of the Association for Com-
putational Linguistics. 56{63.
2. Charniak, E. Statistical parsing with a
context-free grammar and word statistics.
In Proceedings of the Fourteenth National
Conference on Articial Intelligence. AAAI
Press/MIT Press, Menlo Park, CA, 1997,
598{603.
3. Charniak, E. A maximum-entropy-
inspired parser. In Proceedings of the 2000
Conference of the North American Chap-
ter of the Association for Computational
Linguistics. ACL, New Brunswick NJ, 2000.
4. Collins, M. J. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the 34th Annual Meeting of the
ACL. 1996.
5. Collins, M. J. Three generative lexical-
ized models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL. 1997, 16{23.
6. Collins, M. J. Head-Driven Statistical
Models for Natural Language Parsing. Uni-
versity of Pennsylvania, Ph.D. Dissertation,
1999.
7. Collins, M. J. Discriminative reranking
for natural language parsing. In Proceedings
of the International Conference on Machine
Learning (ICML 2000). 2000.
8. Core, M. G. and Schubert, L. K. A syn-
tactic framework for speech repairs and other
disruptions. In Proceedings of the 37th An-
nual Meeting of the Association for Compu-
tational Linguistics. 1999, 413{420.
9. Heeman, P. A. and Allen, J. F. Into-
national boundaries, speech repairs and dis-
course markers: modeling spoken dialog. In
35th Annual Meeting of the Association for
Computational Linguistics and 17th Interna-
tional Conference on Computational Linguis-
tics. 1997, 254{261.
10. Heeman, P. A. and Allen, J. F. Speech
repairs, intonational phrases and discourse
markers: modeling speakers? utterances in
spoken dialogue. Computational Linguistics
254 (1999).
11. Hindle, D. Deterministic parsing of syn-
tactic non-fluencies. In Proceedings of the
21st Annual Meeting of the Association for
Computational Linguistics. 1983, 123{128.
12. Magerman, D. M. Statistical decision-tree
models for parsing. In Proceedings of the 33rd
Annual Meeting of the Association for Com-
putational Linguistics. 1995, 276{283.
13. Nakatani, C. H. and Hirschberg, J. A
corpus-based study of repair cues in sponta-
neous speech. Journal of the Acoustical Soci-
ety of America 953 (1994), 1603{1616.
14. Ratnaparkhi, A. Learning to parse natu-
ral language with maximum entropy models.
Machine Learning 34 1/2/3 (1999), 151{176.
15. Shriberg, E. E. Preliminaries to a The-
ory of Speech Disfluencies. In PhD Disserta-
tion. Department of Psychology, University
of California-Berkeley, 1994.
16. Singer, Y. and Schapire, R. E. Im-
proved boosting algorithms using condence-
based predictions. In Proceedings of the
Eleventh Annual Conference on Computa-
tional Learning Theory. 1998, 80{91.
17. Stolcke, A. and Shriberg, E. Auto-
matic linguistic segmantation of conversa-
tional speech. In Proceedings of the 4th In-
ternational Conference on Spoken Language
Processing (ICSLP-96). 1996.
18. Stolcke, A., Shriberg, E., Bates, R.,
Ostendorf, M., Hakkani, D., Plauche,
M., Tu?r, G. and Lu, Y. Automatic detec-
tion of sentence boundaries and disfluencies
based on recognized words. Proceedings of
the International Conference on Spoken Lan-
guage Processing 5 (1998), 2247{2250.
Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does
Michelle L Gregory
Brown University
mgregory@cog.brown.edu
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Eugene Charniak
Brown University
ec@cs.brown.edu
Abstract
This paper investigates the usefulness of
sentence-internal prosodic cues in syntac-
tic parsing of transcribed speech. Intu-
itively, prosodic cues would seem to pro-
vide much the same information in speech
as punctuation does in text, so we tried to
incorporate them into our parser in much
the same way as punctuation is. We com-
pared the accuracy of a statistical parser
on the LDC Switchboard treebank corpus
of transcribed sentence-segmented speech
using various combinations of punctua-
tion and sentence-internal prosodic infor-
mation (duration, pausing, and f0 cues).
With no prosodic or punctuation informa-
tion the parser?s accuracy (as measured by
F-score) is 86.9%, and adding punctuation
increases its F-score to 88.2%. However,
all of the ways we have tried of adding
prosodic information decrease the parser?s
F-score to between 84.8% to 86.8%, de-
pending on exactly which prosodic infor-
mation is added. This suggests that for
sentence-internal prosodic information to
improve speech transcript parsing, either
different prosodic cues will have to used
or they will have be exploited in the parser
in a way different to that used currently.
1 Introduction
Acoustic cues, generally duration, pausing, and
f0, have been demonstrated to be useful for auto-
S
INTJ
UH
Oh
,
,
NP
PRP
I
VP
VBD
loved
NP
PRP
it
.
.
Figure 1: A treebank style tree in which punctuation
is coded with terminal and preterminal nodes.
matic segmentation of natural speech (Baron et al,
2002; Hirschberg and Nakatani, 1998; Neiman et
al., 1998). In fact, it is generally accepted that
prosodic information is a reliable tool in predict-
ing topic shifts and sentence boundaries (Shriberg
et al, 2000). Sentences are generally demarcated
by a major fall (or rise) in f0, lengthening of the
final syllable, and following pauses. However,
the usefulness of prosodic information in sentence-
internal parsing is less clear. While assumed not
to be a one-to-one mapping, there is evidence
that there is a strong correlation between prosodic
boundaries and sentence-internal syntactic bound-
aries (Altenberg, 1987; Croft, 1995). For exam-
ple, Schepman and Rodway (2000) have shown that
prosodic cues reliably predict ambiguous attach-
ment of relative clauses within coordination con-
structions. Jansen et al (2001) have demonstrated
that prosodic breaks and an increase in pitch range
can distinguish direct quotes from indirect quotes in
a corpus of natural speech.
This paper evaluates the accuracy of a statistical
parser whose input includes prosodic cues. The pur-
pose of this study to determine if prosodic cues im-
prove parsing accuracy in the same way that punc-
tuation does. Punctuation is represented in the vari-
ous Penn treebank corpora as independent word-like
tokens, with corresponding terminal and pretermi-
nal nodes, as shown in Figure 1 (Bies et al, 1995).
Even though this seems linguistically highly un-
natural (e.g., punctuation might indicate supraseg-
mental prosodic properties), statistical parsers gen-
erally perform significantly better when their train-
ing and test data contains punctuation represented
in this way than if the punctuation is stripped out
of the training and test data (Charniak, 2000; En-
gel et al, 2002; Johnson, 1998). On the Switch-
board treebank data set using the experimental setup
described below we obtained an F-score of 0.882
when using punctuation and 0.869 when punctua-
tion was stripped out, replicating previous experi-
ments demonstrating the importance of punctuation.
(F-score is a standard measure of parse accuracy, see
e.g., Manning and Schu?tze (1999) for details).
This paper investigates how prosodic cues, when
encoded in the parser?s input in a manner similar to
the way the Penn treebanks encode punctuation, af-
fect parser accuracy. Our starting point is the ob-
servation that the Penn treebank annotation of punc-
tuation does significantly improve parsing accuracy.
Coupled with the assumption that punctuation and
prosody are encoding similar information, this led
us to try to encode prosodic information in a man-
ner that was as similar as possible to the way that
punctuation is encoded in the Penn treebanks.
For example, commas in text and pauses in speech
seem to convey similar information. In fact, when
transcribing speech, commas are often used to de-
note a pause. Thus, given the correlation between
the two, and the fact that sentence-internal punctu-
ation tends to be commas, we expected that pause
duration, coded in a way similar to punctuation,
would improve parsing accuracy in the same way
that punctuation does.
While it may be the case that the encoding of
prosodic information used in the experiments be-
low is perhaps not optimal and the parser has not
been tuned to use this information, note that exactly
the same objections could be made to the way that
punctuation is encoded and used in modern statis-
tical parsers, and punctuation does in fact dramati-
cally improve parsing accuracy.
We focus in this paper on parsing accuracy in a
modern statistical parsing framework, but it is im-
portant to remember that prosodic cues might help
parsing in other ways as well, even if they do not im-
prove parsing accuracy. No?th et al (2000) point out
that prosodic cues reduce parsing time and increase
recognition accuracy when parsing speech lattices
with the hand-crafted Verbmobil grammar. Page 266
of Kompe (1997) discusses the effect that incorpo-
rating prosodic information has on parse quality in
the Verbmobil system using the TUG unification
grammar parser: out of the 54 parses affected by
the addition of prosodic information, 33 were judged
?better with prosody?, 14 were judged ?better with-
out prosody? and 7 were judged ?unclear?. Our
experiments below differ from the experiments of
No?th and Kompe in many ways. First, we used
speech transcripts rather than speech recognizer lat-
tices. Second, we used a general-purpose broad-
coverage statistical parser rather than a unification
grammar parser with a hand-constructed grammar.
2 Method
The data used for this study is the transcribed ver-
sion of the Switchboard Corpus as released by
the Linguistic Data Consortium. The Switchboard
Corpus is a corpus of telephone conversations be-
tween adult speakers of varying dialects. The cor-
pus was split into training and test data as de-
scribed in Charniak and Johnson (2001). The train-
ing data consisted of all files in sections 2 and 3 of
the Switchboard treebank. The testing corpus con-
sists of files sw4004.mrg to sw4153.mrg, while files
sw4519.mrg to sw4936.mrg were used as develop-
ment corpus.
2.1 Prosodic variables
Prosodic information for the corpus was ob-
tained from forced alignments provided by
Hamaker et al (2003) and Ferrer et al (2002).
Hamaker et al (2003) provided word alignments
between the LDC parsed corpus and new alignments
of the Switchboard Coprus. Most of the differences
between the two alignments were individual lexical
items. In cases of differences, we kept the lexical
item from the LDC version. Ferrer et al (2002)
provided very rich prosodic information including
duration, pausing, f0 information, and individual
speaker statistics for each word in the corpus. The
information obtained from this corpus was aligned
to the LDC corpus.
It is not known exactly which prosodic vari-
ables convey the information about syntactic bound-
aries that is most useful to a modern syntactic
parser, so we investigated many different com-
binations of these variables. We looked for
changes in pitch and duration that we expected
would correspond to syntactic boundaries. While
we tested many combinations of variables, they
were mainly based on the variables PAU DUR N,
NORM LAST RHYME DUR, FOK WRD DIFF MNMN N,
FOK LR MEAN KBASELN and SLOPE MEAN DIFF N in
the data provided by Ferrer et al (2002).
While Ferrer (2002) should be consulted for full
details, PAU DUR N is pause duration normalized by
the speaker?s mean sentence-internal pause dura-
tion, NORM LAST RHYME DUR is the duration of the
phone minus the mean phone duration normalized
by the standard deviation of the phone duration for
each phone in the rhyme, FOK WRD DIFF MNMN NG
is the log of the mean f0 of the current word,
divided by the log mean f0 of the following
word, normalized by the speakers mean range,
FOK LR MEAN KBASELN is the log of the mean f0
of the word normalized by speaker?s baseline, and
SLOPE MEAN DIFF N is the difference in the f0 slope
normalized by the speaker?s mean f0 slope.
These variables all range over continuous values.
Modern statistical parsing technology has been de-
veloped assuming that all of the input variables are
categorical, and currently our parser can only use
categorical inputs. Given the complexity of the dy-
namic programming algorithms used by the parser,
it would be a major research undertaking to develop
a statistical parser of the same quality as the one
used here that is capable of using both categorical
and continuous variables as input.
In the experiments below we binned the contin-
uous prosodic variables to produce the actual cate-
gorical values used in our experiments. Binning in-
volves a trade-off, as fewer bins involve a loss of
information, whereas a large number of bins splits
the data so finely that the statistical models used in
the parser fail to generalize. We binned by first con-
structing a histogram of each feature?s values, and
divided these values into bins in such a way that each
bin contained the same number of samples. In runs
in which a single feature is the sole prosodic feature
we divided that feature?s values into 10 bins, while
runs in which two or more prosodic features were
conjoined we divided each feature into 5 bins.
While not reported here, we experimented with a
wide variety of different binning strategies, includ-
ing using the bins proposed by Ferrer et al (2002).
In fact the number of bins used does not affect the
results markedly; we obtained virtually the same re-
sults with only two bins.
We generated and inserted ?pseudo-punctuation?
symbols based on these binned values that were in-
serted into the parse input as described below. In
general, a pseudo-punctuation symbol is the con-
junction of the binned values of all of the prosodic
features used in a particular run. When map-
ping from binned prosodic variables to pseudo-
punctuation symbols, some of the binned values
can be represented by the absence of a pseudo-
punctuation symbol.
Because we intend these pseudo-punctuation
symbols to be as similar as possible to normal punc-
tuation, we generated pseudo-punctuation symbols
only when the corresponding prosodic variable falls
outside of its typical values. The ranges are given
below, and were chosen so that they align with
bin boundaries and result in each type of pseudo-
punctuation symbol occuring on 40% of words.
Thus when a prosodic feature is used alone only 4 of
its 10 bins are represented by a pseudo-punctuation
symbol.
However, when two or more types of the prosodic
pseudo-punctuation symbols are used at once there
is a larger number of different pseudo-punctuation
symbols and a greater number of words appear-
ing with a following pseudo-punctuation symbol.
For example, when P, R and S prosodic annota-
tions are used together there are 89 distinct types
of prosodic pseudo-punctuation symbols in our cor-
pus, and 54% of words are followed by a prosodic
pseudo-punctuation symbol.
The experiments below make use of the following
types of pseudo-punctuation symbols, either alone
or concatenated in combination. See Figure 2 for
an example tree with pseudo-punctuation symbols
inserted.
Pb This is based on the bin b of the binned
PAU DUR N value, and is only generated when
the PAU DUR N value is greater than 0.285.
Rb This is based on the bin b of the binned
NORM LAST RHYME DUR value, and is only
generated that value is greater than -0.061.
Wb This is based on the bin b of the binned
FOK WRD DIFF MNMN N value, and is only gen-
erated when that value is less than -0.071 or
greater than 0.0814.
Lb This is based on the bin b of the
FOK LR MEAN KBASELN value, and is only
generated when that value is less than 0.157 or
greater than 0.391.
Sb This is based on the bin b of the
SLOPE MEAN DIFF N value, and is only
generated whenever that value is non-zero.
In addition, we also created a binary version of
the P feature in order to evaluate the effect of bina-
rization.
NP This is based on the PAU DUR N value, and is
only generated when that value is greater than
0.285.
We actually experimented with a much wider
range of binned variables, but they all produced re-
sults similar to those described below.
2.2 Parse corpus construction
We tried to incorporate the binned prosodic informa-
tion described in the previous subsection in a manner
that corresponds as closely as possible to the way
that punctuation is represented in this corpus, be-
cause previous experiments have shown that punc-
tuation improves parser performance (Charniak and
Johnson, 2001; Engel et al, 2002). We deleted dis-
fluency tags and EDITED subtrees from our training
and test corpora.
We investigated several combinations of prosodic
pseudo-punctuation symbols. For each of these we
generated a training and test corpus. The pseudo-
punctuation symbols are dominated by a new preter-
minal PROSODY to produce a well-formed tree.
These prosodic local trees are introduced into the
tree following the word they described, and are at-
tached as high as possible in the tree, just as punc-
tuation is in the Penn treebank. Figure 2 depicts
a typical tree that contains P R S prosodic pseudo-
punctuation symbols inserted following the word
they describe.
We experimented with several other ways of in-
corporating prosody into parse trees, none of which
greatly affected the results. For example, we also ex-
perimented with a ?raised? representation in which
the prosodic pseudo-punctuation symbol also serves
as the preterminal label. The corresponding ?raised?
version of the example tree is depicted in Figure 3.
The motivation for raising is as follows. The sta-
tistical parser used for this research generates the
siblings of a head in a sequential fashion, first pre-
dicting the category label of a sibling and later con-
ditioning on that label to predict the remaining sib-
lings. ?Raising? should permit the generative model
to condition not just on the presence of a prosodic
pseudo-punctuation symbol but also on its actual
identity. If some but not all of the prosodic pseudo-
punctuation symbols were especially indicative of
some aspect of phrase structure, then the ?raising?
structures should permit the parsing model to detect
this and condition on just those symbols. Note that
in the Penn treebank annotation scheme, different
types of punctuation are given different preterminal
categories, so punctuation is encoded in the treebank
using a ?raised? representation.
The resulting corpora contain both prosodic and
punctuation information. We prepared our actual
training and testing corpora by selectively remov-
ing subtrees from these corpora. By removing all
punctuation subtrees we obtain corpora that contain
prosodic information but no punctuation, by remov-
ing all prosodic information we obtain the original
treebank data, and by removing both prosodic and
punctuation subtrees we obtain corpora that contain
neither type of information.
2.3 Evaluation
We trained and evaluated the parser on the various
types of corpora described in the previous section.
S
INTJ
UH
Uh
PROSODY
*R4*
,
,
NP
PRP
I
PROSODY
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
PROSODY
*R3*S2*
NN
house
PROSODY
*S4*
,
,
Figure 2: A tree with P R S prosodic pseudo-punctuation symbols inserted following the words they corre-
spond to. (No P prosodic features occured in this utterance).
S
INTJ
UH
Uh
*R4*
*R4*
,
,
NP
PRP
I
*R4*
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
*R3*S2*
*R3*S2*
NN
house
*S4*
*S4*
,
,
Figure 3: The same sentence as in Figure 2, but with prosodic pseudo-punctuation raised to the preterminal
level.
Annotation unraised raised
punctuation 88.212
none 86.891
L 85.632 85.361
NP 86.633 86.633
P 86.754 86.594
R 86.407 86.288
S 86.424 85.75
W 86.031 85.681
P R 86.405 86.282
P W 86.175 85.713
P S 86.328 85.922
P R S 85.64 84.832
Table 1: The F-score of the parser?s output when
trained and tested on corpora with varying prosodic
pseudo-punctuation symbols. The entry ?punc-
tuation? gives the parser?s performance on input
with standard punctuation, while ?none? gives the
parser?s performance on input without any punctua-
tion or prosodic pseudo-punctuation whatsoever.
(We always tested on the type of corpora that corre-
sponded to the training data). We evaluated parser
performance using the methodology described in
Engel et al (2002), which is a simple adaptation of
the well-known PARSEVAL measures in which punc-
tuation and prosody preterminals are ignored. This
evaluation yields precision, recall and F-score values
for each type of training and test corpora.
3 Results
Table 1 presents the results of our experiments. The
RAISED prosody entry corresponds to the raised ver-
sion of the COMBINED corpora, as described above.
We replicated previous results and showed that
punctuation information does help parsing. How-
ever, none of the experiments with prosodic infor-
mation resulted in improved parsing performance;
indeed, adding prosodic information reduced perfor-
mance by 2 percentage points in some cases. This is
a very large amount by the standards of modern sta-
tistical parsers. Notice that the general trend is that
performance decreases as the amount and complex-
ity of the prosodic annotation increased.
4 Discussion and Conclusion
Simple statistical tests show that there is in fact
a significant correlation between the location of
opening and closing phrase boundaries and all of
the prosodic pseudo-punctuation symbols described
above, so there is no doubt that these do con-
vey information about syntactic structure. How-
ever, adding the prosodic pseudo-punctuation sym-
bols uniformly decreased parsing accuracy relative
to input with no prosodic information. There are a
number of reasons why this might be the case.
While we investigated a wide range of prosodic
features, it is possible that different prosodic features
might improve parsing performance, and it would be
interesting to see if improved prosodic feature ex-
traction would improve parsing accuracy.
We suspect that the decrease in accuracy is due
to the fact that the addition of prosodic pseudo-
punctuation symbols effectively excluded other
sources of information from the parser?s statisti-
cal models. For example, as mentioned earlier the
parser uses a mixture of n-gram models to predict
the sequence of categories on the right-hand side
of syntactic rules, backing off ultimately to a dis-
tribution that includes just the head and the preced-
ing sibling?s category. Consider the effect of insert-
ing a prosodic pseudo-punctuation symbol on such
a model. The prosodic pseudo-punctuation symbol
would replace the true preceding sibling?s category
in the model, thus possibly resulting in poorer over-
all performance (note however that the parser also
includes a higher-order backoff distribution in which
the next category is predicted using the preceding
two sibling?s categories, so the true sibling?s cate-
gory would still have some predictive value).
The basic point is that inserting additional in-
formation into the parse tree effectively splits the
conditioning contexts, exacerbating the sparse data
problems that are arguably the bane of all statisti-
cal parsers. Additional information only improves
parsing accuracy if the information it conveys is suf-
ficient to overcome the loss in accuracy incurred by
the increase in data sparseness. It seems that punctu-
ation carries sufficient information to overcome this
loss, but that the prosodic categories we introduced
do not.
It could be that our results reflect the fact that we
are parsing speech transcripts in which the words
(and hence their parts of speech) are very reliably
identified, whereas our prosodic features were auto-
matically extracted directly from the speech signal
and hence might be noisier. If the explanation pro-
posed above is correct, it is perhaps not surprising
that an accurate part of speech label would prove
more useful in a conditioning context used by the
parser than a noisy prosodic feature. Note that this
would not be the case when parsing from speech rec-
ognizer output (since word identity would itself be
uncertain), and it is possible that in such applications
prosodic information would be more useful.
Of course, there are many other ways prosodic in-
formation might be exploited in a parser, and one
of those may yield improved parser performance.
We chose to incorporate prosodic information into
our parser in a way that was similar to the way
that punctuation is annotated in the Penn treebanks
because we assumed that punctuation carries infor-
mation similar to prosody, and it had already been
demonstrated that punctuation annotated in the Penn
treebank fashion does systematically improve pars-
ing accuracy.
But the assumption that prosody conveys infor-
mation about syntactic structure in the same way
that punctuation does could be false. It could also be
that even though prosody encodes information about
syntactic structure, this information is encoded in
a manner that is too complicated for our parser to
utilize. For example, even though commas are of-
ten used to indicate pauses, pauses have many other
functions in fluent speech. Pauses of greater than
200 ms are often associated with planning problems,
which might be correlated with syntactic structure
in ways too complex for the parser to exploit. While
not reported here, we tried various techniques to iso-
late different functions of pauses, such as exclud-
ing pauses of greater than 200 ms. However, all of
these experiments produced results similar to those
reported here.
Finally, there is another possible reason why our
assumption that prosody and punctuation are similar
in their information content could be wrong. Our
prosodic information was automatically extracted
from the speech stream, while punctuation was pro-
duced by human annotators who presumably com-
prehended the utterances being annotated. Given
this, it is perhaps no surprise that our automatically
extracted prosodic annotations proved less useful
than human-produced punctuation.
References
Bengt Altenberg. 1987. Prosodic patterns in spoken En-
glish: studies in the correlation between prosody and
grammar. Lund University Press, Lund.
Don Baron, Elizabeth Shriberg, and Andreas Stolcke.
2002. Automatic punctuation and disfluency detec-
tion in multi-party meetings using prosodic and lex-
ical cues. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 949?952, Denver.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre, 1995. Bracketting Guideliness for Treebank II
style Penn Treebank Project. Linguistic Data Consor-
tium.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
William Croft. 1995. Intonation units and grammatical
structure. Linguistics, 33:839?882.
Donald Engel, Eugene Charniak, and Mark Johnson.
2002. Parsing and disfluency placement. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 49?54.
Luciana Ferrer, Elizabeth Shriberg, and Andreas Stol-
cke. 2002. Is the speaker done yet? faster and more
accurate end-of-utterance detection using prosody in
human-computer dialog. In Proc. Intl. Conf. on Spo-
ken Language Processing, volume 3, pages 2061?
2064, Denver.
Luciana Ferrer. 2002. Prosodic features for the switch-
board database. Technical report, SRI International,
Menlo Park.
Jon Hamaker, Dan Harkins, and Joe Picone. 2003. Man-
ually corrected switchboard word alignments.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proc. Intl. Conf.
on Spoken Language Processing, volume 4, pages
1255?1258, Philadelphia.
Wouter Jansen, Michelle L. Gregory, and Jason M. Bre-
nier. 2001. Prosodic correlates of directly reported
speech: Evidence from conversational speech. In Pro-
ceedings of the ISCA Workshop on Prosody in Speech
Recognition and Understanding, pages 77?80, Red
Banks, NJ.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Ralf Kompe. 1997. Prosody in speech understanding
systems. Springer, Berlin.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Heinrich Neiman, Elmar Noth, Anton Batliner, Jan
Buckow, Florian Gallwitz, Richard Huber, and Volkar
Warnke. 1998. Using prosodic cues in spoken dialog
systems. In Proceedings of the International Work-
shop on Speech and Computer, pages 17?28, St. Pe-
tersburg.
Elmar No?th, Anton Batliner, Andreas Kie?ling, Ralf
Kompe, and Heinrich Niemann. 2000. Verbmobil:
The use of prosody in the linguistic components of a
speech understanding system. IEEE Transactions on
Speech and Auditory Processing, 8(5):519?532.
Astrid Schepman and Paul Rodway. 2000. Prosody
and on-line parsing in coordination structures. The
Quarterly Journal of Experimental Psychology: A,
53(2):377?396.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tur, and Gorkhan Tur. 2000. Prosody-based auto-
matic segmentation of speech into sentences and top-
ics. Speech Communication, 32(1-2):127?154.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152?159,
New York, June 2006. c?2006 Association for Computational Linguistics
Effective Self-Training for Parsing
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
We present a simple, but surprisingly ef-
fective, method of self-training a two-
phase parser-reranker system using read-
ily available unlabeled data. We show
that this type of bootstrapping is possible
for parsing when the bootstrapped parses
are processed by a discriminative reranker.
Our improved model achieves an f -score
of 92.1%, an absolute 1.1% improvement
(12% error reduction) over the previous
best result for Wall Street Journal parsing.
Finally, we provide some analysis to bet-
ter understand the phenomenon.
1 Introduction
In parsing, we attempt to uncover the syntactic struc-
ture from a string of words. Much of the challenge
of this lies in extracting the appropriate parsing
decisions from textual examples. Given sufficient
labelled data, there are several ?supervised? tech-
niques of training high-performance parsers (Char-
niak and Johnson, 2005; Collins, 2000; Henderson,
2004). Other methods are ?semi-supervised? where
they use some labelled data to annotate unlabeled
data. Examples of this include self-training (Char-
niak, 1997) and co-training (Blum and Mitchell,
1998; Steedman et al, 2003). Finally, there are ?un-
supervised? strategies where no data is labeled and
all annotations (including the grammar itself) must
be discovered (Klein and Manning, 2002).
Semi-supervised and unsupervised methods are
important because good labeled data is expensive,
whereas there is no shortage of unlabeled data.
While some domain-language pairs have quite a bit
of labelled data (e.g. news text in English), many
other categories are not as fortunate. Less unsuper-
vised methods are more likely to be portable to these
new domains, since they do not rely as much on ex-
isting annotations.
2 Previous work
A simple method of incorporating unlabeled data
into a new model is self-training. In self-training,
the existing model first labels unlabeled data. The
newly labeled data is then treated as truth and com-
bined with the actual labeled data to train a new
model. This process can be iterated over different
sets of unlabeled data if desired. It is not surprising
that self-training is not normally effective: Charniak
(1997) and Steedman et al (2003) report either mi-
nor improvements or significant damage from using
self-training for parsing. Clark et al (2003) applies
self-training to POS-tagging and reports the same
outcomes. One would assume that errors in the orig-
inal model would be amplified in the new model.
Parser adaptation can be framed as a semi-
supervised or unsupervised learning problem. In
parser adaptation, one is given annotated training
data from a source domain and unannotated data
from a target. In some cases, some annotated data
from the target domain is available as well. The goal
is to use the various data sets to produce a model
that accurately parses the target domain data despite
seeing little or no annotated data from that domain.
Gildea (2001) and Bacchiani et al (2006) show that
out-of-domain training data can improve parsing ac-
152
curacy. The unsupervised adaptation experiment by
Bacchiani et al (2006) is the only successful in-
stance of parsing self-training that we have found.
Our work differs in that all our data is in-domain
while Bacchiani et al uses the Brown corpus as la-
belled data. These correspond to different scenarios.
Additionally, we explore the use of a reranker.
Co-training is another way to train models from
unlabeled data (Blum and Mitchell, 1998). Unlike
self-training, co-training requires multiple learners,
each with a different ?view? of the data. When one
learner is confident of its predictions about the data,
we apply the predicted label of the data to the train-
ing set of the other learners. A variation suggested
by Dasgupta et al (2001) is to add data to the train-
ing set when multiple learners agree on the label. If
this is the case, we can be more confident that the
data was labelled correctly than if only one learner
had labelled it.
Sarkar (2001) and Steedman et al (2003) inves-
tigated using co-training for parsing. These studies
suggest that this type of co-training is most effec-
tive when small amounts of labelled training data is
available. Additionally, co-training for parsing can
be helpful for parser adaptation.
3 Experimental Setup
Our parsing model consists of two phases. First, we
use a generative parser to produce a list of the top n
parses. Next, a discriminative reranker reorders the
n-best list. These components constitute two views
of the data, though the reranker?s view is restricted
to the parses suggested by the first-stage parser. The
reranker is not able to suggest new parses and, more-
over, uses the probability of each parse tree accord-
ing to the parser as a feature to perform the rerank-
ing. Nevertheless, the reranker?s value comes from
its ability to make use of more powerful features.
3.1 The first-stage 50-best parser
The first stage of our parser is the lexicalized proba-
bilistic context-free parser described in (Charniak,
2000) and (Charniak and Johnson, 2005). The
parser?s grammar is a smoothed third-order Markov
grammar, enhanced with lexical heads, their parts
of speech, and parent and grandparent informa-
tion. The parser uses five probability distributions,
one each for heads, their parts-of-speech, head-
constituent, left-of-head constituents, and right-of-
head constituents. As all distributions are condi-
tioned with five or more features, they are all heavily
backed off using Chen back-off (the average-count
method from Chen and Goodman (1996)). Also,
the statistics are lightly pruned to remove those that
are statistically less reliable/useful. As in (Char-
niak and Johnson, 2005) the parser has been mod-
ified to produce n-best parses. However, the n-best
parsing algorithm described in that paper has been
replaced by the much more efficient algorithm de-
scribed in (Jimenez and Marzal, 2000; Huang and
Chang, 2005).
3.2 The MaxEnt Reranker
The second stage of our parser is a Maximum En-
tropy reranker, as described in (Charniak and John-
son, 2005). The reranker takes the 50-best parses
for each sentence produced by the first-stage 50-
best parser and selects the best parse from those
50 parses. It does this using the reranking method-
ology described in Collins (2000), using a Maxi-
mum Entropy model with Gaussian regularization
as described in Johnson et al (1999). Our reranker
classifies each parse with respect to 1,333,519 fea-
tures (most of which only occur on few parses).
The features consist of those described in (Char-
niak and Johnson, 2005), together with an additional
601,577 features. These features consist of the parts-
of-speech, possibly together with the words, that
surround (i.e., precede or follow) the left and right
edges of each constituent. The features actually used
in the parser consist of all singletons and pairs of
such features that have different values for at least
one of the best and non-best parses of at least 5 sen-
tences in the training data. There are 147,456 such
features involving only parts-of-speech and 454,101
features involving parts-of-speech and words. These
additional features are largely responsible for im-
proving the reranker?s performance on section 23
to 91.3% f -score (Charniak and Johnson (2005) re-
ported an f -score of 91.0% on section 23).
3.3 Corpora
Our labeled data comes from the Penn Treebank
(Marcus et al, 1993) and consists of about 40,000
sentences from Wall Street Journal (WSJ) articles
153
annotated with syntactic information. We use the
standard divisions: Sections 2 through 21 are used
for training, section 24 is held-out development, and
section 23 is used for final testing. Our unlabeled
data is the North American News Text corpus, NANC
(Graff, 1995), which is approximately 24 million un-
labeled sentences from various news sources. NANC
contains no syntactic information. Sentence bound-
aries in NANC are induced by a simple discrimina-
tive model. We also perform some basic cleanups on
NANC to ease parsing. NANC contains news articles
from various news sources including the Wall Street
Journal, though for this paper, we only use articles
from the LA Times.
4 Experimental Results
We use the reranking parser to produce 50-best
parses of unlabeled news articles from NANC. Next,
we produce two sets of one-best lists from these 50-
best lists. The parser-best and reranker-best lists
represent the best parse for each sentence accord-
ing to the parser and reranker, respectively. Fi-
nally, we mix a portion of parser-best or reranker-
best lists with the standard Wall Street Journal train-
ing data (sections 2-21) to retrain a new parser (but
not reranker1) model. The Wall Street Journal train-
ing data is combined with the NANC data in the
following way: The count of each parsing event is
the (optionally weighted) sum of the counts of that
event in Wall Street Journal and NANC. Bacchiani
et al (2006) show that count merging is more effec-
tive than creating multiple models and calculating
weights for each model (model interpolation). Intu-
itively, this corresponds to concatenating our train-
ing sets, possibly with multiple copies of each to ac-
count for weighting.
Some notes regarding evaluations: All numbers
reported are f -scores2. In some cases, we evaluate
only the parser?s performance to isolate it from the
reranker. In other cases, we evaluate the reranking
parser as a whole. In these cases, we will use the
term reranking parser.
Table 1 shows the difference in parser?s (not
reranker?s) performance when trained on parser-best
1We attempted to retrain the reranker using the self-trained
sentences, but found no significant improvement.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
Sentences added Parser-best Reranker-best
0 (baseline) 90.3
50k 90.1 90.7
250k 90.1 90.7
500k 90.0 90.9
750k 89.9 91.0
1,000k 90.0 90.8
1,500k 90.0 90.8
2,000k ? 91.0
Table 1: f -scores after adding either parser-best or
reranker-best sentences from NANC to WSJ training
data. While the reranker was used to produce the
reranker-best sentences, we performed this evalua-
tion using only the first-stage parser to parse all sen-
tences from section 22. We did not train a model
where we added 2,000k parser-best sentences.
output versus reranker-best output. Adding parser-
best sentences recreates previous self-training ex-
periments and confirms that it is not beneficial.
However, we see a large improvement from adding
reranker-best sentences. One may expect to see a
monotonic improvement from this technique, but
this is not quite the case, as seen when we add
1,000k sentences. This may be due to some sec-
tions of NANC being less similar to WSJ or contain-
ing more noise. Another possibility is that these
sections contains harder sentences which we can-
not parse as accurately and thus are not as useful
for self-training. For our remaining experiments, we
will only use reranker-best lists.
We also attempt to discover the optimal number
of sentences to add from NANC. Much of the im-
provement comes from the addition of the initial
50,000 sentences, showing that even small amounts
of new data can have a significant effect. As we add
more data, it becomes clear that the maximum ben-
efit to parsing accuracy by strictly adding reranker-
best sentences is about 0.7% and that f -scores will
asymptote around 91.0%. We will return to this
when we consider the relative weightings of WSJ and
NANC data.
One hypothesis we consider is that the reranked
NANC data incorporated some of the features from
the reranker. If this were the case, we would not see
an improvement when evaluating a reranking parser
154
Sentences added 1 22 24
0 (baseline) 91.8 92.1 90.5
50k 91.8 92.4 90.8
250k 91.8 92.3 91.0
500k 92.0 92.4 90.9
750k 92.0 92.4 91.1
1,000k 92.1 92.2 91.3
1,500k 92.1 92.1 91.2
1,750k 92.1 92.0 91.3
2,000k 92.2 92.0 91.3
Table 2: f -scores from evaluating the rerank-
ing parser on three held-out sections after adding
reranked sentences from NANC to WSJ training.
These evaluations were performed on all sentences.
on the same models. In Table 2, we see that the new
NANC data contains some information orthogonal to
the reranker and improves parsing accuracy of the
reranking parser.
Up to this point, we have only considered giving
our true training data a relative weight of one. In-
creasing the weight of the Wall Street Journal data
should improve, or at least not hurt, parsing perfor-
mance. Indeed, this is the case for both the parser
(figure not shown) and reranking parser (Figure 1).
Adding more weight to the Wall Street Journal data
ensures that the counts of our events will be closer
to our more accurate data source while still incorpo-
rating new data from NANC. While it appears that
the performance still levels off after adding about
one million sentences from NANC, the curves cor-
responding to higher WSJ weights achieve a higher
asymptote. Looking at the performance of various
weights across sections 1, 22, and 24, we decided
that the best combination of training data is to give
WSJ a relative weight of 5 and use the first 1,750k
reranker-best sentences from NANC.
Finally, we evaluate our new model on the test
section of Wall Street Journal. In Table 3, we note
that baseline system (i.e. the parser and reranker
trained purely on Wall Street Journal) has improved
by 0.3% over Charniak and Johnson (2005). The
92.1% f -score is the 1.1% absolute improvement
mentioned in the abstract. The improvement from
self-training is significant in both macro and micro
tests (p < 10?5).
 91.7
 91.8
 91.9
 92
 92.1
 92.2
 92.3
 92.4
 0  5  10  15  20  25  30  35  40
f-s
co
re
NANC sentences added (units of 50k sentences)
WSJ x1
WSJ x3
WSJ x5
Figure 1: Effect of giving more relative weight to
WSJ training data on reranking parser f -score. Eval-
uations were done from all sentences from section
1.
Model fparser freranker
Charniak and Johnson (2005) ? 91.0
Current baseline 89.7 91.3
WSJ + NANC 91.0 92.1
Table 3: f -scores on WSJ section 23. fparser and
freranker are the evaluation of the parser and rerank-
ing parser on all sentences, respectively. ?WSJ +
NANC? represents the system trained on WSJ train-
ing (with a relative weight of 5) and 1,750k sen-
tences from the reranker-best list of NANC.
5 Analysis
We performed several types of analysis to better un-
derstand why the new model performs better. We
first look at global changes, and then at changes at
the sentence level.
5.1 Global Changes
It is important to keep in mind that while the
reranker seems to be key to our performance im-
provement, the reranker per se never sees the extra
data. It only sees the 50-best lists produced by the
first-stage parser. Thus, the nature of the changes to
this output is important.
We have already noted that the first-stage parser?s
one-best has significantly improved (see Table 1). In
Table 4, we see that the 50-best oracle rate also im-
155
Model 1-best 10-best 50-best
Baseline 89.0 94.0 95.9
WSJ?1 + 250k 89.8 94.6 96.2
WSJ?5 + 1,750k 90.4 94.8 96.4
Table 4: Oracle f -scores of top n parses produced
by baseline, a small self-trained parser, and the
?best? parser
proves from 95.5% for the original first-stage parser,
to 96.4% for our final model. We do not show it here,
but if we self-train using first-stage one-best, there is
no change in oracle rate.
The first-stage parser also becomes more ?deci-
sive.? The average (geometric mean) of log2(Pr(1-
best) / Pr(50th-best)) (i.e. the ratios between the
probabilities in log space) increases from 11.959 for
the baseline parser, to 14.104 for the final parser. We
have seen earlier that this ?confidence? is deserved,
as the first-stage one-best is so much better.
5.2 Sentence-level Analysis
To this point we have looked at bulk properties of the
data fed to the reranker. It has higher one best and
50-best-oracle rates, and the probabilities are more
skewed (the higher probabilities get higher, the lows
get lower). We now look at sentence-level proper-
ties. In particular, we analyzed the parsers? behav-
ior on 5,039 sentences in sections 1, 22 and 24 of
the Penn treebank. Specifically, we classified each
sentence into one of three classes: those where the
self-trained parser?s f -score increased relative to the
baseline parser?s f -score, those where the f -score
remained the same, and those where the self-trained
parser?s f -score decreased relative to the baseline
parser?s f -score. We analyzed the distribution of
sentences into these classes with respect to four fac-
tors: sentence length, the number of unknown words
(i.e., words not appearing in sections 2?21 of the
Penn treebank) in the sentence, the number of coor-
dinating conjunctions (CC) in the sentence, and the
number of prepositions (IN) in the sentence. The
distributions of classes (better, worse, no change)
with respect to each of these factors individually are
graphed in Figures 2 to 5.
Figure 2 shows how the self-training affects f -
score as a function of sentence length. The top line
0 10 20 30 40 50 60
20
40
60
80
10
0
Sentence length
N
um
be
r o
f s
en
te
nc
es
 (s
mo
oth
ed
)
Better
No change
Worse
Figure 2: How self-training improves performance
as a function of sentence length
shows that the f -score of most sentences remain un-
changed. The middle line is the number of sentences
that improved their f -score, and the bottom are those
which got worse. So, for example, for sentences of
length 30, about 80 were unchanged, 25 improved,
and 22 worsened. It seems clear that there is no
improvement for either very short sentences, or for
very long ones. (For long ones the graph is hard
to read. We show a regression analysis later in this
section that confirms this statement.) While we did
not predict this effect, in retrospect it seems reason-
able. The parser was already doing very well on
short sentences. The very long ones are hopeless,
and the middle ones are just right. We call this the
Goldilocks effect.
As for the other three of these graphs, their stories
are by no means clear. Figure 3 seems to indicate
that the number of unknown words in the sentence
does not predict that the reranker will help. Figure 4
might indicate that the self-training parser improves
prepositional-phrase attachment, but the graph looks
suspiciously like that for sentence length, so the im-
provements might just be due to the Goldilocks ef-
fect. Finally, the improvement in Figure 5 is hard to
judge.
To get a better handle on these effects we did a
factor analysis. The factors we consider are number
of CCs, INs, and unknowns, plus sentence length.
As Figure 2 makes clear, the relative performance
of the self-trained and baseline parsers does not
156
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Unknown words
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 3: How self-training improves performance
as a function of number of unknown words
Estimate Pr(> 0)
(Intercept) -0.25328 0.3649
BinnedLength(10,20] 0.02901 0.9228
BinnedLength(20,30] 0.45556 0.1201
BinnedLength(30,40] 0.40206 0.1808
BinnedLength(40,50] 0.26585 0.4084
BinnedLength(50,200] -0.06507 0.8671
CCs 0.12333 0.0541
Table 5: Factor analysis for the question: does the
self-trained parser improve the parse with the high-
est probability
vary linearly with sentence length, so we introduced
binned sentence length (with each bin of length 10)
as a factor.
Because the self-trained and baseline parsers pro-
duced equivalent output on 3,346 (66%) of the sen-
tences, we restricted attention to the 1,693 sentences
on which the self-trained and baseline parsers? f -
scores differ. We asked the program to consider the
following factors: binned sentence length, number
of PPs, number of unknown words, and number of
CCs. The results are shown in Table 5. The factor
analysis is trying to model the log odds as a sum of
linearly weighted factors. I.e,
log(P (1|x)/(1 ? P (1|x))) = ?0 +
m
?
j=1
?jfj(x)
In Table 5 the first column gives the name of the fac-
0 2 4 6 8 10
20
0
40
0
60
0
Number of INs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 4: How self-training improves performance
as a function of number of prepositions
tor. The second the change in the log-odds resulting
from this factor being present (in the case of CCs
and INs, multiplied by the number of them) and the
last column is the probability that this factor is really
non-zero.
Note that there is no row for either PPs or un-
known words. This is because we also asked the pro-
gram to do a model search using the Akaike Infor-
mation Criterion (AIC) over all single and pairwise
factors. The model it chooses predicts that the self-
trained parser is likely produce a better parse than
the baseline only for sentences of length 20?40 or
sentences containing several CCs. It did not include
the number of unknown words and the number of
INs as factors because they did not receive a weight
significantly different from zero, and the AIC model
search dropped them as factors from the model.
In other words, the self-trained parser is more
likely to be correct for sentences of length 20?
40 and as the number of CCs in the sentence in-
creases. The self-trained parser does not improve
prepositional-phrase attachment or the handling of
unknown words.
This result is mildly perplexing. It is fair to say
that neither we, nor anyone we talked to, thought
conjunction handling would be improved. Conjunc-
tions are about the hardest things in parsing, and we
have no grip on exactly what it takes to help parse
them. Conversely, everyone expected improvements
on unknown words, as the self-training should dras-
157
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Number of CCs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 5: How self-training improves performance
as a function of number of conjunctions
tically reduce the number of them. It is also the case
that we thought PP attachment might be improved
because of the increased coverage of preposition-
noun and preposition-verb combinations that work
such as (Hindle and Rooth, 1993) show to be so im-
portant.
Currently, our best conjecture is that unknowns
are not improved because the words that are un-
known in the WSJ are not significantly represented
in the LA Times we used for self-training. CCs
are difficult for parsers because each conjunct has
only one secure boundary. This is particularly the
case with longer conjunctions, those of VPs and Ss.
One thing we know is that self-training always im-
proves performance of the parsing model when used
as a language model. We think CC improvement is
connected with this fact and our earlier point that
the probabilities of the 50-best parses are becoming
more skewed. In essence the model is learning, in
general, what VPs and Ss look like so it is becom-
ing easier to pull them out of the stew surrounding
the conjunct. Conversely, language modeling has
comparatively less reason to help PP attachment. As
long as the parser is doing it consistently, attaching
the PP either way will work almost as well.
6 Conclusion
Contrary to received wisdom, self-training can im-
prove parsing. In particular we have achieved an ab-
solute improvement of 0.8% over the baseline per-
formance. Together with a 0.3% improvement due
to superior reranking features, this is a 1.1% im-
provement over the previous best parser results for
section 23 of the Penn Treebank (from 91.0% to
92.1%). This corresponds to a 12% error reduc-
tion assuming that a 100% performance is possible,
which it is not. The preponderance of evidence sug-
gests that it is somehow the reranking aspect of the
parser that makes this possible, but given no idea of
why this should be, so we reserve final judgement
on this matter.
Also contrary to expectations, the error analy-
sis suggests that there has been no improvement in
either the handing of unknown words, nor prepo-
sitional phrases. Rather, there is a general im-
provement in intermediate-length sentences (20-50
words), but no improvement at the extremes: a phe-
nomenon we call the Goldilocks effect. The only
specific syntactic phenomenon that seems to be af-
fected is conjunctions. However, this is good news
since conjunctions have long been considered the
hardest of parsing problems.
There are many ways in which this research
should be continued. First, the error analysis needs
to be improved. Our tentative guess for why sen-
tences with unknown words failed to improve should
be verified or disproven. Second, there are many
other ways to use self-trained information in pars-
ing. Indeed, the current research was undertaken
as the control experiment in a program to try much
more complicated methods. We still have them
to try: restricting consideration to more accurately
parsed sentences as training data (sentence selec-
tion), trying to learn grammatical generalizations di-
rectly rather than simply including the data for train-
ing, etc.
Next there is the question of practicality. In terms
of speed, once the data is loaded, the new parser is
pretty much the same speed as the old ? just un-
der a second a sentence on average for treebank sen-
tences. However, the memory requirements are lar-
gish, about half a gigabyte just to store the data. We
are making our current best self-trained parser avail-
able3 as machines with a gigabyte or more of RAM
are becoming commonplace. Nevertheless, it would
be interesting to know if the data can be pruned to
3ftp://ftp.cs.brown.edu/pub/nlparser
158
make the entire system less bulky.
Finally, there is also the nature of the self-trained
data themselves. The data we use are from the LA
Times. Those of us in parsing have learned to expect
significant decreases in parsing accuracy even when
moving the short distance from LA Times to Wall
Street Journal. This seemingly has not occurred.
Does this mean that the reranking parser somehow
overcomes at least small genre differences? On this
point, we have some pilot experiments that show
great promise.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
We would like to thank Michael Collins, Brian Roark, James
Henderson, Miles Osborne, and the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory (COLT-98).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, Menlo Park. AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In 1st Annual Meeting of the NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Arivind Joshi and Martha Palmer, editors,
Proceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping POS-taggers using unlabelled data. In
Proceedings of CoNLL-2003.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the 17th International Conference (ICML
2000), pages 175?182, Stanford, California.
Sanjoy Dasgupta, M.L. Littman, and D. McAllester.
2001. PAC generalization bounds for co-training. In
Advances in Neural Information Processing Systems
(NIPS), 2001.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proc. 42nd Meet-
ing of Association for Computational Linguistics (ACL
2004), Barcelona, Spain.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic ?unification-based? grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535?541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Anoop Sarkar. 2001. Applying cotraining methods to
statistical parsing. In Proceedings of the 2001 NAACL
Conference.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of EACL 03.
159
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilevel Coarse-to-fine PCFG Parsing
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
1 Introduction
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the ?gold constituents?) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
168
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
2 Previous Research
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman?s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
p(nki,j | s) =
?(nki,j)?(nki,j)
p(s) . (1)
169
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A? for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A? search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing ?
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
S1
{
S1
{
S1
{
S1
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S
?
?
?
?
?
?
?
?
?
?
?
?
?
S
VP
UCP
SQ
SBAR
SBARQ
SINV
N
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NAC
NX
LST
X
UCP
FRAG
MP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A
?
?
?
?
?
?
?
?
?
?
?
?
?
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
P
?
?
?
?
?
?
?
?
?
?
?
?
?
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
Figure 1: The levels of nonterminal labels
after most constituents have been pruned away.
3 Multilevel Course-to-fine Parsing
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
170
S1
P
P
PRP
He
P
VBD
ate
P
IN
at
P
DT
the
NN
mall
.
.
S1
HP
HP
PRP
He
HP
VBD
ate
MP
IN
at
HP
DT
the
NN
mall
.
.
S1
S_
N_
PRP
He
S_
VBD
ate
P_
IN
at
N_
DT
the
NN
mall
.
.
S1
S
NP
PRP
He
VP
VBD
ate
PP
IN
at
NP
DT
the
NN
mall
.
.
Figure 2: A tree represented at levels 0 to 3
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence ?He ate at the
mall.? at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S ?NP VP .
would be translated into the rule
HP ?HP HP .
That is, each constituent type found in ?S ?NP
VP .? is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A ?a b c d e
where c is the head constituent. We binarize
this as follows:
A ?A1 e
A1 ?A2 d
A2 ?a A3
A3 ?b c
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A ?Ad,c e
Ad,c ?Aa,c d
Aa,c ?a Ab,c
Ab,c ?b c
So, for example, the rule ?A ?Ad,c e? would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
AQ ?Ad,c eA
Ad,c ?Aa,c dA
Aa,c ?aA Ab,c
Ab,c ?bA cA
171
10?8 10?7 10?6 10?5 10?4 10?3
0.0001
0.001
0.01
0.1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j | s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
?(?) and ?(?) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
10?8 10?7 10?6 10?5 10?4 10?3
0.001
0.01
0.1
1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
4 Results
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10?4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10?4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
172
Level Constits Constits % Pruned
Produced Pruned
?106 ?106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 ? ?
3-only 392.0 0 0
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length ? 100
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at ? 5 ? 10?4,
? 10?5, and ? 10?4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 ? 106. Of those
7.55 ? 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 ? 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 ? 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392 ? 106 constituents. This is the factor-of-10
Level Time for Level Running Total
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 ?
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length ? 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
173
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ? 100
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two ?UH?s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
5 Conclusion and Future Research
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is ?holistic? in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
174
son to think it is anywhere close to optimal. It
should be possible to define ?optimal? formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
Acknowledgments
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
References
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127?133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457?464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549?568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40?46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33?
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL?03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
175
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 73?76,
New York, June 2006. c?2006 Association for Computational Linguistics
Early Deletion of Fillers In Processing Conversational Speech
Matthew Lease and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{mlease,mj}@cs.brown.edu
Abstract
This paper evaluates the benefit of deleting fillers
(e.g. you know, like) early in parsing conver-
sational speech. Readability studies have shown
that disfluencies (fillers and speech repairs) may
be deleted from transcripts without compromising
meaning (Jones et al, 2003), and deleting repairs
prior to parsing has been shown to improve its
accuracy (Charniak and Johnson, 2001). We ex-
plore whether this strategy of early deletion is also
beneficial with regard to fillers. Reported exper-
iments measure the effect of early deletion under
in-domain and out-of-domain parser training con-
ditions using a state-of-the-art parser (Charniak,
2000). While early deletion is found to yield only
modest benefit for in-domain parsing, significant
improvement is achieved for out-of-domain adap-
tation. This suggests a potentially broader role for
disfluency modeling in adapting text-based tools
for processing conversational speech.
1 Introduction
This paper evaluates the benefit of deleting fillers
early in parsing conversational speech. We follow
LDC (2004) conventions in using the term filler to
encompass a broad set of vocalized space-fillers that
can introduce syntactic (and semantic) ambiguity.
For example, in the questions
Did you know I do that?
Is it like that one?
colloquial use of fillers, indicated below through use
of commas, can yield alternative readings
Did, you know, I do that?
Is it, like, that one?
Readings of the first example differ in querying lis-
tener knowledge versus speaker action, while read-
ings of the second differ in querying similarity ver-
sus exact match. Though an engaged listener rarely
has difficulty distinguishing between such alterna-
tives, studies show that deleting disfluencies from
transcripts improves readability with no reduction in
reading comprehension (Jones et al, 2003).
The fact that disfluencies can be completely re-
moved without compromising meaning is important.
Earlier work had already made this claim regard-
ing speech repairs1 and argued that there was con-
sequently little value in syntactically analyzing re-
pairs or evaluating our ability to do so (Charniak
and Johnson, 2001). Moreover, this work showed
that collateral damage to parse accuracy caused by
repairs could be averted by deleting them prior to
parsing, and this finding has been confirmed in sub-
sequent studies (Kahn et al, 2005; Harper et al,
2005). But whereas speech repairs have received
significant attention in the parsing literature, fillers
have been relatively neglected. While one study
has shown that the presence of interjection and par-
enthetical constituents in conversational speech re-
duces parse accuracy (Engel et al, 2002), these con-
stituent types are defined to cover both fluent and
disfluent speech phenomena (Taylor, 1996), leaving
the impact of fillers alone unclear.
In our study, disfluency annotations (Taylor,
1995) are leveraged to identify fillers precisely, and
these annotations are merged with treebank syn-
tax. Extending the arguments of Charniak and John-
son with regard to repairs (2001), we argue there
is little value in recovering the syntactic structure
1See (Core and Schubert, 1999) for a prototypical counter-
example that rarely occurs in practice.
73
of fillers, and we relax evaluation metrics accord-
ingly (?3.2). Experiments performed (?3.3) use a
state-of-the-art parser (Charniak, 2000) to study the
impact of early filler deletion under in-domain and
out-of-domain (i.e. adaptation) training conditions.
In terms of adaptation, there is tremendous poten-
tial in applying textual tools and training data to
processing transcribed speech (e.g. machine trans-
lation, information extraction, etc.), and bleaching
speech data to more closely resemble text has been
shown to improve accuracy with some text-based
processing tasks (Rosenfeld et al, 1995). For our
study, a state-of-the-art filler detector (Johnson et al,
2004) is employed to delete fillers prior to parsing.
Results show parse accuracy improves significantly,
suggesting disfluency filtering may have a broad role
in enabling text-based processing of speech data.
2 Disfluency in Brief
In this section we give a brief introduction to disflu-
ency, providing an excerpt from Switchboard (Graff
and Bird, 2000) that demonstrates typical production
of repairs and fillers in conversational speech.
We follow previous work (Shriberg, 1994) in de-
scribing a repair in terms of three parts: the reparan-
dum (the material repaired), the corrected alteration,
and between these an optional interregnum (or edit-
ing term) consisting of one or more fillers. Our no-
tion of fillers encompasses filled pauses (e.g. uh,
um, ah) as well as other vocalized space-fillers
annotated by LDC (Taylor, 1995), such as you
know, i mean, like, so, well, etc. An-
notations shown here are typeset with the following
conventions: fillers are bold, [reparanda] are square-
bracketed, and alterations are underlined.
S1: Uh first um i need to know uh how
do you feel [about] uh about sending uh
an elderly uh family member to a nursing
home
S2: Well of course [it?s] you know it?s one
of the last few things in the world you?d
ever want to do you know unless it?s just
you know really you know uh [for their]
uh you know for their own good
Though disfluencies rarely complicate understand-
ing for an engaged listener, deleting them from tran-
scripts improves readability with no reduction in
reading comprehension (Jones et al, 2003). For au-
tomated analysis of speech data, this means we may
freely explore processing alternatives which delete
disfluencies without compromising meaning.
3 Experiments
This section reports parsing experiments studying
the effect of early deletion under in-domain and out-
of-domain parser training conditions using the Au-
gust 2005 release of the Charniak parser (2000). We
describe data and evaluation metrics used, then pro-
ceed to describe the experiments.
3.1 Data
Conversational speech data was drawn from the
Switchboard corpus (Graff and Bird, 2000), which
annotates disfluency (Taylor, 1995) as well as syn-
tax. Our division of the corpus follows that used
in (Charniak and Johnson, 2001). Speech recognizer
(ASR) output is approximated by removing punctua-
tion, partial words, and capitalization, but we do use
reference words, representing an upperbound condi-
tion of perfect ASR. Likewise, annotated sentence
boundaries are taken to represent oracle boundary
detection. Because fillers are annotated only in
disfluency markup, we perform an automatic tree
transform to merge these two levels of annotation:
each span of contiguous filler words were pruned
from their corresponding tree and then reinserted at
the same position under a flat FILLER constituent,
attached as highly as possible. Transforms were
achieved using TSurgeon2 and Lingua::Treebank3.
For our out-of-domain training condition, the
parser was trained on sections 2-21 of the Wall Street
Journal (WSJ) corpus (Marcus et al, 1993). Punctu-
ation and capitalization were removed to bleach our
our textual training data to more closely resemble
speech (Rosenfeld et al, 1995). We also tried auto-
matically changing numbers, symbols, and abbrevi-
ations in the training text to match how they would
be read (Roark, 2002), but this did not improve ac-
curacy and so is not discussed further.
3.2 Evaluation Metrics
As discussed earlier (?1), Charniak and John-
son (2001) have argued that speech repairs do not
2http://nlp.stanford.edu/software/tsurgeon.shtml
3http://www.cpan.org
74
contribute to meaning and so there is little value
in syntactically analyzing repairs or evaluating our
ability to do so. Consequently, they relaxed stan-
dard PARSEVAL (Black et al, 1991) to treat EDITED
constituents like punctuation: adjacent EDITED con-
stituents are merged, and the internal structure and
attachment of EDITED constituents is not evaluated.
We propose generalizing this approach to disfluency
at large, i.e. fillers as well as repairs. Note that the
details of appropriate evaluation metrics for parsed
speech data is orthogonal to the parsing methods
proposed here: however parsing is performed, we
should avoid wasting metric attention evaluating
syntax of words that do not contribute toward mean-
ing and instead evaluate only how well such words
can be identified.
Relaxed metric treatment of disfluency was
achieved via simple parameterization of the SPar-
seval tool (Harper et al, 2005). SParseval also
has the added benefit of calculating a dependency-
based evaluation alongside PARSEVAL?s bracket-
based measure. The dependency metric performs
syntactic head-matching for each word using a set
of given head percolation rules (derived from Char-
niak?s parser (2000)), and its relaxed formulation
ignores terminals spanned by FILLER and EDITED
constituents. We found this metric offered additional
insights in analyzing some of our results.
3.3 Results
In the first set of experiments, we train the parser on
Switchboard and contrast early deletion of disfluen-
cies (identified by an oracle) versus parsing in the
more usual fashion. Our method for early deletion
generalizes the approach used with repairs in (Char-
niak and Johnson, 2001): contiguous filler and edit
words are deleted from the input strings, the strings
are parsed, and the removed words are reinserted
into the output trees under the appropriate flat con-
stituent, FILLER or EDITED.
Results in Table 1 give F-scores for PARSEVAL
and dependency-based parse accuracy (?3.2), as well
as per-word edit and filler detection accuracy (i.e.
how well the parser does in identifying which termi-
nals should be spanned by EDITED and FILLER con-
stituents when early deletion is not performed). We
see that the parser correctly identifies filler words
with 93.1% f-score, and that early deletion of fillers
Table 1: F-scores on Switchboard when trained in-
domain. LB and Dep refer to relaxed labelled-
bracket and dependency parse metrics (?3.2). Edit
and filler word detection f-scores are also shown.
Edits Fillers Edit F Filler F LB Dep
oracle oracle 100.0 100.0 88.9 88.5
oracle parser 100.0 93.1 87.8 87.9
parser oracle 64.3 100.0 85.0 85.6
parser parser 62.4 94.1 83.9 85.0
(via oracle knowledge) yields only a modest im-
provement in parsing accuracy (87.8% to 88.9%
bracket-based, 87.9% to 88.5% dependency-based).
We conclude from this that for in-domain training,
early deletion of fillers has limited potential to im-
prove parsing accuracy relative to what has been
seen with repairs. It is still worth noting, however,
that the parser does perform better when fillers are
absent, consistent with Engel et al?s findings (2002).
While fillers have been reported to often occur at
major clause boundaries (Shriberg, 1994), suggest-
ing their presence may benefit parsing, we do not
find this to be the case. Results shown for repair de-
tection accuracy and its impact on parsing are con-
sistent with previous work (Charniak and Johnson,
2001; Kahn et al, 2005; Harper et al, 2005).
Our second set of experiments reports the effect
of deleting fillers early when the parser is trained on
text only (WSJ, ?3.1). Our motivation here is to see
if disfluency modeling, particularly filler detection,
can help bleach speech data to more closely resem-
ble text, thereby improving our ability to process it
using text-based methods and training data (Rosen-
feld et al, 1995). Again we contrast standard
parsing with deleting disfluencies early (via oracle
knowledge). Given our particular interest in fillers,
we also report the effect of detecting them via a
state-of-the-art system (Johnson et al, 2004).
Results appear in Table 2. It is worth noting that
since our text-trained parser never produces FILLER
or EDITED constituents, the bracket-based metric
penalizes it for each such constituent appearing in
the gold trees. Similarly, since the dependency
metric ignores terminals occurring under these con-
stituents in the gold trees, the metric penalizes the
parser for producing dependencies for these termi-
75
Table 2: F-scores parsing Switchboard when trained
on WSJ. Edit word detection varies between parser
and oracle, and filler word detection varies between
none, system (Johnson et al, 2004), and oracle.
Filler F, LB, and Dep are defined as in Table 1.
Edits Fillers Filler F LB Dep
oracle oracle 100.0 83.6 81.4
oracle detect 89.3 81.6 80.5
oracle none - 71.8 75.4
none oracle 100.0 76.3 76.7
none detect 74.6 75.9 91.3
none none - 66.8 71.5
nals. Taken together, the two metrics provide a com-
plementary perspective in interpreting results.
The trend observed across metrics and edit detec-
tion conditions shows that early deletion of system-
detected fillers improves parsing accuracy 5-10%.
As seen with in-domain training, early deletion of
repairs is again seen to have a significant effect.
Given that state-of-the-art edit detection performs at
about 80% f-measure (Johnson and Charniak, 2004),
much of the benefit derived here from oracle re-
pair detection should be realizable in practice. The
broader conclusion we draw from these results is
that disfluency modeling has significant potential to
improve text-based processing of speech data.
4 Conclusion
While early deletion of fillers has limited benefit for
in-domain parsing of speech data, it can play an im-
portant role in bleaching speech data for more accu-
rate text-based processing. Alternative methods of
integrating detected filler information, such as parse
reranking (Kahn et al, 2005), also merit investiga-
tion. It will also be important to evaluate the inter-
action with ASR error and sentence boundary de-
tection error. In terms of bleaching, we saw that
even with oracle detection of disfluency, our text-
trained model still significantly under-performed the
in-domain model, indicating additional methods for
bleaching are still needed. We also plan to evaluat-
ing the benefit of disfluency modeling in bleaching
speech data for text-based machine translation.
Acknowledgments
This work was supported by NSF grants 0121285, LIS9720368,
and IIS0095940, and DARPA GALE contract HR0011-06-2-
0001. We would like to thank Brian Roark, Mary Harper, and
the rest of the JHU PASSED team for its support of this work.
References
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proc. NAACL, pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL, pages 132?139.
M.G. Core and L.K. Schubert. 1999. A syntactic framework
for speech repairs and other disruptions. In Proc. ACL, pages
413?420.
E. Black et al 1991. Procedure for quantitatively comparing
the syntactic coverage of English grammars. In Proc. Work-
shop on Speech and Natural Language, pages 306?311.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing and
disfluency placement. In Proc. EMNLP, pages 49?54.
D. Graff and S. Bird. 2000. Many uses, many annotations for
large speech corpora: Switchboard and TDT as case studies.
In Proc. LREC, pages 427?433.
M. Harper et al 2005 Johns Hopkins Summer Workshop Final
Report on Parsing and Spoken Structural Event Detection.
J.G. Kahn et al 2005. Effective use of prosody in parsing
conversational speech. In Proc. HLT/EMNLP, 233?240.
M. Johnson and E. Charniak. 2004. A TAG-based noisy chan-
nel model of speech repairs. In Proc. ACL, pages 33?39.
M. Johnson, E. Charniak, and M. Lease. 2004. An improved
model for recognizing disfluencies in conversational speech.
In Proc. Rich Text 2004 Fall Workshop (RT-04F).
D. Jones et al 2003. Measuring the readability of automatic
speech-to-text transcripts. In Proc. Eurospeech, 1585?1588.
Linguistic Data Consortium (LDC). 2004. Simple metadata
annotation specification version 6.2.
M. Marcus et al 1993. Building a large annotated corpus of
English: The Penn Treebank. Computational Linguistics,
19(2): 313?330.
B. Roark. 2002. Markov parsing: Lattice rescoring with a sta-
tistical parser. In Proc. ACL, pages 287?294.
R. Rosenfeld et al 1995. Error analysis and disfluency mod-
eling in the Swichboard domain: 1995 JHU Summer Work-
shop project team report.
E. Shriberg. 1994. Preliminaries to a Theory of Speech Disflu-
encies. Ph.D. thesis, UC Berkeley.
A. Taylor, 1995. Revision of Meteer et al?s Dysfluency Annota-
tion Stylebook for the Switchboard Corpus. LDC.
A. Taylor, 1996. Bracketing Switchboard: An addendum to the
Treebank II Bracketing Guidelines. LDC.
76
Proceedings of NAACL HLT 2007, pages 139?146,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Bayesian Inference for PCFGs via Markov chain Monte Carlo
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Mark Johnson@brown.edu
Thomas L. Griffiths
Department of Psychology
University of California, Berkeley
Tom Griffiths@berkeley.edu
Sharon Goldwater
Department of Linguistics
Stanford University
sgwater@stanford.edu
Abstract
This paper presents two Markov chain
Monte Carlo (MCMC) algorithms for
Bayesian inference of probabilistic con-
text free grammars (PCFGs) from ter-
minal strings, providing an alternative
to maximum-likelihood estimation using
the Inside-Outside algorithm. We illus-
trate these methods by estimating a sparse
grammar describing the morphology of
the Bantu language Sesotho, demonstrat-
ing that with suitable priors Bayesian
techniques can infer linguistic structure
in situations where maximum likelihood
methods such as the Inside-Outside algo-
rithm only produce a trivial grammar.
1 Introduction
The standard methods for inferring the parameters of
probabilistic models in computational linguistics are
based on the principle of maximum-likelihood esti-
mation; for example, the parameters of Probabilistic
Context-Free Grammars (PCFGs) are typically es-
timated from strings of terminals using the Inside-
Outside (IO) algorithm, an instance of the Ex-
pectation Maximization (EM) procedure (Lari and
Young, 1990). However, much recent work in ma-
chine learning and statistics has turned away from
maximum-likelihood in favor of Bayesian methods,
and there is increasing interest in Bayesian methods
in computational linguistics as well (Finkel et al,
2006). This paper presents two Markov chain Monte
Carlo (MCMC) algorithms for inferring PCFGs and
their parses from strings alone. These can be viewed
as Bayesian alternatives to the IO algorithm.
The goal of Bayesian inference is to compute a
distribution over plausible parameter values. This
?posterior? distribution is obtained by combining the
likelihood with a ?prior? distribution P(?) over pa-
rameter values ?. In the case of PCFG inference ? is
the vector of rule probabilities, and the prior might
assert a preference for a sparse grammar (see be-
low). The posterior probability of each value of ?
is given by Bayes? rule:
P(?|D) ? P(D|?)P(?). (1)
In principle Equation 1 defines the posterior prob-
ability of any value of ?, but computing this may
not be tractable analytically or numerically. For this
reason a variety of methods have been developed to
support approximate Bayesian inference. One of the
most popular methods is Markov chain Monte Carlo
(MCMC), in which a Markov chain is used to sam-
ple from the posterior distribution.
This paper presents two new MCMC algorithms
for inferring the posterior distribution over parses
and rule probabilities given a corpus of strings. The
first algorithm is a component-wise Gibbs sampler
which is very similar in spirit to the EM algo-
rithm, drawing parse trees conditioned on the cur-
rent parameter values and then sampling the param-
eters conditioned on the current set of parse trees.
The second algorithm is a component-wise Hastings
sampler that ?collapses? the probabilistic model, in-
tegrating over the rule probabilities of the PCFG,
with the goal of speeding convergence. Both algo-
139
rithms use an efficient dynamic programming tech-
nique to sample parse trees.
Given their usefulness in other disciplines, we
believe that Bayesian methods like these are likely
to be of general utility in computational linguis-
tics as well. As a simple illustrative example, we
use these methods to infer morphological parses for
verbs from Sesotho, a southern Bantu language with
agglutinating morphology. Our results illustrate that
Bayesian inference using a prior that favors sparsity
can produce linguistically reasonable analyses in sit-
uations in which EM does not.
The rest of this paper is structured as follows.
The next section introduces the background for our
paper, summarizing the key ideas behind PCFGs,
Bayesian inference, and MCMC. Section 3 intro-
duces our first MCMC algorithm, a Gibbs sampler
for PCFGs. Section 4 describes an algorithm for
sampling trees from the distribution over trees de-
fined by a PCFG. Section 5 shows how to integrate
out the rule weight parameters ? in a PCFG, allow-
ing us to sample directly from the posterior distribu-
tion over parses for a corpus of strings. Finally, Sec-
tion 6 illustrates these methods in learning Sesotho
morphology.
2 Background
2.1 Probabilistic context-free grammars
Let G = (T,N, S,R) be a Context-Free Grammar
in Chomsky normal form with no useless produc-
tions, where T is a finite set of terminal symbols, N
is a finite set of nonterminal symbols (disjoint from
T ), S ? N is a distinguished nonterminal called the
start symbol, and R is a finite set of productions of
the form A ? BC or A ? w, where A,B,C ? N
and w ? T . In what follows we use ? as a variable
ranging over (N ?N) ? T .
A Probabilistic Context-Free Grammar (G, ?) is
a pair consisting of a context-free grammar G and
a real-valued vector ? of length |R| indexed by pro-
ductions, where ?A?? is the production probability
associated with the production A ? ? ? R. We
require that ?A?? ? 0 and that for all nonterminals
A ? N , ?A???R ?A?? = 1.
A PCFG (G, ?) defines a probability distribution
over trees t as follows:
PG(t|?) =
?
r?R
?fr(t)r
where t is generated by G and fr(t) is the number
of times the production r = A ? ? ? R is used
in the derivation of t. If G does not generate t let
PG(t|?) = 0. The yield y(t) of a parse tree t is
the sequence of terminals labeling its leaves. The
probability of a string w ? T+ of terminals is the
sum of the probability of all trees with yield w, i.e.:
PG(w|?) =
?
t:y(t)=w
PG(t|?).
2.2 Bayesian inference for PCFGs
Given a corpus of strings w = (w1, . . . , wn), where
each wi is a string of terminals generated by a known
CFG G, we would like to be able to infer the pro-
duction probabilities ? that best describe that corpus.
Taking w to be our data, we can apply Bayes? rule
(Equation 1) to obtain:
P(?|w) ? PG(w|?)P(?), where
PG(w|?) =
n
?
i=1
PG(wi|?).
Using t to denote a sequence of parse trees for w,
we can compute the joint posterior distribution over
t and ?, and then marginalize over t, with P(?|w) =
?
t P(t, ?|w). The joint posterior distribution on t
and ? is given by:
P(t, ?|w) ? P(w|t)P(t|?)P(?)
=
( n
?
i=1
P(wi|ti)P(ti|?)
)
P(?)
with P(wi|ti) = 1 if y(ti) = wi, and 0 otherwise.
2.3 Dirichlet priors
The first step towards computing the posterior dis-
tribution is to define a prior on ?. We take P(?) to
be a product of Dirichlet distributions, with one dis-
tribution for each non-terminal A ? N . The prior
is parameterized by a positive real valued vector ?
indexed by productions R, so each production prob-
ability ?A?? has a corresponding Dirichlet param-
eter ?A??. Let RA be the set of productions in R
140
with left-hand side A, and let ?A and ?A refer to
the component subvectors of ? and ? respectively
indexed by productions in RA. The Dirichlet prior
PD(?|?) is:
PD(?|?) =
?
A?N
PD(?A|?A), where
PD(?A|?A) =
1
C(?A)
?
r?RA
??r?1r and
C(?A) =
?
r?RA ?(?r)
?(?r?RA ?r)
(2)
where ? is the generalized factorial function and
C(?) is a normalization constant that does not de-
pend on ?A.
Dirichlet priors are useful because they are con-
jugate to the distribution over trees defined by a
PCFG. This means that the posterior distribution
on ? given a set of parse trees, P(?|t, ?), is also a
Dirichlet distribution. Applying Bayes? rule,
PG(?|t, ?) ? PG(t|?) PD(?|?)
?
(
?
r?R
?fr(t)r
)(
?
r?R
??r?1r
)
=
?
r?R
?fr(t)+?r?1r
which is a Dirichlet distribution with parameters
f(t) + ?, where f(t) is the vector of production
counts in t indexed by r ? R. We can thus write:
PG(?|t, ?) = PD(?|f(t) + ?)
which makes it clear that the production counts com-
bine directly with the parameters of the prior.
2.4 Markov chain Monte Carlo
Having defined a prior on ?, the posterior distribu-
tion over t and ? is fully determined by a corpus
w. Unfortunately, computing the posterior probabil-
ity of even a single choice of t and ? is intractable,
as evaluating the normalizing constant for this dis-
tribution requires summing over all possible parses
for the entire corpus and all sets of production prob-
abilities. Nonetheless, it is possible to define al-
gorithms that sample from this distribution using
Markov chain Monte Carlo (MCMC).
MCMC algorithms construct a Markov chain
whose states s ? S are the objects we wish to sam-
ple. The state space S is typically astronomically
large ? in our case, the state space includes all pos-
sible parses of the entire training corpus w ? and
the transition probabilities P(s?|s) are specified via a
scheme guaranteed to converge to the desired distri-
bution ?(s) (in our case, the posterior distribution).
We ?run? the Markov chain (i.e., starting in initial
state s0, sample a state s1 from P(s?|s0), then sam-
ple state s2 from P(s?|s1), and so on), with the prob-
ability that the Markov chain is in a particular state,
P(si), converging to ?(si) as i ??.
After the chain has run long enough for it to ap-
proach its stationary distribution, the expectation
E?[f ] of any function f(s) of the state s will be
approximated by the average of that function over
the set of sample states produced by the algorithm.
For example, in our case, given samples (ti, ?i) for
i = 1, . . . , ? produced by an MCMC algorithm, we
can estimate ? as
E?[?] ?
1
?
?
?
i=1
?i
The remainder of this paper presents two MCMC
algorithms for PCFGs. Both algorithms proceed by
setting the initial state of the Markov chain to a guess
for (t, ?) and then sampling successive states using
a particular transition matrix. The key difference be-
twen the two algorithms is the form of the transition
matrix they assume.
3 A Gibbs sampler for P(t, ?|w, ?)
The Gibbs sampler (Geman and Geman, 1984) is
one of the simplest MCMC methods, in which tran-
sitions between states of the Markov chain result
from sampling each component of the state condi-
tioned on the current value of all other variables. In
our case, this means alternating between sampling
from two distributions:
P(t|?,w, ?) =
n
?
i=1
P(ti|wi, ?), and
P(?|t,w, ?) = PD(?|f(t) + ?)
=
?
A?N
PD(?A|fA(t) + ?A).
Thus every two steps we generate a new sample of
t and ?. This alternation between parsing and up-
dating ? is reminiscent of the EM algorithm, with
141
tit1 tn
w1 wi wn
?Aj. . .?A1 . . . ?A|N|
?A1 . . .. . . ?Aj ?A|N|
. . .
. . .. . .
. . .
Figure 1: A Bayes net representation of dependen-
cies among the variables in a PCFG.
the Expectation step replaced by sampling t and the
Maximization step replaced by sampling ?.
The dependencies among variables in a PCFG are
depicted graphically in Figure 1, which makes clear
that the Gibbs sampler is highly parallelizable (just
like the EM algorithm). Specifically, the parses ti
are independent given ? and so can be sampled in
parallel from the following distribution as described
in the next section.
PG(ti|wi, ?) =
PG(ti|?)
PG(wi|?)
We make use of the fact that the posterior is a
product of independent Dirichlet distributions in or-
der to sample ? from PD(?|t, ?). The production
probabilities ?A for each nonterminal A ? N are
sampled from a Dirchlet distibution with parameters
??A = fA(t) + ?A. There are several methods for
sampling ? = (?1, . . . , ?m) from a Dirichlet distri-
bution with parameters ? = (?1, . . . , ?m), with the
simplest being sampling xj from a Gamma(?j) dis-
tribution for j = 1, . . . ,m and then setting ?j =
xj/
?m
k=1 xk (Gentle, 2003).
4 Efficiently sampling from P(t|w, ?)
This section completes the description of the Gibbs
sampler for (t, ?) by describing a dynamic program-
ming algorithm for sampling trees from the set of
parses for a string generated by a PCFG. This al-
gorithm appears fairly widely known: it was de-
scribed by Goodman (1998) and Finkel et al(2006)
and used by Ding et al(2005), and is very simi-
lar to other dynamic programming algorithms for
CFGs, so we only summarize it here. The algo-
rithm consists of two steps. The first step con-
structs a standard ?inside? table or chart, as used in
the Inside-Outside algorithm for PCFGs (Lari and
Young, 1990). The second step involves a recursion
from larger to smaller strings, sampling from the
productions that expand each string and construct-
ing the corresponding tree in a top-down fashion.
In this section we take w to be a string of terminal
symbols w = (w1, . . . , wn) where each wi ? T ,
and define wi,k = (wi+1, . . . , wk) (i.e., the sub-
string from wi+1 up to wk). Further, let GA =
(T,N,A,R), i.e., a CFG just like G except that the
start symbol has been replaced with A, so, PGA(t|?)
is the probability of a tree t whose root node is la-
beled A and PGA(w|?) is the sum of the probabili-
ties of all trees whose root nodes are labeled A with
yield w.
The Inside algorithm takes as input a PCFG
(G, ?) and a string w = w0,n and constructs a ta-
ble with entries pA,i,k for each A ? N and 0 ?
i < k ? n, where pA,i,k = PGA(wi,k|?), i.e., the
probability of A rewriting to wi,k. The table entries
are recursively defined below, and computed by enu-
merating all feasible i, k and A in any order such that
all smaller values of k?i are enumerated before any
larger values.
pA,k?1,k = ?A?wk
pA,i,k =
?
A?B C?R
?
i<j<k
?A?B C pB,i,j pC,j,k
for all A,B,C ? N and 0 ? i < j < k ? n. At the
end of the Inside algorithm, PG(w|?) = pS,0,n.
The second step of the sampling algorithm uses
the function SAMPLE, which returns a sample from
PG(t|w, ?) given the PCFG (G, ?) and the inside
table pA,i,k. SAMPLE takes as arguments a non-
terminal A ? N and a pair of string positions
0 ? i < k ? n and returns a tree drawn from
PGA(t|wi,k, ?). It functions in a top-down fashion,
selecting the production A ? BC to expand the A,
and then recursively calling itself to expand B and
C respectively.
function SAMPLE(A, i, k) :
if k ? i = 1 then return TREE(A,wk)
(j,B,C) = MULTI(A, i, k)
return TREE(A, SAMPLE(B, i, j), SAMPLE(C, j, k))
In this pseudo-code, TREE is a function that con-
structs unary or binary tree nodes respectively, and
142
MULTI is a function that produces samples from
a multinomial distribution over the possible ?split?
positions j and nonterminal children B and C ,
where:
P(j,B,C) = ?A?BC PGB (wi,j|?) PGC (wj,k|?)PGA(wi,k|?)
5 A Hastings sampler for P(t|w, ?)
The Gibbs sampler described in Section 3 has
the disadvantage that each sample of ? re-
quires reparsing the training corpus w. In
this section, we describe a component-wise
Hastings algorithm for sampling directly from
P(t|w, ?), marginalizing over the produc-
tion probabilities ?. Transitions between
states are produced by sampling parses ti from
P(ti|wi, t?i, ?) for each string wi in turn, where
t?i = (t1, . . . , ti?1, ti+1, . . . , tn) is the current set
of parses for w?i = (w1, . . . , wi?1, wi+1, . . . , wn).
Marginalizing over ? effectively means that the
production probabilities are updated after each
sentence is parsed, so it is reasonable to expect
that this algorithm will converge faster than the
Gibbs sampler described earlier. While the sampler
does not explicitly provide samples of ?, the results
outlined in Sections 2.3 and 3 can be used to sample
the posterior distribution over ? for each sample of
t if required.
Let PD(?|?) be a Dirichlet product prior, and let
? be the probability simplex for ?. Then by inte-
grating over the posterior Dirichlet distributions we
have:
P(t|?) =
?
?
PG(t|?)PD(?|?)d?
=
?
A?N
C(?A + fA(t))
C(?A)
(3)
where C was defined in Equation 2. Because we
are marginalizing over ?, the trees ti become depen-
dent upon one another. Intuitively, this is because
wi may provide information about ? that influences
how some other string wj should be parsed.
We can use Equation 3 to compute the conditional
probability P(ti|t?i, ?) as follows:
P(ti|t?i, ?) =
P(t|?)
P(t?i|?)
=
?
A?N
C(?A + fA(t))
C(?A + fA(t?i))
Now, if we could sample from
P(ti|wi, t?i, ?) =
P(wi|ti)P(ti|t?i, ?)
P(wi|t?i, ?)
we could construct a Gibbs sampler whose states
were the parse trees t. Unfortunately, we don?t even
know if there is an efficient algorithm for calculat-
ing P(wi|t?i, ?), let alne an efficient sampling al-
gorithm for this distribution.
Fortunately, this difficulty is not fatal. A Hast-
ings sampler for a probability distribution ?(s) is
an MCMC algorithm that makes use of a proposal
distribution Q(s?|s) from which it draws samples,
and uses an acceptance/rejection scheme to define a
transition kernel with the desired distribution ?(s).
Specifically, given the current state s, a sample s? 6=
s drawn from Q(s?|s) is accepted as the next state
with probability
A(s, s?) = min
{
1, ?(s
?)Q(s|s?)
?(s)Q(s?|s)
}
and with probability 1 ?A(s, s?) the proposal is re-
jected and the next state is the current state s.
We use a component-wise proposal distribution,
generating new proposed values for ti, where i is
chosen at random. Our proposal distribution is the
posterior distribution over parse trees generated by
the PCFG with grammar G and production proba-
bilities ??, where ?? is chosen based on the current
t?i as described below. Each step of our Hastings
sampler is as follows. First, we compute ?? from
t?i as described below. Then we sample t?i from
P(ti|wi, ??) using the algorithm described in Sec-
tion 4. Finally, we accept the proposal t?i given the
old parse ti for wi with probability:
A(ti, t?i) = min
{
1, P(t
?
i|wi, t?i, ?)P(ti|wi, ??)
P(ti|wi, t?i, ?)P(t?i|wi, ??)
}
= min
{
1, P(t
?
i|t?i, ?)P(ti|wi, ??)
P(ti|t?i, ?)P(t?i|wi, ??)
}
The key advantage of the Hastings sampler over the
Gibbs sampler here is that because the acceptance
probability is a ratio of probabilities, the difficult to
143
compute P(wi|t?i, ?) is a common factor of both
the numerator and denominator, and hence is not re-
quired. The P (wi|ti) term also disappears, being 1
for both the numerator and the denominator since
our proposal distribution can only generate trees for
which wi is the yield.
All that remains is to specify the production prob-
abilities ?? of the proposal distribution P(t?i|wi, ??).
While the acceptance rule used in the Hastings
algorithm ensures that it produces samples from
P(ti|wi, t?i, ?) with any proposal grammar ?? in
which all productions have nonzero probability, the
algorithm is more efficient (i.e., fewer proposals are
rejected) if the proposal distribution is close to the
distribution to be sampled.
Given the observations above about the corre-
spondence between terms in P(ti|t?i, ?) and the
relative frequency of the corresponding productions
in t?i, we set ?? to the expected value E[?|t?i, ?] of
? given t?i and ? as follows:
??r =
fr(t?i) + ?r
?
r??RA fr?(t?i) + ?r?
6 Inferring sparse grammars
As stated in the introduction, the primary contribu-
tion of this paper is introducing MCMC methods
for Bayesian inference to computational linguistics.
Bayesian inference using MCMC is a technique of
generic utility, much like Expectation-Maximization
and other general inference techniques, and we be-
lieve that it belongs in every computational linguist?s
toolbox alongside these other techniques.
Inferring a PCFG to describe the syntac-
tic structure of a natural language is an obvi-
ous application of grammar inference techniques,
and it is well-known that PCFG inference us-
ing maximum-likelihood techniques such as the
Inside-Outside (IO) algorithm, a dynamic program-
ming Expectation-Maximization (EM) algorithm for
PCFGs, performs extremely poorly on such tasks.
We have applied the Bayesian MCMC methods de-
scribed here to such problems and obtain results
very similar to those produced using IO. We be-
lieve that the primary reason why both IO and the
Bayesian methods perform so poorly on this task
is that simple PCFGs are not accurate models of
English syntactic structure. We know that PCFGs
? = (0.1, 1.0)
? = (0.5, 1.0)
? = (1.0, 1.0)
Binomial parameter ?1
P(?1|?)
10.80.60.40.20
5
4
3
2
1
0
Figure 2: A Dirichlet prior ? on a binomial parame-
ter ?1. As ?1 ? 0, P(?1|?) is increasingly concen-
trated around 0.
that represent only major phrasal categories ignore
a wide variety of lexical and syntactic dependen-
cies in natural language. State-of-the-art systems
for unsupervised syntactic structure induction sys-
tem uses models that are very different to these kinds
of PCFGs (Klein and Manning, 2004; Smith and
Eisner, 2006).1
Our goal in this section is modest: we aim merely
to provide an illustrative example of Bayesian infer-
ence using MCMC. As Figure 2 shows, when the
Dirichlet prior parameter ?r approaches 0 the prior
probability PD(?r|?) becomes increasingly concen-
trated around 0. This ability to bias the sampler
toward sparse grammars (i.e., grammars in which
many productions have probabilities close to 0) is
useful when we attempt to identify relevant produc-
tions from a much larger set of possible productions
via parameter estimation.
The Bantu language Sesotho is a richly agglutina-
tive language, in which verbs consist of a sequence
of morphemes, including optional Subject Markers
(SM), Tense (T), Object Markers (OM), Mood (M)
and derivational affixes as well as the obligatory
Verb stem (V), as shown in the following example:
re
SM
-a
T
-di
OM
-bon
V
-a
M
?We see them?
1It is easy to demonstrate that the poor quality of the PCFG
models is the cause of these problems rather than search or other
algorithmic issues. If one initializes either the IO or Bayesian
estimation procedures with treebank parses and then runs the
procedure using the yields alone, the accuracy of the parses uni-
formly decreases while the (posterior) likelihood uniformly in-
creases with each iteration, demonstrating that improving the
(posterior) likelihood of such models does not improve parse
accuracy.
144
We used an implementation of the Hastings sampler
described in Section 5 to infer morphological parses
t for a corpus w of 2,283 unsegmented Sesotho
verb types extracted from the Sesotho corpus avail-
able from CHILDES (MacWhinney and Snow, 1985;
Demuth, 1992). We chose this corpus because the
words have been morphologically segmented manu-
ally, making it possible for us to evaluate the mor-
phological parses produced by our system. We con-
structed a CFG G containing the following produc-
tions
Word ? V
Word ? V M
Word ? SM V M
Word ? SM T V M
Word ? SM T OM V M
together with productions expanding the pretermi-
nals SM,T,OM,V and M to each of the 16,350 dis-
tinct substrings occuring anywhere in the corpus,
producting a grammar with 81,755 productions in
all. In effect, G encodes the basic morphologi-
cal structure of the Sesotho verb (ignoring factors
such as derivation morphology and irregular forms),
but provides no information about the phonological
identity of the morphemes.
Note that G actually generates a finite language.
However, G parameterizes the probability distribu-
tion over the strings it generates in a manner that
would be difficult to succintly characterize except
in terms of the productions given above. Moreover,
with approximately 20 times more productions than
training strings, each string is highly ambiguous and
estimation is highly underconstrained, so it provides
an excellent test-bed for sparse priors.
We estimated the morphological parses t in two
ways. First, we ran the IO algorithm initialized
with a uniform initial estimate ?0 for ? to produce
an estimate of the MLE ??, and then computed the
Viterbi parses t? of the training corpus w with respect
to the PCFG (G, ??). Second, we ran the Hastings
sampler initialized with trees sampled from (G, ?0)
with several different values for the parameters of
the prior. We experimented with a number of tech-
niques for speeding convergence of both the IO and
Hastings algorithms, and two of these were particu-
larly effective on this problem. Annealing, i.e., us-
ing P(t|w)1/? in place of P(t|w) where ? is a ?tem-
perature? parameter starting around 5 and slowly ad-
justed toward 1, sped the convergence of both algo-
rithms. We ran both algorithms for several thousand
iterations over the corpus, and both seemed to con-
verge fairly quickly once ? was set to 1. ?Jittering?
the initial estimate of ? used in the IO algorithm also
sped its convergence.
The IO algorithm converges to a solution where
?Word? V = 1, and every string w ? w is analysed
as a single morpheme V. (In fact, in this grammar
P(wi|?) is the empirical probability of wi, and it is
easy to prove that this ? is the MLE).
The samples t produced by the Hastings algo-
rithm depend on the parameters of the Dirichlet
prior. We set ?r to a single value ? for all pro-
ductions r. We found that for ? > 10?2 the sam-
ples produced by the Hastings algorithm were the
same trivial analyses as those produced by the IO
algorithm, but as ? was reduced below this t be-
gan to exhibit nontrivial structure. We evaluated
the quality of the segmentations in the morpholog-
ical analyses t in terms of unlabeled precision, re-
call, f-score and exact match (the fraction of words
correctly segmented into morphemes; we ignored
morpheme labels because the manual morphological
analyses contain many morpheme labels that we did
not include in G). Figure 3 contains a plot of how
these quantities vary with ?; obtaining an f-score of
0.75 and an exact word match accuracy of 0.54 at
? = 10?5 (the corresponding values for the MLE ??
are both 0). Note that we obtained good results as ?
was varied over several orders of magnitude, so the
actual value of ? is not critical. Thus in this appli-
cation the ability to prefer sparse grammars enables
us to find linguistically meaningful analyses. This
ability to find linguistically meaningful structure is
relatively rare in our experience with unsupervised
PCFG induction.
We also experimented with a version of IO modi-
fied to perform Bayesian MAP estimation, where the
Maximization step of the IO procedure is replaced
with Bayesian inference using a Dirichlet prior, i.e.,
where the rule probabilities ?(k) at iteration k are es-
timated using:
?(k)r ? max(0,E[fr|w, ?(k?1)] + ?? 1).
Clearly such an approach is very closely related to
the Bayesian procedures presented in this article,
145
Exact
Recall
Precision
F-score
Dirichlet prior parameter ?r
1 0.01 1e-04 1e-06 1e-08 1e-10
1
0.75
0.5
0.25
0
Figure 3: Accuracy of morphological segmentations
of Sesotho verbs proposed by the Hastings algo-
rithms as a function of Dirichlet prior parameter
?. F-score, precision and recall are unlabeled mor-
pheme scores, while Exact is the fraction of words
correctly segmented.
and in some circumstances this may be a useful
estimator. However, in our experiments with the
Sesotho data above we found that for the small val-
ues of ? necessary to obtain a sparse solution,the
expected rule count E[fr] for many rules r was less
than 1??. Thus on the next iteration ?r = 0, result-
ing in there being no parse whatsoever for many of
the strings in the training data. Variational Bayesian
techniques offer a systematic way of dealing with
these problems, but we leave this for further work.
7 Conclusion
This paper has described basic algorithms for per-
forming Bayesian inference over PCFGs given ter-
minal strings. We presented two Markov chain
Monte Carlo algorithms (a Gibbs and a Hastings
sampling algorithm) for sampling from the posterior
distribution over parse trees given a corpus of their
yields and a Dirichlet product prior over the produc-
tion probabilities. As a component of these algo-
rithms we described an efficient dynamic program-
ming algorithm for sampling trees from a PCFG
which is useful in its own right. We used these
sampling algorithms to infer morphological analy-
ses of Sesotho verbs given their strings (a task on
which the standard Maximum Likelihood estimator
returns a trivial and linguistically uninteresting so-
lution), achieving 0.75 unlabeled morpheme f-score
and 0.54 exact word match accuracy. Thus this
is one of the few cases we are aware of in which
a PCFG estimation procedure returns linguistically
meaningful structure. We attribute this to the ability
of the Bayesian prior to prefer sparse grammars.
We expect that these algorithms will be of inter-
est to the computational linguistics community both
because a Bayesian approach to PCFG estimation is
more flexible than the Maximum Likelihood meth-
ods that currently dominate the field (c.f., the use
of a prior as a bias towards sparse solutions), and
because these techniques provide essential building
blocks for more complex models.
References
Katherine Demuth. 1992. Acquisition of Sesotho. In Dan
Slobin, editor, The Cross-Linguistic Study of Language Ac-
quisition, volume 3, pages 557?638. Lawrence Erlbaum As-
sociates, Hillsdale, N.J.
Ye Ding, Chi Yu Chan, and Charles E. Lawrence. 2005. RNA
secondary structure prediction by centroids in a Boltzmann
weighted ensemble. RNA, 11:1157?1166.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annotation
pipelines. In Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing, pages 618?
626, Sydney, Australia. Association for Computational Lin-
guistics.
Stuart Geman and Donald Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 6:721?741.
James E. Gentle. 2003. Random number generation and Monte
Carlo methods. Springer, New York, 2nd edition.
Joshua Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Harvard University. available from
http://research.microsoft.com/?joshuago/.
Dan Klein and Chris Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and con-
stituency. In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages 478?485.
K. Lari and S.J. Young. 1990. The estimation of Stochastic
Context-Free Grammars using the Inside-Outside algorithm.
Computer Speech and Language, 4(35-56).
Brian MacWhinney and Catherine Snow. 1985. The child lan-
guage data exchange system. Journal of Child Language,
12:271?296.
Noah A. Smith and Jason Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 569?576, Sydney,
Australia. Association for Computational Linguistics.
146
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 101?109,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Unsupervised Dependency Parsing
with Richer Contexts and Smoothing
William P. Headden III, Mark Johnson, David McClosky
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,mj,dmcc}@cs.brown.edu
Abstract
Unsupervised grammar induction models tend
to employ relatively simple models of syntax
when compared to their supervised counter-
parts. Traditionally, the unsupervised mod-
els have been kept simple due to tractabil-
ity and data sparsity concerns. In this paper,
we introduce basic valence frames and lexi-
cal information into an unsupervised depen-
dency grammar inducer and show how this
additional information can be leveraged via
smoothing. Our model produces state-of-the-
art results on the task of unsupervised gram-
mar induction, improving over the best previ-
ous work by almost 10 percentage points.
1 Introduction
The last decade has seen great strides in statisti-
cal natural language parsing. Supervised and semi-
supervised methods now provide highly accurate
parsers for a number of languages, but require train-
ing from corpora hand-annotated with parse trees.
Unfortunately, manually annotating corpora with
parse trees is expensive and time consuming so for
languages and domains with minimal resources it is
valuable to study methods for parsing without re-
quiring annotated sentences.
In this work, we focus on unsupervised depen-
dency parsing. Our goal is to produce a directed
graph of dependency relations (e.g. Figure 1) where
each edge indicates a head-argument relation. Since
the task is unsupervised, we are not given any ex-
amples of correct dependency graphs and only take
words and their parts of speech as input. Most
of the recent work in this area (Smith, 2006; Co-
hen et al, 2008) has focused on variants of the
The big dog barks
Figure 1: Example dependency parse.
Dependency Model with Valence (DMV) by Klein
and Manning (2004). DMV was the first unsu-
pervised dependency grammar induction system to
achieve accuracy above a right-branching baseline.
However, DMV is not able to capture some of the
more complex aspects of language. Borrowing some
ideas from the supervised parsing literature, we
present two new models: Extended Valence Gram-
mar (EVG) and its lexicalized extension (L-EVG).
The primary difference between EVG and DMV is
that DMV uses valence information to determine the
number of arguments a head takes but not their cat-
egories. In contrast, EVG allows different distri-
butions over arguments for different valence slots.
L-EVG extends EVG by conditioning on lexical in-
formation as well. This allows L-EVG to potentially
capture subcategorizations. The downside of adding
additional conditioning events is that we introduce
data sparsity problems. Incorporating more valence
and lexical information increases the number of pa-
rameters to estimate. A common solution to data
sparsity in supervised parsing is to add smoothing.
We show that smoothing can be employed in an un-
supervised fashion as well, and show that mixing
DMV, EVG, and L-EVG together produces state-of-
the-art results on this task. To our knowledge, this is
the first time that grammars with differing levels of
detail have been successfully combined for unsuper-
vised dependency parsing.
A brief overview of the paper follows. In Section
2, we discuss the relevant background. Section 3
presents how we will extend DMV with additional
101
features. We describe smoothing in an unsupervised
context in Section 4. In Section 5, we discuss search
issues. We present our experiments in Section 6 and
conclude in Section 7.
2 Background
In this paper, the observed variables will be a corpus
of n sentences of text s = s1 . . . sn, and for each
word sij an associated part-of-speech ?ij . We denote
the set of all words as Vw and the set of all parts-of-
speech as V? . The hidden variables are parse trees
t = t1 . . . tn and parameters ?? which specify a dis-
tribution over t. A dependency tree ti is a directed
acyclic graph whose nodes are the words in si. The
graph has a single incoming edge for each word in
each sentence, except one called the root of ti. An
edge from word i to word j means that word j is
an argument of word i or alternatively, word i is the
head of word j. Note that each word token may be
the argument of at most one head, but a head may
have several arguments.
If parse tree ti can be drawn on a plane above the
sentence with no crossing edges, it is called projec-
tive. Otherwise it is nonprojective. As in previous
work, we restrict ourselves to projective dependency
trees. The dependency models in this paper will be
formulated as a particular kind of Probabilistic Con-
text Free Grammar (PCFG), described below.
2.1 Tied Probabilistic Context Free Grammars
In order to perform smoothing, we will find useful a
class of PCFGs in which the probabilities of certain
rules are required to be the same. This will allow
us to make independence assumptions for smooth-
ing purposes without losing information, by giving
analogous rules the same probability.
Let G = (N ,T , S,R, ?) be a Probabilistic Con-
text Free Grammar with nonterminal symbols N ,
terminal symbols T , start symbol S ? N , set of
productions R of the form N ? ?, N ? N , ? ?
(N ? T )?. Let RN indicate the subset of R whose
left-hand sides are N . ? is a vector of length |R|, in-
dexed by productions N ? ? ? R. ?N?? specifies
the probability that N rewrites to ?. We will let ?N
indicate the subvector of ? corresponding to RN .
A tied PCFG constrains a PCFG G with a tying
relation, which is an equivalence relation over rules
that satisfies the following properties:
1. Tied rules have the same probability.
2. Rules expanding the same nonterminal are
never tied.
3. If N1 ? ?1 and N2 ? ?2 are tied then the ty-
ing relation defines a one-to-one mapping be-
tween rules in RN1 and RN2 , and we say that
N1 and N2 are tied nonterminals.
As we see below, we can estimate tied PCFGs using
standard techniques. Clearly, the tying relation also
defines an equivalence class over nonterminals. The
tying relation allows us to formulate the distribu-
tions over trees in terms of rule equivalence classes
and nonterminal equivalence classes. Suppose R? is
the set of rule equivalence classes and N? is the set
of nonterminal equivalence classes. Since all rules
in an equivalence class r? have the same probability
(condition 1), and since all the nonterminals in an
equivalence class N? ? N? have the same distribu-
tion over rule equivalence classes (condition 1 and
3), we can define the set of rule equivalence classes
R?N? associated with a nonterminal equivalence class
N? , and a vector ?? of probabilities, indexed by rule
equivalence classes r? ? R? . ??N? refers to the sub-
vector of ?? associated with nonterminal equivalence
class N? , indexed by r? ? R?N? . Since rules in the
same equivalence class have the same probability,
we have that for each r ? r?, ?r = ??r?.
Let f(t, r) denote the number of times rule r ap-
pears in tree t, and let f(t, r?) = ?r?r? f(t, r). We
see that the complete data likelihood is
P (s, t|?) = ?
r??R?
?
r?r?
?f(t,r)r =
?
r??R?
??f(t,r?)r?
That is, the likelihood is a product of multinomi-
als, one for each nonterminal equivalence class, and
there are no constraints placed on the parameters of
these multinomials besides being positive and sum-
ming to one. This means that all the standard es-
timation methods (e.g. Expectation Maximization,
Variational Bayes) extend directly to tied PCFGs.
Maximum likelihood estimation provides a point
estimate of ??. However, often we want to incorpo-
rate information about ?? by modeling its prior distri-
bution. As a prior, for each N? ? N? we will specify a
102
Dirichlet distribution over ??N? with hyperparameters
?N? . The Dirichlet has the density function:
P (??N? |?N? ) =
?(?r??R?N? ?r?)?
r??R?N? ?(?r?)
?
r??R?N?
???r??1r? ,
Thus the prior over ?? is a product of Dirichlets,which
is conjugate to the PCFG likelihood function (John-
son et al, 2007). That is, the posterior P (??|s, t, ?)
is also a product of Dirichlets, also factoring into a
Dirichlet for each nonterminal N? , where the param-
eters ?r? are augmented by the number of times rule
r? is observed in tree t:
P (??|s, t, ?) ? P (s, t|??)P (??|?)
?
?
r??R?
??f(t,r?)+?r??1r?
We can see that ?r? acts as a pseudocount of the num-
ber of times r? is observed prior to t.
To make use of this prior, we use the Variational
Bayes (VB) technique for PCFGs with Dirichlet Pri-
ors presented by Kurihara and Sato (2004). VB es-
timates a distribution over ??. In contrast, Expec-
tation Maximization estimates merely a point esti-
mate of ??. In VB, one estimates Q(t, ??), called
the variational distribution, which approximates the
posterior distribution P (t, ??|s, ?) by minimizing the
KL divergence of P from Q. Minimizing the KL
divergence, it turns out, is equivalent to maximiz-
ing a lower bound F of the log marginal likelihood
log P (s|?).
log P (s|?) ? ?
t
?
??
Q(t, ??) log P (s, t, ??|?)Q(t, ??) = F
The negative of the lower bound, ?F , is sometimes
called the free energy.
As is typical in variational approaches, Kuri-
hara and Sato (2004) make certain independence as-
sumptions about the hidden variables in the vari-
ational posterior, which will make estimating it
simpler. It factors Q(t, ??) = Q(t)Q(??) =?n
i=1 Qi(ti)
?
N??N? Q(??N? ). The goal is to recover
Q(??), the estimate of the posterior distribution over
parameters and Q(t), the estimate of the posterior
distribution over trees. Finding a local maximum of
F is done via an alternating maximization of Q(??)
and Q(t). Kurihara and Sato (2004) show that each
Q(??N? ) is a Dirichlet distribution with parameters
??r = ?r + EQ(t)f(t, r).
2.2 Split-head Bilexical CFGs
In the sections that follow, we frame various de-
pendency models as a particular variety of CFGs
known as split-head bilexical CFGs (Eisner and
Satta, 1999). These allow us to use the fast Eisner
and Satta (1999) parsing algorithm to compute the
expectations required by VB in O(m3) time (Eis-
ner and Blatz, 2007; Johnson, 2007) where m is the
length of the sentence.1
In the split-head bilexical CFG framework, each
nonterminal in the grammar is annotated with a ter-
minal symbol. For dependency grammars, these
annotations correspond to words and/or parts-of-
speech. Additionally, split-head bilexical CFGs re-
quire that each word sij in sentence si is represented
in a split form by two terminals called its left part
sijL and right part sijR. The set of these parts con-
stitutes the terminal symbols of the grammar. This
split-head property relates to a particular type of de-
pendency grammar in which the left and right depen-
dents of a head are generated independently. Note
that like CFGs, split-head bilexical CFGs can be
made probabilistic.
2.3 Dependency Model with Valence
The most successful recent work on dependency
induction has focused on the Dependency Model
with Valence (DMV) by Klein and Manning (2004).
DMV is a generative model in which the head of
the sentence is generated and then each head recur-
sively generates its left and right dependents. The
arguments of head H in direction d are generated
by repeatedly deciding whether to generate another
new argument or to stop and then generating the
argument if required. The probability of deciding
whether to generate another argument is conditioned
on H , d and whether this would be the first argument
(this is the sense in which it models valence). When
DMV generates an argument, the part-of-speech of
that argument A is generated given H and d.
1Efficiently parsable versions of split-head bilexical CFGs
for the models described in this paper can be derived using the
fold-unfold grammar transform (Eisner and Blatz, 2007; John-
son, 2007).
103
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L1H CONT | dir = L, head = H, val = 0
L?H ? HL STOP | dir = L, head = H,val = 1
L?H ? L1H CONT | dir = L, head = H, val = 1
L1H ? YA L?H Arg A | dir = L, head = H
Figure 2: Rule schema for DMV. For brevity, we omit
the portion of the grammar that handles the right argu-
ments since they are symmetric to the left (all rules are
the same except for the attachment rule where the RHS is
reversed). val ? {0, 1} indicates whether we have made
any attachments.
The grammar schema for this model is shown in
Figure 2. The first rule generates the root of the sen-
tence. Note that these rules are for ?H,A ? V? so
there is an instance of the first schema rule for each
part-of-speech. YH splits words into their left and
right components. LH encodes the stopping deci-
sion given that we have not generated any arguments
so far. L?H encodes the same decision after generat-
ing one or more arguments. L1H represents the distri-
bution over left attachments. To extract dependency
relations from these parse trees, we scan for attach-
ment rules (e.g., L1H ? YA L?H) and record that
A depends on H . The schema omits the rules for
right arguments since they are symmetric. We show
a parse of ?The big dog barks? in Figure 3.2
Much of the extensions to this work have fo-
cused on estimation procedures. Klein and Manning
(2004) use Expectation Maximization to estimate
the model parameters. Smith and Eisner (2005) and
Smith (2006) investigate using Contrastive Estima-
tion to estimate DMV. Contrastive Estimation max-
imizes the conditional probability of the observed
sentences given a neighborhood of similar unseen
sequences. The results of this approach vary widely
based on regularization and neighborhood, but often
outperforms EM.
2Note that our examples use words as leaf nodes but in our
unlexicalized models, the leaf nodes are in fact parts-of-speech.
S
Ybarks
Lbarks
L1barks
Ydog
Ldog
L1dog
YThe
LThe
TheL
RThe
TheR
L?dog
L1dog
Ybig
Lbig
bigL
Rbig
bigR
L?dog
dogL
Rdog
dogR
L?barks
barksL
Rbarks
barksR
Figure 3: DMV split-head bilexical CFG parse of ?The
big dog barks.?
Smith (2006) also investigates two techniques for
maximizing likelihood while incorporating the lo-
cality bias encoded in the harmonic initializer for
DMV. One technique, skewed deterministic anneal-
ing, ameliorates the local maximum problem by flat-
tening the likelihood and adding a bias towards the
Klein and Manning initializer, which is decreased
during learning. The second technique is structural
annealing (Smith and Eisner, 2006; Smith, 2006)
which penalizes long dependencies initially, grad-
ually weakening the penalty during estimation. If
hand-annotated dependencies on a held-out set are
available for parameter selection, this performs far
better than EM; however, performing parameter se-
lection on a held-out set without the use of gold de-
pendencies does not perform as well.
Cohen et al (2008) investigate using Bayesian
Priors with DMV. The two priors they use are the
Dirichlet (which we use here) and the Logistic Nor-
mal prior, which allows the model to capture correla-
tions between different distributions. They initialize
using the harmonic initializer of Klein and Manning
(2004). They find that the Logistic Normal distri-
bution performs much better than the Dirichlet with
this initialization scheme.
Cohen and Smith (2009), investigate (concur-
104
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L?H CONT | dir = L, head = H, val = 0
L?H ? L1H STOP | dir = L, head = H,val = 1
L?H ? L2H CONT | dir = L, head = H, val = 1
L2H ? YA L?H Arg A | dir = L, head = H,val = 1
L1H ? YA HL Arg A | dir = L, head = H,val = 0
Figure 4: Extended Valence Grammar schema. As be-
fore, we omit rules involving the right parts of words. In
this case, val ? {0, 1} indicates whether we are generat-
ing the nearest argument (0) or not (1).
rently with our work) an extension of this, the
Shared Logistic Normal prior, which allows differ-
ent PCFG rule distributions to share components.
They use this machinery to investigate smoothing
the attachment distributions for (nouns/verbs), and
for learning using multiple languages.
3 Enriched Contexts
DMV models the distribution over arguments iden-
tically without regard to their order. Instead, we
propose to distinguish the distribution over the argu-
ment nearest the head from the distribution of sub-
sequent arguments. 3
Consider the following changes to the DMV
grammar (results shown in Figure 4). First, we will
introduce the rule L2H ? YA L?H to denote the deci-
sion of what argument to generate for positions not
nearest to the head. Next, instead of having L?H ex-
pand to HL or L1H , we will expand it to L1H (attach
to nearest argument and stop) or L2H (attach to non-
nearest argument and continue). We call this the Ex-
tended Valence Grammar (EVG).
As a concrete example, consider the phrase ?the
big hungry dog? (Figure 5). We would expect that
distribution over the nearest left argument for ?dog?
to be different than farther left arguments. The fig-
3McClosky (2008) explores this idea further in an un-
smoothed grammar.
.
.
.
Ldog
L1dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
L?dog
dogL
.
.
.
Ldog
L?dog
L2dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
dogL
Figure 5: An example of moving from DMV to EVG
for a fragment of ?The big dog.? Boxed nodes indicate
changes. The key difference is that EVG distinguishes
between the distributions over the argument nearest the
head (big) from arguments farther away (The).
ure shows that EVG allows these two distributions to
be different (nonterminals L2dog and L1dog) whereas
DMV forces them to be equivalent (both use L1dog as
the nonterminal).
3.1 Lexicalization
All of the probabilistic models discussed thus far
have incorporated only part-of-speech information
(see Footnote 2). In supervised parsing of both de-
pendencies and constituency, lexical information is
critical (Collins, 1999). We incorporate lexical in-
formation into EVG (henceforth L-EVG) by extend-
ing the distributions over argument parts-of-speech
A to condition on the head word h in addition to the
head part-of-speech H , direction d and argument po-
sition v. The argument word a distribution is merely
conditioned on part-of-speech A; we leave refining
this model to future work.
In order to incorporate lexicalization, we extend
the EVG CFG to allow the nonterminals to be anno-
tated with both the word and part-of-speech of the
head. We first remove the old rules YH ? LH RH
for each H ? V? . Then we mark each nonter-
minal which is annotated with a part-of-speech as
also annotated with its head, with a single excep-
tion: YH . We add a new nonterminal YH,h for each
H ? V? , h ? Vw, and the rules YH ? YH,h and
YH,h ? LH,h RH,h. The rule YH ? YH,h cor-
responds to selecting the word, given its part-of-
speech.
105
4 Smoothing
In supervised estimation one common smoothing
technique is linear interpolation, (Jelinek, 1997).
This section explains how linear interpolation can
be represented using a PCFG with tied rule proba-
bilities, and how one might estimate smoothing pa-
rameters in an unsupervised framework.
In many probabilistic models it is common to esti-
mate the distribution of some event x conditioned on
some set of context information P (x|N(1) . . . N(k))
by smoothing it with less complicated condi-
tional distributions. Using linear interpolation
we model P (x|N(1) . . . N(k)) as a weighted aver-
age of two distributions ?1P1(x|N(1), . . . , N(k)) +
?2P2(x|N(1), . . . , N(k?1)), where the distribution
P2 makes an independence assumption by dropping
the conditioning event N(k).
In a PCFG a nonterminal N can encode a collec-
tion of conditioning events N(1) . . . N(k), and ?N de-
termines a distribution conditioned on N(1) . . . N(k)
over events represented by the rules r ? RN . For
example, in EVG the nonterminal L1NN encodes
three separate pieces of conditioning information:
the direction d = left , the head part-of-speech
H = NN , and the argument position v = 0;
?L1NN?YJJ NNL represents the probability of gener-
ating JJ as the first left argument of NN . Sup-
pose in EVG we are interested in smoothing P (A |
d,H, v) with a component that excludes the head
conditioning event. Using linear interpolation, this
would be:
P (A | d,H, v) = ?1P1(A | d,H, v)+?2P2(A | d, v)
We will estimate PCFG rules with linearly interpo-
lated probabilities by creating a tied PCFG which
is extended by adding rules that select between the
main distribution P1 and the backoff distribution P2,
and also rules that correspond to draws from those
distributions. We will make use of tied rule proba-
bilities to make the independence assumption in the
backoff distribution.
We still use the original grammar to parse the sen-
tence. However, we estimate the parameters in the
extended grammar and then translate them back into
the original grammar for parsing.
More formally, suppose B ? N is a set of non-
terminals (called the backoff set) with conditioning
events N(1) . . . N(k?1) in common (differing in a
conditioning event N(k)), and with rule sets of the
same cardinality. If G is our model?s PCFG, we can
define a new tied PCFG G? = (N ?,T , S,R?, ?),
where N ? = N ? {N b? | N ? B, ? ? {1, 2}},
meaning for each nonterminal N in the backoff
set we add two nonterminals N b1 , N b2 represent-
ing each distribution P1 and P2. The new rule
set R? = (?N?N ?R?N ) where for all N ? B
rule set R?N =
{
N ? N b? | ? ? {1, 2}}, mean-
ing at N in G? we decide which distribution P1, P2
to use; and for N ? B and ? ? {1, 2} ,
R?Nb? =
{
N b? ? ? | N ? ? ? RN
}
indicating a
draw from distribution P?. For nonterminals N 6? B,
R?N = RN . Finally, for each N,M ? B we
specify a tying relation between the rules in R?Nb2
and R?Mb2 , grouping together analogous rules. This
has the effect of making an independence assump-
tion about P2, namely that it ignores the condition-
ing event N(k), drawing from a common distribution
each time a nonterminal N b2 is rewritten.
For example, in EVG to smooth P (A = DT |
d = left ,H = NN , v = 0) with P2(A = DT |
d = left , v = 0) we define the backoff set to
be
{
L1H | H ? V?
}
. In the extended grammar we
define the tying relation to form rule equivalence
classes by the argument they generate, i.e. for each
argument A ? V? , we have a rule equivalence class{
L1b2H ? YA HL | H ? V?
}
.
We can see that in grammar G? each N ? B even-
tually ends up rewriting to one of N ?s expansions ?
in G. There are two indirect paths, one through N b1
and one through N b2 . Thus this defines the proba-
bility of N ? ? in G, ?N??, as the probability of
rewriting N as ? in G? via N b1 and N b2 . That is:
?N?? = ?N?Nb1?Nb1?? + ?N?Nb2?Nb2??
The example in Figure 6 shows the probability that
L1dog rewrites to Ybig dogL in grammar G.
Typically when smoothing we need to incorporate
the prior knowledge that conditioning events that
have been seen fewer times should be more strongly
smoothed. We accomplish this by setting the Dirich-
let hyperparameters for each N ? N b1 , N ? N b2
decision to (K, 2K), where K = |RNb1 | is the num-
ber of rewrite rules for A. This ensures that the
model will only start to ignore the backoff distribu-
106
PG
0
B
B
@
L1dog
Ybig dogL
1
C
C
A
= PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b1dog
Ybig dogL
1
C
C
C
C
C
C
C
A
+ PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b2dog
Ybig dogL
1
C
C
C
C
C
C
C
A
Figure 6: Using linear interpolation to smooth L1dog ?
Ybig dogL: The first component represents the distri-
bution fully conditioned on head dog, while the second
component represents the distribution ignoring the head
conditioning event. This later is accomplished by tying
the rule L1b2dog ? Ybig dogL to, for instance, L1b2cat ?
Ybig catL, L1b2fish ? Ybig fishL etc.
tion after having seen a sufficiently large number of
training examples. 4
4.1 Smoothed Dependency Models
Our first experiments examine smoothing the dis-
tributions over an argument in the DMV and EVG
models. In DMV we smooth the probability of argu-
ment A given head part-of-speech H and direction d
with a distribution that ignores H . In EVG, which
conditions on H , d and argument position v we back
off two ways. The first is to ignore v and use back-
off conditioning event H, d. This yields a backoff
distribution with the same conditioning information
as the argument distribution from DMV. We call this
EVG smoothed-skip-val.
The second possibility is to have the backoff
distribution ignore the head part-of-speech H and
use backoff conditioning event v, d. This assumes
that arguments share a common distribution across
heads. We call this EVG smoothed-skip-head. As
we see below, backing off by ignoring the part-of-
speech of the head H worked better than ignoring
the argument position v.
For L-EVG we smooth the argument part-of-
speech distribution (conditioned on the head word)
with the unlexicalized EVG smoothed-skip-head
model.
5 Initialization and Search issues
Klein and Manning (2004) strongly emphasize the
importance of smart initialization in getting good
performance from DMV. The likelihood function is
full of local maxima and different initial parameter
values yield vastly different quality solutions. They
offer what they call a ?harmonic initializer? which
4We set the other Dirichlet hyperparameters to 1.
initializes the attachment probabilities to favor ar-
guments that appear more closely in the data. This
starts EM in a state preferring shorter attachments.
Since our goal is to expand the model to incor-
porate lexical information, we want an initializa-
tion scheme which does not depend on the details
of DMV. The method we use is to create M sets of
B random initial settings and to run VB some small
number of iterations (40 in all our experiments) for
each initial setting. For each of the M sets, the
model with the best free energy of the B runs is
then run out until convergence (as measured by like-
lihood of a held-out data set); the other models are
pruned away. In this paper we use B = 20 and
M = 50.
For the bth setting, we draw a random sample
from the prior ??(b). We set the initial Q(t) =
P (t|s, ??(b)) which can be calculated using the
Expectation-Maximization E-Step. Q(??) is then ini-
tialized using the standard VB M-step.
For the Lexicalized-EVG, we modify this proce-
dure slightly, by first running MB smoothed EVG
models for 40 iterations each and selecting the best
model in each cohort as before; each L-EVG dis-
tribution is initialized from its corresponding EVG
distribution. The new P (A|h,H, d, v) distributions
are set initially to their corresponding P (A|H, d, v)
values.
6 Results
We trained on the standard Penn Treebank WSJ cor-
pus (Marcus et al, 1993). Following Klein and Man-
ning (2002), sentences longer than 10 words after
removing punctuation are ignored. We refer to this
variant as WSJ10. Following Cohen et al (2008),
we train on sections 2-21, used 22 as a held-out de-
velopment corpus, and present results evaluated on
section 23. The models were all trained using Varia-
tional Bayes, and initialized as described in Section
5. To evaluate, we follow Cohen et al (2008) in us-
ing the mean of the variational posterior Dirichlets
as a point estimate ???. For the unsmoothed models
we decode by selecting the Viterbi parse given ???, or
argmaxtP (t|s, ???).
For the smoothed models we find the Viterbi parse
of the unsmoothed CFG, but use the smoothed prob-
abilities. We evaluate against the gold standard
107
Model Variant Dir. Acc.
DMV harmonic init 46.9*
DMV random init 55.7 (8.0)
DMV log normal-families 59.4*
DMV shared log normal-families 62.4?
DMV smoothed 61.2 (1.2)
EVG random init 53.3 (7.1)
EVG smoothed-skip-val 62.1 (1.9)
EVG smoothed-skip-head 65.0 (5.7)
L-EVG smoothed 68.8 (4.5)
Table 1: Directed accuracy (DA) for WSJ10, section 23.
*,? indicate results reported by Cohen et al (2008), Co-
hen and Smith (2009) respectively. Standard deviations
over 10 runs are given in parentheses
dependencies for section 23, which were extracted
from the phrase structure trees using the standard
rules by Yamada and Matsumoto (2003). We mea-
sure the percent accuracy of the directed dependency
edges. For the lexicalized model, we replaced all
words that were seen fewer than 100 times with
?UNK.? We ran each of our systems 10 times, and
report the average directed accuracy achieved. The
results are shown in Table 1. We compare to work
by Cohen et al (2008) and Cohen and Smith (2009).
Looking at Table 1, we can first of all see the
benefit of randomized initialization over the har-
monic initializer for DMV. We can also see a large
gain by adding smoothing to DMV, topping even
the logistic normal prior. The unsmoothed EVG ac-
tually performs worse than unsmoothed DMV, but
both smoothed versions improve even on smoothed
DMV. Adding lexical information (L-EVG) yields a
moderate further improvement.
As the greatest improvement comes from moving
to model EVG smoothed-skip-head, we show in Ta-
ble 2 the most probable arguments for each val, dir,
using the mean of the appropriate variational Dirich-
let. For d = right, v = 1, P (A|v, d) largely seems
to acts as a way of grouping together various verb
types, while for d = left, v = 0 the model finds
that nouns tend to act as the closest left argument.
Dir,Val Arg Prob Dir,Val Arg Prob
left, 0 NN 0.65 right, 0 NN 0.26
NNP 0.18 RB 0.23
DT 0.12 NNS 0.12
IN 0.11
left, 1 CC 0.35 right, 1 IN 0.78
RB 0.27
IN 0.18
Table 2: Most likely arguments given valence and direc-
tion, according to smoothing distributionP (arg|dir, val)
in EVG smoothed-skip-head model with lowest free en-
ergy.
7 Conclusion
We present a smoothing technique for unsupervised
PCFG estimation which allows us to explore more
sophisticated dependency grammars. Our method
combines linear interpolation with a Bayesian prior
that ensures the backoff distribution receives proba-
bility mass. Estimating the smoothed model requires
running the standard Variational Bayes on an ex-
tended PCFG. We used this technique to estimate a
series of dependency grammars which extend DMV
with additional valence and lexical information. We
found that both were helpful in learning English de-
pendency grammars. Our L-EVG model gives the
best reported accuracy to date on the WSJ10 corpus.
Future work includes using lexical information
more deeply in the model by conditioning argument
words and valence on the lexical head. We suspect
that successfully doing so will require using much
larger datasets. We would also like to explore us-
ing our smoothing technique in other models such
as HMMs. For instance, we could do unsupervised
HMM part-of-speech induction by smooth a tritag
model with a bitag model. Finally, we would like to
learn the parts-of-speech in our dependency model
from text and not rely on the gold-standard tags.
Acknowledgements
This research is based upon work supported by
National Science Foundation grants 0544127 and
0631667 and DARPA GALE contract HR0011-06-
2-0001. We thank members of BLLIP for their feed-
back.
108
References
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL-HLT 2009.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems 21.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Jason Eisner and John Blatz. 2007. Program transforma-
tions for optimization of parsing algorithms and other
weighted logic programs. In Proceedings of the 11th
Conference on Formal Grammar.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of ACL 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of ACL 2007.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of ACL 2002.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, July.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational bayesian approach to probabilis-
tics context-free grammars. In IJCNLP 2004 Work-
shop Beyond Shallow Analyses.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky. 2008. Modeling valence effects in un-
supervised grammar induction. Technical Report CS-
09-01, Brown University, Providence, RI, USA.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In International Joint Conference on Artificial
Intelligence Workshop on Grammatical Inference Ap-
plications.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
In Proceedings of COLING-ACL 2006.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Department of Computer
Science, Johns Hopkins University.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
In Proceedings of the International Workshop on Pars-
ing Technologies.
109
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Structured Generative Models for Unsupervised Named-Entity Clustering
Micha Elsner, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec,mj}@cs.brown.edu
Abstract
We describe a generative model for clustering
named entities which also models named en-
tity internal structure, clustering related words
by role. The model is entirely unsupervised;
it uses features from the named entity itself
and its syntactic context, and coreference in-
formation from an unsupervised pronoun re-
solver. The model scores 86% on the MUC-7
named-entity dataset. To our knowledge, this
is the best reported score for a fully unsuper-
vised model, and the best score for a genera-
tive model.
1 Introduction
Named entity clustering is a classic task in NLP, and
one for which both supervised and semi-supervised
systems have excellent performance (Mikheev et al,
1998; Chinchor, 1998). In this paper, we describe a
fully unsupervised system (using no ?seed rules? or
initial heuristics); to our knowledge this is the best
such system reported on the MUC-7 dataset. In ad-
dition, the model clusters the words which appear
in named entities, discovering groups of words with
similar roles such as first names and types of orga-
nization. Finally, the model defines a notion of con-
sistency between different references to the same en-
tity; this component of the model yields a significant
increase in performance.
The main motivation for our system is the re-
cent success of unsupervised generative models for
coreference resolution. The model of Haghighi
and Klein (2007) incorporated a latent variable for
named entity class. They report a named entity score
of 61.2 percent, well above the baseline of 46.4, but
still far behind existing named-entity systems.
We suspect that better models for named entities
could aid in the coreference task. The easiest way to
incorporate a better model is simply to run a super-
vised or semi-supervised system as a preprocess. To
perform joint inference, however, requires an unsu-
pervised generative model for named entities. As far
as we know, this work is the best such model.
Named entities also pose another problem with
the Haghighi and Klein (2007) coreference model;
since it models only the heads of NPs, it will fail to
resolve some references to named entities: (?Ford
Motor Co.?, ?Ford?), while erroneously merging
others: (?Ford Motor Co.?, ?Lockheed Martin Co.?).
Ng (2008) showed that better features for match-
ing named entities? exact string match and an ?alias
detector? looking for acronyms, abbreviations and
name variants? improve the model?s performance
substantially. Yet building an alias detector is non-
trivial (Uryupina, 2004). English speakers know that
?President Clinton? is the same person as ?Bill Clin-
ton? , not ?President Bush?. But this cannot be im-
plemented by simple substring matching. It requires
some concept of the role of each word in the string.
Our model attempts to learn this role information by
clustering the words within named entities.
2 Related Work
Supervised named entity recognition now performs
almost as well as human annotation in English
(Chinchor, 1998) and has excellent performance on
other languages (Tjong Kim Sang and De Meul-
der, 2003). For a survey of the state of the art,
164
see Nadeau and Sekine (2007). Of the features
we explore here, all but the pronoun information
were introduced in supervised work. Supervised ap-
proaches such as Black et al (1998) have used clus-
tering to group together different nominals referring
to the same entity in ways similar to the ?consis-
tency? approach outlined below in section 3.2.
Semi-supervised approaches have also achieved
notable success on the task. Co-training (Riloff and
Jones, 1999; Collins and Singer, 1999) begins with
a small set of labeling heuristics and gradually adds
examples to the training data. Various co-training
approaches presented in Collins and Singer (1999)
all score about 91% on a dataset of named entities;
the inital labels were assigned using 7 hand-written
seed rules. However, Collins and Singer (1999)
show that a mixture-of-naive-Bayes generative clus-
tering model (which they call an EM model), initial-
ized with the same seed rules, performs much more
poorly at 83%.
Much later work (Evans, 2003; Etzioni et al,
2005; Cucerzan, 2007; Pasca, 2004) relies on the
use of extremely large corpora which allow very
precise, but sparse features. For instance Etzioni
et al (2005) and Pasca (2004) use web queries to
count occurrences of ?cities such as X? and simi-
lar phrases. Although our research makes use of a
fairly large amount of data, our method is designed
to make better use of relatively common contextual
features, rather than searching for high-quality se-
mantic features elsewhere.
Models of the internal structure of names have
been used for cross-document coreference (Li et al,
2004; Bhattacharya and Getoor, 2006) and a goal in
their own right (Charniak, 2001). Li et al (2004)
take named entity classes as a given, and develops
both generative and discriminative models to detect
coreference between members of each class. Their
generative model designates a particular mention of
a name as a ?representative? and generates all other
mentions from it according to an editing process.
Bhattacharya and Getoor (2006) operates only on
authors of scientific papers. Their model accounts
for a wider variety of name variants than ours, in-
cluding misspellings and initials. In addition, they
confirm our intuition that Gibbs sampling for infer-
ence has insufficient mobility; rather than using a
heuristic algorithm as we do (see section 3.5), they
use a data-driven block sampler. Charniak (2001)
uses a Markov chain to generate 6 different com-
ponents of people?s names, again assuming that the
class of personal names can be pre-distinguished us-
ing a name list. He infers coreference relationships
between similar names appearing in the same docu-
ment, using the same notion of consistency between
names as our model. As with our model, the clusters
found are relatively good, although with some mis-
takes even on frequent items (for example, ?John? is
sometimes treated as a descriptor like ?Secretary?).
3 System Description
Like Collins and Singer (1999), we assume that the
named entities have already been correctly extracted
from the text, and our task is merely to label them.
We assume that all entities fit into one of the three
MUC-7 categories, LOC (locations), ORG (organi-
zations), and PER (people). This is an oversimplifi-
cation; Collins and Singer (1999) show that about
12% of examples do not fit into these categories.
However, while using the MUC-7 data, we have no
way to evaluate on such examples.
As a framework for our models, we adopt adap-
tor grammars (Johnson et al, 2007), a framework
for non-parametric Bayesian inference over context-
free grammars. Although our system does not re-
quire the full expressive power of PCFGs, the adap-
tor grammar framework allows for easy develop-
ment of structured priors, and supplies a flexible
generic inference algorithm. An adaptor grammar is
a hierarchical Pitman-Yor process (Pitman and Yor,
1997). The grammar has two parts: a base PCFG
and a set of adapted nonterminals. Each adapted
nonterminal is a Pitman-Yor process which expands
either to a previously used subtree or to a sample
from the base PCFG. The end result is a posterior
distribution over PCFGs and over parse trees for
each example in our dataset.
Each of our models is an adaptor grammar based
on a particular base PCFG where the top nonter-
minal of each parse tree represents a named entity
class.
3.1 Core NP Model
We begin our analysis by reducing each named-
entity reference to the contiguous substring of
165
ROOT ?NE 0|NE 1|NE 2
NE 0 ?(NE 00)(NE 10)(NE 20)(NE 30)(NE 40)
?NE 00 ?Words
?Words ?Word (Words)
Word ?Bill . . .
Figure 1: Part of the grammar for core phrases. (Paren-
theses) mark optional nonterminals. *Starred nontermi-
nals are adapted.
proper nouns which surrounds its head, which we
call the core (Figure 1). To analyze the core, we use
a grammar with three main symbols (NEx), one for
each named entity class x. Each class has an asso-
ciated set of lexical symbols, which occur in a strict
order (NE ix is the ith symbol for class x). We can
think of the NE i as the semantic parts of a proper
name; for people, NE 0PER might generate titles and
NE 1PER first names. Each NE i is adapted, and can
expand to any string of words; the ability to gen-
erate multiple words from a single symbol is use-
ful both because it can learn to group collocations
like ?New York? and because it allows the system to
handle entities longer than four words. However, we
set the prior on multi-word expansions very low, to
avoid degenerate solutions where most phrases are
analyzed with a single symbol. The system learns
a separate probability for each ordered subset of the
NE i (for instance the rule NE 0 ? NE 00 NE 20 NE 40),
so that it can represent constraints on possible refer-
ences; for instance, a last name can occur on its own,
but not a title.
3.2 Consistency Model
This system captures some of our intuitions about
core phrases, but not all: our representation for ?Bill
Clinton? does not share any information with ?Presi-
dent Bill Clinton? except the named-entity class. To
remedy this, we introduce a set of ?entity? nonter-
minals Ek, which enforce a weak notion of consis-
tency. We follow Charniak (2001) in assuming that
two names are consistent (can be references to the
same entity) if they do not have different expansions
for any lexical symbol. In other words, a particu-
lar entity EPER,Clinton has a title E0PER,Clinton =
ROOT ?NE 0|NE 1|NE 2
NE 0 ?E00|E01 . . . E0k
E00 ?(E000)(E100)(E200)(E300)(E400)
? ? E000 ?NE 00
?NE 00 ?Words . . .
Figure 2: Part of the consistency-enforcing grammar for
core phrases. There are an infinite number of entities
Exk, all with their own lexical symbols. Each lexical
symbol Eixk expands to a single NE ix.
?President?, a first name E1PER,Clinton = ?Bill? etc.
These are generated from the class-specific distribu-
tions, for instance E0PER,Clinton ? E0PER, which
we intend to be a distribution over titles in general.
The resulting grammar is shown in Figure 2; the
prior parameters for the entity-specific symbols Eixk
are fixed so that, with overwhelming probability,
only one expansion occurs. We can represent any
fixed number of entities Ek with a standard adap-
tor grammar, but since we do not know the correct
number, we must extend the adaptor model slightly
to allow for an unbounded number. We generate the
Ek from a Chinese Restaurant process prior. (Gen-
eral grammars with infinite numbers of nonterminals
were studied by (Liang et al, 2007b)).
3.3 Modifiers, Prepositions and Pronouns
Next, we introduce two types of context information
derived from Collins and Singer (1999): nominal
modifiers and prepositional information. A nominal
modifier is either the head of an appositive phrase
(?Maury Cooper, a vice president?) or a non-proper
prenominal (?spokesman John Smith?)1. If the en-
tity is the complement of a preposition, we extract
the preposition and the head of the governing NP (?a
federally funded sewage plant in Georgia?). These
are added to the grammar at the named-entity class
level (separated from the core by a special punctua-
tion symbol).
Finally, we add information about pronouns and
wh-complementizers (Figure 3). Our pronoun infor-
mation is derived from an unsupervised coreference
algorithm which does not use named entity informa-
1We stem modifiers with the Porter stemmer.
166
ROOT ?Modifiers0 # NE 0 #
Prepositions0 # Pronouns0 #
. . .
Pronouns0 ?Pronoun0 Pronouns0
Pronouns0 ?
Pronoun0 ?pers|loc|org |any
pers ?i |he|she|who|me . . .
loc ?where|which|it |its
org ?which|it |they |we . . .
Figure 3: A fragment of the full grammar. The symbol
# represents punctuation between different feature types.
The prior for class 0 is concentrated around personal pro-
nouns, although other types are possible.
tion (Charniak and Elsner, 2009). This algorithm
uses EM to learn a generative model with syntactic,
number and gender parameters. Like Haghighi and
Klein (2007), we give our model information about
the basic types of pronouns in English. By setting
up the base grammar so that each named-entity class
prefers to associate to a single type of pronoun, we
can also determine the correspondence between our
named-entity symbols and the actual named-entity
labels? for the models without pronoun information,
this matching is arbitrary and must be inferred dur-
ing the evaluation process.
3.4 Data Preparation
To prepare data for clustering with our system, we
first parse it with the parser of Charniak and Johnson
(2005). We then annotate pronouns with Charniak
and Elsner (2009). For the evaluation set, we use the
named entity data from MUC-7. Here, we extract
all strings in <ne> tags and determine their cores,
plus any relevant modifiers, governing prepositions
and pronouns, by examining the parse trees. In addi-
tion, we supply the system with additional data from
the North American News Corpus (NANC). Here
we extract all NPs headed by proper nouns.
We then process our data by merging all exam-
ples with the same core; some merged examples
from our dataset are shown in Figure 4. When two
examples are merged, we concatenate their lists of
attack airlift airlift rescu # wing # of-commander
of-command with-run # #
# air-india # # #
# abels # # it #
# gaudreau # # they he #
# priddy # # he #
spokesman bird bird bird director bird ford clin-
ton director bird # johnson # before-hearing
to-happened of-cartoon on-pressure under-medicare
to-according to-allied with-stuck of-government of-
photographs of-daughter of-photo for-embarrassing
under-instituted about-allegations for-worked
before-hearing to-secretary than-proposition of-
typical # he he his he my himself his he he he he i
he his his i i i he his #
Figure 4: Some merged examples from an input file. (#
separates different feature types.)
modifiers, prepositions and pronouns (capping the
length of each list at 20 to keep inference tractable).
For instance, ?air-india? has no features outside the
core, while ?wing? has some nominals (?attack?
&c.) and some prepositions (?commander-of? &c.).
This merging is useful because it allows us to do in-
ference based on types rather than tokens (Goldwa-
ter et al, 2006). It is well known that, to interpo-
late between types and tokens, Hierarchical Dirich-
let Processes (including adaptor grammars) require
a deeper hierarchy, which slows down inference and
reduces the mobility of sampling schemes. By merg-
ing examples, we avoid using this more complicated
model. Each merged example also represents many
examples from the training data, so we can summa-
rize features (such as modifiers) observed through-
out a large input corpus while keeping the size of
our input file small.
To create an input file, we first add all the MUC-
7 examples. We then draw additional examples
from NANC, ranking them by how many features
they have, until we reach a specified number (larger
datasets take longer, but without enough data, results
tend to be poor).
3.5 Inference
Our implementation of adaptor grammars is a mod-
ified version of the Pitman-Yor adaptor grammar
167
sampler2, altered to deal with the infinite number of
entities. It carries out inference using a Metropolis-
within-Gibbs algorithm (Johnson et al, 2007), in
which it repeatedly parses each input line using the
CYK algorithm, samples a parse, and proposes this
as the new tree.
To do Gibbs sampling for our consistency-
enforcing model, we would need to sample a parse
for an example from the posterior over every pos-
sible entity. However, since there are thousands of
entities (the number grows roughly linearly with the
number of merged examples in the data file), this is
not tractable. Instead, we perform a restricted Gibbs
sampling search, where we enumerate the posterior
only for entities which share a word in their core
with the example in question. In fact, if the shared
word is very common (occuring in more than .001 of
examples), we compute the posterior for that entity
only .05 of the time3. These restrictions mean that
we do not compute the exact posterior. In particular,
the actual model allows entities to contain examples
with no words in common, but our search procedure
does not explore these solutions.
For our model, inference with the Gibbs algo-
rithm seems to lack mobility, sometimes falling into
very poor local minima from which it does not seem
to escape. This is because, if there are several ref-
erences to the same named entity with slightly dif-
ferent core phrases, once they are all assigned to
the wrong class, it requires a low-probability se-
ries of individual Gibbs moves to pull them out.
Similarly, the consistency-enforcing model gener-
ally does not fully cluster references to common en-
tities; there are usually several ?Bill Clinton? clus-
ters which it would be best to combine, but the se-
quence of moves that does so is too improbable. The
data-merging process described above is one attempt
to improve mobility by reducing the number of du-
plicate examples. In addition, we found that it was a
better use of CPU time to run multiple samplers with
different initialization than to perform many itera-
tions. In the experiments below, we use 20 chains,
initializing with 50 iterations without using consis-
tency, then 50 more using the consistency model,
and evaluate the last sample from each. We discard
2Available at http://www.cog.brown.edu/ mj/Software.htm
3We ignore the corresponding Hastings correction, as in
practice it leads to too many rejections.
the 10 samples with worst log-likelihood and report
the average score for the other 10.
3.6 Parameters
In addition to the base PCFG itself, the system re-
quires a few hyperparameter settings: Dirichlet pri-
ors for the rule weights of rules in the base PCFG.
Pitman-Yor parameters for the adapted nonterminals
are sampled from vague priors using a slice sam-
pler (Neal, 2003). The prior over core words was
set to the uniform distribution (Dirichlet 1.0) and the
prior for all modifiers, prepositions and pronouns to
a sparse value of .01. Beyond setting these param-
eters to a priori reasonable values, we did not opti-
mize them. To encourage the system to learn that
some lexical symbols were more common than oth-
ers, we set a sparse prior over expansions to sym-
bols4. There are two really important hyperparame-
ters: an extremely biased prior on class-to-pronoun-
type probabilities (1000 for the desired class, .0001
for everything else), and a prior of .0001 for the
Word ?Word Words rule to discourage symbols
expanding to multiword strings.
4 Experiments
We performed experiments on the named entity
dataset from MUC-7 (Chinchor, 1998), using the
training set as development data and the formal test
set as test data. The development set has 4936
named entities, of which 1575 (31.9%) are locations,
2096 (42.5%) are organizations and 1265 (25.6%)
people. The test set has 4069 named entities, 1321
(32.5%) locations, 1862 (45.8%) organizations and
876 (21.5%) people5. We use a baseline which
gives all named entities the same label; this label is
mapped to ?organization?.
In most of our experiments, we use an input file of
40000 lines. For dev experiments, the labeled data
contributes 1585 merged examples; for test experi-
ments, only 1320. The remaining lines are derived
4Expansions that used only the middle three symbols
NE1,2,3x got a prior of .005, expansions whose outermost sym-
bol was NE0,4x got .0025, and so forth. This is not so impor-
tant for our final system, which has only 5 symbols, but was
designed during development to handle systems with up to 10
symbols.
510 entities are labeled location|organization; since this
fraction of the dataset is insignificant we score them as wrong.
168
Model Accuracy
Baseline (All Org) 42.5
Core NPs (no consistency) 45.5
Core NPs (consistency) 48.5
Context Features 83.3
Pronouns 87.1
Table 1: Accuracy of various models on development
data.
Model Accuracy
Baseline (All Org) 45.8
Pronouns 86.0
Table 2: Accuracy of the final model on test data.
using the process described in section 3.4 from 5
million words of NANC.
To evaluate our results, we map our three induced
labels to their corresponding gold label, then count
the overlap; as stated, this mapping is predictably
encoded in the prior when we use the pronoun fea-
tures. Our experimental results are shown in Table
1. All models perform above baseline, and all fea-
tures contribute significantly to the final result. Test
results for our final model are shown in Table 2.
A confusion matrix for our highest-likelihood test
solution is shown as Figure 5. The highest confusion
class is ?organization?, which is confused most often
with ?location? but also with ?person?. ?location? is
likewise confused with ?organization?. ?person? is
the easiest class to identify? we believe this explains
the slight decline in performance from dev to test,
since dev has proportionally more people.
Our mapping from grammar symbols to words ap-
pears in Table 3; the learned prepositional and mod-
ifier information is in Table 4. Overall the results
are good, but not perfect; for instance, the Pers
states are mostly interpretable as a sequence of ti-
tle - first name - middle name or initial - last name -
loc org per
LOC 1187 97 37
ORG 223 1517 122
PER 36 20 820
Figure 5: Confusion matrix for highest-likelihood test
run. Gold labels in CAPS, induced labels italicized. Or-
ganizations are most frequently confused.
last name or post-title (similar to (Charniak, 2001)).
The organization symbols tend to put nationalities
and other modifiers first, and end with institutional
types like ?inc.? or ?center?, although there is a sim-
ilar (but smaller) cluster of types at Org2, suggest-
ing the system has incorrectly found two analyses
for these names. Location symbols seem to put en-
tities with a single, non-analyzable name into Loc2,
and use symbols 0, 1 and 3 for compound names.
Loc4 has been recruited for time expressions, since
our NANC dataset includes many of these, but we
failed to account for them in the model. Since
they appear in a single class here, we are optimistic
that they could be clustered separately if another
class and some appropriate features were added to
the prior. Some errors do appear (?supreme court?
and ?house? as locations, ?minister? and ?chairman?
as middle names, ?newt gingrich? as a multiword
phrase). The table also reveals an unforeseen issue
with the parser: it tends to analyze the dateline be-
ginning a news story along with the following NP
(?WASHINGTON Bill Clinton said...?). Thus com-
mon datelines (?washington?, ?new york? and ?los
angeles?) appear in state 0 for each class.
5 Discussion
As stated above, we aim to build an unsupervised
generative model for named entity clustering, since
such a model could be integrated with unsupervised
coreference models like Haghighi and Klein (2007)
for joint inference. To our knowledge, the closest
existing system to such a model is the EM mix-
ture model used as a baseline in Collins and Singer
(1999). Our system improves on this EM system
in several ways. While they initialize with minimal
supervision in the form of 7 seed heuristics, ours is
fully unsupervised. Their results cover only exam-
ples which have a prepositional or modifier feature;
we adopt these features from their work, but label
all entities in the predefined test set, including those
that appear without these features. Finally, as dis-
cussed, we find the ?person? category to be the eas-
iest to label. 33% of the test items in Collins and
Singer (1999) were people, as opposed to 21% of
ours. However, even without the pronoun features,
that is, using the same feature set, our system scores
equivalently to the EM model, at 83% (this score is
169
Pers0 Pers1 Pers2 Pers3 Pers4
rep. john (767) minister brown jr.
sen. (256) robert (495) j. smith (97) a
washington david john (242) b smith (111)
dr. michael l. johnson iii
los angeles james chairman newt gingrich williams
senate president e. king wilson
house richard m. miller brown
new york william (317) william (173) kennedy clinton
president sen. (236) robert (155) martin simpson
republican george r. davis b
Org0 Org1 Org2 Org3 Org4
american (137) national university research association
washington american (182) inc. (166) medical center
washington the new york corp. (156) news inc. (257)
national international (136) college health corp. (252)
first public institute (87) services co.
los angeles united group communications committee
new house hospital development institute
royal federal museum policy council
british home press affairs fund
california world international (61) defense act
Loc0 Loc1 Loc2 Loc3 Loc4
washington (92) the texas county monday
los angeles st. new york city thursday
south new washington (22) beach river (57)
north national (69) united states valley tuesday
old east (65) baltimore island wednesday
grand mount california river (71) hotel
black fort capitol park friday
west (22) west (56) christmas bay hall
east (21) lake bosnia house center
haiti great san juan supreme court building
Table 3: 10 most common words for each grammar symbol. Words which appear in multiple places have observed
counts indicated in parentheses.
170
Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-mod
according-to (1044) director president-of $ university-of calif.
played-by spokesman chairman-of giant city-of newspap[er]
directed-by leader director-of opposit[e] from-to state
led-by presid[ent] according-to (786) group town-of downtown
meeting-with attorney professor-at pp state-of n.y.
from-to candid[ate] head-of compan[y] center-in warrant
met-with lawyer department-of journal out-of va.
letter-to chairman member-of firm is-in fla.
secretary-of counsel members-of state house-of p.m.
known-as actor spokesman-for agenc[y] known-as itself
Table 4: 10 most common prepositional and modifier features for each named entity class. Modifiers were Porter-
stemmed; for clarity a reconstructed stem is shown in brackets.
on dev, 25% people). When the pronoun features are
added, our system?s performance increases to 86%,
significantly better than the EM system.
One motivation for our use of a structured model
which defined a notion of consistency between en-
tities was that it might allow the construction of
an unsupervised alias detector. According to the
model, two entities are consistent if they are in the
same class, and do not have conflicting assignments
of words to lexical symbols. Results here are at
best equivocal. The model is reasonable at pass-
ing basic tests? ?Dr. Seuss? is not consistent with
?Dr. Strangelove?, ?Dr. Quinn? etc, despite their
shared title, because the model identifies the sec-
ond element of each as a last name. Also correctly,
?Dr. William F. Gibson? is judged consistent with
?Dr. Gibson? and ?Gibson? despite the missing el-
ements. But mistakes are commonplace. In the
?Gibson? case, the string ?William F.? is misana-
lyzed as a multiword string, making the name in-
consistent with ?William Gibson?; this is probably
the result of a search error, which, as we explained,
Gibbs sampling is unlikely to correct. In other cases,
the system clusters a family group together under
a single ?entity? nonterminal by forcing their first
names into inappropriate states, for instance assign-
ing Pers1 Bruce, Pers2 Ellen, Pers3 Jarvis, where
Pers2 (usually a middle name) actually contains the
first name of a different individual. To improve this
aspect of our system, we might incorporate name-
specific features into the prior, such as abbreviations
and the concept of a family name. The most critical
improvement, however, would be integration with a
generative coreference system, since the document
context probably provides hints about which entities
are and are not coreferent.
The other key issue with our system is inference.
Currently we are extremely vulnerable to falling into
local minima, since the complex structure of the
model can easily lock a small group of examples
into a poor configuration. (The ?William F. Gibson?
case above seems to be one of these.) In addition to
the block sampler used by Bhattacharya and Getoor
(2006), we are investigating general-purpose split-
merge samplers (Jain and Neal, 2000) and the per-
mutation sampler (Liang et al, 2007a). One inter-
esting question is how well these samplers perform
when faced with thousands of clusters (entities).
Despite these issues, we clearly show that it is
possible to build a good model of named entity class
while retaining compatibility with generative sys-
tems and without supervision. In addition, we do a
reasonable job learning the latent structure of names
in each named entity class. Our system improves
over the latent named-entity tagging in Haghighi
and Klein (2007), from 61% to 87%. This sug-
gests that it should indeed be possible to improve
on their coreference results without using a super-
vised named-entity model. How much improvement
is possible in practice, and whether joint inference
can also improve named-entity performance, remain
interesting questions for future work.
Acknowledgements
We thank three reviewers for their comments, and
NSF for support via grants 0544127 and 0631667.
171
References
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
William J. Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In In Proceedings of the 7th Message Under-
standing Conference.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In NAACL-01.
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7)
named entity task definition. In Proceedings of the
Seventh Message Understanding Conference (MUC-
7), page 21 pages, Fairfax, VA, April. version 3.5,
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Michael Collins and Yorav Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP 99.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
EMNLP-CoNLL, pages 708?716, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Richard Evans. 2003. A framework for named en-
tity recognition in the open domain. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP-2003), pages 137 ? 144, Borovetz, Bulgaria,
September.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by es-
timating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848?855. Association for Computational Linguistics.
Sonia Jain and Radford M. Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158?182.
Mark Johnson, Tom L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In AAAI, pages 419?424.
Percy Liang, Michael I. Jordan, and Ben Taskar. 2007a.
A permutation-augmented sampler for DP mixture
models. In Proceedings of ICML, pages 545?552,
New York, NY, USA. ACM.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007b. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP-CoNLL, pages
688?697, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG System Used for MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence (MUC-7), Fairfax, Virginia.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ?04: Proceedings of
the thirteenth ACM international conference on Infor-
mation and knowledge management, pages 137?145,
New York, NY, USA. ACM.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Ann. Probab., 25:855?900.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 472?479.
AAAI.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142?147. Edmonton, Canada.
Olga Uryupina. 2004. Evaluating name-matching for
coreference resolution. In Proceedings of LREC 04,
Lisbon.
172
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317?325,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars
Mark Johnson
Brown University
Providence, RI
Mark Johnson@Brown.edu
Sharon Goldwater
University of Edinburgh
Edinburgh EH8 9AB
sgwater@inf.ed.ac.uk
Abstract
One of the reasons nonparametric Bayesian
inference is attracting attention in computa-
tional linguistics is because it provides a prin-
cipled way of learning the units of generaliza-
tion together with their probabilities. Adaptor
grammars are a framework for defining a va-
riety of hierarchical nonparametric Bayesian
models. This paper investigates some of
the choices that arise in formulating adap-
tor grammars and associated inference proce-
dures, and shows that they can have a dra-
matic impact on performance in an unsuper-
vised word segmentation task. With appro-
priate adaptor grammars and inference proce-
dures we achieve an 87% word token f-score
on the standard Brent version of the Bernstein-
Ratner corpus, which is an error reduction of
over 35% over the best previously reported re-
sults for this corpus.
1 Introduction
Most machine learning algorithms used in computa-
tional linguistics are parametric, i.e., they learn a nu-
merical weight (e.g., a probability) associated with
each feature, where the set of features is fixed be-
fore learning begins. Such procedures can be used
to learn features or structural units by embedding
them in a ?propose-and-prune? algorithm: a feature
proposal component proposes potentially useful fea-
tures (e.g., combinations of the currently most useful
features), which are then fed to a parametric learner
that estimates their weights. After estimating fea-
ture weights and pruning ?useless? low-weight fea-
tures, the cycle repeats. While such algorithms can
achieve impressive results (Stolcke and Omohundro,
1994), their effectiveness depends on how well the
feature proposal step relates to the overall learning
objective, and it can take considerable insight and
experimentation to devise good feature proposals.
One of the main reasons for the recent interest in
nonparametric Bayesian inference is that it offers a
systematic framework for structural inference, i.e.,
inferring the features relevant to a particular prob-
lem as well as their weights. (Here ?nonparamet-
ric? means that the models do not have a fixed set of
parameters; our nonparametric models do have pa-
rameters, but the particular parameters in a model
are learned along with their values). Dirichlet Pro-
cesses and their associated predictive distributions,
Chinese Restaurant Processes, are one kind of non-
parametric Bayesian model that has received consid-
erable attention recently, in part because they can be
composed in hierarchical fashion to form Hierarchi-
cal Dirichlet Processes (HDP) (Teh et al, 2006).
Lexical acquisition is an ideal test-bed for explor-
ing methods for inferring structure, where the fea-
tures learned are the words of the language. (Even
the most hard-core nativists agree that the words of a
language must be learned). We use the unsupervised
word segmentation problem as a test case for eval-
uating structural inference in this paper. Nonpara-
metric Bayesian methods produce state-of-the-art
performance on this task (Goldwater et al, 2006a;
Goldwater et al, 2007; Johnson, 2008).
In a computational linguistics setting it is natu-
ral to try to align the HDP hierarchy with the hi-
erarchy defined by a grammar. Adaptor grammars,
which are one way of doing this, make it easy to ex-
plore a wide variety of HDP grammar-based mod-
els. Given an appropriate adaptor grammar, the fea-
317
tures learned by adaptor grammars can correspond
to linguistic units such as words, syllables and col-
locations. Different adaptor grammars encode dif-
ferent assumptions about the structure of these units
and how they relate to each other. A generic adaptor
grammar inference program infers these units from
training data, making it easy to investigate how these
assumptions affect learning (Johnson, 2008).1
However, there are a number of choices in the de-
sign of adaptor grammars and the associated infer-
ence procedure. While this paper studies the im-
pact of these on the word segmentation task, these
choices arise in other nonparametric Bayesian infer-
ence problems as well, so our results should be use-
ful more generally. The rest of this paper is orga-
nized as follows. The next section reviews adaptor
grammars and presents three different adaptor gram-
mars for word segmentation that serve as running
examples in this paper. Adaptor grammars contain
a large number of adjustable parameters, and Sec-
tion 3 discusses how these can be estimated using
Bayesian techniques. Section 4 examines several
implementation options within the adaptor grammar
inference algorithm and shows that they can make
a significant impact on performance. Cumulatively
these changes make a significant difference in word
segmentation accuracy: our final adaptor grammar
performs unsupervised word segmentation with an
87% token f-score on the standard Brent version
of the Bernstein-Ratner corpus (Bernstein-Ratner,
1987; Brent and Cartwright, 1996), which is an er-
ror reduction of over 35% compared to the best pre-
viously reported results on this corpus.
2 Adaptor grammars
This section informally introduces adaptor gram-
mars using unsupervised word segmentation as a
motivating application; see Johnson et al (2007b)
for a formal definition of adaptor grammars.
Consider the problem of learning language from
continuous speech: segmenting each utterance into
words is a nontrivial problem that language learn-
ers must solve. Elman (1990) introduced an ideal-
ized version of this task, and Brent and Cartwright
(1996) presented a version of it where the data
consists of unsegmented phonemic representations
of the sentences in the Bernstein-Ratner corpus of
1The adaptor grammar inference program is available for
download at http://www.cog.brown.edu/?mj/Software.htm.
child-directed speech (Bernstein-Ratner, 1987). Be-
cause these phonemic representations are obtained
by looking up orthographic forms in a pronounc-
ing dictionary and appending the results, identifying
the word tokens is equivalent to finding the locations
of the word boundaries. For example, the phoneme
string corresponding to ?you want to see the book?
(with its correct segmentation indicated) is as fol-
lows:
y ?u Nw ?a ?n ?t Nt ?u Ns ?i ND ?6 Nb ?U ?k
We can represent any possible segmentation of any
possible sentence as a tree generated by the follow-
ing unigram grammar.
Sentence ? Word+
Word ? Phoneme+
The nonterminal Phoneme expands to each pos-
sible phoneme; the underlining, which identifies
?adapted nonterminals?, will be explained below. In
this paper ?+? abbreviates right-recursion through a
dummy nonterminal, i.e., the unigram grammar ac-
tually is:
Sentence ? Word
Sentence ? Word Sentence
Word ? Phonemes
Phonemes ? Phoneme
Phonemes ? Phoneme Phonemes
A PCFG with these productions can represent all
possible segmentations of any Sentence into a se-
quence of Words. But because it assumes that the
probability of a word is determined purely by mul-
tiplying together the probability of its individual
phonemes, it has no way to encode the fact that cer-
tain strings of phonemes (the words of the language)
have much higher probabilities than other strings
containing the same phonemes. In order to do this,
a PCFG would need productions like the following
one, which encodes the fact that ?want? is a Word.
Word ? w a n t
Adaptor grammars can be viewed as a way of for-
malizing this idea. Adaptor grammars learn the
probabilities of entire subtrees, much as in tree sub-
stitution grammar (Joshi, 2003) and DOP (Bod,
318
1998). (For computational efficiency reasons adap-
tor grammars require these subtrees to expand to ter-
minals). The set of possible adapted tree fragments
is the set of all subtrees generated by the CFG whose
root label is a member of the set of adapted non-
terminals A (adapted nonterminals are indicated by
underlining in this paper). For example, in the uni-
gram adaptor grammar A = {Word}, which means
that the adaptor grammar inference procedure learns
the probability of each possible Word subtree. Thus
adaptor grammars are simple models of structure
learning in which adapted subtrees are the units of
generalization.
One might try to reduce adaptor grammar infer-
ence to PCFG parameter estimation by introducing
a context-free rule for each possible adapted subtree,
but such an attempt would fail because the number
of such adapted subtrees, and hence the number of
corresponding rules, is unbounded. However non-
parametric Bayesian inference techniques permit us
to sample from this infinite set of adapted subtrees,
and only require us to instantiate the finite number
of them needed to analyse the finite training data.
An adaptor grammar is a 7-tuple
(N,W,R, S,?, A,C) where (N,W,R, S,?) is
a PCFG with nonterminals N , terminals W , rules
R, start symbol S ? N and rule probabilities ?,
where ?r is the probability of rule r ? R, A ? N is
the set of adapted nonterminals and C is a vector
of adaptors indexed by elements of A, so CX is the
adaptor for adapted nonterminal X ? A.
Informally, an adaptor CX nondeterministically
maps a stream of trees from a base distribution HX
whose support is TX (the set of subtrees whose root
node is X ? N generated by the grammar?s rules)
into another stream of trees whose support is also
TX . In adaptor grammars the base distributions HX
are determined by the PCFG rules expanding X and
the other adapted distributions, as explained in John-
son et al (2007b). When called upon to generate an-
other sample tree, the adaptor either generates and
returns a fresh tree from HX or regenerates a tree
it has previously emitted, so in general the adapted
distribution differs from the base distribution.
This paper uses adaptors based on Chinese
Restaurant Processes (CRPs) or Pitman-Yor Pro-
cesses (PYPs) (Pitman, 1995; Pitman and Yor, 1997;
Ishwaran and James, 2003). CRPs and PYPs non-
deterministically generate infinite sequences of nat-
ural numbers z1, z2, . . ., where z1 = 1 and each
zn+1 ? m+ 1 where m = max(z1, . . . , zn). In the
?Chinese Restaurant? metaphor samples produced
by the adaptor are viewed as ?customers? and zn
is the index of the ?table? that the nth customer is
seated at. In adaptor grammars each table in the
adaptor CX is labeled with a tree sampled from the
base distribution HX that is shared by all customers
at that table; thus the nth sample tree from the adap-
tor CX is the znth sample from HX .
CRPs and PYPs differ in exactly how the
sequence {zk} is generated. Suppose z =
(z1, . . . , zn) have already been generated and m =
max(z). Then a CRP generates the next table index
zn+1 according to the following distribution:
P(Zn+1 = k | z) ?
{
nk(z) if k ? m
? if k = m+ 1
where nk(z) is the number of times table k appears
in z and ? > 0 is an adjustable parameter that deter-
mines how often a new table is chosen. This means
that if CX is a CRP adaptor then the next tree tn+1
it generates is the same as a previously generated
tree t? with probability proportional to the number
of times CX has generated t? before, and is a ?fresh?
tree t sampled from HX with probability propor-
tional to ?XHX(t). This leads to a powerful ?rich-
get-richer? effect in which popular trees are gener-
ated with increasingly high probabilities.
Pitman-Yor Processes can control the strength of
this effect somewhat by moving mass from existing
tables to the base distribution. The PYP predictive
distribution is:
P(Zn+1 = k | z) ?
{
nk(z)? a if k ? m
ma+ b if k = m+ 1
where a ? [0, 1] and b > 0 are adjustable parame-
ters. It?s easy to see that the CRP is a special case of
the PRP where a = 0 and b = ?.
Each adaptor in an adaptor grammar can be
viewed as estimating the probability of each adapted
subtree t; this probability can differ substantially
from t?s probability HX(t) under the base distribu-
tion. Because Words are adapted in the unigram
adaptor grammar it effectively estimates the proba-
bility of each Word tree separately; the sampling es-
timators described in section 4 only instantiate those
Words actually used in the analysis of Sentences in
the corpus. While the Word adaptor will generally
319
prefer to reuse Words that have been used elsewhere
in the corpus, it is always possible to generate a fresh
Word using the CFG rules expanding Word into a
string of Phonemes.
We assume for now that all CFG rules RX ex-
panding the nonterminal X ? N have the same
probability (although we will explore estimating ?
below), so the base distribution HWord is a ?mon-
keys banging on typewriters? model. That means the
unigram adaptor grammar implements the Goldwa-
ter et al (2006a) unigram word segmentation model,
and in fact it produces segmentations of similar ac-
curacies, and exhibits the same characteristic under-
segmentation errors. As Goldwater et al point out,
because Words are the only units of generalization
available to a unigram model it tends to misanal-
yse collocations as words, resulting in a marked ten-
dancy to undersegment.
Goldwater et al demonstrate that modelling bi-
gram dependencies mitigates this undersegmenta-
tion. While adaptor grammars cannot express the
Goldwater et al bigram model, they can get much
the same effect by directly modelling collocations
(Johnson, 2008). A collocation adaptor grammar
generates a Sentence as a sequence of Collocations,
each of which expands to a sequence of Words.
Sentence ? Colloc+
Colloc ? Word+
Word ? Phoneme+
Because Colloc is adapted, the collocation adap-
tor grammar learns Collocations as well as Words.
(Presumably these approximate syntactic, semantic
and pragmatic interword dependencies). Johnson
reported that the collocation adaptor grammar seg-
ments as well as the Goldwater et al bigram model,
which we confirm here.
Recently other researchers have emphasised the
utility of phonotactic constraints (i.e., modeling
the allowable phoneme sequences at word onsets
and endings) for word segmentation (Blanchard and
Heinz, 2008; Fleck, 2008). Johnson (2008) points
out that adaptor grammars that model words as se-
quences of syllables can learn and exploit these con-
straints, significantly improving segmentation accu-
racy. Here we present an adaptor grammar that mod-
els collocations together with these phonotactic con-
straints. This grammar is quite complex, permitting
us to study the effects of the various model and im-
plementation choices described below on a complex
hierarchical nonparametric Bayesian model.
The collocation-syllable adaptor grammar gen-
erates a Sentence in terms of three levels of
Collocations (enabling it to capture a wider range
of interword dependencies), and generates Words as
sequences of 1 to 4 Syllables. Syllables are subcat-
egorized as to whether they are initial (I), final (F) or
both (IF).
Sentence ? Colloc3+
Colloc3 ? Colloc2+
Colloc2 ? Colloc1+
Colloc1 ? Word+
Word ? SyllableIF
Word ? SyllableI (Syllable) (Syllable) SyllableF
Syllable ? Onset Rhyme
Onset ? Consonant+
Rhyme ? Nucleus Coda
Nucleus ? Vowel+
Coda ? Consonant+
SyllableIF ? OnsetI RhymeF
OnsetI ? Consonant+
RhymeF ? Nucleus CodaF
CodaF ? Consonant+
SyllableI ? OnsetI Rhyme
SyllableF ? Onset RhymeF
Here Consonant and Vowel expand to all possible
consonants and vowels respectively, and the paren-
theses in the expansion of Word indicate optional-
ity. Because Onsets and Codas are adapted, the
collocation-syllable adaptor grammar learns the pos-
sible consonant sequences that begin and end syl-
lables. Moreover, because Onsets and Codas are
subcategorized based on whether they are word-
peripheral, the adaptor grammar learns which con-
sonant clusters typically appear at word boundaries,
even though the input contains no explicit word
boundary information (apart from what it can glean
from the sentence boundaries).
3 Bayesian estimation of adaptor
grammar parameters
Adaptor grammars as defined in section 2 have a
large number of free parameters that have to be
chosen by the grammar designer; a rule probabil-
ity ?r for each PCFG rule r ? R and either one or
two hyperparameters for each adapted nonterminal
X ? A, depending on whether Chinese Restaurant
320
or Pitman-Yor Processes are used as adaptors. It?s
difficult to have intuitions about the appropriate set-
tings for the latter parameters, and finding the opti-
mal values for these parameters by some kind of ex-
haustive search is usually computationally impracti-
cal. Previous work has adopted an expedient such as
parameter tying. For example, Johnson (2008) set
? by requiring all productions expanding the same
nonterminal to have the same probability, and used
Chinese Restaurant Process adaptors with tied pa-
rameters ?X , which was set using a grid search.
We now describe two methods of dealing with the
large number of parameters in these models that are
both more principled and more practical than the ap-
proaches described above. First, we can integrate
out ?, and second, we can infer values for the adap-
tor hyperparameters using sampling. These meth-
ods (the latter in particular) make it practical to use
Pitman-Yor Process adaptors in complex grammars
such as the collocation-syllable adaptor grammar,
where it is impractical to try to find optimal parame-
ter values by grid search. As we will show, they also
improve segmentation accuracy, sometimes dramat-
ically.
3.1 Integrating out ?
Johnson et al (2007a) describe Gibbs samplers for
Bayesian inference of PCFG rule probabilities ?,
and these techniques can be used directly with adap-
tor grammars as well. Just as in that paper, we
place Dirichlet priors on ?: here ?X is the subvector
of ? corresponding to rules expanding nonterminal
X ? N , and ?X is a corresponding vector of posi-
tive real numbers specifying the hyperparameters of
the corresponding Dirichlet distributions:
P(? | ?) = ?
X?N
Dir(?X | ?X)
Because the Dirichlet distribution is conjugate to the
multinomial distribution, it is possible to integrate
out the rule probabilities ?, producing the ?collapsed
sampler? described in Johnson et al (2007a).
In our experiments we chose an uniform prior
?r = 1 for all rules r ? R. As Table 1 shows,
integrating out ? only has a major effect on re-
sults when the adaptor hyperparameters themselves
are not sampled, and even then it did not have
a large effect on the collocation-syllable adaptor
grammar. This is not too surprising: because the
Onset, Nucleus and Coda adaptors in this gram-
mar learn the probabilities of these building blocks
of words, the phoneme probabilities (which is most
of what ? encodes) play less important a role.
3.2 Slice sampling adaptor hyperparameters
As far as we know, there are no conjugate priors for
the adaptor hyperparameters aX or bX (which cor-
responds to ?X in a Chinese Restaurant Process),
so it is not possible to integrate them out as we did
with the rule probabilities ?. However, it is possible
to perform Bayesian inference by putting a prior on
them and sampling their values.
Because we have no strong intuitions about the
values of these parameters we chose uninformative
priors. We chose a uniform Beta(1, 1) prior on aX ,
and a ?vague? Gamma(10, 0.1) prior on bX = ?X
(MacKay, 2003). (We experimented with other pa-
rameters in the Gamma prior, but found no signifi-
cant difference in performance).
After each Gibbs sweep through the parse trees t
we resampled each of the adaptor parameters from
the posterior distribution of the parameter using a
slice sampler 10 times. For example, we resample
each bX from:
P(bX | t) ? P(t | bX) Gamma(bX | 10, 0.1)
Here P(t | bX) is the likelihood of the current se-
quence of sample parse trees (we only need the fac-
tors that depend on bX ) and Gamma(bX | 10, 0.1)
is the prior. The same formula is used for sampling
aX , except that the prior is now a flat Beta(1, 1) dis-
tribution.
In general we cannot even compute the normaliz-
ing constants for these posterior distributions, so we
chose a sampler that does not require this. We use a
slice sampler here because it does not require a pro-
posal distribution (Neal, 2003). (We initially tried
a Metropolis-Hastings sampler but were unable to
find a proposal distribution that had reasonable ac-
ceptance ratios for all of our adaptor grammars).
As Table 1 makes clear, sampling the adaptor pa-
rameters makes a significant difference, especially
on the collocation-syllable adaptor grammar. This
is not surprising, as the adaptors in that grammar
play many different roles and there is no reason to
to expect the optimal values of their parameters to
be similar.
321
Condition Word token f-scores
Sample average Max. Marginal
B
at
ch
in
iti
al
iz
at
io
n
Ta
bl
e
la
be
lr
es
am
pl
in
g
In
te
gr
at
e
o
u
t?
Sa
m
pl
e
? X
=
b X
Sa
m
pl
e
a X
u
n
ig
ra
m
co
llo
c
co
llo
c-
sy
ll
u
n
ig
ra
m
co
llo
c
co
llo
c-
sy
ll
? ? ? ? ? 0.55 0.74 0.85 0.56 0.76 0.87
? ? ? ? 0.55 0.72 0.84 0.56 0.74 0.84
? ? ? 0.55 0.72 0.78 0.57 0.75 0.78
? ? 0.54 0.66 0.75 0.56 0.69 0.76
? ? ? ? 0.54 0.70 0.87 0.56 0.74 0.88
? ? ? ? 0.55 0.42 0.54 0.57 0.51 0.55
? ? ? ? 0.74 0.83 0.88 0.81 0.86 0.89
? ? ? 0.75 0.43 0.74 0.80 0.56 0.82
? ? 0.71 0.41 0.76 0.77 0.49 0.82
? ? ? 0.71 0.73 0.87 0.77 0.75 0.88
Table 1: Word segmentation accuracy measured by word token f-scores on Brent?s version of the Bernstein-Ratner
corpus as a function of adaptor grammar, adaptor and estimation procedure. Pitman-Yor Process adaptors were used
when aX was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where ? was not integrated
out it was set uniformly, and all ?X = bX were set to 100 they were not sampled.
4 Inference for adaptor grammars
Johnson et al (2007b) describe the basic adaptor
grammar inference procedure that we use here. That
paper leaves unspecified a number of implemen-
tation details, which we show can make a crucial
difference to segmentation accuracy. The adaptor
grammar algorithm is basically a Gibbs sampler of
the kind widely used for nonparametric Bayesian in-
ference (Blei et al, 2004; Goldwater et al, 2006b;
Goldwater et al, 2006a), so it seems reasonable to
expect that at least some of the details discussed be-
low will be relevant to other applications as well.
The inference algorithm maintains a vector t =
(t1, . . . , tn) of sample parses, where ti ? TS is a
parse for the ith sentence wi. It repeatedly chooses a
sentence wi at random and resamples the parse tree
ti for wi from P(ti | t?i, wi), i.e., conditioned on wi
and the parses t?i of all sentences except wi.
4.1 Maximum marginal decoding
Sampling algorithms like ours produce a stream of
samples from the posterior distribution over parses
of the training data. It is standard to take the out-
put of the algorithm to be the last sample produced,
and evaluate those parses. In some other applica-
tions of nonparametric Bayesian inference involv-
ing latent structure (e.g., clustering) it is difficult to
usefully exploit multiple samples, but that is not the
case here.
In maximum marginal decoding we map each
sample parse tree t onto its corresponding word seg-
mentation s, marginalizing out irrelevant detail in
t. (For example, the collocation-syllable adaptor
grammar contains a syllabification and collocational
structure that is irrelevant for word segmentation).
Given a set of sample parse trees for a sentence we
compute the set of corresponding word segmenta-
tions, and return the one that occurs most frequently
(this is a sampling approximation to the maximum
probability marginal structure).
For each setting in the experiments described in
Table 1 we ran 8 samplers for 2,000 iterations (i.e.,
passes through the training data), and kept the sam-
ple parse trees from every 10th iteration after itera-
tion 1000, resulting in 800 sample parses for every
sentence. (An examination of the posterior proba-
bilities suggests that all of the samplers using batch
initialization and table label resampling had ?burnt
322
batch initialization, table label resampling
incremental initialization, table label resampling
batch initialization, no table label resampling
2000150010005000
220000
215000
210000
205000
200000
195000
190000
185000
Figure 1: Negative log posterior probability (lower is bet-
ter) as a function of iteration for 24 runs of the collo-
cation adaptor grammar samplers with Pitman-Yor adap-
tors. The upper 8 runs use batch initialization but no ta-
ble label resampling, the middle 8 runs use incremental
initialization and table label resampling, while the lower
8 runs use batch initialization and table label resampling.
in? by iteration 1000). We evaluated the word to-
ken f-score of the most frequent marginal word seg-
mentation, and compared that to average of the word
token f-score for the 800 samples, which is also re-
ported in Table 1. For each grammar and setting we
tried, the maximum marginal segmentation was bet-
ter than the sample average, sometimes by a large
margin. Given its simplicity, this suggests that max-
imum marginal decoding is probably worth trying
when applicable.
4.2 Batch initialization
The Gibbs sampling algorithm is initialized with a
set of sample parses t for each sentence in the train-
ing data. While the fundamental theorem of Markov
Chain Monte Carlo guarantees that eventually sam-
ples will converge to the posterior distribution, it
says nothing about how long the ?burn in? phase
might last (Robert and Casella, 2004). In practice
initialization can make a huge difference to the per-
formance of Gibbs samplers (just as it can with other
unsupervised estimation procedures such as Expec-
tation Maximization).
There are many different ways in which we could
generate the initial trees t; we only study two of the
obvious methods here. Batch initialization assigns
every sentence a random parse tree in parallel. In
more detail, the initial parse tree ti for sentence wi
is sampled from P(t | wi, G?), where G? is the PCFG
obtained from the adaptor grammar by ignoring its
last two components A and C (i.e., the adapted non-
terminals and their adaptors), and seated at a new
table. This means that in batch initialization each
initial parse tree is randomly generated without any
adaptation at all.
Incremental initialization assigns the initial parse
trees ti to sentences wi in order, updating the adaptor
grammar as it goes. That is, ti is sampled from P(t |
wi, t1, . . . , ti?1). This is easy to do in the context
of Gibbs sampling, since this distribution is a minor
variant of the distribution P(ti | t?i, wi) used during
Gibbs sampling itself.
Incremental initialization is greedier than batch
initialization, and produces initial sample trees with
much higher probability. As Table 1 shows, across
all grammars and conditions after 2,000 iterations
incremental initialization produces samples with
much better word segmentation token f-score than
does batch initialization, with the largest improve-
ment on the unigram adaptor grammar.
However, incremental initialization results in
sample parses with lower posterior probability for
the unigram and collocation adaptor grammars (but
not for the collocation-syllable adaptor grammar).
Figure 1 plots the posterior probabilities of the sam-
ple trees t at each iteration for the collocation adap-
tor grammar, showing that even after 2,000 itera-
tions incremental initialization results in trees that
are much less likely than those produced by batch
initialization. It seems that with incremental initial-
ization the Gibbs sampler gets stuck in a local op-
timum which it is extremely unlikely to move away
from.
It is interesting that incremental initialization re-
sults in more accurate word segmentation, even
though the trees it produces have lower posterior
probability. This seems to be because the most prob-
able analyses produced by the unigram and, to a
lesser extent, the collocation adaptor grammars tend
to undersegment. Incremental initialization greed-
ily searches for common substrings, and because
such substrings are more likely to be short rather
than long, it tends to produce analyses with shorter
words than batch initialization does. Goldwater et
al. (2006a) show that Brent?s incremental segmenta-
tion algorithm (Brent, 1999) has a similar property.
We favor batch initialization because we are in-
323
terested in understanding the properties of our mod-
els (expressed here as adaptor grammars), and batch
initialization does a better job of finding the most
probable analyses under these models. However, it
might be possible to justify incremental initializa-
tion as (say) cognitively more plausible.
4.3 Table label resampling
Unlike the previous two implementation choices
which apply to a broad range of algorithms, table
label resampling is a specialized kind of Gibbs step
for adaptor grammars and similar hierarchical mod-
els that is designed to improve mobility. The adap-
tor grammar algorithm described in Johnson et al
(2007b) repeatedly resamples parses for the sen-
tences of the training data. However, the adaptor
grammar sampler itself maintains of a hierarchy of
Chinese Restaurant Processes or Pitman-Yor Pro-
cesses, one per adapted nonterminal X ? A, that
cache subtrees from TX . In general each of these
subtrees will occur many times in the parses for the
training data sentences. Table label resampling re-
samples the trees in these adaptors (i.e., the table
labels, to use the restaurant metaphor), potentially
changing the analysis of many sentences at once.
For example, each Collocation in the collocation
adaptor grammar can occur in many Sentences, and
each Word can occur in many Collocations. Resam-
pling a single Collocation can change the way it is
analysed into Words, thus changing the analysis of
all of the Sentences containing that Collocation.
Table label resampling is an additional resam-
pling step performed after each Gibbs sweep
through the training data in which we resample the
parse trees labeling the tables in the adaptor for each
X ? A. Specifically, if the adaptor CX for X ? A
currently contains m tables labeled with the trees
t = (t1, . . . , tm) then table label resampling re-
places each tj , j ? 1, . . . ,m in turn with a tree sam-
pled from P(t | t?j , wj), where wj is the terminal
yield of tj . (Within each adaptor we actually resam-
ple all of the trees t in a randomly chosen order).
Table label resampling is a kind of Gibbs sweep,
but at a higher level in the Bayesian hierarchy than
the standard Gibbs sweep. It?s easy to show that ta-
ble label resampling preserves detailed balance for
the adaptor grammars presented in this paper, so in-
terposing table label resampling steps with the stan-
dard Gibbs steps also preserves detailed balance.
We expect table label resampling to have the
greatest impact on models with a rich hierarchi-
cal structure, and the experimental results in Ta-
ble 1 confirm this. The unigram adaptor grammar
does not involve nested adapted nonterminals, so
we would not expect table label resampling to have
any effect on its analyses. On the other hand, the
collocation-syllable adaptor grammar involves a rich
hierarchical structure, and in fact without table la-
bel resampling our sampler did not burn in or mix
within 2,000 iterations. As Figure 1 shows, table
label resampling produces parses with higher pos-
terior probability, and Table 1 shows that table la-
bel resampling makes a significant difference in the
word segmentation f-score of the collocation and
collocation-syllable adaptor grammars.
5 Conclusion
This paper has examined adaptor grammar infer-
ence procedures and their effect on the word seg-
mentation problem. Some of the techniques inves-
tigated here, such as batch versus incremental ini-
tialization, are quite general and may be applica-
ble to a wide range of other algorithms, but some
of the other techniques, such as table label resam-
pling, are specialized to nonparametric hierarchi-
cal Bayesian inference. We?ve shown that sampling
adaptor hyperparameters is feasible, and demon-
strated that this improves word segmentation accu-
racy of the collocation-syllable adaptor grammar by
almost 10%, corresponding to an error reduction of
over 35% compared to the best results presented in
Johnson (2008). We also described and investigated
table label resampling, which dramatically improves
the effectiveness of Gibbs sampling estimators for
complex adaptor grammars, and makes it possible
to work with adaptor grammars with complex hier-
archical structure.
Acknowledgments
We thank Erik Sudderth for suggesting sampling the
Pitman-Yor hyperparameters and the ACL review-
ers for their insightful comments. This research was
funded by NSF awards 0544127 and 0631667 to
Mark Johnson.
324
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Daniel Blanchard and Jeffrey Heinz. 2008. Improv-
ing word segmentation by simultaneously learning
phonotactics. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 65?72, Manchester, England,
August.
David Blei, Thomas L. Griffiths, Michael I. Jordan, and
Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process.
In Sebastian Thrun, Lawrence Saul, and Bernhard
Scho?lkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71?105.
Jeffrey Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:197?211.
Margaret M. Fleck. 2008. Lexicalized phonotactic
word segmentation. In Proceedings of ACL-08: HLT,
pages 130?138, Columbus, Ohio, June. Association
for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673?680, Sydney, Aus-
tralia. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239?250, Somerville,
MA. Cascadilla Press.
H. Ishwaran and L. F. James. 2003. Generalized
weighted Chinese restaurant processes for species
sampling mixture models. Statistica Sinica, 13:1211?
1235.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework
for specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman, ed-
itors, Advances in Neural Information Processing Sys-
tems 19, pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Compu-
tational Linguistics, pages 483?501. Oxford Univer-
sity Press, Oxford, England.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855?900.
J. Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145?158.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Andreas Stolcke and Stephen Omohundro. 1994. Induc-
ing probabilistic grammars by Bayesian model merg-
ing. In Rafael C. Carrasco and Jose Oncina, editors,
Grammatical Inference and Applications, pages 106?
118. Springer, New York.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566?1581.
325
Lexicalized Stochastic Modeling of Constraint-Based Grammars
using Log-Linear Measures and EM Training
Stefan Riezler
IMS, Universit?t Stuttgart
riezler@ims.uni-stuttgart.de
Detlef Prescher
IMS, Universit?t Stuttgart
prescher@ims.uni-stuttgart.de
Jonas Kuhn
IMS, Universit?t Stuttgart
jonas@ims.uni-stuttgart.de
Mark Johnson
Cog. & Ling. Sciences, Brown University
Mark_Johnson@brown.edu
Abstract
We present a new approach to
stochastic modeling of constraint-
based grammars that is based on log-
linear models and uses EM for esti-
mation from unannotated data. The
techniques are applied to an LFG
grammar for German. Evaluation on
an exact match task yields 86% pre-
cision for an ambiguity rate of 5.4,
and 90% precision on a subcat frame
match for an ambiguity rate of 25.
Experimental comparison to train-
ing from a parsebank shows a 10%
gain from EM training. Also, a new
class-based grammar lexicalization is
presented, showing a 10% gain over
unlexicalized models.
1 Introduction
Stochastic parsing models capturing contex-
tual constraints beyond the dependencies of
probabilistic context-free grammars (PCFGs)
are currently the subject of intensive research.
An interesting feature common to most such
models is the incorporation of contextual de-
pendencies on individual head words into rule-
based probability models. Such word-based
lexicalizations of probability models are used
successfully in the statistical parsing mod-
els of, e.g., Collins (1997), Charniak (1997),
or Ratnaparkhi (1997). However, it is still
an open question which kind of lexicaliza-
tion, e.g., statistics on individual words or
statistics based upon word classes, is the best
choice. Secondly, these approaches have in
common the fact that the probability models
are trained on treebanks, i.e., corpora of man-
ually disambiguated sentences, and not from
corpora of unannotated sentences. In all of the
cited approaches, the Penn Wall Street Jour-
nal Treebank (Marcus et al, 1993) is used,
the availability of which obviates the standard
eort required for treebank traininghand-
annotating large corpora of specic domains
of specic languages with specic parse types.
Moreover, common wisdom is that training
from unannotated data via the expectation-
maximization (EM) algorithm (Dempster et
al., 1977) yields poor results unless at
least partial annotation is applied. Experi-
mental results conrming this wisdom have
been presented, e.g., by Elworthy (1994) and
Pereira and Schabes (1992) for EM training
of Hidden Markov Models and PCFGs.
In this paper, we present a new lexicalized
stochastic model for constraint-based gram-
mars that employs a combination of head-
word frequencies and EM-based clustering
for grammar lexicalization. Furthermore, we
make crucial use of EM for estimating the
parameters of the stochastic grammar from
unannotated data. Our usage of EM was ini-
tiated by the current lack of large unication-
based treebanks for German. However, our ex-
perimental results also show an exception to
the common wisdom of the insuciency of EM
for highly accurate statistical modeling.
Our approach to lexicalized stochastic mod-
eling is based on the parametric family of log-
linear probability models, which is used to de-
ne a probability distribution on the parses
of a Lexical-Functional Grammar (LFG) for
German. In previous work on log-linear mod-
els for LFG by Johnson et al (1999), pseudo-
likelihood estimation from annotated corpora
has been introduced and experimented with
on a small scale. However, to our knowledge,
to date no large LFG annotated corpora of
unrestricted German text are available. For-
tunately, algorithms exist for statistical infer-
ence of log-linear models from unannotated
data (Riezler, 1999). We apply this algorithm
to estimate log-linear LFG models from large
corpora of newspaper text. In our largest ex-
periment, we used 250,000 parses which were
produced by parsing 36,000 newspaper sen-
tences with the German LFG. Experimental
evaluation of our models on an exact-match
task (i.e. percentage of exact match of most
probable parse with correct parse) on 550
manually examined examples with on average
5.4 analyses gave 86% precision. Another eval-
uation on a verb frame recognition task (i.e.
percentage of agreement between subcatego-
rization frames of main verb of most proba-
ble parse and correct parse) gave 90% pre-
cision on 375 manually disambiguated exam-
ples with an average ambiguity of 25. Clearly,
a direct comparison of these results to state-
of-the-art statistical parsers cannot be made
because of dierent training and test data and
other evaluation measures. However, we would
like to draw the following conclusions from our
experiments:
 The problem of chaotic convergence be-
haviour of EM estimation can be solved
for log-linear models.
 EM does help constraint-based gram-
mars, e.g. using about 10 times more sen-
tences and about 100 times more parses
for EM training than for training from an
automatically constructed parsebank can
improve precision by about 10%.
 Class-based lexicalization can yield a gain
in precision of about 10%.
In the rest of this paper we intro-
duce incomplete-data estimation for log-linear
models (Sec. 2), and present the actual design
of our models (Sec. 3) and report our experi-
mental results (Sec. 4).
2 Incomplete-Data Estimation for
Log-Linear Models
2.1 Log-Linear Models
A log-linear distribution p

(x) on the set of
analyses X of a constraint-based grammar can
be dened as follows:
p

(x) = Z

 1
e
(x)
p
0
(x)
where Z

=
P
x2X
e
(x)
p
0
(x) is a normal-
izing constant,  = (
1
; : : : ; 
n
) 2 IR
n
is a
vector of log-parameters,  = (
1
; : : : ; 
n
) is
a vector of property-functions 
i
: X ! IR for
i = 1; : : : ; n,   (x) is the vector dot prod-
uct
P
n
i=1

i

i
(x), and p
0
is a xed reference
distribution.
The task of probabilistic modeling with log-
linear distributions is to build salient proper-
ties of the data as property-functions 
i
into
the probability model. For a given vector  of
property-functions, the task of statistical in-
ference is to tune the parameters  to best
reect the empirical distribution of the train-
ing data.
2.2 Incomplete-Data Estimation
Standard numerical methods for statis-
tical inference of log-linear models from
fully annotated dataso-called complete
dataare the iterative scaling meth-
ods of Darroch and Ratcli (1972) and
Della Pietra et al (1997). For data consisting
of unannotated sentencesso-called incom-
plete datathe iterative method of the EM
algorithm (Dempster et al, 1977) has to be
employed. However, since even complete-data
estimation for log-linear models requires
iterative methods, an application of EM to
log-linear models results in an algorithm
which is expensive since it is doubly-iterative.
A singly-iterative algorithm interleaving EM
and iterative scaling into a mathematically
well-dened estimation method for log-linear
models from incomplete data is the IM
algorithm of Riezler (1999). Applying this
algorithm to stochastic constraint-based
grammars, we assume the following to be
given: A training sample of unannotated sen-
tences y from a set Y, observed with empirical
Input Reference model p
0
, property-functions vector  with constant 
#
, parses
X(y) for each y in incomplete-data sample from Y.
Output MLE model p


on X .
Procedure
Until convergence do
Compute p

; k

, based on  = (
1
; : : : ; 
n
),
For i from 1 to n do

i
:=
1

#
ln
P
y2Y
~p(y)
P
x2X(y)
k

(xjy)
i
(x)
P
x2X
p

(x)
i
(x)
,

i
:= 
i
+ 
i
,
Return 

= (
1
; : : : ; 
n
).
Figure 1: Closed-form version of IM algorithm
probability ~p(y), a constraint-based grammar
yielding a set X(y) of parses for each sentence
y, and a log-linear model p

() on the parses
X =
P
y2Yj~p(y)>0
X(y) for the sentences in
the training corpus, with known values of
property-functions  and unknown values
of . The aim of incomplete-data maximum
likelihood estimation (MLE) is to nd a value


that maximizes the incomplete-data log-
likelihood L =
P
y2Y
~p(y) ln
P
x2X(y)
p

(x),
i.e.,


= argmax
2IR
n
L():
Closed-form parameter-updates for this prob-
lem can be computed by the algorithm of Fig.
1, where 
#
(x) =
P
n
i=1

i
(x), and k

(xjy) =
p

(x)=
P
x2X(y)
p

(x) is the conditional prob-
ability of a parse x given the sentence y and
the current parameter value .
The constancy requirement on 
#
can be
enforced by adding a correction property-
function 
l
:
Choose K = max
x2X

#
(x) and

l
(x) = K   
#
(x) for all x 2 X .
Then
P
l
i=1

i
(x) = K for all x 2 X .
Note that because of the restriction of X to
the parses obtainable by a grammar from the
training corpus, we have a log-linear probabil-
ity measure only on those parses and not on
all possible parses of the grammar. We shall
therefore speak of mere log-linear measures in
our application of disambiguation.
2.3 Searching for Order in Chaos
For incomplete-data estimation, a sequence
of likelihood values is guaranteed to converge
to a critical point of the likelihood function
L. This is shown for the IM algorithm in
Riezler (1999). The process of nding likeli-
hood maxima is chaotic in that the nal likeli-
hood value is extremely sensitive to the start-
ing values of , i.e. limit points can be lo-
cal maxima (or saddlepoints), which are not
necessarily also global maxima. A way to
search for order in this chaos is to search for
starting values which are hopefully attracted
by the global maximum of L. This problem
can best be explained in terms of the mini-
mum divergence paradigm (Kullback, 1959),
which is equivalent to the maximum likeli-
hood paradigm by the following theorem. Let
p[f ] =
P
x2X
p(x)f(x) be the expectation of
a function f with respect to a distribution p:
The probability distribution p

that
minimizes the divergence D(pjjp
0
) to
a reference model p
0
subject to the
constraints p[
i
] = q[
i
]; i = 1; : : : ; n
is the model in the parametric fam-
ily of log-linear distributions p

that
maximizes the likelihood L() =
q[ln p

] of the training data
1
.
1
If the training sample consists of complete data
Reasonable starting values for minimum di-
vergence estimation is to set 
i
= 0 for
i = 1; : : : ; n. This yields a distribution which
minimizes the divergence to p
0
, over the
set of models p to which the constraints
p[
i
] = q[
i
]; i = 1; : : : ; n have yet to be ap-
plied. Clearly, this argument applies to both
complete-data and incomplete-data estima-
tion. Note that for a uniformly distributed
reference model p
0
, the minimum divergence
model is a maximum entropy model (Jaynes,
1957). In Sec. 4, we will demonstrate that
a uniform initialization of the IM algorithm
shows a signicant improvement in likelihood
maximization as well as in linguistic perfor-
mance when compared to standard random
initialization.
3 Property Design and
Lexicalization
3.1 Basic Congurational Properties
The basic 190 properties employed in our
models are similar to the properties of
Johnson et al (1999) which incorporate gen-
eral linguistic principles into a log-linear
model. They refer to both the c(onstituent)-
structure and the f(eature)-structure of the
LFG parses. Examples are properties for
 c-structure nodes, corresponding to stan-
dard production properties,
 c-structure subtrees, indicating argument
versus adjunct attachment,
 f-structure attributes, corresponding to
grammatical functions used in LFG,
 atomic attribute-value pairs in f-
structures,
 complexity of the phrase being attached
to, thus indicating both high and low at-
tachment,
 non-right-branching behavior of nonter-
minal nodes,
 non-parallelism of coordinations.
x 2 X , the expectation q[] corresponds to the em-
pirical expectation ~p[]. If we observe incomplete data
y 2 Y, the expectation q[] is replaced by the condi-
tional expectation ~p[k

0
[]] given the observed data y
and the current parameter value 
0
.
3.2 Class-Based Lexicalization
Our approach to grammar lexicalization is
class-based in the sense that we use class-
based estimated frequencies f
c
(v; n) of head-
verbs v and argument head-nouns n in-
stead of pure frequency statistics or class-
based probabilities of head word dependen-
cies. Class-based estimated frequencies are in-
troduced in Prescher et al (2000) as the fre-
quency f(v; n) of a (v; n)-pair in the train-
ing corpus, weighted by the best estimate of
the class-membership probability p(cjv; n) of
an EM-based clustering model on (v; n)-pairs,
i.e., f
c
(v; n) = max
c2C
p(cjv; n)(f(v; n) + 1).
As is shown in Prescher et al (2000) in an
evaluation on lexical ambiguity resolution, a
gain of about 7% can be obtained by using
the class-based estimated frequency f
c
(v; n)
as disambiguation criterion instead of class-
based probabilities p(njv). In order to make
the most direct use possible of this fact, we
incorporated the decisions of the disambigua-
tor directly into 45 additional properties for
the grammatical relations of the subject, di-
rect object, indirect object, innitival object,
oblique and adjunctival dative and accusative
preposition, for active and passive forms of the
rst three verbs in each parse. Let v
r
(x) be the
verbal head of grammatical relation r in parse
x, and n
r
(x) the nominal head of grammatical
relation r in x. Then a lexicalized property 
r
for grammatical relation r is dened as

r
(x) =
8
<
:
1
if f
c
(v
r
(x); n
r
(x)) 
f
c
(v
r
(x
0
); n
r
(x
0
)) 8x
0
2 X(y);
0 otherwise:
The property-function 
r
thus pre-
disambiguates the parses x 2 X(y) of a
sentence y according to f
c
(v; n), and stores
the best parse directly instead of taking the
actual estimated frequencies as its value. In
Sec. 4, we will see that an incorporation of
this pre-disambiguation routine into the mod-
els improves performance in disambiguation
by about 10%.
exact match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 68
E: 59.6
P: 73.9
E: 71.6
P: 74.3
E: 71.8
incomplete-data
estimation
P: 73
E: 65.4
P: 86
E: 85.2
P: 86.1
E: 85.4
Figure 2: Evaluation on exact match task for 550 examples with average ambiguity 5.4
frame match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 80.6
E: 70.4
P: 82.7
E: 76.4
P: 83.4
E: 76
incomplete-data
estimation
P: 84.5
E: 73.1
P: 88.5
E: 84.9
P: 90
E: 86.3
Figure 3: Evaluation on frame match task for 375 examples with average ambiguity 25
4 Experiments
4.1 Incomplete Data and Parsebanks
In our experiments, we used an LFG grammar
for German
2
for parsing unrestricted text.
Since training was faster than parsing, we
parsed in advance and stored the resulting
packed c/f-structures. The low ambiguity rate
of the German LFG grammar allowed us to
restrict the training data to sentences with
at most 20 parses. The resulting training cor-
pus of unannotated, incomplete data consists
of approximately 36,000 sentences of online
available German newspaper text, comprising
approximately 250,000 parses.
In order to compare the contribution of un-
ambiguous and ambiguous sentences to the es-
timation results, we extracted a subcorpus of
4,000 sentences, for which the LFG grammar
produced a unique parse, from the full train-
2
The German LFG grammar is being imple-
mented in the Xerox Linguistic Environment (XLE,
see Maxwell and Kaplan (1996)) as part of the Paral-
lel Grammar (ParGram) project at the IMS Stuttgart.
The coverage of the grammar is about 50% for unre-
stricted newspaper text. For the experiments reported
here, the eective coverage was lower, since the cor-
pus preprocessing we applied was minimal. Note that
for the disambiguation task we were interested in,
the overall grammar coverage was of subordinate rel-
evance.
ing corpus. The average sentence length of
7.5 for this automatically constructed parse-
bank is only slightly smaller than that of
10.5 for the full set of 36,000 training sen-
tences and 250,000 parses. Thus, we conjec-
ture that the parsebank includes a representa-
tive variety of linguistic phenomena. Estima-
tion from this automatically disambiguated
parsebank enjoys the same complete-data es-
timation properties
3
as training from manu-
ally disambiguated treebanks. This makes a
comparison of complete-data estimation from
this parsebank to incomplete-data estimation
from the full set of training data interesting.
4.2 Test Data and Evaluation Tasks
To evaluate our models, we constructed
two dierent test corpora. We rst parsed
with the LFG grammar 550 sentences
which are used for illustrative purposes in
the foreign language learner's grammar of
Helbig and Buscha (1996). In a next step, the
correct parse was indicated by a human dis-
ambiguator, according to the reading intended
in Helbig and Buscha (1996). Thus a precise
3
For example, convergence to the global maximum
of the complete-data log-likelihood function is guar-
anteed, which is a good condition for highly precise
statistical disambiguation.
indication of correct c/f-structure pairs was
possible. However, the average ambiguity of
this corpus is only 5.4 parses per sentence, for
sentences with on average 7.5 words. In order
to evaluate on sentences with higher ambigu-
ity rate, we manually disambiguated further
375 sentences of LFG-parsed newspaper text.
The sentences of this corpus have on average
25 parses and 11.2 words.
We tested our models on two evalua-
tion tasks. The statistical disambiguator was
tested on an exact match task, where ex-
act correspondence of the full c/f-structure
pair of the hand-annotated correct parse and
the most probable parse is checked. Another
evaluation was done on a frame match task,
where exact correspondence only of the sub-
categorization frame of the main verb of the
most probable parse and the correct parse is
checked. Clearly, the latter task involves a
smaller eective ambiguity rate, and is thus
to be interpreted as an evaluation of the com-
bined system of highly-constrained symbolic
parsing and statistical disambiguation.
Performance on these two evaluation tasks
was assessed according to the following evalu-
ation measures:
Precision =
#correct
#correct+#incorrect
,
Eectiveness =
#correct
#correct+#incorrect+#don't know
.
Correct and incorrect species a suc-
cess/failure on the respective evaluation tasks;
don't know cases are cases where the system
is unable to make a decision, i.e. cases with
more than one most probable parse.
4.3 Experimental Results
For each task and each test corpus, we cal-
culated a random baseline by averaging over
several models with randomly chosen pa-
rameter values. This baseline measures the
disambiguation power of the pure symbolic
parser. The results of an exact-match evalu-
ation on the Helbig-Buscha corpus is shown
in Fig. 2. The random baseline was around
33% for this case. The columns list dierent
models according to their property-vectors.
Basic models consist of 190 congurational
properties as described in Sec. 3.1. Lexical-
ized models are extended by 45 lexical pre-
disambiguation properties as described in Sec.
3.2. Selected + lexicalized models result
from a simple property selection procedure
where a cuto on the number of parses with
non-negative value of the property-functions
was set. Estimation of basic models from com-
plete data gave 68% precision (P), whereas
training lexicalized and selected models from
incomplete data gave 86.1% precision, which
is an improvement of 18%. Comparing lex-
icalized models in the estimation method
shows that incomplete-data estimation gives
an improvement of 12% precision over train-
ing from the parsebank. A comparison of mod-
els trained from incomplete data shows that
lexicalization yields a gain of 13% in preci-
sion. Note also the gain in eectiveness (E)
due to the pre-disambigution routine included
in the lexicalized properties. The gain due to
property selection both in precision and eec-
tiveness is minimal. A similar pattern of per-
formance arises in an exact match evaluation
on the newspaper corpus with an ambiguity
rate of 25. The lexicalized and selected model
trained from incomplete data achieved here
60.1% precision and 57.9% eectiveness, for a
random baseline of around 17%.
As shown in Fig. 3, the improvement in per-
formance due to both lexicalization and EM
training is smaller for the easier task of frame
evaluation. Here the random baseline is 70%
for frame evaluation on the newspaper corpus
with an ambiguity rate of 25. An overall gain
of roughly 10% can be achieved by going from
unlexicalized parsebank models (80.6% preci-
sion) to lexicalized EM-trained models (90%
precision). Again, the contribution to this im-
provement is about the same for lexicalization
and incomplete-data training. Applying the
same evaluation to the Helbig-Buscha corpus
shows 97.6% precision and 96.7% eectiveness
for the lexicalized and selected incomplete-
data model, compared to around 80% for the
random baseline.
Optimal iteration numbers were decided by
repeated evaluation of the models at every
fth iteration. Fig. 4 shows the precision of
lexicalized and selected models on the exact
68
70
72
74
76
78
80
82
84
86
88
10 20 30 40 50 60 70 80 90
pre
cis
ion
number of iterations
complete-data estimation
incomplete-data estimation
Figure 4: Precision on exact match task in number of training iterations
match task plotted against the number of it-
erations of the training algorithm. For parse-
bank training, the maximal precision value
is obtained at 35 iterations. Iterating fur-
ther shows a clear overtraining eect. For
incomplete-data estimation more iterations
are necessary to reach a maximal precision
value. A comparison of models with random
or uniform starting values shows an increase
in precision of 10% to 40% for the latter.
In terms of maximization of likelihood, this
corresponds to the fact that uniform starting
values immediately push the likelihood up to
nearly its nal value, whereas random starting
values yield an initial likelihood which has to
be increased by factors of 2 to 20 to an often
lower nal value.
5 Discussion
The most direct points of compar-
ison of our method are the ap-
proaches of Johnson et al (1999) and
Johnson and Riezler (2000). In the rst ap-
proach, log-linear models on LFG grammars
using about 200 congurational properties
were trained on treebanks of about 400
sentences by maximum pseudo-likelihood
estimation. Precision was evaluated on an
exact match task in a 10-way cross valida-
tion paradigm for an ambiguity rate of 10,
and achieved 59% for the rst approach.
Johnson and Riezler (2000) achieved a gain
of 1% over this result by including a class-
based lexicalization. Our best models clearly
outperform these results, both in terms of
precision relative to ambiguity and in terms
of relative gain due to lexicalization. A
comparison of performance is more dicult
for the lexicalized PCFG of Beil et al (1999)
which was trained by EM on 450,000 sen-
tences of German newspaper text. There, a
70.4% precision is reported on a verb frame
recognition task on 584 examples. However,
the gain achieved by Beil et al (1999) due to
grammar lexicalizaton is only 2%, compared
to about 10% in our case. A comparison
is dicult also for most other state-of-the-
art PCFG-based statistical parsers, since
dierent training and test data, and most
importantly, dierent evaluation criteria were
used. A comparison of the performance gain
due to grammar lexicalization shows that our
results are on a par with that reported in
Charniak (1997).
6 Conclusion
We have presented a new approach to stochas-
tic modeling of constraint-based grammars.
Our experimental results show that EM train-
ing can in fact be very helpful for accurate
stochastic modeling in natural language pro-
cessing. We conjecture that this result is due
partly to the fact that the space of parses
produced by a constraint-based grammar is
only mildly incomplete, i.e. the ambiguity
rate can be kept relatively low. Another rea-
son may be that EM is especially useful for
log-linear models, where the search space in
maximization can be kept under control. Fur-
thermore, we have introduced a new class-
based grammar lexicalization, which again
uses EM training and incorporates a pre-
disambiguation routine into log-linear models.
An impressive gain in performance could also
be demonstrated for this method. Clearly, a
central task of future work is a further explo-
ration of the relation between complete-data
and incomplete-data estimation for larger,
manually disambiguated treebanks. An inter-
esting question is whether a systematic vari-
ation of training data size along the lines
of the EM-experiments of Nigam et al (2000)
for text classication will show similar results,
namely a systematic dependence of the rela-
tive gain due to EM training from the relative
sizes of unannotated and annotated data. Fur-
thermore, it is important to show that EM-
based methods can be applied successfully
also to other statistical parsing frameworks.
Acknowledgements
We thank Stefanie Dipper and Bettina
Schrader for help with disambiguation of the
test suites, and the anonymous ACL review-
ers for helpful suggestions. This research was
supported by the ParGram project and the
project B7 of the SFB 340 of the DFG.
References
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan
Riezler, and Mats Rooth. 1999. Inside-outside
estimation of a lexicalized PCFG for German.
In Proceedings of the 37th ACL, College Park,
MD.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the 14th AAAI, Menlo Park, CA.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th ACL, Madrid.
J.N. Darroch and D. Ratcli. 1972. General-
ized iterative scaling for log-linear models. The
Annals of Mathematical Statistics, 43(5):1470
1480.
Stephen Della Pietra, Vincent Della Pietra, and
John Laerty. 1997. Inducing features of ran-
dom elds. IEEE PAMI, 19(4):380393.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(B):138.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ANLP, Stuttgart.
Gerhard Helbig and Joachim Buscha. 1996.
Deutsche Grammatik. Ein Handbuch f?r den
Ausl?nderunterricht. Langenscheidt, Leipzig.
Edwin T. Jaynes. 1957. Information theory
and statistical mechanics. Physical Review,
106:620630.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
unication-based grammars. In Proceedings of
the 1st NAACL, Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic unication-based grammars. In
Proceedings of the 37th ACL, College Park, MD.
Solomon Kullback. 1959. Information Theory and
Statistics. Wiley, New York.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of english: The
Penn treebank. Computational Linguistics,
19(2):313330.
John Maxwell and R. Kaplan. 1996. Unication-
based parsers that automatically take ad-
vantage of context freeness. Unpublished
manuscript, Xerox Palo Alto Research Center.
Kamal Nigam, Andrew McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classi-
cation from labeled and unlabeled documents
using EM. Machine Learning, 39(2/4):103134.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed
corpora. In Proceedings of the 30th ACL,
Newark, Delaware.
Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000. Using a probabilistic class-based lexicon
for lexical ambiguity resolution. In Proceedings
of the 18th COLING, Saarbr?cken.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of EMNLP-2.
Stefan Riezler. 1999. Probabilistic Constraint
Logic Programming Ph.D. thesis, Seminar
f?r Sprachwissenschaft, Universit?t T?bingen.
AIMS Report, 5(1), IMS, Universit?t Stuttgart.
Joint and conditional estimation of tagging and parsing models?
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Abstract
This paper compares two different ways
of estimating statistical language mod-
els. Many statistical NLP tagging and
parsing models are estimated by max-
imizing the (joint) likelihood of the
fully-observed training data. How-
ever, since these applications only re-
quire the conditional probability distri-
butions, these distributions can in prin-
ciple be learnt by maximizing the con-
ditional likelihood of the training data.
Perhaps somewhat surprisingly, models
estimated by maximizing the joint were
superior to models estimated by max-
imizing the conditional, even though
some of the latter models intuitively
had access to ?more information?.
1 Introduction
Many statistical NLP applications, such as tag-
ging and parsing, involve finding the value
of some hidden variable Y (e.g., a tag or a
parse tree) which maximizes a conditional prob-
ability distribution P?(Y |X), where X is a
given word string. The model parameters ?
are typically estimated by maximum likelihood:
i.e., maximizing the likelihood of the training
?I would like to thank Eugene Charniak and the other
members of BLLIP for their comments and suggestions. Fer-
nando Pereira was especially generous with comments and
suggestions, as were the ACL reviewers; I apologize for not
being able to follow up all of your good suggestions. This re-
search was supported by NSF awards 9720368 and 9721276
and NIH award R01 MH60922-01A2.
data. Given a (fully observed) training cor-
pus D = ((y1, x1), . . . , (yn, xn)), the maximum
(joint) likelihood estimate (MLE) of ? is:
?? = argmax
?
n
?
i=1
P?(yi, xi). (1)
However, it turns out there is another maximum
likelihood estimation method which maximizes
the conditional likelihood or ?pseudo-likelihood?
of the training data (Besag, 1975). Maximum
conditional likelihood is consistent for the con-
ditional distribution. Given a training corpus
D, the maximum conditional likelihood estimate
(MCLE) of the model parameters ? is:
?? = argmax
?
n
?
i=1
P?(yi|xi). (2)
Figure 1 graphically depicts the difference be-
tween the MLE and MCLE. Let ? be the universe
of all possible pairs (y, x) of hidden and visible
values. Informally, the MLE selects the model
parameter ? which make the training data pairs
(yi, xi) as likely as possible relative to all other
pairs (y?, x?) in ?. The MCLE, on the other hand,
selects the model parameter ? in order to make the
training data pair (yi, xi) more likely than other
pairs (y?, xi) in ?, i.e., pairs with the same visible
value xi as the training datum.
In statistical computational linguistics, max-
imum conditional likelihood estimators have
mostly been used with general exponential or
?maximum entropy? models because standard
maximum likelihood estimation is usually com-
putationally intractable (Berger et al, 1996; Della
Pietra et al, 1997; Jelinek, 1997). Well-
known computational linguistic models such as
(MLE)
(MCLE)
? Y = yi, X = xi
?
X = xi
Y = yi, X = xi
Figure 1: The MLE makes the training data (yi, xi) as
likely as possible (relative to ?), while the MCLE makes
(yi, xi) as likely as possible relative to other pairs (y?, xi).
Maximum-Entropy Markov Models (McCallum
et al, 2000) and Stochastic Unification-based
Grammars (Johnson et al, 1999) are standardly
estimated with conditional estimators, and it
would be interesting to know whether conditional
estimation affects the quality of the estimated
model. It should be noted that in practice, the
MCLE of a model with a large number of features
with complex dependencies may yield far better
performance than the MLE of the much smaller
model that could be estimated with the same
computational effort. Nevertheless, as this paper
shows, conditional estimators can be used with
other kinds of models besides MaxEnt models,
and in any event it is interesting to ask whether
the MLE differs from the MCLE in actual appli-
cations, and if so, how.
Because the MLE is consistent for the joint
distribution P(Y,X) (e.g., in a tagging applica-
tion, the distribution of word-tag sequences), it
is also consistent for the conditional distribution
P(Y |X) (e.g., the distribution of tag sequences
given word sequences) and the marginal distribu-
tion P(X) (e.g., the distribution of word strings).
On the other hand, the MCLE is consistent for the
conditional distribution P(Y |X) alone, and pro-
vides no information about either the joint or the
marginal distributions. Applications such as lan-
guage modelling for speech recognition and EM
procedures for estimating from hidden data ei-
ther explicitly or implicitly require marginal dis-
tributions over the visible data (i.e., word strings),
so it is not statistically sound to use MCLEs for
such applications. On the other hand, applications
which involve predicting the value of the hidden
variable from the visible variable (such as tagging
or parsing) usually only involve the conditional
distribution, which the MCLE estimates directly.
Since both the MLE and MCLE are consistent
for the conditional distribution, both converge in
the limit to the ?true? distribution if the true dis-
tribution is in the model class. However, given
that we often have insufficient data in computa-
tional linguistics, and there are good reasons to
believe that the true distribution of sentences or
parses cannot be described by our models, there
is no reason to expect these asymptotic results to
hold in practice, and in the experiments reported
below the MLE and MCLE behave differently ex-
perimentally.
A priori, one can advance plausible arguments
in favour of both the MLE and the MCLE. Infor-
mally, the MLE and the MCLE differ in the fol-
lowing way. Since the MLE is obtained by maxi-
mizing
?
i P?(yi|xi)P?(xi), the MLE exploits in-
formation about the distribution of word strings xi
in the training data that the MCLE does not. Thus
one might expect the MLE to converge faster than
the MCLE in situations where training data is not
over-abundant, which is often the case in compu-
tational linguistics.
On the other hand, since the intended applica-
tion requires a conditional distribution, it seems
reasonable to directly estimate this conditional
distribution from the training data as the MCLE
does. Furthermore, suppose that the model class
is wrong (as is surely true of all our current lan-
guage models), i.e., the ?true? model P(Y,X) 6=
P?(Y,X) for all ?, and that our best models are
particularly poor approximations to the true dis-
tribution of word strings P(X). Then ignoring
the distribution of word strings in the training data
as the MCLE does might indeed be a reasonable
thing to do.
The rest of this paper is structured as fol-
lows. The next section formulates the MCLEs
for HMMs and PCFGs as constrained optimiza-
tion problems and describes an iterative dynamic-
programming method for solving them. Because
of the computational complexity of these prob-
lems, the method is only applied to a simple
PCFG based on the ATIS corpus. For this ex-
ample, the MCLE PCFG does perhaps produce
slightly better parsing results than the standard
MLE (relative-frequency) PCFG, although the re-
sult does not reach statistical significance.
It seems to be difficult to find model classes for
which the MLE and MCLE are both easy to com-
pute. However, often it is possible to find two
closely related model classes, one of which has
an easily computed MLE and the other which has
an easily computed MCLE. Typically, the model
classes which have an easily computed MLE de-
fine joint probability distributions over both the
hidden and the visible data (e.g., over word-
tag pair sequences for tagging), while the model
classes which have an easily computed MCLE de-
fine conditional probability distributions over the
hidden data given the visible data (e.g., over tag
sequences given word sequences).
Section 3 investigates closely related joint
and conditional tagging models (the lat-
ter can be regarded as a simplification of
the Maximum Entropy Markov Models of
McCallum et al (2000)), and shows that MLEs
outperform the MCLEs in this application. The
final empirical section investigates two different
kinds of stochastic shift-reduce parsers, and
shows that the model estimated by the MLE
outperforms the model estimated by the MCLE.
2 PCFG parsing
In this application, the pairs (y, x) consist of a
parse tree y and its terminal string or yield x (it
may be simpler to think of y containing all of the
parse tree except for the string x). Recall that
in a PCFG with production set R, each produc-
tion (A??) ? R is associated with a parameter
?A??. These parameters satisfy a normalization
constraint for each nonterminal A:
?
?:(A??)?R
?A?? = 1 (3)
For each production r ? R, let fr(y) be the num-
ber of times r is used in the derivation of the tree
y. Then the PCFG defines a probability distribu-
tion over trees:
P?(Y ) =
?
(A??)?R
?A??fA??(Y )
The MLE for ? is the well-known ?relative-
frequency? estimator:
??A?? =
?n
i=1 fA??(yi)
?n
i=1
?
??:(A???)?R fA???(yi)
.
Unfortunately the MCLE for a PCFG is more
complicated. If x is a word string, then let ?(x) be
the set of parse trees with terminal string or yield
x generated by the PCFG. Then given a training
corpus D = ((y1, x1), . . . , (yn, xn)), where yi is
a parse tree for the string xi, the log conditional
likelihood of the training data log P(~y|~x) and its
derivative are given by:
log P(~y|~x) =
n
?
i=1
?
?log P?(yi) ? log
?
y??(xi)
P?(y)
?
?
? log P(~y|~x)
??A??
= 1?A??
n
?
i=1
(fA??(yi) ? E?(fA??|xi))
Here E?(f |x) denotes the expectation of f with
respect to P? conditioned on Y ? ?(x). There
does not seem to be a closed-form solution for
the ? that maximizes P(~y|~x) subject to the con-
straints (3), so we used an iterative numerical gra-
dient ascent method, with the constraints (3) im-
posed at each iteration using Lagrange multipli-
ers. Note that
?n
i=1 E?(fA??|xi) is a quantity
calculated in the Inside-Outside algorithm (Lari
and Young, 1990) and P(~y|~x) is easily computed
as a by-product of the same dynamic program-
ming calculation.
Since the expected production counts E?(f |x)
depend on the production weights ?, the entire
training corpus must be reparsed on each itera-
tion (as is true of the Inside-Outside algorithm).
This is computationally expensive with a large
grammar and training corpus; for this reason the
MCLE PCFG experiments described here were
performed with the relatively small ATIS tree-
bank corpus of air travel reservations distributed
by LDC.
In this experiment, the PCFGs were always
trained on the 1088 sentences of the ATIS1 corpus
and evaluated on the 294 sentences of the ATIS2
corpus. Lexical items were ignored; the PCFGs
generate preterminal strings. The iterative algo-
rithm for the MCLE was initialized with the MLE
parameters, i.e., the ?standard? PCFG estimated
from a treebank. Table 1 compares the MLE and
MCLE PCFGs.
The data in table 1 shows that compared to the
MLE PCFG, the MCLE PCFG assigns a higher
conditional probability of the parses in the train-
ing data given their yields, at the expense of as-
signing a lower marginal probability to the yields
themselves. The labelled precision and recall
parsing results for the MCLE PCFG were slightly
higher than those of the MLE PCFG. Because
MLE MCLE
? log P(~y) 13857 13896
? log P(~y|~x) 1833 1769
? log P(~x) 12025 12127
Labelled precision 0.815 0.817
Labelled recall 0.789 0.794
Table 1: The likelihood P(~y) and conditional likelihood
P(~y|~x) of the ATIS1 training trees, and the marginal likeli-
hood P(~x) of the ATIS1 training strings, as well as the la-
belled precision and recall of the ATIS2 test trees, using the
MLE and MCLE PCFGs.
both the test data set and the differences are so
small, the significance of these results was esti-
mated using a bootstrap method with the differ-
ence in F-score in precision and recall as the test
statistic (Cohen, 1995). This test showed that the
difference was not significant (p ? 0.1). Thus the
MCLE PCFG did not perform significantly bet-
ter than the MLE PCFG in terms of precision and
recall.
3 HMM tagging
As noted in the previous section, maximizing the
conditional likelihood of a PCFG or a HMM can
be computationally intensive. This section and
the next pursues an alternative strategy for com-
paring MLEs and MCLEs: we compare similiar
(but not identical) model classes, one of which
has an easily computed MLE, and the other of
which has an easily computed MCLE. The appli-
cation considered in this section is bitag POS tag-
ging, but the techniques extend straight-forwardly
to n-tag tagging. In this application, the data pairs
(y, x) consist of a tag sequence y = t1 . . . tm
and a word sequence x = w1 . . . wm, where tj
is the tag for word wj (to simplify the formu-
lae, w0, t0, wm+1 and tm+1 are always taken to
be end-markers). Standard HMM tagging models
define a joint distribution over word-tag sequence
pairs; these are most straight-forwardly estimated
by maximizing the likelihood of the joint train-
ing distribution. However, it is straight-forward
to devise closely related HMM tagging models
which define a conditional distribution over tag
sequences given word sequences, and which are
most straight-forwardly estimated by maximizing
the conditional likelihood of the distribution of
tag sequences given word sequences in the train-
ing data.
(4) ? ? ? // Tj //

Tj+1 //

? ? ?
Wj Wj+1
(5) ? ? ? // Tj // Tj+1 // ? ? ?
Wj
OO
Wj+1
OO
(6) ? ? ? // Tj //

Tj+1 //

? ? ?
==|||||||||||| Wj
;;xxxxxxxxxx
Wj+1
==|||||||||||
(7) ? ? ? //
!!D
DD
DD
DD
DD
DD Tj //
##F
FF
FF
FF
FF
F
Tj+1 //
!!B
BB
BB
BB
BB
BB
B
? ? ?
Wj
OO
Wj+1
OO
Figure 2: The HMMs depicted as ?Bayes net? graphical
models.
All of the HMM models investigated in this
section are instances of a certain kind of graph-
ical model that Pearl (1988) calls ?Bayes nets?;
Figure 2 sketches the networks that correspond to
all of the models discussed here. (In such a graph,
the set of incoming arcs to a node depicting a vari-
able indicate the set of variables on which this
variable is conditioned).
Recall the standard bitag HMM model, which
defines a joint distribution over word and tag se-
quences:
P(Y,X) =
m+1
?
j=1
P?(Tj |Tj?1)P?(Wj |Tj) (4)
As is well-known, the MLE for (4) sets P? to the
empirical distributions on the training data.
Now consider the following conditional model
of the conditional distribution of tags given words
(this is a simplified form of the model described
in McCallum et al (2000)):
P(Y |X) =
m+1
?
j=1
P0(Tj |Wj , Tj?1) (5)
The MCLE of (5) is easily calculated: P0 should
be set the empirical distribution of the training
data. However, to minimize sparse data prob-
lems we estimated P0(Tj |Wj, Tj?1) as a mixture
of P?(Tj |Wj), P?(Tj |Tj?1) and P?(Tj |Wj , Tj?1),
where the P? are empirical probabilities and the
(bucketted) mixing parameters are determined us-
ing deleted interpolation from heldout data (Je-
linek, 1997).
These models were trained on sections 2-21
of the Penn tree-bank corpus. Section 22 was
used as heldout data to evaluate the interpola-
tion parameters ?. The tagging accuracy of the
models was evaluated on section 23 of the tree-
bank corpus (in both cases, the tag tj assigned to
word wj is the one which maximizes the marginal
P(tj|w1 . . . wm), since this minimizes the ex-
pected loss on a tag-by-tag basis).
The conditional model (5) has the worst perfor-
mance of any of the tagging models investigated
in this section: its tagging accuracy is 94.4%. The
joint model (4) has a considerably lower error
rate: its tagging accuracy is 95.5%.
One possible explanation for this result is that
the way in which the interpolated estimate of P0
is calculated, rather than conditional likelihood
estimation per se, is lowering tagger accuracy
somehow. To investigate this possibility, two ad-
ditional joint models were estimated and tested,
based on the formulae below.
P(Y,X) =
m+1
?
j=1
P?(Wj |Tj)P1(Tj |Wj?1, Tj?1) (6)
P(Y,X) =
m+1
?
j=1
P0(Tj |Wj, Tj?1)P?(Wj |Tj?1) (7)
The MLEs for both (6) and (7) are easy to cal-
culate. (6) contains a conditional distribution P1
which would seem to be of roughly equal com-
plexity to P0, and it was estimated using deleted
interpolation in exactly the same way as P0, so
if the poor performance of the conditional model
was due to some artifact of the interpolation pro-
cedure, we would expect the model based on (6)
to perform poorly. Yet the tagger based on (6)
performs the best of all the taggers investigated in
this section: its tagging accuracy is 96.2%.
(7) is admitted a rather strange model, since
the right hand term in effect predicts the follow-
ing word from the current word?s tag. However,
note that (7) differs from (5) only via the pres-
ence of this rather unusual term, which effectively
converts (5) from a conditional model to a joint
model. Yet adding this term improves tagging ac-
curacy considerably, to 95.3%. Thus for bitag tag-
ging at least, the conditional model has a consid-
erably higher error rate than any of the joint mod-
els examined here. (While a test of significance
was not conducted here, previous experience with
this test set shows that performance differences
of this magnitude are extremely significant statis-
tically).
4 Shift-reduce parsing
The previous section compared similiar joint and
conditional tagging models. This section com-
pares a pair of joint and conditional parsing mod-
els. The models are both stochastic shift-reduce
parsers; they differ only in how the distribution
over possible next moves are calculated. These
parsers are direct simplifications of the Structured
Language Model (Jelinek, 2000). Because the
parsers? moves are determined solely by the top
two category labels on the stack and possibly the
look-ahead symbol, they are much simpler than
stochastic LR parsers (Briscoe and Carroll, 1993;
Inui et al, 1997). The distribution over trees
generated by the joint model is a probabilistic
context-free language (Abney et al, 1999). As
with the PCFG models discussed earlier, these
parsers are not lexicalized; lexical items are ig-
nored, and the POS tags are used as the terminals.
These two parsers only produce trees with
unary or binary nodes, so we binarized the train-
ing data before training the parser, and debina-
rize the trees the parsers produce before evaluat-
ing them with respect to the test data (Johnson,
1998). We binarized by inserting n? 2 additional
nodes into each local tree with n > 2 children.
We binarized by first joining the head to all of the
constituents to its right, and then joining the re-
sulting structure with constituents to the left. The
label of a new node is the label of the head fol-
lowed by the suffix ?-1? if the head is (contained
in) the right child or ?-2? if the head is (contained
in) the left child. Figure 3 depicts an example of
this transformation.
The Structured Language Model is described
in detail in Jelinek (2000), so it is only reviewed
here. Each parser?s stack is a sequence of node
(b)
(a)
VP
RB
usually
VBZ-1
RB
only
VBZ-2
VBZ-2
VBZ
eats
NP
pizza
ADVP
quickly
ADVP
quickly
VP
RB
usually
RB
only
VBZ
eats
NP
pizza
Figure 3: The binarization transformation used in the shift-
reduce parser experiments transforms tree (a) into tree (b).
labels (possibly including labels introduced by bi-
narization). In what follows, s1 refers to the top
element of the stack, or ??? if the stack is empty;
similarly s2 refers to the next-to-top element of
the stack or ??? if the stack contains less than two
elements. We also append a ??? to end of the ac-
tual terminal string being parsed (just as with the
HMMs above), as this simplifies the formulation
of the parsers, i.e., if the string to be parsed is
w1 . . . wm, then we take wm+1 = ?.
A shift-reduce parse is defined in terms of
moves. A move is either shift(w), reduce1(c) or
reduce2(c), where c is a nonterminal label and w
is either a terminal label or ???. Moves are par-
tial functions from stacks to stacks: a shift(w)
move pushes a w onto the top of stack, while a
reducei(c) move pops the top i terminal or non-
terminal labels off the stack and pushes a c onto
the stack. A shift-reduce parse is a sequence of
moves which (when composed) map the empty
stack to the two-element stack whose top element
is ??? and whose next-to-top element is the start
symbol. (Note that the last move in a shift-reduce
parse must always be a shift(?) move; this cor-
responds to the final ?accept? move in an LR
parser). The isomorphism between shift-reduce
parses and standard parse trees is well-known
(Hopcroft and Ullman, 1979), and so is not de-
scribed here.
A (joint) shift-reduce parser is defined by
a distribution P(m|s1, s2) over next moves m
given the top and next-to-top stack labels s1
and s2. To ensure that the next move is in
fact a possible move given the current stack,
we require that P(reduce1(c)|?, ?) = 0 and
P(reduce2(c)|c?, ?) = 0 for all c, c?, and that
P(shift(?)|s1, s2) = 0 unless s1 is the start sym-
bol and s2 = ?. Note that this extends to a
probability distribution over shift-reduce parses
(and hence parse trees) in a particularly simple
way: the probability of a parse is the product of
the probabilities of the moves it consists of. As-
suming that P meets certain tightness conditions,
this distribution over parses is properly normal-
ized because there are no ?dead? stack configura-
tions: we require that the distribution over moves
be defined for all possible stacks.
A conditional shift-reduce parser differs only
minimally from the shift-reduce parser just
described: it is defined by a distribution
P(m|s1, s2, t) over next moves m given the top
and next-to-top stack labels s1, s2 and the next
input symbol w (w is called the look-ahead sym-
bol). In addition to the requirements on P
above, we also require that if w? 6= w then
P(shift(w?)|s1, s2, w) = 0 for all s1, s2; i.e.,
shift moves can only shift the current look-ahead
symbol. This restriction implies that all non-zero
probability derivations are derivations of the parse
string, since the parse string forces a single se-
quence of symbols to be shifted in all derivations.
As before, since there are no ?dead? stack con-
figurations, so long as P obeys certain tightness
conditions, this defines a properly normalized dis-
tribution over parses. Since all the parses are re-
quired to be parses of of the input string, this de-
fines a conditional distribution over parses given
the input string.
It is easy to show that the MLE for the joint
model, and the MCLE for the conditional model,
are just the empirical distributions from the train-
ing data. We ran into sparse data problems using
the empirical training distribution as an estimate
for P(m|s1, s2, w) in the conditional model, so
in fact we used deleted interpolation to interpo-
late P?(m|s1, s2, w), and P?(m|s1, s2) to estimate
P(m|s1, s2, w). The models were estimated from
sections 2?21 of the Penn treebank, and tested on
the 2245 sentences of length 40 or less in section
23. The deleted interpolation parameters were es-
timated using heldout training data from section
Joint SR Conditional SR PCFG
Precision 0.666 0.633 0.700
Recall 0.650 0.639 0.657
Table 2: Labelled precision and recall results for joint and
conditional shift-reduce parsers, and for a PCFG.
22.
We calculated the most probable parses using
a dynamic programming algorithm based on the
one described in Jelinek (2000). Jelinek notes that
this algorithm?s running time is n6 (where n is the
length of sentence being parsed), and we found
exhaustive parsing to be computationally imprac-
tical. We used a beam search procedure which
thresholded the best analyses of each prefix of the
string being parsed, and only considered analyses
whose top two stack symbols had been observed
in the training data. In order to help guard against
the possibility that this stochastic pruning influ-
enced the results, we ran the parsers twice, once
with a beam threshold of 10?6 (i.e., edges whose
probability was less than 10?6 of the best edge
spanning the same prefix were pruned) and again
with a beam threshold of 10?9. The results of
the latter runs are reported in table 2; the labelled
precision and recall results from the run with the
more restrictive beam threshold differ by less than
0.001, i.e., at the level of precision reported here,
are identical with the results presented in table 2
except for the Precision of the Joint SR parser,
which was 0.665. For comparision, table 2 also
reports results from the non-lexicalized treebank
PCFG estimated from the transformed trees in
sections 2-21 of the treebank; here exhaustive
CKY parsing was used to find the most probable
parses.
All of the precision and recall results, including
those for the PCFG, presented in table 2 are much
lower than those from a standard treebank PCFG;
presumably this is because the binarization trans-
formation depicted in Figure 3 loses informa-
tion about pairs of non-head constituents in the
same local tree (Johnson (1998) reports similiar
performance degradation for other binarization
transformations). Both the joint and the condi-
tional shift-reduce parsers performed much worse
than the PCFG. This may be due to the pruning
effect of the beam search, although this seems
unlikely given that varying the beam threshold
did not affect the results. The performance dif-
ference between the joint and conditional shift-
reduce parsers bears directly on the issue ad-
dressed by this paper: the joint shift-reduce parser
performed much better than the conditional shift-
reduce parser. The differences are around a per-
centage point, which is quite large in parsing re-
search (and certainly highly significant).
The fact that the joint shift-reduce parser out-
performs the conditional shift-reduce parser is
somewhat surprising. Because the conditional
parser predicts its next move on the basis of the
lookahead symbol as well as the two top stack
categories, one might expect it to predict this next
move more accurately than the joint shift-reduce
parser. The results presented here show that this
is not the case, at least for non-lexicalized pars-
ing. The label bias of conditional models may be
responsible for this (Bottou, 1991; Lafferty et al,
2001).
5 Conclusion
This paper has investigated the difference be-
tween maximum likelihood estimation and max-
imum conditional likelihood estimation for three
different kinds of models: PCFG parsers, HMM
taggers and shift-reduce parsers. The results for
the PCFG parsers suggested that conditional es-
timation might provide a slight performance im-
provement, although the results were not statis-
tically significant since computational difficulty
of conditional estimation of a PCFG made it
necessary to perform the experiment on a tiny
training and test corpus. In order to avoid the
computational difficulty of conditional estima-
tion, we compared closely related (but not identi-
cal) HMM tagging and shift-reduce parsing mod-
els, for some of which the maximum likelihood
estimates were easy to compute and for others of
which the maximum conditional likelihood esti-
mates could be easily computed. In both cases,
the joint models outperformed the conditional
models by quite large amounts. This suggests
that it may be worthwhile investigating meth-
ods for maximum (joint) likelihood estimation
for model classes for which only maximum con-
ditional likelihood estimators are currently used,
such as Maximum Entropy models and MEMMs,
since if the results of the experiments presented
in this paper extend to these models, one might
expect a modest performance improvement.
As explained in the introduction, because max-
imum likelihood estimation exploits not just the
conditional distribution of hidden variable (e.g.,
the tags or the parse) conditioned on the visible
variable (the terminal string) but also the marginal
distribution of the visible variable, it is reason-
able to expect that it should outperform maxi-
mum conditional likelihood estimation. Yet it
is counter-intuitive that joint tagging and shift-
reduce parsing models, which predict the next tag
or parsing move on the basis of what seems to
be less information than the corresponding con-
ditional model, should nevertheless outperform
that conditional model, as the experimental re-
sults presented here show. The recent theoreti-
cal and simulation results of Lafferty et al (2001)
suggest that conditional models may suffer from
label bias (the discovery of which Lafferty et. al.
attribute to Bottou (1991)), which may provide an
insightful explanation of these results.
None of the models investigated here are state-
of-the-art; the goal here is to compare two dif-
ferent estimation procedures, and for that rea-
son this paper concentrated on simple, easily im-
plemented models. However, it would also be
interesting to compare the performance of joint
and conditional estimators on more sophisticated
models.
References
Steven Abney, David McAllester, and Fernando
Pereira. 1999. Relating probabilistic grammars and
automata. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 542?549, San Francisco. Morgan Kauf-
mann.
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
J. Besag. 1975. Statistical analysis of non-lattice data.
The Statistician, 24:179?195.
Le?on Bottou. 1991. Une Approche the?orique de
l?Apprentissage Connexionniste: Applications a` la
Reconnaissance de la Parole. Ph.D. thesis, Univer-
site? de Paris XI.
Ted Briscoe and John Carroll. 1993. Generalized
probabilistic LR parsing of natural language (cor-
pora) with unification-based methods. Computa-
tional Linguistics, 19:25?59.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cial Intelligence. The MIT Press, Cambridge, Mas-
sachusetts.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380?393.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Com-
putation. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Toku-
naga. 1997. A new formalization of probabilistic
GLR parsing. In Proceedings of the Fifth Interna-
tional Workshop on Parsing Technologies (IWPT-
97), pages 123?134, MIT.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. The MIT Press, Cambridge,
Massachusetts.
Frederick Jelinek. 2000. Stochastic analysis of struc-
tured language modeling. Technical report, Center
for Language and Speech Modeling, Johns Hopkins
University.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In The
Proceedings of the 37th Annual Conference of the
Association for Computational Linguistics, pages
535?541, San Francisco. Morgan Kaufmann.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Machine Learning: Proceedings
of the Eighteenth International Conference (ICML
2001).
K. Lari and S.J. Young. 1990. The estimation
of Stochastic Context-Free Grammars using the
Inside-Outside algorithm. Computer Speech and
Language, 4(35-56).
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for information extraction and segmentation. In
Machine Learning: Proceedings of the Seventeenth
International Conference (ICML 2000), pages 591?
598, Stanford, California.
Judea Pearl. 1988. Probabalistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, California.
A simple pattern-matching algorithm for recovering empty nodes
and their antecedents?
Mark Johnson
Brown Laboratory for Linguistic Information Processing
Brown University
Mark Johnson@Brown.edu
Abstract
This paper describes a simple pattern-
matching algorithm for recovering empty
nodes and identifying their co-indexed an-
tecedents in phrase structure trees that do
not contain this information. The pat-
terns are minimal connected tree frag-
ments containing an empty node and all
other nodes co-indexed with it. This pa-
per also proposes an evaluation proce-
dure for empty node recovery procedures
which is independent of most of the de-
tails of phrase structure, which makes it
possible to compare the performance of
empty node recovery on parser output
with the empty node annotations in a gold-
standard corpus. Evaluating the algorithm
on the output of Charniak?s parser (Char-
niak, 2000) and the Penn treebank (Mar-
cus et al, 1993) shows that the pattern-
matching algorithm does surprisingly well
on the most frequently occuring types of
empty nodes given its simplicity.
1 Introduction
One of the main motivations for research on pars-
ing is that syntactic structure provides important in-
formation for semantic interpretation; hence syntac-
tic parsing is an important rst step in a variety of
? I would like to thank my colleages in the Brown Labora-
tory for Linguistic Information Processing (BLLIP) as well as
Michael Collins for their advice. This research was supported
by NSF awards DMS 0074276 and ITR IIS 0085940.
useful tasks. Broad coverage syntactic parsers with
good performance have recently become available
(Charniak, 2000; Collins, 2000), but these typically
produce as output a parse tree that only encodes lo-
cal syntactic information, i.e., a tree that does not
include any ?empty nodes?. (Collins (1997) dis-
cusses the recovery of one kind of empty node, viz.,
WH-traces). This paper describes a simple pattern-
matching algorithm for post-processing the output
of such parsers to add a wide variety of empty nodes
to its parse trees.
Empty nodes encode additional information about
non-local dependencies between words and phrases
which is important for the interpretation of construc-
tions such as WH-questions, relative clauses, etc.1
For example, in the noun phrase the man Sam likes
the fact the man is interpreted as the direct object of
the verb likes is indicated in Penn treebank notation
by empty nodes and coindexation as shown in Fig-
ure 1 (see the next section for an explanation of why
likes is tagged VBZ t rather than the standard VBZ).
The broad-coverage statistical parsers just men-
tioned produce a simpler tree structure for such a rel-
ative clause that contains neither of the empty nodes
just indicated. Rather, they produce trees of the kind
shown in Figure 2. Unlike the tree depicted in Fig-
ure 1, this type of tree does not explicitly represent
the relationship between likes and the man.
This paper presents an algorithm that takes as its
input a tree without empty nodes of the kind shown
1There are other ways to represent this information that do
not require empty nodes; however, information about non-local
dependencies must be represented somehow in order to interpret
these constructions.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 136-143.
                         Proceedings of the 40th Annual Meeting of the Association for
NP
NP
DT
the
NN
man
SBAR
WHNP-1
-NONE-
0
S
NP
NNP
Sam
VP
VBZ t
likes
NP
-NONE-
*T*-1
Figure 1: A tree containing empty nodes.
in Figure 2 and modies it by inserting empty nodes
and coindexation to produce a the tree shown in Fig-
ure 1. The algorithm is described in detail in sec-
tion 2. The standard Parseval precision and recall
measures for evaluating parse accuracy do not mea-
sure the accuracy of empty node and antecedent re-
covery, but there is a fairly straightforward extension
of them that can evaluate empty node and antecedent
recovery, as described in section 3. The rest of this
section provides a brief introduction to empty nodes,
especially as they are used in the Penn Treebank.
Non-local dependencies and displacement phe-
nomena, such as Passive and WH-movement, have
been a central topic of generative linguistics since
its inception half a century ago. However, current
linguistic research focuses on explaining the pos-
sible non-local dependencies, and has little to say
about how likely different kinds of dependencies
are. Many current linguistic theories of non-local
dependencies are extremely complex, and would be
difcult to apply with the kind of broad coverage de-
scribed here. Psycholinguists have also investigated
certain kinds of non-local dependencies, and their
theories of parsing preferences might serve as the
basis for specialized algorithms for recovering cer-
tain kinds of non-local dependencies, such as WH
dependencies. All of these approaches require con-
siderably more specialized linguitic knowledge than
the pattern-matching algorithm described here. This
algorithm is both simple and general, and can serve
as a benchmark against which more complex ap-
proaches can be evaluated.
NP
NP
DT
the
NN
man
SBAR
S
NP
NNP
Sam
VP
VBZ t
likes
Figure 2: A typical parse tree produced by broad-
coverage statistical parser lacking empty nodes.
The pattern-matching approach is not tied to any
particular linguistic theory, but it does require a tree-
bank training corpus from which the algorithm ex-
tracts its patterns. We used sections 2?21 of the
Penn Treebank as the training corpus; section 24
was used as the development corpus for experimen-
tation and tuning, while the test corpus (section 23)
was used exactly once (to obtain the results in sec-
tion 3). Chapter 4 of the Penn Treebank tagging
guidelines (Bies et al, 1995) contains an extensive
description of the kinds of empty nodes and the use
of co-indexation in the Penn Treebank. Table 1
contains summary statistics on the distribution of
empty nodes in the Penn Treebank. The entry with
POS SBAR and no label refers to a ?compound?
type of empty structure labelled SBAR consisting of
an empty complementizer and an empty (moved) S
(thus SBAR is really a nonterminal label rather than
a part of speech); a typical example is shown in
Figure 3. As might be expected the distribution is
highly skewed, with most of the empty node tokens
belonging to just a few types. Because of this, a sys-
tem can provide good average performance on all
empty nodes if it performs well on the most frequent
types of empty nodes, and conversely, a system will
perform poorly on average if it does not perform at
least moderately well on the most common types of
empty nodes, irrespective of how well it performs on
more esoteric constructions.
2 A pattern-matching algorithm
This section describes the pattern-matching algo-
rithm in detail. In broad outline the algorithm can
Antecedent POS Label Count Description
NP NP * 18,334 NP trace (e.g., Sam was seen *)
NP * 9,812 NP PRO (e.g., * to sleep is nice)
WHNP NP *T* 8,620 WH trace (e.g., the woman who you saw *T*)
*U* 7,478 Empty units (e.g., $ 25 *U*)
0 5,635 Empty complementizers (e.g., Sam said 0 Sasha snores)
S S *T* 4,063 Moved clauses (e.g., Sam had to go, Sasha explained *T*)
WHADVP ADVP *T* 2,492 WH-trace (e.g., Sam explained how to leave *T*)
SBAR 2,033 Empty clauses (e.g., Sam had to go, Sasha explained (SBAR))
WHNP 0 1,759 Empty relative pronouns (e.g., the woman 0 we saw)
WHADVP 0 575 Empty relative pronouns (e.g., no reason 0 to leave)
Table 1: The distribution of the 10 most frequent types of empty nodes and their antecedents in sections 2?
21 of the Penn Treebank (there are approximately 64,000 empty nodes in total). The ?label? column gives
the terminal label of the empty node, the ?POS? column gives its preterminal label and the ?Antecedent?
column gives the label of its antecedent. The entry with an SBAR POS and empty label corresponds to an
empty compound SBAR subtree, as explained in the text and Figure 3.
SINV
S-1
NP
NNS
changes
VP
VBD
occured
,
,
VP
VBD
said
SBAR
-NONE-
0
S
-NONE-
*T*-1
NP
NNP
Sam
Figure 3: A parse tree containing an empty com-
pound SBAR subtree.
be regarded as an instance of the Memory-Based
Learning approach, where both the pattern extrac-
tion and pattern matching involve recursively visit-
ing all of the subtrees of the tree concerned. It can
also be regarded as a kind of tree transformation, so
the overall system architecture (including the parser)
is an instance of the ?transform-detransform? ap-
proach advocated by Johnson (1998). The algorithm
has two phases. The rst phase of the algorithm
extracts the patterns from the trees in the training
corpus. The second phase of the algorithm uses
these extracted patterns to insert empty nodes and
index their antecedents in trees that do not contain
empty nodes. Before the trees are used in the train-
ing and insertion phases they are passed through a
common preproccessing step, which relabels preter-
minal nodes dominating auxiliary verbs and transi-
tive verbs.
2.1 Auxiliary and transitivity annotation
The preprocessing step relabels auxiliary verbs and
transitive verbs in all trees seen by the algorithm.
This relabelling is deterministic and depends only on
the terminal (i.e., the word) and its preterminal label.
Auxiliary verbs such as is and being are relabelled as
either a AUX or AUXG respectively. The relabelling
of auxiliary verbs was performed primarily because
Charniak?s parser (which produced one of the test
corpora) produces trees with such labels; experi-
ments (on the development section) show that aux-
iliary relabelling has little effect on the algorithm?s
performance.
The transitive verb relabelling sufxes the preter-
minal labels of transitive verbs with ? t?. For ex-
ample, in Figure 1 the verb likes is relabelled VBZ t
in this step. A verb is deemed transitive if its stem
is followed by an NP without any grammatical func-
tion annotation at least 50% of the time in the train-
ing corpus; all such verbs are relabelled whether or
not any particular instance is followed by an NP.
Intuitively, transitivity would seem to be a power-
ful cue that there is an empty node following a verb.
Experiments on the development corpus showed that
transitivity annotation provides a small but useful
improvement to the algorithm?s performance. The
SBAR
WHNP-1
-NONE-
0
S
NP VP
VBZ t NP
-NONE-
*T*-1
Figure 4: A pattern extracted from the tree displayed
in Figure 1.
accuracy of transitivity labelling was not systemati-
cally evaluated here.
2.2 Patterns and matchings
Informally, patterns are minimal connected tree
fragments containing an empty node and all nodes
co-indexed with it. The intuition is that the path
from the empty node to its antecedents species im-
portant aspects of the context in which the empty
node can appear.
There are many different possible ways of realiz-
ing this intuition, but all of the ones tried gave ap-
proximately similar results so we present the sim-
plest one here. The results given below were gener-
ated where the pattern for an empty node is the min-
imal tree fragment (i.e., connected set of local trees)
required to connect the empty node with all of the
nodes coindexed with it. Any indices occuring on
nodes in the pattern are systematically renumbered
beginning with 1. If an empty node does not bear
an index, its pattern is just the local tree containing
it. Figure 4 displays the single pattern that would be
extracted corresponding to the two empty nodes in
the tree depicted in Figure 1.
For this kind of pattern we dene pattern match-
ing informally as follows. If p is a pattern and t is
a tree, then p matches t iff t is an extension of p ig-
noring empty nodes in p. For example, the pattern
displayed in Figure 4 matches the subtree rooted un-
der SBAR depicted in Figure 2.
If a pattern p matches a tree t, then it is possible
to substitute p for the fragment of t that it matches.
For example, the result of substituting the pattern
shown in Figure 4 for the subtree rooted under SBAR
depicted in Figure 2 is the tree shown in Figure 1.
Note that the substitution process must ?standardize
apart? or renumber indices appropriately in order to
avoid accidentally labelling empty nodes inserted by
two independent patterns with the same index.
Pattern matching and substitution can be dened
more rigorously using tree automata (G?ecseg and
Steinby, 1984), but for reasons of space these def-
initions are not given here.
In fact, the actual implementation of pattern
matching and substitution used here is considerably
more complex than just described. It goes to some
lengths to handle complex cases such as adjunction
and where two or more empty nodes? paths cross
(in these cases the pattern extracted consists of the
union of the local trees that constitute the patterns
for each of the empty nodes). However, given the
low frequency of these constructions, there is prob-
ably only one case where this extra complexity is
justied: viz., the empty compound SBAR subtree
shown in Figure 3.
2.3 Empty node insertion
Suppose we have a rank-ordered list of patterns (the
next subsection describes how to obtain such a list).
The procedure that uses these to insert empty nodes
into a tree t not containing empty nodes is as fol-
lows. We perform a pre-order traversal of the sub-
trees of t (i.e., visit parents before their children),
and at each subtree we nd the set of patterns that
match the subtree. If this set is non-empty we sub-
stitute the highest ranked pattern in the set into the
subtree, inserting an empty node and (if required)
co-indexing it with its antecedents.
Note that the use of a pre-order traversal effec-
tively biases the procedure toward ?deeper?, more
embedded patterns. Since empty nodes are typi-
cally located in the most embedded local trees of
patterns (i.e., movement is usually ?upward? in a
tree), if two different patterns (corresponding to dif-
ferent non-local dependencies) could potentially in-
sert empty nodes into the same tree fragment in t,
the deeper pattern will match at a higher node in t,
and hence will be substituted. Since the substitu-
tion of one pattern typically destroys the context for
a match of another pattern, the shallower patterns
no longer match. On the other hand, since shal-
lower patterns contain less structure they are likely
to match a greater variety of trees than the deeper
patterns, they still have ample opportunity to apply.
Finally, the pattern matching process can be
speeded considerably by indexing patterns appropri-
ately, since the number of patterns involved is quite
large (approximately 11,000). For patterns of the
kind described here, patterns can be indexed on their
topmost local tree (i.e., the pattern?s root node label
and the sequence of node labels of its children).
2.4 Pattern extraction
After relabelling preterminals as described above,
patterns are extracted during a traversal of each of
the trees in the training corpus. Table 2 lists the
most frequent patterns extracted from the Penn Tree-
bank training corpus. The algorithm also records
how often each pattern was seen; this is shown in
the ?count? column of Table 2.
The next step of the algorithm determines approx-
imately how many times each pattern can match
some subtree of a version of the training corpus from
which all empty nodes have been removed (regard-
less of whether or not the corresponding substitu-
tions would insert empty nodes correctly). This in-
formation is shown under the ?match? column in Ta-
ble 2, and is used to lter patterns which would most
often be incorrect to apply even though they match.
If c is the count value for a pattern and m is its match
value, then the algorithm discards that pattern when
the lower bound of a 67% condence interval for its
success probability (given c successes out of m tri-
als) is less than 1/2. This is a standard technique
for ?discounting? success probabilities from small
sample size data (Witten and Frank, 2000). (As ex-
plained immediately below, the estimates of c and m
given in Table 2 are inaccurate, so whenever the es-
timate of m is less than c we replace m by c in this
calculation). This pruning removes approximately
2,000 patterns, leaving 9,000 patterns.
The match value is obtained by making a second
pre-order traversal through a version of the train-
ing data from which empty nodes are removed. It
turns out that subtle differences in how the match
value is obtained make a large difference to the algo-
rithm?s performance. Initially we dened the match
value of a pattern to be the number of subtrees that
match that pattern in the training corpus. But as ex-
plained above, the earlier substitution of a deeper
pattern may prevent smaller patterns from applying,
so this simple denition of match value undoubt-
edly over-estimates the number of times shallow pat-
terns might apply. To avoid this over-estimation, af-
ter we have matched all patterns against a node of
a training corpus tree we determine the correct pat-
tern (if any) to apply in order to recover the empty
nodes that were originally present, and reinsert the
relevant empty nodes. This blocks the matching of
shallower patterns, reducing their match values and
hence raising their success probability. (Undoubt-
edly the ?count? values are also over-estimated in
the same way; however, experiments showed that es-
timating count values in a similar manner to the way
in which match values are estimated reduces the al-
gorithm?s performance).
Finally, we rank all of the remaining patterns. We
experimented with several different ranking crite-
ria, including pattern depth, success probability (i.e.,
c/m) and discounted success probability. Perhaps
surprisingly, all produced similiar results on the de-
velopment corpus. We used pattern depth as the
ranking criterion to produce the results reported be-
low because it ensures that ?deep? patterns receive
a chance to apply. For example, this ensures that
the pattern inserting an empty NP * and WHNP can
apply before the pattern inserting an empty comple-
mentizer 0.
3 Empty node recovery evaluation
The previous section described an algorithm for
restoring empty nodes and co-indexing their an-
tecedents. This section describes two evaluation
procedures for such algorithms. The rst, which
measures the accuracy of empty node recovery but
not co-indexation, is just the standard Parseval eval-
uation applied to empty nodes only, viz., precision
and recall and scores derived from these. In this
evaluation, each node is represented by a triple con-
sisting of its category and its left and right string po-
sitions. (Note that because empty nodes dominate
the empty string, their left and right string positions
of empty nodes are always identical).
Let G be the set of such empty node represen-
tations derived from the ?gold standard? evaluation
corpus and T the set of empty node representations
Count Match Pattern
5816 6223 (S (NP (-NONE- *)) VP)
5605 7895 (SBAR (-NONE- 0) S)
5312 5338 (SBAR WHNP-1 (S (NP (-NONE- *T*-1)) VP))
4434 5217 (NP QP (-NONE- *U*))
1682 1682 (NP $ CD (-NONE- *U*))
1327 1593 (VP VBN t (NP (-NONE- *)) PP)
700 700 (ADJP QP (-NONE- *U*))
662 1219 (SBAR (WHNP-1 (-NONE- 0)) (S (NP (-NONE- *T*-1)) VP))
618 635 (S S-1 , NP (VP VBD (SBAR (-NONE- 0) (S (-NONE- *T*-1)))) .)
499 512 (SINV ?? S-1 , ?? (VP VBZ (S (-NONE- *T*-1))) NP .)
361 369 (SINV ?? S-1 , ?? (VP VBD (S (-NONE- *T*-1))) NP .)
352 320 (S NP-1 (VP VBZ (S (NP (-NONE- *-1)) VP)))
346 273 (S NP-1 (VP AUX (VP VBN t (NP (-NONE- *-1)) PP)))
322 467 (VP VBD t (NP (-NONE- *)) PP)
269 275 (S ?? S-1 , ?? NP (VP VBD (S (-NONE- *T*-1))) .)
Table 2: The most common empty node patterns found in the Penn Treebank training corpus. The Count
column is the number of times the pattern was found, and the Match column is an estimate of the number of
times that this pattern matches some subtree in the training corpus during empty node recovery, as explained
in the text.
derived from the corpus to be evaluated. Then as is
standard, the precision P , recall R and f-score f are
calculated as follows:
P = |G ? T ||T |
R = |G ? T ||G|
f = 2P RP + R
Table 3 provides these measures for two different
test corpora: (i) a version of section 23 of the
Penn Treebank from which empty nodes, indices
and unary branching chains consisting of nodes of
the same category were removed, and (ii) the trees
produced by Charniak?s parser on the strings of sec-
tion 23 (Charniak, 2000).
To evaluate co-indexation of empty nodes and
their antecedents, we augment the representation of
empty nodes as follows. The augmented represen-
tation for empty nodes consists of the triple of cat-
egory plus string positions as above, together with
the set of triples of all of the non-empty nodes the
empty node is co-indexed with. (Usually this set
of antecedents is either empty or contains a single
node). Precision, recall and f-score are dened for
these augmented representations as before.
Note that this is a particularly stringent evalua-
tion measure for a system including a parser, since
it is necessary for the parser to produce a non-empty
node of the correct category in the correct location to
serve as an antecedent for the empty node. Table 4
provides these measures for the same two corpora
described earlier.
In an attempt to devise an evaluation measure for
empty node co-indexation that depends less on syn-
tactic structure we experimented with a modied
augmented empty node representation in which each
antecedent is represented by its head?s category and
location. (The intuition behind this is that we do
not want to penalize the empty node antecedent-
nding algorithm if the parser misattaches modi-
ers to the antecedent). In fact this head-based an-
tecedent representation yields scores very similiar
to those obtained using the phrase-based represen-
tation. It seems that in the cases where the parser
does not construct a phrase in the appropriate loca-
tion to serve as the antecedent for an empty node,
the syntactic structure is typically so distorted that
either the pattern-matcher fails or the head-nding
algorithm does not return the ?correct? head either.
Empty node Section 23 Parser output
POS Label P R f P R f
(Overall) 0.93 0.83 0.88 0.85 0.74 0.79
NP * 0.95 0.87 0.91 0.86 0.79 0.82
NP *T* 0.93 0.88 0.91 0.85 0.77 0.81
0 0.94 0.99 0.96 0.86 0.89 0.88
*U* 0.92 0.98 0.95 0.87 0.96 0.92
S *T* 0.98 0.83 0.90 0.97 0.81 0.88
ADVP *T* 0.91 0.52 0.66 0.84 0.42 0.56
SBAR 0.90 0.63 0.74 0.88 0.58 0.70
WHNP 0 0.75 0.79 0.77 0.48 0.46 0.47
Table 3: Evaluation of the empty node restoration procedure ignoring antecedents. Individual results are
reported for all types of empty node that occured more than 100 times in the ?gold standard? corpus (sec-
tion 23 of the Penn Treebank); these are ordered by frequency of occurence in the gold standard. Section 23
is a test corpus consisting of a version of section 23 from which all empty nodes and indices were removed.
The parser output was produced by Charniak?s parser (Charniak, 2000).
Empty node Section 23 Parser output
Antecedant POS Label P R f P R f
(Overall) 0.80 0.70 0.75 0.73 0.63 0.68
NP NP * 0.86 0.50 0.63 0.81 0.48 0.60
WHNP NP *T* 0.93 0.88 0.90 0.85 0.77 0.80
NP * 0.45 0.77 0.57 0.40 0.67 0.50
0 0.94 0.99 0.96 0.86 0.89 0.88
*U* 0.92 0.98 0.95 0.87 0.96 0.92
S S *T* 0.98 0.83 0.90 0.96 0.79 0.87
WHADVP ADVP *T* 0.91 0.52 0.66 0.82 0.42 0.56
SBAR 0.90 0.63 0.74 0.88 0.58 0.70
WHNP 0 0.75 0.79 0.77 0.48 0.46 0.47
Table 4: Evaluation of the empty node restoration procedure including antecedent indexing, using the mea-
sure explained in the text. Other details are the same as in Table 4.
4 Conclusion
This paper described a simple pattern-matching al-
gorithm for restoring empty nodes in parse trees
that do not contain them, and appropriately index-
ing these nodes with their antecedents. The pattern-
matching algorithm combines both simplicity and
reasonable performance over the frequently occur-
ing types of empty nodes.
Performance drops considerably when using trees
produced by the parser, even though this parser?s
precision and recall is around 0.9. Presumably this
is because the pattern matching technique requires
that the parser correctly identify large tree fragments
that encode long-range dependencies not captured
by the parser. If the parser makes a single parsing
error anywhere in the tree fragment matched by a
pattern, the pattern will no longer match. This is
not unlikely since the statistical model used by the
parser does not model these larger tree fragments.
It suggests that one might improve performance by
integrating parsing, empty node recovery and an-
tecedent nding in a single system, in which case the
current algorithm might serve as a useful baseline.
Alternatively, one might try to design a ?sloppy? pat-
tern matching algorithm which in effect recognizes
and corrects common parser errors in these construc-
tions.
Also, it is undoubtedly possible to build pro-
grams that can do better than this algorithm on
special cases. For example, we constructed a
Boosting classier which does recover *U* and
empty complementizers 0 more accurately than
the pattern-matcher described here (although the
pattern-matching algorithm does quite well on these
constructions), but this classier?s performance av-
eraged over all empty node types was approximately
the same as the pattern-matching algorithm.
As a comparison of tables 3 and 4 shows, the
pattern-matching algorithm?s biggest weakness is its
inability to correctly distinguish co-indexed NP *
(i.e., NP PRO) from free (i.e., unindexed) NP *.
This seems to be a hard problem, and lexical infor-
mation (especially the class of the governing verb)
seems relevant. We experimented with specialized
classiers for determining if an NP * is co-indexed,
but they did not perform much better than the algo-
rithm presented here. (Also, while we did not sys-
tematically investigate this, there seems to be a num-
ber of errors in the annotation of free vs. co-indexed
NP * in the treebank).
There are modications and variations on this al-
gorithm that are worth exploring in future work.
We experimented with lexicalizing patterns, but
the simple method we tried did not improve re-
sults. Inspired by results suggesting that the pattern-
matching algorithm suffers from over-learning (e.g.,
testing on the training corpus), we experimented
with more abstract ?skeletal? patterns, which im-
proved performance on some types of empty nodes
but hurt performance on others, leaving overall per-
formance approximately unchanged. Possibly there
is a way to use both skeletal and the original kind of
patterns in a single system.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre, 1995. Bracketting Guideliness for Treebank II
style Penn Treebank Project. Linguistic Data Consor-
tium.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182, Stanford, California.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ian H. Witten and Eibe Frank. 2000. Data mining: prac-
tical machine learning tools and techniques with Java
implementations. Morgan Kaufmann, San Francisco.
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Dynamic programming for parsing and estimation of
stochastic unication-based grammars?
Stuart Geman
Division of Applied Mathematics
Brown University
geman@dam.brown.edu
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Mark Johnson@Brown.edu
Abstract
Stochastic unification-based grammars
(SUBGs) define exponential distributions
over the parses generated by a unification-
based grammar (UBG). Existing algo-
rithms for parsing and estimation require
the enumeration of all of the parses of a
string in order to determine the most likely
one, or in order to calculate the statis-
tics needed to estimate a grammar from
a training corpus. This paper describes a
graph-based dynamic programming algo-
rithm for calculating these statistics from
the packed UBG parse representations of
Maxwell and Kaplan (1995) which does
not require enumerating all parses. Like
many graphical algorithms, the dynamic
programming algorithm?s complexity is
worst-case exponential, but is often poly-
nomial. The key observation is that by
using Maxwell and Kaplan packed repre-
sentations, the required statistics can be
rewritten as either the max or the sum of
a product of functions. This is exactly
the kind of problem which can be solved
by dynamic programming over graphical
models.
? We would like to thank Eugene Charniak, Miyao
Yusuke, Mark Steedman as well as Stefan Riezler and the team
at PARC; naturally all errors remain our own. This research was
supported by NSF awards DMS 0074276 and ITR IIS 0085940.
1 Introduction
Stochastic Unification-Based Grammars (SUBGs)
use log-linear models (also known as exponential or
MaxEnt models and Markov Random Fields) to de-
fine probability distributions over the parses of a uni-
fication grammar. These grammars can incorporate
virtually all kinds of linguistically important con-
straints (including non-local and non-context-free
constraints), and are equipped with a statistically
sound framework for estimation and learning.
Abney (1997) pointed out that the non-context-
free dependencies of a unification grammar require
stochastic models more general than Probabilis-
tic Context-Free Grammars (PCFGs) and Markov
Branching Processes, and proposed the use of log-
linear models for defining probability distributions
over the parses of a unification grammar. Un-
fortunately, the maximum likelihood estimator Ab-
ney proposed for SUBGs seems computationally in-
tractable since it requires statistics that depend on
the set of all parses of all strings generated by the
grammar. This set is infinite (so exhaustive enumer-
ation is impossible) and presumably has a very com-
plex structure (so sampling estimates might take an
extremely long time to converge).
Johnson et al (1999) observed that parsing and
related tasks only require conditional distributions
over parses given strings, and that such conditional
distributions are considerably easier to estimate than
joint distributions of strings and their parses. The
conditional maximum likelihood estimator proposed
by Johnson et al requires statistics that depend on
the set of all parses of the strings in the training cor-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 279-286.
                         Proceedings of the 40th Annual Meeting of the Association for
pus. For most linguistically realistic grammars this
set is finite, and for moderate sized grammars and
training corpora this estimation procedure is quite
feasible.
However, our recent experiments involve training
from the Wall Street Journal Penn Tree-bank, and
repeatedly enumerating the parses of its 50,000 sen-
tences is quite time-consuming. Matters are only
made worse because we have moved some of the
constraints in the grammar from the unification com-
ponent to the stochastic component. This broadens
the coverage of the grammar, but at the expense of
massively expanding the number of possible parses
of each sentence.
In the mid-1990s unification-based parsers were
developed that do not enumerate all parses of a string
but instead manipulate and return a ?packed? rep-
resentation of the set of parses. This paper de-
scribes how to find the most probable parse and
the statistics required for estimating a SUBG from
the packed parse set representations proposed by
Maxwell III and Kaplan (1995). This makes it pos-
sible to avoid explicitly enumerating the parses of
the strings in the training corpus.
The methods proposed here are analogues of
the well-known dynamic programming algorithms
for Probabilistic Context-Free Grammars (PCFGs);
specifically the Viterbi algorithm for finding the
most probable parse of a string, and the Inside-
Outside algorithm for estimating a PCFG from un-
parsed training data.1 In fact, because Maxwell and
Kaplan packed representations are just Truth Main-
tenance System (TMS) representations (Forbus and
de Kleer, 1993), the statistical techniques described
here should extend to non-linguistic applications of
TMSs as well.
Dynamic programming techniques have
been applied to log-linear models before.
Lafferty et al (2001) mention that dynamic
programming can be used to compute the statistics
required for conditional estimation of log-linear
models based on context-free grammars where
the properties can include arbitrary functions of
the input string. Miyao and Tsujii (2002) (which
1However, because we use conditional estimation, also
known as discriminative training, we require at least some dis-
criminating information about the correct parse of a string in
order to estimate a stochastic unification grammar.
appeared after this paper was accepted) is the closest
related work we know of. They describe a technique
for calculating the statistics required to estimate a
log-linear parsing model with non-local properties
from packed feature forests.
The rest of this paper is structured as follows.
The next section describes unification grammars
and Maxwell and Kaplan packed representation.
The following section reviews stochastic unifica-
tion grammars (Abney, 1997) and the statistical
quantities required for efficiently estimating such
grammars from parsed training data (Johnson et al,
1999). The final substantive section of this paper
shows how these quantities can be defined directly
in terms of the Maxwell and Kaplan packed repre-
sentations.
The notation used in this paper is as follows. Vari-
ables are written in upper case italic, e.g., X,Y , etc.,
the sets they range over are written in script, e.g.,
X ,Y , etc., while specific values are written in lower
case italic, e.g., x, y, etc. In the case of vector-valued
entities, subscripts indicate particular components.
2 Maxwell and Kaplan packed
representations
This section characterises the properties of unifica-
tion grammars and the Maxwell and Kaplan packed
parse representations that will be important for what
follows. This characterisation omits many details
about unification grammars and the algorithm by
which the packed representations are actually con-
structed; see Maxwell III and Kaplan (1995) for de-
tails.
A parse generated by a unification grammar is a
nite subset of a set F of features. Features are parse
fragments, e.g., chart edges or arcs from attribute-
value structures, out of which the packed representa-
tions are constructed. For this paper it does not mat-
ter exactly what features are, but they are intended
to be the atomic entities manipulated by a dynamic
programming parsing algorithm. A grammar defines
a set ? of well-formed or grammatical parses. Each
parse ? ? ? is associated with a string of words
Y (?) called its yield. Note that except for trivial
grammars F and ? are infinite.
If y is a string, then let ?(y) = {? ? ?|Y (?) =
y} and F(y) = ????(y){f ? ?}. That is, ?(y) is
the set of parses of a string y and F(y) is the set of
features appearing in the parses of y. In the gram-
mars of interest here ?(y) and hence also F(y) are
finite.
Maxwell and Kaplan?s packed representations of-
ten provide a more compact representation of the
set of parses of a sentence than would be obtained
by merely listing each parse separately. The intu-
ition behind these packed representations is that for
most strings y, many of the features in F(y) occur
in many of the parses ?(y). This is often the case
in natural language, since the same substructure can
appear as a component of many different parses.
Packed feature representations are defined in
terms of conditions on the values assigned to a vec-
tor of variables X . These variables have no direct
linguistic interpretation; rather, each different as-
signment of values to these variables identifies a set
of features which constitutes one of the parses in
the packed representation. A condition a on X is
a function from X to {0, 1}. While for uniformity
we write conditions as functions on the entire vec-
tor X , in practice Maxwell and Kaplan?s approach
produces conditions whose value depends only on a
few of the variables in X , and the efficiency of the
algorithms described here depends on this.
A packed representation of a finite set of parses is
a quadruple R = (F ?, X,N, ?), where:
? F ? ? F(y) is a finite set of features,
? X is a finite vector of variables, where each
variable X` ranges over the finite set X`,
? N is a finite set of conditions on X called the
no-goods,2 and
? ? is a function that maps each feature f ? F ?
to a condition ?f on X .
A vector of values x satises the no-goods N iff
N(x) = 1, where N(x) = ???N ?(x). Each x
that satisfies the no-goods identies a parse ?(x) =
{f ? F ?|?f (x) = 1}, i.e., ? is the set of features
whose conditions are satisfied by x. We require that
each parse be identified by a unique value satisfying
2The name ?no-good? comes from the TMS literature, and
was used by Maxwell and Kaplan. However, here the no-goods
actually identify the good variable assignments.
the no-goods. That is, we require that:
?x, x? ? X if N(x) = N(x?) = 1 and
?(x) = ?(x?) then x = x? (1)
Finally, a packed representation R represents the
set of parses ?(R) that are identified by values
that satisfy the no-goods, i.e., ?(R) = {?(x)|x ?
X , N(x) = 1}.
Maxwell III and Kaplan (1995) describes a pars-
ing algorithm for unification-based grammars that
takes as input a string y and returns a packed rep-
resentation R such that ?(R) = ?(y), i.e., R rep-
resents the set of parses of the string y. The SUBG
parsing and estimation algorithms described in this
paper use Maxwell and Kaplan?s parsing algorithm
as a subroutine.
3 Stochastic Unification-Based Grammars
This section reviews the probabilistic framework
used in SUBGs, and describes the statistics that
must be calculated in order to estimate the pa-
rameters of a SUBG from parsed training data.
For a more detailed exposition and descriptions
of regularization and other important details, see
Johnson et al (1999).
The probability distribution over parses is defined
in terms of a finite vector g = (g1, . . . , gm) of
properties. A property is a real-valued function of
parses ?. Johnson et al (1999) placed no restric-
tions on what functions could be properties, permit-
ting properties to encode arbitrary global informa-
tion about a parse. However, the dynamic program-
ming algorithms presented here require the informa-
tion encoded in properties to be local with respect to
the features F used in the packed parse representa-
tion. Specifically, we require that properties be de-
fined on features rather than parses, i.e., each feature
f ? F is associated with a finite vector of real values
(g1(f), . . . , gm(f)) which define the property func-
tions for parses as follows:
gk(?) =
?
f??
gk(f), for k = 1 . . . m. (2)
That is, the property values of a parse are the sum
of the property values of its features. In the usual
case, some features will be associated with a single
property (i.e., gk(f) is equal to 1 for a specific value
of k and 0 otherwise), and other features will be as-
sociated with no properties at all (i.e., g(f) = 0).
This requires properties be very local with re-
spect to features, which means that we give up the
ability to define properties arbitrarily. Note how-
ever that we can still encode essentially arbitrary
linguistic information in properties by adding spe-
cialised features to the underlying unification gram-
mar. For example, suppose we want a property that
indicates whether the parse contains a reduced rela-
tive clauses headed by a past participle (such ?gar-
den path? constructions are grammatical but often
almost incomprehensible, and alternative parses not
including such constructions would probably be pre-
ferred). Under the current definition of properties,
we can introduce such a property by modifying the
underlying unification grammar to produce a certain
?diacritic? feature in a parse just in case the parse ac-
tually contains the appropriate reduced relative con-
struction. Thus, while properties are required to be
local relative to features, we can use the ability of
the underlying unification grammar to encode essen-
tially arbitrary non-local information in features to
introduce properties that also encode non-local in-
formation.
A Stochastic Unification-Based Grammar is a
triple (U, g, ?), where U is a unification grammar
that defines a set ? of parses as described above,
g = (g1, . . . , gm) is a vector of property functions as
just described, and ? = (?1, . . . , ?m) is a vector of
non-negative real-valued parameters called property
weights. The probability P?(?) of a parse ? ? ? is:
P?(?) =
W?(?)
Z?
, where:
W?(?) =
m
?
j=1
?gj(?)j , and
Z? =
?
????
W?(??)
Intuitively, if gj(?) is the number of times that prop-
erty j occurs in ? then ?j is the ?weight? or ?cost? of
each occurrence of property j and Z? is a normal-
ising constant that ensures that the probability of all
parses sums to 1.
Now we discuss the calculation of several impor-
tant quantities for SUBGs. In each case we show
that the quantity can be expressed as the value that
maximises a product of functions or else as the sum
of a product of functions, each of which depends
on a small subset of the variables X . These are the
kinds of quantities for which dynamic programming
graphical model algorithms have been developed.
3.1 The most probable parse
In parsing applications it is important to be able to
extract the most probable (or MAP) parse ??(y) of
string y with respect to a SUBG. This parse is:
??(y) = argmax
???(y)
W?(?)
Given a packed representation (F ?, X,N, ?) for the
parses ?(y), let x?(y) be the x that identifies ??(y).
Since W?(??(y)) > 0, it can be shown that:
x?(y) = argmax
x?X
N(x)
m
?
j=1
?gj(?(x))j
= argmax
x?X
N(x)
m
?
j=1
?
?
f??(x) gj(f)
j
= argmax
x?X
N(x)
m
?
j=1
?
?
f?F? ?f (x)gj(f)
j
= argmax
x?X
N(x)
m
?
j=1
?
f?F ?
??f (x)gj(f)j
= argmax
x?X
N(x)
?
f?F ?
?
?
m
?
j=1
?gj(f)j
?
?
?f (x)
= argmax
x?X
?
??N
?(x)
?
f?F ?
h?,f (x) (3)
where h?,f (x) = ?mj=1 ?gj(f)j if ?f (x) = 1 and
h?,f (x) = 1 if ?f (x) = 0. Note that h?,f (x) de-
pends on exactly the same variables in X as ?f does.
As (3) makes clear, finding x?(y) involves maximis-
ing a product of functions where each function de-
pends on a subset of the variables X . As explained
below, this is exactly the kind of maximisation that
can be solved using graphical model techniques.
3.2 Conditional likelihood
We now turn to the estimation of the property
weights ? from a training corpus of parsed data D =
(?1, . . . , ?n). As explained in Johnson et al (1999),
one way to do this is to find the ? that maximises the
conditional likelihood of the training corpus parses
given their yields. (Johnson et al actually maximise
conditional likelihood regularized with a Gaussian
prior, but for simplicity we ignore this here). If yi is
the yield of the parse ?i, the conditional likelihood
of the parses given their yields is:
LD(?) =
n
?
i=1
W?(?i)
Z?(?(yi))
where ?(y) is the set of parses with yield y and:
Z?(S) =
?
??S
W?(?).
Then the maximum conditional likelihood estimate
?? of ? is ?? = argmax? LD(?).
Now calculating W?(?i) poses no computational
problems, but since ?(yi) (the set of parses for yi)
can be large, calculating Z?(?(yi)) by enumerating
each ? ? ?(yi) can be computationally expensive.
However, there is an alternative method for calcu-
lating Z?(?(yi)) that does not involve this enumera-
tion. As noted above, for each yield yi, i = 1, . . . , n,
Maxwell?s parsing algorithm returns a packed fea-
ture structure Ri that represents the parses of yi, i.e.,
?(yi) = ?(Ri). A derivation parallel to the one for
(3) shows that for R = (F ?, X,N, ?):
Z?(?(R)) =
?
x?X
?
??N
?(x)
?
f?F ?
h?,f (x) (4)
(This derivation relies on the isomorphism between
parses and variable assignments in (1)). It turns out
that this type of sum can also be calculated using
graphical model techniques.
3.3 Conditional Expectations
In general, iterative numerical procedures are re-
quired to find the property weights ? that maximise
the conditional likelihood LD(?). While there are
a number of different techniques that can be used,
all of the efficient techniques require the calculation
of conditional expectations E?[gk|yi] for each prop-
erty gk and each sentence yi in the training corpus,
where:
E?[g|y] =
?
???(y)
g(?)P?(?|y)
=
?
???(y) g(?)W?(?)
Z?(?(y))
For example, the Conjugate Gradient algorithm,
which was used by Johnson et al, requires the cal-
culation not just of LD(?) but also its derivatives
?LD(?)/??k. It is straight-forward to show:
?LD(?)
??k
= LD(?)?k
n
?
i=1
(gk(?i) ? E?[gk|yi]) .
We have just described the calculation of LD(?),
so if we can calculate E?[gk|yi] then we can calcu-
late the partial derivatives required by the Conjugate
Gradient algorithm as well.
Again, let R = (F ?, X,N, ?) be a packed repre-
sentation such that ?(R) = ?(yi). First, note that
(2) implies that:
E?[gk|yi] =
?
f?F ?
gk(f) P({? : f ? ?}|yi).
Note that P({? : f ? ?}|yi) involves the sum of
weights over all x ? X subject to the conditions
that N(x) = 1 and ?f (x) = 1. Thus P({? : f ?
?}|yi) can also be expressed in a form that is easy
to evaluate using graphical techniques.
Z?(?(R))P?({? : f ? ?}|yi)
=
?
x?X
?f (x)
?
??N
?(x)
?
f ??F ?
h?,f ?(x) (5)
4 Graphical model calculations
In this section we briefly review graphical model
algorithms for maximising and summing products
of functions of the kind presented above. It turns
out that the algorithm for maximisation is a gener-
alisation of the Viterbi algorithm for HMMs, and
the algorithm for computing the summation in (5)
is a generalisation of the forward-backward algo-
rithm for HMMs (Smyth et al, 1997). Viewed
abstractly, these algorithms simplify these expres-
sions by moving common factors over the max or
sum operators respectively. These techniques are
now relatively standard; the most well-known ap-
proach involves junction trees (Pearl, 1988; Cow-
ell, 1999). We adopt the approach approach de-
scribed by Geman and Kochanek (2000), which is
a straightforward generalization of HMM dynamic
programming with minimal assumptions and pro-
gramming overhead. However, in principle any of
the graphical model computational algorithms can
be used.
The quantities (3), (4) and (5) involve maximisa-
tion or summation over a product of functions, each
of which depends only on the values of a subset of
the variables X . There are dynamic programming
algorithms for calculating all of these quantities, but
for reasons of space we only describe an algorithm
for finding the maximum value of a product of func-
tions. These graph algorithms are rather involved.
It may be easier to follow if one reads Example 1
before or in parallel with the definitions below.
To explain the algorithm we use the following no-
tation. If x and x? are both vectors of length m
then x =j x? iff x and x? disagree on at most their
jth components, i.e., xk = x?k for k = 1, . . . , j ?
1, j + 1, . . . m. If f is a function whose domain
is X , we say that f depends on the set of variables
d(f) = {Xj |?x, x? ? X , x =j x?, f(x) 6= f(x?)}.
That is, Xj ? d(f) iff changing the value of Xj can
change the value of f .
The algorithm relies on the fact that the variables
in X = (X1, . . . , Xn) are ordered (e.g., X1 pre-
cedes X2, etc.), and while the algorithm is correct
for any variable ordering, its efficiency may vary
dramatically depending on the ordering as described
below. Let H be any set of functions whose do-
mains are X . We partition H into disjoint subsets
H1, . . . ,Hn+1, where Hj is the subset of H that de-
pend on Xj but do not depend on any variables or-
dered before Xj , and Hn+1 is the subset of H that do
not depend on any variables at all (i.e., they are con-
stants).3 That is, Hj = {H ? H|Xj ? d(H),?i <
j Xi 6? d(H)} and Hn+1 = {H ? H|d(H) = ?}.
As explained in section 3.1, there is a set of func-
tions A such that the quantities we need to calculate
have the general form:
Mmax = maxx?X
?
A?A
A(x) (6)
x? = argmax
x?X
?
A?A
A(x). (7)
Mmax is the maximum value of the product expres-
sion while x? is the value of the variables at which the
maximum occurs. In a SUBG parsing application x?
identifies the MAP parse.
3Strictly speaking this does not necessarily define a parti-
tion, as some of the subsetsHj may be empty.
The procedure depends on two sequences of func-
tions Mi, i = 1, . . . , n + 1 and Vi, i = 1, . . . , n.
Informally, Mi is the maximum value attained by
the subset of the functions A that depend on one of
the variables X1, . . . , Xi, and Vi gives information
about the value of Xi at which this maximum is at-
tained.
To simplify notation we write these functions as
functions of the entire set of variables X , but usu-
ally depend on a much smaller set of variables. The
Mi are real valued, while each Vi ranges over Xi.
Let M = {M1, . . . ,Mn}. Recall that the sets of
functions A and M can be both be partitioned into
disjoint subsets A1, . . . ,An+1 and M1, . . . ,Mn+1
respectively on the basis of the variables each Ai
and Mi depend on. The definition of the Mi and
Vi, i = 1, . . . , n is as follows:
Mi(x) = max
x??X
s.t. x?=ix
?
A?Ai
A(x?)
?
M?Mi
M(x?) (8)
Vi(x) = argmax
x??X
s.t. x?=ix
?
A?Ai
A(x?)
?
M?Mi
M(x?)
Mn+1 receives a special definition, since there is no
variable Xn+1.
Mn+1 =
?
?
?
A?An+1
A
?
?
?
?
?
M?Mn+1
M
?
? (9)
The definition of Mi in (8) may look circular (since
M appears in the right-hand side), but in fact it is
not. First, note that Mi depends only on variables
ordered after Xi, so if Mj ? Mi then j < i. More
specifically,
d(Mi) =
?
?
?
A?Ai
d(A) ?
?
M?Mi
d(M)
?
? \ {Xi}.
Thus we can compute the Mi in the order
M1, . . . ,Mn+1, inserting Mi into the appropriate set
Mk, where k > i, when Mi is computed.
We claim that Mmax = Mn+1. (Note that Mn+1
and Mn are constants, since there are no variables
ordered after Xn). To see this, consider the tree T
whose nodes are the Mi, and which has a directed
edge from Mi to Mj iff Mi ? Mj (i.e., Mi appears
in the right hand side of the definition (8) of Mj).
T has a unique root Mn+1, so there is a path from
every Mi to Mn+1. Let i ? j iff there is a path
from Mi to Mj in this tree. Then a simple induction
shows that Mj is a function from d(Mj) to a max-
imisation over each of the variables Xi where i ? j
of ?i?j,A?Ai A.Further, it is straightforward to show that Vi(x?) =
x?i (the value x? assigns to Xi). By the same argu-
ments as above, d(Vi) only contains variables or-
dered after Xi, so Vn = x?n. Thus we can evaluate
the Vi in the order Vn, . . . , V1 to find the maximising
assignment x?.
Example 1 Let X = { X1, X2, X3, X4, X5,
X6, X7} and set A = {a(X1, X3), b(X2, X4),
c(X3, X4, X5), d(X4, X5), e(X6, X7)}. We can
represent the sharing of variables in A by means of a
undirected graph GA, where the nodes of GA are the
variables X and there is an edge in GA connecting
Xi to Xj iff ?A ? A such that both Xi, Xj ? d(A).
GA is depicted below.
  
  
X1 X3 X5 X6
X2 X4 X7
r r r
rr
r
r
Starting with the variable X1, we compute M1
and V1:
M1(x3) = maxx1?X1
a(x1, x3)
V1(x3) = argmax
x1?X1
a(x1, x3)
We now proceed to the variable X2.
M2(x4) = maxx2?X2 b(x2, x4)
V2(x4) = argmax
x2?X2
b(x2, x4)
Since M1 belongs to M3, it appears in the denition
of M3.
M3(x4, x5) = maxx3?X3
c(x3, x4, x5)M1(x3)
V3(x4, x5) = argmax
x3?X3
c(x3, x4, x5)M1(x3)
Similarly, M4 is dened in terms of M2 and M3.
M4(x5) = maxx4?X4 d(x4, x5)M2(x4)M3(x4, x5)
V4(x5) = argmax
x4?X4
d(x4, x5)M2(x4)M3(x4, x5)
Note that M5 is a constant, reecting the fact that
in GA the node X5 is not connected to any node or-
dered after it.
M5 = maxx5?X5 M4(x5)
V5 = argmax
x5?X5
M4(x5)
The second component is dened in the same way:
M6(x7) = maxx6?X6 e(x6, x7)
V6(x7) = argmax
x6?X6
e(x6, x7)
M7 = maxx7?X7 M6(x7)
V7 = argmax
x7?X7
M6(x7)
The maximum value for the product M8 = Mmax is
dened in terms of M5 and M7.
Mmax = M8 = M5M7
Finally, we evaluate V7, . . . , V1 to nd the maximis-
ing assignment x?.
x?7 = V7
x?6 = V6(x?7)
x?5 = V5
x?4 = V4(x?5)
x?3 = V3(x?4, x?5)
x?2 = V2(x?4)
x?1 = V1(x?3)
We now briefly consider the computational com-
plexity of this process. Clearly, the number of steps
required to compute each Mi is a polynomial of or-
der |d(Mi)|+1, since we need to enumerate all pos-
sible values for the argument variables d(Mi) and
for each of these, maximise over the set Xi. Fur-
ther, it is easy to show that in terms of the graph GA,
d(Mj) consists of those variables Xk, k > j reach-
able by a path starting at Xj and all of whose nodes
except the last are variables that precede Xj .
Since computational effort is bounded above by a
polynomial of order |d(Mi)|+ 1, we seek a variable
ordering that bounds the maximum value of |d(Mi)|.
Unfortunately, finding the ordering that minimises
the maximum value of |d(Mi)| is an NP-complete
problem. However, there are several efficient heuris-
tics that are reputed in graphical models community
to produce good visitation schedules. It may be that
they will perform well in the SUBG parsing applica-
tions as well.
5 Conclusion
This paper shows how to apply dynamic program-
ming methods developed for graphical models to
SUBGs to find the most probable parse and to ob-
tain the statistics needed for estimation directly from
Maxwell and Kaplan packed parse representations.
i.e., without expanding these into individual parses.
The algorithm rests on the observation that so long
as features are local to the parse fragments used in
the packed representations, the statistics required for
parsing and estimation are the kinds of quantities
that dynamic programming algorithms for graphical
models can perform. Since neither Maxwell and Ka-
plan?s packed parsing algorithm nor the procedures
described here depend on the details of the underly-
ing linguistic theory, the approach should apply to
virtually any kind of underlying grammar.
Obviously, an empirical evaluation of the algo-
rithms described here would be extremely useful.
The algorithms described here are exact, but be-
cause we are working with unification grammars
and apparently arbitrary graphical models we can-
not polynomially bound their computational com-
plexity. However, it seems reasonable to expect
that if the linguistic dependencies in a sentence typ-
ically factorize into largely non-interacting cliques
then the dynamic programming methods may offer
dramatic computational savings compared to current
methods that enumerate all possible parses.
It might be interesting to compare these dy-
namic programming algorithms with a standard
unification-based parser using a best-first search
heuristic. (To our knowledge such an approach has
not yet been explored, but it seems straightforward:
the figure of merit could simply be the sum of the
weights of the properties of each partial parse?s frag-
ments). Because such parsers prune the search space
they cannot guarantee correct results, unlike the al-
gorithms proposed here. Such a best-first parser
might be accurate when parsing with a trained gram-
mar, but its results may be poor at the beginning
of parameter weight estimation when the parameter
weight estimates are themselves inaccurate.
Finally, it would be extremely interesting to com-
pare these dynamic programming algorithms to
the ones described by Miyao and Tsujii (2002). It
seems that the Maxwell and Kaplan packed repre-
sentation may permit more compact representations
than the disjunctive representations used by Miyao
et al, but this does not imply that the algorithms
proposed here are more efficient. Further theoreti-
cal and empirical investigation is required.
References
Steven Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23(4):597?617.
Robert Cowell. 1999. Introduction to inference for Bayesian
networks. In Michael Jordan, editor, Learning in Graphi-
cal Models, pages 9?26. The MIT Press, Cambridge, Mas-
sachusetts.
Kenneth D. Forbus and Johan de Kleer. 1993. Building problem
solvers. The MIT Press, Cambridge, Massachusetts.
Stuart Geman and Kevin Kochanek. 2000. Dynamic program-
ming and the representation of soft-decodable codes. Tech-
nical report, Division of Applied Mathematics, Brown Uni-
versity.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In The Proceedings of the 37th Annual
Conference of the Association for Computational Linguis-
tics, pages 535?541, San Francisco. Morgan Kaufmann.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Machine Learn-
ing: Proceedings of the Eighteenth International Conference
(ICML 2001), Stanford, California.
John T. Maxwell III and Ronald M. Kaplan. 1995. A method
for disjunctive constraint satisfaction. In Mary Dalrymple,
Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-
nen, editors, Formal Issues in Lexical-Functional Grammar,
number 47 in CSLI Lecture Notes Series, chapter 14, pages
381?481. CSLI Publications.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum entropy
estimation for feature forests. In Proceedings of Human
Language Technology Conference 2002, March.
Judea Pearl. 1988. Probabalistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference. Morgan Kaufmann,
San Mateo, California.
Padhraic Smyth, David Heckerman, and Michael Jordan. 1997.
Probabilistic Independence Networks for Hidden Markov
Models. Neural Computation, 9(2):227?269.
A TAG-based noisy channel model of speech repairs
Mark Johnson
Brown University
Providence, RI 02912
mj@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
This paper describes a noisy channel model of
speech repairs, which can identify and correct
repairs in speech transcripts. A syntactic parser
is used as the source model, and a novel type
of TAG-based transducer is the channel model.
The use of TAG is motivated by the intuition
that the reparandum is a ?rough copy? of the
repair. The model is trained and tested on the
Switchboard disfluency-annotated corpus.
1 Introduction
Most spontaneous speech contains disfluencies
such as partial words, filled pauses (e.g., ?uh?,
?um?, ?huh?), explicit editing terms (e.g., ?I
mean?), parenthetical asides and repairs. Of
these repairs pose particularly difficult problems
for parsing and related NLP tasks. This paper
presents an explicit generative model of speech
repairs and shows how it can eliminate this kind
of disfluency.
While speech repairs have been studied by
psycholinguists for some time, as far as we know
this is the first time a probabilistic model of
speech repairs based on a model of syntactic
structure has been described in the literature.
Probabilistic models have the advantage over
other kinds of models that they can in principle
be integrated with other probabilistic models to
produce a combined model that uses all avail-
able evidence to select the globally optimal anal-
ysis. Shriberg and Stolcke (1998) studied the lo-
cation and distribution of repairs in the Switch-
board corpus, but did not propose an actual
model of repairs. Heeman and Allen (1999) de-
scribe a noisy channel model of speech repairs,
but leave ?extending the model to incorporate
higher level syntactic . . . processing? to future
work. The previous work most closely related
to the current work is Charniak and Johnson
(2001), who used a boosted decision stub classi-
fier to classify words as edited or not on a word
by word basis, but do not identify or assign a
probability to a repair as a whole.
There are two innovations in this paper.
First, we demonstrate that using a syntactic
parser-based language model Charniak (2001)
instead of bi/trigram language models signifi-
cantly improves the accuracy of repair detection
and correction. Second, we show how Tree Ad-
joining Grammars (TAGs) can be used to pro-
vide a precise formal description and probabilis-
tic model of the crossed dependencies occurring
in speech repairs.
The rest of this paper is structured as fol-
lows. The next section describes the noisy chan-
nel model of speech repairs and the section af-
ter that explains how it can be applied to de-
tect and repair speech repairs. Section 4 evalu-
ates this model on the Penn 3 disfluency-tagged
Switchboard corpus, and section 5 concludes
and discusses future work.
2 A noisy channel model of repairs
We follow Shriberg (1994) and most other work
on speech repairs by dividing a repair into three
parts: the reparandum (the material repaired),
the interregnum that is typically either empty
or consists of a filler, and the repair. Figure 1
shows these three parts for a typical repair.
Most current probabilistic language models
are based on HMMs or PCFGs, which induce
linear or tree-structured dependencies between
words. The relationship between reparandum
and repair seems to be quite different: the
repair is a ?rough copy? of the reparandum,
often incorporating the same or very similar
words in roughly the same word order. That
is, they seem to involve ?crossed? dependen-
cies between the reparandum and the repair,
shown in Figure 1. Languages with an un-
bounded number of crossed dependencies can-
not be described by a context-free or finite-
state grammar, and crossed dependencies like
these have been used to argue natural languages
. . . a flight to Boston,
? ?? ?
Reparandum
uh, I mean,
? ?? ?
Interregnum
to Denver
? ?? ?
Repair
on Friday . . .
Figure 1: The structure of a typical repair, with crossing dependencies between reparandum and
repair.
Imean
uh
a flight to Boston
to Denver on Friday
Figure 2: The ?helical? dependency structure induced by the generative model of speech repairs
for the repair depicted in Figure 1.
are not context-free Shieber (1985). Mildly
context-sensitive grammars, such as Tree Ad-
joining Grammars (TAGs) and Combinatory
Categorial Grammars, can describe such cross-
ing dependencies, and that is why TAGs are
used here.
Figure 2 shows the combined model?s de-
pendency structure for the repair of Figure 1.
Interestingly, if we trace the temporal word
string through this dependency structure, align-
ing words next to the words they are dependent
on, we obtain a ?helical? type of structure famil-
iar from genome models, and in fact TAGs are
being used to model genomes for very similar
reasons.
The noisy channel model described here in-
volves two components. A language model de-
fines a probability distribution P(X) over the
source sentences X, which do not contain re-
pairs. The channel model defines a conditional
probability distribution P(Y |X) of surface sen-
tences Y , which may contain repairs, given
source sentences. In the work reported here,
X is a word string and Y is a speech tran-
scription not containing punctuation or partial
words. We use two language models here: a
bigram language model, which is used in the
search process, and a syntactic parser-based lan-
guage model Charniak (2001), which is used
to rescore a set of the most likely analysis ob-
tained using the bigram model. Because the
language model is responsible for generating the
well-formed sentence X, it is reasonable to ex-
pect that a language model that can model more
global properties of sentences will lead to bet-
ter performance, and the results presented here
show that this is the case. The channel model is
a stochastic TAG-based transducer; it is respon-
sible for generating the repairs in the transcript
Y , and it uses the ability of TAGs to straight-
forwardly model crossed dependencies.
2.1 Informal description
Given an observed sentence Y we wish to find
the most likely source sentence X? , where:
X? = argmax
X
P(X|Y ) = argmax
X
P(Y |X)P(Y ).
This is the same general setup that is used
in statistical speech recognition and machine
translation, and in these applications syntax-
based language models P(Y ) yield state-of-the-
art performance, so we use one such model here.
The channel model P(Y |X) generates sen-
tences Y given a source X. A repair can po-
tentially begin before any word of X. When a
repair has begun, the channel model incremen-
tally processes the succeeding words from the
start of the repair. Before each succeeding word
either the repair can end or else a sequence of
words can be inserted in the reparandum. At
the end of each repair, a (possibly null) inter-
regnum is appended to the reparandum.
The intuition motivating the channel model
design is that the words inserted into the
reparandum are very closely related those in the
repair. Indeed, in our training data over 60% of
the words in the reparandum are exact copies of
words in the repair; this similarity is strong evi-
dence of a repair. The channel model is designed
so that exact copy reparandum words will have
high probability.
We assume that X is a substring of Y , i.e.,
that the source sentence can be obtained by
deleting words from Y , so for a fixed observed
sentence there are only a finite number of pos-
sible source sentences. However, the number of
source sentences grows exponentially with the
length of Y , so exhaustive search is probably
infeasible.
TAGs provide a systematic way of formaliz-
ing the channel model, and their polynomial-
time dynamic programming parsing algorithms
can be used to search for likely repairs, at least
when used with simple language models like a
bigram language model. In this paper we first
identify the 20 most likely analysis of each sen-
tence using the TAG channel model together
with a bigram language model. Then each of
these analysis is rescored using the TAG chan-
nel model and a syntactic parser based language
model.
The TAG channel model?s analysis do not re-
flect the syntactic structure of the sentence be-
ing analyzed; instead they encode the crossed
dependencies of the speech repairs. If we want
to use TAG dynamic programming algorithms
to efficiently search for repairs, it is necessary
that the intersection (in language terms) of the
TAG channel model and the language model it-
self be describable by a TAG. One way to guar-
antee this is to use a finite state language model;
this motivates our use of a bigram language
model.
On the other hand, it seems desirable to use a
language model that is sensitive to more global
properties of the sentence, and we do this by
reranking the initial analysis, replacing the bi-
gram language model with a syntactic parser
based model. We do not need to intersect this
parser based language model with our TAG
channel model since we evaluate each analysis
separately.
2.2 The TAG channel model
The TAG channel model defines a stochastic
mapping of source sentences X into observed
sentences Y . There are several ways to de-
fine transducers using TAGs such as Shieber
and Schabes (1990), but the following simple
method, inspired by finite-state transducers,
suffices for the application here. The TAG de-
fines a language whose vocabulary is the set of
pairs (??{?})?(??{?}), where ? is the vocab-
ulary of the observed sentences Y . A string Z
in this language can be interpreted as a pair
of strings (Y,X), where Y is the concatena-
tion of the projection of the first components
of Z and X is the concatenation of the projec-
tion of the second components. For example,
the string Z = a:a flight:flight to:? Boston:?
uh:? I:? mean:? to:to Denver:Denver on:on Fri-
day:Friday corresponds to the observed string
Y = a flight to Boston uh I mean to Denver
on Friday and the source string X = a flight to
Denver on Friday.
Figure 3 shows the TAG rules used to gen-
erate this example. The nonterminals in this
grammar are of the form Nwx, Rwy:wx and I,
where wx is a word appearing in the source
string and wy is a word appearing in the ob-
served string. Informally, the Nwx nonterminals
indicate that the preceding word wx was an-
alyzed as not being part of a repair, while the
Rwy:wx that the preceding words wy and wx were
part of a repair. The nonterminal I generates
words in the interregnum of a repair. Encoding
the preceding words in the TAGs nonterminals
permits the channel model to be sensitive to
lexical properties of the preceding words. The
start symbol is N$, where ?$? is a distinguished
symbol used to indicate the beginning and end
of sentences.
2.3 Estimating the repair channel
model from data
The model is trained from the disfluency and
POS tagged Switchboard corpus on the LDC
Penn tree bank III CD-ROM (specifically, the
files under dysfl/dps/swbd). This version of the
corpus annotates the beginning and ending po-
sitions of repairs as well as fillers, editing terms,
asides, etc., which might serve as the interreg-
num in a repair. The corpus also includes punc-
tuation and partial words, which are ignored in
both training and evaluation here since we felt
that in realistic applications these would not be
available in speech recognizer output. The tran-
script of the example of Figure 1 would look
something like the following:
a/DT flight/NN [to/IN Boston/NNP +
{F uh/UH} {E I/PRP mean/VBP} to/IN
Denver/NNP] on/IN Friday/NNP
In this transcription the repair is the string
from the opening bracket ?[? to the interrup-
tion point ?+?; the interregnum is the sequence
of braced strings following the interregnum, and
the repair is the string that begins at the end of
the interregnum and ends at the closing bracket
?]?. The interregnum consists of the braced
(?1) Nwant
a:a Na ?
1 ? Pn(repair|a)
(?2) Na
flight:flight Rflight:flight
I?
Pn(repair|flight)
(?3) NDenver
on:on Non ?
1 ? Pn(repair|on)
(?5) I
uh I
I mean
Pi(uh Imean)
(?1) Rflight:flight
to:? Rto:to
R?flight:flight to:to
Pr(copy|flight,flight)
(?2) Rto:to
Boston:? RBoston:Denver
R?to:to Denver:Denver
Pr(subst|to, to)Pr(Boston|subst, to,Denver)
(?3) RBoston:Denver
R?Boston:Denver NDenver ?
Pr(nonrep|Boston,Denver)
(?4) RBoston,Denver
RBoston,tomorrow
R?Boston,Denver tomorrow:tomorrow
Pr(del|Boston,Denver)
(?5) RBoston,Denver
tomorrow:? Rtomorrow,Denver
R?Boston,Denver
Pr(ins|Boston,Denver)
Pr(tomorrow|ins,Boston,Denver)
. . .
?1
?2
?5 ?1
?2
?3
?3
?4
. . .
Nwant
a:a Na
flight:flight Rflight:flight
to:? Rto:to
Boston:? RBoston:Denver
RBoston:Denver
Rto:to
Rflight:flight
I
uh:? I
I:? mean:?
to:to
Denver:Denver
NDenver
on:on Non
Friday:Friday NFriday
. . .
Figure 3: The TAG rules used to generate the example shown in Figure 1 and their respective
weights, and the corresponding derivation and derived trees.
expressions immediately following the interrup-
tion point. We used the disfluency tagged
version of the corpus for training rather than
the parsed version because the parsed version
does not mark the interregnum, but we need
this information for training our repair channel
model. Testing was performed using data from
the parsed version since this data is cleaner, and
it enables a direct comparison with earlier work.
We followed Charniak and Johnson (2001) and
split the corpus into main training data, held-
out training data and test data as follows: main
training consisted of all sw[23]*.dps files, held-
out training consisted of all sw4[5-9]*.dps files
and test consisted of all sw4[0-1]*.mrg files.
We now describe how the weights on the TAG
productions described in subsection 2.2 are es-
timated from this training data. In order to es-
timate these weights we need to know the TAG
derivation of each sentence in the training data.
In order to uniquely determine this we need the
not just the locations of each reparandum, in-
terregnum and repair (which are annotated in
the corpus) but also the crossing dependencies
between the reparandum and repair words, as
indicated in Figure 1.
We obtain these by aligning the reparan-
dum and repair strings of each repair using
a minimum-edit distance string aligner with
the following alignment costs: aligning identi-
cal words costs 0, aligning words with the same
POS tag costs 2, an insertion or a deletion costs
4, aligning words with POS tags that begin with
the same letter costs 5, and an arbitrary sub-
stitution costs 7. These costs were chosen so
that a substitution will be selected over an in-
sertion followed by a deletion, and the lower
cost for substitutions involving POS tags be-
ginning with the same letter is a rough and
easy way of establishing a preference for align-
ing words whose POS tags come from the same
broad class, e.g., it results in aligning singular
and plural nouns, present and past participles,
etc. While we did not evaluate the quality of
the alignments since they are not in themselves
the object of this exercise, they seem to be fairly
good.
From our training data we estimate a number
of conditional probability distributions. These
estimated probability distributions are the lin-
ear interpolation of the corresponding empirical
distributions from the main sub-corpus using
various subsets of conditioning variables (e.g.,
bigram models are mixed with unigram models,
etc.) using Chen?s bucketing scheme Chen and
Goodman (1998). As is commonly done in lan-
guage modelling, the interpolation coefficients
are determined by maximizing the likelihood of
the held out data counts using EM. Special care
was taken to ensure that all distributions over
words ranged over (and assigned non-zero prob-
ability to) every word that occurred in the train-
ing corpora; this turns out to be important as
the size of the training data for the different
distributions varies greatly.
The first distribution is defined over the
words in source sentences (i.e., that do
not contain reparandums or interregnums).
Pn(repair|W ) is the probability of a repair be-
ginning after a word W in the source sentence
X; it is estimated from the training sentences
with reparandums and interregnums removed.
Here and in what follows, W ranges over ? ?
{$}, where ?$? is a distinguished beginning-of-
sentence marker. For example, Pn(repair|flight)
is the probability of a repair beginning after the
word flight. Note that repairs are relatively rare;
in our training data Pn(repair) ? 0.02, which is
a fairly strong bias against repairs.
The other distributions are defined over
aligned reparandum/repair strings, and are es-
timated from the aligned repairs extracted from
the training data. In training we ignored
all overlapping repairs (i.e., cases where the
reparandum of one repair is the repair of an-
other). (Naturally, in testing we have no such
freedom.) We analyze each repair as consisting
of n aligned word pairs (we describe the inter-
regnum model later). Mi is the ith reparan-
dum word and Ri is the corresponding repair
word, so both of these range over ? ? {?}.
We define M0 and R0 to be source sentence
word that preceded the repair (which is ?$? if
the repair begins at the beginning of a sen-
tence). We define M ?i and R?i to be the last non-?
reparandum and repair words respectively, i.e.,
M ?i = Mi if Mi 6= ? and M ?i = M ?i?1 oth-
erwise. Finally, Ti, i = 1 . . . n + 1, which in-
dicates the type of repair that occurs at posi-
tion i, ranges over {copy, subst, ins, del, nonrep},
where Tn+1 = nonrep (indicating that the re-
pair has ended), and for i = 1 . . . n, Ti = copy if
Mi = Ri, Ti = ins if Ri = ?, Ti = del if Mi = ?
and Ti = subst otherwise.
The distributions we estimate from the
aligned repair data are the following.
Pr(Ti|M ?i?1, R?i?1) is the probability of see-
ing repair type Ti following the reparan-
dum word M ?i?1 and repair word R?i?1; e.g.,
Pr(nonrep|Boston,Denver) is the probability of
the repair ending when Boston is the last
reparandum word and Denver is the last repair
word.
Pr(Mi|Ti = ins,M ?i?1, R?i) is the probability
that Mi is the word that is inserted into the
reparandum (i.e., Ri = ?) given that some word
is substituted, and that the preceding reparan-
dum and repair words are M ?i?1 and R?i. For ex-
ample Pr(tomorrow|ins,Boston,Denver) is the
probability that the word tomorrow is inserted
into the reparandum after the words Boston and
Denver, given that some word is inserted.
Pr(Mi|Ti = subst,M ?i?1, R?i) is the prob-
ability that Mi is the word that is substi-
tuted in the reparandum for R?i, given that
some word is substituted. For example,
Pr(Boston|subst, to,Denver) is the probability
that Boston is substituted for Denver, given
that some word is substituted.
Finally, we also estimated a probability dis-
tribution Pi(W ) over interregnum strings as fol-
lows. Our training corpus annotates what we
call interregnum expressions, such as uh and
I mean. We estimated a simple unigram distri-
bution over all of the interregnum expressions
observed in our training corpus, and also ex-
tracted the empirical distribution of the num-
ber of interregnum expressions in each repair.
Interregnums are generated as follows. First,
the number k of interregnum expressions is cho-
sen using the empirical distribution. Then k
interregnum expressions are independently gen-
erated from the unigram distribution of inter-
regnum expressions, and appended to yield the
interregnum string W .
The weighted TAG that constitutes the chan-
nel model is straight forward to define us-
ing these conditional probability distributions.
Note that the language model generates the
source string X. Thus the weights of the TAG
rules condition on the words in X, but do not
generate them.
There are three different schema defining the
initial trees of the TAG. These correspond to
analyzing a source word as not beginning a re-
pair (e.g., ?1 and ?3 in Figure 3), analyzing a
source word as beginning a repair (e.g., ?2), and
generating an interregnum (e.g., ?5).
Auxiliary trees generate the paired reparan-
dum/repair words of a repair. There are five dif-
ferent schema defining the auxiliary trees corre-
sponding to the five different values that Ti can
take. Note that the nonterminal Rm,r expanded
by the auxiliary trees is annotated with the last
reparandum and repair words M ?i?1 and R?i?1
respectively, which makes it possible to condi-
tion the rule?s weight on these words.
Auxiliary trees of the form (?1) gener-
ate reparandum words that are copies of
the corresponding repair words; the weight
on such trees is Pr(copy|M ?i?1, R?i?1). Trees
of the form (?2) substitute a reparan-
dum word for a repair word; their weight
is Pr(subst|M ?i?1, R?i?1)Pr(Mi|subst,M ?i?1, R?i).
Trees of the form (?3) end a repair; their weight
is Pr(nonrep|,M ?i?1, R?i?1). Auxiliary trees of
the form (?3) end a repair; they are weighted
Pr(nonrep|M ?i?1, R?i?1). Auxiliary trees of the
form (?4) permit the repair word R?i?1 to be
deleted in the reparandum; the weight of such
a tree is Pr(del|M ?i?1, R?i?1). Finally, auxiliary
trees of the form (?5) generate a reparandum
word Mi is inserted; the weight of such a tree is
Pr(ins|M ?i?1, R?i?1)Pr(Mi|ins,M ?i?1, R?i?1).
3 Detecting and repairing speech
repairs
The TAG just described is not probabilistic;
informally, it does not include the probability
costs for generating the source words. How-
ever, it is easy to modify the TAG so it does
include a bigram model that does generate the
source words, since each nonterminal encodes
the preceding source word. That is, we multi-
ply the weights of each TAG production given
earlier that introduces a source word Ri by
Pn(Ri|Ri?1). The resulting stochastic TAG is
in fact exactly the intersection of the channel
model TAG with a bigram language model.
The standard n5 bottom-up dynamic pro-
gramming parsing algorithm can be used with
this stochastic TAG. Each different parse of
the observed string Y with this grammar corre-
sponds to a way of analyzing Y in terms of a hy-
pothetical underlying sentence X and a number
of different repairs. In our experiments below
we extract the 20 most likely parses for each sen-
tence. Since the weighted grammar just given
does not generate the source string X, the score
of the parse using the weighted TAG is P(Y |X).
This score multiplied by the probability P(X)
of the source string using the syntactic parser
based language model, is our best estimate of
the probability of an analysis.
However, there is one additional complica-
tion that makes a marked improvement to
the model?s performance. Recall that we use
the standard bottom-up dynamic programming
TAG parsing algorithm to search for candidate
parses. This algorithm has n5 running time,
where n is the length of the string. Even though
our sentences are often long, it is extremely un-
likely that any repair will be longer than, say,
12 words. So to increase processing speed we
only compute analyses for strings of length 12
or less. For every such substring that can be an-
alyzed as a repair we calculate the repair odds,
i.e., the probability of generating this substring
as a repair divided by the probability of gener-
ating this substring via the non-repair rules, or
equivalently, the odds that this substring consti-
tutes a repair. The substrings with high repair
odds are likely to be repairs.
This more local approach has a number of
advantages over computing a global analysis.
First, as just noted it is much more efficient
to compute these partial analyses rather than
to compute global analyses of the entire sen-
tence. Second, there are rare cases in which
the same substring functions as both repair and
reparandum (i.e., the repair string is itself re-
paired again). A single global analysis would
not be able to capture this (since the TAG chan-
nel model does not permit the same substring
to be both a reparandum and a repair), but
we combine these overlapping repair substring
analyses in a post-processing operation to yield
an analysis of the whole sentence. (We do in-
sist that the reparandum and interregnum of a
repair do not overlap with those of any other
repairs in the same analysis).
4 Evaluation
This section describes how we evaluate our noisy
model. As mentioned earlier, following Char-
niak and Johnson (2001) our test data consisted
of all Penn III Switchboard tree-bank sw4[0-
1]*.mrg files. However, our test data differs
from theirs in that in this test we deleted all
partial words and punctuation from the data,
as this results in a more realistic test situation.
Since the immediate goal of this work is to
produce a program that identifies the words of a
sentence that belong to the reparandum of a re-
pair construction (to a first approximation these
words can be ignored in later processing), our
evaluation focuses on the model?s performance
in recovering the words in a reparandum. That
is, the model is used to classify each word in the
sentence as belonging to a reparandum or not,
and all other additional structure produced by
the model is ignored.
We measure model performance using stan-
dard precision p, recall r and f-score f , mea-
sures. If nc is the number of reparandum words
the model correctly classified, nt is the number
of true reparandum words given by the manual
annotations and nm is the number of words the
model predicts to be reparandum words, then
the precision is nc/nm, recall is nc/nt, and f is
2pr/(p + r).
For comparison we include the results of run-
ning the word-by-word classifier described in
Charniak and Johnson (2001), but where par-
tial words and punctuation have been removed
from the training and test data. We also pro-
vide results for our noisy channel model using
a bigram language model and a second trigram
model where the twenty most likely analyses are
rescored. Finally we show the results using the
parser language model.
CJ01? Bigram Trigram Parser
Precision 0.951 0.776 0.774 0.820
Recall 0.631 0.736 0.763 0.778
F-score 0.759 0.756 0.768 0.797
The noisy channel model using a bigram lan-
guage model does a slightly worse job at identi-
fying reparandum and interregnum words than
the classifier proposed in Charniak and Johnson
(2001). Replacing the bigram language model
with a trigram model helps slightly, and parser-
based language model results in a significant
performance improvement over all of the oth-
ers.
5 Conclusion and further work
This paper has proposed a novel noisy chan-
nel model of speech repairs and has used it to
identify reparandum words. One of the advan-
tages of probabilistic models is that they can be
integrated with other probabilistic models in a
principled way, and it would be interesting to
investigate how to integrate this kind of model
of speech repairs with probabilistic speech rec-
ognizers.
There are other kinds of joint models of
reparandum and repair that may produce a bet-
ter reparandum detection system. We have
experimented with versions of the models de-
scribed above based on POS bi-tag dependen-
cies rather than word bigram dependencies, but
with results very close to those presented here.
Still, more sophisticated models may yield bet-
ter performance.
It would also be interesting to combine this
probabilistic model of speech repairs with the
word classifier approach of Charniak and John-
son (2001). That approach may do so well be-
cause many speech repairs are very short, in-
volving only one or two words Shriberg and
Stolcke (1998), so the reparandum, interregnum
and repair are all contained in the surround-
ing word window used as features by the classi-
fier. On the other hand, the probabilistic model
of repairs explored here seems to be most suc-
cessful in identifying long repairs in which the
reparandum and repair are similar enough to be
unlikely to have been generated independently.
Since the two approaches seem to have different
strengths, a combined model may outperform
both of them.
References
Eugene Charniak and Mark Johnson. 2001.
Edit detection and parsing for transcribed
speech. In Proceedings of the 2nd Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
118?126. The Association for Computational
Linguistics.
Eugene Charniak. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting of the Association for
Computational Linguistics. The Association
for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998.
An empirical study of smoothing techniques
for language modeling. Technical Report TR-
10-98, Center for Research in Computing
Technology, Harvard University.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases, and dis-
course markers: Modeling speaker?s utter-
ances in spoken dialogue. Computational
Linguistics, 25(4):527?571.
Stuart M. Shieber and Yves Schabes. 1990.
Synchronous tree-adjoining grammars. In
Proceedings of the 13th International Confer-
ence on Computational Linguistics (COLING
1990), pages 253?258.
Stuart M. Shieber. 1985. Evidence against the
Context-Freeness of natural language. Lin-
guistics and Philosophy, 8(3):333?344.
Elizabeth Shriberg and Andreas Stolcke. 1998.
How far do speakers back up in repairs? a
quantitative model. In Proceedings of the In-
ternational Conference on Spoken Language
Processing, volume 5, pages 2183?2186, Syd-
ney, Australia.
Elizabeth Shriberg. 1994. Preliminaries to a
Theory of Speech Disfluencies. Ph.D. thesis,
University of California, Berkeley.
Attention Shifting for Parsing Speech ?
Keith Hall
Department of Computer Science
Brown University
Providence, RI 02912
kh@cs.brown.edu
Mark Johnson
Department of Cognitive and Linguistic Science
Brown University
Providence, RI 02912
Mark Johnson@Brown.edu
Abstract
We present a technique that improves the efficiency
of word-lattice parsing as used in speech recogni-
tion language modeling. Our technique applies a
probabilistic parser iteratively where on each iter-
ation it focuses on a different subset of the word-
lattice. The parser?s attention is shifted towards
word-lattice subsets for which there are few or no
syntactic analyses posited. This attention-shifting
technique provides a six-times increase in speed
(measured as the number of parser analyses evalu-
ated) while performing equivalently when used as
the first-stage of a multi-stage parsing-based lan-
guage model.
1 Introduction
Success in language modeling has been dominated
by the linear n-gram for the past few decades. A
number of syntactic language models have proven
to be competitive with the n-gram and better than
the most popular n-gram, the trigram (Roark, 2001;
Xu et al, 2002; Charniak, 2001; Hall and Johnson,
2003). Language modeling for speech could well be
the first real problem for which syntactic techniques
are useful.
John ate the pizza on a plate with a fork .
NP:plate NP:fork
PP:withPP:on
IN INVB NP
VP:ate
Figure 1: An incomplete parse tree with head-word an-
notations.
One reason that we expect syntactic models to
perform well is that they are capable of model-
ing long-distance dependencies that simple n-gram
? This research was supported in part by NSF grants 9870676
and 0085940.
models cannot. For example, the model presented
by Chelba and Jelinek (Chelba and Jelinek, 1998;
Xu et al, 2002) uses syntactic structure to identify
lexical items in the left-context which are then mod-
eled as an n-gram process. The model presented
by Charniak (Charniak, 2001) identifies both syn-
tactic structural and lexical dependencies that aid in
language modeling. While there are n-gram mod-
els that attempt to extend the left-context window
through the use of caching and skip models (Good-
man, 2001), we believe that linguistically motivated
models, such as these lexical-syntactic models, are
more robust.
Figure 1 presents a simple example to illustrate
the nature of long-distance dependencies. Using
a syntactic model such as the the Structured Lan-
guage Model (Chelba and Jelinek, 1998), we pre-
dict the word fork given the context {ate, with}
where a trigram model uses the context {with, a}.
Consider the problem of disambiguating between
. . . plate with a fork and . . . plate with effort. The
syntactic model captures the semantic relationship
between the words ate and fork. The syntactic struc-
ture allows us to find lexical contexts for which
there is some semantic relationship (e.g., predicate-
argument).
Unfortunately, syntactic language modeling tech-
niques have proven to be extremely expensive in
terms of computational effort. Many employ the
use of string parsers; in order to utilize such tech-
niques for language modeling one must preselect a
set of strings from the word-lattice and parse each of
them separately, an inherently inefficient procedure.
Of the techniques that can process word-lattices di-
rectly, it takes significant computation to achieve
the same levels of accuracy as the n?best rerank-
ing method. This computational cost is the result of
increasing the search space evaluated with the syn-
tactic model (parser); the larger space resulting from
combining the search for syntactic structure with the
search for paths in the word-lattice.
In this paper we propose a variation of a proba-
bilistic word-lattice parsing technique that increases
0 1
yesterday/0
2and/4.004
3
in/14.73
4tuesday/0
14
tuesday/0 5
to/0.000
6
two/8.769
7it/51.59
to/0
8
outlaw/83.57
9
outline/2.573
10
outlined/12.58
outlines/10.71
outline/0
outlined/8.027
outlines/7.14013
to/0
in/0
of/115.4
a/71.30
the/115.3 11
strategy/0
strategy/0
outline/0
12/0
</s>/0
Figure 2: A partial word-lattice from the NIST HUB-1 dataset.
efficiency while incurring no loss of language mod-
eling performance (measured as Word Error Rate ?
WER). In (Hall and Johnson, 2003) we presented
a modular lattice parsing process that operates in
two stages. The first stage is a PCFG word-lattice
parser that generates a set of candidate parses over
strings in a word-lattice, while the second stage
rescores these candidate edges using a lexicalized
syntactic language model (Charniak, 2001). Under
this paradigm, the first stage is not only responsible
for selecting candidate parses, but also for selecting
paths in the word-lattice. Due to computational and
memory requirements of the lexicalized model, the
second stage parser is capable of rescoring only a
small subset of all parser analyses. For this reason,
the PCFG prunes the set of parser analyses, thereby
indirectly pruning paths in the word lattice.
We propose adding a meta-process to the first-
stage that effectively shifts the selection of word-
lattice paths to the second stage (where lexical in-
formation is available). We achieve this by ensuring
that for each path in the word-lattice the first-stage
parser posits at least one parse.
2 Parsing speech word-lattices
P (A,W ) = P (A|W )P (W ) (1)
The noisy channel model for speech is presented in
Equation 1, where A represents the acoustic data ex-
tracted from a speech signal, and W represents a
word string. The acoustic model P (A|W ) assigns
probability mass to the acoustic data given a word
string and the language model P (W ) defines a dis-
tribution over word strings. Typically the acoustic
model is broken into a series of distributions condi-
tioned on individual words (though these are based
on false independence assumptions).
P (A|w
1
. . . w
i
. . . w
n
) =
n
?
i=1
P (A|w
i
) (2)
The result of the acoustic modeling process is a set
of string hypotheses; each word of each hypothesis
is assigned a probability by the acoustic model.
Word-lattices are a compact representation of
output of the acoustic recognizer; an example is pre-
sented in Figure 2. The word-lattice is a weighted
directed acyclic graph where a path in the graph cor-
responds to a string predicted by the acoustic recog-
nizer. The (sum) product of the (log) weights on the
graph (the acoustic probabilities) is the probability
of the acoustic data given the string. Typically we
want to know the most likely string given the acous-
tic data.
arg maxP (W |A) (3)
= arg max P (A,W )
= arg max P (A|W )P (W )
In Equation 3 we use Bayes? rule to find the opti-
mal string given P (A|W ), the acoustic model, and
P (W ), the language model. Although the language
model can be used to rescore1 the word-lattice, it is
typically used to select a single hypothesis.
We focus our attention in this paper to syntactic
language modeling techniques that perform com-
plete parsing, meaning that parse trees are built
upon the strings in the word-lattice.
2.1 n?best list reranking
Much effort has been put forth in developing effi-
cient probabilistic models for parsing strings (Cara-
ballo and Charniak, 1998; Goldwater et al, 1998;
Blaheta and Charniak, 1999; Charniak, 2000; Char-
niak, 2001); an obvious solution to parsing word-
lattices is to use n?best list reranking. The n?best
list reranking procedure, depicted in Figure 3, uti-
lizes an external language model that selects a set
of strings from the word-lattice. These strings are
analyzed by the parser which computes a language
model probability. This probability is combined
1To rescore a word-lattice, each arch is assigned a new score
(probability) defined by a new model (in combination with the
acoustic model).
w1, ..., wi, ..., wn1
...
Language
Model
w1, ..., wi, ..., wn2
w1, ..., wi, ..., wn3
w1, ..., wi, ..., wn4
w1, ..., wi, ..., wnm
o1, ..., oi, ..., on
8
2
3
5
1 6
4
7 10
9
the/0
man/0
is/0
duh/1.385
man/0 is/0
surely/0
early/0
mans/1.385
man's/1.385
surly/0
surly/0.692
early/0
early/0 n-best 
list
extractor
Figure 3: n?best list reranking
with the acoustic model probability to reranked the
strings according to the joint probability P (A,W ).
There are two significant disadvantages to this ap-
proach. First, we are limited by the performance
of the language model used to select the n?best
lists. Usually, the trigram model is used to se-
lect n paths through the lattice generating at most
n unique strings. The maximum performance that
can be achieved is limited by the performance of
this extractor model. Second, of the strings that
are analyzed by the parser, many will share com-
mon substrings. Much of the work performed by
the parser is duplicated for these substrings. This
second point is the primary motivation behind pars-
ing word-lattices (Hall and Johnson, 2003).
2.2 Multi-stage parsing
?
PCFG Parser
?
?
? ?
Lexicalized 
Parser
Figure 4: Coarse-to-fine lattice parsing.
In Figure 4 we present the general overview of
a multi-stage parsing technique (Goodman, 1997;
Charniak, 2000; Charniak, 2001). This process
1. Parse word-lattice with PCFG parser
2. Overparse, generating additional candidates
3. Compute inside-outside probabilities
4. Prune candidates with probability threshold
Table 1: First stage word-lattice parser
is know as coarse-to-fine modeling, where coarse
models are more efficient but less accurate than
fine models, which are robust but computation-
ally expensive. In this particular parsing model a
PCFG best-first parser (Bobrow, 1990; Caraballo
and Charniak, 1998) is used to search the uncon-
strained space of parses ? over a string. This first
stage performs overparsing which effectively al-
lows it to generate a set of high probability candi-
date parses ??. These parses are then rescored us-
ing a lexicalized syntactic model (Charniak, 2001).
Although the coarse-to-fine model may include any
number of intermediary stages, in this paper we con-
sider this two-stage model.
There is no guarantee that parses favored by the
second stage will be generated by the first stage. In
other words, because the first stage model prunes
the space of parses from which the second stage
rescores, the first stage model may remove solutions
that the second stage would have assigned a high
probability.
In (Hall and Johnson, 2003), we extended the
multi-stage parsing model to work on word-lattices.
The first-stage parser, Table 1, is responsible for
positing a set of candidate parses over the word-
lattice. Were we to run the parser to completion it
would generate all parses for all strings described
by the word-lattice. As with string parsing, we stop
the first stage parser early, generating a subset of
all parses. Only the strings covered by complete
parses are passed on to the second stage parser. This
indirectly prunes the word-lattice of all word-arcs
that were not covered by complete parses in the first
stage.
We use a first stage PCFG parser that performs
a best-first search over the space of parses, which
means that it depends on a heuristic ?figure-of-
merit? (FOM) (Caraballo and Charniak, 1998). A
good FOM attempts to model the true probability
of a chart edge2 P (N i
j,k
). Generally, this proba-
bility is impossible to compute during the parsing
process as it requires knowing both the inside and
outside probabilities (Charniak, 1993; Manning and
Schu?tze, 1999). The FOM we describe is an ap-
proximation to the edge probability and is computed
using an estimate of the inside probability times an
approximation to the outside probability 3.
The inside probability ?(Ni
j,k
) can be computed
incrementally during bottom-up parsing. The nor-
malized acoustic probabilities from the acoustic rec-
ognizer are included in this calculation.
??(N i
j,k
) (4)
=
?
i,l,q,r
fwd(T q
i,j
)p(N i|T q)p(T
r
|N i)bkwd(T r
k,l
)
The outside probability is approximated with a
bitag model and the standard tag/category bound-
ary model (Caraballo and Charniak, 1998; Hall and
Johnson, 2003). Equation 4 presents the approx-
imation to the outside probability. Part-of-speech
tags T q and T r are the candidate tags to the left
and right of the constituent Ni
j,k
. The fwd() and
bkwd() functions are the HMM forward and back-
ward probabilities calculated over a lattice con-
taining the part-of-speech tag, the word, and the
acoustic scores from the word-lattice to the left and
right of the constituent, respectively. p(Ni|T q) and
p(T
r
|N i) are the boundary statistics which are esti-
mated from training data (details of this model can
be found in (Hall and Johnson, 2003)).
FOM(N i
j,k
) = ??(N i
j,k
)?(N i
j,k
)?C(j, k) (5)
The best-first search employed by the first stage
parser uses the FOM defined in Equation 5, where
? is a normalization factor based on path length
C(j, k). The normalization factor prevents small
constituents from consistently being assigned a
2A chart edge Ni
j,k
indicates a grammar category Ni can
be constructed from nodes j to k.
3An alternative to the inside and outside probabilities are
the Viterbi inside and outside probabilities (Goldwater et al,
1998; Hall and Johnson, 2003).
higher probability than larger constituents (Goldwa-
ter et al, 1998).
Although this heuristic works well for directing
the parser towards likely parses over a string, it
is not an ideal model for pruning the word-lattice.
First, the outside approximation of this FOM is
based on a linear part-of-speech tag model (the
bitag). Such a simple syntactic model is unlikely
to provide realistic information when choosing a
word-lattice path to consider. Second, the model is
prone to favoring subsets of the word-lattice caus-
ing it to posit additional parse trees for the favored
sublattice rather than exploring the remainder of the
word-lattice. This second point is the primary moti-
vation for the attention shifting technique presented
in the next section.
3 Attention shifting4
We explore a modification to the multi-stage parsing
algorithm that ensures the first stage parser posits
at least one parse for each path in the word-lattice.
The idea behind this is to intermittently shift the at-
tention of the parser to unexplored parts of the word
lattice.
Identify
Used Edges
Clear Agenda/
Add Edges for 
Unused Words
Is Agenda
Empty? no
Continue 
Multi-stage
Parsing
yes
PCFG
Word-lattice
Parser
Figure 5: Attention shifting parser.
Figure 5 depicts the attention shifting first stage
parsing procedure. A used edge is a parse edge that
has non-zero outside probability. By definition of
4The notion of attention shifting is motivated by the work on
parser FOM compensation presented in (Blaheta and Charniak,
1999).
the outside probability, used edges are constituents
that are part of a complete parse; a parse is com-
plete if there is a root category label (e.g., S for sen-
tence) that spans the entire word-lattice. In order to
identify used edges, we compute the outside prob-
abilities for each parse edge (efficiently computing
the outside probability of an edge requires that the
inside probabilities have already been computed).
In the third step of this algorithm we clear the
agenda, removing all partial analyses evaluated by
the parser. This forces the parser to abandon analy-
ses of parts of the word-lattice for which complete
parses exist. Following this, the agenda is popu-
lated with edges corresponding to the unused words,
priming the parser to consider these words. To en-
sure the parser builds upon at least one of these
unused edges, we further modify the parsing algo-
rithm:
? Only unused edges are added to the agenda.
? When building parses from the bottom up, a
parse is considered complete if it connects to a
used edge.
These modifications ensure that the parser focuses
on edges built upon the unused words. The sec-
ond modification ensures the parser is able to de-
termine when it has connected an unused word with
a previously completed parse. The application of
these constraints directs the attention of the parser
towards new edges that contribute to parse anal-
yses covering unused words. We are guaranteed
that each iteration of the attention shifting algorithm
adds a parse for at least one unused word, meaning
that it will take at most |A| iterations to cover the en-
tire lattice, where A is the set of word-lattice arcs.
This guarantee is trivially provided through the con-
straints just described. The attention-shifting parser
continues until there are no unused words remain-
ing and each parsing iteration runs until it has found
a complete parse using at least one of the unused
words.
As with multi-stage parsing, an adjustable param-
eter determines how much overparsing to perform
on the initial parse. In the attention shifting algo-
rithm an additional parameter specifies the amount
of overparsing for each iteration after the first. The
new parameter allows for independent control of the
attention shifting iterations.
After the attention shifting parser populates a
parse chart with parses covering all paths in the
lattice, the multi-stage parsing algorithm performs
additional pruning based on the probability of the
parse edges (the product of the inside and outside
probabilities). This is necessary in order to con-
strain the size of the hypothesis set passed on to the
second stage parsing model.
The Charniak lexicalized syntactic language
model effectively splits the number of parse states
(an edges in a PCFG parser) by the number of
unique contexts in which the state is found. These
contexts include syntactic structure such as parent
and grandparent category labels as well as lexical
items such as the head of the parent or the head of a
sibling constituent (Charniak, 2001). State splitting
on this level causes the memory requirement of the
lexicalized parser to grow rapidly.
Ideally, we would pass all edges on to the sec-
ond stage, but due to memory limitations, pruning
is necessary. It is likely that edges recently discov-
ered by the attention shifting procedure are pruned.
However, the true PCFG probability model is used
to prune these edges rather than the approximation
used in the FOM. We believe that by considering
parses which have a relatively high probability ac-
cording to the combined PCFG and acoustic models
that we will include most of the analyses for which
the lexicalized parser assigns a high probability.
4 Experiments
The purpose of attention shifting is to reduce the
amount of work exerted by the first stage PCFG
parser while maintaining the same quality of lan-
guage modeling (in the multi-stage system). We
have performed a set of experiments on the NIST
?93 HUB?1 word-lattices. The HUB?1 is a collec-
tion of 213 word-lattices resulting from an acoustic
recognizer?s analysis of speech utterances. Profes-
sional readers reading Wall Street Journal articles
generated the utterances.
The first stage parser is a best-first PCFG parser
trained on sections 2 through 22, and 24 of the Penn
WSJ treebank (Marcus et al, 1993). Prior to train-
ing, the treebank is transformed into speech-like
text, removing punctuation and expanding numer-
als, etc.5 Overparsing is performed using an edge
pop6 multiplicative factor. The parser records the
number of edge pops required to reach the first com-
plete parse. The parser continues to parse a until
multiple of the number of edge pops required for
the first parse are popped off the agenda.
The second stage parser used is a modified ver-
sion of the Charniak language modeling parser de-
scribed in (Charniak, 2001). We trained this parser
5Brian Roark of AT&T provided a tool to perform the
speech normalization.
6An edge pop is the process of the parser removing an edge
from the agenda and placing it in the parse chart.
on the BLLIP99 corpus (Charniak et al, 1999); a
corpus of 30million words automatically parsed us-
ing the Charniak parser (Charniak, 2000).
In order to compare the work done by the n?best
reranking technique to the word-lattice parser, we
generated a set of n?best lattices. 50?best lists were
extracted using the Chelba A* decoder7. A 50?
best lattice is a sublattice of the acoustic lattice that
generates only the strings found in the 50?best list.
Additionally, we provide the results for parsing the
full acoustic lattices (although these work measure-
ments should not be compared to those of n?best
reranking).
We report the amount of work, shown as the
cumulative # edge pops, the oracle WER for the
word-lattices after first stage pruning, and the WER
of the complete multi-stage parser. In all of the
word-lattice parsing experiments, we pruned the set
of posited hypothesis so that no more than 30,000
local-trees are generated8. We chose this thresh-
old due to the memory requirements of the sec-
ond stage parser. Performing pruning at the end of
the first stage prevents the attention shifting parser
from reaching the minimum oracle WER (most no-
table in the full acoustic word-lattice experiments).
While the attention-shifting algorithm ensures all
word-lattice arcs are included in complete parses,
forward-backward pruning, as used here, will elim-
inate some of these parses, indirectly eliminating
some of the word-lattice arcs.
To illustrate the need for pruning, we computed
the number of states used by the Charniak lexi-
calized syntactic language model for 30,000 local
trees. An average of 215 lexicalized states were
generated for each of the 30,000 local trees. This
means that the lexicalized language model, on av-
erage, computes probabilities for over 6.5 million
states when provided with 30,000 local trees.
Model # edge pops O-WER WER
n?best (Charniak) 2.5 million 7.75 11.8
100x LatParse 3.4 million 8.18 12.0
10x AttShift 564,895 7.78 11.9
Table 2: Results for n?best lists and n?best lattices.
Table 2 shows the results for n?best list rerank-
ing and word-lattice parsing of n?best lattices.
We recreated the results of the Charniak language
model parser used for reranking in order to measure
the amount of work required. We ran the first stage
parser with 4-times overparsing for each string in
7The n?best lists were provided by Brian Roark (Roark,
2001)
8A local-tree is an explicit expansion of an edge and its chil-
dren. An example local tree is NP
3,8
? DT
3,4
NN
4,8
.
the n?best list. The LatParse result represents run-
ning the word-lattice parser on the n?best lattices
performing 100?times overparsing in the first stage.
The AttShift model is the attention shifting parser
described in this paper. We used 10?times overpars-
ing for both the initial parse and each of the attention
shifting iterations. When run on the n?best lattice,
this model achieves a comparable WER, while re-
ducing the amount of parser work sixfold (as com-
pared to the regular word-lattice parser).
Model # edge pops O-WER WER
acoustic lats N/A 3.26 N/A
100x LatParse 3.4 million 5.45 13.1
10x AttShift 1.6 million 4.17 13.1
Table 3: Results for acoustic lattices.
In Table 3 we present the results of the word-
lattice parser and the attention shifting parser when
run on full acoustic lattices. While the oracle WER
is reduced, we are considering almost half as many
edges as the standard word-lattice parser. The in-
creased size of the acoustic lattices suggests that it
may not be computationally efficient to consider the
entire lattice and that an additional pruning phase is
necessary.
The most significant constraint of this multi-stage
lattice parsing technique is that the second stage
process has a large memory requirement. While the
attention shifting technique does allow the parser to
propose constituents for every path in the lattice, we
prune some of these constituents prior to performing
analysis by the second stage parser. Currently, prun-
ing is accomplished using the PCFG model. One
solution is to incorporate an intermediate pruning
stage (e.g., lexicalized PCFG) between the PCFG
parser and the full lexicalized model. Doing so will
relax the requirement for aggressive PCFG pruning
and allows for a lexicalized model to influence the
selection of word-lattice paths.
5 Conclusion
We presented a parsing technique that shifts the at-
tention of a word-lattice parser in order to ensure
syntactic analyses for all lattice paths. Attention
shifting can be thought of as a meta-process around
the first stage of a multi-stage word-lattice parser.
We show that this technique reduces the amount of
work exerted by the first stage PCFG parser while
maintaining comparable language modeling perfor-
mance.
Attention shifting is a simple technique that at-
tempts to make word-lattice parsing more efficient.
As suggested by the results for the acoustic lattice
experiments, this technique alone is not sufficient.
Solutions to improve these results include modify-
ing the first-stage grammar by annotating the cat-
egory labels with local syntactic features as sug-
gested in (Johnson, 1998) and (Klein and Manning,
2003) as well as incorporating some level of lexical-
ization. Improving the quality of the parses selected
by the first stage should reduce the need for gen-
erating such a large number of candidates prior to
pruning, improving efficiency as well as overall ac-
curacy. We believe that attention shifting, or some
variety of this technique, will be an integral part of
efficient solutions for word-lattice parsing.
References
Don Blaheta and Eugene Charniak. 1999. Au-
tomatic compensation for parser figure-of-merit
flaws. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics,
pages 513?518.
Robert J. Bobrow. 1990. Statistical agenda pars-
ing. In DARPA Speech and Language Workshop,
pages 222?224.
Sharon Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilis-
tic chart parsing. Computational Linguistics,
24(2):275?298, June.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith
Hall, John Hale, and Mark Johnson. 1999.
BLLIP 1987?89 wsj corpus release 1. LDC cor-
pus LDC2000T43.
Eugene Charniak. 1993. Statistical Language
Learning. MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics., ACL,
New Brunswick, NJ.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics.
Ciprian Chelba and Frederick Jelinek. 1998. A
study on richer syntactic dependencies for struc-
tured language modeling. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International
Conference on Computational Linguistics, pages
225?231.
Sharon Goldwater, Eugene Charniak, and Mark
Johnson. 1998. Best-first edge-based chart pars-
ing. In 6th Annual Workshop for Very Large Cor-
pora, pages 127?133.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 11?25.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extendend version. In Microsoft
Research Technical Report MSR-TR-2001-72.
Keith Hall and Mark Johnson. 2003. Language
modeling using efficient best-first bottom-up
parsing. In Proceedings of IEEE Automated
Speech Recognition and Understanding Work-
shop.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24:617?636.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-03).
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of statistical natural lan-
guage processing. MIT Press.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(3):249?276.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceed-
ings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 191?
198.
Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm
Brian Roark Murat Saraclar
AT&T Labs - Research
{roark,murat}@research.att.com
Michael Collins Mark Johnson
MIT CSAIL Brown University
mcollins@csail.mit.edu Mark Johnson@Brown.edu
Abstract
This paper describes discriminative language modeling
for a large vocabulary speech recognition task. We con-
trast two parameter estimation methods: the perceptron
algorithm, and a method based on conditional random
fields (CRFs). The models are encoded as determin-
istic weighted finite state automata, and are applied by
intersecting the automata with word-lattices that are the
output from a baseline recognizer. The perceptron algo-
rithm has the benefit of automatically selecting a rela-
tively small feature set in just a couple of passes over the
training data. However, using the feature set output from
the perceptron algorithm (initialized with their weights),
CRF training provides an additional 0.5% reduction in
word error rate, for a total 1.8% absolute reduction from
the baseline of 39.2%.
1 Introduction
A crucial component of any speech recognizer is the lan-
guage model (LM), which assigns scores or probabilities
to candidate output strings in a speech recognizer. The
language model is used in combination with an acous-
tic model, to give an overall score to candidate word se-
quences that ranks them in order of probability or plau-
sibility.
A dominant approach in speech recognition has been
to use a ?source-channel?, or ?noisy-channel? model. In
this approach, language modeling is effectively framed
as density estimation: the language model?s task is to
define a distribution over the source ? i.e., the possible
strings in the language. Markov (n-gram) models are of-
ten used for this task, whose parameters are optimized
to maximize the likelihood of a large amount of training
text. Recognition performance is a direct measure of the
effectiveness of a language model; an indirect measure
which is frequently proposed within these approaches is
the perplexity of the LM (i.e., the log probability it as-
signs to some held-out data set).
This paper explores alternative methods for language
modeling, which complement the source-channel ap-
proach through discriminatively trained models. The lan-
guage models we describe do not attempt to estimate a
generative model P (w) over strings. Instead, they are
trained on acoustic sequences with their transcriptions,
in an attempt to directly optimize error-rate. Our work
builds on previous work on language modeling using the
perceptron algorithm, described in Roark et al (2004).
In particular, we explore conditional random field meth-
ods, as an alternative training method to the perceptron.
We describe how these models can be trained over lat-
tices that are the output from a baseline recognizer. We
also give a number of experiments comparing the two ap-
proaches. The perceptron method gave a 1.3% absolute
improvement in recognition error on the Switchboard do-
main; the CRF methods we describe give a further gain,
the final absolute improvement being 1.8%.
A central issue we focus on concerns feature selection.
The number of distinct n-grams in our training data is
close to 45 million, and we show that CRF training con-
verges very slowly even when trained with a subset (of
size 12 million) of these features. Because of this, we ex-
plore methods for picking a small subset of the available
features.1 The perceptron algorithm can be used as one
method for feature selection, selecting around 1.5 million
features in total. The CRF trained with this feature set,
and initialized with parameters from perceptron training,
converges much more quickly than other approaches, and
also gives the optimal performance on the held-out set.
We explore other approaches to feature selection, but find
that the perceptron-based approach gives the best results
in our experiments.
While we focus on n-gram models, we stress that our
methods are applicable to more general language mod-
eling features ? for example, syntactic features, as ex-
plored in, e.g., Khudanpur and Wu (2000). We intend
to explore methods with new features in the future. Ex-
perimental results with n-gram models on 1000-best lists
show a very small drop in accuracy compared to the use
of lattices. This is encouraging, in that it suggests that
models with more flexible features than n-gram models,
which therefore cannot be efficiently used with lattices,
may not be unduly harmed by their restriction to n-best
lists.
1.1 Related Work
Large vocabulary ASR has benefitted from discrimina-
tive estimation of Hidden Markov Model (HMM) param-
eters in the form of Maximum Mutual Information Es-
timation (MMIE) or Conditional Maximum Likelihood
Estimation (CMLE). Woodland and Povey (2000) have
shown the effectiveness of lattice-based MMIE/CMLE in
challenging large scale ASR tasks such as Switchboard.
In fact, state-of-the-art acoustic modeling, as seen, for
example, at annual Switchboard evaluations, invariably
includes some kind of discriminative training.
Discriminative estimation of language models has also
been proposed in recent years. Jelinek (1995) suggested
an acoustic sensitive language model whose parameters
1Note also that in addition to concerns about training time, a lan-
guage model with fewer features is likely to be considerably more effi-
cient when decoding new utterances.
are estimated by minimizing H(W |A), the expected un-
certainty of the spoken text W, given the acoustic se-
quence A. Stolcke and Weintraub (1998) experimented
with various discriminative approaches including MMIE
with mixed results. This work was followed up with
some success by Stolcke et al (2000) where an ?anti-
LM?, estimated from weighted N-best hypotheses of a
baseline ASR system, was used with a negative weight
in combination with the baseline LM. Chen et al (2000)
presented a method based on changing the trigram counts
discriminatively, together with changing the lexicon to
add new words. Kuo et al (2002) used the generalized
probabilistic descent algorithm to train relatively small
language models which attempt to minimize string error
rate on the DARPA Communicator task. Banerjee et al
(2003) used a language model modification algorithm in
the context of a reading tutor that listens. Their algorithm
first uses a classifier to predict what effect each parame-
ter has on the error rate, and then modifies the parameters
to reduce the error rate based on this prediction.
2 Linear Models, the Perceptron
Algorithm, and Conditional Random
Fields
This section describes a general framework, global linear
models, and two parameter estimation methods within
the framework, the perceptron algorithm and a method
based on conditional random fields. The linear models
we describe are general enough to be applicable to a di-
verse range of NLP and speech tasks ? this section gives
a general description of the approach. In the next section
of the paper we describe how global linear models can
be applied to speech recognition. In particular, we focus
on how the decoding and parameter estimation problems
can be implemented over lattices using finite-state tech-
niques.
2.1 Global linear models
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . We assume the following compo-
nents: (1) Training examples (xi, yi) for i = 1 . . . N .
(2) A function GEN which enumerates a set of candi-
dates GEN(x) for an input x. (3) A representation
? mapping each (x, y) ? X ? Y to a feature vector
?(x, y) ? Rd. (4) A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping
from an input x to an output F (x) through
F (x) = argmax
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the y that maximizes
Eq. 1.
2.2 The Perceptron algorithm
We now turn to methods for training the parameters
?? of the model, given a set of training examples
Inputs: Training examples (xi, yi)
Initialization: Set ?? = 0
Algorithm:
For t = 1 . . . T , i = 1 . . . N
Calculate zi = argmaxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Output: Parameters ??
Figure 1: A variant of the perceptron algorithm.
(x1, y1) . . . (xN , yN ). This section describes the per-
ceptron algorithm, which was previously applied to lan-
guage modeling in Roark et al (2004). The next section
describes an alternative method, based on conditional
random fields.
The perceptron algorithm is shown in figure 1. At
each training example (xi, yi), the current best-scoring
hypothesis zi is found, and if it differs from the refer-
ence yi , then the cost of each feature2 is increased by
the count of that feature in zi and decreased by the count
of that feature in yi. The features in the model are up-
dated, and the algorithm moves to the next utterance.
After each pass over the training data, performance on
a held-out data set is evaluated, and the parameterization
with the best performance on the held out set is what is
ultimately produced by the algorithm.
Following Collins (2002), we used the averaged pa-
rameters from the training algorithm in decoding held-
out and test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 Conditional Random Fields
Conditional Random Fields have been applied to NLP
tasks such as parsing (Ratnaparkhi et al, 1994; Johnson
et al, 1999), and tagging or segmentation tasks (Lafferty
et al, 2001; Sha and Pereira, 2003; McCallum and Li,
2003; Pinto et al, 2003). CRFs use the parameters ??
to define a conditional distribution over the members of
GEN(x) for a given input x:
p??(y|x) =
1
Z(x, ??)
exp (?(x, y) ? ??)
where Z(x, ??) =
?
y?GEN(x) exp (?(x, y) ? ??) is a
normalization constant that depends on x and ??.
Given these definitions, the log-likelihood of the train-
ing data under parameters ?? is
LL(??) =
N?
i=1
log p??(yi|xi)
=
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)] (2)
2Note that here lattice weights are interpreted as costs, which
changes the sign in the algorithm presented in figure 1.
Following Johnson et al (1999) and Lafferty et al
(2001), we use a zero-mean Gaussian prior on the pa-
rameters resulting in the regularized objective function:
LLR(??) =
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)]?
||??||2
2?2
(3)
The value ? dictates the relative influence of the log-
likelihood term vs. the prior, and is typically estimated
using held-out data. The optimal parameters under this
criterion are ??? = argmax?? LLR(??).
We use a limited memory variable metric method
(Benson and More?, 2002) to optimize LLR. There is a
general implementation of this method in the Tao/PETSc
software libraries (Balay et al, 2002; Benson et al,
2002). This technique has been shown to be very effec-
tive in a variety of NLP tasks (Malouf, 2002; Wallach,
2002). The main interface between the optimizer and the
training data is a procedure which takes a parameter vec-
tor ?? as input, and in turn returns LLR(??) as well as
the gradient of LLR at ??. The derivative of the objec-
tive function with respect to a parameter ?s at parameter
values ?? is
?LLR
??s
=
N?
i=1
?
??s(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?s(xi, y)
?
??
?s
?2
(4)
Note that LLR(??) is a convex function, so that there is
a globally optimal solution and the optimization method
will find it. The use of the Gaussian prior term ||??||2/2?2
in the objective function has been found to be useful in
several NLP settings. It effectively ensures that there is a
large penalty for parameter values in the model becoming
too large ? as such, it tends to control over-training. The
choice ofLLR as an objective function can be justified as
maximum a-posteriori (MAP) training within a Bayesian
approach. An alternative justification comes through a
connection to support vector machines and other large
margin approaches. SVM-based approaches use an op-
timization criterion that is closely related to LLR ? see
Collins (2004) for more discussion.
3 Linear models for speech recognition
We now describe how the formalism and algorithms in
section 2 can be applied to language modeling for speech
recognition.
3.1 The basic approach
As described in the previous section, linear models re-
quire definitions of X , Y , xi, yi, GEN, ? and a param-
eter estimation method. In the language modeling setting
we take X to be the set of all possible acoustic inputs; Y
is the set of all possible strings, ??, for some vocabu-
lary ?. Each xi is an utterance (a sequence of acous-
tic feature-vectors), and GEN(xi) is the set of possible
transcriptions under a first pass recognizer. (GEN(xi)
is a huge set, but will be represented compactly using a
lattice ? we will discuss this in detail shortly). We take
yi to be the member of GEN(xi) with lowest error rate
with respect to the reference transcription of xi.
All that remains is to define the feature-vector repre-
sentation, ?(x, y). In the general case, each component
?i(x, y) could be essentially any function of the acous-
tic input x and the candidate transcription y. The first
feature we define is ?0(x, y) as the log-probability of y
given x under the lattice produced by the baseline recog-
nizer. Thus this feature will include contributions from
the acoustic model and the original language model. The
remaining features are restricted to be functions over the
transcription y alone and they track all n-grams up to
some length (say n = 3), for example:
?1(x, y) = Number of times ?the the of? is seen in y
At an abstract level, features of this form are introduced
for all n-grams up to length 3 seen in some training data
lattice, i.e., n-grams seen in any word sequence within
the lattices. In practice, we consider methods that search
for sparse parameter vectors ??, thus assigning many n-
grams 0 weight. This will lead to more efficient algo-
rithms that avoid dealing explicitly with the entire set of
n-grams seen in training data.
3.2 Implementation using WFA
We now give a brief sketch of how weighted finite-state
automata (WFA) can be used to implement linear mod-
els for speech recognition. There are several papers de-
scribing the use of weighted automata and transducers
for speech in detail, e.g., Mohri et al (2002), but for clar-
ity and completeness this section gives a brief description
of the operations which we use.
For our purpose, a WFA A = (?, Q, qs, F, E, ?),
where ? is the vocabulary, Q is a (finite) set of states,
qs ? Q is a unique start state, F ? Q is a set of final
states, E is a (finite) set of transitions, and ? : F ? R
is a function from final states to final weights. Each tran-
sition e ? E is a tuple e = (l[e], p[e], n[e], w[e]), where
l[e] ? ? is a label (in our case, words), p[e] ? Q is the
origin state of e, n[e] ? Q is the destination state of e,
and w[e] ? R is the weight of the transition. A suc-
cessful path pi = e1 . . . ej is a sequence of transitions,
such that p[e1] = qs, n[ej ] ? F , and for 1 < k ? j,
n[ek?1] = p[ek]. Let ?A be the set of successful paths pi
in a WFA A. For any pi = e1 . . . ej , l[pi] = l[e1] . . . l[ej ].
The weights of the WFA in our case are always in the
log semiring, which means that the weight of a path pi =
e1 . . . ej ? ?A is defined as:
wA[pi] =
(
j?
k=1
w[ek]
)
+ ?(ej) (5)
By convention, we use negative log probabilities as
weights, so lower weights are better. All WFA that we
will discuss in this paper are deterministic, i.e. there are
no  transitions, and for any two transitions e, e? ? E,
if p[e] = p[e?], then l[e] 6= l[e?]. Thus, for any string
w = w1 . . . wj , there is at most one successful path
pi ? ?A, such that pi = e1 . . . ej and for 1 ? k ? j,
l[ek] = wk, i.e. l[pi] = w. The set of strings w such that
there exists a pi ? ?A with l[pi] = w define a regular
language LA ? ?.
We can now define some operations that will be used
in this paper.
? ?A. For a set of transitions E and ? ? R, define
?E = {(l[e], p[e], n[e], ?w[e]) : e ? E}. Then, for
any WFA A = (?, Q, qs, F, E, ?), define ?A for ? ? R
as follows: ?A = (?, Q, qs, F, ?E, ??).
? A ?A?. The intersection of two deterministic WFAs
A ? A? in the log semiring is a deterministic WFA
such that LA?A? = LA
?
LA? . For any pi ? ?A?A? ,
wA?A? [pi] = wA[pi1] + wA? [pi2], where l[pi] = l[pi1] =
l[pi2].
?BestPath(A). This operation takes a WFA A, and
returns the best scoring path p?i = argminpi??A wA[pi].
? MinErr(A, y). Given a WFA A, a string y, and
an error-function E(y,w), this operation returns p?i =
argminpi??A E(y, l[pi]). This operation will generally be
used with y as the reference transcription for a particular
training example, and E(y,w) as some measure of the
number of errors in w when compared to y. In this case,
the MinErr operation returns the path pi ? ?A such
l[pi] has the smallest number of errors when compared to
y.
? Norm(A). Given a WFA A, this operation yields
a WFA A? such that LA = LA? and for every pi ? ?A
there is a pi? ? ?A? such that l[pi] = l[pi?] and
wA? [pi
?] = wA[pi] + log
(
?
p?i??A
exp(?wA[p?i])
)
(6)
Note that
?
pi?Norm(A)
exp(?wNorm(A)[pi]) = 1 (7)
In other words the weights define a probability distribu-
tion over the paths.
? ExpCount(A,w). Given a WFA A and an n-gram
w, we define the expected count of w in A as
ExpCount(A,w) =
?
pi??A
wNorm(A)[pi]C(w, l[pi])
where C(w, l[pi]) is defined to be the number of times
the n-gram w appears in a string l[pi].
Given an acoustic input x, let Lx be a deterministic
word-lattice produced by the baseline recognizer. The
lattice Lx is an acyclic WFA, representing a weighted set
of possible transcriptions of x under the baseline recog-
nizer. The weights represent the combination of acoustic
and language model scores in the original recognizer.
The new, discriminative language model constructed
during training consists of a deterministic WFA which
we will denote D, together with a single parameter ?0.
The parameter ?0 is the weight for the log probability
feature ?0 given by the baseline recognizer. The WFA
D is constructed so that LD = ?? and for all pi ? ?D
wD[pi] =
d?
j=1
?j(x, l[pi])?j
Recall that ?j(x,w) for j > 0 is the count of the j?th n-
gram in w, and ?j is the parameter associated with that
w  wi-2     i-1 w   wi-1     iwi
wi-1
?
wi
?wi
?
? wi
Figure 2: Representation of a trigram model with failure transitions.
n-gram. Then, by definition, ?0L ? D accepts the same
set of strings as L, but
w?0L?D[pi] =
d?
j=0
?j(x, l[pi])?j
and argmin
pi?L
?(x, l[pi]) ? ?? = BestPath(?0L ? D).
Thus decoding under our new model involves first pro-
ducing a lattice L from the baseline recognizer; second,
scaling L with ?0 and intersecting it with the discrimi-
native language model D; third, finding the best scoring
path in the new WFA.
We now turn to training a model, or more explicitly,
deriving a discriminative language model (D, ?0) from a
set of training examples. Given a training set (xi, ri) for
i = 1 . . . N , where xi is an acoustic sequence, and ri is
a reference transcription, we can construct lattices Li for
i = 1 . . . N using the baseline recognizer. We can also
derive target transcriptions yi = MinErr(Li, ri). The
training algorithm is then a mapping from (Li, yi) for
i = 1 . . . N to a pair (D, ?0). Note that the construction
of the language model requires two choices. The first
concerns the choice of the set of n-gram features ?i for
i = 1 . . . d implemented by D. The second concerns
the choice of parameters ?i for i = 0 . . . d which assign
weights to the n-gram features as well as the baseline
feature ?0.
Before describing methods for training a discrimina-
tive language model using perceptron and CRF algo-
rithms, we give a little more detail about the structure
of D, focusing on how n-gram language models can be
implemented with finite-state techniques.
3.3 Representation of n-gram language models
An n-gram model can be efficiently represented in a de-
terministic WFA, through the use of failure transitions
(Allauzen et al, 2003). Every string accepted by such an
automaton has a single path through the automaton, and
the weight of the string is the sum of the weights of the
transitions in that path. In such a representation, every
state in the automaton represents an n-gram history h,
e.g. wi?2wi?1, and there are transitions leaving the state
for every word wi such that the feature hwi has a weight.
There is also a failure transition leaving the state, labeled
with some reserved symbol ?, which can only be tra-
versed if the next symbol in the input does not match any
transition leaving the state. This failure transition points
to the backoff state h?, i.e. the n-gram history h minus
its initial word. Figure 2 shows how a trigram model can
be represented in such an automaton. See Allauzen et al
(2003) for more details.
Note that in such a deterministic representation, the
entire weight of all features associated with the word
wi following history h must be assigned to the transi-
tion labeled with wi leaving the state h in the automa-
ton. For example, if h = wi?2wi?1, then the trigram
wi?2wi?1wi is a feature, as is the bigram wi?1wi and
the unigram wi. In this case, the weight on the transi-
tion wi leaving state h must be the sum of the trigram,
bigram and unigram feature weights. If only the trigram
feature weight were assigned to the transition, neither the
unigram nor the bigram feature contribution would be in-
cluded in the path weight. In order to ensure that the cor-
rect weights are assigned to each string, every transition
encoding an order k n-gram must carry the sum of the
weights for all n-gram features of orders ? k. To ensure
that every string in ?? receives the correct weight, for
any n-gram hw represented explicitly in the automaton,
h?w must also be represented explicitly in the automaton,
even if its weight is 0.
3.4 The perceptron algorithm
The perceptron algorithm is incremental, meaning that
the language model D is built one training example at
a time, during several passes over the training set. Ini-
tially, we build D to accept all strings in ?? with weight
0. For the perceptron experiments, we chose the param-
eter ?0 to be a fixed constant, chosen by optimization on
the held-out set. The loop in the algorithm in figure 1 is
implemented as:
For t = 1 . . . T, i = 1 . . . N :
? Calculate zi = argmaxy?GEN(x) ?(x, y) ? ??
= BestPath(?0Li ? D)
? If zi 6= MinErr(Li, ri), then update the feature
weights as in figure 1 (modulo the sign, because of
the use of costs), and modify D so as to assign the
correct weight to all strings.
In addition, averaged parameters need to be stored
(see section 2.2). These parameters will replace the un-
averaged parameters in D once training is completed.
Note that the only n-gram features to be included in
D at the end of the training process are those that oc-
cur in either a best scoring path zi or a minimum error
path yi at some point during training. Thus the percep-
tron algorithm is in effect doing feature selection as a
by-product of training. Given N training examples, and
T passes over the training set,O(NT ) n-grams will have
non-zero weight after training. Experiments in Roark et
al. (2004) suggest that the perceptron reaches optimal
performance after a small number of training iterations,
for example T = 1 or T = 2. Thus O(NT ) can be very
small compared to the full number of n-grams seen in
all training lattices. In our experiments, the perceptron
method chose around 1.4 million n-grams with non-zero
weight. This compares to 43.65 million possible n-grams
seen in the training data.
This is a key contrast with conditional random fields,
which optimize the parameters of a fixed feature set. Fea-
ture selection can be critical in our domain, as training
and applying a discriminative language model over all
n-grams seen in the training data (in either correct or in-
correct transcriptions) may be computationally very de-
manding. One training scenario that we will consider
will be using the output of the perceptron algorithm (the
averaged parameters) to provide the feature set and the
initial feature weights for use in the CRF algorithm. This
leads to a model which is reasonably sparse, but has the
benefit of CRF training, which as we will see gives gains
in performance.
3.5 Conditional Random Fields
The CRF methods that we use assume a fixed definition
of the n-gram features ?i for i = 1 . . . d in the model.
In the experimental section we will describe a number of
ways of defining the feature set. The optimization meth-
ods we use begin at some initial setting for ??, and then
search for the parameters ??? which maximize LLR(??)
as defined in Eq. 3.
The optimization method requires calculation of
LLR(??) and the gradient of LLR(??) for a series of val-
ues for ??. The first step in calculating these quantities is
to take the parameter values ??, and to construct an ac-
ceptor D which accepts all strings in ??, such that
wD[pi] =
d?
j=1
?j(x, l[pi])?j
For each training lattice Li, we then construct a new lat-
tice L?i = Norm(?0Li ? D). The lattice L?i represents
(in the log domain) the distribution p??(y|xi) over strings
y ? GEN(xi). The value of log p??(yi|xi) for any i can
be computed by simply taking the path weight of pi such
that l[pi] = yi in the new lattice L?i. Hence computation
of LLR(??) in Eq. 3 is straightforward.
Calculating the n-gram feature gradients for the CRF
optimization is also relatively simple, once L?i has been
constructed. From the derivative in Eq. 4, for each i =
1 . . . N, j = 1 . . . d the quantity
?j(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?j(xi, y) (8)
must be computed. The first term is simply the num-
ber of times the j?th n-gram feature is seen in yi. The
second term is the expected number of times that the
j?th n-gram is seen in the acceptor L?i. If the j?th
n-gram is w1 . . . wn, then this can be computed as
ExpCount(L?i, w1 . . . wn). The GRM library, which
was presented in Allauzen et al (2003), has a direct im-
plementation of the function ExpCount, which simul-
taneously calculates the expected value of all n-grams of
order less than or equal to a given n in a lattice L.
The one non-ngram feature weight that is being esti-
mated is the weight ?0 given to the baseline ASR nega-
tive log probability. Calculation of the gradient of LLR
with respect to this parameter again requires calculation
of the term in Eq. 8 for j = 0 and i = 1 . . . N . Com-
putation of
?
y?GEN(xi)
p??(y|xi)?0(xi, y) turns out to
be not as straightforward as calculating n-gram expec-
tations. To do so, we rely upon the fact that ?0(xi, y),
the negative log probability of the path, decomposes to
the sum of negative log probabilities of each transition
in the path. We index each transition in the lattice Li,
and store its negative log probability under the baseline
model. We can then calculate the required gradient from
L?i, by calculating the expected value in L?i of each in-
dexed transition in Li.
We found that an approximation to the gradient of
?0, however, performed nearly identically to this exact
gradient, while requiring substantially less computation.
Let wn1 be a string of n words, labeling a path in word-
lattice L?i. For brevity, let Pi(wn1 ) = p??(wn1 |xi) be the
conditional probability under the current model, and let
Qi(wn1 ) be the probability of wn1 in the normalized base-
line ASR lattice Norm(Li). Let Li be the set of strings
in the language defined by Li. Then we wish to compute
Ei for i = 1 . . . N , where
Ei =
?
wn1 ?Li
Pi(w
n
1 ) log Qi(w
n
1 )
=
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
1 ) (9)
The approximation is to make the following Markov
assumption:
Ei ?
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
k?2)
=
?
xyz?Si
ExpCount(L?i, xyz) log Qi(z|xy)(10)
where Si is the set of all trigrams seen in Li. The term
log Qi(z|xy) can be calculated once before training for
every lattice in the training set; the ExpCount term is
calculated as before using the GRM library. We have
found this approximation to be effective in practice, and
it was used for the trials reported below.
When the gradients and conditional likelihoods are
collected from all of the utterances in the training set, the
contributions from the regularizer are combined to give
an overall gradient and objective function value. These
values are provided to the parameter estimation routine,
which then returns the parameters for use in the next it-
eration. The accumulation of gradients for the feature set
is the most time consuming part of the approach, but this
is parallelizable, so that the computation can be divided
among many processors.
4 Empirical Results
We present empirical results on the Rich Transcription
2002 evaluation test set (rt02), which we used as our de-
velopment set, as well as on the Rich Transcription 2003
Spring evaluation CTS test set (rt03). The rt02 set con-
sists of 6081 sentences (63804 words) and has three sub-
sets: Switchboard 1, Switchboard 2, Switchboard Cel-
lular. The rt03 set consists of 9050 sentences (76083
words) and has two subsets: Switchboard and Fisher.
We used the same training set as that used in Roark
et al (2004). The training set consists of 276726 tran-
scribed utterances (3047805 words), with an additional
20854 utterances (249774 words) as held out data. For
0 500 100037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticePerceptron, Feat=PN, N=1000CRF, ? = ?, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=PN, N=1000
Figure 3: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
each utterance, a weighted word-lattice was produced,
representing alternative transcriptions, from the ASR
system. From each word-lattice, the oracle best path
was extracted, which gives the best word-error rate from
among all of the hypotheses in the lattice. The oracle
word-error rate for the training set lattices was 12.2%.
We also performed trials with 1000-best lists for the same
training set, rather than lattices. The oracle score for the
1000-best lists was 16.7%.
To produce the word-lattices, each training utterance
was processed by the baseline ASR system. However,
these same utterances are what the acoustic and language
models are built from, which leads to better performance
on the training utterances than can be expected when the
ASR system processes unseen utterances. To somewhat
control for this, the training set was partitioned into 28
sets, and baseline Katz backoff trigram models were built
for each set by including only transcripts from the other
27 sets. Since language models are generally far more
prone to overtrain than standard acoustic models, this
goes a long way toward making the training conditions
similar to testing conditions.
There are three baselines against which we are com-
paring. The first is the ASR baseline, with no reweight-
ing from a discriminatively trained n-gram model. The
other two baselines are with perceptron-trained n-gram
model re-weighting, and were reported in Roark et al
(2004). The first of these is for a pruned-lattice trained
trigram model, which showed a reduction in word er-
ror rate (WER) of 1.3%, from 39.2% to 37.9% on rt02.
The second is for a 1000-best list trained trigram model,
which performed only marginally worse than the lattice-
trained perceptron, at 38.0% on rt02.
4.1 Perceptron feature set
We use the perceptron-trained models as the starting
point for our CRF algorithm: the feature set given to
the CRF algorithm is the feature set selected by the per-
ceptron algorithm; the feature weights are initialized to
those of the averaged perceptron. Figure 3 shows the
performance of our three baselines versus three trials of
0 500 1000 1500 2000 250037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=E,  ?=0.01CRF, ? = 0.5, Feat=E,  ?=0.9
Figure 4: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
the CRF algorithm. In the first two trials, the training
set consists of the pruned lattices, and the feature set
is from the perceptron algorithm trained on pruned lat-
tices. There were 1.4 million features in this feature set.
The first trial set the regularizer constant ? =?, so that
the algorithm was optimizing raw conditional likelihood.
The second trial is with the regularizer constant ? = 0.5,
which we found empirically to be a good parameteriza-
tion on the held-out set. As can be seen from these re-
sults, regularization is critical.
The third trial in this set uses the feature set from the
perceptron algorithm trained on 1000-best lists, and uses
CRF optimization on these on these same 1000-best lists.
There were 0.9 million features in this feature set. For
this trial, we also used ? = 0.5. As with the percep-
tron baselines, the n-best trial performs nearly identically
with the pruned lattices, here also resulting in 37.4%
WER. This may be useful for techniques that would be
more expensive to extend to lattices versus n-best lists
(e.g. models with unbounded dependencies).
These trials demonstrate that the CRF algorithm can
do a better job of estimating feature weights than the per-
ceptron algorithm for the same feature set. As mentioned
in the earlier section, feature selection is a by-product of
the perceptron algorithm, but the CRF algorithm is given
a set of features. The next two trials looked at selecting
feature sets other than those provided by the perceptron
algorithm.
4.2 Other feature sets
In order for the feature weights to be non-zero in this ap-
proach, they must be observed in the training set. The
number of unigram, bigram and trigram features with
non-zero observations in the training set lattices is 43.65
million, or roughly 30 times the size of the perceptron
feature set. Many of these features occur only rarely
with very low conditional probabilities, and hence cannot
meaningfully impact system performance. We pruned
this feature set to include all unigrams and bigrams, but
only those trigrams with an expected count of greater
than 0.01 in the training set. That is, to be included, a
Trial Iter rt02 rt03
ASR Baseline - 39.2 38.2
Perceptron, Lattice - 37.9 36.9
Perceptron, N-best - 38.0 37.2
CRF, Lattice, Percep Feats (1.4M) 769 37.4 36.5
CRF, N-best, Percep Feats (0.9M) 946 37.4 36.6
CRF, Lattice, ? = 0.01 (12M) 2714 37.6 36.5
CRF, Lattice, ? = 0.9 (1.5M) 1679 37.5 36.6
Table 1: Word-error rate results at convergence iteration for
various trials, on both Switchboard 2002 test set (rt02), which
was used as the dev set, and Switchboard 2003 test set (rt03).
trigram must occur in a set of paths, the sum of the con-
ditional probabilities of which must be greater than our
threshold ? = 0.01. This threshold resulted in a feature
set of roughly 12 million features, nearly 10 times the
size of the perceptron feature set. For better comparabil-
ity with that feature set, we set our thresholds higher, so
that trigrams were pruned if their expected count fell be-
low ? = 0.9, and bigrams were pruned if their expected
count fell below ? = 0.1. We were concerned that this
may leave out some of the features on the oracle paths, so
we added back in all bigram and trigram features that oc-
curred on oracle paths, giving a feature set of 1.5 million
features, roughly the same size as the perceptron feature
set.
Figure 4 shows the results for three CRF trials versus
our ASR baseline and the perceptron algorithm baseline
trained on lattices. First, the result using the perceptron
feature set provides us with a WER of 37.4%, as pre-
viously shown. The WER at convergence for the big
feature set (12 million features) is 37.6%; the WER at
convergence for the smaller feature set (1.5 million fea-
tures) is 37.5%. While both of these other feature sets
converge to performance close to that using the percep-
tron features, the number of iterations over the training
data that are required to reach that level of performance
are many more than for the perceptron-initialized feature
set.
Table 1 shows the word-error rate at the convergence
iteration for the various trials, on both rt02 and rt03. All
of the CRF trials are significantly better than the percep-
tron performance, using the Matched Pair Sentence Seg-
ment test for WER included with SCTK (NIST, 2000).
On rt02, the N-best and perceptron initialized CRF trials
were were significantly better than the lattice perceptron
at p < 0.001; the other two CRF trials were significantly
better than the lattice perceptron at p < 0.01. On rt03,
the N-best CRF trial was significantly better than the lat-
tice perceptron at p < 0.002; the other three CRF tri-
als were significantly better than the lattice perceptron at
p < 0.001.
Finally, we measured the time of a single iteration over
the training data on a single machine for the perceptron
algorithm, the CRF algorithm using the approximation to
the gradient of ?0, and the CRF algorithm using an exact
gradient of ?0. Table 2 shows these times in hours. Be-
cause of the frequent update of the weights in the model,
the perceptron algorithm is more expensive than the CRF
algorithm for a single iteration. Further, the CRF algo-
rithm is parallelizable, so that most of the work of an
CRF
Features Percep approx exact
Lattice, Percep Feats (1.4M) 7.10 1.69 3.61
N-best, Percep Feats (0.9M) 3.40 0.96 1.40
Lattice, ? = 0.01 (12M) - 2.24 4.75
Table 2: Time (in hours) for one iteration on a single Intel
Xeon 2.4Ghz processor with 4GB RAM.
iteration can be shared among multiple processors. Our
most common training setup for the CRF algorithm was
parallelized between 20 processors, using the approxi-
mation to the gradient. In that setup, using the 1.4M fea-
ture set, one iteration of the perceptron algorithm took
the same amount of real time as approximately 80 itera-
tions of CRF.
5 Conclusion
We have contrasted two approaches to discriminative
language model estimation on a difficult large vocabu-
lary task, showing that they can indeed scale effectively
to handle this size of a problem. Both algorithms have
their benefits. The perceptron algorithm selects a rela-
tively small subset of the total feature set, and requires
just a couple of passes over the training data. The CRF
algorithm does a better job of parameter estimation for
the same feature set, and is parallelizable, so that each
pass over the training set can require just a fraction of
the real time of the perceptron algorithm.
The best scenario from among those that we investi-
gated was a combination of both approaches, with the
output of the perceptron algorithm taken as the starting
point for CRF estimation.
As a final point, note that the methods we describe do
not replace an existing language model, but rather com-
plement it. The existing language model has the benefit
that it can be trained on a large amount of text that does
not have speech transcriptions. It has the disadvantage
of not being a discriminative model. The new language
model is trained on the speech transcriptions, meaning
that it has less training data, but that it has the advan-
tage of discriminative training ? and in particular, the ad-
vantage of being able to learn negative evidence in the
form of negative weights on n-grams which are rarely
or never seen in natural language text (e.g., ?the of?),
but are produced too frequently by the recognizer. The
methods we describe combines the two language models,
allowing them to complement each other.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized
algorithms for constructing language models. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F.
Smith. 2002. Petsc users manual. Technical Report ANL-95/11-
Revision 2.1.2, Argonne National Laboratory.
Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam.
2003. Improving language models by learning from speech recog-
nition errors in a reading tutor that listens. In Proceedings of the
Second International Conference on Applied Artificial Intelligence,
Fort Panhala, Kolhapur, India.
Steven J. Benson and Jorge J. More?. 2002. A limited memory vari-
able metric method for bound constrained minimization. Preprint
ANL/ACSP909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J. More?, and Jason
Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM-
242-Revision 1.4, Argonne National Laboratory.
Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative
training on language model. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing (ICSLP), Bei-
jing, China.
Michael Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algo-
rithms. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods. In Harry
Bunt, John Carroll, and Giorgio Satta, editors, New Developments
in Parsing Technology. Kluwer.
Yoav Freund and Robert Schapire. 1999. Large margin classification
using the perceptron algorithm. Machine Learning, 3(37):277?296.
Frederick Jelinek. 1995. Acoustic sensitive language modeling. Tech-
nical report, Center for Language and Speech Processing, Johns
Hopkins University, Baltimore, MD.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan
Riezler. 1999. Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 535?541.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques
for exploiting syntactic, semantic and collocational dependencies in
language modeling. Computer Speech and Language, 14(4):355?
372.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-
Hui Lee. 2002. Discriminative training of language models for
speech recognition. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), Orlando,
Florida.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Con-
ditional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289, Williams
College, Williamstown, MA, USA.
Robert Malouf. 2002. A comparison of algorithms for maximum en-
tropy parameter estimation. In Proc. CoNLL, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction and
web-enhanced lexicons. In Proc. CoNLL.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition. Computer
Speech and Language, 16(1):69?88.
NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c.
Available at http://www.nist.gov/speech/tools.
David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003.
Table extraction using conditional random fields. In Proc. ACM SI-
GIR.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A max-
imum entropy model for parsing. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (ICSLP), pages
803?806.
Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective
language modeling for large vocabulary ASR with the perceptron al-
gorithm. In Proceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages 749?752.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL, Edmonton, Canada.
A. Stolcke and M. Weintraub. 1998. Discriminitive language model-
ing. In Proceedings of the 9th Hub-5 Conversational Speech Recog-
nition Workshop.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and
J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech
transcription system. In Proceedings of the NIST Speech Transcrip-
tion Workshop.
Hanna Wallach. 2002. Efficient training of conditional random fields.
Master?s thesis, University of Edinburgh.
P.C. Woodland and D. Povey. 2000. Large scale discriminative training
for speech recognition. In Proc. ISCA ITRW ASR2000, pages 7?16.
Proceedings of the 43rd Annual Meeting of the ACL, pages 173?180,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking
Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{mj|ec}@cs.brown.edu
Abstract
Discriminative reranking is one method
for constructing high-performance statis-
tical parsers (Collins, 2000). A discrim-
inative reranker requires a source of can-
didate parses for each sentence. This pa-
per describes a simple yet novel method
for constructing sets of 50-best parses
based on a coarse-to-fine generative parser
(Charniak, 2000). This method gener-
ates 50-best lists that are of substantially
higher quality than previously obtainable.
We used these parses as the input to a
MaxEnt reranker (Johnson et al, 1999;
Riezler et al, 2002) that selects the best
parse from the set of parses for each sen-
tence, obtaining an f-score of 91.0% on
sentences of length 100 or less.
1 Introduction
We describe a reranking parser which uses a reg-
ularized MaxEnt reranker to select the best parse
from the 50-best parses returned by a generative
parsing model. The 50-best parser is a probabilistic
parser that on its own produces high quality parses;
the maximum probability parse trees (according to
the parser?s model) have an f -score of 0.897 on
section 23 of the Penn Treebank (Charniak, 2000),
which is still state-of-the-art. However, the 50 best
(i.e., the 50 highest probability) parses of a sentence
often contain considerably better parses (in terms of
f -score); this paper describes a 50-best parsing al-
gorithm with an oracle f -score of 96.8 on the same
data.
The reranker attempts to select the best parse for
a sentence from the 50-best list of possible parses
for the sentence. Because the reranker only has
to consider a relatively small number of parses per
sentences, it is not necessary to use dynamic pro-
gramming, which permits the features to be essen-
tially arbitrary functions of the parse trees. While
our reranker does not achieve anything like the ora-
cle f -score, the parses it selects do have an f -score
of 91.0, which is considerably better than the maxi-
mum probability parses of the n-best parser.
In more detail, for each string s the n-best parsing
algorithm described in section 2 returns the n high-
est probability parses Y(s) = {y1(s), . . . , yn(s)}
together with the probability p(y) of each parse y ac-
cording to the parser?s probability model. The num-
ber n of parses was set to 50 for the experiments
described here, but some simple sentences actually
received fewer than 50 parses (so n is actually a
function of s). Each yield or terminal string in the
training, development and test data sets is mapped
to such an n-best list of parse/probability pairs; the
cross-validation scheme described in Collins (2000)
was used to avoid training the n-best parser on the
sentence it was being used to parse.
A feature extractor, described in section 3, is a
vector of m functions f = (f1, . . . , fm), where each
fj maps a parse y to a real number fj(y), which
is the value of the jth feature on y. So a feature
extractor maps each y to a vector of feature values
f(y) = (f1(y), . . . , fm(y)).
Our reranking parser associates a parse with a
173
score v?(y), which is a linear function of the feature
values f(y). That is, each feature fj is associated
with a weight ?j , and the feature values and weights
define the score v?(y) of each parse y as follows:
v?(y) = ? ? f(y) =
m
?
j=1
?jfj(y).
Given a string s, the reranking parser?s output y?(s)
on string s is the highest scoring parse in the n-best
parses Y(s) for s, i.e.,
y?(s) = arg max
y?Y(s)
v?(y).
The feature weight vector ? is estimated from the
labelled training corpus as described in section 4.
Because we use labelled training data we know the
correct parse y?(s) for each sentence s in the training
data. The correct parse y?(s) is not always a mem-
ber of the n-best parser?s output Y(s), but we can
identify the parses Y+(s) in Y(s) with the highest
f -scores. Informally, the estimation procedure finds
a weight vector ? that maximizes the score v?(y) of
the parses y ? Y+(s) relative to the scores of the
other parses in Y(s), for each s in the training data.
2 Recovering the n-best parses using
coarse-to-fine parsing
The major difficulty in n-best parsing, compared to
1-best parsing, is dynamic programming. For exam-
ple, n-best parsing is straight-forward in best-first
search or beam search approaches that do not use
dynamic programming: to generate more than one
parse, one simply allows the search mechanism to
create successive versions to one?s heart?s content.
A good example of this is the Roark parser
(Roark, 2001) which works left-to right through the
sentence, and abjures dynamic programming in fa-
vor of a beam search, keeping some large number of
possibilities to extend by adding the next word, and
then re-pruning. At the end one has a beam-width?s
number of best parses (Roark, 2001).
The Collins parser (Collins, 1997) does use dy-
namic programming in its search. That is, whenever
a constituent with the same history is generated a
second time, it is discarded if its probability is lower
than the original version. If the opposite is true, then
the original is discarded. This is fine if one only
wants the first-best, but obviously it does not directly
enumerate the n-best parses.
However, Collins (Collins, 2000; Collins
and Koo, in submission) has created an n-
best version of his parser by turning off dy-
namic programming (see the user?s guide to
Bikel?s re-implementation of Collins? parser,
http://www.cis.upenn.edu/ dbikel/software.html#stat-
parser). As with Roark?s parser, it is necessary to
add a beam-width constraint to make the search
tractable. With a beam width of 1000 the parser
returns something like a 50-best list (Collins,
personal communication), but the actual number of
parses returned for each sentences varies. However,
turning off dynamic programming results in a loss in
efficiency. Indeed, Collins?s n-best list of parses for
section 24 of the Penn tree-bank has some sentences
with only a single parse, because the n-best parser
could not find any parses.
Now there are two known ways to produce n-best
parses while retaining the use of dynamic program-
ming: the obvious way and the clever way.
The clever way is based upon an algorithm devel-
oped by Schwartz and Chow (1990). Recall the key
insight in the Viterbi algorithm: in the optimal parse
the parsing decisions at each of the choice points that
determine a parse must be optimal, since otherwise
one could find a better parse. This insight extends
to n-best parsing as follows. Consider the second-
best parse: if it is to differ from the best parse, then
at least one of its parsing decisions must be subop-
timal. In fact, all but one of the parsing decisions
in second-best parse must be optimal, and the one
suboptimal decision must be the second-best choice
at that choice point. Further, the nth-best parse can
only involve at most n suboptimal parsing decisions,
and all but one of these must be involved in one of
the second through the n?1th-best parses. Thus the
basic idea behind this approach to n-best parsing is
to first find the best parse, then find the second-best
parse, then the third-best, and so on. The algorithm
was originally described for hidden Markov models.
Since this first draft of this paper we have be-
come aware of two PCFG implementations of this
algorithm (Jimenez and Marzal, 2000; Huang and
Chang, 2005). The first was tried on relatively small
grammars, while the second was implemented on
top of the Bikel re-implementation of the Collins
174
parser (Bikel, 2004) and achieved oracle results for
50-best parses similar to those we report below.
Here, however, we describe how to find n-best
parses in a more straight-forward fashion. Rather
than storing a single best parse of each edge, one
stores n of them. That is, when using dynamic pro-
gramming, rather than throwing away a candidate if
it scores less than the best, one keeps it if it is one
of the top n analyses for this edge discovered so far.
This is really very straight-forward. The problem
is space. Dynamic programming parsing algorithms
for PCFGs require O(m2) dynamic programming
states, where m is the length of the sentence, so an
n-best parsing algorithm requires O(nm2). How-
ever things get much worse when the grammar is bi-
lexicalized. As shown by Eisner (Eisner and Satta,
1999) the dynamic programming algorithms for bi-
lexicalized PCFGs require O(m3) states, so a n-best
parser would require O(nm3) states. Things be-
come worse still in a parser like the one described in
Charniak (2000) because it conditions on (and hence
splits the dynamic programming states according to)
features of the grandparent node in addition to the
parent, thus multiplying the number of possible dy-
namic programming states even more. Thus nobody
has implemented this version.
There is, however, one particular feature of the
Charniak parser that mitigates the space problem: it
is a ?coarse-to-fine? parser. By ?coarse-to-fine? we
mean that it first produces a crude version of the
parse using coarse-grained dynamic programming
states, and then builds fine-grained analyses by split-
ting the most promising of coarse-grained states.
A prime example of this idea is from Goodman
(1997), who describes a method for producing a sim-
ple but crude approximate grammar of a standard
context-free grammar. He parses a sentence using
the approximate grammar, and the results are used
to constrain the search for a parse with the full CFG.
He finds that total parsing time is greatly reduced.
A somewhat different take on this paradigm is
seen in the parser we use in this paper. Here the
parser first creates a parse forest based upon a much
less complex version of the complete grammar. In
particular, it only looks at standard CFG features,
the parent and neighbor labels. Because this gram-
mar encodes relatively little state information, its dy-
namic programming states are relatively coarse and
hence there are comparatively few of them, so it can
be efficiently parsed using a standard dynamic pro-
gramming bottom-up CFG parser. However, pre-
cisely because this first stage uses a grammar that
ignores many important contextual features, the best
parse it finds will not, in general, be the best parse
according to the finer-grained second-stage gram-
mar, so clearly we do not want to perform best-first
parsing with this grammar. Instead, the output of
the first stage is a polynomial-sized packed parse
forest which records the left and right string posi-
tions for each local tree in the parses generated by
this grammar. The edges in the packed parse for-
est are then pruned, to focus attention on the coarse-
grained states that are likely to correspond to high-
probability fine-grained states. The edges are then
pruned according to their marginal probability con-
ditioned on the string s being parsed as follows:
p(nij,k | s) =
?(nij,k)?(nij,k)
p(s) (1)
Here nij,k is a constituent of type i spanning the
words from j to k, ?(nij,k) is the outside probability
of this constituent, and ?(nij,k) is its inside proba-
bility. From parse forest both ? and ? can be com-
puted in time proportional to the size of the compact
forest. The parser then removes all constituents nij,k
whose probability falls below some preset threshold.
In the version of this parser available on the web, this
threshold is on the order of 10?4.
The unpruned edges are then exhaustively eval-
uated according to the fine-grained probabilistic
model; in effect, each coarse-grained dynamic pro-
gramming state is split into one or more fine-grained
dynamic programming states. As noted above, the
fine-grained model conditions on information that is
not available in the coarse-grained model. This in-
cludes the lexical head of one?s parents, the part of
speech of this head, the parent?s and grandparent?s
category labels, etc. The fine-grained states inves-
tigated by the parser are constrained to be refine-
ments of the coarse-grained states, which drastically
reduces the number of fine-grained states that need
to be investigated.
It is certainly possible to do dynamic program-
ming parsing directly with the fine-grained gram-
mar, but precisely because the fine-grained grammar
175
conditions on a wide variety of non-local contex-
tual information there would be a very large number
of different dynamic programming states, so direct
dynamic programming parsing with the fine-grained
grammar would be very expensive in terms of time
and memory.
As the second stage parse evaluates all the re-
maining constituents in all of the contexts in which
they appear (e.g., what are the possible grand-parent
labels) it keeps track of the most probable expansion
of the constituent in that context, and at the end is
able to start at the root and piece together the overall
best parse.
Now comes the easy part. To create a 50-best
parser we simply change the fine-grained version of
1-best algorithm in accordance with the ?obvious?
scheme outlined earlier in this section. The first,
coarse-grained, pass is not changed, but the second,
fine-grained, pass keeps the n-best possibilities at
each dynamic programming state, rather than keep-
ing just first best. When combining two constituents
to form a larger constituent, we keep the best 50 of
the 2500 possibilities they offer. Naturally, if we
keep each 50-best list sorted, we do nothing like
2500 operations.
The experimental question is whether, in practice,
the coarse-to-fine architecture keeps the number of
dynamic programming states sufficiently low that
space considerations do not defeat us.
The answer seems to be yes. We ran the algorithm
on section 24 of the Penn WSJ tree-bank using the
default pruning settings mentioned above. Table 1
shows how the number of fine-grained dynamic pro-
gramming states increases as a function of sentence
length for the sentences in section 24 of the Tree-
bank. There are no sentences of length greater than
69 in this section. Columns two to four show the
number of sentences in each bucket, their average
length, and the average number of fine-grained dy-
namic programming structures per sentence. The fi-
nal column gives the value of the function 100?L1.5
where L is the average length of sentences in the
bucket. Except for bucket 6, which is abnormally
low, it seems that this add-hoc function tracks the
number of structures quite well. Thus the number of
dynamic programming states does not grow as L2,
much less as L3.
To put the number of these structures per sen-
Len Num Av sen Av strs 100 ? L1.5
sents length per sent
0?9 225 6.04 1167 1484
10?19 725 15.0 4246 5808
20?29 795 24.2 9357 11974
30?39 465 33.8 15893 19654
40?49 162 43.2 21015 28440
50?59 35 52.8 30670 38366
60?69 9 62.8 23405 49740
Table 1: Number of structures created as a function
of sentence length
n 1 2 10 25 50
f -score 0.897 0.914 0.948 0.960 0.968
Table 2: Oracle f -score as a function of number n
of n-best parses
tence in perspective, consider the size of such struc-
tures. Each one must contain a probability, the non-
terminal label of the structure, and a vector of point-
ers to it?s children (an average parent has slightly
more than two children). If one were concerned
about every byte this could be made quite small. In
our implementation probably the biggest factor is
the STL overhead on vectors. If we figure we are
using, say, 25 bytes per structure, the total space re-
quired is only 1.25Mb even for 50,000 dynamic pro-
gramming states, so it is clearly not worth worrying
about the memory required.
The resulting n-bests are quite good, as shown in
Table 2. (The results are for all sentences of sec-
tion 23 of the WSJ tree-bank of length ? 100.) From
the 1-best result we see that the base accuracy of the
parser is 89.7%.1 2-best and 10-best show dramatic
oracle-rate improvements. After that things start to
slow down, and we achieve an oracle rate of 0.968
at 50-best. To put this in perspective, Roark (Roark,
2001) reports oracle results of 0.941 (with the same
experimental setup) using his parser to return a vari-
able number of parses. For the case cited his parser
returns, on average, 70 parses per sentence.
Finally, we note that 50-best parsing is only a fac-
1Charniak in (Charniak, 2000) cites an accuracy of 89.5%.
Fixing a few very small bugs discovered by users of the parser
accounts for the difference.
176
tor of two or three slower than 1-best.
3 Features for reranking parses
This section describes how each parse y is mapped
to a feature vector f(y) = (f1(y), . . . , fm(y)). Each
feature fj is a function that maps a parse to a real
number. The first feature f1(y) = log p(y) is the
logarithm of the parse probability p according to
the n-best parser model. The other features are
integer valued; informally, each feature is associ-
ated with a particular configuration, and the feature?s
value fj(y) is the number of times that the config-
uration that fj indicates. For example, the feature
feat pizza(y) counts the number of times that a phrase
in y headed by eat has a complement phrase headed
by pizza.
Features belong to feature schema, which are ab-
stract schema from which specific features are in-
stantiated. For example, the feature feat pizza is an
instance of the ?Heads? schema. Feature schema are
often parameterized in various ways. For example,
the ?Heads? schema is parameterized by the type of
heads that the feature schema identifies. Following
Grimshaw (1997), we associate each phrase with a
lexical head and a function head. For example, the
lexical head of an NP is a noun while the functional
head of an NP is a determiner, and the lexical head
of a VP is a main verb while the functional head of
VP is an auxiliary verb.
We experimented with various kinds of feature
selection, and found that a simple count threshold
performs as well as any of the methods we tried.
Specifically, we ignored all features that did not vary
on the parses of at least t sentences, where t is the
count threshold. In the experiments described below
t = 5, though we also experimented with t = 2.
The rest of this section outlines the feature
schemata used in the experiments below. These fea-
ture schemata used here were developed using the
n-best parses provided to us by Michael Collins
approximately a year before the n-best parser de-
scribed here was developed. We used the division
into preliminary training and preliminary develop-
ment data sets described in Collins (2000) while
experimenting with feature schemata; i.e., the first
36,000 sentences of sections 2?20 were used as pre-
liminary training data, and the remaining sentences
of sections 20 and 21 were used as preliminary de-
velopment data. It is worth noting that develop-
ing feature schemata is much more of an art than
a science, as adding or deleting a single schema
usually does not have a significant effect on perfor-
mance, yet the overall impact of many well-chosen
schemata can be dramatic.
Using the 50-best parser output described here,
there are 1,148,697 features that meet the count
threshold of at least 5 on the main training data
(i.e., Penn treebank sections 2?21). We list each
feature schema?s name, followed by the number of
features in that schema with a count of at least 5, to-
gether with a brief description of the instances of the
schema and the schema?s parameters.
CoPar (10) The instances of this schema indicate
conjunct parallelism at various different depths.
For example, conjuncts which have the same
label are parallel at depth 0, conjuncts with the
same label and whose children have the same
label are parallel at depth 1, etc.
CoLenPar (22) The instances of this schema indi-
cate the binned difference in length (in terms
of number of preterminals dominated) in adja-
cent conjuncts in the same coordinated struc-
tures, conjoined with a boolean flag that indi-
cates whether the pair is final in the coordinated
phrase.
RightBranch (2) This schema enables the reranker
to prefer right-branching trees. One instance of
this schema returns the number of nonterminal
nodes that lie on the path from the root node
to the right-most non-punctuation preterminal
node, and the other instance of this schema
counts the number of the other nonterminal
nodes in the parse tree.
Heavy (1049) This schema classifies nodes by their
category, their binned length (i.e., the number
of preterminals they dominate), whether they
are at the end of the sentence and whether they
are followed by punctuation.
Neighbours (38,245) This schema classifies nodes
by their category, their binned length, and the
part of speech categories of the `1 preterminals
to the node?s left and the `2 preterminals to the
177
node?s right. `1 and `2 are parameters of this
schema; here `1 = 1 or `1 = 2 and `2 = 1.
Rule (271,655) The instances of this schema are
local trees, annotated with varying amounts
of contextual information controlled by the
schema?s parameters. This schema was in-
spired by a similar schema in Collins and Koo
(in submission). The parameters to this schema
control whether nodes are annotated with their
preterminal heads, their terminal heads and
their ancestors? categories. An additional pa-
rameter controls whether the feature is special-
ized to embedded or non-embedded clauses,
which roughly corresponds to Emonds? ?non-
root? and ?root? contexts (Emonds, 1976).
NGram (54,567) The instances of this schema are
`-tuples of adjacent children nodes of the same
parent. This schema was inspired by a simi-
lar schema in Collins and Koo (in submission).
This schema has the same parameters as the
Rule schema, plus the length ` of the tuples of
children (` = 2 here).
Heads (208,599) The instances of this schema are
tuples of head-to-head dependencies, as men-
tioned above. The category of the node that
is the least common ancestor of the head and
the dependent is included in the instance (this
provides a crude distinction between different
classes of arguments). The parameters of this
schema are whether the heads involved are lex-
ical or functional heads, the number of heads
in an instance, and whether the lexical item or
just the head?s part of speech are included in the
instance.
LexFunHeads (2,299) The instances of this feature
are the pairs of parts of speech of the lexical
head and the functional head of nodes in parse
trees.
WProj (158,771) The instances of this schema are
preterminals together with the categories of ` of
their closest maximal projection ancestors. The
parameters of this schema control the number `
of maximal projections, and whether the preter-
minals and the ancestors are lexicalized.
Word (49,097) The instances of this schema are
lexical items together with the categories of `
of their immediate ancestor nodes, where ` is
a schema parameter (` = 2 or ` = 3 here).
This feature was inspired by a similar feature
in Klein and Manning (2003).
HeadTree (72,171) The instances of this schema
are tree fragments consisting of the local trees
consisting of the projections of a preterminal
node and the siblings of such projections. This
schema is parameterized by the head type (lex-
ical or functional) used to determine the pro-
jections of a preterminal, and whether the head
preterminal is lexicalized.
NGramTree (291,909) The instances of this
schema are subtrees rooted in the least com-
mon ancestor of ` contiguous preterminal
nodes. This schema is parameterized by the
number ` of contiguous preterminals (` = 2 or
` = 3 here) and whether these preterminals are
lexicalized.
4 Estimating feature weights
This section explains how we estimate the feature
weights ? = (?1, . . . , ?m) for the feature functions
f = (f1, . . . , fm). We use a MaxEnt estimator to
find the feature weights ??, where L is the loss func-
tion and R is a regularization penalty term:
?? = argmin
?
LD(?) + R(?).
The training data D = (s1, . . . , sn?) is a se-
quence of sentences and their correct parses
y?(s1), . . . , y?(sn). We used the 20-fold cross-
validation technique described in Collins (2000)
to compute the n-best parses Y(s) for each sen-
tence s in D. In general the correct parse y?(s)
is not a member of Y(s), so instead we train the
reranker to identify one of the best parses Y+(s) =
argmaxy?Y(s) Fy?(s)(y) in the n-best parser?s out-
put, where Fy?(y) is the Parseval f -score of y eval-
uated with respect to y?.
Because there may not be a unique best parse for
each sentence (i.e., |Y+(s)| > 1 for some sentences
s) we used the variant of MaxEnt described in Rie-
zler et al (2002) for partially labelled training data.
178
Recall the standard MaxEnt conditional probability
model for a parse y ? Y:
P?(y|Y) =
exp v?(y)
?
y??Y exp v?(y?)
,where
v?(y) = ? ? f(y) =
m
?
j=1
?jfj(y).
The loss function LD proposed in Riezler et al
(2002) is just the negative log conditional likelihood
of the best parses Y+(s) relative to the n-best parser
output Y(s):
LD(?) = ?
n?
?
i=1
log P?(Y+(si)|Y(si)),where
P?(Y+|Y) =
?
y?Y+
P?(y|Y)
The partial derivatives of this loss function, which
are required by the numerical estimation procedure,
are:
?LD
?j
=
n?
?
i=1
E?[fj |Y(si)] ? E?[fj |Y+(si)]
E?[f |Y] =
?
y?Y
f(y)P?(y|Y)
In the experiments reported here, we used a Gaus-
sian or quadratic regularizer R(w) = c?mj=1 w2j ,
where c is an adjustable parameter that controls
the amount of regularization, chosen to optimize
the reranker?s f -score on the development set (sec-
tion 24 of the treebank).
We used the Limited Memory Variable Metric op-
timization algorithm from the PETSc/TAO optimiza-
tion toolkit (Benson et al, 2004) to find the optimal
feature weights ?? because this method seems sub-
stantially faster than comparable methods (Malouf,
2002). The PETSc/TAO toolkit provides a variety of
other optimization algorithms and flags for control-
ling convergence, but preliminary experiments on
the Collins? trees with different algorithms and early
stopping did not show any performance improve-
ments, so we used the default PETSc/TAO setting for
our experiments here.
5 Experimental results
We evaluated the performance of our reranking
parser using the standard PARSEVAL metrics. We
n-best trees f -score
New 0.9102
Collins 0.9037
Table 3: Results on new n-best trees and Collins n-
best trees, with weights estimated from sections 2?
21 and the regularizer constant c adjusted for op-
timal f -score on section 24 and evaluated on sen-
tences of length less than 100 in section 23.
trained the n-best parser on sections 2?21 of the
Penn Treebank, and used section 24 as development
data to tune the mixing parameters of the smooth-
ing model. Similarly, we trained the feature weights
? with the MaxEnt reranker on sections 2?21, and
adjusted the regularizer constant c to maximize the
f -score on section 24 of the treebank. We did this
both on the trees supplied to us by Michael Collins,
and on the output of the n-best parser described in
this paper. The results are presented in Table 3. The
n-best parser?s most probable parses are already of
state-of-the-art quality, but the reranker further im-
proves the f -score.
6 Conclusion
This paper has described a dynamic programming
n-best parsing algorithm that utilizes a heuristic
coarse-to-fine refinement of parses. Because the
coarse-to-fine approach prunes the set of possible
parse edges beforehand, a simple approach which
enumerates the n-best analyses of each parse edge
is not only practical but quite efficient.
We use the 50-best parses produced by this algo-
rithm as input to a MaxEnt discriminative reranker.
The reranker selects the best parse from this set of
parses using a wide variety of features. The sys-
tem we described here has an f -score of 0.91 when
trained and tested using the standard PARSEVAL
framework.
This result is only slightly higher than the highest
reported result for this test-set, Bod?s (.907) (Bod,
2003). More to the point, however, is that the sys-
tem we describe is reasonably efficient so it can
be used for the kind of routine parsing currently
being handled by the Charniak or Collins parsers.
A 91.0 f-score represents a 13% reduction in f-
179
measure error over the best of these parsers.2 Both
the 50-best parser, and the reranking parser can be
found at ftp://ftp.cs.brown.edu/pub/nlparser/, named
parser and reranker respectively.
Acknowledgements We would like to thanks
Michael Collins for the use of his data and many
helpful comments, and Liang Huang for providing
an early draft of his paper and very useful comments
on our paper. Finally thanks to the National Science
Foundation for its support (NSF IIS-0112432, NSF
9721276, and NSF DMS-0074276).
References
Steve Benson, Lois Curfman McInnes, Jorge J. Mor, and
Jason Sarich. 2004. Tao users manual. Technical Re-
port ANL/MCS-TM-242-Revision 1.6, Argonne Na-
tional Laboratory.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Rens Bod. 2003. An efficient implementation of an new
dop model. In Proceedings of the European Chapter
of the Association for Computational Linguists.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
Michael Collins and Terry Koo. in submission. Discrim-
inative reranking for natural language parsing. Tech-
nical report, Computer Science and Artificial Intelli-
gence Laboratory, Massachusetts Institute of Technol-
ogy.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182, Stanford, California.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th Annual
2This probably underestimates the actual improvement.
There are no currently accepted figures for inter-annotater
agreement on Penn WSJ, but it is no doubt well short of 100%.
If we take 97% as a reasonable estimate of the the upper bound
on tree-bank accuracy, we are instead talking about an 18% er-
ror reduction.
Meeting of the Association for Computational Linguis-
tics, pages 457?464.
Joseph Emonds. 1976. A Transformational Approach to
English Syntax: Root, Structure-Preserving and Local
Transformations. Academic Press, New York, NY.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 1997).
Jane Grimshaw. 1997. Projection, heads, and optimality.
Linguistic Inquiry, 28(3):373?422.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic ?unification-based? grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535?541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the Sixth Conference on Natural Language
Learning (CoNLL-2002), pages 49?55.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?278. Morgan Kaufmann.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
R. Schwartz and Y.L. Chow. 1990. The n-best algo-
rithm: An efficient and exact procedure for finding
the n most likely sentence hypotheses. In Proceed-
ings of the IEEE International Conference on Acous-
tic, Speech, and Signal, Processing, pages 81?84.
180
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 337?344,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reranking and Self-Training for Parser Adaptation
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Statistical parsers trained and tested on the
Penn Wall Street Journal (WSJ) treebank
have shown vast improvements over the
last 10 years. Much of this improvement,
however, is based upon an ever-increasing
number of features to be trained on (typi-
cally) the WSJ treebank data. This has led
to concern that such parsers may be too
finely tuned to this corpus at the expense
of portability to other genres. Such wor-
ries have merit. The standard ?Charniak
parser? checks in at a labeled precision-
recall f -measure of 89.7% on the Penn
WSJ test set, but only 82.9% on the test set
from the Brown treebank corpus.
This paper should allay these fears. In par-
ticular, we show that the reranking parser
described in Charniak and Johnson (2005)
improves performance of the parser on
Brown to 85.2%. Furthermore, use of the
self-training techniques described in (Mc-
Closky et al, 2006) raise this to 87.8%
(an error reduction of 28%) again with-
out any use of labeled Brown data. This
is remarkable since training the parser and
reranker on labeled Brown data achieves
only 88.4%.
1 Introduction
Modern statistical parsers require treebanks to
train their parameters, but their performance de-
clines when one parses genres more distant from
the training data?s domain. Furthermore, the tree-
banks required to train said parsers are expensive
and difficult to produce.
Naturally, one of the goals of statistical parsing
is to produce a broad-coverage parser which is rel-
atively insensitive to textual domain. But the lack
of corpora has led to a situation where much of
the current work on parsing is performed on a sin-
gle domain using training data from that domain
? the Wall Street Journal (WSJ) section of the
Penn Treebank (Marcus et al, 1993). Given the
aforementioned costs, it is unlikely that many sig-
nificant treebanks will be created for new genres.
Thus, parser adaptation attempts to leverage ex-
isting labeled data from one domain and create a
parser capable of parsing a different domain.
Unfortunately, the state of the art in parser
portability (i.e. using a parser trained on one do-
main to parse a different domain) is not good. The
?Charniak parser? has a labeled precision-recall
f -measure of 89.7% on WSJ but a lowly 82.9%
on the test set from the Brown corpus treebank.
Furthermore, the treebanked Brown data is mostly
general non-fiction and much closer to WSJ than,
e.g., medical corpora would be. Thus, most work
on parser adaptation resorts to using some labeled
in-domain data to fortify the larger quantity of out-
of-domain data.
In this paper, we present some encouraging re-
sults on parser adaptation without any in-domain
data. (Though we also present results with in-
domain data as a reference point.) In particular we
note the effects of two comparatively recent tech-
niques for parser improvement.
The first of these, parse-reranking (Collins,
2000; Charniak and Johnson, 2005) starts with a
?standard? generative parser, but uses it to gener-
ate the n-best parses rather than a single parse.
Then a reranking phase uses more detailed fea-
tures, features which would (mostly) be impossi-
ble to incorporate in the initial phase, to reorder
337
the list and pick a possibly different best parse.
At first blush one might think that gathering even
more fine-grained features from a WSJ treebank
would not help adaptation. However, we find that
reranking improves the parsers performance from
82.9% to 85.2%.
The second technique is self-training ? pars-
ing unlabeled data and adding it to the training
corpus. Recent work, (McClosky et al, 2006),
has shown that adding many millions of words
of machine parsed and reranked LA Times arti-
cles does, in fact, improve performance of the
parser on the closely related WSJ data. Here we
show that it also helps the father-afield Brown
data. Adding it improves performance yet-again,
this time from 85.2% to 87.8%, for a net error re-
duction of 28%. It is interesting to compare this to
our results for a completely Brown trained system
(i.e. one in which the first-phase parser is trained
on just Brown training data, and the second-phase
reranker is trained on Brown 50-best lists). This
system performs at a 88.4% level ? only slightly
higher than that achieved by our system with only
WSJ data.
2 Related Work
Work in parser adaptation is premised on the as-
sumption that one wants a single parser that can
handle a wide variety of domains. While this is the
goal of the majority of parsing researchers, it is not
quite universal. Sekine (1997) observes that for
parsing a specific domain, data from that domain
is most beneficial, followed by data from the same
class, data from a different class, and data from
a different domain. He also notes that different
domains have very different structures by looking
at frequent grammar productions. For these rea-
sons he takes the position that we should, instead,
simply create treebanks for a large number of do-
mains. While this is a coherent position, it is far
from the majority view.
There are many different approaches to parser
adaptation. Steedman et al (2003) apply co-
training to parser adaptation and find that co-
training can work across domains. The need to
parse biomedical literature inspires (Clegg and
Shepherd, 2005; Lease and Charniak, 2005).
Clegg and Shepherd (2005) provide an extensive
side-by-side performance analysis of several mod-
ern statistical parsers when faced with such data.
They find that techniques which combine differ-
Training Testing f -measureGildea Bacchiani
WSJ WSJ 86.4 87.0
WSJ Brown 80.6 81.1
Brown Brown 84.0 84.7
WSJ+Brown Brown 84.3 85.6
Table 1: Gildea and Bacchiani results on WSJ and
Brown test corpora using different WSJ and Brown
training sets. Gildea evaluates on sentences of
length ? 40, Bacchiani on all sentences.
ent parsers such as voting schemes and parse se-
lection can improve performance on biomedical
data. Lease and Charniak (2005) use the Charniak
parser for biomedical data and find that the use of
out-of-domain trees and in-domain vocabulary in-
formation can considerably improve performance.
However, the work which is most directly com-
parable to ours is that of (Ratnaparkhi, 1999; Hwa,
1999; Gildea, 2001; Bacchiani et al, 2006). All
of these papers look at what happens to mod-
ern WSJ-trained statistical parsers (Ratnaparkhi?s,
Collins?, Gildea?s and Roark?s, respectively) as
training data varies in size or usefulness (because
we are testing on something other than WSJ). We
concentrate particularly on the work of (Gildea,
2001; Bacchiani et al, 2006) as they provide re-
sults which are directly comparable to those pre-
sented in this paper.
Looking at Table 1, the first line shows us
the standard training and testing on WSJ ? both
parsers perform in the 86-87% range. The next
line shows what happens when parsing Brown us-
ing a WSJ-trained parser. As with the Charniak
parser, both parsers take an approximately 6% hit.
It is at this point that our work deviates from
these two papers. Lacking alternatives, both
(Gildea, 2001) and (Bacchiani et al, 2006) give
up on adapting a pure WSJ trained system, instead
looking at the issue of how much of an improve-
ment one gets over a pure Brown system by adding
WSJ data (as seen in the last two lines of Table 1).
Both systems use a ?model-merging? (Bacchiani
et al, 2006) approach. The different corpora are,
in effect, concatenated together. However, (Bac-
chiani et al, 2006) achieve a larger gain by weight-
ing the in-domain (Brown) data more heavily than
the out-of-domain WSJ data. One can imagine, for
instance, five copies of the Brown data concate-
nated with just one copy of WSJ data.
338
3 Corpora
We primarily use three corpora in this paper. Self-
training requires labeled and unlabeled data. We
assume that these sets of data must be in similar
domains (e.g. news articles) though the effective-
ness of self-training across domains is currently an
open question. Thus, we have labeled (WSJ) and
unlabeled (NANC) out-of-domain data and labeled
in-domain data (BROWN). Unfortunately, lacking
a corresponding corpus to NANC for BROWN, we
cannot perform the opposite scenario and adapt
BROWN to WSJ.
3.1 Brown
The BROWN corpus (Francis and Kuc?era, 1979)
consists of many different genres of text, intended
to approximate a ?balanced? corpus. While the
full corpus consists of fiction and nonfiction do-
mains, the sections that have been annotated in
Treebank II bracketing are primarily those con-
taining fiction. Examples of the sections annotated
include science fiction, humor, romance, mystery,
adventure, and ?popular lore.? We use the same
divisions as Bacchiani et al (2006), who base
their divisions on Gildea (2001). Each division of
the corpus consists of sentences from all available
genres. The training division consists of approx-
imately 80% of the data, while held-out develop-
ment and testing divisions each make up 10% of
the data. The treebanked sections contain approx-
imately 25,000 sentences (458,000 words).
3.2 Wall Street Journal
Our out-of-domain data is the Wall Street Journal
(WSJ) portion of the Penn Treebank (Marcus et al,
1993) which consists of about 40,000 sentences
(one million words) annotated with syntactic in-
formation. We use the standard divisions: Sec-
tions 2 through 21 are used for training, section 24
for held-out development, and section 23 for final
testing.
3.3 North American News Corpus
In addition to labeled news data, we make use
of a large quantity of unlabeled news data. The
unlabeled data is the North American News Cor-
pus, NANC (Graff, 1995), which is approximately
24 million unlabeled sentences from various news
sources. NANC contains no syntactic information
and sentence boundaries are induced by a simple
discriminative model. We also perform some basic
cleanups on NANC to ease parsing. NANC contains
news articles from various news sources including
the Wall Street Journal, though for this paper, we
only use articles from the LA Times portion.
To use the data from NANC, we use self-training
(McClosky et al, 2006). First, we take a WSJ
trained reranking parser (i.e. both the parser and
reranker are built from WSJ training data) and
parse the sentences from NANC with the 50-best
(Charniak and Johnson, 2005) parser. Next, the
50-best parses are reordered by the reranker. Fi-
nally, the 1-best parses after reranking are com-
bined with the WSJ training set to retrain the first-
stage parser.1 McClosky et al (2006) find that the
self-trained models help considerably when pars-
ing WSJ.
4 Experiments
We use the Charniak and Johnson (2005) rerank-
ing parser in our experiments. Unless mentioned
otherwise, we use the WSJ-trained reranker (as op-
posed to a BROWN-trained reranker). To evaluate,
we report bracketing f -scores.2 Parser f -scores
reported are for sentences up to 100 words long,
while reranking parser f -scores are over all sen-
tences. For simplicity and ease of comparison,
most of our evaluations are performed on the de-
velopment section of BROWN.
4.1 Adapting self-training
Our first experiment examines the performance
of the self-trained parsers. While the parsers are
created entirely from labeled WSJ data and unla-
beled NANC data, they perform extremely well on
BROWN development (Table 2). The trends are the
same as in (McClosky et al, 2006): Adding NANC
data improves parsing performance on BROWN
development considerably, improving the f -score
from 83.9% to 86.4%. As more NANC data is
added, the f -score appears to approach an asymp-
tote. The NANC data appears to help reduce data
sparsity and fill in some of the gaps in the WSJ
model. Additionally, the reranker provides fur-
ther benefit and adds an absolute 1-2% to the f -
score. The improvements appear to be orthogonal,
as our best performance is reached when we use
the reranker and add 2,500k self-trained sentences
from NANC.
1We trained a new reranker from this data as well, but it
does not seem to get significantly different performance.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
339
Sentences added Parser Reranking Parser
Baseline BROWN 86.4 87.4
Baseline WSJ 83.9 85.8
WSJ+50k 84.8 86.6
WSJ+250k 85.7 87.2
WSJ+500k 86.0 87.3
WSJ+750k 86.1 87.5
WSJ+1,000k 86.2 87.3
WSJ+1,500k 86.2 87.6
WSJ+2,000k 86.1 87.7
WSJ+2,500k 86.4 87.7
Table 2: Effects of adding NANC sentences to WSJ
training data on parsing performance. f -scores
for the parser with and without the WSJ reranker
are shown when evaluating on BROWN develop-
ment. For this experiment, we use the WSJ-trained
reranker.
The results are even more surprising when we
compare against a parser3 trained on the labeled
training section of the BROWN corpus, with pa-
rameters tuned against its held-out section. De-
spite seeing no in-domain data, the WSJ based
parser is able to match the BROWN based parser.
For the remainder of this paper, we will refer
to the model trained on WSJ+2,500k sentences of
NANC as our ?best WSJ+NANC? model. We also
note that this ?best? parser is different from the
?best? parser for parsing WSJ, which was trained
on WSJ with a relative weight4 of 5 and 1,750k
sentences from NANC. For parsing BROWN, the
difference between these two parsers is not large,
though.
Increasing the relative weight of WSJ sentences
versus NANC sentences when testing on BROWN
development does not appear to have a significant
effect. While (McClosky et al, 2006) showed that
this technique was effective when testing on WSJ,
the true distribution was closer to WSJ so it made
sense to emphasize it.
4.2 Incorporating In-Domain Data
Up to this point, we have only considered the sit-
uation where we have no in-domain data. We now
3In this case, only the parser is trained on BROWN. In sec-
tion 4.3, we compare against a fully BROWN-trained rerank-
ing parser as well.
4A relative weight of n is equivalent to using n copies of
the corpus, i.e. an event that occurred x times in the corpus
would occur x?n times in the weighted corpus. Thus, larger
corpora will tend to dominate smaller corpora of the same
relative weight in terms of event counts.
explore different ways of making use of labeled
and unlabeled in-domain data.
Bacchiani et al (2006) applies self-training to
parser adaptation to utilize unlabeled in-domain
data. The authors find that it helps quite a bit when
adapting from BROWN to WSJ. They use a parser
trained from the BROWN train set to parse WSJ and
add the parsed WSJ sentences to their training set.
We perform a similar experiment, using our WSJ-
trained reranking parser to parse BROWN train and
testing on BROWN development. We achieved a
boost from 84.8% to 85.6% when we added the
parsed BROWN sentences to our training. Adding
in 1,000k sentences from NANC as well, we saw a
further increase to 86.3%. However, the technique
does not seem as effective in our case. While the
self-trained BROWN data helps the parser, it ad-
versely affects the performance of the reranking
parser. When self-trained BROWN data is added to
WSJ training, the reranking parser?s performance
drops from 86.6% to 86.1%. We see a similar
degradation as NANC data is added to the train-
ing set as well. We are not yet able to explain this
unusual behavior.
We now turn to the scenario where we have
some labeled in-domain data. The most obvious
way to incorporate labeled in-domain data is to
combine it with the labeled out-of-domain data.
We have already seen the results (Gildea, 2001)
and (Bacchiani et al, 2006) achieve in Table 1.
We explore various combinations of BROWN,
WSJ, and NANC corpora. Because we are
mainly interested in exploring techniques with
self-trained models rather than optimizing perfor-
mance, we only consider weighting each corpus
with a relative weight of one for this paper. The
models generated are tuned on section 24 from
WSJ. The results are summarized in Table 3.
While both WSJ and BROWN models bene-
fit from a small amount of NANC data, adding
more than 250k NANC sentences to the BROWN
or combined models causes their performance to
drop. This is not surprising, though, since adding
?too much? NANC overwhelms the more accurate
BROWN or WSJ counts. By weighting the counts
from each corpus appropriately, this problem can
be avoided.
Another way to incorporate labeled data is to
tune the parser back-off parameters on it. Bac-
chiani et al (2006) report that tuning on held-out
BROWN data gives a large improvement over tun-
340
ing on WSJ data. The improvement is mostly (but
not entirely) in precision. We do not see the same
improvement (Figure 1) but this is likely due to
differences in the parsers. However, we do see
a similar improvement for parsing accuracy once
NANC data has been added. The reranking parser
generally sees an improvement, but it does not ap-
pear to be significant.
4.3 Reranker Portability
We have shown that the WSJ-trained reranker is
actually quite portable to the BROWN fiction do-
main. This is surprising given the large number
of features (over a million in the case of the WSJ
reranker) tuned to adjust for errors made in the 50-
best lists by the first-stage parser. It would seem
the corrections memorized by the reranker are not
as domain-specific as we might expect.
As further evidence, we present the results of
applying the WSJ model to the Switchboard cor-
pus ? a domain much less similar to WSJ than
BROWN. In Table 4, we see that while the parser?s
performance is low, self-training and reranking
provide orthogonal benefits. The improvements
represent a 12% error reduction with no additional
in-domain data. Naturally, in-domain data and
speech-specific handling (e.g. disfluency model-
ing) would probably help dramatically as well.
Finally, to compare against a model fully
trained on BROWN data, we created a BROWN
reranker. We parsed the BROWN training set with
20-fold cross-validation, selected features that oc-
curred 5 times or more in the training set, and
fed the 50-best lists from the parser to a numeri-
cal optimizer to estimate feature weights. The re-
sulting reranker model had approximately 700,000
features, which is about half as many as the WSJ
trained reranker. This may be due to the smaller
size of the BROWN training set or because the
feature schemas for the reranker were developed
on WSJ data. As seen in Table 5, the BROWN
reranker is not a significant improvement over the
WSJ reranker for parsing BROWN data.
5 Analysis
We perform several types of analysis to measure
some of the differences and similarities between
the BROWN-trained and WSJ-trained reranking
parsers. While the two parsers agree on a large
number of parse brackets (Section 5.2), there are
categorical differences between them (as seen in
Parser model Parser f -score Reranker f -score
WSJ 74.0 75.9
WSJ+NANC 75.6 77.0
Table 4: Parser and reranking parser performance
on the SWITCHBOARD development corpus. In
this case, WSJ+NANC is a model created from WSJ
and 1,750k sentences from NANC.
Model 1-best 10-best 25-best 50-best
WSJ 82.6 88.9 90.7 91.9
WSJ+NANC 86.4 92.1 93.5 94.3
BROWN 86.3 92.0 93.3 94.2
Table 6: Oracle f -scores of top n parses pro-
duced by baseline WSJ parser, a combined WSJ and
NANC parser, and a baseline BROWN parser.
Section 5.3).
5.1 Oracle Scores
Table 6 shows the f -scores of an ?oracle reranker?
? i.e. one which would always choose the parse
with the highest f -score in the n-best list. While
the WSJ parser has relatively low f -scores, adding
NANC data results in a parser with comparable ora-
cle scores as the parser trained from BROWN train-
ing. Thus, the WSJ+NANC model has better oracle
rates than the WSJ model (McClosky et al, 2006)
for both the WSJ and BROWN domains.
5.2 Parser Agreement
In this section, we compare the output of the
WSJ+NANC-trained and BROWN-trained rerank-
ing parsers. We use evalb to calculate how sim-
ilar the two sets of output are on a bracket level.
Table 7 shows various statistics. The two parsers
achieved an 88.0% f -score between them. Ad-
ditionally, the two parsers agreed on all brackets
almost half the time. The part of speech tagging
agreement is fairly high as well. Considering they
were created from different corpora, this seems
like a high level of agreement.
5.3 Statistical Analysis
We conducted randomization tests for the signifi-
cance of the difference in corpus f -score, based on
the randomization version of the paired sample t-
test described by Cohen (1995). The null hypoth-
esis is that the two parsers being compared are in
fact behaving identically, so permuting or swap-
ping the parse trees produced by the parsers for
341
WSJ tuned parser
BROWN tuned parser
WSJ tuned reranking parser
BROWN tuned reranking parser
NANC sentences added
f-
sc
o
re
2000k1750k1500k1250k1000k750k500k250k0k
87.8
87.0
86.0
85.0
83.8
Figure 1: Precision and recall f -scores when testing on BROWN development as a function of the number
of NANC sentences added under four test conditions. ?BROWN tuned? indicates that BROWN training data
was used to tune the parameters (since the normal held-out section was being used for testing). For ?WSJ
tuned,? we tuned the parameters from section 24 of WSJ. Tuning on BROWN helps the parser, but not for
the reranking parser.
Parser model Parser alone Reranking parser
WSJ alone 83.9 85.8
WSJ+2,500k NANC 86.4 87.7
BROWN alone 86.3 87.4
BROWN+50k NANC 86.8 88.0
BROWN+250k NANC 86.8 88.1
BROWN+500k NANC 86.7 87.8
WSJ+BROWN 86.5 88.1
WSJ+BROWN+50k NANC 86.8 88.1
WSJ+BROWN+250k NANC 86.8 88.1
WSJ+BROWN+500k NANC 86.6 87.7
Table 3: f -scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN develop-
ment. The reranking parser used the WSJ-trained reranker model. The BROWN parsing model is naturally
better than the WSJ model for this task, but combining the two training corpora results in a better model
(as in Gildea (2001)). Adding small amounts of NANC further improves the models.
Parser model Parser alone WSJ-reranker BROWN-reranker
WSJ 82.9 85.2 85.2
WSJ+NANC 87.1 87.8 87.9
BROWN 86.7 88.2 88.4
Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWN
test. The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser.
The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statisti-
cally significant.
342
Bracketing agreement f -score 88.03%
Complete match 44.92%
Average crossing brackets 0.94
POS Tagging agreement 94.85%
Table 7: Agreement between the WSJ+NANC
parser with the WSJ reranker and the BROWN
parser with the BROWN reranker. Complete match
is how often the two reranking parsers returned the
exact same parse.
the same test sentence should not affect the cor-
pus f -scores. By estimating the proportion of per-
mutations that result in an absolute difference in
corpus f -scores at least as great as that observed
in the actual output, we obtain a distribution-
free estimate of significance that is robust against
parser and evaluator failures. The results of this
test are shown in Table 8. The table shows that
the BROWN reranker is not significantly different
from the WSJ reranker.
In order to better understand the difference be-
tween the reranking parser trained on Brown and
the WSJ+NANC/WSJ reranking parser (a reranking
parser with the first-stage trained on WSJ+NANC
and the second-stage trained on WSJ) on Brown
data, we constructed a logistic regression model
of the difference between the two parsers? f -
scores on the development data using the R sta-
tistical package5. Of the 2,078 sentences in the
development data, 29 sentences were discarded
because evalb failed to evaluate at least one of
the parses.6 A Wilcoxon signed rank test on the
remaining 2,049 paired sentence level f -scores
was significant at p = 0.0003. Of these 2,049
sentences, there were 983 parse pairs with the
same sentence-level f -score. Of the 1,066 sen-
tences for which the parsers produced parses with
different f -scores, there were 580 sentences for
which the BROWN/BROWN parser produced a
parse with a higher sentence-level f -score and 486
sentences for which the WSJ+NANC/WSJ parser
produce a parse with a higher f -score. We
constructed a generalized linear model with a
binomial link with BROWN/BROWN f -score >
WSJ+NANC/WSJ f -score as the predicted variable,
and sentence length, the number of prepositions
(IN), the number of conjunctions (CC) and Brown
5http://www.r-project.org
6This occurs when an apostrophe is analyzed as a posses-
sive marker in the gold tree and a punctuation symbol in the
parse tree, or vice versa.
Feature Estimate z-value Pr(> |z|)
(Intercept) 0.054 0.3 0.77
IN -0.134 -4.4 8.4e-06 ***
ID=G 0.584 2.5 0.011 *
ID=K 0.697 2.9 0.003 **
ID=L 0.552 2.3 0.021 *
ID=M 0.376 0.9 0.33
ID=N 0.642 2.7 0.0055 **
ID=P 0.624 2.7 0.0069 **
ID=R 0.040 0.1 0.90
Table 9: The logistic model of BROWN/BROWN
f -score > WSJ+NANC/WSJ f -score identified by
model selection. The feature IN is the num-
ber prepositions in the sentence, while ID identi-
fies the Brown subcorpus that the sentence comes
from. Stars indicate significance level.
subcorpus ID as explanatory variables. Model
selection (using the ?step? procedure) discarded
all but the IN and Brown ID explanatory vari-
ables. The final estimated model is shown in Ta-
ble 9. It shows that the WSJ+NANC/WSJ parser
becomes more likely to have a higher f -score
than the BROWN/BROWN parser as the number
of prepositions in the sentence increases, and that
the BROWN/BROWN parser is more likely to have
a higher f -score on Brown sections K, N, P, G
and L (these are the general fiction, adventure and
western fiction, romance and love story, letters and
memories, and mystery sections of the Brown cor-
pus, respectively). The three sections of BROWN
not in this list are F, M, and R (popular lore, sci-
ence fiction, and humor).
6 Conclusions and Future Work
We have demonstrated that rerankers and self-
trained models can work well across domains.
Models self-trained on WSJ appear to be better
parsing models in general, the benefits of which
are not limited to the WSJ domain. The WSJ-
trained reranker using out-of-domain LA Times
parses (produced by the WSJ-trained reranker)
achieves a labeled precision-recall f -measure of
87.8% on Brown data, nearly equal to the per-
formance one achieves by using a purely Brown
trained parser-reranker. The 87.8% f -score on
Brown represents a 24% error reduction on the
corpus.
Of course, as corpora differences go, Brown is
relatively close to WSJ. While we also find that our
343
WSJ+NANC/WSJ BROWN/WSJ BROWN/BROWN
WSJ/WSJ 0.025 (0) 0.030 (0) 0.031 (0)
WSJ+NANC/WSJ 0.004 (0.1) 0.006 (0.025)
BROWN/WSJ 0.002 (0.27)
Table 8: The difference in corpus f -score between the various reranking parsers, and the significance of
the difference in parentheses as estimated by a randomization test with 106 samples. ?x/y? indicates that
the first-stage parser was trained on data set x and the second-stage reranker was trained on data set y.
?best? WSJ-parser-reranker improves performance
on the Switchboard corpus, it starts from a much
lower base (74.0%), and achieves a much less sig-
nificant improvement (3% absolute, 11% error re-
duction). Bridging these larger gaps is still for the
future.
One intriguing idea is what we call ?self-trained
bridging-corpora.? We have not yet experimented
with medical text but we expect that the ?best?
WSJ+NANC parser will not perform very well.
However, suppose one does self-training on a bi-
ology textbook instead of the LA Times. One
might hope that such a text will split the differ-
ence between more ?normal? newspaper articles
and the specialized medical text. Thus, a self-
trained parser based upon such text might do much
better than our standard ?best.? This is, of course,
highly speculative.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-
0001. We would like to thank the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the
Assoc. for Computational Linguistics (ACL), pages
173?180.
Andrew B. Clegg and Adrian Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedi-
cal corpus. In Proceedings of the ACL Workshop on
Software.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cial Intelligence. The MIT Press, Cambridge, Mas-
sachusetts.
Michael Collins. 2000. Discriminative reranking
for natural language parsing. In Machine Learn-
ing: Proceedings of the Seventeenth International
Conference (ICML 2000), pages 175?182, Stanford,
California.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 72?80, University of Maryland.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Second International Joint
Conference on Natural Language Processing (IJC-
NLP?05).
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Satoshi Sekine. 1997. The domain dependence of
parsing. In Proc. Applied Natural Language Pro-
cessing (ANLP), pages 96?102.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL), pages
331?338.
344
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 673?680,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Contextual Dependencies in Unsupervised Word Segmentation?
Sharon Goldwater and Thomas L. Griffiths and Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
{Sharon Goldwater,Tom Griffiths,Mark Johnson}@brown.edu
Abstract
Developing better methods for segment-
ing continuous text into words is impor-
tant for improving the processing of Asian
languages, and may shed light on how hu-
mans learn to segment speech. We pro-
pose two new Bayesian word segmenta-
tion methods that assume unigram and bi-
gram models of word dependencies re-
spectively. The bigram model greatly out-
performs the unigram model (and previous
probabilistic models), demonstrating the
importance of such dependencies for word
segmentation. We also show that previous
probabilistic models rely crucially on sub-
optimal search procedures.
1 Introduction
Word segmentation, i.e., discovering word bound-
aries in continuous text or speech, is of interest for
both practical and theoretical reasons. It is the first
step of processing orthographies without explicit
word boundaries, such as Chinese. It is also one
of the key problems that human language learners
must solve as they are learning language.
Many previous methods for unsupervised word
segmentation are based on the observation that
transitions between units (characters, phonemes,
or syllables) within words are generally more pre-
dictable than transitions across word boundaries.
Statistics that have been proposed for measuring
these differences include ?successor frequency?
(Harris, 1954), ?transitional probabilities? (Saf-
fran et al, 1996), mutual information (Sun et al,
?This work was partially supported by the following
grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF
IGERT-DGE-9870676, and the DARPA CALO project.
1998), ?accessor variety? (Feng et al, 2004), and
boundary entropy (Cohen and Adams, 2001).
While methods based on local statistics are
quite successful, here we focus on approaches
based on explicit probabilistic models. Formulat-
ing an explicit probabilistic model permits us to
cleanly separate assumptions about the input and
properties of likely segmentations from details of
algorithms used to find such solutions. Specifi-
cally, this paper demonstrates the importance of
contextual dependencies for word segmentation
by comparing two probabilistic models that dif-
fer only in that the first assumes that the proba-
bility of a word is independent of its local context,
while the second incorporates bigram dependen-
cies between adjacent words. The algorithms we
use to search for likely segmentations do differ,
but so long as the segmentations they produce are
close to optimal we can be confident that any dif-
ferences in the segmentations reflect differences in
the probabilistic models, i.e., in the kinds of de-
pendencies between words.
We are not the first to propose explicit prob-
abilistic models of word segmentation. Two
successful word segmentation systems based on
explicit probabilistic models are those of Brent
(1999) and Venkataraman (2001). Brent?s Model-
Based Dynamic Programming (MBDP) system as-
sumes a unigram word distribution. Venkatara-
man uses standard unigram, bigram, and trigram
language models in three versions of his system,
which we refer to as n-gram Segmentation (NGS).
Despite their rather different generative structure,
the MBDP and NGS segmentation accuracies are
very similar. Moreover, the segmentation accuracy
of the NGS unigram, bigram, and trigram mod-
els hardly differ, suggesting that contextual depen-
dencies are irrelevant to word segmentation. How-
673
ever, the segmentations produced by both these
methods depend crucially on properties of the
search procedures they employ. We show this by
exhibiting for each model a segmentation that is
less accurate but more probable under that model.
In this paper, we present an alternative frame-
work for word segmentation based on the Dirich-
let process, a distribution used in nonparametric
Bayesian statistics. This framework allows us to
develop extensible models that are amenable to
standard inference procedures. We present two
such models incorporating unigram and bigram
word dependencies, respectively. We use Gibbs
sampling to sample from the posterior distribution
of possible segmentations under these models.
The plan of the paper is as follows. In the next
section, we describe MBDP and NGS in detail. In
Section 3 we present the unigram version of our
own model, the Gibbs sampling procedure we use
for inference, and experimental results. Section 4
extends that model to incorporate bigram depen-
dencies, and Section 5 concludes the paper.
2 NGS and MBDP
The NGS and MBDP systems are similar in some
ways: both are designed to find utterance bound-
aries in a corpus of phonemically transcribed ut-
terances, with known utterance boundaries. Both
also use approximate online search procedures,
choosing and fixing a segmentation for each utter-
ance before moving onto the next. In this section,
we focus on the very different probabilistic mod-
els underlying the two systems. We show that the
optimal solution under the NGS model is the un-
segmented corpus, and suggest that this problem
stems from the fact that the model assumes a uni-
form prior over hypotheses. We then present the
MBDP model, which uses a non-uniform prior but
is difficult to extend beyond the unigram case.
2.1 NGS
NGS assumes that each utterance is generated in-
dependently via a standard n-gram model. For
simplicity, we will discuss the unigram version of
the model here, although our argument is equally
applicable to the bigram and trigram versions. The
unigram model generates an utterance u according
to the grammar in Figure 1, so
P (u) = p$(1? p$)n?1
n
?
j=1
P (wj) (1)
1? p$ U?W U
p$ U?W
P (w) W?w ?w ? ??
Figure 1: The unigram NGS grammar.
where u consists of the words w1 . . . wn and p$ is
the probability of the utterance boundary marker
$. This model can be used to find the highest prob-
ability segmentation hypothesis h given the data d
by using Bayes? rule:
P (h|d) ? P (d|h)P (h)
NGS assumes a uniform prior P (h) over hypothe-
ses, so its goal is to find the solution that maxi-
mizes the likelihood P (d|h).
Using this model, NGS?s approximate search
technique delivers competitive results. However,
the true maximum likelihood solution is not com-
petitive, since it contains no utterance-internal
word boundaries. To see why not, consider the
solution in which p$ = 1 and each utterance is a
single ?word?, with probability equal to the empir-
ical probability of that utterance. Any other so-
lution will match the empirical distribution of the
data less well. In particular, a solution with ad-
ditional word boundaries must have 1 ? p$ > 0,
which means it wastes probability mass modeling
unseen data (which can now be generated by con-
catenating observed utterances together).
Intuitively, the NGS model considers the unseg-
mented solution to be optimal because it ranks all
hypotheses equally probable a priori. We know,
however, that hypotheses that memorize the input
data are unlikely to generalize to unseen data, and
are therefore poor solutions. To prevent memo-
rization, we could restrict our hypothesis space to
models with fewer parameters than the number of
utterances in the data. A more general and mathe-
matically satisfactory solution is to assume a non-
uniform prior, assigning higher probability to hy-
potheses with fewer parameters. This is in fact the
route taken by Brent in his MBDP model, as we
shall see in the following section.
2.2 MBDP
MBDP assumes a corpus of utterances is gener-
ated as a single probabilistic event with four steps:
1. Generate L, the number of lexical types.
2. Generate a phonemic representation for each
type (except the utterance boundary type, $).
674
3. Generate a token frequency for each type.
4. Generate an ordering for the set of tokens.
In a final deterministic step, the ordered tokens
are concatenated to create an unsegmented cor-
pus. This means that certain segmented corpora
will produce the observed data with probability 1,
and all others will produce it with probability 0.
The posterior probability of a segmentation given
the data is thus proportional to its prior probability
under the generative model, and the best segmen-
tation is that with the highest prior probability.
There are two important points to note about
the MBDP model. First, the distribution over L
assigns higher probability to models with fewer
lexical items. We have argued that this is neces-
sary to avoid memorization, and indeed the unseg-
mented corpus is not the optimal solution under
this model, as we will show in Section 3. Second,
the factorization into four separate steps makes
it theoretically possible to modify each step in-
dependently in order to investigate the effects of
the various modeling assumptions. However, the
mathematical statement of the model and the ap-
proximations necessary for the search procedure
make it unclear how to modify the model in any
interesting way. In particular, the fourth step uses
a uniform distribution, which creates a unigram
constraint that cannot easily be changed. Since our
research aims to investigate the effects of different
modeling assumptions on lexical acquisition, we
develop in the following sections a far more flex-
ible model that also incorporates a preference for
sparse solutions.
3 Unigram Model
3.1 The Dirichlet Process Model
Our goal is a model of language that prefers
sparse solutions, allows independent modification
of components, and is amenable to standard search
procedures. We achieve this goal by basing our
model on the Dirichlet process (DP), a distribution
used in nonparametric Bayesian statistics. Our un-
igram model of word frequencies is defined as
wi|G ? G
G|?0, P0 ? DP(?0, P0)
where the concentration parameter ?0 and the
base distribution P0 are parameters of the model.
Each word wi in the corpus is drawn from a
distribution G, which consists of a set of pos-
sible words (the lexicon) and probabilities asso-
ciated with those words. G is generated from
a DP(?0, P0) distribution, with the items in the
lexicon being sampled from P0 and their proba-
bilities being determined by ?0, which acts like
the parameter of an infinite-dimensional symmet-
ric Dirichlet distribution. We provide some intu-
ition for the roles of ?0 and P0 below.
Although the DP model makes the distribution
G explicit, we never deal with G directly. We
take a Bayesian approach and integrate over all
possible values of G. The conditional probabil-
ity of choosing to generate a word from a particu-
lar lexical entry is then given by a simple stochas-
tic process known as the Chinese restaurant pro-
cess (CRP) (Aldous, 1985). Imagine a restaurant
with an infinite number of tables, each with infinite
seating capacity. Customers enter the restaurant
and seat themselves. Let zi be the table chosen by
the ith customer. Then
P (zi|z?i) =
?
?
?
n(z?i)k
i?1+?0 0 ? k < K(z?i)
?0
i?1+?0 k = K(z?i)
(2)
where z?i = z1 . . . zi?1, n(z?i)k is the number of
customers already sitting at table k, and K(z?i) is
the total number of occupied tables. In our model,
the tables correspond to (possibly repeated) lexical
entries, having labels generated from the distribu-
tion P0. The seating arrangement thus specifies
a distribution over word tokens, with each cus-
tomer representing one token. This model is an
instance of the two-stage modeling framework de-
scribed by Goldwater et al (2006), with P0 as the
generator and the CRP as the adaptor.
Our model can be viewed intuitively as a cache
model: each word in the corpus is either retrieved
from a cache or generated anew. Summing over
all the tables labeled with the same word yields
the probability distribution for the ith word given
previously observed words w?i:
P (wi|w?i) =
n(w?i)wi
i? 1 + ?0
+ ?0P0(wi)i? 1 + ?0
(3)
where n(w?i)w is the number of instances of w ob-
served in w?i. The first term is the probability
of generating w from the cache (i.e., sitting at an
occupied table), and the second term is the proba-
bility of generating it anew (sitting at an unoccu-
pied table). The actual table assignments z?i only
become important later, in the bigram model.
675
There are several important points to note about
this model. First, the probability of generating a
particular word from the cache increases as more
instances of that word are observed. This rich-
get-richer process creates a power-law distribution
on word frequencies (Goldwater et al, 2006), the
same sort of distribution found empirically in nat-
ural language. Second, the parameter ?0 can be
used to control how sparse the solutions found by
the model are. This parameter determines the total
probability of generating any novel word, a proba-
bility that decreases as more data is observed, but
never disappears. Finally, the parameter P0 can
be used to encode expectations about the nature
of the lexicon, since it defines a probability distri-
bution across different novel words. The fact that
this distribution is defined separately from the dis-
tribution on word frequencies gives the model ad-
ditional flexibility, since either distribution can be
modified independently of the other.
Since the goal of this paper is to investigate the
role of context in word segmentation, we chose
the simplest possible model for P0, i.e. a unigram
phoneme distribution:
P0(w) = p#(1? p#)n?1
n
?
i=1
P (mi) (4)
where word w consists of the phonemes
m1 . . . mn, and p# is the probability of the
word boundary #. For simplicity we used
a uniform distribution over phonemes, and
experimented with different fixed values of p#.1
A final detail of our model is the distribution
on utterance lengths, which is geometric. That is,
we assume a grammar similar to the one shown in
Figure 1, with the addition of a symmetric Beta( ?2 )
prior over the probability of the U productions,2
and the substitution of the DP for the standard
multinomial distribution over the W productions.
3.2 Gibbs Sampling
Having defined our generative model, we are left
with the problem of inference: we must determine
the posterior distribution of hypotheses given our
input corpus. To do so, we use Gibbs sampling,
a standard Markov chain Monte Carlo method
(Gilks et al, 1996). Gibbs sampling is an itera-
tive procedure in which variables are repeatedly
1Note, however, that our model could be extended to learn
both p# and the distribution over phonemes.
2The Beta distribution is a Dirichlet distribution over two
outcomes.
W
U
w1 = w2.w3
UW
U
W
w3
w2
h1: h2:
Figure 2: The two hypotheses considered by the
unigram sampler. Dashed lines indicate possible
additional structure. All rules except those in bold
are part of h?.
sampled from their conditional posterior distribu-
tion given the current values of all other variables
in the model. The sampler defines a Markov chain
whose stationary distribution is P (h|d), so after
convergence samples are from this distribution.
Our Gibbs sampler considers a single possible
boundary point at a time, so each sample is from
a set of two hypotheses, h1 and h2. These hy-
potheses contain all the same boundaries except
at the one position under consideration, where h2
has a boundary and h1 does not. The structures are
shown in Figure 2. In order to sample a hypothe-
sis, we need only calculate the relative probabili-
ties of h1 and h2. Since h1 and h2 are the same ex-
cept for a few rules, this is straightforward. Let h?
be all of the structure shared by the two hypothe-
ses, including n? words, and let d be the observed
data. Then
P (h1|h?, d) = P (w1|h?, d)
= n
(h?)
w1 + ?0P0(w1)
n? + ?0
(5)
where the second line follows from Equation 3
and the properties of the CRP (in particular, that it
is exchangeable, with the probability of a seating
configuration not depending on the order in which
customers arrive (Aldous, 1985)). Also,
P (h2|h?, d)
= P (r, w2, w3|h?, d)
= P (r|h?, d)P (w2|h?, d)P (w3|w2, h?, d)
= nr +
?
2
n? + 1 + ? ?
n(h
?)
w2 + ?0P0(w2)
n? + ?0
?n
(h?)
w3 + I(w2 = w3) + ?0P0(w3)
n? + 1 + ?0
(6)
where nr is the number of branching rules r =
U ? W U in h?, and I(.) is an indicator func-
tion taking on the value 1 when its argument is
676
true, and 0 otherwise. The nr term is derived by
integrating over all possible values of p$, and not-
ing that the total number of U productions in h?
is n? + 1.
Using these equations we can simply proceed
through the data, sampling each potential bound-
ary point in turn. Once the Gibbs sampler con-
verges, these samples will be drawn from the pos-
terior distribution P (h|d).
3.3 Experiments
In our experiments, we used the same corpus
that NGS and MBDP were tested on. The cor-
pus, supplied to us by Brent, consists of 9790
transcribed utterances (33399 words) of child-
directed speech from the Bernstein-Ratner cor-
pus (Bernstein-Ratner, 1987) in the CHILDES
database (MacWhinney and Snow, 1985). The ut-
terances have been converted to a phonemic rep-
resentation using a phonemic dictionary, so that
each occurrence of a word has the same phonemic
transcription. Utterance boundaries are given in
the input to the system; other word boundaries are
not.
Because our Gibbs sampler is slow to converge,
we used annealing to speed inference. We began
with a temperature of ? = 10 and decreased ? in
10 increments to a final value of 1. A temperature
of ? corresponds to raising the probabilities of h1
and h2 to the power of 1? prior to sampling.
We ran our Gibbs sampler for 20,000 iterations
through the corpus (with ? = 1 for the final 2000)
and evaluated our results on a single sample at
that point. We calculated precision (P), recall (R),
and F-score (F) on the word tokens in the corpus,
where both boundaries of a word must be correct
to count the word as correct. The induced lexicon
was also scored for accuracy using these metrics
(LP, LR, LF).
Recall that our DP model has three parameters:
?, p#, and ?0. Given the large number of known
utterance boundaries, we expect the value of ? to
have little effect on our results, so we simply fixed
? = 2 for all experiments. Figure 3 shows the ef-
fects of varying of p# and ?0.3 Lower values of
p# cause longer words, which tends to improve re-
call (and thus F-score) in the lexicon, but decrease
token accuracy. Higher values of ?0 allow more
novel words, which also improves lexicon recall,
3It is worth noting that all these parameters could be in-
ferred. We leave this for future work.
0.1 0.3 0.5 0.7 0.9
50
55
60
(a) Varying P(#)
 
 
1 2 5 10 20 50 100 200 500
50
55
60
(b) Varying ?0
 
 
LF
F
LF
F
Figure 3: Word (F) and lexicon (LF) F-score (a)
as a function of p#, with ?0 = 20 and (b) as a
function of ?0, with p# = .5.
but begins to degrade precision after a point. Due
to the negative correlation between token accuracy
and lexicon accuracy, there is no single best value
for either p# or ?0; further discussion refers to the
solution for p# = .5, ?0 = 20 (though others are
qualitatively similar).
In Table 1(a), we compare the results of our sys-
tem to those of MBDP and NGS.4 Although our
system has higher lexicon accuracy than the oth-
ers, its token accuracy is much worse. This result
occurs because our system often mis-analyzes fre-
quently occurring words. In particular, many of
these words occur in common collocations such
as what?s that and do you, which the system inter-
prets as a single words. It turns out that a full 31%
of the proposed lexicon and nearly 30% of tokens
consist of these kinds of errors.
Upon reflection, it is not surprising that a uni-
gram language model would segment words in this
way. Collocations violate the unigram assumption
in the model, since they exhibit strong word-to-
word dependencies. The only way the model can
capture these dependencies is by assuming that
these collocations are in fact words themselves.
Why don?t the MBDP and NGS unigram mod-
els exhibit these problems? We have already
shown that NGS?s results are due to its search pro-
cedure rather than its model. The same turns out
to be true for MBDP. Table 2 shows the probabili-
4We used the implementations of MBDP and NGS avail-
able at http://www.speech.sri.com/people/anand/ to obtain re-
sults for those systems.
677
(a) P R F LP LR LF
NGS 67.7 70.2 68.9 52.9 51.3 52.0
MBDP 67.0 69.4 68.2 53.6 51.3 52.4
DP 61.9 47.6 53.8 57.0 57.5 57.2
(b) P R F LP LR LF
NGS 76.6 85.8 81.0 60.0 52.4 55.9
MBDP 77.0 86.1 81.3 60.8 53.0 56.6
DP 94.2 97.1 95.6 86.5 62.2 72.4
Table 1: Accuracy of the various systems, with
best scores in bold. The unigram version of NGS
is shown. DP results are with p# = .5 and ?0 =
20. (a) Results on the true corpus. (b) Results on
the permuted corpus.
Seg: True None MBDP NGS DP
NGS 204.5 90.9 210.7 210.8 183.0
MBDP 208.2 321.7 217.0 218.0 189.8
DP 222.4 393.6 231.2 231.6 200.6
Table 2: Negative log probabilities (x 1000) un-
der each model of the true solution, the solution
with no utterance-internal boundaries, and the so-
lutions found by each algorithm. Best solutions
under each model are bold.
ties under each model of various segmentations of
the corpus. From these figures, we can see that
the MBDP model assigns higher probability to the
solution found by our Gibbs sampler than to the
solution found by Brent?s own incremental search
algorithm. In other words, Brent?s model does pre-
fer the lower-accuracy collocation solution, but his
search algorithm instead finds a higher-accuracy
but lower-probability solution.
We performed two experiments suggesting that
our own inference procedure does not suffer from
similar problems. First, we initialized our Gibbs
sampler in three different ways: with no utterance-
internal boundaries, with a boundary after every
character, and with random boundaries. Our re-
sults were virtually the same regardless of initial-
ization. Second, we created an artificial corpus by
randomly permuting the words in the true corpus,
leaving the utterance lengths the same. The ar-
tificial corpus adheres to the unigram assumption
of our model, so if our inference procedure works
correctly, we should be able to correctly identify
the words in the permuted corpus. This is exactly
what we found, as shown in Table 1(b). While all
three models perform better on the artificial cor-
pus, the improvements of the DP model are by far
the most striking.
4 Bigram Model
4.1 The Hierarchical Dirichlet Process Model
The results of our unigram experiments suggested
that word segmentation could be improved by
taking into account dependencies between words.
To test this hypothesis, we extended our model
to incorporate bigram dependencies using a hi-
erarchical Dirichlet process (HDP) (Teh et al,
2005). Our approach is similar to previous n-gram
models using hierarchical Pitman-Yor processes
(Goldwater et al, 2006; Teh, 2006). The HDP is
appropriate for situations in which there are multi-
ple distributions over similar sets of outcomes, and
the distributions are believed to be similar. In our
case, we define a bigram model by assuming each
word has a different distribution over the words
that follow it, but all these distributions are linked.
The definition of our bigram language model as an
HDP is
wi|wi?1 = w,Hw ? Hw ?w
Hw|?1, G ? DP(?1, G) ?w
G|?0, P0 ? DP(?0, P0)
That is, P (wi|wi?1 = w) is distributed accord-
ing to Hw, a DP specific to word w. Hw is linked
to the DPs for all other words by the fact that they
share a common base distribution G, which is gen-
erated from another DP.5
As in the unigram model, we never deal with
Hw or G directly. By integrating over them, we get
a distribution over bigram frequencies that can be
understood in terms of the CRP. Now, each word
type w is associated with its own restaurant, which
represents the distribution over words that follow
w. Different restaurants are not completely inde-
pendent, however: the labels on the tables in the
restaurants are all chosen from a common base
distribution, which is another CRP.
To understand the HDP model in terms of a
grammar, we consider $ as a special word type,
so that wi ranges over ?? ? {$}. After observing
w?i, the HDP grammar is as shown in Figure 4,
5This HDP formulation is an oversimplification, since it
does not account for utterance boundaries properly. The
grammar formulation (see below) does.
678
P2(wi|w?i, z?i) Uwi?1?Wwi Uwi ?wi ? ??,
wi?1 ? ???{$}
P2($|w?i, z?i) Uwi?1?$ ?wi?1 ? ??
1 Wwi ?wi ?wi ? ??
Figure 4: The HDP grammar after observing w?i.
with
P2(wi|h?i) =
n(wi?1,wi) + ?1P1(wi|h?i)
nwi?1 + ?1
(7)
P1(wi|h?i) =
?
?
?
t??+ ?2
t+? ?
twi+?0P0(wi)
t??+?0 wi ? ?
?
t$+ ?2
t+? wi = $
where h?i = (w?i, z?i); t$, t?? , and twi are the
total number of tables (across all words) labeled
with $, non-$, and wi, respectively; t = t$ + t??
is the total number of tables; and n(wi?1,wi) is the
number of occurrences of the bigram (wi?1, wi).
We have suppressed the superscript (w?i) nota-
tion in all cases. The base distribution shared by
all bigrams is given by P1, which can be viewed as
a unigram backoff where the unigram probabilities
are learned from the bigram table labels.
We can perform inference on this HDP bigram
model using a Gibbs sampler similar to our uni-
gram sampler. Details appear in the Appendix.
4.2 Experiments
We used the same basic setup for our experiments
with the HDP model as we used for the DP model.
We experimented with different values of ?0 and
?1, keeping p# = .5 throughout. Some results
of these experiments are plotted in Figure 5. With
appropriate parameter settings, both lexicon and
token accuracy are higher than in the unigram
model (dramatically so, for tokens), and there is
no longer a negative correlation between the two.
Only a few collocations remain in the lexicon, and
most lexicon errors are on low-frequency words.
The best values of ?0 are much larger than in the
unigram model, presumably because all unique
word types must be generated via P0, but in the
bigram model there is an additional level of dis-
counting (the unigram process) before reaching
P0. Smaller values of ?0 lead to fewer word types
with fewer characters on average.
Table 3 compares the optimal results of the
HDP model to the only previous model incorpo-
rating bigram dependencies, NGS. Due to search,
the performance of the bigram NGS model is not
much different from that of the unigram model. In
100 200 500 1000 2000
40
60
80
(a) Varying ?0
 
 
F
LF
5 10 20 50 100 200 500
40
60
80
(b) Varying ?1
 
 
F
LF
Figure 5: Word (F) and lexicon (LF) F-score (a)
as a function of ?0, with ?1 = 10 and (b) as a
function of ?1, with ?0 = 1000.
P R F LP LR LF
NGS 68.1 68.6 68.3 54.5 57.0 55.7
HDP 79.4 74.0 76.6 67.9 58.9 63.1
Table 3: Bigram system accuracy, with best scores
in bold. HDP results are with p# = .5, ?0 =
1000, and ?1 = 10.
contrast, our HDP model performs far better than
our DP model, leading to the highest published ac-
curacy for this corpus on both tokens and lexical
items. Overall, these results strongly support our
hypothesis that modeling bigram dependencies is
important for accurate word segmentation.
5 Conclusion
In this paper, we have introduced a new model-
based approach to word segmentation that draws
on techniques from Bayesian statistics, and we
have developed models incorporating unigram and
bigram dependencies. The use of the Dirichlet
process as the basis of our approach yields sparse
solutions and allows us the flexibility to modify
individual components of the models. We have
presented a method of inference using Gibbs sam-
pling, which is guaranteed to converge to the pos-
terior distribution over possible segmentations of
a corpus.
Our approach to word segmentation allows us to
investigate questions that could not be addressed
satisfactorily in earlier work. We have shown that
the search algorithms used with previous models
of word segmentation do not achieve their ob-
679
P (h1|h?, d) =
n(wl,w1) + ?1P1(w1|h?, d)
nwl + ?1
?
n(w1,wr) + I(wl =w1 =wr) + ?1P1(wr|h?, d)
nw1 + 1 + ?1
P (h2|h?, d) =
n(wl,w2) + ?1P1(w2|h?, d)
nwl + ?1
?
n(w2,w3) + I(wl =w2 =w3) + ?1P1(w3|h?, d)
nw2 + 1 + ?1
?
n(w3,wr) + I(wl =w3, w2 =wr) + I(w2 =w3 =wr) + ?1P1(wr|h?, d)
nw3 + 1 + I(w2 =w4) + ?1
Figure 6: Gibbs sampling equations for the bigram model. All counts are with respect to h?.
jectives, which has led to misleading results. In
particular, previous work suggested that the use
of word-to-word dependencies has little effect on
word segmentation. Our experiments indicate in-
stead that bigram dependencies can be crucial for
avoiding under-segmentation of frequent colloca-
tions. Incorporating these dependencies into our
model greatly improved segmentation accuracy,
and led to better performance than previous ap-
proaches on all measures.
References
D. Aldous. 1985. Exchangeability and related topics. In
?Ecole d?e?te? de probabilite?s de Saint-Flour, XIII?1983,
pages 1?198. Springer, Berlin.
C. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The An-
nals of Statistics, 2:1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent-child
speech. In K. Nelson and A. van Kleeck, editors, Chil-
dren?s Language, volume 6. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound al-
gorithm for segmentation and word discovery. Machine
Learning, 34:71?105.
P. Cohen and N. Adams. 2001. An algorithm for segment-
ing categorical timeseries into meaningful episodes. In
Proceedings of the Fourth Symposium on Intelligent Data
Analysis.
H. Feng, K. Chen, X. Deng, and W. Zheng. 2004. Acces-
sor variety criteria for chinese word extraction. Computa-
tional Lingustics, 30(1).
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Interpo-
lating between types and tokens by estimating power-law
generators. In Advances in Neural Information Process-
ing Systems 18, Cambridge, MA. MIT Press.
Z. Harris. 1954. Distributional structure. Word, 10:146?162.
B. MacWhinney and C. Snow. 1985. The child language data
exchange system. Journal of Child Language, 12:271?
296.
J. Saffran, E. Newport, and R. Aslin. 1996. Word segmenta-
tion: The role of distributional cues. Journal of Memory
and Language, 35:606?621.
M. Sun, D. Shen, and B. Tsou. 1998. Chinese word seg-
mentation without using lexicon and hand-crafted training
data. In Proceedings of COLING-ACL.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2005. Hierarchical
Dirichlet processes. In Advances in Neural Information
Processing Systems 17. MIT Press, Cambridge, MA.
Y. Teh. 2006. A Bayesian interpretation of interpolated
kneser-ney. Technical Report TRA2/06, National Univer-
sity of Singapore, School of Computing.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):351?372.
Appendix
To sample from the posterior distribution over seg-
mentations in the bigram model, we define h1 and
h2 as we did in the unigram sampler so that for the
corpus substring s, h1 has a single word (s = w1)
where h2 has two (s = w2.w3). Let wl and wr be
the words (or $) preceding and following s. Then
the posterior probabilities of h1 and h2 are given
in Figure 6. P1(.) can be calculated exactly using
the equation in Section 4.1, but this requires ex-
plicitly tracking and sampling the assignment of
words to tables. For easier and more efficient im-
plementation, we use an approximation, replacing
each table count twi by its expected value E[twi ].
In a DP(?,P ), the expected number of CRP tables
for an item occurring n times is ? log n+?? (Anto-
niak, 1974), so
E[twi ] = ?1
?
j
log
n(wj ,wi) + ?1
?1
This approximation requires only the bigram
counts, which we must track anyway.
680
Proceedings of ACL-08: HLT, pages 398?406,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using adaptor grammars to identify synergies
in the unsupervised acquisition of linguistic structure
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Abstract
Adaptor grammars (Johnson et al, 2007b) are
a non-parametric Bayesian extension of Prob-
abilistic Context-Free Grammars (PCFGs)
which in effect learn the probabilities of en-
tire subtrees. In practice, this means that an
adaptor grammar learns the structures useful
for generating the training data as well as
their probabilities. We present several differ-
ent adaptor grammars that learn to segment
phonemic input into words by modeling dif-
ferent linguistic properties of the input. One
of the advantages of a grammar-based frame-
work is that it is easy to combine grammars,
and we use this ability to compare models that
capture different kinds of linguistic structure.
We show that incorporating both unsupervised
syllabification and collocation-finding into the
adaptor grammar significantly improves un-
supervised word-segmentation accuracy over
that achieved by adaptor grammars that model
only one of these linguistic phenomena.
1 Introduction
How humans acquire language is arguably the cen-
tral issue in the scientific study of language. Hu-
man language is richly structured, but it is still hotly
debated as to whether this structure can be learnt,
or whether it must be innately specified. Compu-
tational linguistics can contribute to this debate by
identifying which aspects of language can poten-
tially be learnt from the input available to a child.
Here we try to identify linguistic properties that
convey information useful for learning to segment
streams of phonemes into words. We show that si-
multaneously learning syllable structure and collo-
cations improves word segmentation accuracy com-
pared to models that learn these independently. This
suggests that there might be a synergistic interaction
in learning several aspects of linguistic structure si-
multaneously, as compared to learning each kind of
linguistic structure independently.
Because learning collocations and word-initial
syllable onset clusters requires the learner to be able
to identify word boundaries, it might seem that we
face a chicken-and-egg problem here. One of the im-
portant properties of the adaptor grammar inference
procedure is that it gives us a way of learning these
interacting linguistic structures simultaneously.
Adaptor grammars are also interesting because
they can be viewed as directly inferring linguistic
structure. Most well-known machine-learning and
statistical inference procedures are parameter esti-
mation procedures, i.e., the procedure is designed to
find the values of a finite vector of parameters. Stan-
dard methods for learning linguistic structure typi-
cally try to reduce structure learning to parameter
estimation, say, by using an iterative generate-and-
prune procedure in which each iteration consists of
a rule generation step that proposes new rules ac-
cording to some scheme, a parameter estimation step
that estimates the utility of these rules, and pruning
step that removes low utility rules. For example, the
Bayesian unsupervised PCFG estimation procedure
devised by Stolcke (1994) uses a model-merging
procedure to propose new sets of PCFG rules and
a Bayesian version of the EM procedure to estimate
their weights.
398
Recently, methods have been developed in the
statistical community for Bayesian inference of
increasingly sophisticated non-parametric models.
(?Non-parametric? here means that the models are
not characterized by a finite vector of parameters,
so the complexity of the model can vary depending
on the data it describes). Adaptor grammars are a
framework for specifying a wide range of such mod-
els for grammatical inference. They can be viewed
as a nonparametric extension of PCFGs.
Informally, there seem to be at least two natu-
ral ways to construct non-parametric extensions of a
PCFG. First, we can construct an infinite number of
more specialized PCFGs by splitting or refining the
PCFG?s nonterminals into increasingly finer states;
this leads to the iPCFG or ?infinite PCFG? (Liang et
al., 2007). Second, we can generalize over arbitrary
subtrees rather than local trees in much the way done
in DOP or tree substitution grammar (Bod, 1998;
Joshi, 2003), which leads to adaptor grammars.
Informally, the units of generalization of adap-
tor grammars are entire subtrees, rather than just
local trees, as in PCFGs. Just as in tree substitu-
tion grammars, each of these subtrees behaves as
a new context-free rule that expands the subtree?s
root node to its leaves, but unlike a tree substitu-
tion grammar, in which the subtrees are specified
in advance, in an adaptor grammar the subtrees, as
well as their probabilities, are learnt from the train-
ing data. In order to make parsing and inference
tractable we require the leaves of these subtrees to
be terminals, as explained in section 2. Thus adaptor
grammars are simple models of structure learning,
where the subtrees that constitute the units of gen-
eralization are in effect new context-free rules learnt
during the inference process. (In fact, the inference
procedure for adaptor grammars described in John-
son et al (2007b) relies on a PCFG approximation
that contains a rule for each subtree generalization
in the adaptor grammar).
This paper applies adaptor grammars to word seg-
mentation and morphological acquisition. Linguis-
tically, these exhibit considerable cross-linguistic
variation, and so are likely to be learned by human
learners. It?s also plausible that semantics and con-
textual information is less important for their acqui-
sition than, say, syntax.
2 From PCFGs to Adaptor Grammars
This section introduces adaptor grammars as an ex-
tension of PCFGs; for a more detailed exposition see
Johnson et al (2007b). Formally, an adaptor gram-
mar is a PCFG in which a subset M of the nonter-
minals are adapted. An adaptor grammar generates
the same set of trees as the CFG with the same rules,
but instead of defining a fixed probability distribu-
tion over these trees as a PCFG does, it defines a
distribution over distributions over trees. An adaptor
grammar can be viewed as a kind of PCFG in which
each subtree of each adapted nonterminal A ?M is
a potential rule, with its own probability, so an adap-
tor grammar is nonparametric if there are infinitely
many possible adapted subtrees. (An adaptor gram-
mar can thus be viewed as a tree substitution gram-
mar with infinitely many initial trees). But any finite
set of sample parses for any finite corpus can only in-
volve a finite number of such subtrees, so the corre-
sponding PCFG approximation only involves a finite
number of rules, which permits us to build MCMC
samplers for adaptor grammars.
A PCFG can be viewed as a set of recursively-
defined mixture distributions GA over trees, one for
each nonterminal and terminal in the grammar. If A
is a terminal then GA is the distribution that puts all
of its mass on the unit tree (i.e., tree consisting of a
single node) labeled A. If A is a nonterminal then
GA is the distribution over trees with root labeled A
that satisfies:
GA =
?
A?B1...Bn?RA
?A?B1...BnTDA(GB1 , . . . , GBn)
where RA is the set of rules expanding A,
?A?B1,...,Bn is the PCFG ?probability? parame-
ter associated with the rule A ? B1 . . . Bn and
TDA(GB1 , . . . , GBn) is the distribution over trees
with root label A satisfying:
TDA(G1, . . . , Gn)
(
 XX
A
t1 tn. . .
)
=
n
?
i=1
Gi(ti).
That is, TDA(G1, . . . , Gn) is the distribution over
trees whose root node is labeled A and each subtree
ti is generated independently from the distribution
Gi. This independence assumption is what makes
a PCFG ?context-free? (i.e., each subtree is inde-
pendent given its label). Adaptor grammars relax
399
this independence assumption by in effect learning
the probability of the subtrees rooted in a specified
subset M of the nonterminals known as the adapted
nonterminals.
Adaptor grammars achieve this by associating
each adapted nonterminal A ? M with a Dirichlet
Process (DP). A DP is a function of a base distri-
bution H and a concentration parameter ?, and it
returns a distribution over distributions DP(?,H).
There are several different ways to define DPs; one
of the most useful is the characterization of the con-
ditional or sampling distribution of a draw from
DP(?,H) in terms of the Polya urn or Chinese
Restaurant Process (Teh et al, 2006). The Polya urn
initially contains ?H(x) balls of color x. We sample
a distribution from DP(?,H) by repeatedly drawing
a ball at random from the urn and then returning it
plus an additional ball of the same color to the urn.
In an adaptor grammar there is one DP for each
adapted nonterminal A ? M , whose base distribu-
tion HA is the distribution over trees defined using
A?s PCFG rules. This DP ?adapts? A?s PCFG distri-
bution by moving mass from the infrequently to the
frequently occuring subtrees. An adaptor grammar
associates a distribution GA that satisfies the follow-
ing constraints with each nonterminal A:
GA ? DP(?A,HA) if A ?M
GA = HA if A 6?M
HA =
?
A?B1...Bn?RA
?A?B1...BnTDA(GB1 , . . . , GBn)
Unlike a PCFG, an adaptor grammar does not define
a single distribution over trees; rather, each set of
draws from the DPs defines a different distribution.
In the adaptor grammars used in this paper there is
no recursion amongst adapted nonterminals (i.e., an
adapted nonterminal never expands to itself); it is
currently unknown whether there are tree distribu-
tions that satisfy the adaptor grammar constraints for
recursive adaptor grammars.
Inference for an adaptor grammar involves finding
the rule probabilities ? and the adapted distributions
over trees G. We put Dirichlet priors over the rule
probabilities, i.e.:
?A ? DIR(?A)
where ?A is the vector of probabilities for the rules
expanding the nonterminal A and ?A are the corre-
sponding Dirichlet parameters.
The applications described below require unsu-
pervised estimation, i.e., the training data consists
of terminal strings alone. Johnson et al (2007b)
describe an MCMC procedure for inferring the
adapted tree distributions GA, and Johnson et al
(2007a) describe a Bayesian inference procedure for
the PCFG rule parameters ? using a Metropolis-
Hastings MCMC procedure; implementations are
available from the author?s web site.
Informally, the inference procedure proceeds as
follows. We initialize the sampler by randomly as-
signing each string in the training corpus a random
tree generated by the grammar. Then we randomly
select a string to resample, and sample a parse of that
string with a PCFG approximation to the adaptor
grammar. This PCFG contains a production for each
adapted subtree in the parses of the other strings in
the training corpus. A final accept-reject step cor-
rects for the difference in the probability of the sam-
pled tree under the adaptor grammar and the PCFG
approximation.
3 Word segmentation with adaptor
grammars
We now turn to linguistic applications of adap-
tor grammars, specifically, to models of unsu-
pervised word segmentation. We follow previ-
ous work in using the Brent corpus consists of
9790 transcribed utterances (33,399 words) of child-
directed speech from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) in the CHILDES database
(MacWhinney and Snow, 1985). The utterances
have been converted to a phonemic representation
using a phonemic dictionary, so that each occur-
rence of a word has the same phonemic transcrip-
tion. Utterance boundaries are given in the input to
the system; other word boundaries are not. We eval-
uated the f-score of the recovered word constituents
(Goldwater et al, 2006b). Using the adaptor gram-
mar software available on the author?s web site, sam-
plers were run for 10,000 epochs (passes through
the training data). We scored the parses assigned
to the training data at the end of sampling, and for
the last two epochs we annealed at temperature 0.5
(i.e., squared the probability) during sampling in or-
400
1 10 100 1000
U word 0.55 0.55 0.55 0.53
U morph 0.46 0.46 0.42 0.36
U syll 0.52 0.51 0.49 0.46
C word 0.53 0.64 0.74 0.76
C morph 0.56 0.63 0.73 0.63
C syll 0.77 0.77 0.78 0.74
Table 1: Word segmentation f-score results for all mod-
els, as a function of DP concentration parameter ?. ?U?
indicates unigram-based grammars, while ?C? indicates
collocation-based grammars.
Sentence ? Word+
Word ? Phoneme+
Figure 1: The unigram word adaptor grammar, which
uses a unigram model to generate a sequence of words,
where each word is a sequence of phonemes. Adapted
nonterminals are underlined.
der to concentrate mass on high probability parses.
In all experiments below we set ? = 1, which corre-
sponds to a uniform prior on PCFG rule probabilities
?. We tied the Dirichlet Process concentration pa-
rameters ?, and performed runs with ? = 1, 10, 100
and 1000; apart from this, no attempt was made to
optimize the hyperparameters. Table 1 summarizes
the word segmentation f-scores for all models de-
scribed in this paper.
3.1 Unigram word adaptor grammar
Johnson et al (2007a) presented an adaptor gram-
mar that defines a unigram model of word segmen-
tation and showed that it performs as well as the
unigram DP word segmentation model presented by
(Goldwater et al, 2006a). The adaptor grammar that
encodes a unigram word segmentation model shown
in Figure 1.
In this grammar and the grammars below, under-
lining indicates an adapted nonterminal. Phoneme
is a nonterminal that expands to each of the 50 dis-
tinct phonemes present in the Brent corpus. This
grammar defines a Sentence to consist of a sequence
of Words, where a Word consists of a sequence of
Phonemes. The category Word is adapted, which
means that the grammar learns the words that oc-
cur in the training corpus. We present our adap-
Sentence ? Words
Words ? Word
Words ? Word Words
Word ? Phonemes
Phonemes ? Phoneme
Phonemes ? Phoneme Phonemes
Figure 2: The unigram word adaptor grammar of Fig-
ure 1 where regular expressions are expanded using new
unadapted right-branching nonterminals.
Sentence
Word
y u w a n t
Word
t u
Word
s i D 6
Word
b U k
Figure 3: A parse of the phonemic representation of ?you
want to see the book? produced by unigram word adap-
tor grammar of Figure 1. Only nonterminal nodes la-
beled with adapted nonterminals and the start symbol are
shown.
tor grammars using regular expressions for clarity,
but since our implementation does not handle reg-
ular expressions in rules, in the grammars actually
used by the program they are expanded using new
non-adapted nonterminals that rewrite in a uniform
right-branching manner. That is, the adaptor gram-
mar used by the program is shown in Figure 2.
The unigram word adaptor grammar generates
parses such as the one shown in Figure 3. With ? =
1 and ? = 10 we obtained a word segmentation f-
score of 0.55. Depending on the run, between 1, 100
and 1, 400 subtrees (i.e., new rules) were found for
Word. As reported in Goldwater et al (2006a) and
Goldwater et al (2007), a unigram word segmen-
tation model tends to undersegment and misanalyse
collocations as individual words. This is presumably
because the unigram model has no way to capture
dependencies between words in collocations except
to make the collocation into a single word.
3.2 Unigram morphology adaptor grammar
This section investigates whether learning mor-
phology together with word segmentation improves
word segmentation accuracy. Johnson et al (2007a)
presented an adaptor grammar for segmenting verbs
into stems and suffixes that implements the DP-
401
Sentence ? Word+
Word ? Stem (Suffix)
Stem ? Phoneme+
Suffix ? Phoneme+
Figure 4: The unigram morphology adaptor grammar,
which generates each Sentence as a sequence of Words,
and each Word as a Stem optionally followed by a Suffix.
Parentheses indicate optional constituents.
Sentence
Word
Stem
w a n
Suffix
6
Word
Stem
k l o z
Suffix
I t
Sentence
Word
Stem
y u
Suffix
h & v
Word
Stem
t u
Word
Stem
t E l
Suffix
m i
Figure 5: Parses of ?wanna close it? and ?you have to tell
me? produced by the unigram morphology grammar of
Figure 4. The first parse was chosen because it demon-
strates how the grammar is intended to analyse ?wanna?
into a Stem and Suffix, while the second parse shows how
the grammar tends to use Stem and Suffix to capture col-
locations.
based unsupervised morphological analysis model
presented by Goldwater et al (2006b). Here we
combine that adaptor grammar with the unigram
word segmentation grammar to produce the adap-
tor grammar shown in Figure 4, which is designed
to simultaneously learn both word segmentation and
morphology.
Parentheses indicate optional constituents in these
rules, so this grammar says that a Sentence consists
of a sequence of Words, and each Word consists of a
Stem followed by an optional Suffix. The categories
Word, Stem and Suffix are adapted, which means
that the grammar learns the Words, Stems and Suf-
fixes that occur in the training corpus. Technically
this grammar implements a Hierarchical Dirichlet
Process (HDP) (Teh et al, 2006) because the base
distribution for the Word DP is itself constructed
from the Stem and Suffix distributions, which are
themselves generated by DPs.
This grammar recovers words with an f-score of
only 0.46 with ? = 1 or ? = 10, which is consid-
erably less accurate than the unigram model of sec-
tion 3.1. Typical parses are shown in Figure 5. The
unigram morphology grammar tends to misanalyse
even longer collocations as words than the unigram
word grammar does. Inspecting the parses shows
that rather than capturing morphological structure,
the Stem and Suffix categories typically expand to
words themselves, so the Word category expands to
a collocation. It may be possible to correct this by
?tuning? the grammar?s hyperparameters, but we did
not attempt this here.
These results are not too surprising, since the kind
of regular stem-suffix morphology that this grammar
can capture is not common in the Brent corpus. It
is possible that a more sophisticated model of mor-
phology, or even a careful tuning of the Bayesian
prior parameters ? and ?, would produce better re-
sults.
3.3 Unigram syllable adaptor grammar
PCFG estimation procedures have been used to
model the supervised and unsupervised acquisition
of syllable structure (Mu?ller, 2001; Mu?ller, 2002);
and the best performance in unsupervised acquisi-
tion is obtained using a grammar that encodes lin-
guistically detailed properties of syllables whose
rules are inferred using a fairly complex algorithm
(Goldwater and Johnson, 2005). While that work
studied the acquisition of syllable structure from iso-
lated words, here we investigate whether learning
syllable structure together with word segmentation
improves word segmentation accuracy. Modeling
syllable structure is a natural application of adaptor
grammars, since the grammar can learn the possible
onset and coda clusters, rather than requiring them
to be stipulated in the grammar.
In the unigram syllable adaptor grammar shown
in Figure 7, Consonant expands to any consonant
and Vowel expands to any vowel. This gram-
mar defines a Word to consist of up to three Syl-
lables, where each Syllable consists of an Onset
and a Rhyme and a Rhyme consists of a Nucleus
and a Coda. Following Goldwater and Johnson
(2005), the grammar differentiates between OnsetI,
which expands to word-initial onsets, and Onset,
402
Sentence
Word
OnsetI
W
Nucleus
A
CodaF
t s
Word
OnsetI
D
Nucleus
I
CodaF
s
Figure 6: A parse of ?what?s this? produced by the
unigram syllable adaptor grammar of Figure 7. (Only
adapted non-root nonterminals are shown in the parse).
which expands to non-word-initial onsets, and be-
tween CodaF, which expands to word-final codas,
and Coda, which expands to non-word-final codas.
Note that we do not need to distinguish specific posi-
tions within the Onset and Coda clusters as Goldwa-
ter and Johnson (2005) did, since the adaptor gram-
mar learns these clusters directly. Just like the un-
igram morphology grammar, the unigram syllable
grammar also defines a HDP because the base dis-
tribution for Word is defined in terms of the Onset
and Rhyme distributions.
The unigram syllable grammar achieves a word
segmentation f-score of 0.52 at ? = 1, which is also
lower than the unigram word grammar achieves. In-
spection of the parses shows that the unigram sylla-
ble grammar also tends to misanalyse long colloca-
tions as Words. Specifically, it seems to misanalyse
function words as associated with the content words
next to them, perhaps because function words tend
to have simpler initial and final clusters.
We cannot compare our syllabification accuracy
with Goldwater?s and others? previous work because
that work used different, supervised training data
and phonological representations based on British
rather than American pronunciation.
3.4 Collocation word adaptor grammar
Goldwater et al (2006a) showed that modeling de-
pendencies between adjacent words dramatically
improves word segmentation accuracy. It is not
possible to write an adaptor grammar that directly
implements Goldwater?s bigram word segmentation
model because an adaptor grammar has one DP per
adapted nonterminal (so the number of DPs is fixed
in advance) while Goldwater?s bigram model has
one DP per word type, and the number of word
types is not known in advance. However it is pos-
Sentence ? Word+
Word ? SyllableIF
Word ? SyllableI SyllableF
Word ? SyllableI Syllable SyllableF
Syllable ? (Onset) Rhyme
SyllableI ? (OnsetI) Rhyme
SyllableF ? (Onset) RhymeF
SyllableIF ? (OnsetI) RhymeF
Rhyme ? Nucleus (Coda)
RhymeF ? Nucleus (CodaF)
Onset ? Consonant+
OnsetI ? Consonant+
Coda ? Consonant+
CodaF ? Consonant+
Nucleus ? Vowel+
Figure 7: The unigram syllable adaptor grammar, which
generates each word as a sequence of up to three Sylla-
bles. Word-initial Onsets and word-final Codas are distin-
guished using the suffixes ?I? and ?F? respectively; these
are propagated through the grammar to ensure that these
appear in the correct positions.
Sentence ? Colloc+
Colloc ? Word+
Word ? Phoneme+
Figure 8: The collocation word adaptor grammar, which
generates a Sentence as sequence of Colloc(ations), each
of which consists of a sequence of Words.
sible for an adaptor grammar to generate a sentence
as a sequence of collocations, each of which con-
sists of a sequence of words. These collocations give
the grammar a way to model dependencies between
words.
With the DP concentration parameters ? = 1000
we obtained a f-score of 0.76, which is approxi-
mately the same as the results reported by Goldwa-
ter et al (2006a) and Goldwater et al (2007). This
suggests that the collocation word adaptor grammar
can capture inter-word dependencies similar to those
that improve the performance of Goldwater?s bigram
segmentation model.
3.5 Collocation morphology adaptor grammar
One of the advantages of working within a gram-
matical framework is that it is often easy to combine
403
Sentence
Colloc
Word
y u
Word
w a n t
Word
t u
Colloc
Word
s i
Word
D 6
Word
b U k
Figure 9: A parse of ?you want to see the book? produced
by the collocation word adaptor grammar of Figure 8.
Sentence ? Colloc+
Colloc ? Word+
Word ? Stem (Suffix)
Stem ? Phoneme+
Suffix ? Phoneme+
Figure 10: The collocation morphology adaptor gram-
mar, which generates each Sentence as a sequence of Col-
loc(ations), each Colloc as a sequence of Words, and each
Word as a Stem optionally followed by a Suffix.
different grammar fragments into a single grammar.
In this section we combine the collocation aspect
of the previous grammar with the morphology com-
ponent of the grammar presented in section 3.2 to
produce a grammar that generates Sentences as se-
quences of Colloc(ations), where each Colloc con-
sists of a sequence of Words, and each Word consists
of a Stem followed by an optional Suffix, as shown
in Figure 10.
This grammar achieves a word segmentation f-
score of 0.73 at ? = 100, which is much better than
the unigram morphology grammar of section 3.2,
but not as good as the collocation word grammar of
the previous section. Inspecting the parses shows
Sentence
Colloc
Word
Stem
y u
Word
Stem
h & v
Suffix
t u
Colloc
Word
Stem
t E l
Suffix
m i
Figure 11: A parse of the phonemic representation of
?you have to tell me? using the collocation morphology
adaptor grammar of Figure 10.
Sentence
Colloc
Word
OnsetI
h
Nucleus
&
CodaF
v
Colloc
Word
Nucleus
6
Word
OnsetI
d r
Nucleus
I
CodaF
N k
Figure 12: A parse of ?have a drink? produced by the col-
location syllable adaptor grammar. (Only adapted non-
root nonterminals are shown in the parse).
that while the ability to directly model collocations
reduces the number of collocations misanalysed as
words, function words still tend to be misanalysed as
morphemes of two-word collocations. In fact, some
of the misanalyses have a certain plausibility to them
(e.g., ?to? is often analysed as the suffix of verbs
such as ?have?, ?want? and ?like?, while ?me? is of-
ten analysed as a suffix of verbs such as ?show? and
?tell?), but they lower the word f-score considerably.
3.6 Collocation syllable adaptor grammar
The collocation syllable adaptor grammar is the
same as the unigram syllable adaptor grammar of
Figure 7, except that the first production is replaced
with the following pair of productions.
Sentence ? Colloc+
Colloc ? Word+
This grammar generates a Sentence as a sequence of
Colloc(ations), each of which is composed of a se-
quence of Words, each of which in turn is composed
of a sequence of Syll(ables).
This grammar achieves a word segmentation f-
score of 0.78 at ? = 100, which is the highest f-
score of any of the grammars investigated in this pa-
per, including the collocation word grammar, which
models collocations but not syllables. To confirm
that the difference is significant, we ran a Wilcoxon
test to compare the f-scores obtained from 8 runs of
the collocation syllable grammar with ? = 100 and
the collocation word grammar with ? = 1000, and
found that the difference is significant at p = 0.006.
4 Conclusion and future work
This paper has shown how adaptor grammars can
be used to study a variety of different linguistic hy-
404
potheses about the interaction of morphology and
syllable structure with word segmentation. Techni-
cally, adaptor grammars are a way of specifying a
variety of Hierarchical Dirichlet Processes (HDPs)
that can spread their support over an unbounded
number of distinct subtrees, giving them the abil-
ity to learn which subtrees are most useful for de-
scribing the training corpus. Thus adaptor gram-
mars move beyond simple parameter estimation and
provide a principled approach to the Bayesian es-
timation of at least some types of linguistic struc-
ture. Because of this, less linguistic structure needs
to be ?built in? to an adaptor grammar compared to a
comparable PCFG. For example, the adaptor gram-
mars for syllable structure presented in sections 3.3
and 3.6 learn more information about syllable onsets
and codas than the PCFGs presented in Goldwater
and Johnson (2005).
We used adaptor grammars to study the effects
of modeling morphological structure, syllabification
and collocations on the accuracy of a standard unsu-
pervised word segmentation task. We showed how
adaptor grammars can implement a previously in-
vestigated model of unsupervised word segmenta-
tion, the unigram word segmentation model. We
then investigated adaptor grammars that incorpo-
rate one additional kind of information, and found
that modeling collocations provides the greatest im-
provement in word segmentation accuracy, result-
ing in a model that seems to capture many of the
same interword dependencies as the bigram model
of Goldwater et al (2006b).
We then investigated grammars that combine
these kinds of information. There does not seem
to be a straight forward way to design an adaptor
grammar that models both morphology and sylla-
ble structure, as morpheme boundaries typically do
not align with syllable boundaries. However, we
showed that an adaptor grammar that models col-
locations and syllable structure performs word seg-
mentation more accurately than an adaptor grammar
that models either collocations or syllable structure
alone. This is not surprising, since syllable onsets
and codas that occur word-peripherally are typically
different to those that appear word-internally, and
our results suggest that by tracking these onsets and
codas, it is possible to learn more accurate word seg-
mentation.
There are a number of interesting directions for
future work. In this paper all of the hyperparame-
ters ?A were tied and varied simultaneously, but it
is desirable to learn these from data as well. Just
before the camera-ready version of this paper was
due we developed a method for estimating the hyper-
parameters by putting a vague Gamma hyper-prior
on each ?A and sampled using Metropolis-Hastings
with a sequence of increasingly narrow Gamma pro-
posal distributions, producing results for each model
that are as good or better than the best ones reported
in Table 1.
The adaptor grammars presented here barely
scratch the surface of the linguistically interesting
models that can be expressed as Hierarchical Dirich-
let Processes. The models of morphology presented
here are particularly naive?they only capture reg-
ular concatenative morphology consisting of one
paradigm class?which may partially explain why
we obtained such poor results using morphology
adaptor grammars. It?s straight forward to design
an adaptor grammar that can capture a finite number
of concatenative paradigm classes (Goldwater et al,
2006b; Johnson et al, 2007a). We?d like to learn the
number of paradigm classes from the data, but do-
ing this would probably require extending adaptor
grammars to incorporate the kind of adaptive state-
splitting found in the iHMM and iPCFG (Liang et
al., 2007). There is no principled reason why this
could not be done, i.e., why one could not design an
HDP framework that simultaneously learns both the
fragments (as in an adaptor grammar) and the states
(as in an iHMM or iPCFG).
However, inference with these more complex
models will probably itself become more complex.
The MCMC sampler of Johnson et al (2007a) used
here is satifactory for small and medium-sized prob-
lems, but it would be very useful to have more ef-
ficient inference procedures. It may be possible to
adapt efficient split-merge samplers (Jain and Neal,
2007) and Variational Bayes methods (Teh et al,
2008) for DPs to adaptor grammars and other lin-
guistic applications of HDPs.
Acknowledgments
This research was funded by NSF awards 0544127
and 0631667.
405
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
Sharon Goldwater and Mark Johnson. 2005. Repre-
sentational bias in unsupervised learning of syllable
structure. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 112?119, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673?680, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239?250, Somerville,
MA. Cascadilla Press.
Sonia Jain and Radford M. Neal. 2007. Splitting and
merging components of a nonconjugate dirichlet pro-
cess mixture model. Bayesian Analysis, 2(3):445?472.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework
for specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman, ed-
itors, Advances in Neural Information Processing Sys-
tems 19, pages 641?648. MIT Press, Cambridge, MA.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Compu-
tational Linguistics, pages 483?501. Oxford Univer-
sity Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of Child
Language, 12:271?296.
Karin Mu?ller. 2001. Automatic detection of syllable
boundaries combining the advantages of treebank and
bracketed corpora training. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics.
Karin Mu?ller. 2002. Probabilistic context-free grammars
for phonology. In Proceedings of the 6th Workshop
of the ACL Special Interest Group in Computational
Phonology (SIGPHON), pages 70?80, Philadelphia.
Andreas Stolcke. 1994. Bayesian Learning of Proba-
bilistic Language Models. Ph.D. thesis, University of
California, Berkeley.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566?1581.
Yee Whye Teh, Kenichi Kurihara, and Max Welling.
2008. Collapsed variational inference for hdp. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors, Ad-
vances in Neural Information Processing Systems 20.
MIT Press, Cambridge, MA.
406
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Note on the Implementation of
Hierarchical Dirichlet Processes
Phil Blunsom
?
pblunsom@inf.ed.ac.uk
Sharon Goldwater
?
sgwater@inf.ed.ac.uk
Trevor Cohn
?
tcohn@inf.ed.ac.uk
Mark Johnson
?
mark johnson@brown.edu
?
Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
Abstract
The implementation of collapsed Gibbs
samplers for non-parametric Bayesian
models is non-trivial, requiring con-
siderable book-keeping. Goldwater et
al. (2006a) presented an approximation
which significantly reduces the storage
and computation overhead, but we show
here that their formulation was incorrect
and, even after correction, is grossly inac-
curate. We present an alternative formula-
tion which is exact and can be computed
easily. However this approach does not
work for hierarchical models, for which
case we present an efficient data structure
which has a better space complexity than
the naive approach.
1 Introduction
Unsupervised learning of natural language is one
of the most challenging areas in NLP. Recently,
methods from nonparametric Bayesian statistics
have been gaining popularity as a way to approach
unsupervised learning for a variety of tasks,
including language modeling, word and mor-
pheme segmentation, parsing, and machine trans-
lation (Teh et al, 2006; Goldwater et al, 2006a;
Goldwater et al, 2006b; Liang et al, 2007; Finkel
et al, 2007; DeNero et al, 2008). These mod-
els are often based on the Dirichlet process (DP)
(Ferguson, 1973) or hierarchical Dirichlet process
(HDP) (Teh et al, 2006), with Gibbs sampling
as a method of inference. Exact implementation
of such sampling methods requires considerable
bookkeeping of various counts, which motivated
Goldwater et al (2006a) (henceforth, GGJ06) to
develop an approximation using expected counts.
However, we show here that their approximation
is flawed in two respects: 1) It omits an impor-
tant factor in the expectation, and 2) Even after
correction, the approximation is poor for hierar-
chical models, which are commonly used for NLP
applications. We derive an improvedO(1) formula
that gives exact values for the expected counts in
non-hierarchical models. For hierarchical models,
where our formula is not exact, we present an
efficient method for sampling from the HDP (and
related models, such as the hierarchical Pitman-
Yor process) that considerably decreases the mem-
ory footprint of such models as compared to the
naive implementation.
As we have noted, the issues described in this
paper apply to models for various kinds of NLP
tasks; for concreteness, we will focus on n-gram
language modeling for the remainder of the paper,
closely following the presentation in GGJ06.
2 The Chinese Restaurant Process
GGJ06 present two nonparametric Bayesian lan-
guage models: a DP unigram model and an HDP
bigram model. Under the DP model, words in a
corpus w = w
1
. . . w
n
are generated as follows:
G|?
0
, P
0
? DP(?
0
, P
0
)
w
i
|G ? G
where G is a distribution over an infinite set of
possible words, P
0
(the base distribution of the
DP) determines the probability that an item will
be in the support of G, and ?
0
(the concentration
parameter) determines the variance of G.
One way of understanding the predictions that
the DP model makes is through the Chinese restau-
rant process (CRP) (Aldous, 1985). In the CRP,
customers (word tokensw
i
) enter a restaurant with
an infinite number of tables and choose a seat. The
table chosen by the ith customer, z
i
, follows the
distribution:
P (z
i
= k|z
?i
) =
{
n
z
?i
k
i?1+?
0
, 0 ? k < K(z
?i
)
?
0
i?1+?
0
, k = K(z
?i
)
337
The
1
meow
4
cats
2
cats 
3
cats
5
a
b
c
d
e f
g
h
Figure 1. A seating assignment describing the state of
a unigram CRP. Letters and numbers uniquely identify
customers and tables. Note that multiple tables may
share a label.
where z
?i
= z
1
. . . z
i?1
are the table assignments
of the previous customers, n
z
?i
k
is the number of
customers at table k in z
?i
, andK(z
?i
) is the total
number of occupied tables. If we further assume
that table k is labeled with a word type `
k
drawn
from P
0
, then the assignment of tokens to tables
defines a distribution over words, with w
i
= `
z
i
.
See Figure 1 for an example seating arrangement.
Using this model, the predictive probability of
w
i
, conditioned on the previous words, can be
found by summing over possible seating assign-
ments for w
i
, and is given by
P (w
i
= w|w
?i
) =
n
w
?i
w
+ ?
0
P
0
i? 1 + ?
0
(1)
This prediction turns out to be exactly that of the
DP model after integrating out the distribution G.
Note that as long as the base distribution P
0
is
fixed, predictions do not depend on the seating
arrangement z
?i
, only on the count of word w
in the previously observed words (n
w
?i
w
). How-
ever, in many situations, we may wish to estimate
the base distribution itself, creating a hierarchical
model. Since the base distribution generates table
labels, estimates of this distribution are based on
the counts of those labels, i.e., the number of tables
associated with each word type.
An example of such a hierarchical model is the
HDP bigram model of GGJ06, in which each word
typew is associated with its own restaurant, where
customers in that restaurant correspond to words
that follow w in the corpus. All the bigram restau-
rants share a common base distribution P
1
over
unigrams, which must be inferred. Predictions in
this model are as follows:
P
2
(w
i
|h
?i
) =
n
h
?i
(w
i?1
,w
i
)
+ ?
1
P
1
(w
i
|h
?i
)
n
h
?i
(w
i?1
,?)
+ ?
1
P
1
(w
i
|h
?i
) =
t
h
?i
w
i
+ ?
0
P
0
(w
i
)
t
h
?i
?
+ ?
0
(2)
where h
?i
= (w
?i
, z
?i
), t
h
?i
w
i
is the number of
tables labelled with w
i
, and t
h
?i
?
is the total num-
ber of occupied tables. Of particular note for our
discussion is that in order to calculate these condi-
tional distributions we must know the table assign-
ments z
?i
for each of the words in w
?i
. Moreover,
in the Gibbs samplers often used for inference in
1         10         100         1000   
0.1
 
  
  
   
1
 
  
  
   
10
 
  
  
   
100
 
 
Me
an 
num
ber
 of 
lex
ica
l en
trie
s
Word frequency (nw)
 
 
Expectation
Antoniak approx.
Empirical, fixed base
Empirical, inferred base
Figure 2. Comparison of several methods of approx-
imating the number of tables occupied by words of
different frequencies. For each method, results using
? = {100, 1000, 10000, 100000} are shown (from bottom
to top). Solid lines show the expected number of tables,
computed using (3) and assuming P
1
is a fixed uni-
form distribution over a finite vocabulary (values com-
puted using the Digamma formulation (7) are the same).
Dashed lines show the values given by the Antoniak
approximation (4) (the line for ? = 100 falls below the
bottom of the graph). Stars show the mean of empirical
table counts as computed over 1000 samples from an
MCMC sampler in which P
1
is a fixed uniform distri-
bution, as in the unigram LM. Circles show the mean
of empirical table counts when P
1
is inferred, as in the
bigram LM. Standard errors in both cases are no larger
than the marker size. All plots are based on the 30114-
word vocabulary and frequencies found in sections 0-20
of the WSJ corpus.
these kinds of models, the counts are constantly
changing over multiple samples, with tables going
in and out of existence frequently. This can create
significant bookkeeping issues in implementation,
and motivated GGJ06 to present a method of com-
puting approximate table counts based on word
frequencies only.
3 Approximating Table Counts
Rather than explicitly tracking the number of
tables t
w
associated with each word w in their
bigram model, GGJ06 approximate the table
counts using the expectation E[t
w
]. Expected
counts are used in place of t
h
?i
w
i
and t
h
?i
?
in (2).
The exact expectation, due to Antoniak (1974), is
E[t
w
] = ?
1
P
1
(w)
n
w
?
i=1
1
?
1
P
1
(w) + i? 1
(3)
338
Antoniak also gives an approximation to this
expectation:
E[t
w
] ? ?
1
P
1
(w) log
n
w
+ ?
1
P
1
(w)
?
1
P
1
(w)
(4)
but provides no derivation. Due to a misinterpre-
tation of Antoniak (1974), GGJ06 use an approx-
imation that leaves out all the P
1
(w) terms from
(4).
1
Figure 2 compares the approximation to
the exact expectation when the base distribution
is fixed. The approximation is fairly good when
?P
1
(w) > 1 (the scenario assumed by Antoniak);
however, in most NLP applications, ?P
1
(w) <
1 in order to effect a sparse prior. (We return
to the case of non-fixed based distributions in a
moment.) As an extreme case of the paucity of
this approximation consider ?
1
P
1
(w) = 1 and
n
w
= 1 (i.e. only one customer has entered the
restaurant): clearly E[t
w
] should equal 1, but the
approximation gives log(2).
We now provide a derivation for (4), which will
allow us to obtain an O(1) formula for the expec-
tation in (3). First, we rewrite the summation in (3)
as a difference of fractional harmonic numbers:
2
H
(?
1
P
1
(w)+n
w
?1)
?H
(?
1
P
1
(w)?1)
(5)
Using the recurrence for harmonic numbers:
E[t
w
] ? ?
1
P
1
(w)
[
H
(?
1
P
1
(w)+n
w
)
?
1
?
1
P
1
(w) + n
w
?H
(?
1
P
1
(w)+n
w
)
+
1
?
1
P
1
(w)
]
(6)
We then use the asymptotic expansion,
H
F
? logF + ? +
1
2F
, omiting trailing terms
which are O(F
?2
) and smaller powers of F :
3
E[t
w
] ? ?
1
P
1
(w) log
n
w
+?
1
P
1
(w)
?
1
P
1
(w)
+
n
w
2(?
1
P
1
(w)+n
w
)
Omitting the trailing term leads to the
approximation in Antoniak (1974). However, we
can obtain an exact formula for the expecta-
tion by utilising the relationship between the
Digamma function and the harmonic numbers:
?(n) = H
n?1
? ?.
4
Thus we can rewrite (5) as:
5
E[t
w
] = ?
1
P
1
(w)?
[
?(?
1
P
1
(w) + n
w
)? ?(?
1
P
1
(w))
]
(7)
1
The authors of GGJ06 realized this error, and current
implementations of their models no longer use these approx-
imations, instead tracking table counts explicitly.
2
Fractional harmonic numbers between 0 and 1 are given
by H
F
=
R
1
0
1?x
F
1?x
dx. All harmonic numbers follow the
recurrence H
F
= H
F?1
+
1
F
.
3
Here, ? is the Euler-Mascheroni constant.
4
AccurateO(1) approximations of the Digamma function
are readily available.
5
(7) can be derived from (3) using: ?(x+1)??(x) =
1
x
.
Explicit table tracking:
customer(w
i
)? table(z
i
)
n
a : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5
o
table(z
i
)? label(`)
n
1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : cats
o
Histogram:
word type?
{
table occupancy? frequency
}
n
The : {2 : 1}, cats : {1 : 1, 2 : 2}, meow : {1 : 1}
o
Figure 3. The explicit table tracking and histogram rep-
resentations for Figure 1.
A significant caveat here is that the expected
table counts given by (3) and (7) are only valid
when the base distribution is a constant. However,
in hierarchical models such as GGJ06?s bigram
model and HDP models, the base distribution is
not constant and instead must be inferred. As can
be seen in Figure 2, table counts can diverge con-
siderably from the expectations based on fixed
P
1
when P
1
is in fact not fixed. Thus, (7) can
be viewed as an approximation in this case, but
not necessarily an accurate one. Since knowing
the table counts is only necessary for inference
in hierarchical models, but the table counts can-
not be approximated well by any of the formu-
las presented here, we must conclude that the best
inference method is still to keep track of the actual
table counts. The naive method of doing so is to
store which table each customer in the restaurant
is seated at, incrementing and decrementing these
counts as needed during the sampling process. In
the following section, we describe an alternative
method that reduces the amount of memory neces-
sary for implementing HDPs. This method is also
appropriate for hierarchical Pitman-Yor processes,
for which no closed-form approximations to the
table counts have been proposed.
4 Efficient Implementation of HDPs
As we do not have an efficient expected table
count approximation for hierarchical models we
could fall back to explicitly tracking which table
each customer that enters the restaurant sits at.
However, here we describe a more compact repre-
sentation for the state of the restaurant that doesn?t
require explicit table tracking.
6
Instead we main-
tain a histogram for each dish w
i
of the frequency
of a table having a particular number of customers.
Figure 3 depicts the histogram and explicit repre-
sentations for the CRP state in Figure 1.
Our alternative method of inference for hierar-
chical Bayesian models takes advantage of their
6
Teh et al (2006) also note that the exact table assign-
ments for customers are not required for prediction.
339
Algorithm 1 A new customer enters the restaurant
1: w: word type
2: P
w
0
: Base probability for w
3: HD
w
: Seating Histogram for w
4: procedure INCREMENT(w,P
w
0
,HD
w
)
5: p
share
?
n
w
?1
w
n
w
?1
w
+?
0
. share an existing table
6: p
new
?
?
0
?P
w
0
n
w
?1
w
+?
0
. open a new table
7: r ? random(0, p
share
+ p
new
)
8: if r < p
new
or n
w
?1
w
= 0 then
9: HD
w
[1] = HD
w
[1] + 1
10: else
. Sample from the histogram of customers at tables
11: r ? random(0, n
w
?1
w
)
12: for c ? HD
w
do . c: customer count
13: r = r ? (c? HD
w
[c])
14: if r ? 0 then
15: HD
w
[c] = HD
w
[c] + 1
16: Break
17: n
w
w
= n
w
?1
w
+ 1 . Update token count
Algorithm 2 A customer leaves the restaurant
1: w: word type
2: HD
w
: Seating histogram for w
3: procedure DECREMENT(w,P
w
0
,HD
w
)
4: r ? random(0, n
w
w
)
5: for c ? HD
w
do . c: customer count
6: r = r ? (c? HD
w
[c])
7: if r ? 0 then
8: HD
w
[c] = HD
w
[c]? 1
9: if c > 1 then
10: HD
w
[c? 1] = HD
w
[c? 1] + 1
11: Break
12: n
w
w
= n
w
w
? 1 . Update token count
exchangeability, which makes it unnecessary to
know exactly which table each customer is seated
at. The only important information is how many
tables exist with different numbers of customers,
and what their labels are. We simply maintain a
histogram for each word type w, which stores, for
each number of customersm, the number of tables
labeled with w that have m customers. Figure 3
depicts the explicit representation and histogram
for the CRP state in Figure 1.
Algorithms 1 and 2 describe the two operations
required to maintain the state of a CRP.
7
When
a customer enters the restaurant (Alogrithm 1)),
we sample whether or not to open a new table.
If not, we sample an old table proportional to the
counts of how many customers are seated there
and update the histogram. When a customer leaves
the restaurant (Algorithm 2), we decrement one
of the tables at random according to the number
of customers seated there. By exchangeability, it
doesn?t actually matter which table the customer
was ?really? sitting at.
7
A C++ template class that implements
the algorithm presented is made available at:
http://homepages.inf.ed.ac.uk/tcohn/
5 Conclusion
We?ve shown that the HDP approximation pre-
sented in GGJ06 contained errors and inappropri-
ate assumptions such that it significantly diverges
from the true expectations for the most common
scenarios encountered in NLP. As such we empha-
sise that that formulation should not be used.
Although (7) allowsE[t
w
] to be calculated exactly
for constant base distributions, for hierarchical
models this is not valid and no accurate calculation
of the expectations has been proposed. As a rem-
edy we?ve presented an algorithm that efficiently
implements the true HDP without the need for
explicitly tracking customer to table assignments,
while remaining simple to implement.
Acknowledgements
The authors would like to thank Tom Grif-
fiths for providing the code used to produce
Figure 2 and acknowledge the support of the
EPSRC (Blunsom, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
References
D. Aldous. 1985. Exchangeability and related topics. In
?
Ecole d?
?
Et?e de Probabiliti?es de Saint-Flour XIII 1983, 1?
198. Springer.
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite
tree. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating
between types and tokens by estimating power-law gener-
ators. In Y. Weiss, B. Sch?olkopf, J. Platt, eds., Advances
in Neural Information Processing Systems 18, 459?466.
MIT Press, Cambridge, MA.
P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite
PCFG using hierarchical Dirichlet processes. In Proc. of
the 2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2007), 688?697, Prague,
Czech Republic.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
340
Parsing and Disuency Placement
Donald Engel
y
and Eugene Charniak
z
and Mark Johnson
z
Department of Physics, University of Pennsylvania
y
Brown Laboratory for Linguistic Information Processing
z
Brown University
Abstract
It has been suggested that some forms of speech
disuencies, most notable interjections and par-
entheticals, tend to occur disproportionally at
major clause boundaries [6] and thus might
serve to aid parsers in establishing these bound-
aries. We have tested a current statistical parser
[1] on Switchboard text with and without inter-
jections and parentheticals and found that the
parser performed better when not faced with
these extra phenomena. This suggest that for
current parsers, at least, interjection and paren-
thetical placement does not help in the parsing
process.
1 Introduction
It is generally recognized that punctuation
helps in parsing text. For example, Roark [5]
nds that removing punctuation decreases his
parser's accuracy from 86.6% to 83.8%. Our
experiments with the parser described in [1]
show a similar fallo. Unfortunately spoken
English does not come with punctuation, and
even when transcriptions add punctuation, as in
the Switchboard [4] corpus of transcribed (and
parsed) telephone calls, it's utility is small [5]
For this and other reasons there is considerable
interest in nding other aspects of speech that
might serve as a replacement.
One suggestion in this vein is that the place-
ment of some forms of speech errors might
encode useful linguistic information. Speech,
of course, contains many kinds of errors that
can make it more di?cult to parse than text.
Roughly speaking the previously mentioned
Switchboard corpus distinguishes three kinds of
errors:
 interjections (lled pauses) | \I, um, want
to leave"
 parentheticals | \I, you know, want to
leave"
 speech repairs | \I can, I want to leave"
Of these, speech repairs are the most injurious
to parsing. Furthermore, even if one's parser
can parse the sentence as it stands, that is not
su?cient. For example, in \I can, I want to
leave", it is not necessarily the case that the
speaker believes that he or she can, in fact,
leave, only that he or she wants to leave. Thus
in [2] speech repairs were rst detected in a sep-
arate module, and deleted before handing the
remaining text to the parser. The parser then
produced a parse of the text without the re-
paired section.
The other two kinds of errors, interjec-
tions, and parentheticals, (henceforth INTJs
and PRNs) are less problematic. In particular,
if they are left in the text either their seman-
tic content is compatible with the rest of the
utterance or there is no semantic content at all.
For example, Table 1 gives the 40 most common
INTJs, which comprise 97% of the total. (Un-
listed INTJs comprise the remaining 3%.) They
are easily recognized as not carrying much, if
any, content.
PRNs are more diverse. Table 2 lists the 40
most common PRNs. They only comprise 65%
of all cases, and many do contain semantics
content. In such cases, however, the semantic
content is compatible with the rest of the sen-
tence, so leaving them in is perfectly acceptable.
Thus [2], while endeavoring to detect and re-
move speech repairs, left interjections and par-
entheticals in the text for the parser to cope
with.
Indeed [6] nds that both interjections and
parentheticals tend to occur at major sentence
boundaries. Also [7] suggest that this prop-
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 49-54.
                         Proceedings of the Conference on Empirical Methods in Natural
Phrase Num. of Percent
INTJs
uh 17609 27.44
yeah 11310 17.62
uh-huh 7687 11.97
well 5287 8.238
um 3563 5.552
oh 2935 4.573
right 2873 4.477
like 1772 2.761
no 1246 1.941
okay 1237 1.927
yes 982 1.530
so 651 1.014
oh yeah 638 0.994
huh 558 0.869
now 410 0.638
really 279 0.434
sure 276 0.430
oh okay 269 0.419
see 261 0.406
oh really 260 0.405
huh-uh 185 0.288
wow 174 0.271
bye-bye 174 0.271
exactly 156 0.243
all right 146 0.227
yep 115 0.179
boy 111 0.172
oh no 102 0.158
bye 98 0.152
well yeah 91 0.141
gosh 91 0.141
oh gosh 88 0.137
oh yes 84 0.130
hey 75 0.116
uh yeah 71 0.110
anyway 71 0.110
oh uh-huh 70 0.109
say 63 0.098
oh goodness 61 0.095
uh no 56 0.087
Table 1: The 40 Most Common Interjections
Phrase Num. of Percent
PRNs
you know 431 37.02
I mean 105 9.020
I think 86 7.388
I guess 67 5.756
You know 44 3.780
I don't know 38 3.264
let's see 11 0.945
I I mean 10 0.859
I 'd say 9 0.773
I 'm sure 7 0.601
excuse me 6 0.515
what is it 6 0.515
I would say 5 0.429
you you know 5 0.429
let 's say 5 0.429
I think it 's 4 0.343
I 'm sorry 4 0.343
so to speak 3 0.257
I guess it 's 3 0.257
I don't think 3 0.257
I think it was 3 0.257
I would think 3 0.257
it seems 3 0.257
I guess it was 2 0.171
I know 2 0.171
I I I mean 2 0.171
seems like 2 0.171
Shall we say 2 0.171
I guess you could say 2 0.171
You're right 2 0.171
I believe 2 0.171
I think it was uh 2 0.171
I say 2 0.171
What I call 2 0.171
I don't know what part of
New Jersey you're in but 2 0.171
I should say 2 0.171
I guess not a sore thumb 1 0.085
I 'm trying to think 1 0.085
And it's hard to drag
her away 1 0.085
I don't know what you
call that 1 0.085
Table 2: The 40 Most Common Parentheticals
erty accounts for their observation that remov-
ing these disuencies does not help in language
modeling perplexity results. This strongly sug-
gests that INTJ/PRN location information in
speech text might in fact, improve parsing per-
formance by helping the parser locate con-
stituent boundaries with high accuracy. That is,
a statistic parser such as [1] or [3] when trained
on parsed Switchboard text with these phenom-
ena left in, might learn the statistical correla-
tions between them and phrase boundaries just
as they are obviously learning the correlations
between punctuation and phrase boundaries in
written text.
In this paper then we wish to determine if the
presence of INTJs and PRNs do help parsing, at
least for one state-of-the-art statistical parser
[1].
2 Experimental Design
The experimental design used was more com-
plicated than we initially expected. We had an-
ticipated that the experiments would be con-
ducted analogously to the \no punctuation" ex-
periments previously mentioned. In those ex-
periments one removes punctuation from all of
the corpus sentences, both for testing and train-
ing, and then one reports the results before and
after this removal. (Note that one must remove
punctuation from the training data as well so
that it looks like the non-punctuated testing
data it receives.) Parsing accuracy was mea-
sured in the usual way, using labeled precision
recall. Note, however, and this is a critical
point, that precision and recall are only mea-
sured on non-preterminal constituents. That is,
if we have a constituent
(PP (IN of)
(NP (DT the) (NN book)))
our measurements would note if we correctly
found the PP and the NP, but not the preter-
minals IN, DT, and NN. The logic of this is
to avoid confusing parsing results with part-of-
speech tagging, a much simpler problem.
Initially we conducted similarly designed ex-
periments, except rather than removing punc-
tuation, we removed INTJs and PRNs and com-
pared before and after precision/recall numbers.
These numbers seemed to conrm the antici-
pated results: the \after" numbers, the numbers
without INTJ/PRNs were signicantly worse,
suggesting that the presence of INTJ/PRNs
helped the parser.
Unfortunately, although ne for punctuation,
this experimental design is not su?cient for
measuring the eects of INTJ/PRNs on parsing.
The dierence is that punctuation itself is not
measured in the precision-recall numbers. That
is, if we had a phrase like
(NP (NP (DT a) (NN sentence))
(, ,)
(ADJP (JJ like)
(NP (DT this) (DT one))))
we would measure our accuracy on the three
NP's and the ADJP, but not on the pretermi-
nals, and it is only at the preterminal level that
punctuation appears.
The same cannot be said for INTJ/PRNs.
Consider the (slightly simplied) Switchboard
parse for a sentence like \I, you know, want to
leave":
(S (NP I)
(PRN , you know ,)
(VP want (S to leave)))
The parenthetical PRN is a full non-terminal
and thus is counted in precision/recall measure-
ments. Thus removing preterminals is chang-
ing what we wish to measure. In particu-
lar, when our initial results showed that re-
moval of INTJ/PRNs lowered precision/recall we
worried that it might be that INTJ/PRNs are
particularly easy to parse, and thus removing
them made things worse, not because of col-
lateral damage on our ability to parse other
constituents, but simply because we removed
a body of easily parseable constituents, leaving
the more di?cult constituents to be measured.
The above tables of INTJs and PRNs lends cre-
dence to this concern.
Thus in the experiments below all measure-
ments are obtained in the following fashion:
1. The parser is trained on switchboard data
with/without INTJ/PRNs or punctuation,
creating eight congurations: 4 for neither,
both, just INTJs, and just PRNs, times
two for with and without punctuation. We
tested with and without punctuation to
conrm Roark's earlier results showing that
they have little inuence in Switchboard
text.
2. The parser reads the gold standard testing
examples and depending on the congura-
tion INTJs and/or PRNS are removed from
the gold standard parse.
3. Finally the resulting parse is compared
with the gold standard. However, any re-
maining PRNs or INTJs are ignored when
computing the precision and recall statis-
tics for the parse.
To expand a bit on point (3) above, for an
experiment where we are parsing with INTJs,
but not PRNs, the resulting parse will, of course,
contain INTJs, but (a) they are not counted as
present in the gold standard (so we do not aect
recall statistics), and (b) they are not evaluated
in the guessed parse (so if one were labeled, say,
an S, it would not be counted against the parse).
The intent, again, is to not allow the results to
be inuenced by the fact that interjections and
parentheticals are much easier to nd than most
(if not) all other kinds of constituents.
3 Experimental Results
As in [2] the Switchboard parsed/merged cor-
pus directories two and three were used for
training. In directory four, les sw4004.mrg
to sw4153.mrg were used for testing, and
sw4519.mrg to sw4936 for development. To
avoid confounding the results with problems of
edit detection, all edited nodes were deleted
from the gold standard parses.
The results of the experiment are given in
table 3. We have shown results separately
with and without punctuation. A quick look
at the data indicates that both sets show the
same trends but with punctuation helping per-
formance by about 1.0% absolute in both pre-
cision and recall. Within both groups, as is al-
ways the case, we see that the parser does better
when restricted to shorter sentences (40 words
and punctuation or less). We see that removing
PRNs or INTJs separately both improve parsing
accuracy (e.g., from 87.201% to 87.845|that
the eect of removing both is approximately
additive (e.g., from 87201% to 88.863%, again
on the with-punctuation data). Both with and
without punctuation results hint that removing
Punc. PRN INTJ Sentences Sentences
 40  100
+ + + 88.93 87.20
+ + - 89.44 87.85
+ - + 89.13 87.99
+ - - 90.00 88.86
- + + 87.40 86.23
- + - 88.0 86.8
- - + 88.41 87.45
- - - 89.13 88.30
Table 3: Average of labeled precision/recall
data for parsing with/without parentheti-
cals/interjections
parentheticals was usually more helpful than re-
moving interjections. However in one case the
reverse was true (with-punctuation, sentences
 40) and in all cases the dierences are at or
under the edge of statistical reliability. In con-
trast, the dierences between removing neither,
removing one, or removing both INJs and PRNs
are quite comfortably statistically reliable.
4 Discussion
Based upon Tabel 3 our tentative conclusion is
that the information present in parentheticals
and interjections does not help parsing. There
are, however, reasons that this is a tentative con-
clusion.
First, in our eort to prevent the ease of
recognizing these constructions from giving an
unfair advantage to the parser when they are
present, it could be argued that we have given
the parser an unfair advantage when they are
absent. After all, even if these constructions are
easily recognized, the parser is not perfect on
them. While our labeled precision/recall mea-
surements are made in such a way that a mis-
take in the label of, say, an interjection, would
not eect the results, a mistake on it's position
typically would have an eect because the po-
sitions of constituents either before or after it
would be made incorrect. Thus the parser has
a harder task set for it when these constituents
are left in.
It would be preferable to have an experimen-
tal design that would somehow equalize things,
but we have been unable to nd one. Fur-
thermore it is instructive to contrast this situ-
ation with that of punctuation in Wall Street
Journal text. If we had found that parsing
without punctuation made things easier a sim-
ilar argument could be made that the without-
punctuation case was given an unfair advantage
since it had a lot fewer things to worry about.
But punctuation in well-edited text contains
more than enough information to overcome the
disadvantage. This does not seem to be the case
with INTJs and PRNs. Here the net information
content here seems to be negative.
A second, and in our estimation more serious,
objection to our conclusion is that we have only
done the experiment with one parser. Perhaps
there is something specic to this parser that
systematically underestimates the usefulness of
INTJ/PRN information. While we feel reason-
ably condent that any other current parser
would nd similar eects, it is at least possi-
ble to imagine that quite dierent parsers might
not. Statistical parsers condition the probabil-
ity of a constituent on the types of neighbor-
ing constituents. Interjections and parenthet-
icals have the eect of increasing the kinds of
neighbors one might have, thus splitting the
data and making it less reliable. The same is
true for punctuation, of course, but it seems
plausible that well edited punctuation is su?-
ciently regular that this problem is not too bad,
while spontaneous interjections and parentheti-
cals would not be so regular. Of course, nding
a parser design that might overcome this prob-
lem (assuming that this is the problem) is far
from obvious.
5 Conclusion
We have tested a current statistical parser [1] on
Switchboard text with and without interjections
and parentheticals and found that the parser
performs better when not faced with these ex-
tra phenomena. This suggest that for current
parsers, at least, interjection and parenthetical
placement does not help in the parsing process.
This is, of course, a disappointing result. The
phenomena are not going to go away, and what
this means is that there is probably no silver
lining.
We should also note that the idea that they
might help parsing grew from the observation
that interjections and parentheticals typically
occur at major clause boundaries. One might
then ask if our results cast any doubt on this
claim as well. We do not think so. Interjections
and parentheticals do tend to identify clause
boundaries. The problem is that many other
things do so as well, most notably normal gram-
matical word ordering. The question is whether
the information content of disuency placement
is su?cient to overcome the disruption of word
ordering that it entails. The answer, for current
parsers at least, seems to be "no".
6 Acknowledgements
We would like to acknowledge the members of
the Brown Laboratory for Linguistic Informa-
tion Processing, This research has been sup-
ported in part by NSF grants IIS 0085940, IIS
0112435, and DGE 9870676.
References
1. Charniak, E. A maximum-entropy-
inspired parser. In Proceedings of the 2000
Conference of the North American Chapter
of the Association for Computational Lin-
guistics. ACL, New Brunswick NJ, 2000,
132{139.
2. Charniak, E. and Johnson, M. Edit De-
tection and Parsing for Transcribed Speech.
In Proceedings of the North American As-
socation for Computational Linguistics 2001.
2001, 118{126.
3. Collins, M. J. Three generative lexical-
ized models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL. 1997, 16{23.
4. Godfrey, J. J., Holliman, E. C. and
McDaniel, J. SWITCHBOARD: Tele-
phone speech corpus for research and
development. . In Proceedings IEEE Con-
ference on Acoustics, Speech and Signal
Processing . San Francisco, 1992, 517{520 .
5. Roark, B. Robust Probabilistic Predictive
Syntactic Processing: Motivations, Models,
and Applications. In Ph.D. thesis. Depart-
ment of Cognitive Science, Brown University,
Providence, RI, 2001.
6. Shriberg, E. E. Preliminaries to a The-
ory of Speech Disuencies. In Ph.D. Disser-
tation. Department of Psychology, University
of California-Berkeley, 1994.
7. Stolcke, A. and Shriberg, E. Auto-
matic linguistic segmantation of conversa-
tional speech. In Proceedings of the 4th In-
ternational Conference on Spoken Language
Processing (ICSLP-96). 1996.
Investigating Loss Functions and Optimization Methods for Discriminative
Learning of Label Sequences
Yasemin Altun
Computer Science
Brown University
Providence, RI 02912
altun@cs.brown.edu
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
Mark Johnson@brown.edu
Thomas Hofmann
Computer Science
Brown University
Providence, RI 02912
th@cs.brown.edu
Abstract
Discriminative models have been of inter-
est in the NLP community in recent years.
Previous research has shown that they
are advantageous over generative mod-
els. In this paper, we investigate how dif-
ferent objective functions and optimiza-
tion methods affect the performance of the
classifiers in the discriminative learning
framework. We focus on the sequence la-
belling problem, particularly POS tagging
and NER tasks. Our experiments show
that changing the objective function is not
as effective as changing the features in-
cluded in the model.
1 Introduction
Until recent years, generative models were the most
common approach for many NLP tasks. Recently,
there is a growing interest on discriminative mod-
els in the NLP community, and these models were
shown to be successful for different tasks(Lafferty
et al, 2001; Ratnaparkhi, 1999; Collins, 2000). Dis-
criminative models do not only have theoretical ad-
vantages over generative models, as we discuss in
Section 2, but they are also shown to be empirically
favorable over generative models when features and
objective functions are fixed (Klein and Manning,
2002).
In this paper, we use discriminative models to
investigate the optimization of different objective
functions by a variety of optimization methods. We
focus on label sequence learning tasks. Part-of-
Speech (POS) tagging and Named Entity Recogni-
tion (NER) are the most studied applications among
these tasks. However, there are many others, such
as chunking, pitch accent prediction and speech edit
detection. These tasks differ in many aspects, such
as the nature of the label sequences (chunks or indi-
vidual labels), their difficulty and evaluation meth-
ods. Given this variety, we think it is worthwhile to
investigate how optimizing different objective func-
tions affects performance. In this paper, we varied
the scale (exponential vs logarithmic) and the man-
ner of the optimization (sequential vs pointwise) and
using different combinations, we designed 4 differ-
ent objective functions. We optimized these func-
tions on NER and POS tagging tasks. Despite our
intuitions, our experiments show that optimizing ob-
jective functions that vary in scale and manner do
not affect accuracy much. Instead, the selection of
the features has a larger impact.
The choice of the optimization method is impor-
tant for many learning problems. We would like
to use optimization methods that can handle a large
number of features, converge fast and return sparse
classifiers. The importance of the features, and
therefore the importance of the ability to cope with
a larger number of features is well-known. Since
training discriminative models over large corpora
can be expensive, an optimization method that con-
verges fast might be advantageous over others. A
sparse classifier has a shorter test time than a denser
classifier. For applications in which the test time is
crucial, optimization methods that result in sparser
classifiers might be preferable over other methods
   
   
   
  
  
  



  
  
  



  
  
  
x(t+1)x(t)x(t?1) x(t+1)x(t)x(t?1)
y(t+1)y(t)y(t?1) y(t+1)y(t)y(t?1)
a) HMM b)CRF
Figure 1: Graphical representation of HMMs and
CRFs. Shaded areas indicate variables that the
model conditions on.
even if their training time is longer. In this paper we
investigate these aspects for different optimization
methods, i.e. the number of features, training time
and sparseness, as well as the accuracy. In some
cases, an approximate optimization that is more ef-
ficient in one of these aspects might be preferable to
the exact method, if they have similar accuracy. We
experiment with exact versus approximate as well
as parallel versus sequential optimization methods.
For the exact methods, we use an off-the-shelf gradi-
ent based optimization routine. For the approximate
methods, we use a perceptron and a boosting algo-
rithm for sequence labelling which update the fea-
ture weights parallel and sequentially respectively.
2 Discriminative Modeling of Label
Sequences Learning
Label sequence learning is, formally, the problem
of learning a function that maps a sequence of ob-
servations 	

Supersense Tagging of Unknown Nouns in WordNet  
Massimiliano Ciaramita
Brown University
massi@brown.edu
Mark Johnson
Brown University
mark johnson@brown.edu
Abstract
We present a new framework for classify-
ing common nouns that extends named-
entity classification. We used a fixed set
of 26 semantic labels, which we called su-
persenses. These are the labels used by
lexicographers developing WordNet. This
framework has a number of practical ad-
vantages. We show how information con-
tained in the dictionary can be used as ad-
ditional training data that improves accu-
racy in learning new nouns. We also de-
fine a more realistic evaluation procedure
than cross-validation.
1 Introduction
Lexical semantic information is useful in many nat-
ural language processing and information retrieval
applications, particularly tasks that require com-
plex inferences involving world knowledge, such
as question answering or the identification of co-
referential entities (Pasca and Harabagiu, 2001;
Pustejovsky et al, 2002).
However, even large lexical databases such as
WordNet (Fellbaum, 1998) do not include all of
the words encountered in broad-coverage NLP ap-
plications. Ideally, we would like a system that
automatically extends existing lexical resources by

We would like to thank Thomas Hofmann, Brian Roark,
and our colleagues in the Brown Laboratory for Linguistic In-
formation Processing (BLLIP), as well as Jesse Hochstadt for
his editing advice. This material is based upon work supported
by the National Science Foundation under Grant No. 0085940.
identifying the syntactic and semantic properties of
unknown words. In terms of the WordNet lexical
database, one would like to automatically assign un-
known words a position in the synset hierarchy, in-
troducing new synsets and extending the synset hier-
archy where appropriate. Doing this accurately is a
difficult problem, and in this paper we address a sim-
pler problem: automatically determining the broad
semantic class, or supersense, to which unknown
words belong.
Systems for thesaurus extension (Hearst, 1992;
Roark and Charniak, 1998), information extrac-
tion (Riloff and Jones, 1999) or named-entity recog-
nition (Collins and Singer, 1999) each partially ad-
dress this problem in different ways. The goal
in these tasks is automatically tagging words with
semantic labels such as ?vehicle?, ?organization?,
?person?, etc.
In this paper we extend the named-entity recogni-
tion approach to the classification of common nouns
into 26 different supersenses. Rather than define
these ourselves, we adopted the 26 ?lexicographer
class? labels used in WordNet, which include labels
such as person, location, event, quantity, etc. We be-
lieve our general approach should generalize to other
definitions of supersenses.
Using the WordNet lexicographer classes as su-
persenses has a number of practical advantages.
First, we show how information contained in the dic-
tionary can be used as additional training data that
improves the system?s accuracy. Secondly, it is pos-
sible to use a very natural evaluation procedure. A
system can be trained on an earlier release of Word-
Net and tested on the words added in a later release,
1 person 7 cognition 13 attribute 19 quantity 25 plant
2 communication 8 possession 14 object 20 motive 26 relation
3 artifact 9 location 15 process 21 animal
4 act 10 substance 16 Tops 22 body
5 group 11 state 17 phenomenon 23 feeling
6 food 12 time 18 event 24 shape
Table 1. Lexicographer class labels, or supersenses.
since these labels are constant across different re-
leases. This new evaluation defines a realistic lexi-
cal acquisition task which is well defined, well mo-
tivated and easily standardizable.
The heart of our system is a multiclass perceptron
classifier (Crammer and Singer, 2002). The features
used are the standard ones used in word-sense classi-
fication and named-entity extraction tasks, i.e., col-
location, spelling and syntactic context features.
The experiments presented below show that when
the classifier also uses the data contained in the dic-
tionary its accuracy improves over that of a tradition-
ally trained classifier. Finally, we show that there are
both similarities and differences in the results ob-
tained with the new evaluation and standard cross-
validation. This might suggest that in fact that the
new evaluation defines a more realistic task.
The paper is organized as follows. In Section 2
we discuss the problem of unknown words and the
task of semantic classification. In Section 3 we de-
scribe the WordNet lexicographer classes, how to
extract training data from WordNet, the new evalu-
ation method and the relation of this task to named-
entity classification. In Section 4 we describe the
experimental setup, and in Section 5 we explain the
averaged perceptron classifier used. In Section 6 and
7 we discuss the results and the two evaluations.
2 Unknown Words and Semantic
Classification
Language processing systems make use of ?dictio-
naries?, i.e., lists that associate words with useful
information such as the word?s frequency or syn-
tactic category. In tasks that also involve inferences
about world knowledge, it is useful to know some-
thing about the meaning of the word. This lexical
semantic information is often modeled on what is
found in normal dictionaries, e.g., that ?irises? are
flowers or that ?exane? is a solvent.
This information can be crucial in tasks such
as question answering - e.g., to answer a ques-
tion such as ?What kind of flowers did Van Gogh
paint?? (Pasca and Harabagiu, 2001) - or the indi-
viduation of co-referential expressions, as in the pas-
sage ?... the prerun can be performed with 	

... this 

 
 can be considered ...? (Pustejovsky
et al, 2002).
Lexical semantic information can be extracted
from existing dictionaries such as WordNet. How-
ever, these resources are incomplete and systems
that rely on them often encounter unknown words,
even if the dictionary is large. As an example, in the
Bllip corpus (a very large corpus of Wall Street Jour-
nal text) the relative frequency of common nouns
that are unknown to WordNet 1.6 is approximately
0.0054; an unknown noun occurs, on average, ev-
ery eight sentences. WordNet 1.6 lists 95,000 noun
types. For this reason the importance of issues such
as automatically building, extending or customizing
lexical resources has been recognized for some time
in computational linguistics (Zernik, 1991).
Solutions to this problem were first proposed
in AI in the context of story understanding, cf.
(Granger, 1977). The goal is to label words using
a set of semantic labels specified by the dictionary.
Several studies have addressed the problem of ex-
panding one semantic category at a time, such as
?vehicle? or ?organization?, that are relevant to a
particular task (Hearst, 1992; Roark and Charniak,
1998; Riloff and Jones, 1999). In named-entity clas-
sification a large set of named entities (proper nouns)
are classified using a comprehensive set of semantic
labels such as ?organization?, ?person?, ?location?
or ?other? (Collins and Singer, 1999). This latter
approach assigns all named entities in the data set a
semantic label. We extend this approach to the clas-
sification of common nouns using a suitable set of
semantic classes.
3 Lexicographer Classes for Noun
Classification
3.1 WordNet Lexicographer Labels
WordNet (Fellbaum, 1998) is a broad-coverage
machine-readable dictionary. Release 1.71 of the
English version lists about 150,000 entries for all
open-class words, mostly nouns (109,000 types), but
also verbs, adjectives, and adverbs. WordNet is or-
ganized as a network of lexicalized concepts, sets of
synonyms called synsets; e.g., the nouns  chairman,
chairwoman, chair, chairperson  form a synset. A
word that belongs to several synsets is ambiguous.
To facilitate the development of WordNet, lexi-
cographers organize synsets into several domains,
based on syntactic category and semantic coherence.
Each noun synset is assigned one out of 26 broad
categories1. Since these broad categories group to-
gether very many synsets, i.e., word senses, we call
them supersenses. The supersense labels that Word-
Net lexicographers use to organize nouns are listed
in Table 12. Notice that since the lexicographer la-
bels are assigned to synsets, often ambiguity is pre-
served even at this level. For example, chair has
three supersenses: ?person?, ?artifact?, and ?act?.
This set of labels has a number of attractive fea-
tures for the purposes of lexical acquisition. It is
fairly general and therefore small. The reasonable
size of the label set makes it possible to apply state-
of-the-art machine learning methods. Otherwise,
classifying new words at the synset level defines a
multiclass problem with a huge class space - more
than 66,000 noun synsets in WordNet 1.6, more than
75,000 in the newest release, 1.71 (cf. also (Cia-
ramita, 2002) on this problem). At the same time
the labels are not too abstract or vague. Most of the
classes seem natural and easily recognizable. That
is probably why they were chosen by the lexicog-
raphers to facilitate their task. But there are more
important practical and methodological advantages.
3.2 Extra Training Data from WordNet
WordNet contains a great deal of information about
words and word senses.The information contained
1There are also 15 lexicographer classes for verbs, 3 for ad-
jectives and 1 for adverbs.
2The label ?Tops? refers to about 40 very general synsets,
such as ?phenomenon? ?entity? ?object? etc.
in the dictionary?s glosses is very similar to what
is typically listed in normal dictionaries: synonyms,
definitions and example sentences. This suggests a
very simple way in which it can be put into use: it
can be compiled into training data for supersense la-
bels. This data can then be added to the data ex-
tracted from the training corpus.
For several thousand concepts WordNet?s glosses
are very informative. The synset ?chair? for example
looks as follows:
Priors in Bayesian Learning of Phonological Rules
Sharon Goldwater and Mark Johnson
Department of Cognitive and Linguistic Sciences
Box 1978
Brown University
Providence, RI 02912
USA
{sharon goldwater, mark johnson}@brown.edu
Abstract
This paper describes a Bayesian procedure for un-
supervised learning of phonological rules from an
unlabeled corpus of training data. Like Goldsmith?s
Linguistica program (Goldsmith, 2004b), whose
output is taken as the starting point of this proce-
dure, our learner returns a grammar that consists of
a set of signatures, each of which consists of a set
of stems and a set of suffixes. Our grammars dif-
fer from Linguistica?s in that they also contain a set
of phonological rules, specifically insertion, dele-
tion and substitution rules, which permit our gram-
mars to collapse far more words into a signature
than Linguistica can. Interestingly, the choice of
Bayesian prior turns out to be crucial for obtaining a
learner that makes linguistically appropriate gener-
alizations through a range of different sized training
corpora.
1 Introduction
Unsupervised learning presents unusual challenges
to the field of computational linguistics. In super-
vised systems, the task of learning can often be re-
stricted to finding the optimal values for the param-
eters of a pre-specified model. In contrast, an un-
supervised learning system must often propose the
structure of the model itself, as well as the values
for any parameters in that model. In general, there
is a trade-off between the structural complexity of a
model and its ability to explain a set of data. One
way to deal with this trade-off is by using Bayesian
learning techniques, where the objective function
used to evaluate the overall goodness of a system
takes the form
Pr(H)Pr(D|H)
where Pr(H) is the prior probability of the hypoth-
esized model H , and Pr(D|H) is the likelihood of
the data D given that model. In a Bayesian sys-
tem, we want to find the hypothesis H for which
Pr(H)Pr(D|H) is highest (or, equivalently, where
? log Pr(H)? log Pr(D|H) is lowest). While cal-
culating the likelihood of the data given a particu-
lar hypothesis is generally straightforward, the more
difficult question in Bayesian learning is how to de-
termine the prior probabilities of various hypothe-
ses.
In this paper, we compare the results of using
two different prior distributions for an unsupervised
learning task in the domain of morpho-phonology.
Our goal is to learn transformation rules of the form
x ? y / C , where x and y are individual charac-
ters (or the empty character ) and C is some rep-
resentation of the context licensing the transforma-
tion. Our input is an existing segmentation of words
from the Penn Treebank (Marcus et al, 93) into
stems and suffixes. This segmentation is provided
by the Linguistica morphological analyzer (Gold-
smith, 2001; Goldsmith, 2004b), itself an unsuper-
vised algorithm. Using the transformation rules we
learn, we are able to output a new segmentation that
more closely matches our linguistic intuitions.1
We are not the first to apply Bayesian learning
techniques for unsupervised learning of morphol-
ogy and phonology. Several other researchers have
also pursued these methods, usually within a Mini-
mum Description Length (MDL) framework (Ris-
sanen, 1989). In MDL approaches, ? log Pr(H)
is taken to be proportional to the length of H in
some standard encoding, and ? log Pr(D|H) is the
length of D using the encoding specified by H .
MDL-based systems have been relatively successful
for tasks including word segmentation (de Marcken,
1996; Brent and Cartwright, 1996), morphological
1Since we use ordinary text, rather than phonological tran-
scriptions, as input, the rules we learn are really spelling rules,
not phonological rules. We believe that the work discussed
here would be equally applicable, and possibly more success-
ful, with phonological transcriptions. However, since we wish
to have an entirely unsupervised system and we require a mor-
phological segmentation as input, we are currently limited by
the capabilities of Linguistica, which requires standard textual
input. For the remainder of this paper, we use ?phonology? and
?phonological rules? in a broad sense to include orthography as
well.
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
??
?
?
?
?
?
?
?
lift
jump
roll
walk
. . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?

?s
?ed
?ing
. . .
?
?
?
?
?
?
?
?
?
Figure 1: An example signature
segmentation (Goldsmith, 2001; Creutz and Lagus,
2002), discovery of syllabicity and sonority (Elli-
son, 1993), and learning constraints on vowel har-
mony and consonant clusters (Ellison, 1994). How-
ever, our work shows that a straightforward MDL
approach, where the prior ? log Pr(H) depends on
the length of the phonological rules and the rest of
the grammar in the obvious way, does not result in a
successful system for learning phonological rules.
We discuss why this is so, and then present sev-
eral changes that can be made to the prior in order
to learn phonological rules successfully. Our con-
clusion is that, although Bayesian techniques can
be successful for unsupervised learning of linguis-
tic information, careful choice of the prior, with at-
tention to both linguistic and statistical factors, is
important.
In the remainder of this paper, we first review
the basics behind Goldsmith?s Linguistica program,
which serves as the starting point for our own work.
We then explain the additional framework neces-
sary for learning phonological rules, and describe
our search algorithm. In Section 5, we describe the
results of two experiments using our search algo-
rithm, first with an MDL prior, then with a modified
prior. We discuss why the modified prior works bet-
ter for our task, and implications for other Bayesian
learners.
2 Linguistica
Since our algorithm is designed to take as input
a morphological analysis produced by Linguistica,
we first briefly review what that analysis consists of
and how it is arrived at. Linguistica is based on the
MDL principle, which states that the optimal hy-
pothesis to explain a set of data is the one that min-
imizes the total number of bits required to describe
both the hypothesis and the data under that hypoth-
esis. Information theory tells us that the description
length of the data under a given hypothesis is simply
the negative log likelihood of the data, so the MDL
criterion is equivalent to a Bayesian prior favoring
hypotheses that can be described succintly.
Linguistic hypotheses (grammars) all contain
some primitive types. Linguistica uses three primi-
tive types in its grammar: stems, suffixes, and sig-
natures. 2 Each signature is associated with a set
of stems, and each stem is associated with exactly
one signature representing those suffixes with which
it combines freely. For example, walk and jump
might be associated with the signature ?.ed.ing.s?
(see Figure 1), while bad might be associated with
?.ly?. Unanalyzed words can be thought of as be-
longing to the ?? signature. A possible grammar
under this scenario consists of a set of signatures,
where each signature contains a set of stems and a
set of suffixes. Rather than modeling the probabil-
ity of each word in the corpus directly, the gram-
mar assumes that each word consists of a stem and
a (possibly empty) suffix, and assigns a probability
to each word w according to
Pr(w = t + f) = Pr(?)Pr(t|?)Pr(f |?),
where ? is the signature containing stem t. (We
have adopted Goldsmith?s notation here, using f to
denote suffixes, t for stems, and ? for signatures.)
Clearly, grouping words into signatures will cause
their probabilities to be modeled less well than mod-
eling each word individually. The negative log like-
lihood of the corpus will therefore increase, and this
portion of the description length will grow. How-
ever, listing each word individually in the grammar
requires as many stems as there are words. As-
signing words to signatures significantly reduces the
number of stems, and thus the length of the gram-
mar. If the stems are chosen well, then the length of
the grammar will decrease more than the length of
the encoded corpus will increase, leading to an over-
all win. Goldsmith (2001) provides a detailed de-
scription of the exact grammar encoding and search
heuristics used to find the optimal set of stems, suf-
fixes, and signatures under this type of model.
Goldsmith?s algorithm is not without its prob-
lems, however. We concern ourselves here with its
tendency to postulate spurious signatures in cases
where phonological constraints operate. For exam-
ple, many English verb stems ending in e are placed
in the signature ?e.ed.es.ing?, while stems not end-
ing in e have the signature ?.ed.ing.s?. This is due
to the fact that the stem-final e deletes before suf-
fixes beginning in e or i. Similarly, words like match
and index are likely to be given the signature ?.es?,
whereas most nouns would be ?.s?. The toy gram-
mar G1 in Figure 2 illustrates the sort of analysis
produced by Linguistica.
Goldsmith himself has noted the problem of spu-
rious signatures (Goldsmith, 2004a), and recent ver-
2Linguistica actually can perform prefix analysis as well as
suffix analysis, but in our work we used only the suffixing func-
tions.
?1 = ({work, roll}?{, ed, ing, er})
?2 = ({din, bik}?{e, ed, ing, er})
?3 = ({wait}?{, ed, er})
?4 = ({carr}?{y, ied, ier})
?5 = ({carry}?{, ing})
?6 = ({bike, booth, worker}?{, s})
?7 = ({beach, match}?{, es})
Figure 2: G1: A Sample Linguistica Grammar
sions of Linguistica include some functionality de-
voted to detecting allomorphs. Superficially, our
work may seem similar to Goldsmith?s, but in fact it
is quite different. First of all, the allomorphic vari-
ation detected by Linguistica is suffix-based. That
is, suffixes are proposed that operate to delete cer-
tain stem-final material. For example, a suffix (e)ing
could be proposed in order to include both hope
and walk in the signature ?.(e)ing.s?. This suf-
fix is actually separate in the grammar from the
ordinary ing suffix, so there is no recognition of
the fact that any occurrence of ing in any signa-
ture should delete a preceding stem-final e. More-
over, this approach is not really phonological, in
the sense that other suffixes beginning with i might
or might not be analyzed as deleting stem-final
e. While many languages do contain some affix-
specific morpho-phonological processes, our goal
here is to find phonological rules that apply at all
stem-suffix boundaries, given certain context crite-
ria.
A second major difference between the allomor-
phy detection in Linguistica and the work presented
here is that a Linguistica suffix such as (e)ing is as-
sumed to delete any stem-final e, without exception.
While this assumption may be valid in this case,
there are other suffixes and phonological processes
that are not categorical. For example, the English
plural s requires insertion of an e after certain stems,
including those ending in x or s. However, there
is no simple way to describe the context for this
rule based solely on orthography, because of stems
such as beach (+es) and stomach (+s). For this rea-
son, and to add robustness against errors in the input
morphological segmentation, we allow stems to be
listed in the grammar as exceptions to phonological
rules.
In addition to these theoretical differences, the
work presented here covers a wider range of phono-
logical processes than does Linguistica. Linguis-
tica is capable of detecting only stem-final deletion,
whereas our algorithm can also detect insertion (as
in match + s ? matches) and stem-final substitu-
tion (as in carry + ed? carried). In the following
section we discuss the structure of the grammar we
use to describe the words in our corpus.
3 A Morpho-Phonological Grammar
Since the morphology we use as input to our pro-
gram is obtained directly from Linguistica, our
grammar is necessarily similar to the one in that
program. As discussed above, Linguistica contains
three primitives types in its grammar: signatures,
stems, and suffixes. We add one more primitive type
to our grammar, the notion of a rule.
Each rule consists of a transformation, for ex-
ample  ? e or y ? i, and a conditioning con-
text. A context consists of a string of four charac-
ters XtytyfXf , where Xi ? {C, V,#} (consonant,
vowel, end-of-word) and yi is in the set of characters
in our text.3 The first half of the context is from the
end of the stem, and the second half is from the be-
ginning of the suffix. For example, the stem-suffix
pair jump + ed has the context CpeC . All trans-
formations are assumed to occur stem-finally, i.e.
at the second context position (or after the second
position, for insertions). Of course, these contexts
are more detailed than necessary for certain phono-
logical rules, and don?t capture all the information
required for others. In future work, we plan to al-
low for different types of contexts and generaliza-
tion over contexts, but for the present, all contexts
have the same form.
Using these four primitives, we can construct a
grammar in the following way: As in Goldsmith?s
work, we list a set of signatures, each of which
contains a set of stems and suffixes. In addition,
we list a set of phonological rules. In many cases,
only one rule will apply in a particular context, in
which case it applies to all stem-suffix pairs that
meet its context. If more than one rule applies, we
list the rule with the most common transformation
first and assume that it applies unless a particular
stem specifies otherwise. Stems can thus be listed
as exceptions to rules by using a non-default *no-
change* rule with the appropriate context. Note
that the more exceptions a rule has, the more expen-
sive it is to add to the grammar: each new type of
transformation in a particular context must be listed,
and each stem requiring a non-default transforma-
tion must specify the transformation required. Any
prior preferring short grammars will therefore tend
3The knowledge of which characters are consonants and
which are vowels is the only information we provide to our pro-
gram, other than the text corpus and the Linguistica-produced
morphology. Aside from the C/V distinction, our program is
entirely knowledge-free.
?1 = ({work, roll, dine, carry}?{, ed, er, ing})
?2 = ({bike}?{, ed, er, ing, s})
?3 = ({wait}?{, ed, er})
?4 = ({booth (r5), worker, beach, match}?{, s})
r1 = e? / CeeC
r2 = e? / CeiC
r3 = y? i / CyeC
r4 = ?e / Chs#
r5 = *no-change* / Chs#
Figure 3: G2: A Sample Grammar with Transfor-
mation Rules
to reject rules requiring many exceptions (i.e. those
without a consistent application context).
Grammar G2, in Figure 3, shows a sample of the
kind of grammar we use. This grammar generates
exactly the same wordforms as G1, but using fewer
signatures due to the effects of the phonological
rules. All the stem-suffix parings in this grammar
undergo the default rules for their contexts except
for the stem booth, which is listed as an exception
to the e-insertion rule. For booth + s, the grammar
therefore generates booths, not boothes.
Our model generates data in much the same way
as Goldsmith?s: a word is generated by selecting a
signature and then independently generating a stem
and suffix from that signature. This means that
the likelihood of the data takes the same form in
our model as in Goldsmith?s, namely Pr(w) =
Pr(?)Pr(t|?)Pr(f |?), where the word w consists
of a stem t and a suffix f both drawn from the
same signature ?. Our model differs from Gold-
smith?s in the way that stems and suffixes are pro-
duced; because we use phonological rules a great
many more stems and suffixes can belong to a sin-
gle signature. We defer discussion of how we define
the prior probability over grammars to Section 5,
and assume for the moment that we are given prior
and likelihood functions that can evaluate the utility
of a grammar and training data.
4 Search Algorithm
Since it is clearly infeasible to evaluate the utility of
every possible grammar, we need a search algorithm
to guide us toward a good solution. Our algorithm
uses certain heuristics to make small changes to the
initial grammar (the one provided by Linguistica),
evaluating each change using our objective func-
tion, and accepting or rejecting it based on the result
of evaluation. Our algorithm contains three major
components: a procedure to find signatures that are
similar in ways suggesting phonological change, a
procedure to identify possible contexts for phono-
logical change, and a procedure to collapse related
signatures and add phonological rules to the gram-
mar. We discuss each of these components in turn.
4.1 Identifying Similar Signatures
An important first step in simplifying the mor-
phological analysis of our data using phonologi-
cal rules is to identify signatures that might be re-
lated via such rules. Since our algorithm considers
three different types of possible phonological pro-
cesses (deletion, substitution, and insertion), there
are three different ways in which signatures may be
related. We need to look for pairs of signatures that
are similar in any of these three ways.
Insertion We look for potential insertion rules by
finding pairs of signatures in which all suffixes but
one are common to both signatures. The distinct
pair of suffixes must be such that one can be formed
from the other by inserting a single character at the
beginning. Example pairs found by our algorithm
include ?.s?/?.es? and ?.y?/?.ly?. In searching
for these pairs (as well as deletion and substitution
pairs), we consider only pairs where each signature
contains at least two stems. This is partly in the
interests of efficiency and partly due to the fact that
signatures with only one stem are often less reliable.
Deletion Signature pairs exhibiting possible dele-
tion behavior are similar to those exhibiting inser-
tion behavior, except that one of the suffixes not
common to both signatures must be the empty suf-
fix. Examples of possible deletion pairs include
?.ed.ing?/?e.ed.ing? and ?.ed.ing?/?ed.ing.s?.
Substitution In a possible substitution pair, one
signature (the one potentially exhibiting stem-final
substitution) contains suffixes that all begin with
one of two characters: the basic stem-final char-
acter, and the substituted character. The signature
?ied.ier.y? from G1 is such a signature. The other
signature in a possible substitution pair must con-
tain the empty suffix, and the two signatures must
be identical when the first character of each suffix in
the first signature is removed. Possible substitution
pairs include ?ied.ier.y?/?.ed.er? and ?ous.y?/?.us?.
Using the set of similar signatures we have de-
tected, we can propose a set of possible phonolog-
ical processes in our data. Some transformations,
such as e ? , will be suggested by more than one
pair of signatures, while others, such as y ? o,
will occur with only one pair. We create a list of
all the possible transformations, ranked according
to the number of signature pairs attesting to them.
4.2 Identifying possible contexts
Once we have found a set of possible transforma-
tions, we need to identify the contexts in which
those transformations might apply. To see how this
works, suppose we are looking at the proposed e-
deletion rule and our input grammar is G1. Using
one of the signature pairs attesting to this rule, such
as ?.ed.er.ing?/?e.ed.er.ing?, we can find possible
conditioning contexts by examining the set of stems
and suffixes in the second signature. If we want to
reanalyze the stems din and bik as dine and bike,
we hypothesize that each wordform generated us-
ing the suffixes present in both signatures (ed, er,
and ing) must have deleted an e. We can find the
context for this deletion by looking at these suffixes
together with the reanalyzed stems. The contexts
for deletion that we would get from {bike, dine} ?
{ed, ing} are {CeeC , CeiC}.4
Our methods for finding possible contexts for
substitution and insertion rules are similar: reana-
lyze the stems and suffixes in the signature hypoth-
esized to require a phonological rule, combine them,
and note the context generated. In this way, we can
get contexts such as CyeC for the y ? i rule (from
carry + ed) and V xs# for the ? ? e rule (from
index + s).
Just as we ranked the set of possible phonologi-
cal rules according to the number of signature pairs
attesting to them, we can rank the set of contexts
proposed for each rule. We do this by calculating
r = Pr(Xtyt|yfXf )/Pr(Xtyt), the ratio between
the probability of seeing a particular stem context
given a particular suffix context to the prior proba-
bility of the stem context. If a stem context (such
as Ce) is quite common overall but hardly ever ap-
pears before a particular suffix context (iC), this is
good evidence that some phonological process has
modified the stem in the context of that suffix. Low
values of r are therefore better evidence of condi-
tioning for a rule than are high values of r.
4.3 Collapsing signatures
Given a set of similar signature pairs, the rules
relating them, and the possible contexts for those
rules, we need to determine which rules are actu-
ally phonologically legitimate and which are sim-
ply accidents of the data. We do this by simply
considering each rule and context in turn, proceed-
ing from the most attested to least attested rules and
from most likely to least likely contexts. For each
rule-context pair, we add the rule to the grammar
4The reasoning we use to finding conditioning contexts for
deletion rules was also described by Goldsmith (2004a), and is
similar to the much earlier work of Johnson (1984).
FINDPHONORULES()
1 G? grammar produced by Linguistica
2 R? ordered set of possible rules
3 for each r ? R
4 do
5 Cr ? ordered set of possible contexts for r
6 C ? ?
7 while Cr 6= ?
8 do c? next c ? Cr
9 Cr ? Cr \ {c}
10 C ? C ? {c}
11 G? ? collapseInContext(G, r, C)
12 G? ? pruneRules(G?)
13 if score(G?) < score(G)
14 then G? G?
15 return G
COLLAPSEINCONTEXT(G, r, C)
1 for each ?i ? G
2 do for each ?j ? G
3 do if (?i?r ?j) ? (?(t, f) ? ?i, ctx(t, f) ? C)
4 then collapseSigs(?i, ?j)
Figure 4: Pseudocode for our search algorithm
with that context and collapse any pairs of signa-
tures related by the rule, as long as all stem-suffix
pairs contain a context at least as likely as the one
under consideration. Collapsing a pair of signa-
tures means reanalyzing all the stems and suffixes
in one of the signatures, and possibly adding excep-
tions for any stems that don?t fit the rule. We have
found that exceptions are often required to handle
stems that were originally misanalyzed by Linguis-
tica. For that reason, we prune the rules added to the
grammar, and for each rule, if fewer than 2% of the
stems require exceptions, we assume that these are
errors and de-analyze the stems, returning the word-
forms they generated to the ?? signature. We then
evaluate the new analysis using our objective func-
tion, and accept it if it scores better than our previ-
ous analysis. Otherwise, we revert to the previous
analysis and continue trying new rule-context pairs.
Pseudocode for our algorithm is presented in Fig-
ure 4. We use the notation ?i?r ?j to indicate that
?i and ?j are similar with respect to rule r, with ?j
being the more ?basic? signature (i.e. adding r to
the grammar would allow us to move the stems in
?i into ?j).
Note that collapsing a pair of signatures does not
always result in an overall reduction in the number
of signatures in the grammar. To see why this is
Morph Only Morph+Phon
Small Large Small Large
Tokens 100k 888k 100k 888k
Types 11313 35631 11313 35631
?s 435 1634 404 1594
Singleton ?s 280 1231 259 1215
Stems 8255 24529 8186 24379
Non- Stems 2363 7673 2286 7494
Table 1: Grammatical Analysis of our Corpora
so, consider the effect of collapsing ?1 and ?2 and
adding r1 and r2 (the e-deletion rules) to G1. When
the stem bik gets reanalyzed as bike, the algorithm
recognizes that bike is already a stem in the gram-
mar, so rather than placing the reanalyzed stem in
?1, it combines the reanalyzed suffixes {, ed, er,
ing} with the suffixes {, s} from ?6 and creates a
new signature for the stem bike ? ?.ed.er.ing.s?.
The two stems carr and carry are also combined
in this way, but in that case, the combined suffixes
form a signature already present in the grammar, so
no new signature is required.
5 Experiments
For our experiments with learning phonological
rules, we used two different corpora obtained from
the Penn Treebank. The larger corpus contains the
words from sections 2-21 of the treebank, filtered to
remove most numbers, acronyms, and words con-
taining puctuation. This corpus consists of approx-
imately 900,000 tokens. The smaller corpus is sim-
ply the first 100,000 words from the larger corpus.
We ran each corpus through the Linguistica pro-
gram to obtain an initial morphological segmenta-
tion. Statistics on the results of this segmentation
are shown in the left half of Table 1. ?Singleton
signatures? are those containing a single stem, and
?Non- stems? refers to stems in a signature other
than the ?? signature, i.e. those stems that combine
with at least one non- suffix.
The original function we used to evaluate the util-
ity of our grammars was an MDL prior very simi-
lar to the one described by Goldsmith (2001). This
prior is simply the number of bits required to de-
scribe the grammar using a fairly straightforward
encoding. The encoding essentially lists all the suf-
fixes in the grammar along with pointers to each
one; then lists the phonological rules with their
pointers; then lists all the signatures. Each signa-
ture is a list of stems and their pointers, and a list of
pointers to suffixes. Each exceptional stem also has
Init. Grammar Change
# ?s 1617 -10
# Stems 24374 -17
Grammar Size: 1335425 +520
?s, Suffixes 53933 -253
Stems 1280617 +493
Phonology 875 +279
Likelihood: 6478490 +166
Total: 7813915 +686
Table 2: Effects of adding y ? i rules under MDL
prior (large corpus).
a pointer to a phonological rule.5
Our algorithm considered a total of 11 possible
transformations in the small corpus and 40 in the
large corpus, but using this prior, only a single type
of transformation appeared in any rule in the final
grammar: e ? , with seven contexts in the small
corpus and eight contexts in the large corpus. In
analyzing why our algorithm failed to accept any
other types of rules, we realized that there were sev-
eral problems with the MDL prior. Consider what
happens to the overall evaluation when two signa-
tures are collapsed. In general, the likelihood of the
corpus will go down, because the stem and suffix
probabilities in the combined signature will not fit
the true probabilities of the words as well as two
separate signatures could. For large corpora like the
ones we are using, this likelihood drop can be quite
large. In order to counterbalance it, there must be a
large gain in the prior.
But now look at Table 2, which shows the effects
of adding all the y ? i rules to the grammar for
the large corpus under the MDL prior. The first
two lines give the number of signatures and stems
in each grammar. The next line shows the total
length (in bits) of each grammar, and this value is
then broken down into three different components:
the overhead caused by listing the signatures and
their suffixes, the length of the stem list (not in-
cluding the length required to specify exceptions to
rules), and the length of the phonological compo-
nent (including both rules and exception specifica-
tions). Finally, we have the negative log likelihood
under each grammar and the total MDL cost (gram-
mar plus likelihood).
As expected, the likelihood term for the grammar
5There are some additional complexities in the grammar en-
coding that we have not mentioned, due to the fact that stems
can be recursively analyzed using shorter stems. These com-
plexities are irrelevant to the points we wish to make here, but
are described in detail in Goldsmith (2001).
Init. Grammar Change
# ?s 1601 -7
# Stems 24386 -7
Grammar Size: 1249629 -318
?s, Suffixes 241465 -493
Stems 1005887 -111
Phonology 2277 +286
Likelihood: 6478764 +39
Total: 7728393 -279
Table 3: Effects of adding y ? i rules under modi-
fied prior (large corpus).
with y ? i rules has increased, indicating a drop
in the probability of the corpus under this gram-
mar. But notice that the total grammar size has
also increased, leading to an overall evaluation that
is worse than for the original grammar. There are
two main reasons for this increase in grammar size.
Initially, the more puzzling of the two is the fact
that the number of bits required to list all the stems
has increased, despite the fact that the number of
stems has decreased due to reanalyzing some pairs
of stems into single stems. It turns out that this ef-
fect is due to the encoding used for stems, which is
simply a bitwise encoding of each character in the
stem. This encoding means that longer stems re-
quire longer descriptions. When reanalysis requires
shifting a character from a suffix onto the entire set
of stems in a signature (as in {certif, empt, hurr} ?
{ied, y} ? {certify, empty, hurry} ? {, ed}), there
can be a large gain in description length simply due
to the extra characters in the stems. If the number of
stems eliminated through reanalysis is high enough
(as it is for the e ?  rules), this stem length effect
will be outweighed. But when only a few stems are
eliminated relative to the number that get longer, the
overall length of the stem list increases.
However, even without the stem list, the grammar
with y ? i rules would still be slightly longer than
the grammar without them. In this case, the rea-
son in that under our MDL prior, it is quite efficient
to encode a signature and its suffixes. Therefore the
grammar reduction caused by removing a few signa-
tures is not enough to outweigh the increase caused
by adding a few phonological rules.
Using these observations as a guideline, we re-
designed our prior by assigning a fixed cost to each
stem and increasing the overhead cost for signa-
tures. The new overhead function is equal to the
sum of the lengths of all the suffixes in the signature,
times a constant factor. This function means there is
more incentive to collapse two signatures that share
several suffixes, such as ?e.ed.er.ing?/?.ed.er.ing?,
than to collapse signatures sharing only a single suf-
fix, such as ?ing.s?/?.ing?. This behavior is exactly
what we want, since these shorter pairs are more
likely to be accidental. Table 3 shows the effects
of adding the y ? i rules under this new prior.
The starting grammar is somewhat different from
the one in Table 2, because more rules have already
been added by the time the y ? i rules are consid-
ered. The important point, however, is that the cost
of each component of the grammar changes in the
direction we expect it to, and the total grammar cost
is reduced enough to more than make up for the loss
in likelihood.
With this new prior, our algorithm was more suc-
cessful, learning from the large corpus the three ma-
jor transformations for English (e ? ,  ? e, and
y ? i) with a total of 22 contexts. Eight of these
rules, such as ? e / V xs# and y ? i / CyeC ,
had no exceptions. Of the remaining rules, the ex-
ceptions to six of the rules were correctly analyzed
stems (for example, unhappy + ly? unhappily and
necessary + ly? necessarily but sly + ly? slyly),
while the remaining eight rules contained misan-
alyzed exceptions (such as overse + er ? over-
seer, which was listed as an exception to the rule
e? / CeeC , rather than being reanalyzed as over-
see + er). In the small corpus, no y ? i rules were
learned due to the fact that no similar signatures at-
testing to these rules were found.
Using these phonological rules, a total of 31 sig-
natures in the small corpus and 57 signatures in the
large corpus were collapsed, with subsequent re-
analysis of 225 and 528 stems, respectively. This
represents 7-10% of all the non- stems. The final
grammars are summarized in the right half of Table
1.
6 Conclusion
The work described here is clearly preliminary with
respect to learning phonological rules and using
those rules to simplify an existing morphology. Our
notion of context, for example, is somewhat impov-
erished; our system might benefit from using con-
texts with variable lengths and levels of generality,
such as those in Albright and Hayes (2003). We
also cannot handle transformations that require rule
ordering or more than one-character changes. One
reason we have not yet implemented these additions
is the difficulty of designing a heuristic search that
can handle the additional complexity required. We
are therefore working toward implementing a more
general search procedure that will allow us to ex-
plore a larger grammar space, allowing greater flex-
ibility with rules and contexts. Once some of these
improvements have been implemented, we hope to
explore the possibilities for learning in languages
with richer morphology and phonology than En-
glish.
Our point in this paper, however, is not to present
a fully general learner, but to emphasize that in a
Bayesian system, the choice of prior can be crucial
to the success of the learning task. Learning is a
trade-off between finding an explanation that fits the
current data (maximizing the likelihood) and main-
taining the ability to generalize to new data (max-
imizing the prior). The MDL framework is a way
to formalize this trade-off that is intuitively appeal-
ing and seems straightforward to implement, but we
have shown that a simple MDL approach is not the
best way to achieve our particular task. There are at
least two reasons for this. First, the obvious encod-
ing of stems actually penalizes the addition of cer-
tain types of phonological rules, even when adding
these rules reduces the number of stems in the gram-
mar. More importantly, the type of grammar we
want to learn allows two different kinds of general-
izations: the grouping of stems into signatures, and
the addition of phonological rules. Simply speci-
fying a method of encoding each type of general-
ization may not result in a linguistically appropriate
trade-off during learning. In particular, we discov-
ered that our MDL encoding for signatures was too
efficient relative to the encoding for rules, leading
the system to prefer not to add rules. Our large cor-
pus size already puts a great deal of pressure on the
system to keep signatures separate, since this leads
to a better fit of the data. In order to learn most of
the rules, we therefore had to significantly increase
the cost of signatures.
We are not the first to note that with an MDL-
style prior the choice of encoding makes a differ-
ence to the linguistic appropriateness of the result-
ing grammar. Chomsky himself (Chomsky, 1965)
points out that the reason for using certain types
of notation in grammar rules is to make clear the
types of generalizations that lead to shorter gram-
mars. However, our experience emphasizes the fact
that very little is still known about how to choose
appropriate encodings (or, more generally, priors).
As researchers continue to attempt more sophisti-
cated Bayesian learning tasks, they will encounter
more interactions between different kinds of gener-
alizations. As a result, the question of how to de-
sign a good prior will become increasingly impor-
tant. Our primary goal for the future is therefore to
investigate exactly what assumptions go into decid-
ing whether a grammar is linguistically sound, and
to determine how to specify those assumptions ex-
plicitly in a Bayesian prior.
Acknowledgements
The authors would like to thank Eugene Charniak
and the anonymous reviewers for helpful comments.
This work was supported by NSF grants 9870676
and 0085940, NIMH grant 1R0-IMH60922-01A2,
and an NDSEG fellowship.
References
A. Albright and B. Hayes. 2003. Rules vs.
analogy in english pass tenses: a computa-
tional/experimental study. Cognition, 90:119?
161.
M. Brent and T. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful
for segmentation. Cognition, 61:93?125.
N. Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge.
M. Creutz and K. Lagus. 2002. Unsupervised dis-
covery of morphemes. In Proceedings of the
Workshop on Morphological and Phonological
Learning at ACL ?02, pages 21?30.
C. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, Massachusetts Institute of
Technology.
T. M. Ellison. 1993. The Machine Learning of
Phonological Structure. Ph.D. thesis, University
of Western Australia.
T. M. Ellison. 1994. The iterative learning of
phonological constraints. Computational Lin-
guistics, 20(3).
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computa-
tional Linguistics, 27:153?198.
J. Goldsmith. 2004a. An algorithm for the unsuper-
vised learning of morphology. Preliminary draft
as of January 1.
J. Goldsmith. 2004b. Linguis-
tica. Executable available at
http://humanities.uchicago.edu/faculty/goldsmith/.
M. Johnson. 1984. A discovery procedure for cer-
tain phonological rules. In Proceedings of COL-
ING.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
93. Building a large annotated corpus of english:
the penn treebank. Computational Linguistics,
19(2).
Rissanen. 1989. Stochastic Complexity and Statis-
tical Inquiry. World Scientific Co., Singapore.
Multi-Component Word Sense Disambiguation  
Massimiliano Ciaramita Mark Johnson
Brown University
Department of Cognitive and Linguistic Sciences
Providence, RI 02912

massi@brown.edu,mark johnson@brown.edu 
Abstract
This paper describes the system MC-WSD pre-
sented for the English Lexical Sample task. The
system is based on a multicomponent architecture.
It consists of one classifier with two components.
One is trained on the data provided for the task. The
second is trained on this data and, additionally, on
an external training set extracted from the Wordnet
glosses. The goal of the additional component is to
lessen sparse data problems by exploiting the infor-
mation encoded in the ontology.
1 Introduction
One of the main difficulties in word sense classifi-
cation tasks stems from the fact that word senses,
such as Wordnet?s synsets (Fellbaum, 1998), de-
fine very specific classes1 . As a consequence train-
ing instances are often too few in number to cap-
ture extremely fine-grained semantic distinctions.
Word senses, however, are not just independent enti-
ties but are connected by several semantic relations;
e.g., the is-a, which specifies a relation of inclusion
among classes such as ?car is-a vehicle?. Based on
the is-a relation Wordnet defines large and complex
hierarchies for nouns and verbs.
These hierarchical structures encode potentially
useful world-knowledge that can be exploited for
word sense classification purposes, by providing
means for generalizing beyond the narrowest synset
level. To disambiguate an instance of a noun like
?bat? a system might be more successful if, in-
stead of limiting itself to applying what it knows
about the concepts ?bat-mammal? and ?bat-sport-
implement?, it could use additional knowledge
about other ?animals? and ?artifacts?.
Our system implements this intuition in two
steps. First, for each sense of an ambiguous word
we generate an additional set of training instances

We would like to thank Thomas Hofmann and our colleagues
in the Brown Laboratory for Linguistic Information Processing
(BLLIP).
151% of the noun synsets in Wordnet contain only 1 word.
from the Wordnet glosses. This data is not limited to
the specific synset that represents one of the senses
of the word, but concerns also other synsets that are
semantically similar, i.e., close in the hierarchy, to
that synset. Then, we integrate the task-specific and
the external training data with a multicomponent
classifier that simplifies the system for hierarchical
word sense disambiguation presented in (Ciaramita
et al, 2003). The classifier consists of two com-
ponents based on the averaged multiclass percep-
tron (Collins, 2002; Crammer and Singer, 2003).
The first component is trained on the task-specific
data while the second is trained on the former and
on the external training data. When predicting a la-
bel for an instance the classifier combines the pre-
dictions of the two components. Cross-validation
experiments on the training data show the advan-
tages of the multicomponent architecture.
In the following section we describe the features
used by our system. In Section 3 we explain how we
generated the additional training set. In Section 4
we describe the architecture of the classifier and in
Section 5 we discuss the specifics of the final system
and some experimental results.
2 Features
We used a set of features similar to that which
was extensively described and evaluated in (Yoong
and Hwee, 2002). The sentence with POS annota-
tion ?A-DT newspaper-NN and-CC now-RB a-DT
bank-NN have-AUX since-RB taken-VBN over-
RB? serves as an example to illustrate them. The
word to disambiguate is bank (or activate for (7)).
1. part of speech of neighboring words  ,
	
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 112?119, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Representational Bias in Unsupervised Learning of Syllable Structure
Sharon Goldwater and Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
{Sharon Goldwater,Mark Johnson}@brown.edu
Abstract
Unsupervised learning algorithms based
on Expectation Maximization (EM) are
often straightforward to implement and
provably converge on a local likelihood
maximum. However, these algorithms of-
ten do not perform well in practice. Com-
mon wisdom holds that they yield poor
results because they are overly sensitive
to initial parameter values and easily get
stuck in local (but not global) maxima.
We present a series of experiments indi-
cating that for the task of learning sylla-
ble structure, the initial parameter weights
are not crucial. Rather, it is the choice of
model class itself that makes the differ-
ence between successful and unsuccess-
ful learning. We use a language-universal
rule-based algorithm to find a good set of
parameters, and then train the parameter
weights using EM. We achieve word ac-
curacy of 95.9% on German and 97.1% on
English, as compared to 97.4% and 98.1%
respectively for supervised training.
1 Introduction
The use of statistical methods in computational lin-
guistics has produced advances in tasks such as pars-
ing, information retrieval, and machine translation.
However, most of the successful work to date has
used supervised learning techniques. Unsupervised
algorithms that can learn from raw linguistic data,
as humans can, remain a challenge. In a statistical
framework, one method that can be used for unsu-
pervised learning is to devise a probabilistic model
of the data, and then choose the values for the model
parameters that maximize the likelihood of the data
under the model.
If the model contains hidden variables, there is
often no closed-form expression for the maximum
likelihood parameter values, and some iterative ap-
proximation method must be used. Expectation
Maximization (EM) (Neal and Hinton, 1998) is
one way to find parameter values that at least lo-
cally maximize the likelihood for models with hid-
den variables. EM is attractive because at each
iteration, the likelihood of the data is guaranteed
not to decrease. In addition, there are efficient
dynamic-programming versions of the EM algo-
rithm for several classes of models that are important
in computational linguistics, such as the forward-
backward algorithm for training Hidden Markov
Models (HMMs) and the inside-outside algorithm
for training Probabilistic Context-Free Grammars
(PCFGs).
Despite the advantages of maximum likelihood
estimation and its implementation via various in-
stantiations of the EM algorithm, it is widely re-
garded as ineffective for unsupervised language
learning. Merialdo (1994) showed that with only
a tiny amount of tagged training data, supervised
training of an HMM part-of-speech tagger outper-
formed unsupervised EM training. Later results (e.g.
Brill (1995)) seemed to indicate that other methods
of unsupervised learning could be more effective (al-
though the work of Banko and Moore (2004) sug-
gests that the difference may be far less than previ-
112
ously assumed). Klein and Manning (2001; 2002)
recently achieved more encouraging results using an
EM-like algorithm to induce syntactic constituent
grammars, based on a deficient probability model.
It has been suggested that EM often yield poor
results because it is overly sensitive to initial param-
eter values and tends to converge on likelihood max-
ima that are local, but not global (Carroll and Char-
niak, 1992). In this paper, we present a series of
experiments indicating that for the task of learning
a syllable structure grammar, the initial parameter
weights are not crucial. Rather, it is the choice of
the model class, i.e., the representational bias, that
makes the difference between successful and unsuc-
cessful learning.
In the remainder of this paper, we first describe
the task itself and the structure of the two differ-
ent classes of models we experimented with. We
then present a deterministic algorithm for choosing
a good set of parameters for this task. The algo-
rithm is based on language-universal principles of
syllabification, but produces different parameters for
each language. We apply this algorithm to English
and German data, and describe the results of exper-
iments using EM to learn the parameter weights for
the resulting models. We conclude with a discussion
of the implications of our experiments.
2 Statistical Parsing of Syllable Structure
Knowledge of syllable structure is important for
correct pronunciation of spoken words, since cer-
tain phonemes may be pronounced differently de-
pending on their position in the syllable. A num-
ber of different supervised machine learning tech-
niques have been applied to the task of automatic
syllable boundary detection, including decision-tree
classifiers (van den Bosch et al, 1998), weighted
finite state transducers (Kiraz and Mo?bius, 1998),
and PCFGs (Mu?ller, 2001; Mu?ller, 2002). The re-
searchers presenting these systems have generally
argued from the engineering standpoint that sylla-
ble boundary detection is useful for pronunciation of
unknown words in text-to-speech systems. Our mo-
tivation is a more scientific one: we are interested in
the kinds of procedures and representations that can
lead to successful unsupervised language learning in
both computers and humans.
Our work has some similarity to that of Mu?ller,
who trains a PCFG of syllable structure from a
corpus of words with syllable boundaries marked.
We, too, use a model defined by a grammar to de-
scribe syllable structure.1 However, our work dif-
fers from Mu?ller?s in that it focuses on how to learn
the model?s parameters in an unsupervised manner.
Several researchers have worked on unsupervised
learning of phonotactic constraints and word seg-
mentation (Elman, 2003; Brent, 1999; Venkatara-
man, 2001), but to our knowledge there is no pre-
viously published work on unsupervised learning of
syllable structure.
In the work described here, we experimented with
two different classes of models of syllable structure.
Both of these model classes are presented as PCFGs.
The first model class, described in Mu?ller (2002),
encodes information about the positions within a
word or syllable in which each phoneme is likely
to appear. In this positional model, each syllable
is labeled as initial (I), medial (M), final (F), or as
the one syllable in a monosyllabic word (O). Syl-
lables are broken down into an optional onset (the
initial consonant or consonant cluster) followed by a
rhyme. The rhyme consists of a nucleus (the vowel)
followed by an optional coda consonant or cluster.
Each phoneme is labeled with a preterminal cate-
gory of the form CatPos.x.y, where Cat ? {Ons,
Nuc, Cod}, Pos ? {I, M, F, O}, x is the position
of a consonant within its cluster, and y is the total
number of consonants in the cluster. x and y are un-
used when Cat = Nuc, since all nuclei consist of a
single vowel. See Fig. 1 for an example parse.
Rather than directly encoding positional infor-
mation, the second model class we investigate (the
bigram model) models statistical dependencies be-
tween adjacent phonemes and adjacent syllables.
In particular, each onset or coda expands directly
into one or more terminal phonemes, thus capturing
the ordering dependencies between consonants in a
cluster. Also, the shape of each syllable (whether it
contains an onset or coda) depends on the shape of
the previous syllable, so that the model can learn,
for example, that syllables ending in a coda should
be followed by syllables with an onset.2 This kind
1We follow Mu?ller in representing our models as PCFGs be-
cause this representation is easy to present. The languages gen-
erated by these PCFGs are in fact regular, and it is straightfor-
ward to transform the PCFGs into equivalent regular grammars.
2 Many linguists believe that, cross-linguistically, a poten-
113
Word
SylI
RhyI
NucI
@
SylM
OnsM
OnsM.1.2
g
OnsM.2.2
r
RhyM
NucM
i
SylF
OnsF
OnsF.1.1
m
RhyF
NucF
@
CodF
CodF.1.2
n
CodF.2.2
t
Word
WdN
SylN
Nuc
@
WdON
SylON
Ons
g r
Nuc
i
WdON
SylONC
Ons
m
Nuc
@
Cod
n t
Figure 1: Positional analysis (left) and bigram analysis (right) of the word agreement. Groups of terminals
dominated by a Syl* node constitute syllables. Terminals appear in the SAMPA encoding of IPA used by
CELEX.
of bigram dependency between syllables is modeled
using rules of the form WdX ? SylX WdY , where
X and Y are drawn from the set of possible combi-
nations of onset, nucleus, and coda in a syllable: {N,
ON, NC, ONC}. Each SylX category has only one
expansion. See Fig. 1 for an example.
With respect to either of these two model classes,
each way of assigning syllable boundaries to a word
corresponds to exactly one parse of that word. This
makes it simple to train the models from a corpus in
which syllable boundaries are provided, as in Mu?ller
(2001). We used two different corpora for our exper-
iments, one German (from the ECI corpus of news-
paper text) and one English (from the Penn WSJ
corpus). Each corpus was created by converting
the orthographic forms in the original text into their
phonemic transcriptions using the CELEX database
(Baayen et al, 1995). CELEX includes syllable
boundaries, which we used for supervised training
and for evaluation. Any words in the original texts
that were not listed in CELEX were discarded, since
one of our goals is to compare supervised and un-
supervised training.3 From the resulting phonemic
corpora, we created a training set of 20,000 tokens
and a test set of 10,000 tokens. Using standard max-
imum likelihood supervised training procedures, we
obtained similar results for models from the two
model classes. In German, word accuracy (i.e. the
tially ambiguous consonant, such as the b in saber, is always
syllabified as the onset of the second syllable rather than the
coda of the first. We discuss this point further in Section 3.
3Due to the nature of the corpora, the percentage of words
discarded was fairly high: 35.6% of the English tokens (pri-
marily proper nouns, acronyms, and numerals, with a smaller
number of morphologically complex words) and 26.7% of the
German tokens (with compound words making up a somewhat
larger portion of these discards).
percentage of words with no syllabification errors)
was 97.4% for the bigram model and 97.2% for the
positional model,4 while in English it was 98.1%
and 97.6% respectively. These results for English
are in line with previous reported results using other
supervised learning techniques, e.g. van den Bosch
et al (1998). Since many of the words in the data are
monosyllabic (49.1% in German, 61.2% in English)
and therefore contain no ambiguous syllable bound-
aries, we also calculated the multisyllabic word ac-
curacy. This was 94.9% (bigram) and 94.5% (posi-
tional) in German, and 95.2% (bigram) and 93.8%
(positional) in English.
3 Categorical Parsing of Syllable Structure
In the previous section, we described two different
model classes and showed that the maximum like-
lihood estimates with supervised training data yield
good models of syllable structure. In moving to un-
supervised learning, however, there are two prob-
lems that need to be addressed: exactly what class of
models do we want to consider (i.e., what kinds of
rules should the model contain), and how should we
select a particular model from that class (i.e., what
weights should the rules have)? We take as our so-
lution to the latter problem the most straightforward
approach; namely, maximum likelihood estimation
using EM. This leaves us with the question of how
to choose a set of parameters in the first place. In this
section, we describe an algorithm based on two fun-
damental phonological principles that, when given a
set of data from a particular language, will produce a
4Mu?ller reports slightly lower results of 96.88% on German
using the same positional model. We have no explanation for
this discrepancy.
114
set of rules appropriate to that language. These rules
can then be trained using EM.
Given a particular rule schema, it is not imme-
diately clear which of the possible rules should ac-
tually be included in the model. For example, in
the bigram model, should we start off with the rule
Ons ? k n? This rule is unnecessary for English,
and could lead to incorrect parses of words such
as weakness. But /kn/ is a legal onset in German,
and since we want an algorithm that is prepared to
learn any language, disallowing /kn/ as an onset out
of hand is unacceptable. On the other hand, the set
of all combinatorially possible consonant clusters is
infinite, and even limiting ourselves to clusters actu-
ally seen in the data for a particular language yields
extremely unlikely-sounding onsets like /lkj/ (calcu-
late) and /bst/ (substance). Ideally, we should limit
the set of rules to ones that are likely to actually be
used in the language of interest.
The algorithm we have developed for produc-
ing a set of language-appropriate rules is essentially
a simple categorical (i.e., non-statistical) syllable
parser based on the principles of onset maximiza-
tion and sonority sequencing (Blevins, 1995). Onset
maximization is the idea that in word-medial conso-
nant clusters, as many consonants as possible (given
the phonotactics of the language) should be assigned
to onset position. This idea is widely accepted and
has been codified in Optimality Theory (Prince and
Smolensky, 1993) by proposing the existence of a
universal preference for syllables with onsets.5
In addition to onset maximization, our categorical
parser follows the principle of sonority sequencing
whenever possible. This principle states that, within
a syllable, segments that are closer to the nucleus
should be higher in sonority than segments that are
further away. Vowels are considered to be the most
sonorous segments, followed by glides (/j/, /w/), liq-
uids (/l/, /r/), nasals (/n/, /m/, /N/), fricatives (/v/,
/s/, /T/, . . . ), and stops (/b/, /t/, /k/, . . . ). Given a
5An important point, which we return to in Section 5, is
that exceptions to onset maximization may occur at morpheme
boundaries. Some linguists also believe that there are addi-
tional exceptions in certain languages (including English and
German), where stressed syllables attract codas. Under this the-
ory, the correct syllabification for saber would not be sa.ber, but
rather sab.er, or possibly sa[b]er, where the [b] is ambisyllabic.
Since the syllable annotations in the CELEX database follow
simple onset maximization, we take that as our approach as well
and do not consider stress when assigning syllable boundaries.
cluster of consonants between two syllable nuclei,
sonority sequencing states that the syllable boundary
should occur either just before or just after the con-
sonant with lowest sonority. Combining this princi-
ple with onset maximization predicts that the bound-
ary should fall before the lowest-sonority segment.
Predicting syllable boundaries in this way is not
foolproof. In some cases, clusters that are predicted
by sonority sequencing to be acceptable are in fact
illegal in some languages. The illegal English on-
set cluster kn is a good example. In other cases,
such as the English onset str, clusters are allowed
despite violating sonority sequencing. These mis-
matches between universal principles and language-
specific phonotactics lead to errors in the predic-
tions of the categorical parser, such as wea.kness and
ins.tru.ment. In addition, certain consonant clusters
like bst (as in substance) may contain more than
one minimum sonority point. To handle these cases,
the categorical parser follows onset maximization
by adding any consonants occurring between the
two minima to the onset of the second syllable:
sub.stance.
Not surprisingly, the categorical parser does not
perform as well as the supervised statistical parser:
only 92.7% of German words and 94.9% of English
words (85.7% and 86.8%, respectively, of multisyl-
labic words) are syllabified correctly. However, a
more important result of parsing the corpus using
the categorical parser is that its output can be used
to define a model class (i.e., a set of PCFG rules)
from which a model can be learned using EM.
Specifically, our model class contains the set of
rules that were proposed at least once by the cat-
egorical parser in its analysis of the training cor-
pus; in the EM experiments described below, the
rule probabilities are initialized to their frequency
in the categorical parser?s output. Due to the mis-
takes made by the categorical parser, there will be
some rules, like Ons ? k n in English, that are not
present in the model trained on the true syllabifica-
tion, but many possible but spurious rules, such as
Ons ? b s t, will be avoided. Although clusters that
violate sonority sequencing tend to be avoided by
the categorical parser, it does find examples of these
types of clusters at the beginnings and endings of
words, as well as occasionally word-medially (as in
sub.stance). This means that many legal clusters that
115
Bigram Positional
all multi all multi
CP 92.7 85.7 92.7 85.7
CP + EM 95.9 91.9 91.8 84.0
CP-U + EM 95.9 91.9 92.0 84.4
supervised 97.4 94.9 97.2 94.5
SP + EM 71.6 44.3 94.4 89.1
SP-U + EM 71.6 44.3 94.4 89.0
Table 1: Results for German: % of all words (or
multisyllabic words) correctly syllabified.
violate sonority sequencing will also be included in
the set of rules found by this procedure, although
their probabilities may be considerably lower than
those of the supervised model. In the following sec-
tion, we show that these differences in rule probabil-
ities are unimportant; in fact, it is not the rule prob-
abilities estimated from the categorical parser?s out-
put, but only the set of rules itself that matters for
successful task performance.
4 Experiments
In this section, we present a series of experiments us-
ing EM to learn a model of syllable structure. All of
our experiments use the same German and English
20,000-word training corpora and 10,000-word test-
ing corpora as described in Section 2.6
For our first experiment, we ran the categorical
parser on the training corpora and estimated a model
from the parse trees it produced, as described in the
previous section. This is essentially a single step
of Viterbi EM training. We then continued to train
the model by running (standard) EM to convergence.
Results of this experiment with Categorical Pars-
ing + EM (CP + EM) are shown in Tables 1 and
2. For both German and English, using this learn-
ing method with the bigram model yields perfor-
mance that is much better than the categorical parser
alone, though not quite as good as the fully super-
vised regime. On the other hand, training a posi-
tional model from the categorical parser?s output and
then running EM causes performance to degrade.
To determine whether the good performance of
6Of course, for unsupervised learning, it is not necessary to
use a distinct testing corpus. We did so in order to use the same
testing corpus for both supervised and unsupervised learning
experiments, to ensure fair comparison of results.
Bigram Positional
all multi all multi
CP 94.9 86.8 94.9 86.8
CP + EM 97.1 92.6 94.1 84.9
CP-U + EM 97.1 92.6 94.1 84.9
supervised 98.1 95.2 97.6 93.8
SP + EM 86.0 64.0 96.5 90.9
SP-U + EM 86.0 64.0 67.6 16.5
Table 2: Results for English.
the bigram model was simply due to good initial-
ization of the parameter weights, we performed a
second experiment. Again starting with the set of
rules output by the categorical parser, we initialized
the rule weights to the uniform distribution. The re-
sults of this experiment (CP-U + EM) show that for
the class of bigram models, the performance of the
final model found by EM does not depend on the
initial rule probabilities. Performance within the po-
sitional model framework does depend on the initial
rule probabilities, since accuracy in German is dif-
ferent for the two experiments.
As we have pointed out, the rules found by the
categorical parser are not exactly the same as the
rules found using supervised training. This raises
the question of whether the difference in perfor-
mance between the unsupervised and supervised bi-
gram models is due to differences in the rules. To
address this question, we performed two additional
experiments. First, we simply ran EM starting from
the model estimated from supervised training data.
Second, we kept the set of rules from the supervised
training data, but reinitialized the probabilities to a
uniform distribution before running EM. The results
of these experiments are shown as SP + EM and SP-
U + EM, respectively. Again, performance of the
bigram model is invariant with respect to initial pa-
rameter values, while the performance of the posi-
tional model is not. Interestingly, the performance
of the bigram model in these two experiments is far
worse than in the CP experiments. This result is
counterintuitive, since it would seem that the model
rules found by the supervised system are the opti-
mal rules for this task. In the following section, we
explain why these rules are not, in fact, the optimal
rules for unsupervised learning, as well as why we
believe the bigram model performs so much better
116
than the positional model in the unsupervised learn-
ing situation.
5 Discussion
The results of our experiments raise two interesting
questions. First, when starting from the categorical
parser?s output, why does the bigram model improve
after EM training, while the positional model does
not? And second, why does applying EM to the su-
pervised bigram model lead to worse performance
than applying it to the model induced from the cate-
gorical parser?
To answer the first question, notice that one dif-
ference between the bigram model and the posi-
tional model is that onsets and codas in the bigram
model are modeled using the same set of parame-
ters regardless of where in the word they occur. This
means that the bigram model generalizes whatever it
learns about clusters at word edges to word-medial
clusters (and, of course, vice versa). Since the cate-
gorical parser only makes errors word-medially, in-
correct clusters are only a small percentage of clus-
ters overall, and the bigram model can overcome
these errors by reanalyzing the word-medial clus-
ters. The errors that are made after EM training
are mostly due to overgeneralization from clusters
that are very common at word edges, e.g. predicting
le.gi.sla.tion instead of le.gis.la.tion.
In contrast to the bigram model, the positional
model does not generalize over different positions
of the word, which means that it learns and repeats
the word-medial errors of the categorical parser. For
example, this model predicts /E.gzE.kju.tIv/ for ex-
ecutive, just as the categorical parser does, although
/gz/ is never attested in word-initial position. In ad-
dition, each segment in a cluster is generated in-
dependently, which means clusters like /tl/ may be
placed together in an onset because /t/ is common
as the first segment of an onset, and /l/ is common
as the second. While this problem exists even in
the supervised positional model, it is compounded
in the unsupervised version because of the errors of
the categorical parser.
The differences between these two models are an
example of the bias-variance trade-off in probabilis-
tic modeling (Geman et al, 1992): models with low
bias will be able to fit a broad range of observations
fairly closely, but slight changes in the observed data
will cause relatively large changes in the induced
model. On the other hand, models with high bias
are less sensitive to changes in the observed data.
Here, the bigram model induced from the categor-
ical parser has a relatively high bias: regardless of
the parameter weights, it will be a poor model of
data where word-medial onsets and codas are very
different from those at word edges, and it cannot
model data with certain onsets such as /vp/ or /tz/
at all because the rules Ons ? v p and Ons ? t z
are simply absent. The induced positional model
can model both of these situations, and can fit the
true parses more closely as well (as evidenced by
the fact that the likelihood of the data under the su-
pervised positional model is higher than the like-
lihood under the supervised bigram model). As a
result, however, it is more sensitive to the initial
parameter weights and learns to recreate the errors
produced by the categorical parser. This sensitiv-
ity to initial parameter weights also explains the ex-
tremely poor performance of the positional model
in the SP-U + EM experiment on English. Because
the model is so unconstrained, in this case it finds a
completely different local maximum (not the global
maximum) which more or less follows coda max-
imization rather than onset maximization, yielding
syllabifications like synd.ic.ate and tent.at.ive.ly.
The concept of representational bias can also ex-
plain why applying EM to the supervised bigram
model performs so poorly. Examining the model in-
duced from the categorical parser reveals that, not
surprisingly, it contains more rules than the super-
vised bigram model. This is because the categori-
cal parser produces a wider range of onsets and co-
das than there are in the true parses. However, the
induced model is not a superset of the supervised
model. There are four rules (three in English) that
occur in the supervised model but not the induced
model. These are the rules that allow words where
one syllable contains a coda and the following syl-
lable has no onset. These are never produced by the
categorical parser because of its onset-maximization
principle. However, it turns out that a very small per-
centage of words do follow this pattern (about .14%
of English tokens and 1.1% of German tokens). In
English, these examples seem to consist entirely of
words where the unusual syllable boundary occurs at
a morpheme boundary (e.g. un.usually, dis.appoint,
117
week.end, turn.over). In German, all but a handful of
examples occur at morpheme boundaries as well.7
The fact that the induced bigram model is unable
to model words with codas followed by no onset is
a very strong bias, but these words are so infrequent
that the model can still fit the data quite well. The
missing rules have no effect on the accuracy of the
parser, because in the supervised model the proba-
bilities on the rules allowing these kinds of words
are so low that they are never used in the Viterbi
parses anyway. The problem is that if these rules are
included in the model prior to running EM, they add
several extra free parameters, and suddenly EM is
able to reanalyze many of the words in the corpus to
make better use of these parameters. It ends up pre-
ferring certain segments and clusters as onsets and
others as codas, which raises the likelihood of the
corpus but leads to very poor performance. Essen-
tially, it seems that the presence of a certain kind of
morpheme boundary is an additional parameter of
the ?true? model that the bigram model doesn?t in-
clude. Trying to account for the few cases where this
parameter matters requires introducing extra param-
eters that allow EM too much freedom of analysis.
It is far better to constrain the model, disallowing
certain rare analyses but enabling the model to learn
successfully in a way that is robust to variations in
initial conditions and idiosyncracies of the data.
6 Conclusion
We make no claims that our learning system em-
bodies a complete model of syllabification. A full
model would need to account for the effects of mor-
phological boundaries, as well as the fact that some
languages allow resyllabification over word bound-
aries. Nevertheless, we feel that the results presented
here are significant. We have shown that, despite
previous discouraging results (Carroll and Charniak,
1992; Merialdo, 1994), it is possible to achieve good
results using EM to learn linguistic structures in an
unsupervised way. However, the choice of model
parameters is crucial for successful learning. Car-
roll and Charniak, for example, generated all pos-
7The exceptions in our training data were auserkoren ?cho-
sen?, erobern ?capture?, and forms of erinnern ?remind?, all of
which were listed in CELEX as having a syllable boundary, but
no morpheme boundary, after the first consonant. Our knowl-
edge of German is not sufficient to determine whether there is
some other factor that can explain these cases.
sible rules within a particular framework and relied
on EM to remove the ?unnecessary? rules by letting
their probabilities go to zero. We suggest that this
procedure tends to yield models with low bias but
high variance, so that they are extremely sensitive
to the small variations in expected rule counts that
occur with different initialization weights.
Our work suggests that using models with higher
bias but lower variance may lead to much more
successful results. In particular, we used univer-
sal phonological principles to induce a set of rules
within a carefully chosen grammatical framework.
We found that there were several factors that en-
abled our induced bigram model to learn success-
fully where the comparison positional model did
not:
1. The bigram model encodes bigram dependen-
cies of syllable shape and disallows onset-less
syllables following syllables with codas.
2. The bigram model does not distinguish be-
tween different positions in a word, so it can
generalize onset and coda sequences from word
edges to word-medial position.
3. The bigram model learns specific sequences
of legal clusters rather than information about
which positions segments are likely to occur in.
Notice that each of these factors imposes a con-
straint on the kinds of data that can be modeled. We
have already discussed the fact that item 1 rules out
the correct syllabification of certain morphologically
complex words, but since our system currently has
no way to determine morpheme boundaries, it is bet-
ter to do so than to introduce extra free parameters.
One possible extension to this work would be to try
to incorporate morphological boundary information
(either annotated or induced) into the model.
A more interesting constraint is the one imposed
by item 2, since in fact most languages do have some
differences between the onsets and (especially) co-
das allowed at word edges and within words. How-
ever, the proper way to handle this fact is not by
introducing completely independent parameters for
initial, medial, and final positions, since this allows
far too much freedom. It would be extremely sur-
prising to find a language with one set of codas al-
lowed word-internally, and a completely disjoint set
118
allowed word-finally. In fact, the usual situation is
that word-internal onsets and codas are a subset of
those allowed at word edges, and this is exactly why
using word edges to induce our rules was successful.
Considering language more broadly, it is com-
mon to find patterns of linguistic phenomena with
many similarities but some differences as well. For
such cases, adding extra parameters to a supervised
model often yields better performance, since the
augmented model can capture both primary and sec-
ondary effects. But it seems that, at least for the
current state of unsupervised learning, it is better to
limit the number of parameters and focus on those
that capture the main effects in the data. In our task
of learning syllable structure, we were able to use
just a few simple principles to constrain the model
successfully. For more complex tasks such as syn-
tactic parsing, the space of linguistically plausible
models is much larger. We feel that a research pro-
gram integrating results from the study of linguistic
universals, human language acquisition, and compu-
tational modeling is likely to yield the most insight
into the kinds of constraints that are needed for suc-
cessful learning.
Ultimately, of course, we will want to be able to
capture not only the main effects in the data, but
some of the subtler effects as well. However, we
believe that the way to do this is not by introducing
completely free parameters, but by using a Bayesian
prior that would enforce a degree of similarity be-
tween certain parameters. In the meantime, we have
shown that employing linguistic universals to deter-
mine which set of parameters to include in a lan-
guage model for syllable parsing allows us to use
EM for learning the parameter weights in a success-
ful and robust way.
Acknowledgments
We would like to thank Eugene Charniak and our
colleagues in BLLIP for their support and helpful
suggestions. This research was partially supported
by NSF awards IGERT 9870676 and ITR 0085940
and NIMH award 1R0-IMH60922-01A2.
References
R. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database (release 2) [cd-rom].
M. Banko and R. Moore. 2004. A study of unsupervised part-
of-speech tagging. In Proceedings of COLING ?04.
J. Blevins. 1995. The syllable in phonological theory. In
J. Goldsmith, editor, the Handbook of Phonological Theory.
Blackwell, Oxford.
M. Brent. 1999. An efficient, probabilistically sound algorithm
for segmentation and word discovery. Machine Learning,
34:71?105.
E. Brill. 1995. Unsupervised learning of disambiguation rules
for part of speech tagging. In Proceedings of the 3rd Work-
shop on Very Large Corpora, pages 1?13.
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. In Pro-
ceedings of the AAAI Workshop on Statistically-Based Natu-
ral Language Processing Techniques, San Jose, CA.
J. Elman. 2003. Generalization from sparse input. In Proceed-
ings of the 38th Annual Meeting of the Chicago Linguistic
Society.
S. Geman, E. Bienenstock, and R. Doursat. 1992. Neural net-
works and the bias/variance dilemma. Neural Computation,
4:1?58.
G. A. Kiraz and B. Mo?bius. 1998. Multilingual syllabifica-
tion using weighted finite-state transducers. In Proceedings
of the Third European Speech Communication Association
Workshop on Speech Synthesis.
D. Klein and C. Manning. 2001. Distributional phrase struc-
ture induction. In Proceedings of the Conference on Natural
Language Learning, pages 113?120.
D. Klein and C. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proceed-
ings of the ACL.
B. Merialdo. 1994. Tagging english text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
K. Mu?ller. 2001. Automatic detection of syllable boundaries
combining the advantages of treebank and bracketed corpora
training. In Proceedings of the ACL.
K. Mu?ller. 2002. Probabilistic context-free grammars for
phonology. In Proceedings of the Workshop on Morpholog-
ical and Phonological Learning at ACL.
R. Neal and G. Hinton, 1998. A New View of the EM Algorithm
That Justifies Incremental and Other Variants, pages 355?
368. Kluwer.
A. Prince and P. Smolensky. 1993. Optimality theory: Con-
straint interaction in generative grammar. Technical Report
TR-2, Rutgers Center for Cognitive Science, Rutgers Univ.
A. van den Bosch, T. Weijters, and W. Daelemans. 1998. Mod-
ularity in inductively-learned word pronunciation systems.
In New Methods in Language Processing and Computational
Language Learning (NeMLaP3/CoNLL98).
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):351?372.
119
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301?307,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Phrasal Categories
William P. Headden III, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw|ec|mj}@cs.brown.edu
Abstract
In this work we learn clusters of contex-
tual annotations for non-terminals in the
Penn Treebank. Perhaps the best way
to think about this problem is to contrast
our work with that of Klein and Man-
ning (2003). That research used tree-
transformations to create various gram-
mars with different contextual annotations
on the non-terminals. These grammars
were then used in conjunction with a CKY
parser. The authors explored the space
of different annotation combinations by
hand. Here we try to automate the process
? to learn the ?right? combination auto-
matically. Our results are not quite as good
as those carefully created by hand, but they
are close (84.8 vs 85.7).
1 Introduction and Previous Research
It is by now commonplace knowledge that accu-
rate syntactic parsing is not possible given only
a context-free grammar with standard Penn Tree-
bank (Marcus et al, 1993) labels (e.g., S, NP ,
etc.) (Charniak, 1996). Instead researchers
condition parsing decisions on many other fea-
tures, such as parent phrase-marker, and, fa-
mously, the lexical-head of the phrase (Mager-
man, 1995; Collins, 1996; Collins, 1997; Johnson,
1998; Charniak, 2000; Henderson, 2003; Klein
and Manning, 2003; Matsuzaki et al, 2005) (and
others).
One particularly perspicuous way to view the
use of extra conditioning information is that of
tree-transformation (Johnson, 1998; Klein and
Manning, 2003). Rather than imagining the parser
roaming around the tree for picking up the infor-
mation it needs, we rather relabel the nodes to di-
rectly encode this information. Thus rather than
have the parser ?look? to find out that, say, the
parent of some NP is an S, we simply relabel the
NP as an NP [S].
This viewpoint is even more compelling if one
does not intend to smooth the probabilities. For
example, consider p(NP ? PRN | NP [S]) If
we have no intention of backing off this probabil-
ity to p(NP ? PRN | NP ) we can treat NP [S]
as an uninterpreted phrasal category and run all
of the standard PCFG algorithms without change.
The result is a vastly simplified parser. This is ex-
actly what is done by Klein and Manning (2003).
Thus the ?phrasal categories? of our title refer
to these new, hybrid categories, such as NP [S].
We hope to learn which of these categories work
best given that they cannot be made too specific
because that would create sparse data problems.
The Klein and Manning (2003) parser is an un-
lexicalized PCFG with various carefully selected
context annotations. Their model uses some par-
ent annotations, and marks nodes which initiate or
in certain cases conclude unary productions. They
also propose linguistically motivated annotations
for several tags, including V P , IN , CC ,NP and
S. This results in a reasonably accurate unlexical-
ized PCFG parser.
The downside of this approach is that their fea-
tures are very specific, applying different annota-
tions to different treebank nonterminals. For in-
stance, they mark right-recursive NP s and not
V P s (i.e., an NP which is the right-most child
of another NP ). This is because data sparsity is-
sues preclude annotating the nodes in the treebank
too liberally. The goal of our work is to automate
the process a bit, by annotating with more general
features that apply broadly, and by learning clus-
301
ters of these annotations.
Mohri and Roark (2006) tackle this problem by
searching for what they call ?structural zeros?or
sets of events which are individually very likely,
but are unlikely to coincide. This is to be con-
trasted with sets of events that do not appear to-
gether simply because of sparse data. They con-
sider a variety of statistical tests to decide whether
a joint event is a structural zero. They mark the
highest scoring nonterminals that are part of these
joint events in the treebank, and use the resulting
PCFG.
Coming to this problem from the standpoint of
tree transformation, we naturally view our work
as a descendent of Johnson (1998) and Klein and
Manning (2003). In retrospect, however, there are
perhaps even greater similarities to that of (Mager-
man, 1995; Henderson, 2003; Matsuzaki et al,
2005). Consider the approach of Matsuzaki et al
(2005). They posit a series of latent annotations
for each nonterminal, and learn a grammar using
an EM algorithm similar to the inside-outside al-
gorithm. Their approach, however, requires the
number of annotations to be specified ahead of
time, and assigns the same number of annotations
to each treebank nonterminal. We would like to
infer the number of annotations for each nonter-
minal automatically.
However, again in retrospect, it is in the work of
Magerman (1995) that we see the greatest similar-
ity. Rather than talking about clustering nodes, as
we do, Magerman creates a decision tree, but the
differences between clustering and decision trees
are small. Perhaps a more substantial difference
is that by not casting his problem as one of learn-
ing phrasal categories Magerman loses all of the
free PCFG technology that we can leverage. For
instance, Magerman must use heuristic search to
find his parses and incurs search errors because of
it. We use an efficient CKY algorithm to do ex-
haustive search in reasonable time.
Belz (2002) considers the problem in a man-
ner more similar to our approach. Beginning with
both a non-annotated grammar and a parent anno-
tated grammar, using a beam search they search
the space of grammars which can be attained via
merging nonterminals. They guide the search us-
ing the performance on parsing (and several other
tasks) of the grammar at each stage in the search.
In contrast, our approach explores the space of
grammars by starting with few nonterminals and
splitting them. We also consider a much wider
range of contextual information than just parent
phrase-markers.
2 Background
A PCFG is a tuple (V,M,?0, R, q : R ? [0, 1]),
where V is a set of terminal symbols; M = {?i}
is a set of nonterminal symbols; ?0 is a start or
root symbol; R is a set of productions of the form
?i ? ?, where ? is a sequence of terminals and
nonterminals; and q is a family of probability dis-
tributions over rules conditioned on each rule?s
left-hand side.
As in (Johnson, 1998) and (Klein and Man-
ning, 2003), we annotate the Penn treebank non-
terminals with various context information. Sup-
pose ? is a Treebank non-terminal. Let ? = ?[?]
denote the non-terminal category annotated with a
vector of context features ?. A PCFG is derived
from the trees in the usual manner, with produc-
tion rules taken directly from the annotated trees,
and the probability of an annotated rule q(? ?
?) = C(???)C(?) where C(? ? ?) and C(?) are the
number of observations of the production and its
left hand side, respectively.
We refer to the grammar resulting from extract-
ing annotated productions directly out of the tree-
bank as the base grammar.
Our goal is to partition the set of annotated non-
terminals into clusters ? = {?i}. Each possible
clustering corresponds to a PCFG, with the set of
non-terminals corresponding to the set of clusters.
The probability of a production under this PCFG
is
p(?i ? ?j?k) =
C(?i ? ?j?k)
C(?i)
where ?s ? ? are clusters of annotated non-
terminals and where:
C(?i ? ?j?k . . .) =
?
(?i,?j ,?k...)??i??j??k...C(?i ? ?j?k . . .)
We refer to the PCFG of some clustering as the
clustered grammar.
2.1 Features
Most of the features we use are fairly standard.
These include the label of the parent and grand-
parent of a node, its lexical head, and the part of
speech of the head.
Klein and Manning (2003) find marking non-
terminals which have unary rewrites to be helpful.
302
They also find useful annotating two preterminals
(DT ,RB) if they are the product of a unary pro-
duction. We generalize this via two width features:
the first marking a node with the number of non-
terminals to which it rewrites; the second marking
each preterminal with the width of its parent.
Another feature is the span of a nonterminal, or
the number of terminals it dominates, which we
normalize by dividing by the length of the sen-
tence. Hence preterminals have normalized spans
of 1/(length of the sentence), while the root has a
normalized span of 1.
Extending on the notion of a Base NP, intro-
duced by Collins (1996), we mark any nonter-
minal that dominates only preterminals as Base.
Collins inserts a unary NP over any base NPs with-
out NP parents. However, Klein and Manning
(2003) find that this hurts performance relative to
just marking the NPs, and so our Base feature does
not insert.
We have two features describing a node?s posi-
tion in the expansion of its parent. The first, which
we call the inside position, specifies the nonter-
minal?s position relative to the heir of its parent?s
head, (to the left or right) or whether the nontermi-
nal is the heir. (By ?heir? we mean the constituent
donates its head, e.g. the heir of an S is typically
the V P under the S.) The second feature, outside
position, specifies the nonterminal?s position rel-
ative to the boundary of the constituent: it is the
leftmost child, the rightmost child, or neither.
Related to this, we further noticed that several
of Klein & Manning?s (2003) features, such as
marking NP s as right recursive or possessive have
the property of annotating with the label of the
rightmost child (when they are NP and POS re-
spectively). We generalize this by marking all
nodes both with their rightmost child and (an anal-
ogous feature) leftmost child.
We also mark whether or not a node borders
the end of a sentence, save for ending punctuation.
(For instance, in this sentence, all the constituents
with the second ?marked? rightmost in their span
would be marked).
Another Klein and Manning (2003) feature we
try includes the temporal NP feature, where TMP
markings in the treebank are retained, and propa-
gated down the head inheritance path of the tree.
It is worth mentioning that all the features here
come directly from the treebank. For instance, the
part of speech of the head feature has values only
from the raw treebank tag set. When a preterminal
cluster is split, this assignment does not change the
value of this feature.
3 Clustering
The input to the clusterer is a set of annotated
grammar productions and counts. Our clustering
algorithm is a divisive one reminiscent of (Martin
et al, 1995). We start with a single cluster for each
Treebank nonterminal and one additional cluster
for intermediate nodes, which are described in sec-
tion 3.2.
The clustering method has two interleaved
parts: one in which candidate splits are generated,
and one in which we choose a candidate split to
enact.
For each of the initial clusters, we generate a
candidate split, and place that split in a prior-
ity queue. The priority queue is ordered by the
Bayesian Information Criterion (BIC), e.g.(Hastie
et al, 2003).
The BIC of a model M is defined as -2*(log
likelihood of the data according to M ) +dM*(log
number of observations). dM is the number of de-
grees of freedom in the model, which for a PCFG
is the number of productions minus the number
of nonterminals. Thus in this context BIC can be
thought of as optimizing the likelihood, but with a
penalty against grammars with many rules.
While the queue is nonempty, we remove a can-
didate split to reevaluate. Reevaluation is neces-
sary because, if there is a delay between when a
split is proposed and when a split is enacted, the
grammar used to score the split will have changed.
However, we suppose that the old score is close
enough to be a reasonable ordering measure for
the priority queue. If the reevaluated candidate is
no longer better than the second candidate on the
queue, we reinsert it and continue. However, if it
is still the best on the queue, and it improves the
model, we enact the split; otherwise it is discarded.
When a split is enacted, the old cluster is re-
moved from the set of nonterminals, and is re-
placed with the two new nonterminals of the split.
A candidate split for each of the two new clusters
is generated, and placed on the priority queue.
This process of reevaluation, enacting splits,
and generating new candidates continues until the
priority queue is empty of potential splits.
We select a candidate split of a particular cluster
as follows. For each context feature we generate
303
S^ROOT
NP^S
NNP^NP
Rex
CC^NP
and
NNP^NP
Ginger
VP^S
VBD^VP
ran
NP^VP
NN
home
Figure 1: A Parent annotated tree.
a potential nominee split. To do this we first par-
tition randomly the values for the feature into two
buckets. We then repeatedly try to move values
from one bucket to the other. If doing so results
in an improvement to the likelihood of the training
data, we keep the change, otherwise we reject it.
The swapping continues until moving no individ-
ual value results in an improvement in likelihood.
Suppose we have a grammar derived from a cor-
pus of a single tree, whose nodes have been anno-
tated with their parent as in Figure 1. The base
productions for this corpus are:
S[ROOT ] ? NP [S] V P [S] 1/1
V P [S] ? V BD[V P ] NP [V P ] 1/1
NP [S] ? NP [NP ] CC[NP ] NP [NP ] 1/1
NP [V P ] ? NN [NP ] 1/1
NP [NP ] ? NNP [NP ] 2/2
Suppose we are in the initial state, with a single
cluster for each treebank nonterminal. Consider
a potential split of the NP cluster on the par-
ent feature, which in this example has three val-
ues: S, V P , and NP . If the S and V P val-
ues are grouped together in the left bucket, and
the NP value is alone in the right bucket, we get
cluster nonterminals NPL = {NP [S], NP [V P ]}
and NPR = {NP [NP ]}. The resulting grammar
rules and their probabilities are:
S ? NPL V P 1/1
V P ? V BD NPL 1/1
NPL ? NPR CC NPR 1/2
NPL ? NN 1/2
NPR ? NNP 2/2
If however, V P is swapped to the right bucket
with NP , the rules become:
S ? NPL V P 1/1
V P ? V BD NPR 1/1
NPL ? NPR CC NPR 1/1
NPR ? NN 1/3
NPR ? NNP 2/3
The likelihood of the tree in Figure 1 is 1/4 under
the first grammar, but only 4/27 under the second.
Hence in this case we would reject the swap of V P
from the right to the left buckets.
The process of swapping continues until no im-
provement can be made by swapping a single
value.
The likelihood of the training data according to
the clustered grammar is
?
r?R
p(r)C(r)
for R the set of observed productions r = ?i ?
?j . . . in the clustered grammar. Notice that when
we are looking to split a cluster ?, only produc-
tions that contain the nonterminal ? will have
probabilities that change. To evaluate whether a
change increases the likelihood, we consider the
ratio between the likelihood of the new model, and
the likelihood of the old model.
Furthermore, when we move a value from one
bucket to another, only a fraction of the rules will
have their counts change. Suppose we are mov-
ing value x from the left bucket to the right when
splitting ?i. Let ?x ? ?i be the set of base nonter-
minals in ?i that have value x for the feature being
split upon. Only clustered rules that contain base
grammar rules which use nonterminals in ?x will
have their probability change. These observations
allow us to process only a relatively small number
of base grammar rules.
Once we have generated a potential nominee
split for each feature, we select the partitioning
which leads to the greatest improvement in the
BIC as the candidate split of this cluster. This can-
didate is placed on the priority queue.
One odd thing about the above is that in the lo-
cal search phase of the clustering we use likeli-
hood, while in the candidate selection phase we
use BIC. We tried both measures in each phase,
but found that this hybrid measure outperformed
using only one or the other.
3.1 Model Selection
Unfortunately, the grammar that results at the end
of the clustering process seems to overfit the train-
ing data. We resolve this by simply noting period-
ically the intermediate state of the grammar, and
using this grammar to parse a small tuning set (we
use the first 400 sentences of WSJ section 24, and
parse this every 50 times we enact a split). At the
conclusion of clustering, we select the grammar
304
AB C <D> E F
(a)
A
B [C,<D>,E,F]
C [<D>,E,F]
[<D>,E]
D E
F
(b)
Figure 2: (a) A production. (b) The production,
binarized.
with the highest f-score on this tuning set as the
final model.
3.2 Binarization
Since our experiments make use of a CKY
(Kasami, 1965) parser 1 we must modify the tree-
bank derived rules so that each expands to at most
two labels. We perform this in a manner simi-
lar to Klein and Manning (2003) and Matsuzaki
et al (2005) through the creation of intermediate
nodes, as in Figure 2. In this example, the nonter-
minal heir of A?s head is D, indicated in the figure
by marking D with angled brackets. The square
brackets indicate an intermediate node, and the la-
bels inside the brackets indicate that the node will
eventually be expanded into those labels.
Klein and Manning (2003) employ Collins?
(1999) horizontal markovization to desparsify
their intermediate nodes. This means that given
an intermediate node such as [C ?D?EF ] in Fig-
ure 2, we forget those labels which will not be ex-
panded past a certain horizon. Klein and Manning
(2003) use a horizon of two (or less, in some cases)
which means only the next two labels to be ex-
panded are retained. For instance in in this exam-
ple [C ?D?EF ] is markovized to [C ?D? . . . F ],
since C and F are the next two non-intermediate
labels.
Our mechanism lays out the unmarkovized in-
termediate rules in the same way, but we mostly
use our clustering scheme to reduce sparsity. We
do so by aligning the labels contained in the in-
termediate nodes in the order in which they would
be added when increasing the markovization hori-
1The implementation we use was created by Mark John-
son and used for the research in (Johnson, 1998). It is avail-
able at his homepage.
zon from zero to three. We also always keep
the heir label as a feature, following Klein and
Manning (2003). So for instance, [C ?D?EF ]
is represented as having Treebank label ?IN-
TERMEDIATE?, and would have feature vector
(D,C,F,E,D),while [?D?EF ] would have fea-
ture vector (D,F,E,D,?), where the first item
is the heir of the parent?s head. The ?-? in-
dicates that the fourth item to be expanded is
here non-existent. The clusterer would consider
each of these five features as for a single pos-
sible split. We also incorporate our other fea-
tures into the intermediate nodes in two ways.
Some features, such as the parent or grandpar-
ent, will be the same for all the labels in the in-
termediate node, and hence only need to be in-
cluded once. Others, such as the part of speech
of the head, may be different for each label. These
features we align with those of corresponding la-
bel in the Markov ordering. In our running ex-
ample, suppose each child node N has part of
speech of its head PN , and we have a parent fea-
ture. Our aligned intermediate feature vectors then
become (A,D,C, PC , F, PF , E, PE ,D, PD) and
(A,D,F, PF , E, PE ,D, PD,?,?). As these are
somewhat complicated, let us explain them by un-
packing the first, the vector for [C ?D?EF ]. Con-
sulting Figure 2 we see that its parent is A. We
have chosen to put parents first in the vector, thus
explaining (A, ...). Next comes the heir of the
constituent, D. This is followed by the first con-
stituent that is to be unpacked from the binarized
version, C , which in turn is followed by its head
part-of-speech PC , giving us (A,D,C, PC , ...).
We follow with the next non-terminal to be un-
packed from the binarized node and its head part-
of-speech, etc.
It might be fairly objected that this formulation
of binarization loses the information of whether a
label is to the left, right, or is the heir of the par-
ent?s head. This is solved by the inside position
feature, described in Section 2.1 which contains
exactly this information.
3.3 Smoothing
In order to ease comparison between our work
and that of Klein and Manning (2003), we follow
their lead in smoothing no production probabilities
save those going from preterminal to nonterminal.
Our smoothing mechanism runs roughly along the
lines of theirs.
305
LP LR F1 CB 0CB
Klein & Manning 86.3 85.1 85.7 1.31 57.2
Matsuzaki et al 86.1 86.0 86.1 1.39 58.3
This paper 84.8 84.8 84.8 1.47 57.1
Table 1: Parsing results on final test set (Section
23).
Run LP LR F1 CB 0CB
1 85.3 85.6 85.5 1.29 59.5
2 85.8 85.9 85.9 1.29 59.4
3 85.1 85.5 85.3 1.36 58.0
4 85.3 85.7 85.5 1.30 59.9
Table 2: Parsing results for grammars generated
using clusterer with different random seeds. All
numbers here are on the development test set (Sec-
tion 22).
Preterminal rules are smoothed as follows. We
consider several classes of unknown words, based
on capitalization, the presence of digits or hy-
phens, and the suffix. We estimate the probabil-
ity of a tag T given a word (or unknown class)
W , as p(T | W ) = C(T,W )+hp(T |unk)C(W )+h , where
p(T | unk) = C(T, unk)/C(unk) is the prob-
ability of the tag given any unknown word class.
In order to estimate counts of unknown classes,we
let the clusterer see every tree twice: once un-
modified, and once with the unknown class re-
placing each word seen less than five times. The
production probability p(W | T ) is then p(T |
W )p(W )/p(T ) where p(W ) and p(T ) are the re-
spective empirical distributions.
The clusterer does not use smoothed probabil-
ities in allocating annotated preterminals to clus-
ters, but simply the maximum likelihood estimates
as it does elsewhere. Smoothing is only used in the
parser.
4 Experiments
We trained our model on sections 2-21 of the Penn
Wall Street Journal Treebank. We used the first
400 sentences of section 24 for model selection.
Section 22 was used for testing during develop-
ment, while section 23 was used for the final eval-
uation.
5 Discussion
Our results are shown in Table 1. The first three
columns show the labeled precision, recall and f-
measure, respectively. The remaining two show
the number of crossing brackets per sentence,
and the percentage of sentences with no crossing
brackets.
Unfortunately, our model does not perform
quite as well as those of Klein and Manning (2003)
or Matsuzaki et al (2005). It is worth noting that
Matsuzaki?s grammar uses a different parse evalu-
ation scheme than Klein & Manning or we do.
We select the parse with the highest probability
according to the annotated grammar. Matsuzaki,
on the other hand, argues that the proper thing to
do is to find the most likely unannotated parse.
The probability of this parse is the sum over the
probabilities of all annotated parses that reduce
to that unannotated parse. Since calculating the
parse that maximizes this quantity is NP hard, they
try several approximations. One is what Klein &
Manning and we do. However, they have a better
performing approximation which is used in their
reported score. They do not report their score
on section 23 using the most-probable-annotated-
parse method. They do however compare the per-
formance of different methods using development
data, and find that their better approximation gives
an absolute improvement in f-measure in the .5-1
percent range. Hence it is probable that even with
their better method our grammar would not out-
perform theirs.
Table 2 shows the results on the development
test set (Section 22) for four different initial ran-
dom seeds. Recall that when splitting a cluster, the
initial partition of the base grammar nonterminals
is made randomly. The model from the second run
was used for parsing the final test set (Section 23)
in Table 1.
One interesting thing our method allows is for
us to examine which features turn out to be useful
in which contexts. We noted for each trereebank
nonterminal, and for each feature, how many times
that nonterminal was split on that feature, for the
grammar selected in the model selection stage. We
ran the clustering with these four different random
seeds.
We find that in particular, the clusterer only
found the head feature to be useful in very spe-
cific circumstances. It was used quite a bit to
split preterminals; but for phrasals it was only
used to split ADJP ,ADV P ,NP ,PP ,V P ,QP ,
and SBAR. The part of speech of the head was
only used to split NP and V P .
Furthermore, the grandparent tag appears to be
of importance primarily for V P and PP nonter-
306
minals, though it is used once out of the four runs
for NP s.
This indicates that perhaps lexical parsers might
be able to make do by only using lexical head and
grandparent information in very specific instances,
thereby shrinking the sizes of their models, and
speeding parsing. This warrants further investiga-
tion.
6 Conclusion
We have presented a scheme for automatically
discovering phrasal categories for parsing with a
standard CKY parser. The parser achieves 84.8%
precision-recall f-measure on the standard test-
section of the Penn WSJ-Treebank (section 23).
While this is not as accurate as the hand-tailored
grammar of Klein and Manning (2003), it is close,
and we believe there is room for improvement.
For starters, the particular clustering scheme is
only one of many. Our algorithm splits clus-
ters along particular features (e.g., parent, head-
part-of-speech, etc.). One alternative would be to
cluster simultaneously on all the features. It is
not obvious which scheme should be better, and
they could be quite different. Decisions like this
abound, and are worth exploring.
More radically, it is also possible to grow many
decision trees, and thus many alternative gram-
mars. We have been impressed by the success of
random-forest methods in language modeling (Xu
and Jelinek, 2004). In these methods many trees
(the forest) are grown, each trying to predict the
next word. The multiple trees together are much
more powerful than any one individually. The
same might be true for grammars.
Acknowledgement
The research presented here was funded in part by
DARPA GALE contract HR 0011-06-20001.
References
Anja Belz. 2002. Learning grammars for different
parsing tasks by partition search. In Proceedings of
the 19th international conference on Computational
Linguistics, pages 1?7.
Eugene Charniak. 1996. Tree-bank grammars. In
Proceedings of the Thirteenth National Conference
on Artificial Intelligence, pages 1031?1036. AAAI
Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139.
Michael J. Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In The Pro-
ceedings of the 34th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 184?191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, The
University of Pennsylvania.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2003. The Elements of Statistical Learning.
Springer, New York.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of HLT-NAACL 2003, pages 25?31.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Tadao Kasami. 1965. An efficient recognition and syn-
tax algorithm for context-free languages. Technical
Report AF-CRL-65-758, Air Force Cambridge Re-
search Laboratory.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In The Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 276?283.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Sven Martin, Jo?rg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word cluster-
ing. In Proceedings of the European Conference
on Speech, Communication and Technology, pages
1253?1256.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 2005 Meeting of the Association
for Computational Linguistics.
Mehryar Mohri and Brian Roark. 2006. Effective self-
training for parsing. In Proceedings of HLT-NAACL
2006.
Peng Xu and Fred Jelinek. 2004. Random forests
in language modeling. In Proceedings of EMNLP
2004.
307
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20?27,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Unsupervised word segmentation for Sesotho using Adaptor Grammars
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Abstract
This paper describes a variety of non-
parametric Bayesian models of word segmen-
tation based on Adaptor Grammars that model
different aspects of the input and incorporate
different kinds of prior knowledge, and ap-
plies them to the Bantu language Sesotho.
While we find overall word segmentation ac-
curacies lower than these models achieve on
English, we also find some interesting dif-
ferences in which factors contribute to better
word segmentation. Specifically, we found lit-
tle improvement to word segmentation accu-
racy when we modeled contextual dependen-
cies, while modeling morphological structure
did improve segmentation accuracy.
1 Introduction
A Bayesian approach to learning (Bishop, 2006) is
especially useful for computational models of lan-
guage acquisition because we can use it to study
the effect of different kinds and amounts of prior
knowledge on the learning process. The Bayesian
approach is agnostic as to what this prior knowl-
edge might consist of; the prior could encode the
kinds of rich universal grammar hypothesised by
e.g., Chomsky (1986), or it could express a vague
non-linguistic preference for simpler as opposed to
more complex models, as in some of the grammars
discussed below. Clearly there?s a wide range of
possible priors, and one of the exciting possibilities
raised by Bayesian methods is that we may soon be
able to empirically evaluate the potential contribu-
tion of different kinds of prior knowledge to lan-
guage learning.
The Bayesian framework is surprisingly flexible.
The bulk of the work on Bayesian inference is on
parametric models, where the goal is to learn the
value of a set of parameters (much as in Chomsky?s
Principles and Parameters conception of learning).
However, recently Bayesian methods for nonpara-
metric inference have been developed, in which the
parameters themselves, as well as their values, are
learned from data. (The term ?nonparametric? is
perhaps misleading here: it does not mean that the
models have no parameters, rather it means that the
learning process considers models with different sets
of parameters). One can think of the prior as pro-
viding an infinite set of possible parameters, from
which a learner selects a subset with which to model
their language.
If one pairs each of these infinitely-many pa-
rameters with possible structures (or equivalently,
rules that generate such structures) then these non-
parametric Bayesian learning methods can learn
the structures relevant to a language. Determining
whether methods such as these can in fact learn lin-
guistic structure bears on the nature vs. nurture de-
bates in language acquisition, since one of the argu-
ments for the nativist position is that there doesn?t
seem to be a way to learn structure from the input
that children receive.
While there?s no reason why these methods can?t
be used to learn the syntax and semantics of human
languages, much of the work to date has focused on
lower-level learning problems such as morphologi-
cal structure learning (Goldwater et al, 2006b) and
word segmentation, where the learner is given un-
segmented broad-phonemic utterance transcriptions
20
and has to identify the word boundaries (Goldwater
et al, 2006a; Goldwater et al, 2007). One reason for
this is that these problems seem simpler than learn-
ing syntax, where the non-linguistic context plausi-
bly supplies important information to human learn-
ers. Virtually everyone agrees that the set of possible
morphemes and words, if not infinite, is astronom-
ically large, so it seems plausible that humans use
some kind of nonparametric procedure to learn the
lexicon.
Johnson et al (2007) introduced Adaptor Gram-
mars as a framework in which a wide variety
of linguistically-interesting nonparametric inference
problems can be formulated and evaluated, includ-
ing a number of variants of the models described by
Goldwater (2007). Johnson (2008) presented a vari-
ety of different adaptor grammar word segmentation
models and applied them to the problem of segment-
ing Brent?s phonemicized version of the Bernstein-
Ratner corpus of child-directed English (Bernstein-
Ratner, 1987; Brent, 1999). The main results of that
paper were the following:
1. it confirmed the importance of modeling con-
textual dependencies above the word level for
word segmentation (Goldwater et al, 2006a),
2. it showed a small but significant improvement
to segmentation accuracy by learning the possi-
ble syllable structures of the language together
with the lexicon, and
3. it found no significant advantage to learning
morphological structure together with the lex-
icon (indeed, that model confused morphologi-
cal and lexical structure).
Of course the last result is a null result, and it?s pos-
sible that a different model would be able to usefuly
combine morphological learning with word segmen-
tation.
This paper continues that research by applying
the same kinds of models to Sesotho, a Bantu lan-
guage spoken in Southern Africa. Bantu languages
are especially interesting for this kind of study, as
they have rich productive agglutinative morpholo-
gies and relatively transparent phonologies, as com-
pared to languages such as Finnish or Turkish which
have complex harmony processes and other phono-
logical complexities. The relative clarity of Bantu
has inspired previous computational work, such as
the algorithm for learning Swahili morphology by
Hu et al (2005). The Hu et al algorithm uses
a Minimum Description Length procedure (Rissa-
nen, 1989) that is conceptually related to the non-
parametric Bayesian procedure used here. However,
the work here is focused on determining whether the
word segmentation methods that work well for En-
glish generalize to Sesotho and whether modeling
morphological and/or syllable structure improves
Sesotho word segmentation, rather than learning
Sesotho morphological structure per se.
The rest of this paper is structured as follows.
Section 2 informally reviews adaptor grammars and
describes how they are used to specify different
Bayesian models. Section 3 describes the Sesotho
corpus we used and the specific adaptor grammars
we used for word segmentation, and section 5 sum-
marizes and concludes the paper.
2 Adaptor grammars
One reason why Probabilistic Context-Free Gram-
mars (PCFGs) are interesting is because they are
very simple and natural models of hierarchical struc-
ture. They are parametric models because each
PCFG has a fixed number of rules, each of which
has a numerical parameter associated with it. One
way to construct nonparametric Bayesian models is
to take a parametric model class and let one or more
of their components grow unboundedly.
There are two obvious ways to construct nonpara-
metric models from PCFGs. First, we can let the
number of nonterminals grow unboundedly, as in the
Infinite PCFG, where the nonterminals of the gram-
mar can be indefinitely refined versions of a base
PCFG (Liang et al, 2007). Second, we can fix the
set of nonterminals but permit the number of rules
or productions to grow unboundedly, which leads to
Adaptor Grammars (Johnson et al, 2007).
At any point in learning, an Adaptor Grammar has
a finite set of rules, but these can grow unbound-
edly (typically logarithmically) with the size of the
training data. In a word-segmentation application
these rules typically generate words or morphemes,
so the learner is effectively learning the morphemes
and words of its language.
The new rules learnt by an Adaptor Grammar are
21
compositions of old ones (that can themselves be
compositions of other rules), so it?s natural to think
of these new rules as tree fragments, where each
entire fragment is associated with its own proba-
bility. Viewed this way, an adaptor grammar can
be viewed as learning the tree fragments or con-
structions involved in a language, much as in Bod
(1998). For computational reasons adaptor gram-
mars require these fragments to consist of subtrees
(i.e., their yields are terminals).
We now provide an informal description of Adap-
tor Grammars (for a more formal description see
Johnson et al (2007)). An adaptor grammar con-
sists of terminals V , nonterminals N (including a
start symbol S), initial rules R and rule probabilities
p, just as in a PCFG. In addition, it also has a vec-
tor of concentration parameters ?, where ?A ? 0 is
called the (Dirichlet) concentration parameter asso-
ciated with nonterminal A.
The nonterminals A for which ?A > 0 are
adapted, which means that each subtree for A that
can be generated using the initial rules R is consid-
ered as a potential rule in the adaptor grammar. If
?A = 0 then A is unadapted, which means it ex-
pands just as in an ordinary PCFG.
Adaptor grammars are so-called because they
adapt both the subtrees and their probabilities to the
corpus they are generating. Formally, they are Hi-
erarchical Dirichlet Processes that generate a distri-
bution over distributions over trees that can be de-
fined in terms of stick-breaking processes (Teh et al,
2006). It?s probably easiest to understand them in
terms of their conditional or sampling distribution,
which is the probability of generating a new tree T
given the trees (T1, . . . , Tn) that the adaptor gram-
mar has already generated.
An adaptor grammar can be viewed as generating
a tree top-down, just like a PCFG. Suppose we have
a node A to expand. If A is unadapted (i.e., ?A = 0)
then A expands just as in a PCFG, i.e., we pick a
rule A ? ? ? R with probability pA?? and recur-
sively expand ?. If A is adapted and has expanded
nA times before, then:
1. A expands to a subtree ? with probability
n?/(nA+?A), where n? is the number of times
A has expanded to subtree ? before, and
2. A expands to ? where A ? ? ? R with prob-
ability ?A pA??/(nA + ?A).
Thus an adapted nonterminal A expands to a previ-
ously expanded subtree ? with probability propor-
tional to the number n? of times it was used before,
and expands just as in a PCFG (i.e., using R) with
probability proportional to the concentration param-
eter ?A. This parameter specifies how likely A is to
expand into a potentially new subtree; as nA and n?
grow this becomes increasingly unlikely.
We used the publically available adaptor gram-
mar inference software described in Johnson et al
(2007), which we modified slightly as described be-
low. The basic algorithm is a Metropolis-within-
Gibbs or Hybrid MCMC sampler (Robert and
Casella, 2004), which resamples the parse tree for
each sentence in the training data conditioned on the
parses for the other sentences. In order to produce
sample parses efficiently the algorithm constructs a
PCFG approximation to the adaptor grammar which
contains one rule for each adapted subtree ?, and
uses a Metropolis accept/reject step to correct for the
difference between the true adaptor grammar dis-
tribution and the PCFG approximation. With the
datasets described below less than 0.1% of proposal
parses from this PCFG approximation are rejected,
so it is quite a good approximation to the adaptor
grammar distribution.
On the other hand, at convergence this algorithm
produces a sequence of samples from the posterior
distribution over adaptor grammars, and this poste-
rior distribution seems quite broad. For example,
at convergence with the most stable of our models,
each time a sentence?s parse is resampled there is
an approximately 25% chance of the parse chang-
ing. Perhaps this is not surprising given the com-
paratively small amount of training data and the fact
that the models only use fairly crude distributional
information.
As just described, adaptor grammars require the
user to specify a concentration parameter ?A for
each adapted nonterminal A. It?s not obvious how
this should be done. Previous work has treated ?A
as an adjustable parameter, usually tying all of the
?A to some shared value which is adjusted to opti-
mize task performance (say, word segmentation ac-
curacy). Clearly, this is undesirable.
Teh et al (2006) describes how to learn the con-
22
centration parameters ?, and we modified their pro-
cedure for adaptor grammars. Specifically, we put
a vague Gamma(10, 0.1) prior on each ?A, and af-
ter each iteration through the training data we per-
formed 100 Metropolis-Hastings resampling steps
for each ?A from an increasingly narrow Gamma
proposal distribution. We found that the perfor-
mance of the models with automatically learned
concentration parameters ? was generally as good
as the models where ? was tuned by hand (although
admittedly we only tried three or four different val-
ues for ?).
3 Models of Sesotho word segmentation
We wanted to make our Sesotho corpus as similar
as possible to one used in previous work on word
segmentation. We extracted all of the non-child
utterances from the LI?LV files from the Sesotho
corpus of child speech (Demuth, 1992), and used
the Sesotho gloss as our gold-standard corpus (we
did not phonemicize them as Sesotho orthography
is very close to phonemic). This produced 8,503
utterances containing 21,037 word tokens, 30,200
morpheme tokens and 100,113 phonemes. By com-
parison, the Brent corpus contains 9,790 utterances,
33,399 word tokens and 95,809 phonemes. Thus
the Sesotho corpus contains approximately the same
number of utterances and phonemes as the Brent
corpus, but far fewer (and hence far longer) words.
This is not surprising as the Sesotho corpus involves
an older child and Sesotho, being an agglutinative
language, tends to have morphologically complex
words.
In the subsections that follow we describe a vari-
ety of adaptor grammar models for word segmenta-
tion. All of these models were given same Sesotho
data, which consisted of the Sesotho gold-standard
corpus described above with all word boundaries
(spaces) and morpheme boundaries (hyphens) re-
moved. We computed the f-score (geometric aver-
age of precision and recall) with which the models
recovered the words or the morphemes annotated in
the gold-standard corpus.
3.1 Unigram grammar
We begin by describing an adaptor grammar that
simulates the unigram word segmentation model
Model word f-score morpheme f-score
word 0.431 0.352
colloc 0.478 0.387
colloc2 0.467 0.389
word ? syll 0.502 0.349
colloc? syll 0.476 0.372
colloc2? syll 0.490 0.393
word ?morph 0.529 0.321
word ? smorph 0.556 0.378
colloc? smorph 0.537 0.352
Table 1: Summary of word and morpheme f-scores for
the different models discussed in this paper.
proposed by Goldwater et al (2006a). In this model
each utterance is generated as a sequence of words,
and each word is a sequence of phonemes. This
grammar contains three kinds of rules, including
rules that expand the nonterminal Phoneme to all of
the phonemes seen in the training data.
Sentence ? Word+
Word ? Phoneme+
Adapted non-terminals are indicated by underlin-
ing, so in the word grammar only the Word nonter-
minal is adapted. Our software doesn?t permit reg-
ular expressions in rules, so we expand all Kleene
stars in rules into right-recursive structures over new
unadapted nonterminals. Figure 1 shows a sample
parse tree generated by this grammar for the sen-
tence:
u-
SM-
e-
OM-
nk-
take-
il-
PERF-
e
IN
kae
where
?You took it from where??
This sentence shows a typical inflected verb, with a
subject marker (glossed SM), an object marker (OM),
perfect tense marker (PERF) and mood marker (IN).
In order to keep the trees a managable size, we only
display the root node, leaf nodes and nodes labeled
with adapted nonterminals.
The word grammar has a word segmentation f-
score of 43%, which is considerably below the 56%
f-score the same grammar achieves on the Brent cor-
pus. This difference presumably reflects the fact that
Sesotho words are longer and more complex, and so
segmentation is a harder task.
We actually ran the adaptor grammar sampler for
23
Sentence
Word
u e n k i l e
Word
k a e
Figure 1: A sample (correct) parse tree generated by the
word adaptor grammar for a Sesotho utterance.
the word grammar four times (as we did for all gram-
mars discussed in this paper). Because the sampler
is non-deterministic, each run produced a different
series of sample segmentations. However, the av-
erage segmentation f-score seems to be very stable.
The accuracies of the final sample of the four runs
ranges between 42.8% and 43.7%. Similarly, one
can compute the average f-score over the last 100
samples for each run; the average f-score ranges be-
tween 42.6% and 43.7%. Thus while there may
be considerable uncertainty as to where the word
boundaries are in any given sentence (which is re-
flected in fact that the word boundaries are very
likely to change from sample to sample), the aver-
age accuracy of such boundaries seems very stable.
The final sample grammars contained the initial
rules R, together with between 1,772 and 1,827 ad-
ditional expansions for Word, corresponding to the
cached subtrees for the adapted Word nonterminal.
3.2 Collocation grammar
Goldwater et al (2006a) showed that incorporating a
bigram model of word-to-word dependencies signif-
icantly improves word segmentation accuracy in En-
glish. While it is not possible to formulate such a bi-
gram model as an adaptor grammar, Johnson (2008)
showed that a similar improvement can be achieved
in an adaptor grammar by explicitly modeling col-
locations or sequences of words. The colloc adaptor
grammar is:
Sentence ? Colloc+
Colloc ? Word+
Word ? Phoneme+
This grammar generates a Sentence as a sequence
of Colloc(ations), where each Colloc(ation) is a se-
quence of Words. Figure 2 shows a sample parse tree
generated by the colloc grammar. In terms of word
segmentation, this grammar performs much worse
Sentence
Colloc
Word
u e
Word
n
Word
k i l e
Colloc
Word
k a
Colloc
Word
e
Figure 2: A sample parse tree generated by the colloc
grammar. The substrings generated by Word in fact tend
to be morphemes and Colloc tend to be words, which is
how they are evaluated in Table 1.
than the word grammar, with an f-score of 27%.
In fact, it seems that the Word nonterminals typ-
ically expand to morphemes and the Colloc nonter-
minals typically expand to words. It makes sense
that for a language like Sesotho, when given a gram-
mar with a hierarchy of units, the learner would use
the lower-level units as morphemes and the higher-
level units as words. If we simply interpret the Word
trees as morphemes and the Colloc trees as words
we get a better word segmentation accuracy of 48%
f-score.
3.3 Adding more levels
If two levels are better than one, perhaps three levels
would be better than two? More specifically, per-
haps adding another level of adaptation would per-
mit the model to capture the kind of interword con-
text dependencies that improved English word seg-
mentation. Our colloc2 adaptor grammar includes
the following rules:
Sentence ? Colloc+
Colloc ? Word+
Word ? Morph+
Morph ? Phoneme+
This grammar generates sequences of Words
grouped together in collocations, as in the previous
grammar, but each Word now consists of a sequence
of Morph(emes). Figure 3 shows a sample parse tree
generated by the colloc2 grammar.
Interestingly, word segmentation f-score is
46.7%, which is slightly lower than that obtained
by the simpler colloc grammar. Informally, it seems
that when given an extra level of structure the
colloc2 model uses it to describe structure internal
24
Sentence
Colloc
Word
Morph
u
Morph
e
Word
Morph
n k i
Morph
l e
Word
Morph
k a
Morph
e
Figure 3: A sample parse tree generated by the colloc2
grammar.
to the word, rather than to capture interword depen-
dencies. Perhaps this shouldn?t be surprising, since
Sesotho words in this corpus are considerably more
complex than the English words in the Brent corpus.
4 Adding syllable structure
Johnson (2008) found a small but significant im-
provement in word segmentation accuracy by using
an adaptor grammar that models English words as
a sequence of syllables. The word? syll grammar
builds in knowledge that syllables consist of an op-
tional Onset, a Nuc(leus) and an optional Coda, and
knows that Onsets and Codas are composes of con-
sonants and that Nucleii are vocalic (and that syl-
labic consonsants are possible Nucleii), and learns
the possible syllables of the language. The rules in
the adaptor grammars that expand Word are changed
to the following:
Word ? Syll+
Syll ? (Onset) Nuc (Coda)
Syll ? SC
Onset ? C+
Nuc ? V+
Coda ? C+
In this grammar C expands to any consonant and V
expands to any vowel, SC expands to the syllablic
consonants ?l?, ?m? ?n? and ?r?, and parentheses indi-
cate optionality. Figure 4 shows a sample parse tree
produced by the word ? syll adaptor grammar (i.e.,
where Words are generated by a unigram model),
while Figure 5 shows a sample parse tree generated
by the corresponding colloc? syll adaptor grammar
(where Words are generated as a part of a Colloca-
tion).
Sentence
Word
Syll
u
Syll
e
Syll
n k i
Syll
l e
Word
Syll
k a e
Figure 4: A sample parse tree generated by the
word? syll grammar, in which Words consist of se-
quences of Syll(ables).
Sentence
Colloc
Word
Syll
u
Word
Syll
e
Word
Syll
n k i
Syll
l e
Colloc
Word
Syll
k a e
Figure 5: A sample parse tree generated by the
colloc? syll grammar, in which Colloc(ations) consist of
sequences of Words, which in turn consist of sequences
of Syll(ables).
Building in this knowledge of syllable struc-
ture does improve word segmentation accuracy,
but the best performance comes from the simplest
word ? syll grammar (with a word segmentation f-
score of 50%).
4.1 Tracking morphological position
As we noted earlier, the various Colloc grammars
wound up capturing a certain amount of morpholog-
ical structure, even though they only implement a
relatively simple unigram model of morpheme word
order. Here we investigate whether we can im-
prove word segmentation accuracy with more so-
phisticated models of morphological structure.
The word?morph grammar generates a word as
a sequence of one to five morphemes. The relevant
productions are the following:
Word ? T1 (T2 (T3 (T4 (T5))))
T1 ? Phoneme+
T2 ? Phoneme+
T3 ? Phoneme+
T4 ? Phoneme+
T5 ? Phoneme+
25
Sentence
Word
T1
u e
T2
n k i l e
T3
k a e
Figure 6: A sample parse tree generated by the
word?morph grammar, in which Words consist of mor-
phemes T1?T5, each of which is associated with specific
lexical items.
While each morpheme is generated by a unigram
character model, because each of these five mor-
pheme positions is independently adapted, the gram-
mar can learn which morphemes prefer to appear in
which position. Figure 6 contains a sample parse
generated by this grammar. Modifying the gram-
mar in this way significantly improves word seg-
mentation accuracy, achieving a word segmentation
f-score of 53%.
Inspired by this, we decided to see what would
happen if we built-in some specific knowledge of
Sesotho morphology, namely that a word consists of
a stem plus an optional suffix and zero to three op-
tional prefixes. (This kind of information is often
built into morphology learning models, either ex-
plicitly or implicitly via restrictions on the search
procedure). The resulting grammar, which we call
word ? smorph, generates words as follows:
Word ? (P1 (P2 (P3))) T (S)
P1 ? Phoneme+
P2 ? Phoneme+
P3 ? Phoneme+
T ? Phoneme+
S ? Phoneme+
Figure 7 contains a sample parse tree generated
by this grammar. Perhaps not surprisingly, with this
modification the grammar achieves the highest word
segmentation f-score of any of the models examined
in this paper, namely 55.6%.
Of course, this morphological structure is per-
fectly compatible with models which posit higher-
level structure than Words. We can replace the Word
expansion in the colloc grammar with one just given;
the resulting grammar is called colloc? smorph,
and a sample parse tree is given in Figure 8. Interest-
Sentence
Word
P1
u
P2
e
T
n k
S
i l e
Word
T
k a
S
e
Figure 7: A sample parse tree generated by the
word? smorph grammar, in which Words consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
Sentence
Colloc
Word
P1
u e
T
n
S
k i l e
Word
T
k a
S
e
Figure 8: A sample parse tree generated by the
colloc? smorph grammar, in which Colloc(ations) gen-
erate a sequence of Words, which in turn consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
ingly, this grammar achieves a lower accuracy than
either of the two word-based morphology grammars
we considered above.
5 Conclusion
Perhaps the most important conclusion to be drawn
from this paper is that the methods developed for
unsupervised word segmentation for English also
work for Sesotho, despite its having radically dif-
ferent morphological structures to English. Just as
with English, more structured adaptor grammars can
achieve better word-segmentation accuracies than
simpler ones. While we find overall word segmen-
tation accuracies lower than these models achieve
on English, we also found some interesting differ-
ences in which factors contribute to better word seg-
mentation. Perhaps surprisingly, we found little
improvement to word segmentation accuracy when
we modeled contextual dependencies, even though
these are most important in English. But includ-
ing either morphological structure or syllable struc-
ture in the model improved word segmentation accu-
26
racy markedly, with morphological structure making
a larger impact. Given how important morphology is
in Sesotho, perhaps this is no surprise after all.
Acknowledgments
I?d like to thank Katherine Demuth for the Sesotho
data and help with Sesotho morphology, my collabo-
rators Sharon Goldwater and Tom Griffiths for their
comments and suggestions about adaptor grammars,
and the anonymous SIGMORPHON reviewers for
their careful reading and insightful comments on the
original abstract. This research was funded by NSF
awards 0544127 and 0631667.
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71?105.
Noam Chomsky. 1986. Knowledge of Language: Its
Nature, Origin and Use. Praeger, New York.
Katherine Demuth. 1992. Acquisition of Sesotho.
In Dan Slobin, editor, The Cross-Linguistic Study
of Language Acquisition, volume 3, pages 557?638.
Lawrence Erlbaum Associates, Hillsdale, N.J.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673?680, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239?250, Somerville,
MA. Cascadilla Press.
Sharon Goldwater. 2007. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Yu Hu, Irina Matveeva, John Goldsmith, and Colin
Sprague. 2005. Refining the SED heuristic for mor-
pheme discovery: Another look at Swahili. In Pro-
ceedings of the Workshop on Psychocomputational
Models of Human Language Acquisition, pages 28?35,
Ann Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697.
Rissanen. 1989. Stochastic Complexity in Statistical In-
quiry. World Scientific Company, Singapore.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566?1581.
27
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 3?11,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
How the statistical revolution changes (computational) linguistics
Mark Johnson
Cognitive and Linguistic Sciences and Computer Science
Brown University
Mark Johnson@Brown.edu
Abstract
This paper discusses some of the ways that
the ?statistical revolution? has changed
and continues to change the relationship
between linguistics and computational lin-
guistics. I claim that it is more useful in
parsing to make an open world assumption
about possible linguistic structures, rather
than the closed world assumption usu-
ally made in grammar-based approaches to
parsing, and I sketch two different ways in
which grammar-based approaches might
be modified to achieve this. I also de-
scribe some of the ways in which proba-
bilistic models are starting to have a sig-
nificant impact on psycholinguistics and
language acquisition. In language acqui-
sition Bayesian techniques may let us em-
pirically evaluate the role of putative uni-
versals in universal grammar.
1 Introduction
The workshop organizers asked us to write some-
thing controversial to stimulate discussion, and
I?ve attempted to do that here. Usually in my pa-
pers I try to stick to facts and claims that I can sup-
port, but here I have fearlessly and perhaps fool-
ishly gone out on a limb and presented guesses,
hunches and opinions. Take them with a grain of
salt. Inspired by Wanamaker?s well-known quote
about advertising, I expect that half of the ideas
I?m proposing here are wrong, but I don?t know
which half. I hope the conference will help me
figure that out.
Statistical techniques have revolutionized many
scientific fields in the past two decades, including
computational linguistics. This paper discusses
the impact of this on the relationship between
computational linguistics and linguistics. I?m pre-
senting a personal perspective rather than a scien-
tific review here, and for this reason I focus on ar-
eas I have some experience with. I begin by dis-
cussing how the statistical perspective changed my
understanding of the relationship between linguis-
tic theory, grammars and parsing, and then go on
to describe some of the ways that ideas from statis-
tics and machine learning are starting to have an
impact on linguistics today.
Before beginning, I?d like to say something
about what I think computational linguistics is. I
view computational linguistics as having both a
scientific and an engineering side. The engineer-
ing side of computational linguistics, often called
natural language processing (NLP), is largely con-
cerned with building computational tools that do
useful things with language, e.g., machine trans-
lation, summarization, question-answering, etc.
Like any engineering discipline, natural language
processing draws on a variety of different scien-
tific disciplines.
I think it?s fair to say that in the current state
of the art, natural language processing draws far
more heavily on statistics and machine learning
than it does on linguistic theory. For example, one
might claim that all an NLP engineer really needs
to understand about linguistic theory are (say) the
parts of speech (POS). Assuming this is true (I?m
not sure it is), would it indicate that there is some-
thing wrong with either linguistic theory or com-
putational linguistics? I don?t think it does: there?s
no reason to expect an engineering solution to uti-
lize all the scientific knowledge of a related field.
The fact that you can build perfectly good bridges
with Newtonian mechanics says nothing about the
truth of quantum mechanics.
I also believe that there is a scientific field of
computational linguistics. This scientific field ex-
ists not just because computers are incredibly use-
ful for doing linguistics ? I expect that comput-
ers have revolutionized most fields of science ?
but because it makes sense to think of linguis-
3
tic processes as being essentially computational in
nature. If we take computation to be the manip-
ulation of symbols in a meaning-respecting way,
then it seems reasonable to hypothesize that lan-
guage comprehension, production and acquisition
are all computational processes. Viewed this way,
we might expect computational linguistics to in-
teract most strongly with those areas of linguis-
tics that study linguistic processing, namely psy-
cholinguistics and language acquisition. As I ex-
plain in section 3 below, I think we are starting to
see this happen.
2 Grammar-based and statistical parsing
In some ways the 1980s were a golden age for
collaboration and cross-fertilization between lin-
guistic theory and computational linguistics, es-
pecially between syntax and parsing. Gazdar
and colleagues showed that Chomskyian transfor-
mations could be supplanted by computationally
much simpler feature passing mechanisms (Gaz-
dar et al, 1985), and this lead to an explosion of
work on ?unification-based? grammars (Shieber,
1986), including the Lexical-Functional Gram-
mars and Head-driven Phrase Structure Grammars
that are still very actively pursued today. I?ll call
the work on parsing within this general framework
the grammar-based approach in order to contrast
it with the statistical approach that doesn?t rely on
these kinds of grammars. I think the statistical ap-
proach has come to dominate computational lin-
guistics, and in this section I?ll describe why this
happened.
Before beginning I think it?s useful to clarify our
goals for building parsers. There are many reasons
why one might build any computational system
? perhaps it?s a part of a commercial product we
hope will make us rich, or perhaps we want to test
the predictions of a certain theory of processing
? and these reasons should dictate how and even
whether the system is constructed. I?m assuming
in this section that we want to build parsers be-
cause we expect the representations they produce
will be useful for various other NLP engineering
tasks. This means that parser design is itself essen-
tially an engineering task, i.e., we want a device
that returns parses that are accurate as possible for
as many sentences as possible.
I?ll begin by discussing a couple of differ-
ences between the approaches that are often men-
tioned but I don?t think are really that impor-
tant. The grammar-based approaches are some-
times described as producing deeper representa-
tions that are closer to meaning. It certainly is
true that grammar-based analyses typically repre-
sent predicate-argument structure and perhaps also
quantifier scope. But one can recover predicate-
argument structure using statistical methods (see
the work on semantic role labeling and ?Prop-
Bank? parsing (Palmer et al, 2005)), and pre-
sumably similar methods could be used to resolve
quantifier scope as well.
I suspect the main reason why statistical pars-
ing has concentrated on more superficial syntac-
tic structure (such as phrase structure) is because
there aren?t many actual applications for the syn-
tactic analyses our parsers return. Given the cur-
rent state-of-the-art in knowledge representation
and artificial intelligence, even if we could pro-
duce completely accurate logical forms in some
higher-order logic, it?s not clear whether we could
do anything useful with them. It?s hard to find real
applications that benefit from even syntactic infor-
mation, and the information any such applications
actually use is often fairly superficial. For exam-
ple, some research systems for named entity de-
tection and extraction use parsing to identify noun
phrases (which are potentially name entities) as
well as the verbs that govern them, but they ignore
the rest of the syntactic structure. In fact, many
applications of statistical parsers simply use them
as language models, i.e., one parses to obtain the
probability that the parser assigns to the string and
throws away the parses it computes in the process
(Jelinek, 2004). (It seems that such parsing-based
language models are good at preferring strings that
are at least superficially grammatical, e.g., where
each clause contains one verb phrase, which is
useful in applications such as summarization and
machine translation).
Grammar-based approaches are also often de-
scribed as more linguistically based, while sta-
tistical approaches are viewed as less linguisti-
cally informed. I think this view primarily re-
flects the origins of the two approaches: the
grammar-based approach arose from the collab-
oration between linguists and computer scientists
in the 1980s mentioned earlier, while the statisti-
cal approach has its origins in engineering work
in speech recognition in which linguists did not
play a major role. I also think this view is basi-
cally false. In the grammar-based approaches lin-
4
guists write the grammars while in statistical ap-
proaches linguists annotate the corpora with syn-
tactic parses, so linguists play a central role in
both. (It?s an interesting question as to why cor-
pus annotation plus statistical inference seems to
be a more effective way of getting linguistic in-
formation into a computer than manually writing
a grammar).
Rather, I think that computational linguists
working on statistical parsing need a greater level
of linguistic sensitivity at an informal level than
those working on grammar-based approaches.
In the grammar-based approaches all linguistic
knowledge is contained in the grammar, which the
computational linguist implementing the parsing
framework doesn?t actually have to understand.
All she has to do is correctly implement an in-
ference engine for grammars written in the rel-
evant grammar formalism. By contrast, statisti-
cal parsers define the probability of a parse in
terms of its (statistical) features or properties, and
a parser designer needs to choose which features
their parser will use, and many of these features re-
flect at least an intuitive understanding of linguis-
tic dependencies. For example, statistical parsers
from Magerman (1995) on use features based on
head-dependent relationships. (The parsers devel-
oped by the Berkeley group are a notable excep-
tion (Petrov and Klein, 2007)). While it?s true
that only a small fraction of our knowledge about
linguistic structure winds up expressed by fea-
tures in modern statistical parsers, as discussed
above there?s no reason to expect all of our sci-
entific knowledge to be relevant to any engineer-
ing problem. And while many of the features used
in statistical parsers don?t correspond to linguis-
tic constraints, nobody seriously claims that hu-
mans understand language only using linguistic
constraints of the kind expressed in formal gram-
mars. I suspect that many of the features that
have been shown to be useful in statistical parsing
encode psycholinguistic markedness preferences
(e.g., attachment preferences) and at least some
aspects of world knowledge (e.g., that the direct
object of ?eat? is likely to be a food).
Moreover, it?s not necessary for a statistical
model to exactly replicate a linguistic constraint in
order for it to effectively capture the correspond-
ing generalization: all that?s necessary is that the
statistical features ?cover? the relevant examples.
For example, adding a subject-verb agreement fea-
ture to the Charniak-Johnson parser (Charniak and
Johnson, 2005) has no measurable effect on pars-
ing accuracy. After doing this experiment I re-
alized this shouldn?t be surprising: the Charniak
parser already conditions each argument?s part-of-
speech (POS) on its governor?s POS, and since
POS tags distinguish singular and plural nouns and
verbs, these general head-argument POS features
capture most cases of subject-verb agreement.
Note that I?m not claiming that subject-verb
agreement isn?t a real linguistic constraint or that
it doesn?t play an important role in human pars-
ing. I think that the type of input (e.g., treebanks)
and the kinds of abilities (e.g., to exactly count the
occurences of many different constructions) avail-
able to our machines may be so different to what is
available to a child that the features that work best
in our parsers need not bear much relationship to
those used by humans.
Still, I view the design of the features used in
statistical parsers as a fundamentally linguistic is-
sue (albeit one with computational consequences,
since the search problem in parsing is largely de-
termined by the features involved), and I expect
there is still more to learn about which combi-
nations of features are most useful for statisti-
cal parsing. My guess is that the features used
in e.g., the Collins (2003) or Charniak (2000)
parsers are probably close to optimal for English
Penn Treebank parsing (Marcus et al, 1993), but
that other features might improve parsing of other
languages or even other English genres. Un-
fortunately changing the features used in these
parsers typically involves significant reprogram-
ming, which makes it difficult for linguists to ex-
periment with new features. However, it might
be possible to develop a kind of statistical pars-
ing framework that makes it possible to define new
features and integrate them into a statistical parser
without any programming which would make it
easy to explore novel combinations of statistical
features; see Goodman (1998) for an interesting
suggestion along these lines.
From a high-level perspective, the grammar-
based approaches and the statistical approaches
both view parsing fundamentally in the same way,
namely as a specialized kind of inference problem.
These days I view ?parsing as deduction? (one of
the slogans touted by the grammar-based crowd)
as unnecessarily restrictive; after all, psycholin-
guistic research shows that humans are exquisitely
5
sensitive to distributional information, so why
shouldn?t we let our parsers use that information
as well? And as Abney (1997) showed, it is
mathematically straight-forward to define proba-
bility distributions over the representations used
by virtually any theory of grammar (even those of
Chomsky?s Minimalism), which means that theo-
retically the arsenal of statistical methods for pars-
ing and learning can be applied to any grammar
just as well.
In the late 1990s I explored these kinds of sta-
tistical models for Lexical-Functional Grammar
(Bresnan, 1982; Johnson et al, 1999). The hope
was that statistical features based on LFG?s richer
representations (specifically, f -structures) might
result in better parsing accuracy. However, this
seems not to be the case. As mentioned above, Ab-
ney?s formulation of probabilistic models makes
essentially no demands on what linguistic repre-
sentations actually are; all that is required is that
the statistical features are functions that map each
representation to a real number. These are used to
map a set of linguistic representations (say, the set
of all grammatical analyses) to a set of vectors of
real numbers. Then by defining a distribution over
these sets of real-valued vectors we implicitly de-
fine a distribution over the corresponding linguis-
tic representations.
This means that as far as the probabilistic model
is concerned the details of the linguistic represen-
tations don?t actually matter, so long as there are
the right number of them and it is possible to com-
pute the necessary real-valued vectors from them.
For a computational linguist this is actually quite
a liberating point of view; we aren?t restricted
to slavishly reproducing textbook linguistic struc-
tures, but are free to experiment with alternative
representations that might have computational or
other advantages.
In my case, it turned out that the kinds of fea-
tures that were most useful for stochastic LFG
parsing could in fact be directly computed from
phrase-structure trees. The features that involved
f -structure properties could be covered by other
features defined directly on the phrase-structure
trees. (Some of these phrase-structure features
were implemented by rather nasty C++ routines
but that doesn?t matter; Abney-type models make
no assumptions about what the feature functions
are). This meant that I didn?t actually need the
f -structures to define the probability distributions
I was interested in; all I needed were the corre-
sponding c-structure or phrase-structure trees.
And of course there are many ways of obtain-
ing phrase-structure trees. At the time my col-
league Eugene Charniak was developing a statis-
tical phrase-structure parser that was more robust
and had broader coverage than the LFG parser I
was working with, and I found I generally got
better performance if I used the trees his parser
produced, so that?s what I did. This leads to
the discriminative re-ranking approach developed
by Collins and Koo (2005), in which a statistical
parser trained on a treebank is used to produce a
set of candidate parses which are then ?re-ranked?
by an Abney-style probabilistic model.
I suspect these robustness and coverage prob-
lems of grammar-based parsing are symptoms of
a fundamental problem in the standard way that
grammar-based parsing is understood. First, I
think grammar-based approaches face a dilemma:
on the one hand the explosion of ambiguity sug-
gests that some sentences get too many parses,
while the problems of coverage show that some
sentences get too few, i.e., zero, parses. While it?s
possible that there is a single grammar that can
resolve this dilemma, my point here is that each
of these problems suggests we need to modify the
grammars in exactly the opposite way, i.e., gener-
ally tighten the constraints in order to reduce am-
biguity, while generally relax the constraints in or-
der to allow more parses for sentences that have
no parses at all.
Second, I think this dilemma only arises be-
cause the grammar-based approach to parsing is
fundamentally designed around the goal of dis-
tinguishing grammatical from ungrammatical sen-
tences. While I agree with Pullum (2007) that
grammaticality is and should be central to syntac-
tic theory, I suspect it is not helpful to view pars-
ing (by machines or humans) as a byproduct of
proving the grammaticality of a sentence. In most
of the applications I can imagine, what we really
want from a parser is the parse that reflects its best
guess at the intended interpretation of the input,
even if that input is ungrammatical. For example,
given the telegraphese input ?man bites dog? we
want the parser to tell us that ?man? is likely to be
the agent of ?bites? and ?dog? the patient, and not
simply that the sentence is ungrammatical.
These grammars typically distinguish grammat-
ical from ungrammatical analyses by explicitly
6
characterizing the set of grammatical analyses in
some way, and then assuming that all other anal-
yses are ungrammatical. Borrowing terminology
from logic programming (Lloyd, 1987) we might
call this a closed-world assumption: any analysis
the grammar does not generate is assumed to be
ungrammatical.
Interestingly, I think that the probabilistic mod-
els used statistical parsing generally make an
open-world assumption about linguistic analyses.
These probabilistic models prefer certain linguis-
tic structures over others, but the smoothing mech-
anisms that these methods use ensure that every
possible analysis (and hence every possible string)
receives positive probability. In such an approach
the statistical features identify properties of syn-
tactic analyses which make the analysis more or
less likely, so the probabilistic model can prefer,
disprefer or simply be ambivalent about any par-
ticular linguistic feature or construction.
I think an open-world assumption is generally
preferable as a model of syntactic parsing in both
humans and machines. I think it?s not reason-
able to assume that the parser knows all the lex-
ical entries and syntactic constructions of the lan-
guage it is parsing. Even if the parser encoun-
ters a word or construction it doesn?t understand it,
that shouldn?t stop it from interpreting the rest of
the sentence. Statistical parsers are considerably
more open-world. For example, unknown words
don?t present any fundamental problem for statis-
tical parsers; in the absence of specific lexical in-
formation about a word they automatically back
off to generic information about words in general.
Does the closed-world assumption inherent in
the standard approach to grammar-based parsing
mean we have to abandon it? I don?t think so; I
can imagine at least two ways in which the con-
ventional grammar-based approach might be mod-
ified to obtain an open-world parsing model.
One possible approach keeps the standard
closed-world conception that grammars generate
only grammatical analyses, but gives up the idea
that parsing is a byproduct of determining the
grammaticality of the input sentence. Instead, we
might use a noisy channel to map grammatical
analyses generated by the grammar to the actual
input sentences we have to parse. Parsing involves
recovering the grammatical source or underlying
sentence as well as its structure. Presumably the
channel model would be designed to prefer min-
imal distortion, so if the input to be parsed is
in fact grammatical then the channel would pre-
fer the identity transformation, while if the input
is ungrammatical the channel model would map
it to close grammatical sentences. For example,
if such a parser were given the input ?man bites
dog? it might decide that the most probable un-
derlying sentence is ?a man bites a dog? and re-
turn a parse for that sentence. Such an approach
might be regarded as a way of formalizing the idea
that ungrammatical sentences are interpreted by
analogy with grammatical ones. (Charniak and I
proposed a noisy channel model along these lines
for parsing transcribed speech (Johnson and Char-
niak, 2004)).
Another possible approach involves modifying
our interpretation of the grammar itself. We could
obtain an open world model by relaxing our inter-
pretation of some or all of the constraints in the
grammar. Instead of viewing them as hard con-
straints that define a set of grammatical construc-
tions, we reinterpret them as violable, probabilis-
tic features. For example, instead of interpret-
ing subject-verb agreement as a hard constraint
that rules out certain syntactic analyses, we rein-
terpret it as a soft constraint that penalizes analy-
ses in which subject-verb agreement fails. Instead
of assuming that each verb comes with a fixed
set of subcategorization requirements, we might
view subcategorization as preferences for certain
kinds of complements, implemented by features
in an Abney-style statistical model. Unknown
words come with no subcategorization preferences
of their own, so they would inherit the prior or de-
fault preferences. Formally, I think this is fairly
easy to achieve: we replace the hard unification
constraints (e.g., that the subject?s number feature
equals the verb?s number feature) with a stochas-
tic feature that fires whenever the subject?s number
feature differs from the verb?s number feature, and
rely on the statistical model training procedure to
estimate that feature?s weight.
Computationally, I suspect that either of these
options (or any other option that makes the
grammar-based approaches open world) will re-
quire a major rethinking of the parsing process.
Notice that both approaches let ambiguity prolif-
erate (ambiguity is our friend in the fight against
poor coverage), so we would need parsing al-
gorithms capable of handling massive ambiguity.
This is true of most statistical parsing models, so
7
it is possible that the same approaches that have
proven successful in statistical parsing (e.g., using
probabilities to guide search, dynamic program-
ming, coarse-to-fine) will be useful here as well.
3 Statistical models and linguistics
The previous section focused on syntactic parsing,
which is an area in which there?s been a fruitful in-
teraction between linguistic theory and computa-
tional linguistics over a period of several decades.
In this section I want to discuss two other emerg-
ing areas in which I expect the interaction be-
tween linguistics and computational linguistics to
become increasingly important: psycholinguistics
and language acquisition. I think it?s no accident
that these areas both study processing (rather than
an area of theoretical linguistics such as syntax
or semantics), since I believe that the scientific
side of computational linguistics is fundamentally
about such linguistic processes.
Just to be clear: psycholinguistics and language
acquisition are experimental disciplines, and I
don?t expect the average researcher in those fields
to start doing computational linguistics any time
soon. However, I do think there are an emerging
cadre of young researchers in both fields apply-
ing ideas and results from computational linguis-
tics in their work and using experimental results
from their field to develop and improve the compu-
tational models. For example, in psycholinguistics
researchers such as Hale (2006) and Levy (2008)
are using probabilistic models of syntactic struc-
ture to make predictions about human sentence
processing, and Bachrach (2008) is using predic-
tions from the Roark (2001) parser to help explain
the patterns of fMRI activation observed during
sentence comprehension. In the field of language
acquisition computational linguists such as Klein
and Manning (2004) have studied the unsuper-
vised acquisition of syntactic structure, while lin-
guists such as Boersma and Hayes (2001), Gold-
smith (2001), Pater (2008) and Albright and Hayes
(2003) are developing probabilistic models of the
acquisition of phonology and/or morphology, and
Frank et al (2007) experimentally tests the predic-
tions of a Bayesian model of lexical acquisition.
Since I have more experience with computational
models of language acquisition, I?ll concentrate on
this topic for the rest of this section.
Much of this work can be viewed under the slo-
gan ?structured statistical learning?. That is, spec-
ifying the structures over which the learning algo-
rithm generalizes is just as important as specifying
the learning algorithm itself. One of the things I
like about this work is that it gets beyond the naive
nature-versus-nurture arguments that characterize
some of the earlier theoretical work on language
acquisition. Instead, these computational models
become tools for investigating the effect of spe-
cific structural assumptions on the acquisition pro-
cess. For example, Goldwater et al (2007) shows
that modeling inter-word dependencies improves
word segmentation, which shows that the linguis-
tic context contains information that is potentially
very useful for lexical acquisition.
I think it?s no accident that much of the com-
putational work is concerned with phonology and
morphology. These fields seem to be closer to
the data and the structures involved seem simpler
than in, say, syntax and semantics. I suspect that
linguists working in phonology and morphology
find it easier to understand and accept probabilistic
models in large part because of Smolensky?s work
on Optimality Theory (Smolensky and Legendre,
2005). Smolensky found a way of introducing op-
timization into linguistic theory in a way that lin-
guists could understand, and this serves as a very
important bridge for them to probabilistic models.
As I argued above, it?s important with any com-
putational modeling to be clear about exactly what
our computational models are intended to achieve.
Perhaps the most straight-forward goal for compu-
tational models of language acquisition is to view
them as specifying the actual computations that a
human performs when learning a language. Un-
der this conception we expect the computational
model to describe the learning trajectory of lan-
guage acquisition, e.g., if it takes the algorithm
more iterations to learn one word than another,
then we would expect humans to take longer to
that word as well. Much of the work in compu-
tational phonology seems to take this perspective
(Boersma and Hayes, 2001).
Alternatively, we might view our probabilistic
models (rather than the computational procedures
that implementing them) as embodying the scien-
tific claims we want to make. Because these prob-
abilistic models are too complex to analyze ana-
lytically in general we need a computational pro-
cedure to compute the model?s predictions, but the
computational procedure itself is not claimed to
have any psychological reality. For example, we
8
might claim that the grammar a child will learn
is the one that is optimal with respect to a cer-
tain probabilistic model. We need an algorithm for
computing this optimal grammar so we can check
the probabilistic model?s predictions and to con-
vince ourselves we?re not expecting the learner to
perform magic, but we might not want to claim
that humans use this algorithm. To use termi-
nology from the grammar-based approaches men-
tioned earlier, a probabilistic model is a declara-
tive specification of the distribution of certain vari-
ables, but it says nothing about how this distribu-
tion might actually be calculated. I think Marr?s
?three levels? capture this difference nicely: the
question is whether we take our models to be ?al-
gorithmic level? or ?computational level? descrip-
tions of cognitive processes (Marr, 1982).
Looking into the future, I?m very excited about
Bayesian approaches to language acquisition, as I
think they have the potential to let us finally ex-
amine deep questions about language acquisition
in a quantitative way. The Bayesian approach fac-
tors learning problems into two pieces: the likeli-
hood and the prior. The likelihood encodes the in-
formation obtained from the data, while the prior
encodes the information possessed by the learner
before learning commences (Pearl, 1988). In prin-
ciple the prior can encode virtually any informa-
tion, including information claimed to be part of
universal grammar.
Bayesian priors can incorporate the properties
linguists often take to be part of universal gram-
mar, such as X ? theory. A Bayesian prior can
also express soft markedness preferences as well
as hard constraints. Moreover, the prior can also
incorporate preferences that are not specifically
linguistic, such as a preference for shorter gram-
mars or smaller lexicons, i.e., the kinds of prefer-
ences sometimes expressed by an evaluation met-
ric (Chomsky, 1965).
The Bayesian framework therefore provides us
with a tool to quantitatively evaluate the impact
of different purported linguistic universals on lan-
guage acquisition. For example, we can calcu-
late the contribution of, say, hypothetical X ? the-
ory universals on the acquisition of syntax. The
Bayesian framework is flexible enough to also per-
mit us to evaluate the contribution of the non-
linguistic context on learning (Frank et al, to ap-
pear). Finally, non-parametric Bayesian methods
permit us to learn models with an unbounded num-
ber features, perhaps giving us the mathematical
and computational tools to understand the induc-
tion of rules and complex structure (Johnson et al,
2007).
Of course doing this requires developing actual
Bayesian models of language, and this is not easy.
Even though this research is still just beginning,
it?s clear that the details of the models have a huge
impact on how well they work. It?s not enough to
?assume some version of X ? theory?; one needs to
evaluate specific proposals. Still, my hope is that
being able to evaluate the contributions of specific
putative universals may help us measure and un-
derstand their contributions (if any) to the learning
process.
4 Conclusion
In this paper I focused on two areas of interaction
between computational linguistics and linguistic
theory. In the area of parsing I argued that we
should design parsers so they incorporate an open-
world assumption about sentences and their lin-
guistic structures and sketched two ways in which
grammar-based approaches might be modified to
make them do this; both of which involve aban-
doning the idea that parsing is solely a process of
proving the grammaticality of the input.
Then I discussed how probabilistic models are
being applied in the fields of sentence processing
and language acquisition. Here I believe we?re at
the beginning of a very fruitful period of inter-
action between empirical research and computa-
tional modeling, with insights and results flowing
both ways.
But what does all this mean for mainstream
computational linguistics? Can we expect theo-
retical linguistics to play a larger role in compu-
tational linguistics in the near future? If by com-
putational linguistics we mean the NLP engineer-
ing applications that typically receive the bulk of
the attention at today?s Computational Linguistics
conferences, I?m not so sure. While it?s reasonable
to expect that better scientific theories of how hu-
mans understand language will help us build better
computational systems that do the same, I think we
should remember that our machines can do things
that no human can (e.g., count all the 5-grams in
terabytes of data), and so our engineering solu-
tions may differ considerably from the algorithms
and procedures used by humans. But I think it?s
also reasonable to hope that the interdisciplinary
9
work involving statistics, computational models,
psycholinguistics and language acquisition that I
mentioned in the paper will produce new insights
into how language is acquired and used.
Acknowledgments
I?d like to thank Eugene Charniak and Antske
Fokkens for stimulating discussion and helpful
comments on an earlier draft. Of course all opin-
ions expressed here are my own.
References
Steven Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23(4):597?617.
A. Albright and B. Hayes. 2003. Rules vs. analogy in
English past tenses: a computational/experimental study.
Cognition, 90:118?161.
Asaf Bachrach. 2008. Imaging Neural Correlates of Syn-
tactic Complexity in a Naturalistic Context. Ph.D. thesis,
Massachusetts Institute of Technology, Cambridge, Mas-
sachusetts.
P. Boersma and B. Hayes. 2001. Empirical tests of the grad-
ual learning algorithm. Linguistic Inquiry, 32(1):45?86.
Joan Bresnan. 1982. Control and complementation. In Joan
Bresnan, editor, The Mental Representation of Grammati-
cal Relations, pages 282?390. The MIT Press, Cambridge,
Massachusetts.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 173?180, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American Chapter
of the Association for Computational Linguistics, pages
132?139.
Noam Chomsky. 1965. Aspects of the Theory of Syntax. The
MIT Press, Cambridge, Massachusetts.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computational
Linguistics, 31(1):25?70.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?638.
Michael C. Frank, Sharon Goldwater, Vikash Mansinghka,
Tom Griffiths, and Joshua Tenenbaum. 2007. Model-
ing human performance on statistical word segmentation
tasks. In Proceedings of the 29th Annual Meeting of the
Cognitive Science Society.
Michael C. Frank, Noah Goodman, and Joshua Tenenbaum.
to appear. Using speakers referential intentions to model
early cross-situational word learning. Psychological Sci-
ence.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag.
1985. Generalized Phrase Structure Grammar. Basil
Blackwell, Oxford.
J. Goldsmith. 2001. Unsupervised learning of the morphol-
ogy of a natural language. Computational Linguistics,
27:153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson.
2007. Distributional cues to word boundaries: Context is
important. In David Bamman, Tatiana Magnitskaia, and
Colleen Zaller, editors, Proceedings of the 31st Annual
Boston University Conference on Language Development,
pages 239?250, Somerville, MA. Cascadilla Press.
J. Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Harvard University. available from
http://research.microsoft.com/?joshuago/.
John Hale. 2006. Uncertainty about the rest of the sentence.
Cognitive Science, 30:643?672.
Fred Jelinek. 2004. Stochastic analysis of structured lan-
guage modeling. In Mark Johnson, Sanjeev P. Khudan-
pur, Mari Ostendorf, and Roni Rosenfeld, editors, Mathe-
matical Foundations of Speech and Language Processing,
pages 37?72. Springer, New York.
Mark Johnson and Eugene Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?39.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In The Proceedings of the
37th Annual Conference of the Association for Computa-
tional Linguistics, pages 535?541, San Francisco. Morgan
Kaufmann.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for speci-
fying compositional nonparametric Bayesian models. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 641?
648. MIT Press, Cambridge, MA.
Dan Klein and Chris Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency and
constituency. In Proceedings of the 42nd Annual Meeting
of the Association for Computational Linguistics, pages
478?485.
Roger Levy. 2008. Expectation-based syntactic comprehen-
sion. Cognition, 106:1126?1177.
John W. Lloyd. 1987. Foundations of Logic Programming.
Springer, Berlin, 2 edition.
David M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In The Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics,
pages 276?283, San Francisco. The Association for Com-
putational Linguistics, Morgan Kaufman.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
David Marr. 1982. Vision. W.H. Freeman and Company,
New York.
10
Matha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Joe Pater. 2008. Gradual learning and convergence. Linguis-
tic Inquiry, 30(2):334?345.
Judea Pearl. 1988. Probabalistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan Kauf-
mann, San Mateo, California.
Slav Petrov and Dan Klein. 2007. Improved inference for
unlexicalized parsing. In Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 404?411, Rochester,
New York, April. Association for Computational Linguis-
tics.
Geoffrey K. Pullum. 2007. Ungrammaticality, rarity, and
corpus use. Corpus Linguistics and Linguistic Theory,
3:33?47.
Brian Roark. 2001. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2):249?
276.
Stuart M. Shieber. 1986. An Introduction to Unification-
based Approaches to Grammar. CSLI Lecture Notes Se-
ries. Chicago University Press, Chicago.
Paul Smolensky and Ge?raldine Legendre. 2005. The Har-
monic Mind: From Neural Computation To Optimality-
Theoretic Grammar. The MIT Press.
11
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 296?305, Prague, June 2007. c?2007 Association for Computational Linguistics
Why doesn?t EM find good HMM POS-taggers?
Mark Johnson
Microsoft Research Brown University
Redmond, WA Providence, RI
t-majoh@microsoft.com Mark Johnson@Brown.edu
Abstract
This paper investigates why the HMMs es-
timated by Expectation-Maximization (EM)
produce such poor results as Part-of-Speech
(POS) taggers. We find that the HMMs es-
timated by EM generally assign a roughly
equal number of word tokens to each hid-
den state, while the empirical distribution
of tokens to POS tags is highly skewed.
This motivates a Bayesian approach using
a sparse prior to bias the estimator toward
such a skewed distribution. We investigate
Gibbs Sampling (GS) and Variational Bayes
(VB) estimators and show that VB con-
verges faster than GS for this task and that
VB significantly improves 1-to-1 tagging ac-
curacy over EM. We also show that EM does
nearly as well as VB when the number of
hidden HMM states is dramatically reduced.
We also point out the high variance in all
of these estimators, and that they require
many more iterations to approach conver-
gence than usually thought.
1 Introduction
It is well known that Expectation-Maximization
(EM) performs poorly in unsupervised induction
of linguistic structure (Carroll and Charniak, 1992;
Merialdo, 1994; Klein, 2005; Smith, 2006). In ret-
rospect one can certainly find reasons to explain this
failure: after all, likelihood does not appear in the
wide variety of linguistic tests proposed for identi-
fying linguistic structure (Fromkin, 2001).
This paper focuses on unsupervised part-of-
speech (POS) tagging, because it is perhaps the sim-
plest linguistic induction task. We suggest that one
reason for the apparent failure of EM for POS tag-
ging is that it tends to assign relatively equal num-
bers of tokens to each hidden state, while the em-
pirical distribution of POS tags is highly skewed,
like many linguistic (and non-linguistic) phenomena
(Mitzenmacher, 2003). We focus on first-order Hid-
den Markov Models (HMMs) in which the hidden
state is interpreted as a POS tag, also known as bitag
models.
In this setting we show that EM performs poorly
when evaluated using a ?1-to-1 accuracy? evalua-
tion, where each POS tag corresponds to at most one
hidden state, but is more competitive when evaluated
using a ?many-to-1 accuracy? evaluation, where sev-
eral hidden states may correspond to the same POS
tag. We explain this by observing that the distribu-
tion of hidden states to words proposed by the EM-
estimated HMMs is relatively uniform, while the
empirical distribution of POS tags is heavily skewed
towards a few high-frequency tags. Based on this,
we propose a Bayesian prior that biases the sys-
tem toward more skewed distributions and show that
this raises the 1-to-1 accuracy significantly. Finally,
we show that a similar increase in accuracy can be
achieved by reducing the number of hidden states in
the models estimated by EM.
There is certainly much useful information that
bitag HMMs models cannot capture. Toutanova et
al. (2003) describe a wide variety of morphologi-
cal and distributional features useful for POS tag-
ging, and Clark (2003) proposes ways of incorporat-
ing some of these in an unsupervised tagging model.
However, bitag models are rich enough to capture
at least some distributional information (i.e., the tag
296
for a word depends on the tags assigned to its neigh-
bours). Moreover, more complex models add addi-
tional complicating factors that interact in ways still
poorly understood; for example, smoothing is gen-
erally regarded as essential for higher-order HMMs,
yet it is not clear how to integrate smoothing into un-
supervised estimation procedures (Goodman, 2001;
Wang and Schuurmans, 2005).
Most previous work exploiting unsupervised
training data for inferring POS tagging models has
focused on semi-supervised methods in the in which
the learner is provided with a lexicon specifying the
possible tags for each word (Merialdo, 1994; Smith
and Eisner, 2005; Goldwater and Griffiths, 2007)
or a small number of ?prototypes? for each POS
(Haghighi and Klein, 2006). In the context of semi-
supervised learning using a tag lexicon, Wang and
Schuurmans (2005) observe discrepencies between
the empirical and estimated tag frequencies similar
to those observed here, and show that constraining
the estimation procedure to preserve the empirical
frequencies improves tagging accuracy. (This ap-
proach cannot be used in an unsupervised setting
since the empirical tag distribution is not available).
However, as Banko and Moore (2004) point out, the
accuracy achieved by these unsupervised methods
depends strongly on the precise nature of the su-
pervised training data (in their case, the ambiguity
of the tag lexicon available to the system), which
makes it more difficult to understand the behaviour
of such systems.
2 Evaluation
All of the experiments described below have the
same basic structure: an estimator is used to infer
a bitag HMM from the unsupervised training cor-
pus (the words of Penn Treebank (PTB) Wall Street
Journal corpus (Marcus et al, 1993)), and then the
resulting model is used to label each word of that
corpus with one of the HMM?s hidden states. This
section describes how we evaluate how well these
sequences of hidden states correspond to the gold-
standard POS tags for the training corpus (here, the
PTB POS tags). The chief difficulty is determining
the correspondence between the hidden states and
the gold-standard POS tags.
Perhaps the most straightforward method of es-
tablishing this correspondence is to deterministically
map each hidden state to the POS tag it co-occurs
most frequently with, and return the proportion of
the resulting POS tags that are the same as the POS
tags of the gold-standard corpus. We call this the
many-to-1 accuracy of the hidden state sequence be-
cause several hidden states may map to the same
POS tag (and some POS tags may not be mapped
to by any hidden states at all).
As Clark (2003) points out, many-to-1 accuracy
has several defects. If a system is permitted to posit
an unbounded number of hidden states (which is not
the case here) then it can achieve a perfect many-to-
1 accuracy by placing every word token into its own
unique state. Cross-validation, i.e., identifying the
many-to-1 mapping and evaluating on different sub-
sets of the data, would answer many of these objec-
tions. Haghighi and Klein (2006) propose constrain-
ing the mapping from hidden states to POS tags so
that at most one hidden state maps to any POS tag.
This mapping is found by greedily assigning hidden
states to POS tags until either the hidden states or
POS tags are exhausted (note that if the number of
hidden states and POS tags differ, some will be unas-
signed). We call the accuracy of the POS sequence
obtained using this map its 1-to-1 accuracy.
Finally, several authors have proposed using
information-theoretic measures of the divergence
between the hidden state and POS tag sequences.
Goldwater and Griffiths (2007) propose using the
Variation of Information (VI) metric described by
Meila? (2003). We regard the assignments of hid-
den states and POS tags to the words of the cor-
pus as two different ways of clustering those words,
and evaluate the conditional entropy of each clus-
tering conditioned on the other. The VI is the sum
of these conditional entropies. Specifically, given a
corpus labeled with hidden states and POS tags, if
p?(y), p?(t) and p?(y, t) are the empirical probabilities
of a hidden state y, a POS tag t, and the cooccurance
of y and t respectively, then the mutual information
I , entropies H and variation of information VI are
defined as follows:
H(Y ) = ?
?
y
p?(y) log p?(y)
H(T ) = ?
?
t
p?(t) log p?(t)
I(Y, T ) =
?
y,t
p?(y, t) log
p?(y, t)
p?(y)p?(t)
H(Y |T ) = H(Y )? I(Y, T )
297
H(T |Y ) = H(T )? I(Y, T )
VI (Y, T ) = H(Y |T ) +H(T |Y )
As Meila? (2003) shows, VI is a metric on the space
of probability distributions whose value reflects the
divergence between the two distributions, and only
takes the value zero when the two distributions are
identical.
3 Maximum Likelihood via
Expectation-Maximization
There are several excellent textbook presentations of
Hidden Markov Models and the Forward-Backward
algorithm for Expectation-Maximization (Jelinek,
1997; Manning and Schu?tze, 1999; Bishop, 2006),
so we do not cover them in detail here. Conceptu-
ally, a Hidden Markov Model generates a sequence
of observations x = (x0, . . . , xn) (here, the words
of the corpus) by first using a Markov model to gen-
erate a sequence of hidden states y = (y0, . . . , yn)
(which will be mapped to POS tags during evalua-
tion as described above) and then generating each
word xi conditioned on its corresponding state yi.
We insert endmarkers at the beginning and ending
of the corpus and between sentence boundaries, and
constrain the estimator to associate endmarkers with
a state that never appears with any other observation
type (this means each sentence can be processed in-
dependently by first-order HMMs; these endmarkers
are ignored during evaluation).
In more detail, the HMM is specified by multi-
nomials ?y and ?y for each hidden state y, where
?y specifies the distribution over states following y
and ?y specifies the distribution over observations x
given state y.
yi | yi?1 = y ? Multi(?y)
xi | yi = y ? Multi(?y)
(1)
We used the Forward-Backward algorithm to per-
form Expectation-Maximization, which is a proce-
dure that iteratively re-estimates the model param-
eters (?, ?), converging on a local maximum of the
likelihood. Specifically, if the parameter estimate at
time ` is (?(`), ?(`)), then the re-estimated parame-
ters at time `+ 1 are:
?(`+1)y?|y = E[ny?,y]/E[ny] (2)
?(`+1)x|y = E[nx,y]/E[ny]
6.95E+06
7.00E+06
7.05E+06
7.10E+06
7.15E+06
0 250 500 750 1000
?
lo
g 
lik
el
ih
oo
d
Iteration
Figure 1: Variation in negative log likelihood with
increasing iterations for 10 EM runs from different
random starting points.
where nx,y is the number of times observation x oc-
curs with state y, ny?,y is the number of times state
y? follows y and ny is the number of occurences of
state y; all expectations are taken with respect to the
model (?(`), ?(`)).
We took care to implement this and the other al-
gorithms used in this paper efficiently, since optimal
performance was often only achieved after several
hundred iterations. It is well-known that EM often
takes a large number of iterations to converge in like-
lihood, and we found this here too, as shown in Fig-
ure 1. As that figure makes clear, likelihood is still
increasing after several hundred iterations.
Perhaps more surprisingly, we often found dra-
matic changes in accuracy in the order of 5% occur-
ing after several hundred iterations, so we ran 1,000
iterations of EM in all of the experiments described
here; each run took approximately 2.5 days compu-
tation on a 3.6GHz Pentium 4. It?s well-known that
accuracy often decreases after the first few EM it-
erations (which we also observed); however in our
experiments we found that performance improves
again after 100 iterations and continues improving
roughly monotonically. Figure 2 shows how 1-to-1
accuracy varies with iteration during 10 runs from
different random starting points. Note that 1-to-1
accuracy at termination ranges from 0.38 to 0.45; a
spread of 0.07.
We obtained a dramatic speedup by working di-
rectly with probabilities and rescaling after each ob-
servation to avoid underflow, rather than working
with log probabilities (thanks to Yoshimasa Tsu-
298
0.35
0.37
0.39
0.41
0.43
0.45
0.47
0 250 500 750 1000
1-
to
-1
 a
ccura
cy
Iteration
Figure 2: Variation in 1-to-1 accuracy with increas-
ing iterations for 10 EM runs from different random
starting points.
ruoka for pointing this out). Since we evaluated
the accuracy of the estimated tags after each iter-
ation, it was important that decoding be done effi-
ciently as well. While most researchers use Viterbi
decoding to find the most likely state sequence, max-
imum marginal decoding (which labels the observa-
tion xi with the state yi that maximizes the marginal
probability P(yi|x, ?, ?)) is faster because it re-uses
the forward and backward tables already constructed
by the Forward-Backward algorithm. Moreover, in
separate experiments we found that the maximum
marginal state sequence almost always scored higher
than the Viterbi state sequence in all of our evalua-
tions, and at modest numbers of iterations (up to 50)
often scored more than 5% better.
We also noticed a wide variance in the perfor-
mance of models due to random initialization (both
? and ? are initially jittered to break symmetry); this
wide variance was observed with all of the estima-
tors investigated in this paper. This means we cannot
compare estimators on the basis of single runs, so we
ran each estimator 10 times from different random
starting points and report both mean and standard
deviation for all scores.
Finally, we also experimented with annealing, in
which the parameters ? and ? are raised to the power
1/T , where T is a ?temperature? parameter that is
slowly lowered toward 1 at each iteration accord-
ing to some ?annealing schedule?. We experimented
with a variety of starting temperatures and annealing
schedules (e.g., linear, exponential, etc), but were
unable to find any that produced models whose like-
0E+0
1E+5
2E+5
Fre
quen
cy
Tag / hidden state (sorted by frequency)
PT B
V B
EM
EM 25
Figure 3: The average number of words labeled with
each hidden state or tag for the EM, VB (with ?x =
?y = 0.1) and EM-25 estimators (EM-25 is the EM
estimator with 25 hidden states).
lihoods were significantly higher (i.e., the models fit
better) than those found without annealing.
The evaluation of the models produced by the
EM and other estimators is presented in Table 1.
It is difficult to compare these with previous work,
but Haghighi and Klein (2006) report that in a
completely unsupervised setting, their MRF model,
which uses a large set of additional features and a
more complex estimation procedure, achieves an av-
erage 1-to-1 accuracy of 41.3%. Because they pro-
vide no information about the variance in this accu-
racy it is difficult to tell whether there is a signifi-
cant difference between their estimator and the EM
estimator, but it is clear that when EM is run long
enough, the performance of even very simple mod-
els like the bitag HMM is better than generally rec-
ognized.
As Table 1 makes clear, the EM estimator pro-
duces models that are extremely competitive in
many-to-1 accuracy and Variation of Information,
but are significantly worse in 1-to-1 accuracy. We
can understand these results by comparing the dis-
tribution of words to hidden states to the distribution
of words to POS tags in the gold-standard evaluation
corpus. As Figure 3 shows, the distribution of words
to POS tags is highly skewed, with just 6 POS tags,
NN, IN, NNP, DT, JJ and NNS, accounting for over
55% of the tokens in the corpus. By contrast, the
EM distribution is much flatter. This also explains
why the many-to-1 accuracy is so much better than
the one-to-one accuracy; presumably several hidden
299
Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T )
EM (50) 0.40 (0.02) 0.62 (0.01) 4.46 (0.08) 1.75 (0.04) 2.71 (0.06)
VB(0.1, 0.1) (50) 0.47 (0.02) 0.50 (0.02) 4.28 (0.09) 2.39 (0.07) 1.89 (0.06)
VB(0.1, 10?4) (50) 0.46 (0.03) 0.50 (0.02) 4.28 (0.11) 2.39 (0.08) 1.90 (0.07)
VB(10?4, 0.1) (50) 0.42 (0.02) 0.60 (0.01) 4.63 (0.07) 1.86 (0.03) 2.77 (0.05)
VB(10?4, 10?4) (50) 0.42 (0.02) 0.60 (0.01) 4.62 (0.07) 1.85 (0.03) 2.76 (0.06)
GS(0.1, 0.1) (50) 0.37 (0.02) 0.51 (0.01) 5.45 (0.07) 2.35 (0.09) 3.20 (0.03)
GS(0.1, 10?4) (50) 0.38 (0.01) 0.51 (0.01) 5.47 (0.04) 2.26 (0.03) 3.22 (0.01)
GS(10?4, 0.1) (50) 0.36 (0.02) 0.49 (0.01) 5.73 (0.05) 2.41 (0.04) 3.31 (0.03)
GS(10?4, 10?4) (50) 0.37 (0.02) 0.49 (0.01) 5.74 (0.03) 2.42 (0.02) 3.32 (0.02)
EM (40) 0.42 (0.03) 0.60 (0.02) 4.37 (0.14) 1.84 (0.07) 2.55 (0.08)
EM (25) 0.46 (0.03) 0.56 (0.02) 4.23 (0.17) 2.05 (0.09) 2.19 (0.08)
EM (10) 0.41 (0.01) 0.43 (0.01) 4.32 (0.04) 2.74 (0.03) 1.58 (0.05)
Table 1: Evaluation of models produced by the various estimators. The values of the Dirichlet prior param-
eters for ?x and ?y appear in the estimator name for the VB and GS estimators, and the number of hidden
states is given in parentheses. Reported values are means over all runs, followed by standard deviations.
10 runs were performed for each of the EM and VB estimators, while 5 runs were performed for the GS
estimators. Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it-
erations. For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the
same as the standard deviation.
states are being mapped onto a single POS tag. This
is also consistent with the fact that the cross-entropy
H(T |Y ) of tags given hidden states is relatively low
(i.e., given a hidden state, the tag is relatively pre-
dictable), while the cross-entropy H(Y |T ) is rela-
tively high.
4 Bayesian estimation via Gibbs Sampling
and Variational Bayes
A Bayesian estimator combines a likelihood term
P(x|?, ?) and a prior P(?, ?) to estimate the poste-
rior probability of a model or hidden state sequence.
We can use a Bayesian prior to bias our estimator
towards models that generate more skewed distri-
butions. Because HMMs (and PCFGs) are prod-
ucts of multinomials, Dirichlet distributions are a
particularly natural choice for the priors since they
are conjugate to multinomials, which simplifies both
the mathematical and computational aspects of the
problem. The precise form of the model we investi-
gated is:
?y | ?y ? Dir(?y)
?y | ?x ? Dir(?x)
yi | yi?1 = y ? Multi(?y)
xi | yi = y ? Multi(?y)
Informally, ?y controls the sparsity of the state-to-
state transition probabilities while ?x controls the
sparsity of the state-to-observation emission proba-
bilities. As ?x approaches zero the prior strongly
prefers models in which each hidden state emits
as few words as possible. This captures the intu-
ition that most word types only belong to one POS,
since the minimum number of non-zero state-to-
observation transitions occurs when each observa-
tion type is emitted from only one state. Similarly,
as ?y approaches zero the state-to-state transitions
become sparser.
There are two main techniques for Bayesian esti-
mation of such models: Markov Chain Monte Carlo
(MCMC) and Variational Bayes (VB). MCMC en-
compasses a broad range of sampling techniques,
including component-wise Gibbs sampling, which
is the MCMC technique we used here (Robert and
Casella, 2004; Bishop, 2006). In general, MCMC
techniques do not produce a single model that char-
acterizes the posterior, but instead produce a stream
of samples from the posterior. The application of
MCMC techniques, including Gibbs sampling, to
HMM inference problems is relatively well-known:
see Besag (2004) for a tutorial introduction and
Goldwater and Griffiths (2007) for an application
of Gibbs sampling to HMM inference for semi-
300
supervised and unsupervised POS tagging.
The Gibbs sampler produces state sequences y
sampled from the posterior distribution:
P(y|x, ?) ?
?
P(x,y|?, ?)P(?|?y)P(?|?x) d? d?
Because Dirichlet priors are conjugate to multino-
mials, it is possible to integrate out the model pa-
rameters ? and ? to yield the conditional distribu-
tion for yi shown in Figure 4. For each observation
xi in turn, we resample its state yi conditioned on
the states y?i of the other observations; eventually
the distribution of state sequences converges to the
desired posterior.
Each iteration of the Gibbs sampler is much faster
than the Forward-Backward algorithm (both take
time linear in the length of the string, but for an
HMM with s hidden states, each iteration of the
Gibbs sampler takes O(s) time while each iteration
of the Forward-Backward algorithm takes O(s2)
time), so we ran 50,000 iterations of all samplers
(which takes roughly the same elapsed time as 1,000
Forward-Backward iterations).
As can be seen from Table 1, the posterior state
sequences we obtained are not particularly good.
Further, when we examined how the posterior like-
lihoods varied with increasing iterations of Gibbs
sampling, it became apparent that the likelihood was
still increasing after 50,000 iterations. Moreover,
when comparing posterior likelihoods from differ-
ent runs with the same prior parameters but differ-
ent random number seeds, none of the likelihoods
crossed, which one would expect if the samplers
had converged and were mixing well (Robert and
Casella, 2004). Just as with EM, we experimented
with a variety of annealing regimes, but were unable
to find any which significantly improved accuracy or
posterior likelihood.
We also experimented with evaluating state se-
quences found using maximum posterior decoding
(i.e., model parameters are estimated from the pos-
terior sample, and used to perform maximum poste-
rior decoding) rather than the samples from the pos-
terior produced by the Gibbs sampler. We found that
the maximum posterior decoding sequences usually
scored higher than the posterior samples, but the
scores converged after the first thousand iterations.
Since the posterior samples are produced as a by-
product of Gibbs sampling while maximum poste-
rior decoding requires an additional time consuming
step that does not have much impact on scores, we
used the posterior samples to produce the results in
Table 1.
In contrast to MCMC, Variational Bayesian in-
ference attempts to find the function Q(y, ?, ?) that
minimizes an upper bound of the negative log likeli-
hood (Jordan et al, 1999):
? log P(x)
= ? log
?
Q(y, ?, ?)
P(x,y, ?, ?)
Q(y, ?, ?)
dy d? d?
? ?
?
Q(y, ?, ?) log
P(x,y, ?, ?)
Q(y, ?, ?)
dy d? d?(3)
The upper bound in (3) is called the Variational Free
Energy. We make a ?mean-field? assumption that
the posterior can be well approximated by a factor-
ized modelQ in which the state sequence y does not
covary with the model parameters ?, ? (this will be
true if, for example, there is sufficient data that the
posterior distribution has a peaked mode):
P(x,y, ?, ?) ? Q(y, ?, ?) = Q1(y)Q2(?, ?)
The calculus of variations is used to minimize the
KL divergence between the desired posterior distri-
bution and the factorized approximation. It turns
out that if the likelihood and conjugate prior be-
long to exponential families then the optimalQ1 and
Q2 do too, and there is an EM-like iterative pro-
cedure that finds locally-optimal model parameters
(Bishop, 2006).
This procedure is especially attractive for HMM
inference, since it involves only a minor modifica-
tion to the M-step of the Forward-Backward algo-
rithm. MacKay (1997) and Beal (2003) describe
Variational Bayesian (VB) inference for HMMs in
detail, and Kurihara and Sato (2006) describe VB
for PCFGs (which only involves a minor modifica-
tion to the M-step of the Inside-Outside algorithm).
Specifically, the E-step for VB inference for HMMs
is the same as in EM, while theM-step is as follows:
??(`+1)y?|y = f(E[ny?,y] + ?y)/f(E[ny] + s?y) (4)
??(`+1)x|y = f(E[nx,y] + ?x)/f(E[ny] +m?x)
f(v) = exp(?(v))
?(v) = (v > 7) ? g(v ? 12) : (?(v + 1)? 1)/v
g(x) ? log(x) + 0.04167x?2 + 0.00729x?4
+0.00384x?6 ? 0.00413x?8 . . . (5)
301
P(yi|x,y?i, ?) ?
(
nxi,yi + ?x
nyi +m?x
) (
nyi,yi?1 + ?y
nyi?1 + s?y
) (
nyi+1,yi + I(yi?1 = yi = yi+1) + ?y
nyi + I(yi?1 = yi)
)
Figure 4: The conditional distribution for state yi used in the Gibbs sampler, which conditions on the states
y?i for all observations except xi. Here m is the number of possible observations (i.e., the size of the
vocabulary), s is the number of hidden states and I(?) is the indicator function (i.e., equal to one if its
argument is true and zero otherwise), nx,y is the number of times observation x occurs with state y, ny?,y is
the number of times state y? follows y, and ny is the number of times state y occurs; these counts are from
(x?i,y?i), i.e., excluding xi and yi.
 0 1
 2
 0  1  2
Figure 5: The scaling function y = f(x) =
exp?(x) (curved line), which is bounded above by
the line y = x and below by the line y = x? 0.5.
where ? is the digamma function (the derivative of
the log gamma function; (5) gives an asymptotic ap-
proximation), and the remaining quantities are just
as in the EM updates (2), i.e., nx,y is the number of
times observation x occurs with state y, ny?,y is the
number of times state y? follows y, ny is the number
of occurences of state y, s is the number of hidden
states and m is the number of observations; all ex-
pectations are taken with respect to the variational
parameters (??(`), ??(`)).
A comparison between (4) and (2) reveals two dif-
ferences between the EM and VB updates. First,
the Dirichlet prior parameters ? are added to the
expected counts. Second, these posterior counts
(which are in fact parameters of the Dirichlet pos-
terior Q2) are passed through the function f(v) =
exp?(v), which is plotted in Figure 5. When v 
0, f(v) ? v ? 0.5, so roughly speaking, VB for
multinomials involves adding ??0.5 to the expected
counts when they are much larger than zero, where
? is the Dirichlet prior parameter. Thus VB can
be viewed as a more principled version of the well-
known ad hoc technique for approximating Bayesian
estimation with EM that involves adding ??1 to the
expected counts. However, in the ad hoc approach
the expected count plus ??1 may be less than zero,
resulting in a value of zero for the corresponding pa-
rameter (Johnson et al, 2007; Goldwater and Grif-
fiths, 2007). VB avoids this problem because f(v) is
always positive when v > 0, even when v is small.
Note that because the counts are passed through f ,
the updated values for ?? and ?? in (4) are in general
not normalized; this is because the variational free
energy is only an upper bound on the negative log
likelihood (Beal, 2003).
We found that in general VB performed much bet-
ter than GS. Computationally it is very similar to
EM, and each iteration takes essentially the same
time as an EM iteration. Again, we experimented
with annealing in the hope of speeding convergence,
but could not find an annealing schedule that signifi-
cantly lowered the variational free energy (the quan-
tity that VB optimizes). While we had hoped that the
Bayesian prior would bias VB toward a common so-
lution, we found the same sensitivity to initial condi-
tions as we found with EM, so just as for EM, we ran
the estimator for 1,000 iterations with 10 different
random initializations for each combination of prior
parameters. Table 1 presents the results of VB runs
with several different values for the Dirichlet prior
parameters. Interestingly, we obtained our best per-
formance on 1-to-1 accuracy when the Dirchlet prior
?x = 0.1, a relatively large number, but best per-
formance on many-to-1 accuracy was achieved with
a much lower value for the Dirichlet prior, namely
?x = 10?4. The Dirichlet prior ?y that controls
302
sparsity of the state-to-state transitions had little ef-
fect on the results. We did not have computational
resources to fully explore other values for the prior
(a set of 10 runs for one set of parameter values takes
25 computer days).
As Figure 3 shows, VB can produce distributions
of hidden states that are peaked in the same way that
POS tags are. In fact, with the priors used here, VB
produces state sequences in which only a subset of
the possible HMM states are in fact assigned to ob-
servations. This shows that rather than fixing the
number of hidden states in advance, the Bayesian
prior can determine the number of states; this idea is
more fully developed in the infinite HMM of Beal et
al. (2002) and Teh et al (2006).
5 Reducing the number of hidden states
EM already performs well in terms of the many-to-1
accuracy, but we wondered if there might be some
way to improve its 1-to-1 accuracy and VI score. In
section 3 we suggested that one reason for its poor
performance in these evaluations is that the distri-
butions of hidden states it finds tend to be fairly
flat, compared to the empirical distribution of POS
tags. As section 4 showed, a suitable Bayesian prior
can bias the estimator towards more peaked distribu-
tions, but we wondered if there might be a simpler
way of achieving the same result.
We experimented with dramatic reductions in the
number of hidden states in the HMMs estimated
by EM. This should force the hidden states to be
more densely populated and improve 1-to-1 accu-
racy, even though this means that there will be no
hidden states that can possibly map onto the less fre-
quent POS tags (i.e., we will get these words wrong).
In effect, we abandon the low-frequency POS tags
in the hope of improving the 1-to-1 accuracy of the
high-frequency tags.
As Table 1 shows, this markedly improves both
the 1-to-1 accuracy and the VI score. A 25-state
HMM estimated by EM performs effectively as well
as the best VB model in terms of both 1-to-1 accu-
racy and VI score, and runs 4 times faster because it
has only half the number of hidden states.
6 Conclusion and future work
This paper studied why EM seems to do so badly in
HMM estimation for unsupervised POS tagging. In
fact, we found that it doesn?t do so badly at all: the
bitag HMM estimated by EM achieves a mean 1-to-
1 tagging accuracy of 40%, which is approximately
the same as the 41.3% reported by (Haghighi and
Klein, 2006) for their sophisticated MRF model.
Then we noted the distribution of words to hidden
states found by EM is relatively uniform, compared
to the distribution of words to POS tags in the eval-
uation corpus. This provides an explanation of why
the many-to-1 accuracy of EM is so high while the
1-to-1 accuracy and VI of EM is comparatively low.
We showed that either by using a suitable Bayesian
prior or by simply reducing the number of hidden
states it is possible to significantly improve both the
1-to-1 accuracy and the VI score, achieving a 1-to-1
tagging accuracy of 46%.
We also showed that EM and other estimators take
much longer to converge than usually thought, and
often require several hundred iterations to achieve
optimal performance. We also found that there is
considerable variance in the performance of all of
these estimators, so in general multiple runs from
different random starting points are necessary in or-
der to evaluate an estimator?s performance.
Finally, there may be more sophisticated ways of
improving the 1-to-1 accuracy and VI score than
the relatively crude methods used here that primar-
ily reduce the number of available states. For ex-
ample, we might obtain better performance by us-
ing EM to infer an HMM with a large number of
states, and then using some kind of distributional
clustering to group similar HMM states; these clus-
ters, rather than the underlying states, would be in-
terpreted as the POS tag labels. Also, the Bayesian
framework permits a wide variety of different priors
besides Dirichlet priors explored here. For example,
it should be possible to encode linguistic knowledge
such markedness preferences in a prior, and there
are other linguistically uninformative priors, such
the ?entropic priors? of Brand (1999), that may be
worth exploring.
Acknowledgements
I would like to thank Microsoft Research for pro-
viding an excellent environment in which to con-
duct this work, and my friends and colleagues at
Microsoft Research, especially Bob Moore, Chris
Quirk and Kristina Toutanova, for their helpful com-
ments on this paper.
303
References
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In Proceedings, 20th In-
ternational Conference on Computational Linguistics
(Coling 2004), pages 556?561, Geneva, Switzerland.
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002.
The infinite Hidden Markov Model. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, volume 14,
pages 577?584. The MIT Press.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience unit, University College
London.
Julian Besag. 2004. An introduction to Markov Chain
Monte Carlo methods. In Mark Johnson, Sanjeev P.
Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-
itors, Mathematical Foundations of Speech and Lan-
guage Processing, pages 247?270. Springer, New
York.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
M. Brand. 1999. An entropic estimator for structure dis-
covery. Advances in Neural Information Processing
Systems, 11:723?729.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Proceedings of the AAAI Workshop
on Statistically-Based Natural Language Processing
Techniques, San Jose, CA.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
59?66. Association for Computational Linguistics.
Victoria Fromkin, editor. 2001. Linguistics: An Intro-
duction to Linguistic Theory. Blackwell, Oxford, UK.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language, 14:403?
434.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Tom Griffiths, and Sharon Goldwater.
2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York. Association for Computational
Linguistics.
Michael I. Jordan, Zoubin Ghahramani, Tommi S.
Jaakkola, and Lawrence K. Sau. 1999. An introduc-
tion to variational methods for graphical models. Ma-
chine Learning, 37(2):183?233.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language. In
8th International Colloquium on Grammatical Infer-
ence.
David J.C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labora-
tory, Cambridge.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, COLT 2003: The Sixteenth
Annual Conference on Learning Theory, volume 2777
of Lecture Notes in Computer Science, pages 173?187.
Springer.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20:155?171.
M. Mitzenmacher. 2003. A brief history of generative
models for power law and lognormal distributions. In-
ternet Mathematics, 1(2):226?251.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
304
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natu-
ral Language Text. Ph.D. thesis, Johns Hopkins Uni-
versity.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 252?
259.
Qin Iris Wang and Dale Schuurmans. 2005. Improved
estimation for unsupervised part-of-speech tagging. In
Proceedings of the 2005 IEEE International Confer-
ence on Natural Language Processing and Knowledge
Engineering (IEEE NLP-KE?2005), pages 219?224,
Wuhan, China.
305
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168?175,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Transforming Projective Bilexical Dependency Grammars into
efficiently-parsable CFGs with Unfold-Fold
Mark Johnson
Microsoft Research Brown University
Redmond, WA Providence, RI
t-majoh@microsoft.com Mark Johnson@Brown.edu
Abstract
This paper shows how to use the Unfold-
Fold transformation to transform Projective
Bilexical Dependency Grammars (PBDGs)
into ambiguity-preserving weakly equiva-
lent Context-Free Grammars (CFGs). These
CFGs can be parsed in O(n3) time using a
CKY algorithm with appropriate indexing,
rather than the O(n5) time required by a
naive encoding. Informally, using the CKY
algorithm with such a CFG mimics the steps
of the Eisner-Satta O(n3) PBDG parsing al-
gorithm. This transformation makes all of
the techniques developed for CFGs available
to PBDGs. We demonstrate this by describ-
ing a maximum posterior parse decoder for
PBDGs.
1 Introduction
Projective Bilexical Dependency Grammars (PB-
DGs) have attracted attention recently for two rea-
sons. First, because they capture bilexical head-to-
head dependencies they are capable of producing
extremely high-quality parses: state-of-the-art dis-
criminatively trained PBDG parsers rival the accu-
racy of the very best statistical parsers available to-
day (McDonald, 2006). Second, Eisner-Satta O(n3)
PBDG parsing algorithms are extremely fast (Eisner,
1996; Eisner and Satta, 1999; Eisner, 2000).
This paper investigates the relationship between
Context-Free Grammar (CFG) parsing and the Eis-
ner/Satta PBDG parsing algorithms, including their
extension to second-order PBDG parsing (McDon-
ald, 2006; McDonald and Pereira, 2006). Specifi-
cally, we show how to use an off-line preprocessing
step, the Unfold-Fold transformation, to transform a
PBDG into an equivalent CFG that can be parsed in
O(n3) time using a version of the CKY algorithm
with suitable indexing (Younger, 1967), and extend
this transformation so that it captures second-order
PBDG dependencies as well. The transformations
are ambiguity-preserving, i.e., there is a one-to-
one mapping between dependency parses and CFG
parses, so it is possible to map the CFG parses back
to the PBDG parses they correspond to.
The PBDG to CFG reductions make techniques
developed for CFGs available to PBDGs as well. For
example, incremental CFG parsing algorithms can
be used with the CFGs produced by this transform,
as can the Inside-Outside estimation algorithm (Lari
and Young, 1990) and more exotic methods such as
estimating adjoined hidden states (Matsuzaki et al,
2005; Petrov et al, 2006). As an example appli-
cation, we describe a maximum posterior parse de-
coder for PBDGs in Section 8.
The Unfold-Fold transformation is a calculus for
transforming functional and logic programs into
equivalent but (hopefully) faster programs (Burstall
and Darlington, 1977). We use it here to trans-
form CFGs encoding dependency grammars into
other CFGs that are more efficiently parsable. Since
CFGs can be expressed as Horn-clause logic pro-
grams (Pereira and Shieber, 1987) and the Unfold-
Fold transformation is provably correct for such pro-
grams (Sato, 1992; Pettorossi and Proeitti, 1992), it
follows that its application to CFGs is provably cor-
rect as well. The Unfold-Fold transformation is used
here to derive the CFG schemata presented in sec-
tions 5?7. A system that uses these schemata (such
as the one described in section 8) can implement
168
these schemata directly, so the Unfold-Fold trans-
formation plays a theoretical role in this work, justi-
fying the resulting CFG schemata.
The closest related work we are aware of
is McAllester (1999), which also describes a re-
duction of PBDGs to efficiently-parsable CFGs
and directly inspired this work. However, the
CFGs produced by McAllester?s transformation in-
clude epsilon-productions so they require a special-
ized CFG parsing algorithm, while the CFGs pro-
duced by the transformations described here have
binary productions so they can be parsed with
standard CFG parsing algorithms. Further, our
approach extends to second-order PBDG parsing,
while McAllester only discusses first-order PBDGs.
The rest of this paper is structured as follows.
Section 2 defines projective dependency graphs and
grammars and Section 3 reviews the ?naive? encod-
ing of PBDGs as CFGs with an O(n5) parse time,
where n is the length of the string to be parsed. Sec-
tion 4 introduces the ?split-head? CFG encoding of
PBDGs, which has an O(n4) parse time and serves
as the input to the Unfold-Fold transform. Section 5
uses the Unfold-Fold transform to obtain a weakly-
equivalent CFG encoding of PBDGs which can be
parsed in O(n3) time, and presents timing results
showing that the transformation does speed parsing.
Sections 6 and 7 apply Unfold-Fold in slightly more
complex ways to obtain CFG encodings of PBDGs
that also make second-order dependencies available
in O(n3) time parsable CFGs. Section 8 applies a
PBDG to CFG transform to obtain a maximum pos-
terior decoding parser for PBDGs.
2 Projective bilexical dependency parses
and grammars
Let ? be a finite set of terminals (e.g., words),
and let 0 be the root terminal not in ?. If w =
(w1, . . . , wn) ? ??, let w? = (0, w1, . . . , wn), i.e.,
w? is obtained by prefixing w with 0. A dependency
parse G for w is a tree whose root is labeled 0 and
whose other n vertices are labeled with each of the n
terminals in w. If G contains an arc from u to v then
we say that v is a dependent of u, and if G contains
a path from u to v then we say that v is a descendant
of u. If v is dependent of u that also precedes u in
w? then we say that v is a left dependent of u (right
dependent and left and right descendants are defined
similarly).
0 Sandy gave the dog a bone
Figure 1: A projective dependency parse for the sen-
tence ?Sam gave the dog a bone?.
A dependency parse G is projective iff whenever
there is a path from u to v then there is also a path
from u to every word between u and v in w? as well.
Figure 1 depicts a projective dependency parse for
the sentence ?Sam gave the dog a bone?.
A projective dependency grammar defines a set of
projective dependency parses. A Projective Bilexi-
cal Dependency Grammar (PBDG) consists of two
relations and , both defined over (??{0})?
?. A PBDG generates a projective dependency
parse G iff u v for all right dependencies (u, v)
in G and v u for all left dependencies (u, v) in
G. The language generated by a PBDG is the set
of strings that have projective dependency parses
generated by the grammar. The following depen-
dency grammar generates the dependency parse in
Figure 1.
0 gave Sandy gave
gave dog the dog
gave bone a bone
This paper does not consider stochastic depen-
dency grammars directly, but see Section 8 for an
application involving them. However, it is straight-
forward to associate weights with dependencies, and
since the dependencies are preserved by the transfor-
mations, obtain a weighted CFG. Standard methods
for converting weighted CFGs to equivalent PCFGs
can be used if required (Chi, 1999). Alternatively,
one can transform a corpus of dependency parses
into a corpus of the corresponding CFG parses, and
estimate CFG production probabilities directly from
that corpus.
3 A naive encoding of PBDGs
There is a well-known method for encoding a PBDG
as a CFG in which each terminal u ? ? is associated
with a corresponding nonterminal Xu that expands
to u and all of u?s descendants. The nonterminals of
the naive encoding CFG consist of the start symbol
S and symbols Xu for each terminal u ? ?, and
169
the productions of the CFG are the instances of the
following schemata:
S ? Xu where 0 u
Xu ? u
Xu ? Xv Xu where v u
Xu ? Xu Xv where u v
The dependency annotations associated with each
production specify how to interpret a local tree gen-
erated by that production, and permit us to map a
CFG parse to the corresponding dependency parse.
For example, the top-most local tree in Figure 2 was
generated by the production S ? Xgave, and indi-
cate that in this parse 0 gave.
Given a terminal vocabulary of size m the CFG
contains O(m2) productions, so it is impractical to
enumerate all possible productions for even modest
vocabularies. Instead productions relevant to a par-
ticular sentence are generated on the fly.
The naive encoding CFG in general requires
O(n5) parsing time with a conventional CKY pars-
ing algorithm, since tracking the head annotations u
and v multiplies the standard O(n3) CFG parse time
requirements by an additional factor proportional to
the O(n2) productions expanding Xu.
An additional problem with the naive encoding
is that the resulting CFG in general exhibits spuri-
ous ambiguities, i.e., a single dependency parse may
correspond to more than one CFG parse, as shown
in Figure 2. Informally, this is because the CFG per-
mits left and the right dependencies to be arbitrarily
intermingled.
4 Split-head encoding of PBDGs
There are several ways of removing the spurious am-
biguities in the naive CFG encoding just described.
This section presents a method we call the ?split-
head encoding?, which removes the ambiguities and
serves as starting point for the grammar transforms
described below.
The split-head encoding represents each word u
in the input string w by two unique terminals ul
and ur in the CFG parse. A split-head CFG?s ter-
minal vocabulary is ?? = {ul, ur : u ? ?},
where ? is the set of terminals of the PBDG. A
PBDG parse with yield w = (u1, . . . , un) is trans-
formed to a split-head CFG parse with yield w? =
(u1,l, u1,r, . . . , un,l, un,r), so |w?| = 2|w|.
S
the dog
Xthe Xdog
XdogXgave
gave
Xgave Xbone
Xa
a
Xbone
bone
XgaveXSandy
Sandy
Xgave
S
the dog
Xthe Xdog
Xdog
Xbone
Xa
a
Xbone
bone
Xgave
Xgave
gave
XSandy
Sandy
Xgave
Xgave
Figure 2: Two parses using the naive CFG encod-
ing that both correspond to the dependency parse of
Figure 1.
The split-head CFG for a PBDG is given by the
following schemata:
S ? Xu where 0 u
Xu ? Lu uR where u ? ?
Lu ? ul
Lu ? Xv Lu where v u
uR ? ur
uR ? uR Xv where u v
The dependency parse shown in Figure 1 corre-
sponds to the split-head CFG parse shown in Fig-
ure 3. Each Xu expands to two new categories, Lu
and uR. Lu consists of ul and all of u?s left descen-
dants, while uR consists of ur and all of u?s right
descendants. The spurious ambiguity present in the
naive encoding does not arise in the split-head en-
coding because the left and right dependents of a
head are assembled independently and cannot inter-
mingle.
As can be seen by examining the split-head
schemata, the rightmost descendant of Lu is either
Lu or ul, which guarantees that the rightmost termi-
nal dominated by Lu is always ul; similarly the left-
most terminal dominated by uR is always ur. Thus
170
dogR
XSandy
LSandy
Sandyl
Xdog
gavergavel
gaveR
gaveR
La
al
aR
ar
Xa Lbone
bonel
Lbone
boner
boneR
Xbone
SandyR
Sandyr
Lgave
Lgave
Xgave
S
gaveR
Lthe
thel
theR
ther
Xthe Ldog
dogl
Ldog
dogr
Figure 3: The split-head parse corresponding to the dependency graph depicted in Figure 1. Notice that ul
is always the rightmost descendant of Lu and ur is always the leftmost descendant of uR, which means that
these indices are redundant given the constituent spans.
these subscript indices are redundant given the string
positions of the constituents, which means we do not
need to track the index u in Lu and uR but can parse
with just the two categories L and R, and determine
the index from the constituent?s span when required.
It is straight-forward to extend the split-head CFG
to encode the additional state information required
by the head automata of Eisner and Satta (1999);
this corresponds to splitting the non-terminals Lu
and uR. For simplicity we work with PBDGs in this
paper, but all of the Unfold-Fold transformations de-
scribed below extend to split-head grammars with
the additional state structure required by head au-
tomata.
Implementation note: it is possible to directly
parse the ?undoubled? input string w by modifying
both the CKY algorithm and the CFGs described
in this paper. Modify Lu and uR so they both ul-
timately expand to the same terminal u, and special-
case the implementation of production Xu ? Lu uR
and all productions derived from it to permit Lu and
uR to overlap by the terminal u.
The split-head formulation explains what initially
seem unusual properties of existing PBDG algo-
rithms. For example, one of the standard ?sanity
checks? for the Inside-Outside algorithm?that the
outside probability of each terminal is equal to the
sentence?s inside probability?fails for these algo-
rithms. In fact, the outside probability of each ter-
minal is double the sentence?s inside probability be-
cause these algorithms implicitly collapse the two
terminals ul and ur into a single terminal u.
5 A O(n3) split-head grammar
The split-head encoding described in the previous
section requires O(n4) parsing time because the in-
dex v on Xv is not redundant. We can obtain an
equivalent grammar that only requires O(n3) pars-
ing time by transforming the split-head grammar us-
ing Unfold-Fold. We describe the transformation on
Lu; the transformation of uR is symmetric.
We begin with the definition of Lu in the split-
head grammar above (?|? separates the right-hand
sides of productions).
Lu ? ul | Xv Lu where v u
Our first transformation step is to unfold Xv in Lu,
i.e., replace Xv by its expansion, producing the fol-
lowing definition for Lu (ignore the underlining for
now).
Lu ? ul | Lv vR Lu where v u
This removes the offending Xv in Lu, but the result-
ing definition of Lu contains ternary productions and
so still incurs O(n4) parse time. To address this we
define new nonterminals xMy for each x, y ? ?:
xMy ? xR Ly
and fold the underlined children in Lu into vMu:
xMy ? xR Ly where x, y ? ?
Lu ? ul | Lv vMu where v u
171
Sdogrther
LdogtheR
theMdog
thel
Lthe
Ldog
dogl
gaver
gaveR
gaveMdog
gavel
dogR
gaveR
al ar
aR
bonel
Lbone
aMboneLa
Lbone
gaveMbone boneR
boner
gaveRLgave
LgaveSandyR
SandyMgaveLSandy
Sandyl Sandyr
Figure 4: The O(n3) split-head parse corresponding to the dependency graph of Figure 1.
The O(n3) split-head grammar is obtained by un-
folding the occurence of Xu in the S production and
dropping the Xu schema as Xu no longer appears on
the right-hand side of any production. The resulting
O(n3) split-head grammar schemata are as follows:
S ? Lu uR where 0 u
Lu ? ul
Lu ? Lv vMu where v u
uR ? ur
uR ? uMv vR where u v
xMy ? xR Ly where x, y ? ?
As before, the dependency annotations on the pro-
duction schemata permit us to map CFG parses to
the corresponding dependency parse. This grammar
requires O(n3) parsing time to parse because the in-
dices are redundant given the constituent?s string po-
sitions for the reasons described in section 4. Specif-
ically, the rightmost terminal of Lu is always ul, the
leftmost terminal of uR is always ur and the left-
most and rightmost terminals of vMu are vl and ur
respectively.
The O(n3) split-head grammar is closely related
to the O(n3) PBDG parsing algorithm given by Eis-
ner and Satta (1999). Specifically, the steps involved
in parsing with this grammar using the CKY algo-
rithm are essentially the same as those performed
by the Eisner/Satta algorithm. The primary differ-
ence is that the Eisner/Satta algorithm involves two
separate categories that are collapsed into the single
category M here.
To confirm their relative performance we imple-
mented stochastic CKY parsers for the three CFG
schemata described so far. The production schemata
were hard-coded for speed, and the implementation
trick described in section 4 was used to avoid dou-
bling the terminal string. We obtained dependency
weights from our existing discriminatively-trained
PBDG parser (not cited to preserve anonymity). We
compared the parsers? running times on section 24
of the Penn Treebank. Because all three CFGs im-
plement the same dependency grammar their Viterbi
parses have the same dependency accuracy, namely
0.8918. We precompute the dependency weights,
so the times include just the dynamic programming
computation on a 3.6GHz Pentium 4.
CFG schemata sentences parsed / second
Naive O(n5) CFG 45.4
O(n4) CFG 406.2
O(n3) CFG 3580.0
6 An O(n3) adjacent-head grammar
This section shows how to further transform the
O(n3) grammar described above into a form that
encodes second-order dependencies between ad-
jacent dependent heads in much the way that a
Markov PCFG does (McDonald, 2006; McDonald
and Pereira, 2006). We provide a derivation for the
Lu constituents; there is a parallel derivation for uR.
We begin by unfolding Xv in the definition of Lu
in the split-head grammar, producing as before:
Lu ? ul | Lv vR Lu
Now introduce a new nonterminal vM
L
u, which is a
specialized version of M requiring that v is a left-
dependent of u, and fold the underlined constituents
172
Sther
theR
theM
L
dog
thel
Lthe
Ldog dogR
dogl dogr
Lbone
La aM
L
bone
aR
aral
bonel boner
gaver
gaveM
R
dog dogMbone
gaveM
R
bone boneR
gaveR
gavel
Sandyr
SandyR
Sandyl
SandyM
L
gaveLSandy
Lgave
Figure 5: The O(n3) adjacent-head parse corresponding to the dependency graph of Figure 1. The boxed
local tree indicates bone is the dependent of give following the dependent dog, i.e., give dog bone .
into vM
L
u.
vM
L
u ? vR Lu where v u
Lu ? ul | Lv vM
L
u where v u
Now unfold Lu in the definition of vM
L
u, producing:
vM
L
u ? vR ul | vR Lv? v? M
L
u; v v
? u
Note that in the first production expanding vM
L
u, v
is the closest left dependent of u, and in the second
production v and v? are adjacent left-dependents of
u. vM
L
u has a ternary production, so we introduce
xMy as before to fold the underlined constituents
into.
xMy ? xR Ly where x, y ? ?
vM
L
u ? vR ul | vMv? v?M
L
u; v v
? u
The resulting grammar schema is as below, and a
sample parse is given in Figure 5.
S ? Lu uR where 0 u
Lu ? ul u has no left dependents
Lu ? Lv vM
L
u v is u?s last left dep.
vM
L
u ? vR ul v is u?s closest left dep.
vM
L
u ? vMv? v?M
L
u v v
? u
uR ? ur u has no right dependents
uR ? uM
R
v vR v is u?s last right dep.
uM
R
v ? ur Lv v is u?s closest right dep.
uM
R
v ? uM
R
v? v?Mv u v
? v
xMy ? xR Ly where x, y ? ?
As before, the indices on the nonterminals are re-
dundant, as the heads are always located at an edge
of each constituent, so they need not be computed
or stored and the CFG can be parsed in O(n3) time.
The steps involved in CKY parsing with this gram-
mar correspond closely to those of the McDonald
(2006) second-order PBDG parsing algorithm.
7 An O(n3) dependent-head grammar
This section shows a different application of Unfold-
Fold can capture head-to-head-to-head dependen-
cies, i.e., ?vertical? second-order dependencies,
rather than the ?horizontal? ones captured by the
transformation described in the previous section.
Because we expect these vertical dependencies to
be less important linguistically than the horizontal
ones, we only sketch the transformation here.
The derivation differs from the one in Section 6 in
that the dependent vR, rather than the head Lu, is un-
folded in the initial definition of vM
L
u. This results in
a grammar that tracks vertical, rather than horizon-
tal, second-order dependencies. Since left-hand and
right-hand derivations are assembled separately in a
split-head grammar, the grammar in fact only tracks
zig-zag type dependencies (e.g., where a grandpar-
ent has a right dependent, which in turn has a left
dependent).
The resulting grammar is given below, and a sam-
ple parse using this grammar is shown in Figure 6.
Because the subscripts are redundant they can be
omitted and the resulting CFG can be parsed in
173
gaveM
R
bone
gaver
gaveR Lthe
thel
gaveMthe
ther dogl
Ldog
theM
L
dog
dogr
dogR
gaveR La
al ar
gaveMa aM
L
bone
Lbone
bonel boner
boneR
gaveR
Lgave
gavel
Sandyr
SandyM
L
gave
Sandyl
LSandy
gaveM
R
dog
S
Lgave
Figure 6: The n3 dependent-head parse corresponding to the dependency graph of Figure 1. The boxed
local tree indicates that a is a left-dependent of bone, which is in turn a right-dependent of gave, i.e.,
gave a bone .
O(n3) time using the CKY algorithm.
S ? Lu uR where 0 u
Lu ? ul
Lu ? Lv vM
L
u where v u
vM
L
u ? vr Lu where v u
vM
L
u ? vM
R
w wMu where v w u
uR ? ur
uR ? uM
R
v vR where u v
uM
R
v ? uR vl where u v
uM
R
v ? uMw wM
L
v where u w u
xMy ? xR Ly where x, y ? ?
8 Maximum posterior decoding
As noted in the introduction, one consequence of the
PBDG to CFG reductions presented in this paper is
that CFG parsing and estimation techniques are now
available for PBDGs as well. As an example ap-
plication, this section describes Maximum Posterior
Decoding (MPD) for PBDGs.
Goodman (1996) observed that the Viterbi parse
is in general not the optimal parse for evaluation
metrics such as f-score that are based on the number
of correct constituents in a parse. He showed that
MPD improves f-score modestly relative to Viterbi
decoding for PCFGs.
Since dependency parse accuracy is just the pro-
portion of dependencies in the parse that are correct,
Goodman?s observation should hold for PBDG pars-
ing as well. MPD for PBDGs selects the parse that
maximizes the sum of the marginal probabilities of
each of the dependencies in the parse. Such a de-
coder might plausibly produce parses that score bet-
ter on the dependency accuracy metric than Viterbi
parses.
MPD is straightforward given the PBDG to CFG
reductions described in this paper. Specifically, we
use the Inside-Outside algorithm to compute the
posterior probability of the CFG constituents corre-
sponding to each PBDG dependency, and then use
the Viterbi algorithm to find the parse tree that max-
imizes the sum of these posterior probabilities.
We implemented MPD for first-order PBDGs
using dependency weights from our existing
discriminatively-trained PBDG parser (not cited to
preserve anonymity). These weights are estimated
by an online procedure as in McDonald (2006), and
are not intended to define a probability distribution.
In an attempt to heuristically correct for this, in this
experiment we used exp(?wu,v) as the weight of the
dependency between head u and dependent v, where
wu,v is the weight provided by the discriminatively-
trained model and ? is an adjustable scaling parame-
ter tuned to optimize MPD accuracy on development
data.
Unfortunately we found no significant differ-
ence between the accuracy of the MPD and Viterbi
parses. Optimizing MPD on the development data
(section 24 of the PTB) set the scale factor ? =
0.21 and produced MPD parses with an accuracy
of 0.8921, which is approximately the same as the
Viterbi accuracy of 0.8918. On the blind test data
(section 23) the two accuracies are essentially iden-
174
tical (0.8997).
There are several possible explanations for the
failure of MPD to produce more accurate parses than
Viterbi decoding. Perhaps MPD requires weights
that define a probability distribution (e.g., a Max-
Ent model). It is also possible that discriminative
training adjusts the weights in a way that ensures
that the Viterbi parse is close to the maximum pos-
terior parse. This was the case in our experiment,
and if this is true with discriminative training in gen-
eral, then maximum posterior decoding will not have
much to offer to discriminative parsing.
9 Conclusion
This paper shows how to use the Unfold-Fold trans-
form to translate PBDGs into CFGs that can be
parsed in O(n3) time. A key component of this is
the split-head construction, where each word u in the
input is split into two terminals ul and ur of the CFG
parse. We also showed how to systematically trans-
form the split-head CFG into grammars which track
second-order dependencies. We provided one gram-
mar which captures horizontal second-order depen-
dencies (McDonald, 2006), and another which cap-
tures vertical second-order head-to-head-to-head de-
pendencies.
The grammars described here just scratch the sur-
face of what is possible with Unfold-Fold. Notice
that both of the second-order grammars have more
nonterminals than the first-order grammar. If one is
prepared to increase the number of nonterminals still
further, it may be possible to track additional infor-
mation about constituents (although if we insist on
O(n3) parse time we will be unable to track the in-
teraction of more than three heads at once).
References
R.M. Burstall and John Darlington. 1977. A transformation
system for developing recursive programs. Journal of the
Association for Computing Machinery, 24(1):44?67.
Zhiyi Chi. 1999. Statistical properties of probabilistic context-
free grammars. Computational Linguistics, 25(1):131?160.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton gram-
mars. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages 457?480,
University of Maryland.
Jason Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In COLING96: Proceedings
of the 16th International Conference on Computational Lin-
guistics, pages 340?345, Copenhagen. Center for Sprogte-
knologi.
Jason Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Harry Bunt and Anton Nijholt, edi-
tors, Advances in Probabilistic and Other Parsing Technolo-
gies, pages 29?62. Kluwer Academic Publishers.
Joshua T. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 177?183, Santa Cruz,
Ca.
K. Lari and S.J. Young. 1990. The estimation of Stochastic
Context-Free Grammars using the Inside-Outside algorithm.
Computer Speech and Language, 4(35-56).
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 75?82, Ann Arbor,
Michigan, June. Association for Computational Linguistics.
David McAllester. 1999. A reformulation of Eisner and Sata?s
cubic time parser for split head automata grammars. Avail-
able from http://ttic.uchicago.edu/?dmcallester/.
Ryan McDonald and Fernando Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In 11th
Conference of the European Chapter of the Association for
Computational Linguistics, pages 81?88, Trento, Italy.
Ryan McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis, Uni-
versity of Pennyslvania, Philadelphia, PA.
Fernando Pereira and Stuart M. Shieber. 1987. Prolog and Nat-
ural Language Analysis. Center for the Study of Language
and Information, Stanford, CA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics, pages
433?440, Sydney, Australia, July. Association for Computa-
tional Linguistics.
A. Pettorossi and M. Proeitti. 1992. Transformation of logic
programs. In Handbook of Logic in Artificial Intelligence,
volume 5, pages 697?787. Oxford University Press.
Taisuke Sato. 1992. Equivalence-preserving first-order un-
fold/fold transformation systems. Theoretical Computer Sci-
ence, 105(1):57?84.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and Control,
10(2):189?208.
175
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824?831,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Comparative Study of Parameter Estimation Methods for 
Statistical Natural Language Processing 
Jianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova* 
*Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com 
&Brown University, Providence, RI 02912,  mj@cs.brown.edu 
 
Abstract 
This paper presents a comparative study of 
five parameter estimation algorithms on four 
NLP tasks. Three of the five algorithms are 
well-known in the computational linguistics 
community: Maximum Entropy (ME) estima-
tion with L2 regularization, the Averaged 
Perceptron (AP), and Boosting.  We also in-
vestigate ME estimation with L1 regularization 
using a novel optimization algorithm, and 
BLasso, which is a version of Boosting with 
Lasso (L1) regularization.  We first investigate 
all of our estimators on two re-ranking tasks: a 
parse selection task and a language model 
(LM) adaptation task.  Then we apply the best 
of these estimators to two additional tasks 
involving conditional sequence models: a 
Conditional Markov Model (CMM) for part of 
speech tagging and a Conditional Random 
Field (CRF) for Chinese word segmentation. 
Our experiments show that across tasks, three 
of the estimators ? ME estimation with L1 or 
L2 regularization, and AP ? are in a near sta-
tistical tie for first place. 
1 Introduction 
Parameter estimation is fundamental to many sta-
tistical approaches to NLP. Because of the 
high-dimensional nature of natural language, it is 
often easy to generate an extremely large number of 
features.  The challenge of parameter estimation is 
to find a combination of the typically noisy, re-
dundant features that accurately predicts the target 
output variable and avoids overfitting. Intuitively, 
this can be achieved either by selecting a small 
number of highly-effective features and ignoring 
the others, or by averaging over a large number of 
weakly informative features.  The first intuition 
motivates feature selection methods such as 
Boosting and BLasso (e.g., Collins 2000; Zhao and 
Yu, 2004), which usually work best when many 
features are completely irrelevant. L1 or Lasso 
regularization of linear models, introduced by 
Tibshirani (1996), embeds feature selection into 
regularization so that both an assessment of the 
reliability of a feature and the decision about 
whether to remove it are done in the same frame-
work, and has generated a large amount of interest 
in the NLP community recently (e.g., Goodman 
2003; Riezler and Vasserman 2004).  If on the other 
hand most features are noisy but at least weakly 
correlated with the target, it may be reasonable to 
attempt to reduce noise by averaging over all of the 
features.  ME estimators with L2 regularization, 
which have been widely used in NLP tasks (e.g., 
Chen and Rosenfeld 2000; Charniak and Johnson 
2005; Johnson et al 1999), tend to produce models 
that have this property.  In addition, the perceptron 
algorithm and its variants, e.g., the voted or aver-
aged perceptron, is becoming increasingly popular 
due to their competitive performance, simplicity in 
implementation and low computational cost in 
training (e.g., Collins 2002). 
While recent studies claim advantages for L1 
regularization, this study is the first of which we are 
aware to systematically compare it to a range of 
estimators on a diverse set of NLP tasks.  Gao et al 
(2006) showed that BLasso, due to its explicit use of 
L1 regularization, outperformed Boosting in the LM 
adaptation task.  Ng (2004) showed that for logistic 
regression, L1 regularization outperforms L2 regu-
larization on artificial datasets which contain many 
completely irrelevant features.  Goodman (2003) 
showed that in two out of three tasks, an ME esti-
mator with a one-sided Laplacian prior (i.e., L1 
regularization with the constraint that all feature 
weights are positive) outperformed a comparable 
estimator using a Gaussian prior (i.e., L2 regulari-
zation).  Riezler and Vasserman (2004) showed that 
an L1-regularized ME estimator outperformed an 
L2-regularized estimator for ranking the parses of a 
stochastic unification-based grammar. 
824
While these individual estimators are well de-
scribed in the literature, little is known about the 
relative performance of these methods because the 
published results are generally not directly compa-
rable.  For example, in the parse re-ranking task, 
one cannot tell whether the L2- regularized ME 
approach used by Charniak and Johnson (2005) 
significantly outperforms the Boosting method by 
Collins (2000) because different feature sets and 
n-best parses were used in the evaluations of these 
methods.  
This paper conducts a much-needed comparative 
study of these five parameter estimation algorithms 
on four NLP tasks: ME estimation with L1 and L2 
regularization, the Averaged Perceptron (AP), 
Boosting, and BLasso, a version of Boosting with 
Lasso (L1) regularization.  We first investigate all of 
our estimators on two re-ranking tasks: a parse 
selection task and a language model adaptation task. 
Then we apply the best of these estimators to two 
additional tasks involving conditional sequence 
models: a CMM for POS tagging and a CRF for 
Chinese word segmentation.  Our results show that 
ME estimation with L2 regularization achieves the 
best performing estimators in all of the tasks, and 
AP achieves almost as well and requires much less 
training time. L1 (Lasso) regularization also per-
forms well and leads to sparser models. 
2 Estimators 
All the four NLP tasks studied in this paper are 
based on linear models (Collins 2000) which re-
quire learning a mapping from inputs ? ? ? to 
outputs ? ? ?.  We are given: 
? Training samples (?? ,??) for ? = 1??, 
? A procedure ??? to generate a set of candi-
dates ???(?) for an input x,  
? A feature mapping ?:? ? ? ? ??  to map 
each (?,?) to a vector of feature values, and 
? A parameter vector ? ? ?? , which assigns a 
real-valued weight to each feature. 
For all models except the CMM sequence model for 
POS tagging, the components ???, ? and ? di-
rectly define a mapping from an input ? to an output 
?(?) as follows: 
? ? = arg max????? ? ? ?,? ? ?. (1) 
In the CMM sequence classifier, locally normalized 
linear models to predict the tag of each word token 
are chained together to arrive at a probability esti-
mate for the entire tag sequence, resulting in a 
slightly different decision rule. 
Linear models, though simple, can capture very 
complex dependencies because the features can be 
arbitrary functions of the input/output pair.  For 
example, we can define a feature to be the log con-
ditional probability of the output as estimated by 
some other model, which may in turn depend on 
arbitrarily complex interactions of ?basic? features.  
In practice, with an appropriate feature set, linear 
models achieve very good empirical results on 
various NLP tasks.  The focus of this paper however 
is not on feature definition (which requires domain 
knowledge and varies from task to task), but on 
parameter estimation (which is generic across 
tasks).  We assume we are given fixed feature 
templates from which a large number of features are 
generated.  The task of the estimator is to use the 
training samples to choose a parameter vector ?, 
such that the mapping ?(?) is capable of correctly 
classifying unseen examples. We will describe the 
five estimators in our study individually. 
2.1 ME estimation with L2 regularization 
Like many linear models, the ME estimator chooses 
? to minimize the sum of the empirical loss on the 
training set and a regularization term: 
? = arg min?  ? ? + ? ?   . (2) 
In this case, the loss term L(w) is the negative con-
ditional log-likelihood of the training data, 
 ? ? = ? log? ??  ??)
?
?=1 ,  where 
? ? ?) =
exp ? ?,? ? ? 
 exp(? ?,? ? ? ?)? ????? ? 
 
and the regularizer term ? ? = ? ??
2
?  is the 
weighted squared L2 norm of the parameters. Here, 
? is a parameter that controls the amount of regu-
larization, optimized on held-out data.  
This is one of the most popular estimators,  
largely due to its appealing computational proper-
ties: both ? ?  and ?(?) are convex and differen-
tiable, so gradient-based numerical algorithms can 
be used to find the global minimum efficiently.  
In our experiments, we used the limited memory 
quasi-Newton algorithm (or L-BFGS, Nocedal and 
Wright 1999) to find the optimal ? because this 
method has been shown to be substantially faster 
than other methods such as Generalized Iterative 
Scaling (Malouf 2002).  
825
Because for some sentences there are multiple 
best parses (i.e., parses with the same F-Score), we 
used the variant of ME estimator described in 
Riezler et al (2002), where ? ?  is defined as the 
likelihood of the best parses ? ? ?(?) relative to 
the n-best parser output ??? ? ,  (i.e., ? ? ?
???(?)): ? ? = ? log ?(?? |??)????(??)
?
?=1 . 
We applied this variant in our experiments of 
parse re-ranking and LM adaptation, and found that 
on both tasks it leads to a significant improvement 
in performance for the L2-regularied ME estimator 
but not for the L1-regularied ME estimator. 
2.2 ME estimation with L1 regularization 
This estimator also minimizes the negative condi-
tional log-likelihood, but uses an L1 (or Lasso) 
penalty. That is, ?(?) in Equation (2) is defined 
according to ? ? = ?  ??  ? . L1 regularization 
typically leads to sparse solutions in which many 
feature weights are exactly zero, so it is a natural 
candidate when feature selection is desirable. By 
contrast, L2 regularization produces solutions in 
which most weights are small but non-zero. 
Optimizing the L1-regularized objective function 
is challenging because its gradient is discontinuous 
whenever some parameter equals zero. Kazama and 
Tsujii (2003) described an estimation method that 
constructs an equivalent constrained optimization 
problem with twice the number of variables.  
However, we found that this method is impracti-
cally slow for large-scale NLP tasks. In this work 
we use the orthant-wise limited-memory qua-
si-Newton algorithm (OWL-QN), which is a mod-
ification of L-BFGS that allows it to effectively 
handle the discontinuity of the gradient (Andrew 
and Gao 2007). We provide here a high-level de-
scription of the algorithm. 
A quasi-Newton method such as L-BFGS uses 
first order information at each iterate to build an 
approximation to the Hessian matrix, ?, thus mod-
eling the local curvature of the function. At each 
step, a search direction is chosen by minimizing a 
quadratic approximation to the function: 
? ? =
1
2
 ? ? ?0 
?? ? ? ?0 + ?0
? (? ? ?0) 
where ?0 is the current iterate, and ?0 is the func-
tion gradient at ?0 .  If ? is positive definite, the 
minimizing value of ? can be computed analytically 
according to: ?? = ?0 ??
?1?0. 
L-BFGS maintains vectors of the change in gradient 
?? ? ???1 from the most recent iterations, and uses 
them to construct an estimate of the inverse Hessian 
???. Furthermore, it does so in such a way that 
??1?0 can be computed without expanding out the 
full matrix, which is typically unmanageably large. 
The computation requires a number of operations 
linear in the number of variables. 
OWL-QN is based on the observation that when 
restricted to a single orthant, the L1 regularizer is 
differentiable, and is in fact a linear function of ?.  
Thus, so long as each coordinate of any two con-
secutive search points does not pass through zero, 
?(?) does not contribute at all to the curvature of 
the function on the segment joining them.  There-
fore, we can use L-BFGS to approximate the Hes-
sian of ? ?  alone, and use it to build an approxi-
mation to the full regularized objective that is valid 
on a given orthant. To ensure that the next point is in 
the valid region, we project each point during the 
line search back onto the chosen orthant.1 At each 
iteration, we choose the orthant containing the 
current point and into which the direction giving the 
greatest local rate of function decrease points. 
This algorithm, although only a simple modifi-
cation of L-BFGS, works quite well in practice. It 
typically reaches convergence in even fewer itera-
tions than standard L-BFGS takes on the analogous 
L2-regularized objective (which translates to less 
training time, since the time per iteration is only 
negligibly higher, and total time is dominated by 
function evaluations). We describe OWL-QN more 
fully in (Andrew and Gao 2007). We also show that 
it is significantly faster than Kazama and Tsujii?s 
algorithm for L1 regularization and prove that it is 
guaranteed converge to a parameter vector that 
globally optimizes the L1-regularized objective. 
2.3 Boosting 
The Boosting algorithm we used is based on Collins 
(2000).  It optimizes the pairwise exponential loss 
(ExpLoss) function (rather than the logarithmic loss 
optimized by ME).  Given a training sample 
(?? ,??), for each possible output ?? ? ???(??), we 
                                                     
1 This projection just entails zeroing-out any coordinates 
that change sign. Note that it is possible for a variable to 
change sign in two iterations, by moving from a negative 
value to zero, and on a the next iteration moving from 
zero to a positive value. 
826
define the margin of the pair (?? ,?? ) with respect to 
? as ? ?? ,??  = ? ?? ,?? ? ? ?  ? ?? ,??  ? ?. 
Then ExpLoss is defined as 
ExpLoss ? =  exp  ?M yi , yj  
?????? ?? ?
 (3) 
Figure 1 summarizes the Boosting algorithm we 
used. It is an incremental feature selection proce-
dure. After initialization, Steps 2 and 3 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated as follows.  
First, we define Upd(?,?, ?)  as an updated 
model, with the same parameter values as ? with 
the exception of ?? , which is incremented by ?: 
Upd ?, ?, ? = (?1 ,? ,?? + ?,? ,??)  
Then, Steps 2 and 3 in Figure 1 can be rewritten as 
Equations (4) and (5), respectively. 
 ??, ?? = arg min
? ,?
ExpLoss(Upd ?, ?, ? ) (4) 
?? = Upd(???1, ??, ??) (5) 
Because Boosting can overfit we update the weight 
of ??? by a small fixed step size ?, as in Equation (6), 
following the FSLR algorithm (Hastie et al 2001).  
?? = Upd(???1, ??, ? ? sign ?? ) (6) 
By taking such small steps, Boosting imposes a 
kind of implicit regularization, and can closely 
approximate the effect of L1 regularization in a local 
sense (Hastie et al 2001).  Empirically, smaller 
values of ? lead to smaller numbers of test errors. 
2.4 Boosted Lasso 
The Boosted Lasso (BLasso) algorithm was origi-
nally proposed in Zhao and Yu (2004), and was 
adapted for language modeling by Gao et al (2006). 
BLasso can be viewed as a version of Boosting with 
L1 regularization. It optimizes an L1-regularized 
ExpLoss function: 
LassoLoss ? = ExpLoss(?) + ?(?) (7) 
where ? ? = ?  ??  ?  . 
BLasso also uses an incremental feature selec-
tion procedure to learn parameter vector ?, just as 
Boosting does.  Due to the explicit use of the regu-
larization term ?(?), however, there are two major 
differences from Boosting.  
At each iteration, BLasso takes either a forward 
step or a backward step.  Similar to Boosting, at 
each forward step, a feature is selected and its 
weight is updated according to Eq. (8) and (9). 
 ??, ?? = ??? ???
? ,?=??
ExpLoss(Upd ?, ?, ? ) (8) 
?? = Upd(???1, ??, ? ? sign ?? ) (9) 
There is a small but important difference between 
Equations (8) and (4). In Boosting, as shown in 
Equation (4), a feature is selected by its impact on 
reducing the loss with its optimal update ?? . By 
contrast, in BLasso, as shown in Equation (8), 
rather than optimizing over ? for each feature, the 
loss is calculated with an update of either +? or ??, 
i.e., grid search is used for feature weight estima-
tion.  We found in our experiments that this mod-
ification brings a consistent improvement. 
The backward step is unique to BLasso.  At each 
iteration, a feature is selected and the absolute value 
of its weight is reduced by ? if and only if it leads to 
a decrease of the LassoLoss, as shown in Equations 
(10) and (11), where ?  is a tolerance parameter. 
?? = arg min
? :???0
ExpLoss(Upd(?, ?,??sign ?? ) (10) 
?? = Upd(???1 , ??,sign(???) ? ?)  (11) 
if LassoLoss ???1,???1 ? LassoLoss ?? ,?? > ? 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated either backward or forward by a 
fixed amount ?.  Notice that the value of ? is adap-
tively chosen according to the reduction of ExpLoss 
during training.  The algorithm starts with a large 
initial ?, and then at each forward step the value of 
? decreases until ExpLoss stops decreasing.  This is 
intuitively desirable: it is expected that most highly 
effective features are selected in early stages of 
training, so the reduction of ExpLoss at each step in 
early stages are more substantial than in later stages.  
These early steps coincide with the Boosting steps 
most of the time.  In other words, the effect of 
backward steps is more visible at later stages.  It can 
be proved that for a finite number of features and 
? =0, the BLasso algorithm shown in Figure 2 
converges to the Lasso solution when ? ? 0. See 
Gao et al (2006) for implementation details, and 
Zhao and Yu (2004) for a theoretical justification 
for BLasso. 
1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Equation (3) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
827
2.5 Averaged Perceptron 
The perceptron algorithm can be viewed as a form 
of incremental training procedure (e.g., using sto-
chastic approximation) that optimizes a minimum 
square error (MSE) loss function (Mitchell, 1997).  
As shown in Figure 3, it starts with an initial pa-
rameter setting and updates it for each training 
example. In our experiments, we used the Averaged 
Perceptron algorithm of Freund and Schapire 
(1999), a variation that has been shown to be more 
effective than the standard algorithm (Collins 
2002).  Let ??,?  be the parameter vector after the ?th 
training sample has been processed in pass ? over 
the training data. The average parameters are de-
fined as?  =
?
??
  ??,???  where T is the number of 
epochs, and N is the number of training samples. 
3 Evaluations 
From the four tasks we consider, parsing and lan-
guage model adaptation are both examples of 
re-ranking.  In these tasks, we assume that we have 
been given a list of candidates ???(?) for each 
training or test sample  ?,? , generated using a 
baseline model.  Then, a linear model of the form in 
Equation (1) is used to discriminatively re-rank the 
candidate list using additional features which may 
or may not be included in the baseline model.  Since 
the mapping from ? to ? by the linear model may 
make use of arbitrary global features of the output 
and is performed ?all at once?, we call such a linear 
model a global model.  
In the other two tasks (i.e., Chinese word seg-
mentation and POS tagging), there is no explicit 
enumeration of ???(?).  The mapping from ? to ? 
is determined by a sequence model which aggre-
gates the decisions of local linear models via a 
dynamic program.  In the CMM, the local linear 
models are trained independently, while in the CRF 
model, the local models are trained jointly.  We call 
these two linear models local models because they 
dynamically combine the output of models that use 
only local features. 
While it is straightforward to apply the five es-
timators to global models in the re-ranking 
framework, the application of some estimators to 
the local models is problematic. Boosting and 
BLasso are too computationally expensive to be 
applied to CRF training and we compared the other 
three better performing estimation methods for this 
model. The CMM is a probabilistic sequence model 
and the log-loss used by ME estimation is most 
natural for it; thus we limit the comparison to the 
two kinds of ME models for CMMs. Note that our 
goal is not to compare locally trained models to 
globally trained ones; for a study which focuses on 
this issue, see (Punyakanok et al 2005). 
In each task we compared the performance of 
different estimators using task-specific measures. 
We used the Wilcoxon signed rank test to test the 
statistical significance of the difference among the 
competing estimators. We also report other results 
such as number of non-zero features after estima-
tion, number of training iterations, and computation 
time (in minutes of elapsed time on an XEONTM MP 
3.6GHz machine). 
3.1 Parse re-ranking 
We follow the experimental paradigm of parse 
re-ranking outlined in Charniak and Johnson 
(2005), and fed the features extracted by their pro-
gram to the five rerankers we developed.  Each uses 
a linear model trained using one of the five esti-
mators. These rerankers attempt to select the best 
parse ?  for a sentence ?  from the 50-best list of 
possible parses ??? ?  for the sentence. The li-
near model combines the log probability calculated 
by the Charniak (2000) parser as a feature with 
1,219,272 additional features.  We trained the fea-
1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 
for d=1?D. 
2 Take a forward step according to Eq. (8) and (9), and 
the updated model is denoted by w1 
3 Initialize ? = (ExpLoss(w0)-ExpLoss(w1))/? 
4 Take a backward step if and only if it leads to a de-
crease of LassoLoss according to Eq. (10) and (11), 
where ?  = 0; otherwise 
5 Take a forward step according to Eq. (8) and (9); 
update ? = min(?, (ExpLoss(wt-1)-ExpLoss(wt))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
1 Set w0 = 1 and wd = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3    For each training sample (xi, yi), i = 1?N 
4 
?? = arg max
????? ?_? 
? ?? , ? ? ? 
Choose the best candidate zi from GEN(xi) using 
the current model w, 
5       w = w +  ?(?(xi, yi) ? ?(xi, zi)), where ? is the size of 
learning step, optimized on held-out data. 
Figure 3: The perceptron algorithm 
 
828
ture weights w on Sections 2-19 of the Penn Tree-
bank, adjusted the regularizer constant ? to max-
imize the F-Score on Sections 20-21 of the Tree-
bank, and evaluated the rerankers on Section 22.  
The results are presented in Tables 12 and 2, where 
Baseline results were obtained using the parser by 
Charniak (2000).  
The ME estimation with L2 regularization out-
performs all of the other estimators significantly 
except for the AP, which performs almost as well 
and requires an order of magnitude less time in 
training.  Boosting and BLasso are feature selection 
methods in nature, so they achieve the sparsest 
models, but at the cost of slightly lower perfor-
mance and much longer training time. The 
L1-regularized ME estimator also produces a rela-
tively sparse solution whereas the Averaged Per-
ceptron and the L2-regularized ME estimator assign 
almost all features a non-zero weight.  
3.2 Language model adaptation 
Our experiments with LM adaptation are based on 
the work described in Gao et al (2006). The va-
riously trained language models were evaluated 
according to their impact on Japanese text input 
accuracy, where input phonetic symbols ?  are 
mapped into a word string ?. Performance of the 
application is measured in terms of character error 
                                                     
2
 The result of ME/L2 is better than that reported in 
Andrew and Gao (2007) due to the use of the variant of 
L2-regularized ME estimator, as described in Section 2.1. 
 CER # features time (min) #train iter 
Baseline 10.24%    
MAP 7.98%    
ME/L2 6.99% 295,337 27 665 
ME/L1 7.01% 53,342 25 864 
AP 7.23% 167,591 6 56 
Boost 7.54% 32,994 175 71,000 
BLasso 7.20% 33,126 238 250,000 
Table 3. Performance summary of estimators 
(lower is better) on language model adaptation 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  ~ >> >> >> 
ME/L1 ~  >> >> >> 
AP << <<  >> ~ 
Boost << << <<  << 
BLasso << << ~ >>  
Table 4. Statistical significance test results. 
rate (CER), which is the number of characters 
wrongly converted from ? divided by the number of 
characters in the correct transcript. 
Again we evaluated five linear rerankers, one for 
each estimator. These rerankers attempt to select the 
best conversions ? for an input phonetic string ? 
from a 100-best list ???(?)of possible conver-
sions proposed by a baseline system. The linear 
model combines the log probability under a trigram 
language model as base feature and additional 
865,190 word uni/bi-gram features.  These 
uni/bi-gram features were already included in the 
trigram model which was trained on a background 
domain corpus (Nikkei Newspaper). But in the 
linear model their feature weights were trained 
discriminatively on an adaptation domain corpus 
(Encarta Encyclopedia). Thus, this forms a cross 
domain adaptation paradigm.  This also implies that 
the portion of redundant features in this task could 
be much larger than that in the parse re-ranking 
task, especially because the background domain is 
reasonably similar to the adaptation domain.  
We divided the Encarta corpus into three sets 
that do not overlap.  A 72K-sentences set was used 
as training data, a 5K-sentence set as development 
data, and another 5K-sentence set as testing data. 
The results are presented in Tables 3 and 4, where 
Baseline is the word-based trigram model trained 
on background domain corpus, and MAP (maxi-
mum a posteriori) is a traditional model adaptation 
method, where the parameters of the background 
model are adjusted so as to maximize the likelihood 
of the adaptation data.  
 F-Score # features time (min) # train iter 
Baseline 0.8986     
ME/L2 0.9176 1,211,026 62     129  
ME/L1 0.9165 19,121 37 174  
AP 0.9164 939,248 2 8  
Boosting 0.9131 6,714 495 92,600  
BLasso 0.9133 8,085 239 56,500  
Table 1: Performance summary of estimators on 
parsing re-ranking (ME/L2: ME with L2 regulari-
zation; ME/L1:  ME with L1 regularization) 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  >> ~ >> >> 
ME/L1 <<  ~ > ~ 
AP ~ ~  >> > 
Boost << < <<  ~ 
Blasso << ~ < ~  
Table 2: Statistical significance test results (?>>? 
or ?<<? means P-value < 0.01; > or < means 0.01 < 
P-value ? 0.05; ?~? means P-value > 0.05)  
829
The results are more or less similar to those in 
the parsing task with one visible difference: L1 
regularization achieved relatively better perfor-
mance in this task.  For example, while in the 
parsing task ME with L2 regularization significantly 
outperforms ME with L1 regularization, their per-
formance difference is not significant in this task. 
While in the parsing task the performance differ-
ence between BLasso and Boosting is not signifi-
cant, BLasso outperforms Boosting significantly in 
this task.  Considering that a much higher propor-
tion of the features are redundant in this task than 
the parsing task, the results seem to corroborate the 
observation that L1 regularization is robust to the 
presence of many redundant features. 
3.3 Chinese word segmentation 
Our third task is Chinese word segmentation 
(CWS). The goal of CWS is to determine the 
boundaries between words in a section of Chinese 
text.  The model we used is the hybrid Mar-
kov/semi- Markov CRF described by Andrew 
(2006), which was shown to have state-of-the-art 
accuracy. We tested models trained with the various 
estimation methods on the Microsoft Research Asia 
corpus from the Second International Chinese Word 
Segmentation, and we used the same train/test split 
used in the competition.  The model and experi-
mental setup is identical with that of Andrew (2006) 
except for two differences.  First, we extracted 
features from both positive and negative training 
examples, while Andrew (2006) uses only features 
that occur in some positive training example. 
Second, we used the last 4K sentences of the 
training data to select the weight of the regularizers 
and to determine when to stop perceptron training. 
We compared three of the best performing es-
timation procedures on this task: ME with L2 regu-
larization, ME with L1 regularization, and the Av-
eraged Perceptron.  In this case, ME refers to mi-
nimizing the negative log-probability of the correct 
segmentation, which is globally normalized, while 
the perceptron is trained using at each iteration the 
exact maximum-scoring segmentation with the 
current weights. We observed the same pattern as in 
the other tasks: the three algorithms have nearly 
identical performance, while L1 uses only 6% of the 
features, and the Averaged Perceptron requires 
significantly fewer training iterations.  In this case, 
L1 was also several times faster than L2. The results 
are summarized in Table 5.3 
We note that all three algorithms performed 
slightly better than the model used by Andrew 
(2006), which also used L2 regularization (96.84 
F1).  We believe the difference is due to the use of 
features derived from negative training examples. 
3.4 POS tagging 
Finally we studied the impact of the regularization 
methods on a Maximum Entropy conditional 
Markov Model (MEMM, McCallum et al 2000) for 
POS tagging. MEMMs decompose the conditional 
probability of a tag sequence given a word sequence 
as follows: 
? ?1 ? ??  ?1 ??? = ?(??|???1 ????? ,?1 ???)
?
?=1
 
where the probability distributions for each tag 
given its context are ME models.  Following pre-
vious work (Ratnaparkhi, 1996), we assume that the 
tag of a word is independent of the tags of all pre-
ceding words given the tags of the previous two 
words (i.e., ?=2 in the equation above). The local 
models at each position include features of the 
current word, the previous word, the next word, and 
features of the previous two tags.  In addition to 
lexical identity of the words, we used features of 
word suffixes, capitalization, and number/special 
character signatures of the words. 
We used the standard splits of the Penn Treebank 
from the tagging literature (Toutanova et al 2003) 
for training, development and test sets.  The training 
set comprises Sections 0-18, the development set ? 
Sections 19-21, and the test set ? Sections 22-24.  
We compared training the ME models using L1 and 
L2 regularization.  For each of the two types of 
regularization we selected the best value of the 
regularization constant using grid search to optim-
ize the accuracy on the development set.  We report 
final accuracy measures on the test set in Table 6.  
The results on this task confirm the trends we 
have seen so far.  There is almost no difference in 
                                                     
3 Only the L2 vs. AP comparison is significant at a 0.05 
level according to the Wilcoxon signed rank test. 
 Test F1 # features # train iter 
ME/L2 0.9719 8,084,086 713 
ME/L1 0.9713 317,146 201 
AP 0.9703 1,965,719 162 
Table 5. Performance summary of estimators on 
CWS 
 
830
accuracy of the two kinds of regularizations, and 
indeed the differences were not statistically signif-
icant.  Estimation with L1 regularization required 
considerably less time than estimation with L2, and 
resulted in a model which is more than ten times 
smaller.  
4 Conclusions 
We compared five of the most competitive para-
meter estimation methods on four NLP tasks em-
ploying a variety of models, and the results were 
remarkably consistent across tasks.  Three of the 
methods ? ME estimation with L2 regularization, 
ME estimation with L1 regularization, and the Av-
eraged Perceptron ? were nearly indistinguishable 
in terms of test set accuracy, with ME estimation 
with L2 regularization perhaps enjoying a slight 
lead.  Meanwhile, ME estimation with L1 regulari-
zation achieves the same level of performance while 
at the same time producing sparse models, and the 
Averaged Perceptron provides an excellent com-
promise of high performance and fast training. 
These results suggest that when deciding which 
type of parameter estimation to use on these or 
similar NLP tasks, one may choose any of these 
three popular methods and expect to achieve com-
parable performance.  The choice of which to im-
plement should come down to other considerations: 
if model sparsity is desired, choose ME estimation 
with L1 regularization (or feature selection methods 
such as BLasso); if quick implementation and 
training is necessary, use the Averaged Perceptron; 
and ME estimation with L2 regularization may be 
used if it is important to achieve the highest ob-
tainable level of performance. 
References 
Andrew, G. 2006. A hybrid Markov/semi-Markov condi-
tional random field for sequence segmentation. In EMNLP, 
465-472. 
Andrew, G. and Gao, J. 2007. Scalable training of 
L1-regularized log-linear models. In ICML. 
Charniak, E. 2000. A maximum-entropy-inspired parser. In 
NAACL, 132-139. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best 
parsing and MaxEnt discriminative re-ranking. In ACL. 
173-180. 
Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing 
techniques for ME models. IEEE Trans. On Speech and Audio 
Processing, 8(2): 37-50. 
Collins, M. 2000. Discriminative re-ranking for natural 
language parsing. In ICML, 175-182. 
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, 1-8. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. In 
ICML?98.  
Freund, Y. and Schapire, R. E. 1999. Large margin classifica-
tion using the perceptron algorithm. In Machine Learning, 
37(3): 277-296. 
Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of 
statistical learning. Springer-Verlag, New York. 
Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso 
methods for language modeling. In ACL. 
Goodman, J. 2004. Exponential priors for maximum entropy 
models. In NAACL. 
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 
1999. Estimators for stochastic ?Unification-based? 
grammars. In ACL. 
Kazama, J. and Tsujii, J. 2003. Evaluation and extension of 
maximum entropy models with inequality constraints. In 
EMNLP. 
Malouf, R. 2002. A comparison of algorithms for maximum 
entropy parameter estimation. In HLT. 
McCallum A, D. Freitag and F. Pereira. 2000. Maximum 
entropy markov models for information extraction and 
segmentation. In ICML. 
Mitchell, T. M. 1997. Machine learning. The McGraw-Hill 
Companies, Inc. 
Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, 
and rotational invariance. In ICML. 
Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. 
Springer, New York. 
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. 
Learning and inference over constrained output. In IJCAI. 
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech 
tagger. In EMNLP. 
Riezler, S., and Vasserman, A. 2004. Incremental feature 
selection and L1 regularization for relax maximum entro-
py modeling. In EMNLP.  
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., 
and Johnson, M. 2002. Parsing the wall street journal using 
a lexical-functional grammar and discriminative estima-
tion techniques. In ACL. 271-278.  
Tibshirani, R. 1996. Regression shrinkage and selection via 
the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 
2003. Feature-rich Part-of-Speech tagging with a cyclic 
dependency network. In HLT-NAACL, 252-259. 
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics 
Department, U. C. Berkeley. 
 Accuracy (%) # features # train iter 
MEMM/L2 96.39 926,350 467 
MEMM/L1 96.41 84,070 85 
Table 6. Performance summary of estimators on 
POS tagging 
831
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 528?536,
Beijing, August 2010
Unsupervised phonemic Chinese word segmentation using Adaptor
Grammars
Mark Johnson
Department of Computing
Macquarie University
Mark.Johnson@mq.edu.au
Katherine Demuth
Department of Linguistics
Macquarie University
Katherine.Demuth@mq.edu.au
Abstract
Adaptor grammars are a framework for
expressing and performing inference over
a variety of non-parametric linguistic
models. These models currently provide
state-of-the-art performance on unsuper-
vised word segmentation from phonemic
representations of child-directed unseg-
mented English utterances. This paper in-
vestigates the applicability of these mod-
els to unsupervised word segmentation of
Mandarin. We investigate a wide vari-
ety of different segmentation models, and
show that the best segmentation accuracy
is obtained frommodels that capture inter-
word ?collocational? dependencies. Sur-
prisingly, enhancing the models to exploit
syllable structure regularities and to cap-
ture tone information does improve over-
all word segmentation accuracy, perhaps
because the information these elements
convey is redundant when compared to the
inter-word dependencies.
1 Introduction and previous work
The word-segmentation task is an abstraction of
part of the problem facing a child learning its na-
tive language. Fluent speech, even the speech di-
rected at children, doesn?t come with silence or
pauses delineating acoustic words the way that
spaces separate orthographic words in writing sys-
tems like that of English. Instead, as most people
listening to a language they don?t understand can
attest, words in fluent speech ?run together?, and a
language user needs to learn how to segment utter-
ances of the language they are learning into words.
This kind of word segmentation is presumably an
important first step in acquiring a language. It is
scientifically interesting to know what informa-
tion might be useful for word segmentation, and
just how this information might be used. These
scientific questions have motivated a body of re-
search on computational models of word segmen-
tation. Since as far as we can tell any child can
learn any human language, our goal is to develop
a single model that can learn to perform accurate
word segmentation given input from any human
language, rather than a model that specialised to
perform well on a single language. This paper
extends the previous work on word segmentation
by investigating whether one class of models that
work very well with English input also work with
Chinese input. These models will permit us to
study the role that syllable structure constraints
and tone in Chinese might play in word segmenta-
tion.
While learners and fluent speakers undoubt-
edly use a wide variety of cues to perform word
segmentation, computational models since El-
man (1990) have tended to focus on the use
of phonotactic constraints (e.g., syllable-structure
constrains) and distributional information. Brent
and Cartwright (1996) introduced the standard
form of theword segmentation task still studied to-
day. They extracted the orthographic representa-
tions of child-directed speech from the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987) and ?pho-
nologised? them by looking up each word in a
pronouncing dictionary. For example, the or-
thographic utterance you want to see the book
is mapped to the sequence of pronunciations yu
want tu si D6 bUk, (the pronunciations are in an
528
ASCII encoding of the International Phonetic Al-
phabet representation of English phonemes). The
input to the learner is obtained by concatenating
together the phonemic representations of each ut-
terance?s words. The learner?s task is to identify
the locations of the word boundaries in this se-
quence, and hence identify the words (up to ho-
mophony). Brent and Cartwright (1996) pointed
out the importance of both distributional informa-
tion and phonotactic (e.g., syllable-structure) con-
straints for word segmentation (see also Swingley
(2005) and Fleck (2008)).
Recently there has been considerable interest in
applying Bayesian inference techniques for non-
parametric models to this problem. Here the term
?non-parametric? does not mean that the models
have no parameters, rather, it is used to distinguish
these models from the usual ?parametric models?
that have a fixed finite vector of parameters.
Goldwater et al (2006) introduced two non-
parametric Bayesian models of word segmenta-
tion, which are discussed in more detail in (Gold-
water et al, 2009). The unigram model, which as-
sumes that each word is generated independently
to form a sentence, turned out to be equivalent
to a model originally proposed by Brent (1999).
The bigram model improves word segmentation
accuracy by modelling bigram inter-word contex-
tual dependencies, ?explaining away? inter-word
dependencies that would otherwise cause the uni-
gram model to under-segment. Mochihashi et al
(2009) showed that segmentation accuracy could
be improved by using a more sophisticated ?base
distribution? and a dynamic programming sam-
pling algorithm very similar to the one used with
the adaptor grammars below. They also applied
their algorithm to Japanese and Chinese word seg-
mentation, albeit from orthographic rather than
phonemic forms, so unfortunately their results are
not comparable with ours.
Johnson et al (2007) introduced adaptor gram-
mars as a grammar-based framework for express-
ing a variety of non-parametric models, and pro-
vided a dynamic programming Markov Chain
Monte Carlo (MCMC) sampling algorithm for
performing Bayesian inference on these models.
For example, the unigram model can be expressed
as a simple adaptor grammar as shown below, and
the generic adaptor grammar inference procedure
provides a dynamic programming sampling algo-
rithm for this model. Johnson (2008b) showed
how a variety of different word segmentation
models can be expressed as adaptor grammars, and
Johnson and Goldwater (2009) described a num-
ber of extensions and specialisations to the adaptor
grammar framework that improve inference speed
and accuracy (we use these techniques in our work
below).
Previous work on unsupervised word segmen-
tation from phonemic input has tended to concen-
trate on English. However, presumably children
the world over segment their first language input
in the same (innately-specified) way, so a correct
procedure should work for all possible human lan-
guages. However, as far as we are aware there has
been relatively little work on word segmentation
from phonemic input except on English. Johnson
(2008a) investigated whether the adaptor gram-
mars models that do very well on English also ap-
ply to Sesotho (a Bantu language spoken in south-
ern Africa with rich agglutinating morphology).
He found that the models in general do very poorly
(presumably because the adaptor grammars used
cannot model the complex morphology found in
Sesotho) and that the best segmentation accuracy
was considerably worse than that obtained for En-
glish, even when that model incorporated some
Bantu-specific information about morphology. Of
course it may also be that the Sesotho and English
corpora are not really comparable: the Bernstein-
Ratner corpus that Brent and other researchers
have used for English was spoken to pre-linguistic
1-year olds, whilemost non-English corpora are of
child-directed speech to older children who are ca-
pable of talking back, and hence these corpora are
presumably more complex. We discuss this issue
in more detail in section 4 below.
2 A Chinese word segmentation corpus
Our goal here is to prepare a Chinese corpus of
child-directed speech that parallels the English
one used by Brent and other researchers. That
corpus was in broad phonemic form, obtained by
looking each word up in a pronouncing dictio-
nary. Here instead we make use of a corpus in
Pinyin format, which we translate into a broad
529
phonemic IPA format using the freely-available
Pinyin-to-IPA translation program ?Pinyin to
IPA Conversion Tools? version 2.1 available on
http://sourceforge.net/projects/py2ipa.
We used the ?Beijing? corpus (Tardif, 1993)
available from the publicly-distributed Childes
collection of corpora (MacWhinney and Snow,
1985). We are interested in child-directed speech
(rather than children?s speech), so we removed all
utterances from participants with an Id containing
?Child?. (Tardif (1993) points out that Chinese-
speaking children typically have a much richer
social environment involving multiple adult care-
givers than middle-class English-speaking chil-
dren do, so we cannot simply collect only the
mother?s utterances, as was done for the English
corpus). We also ignored all utterances with codes
$INTERJ, $UNINT, $VOC and $PRMPT, as these are
not always linguistic utterances. In addition, we
deleted all words that could not be analysed as a
sequence of syllables, such as ?xxx? and ?hmm?,
and also deleted ?cluck?. The first few utterances
of the corpus in Pinyin format are:
zen3me gei3 ta1 bei1 shang4 lai2 (1.) ?
ta1: (.) a1yi2 gei3 de (.) ta1 gei3 de .
hen3 jian3dan1 .
We then fed these into the Pinyin-to-IPA trans-
lation program, producing output of the following
format:
ts?n214m? kei214 t?a55 pei55 ???51 lai35
t?a55 a55i35 kei214 t? t?a55 kei214 t?
x?n214 t?i?n214tan55
In the IPA format, the superscript indices in-
dicate the tone patterns associated with syllables;
these appear at the end of each syllable, as is stan-
dard. While we believe there are good linguistic
reasons to analyse tones as associated with syl-
lables, we moved all the tones so they immedi-
ately followed the final vowel in each syllable.
We did this because we thought that locating tones
after the syllable-final consonant might give our
models a strong cue as to the location of sylla-
ble boundaries, and since words often end at syl-
lable boundaries, this would make the word seg-
mentation problem artificially easier. (Our models
take a sequence of symbols as input, so the tones
must be located somewhere in the sequence. How-
ever, the linguistically ?correct? solution would
probably be to extend the models so they could
process input in an auto-segmental format (Gold-
smith, 1990) where tones would be on a separate
tier and unordered with respect to the segments
within a syllable.)
In order to evaluate the importance of tone
for our word-segmentation models we also con-
structed a version of our corpus in which all tones
were removed. We present results for all of our
models on two versions of the corpus, one that
contains tones following the vowels, and another
that contains no tones at all. These two cor-
pora constitute the ?gold standard? against which
our word segmentation models will be evaluated.
These corpora contain 50,118 utterances, consist-
ing of 187,533 word tokens.
The training data provided to the word segmen-
tation models is obtained by segmenting the gold
data at all possible boundary locations. Conso-
nant clusters, diphthongs and tones (if present) are
treated as single units, so the training data appears
as follows:
ts ? 214 n m ? k e i 214 t? a 55 p e i 55 ? ? 51 ? l ai 35
t? a 55 a 55 i 35 k e i 214 t ? t? a 55 k e i 214 t ?
x ? 214 n t? i? 214 n t a 55 n
The task of a word-segmentation model is
to identify which of these possible bound-
ary locations correspond to actual word bound-
aries. The training corpus without tones contains
531,384 segments, while the training corpus with
tones contains 712,318 segments.
3 Adaptor grammars for word
segmentation
Adaptor grammars were first introduced by John-
son et al (2007) as a grammar-based frame-
work for specifying hierarchical non-parametric
Bayesian models, and Johnson and Goldwater
(2009) describes a number of implementation de-
tails that significantly improve performance; the
interested reader should consult those papers for a
full technical introduction. Johnson (2008b) pro-
posed a number of adaptor grammars for English
word segmentation, which we review and mini-
mally modify here so they can perform Chinese
530
word segmentation as well. In section 4 we evalu-
ate these adaptor grammars on the Chinese corpus
just described.
The grammars vary along two orthogonal di-
mensions, which correspond to the kinds of gen-
eralisations that the model can learn. The sim-
plest grammar is the unigram adaptor grammar,
which generates an utterance as an i.i.d. sequences
of words, where each word is a sequence of
phonemes. The collocation adaptor grammars
capture dependencies above the word level by
generating collocations, or groups of words, as
memoized units. The syllable adaptor grammars
capture dependencies below the word level by
generating words as sequences of syllables rather
than phonemes.
3.1 Unigram adaptor grammars
In order to motivate adaptor grammars as an ex-
tension to Probabilistic Context-Free Grammars
(PCFGs), consider an attempt to perform unsuper-
vised word segmentation with a PCFG containing
the following rules (ignore the underlining of the
Word non-terminal for now).
Words?Words Word
Words?Word
Word? Phons
Phons? Phon
Phons? Phons Phon
Phons? Phons Tone
Phon? ai | o | ? | ? | t?? | ?
Tone? 35 | 55 | 214 | ?
(1)
In this grammar, Phon expands to all the
phonemes appearing in the phonemic training
data, and Tone expands to all of the tone patterns.
(In this and all of the other grammars in this paper,
the start symbol is the non-terminal symbol of the
first rule in the grammar. This grammar, like all
others in this paper, is crafted so that aWord sub-
tree can never begin with a Tone, so the presence
of tones does not make the segmentation problem
harder).
The trees generated by this grammar are suffi-
ciently expressive to represent any possible seg-
mentation of any sequence of phonemes into
words (including the true segmentation); a typi-
cal segmentation is shown in Figure 1. However,
Words
Words
Word
Phons
Phons
Phons
Phon
p
Phon
u
Tone
35
Word
Phons
Phons
Phons
Phons
Phon
k?
Phon
a
Tone
51
Phon
n
Figure 1: A parse tree generated by the unigram
grammar, where adapted and non-adapted non-
terminals are shown. It depicts a possible segmen-
tation of p u 35 k? a 51 n.
it should also be clear that no matter how we vary
the probabilities on the rules of this grammar, the
grammar itself cannot encode the subset of trees
that correspond to words of the language. In or-
der to do this, a model would need to memorise the
probabilities of entire Word subtrees, since these
are the units that correspond to individual words,
but this PCFG simply is not expressive enough to
do this.
Adaptor grammars learn the probabilities of
subtrees in just this way. An adaptor grammar is
specified via a set of rules or productions, just like
a CFG, and the set of trees that an adaptor gram-
mar generates is exactly the same as the CFG with
those rules.
However, an adaptor grammar defines proba-
bility distributions over trees in a completely dif-
ferent fashion to a PCFG: for simplicity we fo-
cus here on the sampling or predictive distribu-
tion, which defines the probability of generating
an entire corpus of trees. In a PCFG, the prob-
ability of each non-terminal expanding using a
given rule is determined by the probability of that
rule, and is independent of the expansions of the
other non-terminals in the tree. In an adaptor
grammar a subset of the non-terminals are des-
531
ignated as adapted. We indicate adapted non-
terminals by underlining them, so Word is the
only adapted non-terminal in (1). Unadapted non-
terminals expand just as in a PCFG: a produc-
tion is chosen according to the production prob-
abilities. An adapted non-terminal can expand
in two different ways. With probability propor-
tional to n(t)? aA an adapted non-terminal A ex-
pands to a tree t rooted in A that has been pre-
viously generated, while with probability propor-
tional to m(A)aA + bA the adapted non-terminal
A expands using some grammar rule, just as in a
PCFG. Here n(t) is the number of times tree t has
been previously generated,m(A) is the number of
trees rooted in A that have been previously gener-
ated using grammar rules, and 0 ? aA ? 1 and
bA > 0 are adjustable parameters associated with
the adapted non-terminal A.
Technically, this is known as a Pitman-Yor Pro-
cess (PYP) with concentration parameters aA and
bA, where the PCFG rules define the base distri-
bution of the process. (The PYP is a generalisa-
tion of the Chinese Restaurant Process (CRP); a
CRP is a PYP with parameter a = 0). Rather
than setting the concentration parameters by hand
(there are two for each adapted non-terminal in
the grammar) we follow Johnson and Goldwater
(2009) and put uniform Beta and vague Gamma
priors on each of these parameters, and use sam-
pling to explore their posterior values.
Because the probability of selecting a tree t is
proportional to n(t), an adaptor grammar is a kind
of ?rich-get-richer? process that generates power-
law distributions. Depending on the values of aA
and bA, most of the probability mass can wind
up concentrated on just a few trees. An adaptor
grammar is a kind of ?cache? model, in which
previously generated subtrees are stored and more
likely to be reused in later sentences. That is, while
an adapted non-terminal A can expand to any tree
rooted inA that can be constructed with the gram-
mar rules, in practice it is increasingly likely to
reuse the same trees over and over again. It can
be viewed as a kind of tree substitution grammar
(Joshi, 2003), but where the tree fragments (as
well as their probabilities) are learnt from the data.
The unigram grammar is the simplest of the
word segmentation models we investigate in this
paper (it is equivalent to the unigram model inves-
tigated in Goldwater et al (2009)). Because the
grammars we present below rapidly become long
and complicated to read if each grammar rule is
explicitly stated, we adopt the following conven-
tions. We use regular expressions to abbreviate
our grammars, with the understanding that the reg-
ular expressions are always expanded produce a
left-recursive structure. For example, the unigram
grammar in (1) is abbreviated as:
Words?Word+
Word? Phon (Phon | Tone)?
Phon? ai | o | ? | ? | t?? | ?
Tone? 35 | 55 | 214 | ?
(2)
3.2 Collocation adaptor grammars
Goldwater et al (2006) and Goldwater et al
(2009) demonstrated the importance of contex-
tual dependencies for word segmentation, and pro-
posed a bigram model in order to capture some
of these. It turns out that while the bigram model
cannot be expressed as an adaptor grammar, a col-
location model, which captures similar kinds of
contextual dependencies, can be expressed as an
adaptor grammar (Johnson et al, 2007). In a col-
location grammar there are two different adapted
non-terminals; Word and Colloc; Word expands
exactly as in the unigram grammar (2), so it is not
repeated here.
Collocs? Colloc+
Colloc?Words
Words?Word+
(3)
A collocation adaptor grammar caches both
words and collocations (which are sequences of
words). An utterance is generated by generating
one or more collocations. The PYP associated
with collocations either regenerates a previously
generated collocation or else generates a ?fresh?
collocation by generating a sequence of words ac-
cording to the PYP model explained above.
The idea of aggregating words into collocations
can be reapplied at a more abstract level by ag-
gregating collocations into ?super-collocations?,
which are sequences of collocations. This in-
volves adding the following additional rules to the
grammar in (3):
532
Colloc2s? Colloc2+
Colloc2? Collocs+ (4)
There are three PYPs in a grammar with 2 lev-
els of collocations, arranged in a strict Bayesian
hierarchy. It should be clear that this process can
be repeated indefinitely; we investigate grammars
with up to three levels of collocations below. (It
should be possible to use Bayesian techniques to
learn the appropriate number of levels in the hier-
archy, but we leave this for future work).
3.3 Syllable structure adaptor grammars
Brent and Cartwright (1996) and others emphasise
the role that syllable-structure and other phono-
tactic constraints might play in word segmenta-
tion. Johnson (2008b) pointed out that adaptor
grammars can learn at least some of these kinds
of generalisations. It?s not unreasonable to as-
sume that language learners can learn to group
phonemes into syllables, and that they can exploit
this syllabic structure to perform word segmenta-
tion. The syllable-structure grammars we describe
below assume that word boundaries are always
aligned with syllable boundaries; this is not uni-
versally true, but it is reliable enough to dramati-
cally improve unsupervised word segmentation in
English.
There is considerable cross-linguistic varia-
tion in the syllable-structure and phonotactic con-
straints operative in the languages of the world, so
we?d like to avoid ?building in? language-specific
constraints into our model. We therefore make the
relatively conservative assumption that the child
can distinguish vowels from consonants, and that
the child knows that syllables consist of Onsets,
Nuclei and Codas, that Onsets and Codas consist
of arbitrary sequences of consonants while Nuclei
are arbitrary sequences of vowels and tones, and
that Onsets and Codas are optional. Notice that
syllable structure in both English and Chinese is
considerably more constrained than this; we use
this simple model here because it has proved suc-
cessful for English word segmentation.
The syllable-structure adaptor grammars re-
place the rules expanding Words with the follow-
ing rules:
Word? Syll
Word? Syll Syll
Word? Syll Syll Syll
Word? Syll Syll Syll Syll
Syll? (Onset)? Rhy
Onset? C+
Rhy? Nucleus (Coda)?
Nucleus? V (V | Tone)?
Coda? C+
C? ? | t?? | ?
V? ai | o | ?
(5)
In these rules the superscript ??? indicates op-
tionality. We used the relatively cumbersome
mechanism of enumerating each possible number
of syllables per word (we permit words to consist
of from 1 to 4 syllables, although ideally this num-
ber would not be hard-wired into the grammar)
because a relatively trivial modification of this
grammar can distinguish word-initial and word-
final consonant clusters from word-internal clus-
ters. Johnson (2008b) demonstrated that this sig-
nificantly improves English word segmentation
accuracy. We do not expect this to improve Chi-
nese word segmentation because Chinese clusters
do not vary depending on their location within the
word, but it will be interesting to see if the addi-
tional cluster flexibility that is useful for English
segmentation hurts Chinese segmentation.
In this version of the syllable-structure gram-
mar, we replace the Word rules in the syllable
adaptor grammar with the following:
Word? SyllIF
Word? SyllI SyllF
Word? SyllI Syll SyllF
Word? SyllI Syll Syll SyllF
(6)
and add the following rules expanding the new
kinds of syllables to the rules in (5).
SyllIF? (OnsetI)? RhyF
SyllI? (OnsetI)? Rhy
SyllF? (OnsetI)? RhyF
Syll? (Onset)? Rhy
OnsetI? C+
RhyF? Nucleus (CodaF)?
CodaF? C+
(7)
533
Syllables
None General Specialised
Unigram 0.57 0.50 0.50
Colloc 0.69 0.67 0.67
Colloc2 0.72 0.75 0.75
Colloc3 0.64 0.77 0.77
Table 1: F-score accuracies of word segmenta-
tions produced by the adaptor grammar models on
the Chinese corpus with tones.
Syllables
None General Specialised
Unigram 0.56 0.46 0.46
Colloc 0.70 0.65 0.65
Colloc2 0.74 0.74 0.73
Colloc3 0.75 0.76 0.77
Table 2: F-score accuracies of word segmenta-
tions produced by the adaptor grammar models on
the Chinese corpus without tones.
These rules distinguish syllable onsets in word-
initial position and syllable codas in word-final
position; the standard adaptor grammarmachinery
will then learn distributions over onsets and codas
in these positions that possibly differ from those
in word-internal positions.
4 Results on Chinese word segmentation
The previous section described two dimensions
along which adaptor grammars for word segmen-
tation can independently vary. Above the Word
level, there can be from zero to three levels of col-
locations, yielding four different values for this di-
mension. Below theWord level, phonemes can ei-
ther be treated as independent entities, or else they
can be grouped into onset, nuclei and coda clus-
ters, and these can vary depending on where they
appear within a word. Thus there are three dif-
ferent values for the syllable dimension, so there
are twelve different adaptor grammars overall. In
addition, we ran all of these grammars on two ver-
sions of the corpus, one with tones and one with-
out tones, so we report results for 24 different runs
here.
The adaptor grammar inference procedure we
used is the one described in Johnson and Goldwa-
ter (2009). We ran 1,000 iterations of 8 MCMC
chains for each run, and we discarded all but last
200 iterations in order to ?burn-in? the sampler.
The segmentation we predict is the one that occurs
the most frequently in the samples that were not
discarded. As is standard, we evaluate the models
in terms of token f-score; the results are presented
in Tables 1 and 2.
In these tables, ?None? indicates that the gram-
mar does not model syllable structure, ?Gen-
eral? indicates that the grammar does not distin-
guish word-peripheral from word-internal clus-
ters, while ?Specialised? indicates that it does.
?Unigram? indicates that the grammar does not
model collocational structure, otherwise the super-
script indicates the number of collocational levels
that the grammar captures.
Broadly speaking, the results are consistent with
the English word segmentation results using adap-
tor grammars presented by Johnson (2008b). The
unigram grammar segmentation accuracy is simi-
lar to that obtained for English, but the results for
the other models are lower than the results for the
corresponding adaptor grammars on English.
We see a general improvement in segmenta-
tion accuracy as the number of collocation levels
increases, just as for English. However, we do
not see any general improvements associated with
modelling syllables; indeed, it seems modelling
syllables causes accuracy to decrease unless collo-
cational structure is also modelled. This is some-
what surprising, as Chinese has a very regular syl-
labic structure. It is not surprising that distin-
guishing word-peripheral and word-medial clus-
ters does not improve segmentation accuracy, as
Chinese does not distinguish these kinds of clus-
ters. There is also no sign of the ?synergies? when
modelling collocations and syllables together that
Johnson (2008b) reported.
It is also surprising that tones seem to make lit-
tle difference to the segmentation accuracy, since
they are crucial for disambiguating lexical items.
The segmentation accuracy of the models that cap-
ture little or no inter-word dependencies (e.g., Un-
igram, Colloc) improved slightly when the input
contains tones, but the best-performing models
that capture a more complex set of inter-word de-
534
pendencies do equally well on the corpus without
tones as they do on the corpus with tones. Because
these models capture rich inter-word context (they
model three levels of collocational structure), it is
possible that this context provides sufficient infor-
mation to segment words even in the absence of
tone information, i.e., the tonal information is re-
dundant given the richer inter-word dependencies
that these models capture. It is also possible that
word segmentation may simply require less infor-
mation than lexical disambiguation.
One surprising result is the relatively poor per-
formance of the Colloc3 model without syllables
but with tones; we have no explanation for this.
However, all 8 of the MCMC chains in this run
produced lower f-scores, so it unlikely to be sim-
ply a random fluctuation produced by a single out-
lier.
Note that one should be cautious when compar-
ing the absolute f-scores from these experiments
with those of the English study, as the English and
Chinese corpora differ in many ways. As Tardif
(1993) (the creator of the Chinese corpus) empha-
sises, this corpus was collected in a much more
diverse linguistic environment with child-directed
speech from multiple caregivers. The children in-
volved in the Chinese corpus were also older than
the children in the English corpus, which may also
have affected the nature of the corpus.
5 Conclusion
This paper applied adaptor grammar models of
phonemic word segmentation originally devel-
oped for English to Chinese data. While the Chi-
nese data was prepared in a very different way
to the English data, the adaptor grammars used
to perform Chinese word segmentation were very
similar to those used for the English word seg-
mentation. They also achieved quite respectable
f-score accuracies, which suggests that the same
models can do well on both languages.
One puzzling result is that incorporating syl-
lable structure phonotactic constraints, which en-
hances English word segmentation accuracy con-
siderably, doesn?t seem to improve Chinese word
segmentation to a similar extent. This may reflect
the fact that the word segmentation adaptor gram-
mars were originally designed and tuned for En-
glish, and perhaps differently formulated syllable-
structure constraints would work well for Chinese.
But even if one can ?tune? the adaptor grammars
to improve performance on Chinese, the challenge
is doing this in a way that improves performance
on all languages, rather than just one.
Acknowledgments
The authors would like to thank the US NSF for
support for this research, which was begun while
they were on the faculty at Brown University in
the USA. The NSF supported this work through
NSF awards 0544127 and 0631667 to Mark John-
son and Katherine Demuth.
The adaptor grammar software is
freely available for download from
http://web.science.mq.edu.au/?mjohnson, and
the Chinese data was obtained from the Childes
archive.
References
Bernstein-Ratner, N. 1987. The phonology of parent-
child speech. In Nelson, K. and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Brent, M. and T. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61:93?125.
Brent, M. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Elman, Jeffrey. 1990. Finding structure in time. Cog-
nitive Science, 14:197?211.
Fleck, Margaret M. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Goldsmith, John A. 1990. Autosegmental and Metri-
cal Phonology. Basil Blackwell, Oxford, England.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia. Association for Computational
Linguistics.
535
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21 ? 54.
Johnson, Mark and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North AmericanChapter of the Association for
Computational Linguistics, pages 317?325, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Johnson, Mark, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In Scho?lkopf, B., J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Johnson, Mark. 2008a. Unsupervised word seg-
mentation for Sesotho using adaptor grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Johnson, Mark. 2008b. Using adaptor grammars to
identifying synergies in the unsupervised acquisition
of linguistic structure. In Proceedings of the 46th
AnnualMeeting of the Association of Computational
Linguistics, Columbus, Ohio. Association for Com-
putational Linguistics.
Joshi, Aravind. 2003. Tree adjoining grammars. In
Mikkov, Ruslan, editor, The Oxford Handbook of
Computational Linguistics, pages 483?501. Oxford
University Press, Oxford, England.
MacWhinney, Brian and Catherine Snow. 1985. The
child language data exchange system. Journal of
Child Language, 12:271?296.
Mochihashi, Daichi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
AnnualMeeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 100?108, Suntec, Singapore,
August. Association for Computational Linguistics.
Swingley, Dan. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
Tardif, Twila. 1993. Adult-to-child speech and lan-
guage acquisition in Mandarin Chinese. Ph.D. the-
sis, Yale University.
536
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371?1378,
Beijing, August 2010
Detecting Speech Repairs Incrementally
Using a Noisy Channel Approach
Simon Zwarts, Mark Johnson and Robert Dale
Centre for Language Technology
Macquarie University
{simon.zwarts|mark.johnson|robert.dale}@mq.edu.au
Abstract
Unrehearsed spoken language often
contains disfluencies. In order to cor-
rectly interpret a spoken utterance,
any such disfluencies must be identi-
fied and removed or otherwise dealt
with. Operating on transcripts of
speech which contain disfluencies, our
particular focus here is the identifica-
tion and correction of speech repairs
using a noisy channel model. Our aim
is to develop a high-accuracy mecha-
nism that can identify speech repairs
in an incremental fashion, as the ut-
terance is processed word-by-word.
We also address the issue of the evalu-
ation of such incremental systems. We
propose a novel approach to evalua-
tion, which evaluates performance in
detecting and correcting disfluencies
incrementally, rather than only assess-
ing performance once the processing of
an utterance is complete. This demon-
strates some shortcomings in our ba-
sic incremental model, and so we then
demonstrate a technique that improves
performance on the detection of disflu-
encies as they happen.
1 Introduction
One of the most obvious differences between
written language and spoken language is the
fact that the latter presents itself incremen-
tally over some time period. Most natural lan-
guage processing applications operate on com-
plete sentences; but for real time spontaneous
speech, there are potential benefits to incre-
mentally processing the input so that a system
can stay responsive and interact directly be-
fore a speaker?s utterance is complete. Work
in psycholinguistics supports the view that the
human parsing mechanism works incremen-
tally, with partial semantic interpretations be-
ing produced before the complete utterance
has been heard (Marslen-Wilson, 1973). Our
interest is in developing similarly incremental
processing techniques for natural language in-
terpretation, so that, for example, a speech
recognizer might be able to interject during
a long utterance to object, cut the speaker
short, or correct a mistaken assumption; such
a mechanism is even required for the appro-
priate timing of backchannel signals. Addi-
tionally the incremental nature of the model
allows potential application of this model in
speech recognition models.
Another feature of unrehearsed spoken lan-
guage that has no obvious correlate in written
language is the presence of disfluencies.1 Dis-
fluencies are of different types, ranging from
simple filled pauses (such as um and uh) to
more complicated structures where the se-
quence of words that make up the utterance is
?repaired? while it is being produced. Whereas
simpler disfluencies may be handled by sim-
ply deleting them from the sequence of words
under consideration, the editing terms in a
speech repair are part of the utterance, and
therefore require more sophisticated process-
ing.
There are three innovations in the present
paper. First, we demonstrate that a noisy
channel model of speech repairs can work ac-
curately in an incremental fashion. Second,
we provide an approach to the evaluation of
1Although some disfluencies can be considered
grammatical errors, they are generally quite distinct
in both cause and nature from the kinds of grammat-
ical errors found in written text.
1371
such an incremental model. Third, we tackle
the problem of the early detection of speech
repairs, and demonstrate a technique that de-
creases the latency (as measured in tokens)
involved in spotting that a disfluency has oc-
curred.
The rest of the paper is structured as fol-
lows. Section 2 provides some background
on speech repairs and existing approaches to
handling them, including Johnson and Char-
niak?s (2004) model, which we use as a start-
ing point for our incremental model. Section
3 describes our model in detail, focusing on
the noisy channel model and the incremental
component of this model. Section 4 introduces
some considerations that arise in the develop-
ment of techniques for the evaluation of in-
cremental disfluency detection; we then pro-
vide a quantitative assessment of our perfor-
mance using these techniques. Our evaluation
reveals that our basic incremental model does
not perform very well at detecting disfluencies
close to where they happen, so in Section 5 we
present a novel approach to optimise detection
of these disfluencies as early as possible. Fi-
nally Section 6 concludes and discusses future
work.
2 Speech Repairs
We adopt the terminology and definitions in-
troduced by Shriberg (1994) to discuss disflu-
ency. We are particularly interested in what
are called repairs. These are the hardest
types of disfluency to identify since they are
not marked by a characteristic vocabulary.
Shriberg (1994) identifies and defines three
distinct parts of a repair, referred to as the
reparandum, the interregnum and the re-
pair. Consider the following utterance:
I want a flight
reparandum? ?? ?
to Boston,
uh, I mean
? ?? ?
interregnum
to Denver
? ?? ?
repair
on Friday (1)
The reparandum to Boston is the part of the
utterance that is being edited out; the inter-
regnum uh is a filler, which may not always be
present; and the repair to Denver replaces the
reparandum.
Given an utterance that contains such a re-
pair, we want to be able to correctly detect
the start and end positions of each of these
three components. We can think of each word
in an utterance as belonging to one of four
categories: fluent material, reparandum, in-
terregnum, or repair. We can then assess the
accuracy of techniques that attempt to detect
disfluencies by computing precision and recall
values for the assignment of the correct cate-
gories to each of the words in the utterance,
as compared to the gold standard as indicated
by annotations in the corpus.
An alternative means of evaluation would
be to simply generate a new signal with the
reparandum and filler removed, and compare
this against a ?cleaned-up? version of the ut-
terance; however, Core and Schubert (1999)
argue that, especially in the case of speech
repairs, it is important not to simply throw
away the disfluent elements of an utterance,
since they can carry meaning that needs to
be recovered for proper interpretation of the
utterance. We are therefore interested in the
first instance in a model of speech error detec-
tion, rather than a model of correction.
Johnson and Charniak (2004) describe such
a model, using a noisy-channel based approach
to the detection of the start and end points of
reparanda, interregna and repairs. Since we
use this model as our starting point, we pro-
vide a more detailed explanation in Section 3.
The idea of using a noisy channel model
to identify speech repairs has been explored
for languages other than English. Honal and
Schultz (2003) use such a model, compar-
ing speech disfluency detection in spontaneous
spoken Mandarin against that in English. The
approach performs well in Mandarin, although
better still in English.
Both the models just described operate on
transcripts of completed utterances. Ideally,
however, when we deal with speech we would
like to process the input word by word as it is
received. Being able to do this would enable
tighter integration in both speech recognition
1372
and interpretation, which might in turn im-
prove overall accuracy.
The requirement for incrementality is recog-
nised by Schuler et al (2010), who employ
an incremental Hierarchical Hidden Markov
Model (HHMM) to detect speech disfluen-
cies. The HHMM is trained on manually an-
notated parse trees which are transformed by
a right corner transformation; the HHMM is
then used in an incremental fashion on un-
seen data, growing the parse structure each
time a new token comes in. Special subtrees
in this parse can carry a marker indicating
that the span of the subtree consists of tokens
corresponding to a speech disfluency. Schuler
et al?s approach thus provides scope for de-
tecting disfluencies in an incremental fashion.
However, their reported accuracy scores are
not as good as those of Johnson and Char-
niak (2004): they report an F-score of 0.690
for their HHMM+RCT model, as compared
to 0.797 for Johnson and Charniak?s parser
model.
Our aim in this paper, then, is to investigate
whether it is possible to adapt Johnson and
Charniak?s model to process utterances incre-
mentally, without any loss of accuracy. To
define the incremental component more pre-
cisely, we investigate the possibility of mark-
ing the disfluencies as soon as possible during
the processing of the input. Given two models
that provide comparable accuracy measured
on utterance completion, we would prefer a
model which detects disfluencies earlier.
3 The Model
In this section, we describe Johnson and Char-
niak?s (2004) noisy channel model, and show
how this model can be made incremental.
As a data set to work with, we use the
Switchboard part of the Penn Treebank 3 cor-
pus. The Switchboard corpus is a corpus of
spontaneous conversations between two par-
ties. In Penn Treebank 3, the disfluencies are
manually annotated. Following Johnson and
Charniak (2004), we use all of sections 2 and
3 for training; we use conversations 4[5-9]* for
a held-out training set; and conversations 40*,
41[0-4]* and 415[0-3]* as the held-out test set.
3.1 The Noisy Channel Model
To find the repair disfluencies a noisy channel
model is used. For an observed utterance with
disfluencies, y, we wish to find the most likely
source utterance, x?, where:
x? = argmaxx p(x | y) (2)
= argmaxx p(y | x) p(x)
Here we have a channel model p(y|x) which
generates an utterance y given a source x and
a language model p(x). We assume that x
is a substring of y, i.e., the source utterance
can be obtained by marking words in y as a
disfluency and effectively removing them from
this utterance.
Johnson and Charniak (2004) experiment
with variations on the language model; they
report results for a bigram model, a trigram
model, and a language model using the Char-
niak Parser (Charniak, 2001). Their parser
model outperforms the bigram model by 5%.
The channel model is based on the intuition
that a reparandum and a repair are generally
very alike: a repair is often almost a copy of
the reparandum. In the training data, over
60% of the words in a reparandum are lexically
identical to the words in the repair. Exam-
ple 1 provides an example of this: half of the
repair is lexically identical to the reparandum.
The channel model therefore gives the high-
est probability when the reparandum and re-
pair are lexically equivalent. When the poten-
tial reparandum and potential repair are not
identical, the channel model performs dele-
tion, insertion or substitution. The proba-
bilities for these operations are defined on a
lexical level and are derived from the training
data. This channel model is formalised us-
ing a Synchronous Tree Adjoining Grammar
(STAG) (Shieber and Schabes, 1990), which
matches words from the reparandum to the
repair. The weights for these STAG rules are
learnt from the training text, where reparanda
and repairs are aligned to each other using a
minimum edit-distance string aligner.
1373
For a given utterance, every possible ut-
terance position might be the start of a
reparandum, and every given utterance po-
sition thereafter might be the start of a re-
pair (to limit complexity, a maximum distance
between these two points is imposed). Ev-
ery disfluency in turn can have an arbitrary
length (again up to some maximum to limit
complexity). After every possible disfluency
other new reparanda and repairs might occur;
the model does not attempt to generate cross-
ing or nested disfluencies, although they do
very occasionally occur in practice. To find
the optimal selection for reparanda and re-
pairs, all possibilities are calculated and the
one with the highest probability is selected.
A chart is filled with all the possible start
and end positions of reparanda, interregna
and repairs; each entry consists of a tuple
?rmbegin, irbegin, rrbegin, rrend?, where rm is the
reparandum, ir is the interregnum and rr is
the repair. A Viterbi algorithm is used to find
the optimal path through the utterance, rank-
ing each chart entry using the language model
and channel model. The language model, a
bigram model, can be easily calculated given
the start and end positions of all disfluency
components. The channel model is slightly
more complicated because an optimal align-
ment between reparandum and repair needs
to be calculated. This is done by extending
each partial analysis by adding a word to the
reparandum, the repair or both. The start po-
sition and end position of the reparandum and
repair are given for this particular entry. The
task of the channel model is to calculate the
highest probable alignment between reparan-
dum and repair. This is done by initialising
with an empty reparandum and repair, and
?growing? the analysis one word at a time. Us-
ing a similar approach to that used in calculat-
ing the edit-distance between reparandum and
repair, the reparandum and repair can both be
extended with one of four operations: deletion
(only the reparandum grows), insertion (only
the repair grows), substitution (both grow),
or copy (both grow). When the reparandum
and the repair have their length correspond-
ing to the current entry in the chart, the chan-
nel probability can be calculated. Since there
are multiple alignment possibilities, we use dy-
namic programming to select the most proba-
ble solutions. The probabilities for insertion,
deletion or substitution are estimated from
the training corpus. We use a beam-search
strategy to find the final optimum when com-
bining the channel model and the language
model.
3.2 Incrementality
Taking Johnson and Charniak?s model as a
starting point, we would like to develop an in-
cremental version of that algorithm. We sim-
ulate incrementality by maintaining for each
utterance to be processed an end-of-prefix
boundary; tokens after this boundary are
not available for the model to use. At each
step in our incremental model, we advance this
boundary by one token (the increment), un-
til finally the entire utterance is available. We
make use of the notion of a prefix, which is
a substring of the utterance consisting of all
tokens up to this boundary marker.
Just as in the non-incremental model, we
keep track of all the possible reparanda and re-
pairs in a chart. Every time the end-of-prefix
boundary advances, we update the chart: we
add all possible disfluencies which have the
end position of the repair located one token
before the end-of-prefix boundary, and we add
all possible start points for the reparandum,
interregna and repair, and end points for the
reparandum and interregna, given the order-
ing constraints of these components.
In our basic incremental model, we leave the
remainder of the algorithm untouched. When
the end-of-prefix boundary reaches the end of
the utterance, and thus the entire utterance
is available, this model results in an iden-
tical analysis to that provided by the non-
incremental model, since the chart contains
identical entries, although calculated in a dif-
ferent order. Intuitively, this model should
perform well when the current prefix is very
close to being a complete utterance; and it
should perform less well when a potential dis-
1374
fluency is still under construction, since these
situations are not typically found in the train-
ing data. We will return to this point further
below.
We do not change the training phase of the
model and we assume that the optimal values
found for the non-incremental model are also
optimal for the incremental model, since most
weights which need to be learned are based on
lexical values. Other weights are bigram based
values, and values dealing with unknown to-
kens (i.e., tokens which occur in the test data,
but not in the training data); it is not unrea-
sonable to assume these weights are identical
or very similar in both the incremental and
the non-incremental model.
4 Evaluation Models and Their
Application
As well as evaluating the accuracy of the anal-
ysis returned at the end of the utterance, it
seems reasonable to also evaluate how quickly
and accurately an incremental algorithm de-
tects disfluencies on a word-by-word basis as
the utterance is processed. In this section, we
provide the methodological background to our
approach, and in Section 5.2 we discuss the
performance of our model when evaluated in
this way.
Incremental systems are often judged solely
on the basis of their output when the utter-
ance being processed is completed. Although
this does give an insight into how well a system
performs overall, it does not indicate how well
the incremental aspects of the mechanism per-
form. In this section we present an approach
to the evaluation of a model of speech repair
detection which measures the performance of
the incremental component.
One might calculate the accuracy over all
prefixes using a simple word accuracy score.
However, because each prefix is a superstring
of each previous prefix, such a calculation
would not be fair: tokens that appear in early
in the utterance will be counted more often
than tokens that appear later in the utterance.
In theory, the analysis of the early tokens can
change at each prefix, so arguably it would
make sense to reevaluate the complete analy-
sis so far at every step. In practice, however,
these changes do not happen, and so this mea-
surement would not reflect the performance of
the system correctly.
Our approach is to define a measure of re-
sponsiveness: that is, how soon is a dis-
fluency detected? We propose to measure
responsiveness in two ways. The time-to-
detection score indicates how many tokens
following a disfluency are read before the given
disfluency is marked as one; the delayed ac-
curacy score looks n tokens back from the
boundary of the available utterance and, when
there is a gold standard disfluency-marked to-
ken at that distance, counts how often these
tokens are marked correctly.
We measure the time-to-detection score by
two numbers, corresponding to the number of
tokens from the start of the reparandum and
the number of tokens from the start of the re-
pair. We do this because disfluencies can be of
different lengths. We assume it is unlikely that
a disfluency will be found before the reparan-
dum is completed, since the reparandum it-
self is often fluent. We measure the time-to-
detection by the first time a given disfluency
appears as one.
Since the model is a statistical model, it
is possible that the most probable analysis
marks a given word at position j as a disflu-
ency, while in the next prefix the word in the
same position is now no longer marked as be-
ing disfluent. A prefix later this word might
be marked as disfluent again. This presents
us with a problem. How do we measure when
this word was correctly identified as disfluent:
the first time it was marked as such or the sec-
ond time? Because of the possibility of such
oscillations, we take the first marking of the
disfluency as the measure point. Disfluencies
which are never correctly detected are not part
of the time-to-detection score.
Since the evaluation starts with disfluencies
found by the model, this measurement has
precision-like properties only. Consequently,
there are easy ways to inflate the score arti-
ficially at the cost of recall. We address this
1375
by also calculating the delayed accuracy. This
is calculated at each prefix by looking back n
tokens from the prefix boundary, where n = 0
for the prefix boundary. For each n we cal-
culate the accuracy score at that point over
all prefixes. Each token is only assessed once
given a set value of n, so we do not suffer
from early prefixes being assessed more often.
However, larger values of n do not take all to-
kens into account, since the last y tokens of
an utterance will not play a part in the ac-
curacy when y < n. Since we evaluate given
a gold standard disfluency, this measurement
has recall-like properties.
Together with the final accuracy score over
the entire utterance, the time-to-detection
and delayed accuracy scores provide different
insights and together give a good measure-
ment of the responsiveness and performance
of the model.
Our incremental model has the same fi-
nal accuracy as the original non-incremental
model; this corresponds to an F-score (har-
monic mean) of 0.778 on a word basis.
We found the average time to detection,
measured in tokens for this model to be 8.3
measured from the start of reparandum and
5.1 from the start of repair. There are situ-
ations where disfluencies can be detected be-
fore the end of the repair; by counting from
the start rather than the end of the disfluency
components, we provide a way of scoring in
such cases. To provide a better insight into
what is happening, we also report the average
distance since the start of the reparandum.
We find that the time to detect is larger than
the average repair length; this implies that,
under this particular model, most disfluencies
are only detected after the repair is finished.
In fact the difference is greater than 1, which
means that in most cases it takes one more to-
ken after the repair before the model identifies
the disfluency.
Table 1 shows the delayed accuracy. We can
see that the score first rises quickly after which
the increases become much smaller. As men-
tioned above, a given disfluency detection in
theory might oscillate. In practice, however,
oscillating disfluencies are very rare, possibly
because a bigram model operates on a very lo-
cal level. Given that oscillation is rare, a quick
stabilisation of the score indicates that, when
we correctly detect a disfluency, this happens
rather quickly after the disfluency has com-
pleted, since the accuracy for the large n is
calculated over the same tokens as the accu-
racy for the smaller n (although not in the
same prefix).
5 Disfluencies around Prefix
Boundaries
5.1 Early detection algorithm
Our model uses a language model and a chan-
nel model to locate disfluencies. It calculates
a language model probability for the utterance
with the disfluency taken out, and it calculates
the probability of the disfluency itself with the
STAG channel model.
Consider the following example utterance
fragment where a repair disfluency occurs:
. . . wi
reparandum? ?? ?rni+1 rni+2
repair? ?? ?rri+3 rri+4 wi+5 . . . (3)
Here, the subscripts indicate token position in
sequence; w is a token outside the disfluency;
and rn is a reparandum being repaired by
the repair rr. The language model estimates
the continuation of the utterance without the
disfluency. The model considers whether the
utterance continuation after the disfluency is
probable given the language model; the rel-
evant bigram here is p(rri+3|wi), continuing
with p(rri+4|rri+3). However, under the in-
cremental model, it is possible the utterance
has only been read as far as token i + 3, in
which case the probability p(wi+4|wi+3) is un-
defined.
We would like to address the issue of look-
ing beyond a disfluency under construction.
We assume the issue of not being able to look
for an utterance continuation after the repair
component of the disfluency can be found back
in the incremental model scores. A disfluency
is usually only detected after the disfluency is
completely uttered, and always requires one
1376
n tokens back 1 2 3 4 5 6
accuracy 0.500 0.558 0.631 0.665 0.701 0.714
Table 1: delayed accuracy, n tokens back from the end of prefixes
n tokens back 1 2 3 4 5 6
accuracy 0.578 0.633 0.697 0.725 0.758 0.770
Table 2: delayed accuracy under the updated model
more token in the basic model. In the given
instance this means it is unlikely that we will
detect the disfluency before i + 5.
In order to make our model more respon-
sive, we propose a change which makes it
possible for the model to calculate channel
probabilities and language model probabili-
ties before the repair is completed. Assum-
ing we have not yet reached the end of utter-
ance, we would like to estimate the continua-
tion of the utterance with the relevant bigram
p(rri+4|rri+3). Since rri+4 is not yet avail-
able we cannot calculate this probability. The
correct thing to do is to sum over all possible
continuations, including the end of utterance
token (for the complete utterance, as opposed
to the current prefix). This results in the fol-
lowing bigram estimation:
?
t?vocabulary
p(t|wi) (4)
This estimation is not one we need to derive
from our data set, since p is a true probability.
In this case, the sum over all possible continu-
ations (this might include an end of utterance
marker, in which case the utterance is already
complete) equals 1. We therefore modify the
algorithm so that it takes this into account.
This solves the problem of the language model
assessing the utterance with the disfluency cut
out, when nothing from the utterance contin-
uation after a disfluency is available.
The other issue which needs to be addressed
is the alignment of the reparandum with the
repair when the repair is not yet fully avail-
able. Currently the model is encouraged to
align the individual tokens of the reparandum
with those of the repair. The algorithm has
lower estimations when the reparandum can-
not be fully aligned with the repair because
the reparandum and repair differ considerably
in length.
We note that most disfluencies are very
short: reparanda and repairs are often only
one or two tokens each in length, and the inter-
regnum is often empty. To remove the penalty
for an incomplete repair, we allow the repair to
grow one token beyond the prefix boundary;
given the relative shortness of the disfluencies,
this seems reasonable. Since this token is not
available, we cannot calculate the lexical sub-
stitution value. Instead we define a new opera-
tion in the channel model: in addition to dele-
tion, insertion, copy, and substitution, we add
an additional substitution operation, the in-
cremental completion substitution. This
operation does not compete with the copy op-
eration or the normal substitution operation,
since it is only defined when the last token of
the repair falls at the prefix boundary.
5.2 Results for the Early detection
algorithm
The results of these changes are reflected
in new time-to-detection and delayed accu-
racy scores. Again we calculated the time-
to-detection, and found this to be 7.5 from
the start of reparandum and 4.6 from the
start of repair. Table 2 shows the results un-
der the new early completion model using the
delayed accuracy method. We see that the
updated model has lower time-to-detection
scores (close to a full token earlier); for de-
layed accuracy, we note that the scores sta-
bilise in a similar fashion, but the scores for
the updated model rise slightly more quickly.
1377
6 Conclusions and Future Work
We have demonstrated an incremental model
for finding speech disfluencies in spoken lan-
guage transcripts. When we consider com-
plete utterances, the incremental model pro-
vides identical results to those of a non-
incremental model that delivers state-of-the-
art accuracy in speech repair detection. We
have investigated a number of measures which
allow us to evaluate the model on an incremen-
tal level. Most disfluencies are identified very
quickly, typically one or two tokens after the
disfluency has been completed. We addressed
the problems of the model around the end of
prefix boundaries. These are repairs which are
either still in the process of being uttered or
have just been completed. We have addressed
this issue by making some changes to how the
model deals with prefix boundaries, and we
have shown that this improves the responsive-
ness of the model.
The work reported in this paper uses a n-
gram model as a language model and a STAG
based model for the repair. We would like
to replace the n-gram language model with a
better language model. Previous work (John-
son and Charniak, 2004) has shown that dis-
fluency detection can be improved by replac-
ing the n-gram language model with a statis-
tical parser. Besides a reported 5% accuracy
improvement, this also provides a structural
analysis, something which an n-gram model
does not. We would like to investigate a sim-
ilar extension in our incremental approach,
which will require the integration of an in-
cremental statistical parser with our noisy
channel model. While transcripts of spoken
texts come with manually annotated sentence
boundaries, real time spoken language does
not. The language model in particular takes
these sentence boundaries into account. We
therefore propose to investigate the proper-
ties of this model when sentence boundaries
are removed.
Acknowledgements
This work was supported by the Australian
Research Council as part of the Thinking
Head Project, ARC/NHMRC Special Re-
search Initiative Grant # TS0669874. We
thank the anonymous reviewers for their help-
ful comments.
References
Charniak, Eugene. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting on Association for Com-
putational Linguistics, pages 124?131.
Core, Mark and Lenhart Schubert. 1999. A model
of speech repairs and other disruptions. In
AAAI Fall Symposium on Psychological Mod-
els of Communication in Collaborative Systems,
pages 48?53.
Honal, Matthias and Tanja Schultz. 2003. Correc-
tion of Disfluencies in Spontaneous Speech us-
ing a Noisy-Channel Approach. In Proceedings
of the 8th Eurospeech Conference.
Johnson, Mark and Eugene Charniak. 2004. A
tag-based noisy channel model of speech repairs.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 33?39.
Marslen-Wilson, W. 1973. Linguistic structure
and speech shadowing at very short latencies.
Nature, 244:522?533.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-
Coverage Parsing using Human-Like Mem-
ory Constraints. Computational Linguistics,
36(1):1?30.
Shieber, Stuart M. and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceed-
ings of the 13th International Conference on
Computational Linguistics, pages 253?258.
Shriberg, Elizabeth. 1994. Preliminaries to a
Theory of Speech Disuencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
1378
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2326?2334, Dublin, Ireland, August 23-29 2014.
Unsupervised Word Segmentation in Context
Gabriel Synnaeve and Isabelle Dautriche
LSCP, DEC
ENS Ulm, Paris, France
gabriel.synnaeve@gmail.com
isabelle.dautriche@gmail.com
Benjamin B
?
orschinger
Institut f?ur Computerlinguistik
Universit?at Heidelberg, Heidelberg, Germany.
benjamin.boerschinger@gmail.com
Mark Johnson
Department of Computer Science
Macquarie University, Sydney, Australia
mark.johnson@mq.edu.au
Emmanuel Dupoux
LSCP, DEC
EHESS, Paris, France
emmanuel.dupoux@gmail.com
Abstract
This paper extends existing word segmentation models to take non-linguistic context into ac-
count. It improves the token F-score of a top performing segmentation models by 2.5% on a 27k
utterances dataset. We posit that word segmentation is easier in-context because the learner is
not trying to access irrelevant lexical items. We use topics from a Latent Dirichlet Allocation
model as a proxy for ?activities? contexts, to label the Providence corpus. We present Adaptor
Grammar models that use these context labels, and we study their performance with and without
context annotations at test time.
1 Introduction and Previous Works
Segmentation of the speech stream into lexical units plays a central role in early language acquisition.
Because words are generally not uttered in isolation, one of the first task for infants learning a language is
to extract the words that make up the utterances they hear. Experimental research has shown that infants
are able to segment fluent speech into word-like units within the first year of life (Jusczyk and Aslin,
1995). How does this ability emerge? There is evidence that infants use a broad array of linguistic cues
to perform word segmentation (e.g., phonotactics (Jusczyk et al., 1993a), prosodic information (Jusczyk
et al., 1993b), statistical regularities (Saffran et al., 1996)). Past experimental and modeling research on
speech segmentation has mainly focused on linguistic cues, treating them as independent from other non-
linguistic cues naturally occurring in the child learning environment. Yet, language appears in context
and is constrained by the events occurring in the daily life of the child. For example, during an eating
event one is most likely to speak about food, while during a zoo-visit event, people are more likely
to talk about the animals they see. Activity contexts may provide a natural structure to speech that
would be readily be accessible to children. A recent study using dense recordings of a single child?s
language development (Roy et al., 2006) showed that words appearing in specific activity contexts are
learned faster (Roy et al., 2012). Relatedly, Johnson et al. (2010) showed that Adaptor Grammars (AGs)
performed better on a segmentation task when the model has access to a hand-annotated set of objects
present in the environment, that it can use to learn simultaneously word-object associations (see also
(Frank et al., 2009)). This supports the view that integrating multiple sources of information, linguistic
and non-linguistic, can improve learning.
Following this idea, we posit that information from the broader context in which a word has been
uttered may simplify the learning problem faced by the child. In particular, our hypothesis postulates
that speech segmentation is easier when using vocabularies that are related to a specific activity (eating,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2326
Table 1: Most probable words in the 7 final topics
egg book ball truck name color block
apple shape cat car school bear battery
banana square hat fire time crayon minute
milk circle tree piece today hair phone
butter triangle fish train day head puzzle
?food ?shapes ?playing ?toys ?time ?drawing ?garbage?
playing...), or place (kitchen, bedroom...). To evaluate this hypothesis, we applied topic modeling (Blei
et al., 2003) to automatically derive activity contexts on a corpus of child directed speech, the Provi-
dence corpus (Demuth et al., 2006), and tested the influence of such topics on a word segmentation task
extending the AG models used in (B?orschinger et al., 2012). We found that a model augmented with
the assumption that words are dependent upon the topic of the discourse (as a proxy for activity context)
performs better than the same model without access to the discourse topic. This suggests that the broader
context in which sentences are uttered may help in the word segmentation process, and could presumably
be used at various stages of language development.
The paper is structured as follows. Section 2 presents a novel approach to augment a corpus with
contextual annotations derived from topic models. Section 3 quickly explains Adaptor Grammars, the
framework that we used to express all our models. Section 4 presents all the models that were used in
the results. Section 5 describes the Providence corpus and the experimental setup. Section 6 shows our
quantitative and qualitative results. Finally, we discuss the implications for models of language learning.
2 Topics as Proxies for Contexts
Roy et al. (2012) found high correlations between human-annotated activity contexts and topics from a
latent Dirichlet allocation model (LDA) (Blei et al., 2003), thus showing that using topics as proxies for
contexts is a sound approach. Topic modeling infers a topic distribution for each ?document? (a bag of
words) in the corpus. Since ?documents? were not annotated in our corpus, we developed the following
3-step approach to automatically segment it into documents.
Firstly, for all the children of the Providence corpus, we used recording sessions as hard document
boundaries. We considered as a ?possible document? every contiguous sequence of sentences separated
by at least 10 seconds of silence, according to the orthographic transcript. We also identified ?possible
documents? using cues such as ?bye/hi?, indicating a change of participants. This segmentation resulted
in an over-segmented corpus (compared to context switches), yielding a total of 16, 742 documents.
Secondly, we used the gensim software (
?
Reh?u?rek and Sojka, 2010) to train a topic model (LDA)
1
,
and get the topic distributions for each of these documents. We used the symmetric KL-divergence to
measure the distance between two topic distributions before and after a ?possible document? boundary.
If the distance was above a threshold, we considered this boundary as a document boundary. Otherwise
we merged both ?possible documents? through this silence. The threshold was set empirically to dis-
criminate between two topic distributions that correspond to different activity contexts. After this step,
we assume that each of the resulting 8, 634 documents maps to an activity context.
Thirdly, we applied LDA again on this new segmentation to get the topic distribution, hence the activity
context, of each document. The number of topics is qualitatively chosen to correspond to the number of
main activity contexts (eating / playing / drawing / etc.) that occur in the Providence dataset (we used 7
topics), the resulting most topic specific words are shown in Table 1. Finally, for each document, we got
a distribution on topics, and we annotated the document with the most probable topic. By doing that, we
throw away graded information about the distribution on topics for each document. We could make use
of the full distribution, but here we are only interested in the most probable topic as a proxy for activity
context. We do not posit that the infants learn the topic models on linguistic cues while bootstrapping
speech and segmentation, but rather that they get activity context from non-linguistic cues.
1
We did LDA only on nouns (as they contain most of the semantics), weighted by TF-IDF.
2327
3 Adaptor Grammars
Adaptor Grammars (Johnson et al., 2007) are an extension of probabilistic context-free gram-
mars (PCFGs) that learn probability of entire subtrees as well as probabilities of rules. A PCFG
(N,W,R, S, ?) consists of a start symbol S, N and W disjoints sets of nonterminals and terminal sym-
bols respectively. R is a set of rules producing elements of N or W . Finally, ? is a set of distributions
over the rules R
X
,?X ? N (R
X
are the rules that expand X). An AG (N,W,R, S, ?, A,C) extends
the above PCFG with a subset (A ? N ) of adapted nonterminals, each of them (X ? A) having an
associated adaptor (C
X
? C). An AG defines a distribution over trees G
X
, ?X ? N ?W . If X /? A,
then G
X
is defined exactly as for a PCFG:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(G
Y
1
. . . G
Y
n
)
With TD
X
(G
1
. . . G
n
) the distribution over trees with root node X and each subtree t
i
? G
i
i.i.d. If
X ? A, then there is an additional indirection (composition) with the distribution H
X
:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(H
Y
1
. . . H
Y
n
)
H
X
? C
X
(G
X
)
We used C
X
adaptors following the Pitman-Yor process (PYP) (Perman et al., 1992; Teh, 2006) with
parameters a and b. The PYP generates (Zipfian) type frequencies that are similar to those that occur
in natural language (Goldwater et al., 2011). Metaphorically, if there are n customers and m tables, the
n+ 1th customer is assigned to table z
n+1
according to (?
k
is the Kronecker delta function):
z
n+1
|z
1
. . . z
n
?
ma+ b
n+ b
?
m+1
+
m
?
k=1
n
k
? a
n+ b
?
k
For an AG, this means that adapted non-terminals (X ? A) either expand to a previously generated
subtree (T (X)
k
) with probability proportional to how often it was visited (n
k
), or to a new subtree
(T (X)
m+1
) generated through the PCFG with probability proportional to ma+ b.
4 Word segmentation models
4.1 Unigram model
This most basic model just generates words as sequences of phonemes. AsWord is underlined, it means
it is adapted, and thus we learn a ?word unit -like? vocabulary. Phon is a nonterminal that expands to
all the phonemes of the language under consideration.
Sentence?Word
+
Word? Phon
+
where :
Word
+
?
{
Words?Word
Words?Word Words
4.2 Collocations and Syllabification
The baseline that we are using is commonly called the ?colloc-syll? model (Johnson, 2008; B?orschinger
et al., 2012) and is reported at 78% token F-score on the standard Brent version of the Bernstein-Ratner
corpus corpus (Johnson, 2008). It posits that sentences are collocations of words, and words are com-
posed of syllables. (Goldwater et al., 2009) showed how an assumption of independence between words
(a unigram model) led to under-segmentation. So, above the Word level, we take the collocations (co-
occurring sequences) of words into account.
2328
Furthermore, there is evidence that 8-month-old infants track syllable frequencies (Saffran et al.,
1996), and the ?colloc-syll? model can take that into account. Word splits into general syllables and
initial- or final- specific syllables. Syllables consist of onsets or codas (producing consonants), and nu-
clei (vowels). Onsets, nuclei and codas are adapted, thus allowing this model to memorize sequences or
consonants or sequences of vowels, dependent on their position in the word. Consonants and vowels are
the pre-terminals, their derivation is specified in the grammar into phonemes of the language.
Sentence? Colloc
+
Colloc?Word
+
Word? StructSyll
For notations purposes, all this syllabification is appended after Word by Word ? StructSyll.
All details about the collocations and syllabification grammars can be found in (Johnson, 2008). Here
is an example of a (good) parse of ?yuwanttusiD6bUk? with this model, skipping the StructSyll
derivations:
Sentence
Colloc
Word
bUk
Word
D6
Colloc
Word
si
Colloc
Word
tu
Word
want
Word
yu
4.3 Including topics (contexts)
To allow for the model to make use of the topics (used as proxies for contexts), we modify the grammar
by prefixing utterances with topic number (similarly to (Johnson et al., 2010)), ?K ? #topics:
Sentence? tK Colloc
+
tK
Colloc
tK
?Word
+
tK
For each Word
tK
, we can derive it into a common adapted Word by Word
tK
?Word. Consider this
lower level adaptor (Word): it learns a shared vocabulary independently of the topic (all contexts that
will derive b U k will increment the Word(b U k) pseudo-count). This Word-hierarchical model is
called share vocab.
Alternatively, we can learn a separate vocabulary for each topic, by having directly: Word
tK
?
StructSyll (note that all words then share the same syllabic structure). Words are split across different
topics and need to be adapted for each topic in which they appear. This flat structure vocabulary model
is called split vocab.
4.4 Allowing for non context-specific words
Sentences are not composed only of context-specific words, thus we need a third type of extension that
allows for topic-independent and topic-specific words to mix. For this, we add topic-independent types
of Colloc and Word that can be used across all topics, but we force each sentence to have at least one
topical collocation:
Sentence ? tK (Colloc
+
|Colloc
+
tK
) Colloc
+
tK
(Colloc
+
|Colloc
+
tK
)
Colloc
tK
? Word
+
tK
Colloc ? Word
+
Word
tK
? StructSyll
Word ? StructSyll
2329
Parentheses denote that these terms are optionals, and ?|? denotes ?or?. Both Word
tK
and Word are
adapted, but this time on the same level of hierarchy. This model allows the use of both topic-specific and
common words in sentences, and it learns #topics + 1 vocabularies. We call this model with common.
An example of a correct parse with this model is given by:
Sentence
Colloc t3
Word t3
bUk
Word t3
D6
Word t3
si
Colloc
Word
tu
Word
want
Word
yu
t3
5 Experimental setup
The Providence corpus (Demuth et al., 2006) consists of audio and video, weekly or bi-weekly, record-
ings of 6 monolingual English-speaking children home interactions. Each recording is approximatively
1 hour long. This corpus spans approximatively from their first to third year. We used the whole corpus
to extract the topics to get more stable and general activity contexts. For all the following results, we
used only the Naima portion between 11 months and 24 months, consisting in 26,425 utterances (sen-
tences) and 135,389 tokens (words). The input consist in DARPABET-encoded sequences of phonemes
with about 4200 word-types in the Naima subset. We followed the same preparation procedure as in
(B?orschinger et al., 2012), where more details about the corpus can be found.
We used the last version of Mark Johnson?s Adaptor Grammars software
2
. All the additional code
(preparation, topics, grammars, learning) to reproduce these experiments and results is freely available
online
3
, along with the datasets annotations derived from topic modeling
4
. For the adaptors, we used a
Beta(1, 1) (uniform) prior on the PYP a parameter, and a sparse Gamma(100, 0.01) prior on the PYP
b parameter. We ran 500 iterations (finishing at ? 0.05% of log posterior variation between the lasts
iterations) with several runs for each subset of the Naima dataset.
6 Results
6.1 Unsupervised words segmentation
Table 2: Mean (token and boundary) F-scores (f), precisions (p), and recalls (r) for different models
depending on the size of dataset (age range).
months baseline share vocab split vocab with common
token f p r f p r f p r f p r
11-12 .80 .79 .81 .77 .76 .78 .77 .75 .78 .77 .75 .78
11-15 .81 .81 .82 .76 .78 .75 .81 .79 .82 .82 .81 .83
11-19 .82 .82 .83 .77 .78 .76 .81 .81 .82 .83 .82 .84
11-22 .81 .82 .81 .77 .79 .75 .82 .81 .83 .83 .82 .84
boundary f p r f p r f p r f p r
11-12 .90 .88 .91 .88 .87 .89 .87 .85 .90 .88 .85 .90
11-15 .91 .91 .92 .89 .91 .86 .91 .89 .92 .91 .90 .93
11-19 .92 .92 .93 .90 .92 .88 .92 .91 .93 .92 .91 .94
11-22 .92 .93 .91 .90 .93 .87 .92 .91 .93 .93 .91 .94
The key metric of interest is the token F-score (harmonic mean of precision and recall of words).
Table 2 gives all the scores for an increasingly large dataset (as in (B?orschinger et al., 2012)). Figure 1
shows the month-by-month evolution of the token F-score of the different models. We can see that
2
http://web.science.mq.edu.au/
?
mjohnson/
3
https://github.com/SnippyHolloW/contextual_word_segmentation
4
https://github.com/SnippyHolloW/contextual_word_segmentation/tree/master/
ProvidenceFinal/Final
2330
Figure 1: Token F-scores (and standard deviations) evolution with an increasingly bigger and richer
dataset (11 months to ?X-axis value? months), computed on 8 runs of 500 iterations per data point.
0.750
0.775
0.800
0.825
24232221201918171615141312 age in months
token f
?score modelbaselineshare vocabsplit vocabwith common
context-based models need more data to get good performances (several vocabularies to learn), but they
seem more resilient to over-segmentation.
Preliminary results confirm the trend of baseline scores getting slowly worse at 25 and 26 months
while with common and split vocab stabilize (not plotted here). We also tried models for which we
can have the ?common vocabulary? derived only at the level of the collocations (making topic-specific
collocations topic-pure as in split vocab for instance), or only at the level of the words (allowing for
topic-specific collocations deriving in only common words if needed). Both models are worse than split
vocab and with common.
Using a shared global vocabulary while being able to learn (through adaptation) different topic-specific
vocabularies does not seem to be a solution: share vocab performs worse than the baseline. Token recall
and boundary recall are worse off (see Table 2), suggesting that fewer words are correctly adapted.
Maybe that is because this is the only model with two levels of adapted word hierarchies (Word
tK
and
Word). Sharing a lower-level vocabulary (Word) still does not allow for context vocabularies (Word
tK
)
to mix, thus is simply harder to train. Having only one vocabulary per context (split vocab) is a slight
improvement over the baseline, even though it is not significant (95% confidence interval) before 22
months. Models allowing for both topic-specific vocabularies and a common vocabulary to be learned
are the best: with common is significantly (95% confidence interval) better than the baseline, starting
from 20 months (Figure 1). The improvement seems to be due to better token (and boundary) recall
(Table 2), suggesting that more words are learned. By looking at their lexicons at 24 months, topic-
dependent models have slightly larger lexicon recalls and worse lexicon precisions than the baseline.
This means that the additional true word-types that they learn are more frequently correctly used than the
false word-types (otherwise the token F-scores would be reversed, e.g. between split vocab and baseline).
2331
Figure 2: Mean token F-scores (and standard deviations) on 20% held-out test data for 6 different random
splits of Naima from 11 to 22 months, 500 iterations each. Grey for baseline on test, green and blue for
context-dependent models on test and no prefix conditions respectively.
Table 3: Most probable words (? P (word|topic = k)) in the 7 recovered topics at test time without topic
annotations (no prefix condition) for the with common model (we omitted phonemes clusters yielding
non-words).
bread elephant lego Michael skinny stick bubble
delicious owl doctor shorts massage remember pasta
avocado wearing brush towel ostrich track spirals
porridge turkey change shirts nurse forget squirrels
raisin haircut squeeze pirates hammer oink thumb
biscuit turtle music tangled ruby towed pentagon
food animals play clothes (messy) verbs ?shapes
6.2 Recovery of the topics on held-out data
To check whether these models generalize to unseen utterances, and possibly unseen vocabulary, we
looked at the scores of held-out data (80/20% train/test split of the Naima 11 to 22 months dataset).
Token F-scores for this test condition are shown in green and grey in Figure 2. This separates low-
frequency collocations to be used at test time and those seen at training time, both for context aware
models and the basic baseline model. The F-scores show the same pattern as in the previous experiment,
with context-aware models (with common and split vocab here) performing better than the baseline.
The topics are learned on the orthographic transcription of the whole Providence corpus (6 children),
while we test only on the Naima dataset. Still, to check that these results are not simply due to additional
information (leaked somehow in the form of the tK prefix), we produced another held-out condition,
without topic ( tK) prefixes. Models can use topic-specific vocabularies learned during training, but
they are given no context information at test time. Token F-scores for this no prefix condition are shown
in blue (and grey for the baseline) in Figure 2. The fact that no prefix performance is on par with the
test condition means that contextual cues are not only important at test time, but particularly so while
learning the vocabulary. In other words, the model acquires its vocabularies making use of the additional
context. In the test setting, it is evaluated on novel utterances for which additional context information
is available. In the no prefix condition it is evaluated on novel utterances for which no additional context
information is available. This means that topic-specific vocabulary learned during training is successfully
used in a consistent way at test time. To confirm this qualitatively, we looked at the most probable words
(after unsupervised segmentation from the phonemic input) in recovered topics at test time in the no
prefix condition. They are shown in Table 3, and they exhibit some of the topics that were found on the
orthographic transcript (as they are not limited to nouns, a topic for ?verbs? appears).
2332
7 Conclusion
We have shown that contextual information helps segmenting speech into word-like units. We used
topic modeling as a proxy for richer contextual annotations, as (Roy et al., 2012) have shown high cor-
relation between contexts and automatically derived topics. We modified existing Adaptor Grammar
segmentation models (Johnson, 2008; Johnson and Goldwater, 2009), to be able to learn topic-specific
vocabularies. We applied this approach to a large child directed speech corpus that was previously used
for segmentation (B?orschinger et al., 2012). Our model with the capacity to use both a topic-specific
vocabulary and a common vocabulary (with common) produces better segmentation scores, ending up
with at least 2.5% better absolute F-scores than its context-oblivious counterpart (baseline). More gen-
erally, both models that learn specialized vocabularies do not get worse F-scores with increasing data
(Figure 1). Particularly, they seem to fix a well-known problem of previous models like ?colloc-syll?
(our baseline), that ?overlearn? by over-segmenting frequent morphemes as single words (B?orschinger
et al., 2012). We have controlled for the additional information of giving the topic ( tK), and we have
found out that contextual information helps at training time.
It would be interesting to look into the link between semantics and syntax in recovered topics. Fur-
ther work should integrate syntax (e.g. function words), stress cues and prosody from the audio signal
(B?orschinger and Johnson, 2014), use even less supervision for contexts, and be applied to other lan-
guages. We believe that language acquisition is not a simple sequential process and that segmentation,
syntax, and word meaning bootstrap each others. This is only a first step towards integrating multiple
sources of information and different modalities at all steps of language acquisition.
Acknowledgments
This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOT-
PHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02
PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIM
cerveau et pense).
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Benjamin B?orschinger and Mark Johnson. 2014. Exploring the role of stress in Bayesian word segmentation using
Adaptor Grammars. Transactions of the Association of Computational Linguistics, 2:93?104, February.
Benjamin B?orschinger, Katherine Demuth, and Mark Johnson. 2012. Studying the effect of input size for bayesian
word segmentation on the providence corpus. In COLING, pages 325?340.
Katherine Demuth, Jennifer Culbertson, and Jennifer Alter. 2006. Word-minimality, epenthesis and coda licensing
in the early acquisition of english. Language and Speech, 49(2):137?173.
Michael C Frank, Noah D Goodman, and Joshua B Tenenbaum. 2009. Using speakers? referential intentions to
model early cross-situational word learning. Psychological Science, 20(5):578?585.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation:
Exploring the effects of context. Cognition, 112(1):21?54.
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping
word frequencies with two-stage language models. Journal of Machine Learning Research, 12(Jul):2335?2382.
Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 317?325. Association for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying
compositional nonparametric bayesian models. Advances in neural information processing systems, 19:641.
2333
Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones. 2010. Synergies in learning words and their
referents. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic
structure. In ACL, pages 398?406.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants detection of the sound patterns of words in fluent speech.
Cognitive psychology, 29(1):123.
Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz. 1993a. Infants? preference for the predominant stress
patterns of english words. Child development, 64(3):675687.
Peter W. Jusczyk, Angela D. Friederici, Jeanine MI Wessels, Vigdis Y. Svenkerud, and Ann Marie Jusczyk.
1993b. Infants sensitivity to the sound patterns of native language words. Journal of Memory and Language,
32(3):402420.
Mihael Perman, Jim Pitman, and Marc Yor. 1992. Size-biased sampling of poisson point processes and excursions.
Probability Theory and Related Fields, 92(1):21?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceed-
ings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA. http://is.muni.cz/publication/884893/en.
Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat, Michael Fleischman, Brandon Roy, Nikolaos Mavridis, Ste-
fanie Tellex, Alexia Salata, Jethran Guinness, et al. 2006. The human speechome project. In Symbol Grounding
and Beyond, pages 192?196. Springer.
Brandon C Roy, Michael C Frank, and Deb Roy. 2012. Relating activity contexts to early word learning in dense
longitudinal data. In Proceedings of the 34th Annual Cognitive Science Conference.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. 1996. Statistical learning by 8-month old infants.
Science, 274(5294):1926?1928.
Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992.
2334
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234?1244,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root ? Auxiliary Noun ? Adjective
Root ? Verb Noun ? Article
Verb ? Noun Noun ? Noun
Verb ? Pronoun Noun ? Numeral
Verb ? Adverb Preposition ? Noun
Verb ? Verb Adjective ? Adverb
Auxiliary ? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
1234
languages. Furthermore, these universal rules are
compact and well-understood, making them easy to
manually construct.
In addition to these universal dependencies, each
specific language typically possesses its own id-
iosyncratic set of dependencies. We address this
challenge by requiring the universal constraints to
only hold in expectation rather than absolutely, i.e.,
we permit a certain number of violations of the con-
straints.
We formulate a generative Bayesian model that
explains the observed data while accounting for
declarative linguistic rules during inference. These
rules are used as expectation constraints on the
posterior distribution over dependency structures.
This approach is based on the posterior regular-
ization technique (Grac?a et al, 2009), which we
apply to a variational inference algorithm for our
parsing model. Our model can also optionally re-
fine common high-level syntactic categories into
per-language categories by inducing a clustering of
words using Dirichlet Processes (Ferguson, 1973).
Since the universals guide induction toward linguis-
tically plausible structures, automatic refinement be-
comes feasible even in the absence of manually an-
notated syntactic trees.
We test the effectiveness of our grammar induc-
tion model on six Indo-European languages from
three language groups: English, Danish, Portuguese,
Slovene, Spanish, and Swedish. Though these lan-
guages share a high-level Indo-European ancestry,
they cover a diverse range of syntactic phenomenon.
Our results demonstrate that universal rules greatly
improve the accuracy of dependency parsing across
all of these languages, outperforming current state-
of-the-art unsupervised grammar induction meth-
ods (Headden III et al, 2009; Berg-Kirkpatrick and
Klein, 2010).
2 Related Work
Learning with Linguistic Constraints Our work
is situated within a broader class of unsupervised ap-
proaches that employ declarative knowledge to im-
prove learning of linguistic structure (Haghighi and
Klein, 2006; Chang et al, 2007; Grac?a et al, 2007;
Cohen and Smith, 2009b; Druck et al, 2009; Liang
et al, 2009a). The way we apply constraints is clos-
est to the latter two approaches of posterior regular-
ization and generalized expectation criteria.
In the posterior regularization framework, con-
straints are expressed in the form of expectations on
posteriors (Grac?a et al, 2007; Ganchev et al, 2009;
Grac?a et al, 2009; Ganchev et al, 2010). This de-
sign enables the model to reflect constraints that are
difficult to encode via the model structure or as pri-
ors on its parameters. In their approach, parame-
ters are estimated using a modified EM algorithm,
where the E-step minimizes the KL-divergence be-
tween the model posterior and the set of distributions
that satisfies the constraints. Our approach also ex-
presses constraints as expectations on the posterior;
we utilize the machinery of their framework within
a variational inference algorithm with a mean field
approximation.
Generalized expectation criteria, another tech-
nique for declaratively specifying expectation con-
straints, has previously been successfully applied to
the task of dependency parsing (Druck et al, 2009).
This objective expresses constraints in the form of
preferences over model expectations. The objective
is penalized by the square distance between model
expectations and the prespecified values of the ex-
pectation. This approach yields significant gains
compared to a fully unsupervised counterpart. The
constraints they studied are corpus- and language-
specific. Our work demonstrates that a small set of
language-independent universals can also serve as
effective constraints. Furthermore, we find that our
method outperforms the generalized expectation ap-
proach using corpus-specific constraints.
Learning to Refine Syntactic Categories Recent
research has demonstrated the usefulness of auto-
matically refining the granularity of syntactic cat-
egories. While most of the existing approaches
are implemented in the supervised setting (Finkel
et al, 2007; Petrov and Klein, 2007), Liang et al
(2007) propose a non-parametric Bayesian model
that learns the granularity of PCFG categories in
an unsupervised fashion. For each non-terminal
grammar symbol, the model posits a Hierarchical
Dirichlet Process over its refinements (subsymbols)
to automatically learn the granularity of syntactic
categories. As with their work, we also use non-
parametric priors for category refinement and em-
1235
ploy variational methods for inference. However,
our goal is to apply category refinement to depen-
dency parsing, rather than to PCFGs, requiring a
substantially different model formulation. While
Liang et al (2007) demonstrated empirical gains on
a synthetic corpus, our experiments focus on unsu-
pervised category refinement on real language data.
Universal Rules in NLP Despite the recent surge
of interest in multilingual learning (Kuhn, 2004; Co-
hen and Smith, 2009a; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010), there is surprisingly
little computational work on linguistic universals.
On the acquisition side, Daume? III and Campbell
(2007) proposed a computational technique for dis-
covering universal implications in typological fea-
tures. More closely related to our work is the posi-
tion paper by Bender (2009), which advocates the
use of manually-encoded cross-lingual generaliza-
tions for the development of NLP systems. She ar-
gues that a system employing such knowledge could
be easily adapted to a particular language by spe-
cializing this high level knowledge based on the ty-
pological features of the language. We also argue
that cross-language universals are beneficial for au-
tomatic language processing; however, our focus is
on learning language-specific adaptations of these
rules from data.
3 Model
The central hypothesis of this work is that unsu-
pervised dependency grammar induction can be im-
proved using universal linguistic knowledge. To-
ward this end our approach is comprised of two
components: a probabilistic model that explains
how sentences are generated from latent dependency
structures and a technique for incorporating declar-
ative rules into the inference process.
We first describe the generative story in this sec-
tion before turning to how constraints are applied
during inference in Section 4. Our model takes as
input (i.e., as observed) a set of sentences where
each word is annotated with a coarse part-of-speech
tag. Table 2 provides a detailed technical descrip-
tion of our model?s generative process, and Figure 1
presents a model diagram.
For each observed coarse symbol s:
1. Draw top-level infinite multinomial over
subsymbols ?s ? GEM(?).
2. For each subsymbol z of symbol s:
(a) Draw word emission multinomial
?sz ? Dir(?0).
(b) For each context value c:
i. Draw child symbol generation
multinomial ?szc ? Dir(?0).
ii. For each child symbol s?:
A. Draw second-level infinite
multinomial over subsymbols
pis?szc ? DP(?, ?s?).
For each tree node i generated in context c by
parent symbol s? and parent subsymbol z?:
1. Draw coarse symbol si ? Mult(?s?z?).
2. Draw subsymbol zi ? Mult(pisis?z?c).
3. Draw word xi ? Mult(?sizi).
Table 2: The generative process for model parameters
and parses. In the above GEM, DP, Dir, and Mult refer
respectively to the stick breaking distribution, Dirichlet
process, Dirichlet distribution, and multinomial distribu-
tion.
Generating Symbols and Words We describe
how a single node of the tree is generated before
discussing how the entire tree structure is formed.
Each node of the dependency tree is comprised of
three random variables: an observed coarse symbol
s, a hidden refined subsymbol z, and an observed
word x. In the following let the parent of the cur-
rent node have symbol s? and subsymbol z?; the root
node is generated from separate root-specific distri-
butions. Subsymbol refinement is an optional com-
ponent of the full model and can be omitted by de-
terministically equating s and z. As we explain at
the end of this section, without this aspect the gener-
ative story closely resembles the classic dependency
model with valence (DMV) of Klein and Manning
(2004).
First we draw symbol s from a finite multinomial
1236
s - coarse symbol (observed)
z - refined subsymbol
x - word (observed)
?szc - distr over child coarse symbols for
each parent s and z and context c
?s - top-level distr over subsymbols for s
piss?z?c - distr over subsymbols for each s,
parent s? and z?, and context c
?sz - distr over words for s and z
Figure 1: Graphical representation of the model and a summary of the notation. There is a copy of the outer plate for
each distinct symbol in the observed coarse tags. Here, node 3 is shown to be the parent of nodes 1 and 2. Shaded
variables are observed, square variables are hyperparameters. The elongated oval around s and z represents the two
variables jointly. For clarity the diagram omits some arrows from ? to each s, pi to each z, and ? to each x.
distribution with parameters ?s?z?c. As the indices
indicate, we have one such set of multinomial pa-
rameters for every combination of parent symbol
s? and subsymbol z? along with a context c. Here
the context of the current node can take one of six
values corresponding to every combination of di-
rection (left or right) and valence (first, second, or
third or higher child) with respect to its parent. The
prior (base distribution) for each ?s?z?c is a symmet-
ric Dirichlet with hyperparameter ?0.
Next we draw the refined syntactic category sub-
symbol z from an infinite multinomial with parame-
ters piss?z?c. Here the selection of pi is indexed by the
current node?s coarse symbol s, the symbol s? and
subsymbol z? of the parent node, and the context c
of the current node.
For each unique coarse symbol s we tie together
the distributions piss?z?c for all possible parent and
context combinations (i.e., s?, z?, and c) using a Hi-
erarchical Dirichlet Process (HDP). Specifically, for
a single s, each distribution piss?z?c over subsymbols
is drawn from a DP with concentration parameter
? and base distribution ?s over subsymbols. This
base distribution ?s is itself drawn from a GEM prior
with concentration parameter ?. By formulating the
generation of z as an HDP, we can share parame-
ters for a single coarse symbol?s subsymbol distribu-
tion while allowing for individual variability based
on node parent and context. Note that parameters
are not shared across different coarse symbols, pre-
serving the distinctions expressed via the coarse tag
annotations.
Finally, we generate the word x from a finite
multinomial with parameters ?sz , where s and z are
the symbol and subsymbol of the current node. The
? distributions are drawn from a symmetric Dirich-
let prior.
Generating the Tree Structure We now consider
how the structure of the tree arises. We follow
an approach similar to the widely-referenced DMV
model (Klein and Manning, 2004), which forms
the basis of the current state-of-the-art unsuper-
vised grammar induction model (Headden III et al,
2009). After a node is drawn we generate children
on each side until we produce a designated STOP
symbol. We encode more detailed valence informa-
tion than Klein and Manning (2004) and condition
child generation on parent valence. Specifically, af-
ter drawing a node we first decide whether to pro-
ceed to generate a child or to stop conditioned on the
parent symbol and subsymbol and the current con-
text (direction and valence). If we decide to gener-
ate a child we follow the previously described pro-
cess for constructing a node. We can combine the
stopping decision with the generation of the child
symbol by including a distinguished STOP symbol
as a possible outcome in distribution ?.
No-Split Model Variant In the absence of sub-
symbol refinement (i.e., when subsymbol z is set to
be identical to coarse symbol s), our model simpli-
fies in some respects. In particular, the HDP gener-
1237
ation of z is obviated and word x is drawn from a
word distribution ?s indexed solely by coarse sym-
bol s. The resulting simplified model closely resem-
bles DMV (Klein and Manning, 2004), except that it
1) explicitly generate words x rather than only part-
of-speech tags s, 2) encodes richer context and va-
lence information, and 3) imposes a Dirichlet prior
on the symbol distribution ?.
4 Inference with Constraints
We now describe how to augment our generative
model of dependency structure with constraints de-
rived from linguistic knowledge. Incorporating arbi-
trary linguistic rules directly in the generative story
is challenging as it requires careful tuning of either
the model structure or priors for each constraint. In-
stead, following the approach of Grac?a et al (2007),
we constrain the posterior to satisfy the rules in ex-
pectation during inference. This effectively biases
the inference toward linguistically plausible settings.
In standard variational inference, an intractable
true posterior is approximated by a distribution from
a tractable set (Bishop, 2006). This tractable set typ-
ically makes stronger independence assumptions be-
tween model parameters than the model itself. To in-
corporate the constraints, we further restrict the set
to only include distributions that satisfy the specified
expectation constraints over hidden variables.
In general, for some given model, let ? denote
the entire set of model parameters and z and x de-
note the hidden structure and observations respec-
tively. We are interested in estimating the posterior
p(?, z | x). Variational inference transforms this
problem into an optimization problem where we try
to find a distribution q(?, z) from a restricted set Q
that minimizes the KL-divergence between q(?, z)
and p(?, z | x):
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x).
Rearranging the above yields:
log p(x) = KL(q(?, z) ? p(?, z | x)) + F ,
where F is defined as
F ?
?
q(?, z) log
p(?, z, x)
q(?, z)
d?dz. (1)
Thus F is a lower bound on likelihood. Maximizing
this lower bound is equivalent to minimizing the KL-
divergence between p(?, z | x) and q(?, z). To make
this maximization tractable we make a mean field
assumption that q belongs to a set Q of distributions
that factorize as follows:
q(?, z) = q(?)q(z).
We further constrain q to be from the subset of Q
that satisfies the expectation constraintEq[f(z)] ? b
where f is a deterministically computable function
of the hidden structures. In our model, for exam-
ple, f counts the dependency edges that are an in-
stance of one of the declaratively specified depen-
dency rules, while b is the proportion of the total
dependencies that we expect should fulfill this con-
straint.2
With the mean field factorization and the expec-
tation constraints in place, solving the maximization
of F in (1) separately for each factor yields the fol-
lowing updates:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[f(z)] ? b, (3)
where
q?(?) ? expEq(z)[log p(?, z, x)], (4)
q?(z) ? expEq(?)[log p(?, z, x)]. (5)
We can solve (2) by setting q(?) to q?(?) ? since
q(z) is held fixed while updating q(?), the expecta-
tion function of the constraint remains constant dur-
ing this update. As shown by Grac?a et al (2007), the
update in (3) is a constrained optimization problem
and can be solved by performing gradient search on
its dual:
argmin
?
?>b + log
?
z
q?(z) exp(??>f(z)) (6)
For a fixed value of ? the optimal q(z) ?
q?(z) exp(??>f(z)). By updating q(?) and q(z)
as in (2) and (3) we are effectively maximizing the
lower bound F .
2Constraints of the form Eq[f(z)] ? b are easily imposed
by negating f(z) and b.
1238
4.1 Variational Updates
We now derive the specific variational updates for
our dependency induction model. First we assume
the following mean-field factorization of our varia-
tional distribution:
q(?, ?, pi, ?, z)
= q(z) ?
?
s?
q(?s?) ?
T?
z?=1
q(?s?z?)?
?
c
q(?s?z?c) ?
?
s
q(piss?z?c), (7)
where s? varies over the set of unique symbols in the
observed tags, z? denotes subsymbols for each sym-
bol, c varies over context values comprising a pair
of direction (left or right) and valence (first, second,
or third or higher) values, and s corresponds to child
symbols.
We restrict q(?s?z?c) and q(?s?z?) to be Dirichlet
distributions and q(z) to be multinomial. As with
prior work (Liang et al, 2009b), we assume a de-
generate q(?) ? ???(?) for tractability reasons, i.e.,
all mass is concentrated on some single ??. We also
assume that the top level stick-breaking distribution
is truncated at T , i.e., q(?) assigns zero probability
to integers greater than T . Because of the truncation
of ?, we can approximate q(piss?z?c) with an asym-
metric finite dimensional Dirichlet.
The factors are updated one at a time holding all
other factors fixed. The variational update for q(pi)
is given by:
q(piss?z?c) = Dir
(
piss?z?c;?? + Eq(z)[Css?z?c(z)]
)
,
where term Eq(z)[Css?z?c(z)] is the expected count
w.r.t. q(z) of child symbol s and subsymbol z in
context c when generated by parent symbol s? and
subsymbol z?.
Similarly, the updates for q(?) and q(?) are given
by:
q(?s?z?c) = Dir
(
?s?z?c; ?0 + Eq(z)[Cs?z?c(s)]
)
,
q(?s?z?) = Dir
(
?s?z? ;?0 + Eq(z)[Cs?z?(x)]
)
,
where Cs?z?c(s) is the count of child symbol s being
generated by the parent symbol s? and subsymbol z?
in context c and Cs?z?x is the count of word x being
generated by symbol s? and subsymbol z?.
The only factor affected by the expectation con-
straints is q(z). Recall from the previous section that
the update for q(z) is performed via gradient search
on the dual of a constrained minimization problem
of the form:
q(z) = argmin
q(z)
KL(q(z) ? q?(z)).
Thus we first compute the update for q?(z):
q?(z) ?
N?
n=1
len(n)?
j=1
(expEq(?)[log ?snjznj (xnj)]
? expEq(?)[log ?sh(nj)zh(nj)cnj (snj)]
? expEq(pi)[log pisnjsh(nj)zh(nj)cnj (znj)]),
where N is the total number of sentences, len(n)
is the length of sentence n, and index h(nj) refers
to the head of the jth node of sentence n. Given
this q?(z) a gradient search is performed using (6) to
find the optimal ? and thus the primal solution for
updating q(z).
Finally, we update the degenerate factor q(?s)
with the projected gradient search algorithm used
by Liang et al (2009b).
5 Linguistic Constraints
Universal Dependency Rules We compile a set of
13 universal dependency rules consistent with vari-
ous linguistic accounts (Carnie, 2002; Newmeyer,
2005), shown in Table 1. These rules are defined
over coarse part-of-speech tags: Noun, Verb, Adjec-
tive, Adverb, Pronoun, Article, Auxiliary, Preposi-
tion, Numeral and Conjunction. Each rule specifies
a part-of-speech for the head and argument but does
not provide ordering information.
We require that a minimum proportion of the pos-
terior dependencies be instances of these rules in ex-
pectation. In contrast to prior work on rule-driven
dependency induction (Druck et al, 2009), where
each rule has a separately specified expectation, we
only set a single minimum expectation for the pro-
portion of all dependencies that must match one of
the rules. This setup is more relevant for learn-
ing with universals since individual rule frequencies
vary greatly between languages.
1239
1. Identify non-recursive NPs:
? All nouns, pronouns and possessive
marker are part of an NP.
? All adjectives, conjunctions and deter-
miners immediately preceding an NP
are part of the NP.
2. The first verb or modal in the sentence is the
headword.
3. All words in an NP are headed by the last
word in the NP.
4. The last word in an NP is headed by the
word immediately before the NP if it is a
preposition, otherwise it is headed by the
headword of the sentence if the NP is be-
fore the headword, else it is headed by the
word preceding the NP.
5. For the first word set its head to be the head-
word of the sentence. For each other word
set its headword to be the previous word.
Table 3: English-specific dependency rules.
English-specific Dependency Rules For English,
we also consider a small set of hand-crafted depen-
dency rules designed by Michael Collins3 for deter-
ministic parsing, shown in Table 3. Unlike the uni-
versals from Table 1, these rules alone are enough to
construct a full dependency tree. Thus they allow us
to judge whether the model is able to improve upon
a human-engineered deterministic parser. Moreover,
with this dataset we can assess the additional benefit
of using rules tailored to an individual language as
opposed to universal rules.
6 Experimental Setup
Datasets and Evaluation We test the effective-
ness of our grammar induction approach on English,
Danish, Portuguese, Slovene, Spanish, and Swedish.
For English we use the Penn Treebank (Marcus et
al., 1993), transformed from CFG parses into depen-
3Personal communication.
dencies with the Collins head finding rules (Collins,
1999); for the other languages we use data from the
2006 CoNLL-X Shared Task (Buchholz and Marsi,
2006). Each dataset provides manually annotated
part-of-speech tags that are used for both training
and testing. For comparison purposes with previ-
ous work, we limit the cross-lingual experiments to
sentences of length 10 or less (not counting punc-
tuation). For English, we also explore sentences of
length up to 20.
The final output metric is directed dependency ac-
curacy. This is computed based on the Viterbi parses
produced using the final unnormalized variational
distribution q(z) over dependency structures.
Hyperparameters and Training Regimes Un-
less otherwise stated, in experiments with rule-based
constraints the expected proportion of dependencies
that must satisfy those constraints is set to 0.8. This
threshold value was chosen based on minimal tun-
ing on a single language and ruleset (English with
universal rules) and carried over to each other ex-
perimental condition. A more detailed discussion of
the threshold?s empirical impact is presented in Sec-
tion 7.1.
Variational approximations to the HDP are trun-
cated at 10. All hyperparameter values are fixed to 1
except ? which is fixed to 10.
We also conduct a set of No-Split experiments to
evaluate the importance of syntactic refinement; in
these experiments each coarse symbol corresponds
to only one refined symbol. This is easily effected
during inference by setting the HDP variational ap-
proximation truncation level to one.
For each experiment we run 50 iterations of vari-
ational updates; for each iteration we perform five
steps of gradient search to compute the update for
the variational distribution q(z) over dependency
structures.
7 Results
In the following section we present our primary
cross-lingual results using universal rules (Sec-
tion 7.1) before performing a more in-depth analysis
of model properties such as sensitivity to ruleset se-
lection and inference stability (Section 7.2).
1240
DMV PGI No-Split HDP-DEP
English 47.1 62.3 71.5 71.9 (0.3)
Danish 33.5 41.6 48.8 51.9 (1.6)
Portuguese 38.5 63.0 54.0 71.5 (0.5)
Slovene 38.5 48.4 50.6 50.9 (5.5)
Spanish 28.0 58.4 64.8 67.2 (0.4)
Swedish 45.3 58.3 63.3 62.1 (0.5)
Table 4: Directed dependency accuracy using our model
with universal dependency rules (No-Split and HDP-
DEP), compared to DMV (Klein andManning, 2004) and
PGI (Berg-Kirkpatrick and Klein, 2010). The DMV re-
sults are taken from Berg-Kirkpatrick and Klein (2010).
Bold numbers indicate the best result for each language.
For the full model, the standard deviation in performance
over five runs is indicated in parentheses.
7.1 Main Cross-Lingual Results
Table 4 shows the performance of both our full
model (HDP-DEP) and its No-Split version using
universal dependency rules across six languages.
We also provide the performance of two baselines?
the dependency model with valence (DMV) (Klein
and Manning, 2004) and the phylogenetic grammar
induction (PGI) model (Berg-Kirkpatrick and Klein,
2010).
HDP-DEP outperforms both DMV and PGI
across all six languages. Against DMV we achieve
an average absolute improvement of 24.1%. This
improvement is expected given that DMV does not
have access to the additional information provided
through the universal rules. PGI is more relevant
as a point of comparison, since it is able to lever-
age multilingual data to learn information similar to
what we have declaratively specified using universal
rules. Specifically, PGI reduces induction ambigu-
ity by connecting language-specific parameters via
phylogenetic priors. We find, however, that we out-
perform PGI by an average margin of 7.2%, demon-
strating the benefits of explicit rule specification.
An additional point of comparison is the lexi-
calized unsupervised parser of Headden III et al
(2009), which yields the current state-of-the-art un-
supervised accuracy on English at 68.8%. Our
method also outperforms this approach, without em-
ploying lexicalization and sophisticated smoothing
as they do. This result suggests that combining the
complementary strengths of their approach and ours
English
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 61.0 10.9 5.1
Verb ? Noun 61.4 10.5 14.8
Noun ? Noun 64.4 7.5 10.7
Noun ? Article 64.7 7.2 8.5
Spanish
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 53.4 13.8 8.2
Verb ? Noun 61.9 5.4 12.9
Noun ? Noun 62.6 4.7 2.0
Root ? Verb 65.4 1.8 12.3
Table 5: Ablation experiment results for universal depen-
dency rules on English and Spanish. For each rule, we
evaluate the model using the ruleset excluding that rule,
and list the most significant rules for each language. The
second last column is the absolute loss in performance
compared to the setting where all rules are available. The
last column shows the percentage of the gold dependen-
cies that satisfy the rule.
can yield further performance improvements.
Table 4 also shows the No-Split results where syn-
tactic categories are not refined. We find that such
refinement usually proves to be beneficial, yielding
an average performance gain of 3.7%. However, we
note that the impact of incorporating splitting varies
significantly across languages. Further understand-
ing of this connection is an area of future research.
Finally, we note that our model exhibits low vari-
ance for most languages. This result attests to how
the expectation constraints consistently guide infer-
ence toward high-accuracy areas of the search space.
Ablation Analysis Our next experiment seeks to
understand the relative importance of the various
universal rules from Table 1. We study how accu-
racy is affected when each of the rules is removed
one at a time for English and Spanish. Table 5 lists
the rules with the greatest impact on performance
when removed. We note the high overlap between
the most significant rules for English and Spanish.
We also observe that the relationship between
a rule?s frequency and its importance for high ac-
curacy is not straightforward. For example, the
?Preposition ? Noun? rule, whose removal de-
grades accuracy the most for both English and Span-
1241
50?
55?
60?
65?
70?
75?
Gold? 70? 75? 80? 85? 90?
Ac
cu
rac
y?
Constraints?Threshold?
Average? English?
Figure 2: Accuracy of our model with different threshold
settings, on English only and averaged over all languages.
?Gold? refers to the setting where each language?s thresh-
old is set independently to the proportion of gold depen-
dencies satisfying the rules ? for English this proportion
is 70%, while the average proportion across languages is
63%.
ish, is not the most frequent rule in either language.
This result suggests that some rules are harder to
learn than others regardless of their frequency, so
their presence in the specified ruleset yields stronger
performance gains.
Varying the Constraint Threshold In our main
experiments we require that at least 80% of the ex-
pected dependencies satisfy the rule constraints. We
arrived at this threshold by tuning on the basis of En-
glish only. As shown in Figure 2, for English a broad
band of threshold values from 75% to 90% yields re-
sults within 2.5% of each other, with a slight peak at
80%.
To further study the sensitivity of our method to
how the threshold is set, we perform post hoc ex-
periments with other threshold values on each of the
other languages. As Figure 2 also shows, on average
a value of 80% is optimal across languages, though
again accuracy is stable within 2.5% between thresh-
olds of 75% to 90%. These results demonstrate that
a single threshold is broadly applicable across lan-
guages.
Interestingly, setting the threshold value indepen-
dently for each language to its ?true? proportion
based on the gold dependencies (denoted as the
?Gold? case in Figure 2) does not achieve optimal
Length
? 10 ? 20
Universal Dependency Rules
1 HDP-DEP 71.9 50.4
No Rules (Random Init)
2 HDP-DEP 24.9 24.4
3 Headden III et al (2009) 68.8 -
English-Specific Parsing Rules
4 Deterministic (rules only) 70.0 62.6
5 HDP-DEP 73.8 66.1
Druck et al (2009) Rules
6 Druck et al (2009) 61.3 -
7 HDP-DEP 64.9 42.2
Table 6: Directed accuracy of our model (HDP-DEP) on
sentences of length 10 or less and 20 or less from WSJ
with different rulesets and with no rules, along with vari-
ous baselines from the literature. Entries in this table are
numbered for ease of reference in the text.
performance. Thus, knowledge of the true language-
specific rule proportions is not necessary for high
accuracy.
7.2 Analysis of Model Properties
We perform a set of additional experiments on En-
glish to gain further insight into HDP-DEP?s behav-
ior. Our choice of language is motivated by the
fact that a wide range of prior parsing algorithms
were developed for and tested exclusively on En-
glish. The experiments below demonstrate that 1)
universal rules alone are powerful, but language-
and dataset-tailored rules can further improve per-
formance; 2) our model learns jointly from the
rules and data, outperforming a rules-only deter-
ministic parser; 3) the way we incorporate posterior
constraints outperforms the generalized expectation
constraint framework; and 4) our model exhibits low
variance when seeded with different initializations.
These results are summarized in Table 6 and dis-
cussed in detail below; line numbers refer to entries
in Table 6. Each run of HDP-DEP below is with
syntactic refinement enabled.
Impact of Rules Selection We compare the per-
formance of HDP-DEP using the universal rules ver-
sus a set of rules designed for deterministically pars-
ing the Penn Treebank (see Section 5 for details).
1242
As lines 1 and 5 of Table 6 show, language-specific
rules yield better performance. For sentences of
length 10 or less, the difference between the two
rulesets is a relatively small 1.9%; for longer sen-
tences, however, the difference is a substantially
larger 15.7%. This is likely because longer sen-
tences tend to be more complex and thus exhibit
more language-idiosyncratic dependencies. Such
dependencies can be better captured by the refined
language-specific rules.
We also test model performance when no linguis-
tic rules are available, i.e., performing unconstrained
variational inference. The model performs substan-
tially worse (line 2), confirming that syntactic cat-
egory refinement in a fully unsupervised setup is
challenging.
Learning Beyond Provided Rules Since HDP-
DEP is provided with linguistic rules, a legitimate
question is whether it improves upon what the rules
encode, especially when the rules are complete and
language-specific. We can answer this question by
comparing the performance of our model seeded
with the English-specific rules against a determin-
istic parser that implements the same rules. Lines
4 and 5 of Table 6 demonstrate that the model out-
performs a rules-only deterministic parser by 3.8%
for sentences of length 10 or less and by 3.5% for
sentences of length 20 or less.
Comparison with Alternative Semi-supervised
Parser The dependency parser based on the gen-
eralized expectation criteria (Druck et al, 2009) is
the closest to our reported work in terms of tech-
nique. To compare the two, we run HDP-DEP using
the 20 rules given by Druck et al (2009). Our model
achieves an accuracy of 64.9% (line 7) compared to
61.3% (line 6) reported in their work. Note that we
do not rely on rule-specific expectation information
as they do, instead requiring only a single expecta-
tion constraint parameter.4
Model Stability It is commonly acknowledged
in the literature that unsupervised grammar induc-
tion methods exhibit sensitivity to initialization.
As in the previous section, we find that the pres-
4As explained in Section 5, having a single expectation pa-
rameter is motivated by our focus on parsing with universal
rules.
ence of linguistic rules greatly reduces this sensitiv-
ity: for HDP-DEP, the standard deviation over five
randomly initialized runs with the English-specific
rules is 1.5%, compared to 4.5% for the parser de-
veloped by Headden III et al (2009) and 8.0% for
DMV (Klein and Manning, 2004).
8 Conclusions
In this paper we demonstrated that syntactic uni-
versals encoded as declarative constraints improve
grammar induction. We formulated a generative
model for dependency structure that models syntac-
tic category refinement and biases inference to co-
here with the provided constraints. Our experiments
showed that encoding a compact, well-accepted set
of language-independent constraints significantly
improves accuracy on multiple languages compared
to the current state-of-the-art in unsupervised pars-
ing.
While our present work has yielded substantial
gains over previous unsupervised methods, a large
gap still remains between our method and fully su-
pervised techniques. In future work we intend to
study ways to bridge this gap by 1) incorporat-
ing more sophisticated linguistically-driven gram-
mar rulesets to guide induction, 2) lexicalizing the
model, and 3) combining our constraint-based ap-
proach with richer unsupervised models (e.g., Head-
den III et al (2009)) to benefit from their comple-
mentary strengths.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0904684,
and a Graduate Research Fellowship). We are es-
pecially grateful to Michael Collins for inspiring us
toward this line of inquiry and providing determin-
istic rules for English parsing. Thanks to Taylor
Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridha-
ran, and members of the MIT NLP group for their
suggestions and comments. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors, and do not necessar-
ily reflect the views of the funding organizations.
1243
References
Mark C. Baker. 2001. The Atoms of Language: The
Mind?s Hidden Rules of Grammar. Basic Books.
Emily M. Bender. 2009. Linguistically na??ve != lan-
guage independent: Why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics: Virtuous, Vicious or Vacuous?,
pages 26?32.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In Proceedings of ACL,
pages 1288?1297.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Information Science and Statis-
tics. Springer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Andrew Carnie. 2002. Syntax: A Generative Introduc-
tion (Introducing Linguistics). Blackwell Publishing.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL, pages 280?
287.
Shay B. Cohen and Noah A. Smith. 2009a. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL/HLT, pages 74?82.
Shay B. Cohen and Noah A. Smith. 2009b. Variational
inference for grammar induction with prior knowl-
edge. In Proceedings of ACL/IJCNLP 2009 Confer-
ence Short Papers, pages 1?4.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Hal Daume? III and Lyle Campbell. 2007. A bayesian
model for discovering typological implications. In
Proceedings of ACL, pages 65?72.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of ACL/IJCNLP, pages 360?368.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL/IJCNLP,
pages 369?377.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001?2049.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs. parameter sparsity in la-
tent variable models. In Advances in NIPS, pages 664?
672.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS, pages 569?576.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of ACL, pages
881?888.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of NAACL/HLT, pages 101?109.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 478?485.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of ACL, pages
470?477.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP/CoNLL, pages
688?697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009a.
Learning from measurements in exponential families.
In Proceedings of ICML, pages 641?648.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009b.
Probabilistic grammars and hierarchical Dirichlet pro-
cesses. The Handbook of Applied Bayesian Analysis.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Frederick J. Newmeyer. 2005. Possible and Probable
Languages: A Generative Perspective on Linguistic
Typology. Oxford University Press.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceeding of
AAAI, pages 1663?1666.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL/IJCNLP, pages 73?81.
Lydia White. 2003. Second Language Acquisition and
Universal Grammar. Cambridge University Press.
1244
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416?1425,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Reducing Grounded Learning Tasks to Grammatical Inference
Benjamin B?rschinger
Department of Computing
Macquarie University
Sydney, Australia
benjamin.borschinger@mq.edu.au
Bevan K. Jones
School of Informatics
University of Edinburgh
Edinburgh, UK
b.k.jones@sms.ed.ac.uk
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.au
Abstract
It is often assumed that ?grounded? learning
tasks are beyond the scope of grammatical in-
ference techniques. In this paper, we show
that the grounded task of learning a seman-
tic parser from ambiguous training data as dis-
cussed in Kim and Mooney (2010) can be re-
duced to a Probabilistic Context-Free Gram-
mar learning task in a way that gives state
of the art results. We further show that ad-
ditionally letting our model learn the lan-
guage?s canonical word order improves its
performance and leads to the highest seman-
tic parsing f-scores previously reported in the
literature.1
1 Introduction
One of the most fundamental ideas about language
is that we use it to express our thoughts. Learning a
natural language, then, amounts to (at least) learning
a mapping between the things we utter and the things
we think, and can therefore be seen as the task of
learning a semantic parser, i.e. something that maps
natural language expressions such as sentences into
meaning representations such as logical forms. Ob-
viously, this learning can neither take place in a fully
supervised nor in a fully unsupervised fashion: the
learner does not ?hear? the meanings of the sentences
she observes, but she is also not treating them as
merely meaningless strings. Rather, it seems plau-
sible to assume that she uses extra-linguistic context
1The source code used for our experiments and the evalua-
tion is available as supplementary material for this article.
to assign certain meanings to the linguistic input she
is confronted with.
In this sense, learning a semantic parser seems
to go beyond the well-studied task of unsupervised
grammar induction. It involves not only learning
a grammar for the form-side of language, i.e. lan-
guage expressions such as sentences, but also the
?grounding? of this structure in meaning represen-
tations. It requires going beyond the mere linguistic
input to incorporate, for example, perceptual infor-
mation that provides a clue to the meaning of the ob-
served forms. Essentially, it seems as if ?grounded?
learning tasks like this require dealing with two
different kinds of information, the purely formal
(phonemic) and meaningful (semantic) aspects of
language. Grammatical inference seems to be lim-
ited to dealing with one level of formal information
(Chang and Maia, 2001). For this reason, probably,
approaches to the task of learning a semantic parser
employ a variety of sophisticated and task-specific
techniques that go beyond (but often elaborate on)
the techniques used for grammatical inference (Lu
et al, 2008; Chen and Mooney, 2008; Liang et al,
2009; Kim and Mooney, 2010; Chen et al, 2010).
In this paper, we show that one can reduce the
task of learning a semantic parser to a Probabilistic
Context Free Grammar (PCFG) learning task, and
more generally, that grounded learning tasks are not
in principle beyond the scope of grammatical infer-
ence techniques. In particular, we show how to for-
mulate the task of learning a semantic parser as dis-
cussed by Chen, Kim and Mooney (2008, 2010) as
the task of learning a PCFG from strings. Our model
does not only constitute a proof of concept that this
1416
reduction is possible for certain cases, it also yields
highly competitive results.2
By reducing the problem to the well understood
PCFG formalism, it also becomes easy to consider
extensions, leading to our second contribution. We
demonstrate that a slight modification to our model
so that it also learns the language?s canonical word
order improves its performance even beyond the best
results previously reported in the literature. This
language-independent and linguistically well moti-
vated elaboration allows the model to learn a global
fact about the language?s syntax, its canonical word
order.
Our contribution is two-fold. We provide an illus-
tration of how to reduce grounded learning tasks to
grammatical inference. Secondly, we show that ex-
tending the model so that it can learn linguistically
well motivated generalizations such as the canonical
word order can lead to better results.
The structure of the paper is as follows. First we
give a short overview of the previous work by Chen,
Kim and Mooney and describe their dataset. Then,
we show how to reduce the parsing task addressed
by them to a PCFG-learning task. Finally, we ex-
plain how to let our model additionally learn the lan-
guage?s canonical word order.
2 Previous Work by Chen, Kim and
Mooney
In a series of recent papers, Chen, Kim and Mooney
approach the task of learning a semantic parser from
ambiguous training data (Chen and Mooney, 2008;
Kim and Mooney, 2010; Chen et al, 2010). This
goes beyond previous work on semantic parsing
such as Lu et al (2008) or Zettlemoyer and Collins
(2005) which rely on unambiguous training data
where every sentence is paired only with its mean-
ing. In contrast, Chen, Kim and Mooney allow
their training examples to exhibit the kind of uncer-
tainty about sentence meanings human learners are
likely to have to deal with by allowing for sentences
to be associated with a set of candidate-meanings,
2It has been pointed out to us by one reviewer that the task
we address falls short of what is often called ?grounded learn-
ing?. We acknowledge that semantic parsing constitutes a very
limited kind of grounded learning but want to point out that the
task has been introduced as an instance of grounded learning in
the previous literature such as Chen and Mooney (2008).
and the correct meaning might not even be in this
set. They create the training data by first collect-
ing humanly generated written language comments
on four different RoboCup games. The comments
are recorded with a time-stamp and then associated
with all game events automatically extracted from
the games which occured up to five seconds before
the comment was made. This leads to an ambigu-
ous pairing of comments with candidate meanings
that can be considered similar to the "linguistic in-
put in the context of a rich, relevant, perceptual en-
vironment" to which real language learners prob-
ably have access (Chen and Mooney, 2008). For
evaluation purposes, they manually create a gold-
standard which contains unambiguous natural lan-
guage comment / event pairs. Due to the fact
that some comments refer to events not detected
by their extraction-algorithm, not every natural lan-
guage sentence has a gold matching meaning repre-
sentation. In addition to the inherent ambiguity of
the training examples, the learner therefore has to
somehow deal with those examples which only have
?wrong? meanings associated with them.
Datasets exist for both Korean and English, each
comprising training and gold data for four games.3
Some details about this data are given in Table 1,
such as the number of examples, their average am-
biguity and the number of misleading examples.
For the following short discussion of previous ap-
proaches, we mainly focus on Kim and Mooney
(2010). This is the most recent publication and re-
ports the highest scores.
2.1 The parsing task
Learning a semantic parser from the ambiguous data
is, in fact, just one of three tasks discussed by Kim
and Mooney (2010), henceforth KM. In addition to
parsing, they discuss matching and natural language
generation. We are ignoring the generation task as
we are currently only interested in the parsing prob-
lem, and we treat the matching task, picking the cor-
rect meaning from the set of candidates, merely as
a byproduct of parsing, rather than as a completely
separate task: parsing implicitly requires the model
to disambiguate the data it is learning from.
3The datasets are freely available at http://www.cs.
utexas.edu/~ml/clamp/sportscasting/. We re-
trieved the data used here on March 29th, 2011.
1417
Number of comments Ambiguity
# Training # Training with
Gold Match
# Training with
correct MR
# Gold Noise Avg. # of MRs
English dataset
total 1872 1492 1360 1539 0.2735 2.20
Korean dataset
total 1914 1763 1733 1763 0.0946 2.39
Table 1: Statistics for the Korean and the English datasets. The numbers are basically identical to those reported in
Chen et al (2010) except for minimal differences in the number of training examples (we give one more for every
English training set, and one more for the 2004 Korean training set). In addition, our calculation of the average
sentential ambiguity (Avg. # of MRs) differs because we assume that mutiple occurences of the same event in a
context do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples without
the correct meaning in their context) takes into account that there are training examples which do not have their gold
meaning associated with them in the training data and is therefore slightly higher than the one reported in Chen et al
(2010).
KM?s model builds on previous work by Lu et al
(2008) and is a generative model which defines a
joint probability distribution over natural language
sentences (NLs), meaning representations (MRs)
and hybrid trees. The NLs are the natural language
comments to the games, the MRs are simple log-
ical formulae describing game events and playing
the role of sentence meanings, and a hybrid tree is
a tree structure that represents the correspondence
between a sentence and its meaning. More specif-
ically, if some NL W has as its meaning an MR
m, and m has been generated by a meaning gram-
mar (MG) G, the hybrid tree corresponding to the
pair ?W,m? has as its internal nodes those rules of
G used in the derivation of m, and as its leaves the
words making up W.4 An example hybrid tree for
the pair ?THE PINK GOALIE PASSES THE BALL TO
PINK11,pass(pink1,pink11)? is given in Figure 1.
Their model is trained by a variant of the Inside-
Outside algorithm which deals with the hybrid tree
structure and takes into account the ambiguity of the
training examples.
In addition to learning directly from the ambigu-
ous training data, they also train a semantic parser
in a supervised fashion on data that has been pre-
viously disambiguated by their matching model.
This slightly improves their system?s performance.
Consequently, there are two scores for each of the
4We use SMALL CAPS for words, sans serif for MRs and
MR constituents (concepts), and italics for non-terminals and
Grammars.
S
S? pass PLAYER PLAYER
PLAYER
PLAYER? pink11
PINK11
PASSES THE BALL TOPLAYER
PLAYER? pink1
THE PINK GOALIE
Figure 1: A hybrid tree for the sentence-meaning
pair ?THE PINK GOALIE PASSES THE BALL TO
PINK11,pass(pink1,pink11)? . The internal nodes cor-
respond to the rules used to derive pass(pink1,pink11)
from a given Meaning Grammar, and the leaves corre-
spond to the words or substrings that make up the sen-
tence.
two languages (English and Korean) with which
we compare our own model: those of the parsers
trained directly from the ambiguous data, and those
of the ?supervised? parsers which constitute the cur-
rent state of the art. The details of their evaluation
method are disccused in Section 3.3, and their scores
are given in Table 2, together with our own scores.
3 Learning a Semantic Parser as a
PCFG-learning problem
Given that one can effectively represent both a sen-
tence?s form and its meaning in a hybrid tree, it is in-
teresting to ask whether one can do with a structure
that can be learned by grammatical inference tech-
1418
niques from strings which incorporate the contextual
information. In this section, we show how to reduce
hybrid trees to such ?standard? trees. In effect, we
show via construction that ?grounded? learning tasks
such as learning a semantic parser from semantically
enriched and ambiguous data can be reduced to ?un-
grounded? tasks such as grammatical inference.
Instead of taking the internal nodes of the trees
generated by our model as corresponding to MG
production rules, we take them to correspond to MR
constituents. The MR pass(pink1,pink11), for exam-
ple, has 4 constituents: the whole MR, the predicate
pass, and the two arguments pink1 and pink11. Fig-
ure 2 gives the tree we assume instead of Figure 1
for the sentence-meaning pair ?THE PINK GOALIE
PASSES THE BALL TO PINK11,pass(pink1,pink11)?.
Its root is assumed to correspond to the whole
MR and is labeled Spass(pink1,pink11). The remain-
ing three MR constituents correspond to the root?s
daughters which we label Phrasepink1, Phrasepass
and Phrasepink11. Generally speaking, we assume a
special non-terminal Sm for every MR m generated
by the MG, and a special non-terminal Phrasecon for
each of the terminals of the MG (which loosely cor-
respond to concepts). This is only possible for MGs
which create a finite set of MRs, but the MG used by
Kim and Mooney (2010) obeys this restriction.5
The tree?s terminals are the words that make up
the sentence, and we assume them to be dominated
by concept-specific pre-terminals Wordcon which
correspond to concept-specific probability distribu-
tions over the language?s vocabulary. Since each
Phrasecon may span multiple words, we give trees
rooted in Phrasecon a left-recursive structure that
corresponds to a unigram Markov-process. This
process generates an arbitrary sequence of words
semantically related to con, dominated by the cor-
responding pre-terminal Wordcon in our model, and
words not directly semantically related to con, dom-
inated by a special word pre-terminal Word?. The
sole further restriction is that every Phrasecon must
contain at least one Wordcon.
Trees like the one in Figure 2 can be generated by
a Context-Free Grammar (CFG) which, in turn, can
be trained on strings to yield a PCFG which embod-
5This grammar is given in the Appendix to Chen et al
(2010) and generates a total of 2048 MRs.
ies a semantic parser as will be discussed in Section
3.3. We now describe how to set up such a CFG in a
systematic way and how to train it on the data used
by KM.
3.1 Setting up the PCFG
The training data expresses information of two dif-
ferent kinds ? form and meaning. Every training ex-
ample consists of a natural language string (the for-
mal information) and a set of candidate meanings
for the string (the semantic information, its context),
allowing for the possibility that none of the mean-
ings in the context is the correct one. In order to
learn from data like this within a grammatical in-
ference framework, we have to encode the semantic
information as part of the string. Assigning a spe-
cific MR m to a string corresponds, in our frame-
work, to analyzing it as a tree with Sm as its root.
A sentence?s context constrains which of the many
possible meanings might be expressed by the string.
Thus the role played by the context is adequately
modelled if we ensure that if a string W is associated
with a context {m1,...,mn}, the model only considers
the possibilities that this string might be analyzed as
Sm1,...,Smn.There are 959 different contexts, i.e. 959 dif-
ferent sets of MRs, in the English data set (984
for the Korean data), and we therefore introduce
959 new terminal symbols which play the role of
context-identifiers, for example C1 to C959.6 For-
mally speaking, a context-identifier is a terminal
like any other word of the language and we can
therefore prefix every comment in the training data
with the context-identifier standing for the set of
MRs associated with this comment, an idea taken
from previous work such as Johnson et al (2010).
Thus having incorporated the contextual informa-
tion into the string, we go on to show how our model
makes use of this information, considering the MR
pass(pink1,pink11) as an example. A formal de-
scription of the model is given Figure 3.
Assume that pass(pink1,pink11) is associated
with only one training example and therefore occurs
only in one specific context. If the context-identifier
introduced for this context is C1, we require the
6If we were to consider every possible context, we would
have to consider 22048 contexts because the MG generates 2048
MRs.
1419
Root
Spass(pink1,pink11)
Phrasepink11
PINK11
Phrasepass
Wordpass
TO
PhXpass
Word?
BALL
PhXpass
Word?
THE
PhXpass
Wordpass
PASSES
Phrasepink1
THE PINK GOALIE
C76
Figure 2: The tree-structure we propose instead of the Hybrid Tree structure used by (Kim and Mooney, 2010). The
non-terminal nodes do not correspond to MG productions, but to MR constituents. The internal structure of the
Phrasecon constituents, shown in full detail for Phrasepass, corresponds to a Markov process that generates the wordsthat make up the sentence. The terminal C76 is a context-identifier that restricts the range of Sm non-terminals thatmight dominate the sentence and is only used during training, as described in Section 3.1. The grammar that generates
this trees is described in Figure 3.
right-hand side of all rules with Spass(pink1,pink11) on
their left-hand side to begin with C1. More gener-
ally, if an MR m occurs in the contexts associated
with the context-identifiers CK,...,CL, we require the
right-hand side of all rules with Sm on their left-hand
side to begin with exactly one of CK,...,CL.
In this sense, the context-identifiers can be seen
as providing the model with a top-down constraint
? if it encounters a context-identifier, it can only
try analyses leading to MRs which are licensed by
this context-identifier. On the other hand, the words
have to be generated by concept-specific word-
distributions, and the concepts that are present re-
strict the range of possible Sm non-terminals which
might dominate the whole string. In this sense, the
words the model observes provide it with a bottom-
up constraint ? if it sees words which are semanti-
cally related to certain concepts con1,...,conn, it has
to arrive at an MR which licenses the presence of the
corresponding Phraseconx non-terminals. Of course,the model has to also learn which words are seman-
tically related to which concepts. To enable it to do
this, our grammar allows every Wordx non-terminal
to be rewritten as every word of the language.
Since there are sentences in the training data with-
out the correct meaning in their context, we want
to give our model the possibility of not assigning to
a sentence any of the MRs licensed by its context-
identifier. To do this, we employ another trick of
previous work by Johnson et. al and assume a spe-
cial null meaning ? to be present in every context.
S? may only span words generated by Word?, the
language-specific distribution for words not directly
related to any concept; this also has to be learned by
the model.
As a last complication, we deal with the fact that
syntactic constituents are linearized with respect to
each other. For example, if an MR has 3 proper con-
stituents (i.e. excluding the MR itself), our grammar
allows the corresponding 3 syntactic constituents ?
which we might label Phrasepredicate, Phrasearg1
and Phrasearg2 ? to occur in any of the 6 possible
orders. Therefore, we have an Sm rule for every con-
text in which m occurs and for every possible order
of the proper constituents of m.
A formally explicit description of the rule
1420
schemata used to generate the CFG is given in Fig-
ure 3.7 Instantiating all those schemata leads to a
grammar with 33,101 rules for the English data and
30,731 rules for the Korean data. The difference in
size is due to differences in the size of the vocabu-
lary and the different number of contexts in the data
sets.
These CFGs can now be trained on the training
data using the Inside-Outside algorithm (Lari and
Young, 1990). After training, the resulting PCFG
embodies a semantic parser in the sense that, with
a slight modification we describe in section 3.3, it
can be used to parse a string into its meaning rep-
resentation by determining the most likely syntactic
analysis and reading off the meaning assigned by our
model at the Sm-node.
3.2 Possible objections to our reduction
Before we go on to discuss the details of training
and evaluation of our model, we want to address an
objection that might seem tempting. Isn?t our reduc-
tion impractical and unrealistic as even a highly ab-
stract model of language learning ? after all, setting
up the huge CFG requires knowledge about the vo-
cabulary, the MG and all the complicated rules dis-
cussed which, presumably, is more knowledge than
we want to provide a language learner with, lest we
trivialize the task. To this we reply firstly, that it is
true that our reduction only works for offline or batch
grounded learning tasks where all the data is avail-
able to the model before the actual learning begins
so that it ?knows? the words, the meanings and the
contexts present in the data. This offline constraint
is, however, true of all models which are trained by
iterating multiple times over training data such as
KM?s model. Secondly, the intimidating CFG can in
principle be reduced to a hand-full of intuitive prin-
ciples and is easy to generate automatically.
First of all, the many specific Sm-rewrite rules re-
duce to the heuristic that every semantic constituent
should correspond to a syntactic constituent, and the
fact that natural language expressions are linearly or-
dered. Note that our model does not contain knowl-
edge about the specific word order of the language.
7In our description, we use context-identifiers such as C1with a systematic ambiguity, letting them stand for the terminal
symbol representing a context and, in contexts such as m?C1,for the represented context itself.
It simply allows for the constituents of an MR to oc-
cur in every possible order which is a very unbiased
and empiricist assumption. Of course, this leads to
some limited kind of ?implicit learning? of word or-
der in the sense that for every meaning and for every
context, our model might (and in most cases will) as-
sign different probabilities to the different rules for
every word order; so it can learn that certain specific
MRs such as pass(pink1,pink11) are more often lin-
earized in one way than in any other. It cannot, how-
ever, generalize this to other (or even unseen) MRs,
i.e. it does not learn a global fact about the language.
In a way, it lacks the knowledge that there is such a
thing as word order, a point which we will elaborate
on in Section 4.
The many re-write rules for the pre-terminal
Wordxs are nothing but an explicit version of the
assumption that every word the model encounters
might, in principle, be semantically related to every
concept it knows. Again, this seems to us to be a
reasonable assumption.
Finally, the complicated looking set of rules for
the internal structure of Phrasexs corresponds to
a simple unigram Markov-process for generating
strings. All in all, we do not see that we make any
more assumptions than other approaches; our for-
mulation may make explicit how rich those assump-
tions are but we have not qualitatively changed them.
3.3 Training and Evaluation
The CFG described in the previous section is trained
on the same training data used by KM, except that
we reduce it to strings (without changing the infor-
mation present in the original data) by prefixing ev-
ery sentence with a context-identifier. For training
we run the Inside-Outside algorithm8 with uniform
initialization weights until convergence. For En-
glish, this results in an average number of 76 itera-
tions for each fold, for Korean the average number of
iterations is 50. To deal with the fact that the model
might not observe certain meanings during training,
we apply a simple smoothing technique by using a
Dirichlet prior of ?=0.1 on the rule probabilities. In
effect, this provides our system with a small number
of pseudo-observations for each rule which prevents
8We use Mark Johnson?s freely available implementa-
tion, available at http://web.science.mq.edu.au/
~mjohnson/Software.htm.
1421
Root? Sm m ?M ? {?}
Sm ? c Phrasep(m) c ? C,m ? c,m ? Pred0(M)
Sm ? c {Phrasep(m), Phrasea1(m)} c ? C,m ? c,m ? Pred1(M)
Sm ? c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ? C,m ? c,m ? Pred2(M)
S? ? c Phrase? c ? C
Phrase? ?Word?
Phrase? ? Phrase?Word?
Phrasex ?Wordx x ? T
Phrasex ? PhXxWordx x ? T
Phrasex ? PhxWord? x ? T
PhXx ?Wordr x ? T, r ? {x, ?}
PhXx ? PhXxWordr x ? T, r ? {x, ?}
Phx ? PhXxWordx x ? T
Phx ? PhxWord? x ? T
Phx ?Wordx x ? T
Wordx ? v x ? T ? {?}, v ? V
Figure 3: The rule-schemata used to generate the NoWo-PCFG. Root is the unique start-symbol, M is the set of all
MRs present in the corpus, C is set the of all context-identifiers present in the corpus, T is the set of terminals of the
MG, V is the vocabulary of the corpus. Pred0(M) is the subset of all MRs in M of the form predicate, Pred1(M)
is the subset of all MRs in M of the form predicate(arg1) and Pred2(M) is the subset of all MRs in M of the form
predicate(arg1,arg2). p(m) is the predicate of the MR m, a1(m) is the first argument of the MR m, a2(m) is the
second argument of the MR m. The rules expanding Phrasex ensure that it contains at least one Wordx. A set on theright-hand side of a rule is shorthand for all possible orderings of the elements of the set.
the automatic assignment of zero probability to rules
not used during training.9
For parsing, the resulting PCFG is slightly mod-
ified by removing the context-identifiers. This is
done because the task of a semantic parser is to es-
tablish a mapping between NLs and MRs, irrespec-
tive of contexts which were only used for learning
the parser and should not play a role in its final per-
formance. To do this, we add up the probability of
all rules which differ only in the context-identifier
which can be thought of as marginalizing out the dif-
ferent contexts, giving our first model which we call
NoWo-PCFG.10
Note that the context-deletion (and the simple
smoothing) enables NoWo-PCFG to parse sentences
into meanings not present in the data it was trained
on which, in fact, happens. For example, there are
81 meanings in the training data for the first English
9We experimented with ?=0.1, ?=0.5 and ?=1.0 and found
that overall, 0.1 yields the best results. We also tried jittering
the initial rule weights during training and found that our re-
sults are very robust and seem to be independent of a specific
initialization.
10NoWo because this model, unlike the one described in Sec-
tion 4, does not make explicit use of word order generalisa-
tions.
match that are not present in any of the other games?
training data. The PCFG trained on games 2, 3 and 4
is still able to correctly assign 12 of those 81 mean-
ings which it has not seen during the training phase
which shows the effectiveness of the bottom-up con-
straint.
For evaluation, we employ 4-fold cross validation
as described in detail in Chen and Mooney (2008)
and used by KM: the model is trained on all possible
combinations of 3 of the 4 games and is then used
to produce an MR for all sentences of the held-out
game for which there is a matching gold-standard
meaning. For an NL W, our model produces an MR
m by finding the most probable parse of W with the
CKY algorithm and reading off m at the Sm-node.11
An MR is considered correct if and only if it matches
the gold-standard MR exactly; the final evaluation
result is averaged over all 4 folds. Our evaluation
results for NoWo-PCFG are given in Table 2. All
scores are reported in F-measure which is the har-
monic mean of Precision and Recall. In this specific
case, precision is the fraction of correct parses out
11For parsing, we use Mark Johnson?s freely available CKY
implementation which can be downloaded at http://web.
science.mq.edu.au/~mjohnson/Software.htm.
1422
English Korean
KM 0.742 0.764
KM ?supervised? 0.810 0.808
Chen et al (2010) 0.801 0.812
NoWo-PCFG 0.742 0.718
WO-PCFG 0.860 0.829
Table 2: A summary of results for the parsing task, in F-
measure. We also show the results of Chen et al (2010),
as given in Kim and Mooney (2010), which to our knowl-
edge are the highest previously reported scores for Ko-
rean. WO-PCFG, described in Section 4 performs better
than all previously reported models, but only slightly so
for Korean.
of the total number of parses the model returns. Re-
call is the fraction of correct parses out of the total
number of test sentences.12
NoWo-PCFG performs a little worse than KM?s
model. Its scores are virtually identical for English
(0.742) and worse for Korean (0.718 vs 0.764). We
are not sure as to why our model performs worse on
the Korean data, but it might have to do with the fact
that the Korean average ambiguity is higher than for
the English data.
This shows that it is not only possible to re-
duce the task of learning a semantic parser to stan-
dard grammatical inference, but that this way of ap-
proaching the problem yields comparable results.
The remainder of the paper focuses on our second
main point: that letting the model learn additional
kinds of information, such as the language?s canoni-
cal word order, can further improve its performance.
In order to do this we propose a model that learns
the word order as well as the mapping from NLs
to MRs, and compare its performance to that of the
other models.
4 Extending NoWo-PCFG to WO-PCFG
We already pointed out that our model considers ev-
ery possible linear order of syntactic constituents.
Our NoWo-PCFG model considers each of the pos-
sible word orders for every meaning and context in
isolation: it is unable to infer from the fact that most
meanings it has observed are most likely to be ex-
pressed with a certain word order that new meanings
12Because our model parses every sentence, for it Recall and
Precision are identical and F-measure is identical to Accuracy.
it will encounter are also more likely to be expressed
with this word order. It seems, however, to be at
least a soft fact about languages that they do have
a canonical word order that is more likely to be re-
alized in its sentences than any other possible word
order. In order to test whether trying to learn this
order helps our model, we modify the CFG used for
NoWo-PCFG so it can learn word order generaliza-
tions, and train it in the same way to yield another
semantic parser, WO-PCFG.
4.1 Setting up WO-PCFG
For every possible ordering of the constituents cor-
responding to an MR, our grammar contains a rule.
In NoWo-PCFG, these different rules all share the
same parent which prevents the model from learn-
ing the probability of the different word orders cor-
responding to the many rules. A straight-forward
way to overcome this is to annotate every Sm node
with the word order of its daughter. We split every
Sm non-terminal in multiple Swo_m non-terminals,
where wo ? {v,sv,vs,svo,sov,osv,ovs,vso,vos} indi-
cates the linear order of the constituents the non-
terminal rewrites as.13
This in itself does not yet alow our model to use
word order as a means of generalization. To model
that whenever it encounters a specific example that
is indicative of a certain word order, this word or-
der becomes slightly more probable for every other
example as well, we have to make a further slight
change to the CFG which we now describe. A for-
mally explicit description of the necessary changes
which we go on to describe is given in Figure 4.
We introduce six new non-terminals, correspond-
ing to the six possible word orders SVO, SOV, VSO,
VOS, OSV and OVS and require every Swo_m non-
terminal to be dominated by the non-terminal com-
patible with its daughters linear order. As an exam-
ple, consider the two syntactic non-terminals cor-
responding to the MR kick(pink1), Svs_kick(pink11)
and Ssv_kick(pink11). Whenever an example is suc-
cessfully analyzed as Svs_kick(pink11), this should
strengthen our model?s expectation of encountering
13We assume, somewhat simplifying, that an MR?s predicate
corresponds to a V(erb), its first argument corresponds to the
S(ubject) and its second argument corresponds to the O(bject).
These are purely formal categories that are not constrained to
correspond to specific linguistic categories.
1423
Root? wo wo ?WO
wo? Sx_m wo ?WO,x ?WOS, x ? wo,m ?M
Sv_m ? c Phrasep(m) c ? C,m ? c,m ? Pred0(M)
Sx_m ? c {Phrasep(m), Phrasea1(m)} c ? C,m ? c,m ? Pred1(M), x ? {sv, vs}
Sx_m ? c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ? C,m ? c,m ? Pred2(M), x ?WOS
Sv_? ? c Phrase? c ? C
Figure 4: In order to turn NoWo-PCFG described in Figure 3 into the WO-PCFG described in the
text, substitute the first five rule-schemata with the schemata given here. WO is the set of word
order non-terminals {SV O, SOV,OSV,OV S, V SO, V OS}, WOS is the set of word order annotations
{v, sv, vs, svo, svo, ovs, osv, vso, vos}. We take x ? wo to mean that x is compatible with wo, where v is com-
patible with all word orders, sv is compatible with SVO,SOV and OSV, and so on. For rule-schemata 4 and 5, the
choice of x determines the order of the elements of the set on the right-hand side. All other symbols have the same
meaning as explained in Figure 3.
more examples where the verb precedes the sub-
ject, i.e. of the language being pre-dominantly VSO,
VOS or OVS. Therefore, we allow VSO, VOS and
OVS to be rewritten as Svs_kick(pink11). More gener-
ally, every word order non-terminal can rewrite as
any of the Swo_m non-terminals that are compatible
with it. Adding this additional layer of word order
abstraction leads to a grammar with 36,019 rules for
English and a grammar with 33,715 rules for Ko-
rean.
4.2 Evaluation of WO-PCFG
Training and evaluating WO-PCFG in exactly the
same way as the previous grammar gives an F-
measure of 0.860 for English and an F-measure of
0.829 for Korean. Those scores are, to our knowl-
edge, the highest scores previously reported for this
parsing task and establish our second main point:
letting the model learn the language?s word order in
addition to learning the mapping from sentences to
MR increases semantic parsing accuracy.14
An intuitive explanation for the increase in perfor-
mance is that by allowing the model to learn word
order, we are providing it with a new dimension
along which it can generalize.
In this sense, we can look at our refinement as
providing the model with abstract linguistic knowl-
edge, namely that languages tend to have a canon-
14Liang et al (2009)?s model can be seen as capturing some-
thing similar to our word order generalization with the help of
a Field Choice Model which primarily captures discourse co-
herence and salience properties. It differs, however, in that it
can only learn one generalization for each predicate type and
no language wide generalization.
ical word order. The usefulness of this kind of in-
formation is impressive ? for English, it improves
the accuracy of semantic parsing by almost 12% in
F-measure and for Korean by 11.1%. In addition,
our model correctly learns that English?s predomi-
nant word order is SVO and that Korean is predomi-
nantly SOV, assigning by far the highest probability
to the corresponding Root rewrite rule (0.91 for En-
glish and 0.98 for Korean). This kind of information
is useful in its own right and could, for example, be
exploited by coupling word order with other linguis-
tic properties, perhaps following Greenberg (1966)?s
implicational universals.
In this sense, the reduction of grounded learning
problems to grammatical inference does not only
make possible the application of a wide variety of
tools and insights developed over years of research,
it might also make it easier to bring abstract (and not
so abstract) linguistic knowledge to bear on those
tasks.
The overall slightly worse performance of our
system on Korean data might stem from the fact that
Korean, unlike English, has a rich morphology, and
that our model does not learn anything about mor-
phology at all. We plan on further investigating ef-
fects like this in the future, as well as applying more
advanced grammatical inference algorithms.
5 Conclusion and Future Work
We have shown that certain grounded learning tasks
such as learning a semantic parser from semantically
enriched training data can be reduced to a gram-
matical inference problem over strings. This allows
1424
for the application of techniques and insights devel-
oped for grammatical inference to grounded learn-
ing tasks. In addition, we have shown that letting
the model learn the language?s canonical word or-
der improves parsing performance, beyond the top
scores previously reported, thus illustrating the use-
fullnes of linguistic knowledge for tasks like this.
In future research, we plan to address the limi-
tation of our model to a finite set of meaning rep-
resentations, in particular through the use of non-
parametric Bayesian models such as the Infinite
PCFG model of Liang et al (2007) and the Infi-
nite Tree model of Finkel et al (2007); both allow
for a potentially infinite set of non-terminals, hence
directly addressing this problem. In addition, we
are thinking about using an extension of the PCFG
formalism that allows for some kind of ?feature-
passing? which could lead to much smaller and more
general grammars.
References
N. C. Chang and T. V. Maia. 2001. Grounded learning
of grammatical constructions. In 2001 AAAI Spring
Symposium on Learning Grounded Representations.
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: A test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine Learning (ICML).
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Jenny R. Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272?279.
Joseph H. Greenberg. 1966. Some universals of gram-
mar with particular reference to the order of meaning-
ful elements. In Joseph H. Greenberg, editor, Univer-
sals of Language, chapter 5, pages 73?113. The MIT
Press, Cambridge, Massachusetts.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018?1026.
Joohyun Kim and Raymond J. Mooney. 2010. Genera-
tive alignment and semantic parsing for learning from
ambiguous supervision. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010).
K. Lari and S.J. Young. 1990. The estimation of Stochas-
tic Context-Free Grammars using the Inside-Outside
algorithm. Computer Speech and Language, 4(35-56).
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natu-
ral language to meaning representations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 783?792.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI 2005.
1425
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 699?709, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Adaptor Grammars for Native Language Identification
Sze-Meng Jojo Wong Mark Dras Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{sze.wong,mark.dras,mark.johnson}@mq.edu.au
Abstract
The task of inferring the native language of
an author based on texts written in a second
language has generally been tackled as a clas-
sification problem, typically using as features
a mix of n-grams over characters and part of
speech tags (for small and fixed n) and un-
igram function words. To capture arbitrar-
ily long n-grams that syntax-based approaches
have suggested are useful, adaptor grammars
have some promise. In this work we investi-
gate their extension to identifying n-gram col-
locations of arbitrary length over a mix of PoS
tags and words, using both maxent and in-
duced syntactic language model approaches to
classification. After presenting a new, simple
baseline, we show that learned collocations
used as features in a maxent model perform
better still, but that the story is more mixed for
the syntactic language model.
1 Introduction
The task of inferring the native language of an author
based on texts written in a second language ? na-
tive language identification (NLI) ? has, since the
seminal work of Koppel et al2005), been primarily
tackled as a text classification task using supervised
machine learning techniques. Lexical features, such
as function words, character n-grams, and part-of-
speech (PoS) n-grams, have been proven to be use-
ful in NLI (Koppel et al2005; Tsur and Rappoport,
2007; Estival et al2007). The recent work of Wong
and Dras (2011), motivated by ideas from Second
Language Acquisition (SLA), has shown that syn-
tactic features ? potentially capturing syntactic er-
rors characteristic of a particular native language ?
improve performance over purely lexical ones.
PoS n-grams can be leveraged to characterise sur-
face syntactic structures: in Koppel et al2005),
for example, ungrammatical structures were approx-
imated by rare PoS bigrams. For the purpose of NLI,
small n-gram sizes like bigram or trigram might not
suffice to capture sequences that are characteristic of
a particular native language. On the other hand, an
attempt to represent these with larger n-grams would
not just lead to feature sparsity problems, but also
computational efficiency issues. Some form of fea-
ture selection should then come into play.
Adaptor grammars (Johnson, 2010), a hierarchi-
cal non-parametric extension of PCFGs (and also in-
terpretable as an extension of LDA-based topic mod-
els), hold out some promise here. In that initial
work, Johnson?s model learnt collocations of arbi-
trary length such as gradient descent and cost func-
tion, under a topic associated with machine learning.
Hardisty et al2010) applied this idea to perspective
classification, learning collocations such as pales-
tinian violence and palestinian freedom, the use of
which as features was demonstrated to help the clas-
sification of texts from the Bitter Lemons corpus as
either Palestinian or Israeli perspective.
Typically in NLI and other authorship attribu-
tion tasks, the feature sets exclude content words,
to avoid unfair cues due to potentially different do-
mains of discourse. In our context, then, what we are
interested in are ?quasi-syntactic collocations? of ei-
ther pure PoS (e.g. NN IN NN) or a mixture of PoS
with function words (e.g. NN of NN). The partic-
ular question of interest for this paper, then, is to
699
investigate whether the power of adaptor grammars
to discover collocations ? specifically, ones of ar-
bitrary length that are useful for classification ? ex-
tends to features beyond the purely lexical.
We examine two different approaches in this pa-
per. We first utilise adaptor grammars for discovery
of high performing ?quasi-syntactic collocations? of
arbitrary length as mentioned above and use them
as classification features in a conventional maximum
entropy (maxent) model for identifying the author?s
native language. In the second approach, we adopt
a grammar induction technique to learn a grammar-
based language model in a Bayesian setting. The
grammar learned can then be used to infer the most
probable native language that a given text written
in a second language is associated with. The latter
approach is actually closer to the work of Hardisty
et al2010) using adaptor grammars for perspec-
tive modeling, which inspired our general approach.
This alternative approach is also similar in nature
to the work of Bo?rschinger et al2011) in which
grounded learning of semantic parsers was reduced
to a grammatical inference task.
The structure of the paper is as follows. In Sec-
tion 2, we review the existing work of NLI as well
as the mechanics of adaptor grammars along with
their applications to classification. Section 3 details
the supervised maxent classification of NLI with
collocation (n-gram) features discovered by adaptor
grammars. The language model-based classifier is
described in Section 4. Finally, we present a dis-
cussion in Section 5 and follow with concluding re-
marks.
2 Related Work
2.1 Native Language Identification
Most of the existing research treats the task of na-
tive language identification as a form of text classi-
fication deploying supervised machine learning ap-
proaches.
The earliest notable work in this classification
paradigm is that of Koppel et al2005) using as
features function words, character n-grams, and PoS
bigrams, together with some spelling errors. Their
experiments were conducted on English essays writ-
ten by authors whose native language one of Bulgar-
ian, Czech, French, Russian, or Spanish. The cor-
pus used is the first version of International Corpus
of Learner English (ICLE). Apart from investigating
lexical features, syntactic features (errors in particu-
lar) were highlighted by Koppel et al2005) as po-
tentially useful features, but they only explored this
by characterising ungrammatical structures with rare
PoS bigrams: they chose 250 rare bigrams from the
Brown corpus.
Features for this task can include content words
or not: Koppel et al2009), in reviewing work in
the general area of authorship attribution (including
NLI), discuss the (perhaps unreasonable) advantage
that content word features can provide, and com-
ment that consequently they ?are careful . . . to dis-
tinguish results that exploit content-based features
from those that do not?. We will not be using con-
tent words as features; we therefore note only ap-
proaches to NLI that similarly do not use them.
Following Koppel et al2005), Tsur and Rap-
poport (2007) replicated their work and hypothe-
sised that word choices in second language writing
is highly influenced by the frequency of native lan-
guage syllables. They investigated this through mea-
suring classification performance with only charac-
ter bigrams as features.
Estival et al2007) tackled the broader task of
developing profiles of authors, including native lan-
guage and various other demographic and psycho-
metric author traits, across a smaller set of languages
(English, Spanish and Arabic). To this end, they de-
ployed various lexical and document structure fea-
tures.
Wong and Dras (2011), starting from the Kop-
pel et al2005) approach, explored the usefulness
of syntactic features in a broader sense in which
they characterised syntactic errors with cross sec-
tions of parse trees obtained from statistical parsers,
both horizontal slices of the parse trees in the form
of CFG production rules, and the feature schemata
used in discriminative parse reranking (Charniak
and Johnson, 2005); they also found that using the
top 200 PoS bigrams helped. Their results on the
second version of the ICLE corpus, across seven
languages (those of Koppel et alplus two Orien-
tal languages, Chinese and Japanese) demonstrated
that syntactic features of these kinds lead to signifi-
cantly better performance than the Koppel et alea-
tures alone, with a top accuracy (on 5-fold cross-
validation) of 77.75%.
700
Subsequently, Wong et al2011) explored
Bayesian topic modeling (Blei et al2003; Griffiths
and Steyvers, 2004) as a form of feature dimension-
ality reduction technique to discover coherent latent
factors (?topics?) that might capture predictive fea-
tures for individual native languages. Their topics,
rather than the typical word n-grams, consisted of
bigrams over (only) PoS. However, while there was
some evidence of topic cluster coherence, this did
not improve classification performance.
The work of the present paper differs in that it
uses Bayesian techniques to discover collocations of
arbitrary length for use in classification, over a mix
of both PoS and function words, rather than for use
as feature dimensionality reduction.
2.2 Adaptor Grammars
Adaptor Grammars are a non-parametric extension
to PCFGs that are associated with a Bayesian in-
ference procedure. Here we provide an informal
introduction to Adaptor Grammars; Johnson et al
(2007) provide a definition of Adaptor Grammars as
a hierarchy of mixtures of Dirichlet (or 2-parameter
Poisson-Dirichlet) Processes to which the reader
should turn for further details.
Adaptor Grammars can be viewed as extending
PCFGs by permitting the grammar to contain an
unbounded number of productions; they are non-
parametric in the sense that the particular produc-
tions used to analyse a corpus depends on the cor-
pus itself. Because the set of possible productions
is unbounded, they cannot be specified by simply
enumerating them, as is standard with PCFGs. In-
stead, the productions used in an adaptor gram-
mar are specified indirectly using a base grammar:
the subtrees of the base grammar?s ?adapted non-
terminals? serve as the possible productions of the
adaptor grammar (Johnson et al2007), much in
the way that subtrees function as productions in Tree
Substitution Grammars .1
Another way to view Adaptor Grammars is that
they relax the independence assumptions associated
with PCFGs. In a PCFG productions are gener-
ated independently conditioned on the parent non-
terminal, while in an Adaptor Grammar the proba-
bility of generating a subtree rooted in an adapted
1For computational efficiency reasons Adaptor Grammars
require the subtrees to completely expand to terminals. The
Fragment Grammars of O?Donnell (2011) lift this restriction.
non-terminal is roughly proportional to the number
of times it has been previously generated (a certain
amount of mass is reserved to generate ?new? sub-
trees). This means that the distribution generated by
an Adaptor Grammar ?adapts? based on the corpus
being generated.
2.2.1 Mechanics of adaptor grammars
Adaptor Grammars are specified by a PCFG G,
plus a subset of G?s non-terminals that are called
the adapted non-terminals, as well as a discount
parameter aA, where 0 ? aA < 1 and a con-
centration parameter bA, where b > ?a, for each
adapted non-terminal A. An adaptor grammar de-
fines a two-parameter Poisson-Dirichlet Process for
each adapted non-terminal A governed by the pa-
rameters aA and bA. For computational purposes it
is convenient to integrate out the Poisson-Dirichlet
Process, resulting in a predictive distribution spec-
ified by a Pitman-Yor Process (PYP). A PYP can
be understood in terms of a ?Chinese Restaurant?
metaphor in which ?customers? (observations) are
seated at ?tables?, each of which is labelled with a
sample from a ?base distribution? (Pitman and Yor,
1997).
In an Adaptor Grammar, unadapted non-terminals
expand just as they do in a PCFG; a production r ex-
panding the non-terminal is selected according to the
multinomial distribution ?r over productions speci-
fied in the grammar. Each adapted non-terminalA is
associated with its own Chinese Restaurant, where
the tables are labelled with subtrees generated by
the grammar rooted in A. In the Chinese Restau-
rant metaphor, the customers are expansions of A,
each table corresponds to a particular subtree ex-
panding A, and the PCFG specifies the base distri-
bution for each of the adapted non-terminals. An
adapted non-terminal A expands as follows. A ex-
pands to a subtree t with probability proportional to
nt, where nt is the number of times t has been pre-
viously generated. In addition, A expands using a
PCFG rule r expanding A with probability propor-
tional to (mA aA + bA) ?r, where mA is the number
of subtrees expanding A (i.e., the number of tables
in A?s restaurant). Because the underlying Pitman-
Yor Processes have a ?rich get richer? property, they
generate power-law distributions over the subtrees
for adapted non-terminals.
701
2.2.2 Adaptor grammars as LDA extension
With the ability to rewrite non-terminals to en-
tire subtrees, adaptor grammars have been used to
extend unigram-based LDA topic models (Johnson,
2010). This allows topic models to capture se-
quences of words with abitrary length rather than
just unigrams of word. It has also been shown that it
is crucial to go beyond the bag-of-words assump-
tion as topical collocations capture more meaning
information and represent more interpretable topics
(Wang et al2007).
Taking the PCFG formulation for the LDA topic
models, it can be modified such that each topic
Topici generates sequences of words by adapting
each of the Topici non-terminals (usually indicated
with an underline in an adaptor grammar). The over-
all schema for capturing topical collocations with an
adaptor grammar is as follows:
Sentence? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ?Words i ? 1, . . . , t
Words?Word
Words?Words Word
Word? w w ? V
There is a non-grammar-based approach to find-
ing topical collocations as demonstrated by Wang et
al. (2007). Both of these approaches learned use-
ful collocations: for instance, as mentioned in Sec-
tion 1, Johnson (2010) found collocations such gra-
dient descent and cost function associated with the
topic of machine learning; Wang et al2007) found
the topic of human receptive system comprises of
collocations such as visual cortext and motion de-
tector.
Adaptor grammars have also been deployed as a
form of feature selection in discovering useful collo-
cations for perspective classification. Hardisty et al
(2010) argued that indicators of perspectives are of-
ten beyond the length of bigrams and demonstrated
that the use of the adaptor grammar inferred n-grams
of arbitrary length as features establishes the start-
of-the-art performance for perspective classification
on the Bitter Lemons corpus, depicting two differ-
ent perspectives of Israeli and Pelestinian. We are
adopting a similar approach in this paper for classi-
fying texts with respect to the author?s native lan-
guage; but the key difference with Hardisty et al
(2010)?s approach is that our focus is on collocations
that mix PoS and lexical elements, rather than being
purely lexical.
3 Maxent Classification
In this section, we first explain the procedures taken
to set up the conventional supervised classification
task for NLI through the deployment of adaptor
grammars for discovery of ?quasi-syntactic colloca-
tions? of arbitrary length. We then present the classi-
fication results attained based on these selected sets
of n-gram features. In all of our experiments, we
investigate two sets of collocations: pure PoS and
a mixture of PoS and function words. The idea of
examining the latter set is motivated by the results
of Wong and Dras (2011) where inclusion of parse
production rules lexicalised with function words as
features had shown to improve the classification per-
formance relative to unlexicalised ones.
3.1 Experimental Setup
3.1.1 Data and evaluation
The classification experiments are conducted on
the second version of ICLE (Granger et al2009).2
Following our earlier NLI work in Wong and Dras
(2011), our data set consists of 490 texts written
in English by authors of seven different native lan-
guage groups: Bulgarian, Czech, French, Russian,
Spanish, Chinese, and Japanese. Each native lan-
guage contributes 70 out of the 490 texts. As we are
using a relative small data set, we perform k-fold
cross-validation, choosing k = 5.
3.1.2 Adaptor grammars for supervised
classification
We derive two adaptor grammars for the maxent
classification setting, where each is associated with
a different set of vocabulary (i.e. either pure PoS
or the mixture of PoS and function words). We use
2Joel Tetreault and Daniel Blanchard from ETS have pointed
out (personal communication) that there is a subtle issue with
ICLE that could have an impact on the classification perfor-
mance of NLI tasks; in particular, when character n-grams are
used as features, some special characters used in some ICLE
texts might affect performance. For our case, this should not be
of much issue since they will not appear in our collocations.
702
the grammar of Johnson (2010) as presented in Sec-
tion 2.2.2, except that the vocabulary differs: either
w ? Vpos or w ? Vpos+fw. For Vpos, there are
119 distinct PoS tags based on the Brown tagset.
Vpos+fw is extended with 398 function words as per
Wong and Dras (2011). m = 490 is the number of
documents, and t = 25 the number of topics (chosen
as the best performing one from Wong et al2011)).
Rules of the form Docj ? Docj Topici that
encode the possible topics that are associated with
a document j are given similar ? priors as used
in LDA (? = 5/t where t = 25 in our experi-
ments). Likewise, similar ? priors from LDA are
placed on the adapted rules expanding from Topici
? Words, representing the possible sequences of
words that each topic comprises (? = 0.01).3 The
inference algorithm for the adaptor grammars are
based on the Markov Chain Monte Carlo technique
made available online by Johnson (2010).4
3.1.3 Classification models with n-gram
features
Based on the two adaptor grammars inferred, the
resulting collocations (n-grams) are extracted as fea-
tures for the classification task of identifying au-
thors? native language. These n-grams found by the
adaptor grammars are only a (not necessarily proper)
subset of those n-grams that are strongly characteris-
tic of a particular native language. In principle, one
could find all strongly characteristic n-grams by enu-
merating all the possible instances of n-grams up to
a given length if the vocabulary is of a small enough
closed set, such as for PoS tags, but this is infeasi-
ble when the set is extended to PoS plus function
words. The use of adaptor grammars here can be
viewed as a form of feature selection, as in Hardisty
et al2010).
Baseline models To serve as a baseline, we take
the commonly used PoS bigrams as per the previ-
ous work of NLI (Koppel et al2005). A set of
200 PoS bigrams is selected in two ways: the 200
most frequent in the training data (as in Wong and
Dras (2011)) and the 200 with the highest informa-
tion gain (IG) values in the training data (not evalu-
3The values of ? and ? are also based on the established
values presented in Wong et al2011).
4Adaptor grammar software is available on http://web.
science.mq.edu.au/?mjohnson/Software.htm.
ated in other work).
Enumerated n-gram models Here, we enumer-
ate all the possible n-grams up to a fixed length and
select the best of these according to IG, as a general-
isation of the baseline. The first motivation for this
feature set is that, in a sense, this should give a rough
upper bound for the adaptor grammar?s PoS-alone n-
grams, as these latter should most often be a subset
of the former. The second motivation is that it gives
a robust comparison for the mixed PoS and function
word n-grams, where it is infeasible to enumerate all
of them.
ENUM-POS We enumerate all possible n-grams up
to the length of 5, and select those that actually
occur (i.e. of the
?5
i=1 119
i possible n-grams,
this is 218,042 based on the average of 5 folds).
We look at the top n-grams up to length 5 selected
by IG: the top 2,800 and the top 6,500 (for com-
parability with adaptor grammar feature sets, be-
low), as well as the top 10,000 and the top 20,000
(to study the effect of larger feature space).
Adaptor grammar n-gram models The classifi-
cation features are the two sets of selected colloca-
tions inferred by the adaptor grammars which are the
main interest of this paper.
AG-POS This first set of the adaptor grammar-
inferred features comprise of pure PoS n-grams
(i.e. Vpos). The largest length of n-gram found
is 17, but about 97% of the collocations are of
length between 2 to 5. We investigate three vari-
ants of this feature set: top 200 n-grams of all
lengths (based on IG), all n-grams of all lengths
(n = 2, 795 on average), and all n-grams up to
the length of 5 (n = 2, 710 on average).
AG-POS+FW This second set of the adaptor
grammar-inferred features are mixtures of PoS
and function words (i.e. Vpos+fw). The largest
length of n-gram found for this set is 19 and
the total number of different collocations found
is much higher. For the purpose of comparabil-
ity with the first set of adaptor grammar features,
we investigate the following five variants for this
feature set: top 200 n-grams of all lengths, all n-
grams of all lengths (n = 6, 490 on average), all
n-grams up to the length of 5 (n = 6, 417 on av-
erage), top 2,800 n-grams of all different lengths,
703
Features (n-grams) Accuracy
BASELINE-POS [top200 MOST-FREQ] 53.87
BASELINE-POS [top200 IG] 56.12
AG-POS [top200 IG] 61.02
AG-POS [all ?17-gram] (n ? 2800) 68.37
AG-POS [all ? 5-gram] (n ? 2700) 68.57
AG-POS+FW [top200 IG] 58.16
AG-POS+FW [all ?19-gram] (n ? 6500) 74.49
AG-POS+FW [all ?5-gram] (n ? 6400) 74.49
AG-POS+FW [top2800 IG ? 19-gram] 71.84
AG-POS+FW [top2800 IG ? 5-gram] 71.84
ENUM-POS [top2800 IG ? 5-gram] 69.79
ENUM-POS [top6500 IG ? 5-gram] 72.44
ENUM-POS [top10K IG ? 5-gram] 71.02
ENUM-POS [top20K IG ? 5-gram] 71.43
Table 1: Maxent classification results for individual fea-
ture sets (with 5-fold cross validation).
and top 2,800 n-grams up to the length of 5. (All
the selections are based on IG).
In our models, all feature values are of binary
type. For the classifier, we employ a maximum en-
tropy (MaxEnt) machine learner ? MegaM (fifth re-
lease) by Hal Daume? III.5
3.2 Classification results
Table 1 presents all the classification results for the
individual feature sets, along with the baselines. On
the whole, both sets of the collocations inferred by
the adaptor grammars perform better than the two
baselines. We make the following observations:
? Regarding ENUM-POS as a (rough) upper
bound, the adaptor grammar AG-POS with a
comparable number of features performs al-
most as well. However, because it is possible to
enumerate many more n-grams than are found
during the sampling process, ENUM-POS opens
up a gap over AG-POS of around 4%.
? Collocations with a mix of PoS and function
words do in fact lead to higher accuracy as
compared to those of pure PoS (except for the
top 200 n-grams); for instance, compare the
2,800 n-grams up to length 5 from the two cor-
responding sets (71.84 vs. 68.57).
? Furthermore, the adaptor grammar-inferred
collocations with mixtures of PoS and function
5MegaM software is available on http://www.cs.
utah.edu/?hal/megam/.
Features (n-grams) Accuracy
AG-POS [all ? 5-gram] & FW 72.04
ENUM-POS [top2800 ? 5-gram] & FW 73.67
AG-POS+FW & AG-POS a 75.71
AG-POS+FW & AG-POS b 74.90
AG-POS+FW & ENUM-POS [top2800] a 73.88
AG-POS+FW & ENUM-POS [top2800] b 74.69
AG-POS+FW & ENUM-POS [top10K] b 74.90
AG-POS+FW & ENUM-POS [top20K] b 75.10
Table 2: Maxent classification results for combined fea-
ture sets (with 5-fold cross validation). aFeatures from
the two sets are selected based on the overall top 3700
with highest IG; bfeatures from the two sets are just lin-
early concatenated.
words (AG-POS+FW) in general perform better
than our rough upper bound of PoS colloca-
tions, i.e. the enumerated PoS n-grams (ENUM-
POS): the overall best results of the two feature
sets are 74.49 and 72.44 respectively.
Given that the AG-POS+FW n-grams are captur-
ing different sorts of document characteristics, they
could potentially usefully be combined with the
PoS-alone features. We thus combined them with
both AG-POS and ENUM-POS feature sets, and the
classification results are presented in Table 2. We
tried two ways of integrating the feature sets: one
way is to take the overall top 2,800 of the two sets
based on IG; the other way is to just combine the two
sets of features by concatenation of feature vectors
(as indicated by a and b respectively in the result
table). For comparability purposes, we considered
only n-grams up to the length of 5. A baseline ap-
proach to this is just to add in function words as un-
igram features by feature vector concatenation, giv-
ing two further models, AG-POS [all ? 5-gram] &
FW and ENUM-POS [top2800 ? 5-gram] & FW.
Overall, the classification accuracies attained by
the combined feature sets are higher than the in-
dividual feature sets. The best performing of all
the models is achieved by combining the mixed
PoS and function word collocations with the adap-
tor grammar-inferred PoS, producing the best accu-
racy thus far of 75.71. This demonstrates that fea-
tures inferred by adaptor grammars do capture some
useful information and function words are playing
a role. The way of integrating the two feature sets
has different effects on the types of combination. As
seen in Table 2, method a works better for the com-
704
bination of the two adaptor grammar feature sets;
whereas method b works better for combining adap-
tor grammar features with enumerated n-gram fea-
tures.
Using adaptor grammar collocations also outper-
forms the alternative baseline of adding in function
words as unigrams. For instance, the best perform-
ing combined feature set of both AG-POS and AG-
POS+FW does result in higher accuracy as compared
to the two alternative baseline models, comparing
75.71 with 72.04 (and 75.71 with 73.67). This
demonstrates that our more general PoS plus func-
tion word collocations derived from adaptor gram-
mars are indeed useful, and supports the argument
of Wang et al2007) that they are a useful tech-
nique for looking into features beyond just the bag
of words.
4 Language Model-based Classification
In this section, we take a language modeling ap-
proach to native language identification; the idea
here is to adopt grammatical inference to learn
a grammar-based language model to represent the
texts written by non-English native users. The gram-
mar learned is then used to predict the most probable
native language that a document (a sentence) is as-
sociated with.
In a sense, we are using a parser-based language
model to rank the documents with respect to native
language. We draw on the work of Bo?rschinger et
al. (2011) for this section. In that work, the task
was grounded learning of a semantic parser. Train-
ing examples there consisted of natural language
strings (descriptions of a robot soccer game) and
a set of candidate meanings (actions in the robot
soccer game world) for the string; each was tagged
with a context identifier reflecting the actual action
of the game. A grammar was then induced that
would parse the examples, and was used on test data
(where the context identifier was absent) to predict
the context. We take a similar approach to devel-
oping an grammatical induction technique, although
where they used a standard LDA topic model-based
PCFG, we use an adaptor grammar. We expect that
the results will likely to be lower than for the dis-
criminative approach of Section 3. However, the
approach is of interest for a few reasons: because,
whereas the adaptor grammar plays an ancillary, fea-
ture selection role in Section 3, here the feature se-
lection is an organic part of the approach as per the
actual implementation of Hardisty et al2010); be-
cause adaptor grammars can potentially be extended
in a natural way with unlabelled data; and because,
for the purposes of this paper, it constitutes a second,
quite different way to evaluate the use of n-gram col-
locations.
4.1 Language Models
We derive two adaptor grammar-based language
models. One consists of only unigrams and bi-
grams, and the other finds n-gram collocations, in
both cases over either PoS or the mix of PoS and
function words. The assumption that we make is that
each document (each sentence) is a mixture of two
sets of topics: one is the native language-specific
topic (i.e. characteristic of the native language) and
the other is the generic topic (i.e. characteristic of
the second language ? English in our case). The
generic topic is thus shared across all languages,
and will behave quite differently from a language-
specific topic, which is not shared. In other words,
there are eight topics, representing seven native lan-
guage groups that are of interest (Bulgarian, Czech,
French, Russian, Spanish, Chinese, and Japanese)
and the second language English itself.6
Bigram models The following rule schema is
applicable to both vocabulary types of PoS and the
mixture of PoS and function words.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? nullTopic
langTopic?Words
nullTopic?Words
Words?Word Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
N-gram models The grammar is the same as
the above with the exception that the non-terminal
Words is now rewritten as follows in order to
6We could just induce a regular PCFG here, rather than an
adaptor grammar, by taking as terminals all pairs of PoS tags.
We use the adaptor grammar formulation for comparability.
705
capture n-gram collocations of arbitrary length.
Words?Words Word
Words?Word
It should be noted that the two grammars above
can in theory be applied to an entire document or on
individual sentences. For this present work, we work
on the sentence level as the run-time of the current
implementation of the adaptor grammars grows pro-
portional to the cube of the sentence length. For each
grammar we try both sparse and uniform Dirichlet
priors (? = {0.01, 0.1, 1.0}). The sparse priors en-
courage only a minority of the rules to be associated
with high probabilities.
4.2 Training and Evaluation
As we are using the same data set as per the pre-
vious approach, we perform 5-fold cross validation
as well. However, the training for each fold is con-
ducted with a different grammar consisting of only
the vocabulary that occur in each training fold. The
reason is that we are now having a form of super-
vised topic models where the learning process is
guided by the native languages. Hence, each of the
training sentences are prefixed with the (native) lan-
guage identifiers lang, as seen in the Root rules of
the grammar presented above.
To evaluate the grammars learned, as in
Bo?rschinger et al2011) we need to slightly modify
the grammars above by removing the language iden-
tifiers ( lang) from theRoot rules and then parse the
unlabeled sentences using a publicly available CKY
parser.7 The predicted native language is inferred
from the parse output by reading off the langTopics
that the Root is rewritten to. We take that as the
most probable native language for a particular test
sentence. At the document level, we select as the
class the language predicted for the largest number
of sentences in that document.
4.3 Parsing Results
Tables 3 and 4 present the parsing results at the sen-
tence level and the document level, respectively. On
the whole, the results at the sentence level are much
poorer as compared to those at the document level.
In light of the results of Section 3.2, it is surprising
7CKY parser by Mark Johnson is available on
http://web.science.mq.edu.au/?mjohnson/
Software.htm.
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 26.84 27.03 26.77
AG-POS [n-grams] 25.85 25.78 25.62
AG-POS+FW [bigrams] 28.58 28.40 27.43
AG-POS+FW [n-grams] 26.64 27.64 28.75
Table 3: Language modeling-based classification results
based on parsing (at the sentence level).
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 41.22 38.88 39.69
AG-POS [n-grams] 36.12 34.90 35.20
AG-POS+FW [bigrams] 47.45 46.94 44.64
AG-POS+FW [n-grams] 43.97 49.39 50.15
Table 4: Language modeling-based classification results
based on parsing (at the document level).
that bigram models appear to perform better than n-
gram models for both types of vocabulary, with the
exception of AG-POS+FW at the document level. In
fact, one would expect n-gram models to perform
better in general as it is a generalisation that would
contain all the potential bigrams. Nonetheless, the
language models over the mixture of PoS and func-
tion words appear to be a more suitable representa-
tive of our learner corpus as compared to those over
purely PoS, confirming the usefulness of integrated
function words for the NLI classification task.
It should also be noted that sparse priors gen-
erally appear to be more appriopriate; except that
for AG-POS+FW n-grams, uniform priors are indeed
better and resulted in the highest parsing result of
50.15. (Although all the parsing results are much
weaker as compared to the results presented in Sec-
tion 3.2, they are all higher than the majority base-
line of 14.29% i.e. 70/490).
5 Discussion
Here we take a closer look at how well each ap-
proach does in identifying the individual native lan-
guages. The confusion matrix for the best model
of two approaches are presented in Table 5 and Ta-
ble 6. Both approaches perform reasonably well for
the two Oriental languages (Chinese in particular);
this is not a major surprise, as the two languages
are not part of the language family that the rest of
the languages come from (i.e. Indo-European). Un-
der the supervised maxent classification, misclassi-
fications largely are observed in the Romance ones
(French and Spanish) as well as Russian; for the lan-
guage model-based approach, Bulgarian is identi-
706
BL CZ RU FR SP CN JP
BL [52] 5 7 4 2 - -
CZ 5 [50] 5 3 4 - 3
RU 6 8 [46] 5 1 - 4
FR 7 3 5 [43] 8 - 4
SP 7 2 4 9 [47] - 1
CN - - - - - [70] -
JP - - 2 2 1 2 [63]
Table 5: Confusion matrix based on the best performing
model under maxent setting (BL:Bulgarian, CZ:Czech,
RU:Russian, FR:French, SP:Spanish, CN:Chinese,
JP:Japanese).
BL CZ RU FR SP CN JP
BL [20] 32 9 6 - 1 2
CZ 2 [59] 3 1 - - 5
RU 3 41 [19] 2 1 - 4
FR 8 20 4 [31] 4 - 3
SP 7 27 11 12 [9] - 4
CN - 2 - 2 - [62] 4
JP - 19 1 2 - 1 [47]
Table 6: Confusion matrix based on the best
performing model under language modeling setting
(BL:Bulgarian, CZ:Czech, RU:Russian, FR:French,
SP:Spanish, CN:Chinese, JP:Japanese).
fied poorly, and Spanish moreso. However, the latter
approach appears to be better in identifying Czech.
On the whole, the maxent approach results in much
fewer misclassifications compared to its counterpart.
In fact, there is a subtle difference in the exper-
imental setting of the models derived from the two
approaches with respect to the adaptor grammar: the
number of topics. Under the maxent setting, the
number of topics t was set to 25, while we restricted
the models with the language modeling approach to
only eight topics (seven for the individual native lan-
guages and one for the common second language,
English). Looking more deeply into the topics them-
selves reveals that there appears to be at least two out
of the 25 topics (from the supervised models) asso-
ciated with n-grams that are indicative of the native
languages, taking Chinese and Japanese as examples
(see the associated topics in Table 7).8 Perhaps as-
sociating each native language with only one gener-
alised topic is not sufficient.
Furthermore, the distribution of n-grams among
the topics (i.e. subtrees of collocations derived
from the adaptor grammars) are quite different be-
tween the two approaches although the total num-
8Taking the examples from Wong et al2011) as reference,
we found similar n-grams that are indicative of Japanese and
Chinese.
Top 10 Mixture N-grams
Japanese Chinese
topic2 topic23 topic9 topic17
. . NN .
we VB PPSS VB a NN NN NN
our NNS my NN NN NN NNS
our NN CC VBN by NN
NN VBG NP . RB ,
PPSS VB PPSS think NP of NN
about NN : JJ NN
because PPSS VBD ( NN .
it . RB as VBG NN
we are PPSS ? NN NN NN NN NN NN NN
Table 7: Top mixture n-grams (collocations) for 4 out of
the 25 topics representative of Japanese and Chinese (un-
der maxent setting). N-grams of pronoun with verb are
found at the upper end of Topic2 and Topic23 reflecting
the frequent usage of Japanese; n-grams of noun are top
n-grams under Topic9 and Topic17 indicating Chinese?s
common error of determiner-noun disagreement.
ber of n-grams inferred by each approach is about
the same. For the language modeling ones, a high
number of n-grams were associated with the generic
topic nullTopic9 and each language-specific topic
langTopic has a lower number of n-grams relative
to bi-grams (Table 8) associated with it. For the
maxent models, in contrast, the majority of the top-
ics were associated with a higher number of n-grams
(Table 9). The smaller number of n-grams to be used
as features ? and the fact that their extra length
means that they will occur more sparsely in the doc-
uments ? seems to be the core of the problem.
Nonetheless, the language models inferred dis-
cover relevant n-grams that are representative of
individual native languages. For instance, the bi-
gram NN NN, which Wong and Dras (2011) claim
may reflect the error of determiner-noun disagree-
ment commonly found amongst Chinese learners,
was found under the Chinese topic at the top-2 posi-
tion with a probability of 0.052 as compared to the
other languages at the probability range of 0.0005-
0.003. Similarly, one example for Japanese, the mix-
ture bigram PPSS think, indicating frequent us-
age of pronouns within Japanese was seen under the
Japanese topic at the top-9 position with a probabil-
ity of 0.025 in relation to other languages within the
range of 0.0002-0.006: this phenomenon as char-
9This is quite plausible as there should be quite a number of
structures that are representative of native English speakers that
are shared by non-native speakers.
707
Model N-gram Frequency
Types BGTopic CZTopic FRTopic RUTopic SPTopic CNTopic JPTopic NullTopic
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
Bigrams 374 187 352 219 426 165 350 211 351 156 397 351 394 194 867 6169
N-grams 177 159 226 217 151 152 148 202 128 147 357 255 209 226 3089 7794
Table 8: Distribution of n-grams (collocations) for each topic under language modeling setting. (a) subcolumns are
for n-grams of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
N-gram Frequency
Topic1 Topic2 Topic3 Topic4 Topic5 Topic6 Topic7 Topic8 Topic9 Topic10
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
174 443 145 441 136 245 141 341 236 519 169 748 127 340 182 473 109 339 190 236
Topic11 Topic12 Topic13 Topic14 Topic15 Topic16 Topic17 Topic18 Topic19 Topic20
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
57 259 126 455 103 543 211 225 170 459 81 309 238 207 152 475 119 452 333 423
Topic21 Topic22 Topic23 Topic24 Topic25
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
245 341 168 492 194 472 201 366 195 190
Table 9: Distribution of n-grams (collocations) for each topic under maxent setting. (a) subcolumns are for n-grams
of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
Languages Excerpts from ICLE
Chinese ... the overpopulation problem in urban area ...
... The development of country park can directly ...
... when it comes to urban renewal project ...
... As developing new town in ...
... and reserve some country park as ...
Japanese ... I think many people will ...
... I think governments should not ...
... I think culture is the most significant ...
... I think the state should not ...
... I really think we must live ...
Table 10: Excerpts from ICLE illustrating the common
phenomena observed amongst Chinese and Japanese.
acteristic of Japanese speakers has also been noted
for different corpora by Ishikawa (2011). (Note that
this collocation as well as its pure PoS counterpart
PPSS VB are amongst the top n-grams discovered
under the maxent setting as seen in Table 7.) Table
10 presents some excerpts extracted from the corpus
that illustrate these two common phenomena.
To investigate further the issue associated with the
number of topics under the language modeling set-
ting, we attempted to extend the adaptor grammar
with three additional topics that represent the lan-
guage family of the seven native languages of inter-
est: Slavic, Romance, and Oriental. (The resulting
grammar is presented as below.) However, the pars-
ing result does not improve over the initial setting
with eight topics in total.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics familyTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? familyTopic
langTopics? nullTopic
langTopic?Words
familyTopic?Words
nullTopic?Words
Words?Words Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
6 Conclusion and Future Work
This paper has shown that the extension of adap-
tor grammars to discovering collocations beyond the
lexical, in particular a mix of PoS tags and function
words, can produce features useful in the NLI clas-
sification problem. More specifically, when added
to a new baseline presented in this paper, the com-
bined feature set of both types of adaptor grammar
inferred collocations produces the best result in the
context of using n-grams for NLI. The usefulness of
the collocations does vary, however, with the tech-
nique used for classification.
Future work will involve a broader exploration
of the parameter space of the adaptor grammars,
in particular the number of topics and the value
of ?; a look at other non-parametric extensions of
PCFGs, such as infinite PCFGs (Liang et al2007)
for finding a set of non-terminals permitting more
fine-grained topics; and an investigation of how the
approach can be extended to semi-supervised learn-
ing to take advantage of the vast quantity of texts
with errors available on the Web.
708
Acknowledgments
We would like to acknowledge the support of ARC
Linkage Grant LP0776267. We also thank the
anonymous reviewers for useful feedback. Much
gratitude is due to Benjamin Bo?rschinger for his
help with the language modeling implementation.
References
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alation. Journal of Machine
Learning Research, 3:993?1022.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan, June.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 284?292.
Shun?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Project. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and Language Technologies in Teaching, Learn-
ing and Research, pages 3?11. University of Strath-
clyde Press, Glasgow, UK.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
In Advances in Neural Information Processing Sys-
tems 19: Proceedings of the Twentieth Annual Confer-
ence on Neural Information Processing Systems, pages
641?648.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209?217. Springer-Verlag.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchical
dirichlet processes. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 688?697, Prague, Czech Re-
public, June.
Timothy O?Donnell. 2011. Productivity and reuse in
language. Ph.D. thesis, Harvard University.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937?944.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, July.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
709
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844?853,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Syllable weight encodes mostly the same information for English word
segmentation as dictionary stress
John K Pate Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{john.pate,mark.johnson}@mq.edu.au
Abstract
Stress is a useful cue for English word
segmentation. A wide range of computa-
tional models have found that stress cues
enable a 2-10% improvement in segmen-
tation accuracy, depending on the kind of
model, by using input that has been anno-
tated with stress using a pronouncing dic-
tionary. However, stress is neither invari-
ably produced nor unambiguously iden-
tifiable in real speech. Heavy syllables,
i.e. those with long vowels or syllable
codas, attract stress in English. We de-
vise Adaptor Grammar word segmentation
models that exploit either stress, or sylla-
ble weight, or both, and evaluate the util-
ity of syllable weight as a cue to word
boundaries. Our results suggest that sylla-
ble weight encodes largely the same infor-
mation for word segmentation in English
that annotated dictionary stress does.
1 Introduction
One of the first skills a child must develop in the
course of language acquisition is the ability to seg-
ment speech into words. Stress has long been
recognized as a useful cue for English word seg-
mentation, following the observation that words
in English are predominantly stress-initial (Cutler
and Carter, 1987), together with the result that 9-
month-old English-learning infants prefer stress-
initial stimuli (Jusczyk et al., 1993). A range of
statistical (Doyle and Levy, 2013; Christiansen et
al., 1998; B?orschinger and Johnson, 2014) and
rule-based (Yang, 2004; Lignos and Yang, 2010)
models have used stress information to improve
word segmentation. However, that work uses
stress-marked input prepared by marking vowels
that are listed as stressed in a pronouncing dic-
tionary. This pre-processing step glosses over the
fact that stress identification itself involves a non-
trival learning problem, since stress has many pos-
sible phonetic reflexes and no known invariants
(Campbell and Beckman, 1997; Fry, 1955; Fry,
1958). One known strong correlate of stress in
English is syllable weight: heavy syllables, which
end in a consonant or have a long vowel, at-
tract stress in English. We present experiments
with Bayesian Adaptor Grammars (Johnson et al.,
2007) that suggest syllable weight encodes largely
the same information for word segmentation that
dictionary stress information does.
Specifically, we modify the Adaptor
Grammar word segmentation model of
B?orschinger and Johnson (2014) to compare
the utility of syllable weight and stress cues for
finding word boundaries, both individually and in
combination. We describe how a shortcoming of
Adaptor Grammars prevents us from comparing
stress and weight cues in combination with the full
range of phonotactic cues for word segmentation,
and design two experiments to work around this
limitation. The first experiment uses grammars
that provide parallel analyses for syllable weight
and stress, and learns initial/non-initial phonotac-
tic distinctions. In this first experiment, syllable
weight cues are actually more useful than stress
cues at larger input sizes. The second experiment
focuses on incorporating phonotactic cues for
typical word-final consonant clusters (such as
inflectional morphemes), at the expense of parallel
structures. In this second experiment, weight cues
merely match stress cues at larger input sizes,
and the learning curve for the combined weight-
and-stress grammar follows almost perfectly with
the stress-only grammar. This second experiment
suggests that the advantage of weight over stress
in the first experiment was purely due to poor
modeling of word-final consonant clusters by
the stress-only grammar, not weight per se. All
together, these results indicate that syllable weight
844
is highly redundant with dictionary-based stress
for the purposes of English word segmentation;
in fact, in our experiments, there is no detectable
difference between relying on syllable weight and
relying on dictionary stress.
2 Background
Stress is the perception that some syllables are
more prominent than others, and reflects a com-
plex, language-specific interaction between acous-
tic cues (such as loudness and duration), and
phonological patterns (such as syllable shapes).
The details on how stress is assigned, produced,
and perceived vary greatly across languages.
Three aspects of the English stress system are
relevant for this paper. First, although English
stress can shift in different contexts (Liberman and
Prince, 1977), such as from the first syllable of
?fourteen? in isolation to the second syllable when
followed by a stressed syllable, it is largely stable
across different tokens of a given word. Second,
most words in English end up being stress-initial
on a type and token basis. Third, heavy syllables
(those with a long vowel or a consonant coda) at-
tract stress in English.
There is experimental evidence that English-
learning infants prefer stress-initial words from
around the age of seven months (Jusczyk et al.,
1993; Juszcyk et al., 1999; Jusczyk et al., 1993;
Thiessen and Saffran, 2003). A variety of com-
putational models have subsequently been devel-
oped that take stress-annotated input and use this
regularity to improve segmentation accuracy. The
earliest Simple Recurrent Network (SRN) mod-
eling experiments of Christiansen et al. (1998)
and Christiansen and Curtin (1999) found that
stress improved word segmentation from about
39% to 43% token f-score (see Evaluation). Ryt-
ting et al. (2010) applied the SRN model to prob-
ability distributions over phones obtained from a
speech recognition system, and found that the en-
tropy of the probability distribution over phones,
as a proxy to local hyperarticulation and hence a
stress cue, improved token f-score from about 16%
to 23%. In a deterministic approach using pre-
syllabified input, Yang (2004), with follow-ups in
Lignos and Yang (2010) and Lignos (2011; 2012),
showed that a ?Unique Stress Constraint? (USC),
or assuming each word has at most one stressed
syllable, leads to an improvement of about 2.5%
boundary f-score.
Among explicitly probabilistic models,
Doyle and Levy (2013) incorporated stress into
Goldwater et al.?s (2009) Bigram model. They
did this by modifying the base distribution over
lexical forms to generate not simply phone strings
but a sequence of syllables that may or may
not be stressed. The resulting model can learn
that some sequences of syllables (in particular,
sequences that start with a stressed syllable)
are more likely than others. However, observed
stress improved token f-score by only 1%.
B?orschinger and Johnson (2014) used Adaptor
Grammars (Johnson et al., 2007), a generalization
of Goldwater et al.?s (2009) Bigram model that
will be described shortly, and found a clearer
4-10% advantage in token f-score, depending on
the amount of training data.
Together, the experimental and computational
results suggest that infants in fact pay attention
to stress, and that stress carries useful information
for segmenting words in running speech. How-
ever, stress identification is itself a non-trivial
task, as stress has many highly variable, context-
sensitive, and optional phonetic reflexes. How-
ever, one strong phonological cue in English is
syllable weight: heavy syllables attract stress.
Heavy syllables, in turn, are syllables with a
coda and/or a long vowel, which, in English,
are tense vowels. Turk et al. (1995) replicated
the Jusczyk et al. (1993) finding that English-
learning infants prefer stress-initial stimuli (using
non-words), and then examined how stress inter-
acted with syllable weight. They found that sylla-
ble weight was not a necessary condition to trig-
ger the preference: infants preferred stress-initial
stimuli even if the initial syllable was light. How-
ever, they also found that infants most strongly
preferred stimuli whose first syllable was both
stressed and heavy: infants preferred stress-initial
and heavy-initial stimuli to stress-initial and light-
initial stimuli. This result suggests that infants are
sensitive to syllable weight in determining typical
stress and rythmic patterns in their language.
2.1 Models
We will adopt the Adaptor Grammar framework
used by B?orschinger and Johnson (2014) to ex-
plore the utility of syllable weight as a cue
to word segmentation by way of its covariance
with stress. Adaptor Grammars are Probabilis-
tic Context Free Grammars (PCFGs) with a spe-
845
Syll
Onset
k
Rhyme
Nucleus
?
Coda
ts
(a) Basic syllable.
SyllIF
OnsetI
k
RhymeIF
NucleusI
?
CodaF
ts
(b) Mono-syllable with initial Rhyme.
SyllIF
OnsetI
k
RhymeF
NucleusF
?
CodaF
ts
(c) Mono-syllable with final Rhyme.
Figure 1: Different ways to incorporate phonotactics. It is not possible to capture word-final codas and
word initial rhymes in monosyllabic words with factors the size of a PCFG rule.
cial set of adapted non-terminal nodes. We un-
derline adapted non-terminals (X) to distinguish
them from non-adapted non-terminals (Y). While
a vanilla PCFG can only directly model regular-
ities that are expressed by a single re-write rule,
an Adaptor Grammar model caches entire subtrees
that are rooted at adapted non-terminals. Adaptor
Grammars can thus learn the internal structure of
words, such as syllables, syllable onsets, and syl-
lable rhymes, while still learning entire words as
well.
In Adaptor Grammars, parameters are associ-
ated with PCFG rules. While this has been a useful
factorization in previous work, it makes it difficult
to integrate syllable weight and syllable stress in
a linguistically natural way. A syllable is typically
analyzed as having an optional onset followed by a
rhyme, with the rhyme rewriting to a nucleus (the
vowel) followed by an optional coda, as in Fig-
ure 1a. We expect stress and syllable weight to be
useful primarily because initial syllables tend to be
different from non-initial syllables. However, dis-
tinguishing final from non-final codas should be
useful as well, due to the frequency of suffixes in
English, and the importance of edge phenomena in
phonology more generally (Brent and Cartwright,
1996). These principles come into conflict when
modeling monosyllabic words. If we say that a
monosyllable is an Initial and Final SyllIF, and
has an initial Onset and an initial Rhyme, as in
Figure 1b, then we can learn the initial/non-initial
generalization about stressed or heavy rhymes at
the expense of the generalization about final and
non-final codas. If we say that a monosyllable is
an initial onset with a final rhyme, the reverse oc-
curs: we can learn the final/non-final coda gen-
eralization at the expense of the initial/non-initial
regularities. If we split the symbols further, we?d
generalize even less: we?d essentially have to learn
the initial/non-initial patterns separately for mono-
syllables and polysyllables.
The most direct solution would introduce fac-
tors that are ?smaller? than a single PCFG rule. Es-
sentially, we would compute the score of a PCFG
rule in terms of multiple features of its right-hand
side, rather than a single ?one-hot? feature identi-
fying the expansion. We left this direction for fu-
ture work and instead carried out two experiments
using Adaptor Grammars that were designed to
work around this limitation.
Our first experiment focuses on modeling
the initial/non-initial distinction, leaving the
final/non-final coda distinction unmodeled. The
models in this experiment assume parallel struc-
tures for syllable weight and stress, and focus on
providing the most direct comparison between syl-
lable weight and stress with a strictly initial/non-
initial distinction. This first experiment shows that
observing dictionary stress is better early in learn-
ing, but that modeling syllable weight is better
later in learning. However, it is possible that sylla-
ble weight was more useful because modeling syl-
lable weight involves modeling the characteristics
of codas; the advantage may not have been due to
weight per se but due to having learned something
about the effects of suffixes on final codas.
Our second experiment focuses on modeling
some aspects of final codas at the expense of main-
taining a rigid parallelism in the structures for syl-
lable weight and stress. The models in this exper-
iment split only those symbols that are necessary
to bring stress or weight patterns into the expres-
sive power of the model, and focus on comparing
richer models of syllable weight and stress that
account for inital/internal/final distinctions. This
second experiment shows that observing dictio-
nary stress is better early in learning, and that
modeling syllable weight merely catches up to
846
Sentence ? Collocations3
+
(1)
Collocations3 ? Collocations2
+
(2)
Collocations2 ? Collocation
+
(3)
Collocation ? Word
+
(4)
Figure 2: Three levels of collocation; symbols fol-
lowed by
+
may occur one or more times.
stress without surpassing it. Moreover, a com-
bined stress-and-weight model does no better than
a stress model, suggesting that the weight gram-
mar?s contribution is fully redundant, for the pur-
poses of word segmentation, with the stress obser-
vations.
Together, these experiments suggest that sylla-
ble weight eventually encodes everything about
word segmentation that dictionary stress does, and
that any advantage that syllable weight has over
observing dictionary stress is entirely redundant
with knowledge of word-final codas.
3 Experiments
3.1 Adaptor Grammars
We follow B?orschinger and Johnson (2014) in us-
ing a 3-level collocation Adaptor Grammar, as in-
troduced by Johnson and Goldwater (2009) and
presented in Figure 2, as the backbone for all
models, including the baseline. A 3-level collo-
cation grammar assumes that words are grouped
into collocations of words that tend to appear with
each other, and that the collocations themselves
are grouped into larger collocations, up to three
levels of collocations. This collocational struc-
ture allows the model to capture strong word-
to-word dependencies without having to group
frequently-occuring word sequences into a single,
incorrect, undersegmented ?word? as the unigram
model tends to do (Johnson and Goldwater, 2009)
Word rewrites in different ways in Experiment I
and Experiment II, which will be explained in the
relevant experiment section.
3.2 Experimental Set-up
We applied the same experimental set-up used by
B?orschinger and Johnson (2014), to their dataset,
as described below. To understand how different
modeling assumptions interact with corpus size,
we train on prefixes of each corpus with increas-
ing input size: 100, 200, 500, 1,000, 2,000, 5,000,
and 10,000 utterances. Inference closely fol-
lowed B?orschinger and Johnson (2014) and John-
son and Goldwater (2009). We set our hyperpa-
rameters to encourage onset maximization. The
hyperparameter for syllable nodes to rewrite to
an onset followed by a rhyme was 10, and the
hyperparameter for syllable nodes to rewrite to a
rhyme only was 1. Similarly, the hyperparame-
ter for rhyme nodes to include a coda was 1, and
the hyperparameter for rhyme nodes to exclude
the coda was 10. All other hyperparameters spec-
ified vague priors. We ran eight chains of each
model for 1,000 iterations, collecting 20 samples
with a lag of 10 iterations between samples and a
burn-in of 800 iterations. We used the same batch-
initialization and table-label resampling to encour-
age the model to mix.
After gathering the samples, we used them to
perform a single minimum Bayes risk decoding
of a separate, held-out test set. This test set was
constructed by taking the last 1,000 utterances of
each corpus. We use a common test-set instead
of just evaluating on the training data to ensure
that performance figures are comparable across in-
put sizes; when we see learning curves slope up-
ward, we can be confident that the increase is due
to learning rather than easier evaluation sets.
We measured our models? performance with the
usual token f-score metric (Brent, 1999), the har-
monic mean of how many proposed word tokens
are correct (token precision) and how many of the
actual word tokens are recovered (token recall).
For example, a model may propose ?the in side?
when the true segmentation is ?the inside.? This
segmentation would have a token precision of
1
3
,
since one of three predicted words matches the
true word token (even though the other predicted
words are valid word types), and a token recall of
1
2
, since it correctly recovered one of two words,
yield a token f-score of 0.4.
3.3 Dataset
We evaluated on a dataset drawn from the Alex
portion of the Providence corpus (Demuth et al.,
2006). This dataset contains 17, 948 utterances
with 72, 859 word tokens directed to one child
from the age of 16 months to 41 months. We used
a version of this dataset that contained annota-
tions of primary stress that B?orschinger and John-
son (2014) added to this input using an extended
847
RhymeI ? HeavyRhyme
RhymeI ? LightRhyme
Rhyme ? HeavyRhyme
Rhyme ? LightRhyme
HeavyRhyme ? LongVowel
HeavyRhyme ? Vowel Coda
LightRhyme ? ShortVowel
(a) Weight-sensitive grammar
RhymeI ? RhymeS
RhymeI ? RhymeU
Rhyme ? RhymeS
Rhyme ? RhymeU
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
(b) Stress-sensitive grammar
RhymeI ? Vowel (Coda)
Rhyme ? Vowel (Coda)
(c) Baseline grammar
RhymeI ? HeavyRhymeS
RhymeI ? HeavyRhymeU
RhymeI ? LightRhymeS
RhymeI ? LightRhymeU
Rhyme ? HeavyRhymeS
Rhyme ? HeavyRhymeU
Rhyme ? LightRhymeS
Rhyme ? LightRhymeU
HeavyRhymeS ? LongVowel Stress
HeavyRhymeS ? LongVowel Stress Coda
HeavyRhymeU ? LongVowel
HeavyRhymeU ? LongVowel Coda
LightRhymeS ? ShortVowel Stress
LightRhymeU ? ShortVowel
(d) Combined grammar
Figure 3: Experiment I Grammars
version of CMUDict (cmu, 2008).
1
The mean
number of syllables per word token was 1.2, and
only three word tokens had more than five sylla-
bles. Of the 40, 323 word tokens with a stressed
syllable, 27, 258 were monosyllabic. Of the
13, 065 polysyllabic word tokens with a stressed
syllable, 9, 931 were stress-initial. Turning to the
32, 536 word tokens with no stress (i.e., the func-
tion words), all but 23 were monosyllabic (the 23
were primarily contractions, such as ?couldn?t?).
3.4 Experiment I: Parallel Structures
The goal of this first experiment is to provide the
most direct comparison possible between gram-
mars that attend to stress cues and grammars that
attend to syllable weight cues. As these are both
hypothesized to be useful by way of an initial/non-
initial distinction, we defined a word to be an ini-
tial syllable SyllI followed by zero to three sylla-
bles, and syllables to consist of an optional onset
1
This dataset and these Adap-
tor Grammar models are available at:
http://web.science.mq.edu.au/?jpate/stress/
and a rhyme:
Word ? SyllI (Syll)
{0,3}
(5)
SyllI ? (OnsetI) RhymeI (6)
Syll ? (Onset) Rhyme (7)
In the baseline grammar, presented in Figure 3c,
rhymes rewrite to a vowel followed by an optional
consonant coda. Rhymes then rewrite to be heavy
or light in the weight grammar, as in Figure 3a, to
be stressed or unstressed in the stress grammar, as
in Figure 3b. In the combination grammar, rhymes
rewrite to be heavy or light and stressed or un-
stressed, as in Figure 3d. LongVowel and Short-
Vowel both re-write to all vowels. An additional
grammar that restricted them to rewrite to long and
short vowels, respectively, led to virtually identi-
cal performance, suggesting that vowel quantity
can be learned for the purposes of word segmenta-
tion from distributional cues. We will also present
evidence that the model did manage to learn most
of the contrast.
Figure 4 presents learning curves for the gram-
mars in this parallel structured comparison. We
see that observing stress without modeling weight
848
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctions
between Stressed/Unstressed and Heavy/Light syllable rhymes.
?
?
?
?
?
a?
a?
?
e?
?
i?
o?
?
??
u?
LongVowel ShortVowel Vowel
Vo
we
l
1 10 100 1000 7000
Vowel counts by quantity
Figure 5: Heatmap of learned vowels in the Ex-
periment I weight-only grammar. Each cell cor-
responds to the count of a particular vowel being
analyzed as one of the three vowel types. Diph-
thongs are rarely ShortVowel.
outperforms both the baseline and the weight-only
grammar early in learning. The weight-only gram-
mar rapidly improves in performance at larger
training data sizes, increasing its advantage over
the baseline, while the advantage of the stress-
only grammar slows and appears to disappear at
the largest training data size. At 10,000 utterances,
the improvement of the weight-only grammar over
the stress-only grammar is significant according to
an independent samples t-test (t = 7.2, p < 0.001,
14 degrees of freedom). This pattern suggests that
annotated dictionary stress is easy to take advan-
tage of at low data sizes, but that, with sufficient
data, syllable weight can provide even more in-
formation about word boundaries. The best over-
all performance early in learning is obtained by
the combined grammar, suggesting that syllable
weight and dictionary stress provide information
about word segmentation that is not redundant.
An examination of the final segmentation sug-
gests that the weight grammar has learned that
initial syllables tend to be heavy. Specifically,
across eight runs, 98.1% of RhymeI symbols
rewrote to HeavyRhyme, whereas only 54.5% of
Rhyme symbols (i.e. non-initial rhymes) rewrote
to HeavyRhyme.
849
Model Mean TF Std. Dev.
noweight:nostress 0.830 0.005
noweight:stress 0.831 0.008
weight:nostress 0.861 0.008
weight:stress 0.861 0.008
Table 1: Segmentation Token F-score for Experi-
ment I at 10,000 utterances across eight runs.
We also examined the final segmentation to see
well the model learned the distinction between
long vowels and short vowels. Figure 5 presents a
heatmap, with colors on a log-scale, showing how
many times each vowel label rewrote to each pos-
sible vowel in the (translated to IPA). Although the
quantity generalisations are not perfect, we do see
a general trend where ShortVowel rarely rewrites
to diphthongs.
3.5 Experiment II: Word-final Codas
Experiment I suggested that, under a ba-
sic initial/non-initial distinction, syllable weight
eventually encodes more information about word
boundaries than does dictionary stress. This is
a surprising result, since we initially investigated
syllable weight as a noisy proxy for dictionary
stress. One possible source of the ?extra? advan-
tage that the syllable weight grammar exhibited
has to do with the importance of word-final codas,
which can encode word-final morphemes in En-
glish (Brent and Cartwright, 1996). Even though
the grammars did not explicitly model them, the
weight grammar could implicitly capture a bias for
or against having a coda in non-initial position,
while the stress grammar could not. This is be-
cause most word tokens are one or two syllables,
and only one of the two rhyme types of the weight
grammar included a coda. Thus, the HeavyRhyme
symbol could simultaneously capture the most im-
portant aspects of both stress and coda constraints.
To see if the extra advantage of the syllable
weight grammar can be attributed to the influence
of word-final codas, we formulated a set of gram-
mars that model word-final codas and also can
learn stress and/or syllable weight patterns. These
grammars are more similar in structure to the ones
that B?orschinger and Johnson (2014) used. For the
baseline and weight grammar, we again defined
words to consist of up to four syllables with an ini-
tial SyllI syllable, but this time distinguished final
syllables SyllF in polysyllabic words. The non-
stress grammars use the following rules for pro-
ducing syllables:
Word ? SyllIF (8)
Word ? SyllI (Syll)
{0,2}
SyllF (9)
SyllIF ? (OnsetI) RhymeI (10)
SyllI ? (OnsetI) RhymeI (11)
Syll ? (Onset) Rhyme (12)
SyllF ? (Onset) RhymeF (13)
For the stress grammar, we followed
B?orschinger and Johnson (2014) in distin-
guishing stressed and unstressed syllables, rather
than simply stressed rhymes as in Experiment I,
to allow the model to learn likely stress patterns
at the word level. A word can consist of up to
four syllables, and any syllable and any number
of syllables may be stressed, as in Figure 6a.
The baseline grammar is similar to the previous
one, except it distinguishes word-final codas, as
in Figure 6b. The weight grammar, presented in
Figure 6c, rewrites rhymes to a nucleus followed
by an optional coda and distinguishes nuclei in
open syllables according to their position in the
word. The stress grammar, presented in Figure 6d,
is the all-stress-patterns model (without the unique
stress constraint) B?orschinger and Johnson (2014).
This grammar introduces additional distinctions at
the syllable level to learn likely stress patterns,
and distinguishes final from non-final codas. The
combined model is identical to the stress model,
except Vowel non-terminals in closed and word-
internal syllables are replaced with Nucleus non-
terminals, and Vowel non-terminals in word-inital
(-final) open syllables are replaced with NucleusI
(NucleusF) non-terminals.
To summarize, the stress models distinguish
stressed and unstressed syllables in initial, final,
and internal position. The weight models distin-
guish the vowels of initial open syllables, the vow-
els of final open syllables, and other vowels, al-
lowing them to take advantage of an important cue
from syllable weight for word segmentation: if an
initial vowel is open, it should usually be long.
Figure 7 shows segmentation performance on
the Alex corpus with these more complete models.
While the performance of the weight grammars is
virtually unchanged compared to Figure 4, the two
grammars that do not model syllable weight im-
prove dramatically. This result supports our pro-
posal that much of the advantage of the weight
850
Word ? {SyllUIF|SyllSIF}
Word ? {SyllUI|SyllSI} {SyllU|SyllS}
{0,2}
{SyllUF|SyllSF}
(a) The all-patterns stress model
Rhyme ? Vowel (Coda)
RhymeF ? Vowel (CodaF)
(b) Baseline grammar
RhymeI ? NucleusI
RhymeI ? Nucleus Coda
Rhyme ? Nucleus (Coda)
RhymeF ? NucleusF
RhymeF ? Nucleus CodaF
(c) Weight-sensitive grammar
SyllSIF ? OnsetI RhymeSF
SyllUIF ? OnsetI RhymeUF
SyllSI ? Onset RhymeS
SyllUI ? Onset RhymeU
SyllSF ? Onset RhymeSF
SyllUF ? Onset RhymeUF
RhymeSI ? Vowel Stress (Coda)
RhymeUI ? Vowel (Coda)
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
RhymeSF ? Vowel Stress (CodaF)
RhymeUF ? Vowel (CodaF)
(d) Stress-sensitive grammar
Figure 6: Experiment II Grammars.
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotactics
that exploit Stress and Weight.
851
Model Mean TF Std. Dev.
noweight:nostress 0.846 0.007
noweight:stress 0.880 0.005
weight:nostress 0.865 0.011
weight:stress 0.875 0.005
Table 2: Segmentation Token F-score for Experi-
ment II at 10,000 utterances across eight runs.
grammars over stress in Experiment I was due to
modeling of word-final coda phonotactics.
Table 2 presents token f-score at 10,000 train-
ing utterances averaged across eight runs, along
with the standard deviation in f-score. We see that
the noweight:nostress grammar is several standard
deviations than the grammars that model sylla-
ble weight and/or stress, while the syllable weight
and/or stress grammars exhibit a high degree of
overlap.
4 Conclusion
We have presented computational modeling exper-
iments that suggest that syllable weight (eventu-
ally) encodes nearly everything about word seg-
mentation that dictionary stress does. Indeed,
our experiments did not find a persistent advan-
tage to observing stress over modeling syllable
weight. While it is possible that a different mod-
eling approach might find such a persistent advan-
tage, this advantage could not provide more than
13% absolute F-score. This result suggests that
children may be able to learn and exploit impor-
tant rhythm cues to word boundaries purely on
the basis of segmental input. However, this result
also suggests that annotating input with dictionary
stress has missed important aspects of the role
of stress in word segmentation. As mentioned,
Turk et al. (1995) found that infants preferred ini-
tial light syllables to be stressed. Such a prefer-
ence obviously cannot be learned by attending to
syllable weight alone, so infants who have learned
weight distinctions must also be sensitive to non-
segmental acoustic correlates to stress. There was
no long-term advantage to observing stress in ad-
dition to attending to syllable weight in our mod-
els, however, suggesting that annotated dictionary
stress does not capture the relevant non-segmental
phonetic detail. More modeling is necessary to as-
sess the non-segmental phonetic features that dis-
tinguish stressed light syllables from unstressed
light syllables.
This investigation also highlighted a weakness
of current Adaptor Grammar models: the ?small-
est? factors are the size of one PCFG rule. Allow-
ing further factorizations, perhaps using feature
functions of a rule?s right-hand side, would allow
models to capture finer-grained distinctions with-
out fully splitting the symbols that are involved.
References
Benjamin B?orschinger and Mark Johnson. 2014. Ex-
ploring the role of stress in Bayesian word segmen-
tation using Adaptor Grammars. Transactions of the
ACL, 2:93?104.
Michael R Brent and Timothy A Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition, 61:93?125.
Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71?105.
Nick Campbell and Mary Beckman. 1997. Stress,
prominence, and spectral tilt. In Proceedings of an
ESCA workshop, pages 67?70, Athens, Greece.
Morten H. Christiansen and Suzanne L Curtin. 1999.
The power of statistical learning: No need for alge-
braic rules. In Proceedings of the 21st annual con-
ference of the Cognitive Science Society.
Morten H. Christiansen, Joseph Allen, and Mark S.
Seidenberg. 1998. Learning to segment speech
using multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13:221?268.
2008. The CMU pronouncing dictionary.
http://www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Anne Cutler and David M Carter. 1987. The predom-
inance of strong initial syllables in the English vo-
cabulary. Computer Speech & Language, 2(3):133?
142.
Katherine Demuth, Jennifer Culbertson, and Jennifer
Alter. 2006. Word-minimality, epenthesis, and coda
licensing in the acquisition of English. Language
and Speech, 49:137?174.
Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL 2013, pages 117?
126. Association for Computational Linguistics.
D B Fry. 1955. Duration and intensity as physical cor-
relates of linguistic stress. J. Acoust. Soc. of Am.,
27:765?768.
D B Fry. 1958. Experiments in the perception of stress.
Language and Speech, 1:126?152.
852
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparametric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 317?325. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B Schoelkopf, J Platt, and T Hoffmann,
editors, Advances in Neural Information Processing
Systems, volume 19. The MIT Press.
Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.
1993. Infants? preference for the predominant stress
patterns of English words. Child Development,
64(3):675?687.
Peter W Juszcyk, Derek M Houston, and Mary New-
some. 1999. The beginnings of word segmentation
in English-learning infants. Cognitive Psychology,
39(3?4):159?207.
Mark Liberman and Alan Prince. 1977. On stress and
linguistic rhythm. Linguistic Inquiry, 8(2):249?336,
Spring.
Constantine Lignos and Charles Yang. 2010. Reces-
sion segmentation: simpler online word segmenta-
tion using limited resources. In Proceedings of ACL
2010, pages 88?97. Association for Computational
Linguistics.
Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the fifteenth confer-
ence on computational natural language learning,
pages 29?38. Association for Computational Lin-
guistics.
Constantine Lignos. 2012. Infant word segmentation:
An incremental, integrated model. In Proceedings
of the West Coast Conference on Formal Linguistics
30.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: sub-
segmental variation in segmental cues. Journal of
Child Language, 37(3):513?543.
Erik D Thiessen and Jenny R Saffran. 2003. When
cues collide: use of stress and statistical cues to word
boundaries by 7-to-9-month-old infants. Develop-
mental Psychology, 39(4):706?716.
Alice Turk, Peter W Jusczyk, and Louann Gerken.
1995. Do English-learning infants use syllable
weight to determine stress? Language and Speech,
38(2):143?158.
Charles Yang. 2004. Universal grammar, statistics or
both? Trends in Cognitive Science, 8(10):451?456.
853
Weighted and Probabilistic Context-Free
Grammars Are Equally Expressive
Noah A. Smith?
Carnegie Mellon University
Mark Johnson??
Brown University
This article studies the relationship between weighted context-free grammars (WCFGs), where
each production is associated with a positive real-valued weight, and probabilistic context-free
grammars (PCFGs), where the weights of the productions associated with a nonterminal are
constrained to sum to one. Because the class of WCFGs properly includes the PCFGs, one
might expect that WCFGs can describe distributions that PCFGs cannot. However, Z. Chi
(1999, Computational Linguistics, 25(1):131?160) and S. P. Abney, D. A. McAllester, and
P. Pereira (1999, In Proceedings of the 37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 542?549, College Park, MD) proved that every WCFG distribution
is equivalent to some PCFG distribution. We extend their results to conditional distributions,
and show that every WCFG conditional distribution of parses given strings is also the condi-
tional distribution defined by some PCFG, even when the WCFG?s partition function diverges.
This shows that any parsing or labeling accuracy improvement from conditional estimation of
WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov
models (HMMs) is due to the estimation procedure rather than the change in model class,
because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs,
respectively.
1. Introduction
In recent years the field of computational linguistics has turned to machine learning
to aid in the development of accurate tools for language processing. A widely used
example, applied to parsing and tagging tasks of various kinds, is a weighted grammar.
Adding weights to a formal grammar allows disambiguation (more generally, ranking
of analyses) and can lead to more efficient parsing. Machine learning comes in when we
wish to choose those weights empirically.
The predominant approach for many years was to select a probabilistic model?
such as a hidden Markov model (HMM) or probabilistic context-free grammar
(PCFG)?that defined a distribution over the structures allowed by a grammar. Given a
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15217, USA.
E-mail: nasmith@cs.cmu.edu.
?? Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA.
E-mail: Mark Johnson@brown.edu.
Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for
publication: 30 March 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
treebank, maximum likelihood estimation can be applied to learn the probability values
in the model.
More recently, new machine learning methods have been developed or ex-
tended to handle models of grammatical structure. Notably, conditional estimation
(Ratnaparkhi, Roukos, and Ward 1994; Johnson et al 1999; Lafferty, McCallum, and
Pereira 2001), maximum margin estimation (Taskar et al 2004), and unsupervised con-
trastive estimation (Smith and Eisner 2005) have been applied to structured models.
Weighted grammars learned in this way differ in two important ways from traditional,
generative models. First, the weights can be any positive value; they need not sum
to one. Second, features can ?overlap,? and it can be difficult to design a generative
model that uses such features. The benefits of new features and discriminative training
methods are widely documented and recognized.
This article focuses specifically on the first of these differences. It compares the
expressive power of weighted context-free grammars (WCFGs), where each rule is
associated with a positive weight, to that of the corresponding PCFGs, that is, with
the same rules but where the weights of the rules expanding a nonterminal must sum
to one.
One might expect that because normalization removes one or more degrees of free-
dom, unnormalized models should be more expressive than normalized, probabilistic
models. Perhaps counterintuitively, previous work has shown that the classes of proba-
bility distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and
Pereira 1999; Chi 1999).
However, this result does not completely settle the question about the expressive
power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional
distribution from strings to trees even if it does not define a probability distribution
over trees. Because these conditional distributions are what are used in classification
tasks and related tasks such as parsing, we need to know the relationship between
the classes of conditional distributions defined by WCFGs and PCFGs. In fact we
extend the results of Chi and of Abney et al, and show that WCFGs and PCFGs both
define the same class of conditional distribution. Moreover, we present an algorithm
for converting an arbitrary WCFG that defines a conditional distribution over trees
given strings but possibly without a finite partition function into a PCFG with the
same rules as the WCFG and that defines the same conditional distribution over trees
given strings.
This means that maximum conditional likelihood WCFGs are non-identifiable, be-
cause there are an infinite number of rule weights all of which maximize the conditional
likelihood.
2. Weighted CFGs
A CFG G is a tuple ?N, S,?, R? where N is a finite set of nonterminal symbols, S ? N is
the start symbol, ? is a finite set of terminal symbols (disjoint from N), and R is a set of
production rules of the form X ? ? where X ? N and ? ? (N ? ?). A WCFG associates
a positive number called the weight with each rule in R.1 We denote by ?X?? the weight
attached to the rule X ? ?, and the vector of rule weights by ? = {?A?? : A ? ? ? R}.
A weighted grammar is the pair G? = ?G,??.
1 Assigning a weight of zero to a rule equates to excluding it from R.
478
Smith and Johnson Weighted and Probabilistic CFGs
Unless otherwise specified, we assume a fixed underlying context-free grammar G.
Let ?(G) be the set of (finite) trees that G generates. For any ? ? ?(G), the score s?(?) of
? is defined as follows:
s?(?) =
?
(X??)?R
(?X??)
f (X??;?) (1)
where f (X ? ?; ?) is the number of times X ? ? is used in the derivation of the tree ?.
The partition function Z(?) is the sum of the scores of the trees in ?(G).
Z(?) =
?
???(G)
s?(?)
Because we have imposed no constraints on?, the partition function need not equal
one; indeed, as we show subsequently the partition function need not even exist. If Z(?)
is finite then we say that the WCFG is convergent, and we can define a Gibbs probability
distribution over ?(G) by dividing by Z(?):
P?(?) =
s?(?)
Z(?)
A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the
rules expanding each nonterminal is one:
?X ? N,
?
(X??)?R
?X?? = 1 (2)
It is easy to show that if G? is a PCFG then Z(?) ? 1. A tight PCFG is a PCFG G?
for which Z(?) = 1. Necessary conditions and sufficient conditions for a PCFG to be
tight are given in several places, including Booth and Thompson (1973) and Wetherell
(1980).
We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999).
Let G = {G?} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in G
all have the same underlying grammar G but differ in their rule weight vectors ?).
Let GZ<? be the subset of G for which the partition function Z(?) is finite, and let
GZ=? = G \ GZ<? be the subset of G with an infinite partition function. Further let GPCFG
denote the set of PCFGs based on G. In general, GPCFG is a proper subset of GZ<?, that
is, every PCFG is also a WCFG, but because there are weight vectors ? that don?t obey
Equation 2, not all WCFGs are PCFGs.
However, this does not mean that WCFGs are more expressive than PCFGs. As
noted above, the WCFGs GZ<? define Gibbs distributions. Again, for a fixed G, let
PZ<? be the probability distributions over the trees ?(G) defined by the WCFGs GZ<?
and let PPCFG be the probability distributions defined by the PCFGs GPCFG. Chi (Propo-
sition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that PZ<? = PPCFG,
namely, that every WCFG probability distribution is in fact generated by some PCFG.
There is no ?PZ=?? because there is no finite normalizing term Z(?) for such WCFGs.
479
Computational Linguistics Volume 33, Number 4
2.1 Chi?s Algorithm for Converting WCFGs to Equivalent PCFGs
Chi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG. Let
G? be a WCFG in GZ<?. If X ? N is a nonterminal, let ?X(G) be the set of trees rooted
in X that can be built using G. Then define:
ZX(?) =
?
???X (G)
s?(?)
For simplicity, let Zt(?) = 1 for all t ? ?. Chi demonstrated that G? ? GZ<? implies
that ZX(?) is finite for all X ? N ? ?.
For every rule X ? ? in R define:
??X?? =
?X??
|?|
?
i=1
Z?i (?)
ZX(?)
where ?i is the ith element of ? and |?| is the length of ?. Chi proved that G?? is a PCFG
and that P?? (?) = s?(?)/Z(?) for all trees ? ? ?(G).
Chi did not describe how to compute the nonterminal-specific partition functions
ZX(?). The ZX(?) are related by equations of the form
ZX(?) =
?
?:X???R
?X??
|?|
?
i=1
Z?i (?)
which constitute a set of nonlinear polynomial equations in ZX(?). Although a numeri-
cal solver might be employed to find the ZX(?), we have found that in practice iterative
propagation of weights following the method described by Stolcke (1995, Section 4.7.1)
converges quickly when Z(?) is finite.
3. Classifiers and Conditional Distributions
A common application of weighted grammars is parsing. One way to select a parse tree
for a sentence x is to choose the maximum weighted parse that is consistent with the
observation x:
??(x) = argmax
???(G):y(?)=x
s?(?) (3)
where y(?) is the yield of ?. Other decision criteria exist, including minimum-loss de-
coding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic
programming algorithm to optimize over trees, and they also exploit the conditional
distribution of trees given sentence observations. A WCFG defines such a conditional
distribution as follows:
P?(? | x) =
s?(?)
?
????(G):y(?? )=x s?(??)
=
s?(?)
Zx(?)
(4)
480
Smith and Johnson Weighted and Probabilistic CFGs
where Zx(?) is the sum of scores for all parses of x. Note that Equation (4) will be
ill-defined when Zx(?) diverges. Because Zx(?) is constant for a given x, solving Equa-
tion (3) is equivalent to choosing ? to maximize P?(? | x).
We turn now to classes of these conditional distribution families. Let CZ<? (CPCFG)
be the class of conditional distribution families that can be expressed by grammars in
GZ<? (GPCFG, respectively). It should be clear that, because PZ<? = PPCFG, CZ<? = CPCFG
since a conditional family is derived by normalizing a joint distribution by its marginals.
We now define another subset of G. Let GZn<? contain every WCFG G? = ?G,??
such that, for all n ? 0,
Zn(?) =
?
???(G):|y(?)|=n
s?(?) < ? (5)
(Note that, to be fully rigorous, we should quantify n in GZn<?, writing ?G?nZn(?)<?.?
We use the abbreviated form to keep the notation crisp.) For any G? ? GZn<?, it also
follows that, for any x ? L(G), Zx(?) < ?; the converse holds as well.
It follows that any WCFG in GZn<? can be used to construct a conditional dis-
tribution of trees given the sentence, for any sentence x ? L(G). To do so, we only
need to normalize s?(?) by Zx(?) (Equation (4)). Let GZn=? contain the WCFGs where
some Zn(?) diverge; this is a subset of GZ=?.2 To see that GZ=? ? GZn<? = ?, consider
Example 1.
Example 1
?A?A A = 1, ?A?a = 1
This grammar produces binary structures over strings in a+. Every such tree receives
score 1. Because there are infinitely many trees, Z(?) diverges. But for any fixed string
an, the number of parse trees is finite. This grammar defines a uniform conditional
distribution over all binary trees, given the string.
For a grammar G? to be in GZn<?, it is sufficient that, for every nonterminal X ? N,
the sum of scores of all cyclic derivations X ?+ X be finite. Conservatively, this can be
forced by eliminating epsilon rules and unary rules or cycles altogether, or by requiring
the sum of cyclic derivations for every nonterminal X to sum to strictly less than one.
Example 2 gives a grammar in GZn=? with a unary cyclic derivation that does not
?dampen.?
Example 2
?A?A A = 1, ?A?A = 1, ?A?a = 1
For any given an, there are infinitely many equally weighted parse trees, so even the
set of trees for an cannot be normalized into a distribution (Zn(?) =?). Generally
speaking, if there exists a string x ? L(G) such that the set of trees that derive x is not
2 Here, full rigor would require quantification of n, writing ?G?nZn (?)=?.?
481
Computational Linguistics Volume 33, Number 4
finite (i.e., there is no finite bound on the number of derivations for strings in L(G); the
grammar in Example 2 is a simple example), then GZn<? and GZ<? are separable.3
For a given CFG G, a conditional distribution over trees given strings is a function
?? ? (?(G) ? [0, 1]). Our notation for the set of conditional distributions that can be
expressed by GZn<? is CZn<?. Note that there is no ?CZn=?? because an infinite Zn(?)
implies an infinite Z(x) for some sentence x and therefore an ill-formed conditional fam-
ily. Indeed, it is difficult to imagine a scenario in computational linguistics in which non-
dampening cyclic derivations (WCFGs in GZn=?) are desirable, because no linguistic
explanations depend crucially on arbitrary lengthening of cyclic derivations.
We now state our main theorem.
Theorem 1
For a given CFG G, CZn<? = CZ<?.
Proof
Suppose we are given weights ? for G such that G? ? GZn<?. We will show that the
sequence Z1(?), Z2(?), ... is bounded by an exponential function of n, then describe
a transformation on ? resulting in a new grammar G?? that is in GZ<? and defines
the same family of conditional distributions (i.e., ?? ? ?(G),?x ? L(G), P?(? | x) =
P?? (? | x)).
First we prove that for all n ? 1 there exists some c such that Zn(?) ? cn. Given G?,
we construct G??? in CNF that preserves the total score for any x ? L(G). The existence
of G??? was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm for
constructing the value-preserving weighted grammar G??? from G?.
Note that G? = ?N?, S,?, R??, containing possibly more nonterminals and rules than G.
The set of (finite) trees ?
(
G?
)
is different from ?(G); the new trees must be binary and
may include new nonterminals.
Next, collapse the nonterminals in N? into one nonterminal, S. The resulting gram-
mar is G??? = ??{S}, S,?, R??, ???. R? contains the rule S ? S S and rules of the form S ? a
for a ? ?. The weights of these rules are
??S?S S = ? = max(1,
?
(X?Y Z)?R?
??X?Y Z) (6)
??S?a = ? = max(1,
?
(X?b)?R?
??X?b) (7)
The grammar G??? will allow every tree allowed by G??? (modulo labels on nonterminal
nodes, which are now all S). It may allow some additional trees. The score of a tree
under G??? will be at least as great as the sum of scores of all structurally equivalent trees
under G???, because ? and ? are defined to be large enough to absorb all such scores. It
follows that, for all x ? L(G):
s??(x) ? s??(x) = s?(x) (8)
3 We are grateful to an anonymous reviewer for pointing this out, and an even stronger point: for a given
G, G and GZn<? have a nonempty set-difference if and only if G has infinite ambiguity (some x ? L(G)
has infinitely many parse trees).
482
Smith and Johnson Weighted and Probabilistic CFGs
Summing over all trees of any given yield length n, we have
Zn(??) ? Zn(??) = Zn(?) (9)
G? generates all possible binary trees (with internal nodes undifferentiated) over
a given sentence x in L(G). Every tree generated by G? with yield length n will have
the same score: ?n?1?n, because every binary tree with n terminals has exactly n ? 1
nonterminals. Each tree corresponds to a way of bracketing n items, so the total number
of parse trees generated by G? for a string of length n is the number of different ways
of bracketing a sequence of n items. The total number of unlabeled binary bracketings
of an n-length sequence is the nth Catalan number Cn (Graham, Knuth, and Patashnik
1994), which in turn is bounded above by 4n (Vardi 1991). The total number of strings of
length n is |?|n. Therefore
Zn(??) = Cn|?|n?n?1?n ? 4n|?|n?n?1?n ? (4|?|??)n (10)
We now transform the original weights ? as follows. For every rule (X ? ?) ? R,
let
??X?? ?
?X??
(8|?|??)t(?) (11)
where t(?) is the number of ? symbols appearing in ?. This transformation results in
every n-length sentence having its score divided by (8|?|??)n. The relative scores of
trees with the same yield are unaffected, because they are all scaled equally. Therefore
G?? defines the same conditional distribution over trees given sentences as G?, which
implies that G? and G?? have the same highest scoring parses. Note that any sufficiently
large value could stand in for 8|?|?? to both (a) preserve the conditional distribution
and (b) force Zn(?) to converge. We have not found the minimum such value, but 8|?|??
is sufficiently large.
The sequence of Zn(?) now converges:
Zn(??) ?
Zn(?)
(8|?|??)n ?
(
1
2
)n
(12)
Hence Z(??) =
??
n=0 Zn(?
?) ? 2 and G?? ? GZ<?. 
Corollary 1
Given a CFG G, CZn<? = CPCFG.
Proof
By Theorem 1, CZn<? = CZ<?. We know that PZ<? = PPCFG, from which it follows that
CZ<? = CPCFG. Hence CZn<? = CPCFG. To convert a WCFG in CZn<? into a PCFG, first
apply the transformation in the proof of Theorem 1 to get a convergent WCFG, then
apply Chi?s method (our Section 2.1). 
483
Computational Linguistics Volume 33, Number 4
Figure 1
A graphical depiction of the primary result of this article. Given a fixed set of productions, G is
the set of WCFGs with exactly those productions (i.e., they vary only in the production weights),
GZ<? is the subset of G that defines (joint) probability distributions over trees (i.e., that have a
finite partition function Z) and PZ<? is the set of probability distributions defined by grammars
in GZ<?. Chi (1999) and Abney, McAllester, and Pereira (1999) proved that PZ<? is
the same as PPCFG, the set of probability distributions defined by the PCFG GPCFG with the same
productions as G. Thus even though the set of WCFGs properly includes the set of PCFGs,
WCFGs define exactly the same probability distributions over trees as PCFGs. This article
extends these results to conditional distributions over trees conditioned on their strings. Even
though the set GZn<? of WCFGs that define conditional distributions may be larger than GZ<?
and properly includes GPCFG, the set of conditional distributions CZn<? defined by GZn<? is
equal to the set of conditional distributions CPCFG defined by PCFGs. Our proof is constructive:
we give an algorithm which takes as input a WCFG G ? GZn<? and returns a PCFG which
defines the same conditional distribution over trees given strings as G.
Figure 1 presents the main result graphically in the context of earlier results.
4. HMMs and Related Models
Hidden Markov models (HMMs) are a special case of PCFGs. The structures they
produce are labeled sequences, which are equivalent to right-branching trees. We can
write an HMM as a PCFG with restricted types of rules. We will refer to the unweighted,
finite-state grammars that HMMs stochasticize as ?right-linear grammars.? Rather than
using the production rule notation of PCFGs, we will use more traditional HMM nota-
tion and refer to states (interchangeable with nonterminals) and paths (interchangeable
with parse trees).
In the rest of the article we distinguish between HMMs, which are probabilistic
finite-state automata locally normalized just like a PCFG, and chain-structured Markov
random fields (MRFs; Section 4.1), in which moves or transitions are associated with
positive weights and which are globally normalized like a WCFG.4 We also distinguish
two different types of dependency structures in these automata. Abusing the standard
terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal
symbols, whereas in a Moore automaton the states emit terminal symbols.5
4 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFs
also have the Markov property and define the same joint and conditional distributions as HMMs.
5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955;
Moore 1956); we ignore the input symbols here.
484
Smith and Johnson Weighted and Probabilistic CFGs
A Mealy HMM defines a probability distribution over pairs ?x,??, where x is a
length-n sequence ?x1, x2, ..., xn? ? ?n and ? = ??0,?1,?2, ...,?n? ? Nn+1 is a state (or
nonterminal) path. The distribution is given by
PHMM(x,?) =
(
n
?
i=1
p(xi,?i | ?i?1)
)
p(STOP | ?n) (13)
?0 is assumed, for simplicity, to be constant and known; we also assume that every
state transition emits a symbol (no 
 arcs), an assumption made in typical tagging and
chunking applications of HMMs. We can convert a Mealy HMM to a PCFG by including,
for every tuple ?x,?,?? (x ? ? and ?,? ? N) such that p(x,? | ?) > 0, the rule ? ? x ?,
with the same probability as the corresponding HMM transition. For every ? such that
p(STOP | ?), we include the rule ? ? 
, with probability p(STOP | ?).
A Moore HMM factors the distribution p(x,? | ?) into p(x | ?) ? p(? | ?). A Moore
HMM can be converted to a PCFG by adding a new nonterminal ?? for every state ?
and including the rules ? ? ?? (with probability p(? | ?)) and ?? ? x ? (with probability
p(x | ?)). Stop probabilities are added as in the Mealy case. For a fixed number of states,
Moore HMMs are less probabilistically expressive than Mealy HMMs, though we can
convert between the two with a change in the number of states.
We consider Mealy HMMs primarily from here on. If we wish to define the distri-
bution over paths given words, we conditionalize
PHMM(? | x) =
(
?n
i=1 p(xi,?i | ?i?1)
)
p(STOP | ?n)
?
???Nn+1
(
?n
i=1 p(xi,?
?
i | ??i?1)
)
p(STOP | ??n)
(14)
This is how scores are assigned when selecting the best path given a sequence.
For a grammar G that is right-linear, we can therefore talk about the set of HMM
(right-linear) grammars GHMM, the set of probability distributions PHMM defined by those
grammars, and CHMM, the set of conditional distributions over state paths (trees) that they
define.6
4.1 Mealy Markov Random Fields
When the probabilities in Mealy HMMs are replaced by arbitrary positive weights, the
production rules can be seen as features in a Gibbs distribution. The resulting model
is a type of MRF with a chain structure; these have recently become popular in natural
language processing (Lafferty, McCallum, and Pereira 2001). Lafferty et al?s formulation
defined a conditional distribution over paths given sequences by normalizing for each
sequence x:
PCMRF(? | x) =
(
n
?
i=1
??i?1,xi,?i
)
??n,STOP
Zx(?)
(15)
6 Of course, the right-linear grammar is a CFG, so we could also use the notation GPCFG, PPCFG, and CPCFG.
485
Computational Linguistics Volume 33, Number 4
Using a single normalizing term Z(?), we can also define a joint distribution over
states and paths:
PCMRF(x,?) =
(
n
?
i=1
??i?1,xi,?i
)
??n,STOP
Z(?)
(16)
Let G = {G?} denote the set of weighted grammars based on the unweighted right-
linear grammar G. We call these weighted grammars ?Mealy MRFs.? As in the WCFG
case, we can add the constraint Zn(?) < ? (for all n), giving the class GZn<?.
Recall that, in the WCFG case, the move from G to GZn<? had to do with cyclic
derivations. The analogous move in the right-linear grammar case involves 
 emis-
sions (production rules of the form X ? Y). If, as in typical applications of finite-state
models to natural language processing, there are no rules of the form X ? Y, then
GZn<? is empty and GZn<? = G. Our formulae, in fact, assume that there are no 

emissions.
Because Mealy MRFs are a special case of WCFGs, Theorem 1 applies to them.
This means that any random field using Mealy HMM features (Mealy MRF) such that
?n, Zn(?) < ? can be transformed into a Mealy HMM that defines the same conditional
distribution of tags given words.7
Corollary 2
For a given right-linear grammar G, CHMM = CZ<? = CZn<?.
Lafferty, McCallum, and Pereira?s conditional random fields are typically trained to
optimize a different objective function than HMMs (conditional likelihood and joint
likelihood, respectively). Our result shows that optimizing either objective on the set
of Mealy HMMs as opposed to Mealy MRFs will achieve the same result, modulo
imperfections in the numerical search for parameter values.
4.2 Maximum-Entropy Markov Models
While HMMs and chain MRFs represent the same set of conditional distributions, we
can show that the maximum-entropy Markov models (MEMMs) of McCallum, Freitag,
and Pereira (2000) represent a strictly smaller class of distributions.
An MEMM is a similar model with a different event structure. It defines the distri-
bution over paths given words as:
PMEMM(? | x) =
n
?
i=1
p(?i | ?i?1, xi) (17)
Unlike an HMM, the MEMM does not define a distribution over output sequences x.
The name ?maximum entropy Markov model? comes from the fact that the conditional
7 What if we allow additional features? It can be shown that, as long as the vocabulary ? is finite and
known, we can convert any such MRF with potential functions on state transitions and emissions into
an HMM functioning equivalently as a classifier. If ? is not fully known, then we cannot sum over all
emissions from each state, and we cannot use Chi?s method (Section 2.1) to convert to a PCFG (HMM).
486
Smith and Johnson Weighted and Probabilistic CFGs
distributions p(? | ?, x) typically have a log-linear form, rather than a multinomial form,
and are trained to maximize entropy.
Lemma 1
For every MEMM, there is a Mealy MRF that represents the same conditional distribu-
tion over paths given symbols.
Proof
By definition, the features of the MRF include triples ??i?1, xi,?i?. Assign to the
weight ??i,xj,?k the value PMEMM(?i | ?k, xj). Assign to ??i,STOP the value 1. In computing
PCMRF(? | x) (Equation (15)), the normalizing term for each x will be equal to 1. 
MEMMs, like HMMs, are defined by locally normalized conditional multinomial
distributions. This has computational advantages (no potentially infinite Z(?) terms to
compute). However, the set of conditional distributions of labels given terminals that
can be expressed by MEMMs is strictly smaller than those expressible by HMMs (and
by extension, Mealy MRFs).
Theorem 2
For a given right-linear grammar G, CMEMM ? CHMM.
Proof
We give an example of a Mealy HMM whose conditional distribution over paths (trees)
given sentences cannot be represented by an MEMM. We thank Michael Collins for
pointing out to us the existence of examples like this one. Define a Mealy HMM with
three states named 0, 1, and 2, over an alphabet {a, b, c}, as follows. State 0 is the start
state.
Example 3
Under this model, PHMM(0, 1, 1 | a, b) = PHMM(0, 2, 2 | a, c) = 1. These conditional dis-
tributions cannot both be met by any MEMM. To see why, consider
p(1 | 0, a) ? p(1 | 1, b) = p(2 | 0, a) ? p(2 | 2, c) = 1
This implies that
p(1 | 0, a) = p(1 | 1, b) = p(2 | 0, a) = p(2 | 2, c) = 1
487
Computational Linguistics Volume 33, Number 4
But it is impossible for p(1 | 0, a) = p(2 | 0, a) = 1. This holds regardless of the form of
the distribution p(? | ?, x) (e.g., multinomial or log-linear).
Because P(0, 1, 1 | a, b) = P(0, 2, 2 | a, c) cannot be met by any MEMM, there are
distributions in the family allowed by HMMs that cannot be expressed as MEMMs,
and the latter are less expressive. 
It is important to note that this result applies to Mealy HMMs; our result compares
models with the same dependencies among random variables. If the HMM?s distribu-
tion p(xi,?i | ?i?1) is factored into p(xi | ?i) ? p(?i | ?i?1) (i.e., it is a Moore HMM), then
there may exist an MEMM with the same number of states that can represent some
distributions that the Moore HMM cannot.8
One can also imagine MEMMs in which p(?i | ?i?1, xi, ...) is conditioned on more
surrounding context (xi?1 or xi+1, or the entire sequence x, for example). Conditioning
on more context can be done by increasing the order of the Markov model?all of
our models so far have been first-order, with a memory of only the previous state.
Our result can be extended to include higher-order MEMMs. Suppose we allow the
MEMM to ?look ahead? n words, factoring its distribution into p(?i | ?i?1, xi, xi+1, ...,
xi+n).
Corollary 3
A first-order Mealy HMM can represent some classifiers that no MEMM with finite
lookahead can represent.
Proof
Consider again Example 3. Note that, for all m ? 1, it sets
PHMM(0,
m 1?s
? ?? ?
1, ..., 1 | amb) = 1
PHMM(0, 2, ..., 2
? ?? ?
m 2?s
| amc) = 1
Suppose we wish to capture this in an MEMM with n symbols of look-ahead. Letting
m = n + 1,
p(1 | 0, an+1) ? p(1 | 1, anb) ?
n
?
i=1
p(1 | 1, an?ib) = 1
p(2 | 0, an+1) ? p(2 | 2, anc) ?
n
?
i=1
p(2 | 2, an?ic) = 1
The same issue arises as in the proof of Theorem 2: it cannot be that p(1 | 0, an+1) =
p(2 | 0, an+1) = 1, and so this MEMM does not exist. Note that even if we allow the
8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the
distribution.
488
Smith and Johnson Weighted and Probabilistic CFGs
MEMM to ?look back? and condition on earlier symbols (or states), it cannot represent
the distribution in Example 3. 
Generally speaking, this limitation of MEMMs has nothing to do with the estima-
tion procedure (we have committed to no estimation procedure in particular) but rather
with the conditional structure of the model. That some model structures work better
than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning
(2002). Our result?that the class of distributions allowed by MEMMs is a strict subset
of those allowed by Mealy HMMs?makes this unsurprising.
5. Practical Implications
Our result is that weighted generalizations of classical probabilistic grammars (PCFGs
and HMMs) are no more powerful than the probabilistic models. This means that, inso-
far as log-linear models for NLP tasks like tagging and parsing are more successful
than their probabilistic cousins, it is due to either (a) additional features added to
the model, (b) improved estimation procedures (e.g., maximum conditional likelihood
estimation or contrastive estimation), or both. (Note that the choice of estimation proce-
dure (b) is in principle orthogonal to the choice of model, and conditional estimation
should not be conflated with log-linear modeling.) For a given estimation criterion,
weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any
differently than PCFGs and HMMs, respectively, unless they are augmented with more
features.
6. Related Work
Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and
probabilistic models based on push-down automaton operations (e.g., the structured
language model of Chelba and Jelinek, 1998). They proved that, although the conversion
may not be simple (indeed, a blow-up in the automaton?s size may be incurred), given
G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down
automata are weakly equivalent. Importantly, the standard conversion of a CFG into a
shift-reduce PDA, when applied in the stochastic case, does not always preserve the prob-
ability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further
work on the relationship between weighted CFGs and weighted PDAs is described in
Nederhof and Satta (2004).
MacKay (1996) proved that linear Boltzmann chains (a class of weighted models
that is essentially the same as Moore MRFs) express the same set of distributions as
Moore HMMs, under the condition that the Boltzmann chain has a single specific end
state. MacKay avoided the divergence problem by defining the Boltzmann chain always
to condition on the length of the sequence; he tacitly requires all of his models to be in
GZn<?. We have suggested a more applicable notion of model equivalence (equivalence
of the conditional distribution) and our Theorem 1 generalizes to context-free models.
7. Conclusion
We have shown that weighted CFGs that define finite scores for all sentences in their
languages have no greater expressivity than PCFGs, when used to define distributions
489
Computational Linguistics Volume 33, Number 4
over trees given sentences. This implies that the standard Mealy MRF formalism is
no more powerful than Mealy HMMs, for instance. We have also related ?maximum
entropy Markov models? to Mealy Markov random fields, showing that the former is a
strictly less expressive weighted formalism.
Acknowledgments
This work was supported by a Fannie and
John Hertz Foundation fellowship to
N. Smith at Johns Hopkins University. The
views expressed are not necessarily endorsed
by the sponsors. We are grateful to three
anonymous reviewers for feedback that
improved the article, to Michael Collins for
encouraging exploration of this matter and
helpful comments on a draft, and to Jason
Eisner and Dan Klein for insightful
conversations. Any errors are the sole
responsibility of the authors.
References
Abney, Steven P., David A. McAllester,
and Fernando Pereira. 1999. Relating
probabilistic grammars and automata.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 542?549, College Park, MD.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probability measures to
abstract languages. IEEE Transactions on
Computers, 22(5):442?450.
Chelba, Ciprian and Frederick Jelinek.
1998. Exploiting syntactic structure for
language modeling. In Proceedings
of the 36th Annual Meeting of the
Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 325?331,
Montreal, Canada.
Chi, Zhiyi. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131?160.
Goodman, Joshua T. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University,
Cambridge, MA.
Graham, Ronald L., Donald E. Knuth,
and Oren Patashnik. 1994. Concrete
Mathematics. Addison-Wesley,
Reading, MA.
Johnson, Mark. 2001. Joint and conditional
estimation of tagging and parsing models.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics,
pages 314?321, Toulouse, France.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
?unification-based? grammars.
In Proceedings of the 37th Annual
Conference of the Association for
Computational Linguistics, pages 535?541,
College Park, MD.
Klein, Dan and Christopher D. Manning.
2002. Conditional structure versus
conditional estimation in NLP models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 9?16, Philadelphia, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the 18th International
Conference on Machine Learning,
pages 282?289, Williamstown, MA.
MacKay, David J. C. 1996. Equivalence of
linear Boltzmann chains and hidden
Markov models. Neural Computation,
8(1):178?181.
McCallum, Andrew, Dayne Freitag, and
Fernando Pereira. 2000. Maximum
entropy Markov models for information
extraction and segmentation. In
Proceedings of the 17th International
Conference on Machine Learning,
pages 591?598, Palo Alto, CA.
Mealy, G. H. 1955. A method for
synthesizing sequential circuits.
Bell System Technology Journal,
34:1045?1079.
Moore, Edward F. 1956. Gedanken-
experiments on sequential machines.
In Automata Studies, number 34 in
Annals of Mathematics Studies.
Princeton University Press, Princeton,
NJ, pages 129?153.
Nederhof, Mark-Jan and Giorgio Satta.
2004. Probabilistic parsing strategies.
In Proceedings of the 42nd Annual
Meeting of the Association for
Computational Linguistics, pages 543?550,
Barcelona, Spain.
Ratnaparkhi, Adwait, Salim Roukos,
and R. Todd Ward. 1994. A maximum
entropy model for parsing. In Proceedings
of the International Conference on Spoken
Language Processing, pages 803?806,
Yokohama, Japan.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
490
Smith and Johnson Weighted and Probabilistic CFGs
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics, pages 354?362, Ann Arbor, MI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165?201.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Christopher Manning.
2004. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1?8,
Barcelona, Spain.
Vardi, Ilan. 1991. Computational Recreations in
Mathematica. Addison-Wesley, Redwood
City, CA.
Wetherell, C. S. 1980. Probabilistic languages:
A review and some open questions.
Computing Surveys, 12:361?379.
491

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Domain Adaptation for Parsing
David McCloskya,b
aStanford University
Stanford, CA, USA
mcclosky@stanford.edu
Eugene Charniakb
bBrown University
Providence, RI, USA
ec@cs.brown.edu
Mark Johnsonc,b
cMacquarie University
Sydney, NSW, Australia
mjohnson@science.mq.edu.au
Abstract
Current statistical parsers tend to perform well
only on their training domain and nearby gen-
res. While strong performance on a few re-
lated domains is sufficient for many situations,
it is advantageous for parsers to be able to gen-
eralize to a wide variety of domains. When
parsing document collections involving het-
erogeneous domains (e.g. the web), the op-
timal parsing model for each document is typ-
ically not obvious. We study this problem as
a new task ? multiple source parser adapta-
tion. Our system trains on corpora from many
different domains. It learns not only statistics
of those domains but quantitative measures of
domain differences and how those differences
affect parsing accuracy. Given a specific tar-
get text, the resulting system proposes linear
combinations of parsing models trained on the
source corpora. Tested across six domains,
our system outperforms all non-oracle base-
lines including the best domain-independent
parsing model. Thus, we are able to demon-
strate the value of customizing parsing models
to specific domains.
1 Introduction
In statistical parsing literature, it is common to see
parsers trained and tested on the same textual do-
main (Charniak and Johnson, 2005; McClosky et
al., 2006a; Petrov and Klein, 2007; Carreras et al,
2008; Suzuki et al, 2009, among others). Unfor-
tunately, the performance of these systems degrades
on sentences drawn from a different domain. This
issue can be seen across different parsing models
(Sekine, 1997; Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). Given that some aspects of
syntax are domain dependent (typically at the lexi-
cal level), single parsing models tend to not perform
well across all domains (see Table 1). Thus, statis-
tical parsers inevitably learn some domain-specific
properties in addition to the more general properties
of a language?s syntax. Recently, Daume? III (2007)
and Finkel and Manning (2009) showed techniques
for training models that attempt to separate domain-
specific and general properties. However, even when
given models for multiple training domains, it is not
straightforward to determine which model performs
best on an arbitrary piece of novel text.
This problem comes to the fore when one wants
to parse document collections where each document
is potentially its own domain. This shows up par-
ticularly when parsing the web. Recently, there
has been much interest in applying parsers to the
web for the purposes of information extraction and
other forms of analysis (c.f. the CLSP 2009 summer
workshop ?Parsing the Web: Large-Scale Syntactic
Processing?). The scale of the web demands an au-
tomatic solution to the domain detection and adap-
tation problems. Furthermore, it is not obvious that
human annotators can determine the optimal parsing
models for each web page.
Our goal is to study this exact problem. We create
a new parsing task, multiple source parser adapta-
tion, designed to capture cross-domain performance
along with evaluation metrics and baselines. Our
new task involves training parsing models on labeled
and unlabeled corpora from a variety of domains
(source domains). This is in contrast to standard do-
main adaptation tasks where there is a single source
domain. For evaluation, one is given a text (target
text) but not the identity of its domain. The chal-
lenge is determining how to best use the available
28
Test
Train BNC GENIA BROWN SWBD ETT WSJ Average
GENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0
BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9
SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9
ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9
WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0
Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages.
Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate.
resources from training to maximize accuracy across
multiple target texts.
Broadly put, we model how domain differences
influence parsing accuracy. This is done by taking
several computational measures of domain differ-
ences between the target text and each source do-
main. We use these features in a simple linear re-
gression model which is trained to predict the accu-
racy of a parsing model (or, more generally, a mix-
ture of parsing models) on a target text. To parse
the target text, one simply uses the mixture of pars-
ing models with the highest predicted accuracy. We
show that our method is able to predict these accu-
racies quite well and thus effectively rank parsing
models formed from mixtures of labeled and auto-
matically labeled corpora.
In Section 2, we detail recent work on similar
tasks. Our regression-based approach is covered in
Section 3. We describe an evaluation strategy in Sec-
tion 4. Section 5 presents new baselines which are
intended to give a sense of current approaches and
their limitations. The results of our experiments are
detailed in Section 6 where we show that our system
outperforms all non-oracle baselines. We conclude
with a discussion and future work (Section 7).
2 Related work
The closest work to ours is Plank and Sima?an
(2008), where unlabeled text is used to group sen-
tences from WSJ into subdomains. The authors cre-
ate a model for each subdomain which weights trees
from its subdomain more highly than others. Given
the domain specific models, they consider different
parse combination strategies. Unfortunately, these
methods do not yield a statistically significant im-
provement.
Multiple source domain adaptation has been done
for other tasks (e.g. classification in (Blitzer et
al., 2007; Daume? III, 2007; Dredze and Cram-
mer, 2008)) and is related to multitask learning.
Daume? III (2007) shows that an extremely sim-
ple method delivers solid performance on a num-
ber of domain adaptation classification tasks. This is
achieved by making a copy of each feature for each
source domain plus the ?general? pseudodomain
(for capturing domain independent features). This
allows the classifier to directly model which features
are domain-specific. Finkel and Manning (2009)
demonstrate the hierarchical Bayesian extension of
this where domain-specific models draw from a gen-
eral base distribution. This is applied to classifica-
tion (named entity recognition) as well as depen-
dency parsing. These works describe how to train
models in many different domains but sidestep the
problem of domain detection. Thus, our work is or-
thogonal to theirs.
Our domain detection strategy draws on work in
parser accuracy prediction (Ravi et al, 2008; Kawa-
hara and Uchimoto, 2008). These works aim to pre-
dict the parser performance on a given target sen-
tence. Ravi et al (2008) frame this as a regression
problem. Kawahara and Uchimoto (2008) treat it
as a binary classification task and predict whether
a specific parse is at a certain level of accuracy or
higher. Ravi et al (2008) show that their system
can be used to return a ranking over different parsing
models which we extend to the multiple domain set-
ting. They also demonstrate that training their model
on WSJ allows them to accurately predict parsing
accuracy on the BROWN corpus. In contrast, our
models are trained over multiple domains to model
which factors influence cross-domain performance.
29
3 Approach
We start with the assumption that all target domains
are mixtures of our source domains.1 Intuitively,
these mixtures should give higher probability mass
to more similar source domains. This raises the
question of how to measure the similarity between
domains. Our method uses multiple complemen-
tary similarity measures between the target and each
source. We feed these similarity measures into a re-
gression model which learns how domain dissimi-
larities hurt parse accuracy. Thus, to parse a target
domain, we need only find the input that maximizes
the regression function ? that is, the highest scoring
mixture of source domains. Our system is similar to
Ravi et al (2008) in that both use regression to pre-
dict f-scores and some of the features are related.
3.1 Features
Our features are designed to help the regression
model determine if a particular source domain mix-
ture is well suited for a target domain as well as the
quality of a source domain mixture. While we ex-
plored a large number of features, we present here
only the three that were chosen by our feature selec-
tion method (Section 6.2).
Two of our features, COSINETOP50 and UN-
KWORDS, are designed to approximate how simi-
lar the target domain is to a specific source domain.
Only the surface form of the target text and auto-
matic analyses are available (e.g. we can tag or parse
the target text, but cannot use gold tags or trees).
Relative word frequencies are an important in-
dicator of domain. Cosine similarity uses a spa-
tial representation to summarize the word frequen-
cies in a corpus as a single vector. A common
method is to represent each corpus as a vector of
frequencies of the k most frequent words (Schu?tze,
1995). This method assigns high similarity to do-
mains with a large amount of overlap in the high-
frequency vocabulary items. We experimented with
several orders of magnitude for k (our feature selec-
tion method later chose k = 50 ? see Section 6.2).
Our second feature for comparing domains, UN-
1This may seem like a major limitation, but as we will show
later, our method works quite well at incorporating self-trained
(automatically parsed) corpora which can typically be obtained
for any domain.
KWORDS, returns the percentage of words in one
domain which never appear in the other domain.
This can be done on the word type or token level.
We opt for tokens since unknown words pose prob-
lems for parsing each time they occur. UNKWORDS
provides the percentage of words in the source
domain that are never seen in the target domain.
Whereas COSINETOP50 examines how similar the
high frequency words are from one domain, UN-
KWORDS tends to focus on the overlap of low fre-
quency words.
As described, COSINETOP50 and UNKWORDS
are functions only of two source domains and do not
take the mixing weights of source domains into ac-
count. We experimented with several methods of in-
corporating mixing weights into the feature value.
In practice, the one which worked best for us is to
divide the mixture weight of the source domain by
the raw feature value. This has the nice property that
when a source is not used, the adjusted feature value
is zero regardless of the raw feature value.
From pilot studies, we learned that a uniform mix-
ture of available source domains gave strong results
(further details on this in Section 5). Our last feature,
ENTROPY, is intended to let the regression system
leverage this and measures the entropy of the distri-
bution over source domains. This provides a sense
of uniformity.
3.2 Predicting cross-domain accuracy
For a given source domain mixture, we can create
a parsing model by linearly interpolating the pars-
ing model statistics from each source domain. The
key component of our approach is a domain-aware
linear regression model which predicts how well a
specific parsing model will do on a given target text.
The linear regressor is given values from the three
features from the previous section (COSINETOP50,
UNKWORDS, and ENTROPY) and returns an esti-
mate of the f-score the parsing model would achieve
the target text.
Training data for the regressor consists of ex-
amples of source domain mixtures and their ac-
tual f-scores on target texts. To produce this, we
randomly sampled source domain mixtures, created
parsing models for those mixtures, and then evalu-
ated the parsing models on all of our target texts.
We used a simple technique for randomly sam-
30
0 200 400 600 800 1000
Number of mixed parsing model samples
84.0
84.5
85.0
85.5
86.0
86.5
87.0
87.5
o
r
a
c
l
e
 
f
-
s
c
o
r
e
Figure 1: Cumulative oracle f-score (averaged over
all target domains) as more models are randomly
sampled. Most of the improvement comes the first
200 samples indicating that our samples seem to be
sufficient to cover the space of good source domain
mixtures.
pling source domain mixtures. First, we sample the
number of source domains to use. We draw values
from an exponential distribution and take their inte-
ger value until we obtain a number between two and
the number of source domains. This is parametrized
so that we typically only use a few corpora but still
have some chance of using all of them. Once we
know the number of source domains, we sample
their identities uniformly at random without replace-
ment from the list of all source domains. Finally,
we sample the weights for the source domains uni-
formly from a simplex. The dimension of the sim-
plex is the same as the number of source domains
so we end up with a probability distribution over the
sampled source domains.
In total, we sampled 1,040 source domain mix-
tures. We evaluated each of these source domain
mixtures on the six target domains giving us 6,240
data points in total. One may be concerned that
this is insufficient to cover the large space of source
domain mixtures. However, we show in Figure 1
that only about 200 samples are sufficient to achieve
good oracle performance2 in practice.
2We calculate this by picking the best available model for
each target domain and taking the average of their f-scores.
Train Test
Source Target Source Target
C \ {t} C \ {t} C \ {t} {t}
(a) Out-of-domain evaluation
Train Test
Source Target Source Target
C C \ {t} C {t}
(b) In-domain evaluation
Table 2: List of domains allowed in single round of
evaluation. In each round, the evaluation corpus is t.
C is the set of all target domains.
4 Evaluation
Multiple-source domain adaptation is a new task for
parsing and thus some thought must be given to eval-
uation methodology. We describe two evaluation
scenarios which differ in how foreign the target text
is from our source domains. Schemas for these eval-
uation scenarios are shown in Table 2. Note that
training and testing here refer to training and testing
of our regression model, not the parsing models.
In the first scenario, out-of-domain evaluation,
one target domain is completely removed from con-
sideration and only used to evaluate proposed mod-
els at test time. The regressor is trained on training
points that use any of the remaining corpora, C\{t},
as sources or targets. For example, if t = WSJ, we
can train the regressor on all data points which don?t
use WSJ (or any self-trained corpora derived from
WSJ) as a source or target domain. At test time, we
are given the text of WSJ?s test set. From this, our
system creates a parsing model using the remaining
available corpora for parsing the raw WSJ text.
This evaluation scenario is intended to evaluate
how well our system can adapt to an entirely new
domain with only raw text from the new domain
(for example, parsing biomedical text when none
is available in our list of source domains). Ide-
ally, we would have a large number of web pages
or other documents from other domains which we
could use solely for evaluation. Unfortunately, at
this time, only a handful of domains have been an-
notated with constituency structures under the same
This can pick different models for each target domain.
31
annotation guidelines. Instead, we hold out each
hand-annotated domain, t, (including any automat-
ically parsed corpora derived from that source do-
main) as a test set in a round-robin fashion.3 For
each round of the round robin we obtain an f-score
and we report the mean and variance of the f-scores
for each model.
The second scenario, in-domain evaluation, al-
lows the target domain, t, to be used as a source
domain in training but not as a target domain. This
is intended to evaluate the situation where the target
domain is not actually that different from our source
domains. The in-domain evaluation can approxi-
mate how our system would perform when, for ex-
ample, we have WSJ as a source domain and the tar-
get text is news from a source other than WSJ. Thus,
our model still has to learn that WSJ and the North
American News Text corpus (NANC) are good for
parsing news text like WSJ without seeing any direct
evaluations of the sort (WSJ and NANC can be used
in models which are evaluated on all other corpora,
though).
5 Baselines
Given that this is a new task for parsing, we needed
to create baselines which demonstrate the current
approaches to multiple-source domain adaptation.
One approach is to take all available corpora and
mix them together uniformly.4 The UNIFORM base-
line does exactly this using the available hand-built
training corpora. SELF-TRAINED UNIFORM uses
self-trained corpora as well. In the out-of-domain
scenario, these exclude the held out domain, but in
the in-domain setting, the held out domain is in-
cluded. These baselines are similar to the ALL and
WEIGHTED baselines in Daume? III (2007).
Another simple baseline is to use the same pars-
ing model regardless of target domain. This is how
large heterogeneous document collections are typi-
cally parsed currently. We use the WSJ corpus since
it is the best single corpus for parsing all six target
domains (see Table 1). We refer to this baseline as
FIXED SET: WSJ. In the out-of-domain scenario,
we fall back to SELF-TRAINED UNIFORM when the
3Thus, the schemas in Table 2 are schemas for each round.
4Accounting for size so that the larger corpora don?t over-
whelm the smaller ones.
target domain is WSJ while the in-domain scenario
uses the WSJ model throughout.
There are several interesting oracle baselines as
well which serve to measure the limits of our ap-
proach. These baselines examine the resulting
f-scores of models and pick the best model accord-
ing to some criteria. The first oracle baseline is
BEST SINGLE CORPUS which parses each corpus
with the source domain that maximizes performance
on the target domain. In almost all cases, this base-
line selects each corpus to parse itself.
Our second oracle baseline, BEST SEEN, chooses
the best parsing model from all those explored for
each test set. Recall that while training the regres-
sion model in Section 3.2, we needed to explore
many possible source domain mixtures to approxi-
mate the complete space of mixed parsing models.
To the extent that we can fully explore the space of
mixed parsing models, this baseline represents an
upper bound for model mixing approaches. Since
fully exploring the space of possible weightings is
intractable, it is not a true upper bound. While it
is theoretically possible to beat this pseudo-upper
bound, (indeed, this is the mark of a good domain
detection system) it is far from easy. We provide
BEST SINGLE CORPUS and BEST SEEN for both
in-domain and out-of-domain scenarios. The out-of-
domain scenario restricts the set of possible models
to those not including the target domain.
Finally, we searched for the BEST OVERALL
MODEL. This is the model with the highest aver-
age f-score across all six target domains. This base-
line can be thought of as an oracle version of FIXED
SET: WSJ and demonstrates the limit of using a sin-
gle parsing model regardless of target domain. Natu-
rally, the very nature of this baseline places it only in
the in-domain evaluation scenario. Since it was able
to select the model according to f-scores on our six
target domains, its performance on domains outside
that set is not guaranteed.
To provide a better sense of the space of mixed
parsing models, we also provide the WORST SEEN
baseline which picks the worst model available for a
specific target corpus.5
5This turns out to be GENIA for all corpora other than GENIA
and SWBD when the target domain is GENIA.
32
6 Experiments
Our experiments use the Charniak (2000) generative
parser. We describe the corpora used in our nine
source and six target domains in Section 6.1. In Sec-
tion 6.2, we provide a greedy strategy for picking
features to include in our regression model. The re-
sults of our experiments are in Section 6.3.
6.1 Corpora
We aimed to include as many different domains as
possible annotated under compatible schemes. We
also tried to include human-annotated corpora and
automatically labeled corpora (self-trained corpora
as in McClosky et al (2006a) which have been
shown to work well across domains). Our final
set includes text from news (WSJ, NANC), broad-
cast news (ETT), literature (BROWN, GUTENBERG),
biomedical (GENIA, MEDLINE), spontaneous speech
(SWBD), and the British National Corpus (BNC). In
our experiments, self-trained corpora cannot be used
as target domains since we lack gold annotations and
BNC is not used as a source domain due to its size.
An overview of our corpora is shown in Table 3.
We use news articles portion of the Wall Street
Journal corpus (WSJ) from the Penn Treebank (Mar-
cus et al, 1993) in conjunction with the self-trained
North American News Text Corpus (NANC, Graff
(1995)). The English Translation Treebank, ETT
(Bies, 2007), is the translation6 of broadcast news
in Arabic. For literature, we use the BROWN cor-
pus (Francis and Kuc?era, 1979) and the same di-
vision as (Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). We also use raw sen-
tences which we downloaded from Project Guten-
berg7 as a self-trained corpus. The Switchboard cor-
pus (SWBD) consists of transcribed telephone con-
versations. While the original trees include disflu-
ency information, we assume our speech corpora
have had speech repairs excised (e.g. using a sys-
tem such as Johnson et al (2004)). Our biomedi-
cal data comes from the GENIA treebank8 (Tateisi
et al, 2005), a corpus of abstracts from the Med-
line database.9 We downloaded additional sentences
6The transcription and translation were done by humans.
7http://gutenberg.org/
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://www.ncbi.nlm.nih.gov/PubMed/
from Medline for our self-trained MEDLINE corpus.
Unlike the other two self-trained corpora, we include
two versions of MEDLINE. These differ on whether
they were parsed using GENIA or WSJ as a base
model to study the effect on cross-domain perfor-
mance. Finally, we use a small number of sentences
from the British National Corpus (BNC) (Foster and
van Genabith, 2008).10 The sentences were chosen
randomly, so each one is potentially from a different
domain. On the other hand, BNC can be thought of
as its own domain in that it contains significant lex-
ical differences from the American English used in
our other corpora.
We preprocessed the corpora to standardize many
of the annotation differences. Thus, our results on
them may be slightly different than other works on
these corpora. Nevertheless, these changes should
not significantly impact overall the performance.
6.2 Feature selection
While our final model uses only three features, we
considered many other possible features (not de-
scribed due to space constraints). In order to explore
these without hill climbing on our test data, we cre-
ated a round-robin tuning scenario. Since the out-
of-domain evaluation scenario holds out one target
domain, this gives us six test evaluation rounds. For
each of these six rounds, we hold out one of the re-
maining five target domains for tuning. This gives
us 30 tuning evaluation rounds and we pick our fea-
tures to optimize our aggregate performance over all
of them. A model that performs well in this situation
has proven that it has useful features which transfer
to unknown target domains.
The next step is to determine the loss function
to minimize. Our primary guide is oracle f-score
loss which we determine as follows. We take all
test data points (i.e. mixed parsing models evalu-
ated on the target domain) and predict their f-scores
with our model. In particular for this measure, we
are interested in the point with the highest predicted
f-score. We take its actual f-score which we call
the candidate f-score. When tuning, we know the
true f-scores of all test points. The difference be-
tween the highest f-score (the oracle f-score for
10http://nclt.computing.dcu.ie/
?
jfoster/
resources/, downloaded January 8th, 2009.
33
Corpus Source? Target? Average length Train Tune Test
BNC ? 28.3 ? ? 1,000
BROWN ? ? 20.0 19,786 2,082 2,439
ETT ? ? 25.6 2,639 1,029 1,166
GENIA ? ? 27.5 14,326 1,361 1,360
MEDLINE ? 27.2 278,192 ? ?
SWBD ? ? 9.2 92,536 5,895 6,051
WSJ ? ? 25.5 39,832 1,346 2,416
NANC ? 23.2 915,794 ? ?
GUTENBERG ? 26.2 689,782 ? ?
MEDLINE ? 27.2 278,192 ? ?
Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.
Indented rows indicate self-trained corpora parsed using the non-indented row as a base parser.
this dataset) and the candidate f-score is the oracle
f-score loss. Ties need to be handled correctly to
avoid degenerate models.11 If there is a tie for high-
est predicted f-score, the candidate f-score is the
one with the lowest actual f-score. This approach
is conservative but ensures that regression models
which give everything the same predicted f-score do
not receive zero oracle f-score loss.
Armed with a tuning regime and a loss function,
we created a procedure to pick the combination of
features to use. We used a parallelized best-first
search procedure. At each round, it expanded the
current best set of features by adding or removing
each feature where ?best? was determined by the loss
function. We explored over 6,000 settings, though
the best setting of (UNKWORDS, COSINETOP50,
ENTROPY) was found within the first 200 settings
explored. The best setting obtains an oracle f-score
loss of 0.37 and a root mean squared error of 0.48
? these numbers are quite low and show the high
accuracy of our regression model (similar to those
in Ravi et al (2008)). Additionally, the features are
complementary in that UNKWORDS focuses on low
frequency words whereas COSINETOP50 looks only
at high frequency words and ENTROPY functions as
a regularizer.
6.3 Results
We present an overview of our final results for out-
of-domain and in-domain evaluation in Table 4. The
11For example, regression models which assign every parsing
model the same f-score.
results include the f-score macro-averaged over the
six target domains and their standard deviation.
In both situations, the FIXED SET: WSJ baseline
performs fairly poorly. Not surprisingly, assuming
all of our target domains are close enough to WSJ
works badly for our set of target domains and it
does particularly poorly on SWBD and GENIA. On
average, the UNIFORM baseline does slightly bet-
ter for out-of-domain and over 3% better for in-
domain. UNIFORM actually does fairly well on out-
of-domain except on GENIA. In general, using more
source domains is better which partially explains the
success of UNIFORM. This seems to be the case
since even if a source domain is terribly mismatched
with the target domain, it may still be able to fill
in some holes left by the other source domains. Of
course, if it overpowers more relevant domains, per-
formance may suffer. The SELF-TRAINED UNI-
FORM baseline uses even more source domains as
well as the largest ones. In both scenarios, this dra-
matically improves performance and is the second
best non-oracle system. This baseline provides more
evidence as to the power of self-training for improv-
ing parser adaptation. If we excluded all self-trained
corpora, our performance on this task would be sub-
stantially worse. We believe the self-trained cor-
pora are beneficial in this task since they help reduce
data sparsity of smaller corpora. The BEST SINGLE
CORPUS baseline is poor in the out-of-domain sce-
nario primarily because the actual best single corpus
is excluded by the task specification in most cases.
When we move to in-domain, this baseline improves
34
Oracle Baseline or model Average f-score
? Worst seen 62.0 ? 6.1
? Best single corpus 81.0 ? 2.9
Fixed set: WSJ 81.0 ? 3.5
Uniform 81.4 ? 3.6
Self-trained uniform 83.4 ? 2.5
Our model 84.0 ? 2.5
? Best seen 84.3 ? 2.6
(a) Out-of-domain evaluation
Oracle Baseline or model Average f-score
Fixed set: WSJ 82.0 ? 4.8
Uniform 85.4 ? 2.4
? Best single corpus 85.6 ? 2.9
Self-trained uniform 86.1 ? 2.0
? Best overall model 86.2 ? 1.9
Our model 86.9 ? 2.4
? Best seen 87.5 ? 2.1
(b) In-domain evaluation
Table 4: Baselines and final results for the two multiple-source domain adaptation evaluation scenarios.
Results include f-scores, macro-averaged over all six target domains and their standard deviations.
but is still worse than SELF-TRAINED UNIFORM on
average. It beats SELF-TRAINED UNIFORM primar-
ily on WSJ, SWBD, and GENIA indicating that these
three domains are best when not diluted by others.
By definition, the WORST SEEN baseline does terri-
bly, almost 20% worse then BEST SINGLE CORPUS.
Our model is the best non-oracle system for both
evaluation scenarios. For out-of-domain evaluation,
our system is only 0.3% worse than the BEST SEEN
models for each target domain. For the in-domain
scenario, we are within 0.6% of the BEST SEEN
models. For a sense of scale, our out-of-domain and
in-domain f-scores on WSJ are 83.1% and 89.8%
respectively. Both numbers are quite close to the
BEST SEEN baseline. Additionally, our model is
0.7% better than the BEST OVERALL MODEL. Re-
call that the BEST OVERALL MODEL is the single
model with the best performance across all six tar-
get domains.12 By beating this baseline, we show
that there is value in customizing parsing models
to the target domain. It is also interesting that the
BEST OVERALL MODEL is only marginally better
than SELF-TRAINED UNIFORM. Without any fur-
ther information about the target corpus, an unin-
formed prior appears best.
7 Discussion
We have shown that for both out-of-domain and in-
domain evaluations, our system is well adapted to
predicting the effects of domain divergence on pars-
12Somewhat surprisingly, the best overall model uses almost
entirely self-trained corpora consisting of 9.5% GUTENBERG,
60.3% NANC, 26.0% MEDLINE (by GENIA), and 4.2% SWBD.
ing accuracy. Using the parsing model with the
highest predicted f-score leads to great performance
in practice. There is a substantial benefit to doing
this over existing approaches (using the same model
for all domains or mixing all training data together
uniformly). Creating a number of domain-specific
models and mixing them together as needed is a vi-
able approach.
One can think of our system as trying to esti-
mate document-level context. Our representation of
this context is simply a distribution over our source
domains, but one can imagine more complex op-
tions such as a high-dimensional vector space. Ad-
ditionally, our model separates domain and syntax
estimation, but a future direction is to learn these
jointly. This would combine our work with (Daume?
III, 2007; Finkel and Manning, 2009).
We have focused on the Charniak (2000) parser,
the first stage in the two stage Charniak and John-
son (2005) reranking parser. Applying our methods
to other generative parsers (such as (Collins, 1999;
Petrov and Klein, 2007)) is trivial, but it is less clear
how our methods can be applied to the discrimina-
tive reranker component of the two stage parser. One
avenue of approach is to incorporate the domain rep-
resentation into the feature space, as in Daume? III
(2007) but with more complex domain information.
Acknowledgments
This work was performed while the first author was
at Brown and supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank the BLLIP
team and our anonymous reviewers for their comments.
35
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Ann Bies. 2007. GALE Phase 3 Release 1 - English
Translation Treebank. Linguistic Data Consortium.
LDC2007E105.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Association for Computational Linguistics, Prague,
Czech Republic.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL 2008, pages 9?16, Manchester, England, Au-
gust.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the ACL 2005, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Chapter
of the ACL (NAACL), pages 132?139.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL 2007, Prague, Czech
Republic.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the EMNLP 2008, pages 689?697, Honolulu,
Hawaii, October.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL 2009, pages 602?610, Boulder,
Colorado, June.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings LREC 2008,
Marrakech, Morocco, May.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Language
Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Proc. of the Rich Text
2004 Fall Workshop (RT-04F).
Daisuke Kawahara and Kiyotaka Uchimoto. 2008.
Learning reliability of parses for domain adaptation
of dependency parsing. In Third International Joint
Conference on Natural Language Processing (IJCNLP
?08).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comp. Linguis-
tics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006, pages 152?159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337?344, Sydney, Australia, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Barbara Plank and Khalil Sima?an. 2008. Subdomain
sensitive statistical parsing using raw corpora. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 887?896, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96?102.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings EMNLP 2009, pages 551?560, Singa-
pore, August.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. Proceedings of IJCNLP 2005, Compan-
ion volume, pages 222?227.
36
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501?509,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Words and Their Meanings from Unsegmented Child-directed
Speech
Bevan K. Jones & Mark Johnson
Dept of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912, USA
{Bevan Jones,Mark Johnson}@Brown.edu
Michael C. Frank
Dept of Brain and Cognitive Science
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
mcfrank@mit.edu
Abstract
Most work on language acquisition treats
word segmentation?the identification of lin-
guistic segments from continuous speech?
and word learning?the mapping of those seg-
ments to meanings?as separate problems.
These two abilities develop in parallel, how-
ever, raising the question of whether they
might interact. To explore the question, we
present a new Bayesian segmentation model
that incorporates aspects of word learning and
compare it to a model that ignores word mean-
ings. The model that learns word meanings
proposes more adult-like segmentations for
the meaning-bearing words. This result sug-
gests that the non-linguistic context may sup-
ply important information for learning word
segmentations as well as word meanings.
1 Introduction
Acquiring a language entails mastering many learn-
ing tasks simultaneously, including identifying
where words begin and end in continuous speech
and learning meanings for those words. It is com-
mon to treat these tasks as separate, sequential pro-
cesses, where segmentation is a prerequisite to word
learning but otherwise there are few if any depen-
dencies. The earliest evidence of segmentation,
however, is for words bordering a child?s own name
(Bortfeld et al, 2005). In addition, infants begin
learning their first words before they achieve adult-
level competence in segmentation. These two pieces
of evidence raise the question of whether the tasks of
meaning learning and segmentation might mutually
inform one another.
To explore this question we present a joint model
that simultaneously identifies word boundaries and
attempts to associate meanings with words. In do-
ing so we make two contributions. First, by model-
ing the two levels of structure in parallel we simu-
late a more realistic situation. Second, a joint model
allows us to explore possible synergies and interac-
tions. We find evidence that our joint model per-
forms better on a segmentation task than an alterna-
tive model that does not learn word meanings.
The picture in Figure 1 depicts a language learn-
ing situation from our corpus (originally from Fer-
nald and Morikawa, 1993; recoded in Frank et al,
2009) where a mother talks while playing with var-
ious toys. Setting down the dog and picking up the
hand puppet of a pig, she asks, ?Is that the pig??
Starting out, a young learner not only does not know
that the word ?pig? refers to the puppet but does not
even know that ?pig? is a word at all. Our model
simulates the learning task, taking as input the un-
segmented phonemic representation of the speech
along with the set of objects in the non-linguistic
context as shown in Figure 1 (a), and infers both a
segmentation and a word-object mapping as in Fig-
ure 1 (b).
One can formulate the word learning task as
that of finding a reasonably small set of reusable
word-meaning pairs consistent with the underlying
communicative intent. Infant directed speech often
refers to objects in the immediate environment, and
early word learning seems to involve associating fre-
quently co-occurring word-object pairs (Akhtar and
Montague, 1999; Markman, 1990). Several compu-
tational models are based on this idea that a word
501
Figure 1: (a) The input to our system for the utterance
?Is that the pig?? consists of an unsegmented sequence
of phonemes and the set of objects representing the non-
linguistic context. These objects were manually iden-
tified by inspecting the associated video, a frame from
which is shown above. (b) The gold-standard segmenta-
tion and word-object assignments of the same utterance,
against which the output of our system is evaluated (all
words except ?pIg? are mapped to a special ?null? object,
as explained in the text).
that frequently occurs in the presence of an object
and not so frequently in its absence is likely to re-
fer to that object (Frank et al, 2009a; Siskind, 1996;
Yu and Ballard, 2007). Importantly, all these models
assume words are pre-segmented in the input.
While the word segmentation task relates less
clearly to the communicative content, it can be for-
mulated according to a similar objective, that of at-
tempting to explain the sound sequences in the input
in terms of some reasonably small set of reusable
units, or words. Computational models have suc-
cessfully addressed the problem in much this way
(Johnson and Goldwater, 2009; Goldwater et al,
2009; Brent, 1999), and the general approach is con-
sistent with experimental observations that humans
are sensitive to statistics of sound sequences (Saffran
et al, 1996; Frank et al, 2007).
The two tasks can be integrated in a relatively
seamless way, since, as we have just formulated
them, they have a common objective, that of finding
a minimal, consistent set of reusable units. However,
the two deal with different types of information with
different dependencies. The basic idea is that learn-
ing a vocabulary that both meets the constraints of
the word-learning task and is consistent with the ob-
jective of the segmentation task can yield a better
segmentation. That is, we hope to find a synergy in
the joint inference of meaning and segmentation.
Note that to the best of our knowledge there is
very little computational work that combines word
form and word meaning learning (Frank et al 2006
takes a first step but their model is applicable only
to small artificial languages). Frank et al (2009a)
and Regier (2003) review pure word learning mod-
els and, in addition to the papers we have already
cited, Brent (1999) presents a fairly comprehensive
review of previous pure segmentation models. How-
ever, none of the models reviewed make any attempt
to jointly address the two problems. Similarly, in the
behavioral literature on development, we are aware
of only one segmentation study (Graf-Estes et al,
2007) that involves non-linguistic context, though
this study treats the two tasks sequentially rather
than jointly.
We now describe our model and inference proce-
dure and follow with evaluation and discussion.
2 Model Definition
Cross-situational meaning learning in our joint word
learning and segmenting model is inspired by the
model of Frank et al (2009a). Our model can
be viewed as a variant of the Latent Dirichlet Al-
location (LDA) topic model of Blei et al (2003),
where topics are drawn from the objects in the non-
linguistic context. The model associates each utter-
ance with a single referent object, the topic, and ev-
ery word in the utterance is either generated from a
distribution over words associated with that object
or else from a distribution associated with a special
?null? object shared by all utterances. Note that in
this paper we use ?topic? to denote the referent ob-
ject of an utterance, otherwise we depart from topic
modeling convention and use the term ?object? in-
stead.
Segmentation is based on the unigram model pro-
posed by Brent (1999) and reformulated by Goldwa-
ter et al (2009) in terms of a Dirichlet process. Since
both LDA and the unigram segmenter are based on
unigram distributions it is relatively straightforward
502
Figure 2: Topical Unigram Model: Oj is the set of objects
in the non-linguistic context of the jth utterance, zj is the
utterance topic, wji is the ith word of the utterance, xji is
the category of the word (referring or non-referring), and
the other variables are distribution parameters.
to integrate the two to simultaneously infer word
boundaries and word-object associations.
Figure 2 illustrates a slightly simplified form of
the model, and the the relevant distributions are as
follows:
z|O ? Uniform(O)
Gz|z, ?0, ?1, P0 ?
{
DP(?1, P0) if z 6= 0
DP(?0, P0) otherwise
pi ? Beta(1, 1)
x|pi ? Bernoulli(pi)
w|G, z, x ?
{
Gz if x = 1
G0 if x = 0
Note that Uniform(O) denotes a discrete uniform
distribution over the elements of the set O. P0 is
described later.
Briefly, each utterance has a single topic zj , drawn
from the objects in the non-linguistic context Oj ,
and then for each word wji we first flip a coin xji
to determine if it refers to the topic or not. Then, de-
pending on xji the word is either drawn from a dis-
tribution specific to the topic (xji = 1) or from a dis-
tribution associated with the ?null? object (xji = 0).
In slightly greater detail but still glossing over the
details of how the multinomial parameters are gen-
erated, the generative story proceeds as follows:
1. For each utterance, indexed by j
2. (a) Pick a single topic zj uniformly from the set
of objects in the environment Oj
(b) For each word wji of the utterance
(c) i. Determine if it refers to zj or not by set-
ting xji to 1 (referring) with probability pi,
and to 0 (non-referring) otherwise.
ii. if xji is 1, draw wji from the topic specific
distribution over words Gzj .
iii. otherwise, draw wji from G0, the distribu-
tion over words associated with the ?null?
object.
This generative story is a simplification since it
does not describe how we model utterance bound-
aries. It is important for segmentation purposes
to explicitly model utterance boundaries since, un-
like utterance-internal word boundaries, we as-
sume utterance boundaries are observed. Thus,
the story is complicated by the fact that there is
a chance each time we generate a word that we
also generate an utterance boundary. The choice of
whether to terminate the utterance or not is captured
by a Bernoulli(?) random variable $ji indicating
whether the ith word was the last word of the jth
utterance.
? ? Beta(1, 1)
$|? ? Bernoulli(?)
The Gz multinomial parameters are generated
from a Dirichlet process with base distribution over
words, P0, which describes how new word types
are generated from their constituent phonemes.
Phonemes are generated sequentially, i.i.d. uni-
formly from m phonemic types. In addition, there
is a probability p# of generating a word boundary.
P0(w) = (1? p#)|w|?1p#
1
m|w|
The concentration parameters ?0 and ?1 also play
a critical role in the generation of words and word
types. Any given word has a certain probability
of either being produced from the set of previously
seen word types, or from an entirely new one. The
503
greater the concentration parameter, the more likely
the model is to appeal to the base distribution P0 to
introduce a new word type.
Like Frank et al (2009a), we distinguish between
two coarse grammatical categories, referring and
non-referring. Referring words are generated by the
topic, while non-referring words are drawn from G0,
a distribution associated with the ?null? object. The
distinction ensures sparse word-object maps that
obey the principle of mutual exclusion. Otherwise
all words in the utterance would be associated with
the topic object, resulting in a very large set of words
for each object that is very likely to overlap with the
words for other objects. As a further bias toward
a small lexicon, we employ different concentration
parameters (?0 and ?1) for the non-referring and re-
ferring words, using a much smaller value for the
referring words. Intuitively, there should be a rela-
tively small prior probability of introducing a new
word-object pair, corresponding to a small ?1 value.
On the other hand, most other words don?t refer to
the topic object (or any other object for that matter),
corresponding to a much larger ?0 value.
Note that this topical unigram model is a straight-
forward generalization of the unigram segmentation
model (Goldwater et al, 2009) to the case of multi-
ple topics. In fact, if all words were assumed to refer
to the same object (or to no object at all) the models
would be identical.
Unlike LDA, each ?document? has only one topic,
which is necessitated by the fact that in our model
documents correspond single utterances. The ut-
terances in our corpus of child directed speech are
often only four or five words long, whereas the
general LDA model assumes documents are much
larger. Thus, there may not be enough words to in-
fer a useful utterance specific distribution over top-
ics. Consequently, rather than inferring a separate
topic distribution for each utterance, we simply as-
sume a uniform distribution over objects in the non-
linguistic context. In effect, we rely entirely on the
non-linguistic context and word-object associations
to infer topics. Though necessitated by data sparsity
issues, we also note that it is very rare in our cor-
pus for utterances to refer to more than one object in
the non-linguistic context, so the choice of a single
topic may also be a more accurate model. In fact,
even with multi-sentence documents, LDA may per-
form better if only one topic is assumed per sentence
(Gruber et al, 2007).
3 Inference
We use a collapsed Gibbs sampling procedure, in-
tegrating over all possible Gz , pi, and ? values and
then iteratively sample values for each variable con-
ditioned on the current state of all other variables.
We visit each utterance once per iteration, sample a
topic, and then visit each possible word boundary lo-
cation to sample the boundary and word categories
simultaneously according to their joint probability.
A single topic is sampled for each utterance, con-
ditioned on the words and their current determina-
tions as referring or non-referring. Since zj is drawn
from a uniform distribution, this probability is sim-
ply proportionate to the conditional probability of
the words given zj and the xji variables.
P (zj |wj,xj,h?j) ?
?(?Wjw n
(h?)
w,zj + ?1P0(w))
?(?Wjw n
(h)
w,zj + ?1P0(w))
?
Wj
?
w
?(n(h)w,zj + ?1P0(w))
?(n(h?)w,zj + ?1P0(w))
Here, P (zj |wj,xj,h?j) is the probability of topic
zj given the current hypothesis h for all variables ex-
cluding those for the current utterance. Also, n(h
?j)
w,zj
is the count of occurrences of word type w that refer
to topic zj among the current variable assignments,
and Wj is the set of word types appearing in utter-
ance j. The vectors of word and category variables
in utterance j are represented as wj and xj, respec-
tively. Note that only referring words have any bear-
ing on the appropriate selection of zj and so all fac-
tors involving only non-referring words are absorbed
by the constant of proportionality.
The word categories can be sampled conditioned
on the current word boundary states according to the
following conditional probability, where n(h
?ji)
xji is
the number of words categorized according to label
504
xji over the entire corpus excluding word wji.
P (xji|wji, zj ,h?ji) ? P (wji|zj , xji,h?ji)
?P (xji|h?ji)
=
n(h
?ji)
wji,xjizj + ?xjiP0(wji)
n(h
?ji)
?,xjizj + ?xji
? n
(h?ji)
xji + 1
n(h
?ji)
? + 2
(1)
In practice, however, we actually sample the word
category variables jointly with the boundary states,
using a scheme similar to that outlined in Gold-
water et al (2009). We visit each possible word
boundary location (any point between two consec-
utive phonemes) and compute probabilities for the
hypotheses for which the phonemic environment
makes up either one word or two. As illustrated be-
low there are two sets of cases: those where we treat
the segment as a single word, and those where we
treat it as two words.
x1 x2 x3
. . .#w1#. . . vs. . . .#w2#w3#. . .
? ?
The probabilities of the hypotheses can be derived
by application of equation 1. Since the x variables
can each describe two possible events, there are a to-
tal of six different cases to consider for each bound-
ary assignment: two cases without and four with a
word boundary.
The probability of each of the two cases without
a word boundary can be computed as follows:
P (w1, x1|z,h?) =
n(h
?)
w1,x1z + ?x1P0(w1)
n(h
?)
?,x1z + ?x1
?n
(h?)
x1 + 1
n(h
?)
? + 2
?
n(h
?)
$1 + 1
n(h
?)
? + 2
Here h? signifies the current hypothesis for all
variables excluding those for the current segment
and n(h
?)
$1 is the count for h
? of either utterance fi-
nal words if w1 is utterance final or non-utterance
final words if w1 is also not utterance final.
In the four cases with a word boundary, we have
two words and two categories to sample.
P (w2, x2, w3, x3|z,h?) =
n(h
?)
w2,x2z + ?x2P0(w2)
n(h
?)
?,x2z + ?x2
?n
(h?)
x2 + 1
n(h
?)
? + 2
?
n(h
?)
$2=0 + 1
n(h
?)
? + 2
?n
(h?)
w3,x3z + ?x2(x3)?w2(w3) + ?x3P0(w3)
n(h
?)
?,x3z + ?x2(x3) + ?x3
?n
(h?)
x3 + ?x2(x3) + 1
n(h
?)
? + 3
?
n(h
?)
$3 + ?$2($3) + 1
n(h
?)
? + 3
Here ?x(y) is 1 if x = y and 0 otherwise.
4 Results & Model Comparisons
4.1 Corpus
Our training corpus (Fernald and Morikawa, 1993;
Frank et al, 2009b) consists of about 22,000 words
and 5,600 utterances. Video recordings consisting
of mother-child play over pairs of toys were ortho-
graphically transcribed, and each utterance was an-
notated with the set of objects present in the non-
linguistic context. The object referred to by the ut-
terance, if any, was noted, as described in Frank et al
(2009b). We used the VoxForge dictionary to map
orthographic words to phoneme sequences in a pro-
cess similar to that described in Brent (1999).
Figure 1 (a) presents an example of the coding
of phonemic transcription and non-linguistic context
for a single utterance. The input to the system con-
sists solely of the phonemic transcription and the ob-
jects in the non-linguistic context.
4.2 Evaluation
We ran the sampler ten times for 100,000 iterations
with parameter settings of ?1 = 0.01, ?0 = 20, and
p# = 0.5, keeping only the final sample for evalu-
ation. We defined the word-object pairs for a sam-
ple as the words in the referring category that were
paired at least once with a particular topic. These
pairs were then compared against a gold standard
set of word-object pairs, while segmentation perfor-
mance was evaluated by comparing the final bound-
ary assignments against the gold standard segmenta-
tion.
505
4.2.1 Word Learning
To explore the contribution of word boundaries
to the joint word learning and segmenting task, we
compare our full joint model against a variant that
only infers topics, using the gold standard segmen-
tation as input. In this way we also reproduce the
usual assumption of a sequential relationship be-
tween segmentation and word learning and test the
necessity of the simplifying assumption. The re-
sults are shown in Table 2. We compare them with
three different metric types: topic accuracy; preci-
sion, recall, and F-score of the word-object pairs;
and Kullback-Liebler (KL) divergence.
First, treating utterances with no referring words
as though they have no topic, we compute the ac-
curacy of the inferred topics. Note that we don?t
report accuracy for the the variant with no non-
linguistic context, since in this case the objects are
interchangeable, and we have a problem identifying
which cluster corresponds to which topic. Table 2
shows that the joint segmentation and word learning
model gets the topic right for 81% of the utterances.
The variant that assumes pre-segmented input does
comparably well with an accuracy of 79%. Surpris-
ingly, it seems that knowing the gold segmentation
doesn?t add very much, at least for the topic infer-
ence task.
To evaluate how well we discovered the word-
object map, we manually compiled a list of all the
nouns in the corpus that named one of the 30 ob-
jects. We used this set of nouns, cross-referenced
with their topic objects, as a gold standard set of
word-object pairs. By counting the co-occurrences,
we also compute a gold standard probability distri-
bution for the words given the topic, P (w|z, x = 1).
Precision, recall and F-score are computed as per
Frank et al (2009a). In particular, precision is the
fraction of gold pairs among the sampled set and re-
call is the fraction of sampled pairs among the gold
standard pairs.
p = |Sampled ? Gold||Sampled| , r =
|Sampled ? Gold|
|Gold|
KL divergence is a way of measuring the differ-
ence between distributions. Small numbers gener-
ally indicate a close match and is zero only when
the two are equal. Using the empirical distribution
Object Words
BOX thebox box
BRUSH brush
BUNNY rabbit Rosie
BUS bus
CAR car thecar
CHEESE cheese
DOG thedoggy doggy
DOLL doll thedoll yeah benice
DOUGH dough
ERNIE Ernie
Table 1: Subset of an inferred word-object mapping. For
clarity, the proposed words have been converted to stan-
dard English orthography.
p r f KL acc
Joint 0.21 0.45 0.28 2.78 0.81
Gold Seg 0.21 0.60 0.31 1.82 0.79
Table 2: Word Learning Performance. Comparing
precision, recall, and F-score of word-object pairs,
DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-
ics for the full joint model and a variant that only infers
meanings given a gold standard segmentation.
over gold topics P (z), we use the standard formula
for KL divergence to compare the gold standard dis-
tribution P against the inferred distribution Q. I.e.,
we compute DKL(P (w, z)||Q(w, z)).
The model learns fairly meaningful word-object
associations; results are shown in Table 2. As in the
case of topic accuracy, the joint and word learning
only variants perform similarly, this time with some-
what better performance for the easier task with an
F-score and KL divergence of 0.31 and 1.82 vs. 0.28
and 2.78 for the joint task.
Table 1 illustrates the sort of word-object pairs
the model discovers. As can be seen, many of the
errors are due to the segmentation, usually under-
segmentation errors where it segments two words as
one. This is a general problem with the unigram seg-
menter on which our model is based (Goldwater et
al., 2009). Yet, even though these segmentation er-
rors are also counted as word learning errors, they
are often still meaningful in the sense that the true
referring word is a subsequence.
So, word segmentation has an impact on word
learning. Yet, the joint model still tends to uncover
reasonable meanings. The next question is whether
these meanings have an impact on the segmentation.
506
NoCon Random Joint
Referring Nouns 0.36 0.35 0.50
Neighbors 0.33 0.33 0.37
Utt Final Nouns 0.36 0.36 0.52
Entire Corpus 0.53 0.53 0.54
Table 3: Segmentation performance. F-score for three
subsets and the full corpus for three variants: the model
without non-linguistic context, the model with random
topics, and the full joint model.
4.2.2 Word Segmentation
To measure the impact of word learning on seg-
mentation, we again compare the model on the full
joint task against two other variants: one where top-
ics are randomly selected, and one that ignores the
non-linguistic context. For the random topics vari-
ant, we choose each topic during initialization ac-
cording to the empirical distribution over gold topics
and treat these topic assignments as observed vari-
ables for subsequent iterations. The variant that ig-
nores non-linguistic context draws topics uniformly
from the entire set of objects ever discussed in the
corpus, another test of the contribution of the non-
linguistic context to segmentation. We report token
F-score, computed as per Goldwater et al (2009),
where any segment proposed by the model is a true
positive only if it matches the gold segmentation and
is a false positive otherwise. Any segment in the
gold data not found by the model is a false negative.
Table 3 shows the segmentation performance for
various subsets as well as for the entire corpus. Be-
cause we are primarily interested in synergies be-
tween word learning and segmentation, we focus on
the words most directly impacted by the meanings:
gold standard referring nouns and their neighboring
words.
The model behaves the same with randomized
topics as without context; it learns none of the gold
standard pairs (no matter how we identify clusters
with topics for the contextless case). On all subsets,
the full joint model outperforms the other two vari-
ants. In particular, the greatest gain is for the refer-
ring nouns, with a 21% reduction in error. Also, sim-
ilar to the findings of Bortfeld et al (2005) regarding
6 month olds? abilities to segment words adjoining a
familiar name, we also find that neighboring words
benefit from sharing a word boundary with a learned
word.
The model performs exceptionally well on utter-
ance final referring nouns, with a 24% reduction
in error. This may explain certain psycholinguistic
observations. Frank et al (2006) performed an ar-
tificial language experiment with humans subjects
demonstrating that adults were able to learn words
at the same time as they learned to segment the lan-
guage. However, subjects did much better on a word
learning task when the meaning bearing words were
consistently placed at the end of utterances. There
are several possible reasons why this might have
been the case. For instance, it is common in English
for the object noun to occur at the end of the sen-
tence, and since the subjects were all English speak-
ers, they may have found it easier to learn an artifi-
cial language with a similar pattern. However, our
model predicts another simple possibility: the seg-
mentation task is easier at the end because one of
the two word boundaries is already known (the ut-
terance boundary itself).
4.3 Discussion
The word learning model generally prefers a very
sparse word-to-object map. This is enforced by us-
ing a concentration parameter ?1 that is quite small
relative to the ?0 parameter, and it biases the model
so that the distributions over referring words are
very different from that over non-referring words. A
small concentration parameter biases the estimator
to prefer a small set of word types. In contrast, the
relatively large concentration parameter for the non-
referring words tends to result in most of the words
receiving highest probability as non-referring words.
The model thus categorizes words accordingly. It is
in part due to this tendency towards sparse word-
object maps that the model enforces mutual exclu-
sivity, a phenomenon well documented among natu-
ral word learners (Markman, 1990).
Aside from contributing to mutual exclusivity
and specialization among the topical word distri-
butions, the small concentration parameter also has
important implications for the segmentation task.
A very small value for ?1 discourages the learner
from acquiring more word types for each mean-
ing than absolutely necessary, thereby forcing the
segmenter to use fewer types to explain the se-
quence of phonemes. A model without any notion
507
of meaning cannot maintain separate distributions
for different topics, and must in some sense treat all
words as non-referring. A segmenting model with-
out meanings cannot share the word learner?s reluc-
tance to propose new meaning-bearing word types
and might propose three separate types for ?your
book?, ?a book?, and ?the book?. However, with
a small enough prior on new referring word types,
the word learner that discovers a common refer-
ent for all three sequences and, preferring fewer re-
ferring word types, is more likely to discover the
common subsequence ?book?. With a single word-
object pair (?book?, BOOK), the word learner could
explain reference for all three sequences instead of
using the three separate pairs (?yourbook?, BOOK),
(?abook?, BOOK), and (?thebook?, BOOK).
While relying on non-linguistic context helps seg-
ment the meaning-bearing words, the overall im-
provement is small in our current corpus. One rea-
son for this small improvement was that only 9%
of the tokens in the corpus were referring words.
In corpora containing a larger variety of objects ?
and in cases where sub- and super-ordinate labels
like ?eyes? and ?ears? are coded ? this percentage is
likely to be much higher, leading to a greater boost
in overall segmentation performance.
We should acknowledge that the decisions en-
tailed in enriching the annotations are neither triv-
ial nor without theoretic implication, however. It is
not immediately obvious how to represent the non-
linguistic correlates of verbs, for instance. Devel-
opmentally, verbs are typically acquired much later
than nouns, and it has been argued that this may be
due to the difficulty of producing a cognitive rep-
resentation of the associated meaning (Gentner and
Boroditsky, 2001). Even among concrete nouns, not
all are equal. Children tend to have a bias toward
whole objects when mapping novel words to their
non-linguistic counterparts (Markman, 1990). De-
cisions about more sophisticated encoding of non-
linguistic information may thus require more knowl-
edge about children?s representations of the world
around them
5 Conclusion and Future Work
We find (1) that it is possible to jointly infer both
meanings and a segmentation in a fully unsupervised
way and (2) that doing so improves the segmenta-
tion performance of our model. In particular, we
found that although the word learning side suffered
from segmentation errors, and performed worse than
a model that learned from a gold standard segmen-
tation, the loss was only slight. On the other hand,
segmentation performance for the meaning bearing
words improved a great deal. The first result sug-
gests that is not necessary to assume fully segmented
input in order to learn word meanings, and that the
segmentation and word learning tasks can be effec-
tively modeled in parallel, allowing us to explore po-
tential developmental interactions. The second re-
sult suggests that synergies do actually exist and ar-
gue not only that we can model the two as parallel
processes, but that doing so could prove fruitful.
Our model is relatively simple both in terms of
word learning and in terms of word segmentation.
For instance, social cues and shared attention, or dis-
course effects, might all play a role (Frank et al,
2009b). Shared features or other relationships can
also potentially impact how quickly one might gen-
eralize a label to multiple instances (Tenenbaum and
Xu, 2000). There are many ways to elaborate on the
word learning task, with additional potential syner-
gistic implications.
We might also elaborate the linguistic structures
we incorporate into the word learning model. For
instance, Johnson (2008) explores synergies in syl-
lable and morphological structures in word segmen-
tation. Aspects of linguistic structure, such as mor-
phology, may contribute to word meaning learning
beyond its contribution to word segmentation per-
formance.
Acknowledgments
This research was funded by NSF awards 0544127
and 0631667 to Mark Johnson and by NSF DDRIG
0746251 to Michael C. Frank. We would also like
to thank Anne Fernald for providing the corpus and
Maeve Cullinane for help in coding it.
References
Nameera Akhtar and Lisa Montague. 1999. Early lexi-
cal acquisition: The role of cross-situational learning.
First Language, 19(57 Pt 3):347?358.
508
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Heather Bortfeld, James L. Morgan, Roberta Michnick
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speechstream segmentation. Psychological Science,
16(4):298?304.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Anne Fernald and Hiromi Morikawa. 1993. Common
themes and cultural variations in japanese and ameri-
can mothers? speech to infants. In Child Development,
number 3, pages 637?656, June.
Michael C. Frank, Vikash Mansinghka, Edward Gibson,
and Joshua B. Tenenbaum. 2006. Word segmentation
as word learning: Integrating stress and meaning with
distributional cues. In Proceedings of the 31st Annual
Boston University Conference on Language Develop-
ment.
Michael C. Frank, Sharon Goldwater, Vikash Mans-
inghka, Tom Griffiths, and Joshua Tenenbaum. 2007.
Modeling human performance in statistical word seg-
mentation. Proceedings of the 29th Annual Meeting of
the Cognitive Science Society, pages 281?286.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009a. Using speakers? referential inten-
tions to model early cross-situational word learning.
Psychological Science, 5:578?585.
Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-
baum, and Anne Fernald. 2009b. Continuity of dis-
course provides information for word learning.
Dedre Gentner and Lera Boroditsky. 2001. Individua-
tion, relativity, and early word learning. Language,
culture, & cognition, 3:215?56.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,
and Jenny R. Saffran. 2007. Can infants map meaning
to newly segmented words? statistical segmentation
and word learning. Psychological Science, 18(3):254?
260.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Artificial Intelligence
and Statistics (AISTATS), March.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Ellen M. Markman. 1990. Constraints children place on
word learning. Cognitive Science, 14:57?77.
Terry Regier. 2003. Emergent constraints on word-
learning: A computational review. Trends in Cognitive
Sciences, 7:263?268.
Jenny R. Saffran, Elissa L. Newport, and Richard N.
Aslin. 1996. Word segmentation: The role of dis-
tributional cues. Journal of memory and Language,
35:606?621.
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39?91.
Joshua B. Tenenbaum and Fei Xu. 2000. Word learn-
ing as bayesian inference. In Proceedings of the 22nd
Annual Conference of the Cognitive Science Society,
pages 517?522.
Chen Yu and Dana H. Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
509
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665?668,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Reranking the Berkeley and Brown Parsers?
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mjohnson@science.mq.edu.au
Ahmet Engin Ural
Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
aeural@gmail.com
Abstract
The Brown and the Berkeley parsers are two
state-of-the-art generative parsers. Since both
parsers produce n-best lists, it is possible to
apply reranking techniques to the output of
both of these parsers, and to their union. We
note that the standard reranker feature set dis-
tributed with the Brown parser does not do
well with the Berkeley parser, and propose an
extended set that does better. An ablation ex-
periment shows that different parsers benefit
from different reranker features.
1 Introduction
Syntactic parsing is the task of identifying the
phrases and clauses in natural language sentences.
It has been intensively studied primarily because it
is generally believed that identifying syntactic struc-
ture is a first step towards semantic interpretation.
This paper focuses on parsing the Wall Street Jour-
nal (WSJ) section of the University of Pennsylva-
nia treebank corpus (Marcus et al, 1993). There
are a large number of different approaches to this
task. For simplicity we focus on two popular gener-
ative statistical parsing models: Charniak?s ?Maxi-
mum Entropy Inspired? parser (Charniak and John-
son, 2005) and Petrov?s ?split-merge? parser (Petrov
et al, 2006). We follow conventional informal usage
and refer to these as the ?Brown? and the ?Berkeley?
parsers respectively.
? We would like to thank Eugene Charniak and the other
members of BLLIP for their helpful advice on this work. Natu-
rally all errors remain our own.
Briefly, the Berkeley parser is a smoothed PCFG
whose non-terminals are refinements of the original
treebank grammar obtained by an automatic split-
merge procedure, while the Brown parser is effec-
tively a smoothed PCFG whose non-terminals en-
code a wide variety of manually chosen condition-
ing information, such as heads, governors, etc. The
Berkeley parser is usually viewed as unlexicalized
(although the preterminals may be split so finely
that they may be viewed as identifying lexical clus-
ters), while essentially every distribution used in
the Brown parser conditions on lexical information.
Even from this cursory description it is clear that
these parsers parsers extract generalizations from the
training data in different ways.
This paper applies reranking (Collins and Koo,
2005) to the n-best output of both parsers individ-
ually, as well as to an n-best list consisting of the
union of the outputs of both parsers. We are inter-
ested to see whether the same kinds of features im-
prove the performance of both the Berkeley and the
Brown parsers, or whether successful reranking re-
quires features that are specially tuned to the parser
it is applied to. Finally, we are interested in the
performance of the reranker trained on the union n-
best lists. Combining the output of multiple parsers
in other more complex ways has been previously
demonstrated to improve overall accuracy, so it is in-
teresting to see if the relatively simple method used
here improves parsing accuracy as well.
The approach of Zhang et al (2009) is closest to
the work described here. They combine n-best lists
produced by the same parsers as we do, but use only
a relatively small set of features (the log probabil-
665
Trees Reranker features
standard extended
Berkeley 91.6 91.7
Brown 91.8 91.6
Combined 91.8 91.9
Table 1: The f-scores on section 22 of rerankers trained
on folds 1?18 by minimizing a regularized ?MaxEnt? ob-
jective (negative log likelihood with a Gaussian regular-
izer) using L-BFGS. The weight of the regularizer was
tuned to optimize f-score on folds 19?20.
ity of the parses plus a constituent overlap feature),
while we investigate models with millions of fea-
tures here. They report a higher f-score than we do
when they replace the generative Brown parser with
the the self-trained discriminatively-reranked parser
of McClosky et al (2006), but with inputs provided
by the generative Berkeley and Brown n-best parsers
they report an f-score of 91.43 on section 23, which
is consistent with the results reported here.
2 Experimental setup
We ran both parsers in 50-best mode, and con-
structed 20-fold cross-validated training data as de-
scribed in Collins and Koo (2005) and Charniak
and Johnson (2005), i.e., the trees in sections 2?21
of the WSJ treebank were divided into 20 equal-
sized folds, and the parses for each fold were gen-
erated by a parser trained on the trees in the other
folds. Then sections 22, 23 and 24 were parsed us-
ing the standard ?out-of-the-box? parser. Follow-
ing the suggestion in Collins and Koo (2005), in or-
der to avoid over-training on section 23 all rerank-
ing experiments reported here (except the final one)
used folds 1?18 as training data, used folds 19?20
as development data and used section 22 as test data.
(The averaged perceptron algorithm does not require
development data, so the experiments using that al-
gorithm report averages over folds 19?20 and sec-
tion 22).
The Berkeley parser can be run in many modes;
in order to produce the 20-fold training data we ran
the Berkeley trainer with 6 splits, and ran the re-
sulting parsers in ?accurate? mode. It failed to pro-
duce any parses for 12 sentences in sections 2?21
and one sentence in section 24. The Brown parser
was trained using the ?out-of-the-box? settings, and
produced parses for all sentences.
Using the reranker features distributed with the
Brown reranker (Charniak and Johnson, 2005),
which we call the ?standard? set below, we ob-
tained no overall improvement in f-score when ei-
ther reranking the Berkeley parser n-best lists alone,
or when the Berkeley parses were combined with the
Brown parses.
However, it is possible that these results reflect
the fact that the features used by the reranker were
chosen because they improve the Brown parser, i.e.,
they are the result of feature selection based on
reranking the Brown parser?s n-best lists. In order
to determine if this is the case, we developed an ?ex-
tended? feature set that incorporates a wider set of
features, specifically including features that capture
global properties of the tree that might be harder for
the Berkeley parser to learn.
Our extended feature set consists of
4,256,553 features, which are instances of
162 feature classes, which in turn are grouped
into 20 feature ?super-classes?. By contrast, the
standard feature set contains 1,333,950 features in
90 feature classes, grouped into 14 super-classes.
A brief description of the extended feature set
super-classes follows:
Parser: an indicator feature indicating which parsers
generated this parse,
RelLogP: the log probability of this parse according to
each parser,
InterpLogCondP: an indicator feature based on the
binned log conditional probability according to
each parser,
RightBranch: an indicator function of each node that
lies on the right-most branch of the parse tree,
Heavy: an indicator function based on the size and lo-
cation of each nonterminal (designed to identify the
locations of ?heavy? phrases),
LeftBranchLength: an indicator function of the binned
length of each left-branching chain,
RightBranchLength: an indicator function of the
binned length of each right-branching chain,
Rule: an indicator function of parent and children cate-
gories, optionally with head POS annotations,
NNGram: and indicator function of parent and n-gram
sequences of children categories, optionally head
666
 Pa
rse
r
 Re
lLo
gP
 Int
erp
Lo
gC
ond
P
 Ri
ght
Bra
nch
 He
avy
 Le
ftB
ran
chL
eng
th
 Ri
ght
Bra
nch
Le
ngt
h
 Ru
le
 NN
Gra
m
 He
ads
 Sy
nSe
mH
ead
s
 RB
Co
nte
xt
 Su
bjV
erb
Ag
r
 Co
Par
 Co
Le
nPa
r
 W
ord
 W
Pro
j
 W
Ed
ges
 NG
ram
Tre
e
 He
adT
ree
-0.3
-0.2
-0.1
0.0
0.1
Berkeley
Brown
Combined
Figure 1: The average change in f-score on folds 19?20 and section 22 caused by removing a feature superclass
from the extended feature set and retraining. Difference in scores less that 0.1% are probably not significant. In this
experiment all rerankers were trained using the averaged perceptron algorithm. With the full extended feature set,
rerankers trained with the averaged perceptron algorithm achieve f-scores of 91.2% on both the Berkeley and Brown
parses, and 91.6% on the combined parses.
annotated, inspired by the n-gram rule features de-
scribed in Collins and Koo (2005),
Heads: an indicator function of ?head-to-head? depen-
dencies,
SynSemHeads: an indicator function of the pair of syn-
tactic (i.e., functional) and semantic (i.e., lexical)
heads of each non-terminal,
RBContext: an indicator function of how much each
subtree deviates from from right-branching,
SubjVerbAgr: an indicator function of whether subject-
verb agreement is violated,
CoPar: an indicator function that fires when conjoined
phrases in a coordinate structure have approxi-
mately parallel syntactic structure,
CoLenPar: an indicator function that fires when con-
joined phrases in a coordinate structure have ap-
proximately the same length,
Word: an indicator function that identifies words and
their preterminals,
WProj: an indicator function that identifies words and
their phrasal projections up to their maximal pro-
jection,
WEdges: an indicator function that identifies the words
and POS tags appearing at the edges of each nonter-
minal,
NGramTree: an indicator function of the subtree con-
sisting of nodes connecting each pair of adjacent
words in the parse tree, and
HeadTree: a tree fragment consisting of a head word
and its projection up to its maximal projection, plus
all of the siblings of each node in this sequence (this
is like an auxiliary tree in a TAG).
The InterpLogCondP features were designed to
capture non-linearities in the way that the Berke-
ley and Brown parsers assign probabilities to trees.
We deliberately added features that incorporated lin-
guistic notions such as head, governor and maximal
projection, as the Berkeley parser does not explic-
itly condition on such information (in contrast to the
Brown parser, which does).
In fact, as the reader can verify the differences in
f-scores between rerankers containing the extended
features and the standard features is minimal. In
order to better study the importance of the various
features we conducted an ablation study, in which
we trained rerankers which were missing one feature
superclass from the 20 superclasses of the extended
feature set. In order to speed training time we used
the averaged perceptron algorithm (Collins, 2002)
(it converges an order of magnitude faster than the L-
BFGS algorithm we used in the other experiments,
but the f-score of the model estimated with the av-
eraged perceptron is approximately 0.1% lower than
when using L-BFGS). The results from this experi-
ment are shown in Figure 1. The averaged percep-
tron algorithm does not rely on the development data
667
(folds 19?20), so the results we report are average f-
scores on the development data and on section 22
(we did this because the differences are small, so
a larger evaluation set may be able to detect differ-
ences more reliably).
It is interesting that linguistically-informed fea-
tures (such as Heads, SynSemHeads and HeadTree)
seem to be much more important when reranking
the combined n-best lists than when reranking the
output of either parser alone. This suggests that the
log probability scores from both parsers are inter-
nally consistent, but need to be recalibrated when
the parses are combined. The log probability scores
from the parsers themselves (in the form of the In-
terpLogCondP feature) are also supplying useful in-
formation that the reranker features on their own are
not providing. Finally, the WEdges feature, which
identifies the words and POS at the left and right
boundaries of each nonterminal, also provides ex-
tremely useful information, especially for reranking
the Berkeley parser.
3 Conclusion
Reranking is a straight-forward method for improv-
ing the accuracy of n-best parsers. While one
might have hoped that reranking the n-best output
of the Berkeley parser, or the union of the outputs
of the Berkeley and Brown parsers, would dramat-
ically improve overall f-score, this seems not to be
the case. It?s possible that the features of current
rerankers have been implicitly designed to work well
with parsers like the Brown parser, but a reranker
with a dramatically enlarged feature set performs
only marginally better. This result was confirmed
by training a reranker with the extended features on
the union of the output of the Berkeley and Brown
parsers on sections 2?21 and testing on section 23
(i.e., the standard WSJ parsing evaluation), which
achieved an f-score of 91.49%; approximately 0.1%
higher than a reranker with the standard feature set
trained on the output of the Brown parser alone.
Acknowledgments
This research was funded by NSF awards 0530118,
0544127 and 0631667.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia. Association for Computational Linguistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore. Association for Computational
Linguistics.
668
Proceedings of NAACL-HLT 2013, pages 190?200,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Topic Segmentation with a Structured Topic Model
Lan Du
Department of Computing
Macquarie University
Sydney, Australia
lan.du@mq.edu.au
Wray Buntine
Canberra Research Lab
National ICT Australia
Canberra, Australia
wray.buntine@nicta.com.au
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.au
Abstract
We present a new hierarchical Bayesian model
for unsupervised topic segmentation. This new
model integrates a point-wise boundary sam-
pling algorithm used in Bayesian segmenta-
tion into a structured topic model that can cap-
ture a simple hierarchical topic structure latent
in documents. We develop an MCMC infer-
ence algorithm to split/merge segment(s). Ex-
perimental results show that our model out-
performs previous unsupervised segmentation
methods using only lexical information on
Choi?s datasets and two meeting transcripts
and has performance comparable to those pre-
vious methods on two written datasets.
1 Introduction
Documents are usually comprised of topically co-
herent text segments, each of which contains some
number of text passages (e.g., sentences or para-
graphs) (Salton et al, 1996). Within each topically
coherent segment, one would expect that the word
usage demonstrates more consistent lexical distri-
butions (known as lexical cohesion (Eisenstein and
Barzilay, 2008)) than that across segments. A linear
partition of texts into topic segments may reveal in-
formation about, for example, themes of segments
and the overall thematic structure of the text, and
can subsequently be useful for text analysis tasks,
such as information retrieval (e.g., passage retrieval
(Salton et al, 1996)), document summarisation and
discourse analysis (Galley et al, 2003).
In this paper we consider how to automatically
find a topic segmentation. It involves identifying
the most prominent topic changes in a sequence
of text passages, and splits those passages into a
sequence of topically coherent segments (Hearst,
1997; Beeferman et al, 1999). This task can be cast
as an unsupervised machine learning problem: plac-
ing topic boundaries in unannotated text.
Although a variety of cues in text can be used for
topic segmentation, such as cue phases (Beeferman
et al, 1999; Reynar, 1999; Eisenstein and Barzi-
lay, 2008)) and discourse information (Galley et al,
2003), in this paper, we focus on lexical cohesion
and use it as the primary cue in developing an un-
supervised segmentation model. The effectiveness
of lexical cohesion has been demonstrated by Text-
Tiling (Hearst, 1997), c99 (Choi, 2000), MinCut
(Malioutov and Barzilay, 2006), PLDA (Purver et
al., 2006), Bayesseg (Eisenstein and Barzilay, 2008),
TopicTiling (Riedl and Biemann, 2012), etc.
Our work uses recent progress in hierarchi-
cal topic modelling with non-parametric Bayesian
methods (Du et al, 2010; Chen et al, 2011; Du et
al., 2012a), and is based on Bayesian segmentation
methods (Goldwater et al, 2009; Purver et al, 2006;
Eisenstein and Barzilay, 2008) using topic mod-
els. This can also be viewed as a multi-topic exten-
sion of hierarchical Bayesian segmentation (Eisen-
stein, 2009), although our use of hierarchies is used
to improve the performance of linear segmentation,
rather than develop hierarchical segmentation.
Recently, topic models are increasingly used in
various text analysis tasks including topic segmen-
tation. Previous work (Purver et al, 2006; Misra
et al, 2008; Sun et al, 2008; Misra et al, 2009;
Riedl and Biemann, 2012) has shown that using
190
topic assignments or topic distributions instead of
word frequency can significantly improve segmen-
tation performance. Here we consider more ad-
vanced topic models that model dependencies be-
tween (sub-)sections in a document, such as struc-
tured topic models (STMs) presented in (Du et al,
2010; Du et al, 2012b). STMs treat each text as
a sequence of segments, each of which is a set of
text passages (e.g., a paragraph or sentence). Text
passages in a segment share the same prior distribu-
tion on their topics. The topic distributions of seg-
ments in a single document are then encouraged to
be similar via a hierarchical prior. This gives a sub-
stantial improvement in modelling accuracy. How-
ever, instead of explicitly learning the segmentation,
STMs just leverage the existing structure of docu-
ments from the given segmentation.
Given a sequence of text passages, how can we
automatically learn the segmentation? The word
boundary sampling algorithm introduced in (Gold-
water et al, 2009) uses point-wise sampling of word
boundaries after phonemes in an utterance. Simi-
larly, the segmentation method of PLDA (Purver
et al, 2006) samples segment boundaries, but also
jointly samples a topic model. This is different to
other topic modelling approaches that run LDA as
a precursor to a separate segmentation step (Misra
et al, 2009; Riedl and Biemann, 2012). While con-
ceptually similar to PLDA, our non-parametric ap-
proach built on STM required new methods to im-
plement, but the resulting improvement by the stan-
dard segmentation scores is substantial.
This paper presents a new hierarchical Bayesian
unsupervised topic segmentation model, integrating
a point-wise boundary sampling algorithm with a
structured topic model. This new model takes ad-
vantage of the high modelling accuracy of structured
topic models (Du et al, 2010) to produce a topic
segmentation based on the distribution of latent top-
ics. We show that this model provides high quality
segmentation performance on Choi?s dataset, as well
as two sets of meeting transcripts and written texts.
In the following sections we describe our topic
segmentation model and an MCMC inference al-
gorithm for the non-parametric split/merge pro-
cess. The rest of the paper is organised as follows. In
Section 2 we review recent related work in the topic
segmentation literature. Section 3 presents the new
topic segmentation model, followed by the deriva-
tion of a sampling algorithm in Section 4. We report
the experimental results by comparing several re-
lated topic segmentation methods in Section 5. Sec-
tion 6 concludes the paper.
2 Related Work
We are interested in unsupervised topic segmenta-
tion in either written or spoken language. There is a
large body of work on unsupervised topic segmen-
tation of text based on lexical cohesion. It can be
characterised by how lexical cohesion is modelled.
One branch of this work represents the lexical co-
hesion in a vector space by exploring the word co-
occurrence patterns, e.g., TF or TF-IDF. Work fol-
lowing this line includes TextTiling (Hearst, 1997),
which calculates the cosine similarity between two
adjacent blocks of words purely based on the word
frequency; C99 (Choi, 2000), an algorithm based
on divisive clustering with a matrix-ranking scheme;
LSeg (Galley et al, 2003), which uses a lexical
chain to identify and weight word repetitions; U00
(Utiyama and Isahara, 2001), a probalistic approach
using dynamic programming to find a segmenta-
tion with a minimum cost; MinCut (Malioutov and
Barzilay, 2006), which casts segmentation as a graph
cut problem, and APS (Kazantseva and Szpakowicz,
2011), which uses affinity propagation to learn clus-
tering for segmentation.
The other branch of this work characterises the
lexical cohesion using topic models, to which the
model introduced in Section 3 belongs. Lexical co-
hesion in this line of research is modelled by a
probabilistic generative process. PLDA presented by
Purver et al (2006) is an unsupervised topic mod-
elling approach for segmentation. It chains a set of
LDAs (Blei et al, 2003) by assuming a Markov
structure on topic distributions. A binary topic shift
variable is attached to each text passage (i.e., an ut-
terance in (Purver et al, 2006)). It is sampled to in-
dicate whether the jth text passage shares the topic
distribution with the (j ? 1)th passage.
Using a similar Markov structure, SITS (Nguyen
et al, 2012) chains a set of HDP-LDAs (Teh et al,
2006). Unlike PLDA, SITS assumes each text pas-
sage is associated with a speaker identity that is at-
tached to the topic shift variable as supervising in-
191
formation. SITS further assumes speakers have dif-
ferent topic change probabilities that work as pri-
ors on topic shift variables. Instead of assuming
documents in a dataset share the same set of top-
ics, Bayesseg (Eisenstein and Barzilay, 2008) treats
words in a segment generated from a segment spe-
cific multinomial language model, i.e., it assumes
each segment is generated from one topic, and a
later hierarchical extension (Eisenstein, 2009) as-
sumes each segment is generated from one topic or
its parents. Other methods using as input the output
of topic models include (Sun et al, 2008), (Misra et
al., 2009), and (Riedl and Biemann, 2012).
In this paper we take a generative approach ly-
ing between PLDA and SITS. In contrast to PLDA,
which uses a flat topic model (i.e., LDA), we assume
each text has a latent topic structure that can reflect
the topic coherence pattern, and the model adapts its
parameters to the segments to further improve per-
formance. Unlike SITS that targets analysing multi-
party meeting transcripts, where speaker identities
are available, we are interested in more general texts
and assume each text has a specific topic change
probability, since (1) the identity information is not
always available for all kinds of texts (e.g., continu-
ous broadcast news transcripts (Allan et al, 1998)),
(2) even for the same author, topic change probabil-
ities for his/her different articles might be different.
3 Segmentation with Topic Models
In documents, topically coherent segments usually
encapsulate a set of consecutive passages that are
semantically related (Wang et al, 2011). However,
the topic boundaries between segments are often un-
available a priori. Thus we treat all passage bound-
aries (e.g., sentence boundaries, paragraph bound-
aries or pauses between utterances) as possible topic
boundaries. To recover the topic boundaries we de-
velop a structured topic segmentation model by inte-
grating ideas from the segmented topic model (Du et
al., 2010, STM) and Bayesian segmentation models.
The basic idea of our model is that each docu-
ment consists of a set of segments where text pas-
sages in the same segment are generated from the
same topic distribution, called segment level topic
distribution. The segment level topic distribution is
drawn from a topic distribution associated with the
whole document, called document level topic distri-
bution. The relationships between the levels is man-
aged using Bayesian non-parametric methods and a
significant change in segment level topic distribution
indicates a segment change.
Our unsupervised topic segmentation model is
based on the premise that using a hierarchical topic
model like the STM with a point-wise segment
sampling algorithm should allow better detection
of topic boundaries. We believe that (1) segment
change should be associated with significant change
in the topic distribution, (2) topic cohesion can be
reflected in document topic structure, (3) the log-
likelihood of a topically coherent segment is typi-
cally higher than an incoherent segment (Misra et
al., 2008).
Assume we have a corpus of D documents, each
document d consists of a sequence of Ud text pas-
sages, and each passage u contains a set of Nd,u
words denoted by wd,u that are from a vocabulary
W . Our model consists of:
Modelling topic boundary: We assume each
document has its own topic shift probability
pid, a Beta distributed random variable, i.e.,
pid?Beta(?0, ?1). Then, we associate a bound-
ary indicator variable ?d,u with u, like the
topic shift variable in PLDA and SITS. ?d,u
is Bernoulli distributed with parameter pid, i.e.,
?d,u?Bernoulli(pid). It indicates whether there is a
topic boundary after text passage u or not. To sample
?d,u, we use a point-wise sampling algorithm. Con-
sequently, a sequence of ??s defines a set of seg-
ments, i.e., a topic segmentation of d. For example,
let a ? vector ? = (0, 0, 1, 0, 1, 0, 0, 1)1, it gives
us three segments, which are {1, 2, 3}, {4, 5} and
{6, 7, 8}.
Modelling topic structure: Following the idea of
the STM, we assume each document d is associated
with a document level topic distribution ?d, which
is drawn from a Dirichlet distribution with param-
eter ?; and text passages in topic segment s in d
are generated from ?d,s, a segment level topic dis-
tribution. The number of segments Sd can be com-
puted as Sd=1 +
?Ud?1
u=1 ?d,u. Then, a Pitman-Yor
1The last 1 in ? is the document boundary that is know a
priori. This means one does not need to sample it.
192
DK
?
?
?
?
?
?
?
U
s
w
z
S
U
N
Figure 1: The topic segmentation model
process with a discount parameter a and a concen-
tration parameter b is used to link ?d and ?d,s by
?d,s?PYP(a, b, ?d), which forms a simple topic
hierarchy. The idea here is that topics discussed in
segments can be variants of topics of the whole
document. Du et al (2010) have shown that this
topic structure can significantly improve the mod-
elling accuracy, which should contribute to more ac-
curate segmentation. This generative process is dif-
ferent from PLDA. PLDA does not assume the docu-
ment level topic distribution and each time generates
the segment level topic distribution directly from a
Dirichlet distribution.
The complete probabilistic generative process,
shown as a graph in Figure 1 is as follows:
1. For each topic k ? {1, . . . , K}, draw a word distribution
?k ? DirichletW (?).
2. For each document d ? {1, . . . , D},
(a) Draw topic shift probability pid ? Beta(?0, ?1).
(b) Draw ?d ? DirichletK (?).
(c) For each text passage (except last) u ?
{1, . . . , Ud ? 1}, draw ?d,u ? Bernoulli(pid).
(d) Compute Sd the number of segments as 1 +?Ud?1
u=1 ?d,u.
(e) For each segment s ? {1, . . . , Sd}, draw ?d,s ?
PYP(a, b, ?d).
(f) For each text passage u ? {1, . . . , Ud},
i. Set segment sd,u = 1 +
?u?1
v=1 ?d,v .
ii. For each word index n ? {1, . . . , Nd,u},
A. Draw topic zd,u,n ? DiscreteK
(
?d,sd,u
)
.
B. Draw word wd,u,n ? DiscreteK(?zd,u,n).
where sd,u indicates which segment text passage u
belongs to. We assume the dimensionality of the
Dirichlet distribution (i.e., the number of topics) is
known and fixed, and word probabilities are param-
eterized with a K ? Wmatrix ? = (?1, . . . , ?K).
In future work we plan to investigate replace the
Table 1: List of statistics
Mk,w total number of words with topic k.
Mk a vector of Mk,w.
nd,s,k total number of words with topic k in segment
s in document d.
Nd,s total number of words in segment s.
td,s,k table count of topic k in the CRP for segment
s in document d.
td,s a vector of td,s,k for segment s in d.
Td,s total table count in segment s.
cd,1 total number of topic boundaries in d.
cd,0 total number of non-topic boundaries in d.
Dirichlet prior ? on ? with a Pitman-Yor prior (Pit-
man and Yor, 1997) to make the model fully non-
parametric, like SITS.
4 Posterior Inference
In this section we develop a collapsed Gibbs sam-
pling algorithm to do an approximate inference
by integrating out some latent variables (i.e., ??s,
??s and pid?s). The hierarchy in our model can be
well explained with the Chinese restaurant franchise
metaphor introduced in (Teh et al, 2006). For easier
understanding, terminologies of the Chinese Restau-
rant Process (CRP) will be used throughout this sec-
tion, i.e., customers, dishes and restaurants, corre-
spond to words, topics, and segments respectively.
Statistics used are listed in Table 1.
To integrate out the ?d,s?s generated from the
PYP, we use the technique presented in (Chen et
al., 2011), which computes the joint posterior for
the PYP by summing out all the possible seating
arrangements for a sequence of customers (Teh,
2006). In this technique an auxiliary binary variable,
called table indicator (?d,u,n), is introduced to fa-
cilitate computing table count td,s,k for topic k. This
method has two effects: (1) faster mixing of the sam-
pler, and (2) elimination of the need for dynamic
memory to store the populations/counts of each ta-
ble in the CRP. In the CRP each word wd,u,n in topic
k (i.e., where zd,u,n=k) contributes a count to nd,s,k
for u ? s; and, if wd,u,n, as a customer, also opens
a new table to the CRP, it leads to increasing td,s,k
by one. In this case, ?d,u,n=1 indicates wd,u,n is the
first customer on the table, called table head. Thus,
td,s,k =
?
u?s
Nd,u?
n=1
?d,u,n1zd,u,n=k . (1)
Note the two constraints on these two counts, i.e.,
nd,s,k?td,s,k?0 and td,s,k=0 iff nd,s,k=0 (2)
193
can be replaced be a simpler constraint in the table
indicator representation.
The sampler we develop is an MCMC sampler
on the space ? = {z, ?,?} where z defines the
topic assignments of words, ? maintains the needed
CRP configuration (from which t is derived) and ?
defines the segmentation. Moreover, it is not a tra-
ditional Gibbs sampler changing one variable at a
time, but is a block Gibbs sampler where two dif-
ferent kinds of blocks are used. The first block is
(zd,u,n, ?d,u,n) (for each word wd,u,n), which can
be sampled with a table indicator variant of a hier-
archical topic sampler (Du et al, 2010), described
in Section 4.1. This corresponds to Equation (6) in
(Purver et al, 2006). The second kind of block is
a boundary indicator ?d,u together with a particular
constrained set of table counts designed to handle
splitting and merging, which corresponds to Equa-
tion (7) in (Purver et al, 2006). Sampling this sec-
ond kind of block is harder in our non-parametric
model requiring a potentially exponential summa-
tion, a problem we overcome using symmetric poly-
nomials, shown in Section 4.2.
4.1 Sampling Topics
One step in our model is to sample the assignments
of topics to words conditioned on all ??s. As dis-
cussed in Section 3, given the sequence of ?d,u?s,
?d, one can figure out which segment s text passage
u belongs to. Thus, conditioned on a set of segments
s given by ?, the joint posterior distribution ofw, z
and ? is computed as p(z,w, ? |?, ?, a, b, ?)
=
?
d
BetaK
(
?+
?
s td,s
)
BetaK (?)
?
k
BetaW (? +Mk)
BetaW (?)
?
d
?
s?s
(b|a)Td,s
(b)Nd,s
?
k
Snd,s,ktd,s,k,a
(
nd,s,k
td,s,k
)?1
, (3)
where BetaK(?) is a K-dimension Beta function,
(x|y)n the Pochhammer symbol2, and Snt,a the gen-
eralised Stirling number of the second kind (Hsu
and Shiue, 1998)3 precomputed in a table so cost-
2The Pochhammer symbol (x|y)n denotes the rising facto-
rial with a specified increment, i.e., y. It is defined as (x|y)n =
x(x+ y)...(x+ (n? 1)y).
3A Stirling number of the second kind is used to study
the number of ways of partitioning a set of n objects into
k nonempty subsets. The generalised version given by Hsu
and Shiue (1998) has a linear recursion which in our case is
Sn+1m,a = S
n
m?1,a + (n?ma)S
n
m,a.
ing O(1) to use (Buntine and Hutter, 2012).Eq (3)
is an indicator variant of Eq (1) in (Du et al, 2010)
with applying Theorem 1 in (Chen et al, 2011).
Given the current segmentation and topic assign-
ments for all other words, using Bayes rule, we can
derive the following two conditionals from Eq (3):
1. The joint probability of assigning topic k to word
wd,u,n and wd,u,n being a table head, p(zd,u,n =
k, ?d,u,n = 1 |?
?)
=
?wi,j,n +Mk,wi,j,n?
w(?w +Mk,w)
?k +
?
s td,s,k?
k ?k +
?
s,k td,s,k
b+ aTd,s
b+Nd,s
Snd,s,k+1td,s,k+1,a
Snd,s,ktd,s,k,a
td,s,k + 1
nd,s,k + 1
(4)
2. The joint probability of assigning k to wd,u,n
and wd,u,n not being a table head, p(zd,u,n =
k, ?d,u,n = 0 |?
?)
=
?wi,j,l +Mk,wi,j,l?
w ?w +Mk,w
1
b+Nd,s
Snd,s,k+1td,s,k,a
Snd,s,ktd,s,k,a
nd,s,k + 1? td,s,k
nd,s,k + 1
(5)
where ?? = {z?zd,u,n ,w, ???d,u,n ,?,?, a, b,?}.
From the two conditionals, we develop a blocked
Gibbs sampling algorithm for (zd,u,n, ?d,u,n).
4.2 Sampling Segmentation Boundaries
In our model, each segment corresponds to a
Chinese restaurant in the CRP. Sampling topic
boundaries corresponds to splitting/merging restau-
rant(s). This is different from the split-merge process
proposed by Jian and Neal (2004), where one actu-
ally splits/merges table(s). To our knowledge, there
has been no method developed to split/merge restau-
rant(s). We tried different approximations, such
as the minimum-path-assumption (Wallach, 2008),
which in our case assumes one table for each topic
k, and all words in k are placed in the same ta-
ble. Although this simplifies the split-merge pro-
cess, it yielded poor results. We instead developed a
novel approximate block Gibbs sampling algorithm
using symmetric polynomials. Its segmentation per-
formance worked well in our development dataset.
For simplicity, we consider a passage u in doc-
ument d, and assume: (1) If ?d,u=1, there are two
segments, sl and sr; sl ends at text passage u, and sr
starts at text passage u+1. (2) If ?d,u=0, there is one
194
segment, sm, where u is is somewhere in the middle
of sm. The split-merge choice we sample is one to
many, for a given split pair (sl, sr) we consider a set
of merged states sm (represented by different possi-
ble table counts). Then, to compute the Gibbs prob-
ability for splitting/merging restaurant(s), we con-
sider the probability of the single split, the probabil-
ity of the corresponding set of merges, and then if a
merge is selected, we have to sample from the set of
merges. These are as follows:
Splitting: split sm into sr and sl by placing a
boundary after u. Since passages have a fixed order
in each document, all the words are put into sr and
sl based on which passages they belong to. Then,
given all the topic assignments, we first sample all
table indicators ?d,u?,n, for n ? {1, ..., Nd,u?} and
u? ? sm using Bernoulli sampling without replace-
ment. It runs as follows: 1) sample ?d,u?,n according
to probability td,sm,k/nd,sm,k; 2) decrease td,sm,k if
?d,u?,n = 1, otherwise, just decrease nd,sm,k. Us-
ing the sampled ?d,u?,n?s we compute the inferred ta-
ble counts td,s,k (from Eq (1)) and customer counts
nd,s,k respectively for segments s=sl and sr and
topics k. The computation may result in the follow-
ing cases: for a given topic k,
(I) Both sl and sr have nd,s,k>0 and td,s,k?1, which
means both segments have words assigned to k and
words being labelled with table head. According
to constraints (2), after splitting, restaurants corre-
sponding to sl and sr are valid. We do not make any
change on table counts.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0. In this
case, for example, all the words assigned to k in sm
are in sl after splitting, and all those labelled with
table head should also be in sl. sr has no words as-
signed to k. Thus, there is no need to change table
counts.
(III) Either sl or sr has nd,s,k>0 and td,s,k=0. Both seg-
ments have words assigned to k, but those labelled
with table head only exist in one segment. For in-
stance, if they only exist in sl then sr has no table
head, which means the restaurant of sr has customers
eating a dish, but no tables serving that dish. Thus,
we set td,sr,k=1 to make the constraints (2) satisfied.
The Gibbs probability for splitting a segment is
p(?d,u = 1 |?
??) ?
?1 + cd,1
?0 + ?1 + cd,0 + cd,1
(6)
BetaK
(
?+
Sd?
s=1
td,s
) ?
s?{sl,sr}
(b|a)Td,s
(b)Nd,s
?
k
Snd,s,ktd,s,k,a ,
where ??? = {z,w, ?,???d,u ,?, a, b, ?0, ?1}.
Merging: remove the boundary after u, and merge
sr and sl to one segment sm. For this case, both
sr and sl satisfy constraints (2) for all k?s, and set
nd,sm,k=nd,sr,k + nd,sl,k. The following cases are
considered: for a topic k
(I) Both sl and sr have nd,s,k>0 and td,s,k>1. We
compute td,sm,k using Eq (7). Thus table counts
before and after merging are equal.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0. Similar
to the above case, we use Eq (7).
(III) Both sl and sr have nd,s,k>0, and either of them
has td,s,k=1 or both. We have to choose between
Eq (7) and Eq (8), i.e., to decide whether a table
should be removed or not.
td,sm,k = td,sl,k + td,sr,k (7)
td,sm,k = td,sl,k + td,sr,k ? 1 (8)
Note that choosing Eq (8) means we need to de-
crease the table count td,sm,k by one. The idea here
is that we sample to decide whether the remove table
was added due to splitting case (III) or not. Clearly,
we have a one-to-many split-merge choice. To com-
pute the probability of a set of possible merges,
we use elementary symmetric polynomials as fol-
lows: letKS be a set of topic-segment combinations
that satisfy the condition in merging case (III), for
(k, s) ? KS , we sample either Eq (7) or Eq (8).
Let T = {td,s,k : (k, s) ? KS} be the set of table
counts affected by the changes of Eq (7) or Eq (8).
The Gibbs probability for merging two segments is
p(?d,u = 0 |?
???) =
?
T
p(?d,u = 0, T |?
???) (9)
?
?
T
(
?0 + cd,0
?0 + ?1 + cd,0 + cd,1
BetaK
(
?+
Sd?
s=1
td,s
)
(b|a)Td,sm
(b)Nd,sm
?
k
Snd,sm,ktd,sm,k,a
)
,
where ???? = {z,w, t ? T ,???d,u ,?, a, b, ?0, ?1}.
This is converted to a sum on |T | booleans with
independent terms and evaluated recursively in
O(|T |2) by symmetric polynomials. If a merge is
chosen, one then samples according to the terms in
the sum using a similar recursion.
5 Experiments
To demonstrate the effectiveness of our model (de-
noted by TSM) in topic segmentation tasks, we
195
evaluate it on three different kinds of corpora4: a
set of synthetic documents, two meeting transcripts
and two sets of text books (see Tables 2 and 3);
and compare TSM with the following methods: two
baselines (the Random algorithm that places topic
boundaries uniformly at random, and the Even al-
gorithm that places a boundary after every mth text
passage, where m is the average gold-standard seg-
ment length (Beeferman et al, 1999)), C99, MinCut,
Bayesseg, APS (Kazantseva and Szpakowicz, 2011),
and PLDA.
Metrics: We evaluated the segmentation perfor-
mance with PK (Beeferman et al, 1999) and Win-
dowDiff (WDr) (Pevzner and Hearst, 2002), which
are two common metrics used in topic segmenta-
tion. Both move a sliding window of fixed size k
over the document, and compare the inferred seg-
mentation with the gold-standard segmentation for
each window. The window size is usually set to
the half of the average gold-standard segment size
(Pevzner and Hearst, 2002). In addition, we also
used an extended WindowDiff proposed by Lam-
prier et al (2007), denoted by WDe. One problem
of WDr is that errors near the two ends of a text are
penalised less than those in the middle. To solve the
problem WDe adds k fictive text passages at the be-
ginning and the end of the text when computing the
score. We evaluated all the methods with the same
Java code for the three metrics.
Parameter Settings: In order to make all the
methods comparable, we chose for each method
the parameter settings that give the gold-standard
number of segments5. Specifically, we used a
11 ? 11 rank mask for C99, as suggested by
Choi (2000), the configurations included in the code
(http://groups.csail.mit.edu/rbg/code)
for Bayesseg and manually tuned parameters for
MinCut. For APS, a greedy approach was used to
search parameter settings that can approximately
give the gold-standard number of segments. For
PLDA, two randomly initialised Gibbs chains were
used. Each chain ran for 75,000 burn-in iterations,
then 1000 samples were drawn at a lag of 25 from
each chain. For TSM, 10 randomly initialised
4For preprocessing, we only removed stop words.
5The segments learnt by those methods will differ, but just
the segment count will be the same as the gold-standard count.
Table 2: The Choi?s dataset
Range of n 3-11 3-5 6-8 9-11
#docs 400 100 100 100
DocLen
mean 69.7 39.3 69.6 98.6
std 8.2 2.6 2.9 3.5
SegLen
mean 7 4 7 10
std 2.57 0.84 0.87 1.03
Table 3: Real dataset statistics
ICSI Election Fiction Clinical
# doc 25 4 84 227
DocLen
mean 994.5 144.3 325.0 139.5
std 354.5 16.4 230.1 110.4
SegLen
mean 188 7 22 35
std 219.1 8.9 23.8 41.7
Gibbs chains were used. Each chain ran for 30,000
iterations with 25,000 for burn-in, then 200 samples
were drawn. The concentration parameter b in TSM
was sampled using the Adaptive-Reject sampling
scheme introduced in (Du et al, 2012b), the dis-
count parameter a = 0.2, and ?0 = ?1 = 0.1. To
derive the final segmentation for PLDA and TSM,
we first estimated the marginal probabilities of
placing boundaries after text passages from the total
of 2000 samples. These probabilities were then
thresholded to give the gold-standard number of
segments. Precisely, we apply a small amount of
Gaussian smoothing to the marginal probabilities
(except for Choi?s dataset), like Puerver et al (2006)
does. Finally, we used a symmetric Dirichlet prior
in PLDA and STM, the one on topic distributions is
? = 0.1, the other on word distributions ? = 0.01.
5.1 Evaluation on Choi?s Dataset
Choi?s dataset (Choi, 2000) is commonly used in
evaluating topic segmentation methods. It consists
of 700 documents, each being a concatenation of 10
segments. Each segment is the first n sentences of
a randomly selected document from the Brown cor-
pus, s.t. 3 ? n ? 11. Those documents are divided
into 4 subsets with different range of n, as shown in
Table 2. We ran PLDA and STM with 50 topics. Re-
sults in Table 4 show that our model significantly
outperforms all the other methods on the four sub-
sets over all the metrics. Furthermore, comparing to
other published results, this also outperforms (Misra
et al, 2009) (see their table 2), and (Riedl and Bie-
mann, 2012) (they report an average of 1.04 and 1.06
in Tables 1 and 2, whereas TSM averages 0.93). This
gives TSM the best reported results to date.
196
Table 4: Comparison on Choi?s datasets with WD and PK (%)
3-11 3-5 6-8 9-11
WDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PK
Random 51.7 49.1 48.7 51.4 50.0 48.4 52.5 49.9 49.2 52.4 48.9 49.2
Even 49.1 46.7 49.0 46.3 45.8 46.3 38.8 37.3 38.8 30.0 28.6 30.0
MinCut 30.4 29.8 26.7 41.6 41.5 37.3 28.2 27.4 25.5 23.6 22.7 21.6
APS 40.7 38.8 38.4 32.0 30.6 31.8 34.4 32.6 32.7 34.5 32.2 33.2
C99 13.5 12.3 12.3 11.3 10.2 10.8 10.2 9.3 9.8 8.9 8.1 8.6
Bayesseg 11.6 10.9 10.9 11.8 11.5 11.1 7.7 7.2 7.3 6.1 5.7 5.7
PLDA 2.4 2.2 1.8 4.0 3.9 3.3 3.6 3.5 2.7 3.0 2.8 2.0
TSM 0.8 0.8 0.6 1.3 1.3 1.0 1.4 1.4 0.9 1.9 1.8 1.2
Table 5: Comparison on the meeting transcripts and written texts with WD and PK (%)
ICSI Election Fiction Clinical
WDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PK
Random 46.3 41.7 44.1 51.0 49.7 45.1 51.0 48.7 47.5 45.9 38.5 44.1
Even 48.3 43.0 46.4 56.0 55.1 51.2 48.1 45.9 46.3 49.2 42.0 48.8
C99 42.9 37.4 39.9 43.1 41.5 37.0 48.1 45.1 42.1 39.7 31.9 38.7
MinCut 40.6 36.9 36.9 43.6 43.3 39.0 40.5 39.7 37.1 38.2 36.2 36.8
APS 58.2 49.7 54.6 47.7 36.8 40.6 48.0 45.8 45.1 39.9 32.8 39.6
Bayesseg 32.4 29.7 26.7 41.1 41.3 34.1 33.7 32.8 27.8 35.0 28.8 34.0
PLDA 32.6 28.8 29.4 40.6 41.1 32.0 43.0 41.3 36.1 37.3 32.1 32.4
TSM 30.2 26.8 25.8 38.1 38.9 31.3 40.8 38.7 32.5 34.5 29.1 30.6
Note the lexical transitions in these concatenated
documents are very sharp (Malioutov and Barzi-
lay, 2006). The sharp transitions lead to significant
change in segment level topic distributions, which
further implies the variance of these distributions is
large. In TSM, a large variance causes a small con-
centration parameter b. We observed that the sam-
pled b?s (about 0.1) are indeed small for the four sub-
sets, which shows there is no topic sharing among
segments. Therefore, TSM is able to recognise the
segments are unrelated text.
5.2 Evaluation on Meeting Transcripts
We applied our model to segmenting the two meet-
ing transcripts, which are the ICSI meeting tran-
scripts (Janin et al, 2003) and the 2008 presidential
election debates (Boydstun et al, 2011). The ICSI
meeting has 75 transcripts, we used the 25 annotated
transcripts provided by Galley et al (2003) for eval-
uation. For the election debates, we used the four
annotated debates used in (Nguyen et al, 2012). The
statistics are shown in Table 3. PLDA and TSM were
trained with 10 topics on the ICSI and 50 on the
Election. In this set of experiments, we show that
our model is robust to meeting transcripts.
00.2
0.40.6p(l=
1) TSM
0 100 200 300 400 500 600 700 8000
0.20.4
0.6 PLDA
Utterance position in sequence
p(l =
 1)
Figure 2: Probability of a topic boundary, compared with
gold-standard segmentation (shown in red and at the top
of each diagram) on one ICSI transcript.
As shown in Table 5, topic modelling based meth-
ods (i.e., Bayesseg, PLDA and TSM) outperform
those using either TF or TF-IDF, which is consistent
with previously reported results (Misra et al, 2009;
Riedl and Biemann, 2012). Among the topic model
based methods, TSM achieves the best results on all
the three metrics. On the ICSI transcripts, TSM per-
forms 6.8%, 9.7% and 3.4% better than Bayesseg
on the WDr, WDe and PK metrics respectively. Fig-
ure 2 shows an example of how the inferred topic
boundary probabilities at utterances compare with
the gold-standard boundaries on one ICSI meeting
transcript. The gold-standard segmentation is {77,
95, 189, 365, 508, 609, 860}, TSM and PLDA in-
fer {85, 96, 188, 363, 499, 508, 860} and {96, 136,
197
Table 6: Sampled concentration parameters
Choi ICSI Election Fiction Clinical
b 0.1 5.2 5.4 18.4 4.8
203, 226, 361, 508, 860} respectively. Both models
miss the boundary after the 609th utterance, but put a
boundary after the 508th utterance. Note the bound-
aries placed by TSM are always within 10 utterances
with respect to the gold standard.
Although TSM still performs the best on the de-
bates, all the methods have relatively worse perfor-
mance than on the ICSI meeting transcripts. Nguyen
et al (2012) pointed out that the ICSI meetings are
characterised by pragmatic topic changes, in con-
trast, the debates are characterised by strategic topic
changes with strong rewards for setting the agenda,
dodging a question, etc. Thus, considering the prop-
erties of debates might further improve the segmen-
tation performance.
5.3 Evaluation on Written Texts
We further tested TSM on two written text datasets,
Clinical (Eisenstein and Barzilay, 2008) and Fiction
(Kazantseva and Szpakowicz, 2011). The statistics
are shown in Table 3. Each document in the Clinical
dataset is a chapter of a medical textbook. Section
breaks are selected to be the true topic boundaries.
For the Fiction dataset, each document is a fiction
downloaded from Project Gutenberg, the true topic
boundaries are chapter breaks. We trained PLDA
and TSM with 25 topics on the Fiction and 50 on the
Clinical. Results are shown in Table 5. TSM com-
pares favourably with Bayesseg and outperforms the
other methods on the Clinical dataset, but it does not
perform as well as Bayesseg on the Fiction dataset.
In fiction books, the topic boundaries between
sections are usually blurred by the authors for rea-
sons of continuity (Reynar, 1999). We observed that
the sampled concentration (or inverse variance) pa-
rameter b in TSM is about 18.4 on Fiction, but 4.8 on
Clinical, as shown in Table 6. This means the vari-
ance of segment level topic distributions ? learnt by
TSM is not large for the fiction, so chapter breaks
may not necessarily indicate topic changes. For ex-
ample, there is a document in the Fiction dataset
where gold-standard topic boundaries are placed af-
ter each block of text. In contrast, Bayesseg assumes
each segment has its own distribution over words,
i.e., one topic per segment, which means topics are
not shared among segments. We hypothesize that
for certain kinds of documents where the change in
topic distribution is subtle, such as fiction, assuming
one topic per segment can capture subtle changes in
word usage. This is an area for future investigation.
6 Conclusion
In this paper, we have presented a hierarchical
Bayesian model for unsupervised topic segmen-
tation. This new model takes advances of both
Bayesian segmentation and structured topic mod-
elling. It uses a point-wise boundary sampling al-
gorithm to sample a topic segmentation, while con-
currently building a structured topic model. We
have developed a novel approximation to com-
pute the Gibbs probabilities of spliting/merging seg-
ment(s). Our model shows prominent segmentation
performance on both written or spoken texts.
In future work, we would like to make the model
fully nonparametric and investigate the effects of
adding different cues in texts, such as cue phrases,
pronoun usage, prosody, etc. Currently, our model
uses marginal boundary probabilities to generate
the final segmentation. Instead, we could develop a
Metropolis-Hasting sampling algorithm to move one
boundary at a time, given the gold-standard number
of segments. To further study the effectiveness of
our model, we would like to compare it with other
methods, like SITS (Nguyen et al, 2012) and to run
on more datasets, like email (Joty et al, 2010). For
example, in order to compare with SITS, one can
make an assumption that each document just has one
speaker.
Acknowledgments
The authors would like to thank all the anony-
mous reviewers for their valuable comments.
This research was supported under Australian
Research Council?s Discovery Projects funding
scheme (project numbers DP110102506 and
DP110102593). NICTA is funded by the Australian
Government as represented by the Department
of Broadband, Communications and the Digital
Economy and the Australian Research Council
through the ICT Centre of Excellence program.
198
References
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194?218.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Mach.
Learn., 34(1-3):177?210.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
A.E. Boydstun, C. Phillips, and R.A. Glazier. 2011. Its
the economy again, stupid: Agenda control in the 2008
presidential debates.
W. Buntine and M. Hutter. 2012. A Bayesian review
of the Poisson-Dirichlet process. Technical Report
arXiv:1007.0296v2, ArXiv, Cornell.
Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling for the Poisson-Dirichlet process. In Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Database,
pages 296?311.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, NAACL 2000,
pages 26?33.
Lan Du, Wray Buntine, and Huidong Jin. 2010. A
segmented topic model based on the two-parameter
Poisson-Dirichlet process. Mach. Learn., 81(1):5?19.
Lan Du, Wray Buntine, and Huidong Jin. 2012a. Mod-
elling sequential text with an adaptive topic model.
In Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
535?545.
Lan Du, Wray Buntine, Huidong Jin, and Changyou
Chen. 2012b. Sequential latent Dirichlet alocation.
Knowledge and Information Systems, 31(3):475?503.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?08, pages 334?343.
Jacob Eisenstein. 2009. Hierarchical text segmentation
from multi-scale lexical cohesion. In Human Lan-
guage Technologies: Conference of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics, pages 353?361. The Association for Compu-
tational Linguistics.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 562?569.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?53.
Marti A. Hearst. 1997. TextTiling: segmenting text
into multi-paragraph subtopic passages. Comput. Lin-
guist., 23(1):33?64.
Leetsch C. Hsu and Peter Jau-Shyong Shiue. 1998. A
unified approach to generalized Stirling numbers. Adv.
Appl. Math., 20:366?384, April.
Sonia Jain and Radford Neal. 2004. A split-merge
Markov chain Monte Carlo procedure for the Dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13(1):158?182.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proceedings of 2003 IEEE International Conference
on Acoustics, Speech, and Signal (ICASSP ?03), pages
364?367.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 388?
398.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear
text segmentation using affinity propagation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 284?293.
Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and
Frederic Saubion. 2007. On evaluation methodologies
for text segmentation algorithms. In Proceedings of
the 19th IEEE International Conference on Tools with
Artificial Intelligence - Volume 02, ICTAI ?07, pages
19?26.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44, pages 25?32.
Hemant Misra, Olivier Cappe, and Francois Yvon. 2008.
Using LDA to detect semantically incoherent docu-
ments. In Proceedings of CoNLL-08, pages 41?48.
Hemant Misra, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proceedings of the
18th ACM conference on Information and knowledge
management, CIKM ?09, pages 1553?1556.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2012. SITS: A hierarchical nonparametric
199
model using speaker identity for topic segmentation in
multiparty conversations. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 78?87.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19?36.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Diriclet distribution derived from a stable subordina-
tor. Annals Probability, 25:855?900.
Matthew Purver, Thomas L. Griffiths, Konrad P. Ko?rding,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44, pages 17?24.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics,
pages 357?364.
Martin Riedl and Chris Biemann. 2012. How text seg-
mentation algorithms gain from topic models. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Gerard Salton, Amit Singhal, Chris Buckley, and Mandar
Mitra. 1996. Automatic text decomposition using text
segments and text themes. In Proceedings of the the
seventh ACM conference on Hypertext, pages 53?65.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher ker-
nel. In Proceedings of ACL-08: HLT, Short Papers,
pages 269?272.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Y. W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of 39th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 499?506.
H.M. Wallach. 2008. Structured topic models for lan-
guage. doctoral dissertation, Univ. of Cambridge.
Hongning Wang, Duo Zhang, and ChengXiang Zhai.
2011. Structural topic model for latent topical struc-
ture analysis. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 1526?1535.
200
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148?1157,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
PCFGs, Topic Models, Adaptor Grammars and Learning Topical
Collocations and the Structure of Proper Names
Mark Johnson
Department of Computing
Macquarie University
mjohnson@science.mq.edu.au
Abstract
This paper establishes a connection be-
tween two apparently very different kinds
of probabilistic models. Latent Dirich-
let Allocation (LDA) models are used
as ?topic models? to produce a low-
dimensional representation of documents,
while Probabilistic Context-Free Gram-
mars (PCFGs) define distributions over
trees. The paper begins by showing that
LDA topic models can be viewed as a
special kind of PCFG, so Bayesian in-
ference for PCFGs can be used to infer
Topic Models as well. Adaptor Grammars
(AGs) are a hierarchical, non-parameteric
Bayesian extension of PCFGs. Exploit-
ing the close relationship between LDA
and PCFGs just described, we propose
two novel probabilistic models that com-
bine insights from LDA and AG models.
The first replaces the unigram component
of LDA topic models with multi-word se-
quences or collocations generated by an
AG. The second extension builds on the
first one to learn aspects of the internal
structure of proper names.
1 Introduction
Over the last few years there has been consider-
able interest in Bayesian inference for complex hi-
erarchical models both in machine learning and in
computational linguistics. This paper establishes
a theoretical connection between two very differ-
ent kinds of probabilistic models: Probabilistic
Context-Free Grammars (PCFGs) and a class of
models known as Latent Dirichlet Allocation (Blei
et al, 2003; Griffiths and Steyvers, 2004) models
that have been used for a variety of tasks in ma-
chine learning. Specifically, we show that an LDA
model can be expressed as a certain kind of PCFG,
so Bayesian inference for PCFGs can be used to
learn LDA topic models as well. The importance
of this observation is primarily theoretical, as cur-
rent Bayesian inference algorithms for PCFGs are
less efficient than those for LDA inference. How-
ever, once this link is established it suggests a vari-
ety of extensions to the LDA topic models, two of
which we explore in this paper. The first involves
extending the LDA topic model so that it generates
collocations (sequences of words) rather than indi-
vidual words. The second applies this idea to the
problem of automatically learning internal struc-
ture of proper names (NPs), which is useful for
definite NP coreference models and other applica-
tions.
The rest of this paper is structured as follows.
The next section reviews Latent Dirichlet Alloca-
tion (LDA) topic models, and the following sec-
tion reviews Probabilistic Context-Free Grammars
(PCFGs). Section 4 shows how an LDA topic
model can be expressed as a PCFG, which pro-
vides the fundamental connection between LDA
and PCFGs that we exploit in the rest of the
paper, and shows how it can be used to define
a ?sticky topic? version of LDA. The follow-
ing section reviews Adaptor Grammars (AGs), a
non-parametric extension of PCFGs introduced by
Johnson et al (2007b). Section 6 exploits the con-
nection between LDA and PCFGs to propose an
AG-based topic model that extends LDA by defin-
ing distributions over collocations rather than indi-
vidual words, and section 7 applies this extension
to the problem of finding the structure of proper
names.
2 Latent Dirichlet Allocation Models
Latent Dirichlet Allocation (LDA) was introduced
as an explicit probabilistic counterpart to La-
tent Semantic Indexing (LSI) (Blei et al, 2003).
Like LSI, LDA is intended to produce a low-
dimensional characterisation or summary of a doc-
1148
WZ??
??
n m
`
Figure 1: A graphical model ?plate? representa-
tion of an LDA topic model. Here ` is the number
of topics, m is the number of documents and n is
the number of words per document.
ument in a collection of documents for informa-
tion retrieval purposes. Both LSI and LDA do
this by mapping documents to points in a rela-
tively low-dimensional real-valued vector space;
distance in this space is intended to correspond to
document similarity.
An LDA model is an explicit generative proba-
bilistic model of a collection of documents. We
describe the ?smoothed? LDA model here (see
page 1006 of Blei et al (2003)) as it corresponds
precisely to the Bayesian PCFGs described in sec-
tion 4. It generates a collection of documents by
first generating multinomials ?i over the vocab-
ulary V for each topic i ? 1, . . . , `, where ` is
the number of topics and ?i,w is the probability
of generating word w in topic i. Then it gen-
erates each document Dj , j = 1, . . . ,m in turn
by first generating a multinomial ?j over topics,
where ?j,i is the probability of topic i appearing
in document j. (?j serves as the low-dimensional
representation of document Dj). Finally it gener-
ates each of the n words of document Dj by first
selecting a topic z for the word according to ?j ,
and then drawing a word from ?z . Dirichlet priors
with parameters ? and ? respectively are placed
on the ?i and the ?j in order to avoid the zeros
that can arise from maximum likelihood estima-
tion (i.e., sparse data problems).
The LDA generative model can be compactly
expressed as follows, where ??? should be read
as ?is distributed according to?.
?i ? Dir(?) i = 1, . . . , `
?j ? Dir(?) j = 1, . . . ,m
zj,k ? ?j j = 1, . . . ,m; k = 1, . . . , n
wj,k ? ?zj,k j = 1, . . . ,m; k = 1, . . . , n
In inference, the parameters ? and ? of the
Dirichlet priors are either fixed (i.e., chosen by
the model designer), or else themselves inferred,
e.g., by Bayesian inference. (The adaptor gram-
mar software we used in the experiments de-
scribed below automatically does this kind of
hyper-parameter inference).
The inference task is to find the topic probabil-
ity vector ?j of each documentDj given the words
wj,k of the documents; in general this also requires
inferring the topic to word distributions ? and the
topic assigned to each word zj,k. Blei et al (2003)
describe a Variational Bayes inference algorithm
for LDA models based on a mean-field approx-
imation, while Griffiths and Steyvers (2004) de-
scribe an Markov Chain Monte Carlo inference al-
gorithm based on Gibbs sampling; both are quite
effective in practice.
3 Probabilistic Context-Free Grammars
Context-Free Grammars are a simple model of hi-
erarchical structure often used to describe natu-
ral language syntax. A Context-Free Grammar
(CFG) is a quadruple (N,W,R, S) where N and
W are disjoint finite sets of nonterminal and ter-
minal symbols respectively,R is a finite set of pro-
ductions or rules of the formA? ? whereA ? N
and ? ? (N?W )?, and S ? N is the start symbol.
In what follows, it will be useful to interpret a
CFG as generating sets of finite, labelled, ordered
trees TA for each X ? N ? W . Informally, TX
consists of all trees t rooted in X where for each
local tree (B, ?) in t (i.e., where B is a parent?s
label and ? is the sequence of labels of its imme-
diate children) there is a rule B ? ? ? R.
Formally, the sets TX are the smallest sets of
trees that satisfy the following equations.
If X ? W (i.e., if X is a terminal) then TX =
{X}, i.e., TX consists of a single tree, which in
turn only consists of a single node labelled X .
If X ? N (i.e., if X is a nonterminal) then
TX =
?
X?B1...Bn?RX
TREEX(TB1 , . . . , TBn)
where RA = {A ? ? : A ? ? ? R} for each
A ? N , and
TREEX(TB1 , . . . , TBn)
=
{
 PP
X
t1 tn. . .
:
ti ? TBi ,
i = 1, . . . , n
}
That is, TREEX(TB1 , . . . , TBn) consists of the set
of trees with whose root node is labelled X and
whose ith child is a member of TBi .
1149
The set of trees generated by the CFG is TS ,
where S is the start symbol, and the set of strings
generated by the CFG is the set of yields (i.e., ter-
minal strings) of the trees in TS .
A Probabilistic Context-Free Grammar (PCFG)
is a pair consisting of a CFG and set of multino-
mial probability vectors ?X indexed by nontermi-
nals X ? N , where ?X is a distribution over the
rulesRX (i.e., the rules expandingX). Informally,
?X?? is the probability ofX expanding to ? using
the rule X ? ? ? RX . More formally, a PCFG
associates each X ? N ? W with a distribution
GX over the trees TX as follows.
If X ? W (i.e., if X is a terminal) then GX
is the distribution that puts probability 1 on the
single-node tree labelled X .
If X ? N (i.e., if X is a nonterminal) then:
GX =
?
X?B1...Bn?RX
?X?B1...BnTDX(GB1 , . . . , GBn) (1)
where:
TDA(G1, . . . , Gn)
(
 PP
X
t1 tn. . .
)
=
n?
i=1
Gi(ti).
That is, TDA(G1, . . . , Gn) is a distribution over
TA where each subtree ti is generated indepen-
dently from Gi. These equations have solutions
(i.e., the PCFG is said to be ?consistent?) when
the rule probabilities ?A obey certain conditions;
see e.g., Wetherell (1980) for details.
The PCFG generates the distribution over trees
GS , where S is the start symbol. The distribu-
tion over the strings it generates is obtained by
marginalising over the trees.
In a Bayesian PCFG one puts Dirichlet priors
Dir(?X) on each of the multinomial rule proba-
bility vectors ?X for each nonterminal X ? N .
This means that there is one Dirichlet parameter
?X?? for each rule X ? ? ? R in the CFG.
In the ?unsupervised? inference problem for a
PCFG one is given a CFG, parameters ?X for the
Dirichlet priors over the rule probabilities, and a
corpus of strings. The task is to infer the cor-
responding posterior distribution over rule prob-
abilities ?X . Recently Bayesian inference algo-
rithms for PCFGs have been described. Kurihara
and Sato (2006) describe a Variational Bayes algo-
rithm for inferring PCFGs using a mean-field ap-
proximation, while Johnson et al (2007a) describe
a Markov Chain Monte Carlo algorithm based on
Gibbs sampling.
4 LDA topic models as PCFGs
This section explains how to construct a PCFG
that generates the same distribution over a collec-
tion of documents as an LDA model, and where
Bayesian inference for the PCFG?s rule proba-
bilities yields the corresponding distributions as
Bayesian inference of the corresponding LDA
models. (There are several different ways of en-
coding LDA models as PCFGs; the one presented
here is not the most succinct ? it is possible to
collapse the Doc and Doc? nonterminals ? but it
has the advantage that the LDA distributions map
straight-forwardly onto PCFG nonterminals).
The terminals W of the CFG consist of the vo-
cabulary V of the LDA model plus a set of special
?document identifier? terminals ? j? for each doc-
ument j ? 1, . . . ,m, where m is the number of
documents. In the PCFG encoding strings from
document j are prefixed with ? j?; this indicates
to the grammar which document the string comes
from. The nonterminals consist of the start symbol
Sentence, Docj and Doc?j for each j ? 1, . . . ,m,
and Topici for each i ? 1, . . . , `, where ` is the
number of topics in the LDA model.
The rules of the CFG are all instances of the
following schemata:
Sentence? Doc?j j ? 1, . . . ,m
Doc?j ? j j ? 1, . . . ,m
Doc?j ? Doc
?
j Docj j ? 1, . . . ,m
Docj ? Topici i ? 1, . . . , `; j ? 1, . . . ,m
Topici ? w i ? 1, . . . , `;w ? V
Figure 2 depicts a tree generated by such a
CFG. The relationship between the LDA model
and the PCFG can be understood by studying the
trees generated by the CFG. In these trees the left-
branching spine of nodes labelled Doc?j propagate
the document identifier throughout the whole tree.
The nodes labelled Topici indicate the topics as-
signed to particular words, and the local trees ex-
panding Docj to Topici (one per word in the docu-
ment) indicate the distribution of topics in the doc-
ument.
The corresponding Bayesian PCFG associates
probabilities with each of the rules in the CFG.
The probabilities ?Topici associated with the rules
expanding the Topici nonterminals indicate how
words are distributed across topics; the ?Topici
probabilities correspond exactly to to the ?i prob-
abilities in the LDA model. The probabilities
1150
Sentence
Doc3'
Doc3'
Doc3'
Doc3'
Doc3'
_3
Doc3
Topic4
shallow
Doc3
Topic4
circuits
Doc3
Topic4
compute
Doc3
Topic7
faster
Figure 2: A tree generated by the CFG encoding
an LDA topic model. The prefix ? 3? indicates
that this string belongs to document 3. The tree
also indicates the assignment of words to topics.
?Docj associated with rules expanding Docj spec-
ify the distribution of topics in document j; they
correspond exactly to the probabilities ?j of the
LDA model. (The PCFG also specifies several
other distributions that are suppressed in the LDA
model. For example ?Sentence specifies the distri-
bution of documents in the corpus. However, it is
easy to see that these distributions do not influence
the topic distributions; indeed, the expansions of
the Sentence nonterminal are completely deter-
mined by the document distribution in the corpus,
and are not affected by ?Sentence).
A Bayesian PCFG places Dirichlet priors
Dir(?A) on the corresponding rule probabilities
?A for each A ? N . In the PCFG encoding an
LDA model, the ?Topici parameters correspond
exactly to the ? parameters of the LDA model, and
the ?Docj parameters correspond to the ? param-
eters of the LDA model.
As suggested above, each document Dj in the
LDA model is mapped to a string in the corpus
used to train the corresponding PCFG by prefix-
ing it with a document identifier ? j?. Given this
training data, the posterior distribution over rule
probabilities ?Docj? Topici is the same as the pos-
terior distribution over topics given documents ?j,i
in the original LDA model.
As we will see below, this connection between
PCFGs and LDA topic models suggests a num-
ber of interesting variants of both PCFGs and
topic models. Note that we are not suggesting
that Bayesian inference for PCFGs is necessar-
ily a good way of estimating LDA topic models.
Current Bayesian PCFG inference algorithms re-
quire time proportional to the cube of the length of
the longest string in the training corpus, and since
these strings correspond to entire documents in our
embedding, blindly applying a Bayesian PCFG in-
ference algorithm is likely to be impractical.
A little reflection shows that the embedding still
holds if the strings in the PCFG corpus correspond
to sentences or even smaller units of the original
document collection, so a single document would
be mapped to multiple strings in the PCFG infer-
ence task. In this way the cubic time complex-
ity of PCFG inference can be mitigated. Also, the
trees generated by these CFGs have a very spe-
cialized left-branching structure, and it is straight-
forward to modify the general-purpose CFG infer-
ence procedures to avoid the cubic time complex-
ity for such grammars: thus it may be practical to
estimate topic models via grammatical inference.
However, we believe that the primary value of
the embedding of LDA topic models into Bayesian
PCFGs is theoretical: it suggests a number of
novel extensions of both topic models and gram-
mars that may be worth exploring. Our claim here
is not that these models are the best algorithms for
performing these tasks, but that the relationship
we described between LDA models and PCFGs
suggests a variety of interesting novel models.
We end this section with a simple example of
such a modification to LDA. Inspired by the stan-
dard embedding of HMMs into PCFGs, we pro-
pose a ?sticky topic? variant of LDA in which ad-
jacent words are more likely to be assigned the
same topic. Such an LDA extension is easy to
describe as a PCFG (see Fox et al (2008) for a
similar model presented as an extended HMM).
The nonterminals Sentence and Topici for i =
1, . . . , ` have the same interpretation as before, but
we introduce new nonterminals Docj,i that indi-
cate we have just generated a nonterminal in doc-
ument j belonging to topic i. Given a collection of
m documents and ` topics, the rule schemata are
as follows:
Sentence? Docj,i i ? 1, . . . , `;
j ? 1, . . . ,m
Docj,1 ? j j ? 1, . . . ,m
Docj,i ? Docj,i? Topici i, i
? ? 1, . . . , `;
j ? 1, . . . ,m
Topici ? w i ? 1, . . . , `;w ? V
A sample parse generated by a ?sticky topic?
1151
Sentence
Doc3'_
Doc3'T
Doc3'T
Doc3'T
Doc3'p
i3
4oshcT
alwrrou
4oshcT
chmc7hta
4oshcT
cofs7te
4oshc_
watem
Figure 3: A tree generated by the ?sticky topic?
CFG. Here a nonterminal Doc3, 7 indicates we
have just generated a word in document 3 belong-
ing to topic 7.
CFG is shown in Figure 3. The probabilities of
the rules Docj,i ? Docj,i? Topici in this PCFG
encode the probability of shifting from topic i to
topic i? (this PCFG can be viewed as generating
the string from right to left).
We can use non-uniform sparse Dirichlet pri-
ors on the probabilities of these rules to encour-
age ?topic stickiness?. Specifically, by setting
the Dirichlet parameters for the ?topic shift? rules
Docj,i? ? Docj,i Topici where i
? 6= i much lower
than the parameters for the ?topic preservation?
rules Docj,i ? Docj,i Topici, Bayesian inference
will be biased to find distributions in which adja-
cent words will tend to have the same topic.
5 Adaptor Grammars
Non-parametric Bayesian inference, where the in-
ference task involves learning not just the values
of a finite vector of parameters but which parame-
ters are relevant, has been the focus of intense re-
search in machine learning recently. In the topic-
modelling community this has lead to work on
Dirichlet Processes and Chinese Restaurant Pro-
cesses, which can be used to estimate the number
of topics as well as their distribution across docu-
ments (Teh et al, 2006).
There are two obvious non-parametric exten-
sions to PCFGs. In the first we regard the set
of nonterminals N as potentially unbounded, and
try to learn the set of nonterminals required to de-
scribe the training corpus. This approach goes un-
der the name of the ?infinite HMM? or ?infinite
PCFG? (Beal et al, 2002; Liang et al, 2007; Liang
et al, 2009). Informally, we are given a set of ?ba-
sic categories?, say NP,VP, etc., and a set of rules
that use these basic categories, say S ? NP VP.
The inference task is to learn a set of refined cate-
gories and rules (e.g., S7 ? NP2 VP5) as well as
their probabilities; this approach can therefore be
viewed as a Bayesian version of the ?split-merge?
approach to grammar induction (Petrov and Klein,
2007).
In the second approach, which we adopt here,
we regard the set of rules R as potentially un-
bounded, and try to learn the rules required to
describe a training corpus as well as their prob-
abilities. Adaptor grammars are an example of
this approach (Johnson et al, 2007b), where en-
tire subtrees generated by a ?base grammar? can
be viewed as distinct rules (in that we learn a sep-
arate probability for each subtree). The inference
task is non-parametric if there are an unbounded
number of such subtrees.
We review the adaptor grammar generative pro-
cess below; for an informal introduction see John-
son (2008) and for details of the adaptor grammar
inference procedure see Johnson and Goldwater
(2009).
An adaptor grammar (N,W,R, S, ?, A,C) con-
sists of a PCFG (N,W,R, S, ?) in which a sub-
set A ? N of the nonterminals are adapted, and
where each adapted nonterminal X ? A has an
associated adaptor CX . An adaptor CX for X is a
function that maps a distribution over trees TX to
a distribution over distributions over TX (we give
examples of adaptors below).
Just as for a PCFG, an adaptor grammar de-
fines distributions GX over trees TX for each X ?
N ?W . If X ? W or X 6? A then GX is defined
just as for a PCFG above, i.e., using (1). How-
ever, if X ? A then GX is defined in terms of an
additional distribution HX as follows:
GX ? CX(HX)
HX =
?
X?Y1...Ym?RX
?X?Y1...YmTDX(GY1 , . . . , GYm)
That is, the distribution GX associated with an
adapted nonterminal X ? A is a sample from
adapting (i.e., applying CX to) its ?ordinary?
PCFG distribution HX . In general adaptors are
chosen for the specific properties they have. For
example, with the adaptors used hereGX typically
concentrates mass on a smaller subset of the trees
TX than HX does.
Just as with the PCFG, an adaptor grammar gen-
erates the distribution over treesGS , where S ? N
1152
is the start symbol. However, whileGS in a PCFG
is a fixed distribution (given the rule probabili-
ties ?), in an adaptor grammar the distribution GS
is itself a random variable (because each GX for
X ? A is random), i.e., an adaptor grammar gen-
erates a distribution over distributions over trees
TS . However, the posterior joint distribution Pr(t)
of a sequence t = (t1, . . . , tn) of trees in TS is
well-defined:
Pr(t) =
?
GS(t1) . . . GS(tn) dG
where the integral is over all of the random distri-
butions GX , X ? A. The adaptors we use in this
paper are Dirichlet Processes or two-parameter
Poisson-Dirichlet Processes, for which it is pos-
sible to compute this integral. One way to do this
uses the predictive distributions:
Pr(tn+1 | t, HX)
?
?
GX(t1) . . . GX(tn+1)CX(GX | HX) dGX
where t = (t1, . . . , tn) and each ti ? TX . The pre-
dictive distribution for the Dirichlet Process is the
(labeled) Chinese Restaurant Process (CRP), and
the predictive distribution for the two-parameter
Poisson-Dirichlet process is the (labeled) Pitman-
Yor Process (PYP).
In the context of adaptor grammars, the CRP is:
CRP(t | t, ?X , HX) ? nt(t) + ?XHX(t)
where nt(t) is the number of times t appears in t
and ?X > 0 is a user-settable ?concentration pa-
rameter?. In order to generate the next tree tn+1
a CRP either reuses a tree t with probability pro-
portional to number of times t has been previously
generated, or else it ?backs off? to the ?base distri-
bution?HX and generates a fresh tree t with prob-
ability proportional to ?XHX(t).
The PYP is a generalization of the CRP:
PYP(t | t, aX , bX , HX)
? max(0, nt(t)?mt aX) + (maX + bX)HX(t)
Here aX ? [0, 1] and bX > 0 are user-settable
parameters, andmt is the number of times the PYP
has generated t in t from the base distributionHX ,
and m =
?
t?TX
mt is the number of times any
tree has been generated from HX . (In the Chinese
Restaurant metaphor, mt is the number of tables
labeled with t, and m is the number of occupied
tables). If aX = 0 then the PYP is equivalent to
a CRP with ?X = bX , while if aX = 1 then the
PYP generates samples from HX .
Informally, the CRP has a strong preference
to regenerate trees that have been generated fre-
quently before, leading to a ?rich-get-richer? dy-
namics. The PYP can mitigate this somewhat by
reducing the effective count of previously gener-
ated trees and redistributing that probability mass
to new trees generated from HX . As Goldwa-
ter et al (2006) explain, Bayesian inference for
HX given samples from GX is effectively per-
formed from types if aX = 0 and from tokens
if aX = 1, so varying aX smoothly interpolates
between type-based and token-based inference.
Adaptor grammars have previously been used
primarily to study grammatical inference in the
context of language acquisition. The word seg-
mentation task involves segmenting a corpus
of unsegmented phonemic utterance representa-
tions into words (Elman, 1990; Bernstein-Ratner,
1987). For example, the phoneme string corre-
sponding to ?you want to see the book? (with its
correct segmentation indicated) is as follows:
y Mu Nw Ma Mn Mt Nt Mu Ns Mi ND M6 Nb MU Mk
We can represent any possible segmentation of any
possible sentence as a tree generated by the fol-
lowing unigram adaptor grammar.
Sentence?Word
Sentence?Word Sentence
Word? Phonemes
Phonemes? Phoneme
Phonemes? Phoneme Phonemes
The trees generated by this adaptor grammar are
the same as the trees generated by the CFG rules.
For example, the following skeletal parse in which
all but the Word nonterminals are suppressed (the
others are deterministically inferrable) shows the
parse that corresponds to the correct segmentation
of the string above.
(Word y u) (Word w a n t) (Word t u)
(Word s i) (Word d 6) (Word b u k)
Because the Word nonterminal is adapted (indi-
cated here by underlining) the adaptor grammar
learns the probability of the entire Word subtrees
(e.g., the probability that b u k is a Word); see
Johnson (2008) for further details.
1153
6 Topic models with collocations
Here we combine ideas from the unigram word
segmentation adaptor grammar above and the
PCFG encoding of LDA topic models to present
a novel topic model that learns topical colloca-
tions. (For a non-grammar-based approach to this
problem see Wang et al (2007)). Specifically, we
take the PCFG encoding of the LDA topic model
described above, but modify it so that the Topici
nodes generate sequences of words rather than sin-
gle words. Then we adapt each of the Topici non-
terminals, which means that we learn the probabil-
ity of each of the sequences of words it can expand
to.
Sentence? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , `;
j ? 1, . . . ,m
Topic
i
?Words i ? 1, . . . , `
Words?Word
Words?Words Word
Word? w w ? V
In order to demonstrate that this model
works, we implemented this using the publically-
available adaptor grammar inference software,1
and ran it on the NIPS corpus (composed of pub-
lished NIPS abstracts), which has previously been
used for studying collocation-based topic models
(Griffiths et al, 2007). Because there is no gen-
erally accepted evaluation for collocation-finding,
we merely present some of the sample analyses
found by our adaptor grammar. We ran our adap-
tor grammar with ` = 20 topics (i.e., 20 distinct
Topici nonterminals). Adaptor grammar inference
on this corpus is actually relatively efficient be-
cause the corpus provided by Griffiths et al (2007)
is already segmented by punctuation, so the termi-
nal strings are generally rather short. Rather than
set the Dirichlet parameters by hand, we placed
vague priors on them and estimated them as de-
scribed in Johnson and Goldwater (2009).
The following are some examples of colloca-
tions found by our adaptor grammar:
Topic0? cost function
Topic0? fixed point
Topic0? gradient descent
Topic0? learning rates
1http://web.science.mq.edu.au/ ?mjohnson/Software.htm
Topic1? associative memory
Topic1? hamming distance
Topic1? randomly chosen
Topic1? standard deviation
Topic3? action potentials
Topic3? membrane potential
Topic3? primary visual cortex
Topic3? visual system
Topic10? nervous system
Topic10? action potential
Topic10? ocular dominance
Topic10? visual field
The following are skeletal sample parses, where
we have elided all but the adapted nonterminals
(i.e., all we show are the Topic nonterminals, since
the other structure can be inferred deterministi-
cally). Note that because Griffiths et al (2007)
segmented the NIPS abstracts at punctuation sym-
bols, the training corpus contains more than one
string from each abstract.
3 (Topic5 polynomial size)
(Topic15 threshold circuits)
4 (Topic11 studied)
(Topic19 pattern recognition algorithms)
4 (Topic2 feedforward neural network)
(Topic1 implementation)
5 (Topic11 single)
(Topic10 ocular dominance stripe)
(Topic12 low) (Topic3 ocularity)
(Topic12 drift rate)
7 Finding the structure of proper names
Grammars offer structural and positional sensitiv-
ity that is not exploited in the basic LDA topic
models. Here we explore the potential for us-
ing Bayesian inference for learning linear order-
ing constraints that hold between elements within
proper names.
The Penn WSJ treebank is a widely used re-
source within computational linguistics (Marcus
et al, 1993), but one of its weaknesses is that
it does not indicate any structure internal to base
noun phrases (i.e., it presents ?flat? analyses of the
pre-head NP elements). For many applications it
would be extremely useful to have a more elab-
orated analysis of this kind of NP structure. For
example, in an NP coreference application, if we
could determine that Bill and Hillary are both first
1154
names then we could infer that Bill Clinton and
Hillary Clinton are likely to refer to distinct in-
dividuals. On the other hand, because Mr in Mr
Clinton is not a first name, it is possible that Mr
Clinton and Bill Clinton refer to the same individ-
ual (Elsner et al, 2009).
Here we present an adaptor grammar based on
the insights of the PCFG encoding of LDA topic
models that learns some of the structure of proper
names. The key idea is that elements in proper
names typically appear in a fixed order; we expect
honorifics to appear before first names, which ap-
pear before middle names, which in turn appear
before surnames, etc. Similarly, many company
names end in fixed phrases such as Inc. Here
we think of first names as a kind of topic, albeit
one with a restricted positional location. One of
the challenges is that some of these structural ele-
ments can be filled by multiword expressions; e.g.,
de Groot can be a surname. We deal with this by
permitting multi-word collocations to fill the cor-
responding positions, and use the adaptor gram-
mar machinery to learn these collocations.
Inspired by the grammar presented in Elsner
et al (2009), our adaptor grammar is as follows,
where adapted nonterminals are indicated by un-
derlining as before.
NP? (A0) (A1) . . . (A6)
NP? (B0) (B1) . . . (B6)
NP? Unordered+
A0?Word+
. . .
A6?Word+
B0?Word+
. . .
B6?Word+
Unordered?Word+
In this grammar parentheses indicate optional-
ity, and the Kleene plus indicates iteration (these
were manually expanded into ordinary CFG rules
in our experiments). The grammar provides three
different expansions for proper names. The first
expansion says that a proper name can consist of
some subset of the six different collocation classes
A0 through A6 in that order, while the second ex-
pansion says that a proper name can consist of
some subset of the collocation classes B0 through
B6, again in that order. Finally, the third expan-
sion says that a proper name can consist of an ar-
bitrary sequence of ?unordered? collocations (this
is intended as a ?catch-all? expansion to provide
analyses for proper names that don?t fit either of
the first two expansions).
We extracted all of the proper names (i.e.,
phrases of category NNP and NNPS) in the Penn
WSJ treebank and used them as the training cor-
pora for the adaptor grammar just described. The
adaptor grammar inference procedure found skele-
tal sample parses such as the following:
(A0 barrett) (A3 smith)
(A0 albert) (A2 j.) (A3 smith) (A4 jr.)
(A0 robert) (A2 b.) (A3 van dover)
(B0 aim) (B1 prime rate) (B2 plus) (B5
fund) (B6 inc.)
(B0 balfour) (B1 maclaine) (B5 interna-
tional) (B6 ltd.)
(B0 american express) (B1 information
services) (B6 co)
(U abc) (U sports)
(U sports illustrated)
(U sports unlimited)
While a full evaluation will have to await further
study, in general it seems to distinguish person
names from company names reasonably reliably,
and it seems to have discovered that person names
consist of a first name (A0), a middle name or ini-
tial (A2), a surname (A3) and an optional suffix
(A4). Similarly, it seems to have uncovered that
company names typically end in a phrase such as
inc, ltd or co.
8 Conclusion
This paper establishes a connection between two
very different kinds of probabilistic models; LDA
models of the kind used for topic modelling, and
PCFGs, which are a standard model of hierarchi-
cal structure in language. The embedding we pre-
sented shows how to express an LDA model as a
PCFG, and has the property that Bayesian infer-
ence of the parameters of that PCFG produces an
equivalent model to that produced by Bayesian in-
ference of the LDA model?s parameters.
The primary value of this embedding is theoret-
ical rather than practical; we are not advocating
the use of PCFG estimation procedures to infer
LDA models. Instead, we claim that the embed-
ding suggests novel extensions to both the LDA
topic models and PCFG-style grammars. We jus-
tified this claim by presenting several hybrid mod-
els that combine aspects of both topic models and
1155
grammars. We don?t claim that these are neces-
sarily the best models for performing any particu-
lar tasks; rather, we present them as examples of
models inspired by a combination of PCFGs and
LDA topic models. We showed how the LDA
to PCFG embedding suggested a ?sticky topic?
model extension to LDA. We then discussed adap-
tor grammars, and inspired by the LDA topic mod-
els, presented a novel topic model whose prim-
itive elements are multi-word collocations rather
than words. We concluded with an adaptor gram-
mar that learns aspects of the internal structure of
proper names.
Acknowledgments
This research was funded by US NSF awards
0544127 and 0631667, as well as by a start-up
award from Macquarie University. I?d like to
thank the organisers and audience at the Topic
Modeling workshop at NIPS 2009, my former col-
leagues at Brown University (especially Eugene
Charniak, Micha Elsner, Sharon Goldwater, Tom
Griffiths and Erik Sudderth), my new colleagues
at Macquarie University and the ACL reviewers
for their excellent suggestions and comments on
this work. Naturally all errors remain my own.
References
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002.
The infinite Hidden Markov Model. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, volume 14,
pages 577?584. The MIT Press.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jeffrey Elman. 1990. Finding structure in time. Cog-
nitive Science, 14:197?211.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsuper-
vised named-entity clustering. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
164?172, Boulder, Colorado, June. Association for
Computational Linguistics.
E. Fox, E. Sudderth, M. Jordan, and A. Willsky. 2008.
An HDP-HMM for systems with state persistence.
In Andrew McCallum and Sam Roweis, editors,
Proceedings of the 25th Annual International Con-
ference on Machine Learning (ICML 2008), pages
312?319. Omnipress.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neu-
ral Information Processing Systems 18, pages 459?
466, Cambridge, MA. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:52285235.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211244.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York, April.
Association for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using adaptor grammars to iden-
tifying synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, Columbus, Ohio. Association for Com-
putational Linguistics.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language.
In 8th International Colloquium on Grammatical In-
ference.
Percy Liang, Slav Petrov, Michael Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchi-
cal Dirichlet processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 688?
697.
1156
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Probabilistic grammars and hierarchical Dirichlet
processes. In The Oxford Handbook of Applied
Bayesian Analysis. Oxford University Press.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York. Associ-
ation for Computational Linguistics.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
Xuerui Wang, Andrew McCallum, and Xing Wei.
2007. Topical n-grams: Phrase and topic discovery,
with an application to information retrieval. In Pro-
ceedings of the 7th IEEE International Conference
on Data Mining (ICDM), pages 697?702.
C.S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12:361?379.
1157
Proceedings of the ACL 2010 Conference Short Papers, pages 215?219,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SVD and Clustering for Unsupervised POS Tagging 
 
Michael Lamar* 
Division of Applied Mathematics 
Brown University 
Providence, RI, USA 
mlamar@dam.brown.edu 
 
Yariv Maron* 
Gonda Brain Research Center 
Bar-Ilan University 
Ramat-Gan, Israel 
syarivm@yahoo.com 
Mark Johnson 
Department of Computing 
Faculty of Science 
Macquarie University 
Sydney, Australia 
mjohnson@science.mq.edu.au 
Elie Bienenstock 
Division of Applied Mathematics 
and Department of Neuroscience 
Brown University 
Providence, RI, USA 
elie@brown.edu 
  
Abstract 
We revisit the algorithm of Sch?tze 
(1995) for unsupervised part-of-speech 
tagging. The algorithm uses reduced-rank 
singular value decomposition followed 
by clustering to extract latent features 
from context distributions. As imple-
mented here, it achieves state-of-the-art 
tagging accuracy at considerably less cost 
than more recent methods. It can also 
produce a range of finer-grained tag-
gings, with potential applications to vari-
ous tasks. 
1 Introduction 
While supervised approaches are able to solve 
the part-of-speech (POS) tagging problem with 
over 97% accuracy (Collins 2002; Toutanova et 
al. 2003), unsupervised algorithms perform con-
siderably less well. These models attempt to tag 
text without resources such as an annotated cor-
pus, a dictionary, etc. The use of singular value 
decomposition (SVD) for this problem was in-
troduced in Sch?tze (1995). Subsequently, a 
number of methods for POS tagging without a 
dictionary were examined, e.g., by Clark (2000), 
Clark (2003), Haghighi and Klein (2006), John-
son (2007), Goldwater and Griffiths (2007), Gao 
and Johnson (2008), and Gra?a et al (2009).  
The latter two, using Hidden Markov Models 
(HMMs), exhibit the highest performances to 
date for fully unsupervised POS tagging.   
The revisited SVD-based approach presented 
here, which we call ?two-step SVD? or SVD2, 
has four important characteristics. First, it 
achieves state-of-the-art tagging accuracy. 
Second, it requires drastically less computational 
effort than the best currently available models. 
Third, it demonstrates that state-of-the-art accu-
racy can be realized without disambiguation, i.e., 
without attempting to assign different tags to dif-
ferent tokens of the same type. Finally, with no 
significant increase in computational cost, SVD2 
can create much finer-grained labelings than typ-
ically produced by other algorithms. When com-
bined with some minimal supervision in post-
processing, this makes the approach useful for 
tagging languages that lack the resources re-
quired by fully supervised models. 
2 Methods 
Following the original work of Sch?tze (1995), 
we begin by constructing a right context matrix, 
R, and a left context matrix, L.  Rij counts the 
number of times in the corpus a token of word 
type i is immediately followed by a token of 
word type j. Similarly, Lij counts the number of 
times a token of type i is preceded by a token of 
type j. We truncate these matrices, including, in 
the right and left contexts, only the w1 most fre-
quent word types. The resulting L and R are of 
dimension Ntypes?w1, where Ntypes is the number 
of word types (spelling forms) in the corpus, and 
w1 is set to 1000. (The full Ntypes? Ntypes context 
matrices satisfy R = LT.) 
* These authors contributed equally. 
215
Next, both context matrices are factored using 
singular value decomposition: 
L = UL SL VL
T 
R = UR SR VR
T. 
The diagonal matrices SL and SR (each of rank 
1000) are reduced down to rank r1 = 100 by re-
placing the 900 smallest singular values in each 
matrix with zeros, yielding SL
* and SR
*.  We then 
form a pair of latent-descriptor matrices defined 
by:   
L* = UL SL
* 
R* = UR SR
*. 
Row i in matrix L* (resp. R*) is the left (resp. 
right) latent descriptor for word type i. We next 
include a normalization step in which each row 
in each of L* and R* is scaled to unit length, 
yielding matrices L** and R**. Finally, we form a 
single descriptor matrix D by concatenating these 
matrices into D = [L** R**].  Row i in matrix D is 
the complete latent descriptor for word type i; 
this latent descriptor sits on the Cartesian product 
of two 100-dimensional unit spheres, hereafter 
the 2-sphere. 
We next categorize these descriptors into 
k1 = 500 groups, using a k-means clustering algo-
rithm. Centroid initialization is done by placing 
the k initial centroids on the descriptors of the k 
most frequent words in the corpus. As the de-
scriptors sit on the 2-sphere, we measure the 
proximity of a descriptor to a centroid by the dot 
product between them; this is equal to the sum of 
the cosines of the angles?computed on the left 
and right parts?between them. We update each 
cluster?s centroid as the weighted average of its 
constituents, the weight being the frequency of 
the word type; the centroids are then scaled, so 
they sit on the 2-sphere. Typically, only a few 
dozen iterations are required for full convergence 
of the clustering algorithm. 
We then apply a second pass of this entire 
SVD-and-clustering procedure. In this second 
pass, we use the k1 = 500 clusters from the first 
iteration to assemble a new pair of context ma-
trices. Now, Rij counts all the cluster-j (j=1? k1) 
words to the right of word i, and Lij counts all the 
cluster-j words to the left of word i. The new ma-
trices L and R have dimension Ntypes ? k1. 
As in the first pass, we perform reduced-rank 
SVD, this time down to rank r2 = 300, and we 
again normalize the descriptors to unit length, 
yielding a new pair of latent descriptor matrices 
L** and R**.  Finally, we concatenate L** and R** 
into a single matrix of descriptors, and cluster 
these descriptors into k2 groups, where k2 is the 
desired number of induced tags. We use the same 
weighted k-means algorithm as in the first pass, 
again placing the k initial centroids on the de-
scriptors of the k most frequent words in the cor-
pus. The final tag of any token in the corpus is 
the cluster number of its type. 
3 Data and Evaluation 
We ran the SVD2 algorithm described above on 
the full Wall Street Journal part of the Penn 
Treebank (1,173,766 tokens). Capitalization was 
ignored, resulting in Ntypes = 43,766, with only a 
minor effect on accuracy. Evaluation was done 
against the POS-tag annotations of the 45-tag 
PTB tagset (hereafter PTB45), and against the 
Smith and Eisner (2005) coarse version of the 
PTB tagset (hereafter PTB17). We selected the 
three evaluation criteria of Gao and Johnson 
(2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to-
1 are the tagging accuracies under the best many-
to-one map and the greedy one-to-one map re-
spectively; VI is a map-free information-
theoretic criterion?see Gao and Johnson (2008) 
for details. Although we find M-to-1 to be the 
most reliable criterion of the three, we include 
the other two criteria for completeness. 
In addition to the best M-to-1 map, we also 
employ here, for large values of k2, a prototype-
based M-to-1 map.  To construct this map, we 
first find, for each induced tag t, the word type 
with which it co-occurs most frequently; we call 
this word type the prototype of t. We then query 
the annotated data for the most common gold tag 
for each prototype, and we map induced tag t to 
this gold tag. This prototype-based M-to-1 map 
produces accuracy scores no greater?typically 
lower?than the best M-to-1 map. We discuss 
the value of this approach as a minimally-
supervised post-processing step in Section 5. 
4 Results 
Low-k performance. Here we present the per-
formance of the SVD2 model when k2, the num-
ber of induced tags, is the same or roughly the 
same as the number of tags in the gold stan-
dard?hence small. Table 1 compares the per-
formance of SVD2 to other leading models. Fol-
lowing Gao and Johnson (2008), the number of 
induced tags is 17 for PTB17 evaluation and 50 
for PTB45 evaluation. Thus, with the exception 
of Gra?a et al (2009) who use 45 induced tags 
for PTB45, the number of induced tags is the 
same across each column of Table 1. 
216
The performance of SVD2 compares favora-
bly to the HMM models. Note that SVD2 is a 
deterministic algorithm. The table shows, in pa-
rentheses, the standard deviations reported in 
Gra?a et al (2009). For the sake of comparison 
with Gra?a et al (2009), we also note that, with 
k2 = 45, SVD2 scores 0.659 on PTB45. The NVI 
scores (Reichart and Rappoport 2009) corres-
ponding to the VI scores for SVD2 are 0.938 for 
PTB17 and 0.885 for PTB45. To examine the 
sensitivity of the algorithm to its four parameters, 
w1, r1, k1, and r2, we changed each of these para-
meters separately by a multiplicative factor of 
either 0.5 or 2; in neither case did M-to-1 accura-
cy drop by more than 0.014. 
This performance was achieved despite the 
fact that the SVD2 tagger is mathematically 
much simpler than the other models. Our MAT-
LAB implementation of SVD2 takes only a few 
minutes to run on a desktop computer, in contrast 
to HMM training times of several hours or days 
(Gao and Johnson 2008; Johnson 2007). 
 
High-k performance.  Not suffering from the 
same computational limitations as other models, 
SVD2 can easily accommodate high numbers of 
induced tags, resulting in fine-grained labelings. 
The value of this flexibility is discussed in the 
next section. Figure 1 shows, as a function of k2, 
the tagging accuracy of SVD2 under both the 
best and the prototype-based M-to-1 maps (see 
Section 3), for both the PTB45 and the PTB17 
tagsets. The horizontal one-tag-per-word-type 
line in each panel is the theoretical upper limit 
for tagging accuracy in non-disambiguating 
models (such as SVD2). This limit is the fraction 
of all tokens in the corpus whose gold tag is the 
most frequent for their type.  
5 Discussion 
At the heart of the algorithm presented here is 
the reduced-rank SVD method of Sch?tze 
(1995), which transforms bigram counts into la-
tent descriptors. In view of the present work, 
which achieves state-of-the-art performance 
when evaluation is done with the criteria now in 
common use, Sch?tze's original work should 
rightly be praised as ahead of its time. The SVD2 
model presented here differs from Sch?tze's 
work in many details of implementation?not all 
of which are explicitly specified in Sch?tze 
(1995). In what follows, we discuss the features 
of SVD2 that are most critical to its performance. 
Failure to incorporate any one of them signifi-
Figure 1. Performance of the SVD2 algo-
rithm as a function of the number of induced 
tags. Top: PTB45; bottom: PTB17.  Each 
plot shows the tagging accuracy under the 
best and the prototype-based M-to-1 maps, as 
well as the upper limit for non-
disambiguating taggers. 
 M-to-1 1-to-1 VI 
Model PTB17 PTB45 PTB17 PTB45 PTB17 PTB45 
SVD2 0.730 0.660 0.513 0.467 3.02 3.84 
HMM-EM  0.647 0.621 0.431 0.405 3.86 4.48 
HMM-VB  0.637 0.605 0.514 0.461 3.44 4.28 
HMM-GS  0.674 0.660 0.466 0.499 3.46 4.04 
HMM-Sparse(32) 0.702(2.2) 0.654(1.0) 0.495 0.445   
VEM (10-1,10-1) 0.682(0.8) 0.546(1.7) 0.528 0.460   
Table 1.  Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and 
VI, for the full PTB45 tagset and  the reduced PTB17 tagset.  HMM-EM, HMM-VB 
and HMM-GS show the best results from Gao and Johnson (2008); HMM-Sparse(32) 
and VEM (10-1,10-1) show the best results from Gra?a et al (2009). 
 
217
cantly reduces the performance of the algorithm 
(M-to-1 reduced by 0.04 to 0.08). 
First, the reduced-rank left-singular vectors 
(for the right and left context matrices) are 
scaled, i.e., multiplied, by the singular values.  
While the resulting descriptors, the rows of L* 
and R*, live in a much lower-dimensional space 
than the original context vectors, they are 
mapped by an angle-preserving map (defined by 
the matrices of right-singular vectors VL and VR) 
into vectors in the original space. These mapped 
vectors best approximate (in the least-squares 
sense) the original context vectors; they have the 
same geometric relationships as their equivalent 
high-dimensional images, making them good 
candidates for the role of word-type descriptors. 
A second important feature of the SVD2 algo-
rithm is the unit-length normalization of the la-
tent descriptors, along with the computation of 
cluster centroids as the weighted averages of 
their constituent vectors. Thanks to this com-
bined device, rare words are treated equally to 
frequent words regarding the length of their de-
scriptor vectors, yet contribute less to the place-
ment of centroids. 
Finally, while the usual drawback of k-means-
clustering algorithms is the dependency of the 
outcome on the initial?usually random?
placement of centroids, our initialization of the k 
centroids as the descriptors of the k most fre-
quent word types in the corpus makes the algo-
rithm fully deterministic, and improves its per-
formance substantially: M-to-1 PTB45 by 0.043, 
M-to-1 PTB17 by 0.063. 
As noted in the Results section, SVD2 is fairly 
robust to changes in all four parameters w1, r1, k1, 
and r2. The values used here were obtained by a  
coarse, greedy strategy, where each parameter 
was optimized independently. It is worth noting 
that dispensing with the second pass altogether, 
i.e., clustering directly the latent descriptor vec-
tors obtained in the first pass into the desired 
number of induced tags, results in a drop of 
Many-to-1 score of only 0.021 for the PTB45 
tagset and 0.009 for the PTB17 tagset. 
 
Disambiguation. An obvious limitation of 
SVD2 is that it is a non-disambiguating tagger, 
assigning the same label to all tokens of a type. 
However, this limitation per se is unlikely to be 
the main obstacle to the improvement of low-k 
performance, since, as is well known, the theo-
retical upper limit for the tagging accuracy of 
non-disambiguating models (shown in Fig. 1) is 
much higher than the current state-of-the-art for 
unsupervised taggers, whether disambiguating or 
not. 
To further gain insight into how successful 
current models are at disambiguating when they 
have the power to do so, we examined a collec-
tion of HMM-VB runs (Gao and Johnson 2008) 
and asked how the accuracy scores would change 
if, after training was completed, the model were 
forced to assign the same label to all tokens of 
the same type. To answer this question, we de-
termined, for each word type, the modal HMM 
state, i.e., the state most frequently assigned by 
the HMM to tokens of that type. We then re-
labeled all words with their modal label. The ef-
fect of thus eliminating the disambiguation ca-
pacity of the model was to slightly increase the 
tagging accuracy under the best M-to-1 map for 
every HMM-VB run (the average increase was 
0.026  for PTB17, and 0.015 for PTB45).  We 
view this as a further indication that, in the cur-
rent state of the art and with regards to tagging 
accuracy, limiting oneself to non-disambiguating 
models may not adversely affect performance.  
To the contrary, this limitation may actually 
benefit an approach such as SVD2. Indeed, on 
difficult learning tasks, simpler models often be-
have better than more powerful ones (Geman et 
al. 1992). HMMs are powerful since they can, in 
theory, induce both a system of tags and a system 
of contextual patterns that allow them to disam-
biguate word types in terms of these tags. How-
ever, carrying out both of these unsupervised 
learning tasks at once is problematic in view of 
the very large number of parameters to be esti-
mated compared to the size of the training data 
set. 
The POS-tagging subtask of disambiguation 
may then be construed as a challenge in its own 
right: demonstrate effective disambiguation in an 
unsupervised model. Specifically, show that tag-
ging accuracy decreases when the model's dis-
ambiguation capacity is removed, by re-labeling 
all tokens with their modal label, defined above. 
We believe that the SVD2 algorithm presented 
here could provide a launching pad for an ap-
proach that would successfully address the dis-
ambiguation challenge. It would do so by allow-
ing a gradual and carefully controlled amount of 
ambiguity into an initially non-disambiguating 
model. This is left for future work. 
 
Fine-grained labeling. An important feature of 
the SVD2 algorithm is its ability to produce a 
fine-grained labeling of the data, using a number 
of clusters much larger than the number of tags 
218
in a syntax-motivated POS-tag system. Such 
fine-grained labelings can capture additional lin-
guistic features. To achieve a fine-grained labe-
ling, only the final clustering step in the SVD2 
algorithm needs to be changed; the computation-
al cost this entails is negligible. A high-quality 
fine-grained labeling, such as achieved by the 
SVD2 approach, may be of practical interest as 
an input to various types of unsupervised gram-
mar-induction algorithms (Headden et al 2008). 
This application is left for future work. 
 
Prototype-based tagging. One potentially im-
portant practical application of a high-quality 
fine-grained labeling is its use for languages 
which lack any kind of annotated data. By first 
applying the SVD2 algorithm, word types are 
grouped together into a few hundred clusters. 
Then, a prototype word is automatically ex-
tracted from each cluster. This produces, in a 
completely unsupervised way, a list of only a 
few hundred words that need to be hand-tagged 
by an expert. The results shown in Fig. 1 indicate 
that these prototype tags can then be used to tag 
the entire corpus with only a minor decrease in 
accuracy compared to the best M-to-1 map?the 
construction of which requires a fully annotated 
corpus. Fig. 1 also indicates that, with only a few 
hundred prototypes, the gap left between the ac-
curacy thus achieved and the upper bound for 
non-disambiguating models is fairly small. 
References 
Alexander Clark. 2000. Inducing syntactic categories 
by context distribution clustering. In The Fourth 
Conference on Natural Language Learning. 
Alexander Clark. 2003. Combining distributional and 
morphological information for part of speech in-
duction. In 10th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 59?66.  
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proceedings of 
the ACL-02 conference on Empirical methods in 
natural language processing ? Volume 10. 
Jianfeng Gao and Mark Johnson. 2008. A comparison 
of bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proceedings of the 
2008 Conference on Empirical Methods in Natural 
Language Processing, pages 344?352. 
Stuart Geman, Elie Bienenstock and Ren? Doursat. 
1992. Neural Networks and the Bias/Variance Di-
lemma. Neural Computation,  4 (1), pages 1?58. 
Sharon Goldwater and Tom Griffiths. 2007. A fully 
Bayesian approach to unsupervised part-of-speech 
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics, pages 744?751. 
Jo?o V. Gra?a, Kuzman Ganchev, Ben Taskar, and 
Fernando Pereira. 2009. Posterior vs. Parameter 
Sparsity in Latent Variable Models. In Neural In-
formation Processing Systems Conference (NIPS).  
Aria Haghighi and Dan Klein. 2006. Prototype-driven 
learning for sequence models. In Proceedings of 
the Human Language Technology Conference of 
the NAACL, Main Conference, pages 320?327, 
New York City, USA, June. Association for Com-
putational Linguistics. 
William P. Headden, David McClosky, and Eugene 
Charniak. 2008. Evaluating unsupervised part-of-
speech tagging for grammar induction. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING ?08). 
Mark Johnson. 2007. Why doesn?t EM find good 
HMM POS-taggers? In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL), pages 296?
305. 
Marina Meil?. 2003. Comparing clusterings by the 
variation of information. In Bernhard Sch?lkopf 
and Manfred K. Warmuth, editors, COLT 2003: 
The Sixteenth Annual Conference on Learning 
Theory, volume 2777 of Lecture Notes in Comput-
er Science, pages 173?187. Springer. 
Roi Reichart and Ari Rappoport. 2009. The NVI 
Clustering Evaluation Measure. In Proceedings of 
the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL), pages 165?173. 
Hinrich Sch?tze. 1995. Distributional part-of-speech 
tagging. In Proceedings of the seventh conference 
on European chapter of the Association for Com-
putational Linguistics, pages 141?148. 
Noah A. Smith and Jason Eisner. 2005. Contrastive 
estimation: Training log-linear models on unla-
beled data. In Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 354?362. 
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. 
In Proceedings of HLT-NAACL 2003, pages 252-
259. 
219
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703?711,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The impact of language models and loss functions on repair disfluency
detection
Simon Zwarts and Mark Johnson
Centre for Language Technology
Macquarie University
{simon.zwarts|mark.johnson|}@mq.edu.au
Abstract
Unrehearsed spoken language often contains
disfluencies. In order to correctly inter-
pret a spoken utterance, any such disfluen-
cies must be identified and removed or other-
wise dealt with. Operating on transcripts of
speech which contain disfluencies, we study
the effect of language model and loss func-
tion on the performance of a linear reranker
that rescores the 25-best output of a noisy-
channel model. We show that language mod-
els trained on large amounts of non-speech
data improve performance more than a lan-
guage model trained on a more modest amount
of speech data, and that optimising f-score
rather than log loss improves disfluency detec-
tion performance.
Our approach uses a log-linear reranker, oper-
ating on the top n analyses of a noisy chan-
nel model. We use large language models,
introduce new features into this reranker and
examine different optimisation strategies. We
obtain a disfluency detection f-scores of 0.838
which improves upon the current state-of-the-
art.
1 Introduction
Most spontaneous speech contains disfluencies such
as partial words, filled pauses (e.g., ?uh?, ?um?,
?huh?), explicit editing terms (e.g., ?I mean?), par-
enthetical asides and repairs. Of these, repairs
pose particularly difficult problems for parsing and
related Natural Language Processing (NLP) tasks.
This paper presents a model of disfluency detec-
tion based on the noisy channel framework, which
specifically targets the repair disfluencies. By com-
bining language models and using an appropriate
loss function in a log-linear reranker we are able to
achieve f-scores which are higher than previously re-
ported.
Often in natural language processing algorithms,
more data is more important than better algorithms
(Brill and Banko, 2001). It is this insight that drives
the first part of the work described in this paper. This
paper investigates how we can use language models
trained on large corpora to increase repair detection
accuracy performance.
There are three main innovations in this paper.
First, we investigate the use of a variety of language
models trained from text or speech corpora of vari-
ous genres and sizes. The largest available language
models are based on written text: we investigate the
effect of written text language models as opposed to
language models based on speech transcripts. Sec-
ond, we develop a new set of reranker features ex-
plicitly designed to capture important properties of
speech repairs. Many of these features are lexically
grounded and provide a large performance increase.
Third, we utilise a loss function, approximate ex-
pected f-score, that explicitly targets the asymmetric
evaluation metrics used in the disfluency detection
task. We explain how to optimise this loss func-
tion, and show that this leads to a marked improve-
ment in disfluency detection. This is consistent with
Jansche (2005) and Smith and Eisner (2006), who
observed similar improvements when using approx-
imate f-score loss for other problems. Similarly we
introduce a loss function based on the edit-f-score in
our domain.703
Together, these three improvements are enough to
boost detection performance to a higher f-score than
previously reported in literature. Zhang et al (2006)
investigate the use of ?ultra large feature spaces? as
an aid for disfluency detection. Using over 19 mil-
lion features, they report a final f-score in this task of
0.820. Operating on the same body of text (Switch-
board), our work leads to an f-score of 0.838, this is
a 9% relative improvement in residual f-score.
The remainder of this paper is structured as fol-
lows. First in Section 2 we describe related work.
Then in Section 3 we present some background on
disfluencies and their structure. Section 4 describes
appropriate evaluation techniques. In Section 5 we
describe the noisy channel model we are using. The
next three sections describe the new additions: Sec-
tion 6 describe the corpora used for language mod-
els, Section 7 describes features used in the log-
linear model employed by the reranker and Section 8
describes appropriate loss functions which are criti-
cal for our approach. We evaluate the new model in
Section 9. Section 10 draws up a conclusion.
2 Related work
A number of different techniques have been pro-
posed for automatic disfluency detection. Schuler
et al (2010) propose a Hierarchical Hidden Markov
Model approach; this is a statistical approach which
builds up a syntactic analysis of the sentence and
marks those subtrees which it considers to be made
up of disfluent material. Although they are inter-
ested not only in disfluency but also a syntactic anal-
ysis of the utterance, including the disfluencies be-
ing analysed, their model?s final f-score for disflu-
ency detection is lower than that of other models.
Snover et al (2004) investigate the use of purely
lexical features combined with part-of-speech tags
to detect disfluencies. This approach is compared to
approaches which use primarily prosodic cues, and
appears to perform equally well. However, the au-
thors note that this model finds it difficult to identify
disfluencies which by themselves are very fluent. As
we will see later, the individual components of a dis-
fluency do not have to be disfluent by themselves.
This can occur when a speaker edits her speech for
meaning-related reasons, rather than errors that arise
from performance. The edit repairs which are the fo-
cus of our work typically have this characteristic.
Noisy channel models have done well on the dis-
fluency detection task in the past; the work of John-
son and Charniak (2004) first explores such an ap-
proach. Johnson et al (2004) adds some hand-
written rules to the noisy channel model and use a
maximum entropy approach, providing results com-
parable to Zhang et al (2006), which are state-of-the
art results.
Kahn et al (2005) investigated the role of
prosodic cues in disfluency detection, although the
main focus of their work was accurately recovering
and parsing a fluent version of the sentence. They
report a 0.782 f-score for disfluency detection.
3 Speech Disfluencies
We follow the definitions of Shriberg (1994) regard-
ing speech disfluencies. She identifies and defines
three distinct parts of a speech disfluency, referred
to as the reparandum, the interregnum and the re-
pair. Consider the following utterance:
I want a flight
reparandum? ?? ?
to Boston,
uh, I mean
? ?? ?
interregnum
to Denver
? ?? ?
repair
on Friday
(1)
The reparandum to Boston is the part of the utterance
that is ?edited out?; the interregnum uh, I mean is a
filled pause, which need not always be present; and
the repair to Denver replaces the reparandum.
Shriberg and Stolcke (1998) studied the location
and distribution of repairs in the Switchboard cor-
pus (Godfrey and Holliman, 1997), the primary cor-
pus for speech disfluency research, but did not pro-
pose an actual model of repairs. They found that the
overall distribution of speech disfluencies in a large
corpus can be fit well by a model that uses only in-
formation on a very local level. Our model, as ex-
plained in section 5, follows from this observation.
As our domain of interest we use the Switchboard
corpus. This is a large corpus consisting of tran-
scribed telephone conversations between two part-
ners. In the Treebank III (Marcus et al, 1999) cor-
pus there is annotation available for the Switchboard
corpus, which annotates which parts of utterances
are in a reparandum, interregnum or repair.704
4 Evaluation metrics for disfluency
detection systems
Disfluency detection systems like the one described
here identify a subset of the word tokens in each
transcribed utterance as ?edited? or disfluent. Per-
haps the simplest way to evaluate such systems is
to calculate the accuracy of labelling they produce,
i.e., the fraction of words that are correctly labelled
(i.e., either ?edited? or ?not edited?). However,
as Charniak and Johnson (2001) observe, because
only 5.9% of words in the Switchboard corpus are
?edited?, the trivial baseline classifier which assigns
all words the ?not edited? label achieves a labelling
accuracy of 94.1%.
Because the labelling accuracy of the trivial base-
line classifier is so high, it is standard to use a dif-
ferent evaluation metric that focuses more on the de-
tection of ?edited? words. We follow Charniak and
Johnson (2001) and report the f-score of our disflu-
ency detection system. The f-score f is:
f =
2c
g + e
(2)
where g is the number of ?edited? words in the gold
test corpus, e is the number of ?edited? words pro-
posed by the system on that corpus, and c is the num-
ber of the ?edited? words proposed by the system
that are in fact correct. A perfect classifier which
correctly labels every word achieves an f-score of
1, while the trivial baseline classifiers which label
every word as ?edited? or ?not edited? respectively
achieve a very low f-score.
Informally, the f-score metric focuses more on
the ?edited? words than it does on the ?not edited?
words. As we will see in section 8, this has implica-
tions for the choice of loss function used to train the
classifier.
5 Noisy Channel Model
Following Johnson and Charniak (2004), we use a
noisy channel model to propose a 25-best list of
possible speech disfluency analyses. The choice of
this model is driven by the observation that the re-
pairs frequently seem to be a ?rough copy? of the
reparandum, often incorporating the same or very
similar words in roughly the same word order. That
is, they seem to involve ?crossed? dependencies be-
tween the reparandum and the repair. Example (3)
shows the crossing dependencies. As this exam-
ple also shows, the repair often contains many of
the same words that appear in the reparandum. In
fact, in our Switchboard training corpus we found
that 62reparandum also appeared in the associated
repair,
to Boston uh, I mean, to Denver? ?? ?
reparandum
? ?? ?
interregnum
? ?? ?
repair
(3)
5.1 Informal Description
Given an observed sentence Y we wish to find the
most likely source sentence X? , where
X? = argmax
X
P (Y |X)P (X) (4)
In our model the unobserved X is a substring of the
complete utterance Y .
Noisy-channel models are used in a similar way
in statistical speech recognition and machine trans-
lation. The language model assigns a probability
P (X) to the string X , which is a substring of the
observed utterance Y . The channel model P (Y |X)
generates the utterance Y , which is a potentially dis-
fluent version of the source sentence X . A repair
can potentially begin before any word of X . When
a repair has begun, the channel model incrementally
processes the succeeding words from the start of the
repair. Before each succeeding word either the re-
pair can end or else a sequence of words can be in-
serted in the reparandum. At the end of each re-
pair, a (possibly null) interregnum is appended to the
reparandum.
We will look at these two components in the next
two Sections in more detail.
5.2 Language Model
Informally, the task of language model component
of the noisy channel model is to assess fluency of
the sentence with disfluency removed. Ideally we
would like to have a model which assigns a very
high probability to disfluency-free utterances and a
lower probability to utterances still containing dis-
fluencies. For computational complexity reasons, as
described in the next section, inside the noisy chan-
nel model we use a bigram language model. This705
bigram language model is trained on the fluent ver-
sion of the Switchboard corpus (training section).
We realise that a bigram model might not be able
to capture more complex language behaviour. This
motivates our investigation of a range of additional
language models, which are used to define features
used in the log-linear reranker as described below.
5.3 Channel Model
The intuition motivating the channel model design
is that the words inserted into the reparandum are
very closely related to those in the repair. Indeed,
in our training data we find that 62% of the words
in the reparandum are exact copies of words in the
repair; this identity is strong evidence of a repair.
The channel model is designed so that exact copy
reparandum words will have high probability.
Because these repair structures can involve an un-
bounded number of crossed dependencies, they can-
not be described by a context-free or finite-state
grammar. This motivates the use of a more expres-
sive formalism to describe these repair structures.
We assume that X is a substring of Y , i.e., that the
source sentence can be obtained by deleting words
from Y , so for a fixed observed utterance Y there
are only a finite number of possible source sen-
tences. However, the number of possible source sen-
tences, X , grows exponentially with the length of Y ,
so exhaustive search is infeasible. Tree Adjoining
Grammars (TAG) provide a systematic way of for-
malising the channel model, and their polynomial-
time dynamic programming parsing algorithms can
be used to search for likely repairs, at least when
used with simple language models like a bigram
language model. In this paper we first identify the
25 most likely analyses of each sentence using the
TAG channel model together with a bigram lan-
guage model.
Further details of the noisy channel model can be
found in Johnson and Charniak (2004).
5.4 Reranker
To improve performance over the standard noisy
channel model we use a reranker, as previously sug-
gest by Johnson and Charniak (2004). We rerank a
25-best list of analyses. This choice is motivated by
an oracle experiment we performed, probing for the
location of the best analysis in a 100-best list. This
experiment shows that in 99.5% of the cases the best
analysis is located within the first 25, and indicates
that an f-score of 0.958 should be achievable as the
upper bound on a model using the first 25 best anal-
yses. We therefore use the top 25 analyses from the
noisy channel model in the remainder of this paper
and use a reranker to choose the most suitable can-
didate among these.
6 Corpora for language modelling
We would like to use additional data to model
the fluent part of spoken language. However, the
Switchboard corpus is one of the largest widely-
available disfluency-annotated speech corpora. It is
reasonable to believe that for effective disfluency de-
tection Switchboard is not large enough and more
text can provide better analyses. Schwartz et al
(1994), although not focusing on disfluency detec-
tion, show that using written language data for mod-
elling spoken language can improve performance.
We turn to three other bodies of text and investi-
gate the use of these corpora for our task, disfluency
detection. We will describe these corpora in detail
here.
The predictions made by several language models
are likely to be strongly correlated, even if the lan-
guage models are trained on different corpora. This
motivates the choice for log-linear learners, which
are built to handle features which are not necessar-
ily independent. We incorporate information from
the external language models by defining a reranker
feature for each external language model. The value
of this feature is the log probability assigned by the
language model to the candidate underlying fluent
substring X
For each of our corpora (including Switchboard)
we built a 4-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995). For each analy-
sis we calculate the probability under that language
model for the candidate underlying fluent substring
X . We use this log probability as a feature in the
reranker. We use the SRILM toolkit (Stolcke, 2002)
both for estimating the model from the training cor-
pus as well as for computing the probabilities of the
underlying fluent sentences X of the different anal-
ysis.
As previously described, Switchboard is our pri-706
mary corpus for our model. The language model
part of the noisy channel model already uses a bi-
gram language model based on Switchboard, but in
the reranker we would like to also use 4-grams for
reranking. Directly using Switchboard to build a 4-
gram language model is slightly problematic. When
we use the training data of Switchboard both for lan-
guage fluency prediction and the same training data
also for the loss function, the reranker will overesti-
mate the weight associated with the feature derived
from the Switchboard language model, since the flu-
ent sentence itself is part of the language model
training data. We solve this by dividing the Switch-
board training data into 20 folds. For each fold we
use the 19 other folds to construct a language model
and then score the utterance in this fold with that
language model.
The largest widely-available corpus for language
modelling is the Web 1T 5-gram corpus (Brants and
Franz, 2006). This data set, collected by Google
Inc., contains English word n-grams and their ob-
served frequency counts. Frequency counts are pro-
duced from this billion-token corpus of web text.
Because of the noise1 present in this corpus there is
an ongoing debate in the scientific community of the
use of this corpus for serious language modelling.
The Gigaword Corpus (Graff and Cieri, 2003)
is a large body of newswire text. The corpus con-
tains 1.6 ? 109 tokens, however fluent newswire text
is not necessarily of the same domain as disfluency
removed speech.
The Fisher corpora Part I (David et al, 2004) and
Part II (David et al, 2005) are large bodies of tran-
scribed text. Unlike Switchboard there is no disflu-
ency annotation available for Fisher. Together the
two Fisher corpora consist of 2.2 ? 107 tokens.
7 Features
The log-linear reranker, which rescores the 25-best
lists produced by the noisy-channel model, can
also include additional features besides the noisy-
channel log probabilities. As we show below, these
additional features can make a substantial improve-
ment to disfluency detection performance. Our
reranker incorporates two kinds of features. The first
1We do not mean speech disfluencies here, but noise in web-
text; web-text is often poorly written and unedited text.
are log-probabilities of various scores computed by
the noisy-channel model and the external language
models. We only include features which occur at
least 5 times in our training data.
The noisy channel and language model features
consist of:
1. LMP: 4 features indicating the probabilities of
the underlying fluent sentences under the lan-
guage models, as discussed in the previous sec-
tion.
2. NCLogP: The Log Probability of the entire
noisy channel model. Since by itself the noisy
channel model is already doing a very good job,
we do not want this information to be lost.
3. LogFom: This feature is the log of the ?fig-
ure of merit? used to guide search in the noisy
channel model when it is producing the 25-best
list for the reranker. The log figure of merit is
the sum of the log language model probability
and the log channel model probability plus 1.5
times the number of edits in the sentence. This
feature is redundant, i.e., it is a linear combina-
tion of other features available to the reranker
model: we include it here so the reranker has
direct access to all of the features used by the
noisy channel model.
4. NCTransOdd: We include as a feature parts of
the noisy channel model itself, i.e. the channel
model probability. We do this so that the task
to choosing appropriate weights of the channel
model and language model can be moved from
the noisy channel model to the log-linear opti-
misation algorithm.
The boolean indicator features consist of the fol-
lowing 3 groups of features operating on words and
their edit status; the latter indicated by one of three
possible flags: when the word is not part of a dis-
fluency or E when it is part of the reparandum or I
when it is part of the interregnum.
1. CopyFlags X Y: When there is an exact copy
in the input text of length X (1 ? X ? 3) and
the gap between the copies is Y (0 ? Y ? 3)
this feature is the sequence of flags covering the
two copies. Example: CopyFlags 1 0 (E707
) records a feature when two identical words
are present, directly consecutive and the first
one is part of a disfluency (Edited) while the
second one is not. There are 745 different in-
stances of these features.
2. WordsFlags L n R: This feature records the
immediate area around an n-gram (n ? 3).
L denotes how many flags to the left and R
(0 ? R ? 1) how many to the right are includes
in this feature (Both L and R range over 0 and
1). Example: WordsFlags 1 1 0 (need
) is a feature that fires when a fluent word is
followed by the word ?need? (one flag to the
left, none to the right). There are 256808 of
these features present.
3. SentenceEdgeFlags B L: This feature indi-
cates the location of a disfluency in an ut-
terance. The Boolean B indicates whether
this features records sentence initial or sen-
tence final behaviour, L (1 ? L ? 3)
records the length of the flags. Example
SentenceEdgeFlags 1 1 (I) is a fea-
ture recording whether a sentence ends on an
interregnum. There are 22 of these features
present.
We give the following analysis as an example:
but E but that does n?t work
The language model features are the probability
calculated over the fluent part. NCLogP, Log-
Fom and NCTransOdd are present with their asso-
ciated value. The following binary flags are present:
CopyFlags 1 0 (E )
WordsFlags:0:1:0 (but E)
WordsFlags:0:1:0 (but )
WordsFlags:1:1:0 (E but )
WordsFlags:1:1:0 ( that )
WordsFlags:0:2:0 (but E but ) etc.2
SentenceEdgeFlags:0:1 (E)
SentenceEdgeFlags:0:2 (E )
SentenceEdgeFlags:0:3 (E )
These three kinds of boolean indicator features to-
gether constitute the extended feature set.
2An exhaustive list here would be too verbose.
8 Loss functions for reranker training
We formalise the reranker training procedure as fol-
lows. We are given a training corpus T containing
information about n possibly disfluent sentences.
For the ith sentence T specifies the sequence of
words xi, a set Yi of 25-best candidate ?edited? la-
bellings produced by the noisy channel model, as
well as the correct ?edited? labelling y?i ? Yi.3
We are also given a vector f = (f1, . . . , fm)
of feature functions, where each fj maps a word
sequence x and an ?edit? labelling y for x to a
real value fj(x, y). Abusing notation somewhat,
we write f(x, y) = (f1(x, y), . . . , fm(x, y)). We
interpret a vector w = (w1, . . . , wm) of feature
weights as defining a conditional probability distri-
bution over a candidate set Y of ?edited? labellings
for a string x as follows:
Pw(y | x,Y) =
exp(w ? f(x, y))
?
y??Y exp(w ? f(x, y?))
We estimate the feature weights w from the train-
ing data T by finding a feature weight vector w? that
optimises a regularised objective function:
w? = argmin
w
LT (w) + ?
m?
j=1
w2j
Here ? is the regulariser weight and LT is a loss
function. We investigate two different loss functions
in this paper. LogLoss is the negative log conditional
likelihood of the training data:
LogLossT (w) =
m?
i=1
? log P(y?i | xi,Yi)
Optimising LogLoss finds the w? that define (regu-
larised) conditional Maximum Entropy models.
It turns out that optimising LogLoss yields sub-
optimal weight vectors w? here. LogLoss is a sym-
metric loss function (i.e., each mistake is equally
weighted), while our f-score evaluation metric
weights ?edited? labels more highly, as explained
in section 4. Because our data is so skewed (i.e.,
?edited? words are comparatively infrequent), we
3In the situation where the true ?edited? labelling does not
appear in the 25-best list Yi produced by the noisy-channel
model, we choose y?i to be a labelling in Yi closest to the true
labelling.708
can improve performance by using an asymmetric
loss function.
Inspired by our evaluation metric, we devised an
approximate expected f-score loss function FLoss .
FLossT (w) = 1 ?
2Ew[c]
g + Ew[e]
This approximation assumes that the expectations
approximately distribute over the division: see Jan-
sche (2005) and Smith and Eisner (2006) for other
approximations to expected f-score and methods for
optimising them. We experimented with other asym-
metric loss functions (e.g., the expected error rate)
and found that they gave very similar results.
An advantage of FLoss is that it and its deriva-
tives with respect to w (which are required for
numerical optimisation) are easy to calculate ex-
actly. For example, the expected number of correct
?edited? words is:
Ew[c] =
n?
i=1
Ew[cy?i | Yi], where:
Ew[cy?i | Yi] =
?
y?Yi
cy?i (y) Pw(y | xi,Yi)
and cy?(y) is the number of correct ?edited? labels
in y given the gold labelling y?. The derivatives of
FLoss are:
?FLossT
?wj
(w) =
1
g + Ew[e]
(
FLossT (w)
?Ew[e]
?wj
? 2?Ew[c]
?wj
)
where:
?Ew[c]
?wj
=
n?
i=1
?Ew[cy?i | xi,Yi]
?wj
?Ew[cy? | x,Y]
?wj
=
Ew[fjcy? | x,Y] ? Ew[fj | x,Y] Ew[cy? | x,Y].
?E[e]/?wj is given by a similar formula.
9 Results
We follow Charniak and Johnson (2001) and split
the corpus into main training data, held-out train-
ing data and test data as follows: main training con-
sisted of all sw[23]?.dps files, held-out training con-
sisted of all sw4[5-9]?.dps files and test consisted of
all sw4[0-1]?.dps files. However, we follow (John-
son and Charniak, 2004) in deleting all partial words
and punctuation from the training and test data (they
argued that this is more realistic in a speech process-
ing application).
Table 1 shows the results for the different models
on held-out data. To avoid over-fitting on the test
data, we present the f-scores over held-out training
data instead of test data. We used the held-out data
to select the best-performing set of reranker features,
which consisted of features for all of the language
models plus the extended (i.e., indicator) features,
and used this model to analyse the test data. The f-
score of this model on test data was 0.838. In this
table, the set of Extended Features is defined as all
the boolean features as described in Section 7.
We first observe that adding different external lan-
guage models does increase the final score. The
difference between the external language models is
relatively small, although the differences in choice
are several orders of magnitude. Despite the pu-
tative noise in the corpus, a language model built
on Google?s Web1T data seems to perform very
well. Only the model where Switchboard 4-grams
are used scores slightly lower, we explain this be-
cause the internal bigram model of the noisy chan-
nel model is already trained on Switchboard and so
this model adds less new information to the reranker
than the other models do.
Including additional features to describe the prob-
lem space is very productive. Indeed the best per-
forming model is the model which has all extended
features and all language model features. The dif-
ferences among the different language models when
extended features are present are relatively small.
We assume that much of the information expressed
in the language models overlaps with the lexical fea-
tures.
We find that using a loss function related to our
evaluation metric, rather than optimising LogLoss ,
consistently improves edit-word f-score. The stan-
dard LogLoss function, which estimates the ?max-
imum entropy? model, consistently performs worse
than the loss function minimising expected errors.
The best performing model (Base + Ext. Feat.
+ All LM, using expected f-score loss) scores an f-
score of 0.838 on test data. The results as indicated
by the f-score outperform state-of-the-art models re-709
Model F-score
Base (noisy channel, no reranking) 0.756
Model log loss expected f-score loss
Base + Switchboard 0.776 0.791
Base + Fisher 0.771 0.797
Base + Gigaword 0.777 0.797
Base + Web1T 0.781 0.798
Base + Ext. Feat. 0.824 0.827
Base + Ext. Feat. + Switchboard 0.827 0.828
Base + Ext. Feat. + Fisher 0.841 0.856
Base + Ext. Feat. + Gigaword 0.843 0.852
Base + Ext. Feat. + Web1T 0.843 0.850
Base + Ext. Feat. + All LM 0.841 0.857
Table 1: Edited word detection f-score on held-out data for a variety of language models and loss functions
ported in literature operating on identical data, even
though we use vastly less features than other do.
10 Conclusion and Future work
We have described a disfluency detection algorithm
which we believe improves upon current state-of-
the-art competitors. This model is based on a noisy
channel model which scores putative analyses with
a language model; its channel model is inspired by
the observation that reparandum and repair are of-
ten very similar. As Johnson and Charniak (2004)
noted, although this model performs well, a log-
linear reranker can be used to increase performance.
We built language models from a variety of
speech and non-speech corpora, and examine the ef-
fect they have on disfluency detection. We use lan-
guage models derived from different larger corpora
effectively in a maximum reranker setting. We show
that the actual choice for a language model seems
to be less relevant and newswire text can be used
equally well for modelling fluent speech.
We describe different features to improve disflu-
ency detection even further. Especially these fea-
tures seem to boost performance significantly.
Finally we investigate the effect of different loss
functions. We observe that using a loss function di-
rectly optimising our interest yields a performance
increase which is at least at large as the effect of us-
ing very large language models.
We obtained an f-score which outperforms other
models reported in literature operating on identical
data, even though we use vastly fewer features than
others do.
Acknowledgements
This work was supported was supported under Aus-
tralian Research Council?s Discovery Projects fund-
ing scheme (project number DP110102593) and
by the Australian Research Council as part of the
Thinking Head Project the Thinking Head Project,
ARC/NHMRC Special Research Initiative Grant #
TS0669874. We thank the anonymous reviewers for
their helpful comments.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Published by Linguistic Data Consortium,
Philadelphia.
Erik Brill and Michele Banko. 2001. Mitigating the
Paucity-of-Data Problem: Exploring the Effect of
Training Corpus Size on Classifier Performance for
Natural Language Processing. In Proceedings of the
First International Conference on Human Language
Technology Research.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
Christopher Cieri David, David Miller, and Kevin
Walker. 2004. Fisher English Training Speech Part
1 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.710
Christopher Cieri David, David Miller, and Kevin
Walker. 2005. Fisher English Training Speech Part
2 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Published by Linguistic
Data Consortium, Philadelphia.
David Graff and Christopher Cieri. 2003. English gi-
gaword. Published by Linguistic Data Consortium,
Philadelphia.
Martin Jansche. 2005. Maximum Expected F-Measure
Training of Logistic Regression Models. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 692?699, Vancouver, British
Columbia, Canada, October. Association for Compu-
tational Linguistics.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 33?39.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An Improved Model for Recognizing Disfluen-
cies in Conversational Speech. In Proceedings of the
Rich Transcription Fall Workshop.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
Use of Prosody in Parsing Conversational Speech. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 233?240, Vancouver,
British Columbia, Canada.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, pages 181?
184.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Published by Linguistic Data Consortium, Philadel-
phia.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-Coverage Parsing us-
ing Human-Like Memory Constraints. Computational
Linguistics, 36(1):1?30.
Richard Schwartz, Long Nguyen, Francis Kubala,
George Chou, George Zavaliagkos, and John
Makhoul. 1994. On Using Written Language
Training Data for Spoken Language Modeling. In
Proceedings of the Human Language Technology
Workshop, pages 94?98.
Elizabeth Shriberg and Andreas Stolcke. 1998. How
far do speakers back up in repairs? A quantitative
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 2183?
2186.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disuencies. Ph.D. thesis, University of Cali-
fornia, Berkeley.
David A. Smith and Jason Eisner. 2006. Minimum Risk
Annealing for Training Log-Linear Models. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
787?794.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2004. A Lexically-Driven Algorithm for Disfluency
Detection. In Proceedings of Human Language Tech-
nologies and North American Association for Compu-
tational Linguistics, pages 157?160.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 561?568.
711
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488?496,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semantic Parsing with Bayesian Tree Transducers
Bevan Keeley Jones??
b.k.jones@sms.ed.ac.uk
Mark Johnson?
Mark.Johnson@mq.edu.au
Sharon Goldwater?
sgwater@inf.ed.ac.uk
? School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
? Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
Abstract
Many semantic parsing models use tree trans-
formations to map between natural language
and meaning representation. However, while
tree transformations are central to several
state-of-the-art approaches, little use has been
made of the rich literature on tree automata.
This paper makes the connection concrete
with a tree transducer based semantic parsing
model and suggests that other models can be
interpreted in a similar framework, increasing
the generality of their contributions. In par-
ticular, this paper further introduces a varia-
tional Bayesian inference algorithm that is ap-
plicable to a wide class of tree transducers,
producing state-of-the-art semantic parsing re-
sults while remaining applicable to any do-
main employing probabilistic tree transducers.
1 Introduction
Semantic parsing is the task of mapping natural lan-
guage sentences to a formal representation of mean-
ing. Typically, a system is trained on pairs of natural
language sentences (NLs) and their meaning repre-
sentation expressions (MRs), as in figure 1(a), and
the system must generalize to novel sentences.
Most semantic parsing models rely on an assump-
tion of structural similarity between MR and NL.
Since strict isomorphism is overly restrictive, this
assumption is often relaxed by applying transforma-
tions. Several approaches assume a tree structure to
the NL, MR, or both (Ge and Mooney, 2005; Kate
and Mooney, 2006; Wong and Mooney, 2006; Lu
et al, 2008; Bo?rschinger et al, 2011), and often in-
Figure 1: (a) An example sentence/meaning pair, (b) a
tree transformation based mapping, and (c) a tree trans-
ducer that performs the mapping.
volve tree transformations either between two trees
or a tree and a string.
The tree transducer, a formalism from automata
theory which has seen interest in machine transla-
tion (Yamada and Knight, 2001; Graehl et al, 2008)
and has potential applications in many other areas,
is well suited to formalizing such tree transforma-
tion based models. Yet, while many semantic pars-
ing systems resemble the formalism, each was pro-
posed as an independent model requiring custom al-
gorithms, leaving it unclear how developments in
one line of inquiry relate to others. We argue for a
unifying theory of tree transformation based seman-
tic parsing by presenting a tree transducer model and
drawing connections to other similar systems.
We make a further contribution by bringing to
tree transducers the benefits of the Bayesian frame-
work for principled handling of data sparsity and
488
prior knowledge. Graehl et al (2008) present an EM
training procedure for top down tree transducers, but
while there are Bayesian approaches to string trans-
ducers (Chiang et al, 2010) and PCFGs (Kurihara
and Sato, 2006), there has yet to be a proposal for
Bayesian inference in tree transducers. Our vari-
ational algorithm produces better semantic parses
than EM while remaining general to a broad class
of transducers appropriate for other domains.
In short, our contributions are three-fold: we
present a new state-of-the-art semantic parsing
model, propose a broader theory for tree transforma-
tion based semantic parsing, and present a general
inference algorithm for the tree transducer frame-
work. We recommend the last of these as just one
benefit of working within a general theory: contri-
butions are more broadly applicable.
2 Meaning representations and regular
tree grammars
In semantic parsing, an MR is typically an expres-
sion from a machine interpretable language (e.g., a
database query language or a logical language like
Prolog). In this paper we assume MRs can be rep-
resented as trees, either by pre-parsing or because
they are already trees (often the case for functional
languages like LISP).1 More specifically, we assume
the MR language is a regular tree language.
A regular tree grammar (RTG) closely resembles
a context free grammar (CFG), and is a way of de-
scribing a language of trees. Formally, define T? as
the set of trees with symbols from alphabet ?, and
T?(A) as the set of all trees in T??A where symbols
from A only occur at the leaves. Then an RTG is a
tuple (Q,?, qstart,R), where Q is a set of states, ?
is an alphabet, qstart ? Q is the initial state, and R
is a set of grammar rules of the form q ? t, where q
is a state from Q and t is a tree from T?(Q).
A rule typically consists of a parent state (left) and
its child states and output symbol (right). We indi-
cate states using all capital letters:
NUM ? population(PLACE).
Intuitively, an RTG is a CFG where the yield of
every parse is itself a tree. In fact, for any CFG G, it
1See Liang et al (2011) for work in representing lambda
calculus expressions with trees.
is straightforward to produce a corresponding RTG
that generates the set of parses of G. Consequently,
while we assume we have an RTG for the MR lan-
guage, there is no loss of generality if the MR lan-
guage is actually context free.
3 Weighted root-to-frontier, linear,
non-deleting tree-to-string transducers
Tree transducers (Rounds, 1970; Thatcher, 1970) are
generalizations of finite state machines that operate
on trees. Mirroring the branching nature of its in-
put, the transducer may simultaneously transition to
several successor states, assigning a separate state to
each subtree.
There are many classes of transducer with dif-
ferent formal properties (Knight and Greahl, 2005;
Maletti et al, 2009). Figure 1(c) is an example of
a root-to-frontier, linear, non-deleting tree-to-string
transducer. It is defined using rules where the left
hand side identifies a state of the transducer and a
fragment of the input tree, and the right hand side
describes a portion of the output string. Variables
xi stand for entire sub-trees, and state-variable pairs
qj .xi stand for strings produced by applying the
transducer starting at state qj to subtree xi. Fig-
ure 1(b) illustrates an application of the transducer,
taking the tree on the left as input and outputting the
string on the right.
Formally, a weighted root-to-frontier, tree-to-
string transducer is a 5-tuple (Q,?,?, qstart,R). Q
is a finite set of states, ? and ? are the input and out-
put alphabets, qstart is the start state, and R is the
set of rules. Denote a pair of symbols, a and b by
a.b, the cross product of two sets A and B by A.B,
and let X be the set of variables {x0, x1, ...}. Then,
each rule r ? R is of the form [q.t ? u].v, where
v ? ??0 is the rule weight, q ? Q, t ? T?(X ), and
u is a string in (? ? Q.X )? such that every x ? X
in u also occurs in t.
We say q.t is the left hand side of rule r and u its
right hand side. The transducer is linear iff no vari-
able appears more than once on the right hand side.
It is non-deleting iff all variables on the left hand
side also occur on the right hand side. In this paper
we assume that every tree t on the left hand side is ei-
ther a single variable x0 or of the form ?(x0, ...xn),
where ? ? ? (i.e., it is a tree of depth ? 1).
489
A weighted tree transducer may define a probabil-
ity distribution, either a joint distribution over input
and output pairs or a conditional distribution of the
output given the input. Here, we will use joint dis-
tributions, which can be defined by ensuring that the
weights of all rules with the same state on the left-
hand side sum to one. In this case, it can be help-
ful to view the transducer as simultaneously gener-
ating both the input and output, rather than the usual
view of mapping input trees into output strings. A
joint distribution allows us to model with a single
machine both the input and output languages, which
is important during decoding when we want to infer
the input given the output.
4 A generative model of semantic parsing
Like the hybrid tree semantic parser (Lu et al, 2008)
and the synchronous grammar based WASP (Wong
and Mooney, 2006), our model simultaneously gen-
erates the input MR tree and the output NL string.
The MR tree is built up according to the provided
MR grammar, one grammar rule at a time. Coupled
with the application of the MR rule, similar CFG-
like productions are applied to the NL side, repeated
until both the MR and NL are fully generated. In
each step, we select an MR rule and then build the
NL by first choosing a pattern with which to expand
it and then filling out that pattern with words drawn
from a unigram distribution.
This kind of coupled generative process can
be naturally formalized with tree transducer rules,
where the input tree fragment on the left side of each
rule describes the derivation of the MR and the right
describes the corresponding NL derivation.
For a simple example of a tree-to-string trans-
ducer rule consider
q.population(x1) ? ?population of? q.x1 (1)
which simultaneously generates tree fragment
population(x1) on the left and sub-string ?popula-
tion of q.x1? on the right. Variable x1 stands for
an MR subtree under population, and, on the right,
state-variable pair q.x1 stands for the NL substring
generated while processing subtree x1 starting from
q. While this rule can serve as a single step of
an MR-to-NL map such as the example transducer
shown in Figure 1(c), such rules do not model the
NUM ? population(PLACE) (m)
PLACE ? cityid(CITY, STATE) (r)
CITY ? portland (u)
STATE ? maine (v)
qMRm,1.x1 ? qNLr .x1 (2)
qMRr,1 .x1 ? qNLu .x1
qMRr,2 .x1 ? qNLv .x1
qNLm .population(w1, x1, w2) ?
qWm .w1 qMRm,1.x1 qEND.w2 (3)
qNLr .cityid(w1, x1, w2, x2, w3) ?
qEND.w1 qMRr,2 .x2 qWr .w2 qMRr,1 .x1 qEND.w3 (4)
qWm .w1 ? ?population? qWm .w1 (5)
qWm .w1 ? ?of? qWm .w1
qWm .w1 ? ... qWm .w1
qWm .w1 ? ?of? qEND.w1 (6)
qWm .w1 ? ... qEND.w1
qEND.W ? ? (7)
Figure 2: Examples of transducer rules (bottom) that gen-
erate MR and NL associated with MR rules m-v (top).
Transducer rule 2 selects MR rule r from the MR gram-
mar. Rule 3 simultaneously writes the MR associated
with rule m and chooses an NL pattern (as does 4 for
r). Rules 5-7 generate the words associated with m ac-
cording to a unigram distribution specific to m.
grammaticality of the MR and lack flexibility since
sub-strings corresponding to a given tree fragment
must be completely pre-specified. Instead, we break
transductions down into a three stage process of
choosing the (i) MR grammar rule, (ii) NL expan-
sion pattern, and (iii) individual words according to
a unigram distribution. Such a decomposition in-
corporates independence assumptions that improve
generalizability. See Figure 2 for example rules
from our transducer and Figure 3 for a derivation.
To ensure that only grammatical MRs are gener-
ated, each state of our transducer encodes the iden-
tity of exactly one MR grammar rule. Transitions
between qMR and qNL states implicitly select the em-
bedded rule. For instance, rule 2 in Figure 2 selects
490
MR grammar rule r to expand the ith child of the
parent produced by rule m. Aside from ensuring
the grammaticality of the generated MR, rules of
this type also model the probability of the MR, con-
ditioning the probability of a rule both on the par-
ent rule and the index of the child being expanded.
Thus, parent state qMRm,1 encodes not only the identity
of rule m, but also the child index, 1 in this case.
Once the MR rule is selected, qNL states are ap-
plied to select among rules such as 3 and 4 to gen-
erate the MR entity and choose the NL expansion
pattern. These rules determine the word order of the
language by deciding (i) whether or not to generate
words in a given location and (ii) where to insert the
result of processing each MR subtree. Decision (i) is
made by either transitioning to state qWr to generate
words or to qEND to generate the empty string. De-
cision (ii) is made with the order of xi?s on the right
hand side. Rule 4 illustrates the case where port-
land and maine in cityid(portland, maine) would be
realized in reverse order as ?maine ... portland?.
The particular set of patterns that appear on the
right of rules such as 3 embodies the binary word at-
tachment decisions and the particular permutation of
xi in the NL. We allow words to be generated at the
beginning and end of each pattern and between the
xis. Thus, rule 4 is just one of 16 such possible pat-
terns (3 binary decisions and 2 permutations), while
rule 3 is one of 4. We instantiate all such rules and
allow the system to learn weights for them according
to the language of the training data.
Finally, the NL is filled out with words chosen ac-
cording to a unigram distribution, implemented in a
PCFG-like fashion, using a different rule for each
word which recursively chooses the next word un-
til a string termination rule is reached.2 Generating
word sequence ?population of? entails first choosing
rule 5 in Figure 2. State qWr is then recursively ap-
plied to choose rule 6, generating ?of? at the same
time as deciding to terminate the string by transi-
tioning to a new state qEND which deterministically
concludes by writing the empty string ?.
On the MR side, rules 5-7 do very little: the tree
on the left side of rules 5 and 6 consists entirely of a
2There are roughly 25,000 rules in the transducers in our
experiments, and the majority of these implement the unigram
word distributions since every entity in the MR may potentially
produce any of the words it is paired with in training.
subtree variable w1, indicating that nothing is gener-
ated in the MR. Rule 7 subsequently generates these
subtrees as W symbols, marking corresponding lo-
cations where words might be produced in the NL,
which are later removed during post processing.3
Figure 3(b) illustrates the coupled generative pro-
cess. At each step of the derivation, an MR rule is
chosen to expand a node of the MR tree, and then a
corresponding part of the NL is expanded. Step 1.1
of the example chooses MR rule m, NUM ?
population(PLACE). Transducer rule 3 then gener-
ates population in the MR (shown in the left column)
at the same time as choosing an NL expansion pat-
tern (Step 1.2) which is subsequently filled out with
specific words ?population? (1.3) and ?of? (1.4).
This coupled derivation can be represented by a
tree, shown in Figure 3(c), which explicitly repre-
sents the dependency structure of the coupled MR
and NL (a simplified version is shown in (d) for clar-
ity). In our transducer, which defines a joint distri-
bution over both the MR and NL, the probability of
a rule is conditioned on the parent state. Since each
state encodes an MR rule, MR rule specific distribu-
tions are learned for both the words and their order.
5 Relation to existing models
The tree transducer model can be viewed either as
a generative procedure for building up two separate
structures or as a transformative machine that takes
one as input and produces another as output. Dif-
ferent semantic parsing approaches have taken one
or the other view, and both can be captured in this
single framework.
WASP (Wong and Mooney, 2006) is an exam-
ple of the former perspective, coupling the genera-
tion of the MR and NL with a synchronous gram-
mar, a formalism closely related to tree transducers.
The most significant difference from our approach
is that they use machine translation techniques for
automatically extracting rules from parallel corpora;
similar techniques can be applied to tree transduc-
ers (Galley et al, 2004). In fact, synchronous gram-
mars and tree transducers can be seen as instances of
the same more general class of automata (Shieber,
3The addition of W symbols is a convenience; it is easier to
design transducer rules where every substring on the right side
corresponds to a subtree on the left.
491
Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and
the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to
substrings of the output and are removed in a post-processing step. (a) The (MR, NL) pair. (b) Step by step derivation.
(c) The same derivation shown in tree form. (d) The underlying dependency structure of the derivation.
2004). Rather than argue for one or the other, we
suggest that other approaches could also be inter-
preted in terms of general model classes, grounding
them in a broader base of theory.
The hybrid tree model (Lu et al, 2008) takes
a transformative perspective that is in some ways
more similar to our model. In fact, there is a one-
to-one relationship between the multinomial param-
eters of the two models. However, they represent the
MR and NL with a single tree and apply tree walk-
ing algorithms to extract them. Furthermore, they
implement a custom training procedure for search-
ing over the potential MR transformations. The tree
transducer, on the other hand, naturally captures the
same probabilistic dependencies while maintaining
the separation between MR and NL, and further al-
lows us to build upon a larger body of theory.
KRISP (Kate and Mooney, 2006) uses string clas-
sifiers to label substrings of the NL with entities
from the MR. To focus search, they impose an or-
dering constraint based on the structure of the MR
tree, which they relax by allowing the re-ordering
of sibling nodes and devise a procedure for recover-
ing the MR from the permuted tree. This procedure
corresponds to backward-application in tree trans-
ducers, identifying the most likely input tree given a
492
particular output string.
SCISSOR (Ge and Mooney, 2005) takes syntactic
parses rather than NL strings and attempts to trans-
late them into MR expressions. While few seman-
tic parsers attempt to exploit syntactic information,
there are techniques from machine translation for
using tree transducers to map between parsed par-
allel corpora, and these techniques could likely be
applied to semantic parsing.
Bo?rschinger et al (2011) argue for the PCFG as
an alternative model class, permitting conventional
grammar induction techniques, and tree transducers
are similar enough that many techniques are applica-
ble to both. However, the PCFG is less amenable to
conceptualizing correspondences between parallel
structures, and their model is more restrictive, only
applicable to domains with finite MR languages,
since their non-terminals encode entire MRs. The
tree transducer framework, on the other hand, allows
us to condition on individual MR rules.
6 Variational Bayes for tree transducers
As seen in the example in Figure 3(c), tree trans-
ducers not only operate on trees, their derivations
are themselves trees, making them amenable to dy-
namic programming and an EM training procedure
resembling inside-outside (Graehl et al, 2008). EM
assigns zero probability to events not seen in the
training data, however, limiting the ability to gen-
eralize to novel items. The Bayesian framework of-
fers an elegant solution to this problem, introducing
a prior over rule weights which simultaneously en-
sures that all rules receive non-zero probability and
allows the incorporation of prior knowledge and in-
tuitions. Unfortunately, the introduction of a prior
makes exact inference intractable, so we use an ap-
proximate method, variational Bayesian inference
(Bishop, 2006), deriving an algorithm similar to that
for PCFGs (Kurihara and Sato, 2006).
The tree transducer defines a joint distribution
over the input y, output w, and their derivation x
as the product of the weights of the rules appearing
in x. That is,
p(y, x, w|?) =
?
r?R
?(r)cr(x)
where ? is the set of multinomial parameters, r is a
transducer rule, ?(r) is its weight, and cr(x) is the
number of times r appears in x. In EM, we are in-
terested in the point estimate for ? that maximizes
p(Y,W|?), where Y and W are the N input-output
pairs in the training data. In the Bayesian setting,
however, we place a symmetric Dirichlet prior over
? and estimate a posterior distribution over both X
and ?.
p(?,X|Y,W) = p(Y,X ,W, ?)p(Y,W)
= p(?)
?N
i=1 p(yi, xi, wi|?)
?
p(?)?Ni=1
?
x?Xi p(yi, x, wi|?)d?
Since the integral in the denominator is in-
tractable, we look for an appropriate approximation
q(?,X ) ? p(?,X|Y,W). In particular, we assume
the rule weights and the derivations are independent,
i.e., q(?,X ) = q(?)q(X ). The basic idea is then to
define a lower bound F ? ln p(Y,W) in terms of q
and then apply the calculus of variations to find a q
that maximizes F .
ln p(Y,W|?) = lnEq[
p(Y,X ,W|?)
q(?,X ) ]
? Eq[ln
p(Y,X ,W|?)
q(?,X ) ] = F ,
Applying our independence assumption, we arrive at
the following expression for F , where ?t is the par-
ticular parameter vector corresponding to the rules
with parent state t:
F =
?
t?Q
(
Eq(?t)[ln p(?t|?t)]? Eq(?t)[ln q(?t)]
)
+
N
?
i=1
(
Eq[ln p(wi, xi, yi|?)]? Eq(xi)[ln q(xi)]
)
.
We find the q(?t) and q(xi) that maximize F by
taking derivatives of the Lagrangian, setting them to
zero, and solving, which yields:
q(?t) = Dirichlet(?t|??t)
q(xi) =
?
r?R ??(r)cr(xi)
?
x?Xi
?
r?R ??(r)cr(x)
where
??(r) = ?(r) +
?
i
Eq(xi)[cr(xi)]
??(r) = exp
?
??(??(r))??(
?
r:s(r)=t
??(r))
?
? .
493
The parameters of q(?t) are defined with respect
to q(xi) and the parameters of q(xi) with respect
to the parameters of q(?t). q(xi) can be computed
efficiently using inside-outside. Thus, we can per-
form an EM-like alternation between calculating ??
and ??.4
It is also possible to estimate the hyper-
parameters ? from data, a practice known as em-
pirical Bayes, by optimizing F . We explore learn-
ing separate hyper-parameters ?t for each ?t, us-
ing a fixed point update described by Minka (2000),
where kt is the number of rules with parent state t:
??t =
(
1
?t
+ 1kt?2t
(
?2F
??2t
)?1( ?F
??t
)
)?1
7 Training and decoding
We implement our VB training algorithm inside the
tree transducer package Tiburon (May and Knight,
2006), and experiment with both manually set and
automatically estimated priors. For our manually
set priors, we explore different hyper-parameter set-
tings for three different priors, one for each of the
main decision types: MR rule, NL pattern, and word
generation. For the automatic priors, we estimate
separate hyper-parameters for each multinomial (of
which there are hundreds). As is standard, we ini-
tialize the word distributions using a variant of IBM
model 1, and make use of NP lists (a manually cre-
ated list of the constants in the MR language paired
with the words that refer to them in the corpus).
At test time, since finding the most probable MR
for a sentence involves summing over all possible
derivations, we instead find the MR associated with
the most probable derivation.
8 Experimental setup and evaluation
We evaluate the system on GeoQuery (Wong and
Mooney, 2006), a parallel corpus of 880 English
questions and database queries about United States
geography, 250 of which were translated into Span-
ish, Japanese, and Turkish. We present here ad-
ditional translations of the full 880 sentences into
4Because of the resemblance to EM, this procedure has been
called VBEM. Unlike EM, however, this procedure alternates
between two estimation steps and has no maximization step.
German, Greek, and Thai. For evaluation, follow-
ing from Kwiatkowski et al (2010), we reserve 280
sentences for test and train on the remaining 600.
During development, we use cross-validation on the
600 sentence training set. At test, we run once on the
remaining 280 and perform 10 fold cross-validation
on the 250 sentence sets.
To judge correctness, we follow standard prac-
tice and submit each parse as a GeoQuery database
query, and say the parse is correct only if the answer
matches the gold standard. We report raw accuracy
(the percentage of sentences with correct answers),
as well as F1: the harmonic mean of precision (the
proportion of correct answers out of sentences with
a parse) and recall (the proportion of correct answers
out of all sentences).5
We run three other state-of-the-art systems for
comparison. WASP (Wong and Mooney, 2006) and
the hybrid tree (Lu et al, 2008) are chosen to rep-
resent tree transformation based approaches, and,
while this comparison is our primary focus, we also
report UBL-S (Kwiatkowski et al, 2010) as a non-
tree based top-performing system.6 The hybrid tree
is notable as the only other system based on a gen-
erative model, and uni-hybrid, a version that uses a
unigram distribution over words, is very similar to
our own model. We also report the best performing
version, re-hybrid, which incorporates a discrimina-
tive re-ranking step.
We report transducer performance under three dif-
ferent training conditions: tsEM using EM, tsVB-
auto using VB with empirical Bayes, and tsVB-hand
using hyper-parameters manually tuned on the Ger-
man training data (? of 0.3, 0.8, and 0.25 for MR
rule, NL pattern, and word choices, respectively).
Table 1 shows results for 10 fold cross-validation
on the training set. The results highlight the benefit
of the Dirichlet prior, whether manually or automat-
ically set. VB improves over EM considerably, most
likely because (1) the handling of unknown words
and MR entities allows it to return an analysis for all
sentences, and (2) the sparse Dirichlet prior favors
fewer rules, reasonable in this setting where only a
few words are likely to share the same meaning.
5Note that accuracy and f-score reduce to the same formula
if there are no parse failures.
6UBL-S is based on CCG, which can be viewed as a map-
ping between graphs more general than trees.
494
DEV geo600 - 10 fold cross-val
German Greek
Acc F1 Acc F1
UBL-S 76.7 76.9 76.2 76.5
WASP 66.3 75.0 71.2 79.7
uni-hybrid 61.7 66.1 71.0 75.4
re-hybrid 62.3 69.5 70.2 76.8
tsEM 61.7 67.9 67.3 73.2
tsVB-auto 74.0 74.0 ?79.8 ?79.8
tsVB-hand ?78.0 ?78.0 79.0 79.0
English Thai
UBL-S 85.3 85.4 74.0 74.1
WASP 73.5 79.4 69.8 73.9
uni-hybrid 76.3 79.0 71.3 73.7
re-hybrid 77.0 82.2 71.7 76.0
tsEM 73.5 78.1 69.8 72.9
tsVB-auto 81.2 81.2 74.7 74.7
tsVB-hand ?83.7 ?83.7 ?76.7 ?76.7
Table 1: Accuracy and F1 score comparisons on the
geo600 training set. Highest scores are in bold, while
the highest among the tree based models are marked with
a bullet. The dotted line separates the tree based from
non-tree based models.
On the test set (Table 2), we only run the model
variants that perform best on the training set. Test set
accuracy is consistently higher for the VB trained
tree transducer than the other tree transformation
based models (and often highest overall), while f-
score remains competitive.7
9 Conclusion
We have argued that tree transformation based se-
mantic parsing can benefit from the literature on for-
mal language theory and tree automata, and have
taken a step in this direction by presenting a tree
transducer based semantic parser. Drawing this con-
nection facilitates a greater flow of ideas in the
research community, allowing semantic parsing to
leverage ideas from other work with tree automata,
while making clearer how seemingly isolated ef-
forts might relate to one another. We demonstrate
this by both building on previous work in train-
ing tree transducers using EM (Graehl et al, 2008),
7Numbers differ slightly here from previously published re-
sults due to the fact that we have standardized the inputs to the
different systems.
TEST geo880 - 600 train/280 test
German Greek
Acc F1 Acc F1
UBL-S 75.0 75.0 73.6 73.7
WASP 65.7 ? 74.9 70.7 ? 78.6
re-hybrid 62.1 68.5 69.3 74.6
tsVB-hand ? 74.6 74.6 ?75.4 75.4
English Thai
UBL-S 82.1 82.1 66.4 66.4
WASP 71.1 77.7 71.4 75.0
re-hybrid 76.8 ? 81.0 73.6 76.7
tsVB-hand ? 79.3 79.3 ? 78.2 ? 78.2
geo250 - 10 fold cross-val
English Spanish
UBL-S 80.4 80.6 79.7 80.1
WASP 70.0 80.8 72.4 81.0
re-hybrid 74.8 82.6 78.8 ? 86.2
tsVB-hand ? 83.2 ? 83.2 ? 80.0 80.0
Japanese Turkish
UBL-S 80.5 80.6 74.2 74.9
WASP 74.4 ? 82.9 62.4 75.9
re-hybrid 76.8 82.4 66.8 ? 77.5
tsVB-hand ? 78.0 78.0 ? 75.6 75.6
Table 2: Accuracy and F1 score comparisons on the
geo880 and geo250 test sets. Highest scores are in
bold, while the highest among the tree based models are
marked with a bullet. The dotted line separates the tree
based from non-tree based models.7
and describing a general purpose variational infer-
ence algorithm for adapting tree transducers to the
Bayesian framework. The new VB algorithm re-
sults in an overall performance improvement for the
transducer over EM training, and the general effec-
tiveness of the approach is further demonstrated by
the Bayesian transducer achieving highest accuracy
among other tree transformation based approaches.
Acknowledgments
We thank Joel Lang, Michael Auli, Stella Frank,
Prachya Boonkwan, Christos Christodoulopoulos,
Ioannis Konstas, and Tom Kwiatkowski for provid-
ing the new translations of GeoQuery. This research
was supported in part under the Australian Re-
search Council?s Discovery Projects funding scheme
(project number DP110102506).
495
References
Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning. Springer, 2006.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. Reducing grounded learning tasks to grammati-
cal inference. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, 2011.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. Bayesian inference for finite-
state transducers. In Proc. of the annual meeting of
the North American Association for Computational Lin-
guistics, 2010.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a translation rule? In Proc. of the
annual meeting of the North American Association for
Computational Linguistics, 2004.
Ruifang Ge and Raymond J. Mooney. A statistical se-
mantic parser that integrates syntax and semantics. In
Proceedings of the Conference on Computational Natu-
ral Language Learning, 2005.
Jonathon Graehl, Kevin Knight, and Jon May. Training
tree transducers. Computational Linguistics, 34:391?
427, 2008.
Rohit J. Kate and Raymond J. Mooney. Using string-
kernels for learning semantic parsers. In Proc. of the
International Conference on Computational Linguistics
and the annual meeting of the Association for Compu-
tational Linguistics, 2006.
Kevin Knight and Jonathon Greahl. An overview of prob-
abilistic tree transducers for natural language process-
ing. In Proc. of the 6th International Conference on
Intelligent Text Processing and Computational Linguis-
tics, 2005.
Kenichi Kurihara and Taisuke Sato. Variational Bayesian
grammar induction for natural language. In Proc. of
the 8th International Colloquium on Grammatical In-
ference, 2006.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. Inducing probabilistic CCG
grammars from logical form with higher-order unifica-
tion. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, 2010.
Percy Liang, Michael I. Jordan, and Dan Klein. Learning
dependency-based compositional semantics. In Proc.
of the annual meeting of the Association for Computa-
tional Linguistics, 2011.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. A generative model for parsing natural language
to meaning representations. In Proc. of the Conference
on Empirical Methods in Natural Language Processing,
2008.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. The power of extended top-down tree
transducers. SIAM J. Comput., 39:410?430, June 2009.
Jon May and Kevin Knight. Tiburon: A weighted tree au-
tomata toolkit. In Proc. of the International Conference
on Implementation and Application of Automata, 2006.
Tom Minka. Estimating a Dirichlet distribution. Techni-
cal report, M.I.T., 2000.
W.C. Rounds. Mappings and grammars on trees. Mathe-
matical Systems Theory 4, pages 257?287, 1970.
Stuart M. Shieber. Synchronous grammars as tree trans-
ducers. In Proc. of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms,
2004.
J.W. Thatcher. Generalized sequential machine maps. J.
Comput. System Sci. 4, pages 339?367, 1970.
Yuk Wah Wong and Raymond J. Mooney. Learning for
semantic parsing with statistical machine translation. In
Proc. of Human Language Technology Conference and
the annual meeting of the North American Chapter of
the Association for Computational Linguistics, 2006.
Kenji Yamada and Kevin Knight. A syntax-based statis-
tical translation model. In Proc. of the annual meeting
of the Association for Computational Linguistics, 2001.
496
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883?891,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Social Information in Grounded Language Learning via
Grammatical Reductions
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Katherine Demuth
Department of Linguistics
Macquarie University
Sydney, Australia
Katherine.Demuth@MQ.edu.au
Michael Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@Stanford.edu
Abstract
This paper uses an unsupervised model of
grounded language acquisition to study the
role that social cues play in language acqui-
sition. The input to the model consists of (or-
thographically transcribed) child-directed ut-
terances accompanied by the set of objects
present in the non-linguistic context. Each
object is annotated by social cues, indicating
e.g., whether the caregiver is looking at or
touching the object. We show how to model
the task of inferring which objects are be-
ing talked about (and which words refer to
which objects) as standard grammatical in-
ference, and describe PCFG-based unigram
models and adaptor grammar-based colloca-
tion models for the task. Exploiting social
cues improves the performance of all mod-
els. Our models learn the relative importance
of each social cue jointly with word-object
mappings and collocation structure, consis-
tent with the idea that children could discover
the importance of particular social informa-
tion sources during word learning.
1 Introduction
From learning sounds to learning the meanings of
words, social interactions are extremely important
for children?s early language acquisition (Baldwin,
1993; Kuhl et al, 2003). For example, children who
engage in more joint attention (e.g. looking at par-
ticular objects together) with caregivers tend to learn
words faster (Carpenter et al, 1998). Yet compu-
tational or formal models of social interaction are
rare, and those that exist have rarely gone beyond
the stage of cue-weighting models. In order to study
the role that social cues play in language acquisition,
this paper presents a structured statistical model of
grounded learning that learns a mapping between
words and objects from a corpus of child-directed
utterances in a completely unsupervised fashion. It
exploits five different social cues, which indicate
which object (if any) the child is looking at, which
object the child is touching, etc. Our models learn
the salience of each social cue in establishing refer-
ence, relative to their co-occurrence with objects that
are not being referred to. Thus, this work is consis-
tent with a view of language acquisition in which
children learn to learn, discovering organizing prin-
ciples for how language is organized and used so-
cially (Baldwin, 1993; Hollich et al, 2000; Smith et
al., 2002).
We reduce the grounded learning task to a gram-
matical inference problem (Johnson et al, 2010;
Bo?rschinger et al, 2011). The strings presented to
our grammatical learner contain a prefix which en-
codes the objects and their social cues for each ut-
terance, and the rules of the grammar encode rela-
tionships between these objects and specific words.
These rules permit every object to map to every
word (including function words; i.e., there is no
?stop word? list), and the learning process decides
which of these rules will have a non-trivial proba-
bility (these encode the object-word mappings the
system has learned).
This reduction of grounded learning to grammat-
ical inference allows us to use standard grammati-
cal inference procedures to learn our models. Here
we use the adaptor grammar package described in
Johnson et al (2007) and Johnson and Goldwater
(2009) with ?out of the box? default settings; no
parameter tuning whatsoever was done. Adaptor
grammars are a framework for specifying hierarchi-
cal non-parametric models that has been previously
used to model language acquisition (Johnson, 2008).
883
Social cue Value
child.eyes objects child is looking at
child.hands objects child is touching
mom.eyes objects care-giver is looking at
mom.hands objects care-giver is touching
mom.point objects care-giver is pointing to
Figure 1: The 5 social cues in the Frank et al (to appear)
corpus. The value of a social cue for an utterance is a
subset of the available topics (i.e., the objects in the non-
linguistic context) of that utterance.
A semanticist might argue that our view of refer-
ential mapping is flawed: full noun phrases (e.g., the
dog), rather than nouns, refer to specific objects, and
nouns denote properties (e.g., dog denotes the prop-
erty of being a dog). Learning that a noun, e.g., dog,
is part of a phrase used to refer to a specific dog (say,
Fido) does not suffice to determine the noun?s mean-
ing: the noun could denote a specific breed of dog,
or animals in general. But learning word-object rela-
tionships is a plausible first step for any learner: it is
often only the contrast between learned relationships
and novel relationships that allows children to in-
duce super- or sub-ordinate mappings (Clark, 1987).
Nevertheless, in deference to such objections, we
call the object that a phrase containing a given noun
refers to the topic of that noun. (This is also appro-
priate, given that our models are specialisations of
topic models).
Our models are intended as an ?ideal learner? ap-
proach to early social language learning, attempt-
ing to weight the importance of social and structural
factors in the acquisition of word-object correspon-
dences. From this perspective, the primary goal is
to investigate the relationships between acquisition
tasks (Johnson, 2008; Johnson et al, 2010), looking
for synergies (areas of acquisition where attempting
two learning tasks jointly can provide gains in both)
as well as areas where information overlaps.
1.1 A training corpus for social cues
Our work here uses a corpus of child-directed
speech annotated with social cues, described in
Frank et al (to appear). The corpus consists
of 4,763 orthographically-transcribed utterances of
caregivers to their pre-linguistic children (ages 6, 12,
and 18 months) during home visits where children
played with a consistent set of toys. The sessions
were video-taped, and each utterance was annotated
with the five social cues described in Figure 1.
Each utterance in the corpus contains the follow-
ing information:
? the sequence of orthographic words uttered by
the care-giver,
? a set of available topics (i.e., objects in the non-
linguistic objects),
? the values of the social cues, and
? a set of intended topics, which the care-giver
refers to.
Figure 2 presents this information for an example ut-
terance. All of these but the intended topics are pro-
vided to our learning algorithms; the intended top-
ics are used to evaluate the output produced by our
learners.
Generally the intended topics consist of zero or
one elements from the available topics, but not al-
ways: it is possible for the caregiver to refer to two
objects in a single utterance, or to refer to an object
not in the current non-linguistic context (e.g., to a
toy that has been put away). There is a considerable
amount of anaphora in this corpus, which our mod-
els currently ignore.
Frank et al (to appear) give extensive details on
the corpus, including inter-annotator reliability in-
formation for all annotations, and provide detailed
statistical analyses of the relationships between the
various social cues, the available topics and the in-
tended topics. That paper also gives instructions on
obtaining the corpus.
1.2 Previous work
There is a growing body of work on the role of social
cues in language acquisition. The language acqui-
sition research community has long recognized the
importance of social cues for child language acqui-
sition (Baldwin, 1991; Carpenter et al, 1998; Kuhl
et al, 2003).
Siskind (1996) describes one of the first exam-
ples of a model that learns the relationship between
words and topics, albeit in a non-statistical frame-
work. Yu and Ballard (2007) describe an associative
learner that associates words with topics and that
exploits prosodic as well as social cues. The rela-
tive importance of the various social cues are spec-
ified a priori in their model (rather than learned, as
they are here), and unfortunately their training cor-
pus is not available. Frank et al (2008) describes a
Bayesian model that learns the relationship between
words and topics, but the version of their model that
included social cues presented a number of chal-
lenges for inference. The unigram model we de-
scribe below corresponds most closely to the Frank
884
.dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggie
Figure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where?s the
piggie?. Below that, we show the representation of this utterance that serves as the input to our models. The prefix (the
portion of the string before the ?##?) lists the available topics (i.e., the objects in the non-linguistic context) and their
associated social cues (the cues for the pig are child.eyes, mom.eyes and mom.hands, while the dog is not associated
with any social cues). The intended topic is the pig. The learner?s goals are to identify the utterance?s intended topic,
and which words in the utterance are associated with which topic.
Sentence
Topic.pig
T.None
.dog
NotTopical.child.eyes
NotTopical.child.hands
NotTopical.mom.eyes
NotTopical.mom.hands
NotTopical.mom.point
#
Topic.pig
T.pig
.pig
Topical.child.eyes
child.eyes
Topical.child.hands
Topical.mom.eyes
Topical.mom.hands
mom.hands
Topical.mom.point
#
Topic.None
##
Words.pig
Word.None
wheres
Words.pig
Word.None
the
Words.pig
Word.pig
piggie
Figure 3: Sample parse generated by the Unigram PCFG. Nodes coloured red show how the ?pig? topic is propagated
from the prefix (before the ?##? separator) into the utterance. The social cues associated with each object are generated
either from a ?Topical? or a ?NotTopical? nonterminal, depending on whether the corresponding object is topical or
not.
885
et al model. Johnson et al (2010) reduces grounded
learning to grammatical inference for adaptor gram-
mars and shows how it can be used to perform word
segmentation as well as learning word-topic rela-
tionships, but their model does not take social cues
into account.
2 Reducing grounded learning with social
cues to grammatical inference
This section explains how we reduce ground learn-
ing problems with social cues to grammatical in-
ference problems, which lets us apply a wide vari-
ety of grammatical inference algorithms to grounded
learning problems. An advantage of reducing
grounded learning to grammatical inference is that
it suggests new ways to generalise grounded learn-
ing models; we explore three such generalisations
here. The main challenge in this reduction is finding
a way of expressing the non-linguistic information
as part of the strings that serve as the grammatical in-
ference procedure?s input. Here we encode the non-
linguistic information in a ?prefix? to each utterance
as shown in Figure 2, and devise a grammar such
that inference for the grammar corresponds to learn-
ing the word-topic relationships and the salience of
the social cues for grounded learning.
All our models associate each utterance with zero
or one topics (this means we cannot correctly anal-
yse utterances with more than one intended topic).
We analyse an utterance associated with zero topics
as having the special topic None, so we can assume
that every utterance has exactly one topic. All our
grammars generate strings of the form shown in Fig-
ure 2, and they do so by parsing the prefix and the
words of the utterance separately; the top-level rules
of the grammar force the same topic to be associated
with both the prefix and the words of the utterance
(see Figure 3).
2.1 Topic models and the unigram PCFG
As Johnson et al (2010) observe, this kind of
grounded learning can be viewed as a specialised
kind of topic inference in a topic model, where the
utterance topic is constrained by the available ob-
jects (possible topics). We exploit this observation
here using a reduction based on the reduction of
LDA topic models to PCFGs proposed by Johnson
(2010). This leads to our first model, the unigram
grammar, which is a PCFG.1
1In fact, the unigram grammar is equivalent to a HMM,
but the PCFG parameterisation makes clear the relationship
Sentence? Topict Wordst ?t ? T
?
TopicNone ? ##
Topict ? Tt TopicNone ?t ? T
?
Topict ? TNone Topict ?t ? T
Tt ? t Topicalc1 ?t ? T
Topicalci ? (ci) Topicalci+1 i = 1, . . . , `? 1
Topicalc` ? (c`) #
TNone ? t NotTopicalc1 ?t ? T
NotTopicalci ? (ci) NotTopicalci+1 i = 1, . . . , `? 1
NotTopicalc` ? (c`) #
Wordst ?WordNone (Wordst) ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T
Wordt ? w ?t ? T ?, w ?W
Figure 4: The rule schema that generate the unigram
PCFG. Here (c1, . . . , c`) is an ordered list of the so-
cial cues, T is the set of all non-None available topics,
T ? = T ? {None}, and W is the set of words appearing
in the utterances. Parentheses indicate optionality.
Figure 4 presents the rules of the unigram gram-
mar. This grammar has two major parts. The rules
expanding the Topict nonterminals ensure that the
social cues for the available topic t are parsed un-
der the Topical nonterminals. All other available
topics are parsed under TNone nonterminals, so their
social cues are parsed under NotTopical nontermi-
nals. The rules expanding these non-terminals are
specifically designed so that the generation of the so-
cial cues corresponds to a series of binary decisions
about each social cue. For example, the probability
of the rule
Topicalchild.eyes ? .child.eyes Topicalchild.hands
is the probability of an object that is an utterance
topic occuring with the child.eyes social cue. By es-
timating the probabilities of these rules, the model
effectively learns the probability of each social cue
being associated with a Topical or a NotTopical
available topic, respectively.
The nonterminals Wordst expand to a sequence
of Wordt and WordNone nonterminals, each of
which can expand to any word whatsoever. In prac-
tice Wordt will expand to those words most strongly
associated with topic t, while WordNone will expand
to those words not associated with any topic.
between grounded learning and estimation of grammar rule
weights.
886
Sentence? Topict Collocst ?t ? T
?
Collocst ? Colloct (Collocst) ?t ? T ?
Collocst ? CollocNone (Collocst) ?t ? T
Colloct ?Wordst ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T ?
Wordst ?WordNone (Wordst) ?t ? T
Wordt ?Word ?t ? T ?
Word? w ?w ?W
Figure 5: The rule schema that generate the collocation
adaptor grammar. Adapted nonterminals are indicated via
underlining. Here T is the set of all non-None available
topics, T ? = T ? {None}, and W is the set of words ap-
pearing in the utterances. The rules expanding the Topict
nonterminals are exactly as in unigram PCFG.
2.2 Adaptor grammars
Our other grounded learning models are based on
reductions of grounded learning to adaptor gram-
mar inference problems. Adaptor grammars are a
framework for stating a variety of Bayesian non-
parametric models defined in terms of a hierarchy of
Pitman-Yor Processes: see Johnson et al (2007) for
a formal description. Informally, an adaptor gram-
mar is specified by a set of rules just as in a PCFG,
plus a set of adapted nonterminals. The set of
trees generated by an adaptor grammar is the same
as the set of trees generated by a PCFG with the
same rules, but the generative process differs. Non-
adapted nonterminals in an adaptor grammar expand
just as they do in a PCFG: the probability of choos-
ing a rule is specified by its probability. However,
the expansion of an adapted nonterminal depends on
how it expanded in previous derivations. An adapted
nonterminal can directly expand to a subtree with
probability proportional to the number of times that
subtree has been previously generated; it can also
?back off? to expand using a grammar rule, just as
in a PCFG, with probability proportional to a con-
stant.2
Thus an adaptor grammar can be viewed as
caching each tree generated by each adapted non-
terminal, and regenerating it with probability pro-
portional to the number of times it was previously
generated (with some probability mass reserved to
generate ?new? trees). This enables adaptor gram-
2This is a description of Chinese Restaurant Processes,
which are the predictive distributions for Dirichlet Processes.
Our adaptor grammars are actually based on the more general
Pitman-Yor Processes, as described in Johnson and Goldwater
(2009).
Sentence
Topic.pig
...
Collocs.pig
Colloc.None
Words.None
Word.None
Word
wheres
Collocs.pig
Colloc.pig
Words.pig
Word.None
Word
the
Words.pig
Word.pig
Word
piggie
Figure 6: Sample parse generated by the collocation
adaptor grammar. The adapted nonterminals Colloct and
Wordt are shown underlined; the subtrees they dominate
are ?cached? by the adaptor grammar. The prefix (not
shown here) is parsed exactly as in the Unigram PCFG.
mars to generalise over subtrees of arbitrary size.
Generic software is available for adaptor grammar
inference, based either on Variational Bayes (Cohen
et al, 2010) or Markov Chain Monte Carlo (Johnson
and Goldwater, 2009). We used the latter software
because it is capable of performing hyper-parameter
inference for the PCFG rule probabilities and the
Pitman-Yor Process parameters. We used the ?out-
of-the-box? settings for this software, i.e., uniform
priors on all PCFG rule parameters, a Beta(2, 1)
prior on the Pitman-Yor a parameters and a ?vague?
Gamma(100, 0.01) prior on the Pitman-Yor b pa-
rameters. (Presumably performance could be im-
proved if the priors were tuned, but we did not ex-
plore this here).
Here we explore a simple ?collocation? extension
to the unigram PCFG which associates multiword
collocations, rather than individual words, with top-
ics. Hardisty et al (2010) showed that this signifi-
cantly improved performance in a sentiment analy-
sis task.
The collocation adaptor grammar in Figure 5 gen-
erates the words of the utterance as a sequence of
collocations, each of which is a sequence of words.
Each collocation is either associated with the sen-
tence topic or with the None topic, just like words in
the unigram model. Figure 6 shows a sample parse
generated by the collocation adaptor grammar.
We also experimented with a variant of the uni-
gram and collocation grammars in which the topic-
specific word distributions Wordt for each t ? T
887
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.7381
unigram? none 0.3261 0.3767 0.3054 0.4914 0.1893 0.1131 0.5811 0.1167 0.06583 0.5122
unigram? all 0.5117 0.6106 0.4986 0.7875 0.2846 0.1693 0.891 0.1684 0.09402 0.8049
colloc? none 0.5238 0.3419 0.3844 0.3078 0.2551 0.1732 0.4843 0.2162 0.1495 0.3902
colloc? all 0.6492 0.6034 0.6664 0.5514 0.3981 0.2613 0.8354 0.3375 0.2269 0.6585
Figure 7: Utterance topic, word topic and lexicon results for all models, on data with and without social cues. The
results for the variant models, in which Wordt nonterminals expand via WordNone, are shown under unigram? and
colloc?. Utterance topic shows how well the model discovered the intended topics at the utterance level, word topic
shows how well the model associates word tokens with topics, and lexicon shows how well the topic most frequently
associated with a word type matches an external word-topic dictionary. In this figure and below, ?colloc? abbreviates
?collocation?, ?acc.? abbreviates ?accuracy?, ?prec.? abbreviates ?precision? and ?rec.? abbreviates ?recall?.
(the set of non-None available topics) expand via
WordNone non-terminals. That is, in the variant
grammars topical words are generated with the fol-
lowing rule schema:
Wordt ?WordNone ?t ? T
WordNone ?Word
Word? w ?w ?W
In these variant grammars, the WordNone nontermi-
nal generates all the words of the language, so it de-
fines a generic ?background? distribution over all the
words, rather than just the nontopical words. An ef-
fect of this is that the variant grammars tend to iden-
tify fewer words as topical.
3 Experimental evaluation
We performed grammatical inference using the
adaptor grammar software described in Johnson and
Goldwater (2009).3 All experiments involved 4 runs
of 5,000 samples each, of which the first 2,500 were
discarded for ?burn-in?.4 From these samples we
extracted the modal (i.e., most frequent) analysis,
3Because adaptor grammars are a generalisation of PCFGs,
we could use the adaptor grammar software to estimate the un-
igram model.
4We made no effort to optimise the computation, but it
seems the samplers actually stabilised after around a hundred
iterations, so it was probably not necessary to sample so exten-
sively. We estimated the error in our results by running our most
complex model (the colloc? model with all social cues) 20 times
(i.e., 20?8 chains for 5,000 iterations) so we could compute the
variance of each of the evaluation scores (it is reasonable to as-
sume that the simpler models will have smaller variance). The
standard deviation of all utterance topic and word topic mea-
sures is between 0.005 and 0.01; the standard deviation for lex-
icon f-score is 0.02, lexicon precision is 0.01 and lexicon recall
is 0.03. The adaptor grammar software uses a sentence-wise
which we evaluated as described below. The results
of evaluating each model on the corpus with social
cues, and on another corpus identical except that the
social cues have been removed, are presented in Fig-
ure 7.
Each model was evaluated on each corpus as fol-
lows. First, we extracted the utterance?s topic from
the modal parse (this can be read off the Topict
nodes), and compared this to the intended topics an-
notated in the corpus. The frequency with which
the models? predicted topics exactly matches the
intended topics is given under ?utterance topic ac-
curacy?; the f-score, precision and recall of each
model?s topic predictions are also given in the table.
Because our models all associate word tokens
with topics, we can also evaluate the accuracy with
which word tokens are associated with topics. We
constructed a small dictionary which identifies the
words that can be used as the head of a phrase to
refer to the topical objects (e.g., the dictionary in-
dicates that dog, doggie and puppy name the topi-
cal object DOG). Our dictionary is relatively conser-
vative; between one and eight words are associated
with each topic. We scored the topic label on each
word token in our corpus as follows. A topic label is
scored as correct if it is given in our dictionary and
the topic is one of the intended topics for the utter-
ance. The ?word topic? entries in Figure 7 give the
results of this evaluation.
blocked sampler, so it requires fewer iterations than a point-
wise sampler. We used 5,000 iterations because this is the soft-
ware?s default setting; evaluating the trace output suggests it
only takes several hundred iterations to ?burn in?. However, we
ran 8 chains for 25,000 iterations of the colloc? model; as ex-
pected the results of this run are within two standard deviations
of the results reported above.
888
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram +child.eyes 0.4573 0.5725 0.4559 0.7694 0.2891 0.1724 0.8951 0.1362 0.07415 0.8333
unigram +child.hands 0.3399 0.4011 0.3246 0.5247 0.2008 0.121 0.5892 0.09705 0.05324 0.5476
unigram +mom.eyes 0.338 0.4023 0.3234 0.5322 0.1992 0.1198 0.5908 0.09664 0.053 0.5476
unigram +mom.hands 0.3563 0.4279 0.3437 0.5667 0.1984 0.1191 0.5948 0.09959 0.05455 0.5714
unigram +mom.point 0.3063 0.3548 0.285 0.4698 0.1806 0.1086 0.5359 0.09224 0.05057 0.5238
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc +child.eyes 0.5159 0.5006 0.4652 0.542 0.351 0.2309 0.7312 0.1432 0.07989 0.6905
colloc +child.hands 0.4827 0.4275 0.3999 0.4592 0.2897 0.1913 0.5964 0.1192 0.06686 0.5476
colloc +mom.eyes 0.4697 0.4171 0.3869 0.4525 0.2708 0.1781 0.5642 0.1013 0.05666 0.4762
colloc +mom.hands 0.4747 0.4251 0.3942 0.4612 0.274 0.1806 0.5666 0.09548 0.05337 0.4524
colloc +mom.point 0.4228 0.3378 0.3151 0.3639 0.2575 0.1716 0.5157 0.09278 0.05202 0.4286
Figure 8: Effect of using just one social cue on the experimental results for the unigram and collocation models. The
?importance? of a social cue can be quantified by the degree to which the model?s evaluation score improves when
using a corpus containing that social cue relative to its evaluation score when using a corpus without any social cues.
The most important social cue is the one which causes performance to improve the most.
Finally, we extracted a lexicon from the parsed
corpus produced by each model. We counted how
often each word type was associated with each topic
in our sampler?s output (including the None topic),
and assigned the word to its most frequent topic.
The ?lexicon? entries in Figure 7 show how well
the entries in these lexicons match the entries in the
manually-constructed dictionary discussed above.
There are 10 different evaluation scores, and no
model dominates in all of them. However, the top-
scoring result in every evaluation is always for a
model trained using social cues, demonstrating the
importance of these social cues. The variant colloca-
tion model (trained on data with social cues) was the
top-scoring model on four evaluation scores, which
is more than any other model.
One striking thing about this evaluation is that the
recall scores are all much higher than the precision
scores, for each evaluation. This indicates that all
of the models, especially the unigram model, are la-
belling too many words as topical. This is perhaps
not too surprising: because our models completely
lack any notion of syntactic structure and simply
model the association between words and topics,
they label many non-nouns with topics (e.g., woof
is typically labelled with the topic DOG).
3.1 Evaluating the importance of social cues
It is scientifically interesting to be able to evalu-
ate the importance of each of the social cues to
grounded learning. One way to do this is to study
the effect of adding or removing social cues from
the corpus on the ability of our models to perform
grounded learning. An important social cue should
have a large impact on our models? performance; an
unimportant cue should have little or no impact.
Figure 8 compares the performance of the uni-
gram and collocation models on corpora containing
a single social cue to their performance on the cor-
pus without any social cues, while Figure 9 com-
pares the performance of these models on corpora
containing all but one social cue to the corpus con-
taining all of the social cues. In both of these evalua-
tions, with respect to all 10 evaluation measures, the
child.eyes social cue had the most impact on model
performance.
Why would the child?s own gaze be more impor-
tant than the caregiver?s? Perhaps caregivers are fol-
lowing in, i.e., talking about objects that their chil-
dren are interested in (Baldwin, 1991). However, an-
other possible explanation is that this result is due to
the general continuity of conversational topics over
time. Frank et al (to appear) show that for the cur-
rent corpus, the topic of the preceding utterance is
very likely to be the topic of the current one also.
Thus, the child?s eyes might be a good predictor be-
cause they reflect the fact that the child?s attention
has been drawn to an object by previous utterances.
Notice that these two possible explanations of the
importance of the child.eyes cue are diametrically
opposed; the first explanation claims that the cue is
important because the child is driving the discourse,
while the second explanation claims that the cue is
important because the child?s gaze follows the topic
of the caregiver?s previous utterance. This sort of
question about causal relationships in conversations
may be very difficult to answer using standard de-
scriptive techniques, but it may be an interesting av-
889
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
unigram ?child.eyes 0.3836 0.4659 0.3738 0.6184 0.2149 0.1286 0.6546 0.1111 0.06089 0.6341
unigram ?child.hands 0.4907 0.6063 0.4863 0.8051 0.296 0.1769 0.9056 0.1525 0.08353 0.878
unigram ?mom.eyes 0.4799 0.5974 0.4768 0.7996 0.2898 0.1727 0.9007 0.1551 0.08486 0.9024
unigram ?mom.hands 0.4871 0.5996 0.4815 0.7945 0.2925 0.1746 0.8991 0.1561 0.08545 0.9024
unigram ?mom.point 0.4875 0.6033 0.4841 0.8004 0.2934 0.1752 0.9007 0.1558 0.08525 0.9024
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.738
colloc ?child.eyes 0.5604 0.5746 0.529 0.6286 0.39 0.2561 0.8176 0.1534 0.08642 0.6829
colloc ?child.hands 0.5849 0.6 0.5609 0.6451 0.4145 0.273 0.8612 0.1662 0.09375 0.7317
colloc ?mom.eyes 0.5709 0.5829 0.5457 0.6255 0.4036 0.2655 0.8418 0.1662 0.09375 0.7317
colloc ?mom.hands 0.5795 0.5935 0.5571 0.6349 0.4038 0.2653 0.8442 0.1788 0.1009 0.7805
colloc ?mom.point 0.5851 0.6006 0.5607 0.6467 0.4097 0.2685 0.8644 0.1742 0.09841 0.7561
Figure 9: Effect of using all but one social cue on the experimental results for the unigram and collocation models.
The ?importance? of a social cue can be quantified by the degree to which the model?s evaluation score degrades when
that just social cue is removed from the corpus, relative to its evaluation score when using a corpus without all social
cues. The most important social cue is the one which causes performance to degrade the most.
enue for future investigation using more structured
models such as those proposed here.5
4 Conclusion and future work
This paper presented four different grounded learn-
ing models that exploit social cues. These models
are all expressed via reductions to grammatical in-
ference problems, so standard ?off the shelf? gram-
matical inference tools can be used to learn them.
Here we used the same adaptor grammar software
tools to learn all these models, so we can be rel-
atively certain that any differences we observe are
due to differences in the models, rather than quirks
in the software.
Because the adaptor grammar software performs
full Bayesian inference, including for model param-
eters, an unusual feature of our models is that we
did not need to perform any parameter tuning what-
soever. This feature is particularly interesting with
respect to the parameters on social cues. Psycholog-
ical proposals have suggested that children may dis-
cover that particular social cues help in establishing
reference (Baldwin, 1993; Hollich et al, 2000), but
prior modeling work has often assumed that cues,
cue weights, or both are prespecified. In contrast, the
models described here could in principle discover a
wide range of different social conventions.
5A reviewer suggested that we can test whether child.eyes
effectively provides the same information as the previous topic
by adding the previous topic as a (pseudo-) social cue. We tried
this, and child.eyes and previous.topic do in fact seem to convey
very similar information: e.g., the model with previous.topic
and without child.eyes scores essentially the same as the model
with all social cues.
Our work instantiates the strategy of investigating
the structure of children?s learning environment us-
ing ?ideal learner? models. We used our models to
investigate scientific questions about the role of so-
cial cues in grounded language learning. Because
the performance of all four models studied in this
paper improve dramatically when provided with so-
cial cues in all ten evaluation metrics, this paper pro-
vides strong support for the view that social cues are
a crucial information source for grounded language
learning.
We also showed that the importance of the differ-
ent social cues in grounded language learning can
be evaluated using ?add one cue? and ?subtract one
cue? methodologies. According to both of these, the
child.eyes cue is the most important of the five so-
cial cues studied here. There are at least two pos-
sible reasons for this: the caregiver?s topic could
be determined by the child?s gaze, or the child.eyes
cue could be providing our models with information
about the topic of the previous utterance.
Incorporating topic continuity and anaphoric de-
pendencies into our models would be likely to im-
prove performance. This improvement might also
help us distinguish the two hypotheses about the
child.eyes cue. If the child.eyes cue is just provid-
ing indirect information about topic continuity, then
the importance of the child.eyes cue should decrease
when we incorporate topic continuity into our mod-
els. But if the child?s gaze is in fact determining the
care-giver?s topic, then child.eyes should remain a
strong cue even when anaphoric dependencies and
topic continuity are incorporated into our models.
890
Acknowledgements
This research was supported under the Australian
Research Council?s Discovery Projects funding
scheme (project number DP110102506).
References
Dare A. Baldwin. 1991. Infants? contribution to the
achievement of joint reference. Child Development,
62(5):874?890.
Dare A. Baldwin. 1993. Infants? ability to consult the
speaker for clues to word reference. Journal of Child
Language, 20:395?395.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
M. Carpenter, K. Nagell, M. Tomasello, G. Butterworth,
and C. Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15 months
of age. Monographs of the society for research in child
development.
E.V. Clark. 1987. The principle of contrast: A constraint
on language acquisition. Mechanisms of language ac-
quisition, 1:33.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564?
572, Los Angeles, California, June. Association for
Computational Linguistics.
Michael Frank, Noah Goodman, and Joshua Tenenbaum.
2008. A Bayesian framework for cross-situational
word-learning. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 457?464, Cambridge,
MA. MIT Press.
Michael C. Frank, Joshua Tenenbaum, and Anne Fernald.
to appear. Social and discourse contributions to the
determination of reference in cross-situational word
learning. Language, Learning, and Development.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
G.J. Hollich, K. Hirsh-Pasek, and R. Golinkoff. 2000.
Breaking the language barrier: An emergentist coali-
tion model for the origins of word learning. Mono-
graphs of the Society for Research in Child Develop-
ment.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, pages 398?406, Columbus, Ohio. Association for
Computational Linguistics.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu.
2003. Foreign-language experience in infancy: Effects
of short-term exposure and social interaction on pho-
netic learning. Proceedings of the National Academy
of Sciences USA, 100(15):9096?9101.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61(1-2):39?91.
L.B. Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe,
and L. Samuelson. 2002. Object name learning pro-
vides on-the-job training for attention. Psychological
Science, 13(1):13.
Chen Yu and Dana H Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
891
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 85?89,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Using Rejuvenation to Improve Particle Filtering for Bayesian Word
Segmentation
Benjamin Bo?rschinger*?
benjamin.borschinger@mq.edu.au
Mark Johnson*
mark.johnson@mq.edu.au
*Department of Computing
Macquarie University
Sydney, Australia
?Department of Computational Linguistics
Heidelberg University
Heidelberg, Germany
Abstract
We present a novel extension to a recently pro-
posed incremental learning algorithm for the
word segmentation problem originally intro-
duced in Goldwater (2006). By adding rejuve-
nation to a particle filter, we are able to consid-
erably improve its performance, both in terms
of finding higher probability and higher accu-
racy solutions.
1 Introduction
The goal of word segmentation is to segment a
stream of segments, e.g. characters or phonemes,
into words. For example, given the sequence
?youwanttoseethebook?, the goal is to recover the
segmented string ?you want to see the book?. The
models introduced in Goldwater (2006) solve this
problem in a fully unsupervised way by defining a
generative process for word sequences, making use
of the Dirichlet Process (DP) prior.
Until recently, the only inference algorithm
applied to these models were batch Markov
Chain Monte Carlo (MCMC) sampling algorithms.
Bo?rschinger and Johnson (2011) proposed a strictly
incremental particle filter algorithm that, however,
performed considerably worse than the standard
batch algorithms, in particular for the Bigram model.
We extend that algorithm by adding rejuvenation
steps and show that this leads to considerable im-
provements, thus strengthening the case for particle
filters as another tool for Bayesian inference in com-
putational linguistics.
The rest of the paper is structured as follows. Sec-
tions 2 and 3 provide the relevant background about
word segmentation and previous work. Section 4 de-
scribes our algorithm. Section 5 reports on an ex-
perimental evaluation of our algorithm, and section
6 concludes and suggests possible directions for fu-
ture research.
2 Model description
The Unigram model assumes that words in a se-
quence are generated independently whereas the Bi-
gram model models dependencies between adjacent
words. This has been shown by Goldwater (2006) to
markedly improve segmentation performance. We
perform experiments on both models but, for rea-
sons of space, only give an overview of the Unigram
model, referring the reader to the original papers for
more detailed descriptions. (Goldwater, 2006; Gold-
water et al, 2009)
A sequence of words or utterance is generated by
making independent draws from a discrete distribu-
tion over words, G. As neither the actual ?true?
words nor their number is known in advance, G is
modelled as a draw from a DP. A DP is parametrized
by a base distribution P0 and a concentration param-
eter ?. Here, P0 assigns a probability to every possi-
ble word, i.e. sequence of segments, and ? controls
the sparsity of G; the smaller ?, the sparser G tends
to be.
To computationally cope with the unbounded
nature of draws from a DP, they can be ?inte-
grated out?, yielding the Chinese Restaurant Process
(CRP), an infinitely exchangeable conditional pre-
dictive distribution. The CRP also provides an in-
tuitive generative story for the observed data. Each
generated word token corresponds to a customer sit-
85
ting at one of the unboundedly many tables in an
imaginary Chinese restaurant. Customers choose
their seats sequentially, and they sit either at an al-
ready occupied or a new table. The former hap-
pens with probability proportional to the number of
customers already sitting at a table and corresponds
to generating one more token of the word type all
customers at a table instantiate. The latter happens
with probability proportional to ? and corresponds
to generating a token by sampling from the base dis-
tribution, thus also determining the type for all po-
tential future customers at the new table.
Given this generative process, word segmentation
can be cast as a probabilistic inference problem. For
a fixed input, in our case a sequence of phonemes,
our goal is to determine the posterior distribution
over segmentations. This is usually infeasible to do
exactly, leading to the use of approximate inference
methods.
3 Previous Work
The ?standard? inference algorithms for the Uni-
gram and Bigram model are MCMC samplers that
are batch algorithms making multiple iterations over
the data to non-deterministically explore the state
space of possible segmentations. If an MCMC algo-
rithm runs long enough, the probability of it visiting
any specific segmentation is the probability of that
segmentation under the target posterior distribution,
here, the distribution over segmentations given the
observed data.
The MCMC algorithm of Goldwater et al (2009)
is a Gibbs sampler that makes very small moves
through the state space by changing individual word
boundaries one at a time. An alternative MCMC al-
gorithm that samples segmentations for entire utter-
ances was proposed by Mochihashi et al (2009).
Below, we correct a minor error in the algorithm, re-
casting it as a Metropolis-within-Gibbs sampler.
Moving beyond MCMC algorithms, Pearl et al
(2010) describe an algorithm that can be seen as
a degenerate limiting case of a particle filter with
only one particle. Their Dynamic Programming
Sampling algorithm makes a single pass through the
data, processing one utterance at a time by sampling
a segmentation given the choices made for all pre-
vious utterances. While their algorithm comes with
no guarantee that it converges on the intended pos-
terior distribution, Bo?rschinger and Johnson (2011)
showed how to construct a particle filter that is
asymptotically correct, although experiments sug-
gested that the number of particles required for good
performance is impractically large.
This paper shows how their algorithm can be im-
proved by adding rejuvenation steps, which we will
describe in the next section.
4 A Particle Filter with Rejuvenation
The core idea of a particle filter is to sequentially
approximate a target posterior distribution P by N
weighted point samples or ?particles?. Each parti-
cle is updated one observation at a time, exploiting
the insight that Bayes? Theorem can be applied re-
cursively, as illustratively shown for the case of cal-
culating the posterior probability of a hypothesis H
given two observations O1 and O2:
P (H|O1) ? P (O1|H)P (H) (1)
P (H|O1, O2) ? P (O2|H)P (H|O1) (2)
If the observations are conditionally independent
given the hypothesis, one can simply take the poste-
rior at time step t as the prior for the posterior update
at time step t+ 1.
Here, each particle corresponds to a specific seg-
mentation of the data observed so far, or more pre-
cisely, the specific CRP seating of word tokens in
this segmentation; we refer to this as its history. Its
weight indicates how well a particle is supported by
the data, and each observation corresponds to an un-
segmented utterance. With this, the basic particle
filter algorithm can be described as follows: Begin
with N ?empty? particles. To get the particles at time
t+1 from the particles at time t, update each particle
using the observation at time t+1 as follows: sample
a segmentation for this observation, given the parti-
cle?s history, then add the words in this segmentation
to that history. After each particle has been updated,
their weights are adjusted to reflect how well they
are now supported by the observations. The set of
updated and reweighted particles constitutes the ap-
proximation of the posterior at time t+ 1.
To overcome the problem of degeneracy (the sit-
uation where only very few particles have non-
negligible weights), Bo?rschinger and Johnson use
86
resampling; basically, high-probability particles are
permitted to have multiple descendants that can
replace low-probability particles. For reasons of
space, we refer the reader to Bo?rschinger and John-
son (2011) for the details of these steps.
While necessary to address the degeneracy prob-
lem, resampling leads to a loss of sample diversity;
very quickly, almost all particles have an identical
history, descending from only a small number of
(previously) high probability particles. With a strict
online learning constraint, this can only be counter-
acted by using an extremely large number of parti-
cles. An alternative strategy which we explore here
is to use rejuvenation; the core idea is to restore
sample diversity after each resampling step by per-
forming MCMC resampling steps on each particle?s
history, thus leading to particles with different his-
tories in each generation, even if they all have the
same parent. (e.g., Canini et al (2009)) This makes
it necessary to store previously processed observa-
tions and thus no longer qualifies as online learn-
ing in a strict sense, but it still yields an incremental
algorithm that learns as the observations arrive se-
quentially, instead of delaying learning until all ob-
servations are available.
In our setting, rejuvenation works as follows. Af-
ter each resampling step, for each particle the algo-
rithm performs a fixed number of the following re-
juvenation steps:
1. randomly choose a previously observed utter-
ance
2. resample the segmentation for this utterance
and update the particle accordingly
For the resampling step, we use Mochihashi et al
(2009)?s algorithm to efficiently sample segmenta-
tions for an unsegmented utterance o, given a se-
quence of n previously observed words W1:n. As
the CRP is exchangeable, during resampling we can
treat every utterance as if it were the last, making
it possible to use this algorithm for any utterance,
irrespective of its actual position in the data. Cru-
cially, however, the distribution over segmentations
that this algorithm samples from is not the true pos-
terior distribution P (?|o, ?,W1:n) as defined by the
CRP, but a slightly different proposal distribution
Q(?|o, ?,W1:n) that does not take into account the
intra-sentential word dependencies for a segmenta-
tion of o. It is precisely because we ignore these de-
pendencies that an efficient dynamic programming
algorithm is possible, but because Q is different
from the target conditional distribution P , our algo-
rithm that uses Q instead of P needs to correct for
this. In a particle filter, this is done when the par-
ticle weights are calculated (Bo?rschinger and John-
son, 2011). For an MCMC algorithm or our rejuve-
nation step, a Metropolis-Hastings accept/reject step
is required, as described in detail by Johnson et al
(2007) in the context of grammatical inference.1
In our case, during rejuvenation an utterance u
with current segmentation s is reanalyzed as fol-
lows:
? remove all the words contained in s from the
particle?s current state L, yielding state L?
? sample a proposal segmentation s? for u from
Q(?|u, L?, ?), using Mochihashi et al (2009)?s
dynamic programming algorithm
? calculate m = min{1, P (s
?|L?,?)Q(s|L?,?)
P (s|L?,?)Q(s?|L?,?)}
? with probability m, accept the new sample and
update L? accordingly, else keep the original
segmentation and set the particle?s state back
to L
This completes the description of our extension to
the algorithm. The remainder of the paper empiri-
cally evaluates the particle filter with rejuvenation.
5 Experiments
We compare the performance of a batch Metropolis-
Hastings sampler for the Unigram and Bigram
model with that of particle filter learners both with
and without rejuvenation, as described in the previ-
ous section. For the batch samplers, we use simu-
lated annealing to facilitate the finding of high prob-
ability solutions, and for the particle filters, we com-
pare the performance of a ?degenerate? 1-particle
learner with a 16-particle learner in the rejuvenation
setting.
To get an impression of the contribution of par-
ticle number and rejuvenation steps, we compare
1Because Mochihashi et al (2009)?s algorithm samples di-
rectly from the proposal distribution without the accept-reject
step, it is not actually sampling from the intended posterior dis-
tribution. Because Q approaches the true conditional distribu-
tion as the size of the training data increases, however, there
may be almost no noticeable difference between using and not
using the accept/reject step, though strictly speaking, it is re-
quired to guarantee convergence to the the target posterior.
87
Unigram Bigram
TF logProb TF logProb
MHS 50.39 -196.74 70.93 -237.24
PF1 55.82 -248.21 49.43 -265.40
PF16 62.34 -239.22 50.14 -262.34
PF1000 64.11 -234.87 57.88 -254.17
PF1,100 63.17 -245.32 66.88 -257.65
PF16,100 68.05 -235.71 70.05 -251.66
PF1,1600 77.06 -228.79 74.47 -249.78
Table 1: Results for both the Unigram and the Bigram
model. MHS is a Metropolis-Hastings batch sampler.
PFx is a particle filter with x particles and no rejuve-
nation. PFx,s is a particle filter with x particles and s
rejuvenation steps. TF is token f-score, logProb is the
log-probability (?103) of the training-data at the end of
learning. Less negative logProb indicates a better solu-
tion according to the model, higher TF indicates a better
quality segmentation. All results are averaged across 4
runs. Results for the 1000 particle setting are taken from
Bo?rschinger and Johnson (2011).
the 16-particle learner with rejuvenation with a 1-
particle learner that performs 16 times as many re-
juvenation samples. For comparison, we also cite
previous results for the 1000-particle learners with-
out rejuvenation reported in Bo?rschinger and John-
son (2011), using their choice of parameters to allow
for a direct comparison: ? = 20 for the Unigram
model, ?0 = 3000, ?1 = 100 for the Bigram model,
and we use their base-distribution which differs from
the one described in Goldwater et al (2009) in that it
doesn?t assume a uniform distribution over segments
in the base-distribution but puts a Dirichlet Prior on
it.
We apply each learner to the Bernstein-Ratner
corpus (Brent, 1999) that is standardly used in
the word segmentation literature, which consists
of 9790 unsegmented and phonemically transcribed
child-directed speech utterances. We evaluate each
algorithm in two ways: inference performance, for
which the final log-probability of the training data
is the criterion, and segmentation performance, for
which we consider token f-score to be the best mea-
sure, since it indicates how well the actual word to-
kens in the data are recovered.Note that these two
measures can diverge, as previously documented for
the Unigram model (Goldwater, 2006) and, less so,
for the Bigram model (Pearl et al, 2010). Table 1
gives the results for our experiments.
For both models, adding rejuvenation always
improves performance markedly as compared to
the corresponding run without rejuvenation both in
terms of log-probability and segmentation f-score.
Note in particular that for the Bigram model, us-
ing 16 particles with 100 rejuvenation steps leads to
an improvement in token f-score of more than 10%
points over 1000 particles without rejuvenation.
Comparing the 1-particle learner with 1600 reju-
venation steps to the 16-particle learner with 100 re-
juvenation steps, for both models the former outper-
forms the latter in both log-probability and token f-
score. This suggests that if one has to trade-off par-
ticle number against rejuvenation steps, one may be
better off favouring the latter.
Despite the dramatic improvement over not us-
ing rejuvenation, there is still a considerable gap
between all the incremental learners and the batch
sampling algorithm in terms of log-probability. A
similar observation was made by Johnson and Gold-
water (2009) for incremental initialisation in word
segmentation using adaptor grammars. Their batch
sampler converged on higher token f-score but lower
probability solutions in some settings when initial-
ized in an incremental fashion as opposed to ran-
domly. We agree with their suggestion that this may
be due to the ?greedy? character of an incremental
learner.
6 Conclusion and outlook
We have shown that adding rejuvenation to a par-
ticle filter improves segmentation scores and log-
probabilities. Yet, our incremental algorithm still
finds lower probability but high quality token f-
scores compared to its batch counterpart. While
in principle, increasing the number of rejuvenation
steps and particles will make this gap smaller and
smaller, we believe the existence of the gap to be
interesting in its own right, suggesting a general dif-
ference in learning behaviour between batch and in-
cremental learners, especially given the similar re-
sults in Johnson and Goldwater (2009). Further
research into incremental learning algorithms may
help us better understand how processing limitations
can affect learning and why this may be beneficial
for language acquisition, as suggested, for example,
in Newport (1988).
88
References
Benjamin Bo?rschinger and Mark Johnson. 2011. A parti-
cle filter algorithm for bayesian wordsegmentation. In
Proceedings of the Australasian Language Technology
Association Workshop 2011, pages 10?18, Canberra,
Australia, December.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34(1-3):71?105.
Kevin R. Canini, Lei Shi, and Thomas L. Griffiths. 2009.
Online inference of topics with latent Dirichlet aloca-
tion. In David van Dyk and Max Welling, editors, Pro-
ceeings of the 12th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages 65?
72.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Sharon Goldwater. 2006. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: Experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Boulder, Colorado.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for pcfgs via markov
chain monte carlo. In Proceedings of Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100?108, Suntec, Singapore, August. Association for
Computational Linguistics.
Elissa L Newport. 1988. Constraints on learning and
their role in language acquisition: Studies of the acqui-
sition of american sign language. Language Sciences,
10:147?172.
Lisa Pearl, Sharon Goldwater, and Mark Steyvers. 2010.
Online learning mechanisms for bayesian models of
word segmentation. Research on Language and Com-
putation, 8(2):107?132.
89
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1033?1041,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The effect of non-tightness on Bayesian estimation of PCFGs
Shay B. Cohen
Department of Computer Science
Columbia University
scohen@cs.columbia.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
Probabilistic context-free grammars have
the unusual property of not always defin-
ing tight distributions (i.e., the sum of the
?probabilities? of the trees the grammar
generates can be less than one). This paper
reviews how this non-tightness can arise
and discusses its impact on Bayesian es-
timation of PCFGs. We begin by present-
ing the notion of ?almost everywhere tight
grammars? and show that linear CFGs fol-
low it. We then propose three different
ways of reinterpreting non-tight PCFGs to
make them tight, show that the Bayesian
estimators in Johnson et al (2007) are
correct under one of them, and provide
MCMC samplers for the other two. We
conclude with a discussion of the impact
of tightness empirically.
1 Introduction
Probabilistic Context-Free Grammars (PCFGs)
play a special role in computational linguistics be-
cause they are perhaps the simplest probabilistic
models of hierarchical structures. Their simplicity
enables us to mathematically analyze their prop-
erties to a detail that would be difficult with lin-
guistically more accurate models. Such analysis
is useful because it is reasonable to expect more
complex models to exhibit similar properties as
well.
The problem of inferring PCFG rule probabil-
ities from training data consisting of yields or
strings alone is interesting from both cognitive and
engineering perspectives. Cognitively it is implau-
sible that children can perceive the parse trees of
the language they are learning, but it is more rea-
sonable to assume that they can obtain the terminal
strings or yield of these trees. Unsupervised meth-
ods for learning a grammar from terminal strings
alone is also interesting from an engineering per-
spective because such training data is cheap and
plentiful, while the manually parsed data required
by supervised methods are expensive to produce
and relatively rare.
Cohen and Smith (2012) show that inferring
PCFG rule probabilities from strings alone is com-
putationally intractable, so we should not expect
to find an efficient, general-purpose algorithm for
the unsupervised problem. Instead, approxima-
tion algorithms are standardly used. For exam-
ple, the Inside-Outside (IO) algorithm efficiently
implements the Expectation-Maximization (EM)
procedure for approximating a Maximum Likeli-
hood estimator (Lari and Young, 1990). Bayesian
estimators for PCFG rule probabilities have also
been attracting attention because they provide a
theoretically-principled way of incorporating prior
information. Kurihara and Sato (2006) proposed
a Variational Bayes estimator based on a mean-
field approximation, and Johnson et al (2007) pro-
posed MCMC samplers for the posterior distribu-
tion over rule probabilities and the parse trees of
the training data strings.
PCFGs have the interesting property (which we
expect most linguistically more realistic models to
also possess) that the distributions they define are
not always properly normalized or ?tight?. In a
non-tight PCFG the partition function (i.e., sum
of the ?probabilities? of all the trees generated by
the PCFG) is less than one. (Booth and Thomp-
son, 1973, called such non-tight PCFGs ?incon-
sistent?, but we follow Chi and Geman (1998)
in calling them ?non-tight? to avoid confusion
with the consistency of statistical estimators). Chi
(1999) showed that renormalized non-tight PCFGs
(which he called ?Gibbs CFGs?) define the same
class of distributions over trees as do tight PCFGs
with the same rules, and provided an algorithm for
mapping any PCFG to a tight PCFG with the same
rules that defines the same distribution over trees.
An obvious question is then: how does tightness
affect the inference of PCFGs? Chi and Geman
(1998) studied the question for Maximum Likeli-
hood (ML) estimation, and showed that ML es-
1033
timates are always tight for both the supervised
case (where the input consists of parse trees) and
the unsupervised case (where the input consists of
yields or terminal strings). This means that ML
estimators can simply ignore issues of tightness,
and rest assured that the PCFGs they estimate are
in fact tight.
The situation is more subtle with Bayesian es-
timators. We show that for the special case of
linear PCFGs (which include HMMs) with non-
degenerate priors the posterior puts zero mass on
non-tight PCFGs, so tightness is not an issue with
Bayesian estimation of such grammars. However,
because all of the commonly used priors (such as
the Dirichlet or the logistic normal) assign non-
zero probability across the whole probability sim-
plex, in general the posterior may assign non-zero
probability to non-tight PCFGs. We discuss three
different possible approaches to this in this paper:
1. the only-tight approach, where we modify the
prior so it only assigns non-zero probability
to tight PCFGs,
2. the renormalization approach, where we
renormalize non-tight PCFGs so they define
a probability distribution over trees, and
3. the sink-element approach, where we reinter-
pret non-tight PCFGs as assigning non-zero
probability to a ?sink element?, so both tight
and non-tight PCFGs are properly normal-
ized.
We show how to modify the Gibbs sampler de-
scribed by Johnson et al (2007) so it produces
samples from the posterior distributions defined
by the only-tight and renormalization approaches.
Perhaps surprisingly, we show that Gibbs sampler
as defined by Johnson et al actually produces
samples from the posterior distributions defined by
the sink-element approach.
We conclude by studying the effect of requir-
ing tightness on the estimation of some simple
PCFGs. Because the Bayesian posterior converges
around the (tight) ML estimate as the size of
the data grows, requiring tightness only seems to
make a difference with highly biased priors or with
very small training corpora.
2 PCFGs and tightness
LetG = (T,N, S,R) be a Context-Free Grammar
in Chomsky normal form with no useless produc-
tions, where T is a finite set of terminal symbols,
N is a finite set of nonterminal symbols (disjoint
from T ), S ? N is a distinguished nonterminal
called the start symbol, andR is a finite set of pro-
ductions of the form A ? BC or A ? w, where
A,B,C ? N and w ? T . In what follows we use
? as a variable ranging over (N ?N) ? T .
A Probabilistic Context-Free Grammar (G,?)
is a pair consisting of a context-free grammar G
and a real-valued vector ? of length |R| indexed
by productions, where ?A?? is the production
probability associated with the production A ?
? ? R. We require that ?A?? ? 0 and that for
all nonterminals A ? N , ?A???RA ?A?? = 1,where RA is the subset of rules R expanding the
nonterminal A.
A PCFG (G,?) defines a measure ?? over
trees t as follows:
??(t) =
?
r?R
?fr(t)r
where fr(t) is the number of times the production
r = A? ? ? R is used in the derivation of t.
The partition function Z or measure of all pos-
sible trees is:
Z(?) =
?
t??T
?
r?R
?fr(t?)r
where T is the set of all (finite) trees generated
by G. A PCFG is tight iff the partition function
Z(?) = 1. In this paper we use ?? to denote the
set of rule probability vectors ? for which G is
non-tight. Nederhof and Satta (2008) survey sev-
eral algorithms for computing Z(?), and hence
for determining whether a PCFG is tight.1
Non-tightness can arise in very simple PCFGs,
such as the ?Catalan? PCFG S ? S S | a. This
grammar produces binary trees where all internal
nodes are labeled as S and the yield of these trees
is a sequence of as. If the probability of the rule
S ? S S is greater than 0.5 then this PCFG is
non-tight.
Perhaps the most straight-forward way to under-
stand this non-tightness is to view this grammar as
defining a branching process where an S can either
?reproduce? with probability ?S?S S or ?die out?
1We found out that finding whether a PCFG is tight by
directly inspecting the partition function value is less stable
than using the method in Wetherell (1980). For this reason,
we used Wetherell?s approach, which is based on finding the
principal eigenvalue of the matrix M .
1034
with probability ?S?a. When ?S?S S > ?S?a the
S nodes reproduce at a faster rate than they die
out, so the derivation has a non-zero probability of
endlessly rewriting (Atherya and Ney, 1972).
3 Bayesian inference for PCFGs
The goal of Bayesian inference for PCFGs is to in-
fer a posterior distribution over the rule probabil-
ity vectors ? given observed data D. This poste-
rior distribution is obtained by combining the like-
lihood P(D | ?) with a prior distribution P(?)
over ? using Bayes Rule.
P(? | D) ? P(D | ?) P(?)
We now formally define the three approaches to
handling non-tightness mentioned earlier:
the only-tight approach: we only permit priors
where P(??) = 0, i.e., we insist that the
prior assign zero mass to non-tight rule prob-
ability vectors, so Z = 1. This means we can
define:
P(t | ?) = ??(t)
the renormalization approach: we renormalize
non-tight PCFGs by dividing by the partition
function:
P(t | ?) = 1Z(?) ??(t) (1)
the sink-element approach: we redefine our
probability distribution so its domain is a set
T ? = T ? {?}, where T is the set of (finite)
trees generated by G and ? 6? T is a new
element that serves as a ?sink state? to which
the ?missing mass? 1 ? Z(?) is assigned.
Then we define:2
P(t | ?) =
{
??(t) if t ? T
1? Z(?) if t = ?
2This definition of a distribution over trees can be induced
by a tight PCFG with a special ? symbol in its vocabulary.
Given G, the first step is to create a tight grammar G0 using
the renormalization approach. Then, a new start symbol is
added to G0, S0, and also rules S0 ? S (where S is the
old start symbol in G0) and S0 ? ?. The first rule is given
probability Z(?) and the second rule is given probability 1?
Z(?). It can be then readily shown that the new tight PCFG
G0 induces a distribution over trees just like in Eq. 3, only
with additional S0 on top of all trees.
With this in hand, we can now define the likeli-
hood term. We consider two types of data D here.
In the supervised setting the data D consists of a
corpus of parse trees D = (t1, . . . , tn) where each
tree ti is generated by the PCFG G, so
P(D | ?) =
n?
i=1
P(ti | ?)
In the unsupervised setting the data D consists
of a corpus of strings D = (w1, . . . , wn) where
each string wi is the yield of one or more trees
generated by G. In this setting
P(D | ?) =
n?
i=1
P(wi | ?),where:
P(w | ?) =
?
t?T :yield(t)=w
P(t | ?)
4 The special case of linear PCFGs
One way to handle the issue of tightness is to iden-
tify a family of CFGs for which practically any pa-
rameter setting will yield a tight PCFG. This is the
focus of this section, in which we identify a sub-
set of CFGs, which are ?almost everywhere? tight.
This family of CFGs includes many of the CFGs
used in NLP applications.
We cannot expect that a CFG will yield a tight
PCFG for any assignment to the rule probabilities
(i.e. that ?? = ?). Even in simple cases, such as
the grammar S ? S|a, the assignment of proba-
bility 1 to S ? S and 0 to the other rule renders
the S nonterminal useless, and places all of the
probability mass on infinite structures of the form
S ? S ? S ? . . ..
However, we can weaken our requirement so
that the cases in which parameter assignment
yields a non-tight PCFG are rare, or have measure
zero. To put it more formally, we say that a prior
P(?) is ?tight almost everywhere for G? if
P(??) =
?
????
P(?) d? = 0.
We now provide a sufficient condition (linear-
ity) for CFGs under which they are tight almost
everywhere with any continuous prior.
For a nonterminal A ? N and ? ? (N ? T )?,
we use A?k ? to denote that A can be re-written
using a sequence of rules from R to the sentential
form ? in k derivation steps. We use A ?+ ? to
denote that there exists a k > 0 such thatA?k ?.
1035
Definition 1 A context-free grammarG is linear if
there are no A ? N such that
A?+ . . . A . . . A . . . .
Definition 2 A nonterminal A ? N in a proba-
bilistic context-free grammar G with parameters
? is nonterminating if
PG(A?+ . . . A . . . |?) = 1.
Here P(A?+ . . . A . . . |?) is defined as:
?
?:?=...A...
PG(A?+ ?|?).
Lemma 1 A linear PCFG G with parameters ?
which does not have any nonterminating nonter-
minals is tight.
Proof: Our proof relies on the properties of a cer-
tain |N | ? |N | matrix M where:
MAB =
?
A???RA
n(?,B) ?A??
where n(?,B) is the number of appearances of the
nonterminal B in the sequence ?. MAB is the ex-
pected number of B nonterminals generated from
an A nonterminal in one single derivational step,
so [Mk]AB is the expected number ofB nontermi-
nals generated from an A nonterminal in a k-step
derivation (Wetherell, 1980).
Since M is a non-negative matrix, under some
regularity conditions, the Frobenius-Perron theo-
rem states that the largest eigenvalue of this ma-
trix (in absolute value) is a real number. Let this
eigenvalue be denoted by ?.
A PCFG is called ?subcritical? if ? < 1 and
supercritical if ? > 1. Then, in turn, a PCFG is
tight if it is subcritical. It is not tight if it is su-
percritical. The case of ? = 1 is a borderline case
that does not give sufficient information to know
whether the PCFG is tight or not. In the Bayesian
case, for a continuous prior such as the Dirichlet
prior, this borderline case will have measure zero
under the prior.
Now let A ? N . Since the grammar is lin-
ear, there is no derivation A ?+ . . . A . . . A . . ..
Therefore, any derivation of the form A ?+
. . . A . . . includes A on the right hand-side exactly
once. Because the grammar has no useless non-
terminals, the probability of such a derivation is
strictly smaller than 1.
For each A ? N , define:
pA =
?
?=...A...
P(A?|N | ?|?).
Since A is not useless, then pA < 1. Therefore
q = maxA pA < 1. Since any derivation of length
k of the formA? . . . A . . . can be decomposed to
at least k2|N | cycles that start at a terminal B ? N
and end in the same nonterminal B ? N , it holds
that:
[Mk]AA ? q
k
2|N| k??? 0.
This means that trace(Mk) k??? 0. This means
that the eigenvalue of M is strictly smaller than 1
(linear algebra), and therefore the PCFG is tight.
Proposition 1 Any continuous prior P(?) on a
linear grammar G is tight almost everywhere for
G.
Proof: Let G be a linear grammar. With a contin-
uous prior, the probability ofG getting parameters
from the prior which yield a useless non-terminal
is 0 ? it would require setting at least one rule in
the grammar with rule probability which is exactly
1. Therefore, with probability 1, the parameters
taken from the prior yield a PCFG which is linear
and does not have nonterminating nonterminals.
According to Lemma 1, this means the PCFG is
tight. 
Deciding whether a grammar G is linear can
be done in polynomial time using the construction
from Bar-Hillel et al (1964). We can first elimi-
nate the differences between nonterminals and ter-
minal symbols by adding a rule A ? cA for each
nonterminal A ? N , after extending the set of
terminal symbols A with {cA|A ? N}. Let GA
be the grammar G with the start symbol being re-
placed with A. We can then intersect the grammar
GA with the regular language T ?cAT ?cAT ? (for
each nonterminal A ? N ). If for any nontermi-
nal A the intersection is not the empty set (with
respect to the language that the intersection gen-
erates), then the grammar is not linear. Checking
whether the intersection is the empty set or not can
be done in polynomial time.
We conclude this section by remarking that
many of the models used in computational lin-
guistics are in fact equivalent to linear PCFGs, so
continuous Bayesian priors are almost everywhere
tight. For example, HMMs and many kinds of
?stacked? finite-state machines are equivalent to
1036
linear PCFGs, as are the example PCFGs given in
Johnson et al (2007) to motivate the MCMC esti-
mation procedures.
5 Dirichlet priors
The first step in Bayesian inference is to specify a
prior on ?. In the rest of this paper we take P(?)
to be a product of Dirichlet distributions, with one
distribution for each non-terminal A ? N , as this
turns out to simplify the computations consider-
ably. The prior is parameterized by a positive real
valued vector ? indexed by productionsR, so each
production probability ?A?? has a corresponding
Dirichlet parameter ?A?? . As before, let RA be
the set of productions in R with left-hand side A,
and let ?A and ?A refer to the component subvec-
tors of ? and ? respectively indexed by produc-
tions in RA. The Dirichlet prior P(? | ?) is:
P(? | ?) =
?
A?N
PD(?A | ?A),
where
PD(?A | ?A) =
1
C(?A)
?
r?RA
??r?1r and
C(?A) =
?
r?RA ?(?r)
?(
?
r?RA ?r)
where ? is the generalized factorial function and
C(?) is a normalization constant that does not de-
pend on ?A.
Dirichlet priors are useful because they are con-
jugate to the multinomial distribution, which is
the building block of PCFGs. Ignoring issues of
tightness for the moment and setting P(t | ?) =
??(t), this means that in the supervised setting the
posterior distribution P(? | t, ?) given a set of
parse trees t = (t1, . . . , tn) is also a product of
Dirichlets distribution.
P(? | t, ?) ? P(t | ?) P(? | ?)
?
(?
r?R
?fr(t)r
)(?
r?R
??r?1r
)
=
?
r?R
?fr(t)+?r?1r
which is a product of Dirichlet distributions with
parameters f(t) + ?, where f(t) is the vector of
rule counts in t indexed by r ? R. We can thus
write:
P(? | t, ?) = P(? | f(t) + ?)
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous parameters ?0.
Result: A vector of parameters ?
repeat
draw ? from products of Dirichlet with
hyperparameters ?+ f(t)
until ? is tight for G;
return ?
Algorithm 1: An algorithm for generating sam-
ples from P(? | t, ?) for the only-tight ap-
proach.
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous rule parameters
?0.
Result: A vector of parameters ?
draw a proposal ?? from a product of Dirichlets with
parameters ?+ f(t).
draw a uniform number u from [0, 1].
if u < min{1,
(
Z(?(i?1))/Z(??)
)n
} return ??.
return ?0.
Algorithm 2: One step of Metropolis-Hastings
algorithm for generating samples from P(? |
t, ?) for the renormalization approach.
which makes it clear that the rule counts are di-
rectly added to the parameters of the prior to pro-
duce the parameters of the posterior.
6 Inference in the supervised setting
We first discuss Bayesian inference in the super-
vised setting, as inference in the unsupervised set-
ting is based on inference for the supervised set-
ting. For each of the three approaches to non-
tightness we provide an algorithm that character-
izes the posterior P(? | t), where t = (t1, . . . , tn)
is a sequence of trees, by generating samples from
that posterior. Our MCMC algorithms for the un-
supervised setting build on these samplers for the
supervised setting.
6.1 The only-tight approach
The ?only-tight? approach requires that the prior
assign zero mass to non-tight rule probability vec-
tors ??. One way to define such a distribution is
to restrict the domain of an existing prior distribu-
tion with the set of tight ? and renormalize. In
more detail, if P(?) is a prior over rule probabili-
ties, then its renormalization is the prior P? defined
as:
P?(?) = P(?)I(? /? ?
?)
Z(??) . (2)
where Z(??) = ?? P(?)I(? /? ??)d?.
1037
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous parameters ?0.
Result: A vector of parameters ?
draw ? from products of Dirichlet with
hyperparameters ?+ f(t)
return ?
Algorithm 3: An algorithm for generating sam-
ples from P(? | t, ?) for the sink-state approach.
Perhaps surprisingly, it turns out that if P(?)
belongs to a family of conjugate priors, then P?(?)
also belongs to a (different) family of conjugate
priors as well.
Proposition 2 Let P(?|?) be a prior with hyper-
parameters ? over the parameters of G such that
P is conjugate to the grammar likelihood. Then
P?, defined in Eq. 2, is conjugate to the grammar
likelihood as well.
Proof: Assume that trees t are observed, and the
prior over the grammar parameters is the prior de-
fined in Eq. 2. Therefore, the posterior is:
P(?|t, ?) ? P?(?|?)p(t|?)
= P(?|?)p(t|?)I(? /? ?
?)
Z(??)
? P(?|t, ?)I(? /? ?
?)
Z(??) .
Since P(?|?) is a conjugate prior to the PCFG
likelihood, then there exists ?? = ??(t) such that
P(?|t, ?) = P?(?|??). Therefore:
P(?|t, ?) ? P(?|?
?)I(? /? ??)
Z(??) .
which exactly equals P?(?|??). 
Sampling from the posterior over the parame-
ters given a set of trees t is therefore quite sim-
ple when assuming the base prior being renormal-
ized is a product of Dirichlets. Algorithm 1 sam-
ples from a product of Dirichlets distribution with
hyperparameters ? + f(t) repeatedly, each time
checking and rejecting the sample until we obtain
a tight PCFG.
The more mass the Dirichlet distribution with
hyperparameters ? + f(t) puts on non-tight
PCFGs, the more rejections will happen. In gen-
eral, if the probability mass on non-tight PCFGs is
q?, then it would require, on average 1/(1 ? q?)
samples from this distribution in order to obtain a
tight PCFG.
6.2 The renormalization approach
The renormalization approach modifies the likeli-
hood function instead of the prior. Here we use a
product of Dirichlets prior P(? | ?) on rule prob-
ability vectors ?, but the presence of the partition
functionZ(?) in Eq. 1 means that the likelihood is
no longer conjugate to the prior. Instead we have:
P(? | t) =
n?
i=1
??(ti)
Z(?) P(? | ?)
? 1Z(?)n P(? | ?+ f(t)). (3)
Note that the factor Z(?) depends on ?, and
therefore cannot be absorbed into the constant. Al-
gorithm 2 describes a Metropolis-Hastings sam-
pler for sampling from the posterior in Eq. 3
that uses a product of Dirichlets with parameters
?+ f(t) as a proposal distribution.
In our experiments, we use the algorithm from
Nederhof and Satta (2008) to compute the parti-
tion function which is needed in Algorithm 2.
6.3 The ?sink element? approach
The ?sink element? approach does not affect the
likelihood (since the probability of a tree t is just
the product of the probabilities of the rules used
to generate it), nor does it require a change to the
prior. (The sink element ? is not a member of the
set of trees T , so it cannot appear in the data t).
This means that the conjugacy argument given
at the bottom of section 5 holds in this approach,
so the posterior P(? | t, ?) is a product of Dirich-
lets with parameters f(t) + ?. Algorithm 3 gives
a sampler for P(? | t, ?) for the sink element ap-
proach.
7 Inference in the unsupervised setting
Johnson et al (2007) provide two Markov chain
Monte Carlo algorithms for Bayesian inference for
PCFG rule probabilities in the unsupervised set-
ting (i.e., where the data consists of a corpus of
strings w = (w1, . . . , wn) alone). The algorithms
we give here are based on their Gibbs sampler,
which in each iteration first samples parse trees
t = (t1, . . . , tn), where each ti is a parse for
wi, from P(t | w,?), and then samples ? from
P(? | t, ?).
Notice that the conditional distribution P(t |
w,?) is unaffected in each of our three ap-
proaches (the partition functions cancel in the
1038
Input: Grammar G, vector of hyperparameters ?,
vector of strings w = (w1, . . . , wn), previous
rule parameters ?0.
Result: A vector of parameters ?
for i? 1 to n do
draw ti from P(ti|wi,?0)
end
use Algorithm 2 to sample ? given G, t, ? and ?0
return ?
Algorithm 4: One step of the Metropolis-within-
Gibbs sampler for the renormalization approach.
renormalization approach), so the algorithm for
sampling from P(t | w,?) given by Johnson et
al. applies in each of our three approaches as well.
Johnson et al ignored tightness and assumed
that P(? | t, ?) is a product of Dirichlets with
parameters f(t) + ?. As we noted in section 6.3,
this assumption holds for the sink-state approach
to non-tightness, so their sampler is in fact correct
for the sink-state approach.
In fact, we obtain samplers for the unsupervised
setting for each of our approaches by ?plugging
in? the corresponding sampling algorithm (Eq. 1?
3) for P(? | t, ?) into the generic Gibbs sampler
framework of Johnson et al
The one complication is that because we use a
Metropolis-Hastings procedure to generate sam-
ples from P(? | t, ?) in the renormalization ap-
proach, we use the Metropolis-within-Gibbs pro-
cedure given in Algorithm 4 (Robert and Casella,
2004).
8 The expressive power of the three
approaches
Probably the most important question to ask with
respect to the three different approaches to non-
tightness is whether they differ in terms of expres-
sive power. Clearly the three approaches differ in
terms of the grammars they admit (the only-tight
approach requires the prior to only assign non-zero
probability to tight PCFGs, while the other two ap-
proaches permit the prior to assign non-zero prob-
ability to non-tight PCFGs as well). However, if
we regard a grammar as merely a device for defin-
ing a distribution over trees and a prior as defining
a distribution over distributions over trees, it is rea-
sonable to ask whether the class of distributions
over distributions of trees that each of these ap-
proaches define are the same or differ. We believe,
but have not proved, that all three approaches de-
fine the same class of distributions over distribu-
tions of trees in the following sense: any prior used
with one of the approaches can be transformed
into a different prior that can be used with one of
the other approaches, and yield the same posterior
over trees conditioned on a string, marginalizing
out the parameters.
This does not mean that the three approaches
are equivalent, however. In this section we pro-
vide a grammar such that with a uniform prior over
rule probabilities, the conditional distribution over
trees given a fixed string varies under each of the
three different approaches.
The grammar we consider has three rules S ?
S S S|S S|a with probabilities ?1, ?2 and 1? ?1?
?2, respectively. The ? parameters are required to
satisfy ?1 + ?2 ? 1 and ?i ? 0 for i = 1, 2.
We compute the posterior distribution over
parse trees for the string w = a a a. The gram-
mar generates three parse trees for w1, namely:
t1 = S
S
a
S
a
S
a
t2 = S
S
a
S
S
a
S
a
t3 = S
S
S
a
S
a
S
a
The partition function Z for this grammar is the
smallest positive root of the cubic equation:
Z = ?1Z3 + ?2Z2 + (1? ?1 ? ?2)
We used Mathematica to find an analytic solution
for Z in this equation, obtaining not only an ex-
pression for the partition function Z(?) but also
identifying the non-tight region ??.
In order to compute P(t1|w), we used Mathe-
matica to first compute the following quantities:
qsinkElement(ti) =
?
?
??(ti) d?
qtightOnly(ti) =
?
?
??(ti) I(? /? ??) d?
qrenormalization(ti) =
?
?
??(ti)/Z(?) d?
where i ? {1, 2, 3}. We used Mathematica to ana-
lytically compute q(ti) for each approach and each
i ? {1, 2, 3}. Then it?s easy to show that:
P(ti | w) =
q(ti)?3
i?=1 q(ti?)
where the q used is based on the approach to
tightness desired. For the sink-element approach,
1039
010
20
30
0.35 0.40 0.45 0.50 0.55Average f?score
De
nsi
ty
Inference
only?tight
sink?state
renormalise
Figure 1: The density of the F1-scores with the
three approaches. The prior used is a symmetric
Dirichlet with ? = 0.1.
P(t1|w) = 711 ? 0.636364. For the only-tightapproach P(t1|w) = 1117917221 ? 0.649149. Forthe renormalization approach the analytic ex-
pression is too complex to include in this paper,
but it approximately equals 0.619893. A log
of our Mathematica calculations is available
at http://www.cs.columbia.edu/?scohen/
acl13tightness-mathematica.pdf, and we
confirmed these results to three decimal places us-
ing the samplers described above (which required
107 samples per approach).
While the differences between these conditional
probabilities are not great, the conditional prob-
abilities are clearly different, so the three ap-
proaches do in fact define different distributions
over trees under a uniform prior on rule probabili-
ties.
9 Empirical effects of the three
approaches in unsupervised grammar
induction
In this section we present experiments using the
three samplers just described in an unsupervised
grammar induction problem. Our goal here is
not to improve the state-of-the-art in unsupervised
grammar induction, but to try to measure empir-
ical differences in the estimates produced by the
three different approaches to tightness just de-
scribed. The bottom line of our experiments is that
we could not detect any significant difference in
the estimates produced by samplers for these three
different approaches.
In our experiments we used the English Penn
treebank (Marcus et al, 1993). We use the part-
of-speech tag sequences of sentences shorter than
11 words in sections 2?21. The grammar we use is
the PCFG version of the dependency model with
valence (Klein and Manning, 2004), as it appears
in Smith (2006).
We used a symmetric Dirichlet prior with hy-
perparameter ? = 0.1. For each of the three ap-
proaches for handling tightness, we ran 100 times
the samplers in ?7, each for 1,000 iterations. We
discarded the first 900 sweeps of each run, and cal-
culated the F1-scores of the sampled trees every
10th sweep from the last 100 sweeps. For each
run we calculated the average F1-score over the
10 sweeps we evaluated. We thus have 100 aver-
age F1-scores for each of the samplers.
Figure 1 plots the density of F1 scores (com-
pared to the gold standard) resulting from the
Gibbs sampler, using all three approaches. The
mean value for each of the approaches is 0.41
with standard deviation 0.06 (only-tight), 0.41
with standard deviation 0.05 (renormalization)
and 0.42 with standard deviation 0.06 (sink ele-
ment). In addition, the only-tight approach results
in an average of 437 (s.d., 142) rejected propos-
als in 1,000 samples, while the renormalization
approach results in an average of 232 (s.d., 114)
rejected proposals in 1,000 samples. (It?s not sur-
prising that the only-tight approach results in more
rejections as it keeps proposing new ? until a tight
proposal is found, while the renormalization ap-
proach simply uses the old ?).
We performed two-sample Kolmogorov-
Smirnov tests (which are non-parametric tests
designed to determine if two distributions are
different; see DeGroot, 1991) on each of the three
pairs of 100 F1-scores. None of the tests were
close to significant; the p-values were all above
0.5. Thus our experiments provided no evidence
that the samplers produced different distributions
over trees, although it?s reasonable to expect that
these distributions do indeed differ.
In terms of running time, our implementation
of the renormalization approach was several times
slower than our implementations of the other two
approaches because we used the naive fixed-point
algorithm to compute the partition function: per-
haps this could be improved using one of the
more sophisticated partition function algorithms
described in Nederhof and Satta (2008).
1040
10 Conclusion
In this paper we characterized the notion of an al-
most everywhere tight grammar in the Bayesian
setting and showed it holds for linear CFGs. For
non-linear CFGs, we described three different ap-
proaches to handle non-tightness. The ?only-
tight? approach restricts attention to tight PCFGs,
and perhaps surprisingly, we showed that conju-
gacy still obtains when the domain of a product
of Dirichlets prior is restricted to the subset of
tight grammars. The renormalization approach in-
volves renormalizing the PCFG measure ? over
trees when the grammar is non-tight, which de-
stroys conjugacy with a product of Dirichlets prior.
Perhaps most surprisingly of all, the sink-element
approach, which assigns the missing mass in non-
tight PCFG to a sink element ?, turns out to be
equivalent to existing practice where tightness is
ignored.
We studied the posterior distributions over trees
induced by the three approaches under a uniform
prior for a simple grammar and showed that they
differ. We leave for future work the important
question of whether the classes of distributions
over distributions over trees that the three ap-
proaches define are the same or different.
We described samplers for the supervised
and unsupervised settings for each of these ap-
proaches, and applied them to an unsupervised
grammar induction problem. (The code for the
unsupervised samplers is available from http://
web.science.mq.edu.au/?mjohnson).
We could not detect any difference in the pos-
terior distributions over trees produced by these
samplers, despite devoting considerable computa-
tional resources to the problem. This suggests that
for these kinds of problems at least, tightness is
not of practical concern for Bayesian inference of
PCFGs.
Acknowledgements
We thank the anonymous reviewers and Gior-
gio Satta for their valuable comments. Shay
Cohen was supported by the National Science
Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows
Project, and Mark Johnson was supported by the
Australian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
K. B. Atherya and P. E. Ney. 1972. Branching Pro-
cesses. Dover Publications.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. Language and Information: Selected Essays
on Their Theory and Application, pages 116?150.
T. L. Booth and R. A. Thompson. 1973. Applying
probability measures to abstract languages. IEEE
Transactions on Computers, C-22:442?450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguis-
tics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
S. B. Cohen and N. A. Smith. 2012. Empirical risk
minimization for probabilistic grammars: Sample
complexity and hardness of learning. Computa-
tional Linguistics, 38(3):479?526.
M. H. DeGroot. 1991. Probability and Statistics (3rd
edition). Addison-Wesley.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In 8th In-
ternational Colloquium on Grammatical Inference.
K. Lari and S.J. Young. 1990. The estimation of
Stochastic Context-Free Grammars using the Inside-
Outside algorithm. Computer Speech and Lan-
guage, 4(35-56).
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
M.-J. Nederhof and G. Satta. 2008. Computing par-
tition functions of PCFGs. Research on Language
and Computation, 6(2):139?162.
C. P. Robert and G. Casella. 2004. Monte Carlo Sta-
tistical Methods. Springer-Verlag New York.
N. A. Smith. 2006. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Johns Hopkins Univer-
sity.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12:361?379.
1041
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508?1516,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A joint model of word segmentation and phonological variation for
English word-final /t/-deletion
Benjamin Bo?rschinger1,3 and Mark Johnson1 and Katherine Demuth2
(1) Department of Computing, Macquarie University
(2) Department of Linguistics, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.au
Abstract
Word-final /t/-deletion refers to a common
phenomenon in spoken English where
words such as /wEst/ ?west? are pro-
nounced as [wEs] ?wes? in certain con-
texts. Phonological variation like this is
common in naturally occurring speech.
Current computational models of unsu-
pervised word segmentation usually as-
sume idealized input that is devoid of
these kinds of variation. We extend a
non-parametric model of word segmenta-
tion by adding phonological rules that map
from underlying forms to surface forms
to produce a mathematically well-defined
joint model as a first step towards han-
dling variation and segmentation in a sin-
gle model. We analyse how our model
handles /t/-deletion on a large corpus of
transcribed speech, and show that the joint
model can perform word segmentation and
recover underlying /t/s. We find that Bi-
gram dependencies are important for per-
forming well on real data and for learning
appropriate deletion probabilities for dif-
ferent contexts.1
1 Introduction
Computational models of word segmentation try
to solve one of the first problems language learn-
ers have to face: breaking an unsegmented stream
of sound segments into individual words. Cur-
rently, most such models assume that the input
consists of sequences of phonemes with no pro-
nunciation variation across different occurrences
of the same word type. In this paper we describe
1The implementation of our model as well as
scripts to prepare the data will be made available at
http://web.science.mq.edu.au/~bborschi.
We can?t release our version of the Buckeye Corpus (Pitt et
al., 2007) directly because of licensing issues.
an extension of the Bayesian models of Gold-
water et al (2009) that incorporates phonologi-
cal rules to ?explain away? surface variation. As
a concrete example, we focus on word-final /t/-
deletion in English, although our approach is not
limited to this case. We choose /t/-deletion be-
cause it is a very common and well-studied phe-
nomenon (see Coetzee (2004, Chapter 5) for a
review) and segmental deletion is an interesting
test-case for our architecture. Recent work has
found that /t/-deletion (among other things) is in-
deed common in child-directed speech (CDS) and,
importantly, that its distribution is similar to that in
adult-directed speech (ADS) (Dilley et al, to ap-
pear). This justifies our using ADS to evaluate our
model, as discussed below.
Our experiments are consistent with long-
standing and recent findings in linguistics, in par-
ticular that /t/-deletion heavily depends on the im-
mediate context and that models ignoring context
work poorly on real data. We also examine how
well our models identify the probability of /t/-
deletion in different contexts. We find that models
that capture bigram dependencies between under-
lying forms provide considerably more accurate
estimates of those probabilities than correspond-
ing unigram or ?bag of words? models of underly-
ing forms.
In section 2 we discuss related work on han-
dling variation in computational models and on /t/-
deletion. Section 3 describes our computational
model and section 4 discusses its performance for
recovering deleted /t/s. We look at both a sit-
uation where word boundaries are pre-specified
and only inference for underlying forms has to
be performed; and the problem of jointly finding
the word boundaries and recovering deleted un-
derlying /t/s. Section 5 discusses our findings, and
section 6 concludes with directions for further re-
search.
1508
2 Background and related work
The work of Elsner et al (2012) is most closely
related to our goal of building a model that han-
dles variation. They propose a pipe-line archi-
tecture involving two separate generative models,
one for word-segmentation and one for phonolog-
ical variation. They model the mapping to sur-
face forms using a probabilistic finite-state trans-
ducer. This allows their architecture to handle
virtually arbitrary pronunciation variation. How-
ever, as they point out, combining the segmenta-
tion and the variation model into one joint model
is not straight-forward and usual inference proce-
dures are infeasible, which requires the use of sev-
eral heuristics. We pursue an alternative research
strategy here, starting with a single well-studied
example of phonological variation. This permits
us to develop a joint generative model for both
word segmentation and variation which we plan to
extend to handle more phenomena in future work.
An earlier work that is close to the spirit of our
approach is Naradowsky and Goldwater (2009),
who learn spelling rules jointly with a simple
stem-suffix model of English verb morphology.
Their model, however, doesn?t naturally extend to
the segmentation of entire utterances.
/t/-deletion has received a lot of attention within
linguistics, and we point the interested reader to
Coetzee (2004, Chapter 5) for a thorough review.
Briefly, the phenomenon is as follows: word-final
instances of /t/ may undergo deletion in natural
speech, such that /wEst/ ?west? is actually pro-
nounced as [wEs] ?wes?.2 While the frequency of
this phenomenon varies across social and dialectal
groups, within groups it has been found to be ro-
bust, and the probability of deletion depends on
its phonological context: a /t/ is more likely to
be dropped when followed by a consonant than
a vowel or a pause, and it is more likely to be
dropped when following a consonant than a vowel
as well. We point out two recent publications that
are of direct relevance to our research. Dilley et al
(to appear) study word-final variation in stop con-
sonants in CDS, the kind of input we ideally would
like to evaluate our models on. They find that ?in-
fants largely experience statistical distributions of
non-canonical consonantal pronunciation variants
[including deletion] that mirror those experienced
by adults.? This both directly establishes the need
2Following the convention in phonology, we give under-
lying forms within ?/. . . /? and surface forms within ?[. . . ]?.
for computational models to handle this dimension
of variation, and justifies our choice of using ADS
for evaluation, as mentioned above.
Coetzee and Kawahara (2013) provide a com-
putational study of (among other things) /t/-
deletion within the framework of Harmonic Gram-
mar. They do not aim for a joint model that also
handles word segmentation, however, and rather
than training their model on an actual corpus, they
evaluate on constructed lists of examples, mimick-
ing frequencies of real data. Overall, our findings
agree with theirs, in particular that capturing the
probability of deletion in different contexts does
not automatically result in good performance for
recovering individual deleted /t/s. We will come
back to this point in our discussion at the end of
the paper.
3 The computational model
Our models build on the Unigram and the Bigram
model introduced in Goldwater et al (2009). Fig-
ure 1 shows the graphical model for our joint Bi-
gram model (the Unigram case is trivially recov-
ered by generating the Ui,js directly from L rather
than from LUi,j?1). Figure 2 gives the mathemati-
cal description of the graphical model and Table 1
provides a key to the variables of our model.
The model generates a latent sequence of un-
derlying word-tokens U1, . . . , Un. Each word to-
ken is itself a non-empty sequence of segments or
phonemes, and each Uj corresponds to an under-
lying word form, prior to the application of any
phonological rule. This generative process is re-
peated for each utterance i, leading to multiple
utterances of the form Ui,1, . . . , Ui,ni where ni is
the number of words in the ith utterance, and Ui,j
is the jth word in the ith utterance. Each utter-
ance is padded by an observed utterance bound-
ary symbol $ to the left and to the right, hence
Ui,0 = Ui,ni+1 = $.3 Each Ui,j+1 is generated
conditionally on its predecessor Ui,j from LUi,j ,
as shown in the first row of the lower plate in Fig-
ure 1. Each Lw is a distribution over the pos-
sible words that can follow a token of w and L
is a global distribution over possible words, used
as back-off for all Lw. Just as in Goldwater et
al. (2009), L is drawn from a Dirichlet Process
(DP) with base distribution B and concentration
3Each utterance terminates as soon as a $ is generated,
thus determining the number of words ni in the ith utterance.
See Goldwater et al (2009) for discussion.
1509
Figure 1: The graphical model for our joint
model of word-final /t/-deletion and Bigram
word segmentation. The corresponding math-
ematical description is given in Figure 2. The
generative process mimics the intuitively plau-
sible idea of generating underlying forms from
some kind of syntactic model (here, a Bi-
gram language model) and then mapping the
underlying form to an observed surface-form
through the application of a phonological rule
component, here represented by the collection
of rule probabilities ?c.
L |?, ?0 ?DP (?0, B(? | ?))
Lw |L,?1 ?DP (?1, L)
?c |? ?Beta(1, 1)
Ui,0 = $
Si,0 = $
Ui,j+1 |Ui,j , LUi,j ?LUi,j
Si,j |Ui,j , Ui,j+1,? =PR(? | Ui,j , Ui,j+1)
Wi |Si,1, . . . , Si,ni = CAT(Si,0, . . . , Si,ni)
Figure 2: Mathematical description of our joint
Bigram model. The lexical generator B(? | ?)
is specified in Figure 3 and PR is explained in
the text below. CAT stands for concatenation
without word-boundaries, ni refers to the num-
ber of words in utterance i.
Variable Explanation
B base distribution over possible words
L back-off distribution over words
Lw distribution over words following w
Ui,j underlying form, a word
Si,j surface realization of Ui,j , a word
?c /t/-deletion probability in context c
Wi observed segments for ith utterance
Table 1: Key for the variables in Figure 1 and
Figure 2. See Figure 3 for the definition of B.
parameter ?0, and the word type specific distri-
butions Lw are drawn from a DP (L,?1), result-
ing in a hierarchical DP model (Teh et al, 2006).
The base distribution B functions as a lexical gen-
erator, defining a prior distribution over possible
words. In principle, B can incorporate arbitrary
prior knowledge about possible words, for exam-
ple syllable structure (cf. Johnson (2008)). In-
spired by Norris et al (1997), we use a simpler
possible word constraint that only rules out se-
quences that lack a vowel (see Figure 3). While
this is clearly a simplification it is a plausible as-
sumption for English data.
Instead of generating the observed sequence of
segments W directly by concatenating the under-
lying forms as in Goldwater et al (2009), we
map each Ui,j to a corresponding surface-form
Si,j by a probabilistic rule component PR. The
values over which the Si,j range are determined
by the available phonological processes. In the
model we study here, the phonological processes
only include a rule for deleting word-final /t/s
but in principle, PR can be used to encode a
wide variety of phonological rules. Here, Si,j ?
{Ui,j ,DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =
Ui,j otherwise, where DELF(u) refers to the same
word as u except that it lacks u?s final segment.
We look at three kinds of contexts on which a
rule?s probability of applying depends:
1. a uniform context that applies to every word-
final position
2. a right context that also considers the follow-
ing segment
3. a left-right context that additionally takes the
preceeding segment into account
For each possible context c there is a prob-
ability ?c which stands for the probability of
the rule applying in this context. Writing
1510
? ?Dir(?0.01, . . . , 0.01?)
B(w = x1:n | ?) =
{ [?ni=1 ?xi ]?#
Z if V(w)
0.0 if ?V(w)
Figure 3: Lexical generator with possible word-
constraint for words in ?+, ? being the alphabet
of available phonemes. x1:n is a sequence of ele-
ments of ? of length n. ? is a probability vector
of length |?| + 1 drawn from a sparse Dirichlet
prior, giving the probability for each phoneme and
the special word-boundary symbol #. The pred-
icate V holds of all sequences containing at least
one vowel. Z is a normalization constant that ad-
justs for the mass assigned to the empty and non-
possible words.
contexts in the notation familiar from genera-
tive phonology (Chomsky and Halle, 1968), our
model can be seen as implementing the fol-
lowing rules under the different assumptions:4
uniform /t/ ? ? / ]word
right /t/ ? ? / ]word ?
left-right /t/ ? ? / ? ]word ?
We let ? range over V(owel), C(onsonant) and $
(utterance-boundary), and ? over V and C. We
define a function CONT that maps a pair of ad-
jacent underlying forms Ui,j , Ui,j+1 to the con-
text of the final segment of Ui,j . For example,
CONT(/wEst/,/@v/) returns ?C ]word V? in the
left-right setting, or simply ? ]word? in the uni-
form setting. CONT returns a special NOT con-
text if Ui,j doesn?t end in a /t/. We stipulate that
?NOT = 0.0. Then we can define PR as follows:
PR(DELFINAL(u) | u, r)) = ?CONT(u,r)
PR(u | u, r) = 1? ?CONT(u,r)
Depending on the context setting used, our
model includes one (uniform), three (right) or six
(left-right) /t/-deletion probabilities ?c. We place a
uniform Beta prior on each of those so as to learn
their values in the LEARN-? experiments below.
Finally, the observed unsegmented utterances
Wi are generated by concatenating all Si,j using
the function CAT.
We briefly comment on the central intuition
of this model, i.e. why it can infer underlying
4For right there are three and for left-right six different
rules, one for every instantiation of the context-template.
from surface forms. Bayesian word segmentation
models try to compactly represent the observed
data in terms of a small set of units (word types)
and a short analysis (a small number of word
tokens). Phonological rules such as /t/-deletion
can ?explain away? an observed surface type such
as [wEs]] in terms of the underlying type /wEst/
which is independently needed for surface tokens
of [wEst]. Thus, the /t/? ? rule makes possi-
ble a smaller lexicon for a given number of sur-
face tokens. Obviously, human learners have ac-
cess to additional cues, such as the meaning of
words, knowledge of phonological similarity be-
tween segments and so forth. One of the advan-
tages of an explicitly defined generative model
such as ours is that it is straight-forward to grad-
ually extend it by adding more cues, as we point
out in the discussion.
3.1 Inference
Just as for the Goldwater et al (2009) segmen-
tation models, exact inference is infeasible for
our joint model. We extend the collapsed Gibbs
breakpoint-sampler described in Goldwater et al
(2009) to perform inference for our extended mod-
els. We refer the reader to their paper for addi-
tional details such as how to calculate the Bigram
probabilities in Figure 4. Here we focus on the
required changes to the sampler so as to perform
inference under our richer model. We consider the
case of a single surface string W , so we drop the
i-index in the following discussion.
Knowing W , the problem is to recover the un-
derlying forms U1, . . . , Un and the surface forms
S1, . . . , Sn for unknown n. A major insight in
Goldwater?s work is that rather than sampling over
the latent variables in the model directly (the num-
ber of which we don?t even know), we can instead
perform Gibbs sampling over a set of boundary
variables b1, . . . , b|W |?1 that jointly determine the
values for our variables of interest where |W | is
the length of the surface string W . For our model,
each bj ? {0, 1, t}, where bj = 0 indicates ab-
sence of a word boundary, bj = 1 indicates pres-
ence of a boundary and bj = t indicates pres-
ence of a boundary with a preceeding underlying
/t/. The relation between the bj and the S1, . . . , Sn
and U1, . . . , Un is illustrated in Figure 5. The re-
quired sampling equations are given in Figure 4.
1511
P (bj = 0 | b?j) ? P (w12,u | wl,u, b?j)? Pr(w12,s | w12,u, wr,u)? P (wr,u | w12,u, b?j ? ?wl,u, w12,u?) (1)
P (bj = t | b?j) ? P (w1,t | wl,u, b?j)? Pr(w1,s | w1,t, w2,u)? P (w2,u | w1,t, b?j ? ?wl,u, w1,t?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,t? ? ?w1,t, w2,u?) (2)
P (bj = 1 | b?j) ? P (w1,s | wl,u, b?j)? Pr(w1,s | w1,s, w2,u)? P (w2,u | w1,s, b?j ? ?wl,u, w1,s?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,s? ? ?w1,s, w2,u?) (3)
Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds
to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a
boundary with no additional underlying /t/. We use b?j for the statistics determined by all but the jth
position and b?j ? ?r, l? for these statistics plus an additional count of the bigram ?r, l?. P (w | l, b)
refers to the bigram probability of ?l, w? given the the statistics b; we refer the reader to Goldwater et
al. (2009) for the details of calculating these bigram probabilities and details about the required statistics
for the collapsed sampler. PR is defined in the text.
1 10 t 1I h      i  i       t $
underlyingsurfaceboundariesobserved I h i i t $
I h      i       t  i       t $
Figure 5: The relation between the observed se-
quence of segments (bottom), the boundary vari-
ables b1, . . . , b|W |?1 the Gibbs sampler operates
over (in squares), the latent sequence of sur-
face forms and the latent sequence of underly-
ing forms. When sampling a new value for
b3 = t, the different word-variables in fig-
ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,
w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need
a boundary variable at the end of the utterance as
there might be an underlying /t/ at this position as
well. The final boundary variable is set to 1, not t,
because the /t/ in it is observed.
4 Experiments
4.1 The data
We are interested in how well our model han-
dles /t/-deletion in real data. Ideally, we?d eval-
uate it on CDS but as of now, we know of no
available large enough corpus of accurately hand-
transcribed CDS. Instead, we used the Buckeye
Corpus (Pitt et al, 2007) for our experiments,
a large ADS corpus of interviews with English
speakers that have been transcribed with relatively
fine phonetic detail, with /t/-deletion among the
things manually annotated. Pointing to the re-
cent work by Dilley et al (to appear) we want
to emphasize that the statistical distribution of /t/-
deletion has been found to be similar for ADS and
orthographic I don?t intend to
transcript /aI R oU n I n t E n d @/
idealized /aI d oU n t I n t E n d t U/
t-drop /aI d oU n I n t E n d t U/
Figure 6: An example fragment from the Buckeye-
corpus in orthographic form, the fine transcript
available in the Buckeye corpus, a fully idealized
pronunciation with canonical dictionary pronunci-
ations and our version of the data with dropped
/t/s.
CDS, at least for read speech.
We automatically derived a corpus of 285,792
word tokens across 48,795 utterances from the
Buckeye Corpus by collecting utterances across all
interviews and heuristically splitting utterances at
speaker-turn changes and indicated silences. The
Buckeye corpus lists for each word token a man-
ually transcribed pronunciation in context as well
as its canonical pronunciation as given in a pro-
nouncing dictionary. As input to our model, we
use the canonical pronunciation unless the pronun-
ciation in context indicates that the final /t/ has
been deleted in which case we also delete the final
/t/ of the canonical pronunciation Figure 6 shows
an example from the Buckeye Corpus, indicating
how the original data, a fully idealized version
and our derived input that takes into account /t/-
deletions looks like.
Overall, /t/-deletion is a quite frequent phe-
nomenon with roughly 29% of all underlying /t/s
being dropped. The probabilities become more
peaked when looking at finer context; see Table 3
for the empirical distribution of /t/-dropping for
the six different contexts of the left-right setting.
1512
4.2 Recovering deleted /t/s, given word
boundaries
In this set of experiments we are interested in how
well our model recovers /t/s when it is provided
with the gold word boundaries. This allows us
to investigate the strength of the statistical sig-
nal for the deletion rule without confounding it
with the word segmentation performance, and to
see how the different contextual settings uniform,
right and left-right handle the data. Concretely,
for the example in Figure 6 this means that we tell
the model that there are boundaries between /aI/,
/doUn/, /IntEnd/, /tu/ and /liv/ but we don?t tell it
whether or not these words end in an underlying
/t/. Even in this simple example, there are 5 possi-
ble positions for the model to posit an underlying
/t/. We evaluate the model in terms of F-score, the
harmonic mean of recall (the fraction of underly-
ing /t/s the model correctly recovered) and preci-
sion (the fraction of underlying /t/s the model pre-
dicted that were correct).
In these experiments, we ran a total of 2500 it-
erations with a burnin of 2000. We collect sam-
ples with a lag of 10 for the last 500 iterations and
perform maximum marginal decoding over these
samples (Johnson and Goldwater, 2009), as well
as running two chains so as to get an idea of the
variance.5
We are also interested in how well the model
can infer the rule probabilities from the data, that
is, whether it can learn values for the different ?c
parameters. We compare two settings, one where
we perform inference for these parameters assum-
ing a uniform Beta prior on each ?c (LEARN-?)
and one where we provide the model with the em-
pirical probabilities for each ?c as estimated off
the gold-data (GOLD-?), e.g., for the uniform con-
dition 0.29. The results are shown in Table 2.
Best performance for both the Unigram and
the Bigram model in the GOLD-? condition is
achieved under the left-right setting, in line with
the standard analyses of /t/-deletion as primarily
being determined by the preceding and the follow-
ing context. For the LEARN-? condition, the Bi-
gram model still performs best in the left-right set-
ting but the Unigram model?s performance drops
5As manually setting the hyper-parameters for the DPs in
our model proved to be complicated and may be objected to
on principled grounds, we perform inference for them under
a vague gamma prior, as suggested by Teh et al (2006) and
Johnson and Goldwater (2009), using our own implementa-
tion of a slice-sampler (Neal, 2003).
uniform right left-right
Unigram LEARN-? 56.52 39.28 23.59GOLD-? 62.08 60.80 66.15
Bigram LEARN-? 60.85 62.98 77.76GOLD-? 69.06 69.98 73.45
Table 2: F-score of recovered /t/s with known
word boundaries on real data for the three differ-
ent context settings, averaged over two runs (all
standard errors below 2%). Note how the Uni-
gram model always suffers in the LEARN-? condi-
tion whereas the Bigram model?s performance is
actually best for LEARN-? in the left-right setting.
C C C V C $ V C V V V $
empirical 0.62 0.42 0.36 0.23 0.15 0.07
Unigram 0.41 0.33 0.17 0.07 0.05 0.00
Bigram 0.70 0.58 0.43 0.17 0.13 0.06
Table 3: Inferred rule-probabilities for different
contexts in the left-right setting from one of the
runs. ?C C? stands for the context where the
deleted /t/ is preceded and followed by a conso-
nant, ?V $? stands for the context where it is pre-
ceded by a vowel and followed by the utterance
boundary. Note how the Unigram model severely
under-estimates and the Bigram model slightly
over-estimates the probabilities.
in all settings and is now worst in the left-right and
best in the uniform setting.
In fact, comparing the inferred probabilities
to the ?ground truth? indicates that the Bigram
model estimates the true probabilities more ac-
curately than the Unigram model, as illustrated
in Table 3 for the left-right setting. The Bi-
gram model somewhat overestimates the probabil-
ity for all post-consonantal contexts but the Uni-
gram model severely underestimates the probabil-
ity of /t/-deletion across all contexts.
4.3 Artificial data experiments
To test our Gibbs sampling inference procedure,
we ran it on artificial data generated according to
the model itself. If our inference procedure fails
to recover the underlying /t/s accurately in this set-
ting, we should not expect it to work well on actual
data. We generated our artificial data as follows.
We transformed the sequence of canonical pronun-
ciations in the Buckeye corpus (which we take to
be underlying forms here) by randomly deleting
final /t/s using empirical probabilities as shown in
Table 3 to generate a sequence of artificial sur-
face forms that serve as input to our models. We
1513
uniform right left-right
Unigram LEARN-? 94.35 23.55 (+) 63.06GOLD-? 94.45 94.20 91.83
Bigram LEARN-? 92.72 91.64 88.48GOLD-? 92.88 92.33 89.32
Table 4: F-score of /t/-recovery with known word
boundaries on artificial data, each condition tested
on data that corresponds to the assumption, aver-
aged over two runs (standard errors less than 2%
except (+) = 3.68%)).
Unigram Bigram
LEARN-? 33.58 55.64
GOLD-? 55.92 57.62
Table 5: /t/-recovery F-scores when performing
joint word segmention in the left-right setting, av-
eraged over two runs (standard errors less than
2%). See Table 6 for the corresponding segmenta-
tion F-scores.
did this for all three context settings, always es-
timating the deletion probability for each context
from the gold-standard. The results of these exper-
iments are given in table 4. Interestingly, perfor-
mance on these artificial data is considerably bet-
ter than on the real data. In particular the Bigram
model is able to get consistently high F-scores for
both the LEARN-? and the GOLD-? setting. For
the Unigram model, we again observe the severe
drop in the LEARN-? setting for the right and left-
right settings although it does remarkably well in
the uniform setting, and performs well across all
settings in the GOLD-? condition. We take this to
show that our inference algorithm is in fact work-
ing as expected.
4.4 Segmentation experiments
Finally, we are also interested to learn how well
we can do word segmentation and underlying /t/-
recovery jointly. Again, we look at both the
LEARN-? and GOLD-? conditions but focus on the
left-right setting as this worked best in the exper-
iments above. For these experiments, we perform
simulated annealing throughout the initial 2000 it-
erations, gradually cooling the temperature from
5 to 1, following the observation by Goldwater
et al (2009) that without annealing, the Bigram
model gets stuck in sub-optimal parts of the solu-
tion space early on. During the annealing stage,
we prevent the model from performing inference
for underlying /t/s so that the annealing stage can
be seen as an elaborate initialisation scheme, and
we perform joint inference for the remaining 500
iterations, evaluating on the last sample and av-
eraging over two runs. As neither the Unigram
nor the Bigram model performs ?perfect? word
segmentation, we expect to see a degradation in
/t/-recovery performance and this is what we find
indeed. To give an impression of the impact of
/t/-deletion, we also report numbers for running
only the segmentation model on the Buckeye data
with no deleted /t/s and on the data with deleted
/t/s. The /t/-recovery scores are given in Table 5
and segmentation scores in Table 6. Again the
Unigram model?s /t/-recovery score degrades dra-
matically in the LEARN-? condition. Looking at
the segmentation performance this isn?t too sur-
prising: the Unigram model?s poorer token F-
score, the standard measure of segmentation per-
formance on a word token level, suggests that it
misses many more boundaries than the Bigram
model to begin with and, consequently, can?t re-
cover any potential underlying /t/s at these bound-
aries. Also note that in the GOLD-? condition, our
joint Bigram model performs almost as well on
data with /t/-deletions as the word segmentation
model on data that includes no variation at all.
The generally worse performance of handling
variation as measured by /t/-recovery F-score
when performing joint segmentation is consistent
with the finding of Elsner et al (2012) who report
considerable performance drops for their phono-
logical learner when working with induced bound-
aries (note, however, that their model does not per-
form joint inference, rather the induced boundaries
are given to their phonological learner as ground-
truth).
5 Discussion
There are two interesting findings from our exper-
iments. First of all, we find a much larger differ-
ence between the Unigram and the Bigram model
in the LEARN-? condition than in the GOLD-? con-
dition. We suggest that this is due to the Unigram
model?s lack of dependencies between underlying
forms, depriving it of an important source of ev-
idence. Bigram dependencies provide additional
evidence for underlying /t/ that are deleted on the
surface, and because the Bigram model identifies
these underlying /t/ more accurately, it can also es-
timate the /t/ deletion probability more accurately.
1514
Unigram Bigram
LEARN-? 54.53 72.55 (2.3%)
GOLD-? 54.51 73.18
NO-? 54.61 70.12
NO-VAR 54.12 73.99
Table 6: Word segmentation F-scores for the /t/-
recovery F-scores in Table 5 averaged over two
runs (standard errors less than 2% unless given).
NO-? are scores for running just the word segmen-
tation model with no /t/-deletion rule on the data
that includes /t/-deletion, NO-VAR for running just
the word segmentation model on the data with no
/t/-deletions.
For example, /t/ dropping in ?don?t you? yields
surface forms ?don you?. Because the word bi-
gram probability P (you | don?t) is high, the bi-
gram model prefers to analyse surface ?don? as
underlying ?don?t?. The Unigram model does not
have access to word bigram information so the
underlying forms it posits are less accurate (as
shown in Table 2), and hence the estimate of the
/t/-deletion probability is also less accurate. When
the probabilities of deletion are pre-specified the
Unigram model performs better but still consider-
ably worse than the Bigram model when the word
boundaries are known, suggesting the importance
of non-phonological contextual effects that the Bi-
gram model but not the Unigram model can cap-
ture. This suggests that for example word pre-
dictability in context might be an important factor
contributing to /t/-deletion.
The other striking finding is the considerable
drop in performance between running on natural-
istic and artificially created data. This suggests
that the natural distribution of /t/-deletion is much
more complex than can be captured by statistics
over the phonological contexts we examined. Fol-
lowing Guy (1991), a finer-grained distinction for
the preceeding segments might address this prob-
lem.
Yet another suggestion comes from the recent
work in Coetzee and Kawahara (2013) who claim
that ?[a] model that accounts perfectly for the
overall rate of application of some variable pro-
cess therefore does not necessarily account very
well for the actual application of the process to in-
dividual words.? They argue that in particular the
extremely high deletion rates typical of high fre-
quency items aren?t accurately captured when the
deletion probability is estimated across all types.
A look at the error patterns of our model on a sam-
ple from the Bigram model in the LEARN-? setting
on the naturalistic data suggests that this is in fact a
problem. For example, the word ?just? has an ex-
tremely high rate of deletion with 17462442 = 0.71%.While many tokens of ?jus? are ?explained away?
through predicting underlying /t/s, the (literally)
extra-ordinary frequency of ?jus?-tokens lets our
model still posit it as an underlying form, although
with a much dampened frequency (of the 1746 sur-
face tokens, 1081 are analysed as being realiza-
tions of an underlying ?just?).
The /t/-recovery performance drop when per-
forming joint word segmentation isn?t surprising
as even the Bigram model doesn?t deliver a very
high-quality segmentation to begin with, leading
to both sparsity (through missed word-boundaries)
and potential noise (through misplaced word-
boundaries). Using a more realistic generative
process for the underlying forms, for example an
Adaptor Grammar (Johnson et al, 2007), could
address this shortcoming in future work without
changing the overall architecture of the model al-
though novel inference algorithms might be re-
quired.
6 Conclusion and outlook
We presented a joint model for word segmentation
and the learning of phonological rule probabili-
ties from a corpus of transcribed speech. We find
that our Bigram model reaches 77% /t/-recovery
F-score when run with knowledge of true word-
boundaries and when it can make use of both the
preceeding and the following phonological con-
text, and that unlike the Unigram model it is able
to learn the probability of /t/-deletion in different
contexts. When performing joint word segmen-
tation on the Buckeye corpus, our Bigram model
reaches around above 55% F-score for recovering
deleted /t/s with a word segmentation F-score of
around 72% which is 2% better than running a Bi-
gram model that does not model /t/-deletion.
We identified additional factors that might help
handling /t/-deletion and similar phenomena. A
major advantage of our generative model is the
ease and transparency with which its assump-
tions can be modified and extended. For fu-
ture work we plan to incorporate into our model
richer phonological contexts, item- and frequency-
specific probabilities and more direct use of word
1515
predictability. We also plan to extend our model
to handle additional phenomena, an obvious can-
didate being /d/-deletion.
Also, the two-level architecture we present is
not limited to the mapping being defined in terms
of rules rather than constraints in the spirit of Op-
timality Theory (Prince and Smolensky, 2004); we
plan to explore this alternative path as well in fu-
ture work.
To conclude, we presented a model that pro-
vides a clean framework to test the usefulness of
different factors for word segmentation and han-
dling phonological variation in a controlled man-
ner.
Acknowledgements
We thank the anonymous reviewers for their
valuable comments. This research was sup-
ported under Australian Research Council?s Dis-
covery Projects funding scheme (project numbers
DP110102506 and DP110102593).
References
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Haper & Row, New York.
Andries W. Coetzee and Shigeto Kawahara. 2013. Fre-
quency biases in phonological variation. Natural
Language and Linguisic Theory, 31:47?89.
Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts ,
Amherst.
Laura Dilley, Amanda Millett, J. Devin McAuley, and
Tonya R. Bergeson. to appear. Phonetic variation
in consonants in infant-directed and adult-directed
speech: The case of regressive place assimilation
in word-final alveolar stops. Journal of Child Lan-
guage.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 184?193, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3(2):223?39.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1531?
1536, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Dennis Norris, James M. Mcqueen, Anne Cutler, and
Sally Butterfield. 1997. The possible-word con-
straint in the segmentation of continuous speech.
Cognitive Psychology, 34(3):191 ? 243.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
1516
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 282?292,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Modelling function words improves unsupervised word segmentation
Mark Johnson
1,2
, Anne Christophe
3,4
, Katherine Demuth
2,6
and Emmanuel Dupoux
3,5
1
Department of Computing, Macquarie University, Sydney, Australia
2
Santa Fe Institute, Santa Fe, New Mexico, USA
3
Ecole Normale Sup?erieure, Paris, France
4
Centre National de la Recherche Scientifique, Paris, France
5
Ecole des Hautes Etudes en Sciences Sociales, Paris, France
6
Department of Linguistics, Macquarie University, Sydney, Australia
Abstract
Inspired by experimental psychological
findings suggesting that function words
play a special role in word learning, we
make a simple modification to an Adaptor
Grammar based Bayesian word segmenta-
tion model to allow it to learn sequences
of monosyllabic ?function words? at the
beginnings and endings of collocations
of (possibly multi-syllabic) words. This
modification improves unsupervised word
segmentation on the standard Bernstein-
Ratner (1987) corpus of child-directed En-
glish by more than 4% token f-score com-
pared to a model identical except that it
does not special-case ?function words?,
setting a new state-of-the-art of 92.4% to-
ken f-score. Our function word model as-
sumes that function words appear at the
left periphery, and while this is true of
languages such as English, it is not true
universally. We show that a learner can
use Bayesian model selection to determine
the location of function words in their lan-
guage, even though the input to the model
only consists of unsegmented sequences of
phones. Thus our computational models
support the hypothesis that function words
play a special role in word learning.
1 Introduction
Over the past two decades psychologists have in-
vestigated the role that function words might play
in human language acquisition. Their experiments
suggest that function words play a special role in
the acquisition process: children learn function
words before they learn the vast bulk of the asso-
ciated content words, and they use function words
to help identify context words.
The goal of this paper is to determine whether
computational models of human language acqui-
sition can provide support for the hypothesis that
function words are treated specially in human
language acquisition. We do this by comparing
two computational models of word segmentation
which differ solely in the way that they model
function words. Following Elman et al (1996)
and Brent (1999) our word segmentation models
identify word boundaries from unsegmented se-
quences of phonemes corresponding to utterances,
effectively performing unsupervised learning of a
lexicon. For example, given input consisting of
unsegmented utterances such as the following:
j u w ? n t t u s i ? ? b ? k
a word segmentation model should segment this as
ju w?nt tu si ?? b?k, which is the IPA representation
of ?you want to see the book?.
We show that a model equipped with the abil-
ity to learn some rudimentary properties of the
target language?s function words is able to learn
the vocabulary of that language more accurately
than a model that is identical except that it is inca-
pable of learning these generalisations about func-
tion words. This suggests that there are acqui-
sition advantages to treating function words spe-
cially that human learners could take advantage of
(at least to the extent that they are learning similar
generalisations as our models), and thus supports
the hypothesis that function words are treated spe-
cially in human lexical acquisition. As a reviewer
points out, we present no evidence that children
use function words in the way that our model does,
and we want to emphasise we make no such claim.
While absolute accuracy is not directly relevant
to the main point of the paper, we note that the
models that learn generalisations about function
words perform unsupervised word segmentation
at 92.5% token f-score on the standard Bernstein-
Ratner (1987) corpus, which improves the previ-
ous state-of-the-art by more than 4%.
As a reviewer points out, the changes we make
to our models to incorporate function words can
be viewed as ?building in? substantive informa-
tion about possible human languages. The model
282
that achieves the best token f-score expects func-
tion words to appear at the left edge of phrases.
While this is true for languages such as English,
it is not true universally. By comparing the pos-
terior probability of two models ? one in which
function words appear at the left edges of phrases,
and another in which function words appear at the
right edges of phrases ? we show that a learner
could use Bayesian posterior probabilities to deter-
mine that function words appear at the left edges
of phrases in English, even though they are not
told the locations of word boundaries or which
words are function words.
This paper is structured as follows. Section 2
describes the specific word segmentation mod-
els studied in this paper, and the way we ex-
tended them to capture certain properties of func-
tion words. The word segmentation experiments
are presented in section 3, and section 4 discusses
how a learner could determine whether function
words occur on the left-periphery or the right-
periphery in the language they are learning. Sec-
tion 5 concludes and describes possible future
work. The rest of this introduction provides back-
ground on function words, the Adaptor Grammar
models we use to describe lexical acquisition and
the Bayesian inference procedures we use to infer
these models.
1.1 Psychological evidence for the role of
function words in word learning
Traditional descriptive linguistics distinguishes
function words, such as determiners and prepo-
sitions, from content words, such as nouns and
verbs, corresponding roughly to the distinction be-
tween functional categories and lexical categories
of modern generative linguistics (Fromkin, 2001).
Function words differ from content words in at
least the following ways:
1. there are usually far fewer function word
types than content word types in a language
2. function word types typically have much
higher token frequency than content word
types
3. function words are typically morphologically
and phonologically simple (e.g., they are typ-
ically monosyllabic)
4. function words typically appear in peripheral
positions of phrases (e.g., prepositions typi-
cally appear at the beginning of prepositional
phrases)
5. each function word class is associated with
specific content word classes (e.g., deter-
miners and prepositions are associated with
nouns, auxiliary verbs and complementisers
are associated with main verbs)
6. semantically, content words denote sets of
objects or events, while function words de-
note more complex relationships over the en-
tities denoted by content words
7. historically, the rate of innovation of function
words is much lower than the rate of innova-
tion of content words (i.e., function words are
typically ?closed class?, while content words
are ?open class?)
Properties 1?4 suggest that function words
might play a special role in language acquisition
because they are especially easy to identify, while
property 5 suggests that they might be useful for
identifying lexical categories. The models we
study here focus on properties 3 and 4, in that
they are capable of learning specific sequences of
monosyllabic words in peripheral (i.e., initial or
final) positions of phrase-like units.
A number of psychological experiments have
shown that infants are sensitive to the function
words of their language within their first year of
life (Shi et al, 2006; Hall?e et al, 2008; Shafer
et al, 1998), often before they have experienced
the ?word learning spurt?. Crucially for our pur-
pose, infants of this age were shown to exploit
frequent function words to segment neighboring
content words (Shi and Lepage, 2008; Hall?e et
al., 2008). In addition, 14 to 18-month-old
children were shown to exploit function words to
constrain lexical access to known words - for in-
stance, they expect a noun after a determiner (Cau-
vet et al, 2014; Kedar et al, 2006; Zangl and
Fernald, 2007). In addition, it is plausible that
function words play a crucial role in children?s
acquisition of more complex syntactic phenom-
ena (Christophe et al, 2008; Demuth and McCul-
lough, 2009), so it is interesting to investigate the
roles they might play in computational models of
language acquisition.
1.2 Adaptor grammars
Adaptor grammars are a framework for Bayesian
inference of a certain class of hierarchical non-
parametric models (Johnson et al, 2007b). They
define distributions over the trees specified by
a context-free grammar, but unlike probabilistic
context-free grammars, they ?learn? distributions
over the possible subtrees of a user-specified set of
?adapted? nonterminals. (Adaptor grammars are
non-parametric, i.e., not characterisable by a finite
283
set of parameters, if the set of possible subtrees
of the adapted nonterminals is infinite). Adaptor
grammars are useful when the goal is to learn a
potentially unbounded set of entities that need to
satisfy hierarchical constraints. As section 2 ex-
plains in more detail, word segmentation is such
a case: words are composed of syllables and be-
long to phrases or collocations, and modelling this
structure improves word segmentation accuracy.
Adaptor Grammars are formally defined in
Johnson et al (2007b), which should be consulted
for technical details. Adaptor Grammars (AGs)
are an extension of Probabilistic Context-Free
Grammars (PCFGs), which we describe first. A
Context-Free Grammar (CFG) G = (N,W,R, S)
consists of disjoint finite sets of nonterminal sym-
bols N and terminal symbols W , a finite set of
rules R of the form A?? where A ? N and
? ? (N ? W )
?
, and a start symbol S ? N . (We
assume there are no ??-rules? inR, i.e., we require
that |?| ? 1 for each A?? ? R).
A Probabilistic Context-Free Grammar (PCFG)
is a quintuple (N,W,R, S,?) where N , W , R
and S are the nonterminals, terminals, rules and
start symbol of a CFG respectively, and ? is a vec-
tor of non-negative reals indexed by R that sat-
isfy
?
??R
A
?
A??
= 1 for each A ? N , where
R
A
= {A?? : A?? ? R} is the set of rules
expanding A.
Informally, ?
A??
is the probability of a node
labelled A expanding to a sequence of nodes la-
belled ?, and the probability of a tree is the prod-
uct of the probabilities of the rules used to con-
struct each non-leaf node in it. More precisely, for
each X ? N ?W a PCFG associates distributions
G
X
over the set of trees T
X
generated by X as
follows:
If X ? W (i.e., if X is a terminal) then G
X
is the distribution that puts probability 1 on the
single-node tree labelled X .
If X ? N (i.e., if X is a nonterminal) then:
G
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
) (1)
where R
X
is the subset of rules in R expanding
nonterminal X ? N , and:
TD
X
(G
1
, . . . , G
n
)
(
.
X
t
1
t
n
. . .
)
=
n
?
i=1
G
i
(t
i
).
That is, TD
X
(G
1
, . . . , G
n
) is a distribution over
the set of trees T
X
generated by nonterminal X ,
where each subtree t
i
is generated independently
from G
i
. The PCFG generates the distribution G
S
over the set of trees T
S
generated by the start sym-
bol S; the distribution over the strings it generates
is obtained by marginalising over the trees.
In a Bayesian PCFG one puts Dirichlet priors
Dir(?) on the rule probability vector ?, such that
there is one Dirichlet parameter ?
A??
for each
rule A?? ? R. There are Markov Chain Monte
Carlo (MCMC) and Variational Bayes procedures
for estimating the posterior distribution over rule
probabilities ? and parse trees given data consist-
ing of terminal strings alone (Kurihara and Sato,
2006; Johnson et al, 2007a).
PCFGs can be viewed as recursive mixture
models over trees. While PCFGs are expres-
sive enough to describe a range of linguistically-
interesting phenomena, PCFGs are parametric
models, which limits their ability to describe phe-
nomena where the set of basic units, as well as
their properties, are the target of learning. Lexi-
cal acqusition is an example of a phenomenon that
is naturally viewed as non-parametric inference,
where the number of lexical entries (i.e., words)
as well as their properties must be learnt from the
data.
It turns out there is a straight-forward modifica-
tion to the PCFG distribution (1) that makes it suit-
ably non-parametric. As Johnson et al (2007b)
explain, by inserting a Dirichlet Process (DP)
or Pitman-Yor Process (PYP) into the generative
mechanism (1) the model ?concentrates? mass on
a subset of trees (Teh et al, 2006). Specifically,
an Adaptor Grammar identifies a subset A ? N
of adapted nonterminals. In an Adaptor Gram-
mar the unadapted nonterminals N \ A expand
via (1), just as in a PCFG, but the distributions of
the adapted nonterminals A are ?concentrated? by
passing them through a DP or PYP:
H
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
)
G
X
= PYP(H
X
, a
X
, b
X
)
Here a
X
and b
X
are parameters of the PYP asso-
ciated with the adapted nonterminal X . As Gold-
water et al (2011) explain, such Pitman-Yor Pro-
cesses naturally generate power-law distributed
data.
Informally, Adaptor Grammars can be viewed
as caching entire subtrees of the adapted nonter-
minals. Roughly speaking, the probability of gen-
erating a particular subtree of an adapted nonter-
minal is proportional to the number of times that
subtree has been generated before. This ?rich get
284
richer? behaviour causes the distribution of sub-
trees to follow a power-law (the power is speci-
fied by the a
X
parameter of the PYP). The PCFG
rules expanding an adapted nonterminal X de-
fine the ?base distribution? of the associated DP
or PYP, and the a
X
and b
X
parameters determine
how much mass is reserved for ?new? trees.
There are several different procedures for infer-
ring the parse trees and the rule probabilities given
a corpus of strings: Johnson et al (2007b) describe
aMCMC sampler and Cohen et al (2010) describe
a Variational Bayes procedure. We use the MCMC
procedure here since this has been successfully ap-
plied to word segmentation problems in previous
work (Johnson, 2008).
2 Word segmentation with Adaptor
Grammars
Perhaps the simplest word segmentation model is
the unigram model, where utterances are modeled
as sequences of words, and where each word is
a sequence of segments (Brent, 1999; Goldwater
et al, 2009). A unigram model can be expressed
as an Adaptor Grammar with one adapted non-
terminal Word (we indicate adapted nonterminals
by underlining them in grammars here; regular ex-
pressions are expanded into right-branching pro-
ductions).
Sentence?Word
+
(2)
Word?Phone
+
(3)
The first rule (2) says that a sentence consists of
one or more Words, while the second rule (3)
states that a Word consists of a sequence of one or
more Phones; we assume that there are rules ex-
panding Phone into all possible phones. Because
Word is an adapted nonterminal, the adaptor gram-
mar memoises Word subtrees, which corresponds
to learning the phone sequences for the words of
the language.
The more sophisticated Adaptor Grammars dis-
cussed below can be understood as specialis-
ing either the first or the second of the rules
in (2?3). The next two subsections review the
Adaptor Grammar word segmentation models pre-
sented in Johnson (2008) and Johnson and Gold-
water (2009): section 2.1 reviews how phonotac-
tic syllable-structure constraints can be expressed
with Adaptor Grammars, while section 2.2 re-
views how phrase-like units called ?collocations?
capture inter-word dependencies. Section 2.3
presents the major novel contribution of this paper
by explaining how we modify these adaptor gram-
mars to capture some of the special properties of
function words.
2.1 Syllable structure and phonotactics
The rule (3) models words as sequences of inde-
pendently generated phones: this is what Gold-
water et al (2009) called the ?monkey model? of
word generation (it instantiates the metaphor that
word types are generated by a monkey randomly
banging on the keys of a typewriter). However, the
words of a language are typically composed of one
or more syllables, and explicitly modelling the in-
ternal structure of words typically improves word
segmentation considerably.
Johnson (2008) suggested replacing (3) with the
following model of word structure:
Word? Syllable
1:4
(4)
Syllable?(Onset)Rhyme (5)
Onset?Consonant
+
(6)
Rhyme?Nucleus (Coda) (7)
Nucleus?Vowel
+
(8)
Coda?Consonant
+
(9)
Here and below superscripts indicate iteration
(e.g., a Word consists of 1 to 4 Syllables), while
an Onset consists of an unbounded number of
Consonants), while parentheses indicate option-
ality (e.g., a Rhyme consists of an obligatory
Nucleus followed by an optional Coda). We as-
sume that there are rules expanding Consonant
and Vowel to the set of all consonants and vow-
els respectively (this amounts to assuming that the
learner can distinguish consonants from vowels).
Because Onset, Nucleus and Coda are adapted,
this model learns the possible syllable onsets, nu-
cleii and coda of the language, even though neither
syllable structure nor word boundaries are explic-
itly indicated in the input to the model.
The model just described assumes that word-
internal syllables have the same structure as word-
peripheral syllables, but in languages such as
English word-peripheral onsets and codas can
be more complex than the corresponding word-
internal onsets and codas. For example, the
word ?string? begins with the onset cluster str,
which is relatively rare word-internally. Johnson
(2008) showed that word segmentation accuracy
improves if the model can learn different conso-
nant sequences for word-inital onsets and word-
final codas. It is easy to express this as an Adaptor
285
Grammar: (4) is replaced with (10?11) and (12?
17) are added to the grammar.
Word? SyllableIF (10)
Word? SyllableI Syllable
0:2
SyllableF (11)
SyllableIF?(OnsetI)RhymeF (12)
SyllableI?(OnsetI)Rhyme (13)
SyllableF?(Onset)RhymeF (14)
OnsetI?Consonant
+
(15)
RhymeF?Nucleus (CodaF) (16)
CodaF?Consonant
+
(17)
In this grammar the suffix ?I? indicates a word-
initial element, and ?F? indicates a word-final el-
ement. Note that the model simply has the abil-
ity to learn that different clusters can occur word-
peripherally and word-internally; it is not given
any information about the relative complexity of
these clusters.
2.2 Collocation models of inter-word
dependencies
Goldwater et al (2009) point out the detrimental
effect that inter-word dependencies can have on
word segmentation models that assume that the
words of an utterance are independently gener-
ated. Informally, a model that generates words in-
dependently is likely to incorrectly segment multi-
word expressions such as ?the doggie? as single
words because the model has no way to capture
word-to-word dependencies, e.g., that ?doggie? is
typically preceded by ?the?. Goldwater et alshow
that word segmentation accuracy improves when
the model is extended to capture bigram depen-
dencies.
Adaptor grammar models cannot express bi-
gram dependencies, but they can capture similiar
inter-word dependencies using phrase-like units
that Johnson (2008) calls collocations. John-
son and Goldwater (2009) showed that word seg-
mentation accuracy improves further if the model
learns a nested hierarchy of collocations. This can
be achieved by replacing (2) with (18?21).
Sentence?Colloc3
+
(18)
Colloc3?Colloc2
+
(19)
Colloc2?Colloc1
+
(20)
Colloc1?Word
+
(21)
Informally, Colloc1, Colloc2 and Colloc3 define a
nested hierarchy of phrase-like units. While not
designed to correspond to syntactic phrases, by ex-
amining the sample parses induced by the Adaptor
Grammar we noticed that the collocations often
correspond to noun phrases, prepositional phrases
or verb phrases. This motivates the extension to
the Adaptor Grammar discussed below.
2.3 Incorporating ?function words? into
collocation models
The starting point and baseline for our extension
is the adaptor grammar with syllable structure
phonotactic constraints and three levels of collo-
cational structure (5-21), as prior work has found
that this yields the highest word segmentation to-
ken f-score (Johnson and Goldwater, 2009).
Our extension assumes that the Colloc1 ?
Colloc3 constituents are in fact phrase-like, so we
extend the rules (19?21) to permit an optional se-
quence of monosyllabic words at the left edge
of each of these constituents. Our model thus
captures two of the properties of function words
discussed in section 1.1: they are monosyllabic
(and thus phonologically simple), and they appear
on the periphery of phrases. (We put ?function
words? in scare quotes below because our model
only approximately captures the linguistic proper-
ties of function words).
Specifically, we replace rules (19?21) with the
following sequence of rules:
Colloc3?(FuncWords3)Colloc2
+
(22)
Colloc2?(FuncWords2)Colloc1
+
(23)
Colloc1?(FuncWords1)Word
+
(24)
FuncWords3?FuncWord3
+
(25)
FuncWord3? SyllableIF (26)
FuncWords2?FuncWord2
+
(27)
FuncWord2? SyllableIF (28)
FuncWords1?FuncWord1
+
(29)
FuncWord1? SyllableIF (30)
This model memoises (i.e., learns) both the in-
dividual ?function words? and the sequences of
?function words? that modify the Colloc1 ?
Colloc3 constituents. Note also that ?function
words? expand directly to SyllableIF, which in
turn expands to a monosyllable with a word-initial
onset and word-final coda. This means that ?func-
tion words? are memoised independently of the
?content words? that Word expands to; i.e., the
model learns distinct ?function word? and ?con-
tent word? vocabularies. Figure 1 depicts a sample
parse generated by this grammar.
286
.Sentence
Colloc3
FuncWords3
FuncWord3
you
FuncWord3
want
FuncWord3
to
Colloc2
Colloc1
Word
see
Colloc1
FuncWords1
FuncWord1
the
Word
book
Figure 1: A sample parse generated by the ?func-
tion word? Adaptor Grammar with rules (10?18)
and (22?30). To simplify the parse we only show
the root node and the adapted nonterminals, and
replace word-internal structure by the word?s or-
thographic form.
This grammar builds in the fact that function
words appear on the left periphery of phrases. This
is true of languages such as English, but is not true
cross-linguistically. For comparison purposes we
also include results for a mirror-image model that
permits ?function words? on the right periphery,
a model which permits ?function words? on both
the left and right periphery (achieved by changing
rules 22?24), as well as a model that analyses all
words as monosyllabic.
Section 4 explains how a learner could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the posterior probability of the data
under our ?function word? Adaptor Grammar to
that obtained using a grammar which is identi-
cal except that rules (22?24) are replaced with the
mirror-image rules in which ?function words? are
attached to the right periphery.
3 Word segmentation results
This section presents results of running our Adap-
tor Grammar models on subsets of the Bernstein-
Ratner (1987) corpus of child-directed English.
We use the Adaptor Grammar software available
from http://web.science.mq.edu.au/?mjohnson/
with the same settings as described in Johnson
and Goldwater (2009), i.e., we perform Bayesian
inference with ?vague? priors for all hyperpa-
rameters (so there are no adjustable parameters
in our models), and perform 8 different MCMC
runs of each condition with table-label resampling
for 2,000 sweeps of the training data. At every
10th sweep of the last 1,000 sweeps we use the
model to segment the entire corpus (even if it
is only trained on a subset of it), so we collect
Model
Token
f-score
Boundary
precision
Boundary
recall
Baseline 0.872 0.918 0.956
+ left FWs 0.924 0.935 0.990
+ left + right FWs 0.912 0.957 0.953
Table 1: Mean token f-scores and boundary preci-
sion and recall results averaged over 8 trials, each
consisting of 8 MCMC runs of models trained
and tested on the full Bernstein-Ratner (1987) cor-
pus (the standard deviations of all values are less
than 0.006; Wilcox sign tests show the means of
all token f-scores differ p < 2e-4).
800 sample segmentations of each utterance.
The most frequent segmentation in these 800
sample segmentations is the one we score in the
evaluations below.
3.1 Word segmentation with ?function word?
models
Here we evaluate the word segmentations found
by the ?function word? Adaptor Grammar model
described in section 2.3 and compare it to the base-
line grammar with collocations and phonotactics
from Johnson and Goldwater (2009). Figure 2
presents the standard token and lexicon (i.e., type)
f-score evaluations for word segmentations pro-
posed by these models (Brent, 1999), and Table 1
summarises the token and lexicon f-scores for the
major models discussed in this paper. It is interest-
ing to note that adding ?function words? improves
token f-score by more than 4%, corresponding to
a 40% reduction in overall error rate.
When the training data is very small the Mono-
syllabic grammar produces the highest accuracy
results, presumably because a large proportion of
the words in child-directed speech are monosyl-
labic. However, at around 25 sentences the more
complex models that are capable of finding multi-
syllabic words start to become more accurate.
It?s interesting that after about 1,000 sentences
the model that allows ?function words? only on
the right periphery is considerably less accurate
than the baseline model. Presumably this is be-
cause it tends to misanalyse multi-syllabic words
on the right periphery as sequences of monosyl-
labic words.
The model that allows ?function words? only on
the left periphery is more accurate than the model
that allows them on both the left and right periph-
ery when the input data ranges from about 100 to
about 1,000 sentences, but when the training data
287
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Tok
enWf
-sco
re
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WFWs
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Lex
icon
Wf-sc
ore
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WrightWFWs
Figure 2: Token and lexicon (i.e., type) f-score on the Bernstein-Ratner (1987) corpus as a function of
training data size for the baseline model, the model where ?function words? can appear on the left pe-
riphery, a model where ?function words? can appear on the right periphery, and a model where ?function
words? can appear on both the left and the right periphery. For comparison purposes we also include
results for a model that assumes that all words are monosyllabic.
is larger than about 1,000 sentences both models
are equally accurate.
3.2 Content and function words found by
?function word? model
As noted earlier, the ?function word? model gen-
erates function words via adapted nonterminals
other than the Word category. In order to bet-
ter understand just how the model works, we give
the 5 most frequent words in each word category
found during 8 MCMC runs of the left-peripheral
?function word? grammar above:
Word : book, doggy, house, want, I
FuncWord1 : a, the, your, little
1
, in
FuncWord2 : to, in, you, what, put
FuncWord3 : you, a, what, no, can
Interestingly, these categories seem fairly rea-
sonable. The Word category includes open-class
nouns and verbs, the FuncWord1 category in-
cludes noun modifiers such as determiners, while
the FuncWord2 and FuncWord3 categories in-
clude prepositions, pronouns and auxiliary verbs.
1
The phone ?l? is generated by both Consonant and
Vowel, so ?little? can be (incorrectly) analysed as one syl-
lable.
Thus, the present model, initially aimed at seg-
menting words from continuous speech, shows
three interesting characteristics that are also ex-
hibited by human infants: it distinguishes be-
tween function words and content words (Shi and
Werker, 2001), it allows learners to acquire at least
some of the function words of their language (e.g.
(Shi et al, 2006)); and furthermore, it may also al-
low them to start grouping together function words
according to their category (Cauvet et al, 2014;
Shi and Melanc?on, 2010).
4 Are ?function words? on the left or
right periphery?
We have shown that a model that expects function
words on the left periphery performs more accu-
rate word segmentation on English, where func-
tion words do indeed typically occur on the left
periphery, leaving open the question: how could
a learner determine whether function words gen-
erally appear on the left or the right periphery of
phrases in the language they are learning? This
question is important because knowing the side
where function words preferentially occur is re-
288
lated to the question of the direction of syntac-
tic headedness in the language, and an accurate
method for identifying the location of function
words might be useful for initialising a syntac-
tic learner. Experimental evidence suggests that
infants as young as 8 months of age already ex-
pect function words on the correct side for their
language ? left-periphery for Italian infants and
right-periphery for Japanese infants (Gervain et
al., 2008) ? so it is interesting to see whether
purely distributional learners such as the ones
studied here can identify the correct location of
function words in phrases.
We experimented with a variety of approaches
that use a single adaptor grammar inference pro-
cess, but none of these were successful. For ex-
ample, we hoped that given an Adaptor Gram-
mar that permits ?function words? on both the
left and right periphery, the inference procedure
would decide that the right-periphery rules simply
are not used in a language like English. Unfortu-
nately we did not find this in our experiments; the
right-periphery rules were used almost as often as
the left-periphery rules (recall that a large fraction
of the words in English child-directed speech are
monosyllabic).
In this section, we show that learners could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the marginal probability of the data
for the left-periphery and the right-periphery mod-
els.
Instead, we used Bayesian model selection
techniques to determine whether left-peripheral
or a right-peripheral model better fits the un-
segmented utterances that constitute the training
data.
2
While Bayesian model selection is in prin-
ciple straight-forward, it turns out to require the ra-
tio of two integrals (for the ?evidence? or marginal
likelihood) that are often intractable to compute.
Specifically, given a training corpusD of unseg-
mented sentences and model families G
1
and G
2
(here the ?function word? adaptor grammars with
left-peripheral and right-peripheral attachment re-
spectively), the Bayes factor K is the ratio of the
marginal likelihoods of the data:
K =
P(D | G
1
)
P(D | G
2
)
2
Note that neither the left-peripheral nor the right-
peripheral model is correct: even strongly left-headed lan-
guages like English typically contain a few right-headed con-
structions. For example, ?ago? is arguably the head of the
phrase ?ten years ago?.
0
2000
4000
6000
1 10 100 1000 10000Number of training sentences
log B
ayes
 fact
or
Figure 3: Bayes factor in favour of left-peripheral
?function word? attachment as a function of the
number of sentences in the training corpus, cal-
culated using the Harmonic Mean estimator (see
warning in text).
where the marginal likelihood or ?evidence? for a
model G is obtained by integrating over all of the
hidden or latent structure and parameters ?:
P(D | G) =
?
?
P(D,? | G) d? (31)
Here the variable ? ranges over the space ? of all
possible parses for the utterances inD and all pos-
sible configurations of the Pitman-Yor processes
and their parameters that constitute the ?state? of
the Adaptor Grammar G. While the probability of
any specific Adaptor Grammar configuration ? is
not too hard to calculate (the MCMC sampler for
Adaptor Grammars can print this after each sweep
through D), the integral in (31) is in general in-
tractable.
Textbooks such as Murphy (2012) describe a
number of methods for calculating P(D | G), but
most of them assume that the parameter space ?
is continuous and so cannot be directly applied
here. The Harmonic Mean estimator (32) for (31),
which we used here, is a popular estimator for
(31) because it only requires the ability to calcu-
late P(D,? | G) for samples from P(? | D,G):
P(D | G) ?
(
1
n
n
?
i=1
1
P(D,?
i
| G)
)
?1
where ?
i
, . . . ,?
n
are n samples from P(? |
289
D,G), which can be generated by the MCMC pro-
cedure.
Figure 3 depicts how the Bayes factor in favour
of left-peripheral attachment of ?function words?
varies as a function of the number of utter-
ances in the training data D (calculated from the
last 1000 sweeps of 8 MCMC runs of the cor-
responding adaptor grammars). As that figure
shows, once the training data contains more than
about 1,000 sentences the evidence for the left-
peripheral grammar becomes very strong. On the
full training data the estimated log Bayes factor is
over 6,000, which would constitute overwhelming
evidence in favour of left-peripheral attachment.
Unfortunately, as Murphy and others warn, the
Harmonic Mean estimator is extremely unstable
(Radford Neal calls it ?the worst MCMC method
ever? in his blog), so we think it is important to
confirm these results using a more stable estima-
tor. However, given the magnitude of the differ-
ences and the fact that the two models being com-
pared are of similar complexity, we believe that
these results suggest that Bayesian model selec-
tion can be used to determine properties of the lan-
guage being learned.
5 Conclusions and future work
This paper showed that the word segmentation
accuracy of a state-of-the-art Adaptor Grammar
model is significantly improved by extending it
so that it explicitly models some properties of
function words. We also showed how Bayesian
model selection can be used to identify that func-
tion words appear on the left periphery of phrases
in English, even though the input to the model only
consists of an unsegmented sequence of phones.
Of course this work only scratches the surface
in terms of investigating the role of function words
in language acquisition. It would clearly be very
interesting to examine the performance of these
models on other corpora of child-directed English,
as well as on corpora of child-directed speech in
other languages. Our evaluation focused on word-
segmentation, but we could also evaluate the ef-
fect that modelling ?function words? has on other
aspects of the model, such as its ability to learn
syllable structure.
The models of ?function words? we investi-
gated here only capture two of the 7 linguistic
properties of function words identified in section 1
(i.e., that function words tend to be monosyllabic,
and that they tend to appear phrase-peripherally),
so it would be interesting to develop and explore
models that capture other linguistic properties of
function words. For example, following the sug-
gestion by Hochmann et al (2010) that human
learners use frequency cues to identify function
words, it might be interesting to develop computa-
tional models that do the same thing. In an Adap-
tor Grammar the frequency distribution of func-
tion words might be modelled by specifying the
prior for the Pitman-Yor Process parameters asso-
ciated with the function words? adapted nontermi-
nals so that it prefers to generate a small number
of high-frequency items.
It should also be possible to develop models
which capture the fact that function words tend not
to be topic-specific. Johnson et al (2010) and
Johnson et al (2012) show how Adaptor Gram-
mars can model the association between words
and non-linguistic ?topics?; perhaps these models
could be extended to capture some of the semantic
properties of function words.
It would also be interesting to further explore
the extent to which Bayesian model selection is a
useful approach to linguistic ?parameter setting?.
In order to do this it is imperative to develop better
methods than the problematic ?Harmonic Mean?
estimator used here for calculating the evidence
(i.e., the marginal probability of the data) that can
handle the combination of discrete and continuous
hidden structure that occur in computational lin-
guistic models.
As well as substantially improving the accuracy
of unsupervised word segmentation, this work is
interesting because it suggests a connection be-
tween unsupervised word segmentation and the in-
duction of syntactic structure. It is reasonable to
expect that hierarchical non-parametric Bayesian
models such as Adaptor Grammars may be useful
tools for exploring such a connection.
Acknowledgments
This work was supported in part by the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593), the European Research Coun-
cil (ERC-2011-AdG-295810 BOOTPHON), the
Agence Nationale pour la Recherche (ANR-10-
LABX-0087 IEC, and ANR-10-IDEX-0001-02
PSL*), and the Mairie de Paris, Ecole des Hautes
Etudes en Sciences Sociales, the Ecole Normale
Sup?erieure, and the Fondation Pierre Gilles de
Gennes.
290
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, ed-
itors, Children?s Language, volume 6, pages 159?
174. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
E. Cauvet, R. Limissuri, S. Millotte, K. Skoruppa,
D. Cabrol, and A. Christophe. 2014. Function
words constrain on-line recognition of verbs and
nouns in French 18-month-olds. Language Learn-
ing and Development, pages 1?18.
A. Christophe, S. Millotte, S. Bernal, and J. Lidz.
2008. Bootstrapping lexical and syntactic acquisi-
tion. Language and Speech, 51(1-2):61?75.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Vari-
ational inference for adaptor grammars. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 564?572,
Los Angeles, California, June. Association for Com-
putational Linguistics.
K. Demuth and E. McCullough. 2009. The prosodic
(re-)organization of childrens early English articles.
Journal of Child Language, 36(1):173?200.
J. Elman, E. Bates, M. H. Johnson, A. Karmiloff-
Smith, D. Parisi, and K. Plunkett. 1996. Rethink-
ing Innateness: A Connectionist Perspective on De-
velopment. MIT Press/Bradford Books, Cambridge,
MA.
V. Fromkin, editor. 2001. Linguistics: An Introduction
to Linguistic Theory. Blackwell, Oxford, UK.
J. Gervain, M. Nespor, R. Mazuka, R. Horie, and
J. Mehler. 2008. Bootstrapping word order in
prelexical infants: A japaneseitalian cross-linguistic
study. Cognitive Psychology, 57(1):56 ? 74.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition, 112(1):21?
54.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2011.
Producing power-law distributions and damping
word frequencies with two-stage language models.
Journal of Machine Learning Research, 12:2335?
2382.
P. A. Hall?e, C. Durand, and B. de Boysson-Bardies.
2008. Do 11-month-old French infants process ar-
ticles? Language and Speech, 51(1-2):23?44.
J.-R. Hochmann, A. D. Endress, and J. Mehler. 2010.
Word frequency as a cue for identifying function
words in infancy. Cognition, 115(3):444 ? 457.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
M. Johnson, T. Griffiths, and S. Goldwater. 2007a.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?
146, Rochester, New York. Association for Compu-
tational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007b.
Adaptor Grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Sch?olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 641?648. MIT Press, Cambridge, MA.
M. Johnson, K. Demuth, M. Frank, and B. Jones.
2010. Synergies in learning words and their refer-
ents. In J. Lafferty, C. K. I. Williams, J. Shawe-
Taylor, R. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems 23, pages
1018?1026.
M. Johnson, K. Demuth, and M. Frank. 2012. Exploit-
ing social information in grounded language learn-
ing via grammatical reduction. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 883?891, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
M. Johnson. 2008. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Lin-
guistics, pages 398?406, Columbus, Ohio. Associ-
ation for Computational Linguistics.
Y. Kedar, M. Casasola, and B. Lust. 2006. Getting
there faster: 18- and 24-month-old infants? use of
function words to determine reference. Child De-
velopment, 77(2):325?338.
K. Kurihara and T. Sato. 2006. Variational
Bayesian grammar induction for natural language.
In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino,
and E. Tomita, editors, Grammatical Inference: Al-
gorithms and Applications, pages 84?96. Springer.
K. P. Murphy. 2012. Machine learning: a probabilistic
perspective. The MIT Press.
V. L. Shafer, D. W. Shucard, J. L. Shucard, and
L. Gerken. 1998. An electrophysiological study of
infants? sensitivity to the sound patterns of English
291
speech. Journal of Speech, Language and Hearing
Research, 41(4):874.
R. Shi and M. Lepage. 2008. The effect of functional
morphemes on word segmentation in preverbal in-
fants. Developmental Science, 11(3):407?413.
R. Shi and A. Melanc?on. 2010. Syntactic categoriza-
tion in French-learning infants. Infancy, 15(517?
533).
R. Shi and J. Werker. 2001. Six-months old infants?
preference for lexical words. Psychological Science,
12:71?76.
R. Shi, A. Cutler, J. Werker, and M. Cruickshank.
2006. Frequency and form as determinants of func-
tor sensitivity in English-acquiring infants. The
Journal of the Acoustical Society of America,
119(6):EL61?EL67.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
R. Zangl and A. Fernald. 2007. Increasing flexibil-
ity in children?s online processing of grammatical
and nonce determiners in fluent speech. Language
Learning and Development, 3(3):199?231.
292
Parsing entire discourses as very long strings: Capturing topic continuity in
grounded language learning
Minh-Thang Luong
Department of Computer Science
Stanford University
Stanford, California
lmthang@stanford.edu
Michael C. Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@stanford.edu
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Abstract
Grounded language learning, the task of map-
ping from natural language to a representation
of meaning, has attracted more and more in-
terest in recent years. In most work on this
topic, however, utterances in a conversation
are treated independently and discourse struc-
ture information is largely ignored. In the
context of language acquisition, this indepen-
dence assumption discards cues that are im-
portant to the learner, e.g., the fact that con-
secutive utterances are likely to share the same
referent (Frank et al, 2013). The current pa-
per describes an approach to the problem of
simultaneously modeling grounded language
at the sentence and discourse levels. We com-
bine ideas from parsing and grammar induc-
tion to produce a parser that can handle long
input strings with thousands of tokens, creat-
ing parse trees that represent full discourses.
By casting grounded language learning as a
grammatical inference task, we use our parser
to extend the work of Johnson et al (2012),
investigating the importance of discourse con-
tinuity in children?s language acquisition and
its interaction with social cues. Our model
boosts performance in a language acquisition
task and yields good discourse segmentations
compared with human annotators.
1 Introduction
Learning mappings between natural language (NL)
and meaning representations (MR) is an important
goal for both computational linguistics and cognitive
science. Accurately learning novel mappings is cru-
cial in grounded language understanding tasks and
such systems can suggest insights into the nature of
children language learning.
Two influential examples of grounded language
learning tasks are the sportscasting task, RoboCup,
where the NL is the set of running commentary and
the MR is the set of logical forms representing ac-
tions like kicking or passing (Chen and Mooney,
2008), and the cross-situational word-learning task,
where the NL is the caregiver?s utterances and the
MR is the set of objects present in the context
(Siskind, 1996; Yu and Ballard, 2007). Work
in these domains suggests that, based on the co-
occurrence between words and their referents in
context, it is possible to learn mappings between NL
and MR even under substantial ambiguity.
Nevertheless, contexts like RoboCup?where ev-
ery single utterance is grounded?are extremely
rare. Much more common are cases where a sin-
gle topic is introduced and then discussed at length
throughout a discourse. In a television news show,
for example, a topic might be introduced by present-
ing a relevant picture or video clip. Once the topic
is introduced, the anchors can discuss it by name
or even using a pronoun without showing a picture.
The discourse is grounded without having to ground
every utterance.
Moreover, although previous work has largely
treated utterance order as independent, the order of
utterances is critical in grounded discourse contexts:
if the order is scrambled, it can become impossible
to recover the topic. Supporting this idea, Frank et
al. (2013) found that topic continuity?the tendency
to talk about the same topic in multiple utterances
that are contiguous in time?is both prevalent and
informative for word learning. This paper examines
the importance of topic continuity through a gram-
matical inference problem. We build on Johnson et
al. (2012)?s work that used grammatical inference to
315
Transactions of the Association for Computational Linguistics, 1 (2013) 315?326. Action Editor: Mark Steedman.
Submitted 2/2013; Revised 6/2013; Published 7/2013. c?2013 Association for Computational Linguistics.

	
 









  	
 	   	 	 
	

	





	
	

	
	

	


	


	

	

	
	

	
	

	


	


	

	
	Exploring the Role of Stress in Bayesian Word Segmentation using Adaptor
Grammars
Benjamin Bo?rschinger1,2 Mark Johnson1,3
1Department of Computing, Macquarie University, Sydney, Australia
2Department of Computational Linguistics, Heidelberg University, Heidelberg, Germany
3Santa Fe Institute, Santa Fe, USA
{benjamin.borschinger|mark.johnson}@mq.edu.au
Abstract
Stress has long been established as a major cue
in word segmentation for English infants. We
show that enabling a current state-of-the-art
Bayesian word segmentation model to take ad-
vantage of stress cues noticeably improves its
performance. We find that the improvements
range from 10 to 4%, depending on both the
use of phonotactic cues and, to a lesser ex-
tent, the amount of evidence available to the
learner. We also find that in particular early
on, stress cues are much more useful for our
model than phonotactic cues by themselves,
consistent with the finding that children do
seem to use stress cues before they use phono-
tactic cues. Finally, we study how the model?s
knowledge about stress patterns evolves over
time. We not only find that our model cor-
rectly acquires the most frequent patterns rel-
atively quickly but also that the Unique Stress
Constraint that is at the heart of a previously
proposed model does not need to be built in
but can be acquired jointly with word segmen-
tation.
1 Introduction
Among the first tasks a child language learner has to
solve is picking out words from the fluent speech
that constitutes its linguistic input.1 For English,
stress has long been claimed to be a useful cue
in infant word segmentation (Jusczyk et al., 1993;
Jusczyk et al., 1999b), following the demonstra-
1The datasets and software to replicate our experiments
are available from http://web.science.mq.edu.au/
?bborschi/
tion of its effectiveness in adult speech process-
ing (Cutler et al., 1986). Several studies have
investigated the role of stress in word segmenta-
tion using computational models, using both neu-
ral network and ?algebraic? (as opposed to ?statis-
tical?) approaches (Christiansen et al., 1998; Yang,
2004; Lignos and Yang, 2010; Lignos, 2011; Lig-
nos, 2012). Bayesian models of word segmenta-
tion (Brent, 1999; Goldwater, 2007), however, have
until recently completely ignored stress. The sole
exception in this respect is Doyle and Levy (2013)
who added stress cues to the Bigram model (Gold-
water et al., 2009), demonstrating that this leads to
an improvement in segmentation performance. In
this paper, we extend their work and show how to
integrate stress cues into the flexible Adaptor Gram-
mar framework (Johnson et al., 2007). This allows
us to both start from a stronger baseline model and
to investigate how the role of stress cues interacts
with other aspects of the model. In particular, we
find that phonotactic cues to word-boundaries inter-
act with stress cues, indicating synergistic effects for
small inputs and partial redundancy for larger in-
puts. Overall, we find that stress cues add roughly
6% token f-score to a model that does not account
for phonotactics and 4% to a model that already in-
corporates phonotactics. Relatedly and in line with
the finding that stress cues are used by infants be-
fore phonotactic cues (Jusczyk et al., 1999a), we ob-
serve that phonotactic cues require more input than
stress cues to be used efficiently. A closer look at
the knowledge acquired by our models shows that
the Unique Stress Constraint of Yang (2004) can be
acquired jointly with segmenting the input instead
93
Transactions of the Association for Computational Linguistics, 2 (2014) 93?104. Action Editor: Stefan Riezler.
Submitted 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
of having to be pre-specified; and that our models
correctly identify the predominant stress pattern of
the input but underestimate the frequency of iambic
words, which have been found to be missegmented
by infant learners.
The outline of the paper is as follows. In Section 2
we review prior work. In Section 3 we introduce our
own models. In Section 4 we explain our experimen-
tal evaluation and its results. Section 5 discusses our
findings, and Section 6 concludes and provides some
suggestions for future research.
2 Background and related work
Lexical stress is the ?accentuation of syllables
within words? (Cutler, 2005) and has long been ar-
gued to play an important role in adult word recog-
nition. Following Cutler and Carter (1987)?s obser-
vation that stressed syllables tend to occur at the be-
ginnings of words in English, Jusczyk et al. (1993)
investigated whether infants acquiring English take
advantage of this fact. Their study demonstrated
that this is indeed the case for 9 month olds, al-
though they found no indication of using stressed
syllables as cues for word boundaries in 6 month
olds. Their findings have been replicated and ex-
tended in subsequent work (Jusczyk et al., 1999b;
Thiessen and Saffran, 2003; Curtin et al., 2005;
Thiessen and Saffran, 2007). A brief summary
of the key findings is as follows: English infants
treat stressed syllables as cues for the beginnings of
words from roughly 7 months of age, suggesting that
the role played by stress needs to be acquired, and
that this requires antecedent segmentation by non-
stress-based means (Thiessen and Saffran, 2007).
They also exhibit a preference for low-pass filtered
stress-initial words from this age, suggesting that it
is indeed stress and not other phonetic or phono-
tactic properties that are treated as a cue for word-
beginnings (Jusczyk et al., 1993). In fact, phontactic
cues seem to be used later than stress cues (Jusczyk
et al., 1999a) and seem to be outweighed by stress
cues (Mattys and Jusczyk, 2000).
The earliest computational model for word seg-
mentation incorporating stress cues we are aware of
is the recurrent network model of Christiansen et al.
(1998) and Christiansen and Curtin (1999). They
only reported a word-token f-score of 44% (roughly,
segmentation accuracy: see Section 4), which is
considerably below the performance of subsequent
models, making a direct comparison complicated.
Yang (2004) introduced a simple incremental algo-
rithm that relies on stress by embodying a Unique
Stress Constraint (USC) that allows at most a sin-
gle stressed syllable per word. On pre-syllabified
child directed speech, he reported a word token f-
score of 85.6% for a non-statistical algorithm that
exploits the USC. While the USC has been argued
to be near-to-universal and follows from the ?cul-
minative function of stress? (Fromkin, 2001; Cutler,
2005), the high score Yang reported crucially de-
pends on every word token carrying stress, including
function words. More recently, Lignos (2010, 2011,
2012) further explored Yang?s original algorithm,
taking into account that function words should not
be assumed to possess lexical stress cues. While
his scores are in line with those reported by Yang,
the importance of stress for this learner were more
modest, providing a gain of around 2.5% (Lignos,
2011). Also, the Yang/Lignos learner is unable to
acquire knowledge about the role stress plays in the
language, e.g. that stress tends to fall on particular
positions within words.
Doyle and Levy (2013) extend the Bigram
model of Goldwater et al. (2009) by adding stress-
templates to the lexical generator. A stress-template
indicates how many syllables the word has, and
which of these syllables (if any) are stressed. This
allows the model to acquire knowledge about the
stress patterns of its input by assigning different
probabilities to the different stress-templates. How-
ever, Doyle and Levy (2013) do not directly exam-
ine the probabilities assigned to the stress-templates;
they only report that their model does slightly prefer
stress-initial words over the baseline model by cal-
culating the fraction of stress-initial word types in
the output segmentations of their models. They also
demonstrate that stress cues do indeed aid segmen-
tation, although their reported gain of 1% in token
f-score is even smaller than that reported by Lig-
nos (2011). Our own approach differs from theirs
in assuming phonemic rather than pre-syllabified in-
put (although our model could, trivially, be run on
syllabified input as well) and makes use of Adap-
tor Grammars instead of the Goldwater et al. (2009)
Bigram model, providing us with a flexible frame-
work for exploring the usefulness of stress in differ-
94
ent models.
Adaptor Grammar (Johnson et al., 2007) is
a grammar-based formalism for specifying non-
parametric hierarchical models. Previous work ex-
plored the usefulness of, for example, syllable-
structure (Johnson, 2008b; Johnson and Goldwa-
ter, 2009) or morphology (Johnson, 2008b; Johnson,
2008a) in word segmentation. The closest work to
our own is Johnson and Demuth (2010) who investi-
gate the usefulness of tones for Mandarin phonemic
segmentation. Their way of adding tones to a model
of word segmentation is very similar to our way of
incorporating stress.
3 Models
We give an intuitive description of the mathemati-
cal background of Adaptor Grammars in 3.1, refer-
ring the reader to Johnson et al. (2007) for technical
details. The models we examine are derived from
the collocational model of Johnson and Goldwater
(2009) by varying three parameters, resulting in 6
models: two baselines that do not take advantage of
stress cues and either do or do not use phonotactics,
as described in Section 3.2; and four stress models
that differ with respect to the use of phonotactics,
and as to whether they embody the Unique Stress
Constraint introduced by Yang (2004). We describe
these models in section 3.3.
3.1 Adaptor Grammars
Briefly, an Adaptor Grammar (AG) can be seen as
a probabilistic context-free grammar (PCFG) with a
special set of adapted non-terminals. We use un-
derlining to distinguish adapted non-terminals (X )
from non-adapted non-terminals (Y ). The distri-
bution for each adapted non-terminal X is drawn
from a Pitman-Yor Process which takes as its base-
distribution the tree-distribution over trees rooted
in X as defined by the PCFG. As an effect, each
adapted non-terminal can be seen as having associ-
ated with it a cache of previously-generated subtrees
that can be reused without having to be regenerated
using the individual PCFG rules. This allows AGs to
learn reusable sub-trees such as words, sequences of
words, or smaller units such as Onsets and Codas.
Thus, while ordinary PCFGs have a finite number
of parameters (one probability for each rule), Adap-
tor Grammars in addition have a parameter for every
possible complete tree rooted in any of its adapted
non-terminals, leading to a potentially infinite num-
ber of such parameters. The Pitman-Yor Process in-
duces a rich-get-richer dynamics, biasing the model
towards identifying a small set of units that can be
reused as often as possible. In the case of word seg-
mentation, the model will try to identify as compact
a lexicon as possible to segment the unsegmented
input.
3.2 Baseline models
Our starting point is the state-of-the-art AG model
for word segmentation, Johnson and Goldwater
(2009)?s colloc3-syll model, reproduced in Fig-
ure 1.2 The model assumes that words are grouped
into larger collocational units that themselves can be
grouped into even larger collocational units. This
accounts for the fact that in natural language, there
are strong word-to-word dependencies that need to
be accounted for if severe undersegmentations of
the form ?is in the? are to be avoided (Goldwater,
2007; Johnson and Goldwater, 2009; Bo?rschinger et
al., 2012). It also uses a language-independent form
of syllable structure to constrain the space of possi-
ble words. Finally, this model can learn word-initial
onsets and word-final codas. In a language like En-
glish, this ability provides additional cues to word-
boundaries as certain onsets are much more likely
to occur word-initially than medially (e.g. ?bl? in
?black?), and analogously for certain codas (e.g.
?dth? in ?width? or ?ngth? in ?strength?).
We define an additional baseline model by replac-
ing rules (5) and (6) by (17), and deleting rules (7) to
(12). This removes the model?s ability to use phono-
tactic cues to word-boundaries.
Word ? Syll ( Syll ) ( Syll ) ( Syll ) (17)
We refer to the model in Figure 1 as the colloc3-
phon model, and the model that results from sub-
stituting and removing rules as described as the
colloc3-nophon model. Alternatively, one could
limit the models ability to capture word-to-word de-
pendencies by removing rules (1) to (3). This results
2We follow Johnson and Goldwater (2009) in limiting the
length of possible words to four syllables to speed up runtime.
In pilot experiments, this choice did not have a noticeable effect
on segmentation performance.
95
Collocations3 ? Collocation3+ (1)
Collocation3 ? Collocation2+ (2)
Collocation2 ? Collocation+ (3)
Collocation ? Word+ (4)
Word ? SyllIF (5)
Word ? SyllI ( Syll ) ( Syll ) SyllF (6)
SyllIF ? (OnsetI )RhymeF (7)
SyllI ? (OnsetI )Rhyme (8)
SyllF ? (Onset )RhymeF (9)
CodaF ? Consonant+ (10)
RhymeF ? Vowel (CodaF ) (11)
OnsetI ? Consonant+ (12)
Syll ? (Onset )Rhyme (13)
Rhyme ? Vowel (Coda ) (14)
Onset ? Consonant+ (15)
Coda ? Consonant+ (16)
Figure 1: The baseline model. We use regular-expression
notation to abbreviate multiple rules. X {n} stands for up
to n repetitions of X , brackets indicate optionality, and
X+ stands for one or more repetitions of X . X indicates
an adapted non-terminal. Rules that introduce terminals
for the pre-terminals Vowel , Consonant are omitted.
Refer to the main text for an explanation of the grammar.
in the colloc-model (Johnson, 2008b) that has previ-
ously been found to behave similarly to the Bigram
model used in Doyle and Levy (2013) (Johnson,
2008b; Bo?rschinger et al., 2012). We performed ex-
periments with the colloc-model as well and found
similar results to Doyle and Levy (2013) which are,
while overall worse, similar in trend to the results
obtained for the colloc3-models. For the rest of the
paper, therefore, we will focus on variants of the
colloc3-model.
3.3 Stress-based models
In order for stress cues to be helpful, the model must
have some way of associating the position of stress
with word-boundaries. Intuitively, the reason stress
helps infants in segmenting English is that a stressed
syllable is a reliable indicator of the beginning of
a word (Jusczyk et al., 1993). More generally, if
there is a (reasonably) reliable relationship between
the position of stressed syllables and beginnings (or
Word ? {SSyll | USyll }{1,4} (18)
SSyll ? (Onset )RhymeS (19)
USyll ? (Onset )RhymeU (20)
RhymeS ? Vowel ? ( Coda ) (21)
RhymeU ? Vowel (Coda ) (22)
Onset ? Consonant+ (23)
Coda ? Consonant+ (24)
Figure 2: Description of the all-stress-patterns model. We
use X {m,n} for ?at least m and at most n repetitions of
X ? and {X | Y } for ?either X or Y ?. Stress is asso-
ciated with a vowel by suffixing it with the special termi-
nal symbol ? , leading to a distinction between stressed
(SSyll ) and unstressed (USyll ) syllables. A word can
consist of any possible sequence of up to four syllables,
as indicated by the regular-expression notation. By ad-
ditionally adding initial and final variants of SSyll and
USyll as in Figure 1, phonotactics can be combined with
stress cues.
endings) of words, a learner might exploit this rela-
tionship. In a Bayesian model, this intuition can be
captured by modifying the lexical generator, that is,
the distribution that generates Word s.
Here, changing the lexical generator corresponds
to modifying the rules expanding Word . A straight-
forward way to modify it accordingly is to enu-
merate all possible sequences of stressed and un-
stressed syllables.3 A lexical generator like this is
given in Figure 2. In the data, stress cues are rep-
resented using a special terminal ? ? ? that follows
a stressed vowel, as illustrated in Figure 3. In the
grammar, ? ? ? is constrained to only surface follow-
ing a Vowel , rendering a syllable in which it occurs
stressed (SSyll ). Syllables that do not contain a ? ? ?
are considered unstressed (USyll ). By performing
inference for the probabilities assigned to the dif-
ferent expansions of rule (18), our models can, for
example, learn that a bi-syllabic word that is stress-
initial (a trochee) is more probable than one that puts
stress on the second syllable (an iamb). This (partly)
captures the tendency of English for stress-initial
words and thus provide an additional cue for identi-
fying words; and it is exactly the kind of preference
infant learners of English seem to acquire (Jusczyk
3This is, in essence, also the strategy chosen by Doyle and
Levy (2013).
96
grammar phon stress USC
colloc3-nophon
colloc3-phon ?
colloc3-nophon-stress ?
colloc3-phon-stress ? ?
colloc3-nophon-stress-usc ? ?
colloc3-phon-stress-usc ? ? ?
Table 1: The different models used in our experiments.
?phon? indicates whether phonotactics are used, ?stress?
whether stress cues are used and ?usc? whether the
Unique Stress Constraint is assumed.
orthographic the do-gie
no-stress dh ah d ao g iy
stress dh ah d ao * g iy
Figure 3: Illustration of the input-representation we
choose. We indicate primary stress according to the dic-
tionary with bold-face in the orthography. The phonemic
transcription uses ARPABET and is produced using an
extended version of CMUDict. Primary stress is indi-
cated by inserting the special symbol ?*? after the vowel
of a stressed syllable.
et al., 1993).
We can combine this lexical generator with the
colloc3-nophon baseline, resulting in the colloc3-
nophon-stress model. We can also add phonotac-
tics to the lexical generator in Figure 2 by adding
initial and final variants of SSyll and USyll , anal-
ogous to rules (5) to (12) in Figure 1. This yields
the colloc3-phon-stress model. We can also add
the Unique Stress Constraint (USC) (Yang, 2004)
by excluding all variants of rule (18) that generate
two or more stressed syllables. For example, while
the lexical generator for the colloc3-nophon-stress
model will include the rule Word ? SSyll SSyll ,
the lexical generator embodying the USC lacks this
rule. We refer to the models that include the USC as
colloc3-nophon-stress-usc and colloc3-phon-stress-
usc models. A compact overview of the six different
models is given in Table 1.
4 Experiments
We evaluate our models on several corpora of child
directed speech. We first describe the corpora we
used, then the experimental methodology employed
and finally the experimental results. As the trend is
comparable across all corpora, we only discuss in
detail results obtained on the Alex corpus. For com-
pleteness, however, Table 3 reports the ?standard?
evaluation of performing inference over all of the
three corpora.
4.1 Corpora and corpus creation
Following Christiansen et al. (1998) and Doyle and
Levy (2013), we use the Korman corpus (Korman,
1984) as one of our corpora. It comprises child-
directed speech for very young infants, aged be-
tween 6 and 16 weeks and, like all other cor-
pora used in this paper, is available through the
CHILDES database (MacWhinney, 2000). We de-
rive a phonemicized version of the corpus using
an extended version of CMUDict (Carnegie Mellon
University, 2008)4, as we were unable to obtain the
stress-annotated version of this corpus used in previ-
ous experiments. The phonemicized version is pro-
duced by replacing each orthographic word in the
transcript with the first pronunciation given by the
dictionary. CMUDict also annotates lexical stress,
and we use this information to add stress cues to the
corpus. We only code primary lexical stresses in the
input, ignoring secondary stresses in line with ex-
perimental work that indicates that human listeners
are capable of reliably distinguishing primary and
secondary stress (Mattys, 2000). Due to the very
low frequency of words with 3 or more syllables in
these corpora, this choice has very little effect on the
number of stress cues available in the input. Our ver-
sion of the Korman corpus contains, in total, 11413
utterances. Unlike Christiansen et al. (1998), Yang
(2004), and Doyle and Levy (2013), we follow Lig-
nos and Yang (2010) in making the more realistic as-
sumption that the 94 mono-syllabic function words
listed by Selkirk (1984) never surface with lexical
stress. As function words account for roughly 50%
of the tokens but only roughly 5% of the types in our
corpora, this means that the type and token distribu-
tion of stress patterns differs dramatically in all our
corpora, as can be seen from Table 2.
We also added stress information to the Brent-
Bernstein-Ratner corpus (Bernstein-Ratner, 1987;
Brent, 1999), following the procedure just out-
lined. This corpus is a de-facto standard for evaluat-
4http://svn.code.sf.net/p/cmusphinx/
code/trunk/cmudict/cmudict.0.7a
97
Pattern brent korman alexTok Typ Tok Typ Tok Typ
W+ .48 .07 .47 .08 .44 .05
SW? .49 .86 .49 .86 .52 .87
WSW? .03 .07 .03 .06 .04 .07
Other .00 .00 .00 .00 .00 .00
Table 2: Relative frequencies for stress patterns for the
corpora used in our study. X? stands for 0 or more, X+
for one or more repetitions of X , and S for a stressed and
W for an unstressed syllable. Note the stark asymmetry
between type and token frequencies for unstressed words.
Up to two-decimal places, patterns other than the ones
given have relative frequency 0.00 (frequencies might not
sum to 1 as an artefact of rounding to 2 decimal places).
ing models of Bayesian word segmentation (Brent,
1999; Goldwater, 2007; Goldwater et al., 2009;
Johnson and Goldwater, 2009), comprising in total
9790 utterances.
As our third corpus, we use the Alex portion
of the Providence corpus (Demuth et al., 2006;
Bo?rschinger et al., 2012). A major benefit of the
Providence corpus is that the video-recordings from
which the transcripts were produced are available
through CHILDES alongside the transcripts. This
will allow future work to rely on even more realis-
tic stress cues that can be derived directly from the
acoustic signal. While beyond the scope of this pa-
per, we believe choosing a corpus that makes richer
information available will be important for future
work on stress (and other acoustic) cues. Another
major benefit of the Alex corpus is that it provides
longitudinal data for a single infant, rather than be-
ing a concatenation of transcripts collected from
multiple children, such as the Korman and the Brent-
Bernstein-Ratner corpus. In total, the Alex corpus
comprises 17948 utterances.
Note that despite the differences in age of the in-
fants and overall make-up of the corpora, the dis-
tribution of stress patterns across the corpora is
roughly the same, as shown by Table 2 for the first
10,000 utterances of each of the corpora. This sug-
gests that the distribution of stress patterns both at a
token and type level is a robust property of English
child-directed speech.
4.2 Evaluation procedure
The aim of our experiments is to understand the
contribution of stress cues to the Bayesian word
segmentation models described in Section 3. To
get an idea of how input size interacts with this,
we look at prefixes of the corpora with increasing
sizes (100, 200, 500, 1000, 2000, 5000, and 10,000
utterances). In addition, we are interested in under-
standing what kind of stress pattern preferences our
models acquire. For this, we also collect samples of
the probabilities assigned to the different expansions
of rule (18), allowing us to examine this directly.
The standard evaluation of segmentation models in-
volves having them segment their input in an un-
supervised manner and evaluating performance on
how well they segmented that input. We addition-
ally evaluate the models on a test set for each cor-
pus. Use of a separate test set has previously been
suggested as a means of testing how well the knowl-
edge a learner acquired generalizes to novel utter-
ances (Pearl et al., 2011), and is required for the kind
of comparison across different sizes of input we are
interested in to determine whether there the role of
stress cues interacts with the input size.
We create the test-sets by taking the final 1000 ut-
terances for each corpus. These 1000 utterances will
be segmented by the model after it has performed
inference on its input, without making any further
changes to the lexicon that the model has induced.
In other words, the model will have to segment each
of the test utterances using only the lexicon (and any
additional knowledge about co-occurrences, phono-
tactics, and stress) it has acquired from the training
portion of the corpus during inference.
We measure segmentation performance using the
standard metric of token f-score (Brent, 1999) which
is the harmonic mean of token precision and recall.
Token f-score provides an overall impression of how
accurate individual word tokens were identified. To
illustrate, if the gold segmentation is ?the dog?, the
segmentation ?th e dog? has a token precision of 13(one out of three predicted words is correct); a token
recall of 12 (one of the two gold words was correctlyidentified); and a token f-score of 0.4.
4.3 Inference
For inference, we closely follow Johnson and Gold-
water (2009): we put vague priors on all the hyper-
98
p s usc alex korman brenttrain test train test train test
.81 .81 .85 .83 .82 .82
? .85 .84 .86 .84 .86 .86
? .86 .87 .87 .86 .86 .87
? ? .88 .88 .88 .87 .87 .87
? ? .87 .88 .87 .88 .86 .87
? ? ? .88 .88 .88 .87 .87 .88
Table 3: Token f-scores on both train and test portions
for all three corpora when inference is performed over
the full corpus. Note that the benefit of stress is clearer
when evaluating on the test set, and that overall, perfor-
mance of the different models is comparable across all
three corpora. Models are coded according to the key in
Table 1.
parameters of our models and run 4 chains for 1000
iterations, collecting 20 samples from each chain
with a lag of 10 iterations between each sample af-
ter a burn-in of 800 iterations, using both batch-
initialization and table-label resampling to ensure
good convergence of the sampler. We construct a
single segmentation from the posterior samples us-
ing their minimum Bayes risk decoding, providing a
single score for each condition.
4.4 Experimental conditions
Each of our six models is evaluated on inputs of in-
creasing size, starting at 100 and ending at 10,000
utterances, allowing us to investigate both how per-
formance and ?knowledge? of the learner varies as
a function of input size. For completeness, we also
report the ?standard? evaluation, i.e. performance of
our models on all corpora when trained on the entire
input in Table 3. We will focus our discussion on the
results obtained on the Alex corpus, which are de-
picted in Figure 4, where the input size is depicted
on the x-axis, and the segmentation f-score for the
test-set on the y-axis.
5 Discussion
We find a clear improvement for the stress-models
over both the colloc3-nophon and the colloc3-phon
models. As can be seen in Table 3, the overall
trend is the same for all three corpora, both when
evaluating on the input and the separate test-set.5
5We performed Wilcox rank sum tests on the individual
scores of the 4 independent chains for each model on the full
training data sets and found that the stress-models were always
Note how the relative gain for stress is roughly
1% higher when evaluating on the test-set; this
might have to do with Jusczyk (1997)?s observa-
tion that the advantage of stress ?might be more
evident for relatively unexpected or unfamiliarized
strings? (Jusczyk, 1997). A closer look at Figure 4
indicates further interesting differences between the
colloc3-nophon and the colloc3-phon models that
only become evident when considering different in-
put sizes.
5.1 Stress cues without phonotactics
For the colloc3-nophon models, we observe a rel-
atively stable improvement by adding stress cues
of 6-7%, irrespective of input size and whether or
not the Unique Stress Constraint (USC) is assumed.
The sole exception to this occurs when the learner
only gets to see 100 utterances: in this case, the
colloc-nophon-stress model only shows a 3% im-
provement, whereas the colloc3-nophon-stress-usc
model obtains a boost of roughly 8%. Noticeable
consistent differences between the colloc3-nophon-
stress and colloc3-nophon-stress-usc model, how-
ever, all but disappear starting from around 500 ut-
terances. This is somewhat surprising, considering
that it is the USC that was argued by Yang (2004) to
be key for taking advantage of stress.6
We take this behaviour to indicate that even
with as little evidence as 200 to 500 utterances,
a Bayesian ideal learner can effectively infer that
something like the USC is true of English. This
also becomes clear when examining how the learn-
ers? preferences for different stress patterns evolve
over time, as we do in Section 5.3 below.
5.2 Stress cues and phonotactics
Overall, the models including phonotactic cues per-
form better than those that do not rely on phono-
tactics. However, the overall gain contributed by
stress to the colloc3-phon baseline is smaller, al-
significantly more accurate (p < 0.05) than the baseline models
except when evaluating on the training data for the Korman and
Brent corpora.
6On data in which function words are marked for stress (as
in Yang (2004) and Doyle and Levy (2013)), the USC yields ex-
tremely high scores across all models, simply because roughly
every second word is a function word. Given that this assump-
tion is extremely unnatural, we do not take this as an argument
for the USC.
99
0.65
0.70
0.75
0.80
0.85
100 200 500 1000 2000 5000 10000number of input utterances
segm
enta
tion f
?sco
re
colloc3?nophon  
colloc3?phon  
colloc3?nophon?stress  
colloc3?phon?stress  
colloc3?nophon?stress?usc  
colloc3?phon?stress?usc
Figure 4: Segmentation performance of the different models, across different input sizes and as evaluated on the
test-set for the Alex corpus. The no-stress baselines are given in red, the stress-models without the Unique Stress
Constraint (USC) in green and the ones including the USC in black. Solid lines indicate models that use, dashed lines
models that do not use phonotactics. Refer to the text for discussion.
though this seems to depend on the size of the input.
While phonotactics by itself appears to be a pow-
erful cue, yielding a noticeable 4-5% improvement
over the colloc3-nophon baseline, the learner seems
to require at least around 500 utterances before the
colloc3-phon model becomes clearly more accurate
than the colloc3-nophon model. In contrast, even
for only 100 utterances stress cues by themselves
provide a 3% improvement to the colloc3-nophon
model, indicating that they can be taken advantage
of earlier. While the number of utterances processed
by a Bayesian ideal learner is not directly related to
developmental stages, this observation is consistent
with the psycholinguists? claim that phonotactics are
used by infants for word segmentation after they
have begun to use stress for segmentation (Jusczyk
et al., 1999a).
Turning to the interaction between stress and
phonotactics, we see that there is no consistent ad-
vantage of including the USC in the model. This
is, in fact, even clearer than for the colloc3-nophon
model where at least for small inputs of size 100,
the USC added almost 5% in performance. For the
colloc3-phon models, we only observe a 1-2% im-
provement by adding the USC up until 500 utter-
ances. This further strengthens the point that even in
the absence of such an innate constraint, a statisti-
cal learner can take advantage of stress cues and, as
we show below, actually acquire something like the
USC from the input.
The 4% difference between the colloc3-phon-
stress / colloc3-phon-stress-usc models to the
colloc3-phon baseline is smaller than the 7% dif-
ference between the colloc3-nophon and colloc3-
nophon-stress models. This shows that there is a
redundancy between phonotactic and stress cues in
large amounts of data, as their joint contribution to
the colloc3-nophon baseline is less than the sum of
their individual contributions at 10,000 utterances,
of 4% (for phonotactics) and 7% (for stress).
Unlike for the colloc3-nophon models, we also
see a clear impact of input size. In particular, at
100 utterances the addition of stress cues leads to
an 8 ? 10% improvement, depending on whether or
not the USC is assumed, whereas for the colloc3-
nophon model we only observed a 3 ? 8% improve-
ment. This is particularly striking when we con-
sider that by themselves, the phonotactic cues only
contribute a 1% improvement to the colloc3-nophon
baseline when trained on the 100 utterance corpus,
100
indicating a synergistic interaction (rather than re-
dundancy) between phonotactics and stress for small
inputs. This effect disappears starting from around
1000 utterances; for inputs of size 1000 and larger,
the net-gain of stress drops from roughly 10% to a
3?4% improvement. That is, while we did not notice
any relationship between input size and impact of
stress cues for the colloc3-nophon model, we do see
such an interaction for the combination of phonotac-
tics and stress cues which, taken together, lead to a
larger relative gain in performance on smaller inputs
than on large ones.
5.3 Acquisition of stress patterns
In addition to acquiring a lexicon, the Bayesian
learner acquires knowledge about the possible stress
patterns of English words. The fact that this knowl-
edge is explicitly represented through the PCFG
rules and their probabilities that define the lexi-
cal generator allows us to study the generalisations
about stress the model actually acquires. While
Doyle and Levy (2013) suggest carrying out such
an analysis, they restrict themselves to estimating
the fraction of stress patterns in the segmented out-
put. As shown in Table 2, however, the type and
token distributions of stress patterns can differ sub-
stantially. We therefore investigate the stress pref-
erences acquired by our learner by examining the
probabilities assigned to the different expansions of
rule (18), aggregating the probabilities of the indi-
vidual rules into patterns. For example, the rules
Word ? SSyll (USyll ){0,3} correspond to the
pattern ?Stress on the first syllable?, whereas the
rules Word ? USyll {1,4} correspond to the pat-
tern ?Unstressed word?. By computing the respec-
tive probabilities, we get the overall probability as-
signed by a learner to the pattern.
Figure 5 provides this information for several dif-
ferent rule patterns. Additionally, these plots in-
clude the empirical type (red dotted) and token pro-
portions (red double-dashed) for the input corpus.
Note how for the two major patterns, all models suc-
cessfully track the type, rather than the token fre-
quency, correctly developing a preference for stress-
initial over unstressed words, despite the compa-
rable token frequency of these two patterns. This
is compatible with a recent proposal by Thiessen
and Saffran (2007), who argue that infants infer the
stress pattern over their lexicon. For a Bayesian
model such as ours or Goldwater et al. (2009)?s,
there is no need to pre-specify that the distribution
ought to be learned over types rather than tokens, as
the models automatically interpolate between type
and token statistics according to the properties of
their input (Goldwater et al., 2006). In addition,
a Bayesian framework provides a simple answer to
the question of how a learner might identify the role
of stress in its language without already having ac-
quired at least some words. By combining differ-
ent kinds of cues, e.g. distributional, phonotactic
and prosodic, in a principled manner a Bayesian
learner can jointly segment its input and learn the
appropriate role of each cue, without having to pre-
specify specific preferences that might differ across
languages.
The iambic rule pattern that puts stress on the sec-
ond syllable is much more infrequent on a token
level. All models track this low token frequency,
underestimating the type frequency of this pattern
by a fair amount. This suggests that learning this
pattern correctly requires considerably more input
than for the other patterns. Indeed, the iambic pat-
tern is known to pose problems for infants when they
start using stress as an effective cue. It is only from
roughly 10 months of age that infants successfully
segment iambic words (Jusczyk et al., 1999b). Not
surprisingly, the USC doesn?t aid in learning about
this pattern because it is completely silent on where
stress might fall (and does not noticeably improve
segmentation performance to begin with).
Finally, we can also investigate whether the
models that lack the USC nevertheless learn that
words contain at most one lexically stressed syl-
lable. The bottom-right graph in Figure 5 plots
the probability assigned by the models to patterns
that violate the USC. This includes, for example,
the rules Word ? SyllS SyllS and Word ?
SyllS SyllU SyllS . Note how the probabilities as-
signed to these rules approaches zero, indicating that
the learner becomes more certain that there are no
words that contain more than one syllable with lex-
ical stress. As we argued above, this suggests that a
Bayesian learner can acquire the USC from a mod-
est amount of data ? it will properly infer that the
unnatural patterns are simply not supported by the
input. To summarize, by examining the internal
101
0.55
0.60
0.65
0.70
0.75
0.80
0.85
100 200 500 1000 2000 5000 10000
P(St
ress
 on f
irst)
0.02
0.03
0.04
0.05
0.06
0.07
100 200 500 1000 2000 5000 10000
number of input utterances
P(St
ress
 on s
econ
d)
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
100 200 500 1000 2000 5000 10000
P(Un
stres
sed 
word
)
0.05
0.10
100 200 500 1000 2000 5000 10000
number of input utterances
P(Vi
olate
s US
C)
colloc3?nophon?stress      colloc3?phon?stress      colloc3?nophon?stress?usc      colloc3?phon?stress?usc
Figure 5: Evolution of the knowledge the learner acquires on the Alex corpus. The red dotted line indicates the
empirical type distribution of a specific pattern, and the double-dashed line the empirical token distribution. Top-Left:
Stress-initial pattern, Top-Right: Unstressed Words, Bottom-Left: Stress-second pattern, Bottom-Right: Patterns that
violate the USC.
state of the Bayesian learners we can characterise
how their knowledge about the stress preferences of
their languages develops, rather than merely measur-
ing how well they perform word segmentation. We
find that the iambic pattern that has been observed to
pose problems for infant learners also is harder for
the Bayesian learner to acquire, arguably due to its
extremely low token-frequency.
6 Conclusion and Future Work
We have presented Adaptor Grammar models of
word segmentation that are able to take advantage
of stress cues and are able to learn from phonemic
input. We find that phonotactics and stress interact
in interesting ways, and that stress cues makes a sta-
ble contribution to existing word segmentation mod-
els, improving their performance by 4-6% token f-
score. We also find that the USC introduced by Yang
(2004) need not be prebuilt into a model but can be
acquired by a Bayesian learner from the data. Sim-
ilarly, we directly investigate the stress preferences
acquired by our models and find that for stress-initial
and unstressed words, they track type rather than
token frequencies. The rare stress-second pattern
seems to require more input to be properly acquired,
which is compatible with infant development data.
An important goal for future research is to eval-
uate segmentation models on typologically different
languages and to study the relative usefulness of dif-
ferent cues cross-lingually. For example, languages
such as French lack lexical stress; it would be inter-
esting to know whether in such a case, phonotactic
(or other) cues are more important. Relatedly, recent
work such as Bo?rschinger et al. (2013) has found
that artificially created data often masks the com-
plexity exhibited by real speech. This suggests that
future work should use data directly derived from
the acoustic signal to account for contextual effects,
rather than using dictionary look-up or other heuris-
tics. In using the Alex corpus, for which good qual-
ity audio is available, we have taken a first step in
this direction.
102
Acknowledgements
This research was supported by the Australian
Research Council?s Discovery Projects funding
scheme (project numbers DP110102506 and
DP110102593). We?d like to thank Professor
Dupoux and our other colleagues at the Laboratoire
de Sciences Cognitives et Psycholinguistique in
Paris for hosting us while this research was per-
formed, as well as the Mairie de Paris, the fondation
Pierre Gilles de Gennes, the Ecole des Hautes
Etudes en Sciences Sociales, the Ecole Normale
Supe?rieure, The Region Ile de France, the European
Research Council (ERC-2011-AdG-295810 BOOT-
PHON), the Agence Nationale pour la Recherche
(ANR-2010-BLAN-1901-1 BOOTLANG, ANR-
10-IDEX-0001-02 and ANR-10-LABX-0087) and
the Fondation de France. We?d also like to thank
three anonymous reviewers for helpful comments
and suggestions.
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Benjamin Bo?rschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size for
Bayesian word segmentation on the Providence cor-
pus. In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling 2012),
pages 325?340. Coling 2012 Organizing Committee.
Benjamin Bo?rschinger, Mark Johnson, and Katherine De-
muth. 2013. A joint model of word segmentation
and phonological variation for English word-final /t/-
deletion. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1508?1516. Association
for Computational Linguistics.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71?105.
M. Christiansen and S. Curtin. 1999. The power of sta-
tistical learning: No need for algebraic rules. In Pro-
ceedings of the 21st Annual Conference of the Cogni-
tive Science Society.
Morten H Christiansen, Joseph Allen, and Mark S Sei-
denberg. 1998. Learning to segment speech using
multiple cues: A connectionist model. Language and
Cognitive Processes, 13(2-3):221?268.
Suzanne Curtin, Toben H Mintz, and Morten H Chris-
tiansen. 2005. Stress changes the representational
landscape: Evidence from word segmentation. Cog-
nition, 96(3):233?262.
Anne Cutler and David M Carter. 1987. The predomi-
nance of strong initial syllables in the English vocabu-
lary. Computer Speech and Language, 2(3):133?142.
Anne Cutler, Jacques Mehler, Dennis Norris, and Juan
Segui. 1986. The syllable?s differing role in the seg-
mentation of French and English. Journal of Memory
and Language, 25(4):385 ? 400.
Anne Cutler. 2005. Lexical stress. In David B.
Pisoni and Robert E. Remez, editors, The Handbook
of Speech Perception, pages 264?289. Blackwell Pub-
lishing.
K. Demuth, J. Culbertson, and J. Alter. 2006. Word-
minimality, epenthesis, and coda licensing in the ac-
quisition of English. Language and Speech, 49:137?
174.
Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in Bayesian word segmenta-
tion. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 117?126. Association for Computational
Linguistics.
Victoria Fromkin, editor. 2001. Linguistics: An Intro-
duction to Linguistic Theory. Blackwell, Oxford, UK.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466.
MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Sharon Goldwater. 2007. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Mark Johnson and Katherine Demuth. 2010. Unsu-
pervised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 528?536. Coling 2010 Organiz-
ing Committee.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
103
Linguistics, pages 317?325. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008a. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group
on Computational Morphology and Phonology, pages
20?27. Association for Computational Linguistics.
Mark Johnson. 2008b. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, pages 398?406. Association for Computational
Linguistics.
Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.
1993. Infants? preference for the predominant stress
patterns of English words. Child Development,
64(3):675?687.
Peter W. Jusczyk, E. A. Hohne, and A. Bauman. 1999a.
Infants? sensitivity to allophonic cues for word seg-
mentation. Perception and Psychophysics, 61:1465?
1476.
Peter W. Jusczyk, Derek M. Houston, and Mary New-
some. 1999b. The beginnings of word segmentation in
English-learning infants. Cognitive Psychology, 39(3-
4):159?207.
Peter Jusczyk. 1997. The discovery of spoken language.
MIT Press, Cambridge, MA.
Myron Korman. 1984. Adaptive aspects of maternal vo-
calizations in differing contexts at ten weeks. First
Language, 5:44?45.
Constantine Lignos and Charles Yang. 2010. Reces-
sion segmentation: simpler online word segmentation
using limited resources. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 88?97. Association for Com-
putational Linguistics.
Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
29?38. Association for Computational Linguistics.
Constantine Lignos. 2012. Infant word segmentation:
An incremental, integrated model. In Proceedings of
the West Coast Conference on Formal Linguistics 30.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk: Volume I: Transcription format and
programs, volume II: The database. Computational
Linguistics, 26(4):657?657.
Sven L Mattys and Peter W Jusczyk. 2000. Phonotac-
tic cues for segmentation of fluent speech by infants.
Cognition, 78(2):91?121.
Sven L Mattys. 2000. The perception of primary and
secondary stress in English. Perception and Psy-
chophysics, 62(2):253?265.
Lisa Pearl, Sharon Goldwater, and Mark Steyvers. 2011.
Online learning mechanisms for Bayesian models of
word segmentation. Research on Language and Com-
putation, 8(2):107?132.
Elisabeth O. Selkirk. 1984. Phonology and Syntax: The
Relation Between Sound and Structure. MIT Press.
Erik D Thiessen and Jenny R Saffran. 2003. When
cues collide: use of stress and statistical cues to word
boundaries by 7-to 9-month-old infants. Developmen-
tal Psychology, 39(4):706.
Erik D Thiessen and Jenny R Saffran. 2007. Learning to
learn: Infants acquisition of stress-based strategies for
word segmentation. Language Learning and Develop-
ment, 3(1):73?100.
Carnegie Mellon University. 2008. The CMU pronounc-
ing dictionary, v.0.7a.
Charles Yang. 2004. Universal grammar, statistics or
both? Trends in Cognitive Sciences, 8(10):451?456.
104
Joint Incremental Disfluency Detection and Dependency Parsing
Matthew Honnibal
Department of Computing
Macquarie University
Sydney, Australia
matthew.honnibal@mq.edu.edu.au
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.edu.au
Abstract
We present an incremental dependency
parsing model that jointly performs disflu-
ency detection. The model handles speech
repairs using a novel non-monotonic tran-
sition system, and includes several novel
classes of features. For comparison,
we evaluated two pipeline systems, us-
ing state-of-the-art disfluency detectors.
The joint model performed better on both
tasks, with a parse accuracy of 90.5% and
84.0% accuracy at disfluency detection.
The model runs in expected linear time,
and processes over 550 tokens a second.
1 Introduction
Most unscripted speech contains filled pauses
(ums and uhs), and errors that are usually edited
on-the-fly by the speaker. Disfluency detection is
the task of detecting these infelicities in spoken
language transcripts. The task has some imme-
diate value, as disfluencies have been shown to
make speech recognition output much more dif-
ficult to read (Jones et al., 2003), but has also
been motivated as a module in a natural language
understanding pipeline, because disfluencies have
proven problematic for PCFG parsing models.
Instead of a pipeline approach, we build on re-
cent work in transition-based dependency parsing,
to perform the two tasks jointly. There have been
two small studies of dependency parsing on un-
scripted speech, both using entirely greedy pars-
ing strategies, without a direct comparison against
a pipeline architecture (Jorgensen, 2007; Rasooli
and Tetreault, 2013). We go substantially beyond
these pilot studies, and present a system that com-
pares favourably to a pipeline consisting of state-
of-the-art components. Our parser largely follows
the design of Zhang and Clark (2011). We use a
structured averaged perceptron model with beam-
search decoding (Collins, 2002). Our feature set
is based on Zhang and Clark (2011), and our
transition-system is based on the arc-eager system
of Nivre (2003).
We extend the transition system with a novel
non-monotonic transition, Edit. It allows sen-
tences like ?Pass the pepper uh salt? to be parsed
incrementally, without the need to guess early
that pepper is disfluent. This is achieved by re-
processing the leftward children of the word Edit
marks as disfluent. For instance, if the parser at-
taches the to pepper, but subsequently marks pep-
per as disfluent, the will be returned to the stack.
We also exploit the ease with which the model can
incorporate arbitrary features, and design a set of
features that capture the ?rough copy? structure of
some speech repairs, which motivated the Johnson
and Charniak (2004) noisy channel model.
Our main comparison is against two pipeline
systems, which use the two current state-of-the-
art disfluency detection systems as pre-processors
to our parser, minus the custom disfluency fea-
tures and transition. The joint model compared
favourably to the pipeline parsers at both tasks,
with an unlabelled attachment score of 90.5%, and
84.0% accuracy at detecting speech repairs. An ef-
ficient implementation is available under an open-
source license.1 The future prospects of the sys-
tem are also quite promising. Because the parser
is incremental, it should be well suited to un-
segmented text such as the output of a speech-
recognition system. We consider our main con-
tributions to be:
? a novel non-monotonic transition system, for
speech repairs and restarts,
1http://github.com/syllog1sm/redshift
131
Transactions of the Association for Computational Linguistics, 2 (2014) 131?142. Action Editor: Joakim Nivre.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
A flight to um????
FP
Boston? ?? ?
RM
I mean? ?? ?
IM
Denver? ?? ?
RP
Tuesday
Figure 1: A sentence with disfluencies annotated in
the style of Shriberg (1994) and the Switchboard cor-
pus. FP=Filled Pause, RM=Reparandum, IM=Interregnum,
RP=Repair. We follow previous work in evaluating the sys-
tem on the accuracy with which it identifies speech-repairs,
marked reparandum above.
? several novel feature classes,
? direct comparison against the two best disflu-
ency pre-processors, and
? state-of-the-art accuracy for both speech
parsing and disfluency detection.
2 Switchboard Disfluency Annotations
The Switchboard portion of the Penn Treebank
(Marcus et al., 1993) consists of telephone conver-
sations between strangers about an assigned topic.
Two annotation layers are provided: one for syn-
tactic bracketing (MRG files), and one for disflu-
encies (DPS files). The disfluency layer marks el-
ements with little or no syntactic function, such as
filled pauses and discourse markers, and annotates
speech repairs using the Shriberg (1994) system
of reparandum/interregnum/repair. An example is
shown in Figure 1.
In the syntactic annotation, edited words are
covered by a special node labelled EDITED. The
idea is to mark text which, if excised, would re-
sult in a grammatical sentence. The MRG files do
not mark other types of disfluencies. We follow
the evaluation defined by Charniak and Johnson
(2001), which evaluates the accuracy of identify-
ing speech repairs and restarts. This definition of
the task is the standard in recent work. The reason
for this is that filled pauses can be detected using
a simple rule-based approach, and parentheticals
have less impact on readability and down-stream
processing accuracy.
The MRG and DPS layers have high but im-
perfect agreement over what tokens they mark as
speech repairs: of the text annotated with both lay-
ers, 33,720 tokens are marked as disfluent in at
least one layer, 32,310 are only marked as disflu-
ent by the DPS files, and 32,742 are only marked
as disfluent by the MRG layer.
The Switchboard annotation project was not
fully completed. Because disfluency annotation is
cheaper to produce, many of the DPS training files
do not have matching MRG files. Only 619,236
of the 1,482,845 tokens in the DPS disfluency-
detection training data have gold-standard syntac-
tic parses. Our system requires the more expen-
sive syntactic annotation, but we find that it out-
performs the previous state-of-the-art (Qian and
Liu, 2013), despite training on less than half the
data.
2.1 Dependency Conversion
As is standard in statistical dependency parsing
of English, we acquire our gold-standard depen-
dencies from phrase-structure trees. We used the
2013-04-05 version of the Stanford dependency
converter (de Marneffe et al., 2006). As is standard
for English dependency parsing, we use the Ba-
sic Dependencies scheme, which produces strictly
projective representations.
At first we feared that the filled pauses, disfluen-
cies and meta-data tokens in the Switchboard cor-
pus might disrupt the conversion process, by mak-
ing it more difficult for the converter to recognise
the underlying production rules.
To test this, we performed a small experiment.
We prepared two versions of the corpus: one
where EDITED nodes, filled pauses and meta-data
were removed before the trees were transformed
by the Stanford converter, and one where the dis-
fluency removal was performed after the depen-
dency conversion. The resulting corpora were
largely identical: 99.54% of unlabelled and 98.7%
of labelled dependencies were the same. The fact
that the Stanford converter is quite robust to dis-
fluencies was useful for our baseline joint model,
which is trained on dependency trees that also in-
cluded governors for disfluent words.
We follow previous work on disfluency detec-
tion by lower-casing the text and removing punc-
tuation and partial words (words tagged XX and
words ending in ?-?). We also remove one-token
sentences, as their syntactic analyses are trivial.
We found that two additional simple pre-processes
improved our results: discarding all ?um? and ?uh?
tokens; and merging ?you know? and ?i mean? into
single tokens.
These pre-processes can be completed on the in-
put string without losing information: none of the
?um? or ?uh? tokens are semantically significant,
and the bigrams you know and i mean have a de-
pendency between the two tokens over 99.9% of
the times they occur in the treebank, with you and
I never having any children. This makes it easy
to unmerge the tokens deterministically after pars-
132
ing: all incoming and outgoing arcs will point to
know or mean. The same pre-processing was per-
formed for all our parsing systems.
3 Transition-based Dependency Parsing
A transition-based parser predicts the syntactic
structure of a sentence incrementally, by making
a sequence of classification decisions. We follow
the architecture of Zhang and Clark (2011), who
use beam-search for decoding, and a structured av-
eraged perceptron for training. Despite its simplic-
ity, this type of parser has produced highly com-
petitive results on the Wall Street Journal: with the
extended feature set described by Zhang and Nivre
(2011), it achieves 93.5% unlabelled accuracy on
Stanford basic dependencies (de Marneffe et al.,
2006). Converting the constituency trees produced
by the Charniak and Johnson (2005) reranking
parser results in similar accuracy.
Briefly, the transition-based parser consists of a
configuration (or ?state?) which is sequentially ma-
nipulated by a set of possible transitions. For us, a
state is a 4-tuple c = (?, ?,A,D), where ? and ?
are disjoint sets of word indices termed the stack
and buffer respectively, A is the set of dependency
arcs, and D is the set of word indices marked dis-
fluent. There are no arcs to or from members ofD,
so the dependencies and disfluencies can be imple-
mented as a single vector (in our parser, a token is
marked as disfluent by setting it as its own head).
We use the arc-eager transition system (Nivre,
2003, 2008), which consists of four parsing ac-
tions: Shift, Left-Arc, Right-Arc and Reduce. We
denote the stack with its topmost element to the
right, and the buffer with its first element to the
left. A vertical bar is used to indicate concate-
nation to the stack or buffer, e.g. ?|i indicates a
stack with the topmost element i and remaining
elements ?. A dependency from a governor i to
a child j is denoted i ? j. The four arc-eager
transitions are shown in Figure 2.
The Shift action moves the first item of the
buffer onto the stack. The Right-Arc does the
same, but also adds an arc, so that the top two
items on the stack are connected. The Reduce
move and the Left-Arc both pop the stack, but the
Left-Arc first adds an arc from the first word of
the buffer to the word on top of the stack. Con-
straints on the Reduce and Left-Arc moves ensure
that every word is assigned exactly one head in
the final configuration. We follow the suggestion
(?, i|?,A,D) ` (?|i, ?, A,D) S
(?|i, j|?,A,D) ` (?, j|?,A ? {j ? i}, D) L
Only if i does not have an incoming arc.
(?|i, j|?,A,D) ` (?|i|j, ?,A ? {i? j}, D) R
(?|i, ?, A,D) ` (?, ?,A,D) D
Only if i has an incoming arc.
(?|i, j|?,A,D) ` (?|[x1, xn], j|?,A?, D?) E
Where
A? = A \ {x? y or y ? x : ?x ? [i, j), ?y ? N}
D? = D ? [i, j)
x1...xn are the former left children of i
Figure 2: Our parser?s transition system. The first four
transitions are the standard arc-eager system; the fifth is our
novel Edit transition.
of Ballesteros and Nivre (2013) and add a dummy
token that governs root dependencies to the end of
the sentence. Parsing terminates when this token
is at the start of the buffer, and the stack is empty.
Disfluencies are added toD via the Edit transition,
E, which we now define.
4 A Non-Monotonic Edit Transition
One of the reasons disfluent sentences are hard to
parse is that there often appear to be syntactic re-
lationships between words in the reparandum and
the fluent sentence. When these relations are con-
sidered in addition to the dependencies between
fluent words, the resulting structure is not neces-
sarily a projective tree.
Figure 3 shows a simple example, where the re-
pair square replaces the reparandum rectangle. An
incremental parser could easily become ?garden-
pathed? and attach the repair square to the preced-
ing words, constructing the dependencies shown
dotted in Figure 3. Rather than attempting to de-
vise an incremental model that avoids construct-
ing such dependencies, we allow the parser to con-
struct these dependencies and later delete them if
the governor or child are marked disfluent.
Psycholinguistic models of human sentence
processing have long posited repair mechanisms
(Frazier and Rayner, 1982). Recently, Honnibal
et al. (2013) showed that a limited amount of ?non-
monotonic? behaviour can improve an incremen-
tal parser?s accuracy. We here introduce a non-
monotonic transition, Edit, for speech repairs.
The Edit transition marks the word i on top
of the stack ?|i as disfluent, along with its right-
ward descendents ? i.e., all words in the sequence
i...j ? 1, where j is the word at the start of the
buffer. It then restores the words both preceding
and formerly governed by i to the stack.
In other words, the word on top of the stack and
133
Pass me the red rectangle uh I mean square
Figure 3: Example where apparent dependencies between
the reparandum and the fluent sentence complicate parsing.
The dotted edges are difficult for an incremental parser to
avoid, but cannot be part of the final parse if it is to be a
projective tree. Our solution is to make the transition system
non-monotonic: the parser is able to delete edges.
its rightward descendents are all marked as dis-
fluent, and the stack is popped. We then restore
its leftward children to the stack, and all depen-
dencies to and from words marked disfluent are
deleted. The transition is non-monotonic in the
sense that it can delete dependencies created by
a previous transition, and replace tokens onto the
stack that had been popped.
Why revisit the leftward children, but not the
right? We are concerned about dependencies
which might be mirrored between the reparandum
and the repair. The rightward subtree of the disflu-
ency might well be incorrect, but if it is, it would
still be incorrect if the word on top of the stack
were actually fluent. We therefore regard these
as parsing errors that we will train our model to
avoid. In contrast, avoiding the Left-Arc transi-
tions would require the parser to predict that the
head is disfluent when it has not necessarily seen
any evidence indicating that.
4.1 Worked Example
Figure 4 shows a gold-standard derivation for
a disfluent sentence from the development data.
Line 1 shows the state resulting from the initial
Shift action. In the next three states, His is Left-
Arced to company, which is then Shifted onto the
stack, and Left-Arced to went in Line 4.
The dependency between went and company is
not part of the gold-standard, because went is dis-
fluent. The correct governor of company is the sec-
ond went in the sentence. The Left-Arc move in
Line 4 can still be considered correct, however, be-
cause the gold-standard analysis is still derivable
from the resulting configuration, via the Edit tran-
sition. Another non-gold dependency is created in
Line 6, between broke and went, before broke is
Reduced from the stack in Line 7.
Lines 9 and 10 show the states before and after
the Edit transition. The word on top of the stack in
Line 9, went, has one leftward child, and one right-
1. S His company went broke i mean went bankrupt
2. L His company went broke i mean went bankrupt
3. S His company went broke i mean went bankrupt
4. L His company went broke i mean went bankrupt
5. S His company went broke i mean went bankrupt
6. R His company went broke i mean went bankrupt
7. D His company went broke i mean went bankrupt
8. S His company went broke i mean went bankrupt
9. L His company went broke i mean went bankrupt
10. E His company went broke i mean went bankrupt
11. L His company went broke i mean went bankrupt
12. S His company went broke i mean went bankrupt
12. R His company went broke i mean went bankrupt
13. D His company went broke i mean went bankrupt
Figure 4: A gold-standard transition sequence using our
EDIT transition. Each line specifies an action and shows the
state resulting from it. Words on the stack are circled, and
the arrow indicates the start of the buffer. Disfluent words are
struck-through.
ward child. After the Edit transition is applied,
went and its rightward child broke are both marked
disfluent, and company is returned to the stack. All
of the previous dependencies to and from went and
broke are deleted.
Parsing then proceeds as normal, with the cor-
rect governor of company being assigned by the
Left-Arc in Line 11, and bankrupt being Right-
Arced to went in Line 12. To conserve space, we
have omitted the dummy ROOT token, which is
placed at the end of the sentence, following the
suggestion of Ballesteros and Nivre (2013). The
final action will be a Left-Arc from the ROOT to-
ken to went.
4.2 Dynamic Oracle Training Algorithm
Our non-monotonic transition system introduces
substantial spurious ambiguity: the gold-standard
parse can be derived via many different transition
134
sequences. Recent work has shown that this can
be advantageous (Sartorio et al., 2013; Honnibal
et al., 2013; Goldberg and Nivre, 2012), because
difficult decisions can sometimes be delayed until
more information is available.
Line 5 of Figure 4 shows a state that introduces
spurious ambiguity. From this configuration, there
are multiple actions that could be considered ?cor-
rect?, in the sense that the gold-standard analysis
can be derived from them. The Edit transition is
correct because went is disfluent, but the Left-Arc
and even the Right-Arc are also correct, in that
there are continuations from them that lead to the
gold-standard analysis.
We regard all transition sequences that can re-
sult in the correct analysis as equally valid, and
want to avoid stipulating one of them during train-
ing. We achieve this by following Goldberg and
Nivre (2012) in using a dynamic oracle to create
partially labelled training data.2 A dynamic oracle
is a function that determines the cost of applying
an action to a state, in terms of gold-standard arcs
that are newly unreachable.
We follow Collins (2002) in training an aver-
aged perceptron model to predict transition se-
quences, rather than individual transitions. This
type of model is often referred to as a struc-
tured perceptron, or sometimes a global percep-
tron. During training, if the model does not pre-
dict the correct sequence, an update is performed,
based on the gold-standard sequence and part of
the sequence predicted by the current weights.
Only part of the sequence is used to calculate the
weight update, in order to account for search er-
rors. We use the maximum violation strategy de-
scribed by Huang et al. (2012) to select the subse-
quence to update from.
To train our model using the dynamic oracle,
we use the latent-variable structured perceptron al-
gorithm described by Sun et al. (2009). Beam-
search is performed to find the highest-scoring
gold-standard sequence, as well as the highest-
scoring prediction. We use the same beam-width
for both search procedures.
4.3 Path Length Normalisation
One problem introduced by the Edit transition is
that the number of actions applied to a sentence is
2 The training data is partially labelled in the sense that in-
stances can have multiple true labels. Equivalently, one might
say that the transitions are latent variables, which generate the
dependencies.
no longer constant ? it is no longer guaranteed to
be 2n ? 1, for a sentence of length n. When the
Edit transition is applied to a word with leftward
children, those children are returned to the stack,
and processed again. This has little to no impact
on the algorithm?s empirical efficiency, although
worst-case complexity is no longer linear, but it
does pose a problem for decoding.
The perceptron model tends to assign large pos-
itive scores to its top prediction. We thus ob-
served a problem when comparing paths of differ-
ent lengths, at the end of the sentence. Paths that
included Edit transitions were longer, so the sum
of their scores tended to be higher.
The same problem has been observed during
incremental PCFG parsing, by Zhu et al. (2013).
They introduce an additional transition, IDLE, to
ensure that paths are the same length. So long as
one candidate in the beam is still being processed,
all other candidates apply the IDLE transition.
We adopt a simpler solution. We normalise the
figure-of-merit for a candidate state, which is used
to rank it in the beam, by the length of its transition
history. The new figure-of-merit is the arithmetic
mean of the candidate?s transition scores, where
previously the figure-of-merit was the sum of the
candidate?s transition scores.
Interestingly, Zhu et al. (2013) report that they
tried exactly this, and that it was less effective than
their solution. We found that the features associ-
ated with the IDLE transition were uninformative
(the state is at termination, so the stack and buffer
are empty), and had nothing to do with how many
edit transitions were earlier applied.
5 Features for the Joint Parser
Our baseline parser uses the feature set described
by Zhang and Nivre (2011). The feature set con-
tains 73 templates that mostly refer to the prop-
erties of 12 context tokens: the top of the stack
(S0), its two leftmost and rightmost children (S0L,
S0L2, S0R, S0R2), its parent and grand-parent
(S0h, S0h2), the first word of the buffer and its two
leftmost children (N0, N0L, N0LL), and the next
two words of the buffer (N1, N2).
Atomic features consist of the word, part-of-
speech tag, or dependency label for these tokens;
and multiple feature atoms are often combined for
feature templates. There are also features for the
string-distance between S0 and N0, and the left
and right valencies (total number of children) of
135
S0 and N0, as well as the set of their children?s de-
pendency labels. We restrict these to the first and
last 2 children for implementation efficiency, as
we found this had no effect on accuracy. Numeric
features (for distance and valency) are binned with
the function ?x : min(x, 5). There is only one bi-
lexical feature template, which pairs the words of
S0 and N0. There are also ten tri-tag templates.
Our feature set includes additional dependency
label features not used by Zhang and Nivre (2011),
as we found that disfluency detection errors often
resulted in ungrammatical dependency label com-
binations. The additional templates combine the
POS tag of S0 with two or three dependency la-
bels from its left and right subtrees. Details can be
found in the supplementary material.
5.1 Brown Cluster Features
The Brown clustering algorithm (Brown et al.,
1992) is a well known source of semi-supervised
features. The clustering algorithm is run over
a large sample of unlabelled data, to generate a
type-to-cluster map. This mapping is then used to
generate features that sometimes generalise better
than lexical features, and are helpful for out-of-
vocabulary words (Turian et al., 2010).
Koo and Collins (2010) found that Brown clus-
ter features greatly improved the performance of a
graph-based dependency parser. On our transition-
based parser, Brown cluster features bring a small
but statistically significant improvement on the
WSJ task (0.1-0.3% UAS). Other developers of
transition-based parsers seem to have found sim-
ilar results (personal communication). Since a
Brown cluster mapping computed by Liang (2005)
is easily available,3 the features are simple to im-
plement and cheap to compute.
Our templates follow Koo and Collins (2010)
in including features that refer to cluster prefix
strings, as well as the full clusters. We adapt their
templates to transition-based parsing by replacing
?head? with ?item on top of the stack? and ?child?
with ?first word of the buffer?. The exact templates
can be found in the supplementary material.
The Brown cluster features are used in our
?baseline? parser, and in the parsers we use as part
of our pipeline systems. They improved develop-
ment set accuracy by 0.4%. We experimented with
the other feature sets in these parsers, but found
that they did not improve accuracy on fluent text.
3http://www.metaoptimize.com/projects/wordreps
5.2 Rough Copy Features
Johnson and Charniak (2004) point out that in
speech repairs, the repair is often a ?rough copy?
of the reparandum. The simplest case of this is
where the repair is a single word repetition. It is
common for the repair to differ from the reparan-
dum by insertion, deletion or substitution of one
or more words.
To capture this regularity, we first extend the
feature-set with three new context tokens:4
1. S0re: The rightmost edge of S0 descendants;
2. S0le: The leftmost edge of S0 descendants;
3. N0le: The leftmost edge of N0 descendants.
If a word has no leftward children, it will be
its own left-edge, and similarly it will be its own
rightward edge if it has no rightward children.
Note that the token S0re is necessarily immedi-
ately before N0le, unless some of the tokens be-
tween them are disfluent. We use the S0le and N0le
to compute the following rough-copy features:
1. How long is the prefix word match between
S0le...S0 and N0le...N0?
If the parser were analysing the red the blue
square, with red on the stack and square at
N0, its value would be 1.
2. How long is the prefix POS tag match be-
tween S0le...S0 and N0le...N0?
3. Do the words in S0le...S0 and N0le...N0
match exactly?
4. Do the POS tags in S0le...S0 and N0le...N0
match exactly?
If the parser were analysing the red square
the blue rectangle, with square on the stack
and rectangle at N0, its value would be true.
The prefix-length features are binned using the
function ?x : min(x, 5).
5.3 Match Features
This class of features ask which pairs of the con-
text tokens match, in word or POS tag. The con-
text tokens in the Zhang and Nivre (2011) fea-
ture set are the top of the stack (S0), its head and
4As is common in this type of parser, our implementation
has a number of vectors for properties that are defined before
parsing, such as word forms, POS tags, Brown clusters, etc. A
context token is an index into these vectors, allowing features
considering these properties to be computed.
136
grandparent (S0h, S0h2), its two left- and right-
most children (S0L, S0L2, S0R, S0R2), the first
three words of the buffer (N0, N1, N2), and the
two leftmost children of N0 (N0L, N0LL). We ex-
tend this set with the S0le, S0re and N0le tokens
described above, and also the first left and right
child of S0 and N0 (S0L0, S0R0, N0L0).
All up, there are 18 context tokens, so(18
2
)
= 153 token pairs. For each pair of these
tokens, we add two binary features, indicating
whether the two tokens match in word form or POS
tag. We also have two further classes of features:
if the words do match, a feature is added indicat-
ing the word form; if the tags match, a feature is
added indicating the tag. These finer grained ver-
sions help the model adjust for the fact that some
words can be duplicated in grammatical sentences
(e.g. ?that that?), while most rare words cannot.
5.4 Edited Neighbour Features
Disfluencies are usually string contiguous, even if
they do not form a single constituent. In these situ-
ations, our model has to make multiple transitions
to mark a single disfluency. For instance, if an ut-
terance begins and the and a, the stack will contain
two entries, for and and the, and two Edit transi-
tions will be required.
To mitigate this disadvantage of our model, we
add four binary features. Two fire when the word
or pair of words immediately preceding N0 have
been marked disfluent; the other two fire when the
word or pair of words immediately following S0
have been marked disfluent. These features pro-
vide an additional string-based view that the parser
would otherwise be missing. Speakers tend be
disfluent in bursts: if the previous word is dis-
fluent, the next word is more likely to be disflu-
ent. These four features are therefore all associ-
ated with positive weights for the Edit transition.
Without these features, we would miss an aspect of
disfluency processing that sequence models natu-
rally capture.
6 Part-of-Speech Tagging
We adopt the standard strategy of using a POS
tagger as a pre-process before parsing. Most
transition-based parsers use a structured averaged
perceptron model with beam-search for tagging,
as this model achieves competitive accuracy and
matches the standard dependency parsing archi-
tecture. Our tagger also uses this architecture.
We performed some additional feature engi-
neering for the tagger, in order to improve its accu-
racy given the lack of case distinctions and punc-
tuation in the data. Our additional features use two
sources of unsupervised information. First, we
follow the suggestion of Manning (2011) by us-
ing Brown cluster features to improve the tagger?s
accuracy on unknown words. Second, we com-
pensate for the lack of case distinctions by includ-
ing features that ask what percentage of the time
a word form was seen title-cased, upper-cased and
lower-cased in the Google Web1T corpus.
Where most previous work uses cross-fold
training for the tagger, to ensure that the parser
is trained on tags that reflect run-time accuracies,
we do online training of the tagger alongside the
parser, using the current tagger model to produce
tags during parser training. This had no impact on
parse accuracy, and made it slightly easier to de-
velop our tagger alongside the parser.
The tagger achieved 96.5% accuracy on the de-
velopment data, but when we ran our final test
experiments, we found its accuracy dropped to
96.0%, indicating some over-fitting during our
feature engineering. On the development data, our
parser accuracy improves by about 1% when gold-
standard tags are used.
7 Experiments
We use the Switchboard portion of the Penn Tree-
bank (Marcus et al., 1993), as described in Sec-
tion 2, to train our joint models and evaluate them
on dependency parsing and disfluency detection.
The pre-processing and dependency conversion
are described in Section 2.1. We use the stan-
dard train/dev/test split from Charniak and John-
son (2001): Sections 2 and 3 for training, and Sec-
tion 4 divided into three held-out sections, the first
of which is used for final evaluation.
Our parser evaluation uses the SPARSEVAL
(Roark et al., 2006) metric. However, we wanted
to use the Stanford dependency converter, for the
reasons described in Section 2.1, so we used our
own implementation. Because we do not need to
deal with recognition errors, we do not need to
report our parsing results using P /R/F -measures.
Instead, we report an unlabelled accuracy score,
which refers to the percentage of fluent words
whose governors were assigned correctly. Note
that words marked as disfluent cannot have any in-
coming or out-going dependencies, so if a word is
137
incorrectly marked as disfluent, all of its depen-
dencies will be incorrect.
We follow Johnson and Charniak (2004) and
others in restricting our disfluency evaluation to
speech repairs, which we identify as words that
have a node labelled EDITED as an ancestor. Un-
like most other disfluency detection research, we
train only on the MRG files, giving us 619,236
words of training data instead of the 1,482,845
used by the pipeline systems. It may be possible
to improve our system?s disfluency detection by
leveraging the additional data that does not have
syntactic annotation in some way.
All parsing models were trained for 15 itera-
tions. We found that optimising the number of
iterations on a development set led to small im-
provements that did not transfer to a second devel-
opment set (part of Section 4, which Charniak and
Johnson (2001) reserved for ?future use?).
We test for statistical significance in our results
by training 20 models for each experimental con-
figuration, using different random seeds. The ran-
dom seeds control how the sentences are shuf-
fled during training, which the perceptron model
is quite sensitive to. We use the Wilcoxon rank-
sums non-parametric test. The standard deviation
in UAS for a sample was typically around 0.05%,
and 0.5% for disfluency F -measure.
All of our models use beam-search decoding,
with a beam width of 32. We found that a beam
width of 64 brought a very small accuracy im-
provement (about 0.1%), at the cost of 50% slower
run-time. Wider beams than this brought no ac-
curacy improvement. Accuracy seems to plateau
with slightly narrower beams than on newswire
text. This is probably due to the shorter sentences
in Switchboard.
The baseline and pipeline systems are config-
ured in the same way, except that the baseline
parser is modified slightly to allow it to predict
disfluencies, using a special dependency label,
ERASED. All descendants of a word attached to its
head by this label are marked as disfluent. Both the
baseline and pipeline/oracle parsers use the same
feature set: the Zhang and Nivre (2011) features,
plus our Brown cluster features.
The baseline system is a standard arc-eager
transition-based parser with a structured averaged
perceptron model and beam-search decoding. The
model is trained in the standard way, with a ?static?
oracle and maximum-violation update, following
(Huang et al., 2012).
7.1 Comparison with Pipeline Approaches
The accuracy of incremental dependency parsers
is well established on the Wall Street Journal, but
there are no dependency parsing results in the lit-
erature that make it easy to put our joint model?s
parsing accuracy into context. We therefore com-
pare our joint model to two pipeline systems,
which consist of a disfluency detector, followed by
our dependency parser. We also evaluate parse ac-
curacies after oracle pre-processing, to gauge the
net effect of disfluencies on our parser?s accuracy.
The dependency parser for the pipeline systems
was trained on text with all disfluencies removed,
following Charniak and Johnson (2001). The two
disfluency detection systems we used were the
Qian and Liu (2013) sequence-tagging model, and
a version of the Johnson and Charniak (2004)
noisy channel model, using the Charniak (2001)
syntactic language model and the reranking fea-
tures of Zwarts and Johnson (2011). They are the
two best published disfluency detection systems.
8 Results
Table 1 shows the development set accuracies for
our joint parser. Both the disfluency features and
the Edit transition make statistically significant
improvements, in both disfluency F -measure, un-
labelled attachment score (UAS), and labelled at-
tachment score (LAS).
The Oracle pipeline system, which uses the
gold-standard to clean disfluencies prior to pars-
ing, shows the total impact of speech-errors on the
parser. The baseline parser, which uses the Zhang
and Nivre (2011) feature set plus the Brown clus-
ter features, scores 1.8% UAS lower than the ora-
cle.
When we add the features described in Sec-
tions 5.2, 5.3 and 5.4, the gap is reduced to 1.2%
(+Features). Finally, the improved transition sys-
tem reduces the gap further still, to 0.8% UAS
(+Edit transition). We also tested these features
in the Oracle parser, but found they were ineffec-
tive on fluent text.
The w/s column shows the tokens analysed per
second for each system, including disfluencies,
with a single thread on a 2.4GHz Intel Xeon. The
additional features reduce efficiency, but the non-
monotonic Edit transition does not. The system is
easily efficient enough for real-time use.
138
P R F UAS LAS w/s
Baseline joint 79.4 70.1 74.5 89.9 86.9 711
+Features 86.0 77.2 81.3 90.5 87.5 539
+Edit transition 92.2 80.2 85.8 90.9 87.9 555
Oracle pipeline 100 100 100 91.7 88.6 782
Table 1: Development results for the joint models. For the
baseline model, disfluencies reduce parse accuracy by 1.7%
Unlabelled Attachment Score (UAS). Our features and Edit
transition reduce the gap to 0.7%, and improve disfluency de-
tection by 11.3% F -measure.
Disfl. F UAS
Johnson et al pipeline 82.1 90.3
Qian and Liu pipeline 83.9 90.1
Baseline joint parser 73.9 89.4
Final joint parser 84.1 90.5
Table 2: Test-set parse and disfluency accuracies. The joint
parser is improved by the features and Edit transition, and is
better than pre-processing the text with state-of-the-art disflu-
ency detectors.
Table 2 shows the final evaluation. Our main
comparison is with the two pipeline systems, de-
scribed in Section 7.1. The Johnson and Char-
niak (2004) system was 1.8% less accurate at dis-
fluency detection than the other disfluency detec-
tor we evaluated, the state-of-the-art Qian and Liu
(2013) system. However, when we evaluated the
two systems as pre-processors before our parser,
we found that the Johnson et al pipeline achieved
0.2% better unlabelled attachment score than the
Qian and Liu pipeline. We attribute this to the
use of the Charniak and Johnson (2001) syntac-
tic language model in the Johnson et al pipeline,
which would help the system produce more syn-
tactically consistent output.
Our joint model achieved an unlabelled at-
tachment score of 90.5%, out-performing both
pipeline systems. The Baseline joint parser,
which did not include the Edit transition or disflu-
ency features, scores 1.1% below the Final joint
parser. All of the parse accuracy differences were
found to be statistically significant (p < 0.001).
The Edit transition and disfluency features to-
gether brought a 10.1% improvement in disfluency
F -measure, which was also found to be statisti-
cally significant. The final joint parser achieved
0.2% higher disfluency detection accuracy than
the previous state-of-the-art, the Qian and Liu
(2013) system,5 despite having approximately half
as much training data (we require syntactic anno-
5 Our scores refer to an updated version of the system that
corrects minor pre-processing problems. We thank Qian Xian
for making his code available.
tation, for which there is less data).
Our significance testing regime involved using
20 different random seeds when training each of
our models, which the perceptron algorithm is sen-
sitive to. This could not be applied to the other two
disfluency detectors, so we cannot test those dif-
ferences for significance. However, we note that
the 20 samples for our disfluency detector ranged
in accuracy from 83.3-84.6, so we doubt that 0.2%
mean improvement over the Qian and Liu (2013)
result is meaningful.
Although we did not systematically optimise
on the development set, our test scores are lower
than our development accuracies. Much of the
over-fitting seems to be in the POS tagger, which
dropped in accuracy by 0.5%.
9 Analysis of Edit Behaviour
In order to understand how the parser applies
the Edit transition, we collected some additional
statistics over the development data. The parser
predicted 2,558 Edit transitions, which together
marked 2,706 words disfluent (2,495 correctly).
The Edit transition can mark multiple words dis-
fluent when S0 has one or more rightward descen-
dants. It turns out this case is uncommon; the
parser largely assigns disfluency labels word-by-
word, only sometimes marking words with right-
ward descendents as disfluent.
Of the 2,558 Edit transitions, there were 682
cases were at least one leftward child was returned
to the stack, and the total number of leftward chil-
dren returned was 1,132. The most common type
of construction that caused the parser to return
words to the stack were disfluent predicates, which
often have subjects and discourse conjunctions as
leftward children. An example of a disfluent pred-
icate with a fluent subject is shown in Figure 4.
There were only 48 cases of the same word be-
ing returned to the stack twice. The possibility of
words being returned to the stack multiple times
is what gives our system worse than linear worst-
case complexity. In the worst case, the ith word
of a sentence of length n could be returned to the
stack n? (i+1) times. Empirically, the Edit tran-
sition made no difference to run-time.
Once a word has been returned to the stack by
the Edit transition, how does the parser end up
analysing it? If it turned out that almost all of
the former leftward children of disfluent words are
subsequently marked as disfluent, there would be
139
little point in returning them to the stack ? we
could just mark them as disfluent in the original
Edit transition. On the other hand, if they are al-
most all marked as fluent, perhaps they can just be
attached as children to the first word of the buffer.
In fact the two cases are almost equally com-
mon. Of the 1,132 words returned to the stack,
547 were subsequently marked disfluent, and 584
were not. The parser was also quite accurate in
its decisions over these tokens. Of the 547 tokens
marked disfluent, 500 were correct ? similar to
the overall development set precision, 92.2%.
Accuracy over the words returned to the stack
might be improved in future by features referring
to their former heads. For instance, in He went
broke uh became bankrupt, we do not currently
have features that record the deleted dependency
became he and went. We thank one of the anony-
mous reviewers for this suggestion.
10 Related Work
The most similar system to ours was published
very recently. Rasooli and Tetreault (2013) de-
scribe a joint model of dependency parsing and
disfluency detection. They introduce a second
classification step, where they first decide whether
to apply a disfluency transition, or a regular pars-
ing move. Disfluency transitions operate either
over a sequence of words before the start of the
buffer, or a sequence of words from the start of
the buffer forward. Instead of the dynamic oracle
training method that we employ, they use a two-
stage bootstrap-style process.
Direct comparison between our model and
theirs is difficult, as they use the Penn2MALT
scheme, and their parser uses greedy decoding,
where we use beam search. They also use gold-
standard part-of-speech tags, which would im-
prove our scores by around 1%. The use of
beam-search may explain much of our perfor-
mance advantage: they report an unlabelled at-
tachment score of 88.6, and a disfluency detec-
tion F -measure of 81.4%. Our training algorithm
would be applicable to a beam-search version of
their parser, as their transition-system also intro-
duces substantial spurious ambiguity, and some
non-monotonic behaviour.
A hybrid transition system would also be possi-
ble, as the two types of Edit transition seem to be
complementary. The Rasooli and Tetreault system
offers a token-based view of disfluencies, which
is useful for examples such as, and the and the,
which would require two applications of our tran-
sition. On the other hand, our Edit transition may
have the advantage for more syntactically compli-
cated examples, particularly for disfluent verbs.
The importance of syntactic features for disflu-
ency detection was demonstrated by Johnson and
Charniak (2004). Despite this, most subsequent
work has used sequence models, rather than syn-
tactic parsers. The other disfluency system that
we compare our model to, developed by Qian and
Liu (2013), uses a cascade of Maximum Margin
Markov Models to perform disfluency detection
with minimal syntactic information.
One motivation for sequential approaches is that
most applications of these models will be over un-
segmented text, as segmenting unpunctuated text
is a difficult task that benefits from syntactic fea-
tures (Zhang et al., 2013).
We consider the most promising aspect of our
system to be that it is naturally incremental, so it
should be straightforward to extend the system to
operate on unsegmented text in subsequent work.
Due to its use of syntactic features, from the joint
model, the system is substantially more accurate
than the previous state-of-the-art in incremental
disfluency detection, 77% (Zwarts et al., 2010).
11 Conclusion
We have presented an efficient and accurate joint
model of dependency parsing and disfluency de-
tection. The model out-performs pipeline ap-
proaches using state-of-the-art disfluency detec-
tors, and is highly efficient, processing over 550
tokens a second. Because the system is incremen-
tal, it should be straight-forward to apply it to un-
segmented text. The success of an incremental,
non-monotonic parser at disfluent speech parsing
may also be of some psycholinguistic interest.
Acknowledgments
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
140
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?
479.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of 39th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 124?131. Associa-
tion for Computational Linguistics, Toulouse,
France.
Eugene Charniak and Mark Johnson. 2001. Edit
detection and parsing for transcribed speech. In
Proceedings of the 2nd Meeting of the North
American Chapter of the Association for Com-
putational Linguistics, pages 118?126. The As-
sociation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 173?180. As-
sociation for Computational Linguistics, Ann
Arbor, Michigan.
Michael Collins. 2002. Discriminative training
methods for hidden Markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1?8. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178?210.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Matthew Honnibal, Yoav Goldberg, and Mark
Johnson. 2013. A non-monotonic arc-eager
transition system for dependency parsing. In
Proceedings of the Seventeenth Conference on
Computational Natural Language Learning,
pages 163?172. Association for Computational
Linguistics, Sofia, Bulgaria.
Liang Huang, Suphan Fayong, and Yang Guo.
2012. Structured perceptron with inexact
search. In Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 142?
151. Association for Computational Linguis-
tics, Montre?al, Canada.
Mark Johnson and Eugene Charniak. 2004. A
TAG-based noisy channel model of speech re-
pairs. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 33?39.
Douglas A. Jones, Florian Wolf, Edward Gib-
son, Elliott Williams, Evelina Fedorenko, Dou-
glas A. Reynolds, and Marc A. Zissman. 2003.
Measuring the readability of automatic speech-
to-text transcripts. In INTERSPEECH. ISCA.
Fredrik Jorgensen. 2007. The effects of disflu-
ency detection in parsing spoken language. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of
the 16th Nordic Conference of Computational
Linguistics NODALIDA-2007, pages 240?244.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?
11.
Percy Liang. 2005. Semi-supervised learning for
natural language. Ph.D. thesis, MIT.
Christopher D. Manning. 2011. Part-of-speech
tagging from 97linguistics? In Proceedings of
the 12th international conference on Computa-
tional linguistics and intelligent text processing
- Volume Part I, CICLing?11, pages 171?189.
Springer-Verlag, Berlin, Heidelberg.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
141
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513?553.
Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Pro-
ceedings of the 2013 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 820?825. Association for Com-
putational Linguistics, Atlanta, Georgia.
Mohammad Sadegh Rasooli and Joel Tetreault.
2013. Joint parsing and disfluency detection in
linear time. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 124?129. Association
for Computational Linguistics, Seattle, Wash-
ington, USA.
Brian Roark, Mary Harper, Eugene Charniak,
Bonnie Dorr, Mark Johnson, Jeremy Kahn,
Yang Liu, Mary Ostendorf, John Hale, Anna
Krasnyanskaya, Matthew Lease, Izhak Shafran,
Matthew Snover, Robin Stewart, and LisaYung.
2006. Sparseval: Evaluation metrics for pars-
ing speech. In Proceedings of Language Re-
source and Evaluation Conference, pages 333?
338. European Language Resources Associa-
tion (ELRA), Genoa, Italy.
Francesco Sartorio, Giorgio Satta, and Joakim
Nivre. 2013. A transition-based dependency
parser using a dynamic parsing strategy. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
135?144. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Elizabeth Shriberg. 1994. Preliminaries to a The-
ory of Speech Disfluencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,
and Jun?ichi Tsujii. 2009. Latent variable per-
ceptron algorithm for structured classification.
In IJCAI, pages 1236?1242.
Joseph Turian, Lev-Arie Ratinov, and Yoshua
Bengio. 2010. Word representations: A simple
and general method for semi-supervised learn-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics, pages 384?394. Association for Computa-
tional Linguistics, Uppsala, Sweden.
Dongdong Zhang, Shuangzhi Wu, Nan Yang, and
Mu Li. 2013. Punctuation prediction with
transition-based parsing. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, pages 752?760. As-
sociation for Computational Linguistics, Sofia,
Bulgaria.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Computational Linguistics,
37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188?193. Association for Computational
Linguistics, Portland, USA.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min
Zhang, and Jingbo Zhu. 2013. Fast and accu-
rate shift-reduce constituent parsing. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
434?443. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Simon Zwarts and Mark Johnson. 2011. The im-
pact of language models and loss functions on
repair disfluency detection. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 703?711. Association for
Computational Linguistics, Portland, USA.
Simon Zwarts, Mark Johnson, and Robert Dale.
2010. Detecting speech repairs incrementally
using a noisy channel approach. In Proceedings
of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages
1371?1378. Coling 2010 Organizing Commit-
tee, Beijing, China.
142
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Whyisenglishsoeasytosegment?
Abdellah Fourtassi1, Benjamin Bo?rschinger2,3
Mark Johnson3 and Emmanuel Dupoux1
(1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
(2) Department of Computing, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au
?
Abstract
Cross-linguistic studies on unsupervised
word segmentation have consistently
shown that English is easier to segment
than other languages. In this paper, we
propose an explanation of this finding
based on the notion of segmentation
ambiguity. We show that English has a
very low segmentation ambiguity com-
pared to Japanese and that this difference
correlates with the segmentation perfor-
mance in a unigram model. We suggest
that segmentation ambiguity is linked
to a trade-off between syllable structure
complexity and word length distribution.
1 Introduction
During the course of language acquisition, in-
fants must learn to segment words from continu-
ous speech. Experimental studies show that they
start doing so from around 7.5 months of age
(Jusczyk and Aslin, 1995). Further studies indi-
cate that infants are sensitive to a number of word
boundary cues, like prosody (Jusczyk et al, 1999;
Mattys et al, 1999), transition probabilities (Saf-
fran et al, 1996; Pelucchi et al, 2009), phonotac-
tics (Mattys et al, 2001), coarticulation (Johnson
and Jusczyk, 2001) and combine these cues with
different weights (Weiss et al, 2010).
Computational models of word segmentation
have played a major role in assessing the relevance
and reliability of different statistical cues present
in the speech input. Some of these models focus
mainly on boundary detection, and assess differ-
ent strategies to identify them (Christiansen et al,
1998; Xanthos, 2004; Swingley, 2005; Daland and
Pierrehumbert, 2011). Other models, sometimes
called lexicon-building algorithms, learn the lexi-
con and the segmentation at the same time and use
knowledge about the extracted lexicon to segment
novel utterances. State-of-the-art lexicon-building
segmentation algorithms are typically reported to
yield better performance than word boundary de-
tection algorithms (Brent, 1999; Venkataraman,
2001; Batchelder, 2002; Goldwater, 2007; John-
son, 2008b; Fleck, 2008; Blanchard et al, 2010).
As seen in Table 1, however, the performance
varies considerably across languages with English
winning by a high margin. This raises a general-
izability issue for NLP applications, but also for
the modeling of language acquisition since, obvi-
ously, it is not the case that in some languages,
infants fail to acquire an adult lexicon. Are these
performance differences only due to the fact that
the algorithms might be optimized for English? Or
do they also reflect some intrinsic linguistic differ-
ences between languages?
Lang. F-score Model Reference
English 0.89 AG Johnson (2009)
Chinese 0.77 AG Johnson (2010)
Spanish 0.58 DP Bigram Fleck (2008)
Arabic 0.56 WordEnds Fleck (2008)
Sesotho 0.55 AG Johnson (2008)
Japanese 0.55 BootLex Batchelder (2002)
French 0.54 NGS-u Boruta (2011)
Table 1: State-of-the-art unsupervised segmentation scores
for eight languages.
The aim of the present work is to understand
why English usually scores better than other lan-
guages, as far as unsupervised segmentation is
concerned. As a comparison point, we chose
Japanese because it is among the languages that
have given the poorest word segmentation scores.
In fact, Boruta et al (2011) found an F-score
around 0.41 using both Brent (1999)?s MBDP-1
and Venkataraman (2001)?s NGS-u models, and
Batchelder (2002) found an F-score that goes
from 0.40 to 0.55 depending on the corpus used.
Japanese also differs typologically from English
along several phonological dimensions such as
1
number of syllabic types, phonotactic constraints
and rhythmic structure. Although most lexicon-
building segmentation algorithms do not attempt
to model these dimensions, they still might be rel-
evant to speech segmentation and help explain the
performance difference.
The structure of the paper is as follows. First,
we present the class of lexical-building segmen-
tation algorithm that we use in this paper (Adap-
tor Grammar), and our English and Japanese cor-
pora. We then present data replicating the basic
finding that segmentation performance is better for
English than for Japanese. We then explore the hy-
pothesis that this finding is due to an intrinsic dif-
ference in segmentation ambiguity in the two lan-
guages, and suggest that the source of this differ-
ence rests in the structure of the phonological lexi-
con in the two languages. Finally, we use these in-
sights to try and reduce the gap between Japanese
and English segmentation through a modification
of the Unigram model where multiple linguistic
levels are learned jointly.
2 Computational Framework and
Corpora
2.1 Adaptor Grammar
In this study, we use the Adaptor Grammar frame-
work (Johnson et al, 2007) to test different mod-
els of word segmentation on English and Japanese
Corpora. This framework makes it possible to
express a class of hierarchical non-parametric
Bayesian models using an extension of probabilis-
tic context-free grammars called Adaptor Gram-
mar (AG). It allows one to easily define models
that incorporate different assumptions about lin-
guistic structure and is therefore a useful practical
tool for exploring different hypotheses about word
segmentation (Johnson, 2008b; Johnson, 2008a;
Johnson et al, 2010; Bo?rschinger et al, 2012).
For mathematical details and a description of
the inference procedure for AGs, we refer the
reader to Johnson et al (2007). Briefly, AG uses
the non-parametric Pitman-Yor-Process (Pitman
and Yor, 1997) which, as in Minimum Descrip-
tion lengths models, finds a compact representa-
tion of the input by re-using frequent structures
(here, words).
2.2 Corpora
In the present study, we used both Child Di-
rected Speech (CDS) and Adult Directed Speech
(ADS) corpora. English CDS was derived from
the Bernstein-Ratner corpus (Bernstein-Ratner,
1987), which consists in transcribed verbal inter-
action of parents with nine children between 1
and 2 years of age. We used the 9,790 utter-
ances that were phonemically transcribed by Brent
and Cartwright (1996). Japanese CDS consists in
the first 10, 000 utterances of the Hamasaki cor-
pus (Hamasaki, 2002). It provides a phonemic
transcript of spontaneous speech to a single child
collected from when the child was 2 up to when
it was 3.5 years old. Both CDS corpora are avail-
able from the CHILDES database (MacWhinney,
2000).
As for English ADS, we used the first 10,000
utterances of the Buckeye Speech Corpus (Pitt et
al., 2007) which consists in spontaneous conver-
sations with 40 speakers in American English. To
make it comparable to the other corpora in this
paper, we only used the idealized phonemic tran-
scription. Finally, for Japanese ADS, we used
the first 10,000 utterances of a phonemic tran-
scription of the Corpus of Spontaneous Japanese
(Maekawa et al, 2000). It consists of recorded
spontaneous conversations, or public speeches in
different fields ranging from engineering to hu-
manities. For each corpus, we present elementary
statistics in Table 2.
3 Unsupervised segmentation with the
Unigram Model
3.1 Setup
In this experiment we used the Adaptor Gram-
mar framework to implement a Unigram model of
word segmentation (Johnson et al, 2007). This
model has been shown to be equivalent to the orig-
inal MBDP-1 segmentation model (see Goldwater
(2007)). The model is defined as:
?
Utterance?Word+
Word? Phoneme+
?
In the AG framework, an underlined non-
terminal indicates that this non-terminal is
adapted, i.e. that the AG will cache (and learn
probabilities for) entire sub-trees rooted in this
non-terminal. Here, Word is the only unit that the
model effectively learns, and there are no depen-
dencies between the words to be learned. This
grammar states that an utterance must be analyzed
in terms of one or more Words, where a Word is a
2
Corpus Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
Tokens
? Utterances 9, 790 10, 000 10, 000 10, 000
? Words 33, 399 27, 362 57, 185 87, 156
? Phonemes 95, 809 108, 427 183, 196 289, 264
Types
? Words 1, 321 2, 389 3, 708 4, 206
? Phonemes 50 30 44 25
Average Lengths
? Words per utterance 3.41 2.74 5.72 8.72
? Phonemes per utterance 9.79 10.84 18.32 28.93
? Phonemes per word 2.87 3.96 3.20 3.32
Table 2 : Characteristics of phonemically transcribed corpora
sequence of Phonemes.
We ran the model twice on each corpus for
2,000 iterations with hyper-parameter sampling
and we collected samples throughout the process,
following the methodology of Johnson and Gold-
water (2009)1. For evaluation, we performed their
Minimum Bayes Risk decoding using the col-
lected samples to get a single score.
3.2 Evaluation
For the evaluation, we used the same measures as
Brent (1999), Venkataraman (2001) and Goldwa-
ter (2007), namely token Precision (P), Recall (R)
and F-score (F). Precision is defined as the num-
ber of correct word tokens found out of all tokens
posited. Recall is the number of correct word to-
kens found out of all tokens in the gold standard.
The F-score is defined as the harmonic mean of
Precision and Recall , F = 2?P?RP+R .
We will refer to these scores as the segmentation
scores. In addition, we define similar measures for
word boundaries and word types in the lexicon.
3.3 Results and discussion
The results are shown in Table 3. As expected,
the model yields substantially better scores in En-
glish than Japanese, for both CDS and ADS. In
addition, we found that in both languages, ADS
yields slightly worse results than CDS. This is to
be expected because ADS uses between 60% and
300% longer utterances than CDS, and as a result
presents the learner with a more difficult segmen-
tation problem. Moreover, ADS includes between
1We used incremental initialization
70% and 280% more word types than CDS, mak-
ing it a more difficult lexical learning problem.
Note, however, that despite these large differences
in corpus statistics, the difference in segmentation
performance between ADS and CDS are small
compared to the differences between Japanese and
English.
An error analysis on English data shows that
most errors come from the Unigrammodel mistak-
ing high frequency collocations for single words
(see also Goldwater (2007)). This leads to an
under-segmentation of chunks like ?a boy? or ?is
it? 2. Yet, the model also tends to break off fre-
quent morphological affixes, especially ?-ing? and
?-s? , leading to an over-segmentation of words
like ?talk ing? or ?black s?.
Similarly, Japanese data shows both over-
and under-segmentation errors. However, over-
segmentation is more severe than for English, as
it does not only affect affixes, but surfaces as
breaking apart multi-syllabic words. In addition,
Japanese segmentation faces another kind of er-
ror which acts across word boundaries. For exam-
ple, ?ni kashite? is segmented as ?nika shite? and
?nurete inakatta? as ?nure tei na katta?. This leads
to an output lexicon that, on the one hand, allows
for a more compact analysis of the corpus than
the true lexicon: the number of word types drops
from 2,389 to 1,463 in CDS and from 4,206 to
2,372 in ADS although the average token length ?
and consequently, overall number of tokens ? does
not change as dramatically, dropping from 3.96 to
2For ease of presentation, we use orthography to present
examples although all experiments are run on phonemic tran-
scripts.
3
? Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
? F P R F P R F P R F P R
Segmentation 0.77 0.76 0.77 0.55 0.51 0.61 0.69 0.66 0.73 0.50 0.48 0.52
Boundaries 0.87 0.87 0.88 0.72 0.63 0.83 0.86 0.81 0.91 0.76 0.74 0.79
Lexicon 0.62 0.65 0.59 0.33 0.43 0.26 0.41 0.48 0.36 0.30 0.42 0.23
Table 3 : Word segmentation scores of the Unigram model
3.31 for CDS and from 3.32 to 3.12 in ADS. On
the other hand, however, most of the output lex-
icon items are not valid Japanese words and this
leads to the bad lexicon F-scores. This, in turn,
leads to the bad overall segmentation performance.
In brief, we have shown that, across two dif-
ferent corpora, English yields consistently better
segmentation results than Japanese for the Uni-
gram model. This confirms and extends the results
of Boruta et al (2011) and Batchelder (2002). It
strongly suggests that the difference is neither due
to a specific choice of model nor to particularities
of the corpora, but reflects a fundamental property
of these two languages.
In the following section, we introduce the no-
tion of segmentation ambiguity, it to English and
Japanese data, and show that it correlates with seg-
mentation performance.
4 Intrinsic Segmentation Ambiguity
Lexicon-based segmentation algorithms like
MBDP-1, NGS-u and the AG Unigram model
learn the lexicon and the segmentation at the
same time. This makes it difficult, in case of
poor performance, to see whether the problem
comes from the intrinsic segmentability of the
language or from the quality of the extracted
lexicon. Our claim is that Japanese is intrinsically
more difficult to segment than English, even when
a good lexicon is already assumed. We explore
this hypothesis by studying segmentation alone,
assuming a perfect (Gold) lexicon.
4.1 Segmentation ambiguity
Without any information, a string of N phonemes
could be segmented in 2N?1 ways. When a lexi-
con is provided, the set of possible segmentations
is reduced to a smaller number. To illustrate this,
suppose we have to segment the input utterance:
/ay s k r iy m/ 3, and that the lexicon contains the
following words : /ay/ (I), /s k r iy m/ (scream),
/ay s/ (ice), /k r iy m/ (cream). Only two segmen-
tations are possible : /ay skriym/ (I scream) and
/ays kriym/ (ice cream).
We are interested in the ambiguity generated by
the different possible parses that result from such a
supervised segmentation. In order to quantify this
idea in general, we define a Normalized Segmenta-
tion Entropy. To do this, we need to assign a prob-
ability to every possible segmentation. To this end,
we use a unigram model where the probability of a
lexical item is its normalized frequency in the cor-
pus and the probability of a parse is the product
of the probabilities of its terms. In order to obtain
a measure that does not depend on the utterance
length, we normalize by the number of possible
boundaries in the utterance. So for an utterance of
length N , the Normalized Segmentation Entropy
(NSE) is computed using Shannon formula (Shan-
non, 1948) as follows:
?
NSE = ?
?
i Pilog2(Pi)/(N ? 1)
?
where Pi is the probability of the parse i .
For CDS data we found Normalized Segmen-
tation Entropies of 0.0021 bits for English and
0.0156 bits for Japanese. In ADS data we
found similar results with 0.0032 bits for English
and 0.0275 bits for Japanese. This means that
Japanese needs between 7 and 8 times more bits
than English to encode segmentation information.
This is a very large difference, which is of the
same magnitude in CDS and ADS. These differ-
ences clearly show that intrinsically, Japanese is
more ambiguous than English with regards to seg-
mentation.
One can refine this analysis by distinguishing
two sources of ambiguity: ambiguity across word
boundaries, as in ?ice cream / [ay s] [k r iy m]?
3We use ARPABET notation to represent phonemic input.
4
Figure 1 : Correlation between Normalized Segmentation Entropy (in bits) and the segmentation F-score for CDS (left) and
ADS (Right)
vs ?I scream / [ay] [s k r iy m]?. And ambigu-
ity within the lexicon, that occurs when a lexical
item is composed of two or more sub-words (like
in ?Butterfly?).
Since we are mainly investigating lexicon-
building models, it is important to measure the am-
biguity within the lexicon itself, in the ideal case
where this lexicon is perfect. To this end, we com-
puted the average number of segmentations for a
lexicon item. For example, the word ?butterfly?
has two possible segmentations : the original word
?butterfly? and a segmentation comprising the two
sub-words : ?butter? and ?fly?. For English to-
kens, we found an average of 1.039 in CDS and
1.057 in ADS. For Japanese tokens, we found an
average of 1.811 in CDS and 1.978 in ADS. En-
glish?s averages are close to 1, indicating that it
doesn?t exhibit lexicon ambiguity. Japanese, how-
ever, has averages close to 2 which means that lex-
ical ambiguity is quite systematic in both CDS and
ADS.
4.2 Segmentation ambiguity and supervised
segmentation
The intrinsic ambiguity in Japanese only shows
that a given sentence has multiple possible seg-
mentations. What remains to be demonstrated is
that these multiple segmentations result in system-
atic segmentation errors. To do this we propose
a supervised segmentation algorithm that enumer-
ates all possible segmentations of an utterance
based on the gold lexicon, and selects the segmen-
tation with the highest probability. In CDS data,
this algorithm yields a segmentation F-score equal
to 0.99 for English and 0.95 for Japanese. In ADS
we find an F-score of 0.96 for English and 0.93 for
Japanese. These results show that lexical informa-
tion alone plus word frequency eliminates almost
all segmentation errors in English, especially for
CDS. As for Japanese, even if the scores remain
impressively high, the lexicon alone is not suffi-
cient to eliminate all the errors. In other words,
even with a gold lexicon, English remains easier
to segment than Japanese.
To quantify the link between segmentation en-
tropy and segmentation errors, we binned the sen-
tences of our corpus in 10 bins according to the
Normalized Segmentation Entropy, and correlate
this with the average segmentation F-score for
each bin. As shown Figure 1, we found significant
correlations: (R = ?0.86, p < 0.001) for CDS
and (R = ?0.93, p < 0.001) for ADS, showing
that segmentation ambiguity has a strong effect
even on supervised segmentation scores. The cor-
relation within language was also significant but
only in the Japanese data : R = ?0.70 for CDS
and R = ?0.62 for ADS.
?
Next, we explore one possible reason for this
structural difference between Japanese and En-
glish, especially at the level of the lexicon.
4.3 Syllable structure and lexical
composition of Japanese and English
One of the most salient differences between En-
glish and Japanese phonology concerns their syl-
lable structure. This is illustrated in Figure 2
(above), where we plotted the frequency of the dif-
ferent syllabic structures of monosyllabic tokens
in English and Japanese CDS. The statistics show
that English has a very rich syllabic composition
where a diversity of consonant clusters is allowed,
whereas Japanese syllable structure is quite simple
and mostly composed of the default CV type. This
difference is bound to have an effect on the struc-
ture of the lexicon. Indeed, Japanese has to use
5
Figure 2 : Trade-off between the complexity of syllable structure (above) and the word token length in terms of syllables
(below) for English and Japanese CDS.
multisyllabic words in order to achieve a large size
lexicon, whereas, in principle, English could use
mostly monosyllables. In Figure 2 (below) we dis-
play the distribution of word length as measured
in syllables in the two languages for the CDS cor-
pora. The English data is indeed mostly composed
of mono-syllabic words whereas the Japanese one
is made of words of more varied lengths. Overall,
we have documented a trade-off between the di-
versity of syllable structure on the one hand, and
the diversity of word lengths on the other (see Ta-
ble 4 for a summary of this tradeoff expressed in
terms of entropy).
? CDS ADS
? Eng. Jap. Eng. Jap.
Syllable types 2.40 1.38 2.58 1.03
Token lengths 0.62 2.04 0.99 1.69
Table 4 : Entropies of syllable types and token lengths in
terms of syllables (in bits)
We suggest that this trade-off is responsible for
the difference in the lexicon ambiguity across the
two languages. Specifically, the combination of
a small number of syllable types and, as a conse-
quence, the tendency for multi-syllabic word types
in Japanese makes it likely that a long word will
be composed of smaller ones. This cannot happen
very often in English, since most words are mono-
syllabic, and words smaller than a syllable are not
allowed.
5 Improving Japanese unsupervised
segmentation
We showed in the previous section that ambigu-
ity impacts segmentation even with a gold lexicon,
mainly because the lexicon itself could be ambigu-
ous. In an unsupervised segmentation setting, the
problem is worse because ambiguity within and
across word boundaries leads to a bad lexicon,
which in turn results in more segmentation errors.
In this section, we explore the possibility of miti-
gating some of these negative consequences.
In section 3, we saw that when the Unigram
model tries to learn Japanese words, it produces an
output lexicon composed of both over- and under-
segmented words in addition to words that re-
sult from a segmentation across word boundaries.
One way to address this is by learning multiple
kinds of units jointly, rather than just words; in-
deed, previous work has shown that richer mod-
els with multiple levels improve segmentation for
English (Johnson, 2008a; Johnson and Goldwater,
2009).
5.1 Two dependency levels
As a first step, we will allow the model to not
just learn words but to also memorize sequences of
words. Johnson (2008a) introduced these units as
?collocations? but we choose to use the more neu-
tral notion of level for reasons that become clear
shortly. Concretely, the grammar is:
6
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.81 0.77 0.86 0.42 0.33 0.55 0.70 0.63 0.78 0.42 0.35 0.50
? Boundaries 0.91 0.84 0.98 0.63 0.47 0.96 0.86 0.76 0.98 0.73 0.61 0.90
? Lexicon 0.64 0.79 0.54 0.18 0.55 0.10 0.36 0.56 0.26 0.15 0.68 0.08
Level 2
? Segmentation 0.33 0.45 0.26 0.59 0.65 0.53 0.50 0.60 0.43 0.45 0.54 0.38
? Boundaries 0.56 0.98 0.40 0.71 0.87 0.60 0.76 0.95 0.64 0.73 0.92 0.60
? Lexicon 0.36 0.25 0.59 0.47 0.44 0.49 0.46 0.38 0.56 0.43 0.37 0.50
Table 5 : Word segmentation scores of the two levels model
?
Utterance? level2+
level2? level1+
level1? Phoneme+
?
We run this model under the same conditions
as the Unigram model but evaluate two different
situations. The model has no inductive bias that
would force it to equate level1 with words, rather
than level2. Consequently, we evaluate the seg-
mentation that is the result of taking there to be a
boundary between every level1 constituent (Level
1 in Table 5) and between every level2 constituent
(Level 2 in Table 5 ). From these results , we see
that English data has better scores when the lower
level represents the Word unit and when the higher
level captures regularities above the word. How-
ever, Japanese data is best segmented when the
higher level is the Word unit and the lower level
captures sub-word regularities.
Level 1 generally tends to over-segment utter-
ances as can be seen by comparing the Boundary
Recall and Precision scores (Goldwater, 2007). In
fact when the Recall is much higher than the Pre-
cision, we can say that the model has a tendency
to over-segment. Conversely, we see that Level 2
tends to under-segment utterances as the Bound-
ary Precision is higher than the Recall.
Over-segmentation at Level 1 seems to benefit
English since it counteracts the tendency of the
Unigram model to cluster high frequency colloca-
tions. As far as segmentation is concerned, this
effect seems to outweigh the negative effect of
breaking words apart (especially in CDS), as En-
glish words are mostly monosyllabic.
For Japanese, under-segmentation at Level 2
seems to be slightly less harmful than over-
segmentation at Level 1, as it prevents, to some
extent, multi-syllabic words to be split. However,
the scores are not very different from the ones we
had with the Unigram model and slightly worse
for the ADS. What seems to be missing is an inter-
mediate level where over- and under-segmentation
would counteract one another.
5.2 Three dependency levels
We add a third dependency level to our model as
follows :
?
Utterance? level3+
level3? level2+
level2? level1+
level1? Phoneme+
?
As with the previous model, we test each of the
three levels as the word unit, the results are shown
in Table 6.
Except for English CDS, all the corpora
have their best scores with this intermediate
level. Level 1 tends to over-segment Japanese
utterances into syllables and English utterances
into morphemes. Level 3, however, tends to
highly under-segment both languages. English
CDS seems to be already under-segmented at
Level 2, very likely caused by the large number
of word collocations like ?is-it? and ?what-is?,
an observation also made by Bo?rschinger et al
(2012) using different English CDS corpora.
English ADS is quantitatively more sensitive to
over-segmentation than CDS mainly because it
has a richer morphological structure and relatively
longer words in terms of syllables (Table 4).
7
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.79 0.74 0.85 0.27 0.20 0.41 0.35 0.28 0.48 0.37 0.30 0.47
? Boundaries 0.89 0.81 0.99 0.56 0.39 0.99 0.68 0.52 0.99 0.70 0.57 0.93
? Lexicon 0.58 0.76 0.46 0.10 0.47 0.05 0.13 0.39 0.07 0.10 0.70 0.05
Level 2
? Segmentation 0.49 0.60 0.42 0.70 0.70 0.70 0.77 0.76 0.79 0.60 0.65 0.55
? Boundaries 0.71 0.97 0.56 0.81 0.82 0.81 0.90 0.88 0.92 0.81 0.90 0.74
? Lexicon 0.51 0.41 0.64 0.53 0.59 0.47 0.58 0.69 0.50 0.51 0.57 0.46
Level 3
? Segmentation 0.18 0.31 0.12 0.39 0.53 0.30 0.43 0.55 0.36 0.28 0.42 0.21
? Boundaries 0.26 0.99 0.15 0.46 0.93 0.31 0.71 0.98 0.55 0.59 0.96 0.43
? Lexicon 0.17 0.10 0.38 0.32 0.25 0.41 0.37 0.28 0.51 0.27 0.20 0.42
Table 6 : Word segmentation scores of the three levels model
6 Conclusion
In this paper we identified a property of lan-
guage, segmentation ambiguity, which we quan-
tified through Normalized Segmentation Entropy.
We showed that this quantity predicts performance
in a supervised segmentation task.
With this tool we found that English was in-
trinsically less ambiguous than Japanese, account-
ing for the systematic difference found in this pa-
per. More generally, we suspect that Segmentation
Ambiguity would, to some extent, explain much
of the difference observed across languages (Ta-
ble 1). Further work needs to be carried out to test
the robustness of this hypothesis on a larger scale.
We showed that allowing the system to learn
at multiple levels of structure generally improves
performance, and compensates partially for the
negative effect of segmentation ambiguity on un-
supervised segmentation (where a bad lexicon am-
plifies the effect of segmentation ambiguity). Yet,
we end up with a situation where the best level of
structure may not be the same across corpora or
languages, which raises the question as to how to
determine which level is the correct lexical level,
i.e., the level that can sustain successful grammat-
ical and semantic learning. Further research is
needed to answer this question.
Generally speaking, ambiguity is a challenge in
many speech and language processing tasks: for
example part-of-speech tagging and word sense
disambiguation tackle lexical ambiguity, proba-
bilistic parsing deals with syntactic ambiguity and
speech act interpretation deals with pragmatic am-
biguities. However, to our knowledge, ambiguity
has rarely been considered as a serious problem in
word segmentation tasks.
As we have shown, the lexicon-based approach
does not completely solve the segmentation am-
biguity problem since the lexicon itself could be
more or less ambiguous depending on the lan-
guage. Evidently, however, infants in all lan-
guages manage to overcome this ambiguity. It has
to be the case, therefore, that they solve this prob-
lem through the use of alternative strategies, for
instance by relying on sub-lexical cues (see Jarosz
and Johnson (2013)) or by incorporating semantic
or syntactic constraints (Johnson et al, 2010). It
remains a major challenge to integrate these strate-
gies within a common model that can learn with
comparable performance across typologically dis-
tinct languages.
Acknowledgements
The research leading to these results has received funding
from the European Research Council (FP/2007-2013) / ERC
Grant Agreement n. ERC-2011-AdG-295810 BOOTPHON,
from the Agence Nationale pour la Recherche (ANR-2010-
BLAN-1901-1 BOOTLANG, ANR-11-0001-02 PSL* and
ANR-10-LABX-0087) and the Fondation de France. This
research was also supported under the Australian Research
Council?s Discovery Projects funding scheme (project num-
bers DP110102506 and DP110102593).
8
References
Eleanor Olds Batchelder. 2002. Bootstrapping the lex-
icon: A computational model of infant speech seg-
mentation. Cognition, 83(2):167?206.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Daniel Blanchard, Jeffrey Heinz, and Roberta
Golinkoff. 2010. Modeling the contribution of
phonotactic cues to the problem of word segmenta-
tion. Journal of Child Language, 37(3):487?511.
Benjamin Bo?rschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size
for Bayesian word segmentation on the Providence
corpus. In Proceedings of the 24th International
Conference on Computational Linguistics (Coling
2012), pages 325?340, Mumbai, India. Coling 2012
Organizing Committee.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
the 2nd Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 1?9, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
M. Brent and T. Cartwright. 1996. Distributional regu-
larity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Morten H Christiansen, Joseph Allen, and Mark S Sei-
denberg. 1998. Learning to segment speech using
multiple cues: A connectionist model. Language
and cognitive processes, 13(2-3):221?268.
Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
Science, 35(1):119?155.
Margaret M. Fleck. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Sharon Goldwater. 2007. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Naomi Hamasaki. 2002. The timing shift of two-year-
olds responses to caretakers yes/no questions. In
Studies in language sciences (2)Papers from the 2nd
Annual Conference of the Japanese Society for Lan-
guage Sciences, pages 193?206.
Gaja Jarosz and J Alex Johnson. 2013. The richness
of distributional cues to word boundaries in speech
to young children. Language Learning and Devel-
opment, (ahead-of-print):1?36.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 528?536, Beijing, China,
August. Coling 2010 Organizing Committee.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Elizabeth K. Johnson and Peter W. Jusczyk. 2001.
Word segmentation by 8-month-olds: When speech
cues count more than statistics. Journal of Memory
and Language, 44:1?20.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York. Associ-
ation for Computational Linguistics.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information Processing
Systems 23, pages 1018?1026.
Mark Johnson. 2008a. Unsupervised word segmen-
tation for Sesotho using Adaptor Grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Mark Johnson. 2008b. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Peter W Jusczyk and Richard N Aslin. 1995. Infants
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Peter W. Jusczyk, E. A. Hohne, and A. Bauman.
1999. Infants? sensitivity to allophonic cues for
word segmentation. Perception and Psychophysics,
61:1465?1476.
9
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Transcription, format and
programs, volume 1. Lawrence Erlbaum.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In proc. LREC, volume 2, pages 947?952.
Sven L Mattys, Peter W Jusczyk, Paul A Luce, James L
Morgan, et al 1999. Phonotactic and prosodic ef-
fects on word segmentation in infants. Cognitive
psychology, 38(4):465?494.
Sven L Mattys, Peter W Jusczyk, et al 2001. Do
infants segment words or recurring contiguous pat-
terns? Journal of experimental psychology, human
perception and performance, 27(3):644?655.
Bruna Pelucchi, Jessica F Hay, and Jenny R Saffran.
2009. Learning in reverse: Eight-month-old infants
track backward transitional probabilities. Cognition,
113(2):244?247.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
J. Saffran, R. Aslin, and E. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27(3):379?423.
Daniel Swingley. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
Daniel J Weiss, Chip Gerfen, and Aaron D Mitchel.
2010. Colliding cues in word segmentation: the
role of cue strength and general cognitive processes.
Language and Cognitive Processes, 25(3):402?422.
Aris Xanthos. 2004. Combining utterance-boundary
and predictability approaches to speech segmenta-
tion. In First Workshop on Psycho-computational
Models of Human Language Acquisition, page 93.
10
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 102?103,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Grammars and Topic ModelsMark Johnson
Centre for Language Sciences and Dept. of Computing
Macquarie University
Sydney, AustraliaMark.Johnson@MQ.edu.au1 Abstract
Context-free grammars have been a cornerstone
of theoretical computer science and computational
linguistics since their inception over half a century
ago. Topic models are a newer development in ma-
chine learning that play an important role in doc-
ument analysis and information retrieval. It turns
out there is a surprising connection between the
two that suggests novel ways of extending both
grammars and topic models. After explaining this
connection, I go on to describe extensions which
identify topical multiword collocations and auto-
matically learn the internal structure of named-
entity phrases.
The adaptor grammar framework is a non-
parametric extension of probabilistic context-free
grammars (Johnson et al, 2007), which was ini-
tially intended to allow fast prototyping of mod-
els of unsupervised language acquisition (John-
son, 2008), but it has been shown to have applica-
tions in text data mining and information retrieval
as well (Johnson and Demuth, 2010; Hardisty et
al., 2010). We?ll see how learning the referents of
words (Johnson et al, 2010) and learning the roles
of social cues in language acquisition (Johnson et
al., 2012) can be viewed as a kind of topic mod-
elling problem that can be reduced to a grammat-
ical inference problem using the techniques de-
scribed in this talk.2 About the Speaker
Mark Johnson is a Professor of Language Science
(CORE) in the Department of Computing at Mac-
quarie University in Sydney, Australia. He was
awarded a BSc (Hons) in 1979 from the Univer-
sity of Sydney, an MA in 1984 from the Univer-
sity of California, San Diego and a PhD in 1987
from Stanford University. He held a postdoc-
toral fellowship at MIT from 1987 until 1988, and
has been a visiting researcher at the University of
Stuttgart, the Xerox Research Centre in Grenoble,
CSAIL at MIT and the Natural Language group
at Microsoft Research. He has worked on a wide
range of topics in computational linguistics, but
his main research areas are computational mod-
els of language acquisition, and parsing and its ap-
plications to text and speech processing. He was
President of the Association for Computational
Linguistics in 2003 and is Vice-President elect of
EMNLP, and was a professor from 1989 until 2009
in the Departments of Cognitive and Linguistic
Sciences and Computer Science at Brown Univer-
sity.References
Eric A. Hardisty, Jordan Boyd-Graber, and Philip
Resnik. 2010. Modeling perspective using adap-
tor grammars. In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 284?292, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics (Coling 2010), pages 528?536, Beijing, China,
August. Coling 2010 Organizing Committee.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information ProcessingSystems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information ProcessingSystems 23, pages 1018?1026.
Mark Johnson, Katherine Demuth, and Michael Frank.
2012. Exploiting social information in grounded
language learning via grammatical reduction. In
102
Proceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 883?
891, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-nual Meeting of the Association of ComputationalLinguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
103
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163?172,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Non-Monotonic Arc-Eager Transition System for Dependency Parsing
Matthew Honnibal
Department of Computing
Macquarie University
Sydney, Australia
matthew.honnibal@mq.edu.edu.au
Yoav Goldberg
Department of Computer Science
Bar Ilan University
Ramat Gan, Israel
yoav.goldberg@gmail.com
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.edu.au
Abstract
Previous incremental parsers have used
monotonic state transitions. However,
transitions can be made to revise previous
decisions quite naturally, based on further
information.
We show that a simple adjustment to the
Arc-Eager transition system to relax its
monotonicity constraints can improve ac-
curacy, so long as the training data in-
cludes examples of mistakes for the non-
monotonic transitions to repair. We eval-
uate the change in the context of a state-
of-the-art system, and obtain a statistically
significant improvement (p < 0.001) on
the English evaluation and 5/10 of the
CoNLL languages.
1 Introduction
Historically, monotonicity has played an im-
portant role in transition-based parsing systems.
Non-monotonic systems, including the one pre-
sented here, typically redundantly generate multi-
ple derivations for each syntactic analysis, leading
to spurious ambiguity (Steedman, 2000). Early,
pre-statistical work on transition-based parsing
such as Abney and Johnson (1991) implicitly as-
sumed that the parser searches the entire space
of possible derivations. The presence of spuri-
ous ambiguity causes this search space to be a di-
rected graph rather than a tree, which considerably
complicates the search, so spurious ambiguity was
avoided whenever possible.
However, we claim that non-monotonicity and
spurious ambiguity are not disadvantages in a
modern statistical parsing system such as ours.
Modern statistical models have much larger search
spaces because almost all possible analyses are al-
lowed, and a numerical score (say, a probability
distribution) is used to distinguish better analy-
ses from worse ones. These search spaces are so
large that we cannot exhaustively search them, so
instead we use the scores associated with partial
analyses to guide a search that explores only a mi-
nuscule fraction of the space (In our case we use
greedy decoding, but even a beam search only ex-
plores a small fraction of the exponentially-many
possible analyses).
In fact, as we show here the additional redun-
dant pathways between search states that non-
monotonicity generates can be advantageous be-
cause they allow the parser to ?correct? an ear-
lier parsing move and provide an opportunity to
recover from formerly ?fatal? mistakes. Infor-
mally, non-monotonicity provides ?many paths up
the mountain? in the hope of making it easier to
find at least one.
We demonstrate this by modifying the Arc-
Eager transition system (Nivre, 2003; Nivre et al,
2004) to allow a limited capability for non-
monotonic transitions. The system normally em-
ploys two deterministic constraints that limit the
parser to actions consistent with the previous his-
tory. We remove these, and update the transitions
so that conflicts are resolved in favour of the latest
prediction.
The non-monotonic behaviour provides an im-
provement of up to 0.2% accuracy over the cur-
rent state-of-the-art in greedy parsing. It is pos-
sible to implement the greedy parser we de-
scribe very efficiently: our implementation, which
can be found at http://www.github.com/
syllog1sm/redshift, parses over 500 sen-
tences a second on commodity hardware.
163
2 The Arc-Eager Transition System
In transition-based parsing, a parser consists of a
state (or a configuration) which is manipulated by
a set of actions. An action is applied to a state
and results in a new state. The parsing process
concludes when the parser reaches a final state, at
which the parse tree is read from the state. A par-
ticular set of states and actions yield a transition-
system. Our starting point in this paper is the pop-
ular Arc-Eager transition system, described in de-
tail by Nivre (2008).
The state of the arc-eager system is composed
of a stack, a buffer and a set of arcs. The stack and
the buffer hold the words of a sentence, and the set
of arcs represent derived dependency relations.
We use a notation in which the stack items are
indicated by Si, with S0 being the top of the stack,
S1 the item previous to it and so on. Similarly,
buffer items are indicated as Bi, with B0 being
the first item on the buffer. The arcs are of the
form (h, l,m), indicating a dependency in which
the word m modifies the word h with label l.
In the initial configuration the stack is empty,
and the buffer contains the words of the sentence
followed by an artificial ROOT token, as sug-
gested by Ballesteros and Nivre (2013). In the fi-
nal configuration the buffer is empty and the stack
contains the ROOT token.
There are four parsing actions (Shift, Left-Arc,
Right-Arc and Reduce, abbreviated as S,L,R,D re-
spectively) that manipulate stack and buffer items.
The Shift action pops the first item from the buffer
and pushes it on the stack (the Shift action has a
natural precondition that the buffer is not empty,
as well as a precondition that ROOT can only be
pushed to an empty stack). The Right-Arc action
is similar to the Shift action, but it also adds a
dependency arc (S0, B0), with the current top of
the stack as the head of the newly pushed item
(the Right action has an additional precondition
that the stack is not empty).1 The Left-Arc action
adds a dependency arc (B0, S0) with the first item
in the buffer as the head of the top of the stack,
and pops the stack (with a precondition that the
stack and buffer are not empty, and that S0 is not
assigned a head yet). Finally, the Reduce action
pops the stack, with a precondition that the stack
is not empty and that S0 is already assigned a head.
1For labelled dependency parsing, the Right-Arc and
Left-Arc actions are parameterized by a label L such that the
action RightL adds an arc (S0, L,B0), similarly for LeftL.
2.1 Monotonicty
The preconditions of the Left-Arc and Reduce ac-
tions ensure that every word is assigned exactly
one head, resulting in a well-formed parse tree.
The single head constraint is enforced by ensur-
ing that once an action has been performed, sub-
sequent actions must be consistent with it. We re-
fer to this consistency as the monotonicity of the
system.
Due to monotonicity, there is a natural pair-
ing between the Right-Arc and Reduce actions
and the Shift and Left-Arc actions: a word which
is pushed into the stack by Right-Arc must be
popped using Reduce, and a word which is pushed
by Shift action must be popped using Left-Arc. As
a consequence of this pairing, a Right-Arc move
determines that the head of the pushed token must
be to its left, while a Shift moves determines a
head to its right. Crucially, the decision whether
to Right-Arc or Shift is often taken in a state of
missing information regarding the continuation of
the sentence, forcing an incorrect head assignment
on a subsequent move.
Consider a sentence pair such as (a)?I saw Jack
and Jill? / (b)?I saw Jack and Jill fall?. In (a), ?Jack
and Jill? is the NP object of ?saw?, while in (b) it is
a subject of the embedded verb ?fall?. The mono-
tonic arc-eager parser has to decide on an analysis
as soon as it sees ?saw? on the top of the stack and
?Jack? at the front of the buffer, without access to
the disambiguating verb ?fall?.
In what follows, we suggest a non-monotonic
variant of the Arc-Eager transition system, allow-
ing the parser to recover from the incorrect head
assignments which are forced by an incorrect res-
olution of a Shift/Right-Arc ambiguity.
3 The Non-Monotonic Arc-Eager System
The Arc-Eager transition system (Nivre et al,
2004) has four moves. Two of them create depen-
dencies, two push a word from the buffer to the
stack, and two remove an item from the stack:
Push Pop
Adds dependency Right-Arc Left-Arc
No new dependency Shift Reduce
Every word in the sentence is pushed once and
popped once; and every word must have exactly
one head. This creates two pairings, along the di-
agonals: (S, L) and (R, D). Either the push move
adds the head or the pop move does, but not both
and not neither.
164
I saw Jack and Jill fall R
S L S R R D R D L R D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill fall R
2 4 5
7
2 5
7
9
Figure 1: State before and after a non-monotonic Left-
Arc. At move 9, fall is the first word of the buffer (marked
with an arrow), and saw and Jack are on the stack (circled).
The arc created at move 4 was incorrect (in red). Arcs are
labelled with the move that created them. After move 9 (the
lower state), the non-monotonic Left-Arc move has replaced
the incorrect dependency with a correct Left-Arc (in green).
Thus in the Arc-Eager system the first move de-
termines the corresponding second move. In our
non-monotonic system the second move can over-
write an attachment made by the first. This change
makes the transition system non-monotonic, be-
cause if the model decides on an incongruent pair-
ing we will have to either undo or add a depen-
dency, depending on whether we correct a prior
Right-Arc, or a prior Shift.
3.1 Non-monotonic Left-Arc
Figure 1 shows a before-and-after view of a non-
monotonic transition. The sequence below the
words shows the transition history. The words that
are circled in the upper and lower line are on the
stack before and after the transition, respectively.
The arrow shows the start of the buffer, and arcs
are labelled with the move that added them.
The parser began correctly by Shifting I and
Left-Arcing it to saw, which was then also Shifted.
The mistake, made at Move 4, was to Right-Arc
Jack instead of Shifting it.
The difficulty of this kind of a decision for an
incremental parser is fundamental. The leftward
context does not constrain the decision, and an ar-
bitrary amount of text could separate Jack from
fall. Eye-tracking experiments show that humans
often perform a saccade while reading such exam-
ples (Frazier and Rayner, 1982).
In moves 5-8 the parser correctly builds the rest
of the NP, and arrives at fall. The monotonicity
constraints would force an incorrect analysis, hav-
ing fall modify Jack or saw, or having saw modify
fall as an embedded verb with no arguments.
I saw Jack and Jill R
S L S S R D R D R D D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill R
2 5
7
2 5
7
9
Figure 2: State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11 the parser
has Jack on the stack (circled), with only the dummy ROOT
token left in the buffer. A monotonic parser must determinis-
tically Left-Arc Jack here to preserve the previous decision,
despite the current state. We remove this constraint, and in-
stead assume that when the model selects Reduce for a head-
less item, it is reversing the previous Shift/Right decision. We
add the appropriate arc, assigning the label that scored high-
est when the Shift/Right decision was made.
We allow Left-Arcs to ?clobber? edges set by
Right-Arcs if the model recommends it. The pre-
vious edge is deleted, and the Left-Arc proceeds
as normal. The effect of this is exactly as if the
model had correctly chosen Shift at move 4. We
simply give the model a second chance to make
the correct choice.
3.2 Non-monotonic Reduce
The upper arcs in Figure 2 show a state resulting
from the opposite error. The parser has Shifted
Jack instead of Right-Arcing it. After building the
NP the buffer is exhausted, except for the ROOT
token, which is used to wrongly Left-Arc Jack as
the sentence?s head word.
Instead of letting the previous choice lock us in
to the pair (Shift, Left-Arc), we let the later deci-
sion reverse it to (Right-Arc, Reduce), if the parser
has predicted Reduce in spite of the signal from its
previous decision. In the context shown in Figure
2, the correctness of the Reduce move is quite pre-
dictable, once the choice is made available.
When the Shift/Right-Arc decision is reversed,
we add an arc between the top of the stack (S0)
and the word preceding it (S1). This is the arc that
would have been created had the parser chosen to
Right-Arc when it chose to Shift. Since our idea is
to reverse this mistake, we select the Right-Arc la-
bel that the model scored most highly at that time.2
2An alternative approach to label assignment is to parame-
terize the Reduce action with a label, similar to the Right-Arc
and Left-Arc actions, and let that label override the previ-
ously predicted label. This would allow the parser to con-
165
To summarize, our Non-Monotnonic Arc-
Eager system differs from the monotonic
Arc-Eager system by:
? Changing the Left-Arc action by removing
the precondition that S0 does not have a head,
and updating the dependency arcs such previ-
ously derived arcs having S0 as a dependent
are removed from the arcs set.
? Changing the Reduce action by removing the
precondition that S0 has a head, and updating
the dependency arcs such that if S0 does not
have a head, S1 is assigned as the head of S0.
4 Why have two push moves?
We have argued above that it is better to trust the
second decision that the model makes, rather than
using the first decision to determine the second.
If this is the case, is the first decision entirely re-
dundant? Instead of defining how pop moves can
correct Shift/Right-Arc mistakes, we could instead
eliminate the ambiguity. There are two possibili-
ties: Shift every token, and create all Right-Arcs
via Reduce; or Right-Arc every token, and replace
them with Left-Arcs where necessary.
Preliminary experiments on the development
data revealed a problem with these approaches. In
many cases the decision whether to Shift or Right-
Arc is quite clear, and its result provides useful
conditioning context to later decisions. The in-
formation that determined those decisions is never
lost, but saving all of the difficulty for later is not
a very good structured prediction strategy.
As an example of the problem, if the Shift move
is eliminated, about half of the Right-Arcs created
will be spurious. All of these arcs will be assigned
labels making important features uselessly noisy.
In the other approach, we avoid creating spurious
arcs, but the model does not predict whether S0 is
attached to S1, or what the label would be, and we
miss useful features.
The non-monotonic transition system we pro-
pose does not have these problems. The model
learns to make Shift vs. Right-Arc decisions as
normal, and conditions on them ? but without
committing to them.
dition its label decision on the new context, which was suf-
ficiently surprising to change its move prediction. For effi-
ciency and simplicity reasons, we chose instead to trust the
label the model proposed when the reduced token was ini-
tially pushed into the stack. This requires an extra vector of
labels to be stored during parsing.
5 Dynamic Oracles
An essential component when training a
transition-based parser is an oracle which,
given a gold-standard tree, dictates the sequence
of moves a parser should make in order to derive
it. Traditionally, these oracles are defined as func-
tions from trees to sequences, mapping a gold tree
to a single sequence of actions deriving it, even
if more than one sequence of actions derives the
gold tree. We call such oracles static. Recently,
Goldberg and Nivre (2012) introduced the concept
of a dynamic oracle, and presented a concrete ora-
cle for the arc-eager system. Instead of mapping
a gold tree to a sequence of actions, the dynamic
oracle maps a ?configuration, gold tree? pair to a
set of optimal transitions. More concretely, the
dynamic oracle presented in Goldberg and Nivre
(2012) maps ?action, configuration, tree? tuples
to an integer, indicating the number of gold arcs
in tree that can be derived from configuration
by some sequence of actions, but could not be
derived after applying action to the configuration.
There are two advantages to this. First, the
ability to label any configuration, rather than only
those along a single path to the gold-standard
derivation, allows much better training data to be
generated. States come with realistic histories, in-
cluding errors ? a critical point for the current
work. Second, the oracle accounts for spurious
ambiguity correctly, as it will label multiple ac-
tions as correct if the optimal parses resulting from
them are equally accurate.
In preliminary experiments in which we trained
the parser using the static oracle but allowed the
non-monotonic repair operations during parsing,
we found that the the repair moves yielded no im-
provement. This is because the static oracle does
not generate any examples of the repair moves dur-
ing training, causing the parser to rarely predict
them in test time. We will first describe the Arc-
Eager dynamic oracle, and then define dynamic
oracles for the non-monotonic transition systems
we present.
5.1 Monotonic Arc-Eager Dynamic Oracle
We now briefly describe the dynamic oracle for the
arc-eager system. For more details, see Goldberg
and Nivre (2012). The oracle is computed by rea-
soning about the arcs which are reachable from a
given state, and counting the number of gold arcs
which will no longer be reachable after applying a
166
given transition at a given state. 3
The reasoning is based on the observations that
in the arc-eager system, new arcs (h, l,m) can be
derived iff the following conditions hold:
(a) There is no existing arc (h?, l?,m) such that
h? 6= h, and (b) Either both h and m are on the
buffer, or one of them is on the buffer and the other
is on the stack. In other words:
(a) once a word acquires a head (in a Left-Arc or
Right-Arc transition) it loses the ability to acquire
any other head.
(b) once a word is moved from the buffer to the
stack (Shift or Right-Arc) it loses the ability to ac-
quire heads that are currently on the stack, as well
as dependents that are currently on the stack and
are not yet assigned a head.4
(c) once a word is removed from the stack (Left-
Arc or Reduce) it loses the ability to acquire any
dependents on the buffer.
Based on these observations, Goldberg and Nivre
(2012) present an oracle C(a, c, t) for the mono-
tonic arc-eager system, computing the number of
arcs in the gold tree t that are reachable from a
parser?s configuration c and are no longer reach-
able from the configuration a(c) resulting from the
application of action a to configuration c.
5.2 Non-monotonic Dynamic Oracles
Given the oracle C(a, c, t) for the monotonic sys-
tem, we adapt it to a non-monotonic variant by
considering the changes from the monotonic to the
non-monotonic system, and adding ? terms ac-
cordingly. We define three novel oracles: CNML,
CNMD and CNML+D for systems with a non-
monotonic Left-Arc, Reduce or both.
CNML(a, c, t) = C(a, c, t) +?NML(a, c, t)
CNMD(a, c, t) = C(a, c, t) +?NMD(a, c, t)
CNML+D(a, c, t) = C(a, c, t) +?NML(a, c, t)
+?NMD(a, c, t)
The terms ?NML and ?NMD reflect the score
adjustments that need to be done to the arc-eager
oracle due to the changes of the Left-Arc and Re-
duce actions, respectively, and are detailed below.
3The correctness of the oracle is based on a property of
the arc-eager system, stating that if a set of arcs which can be
extended to a projective tree can be individually derived from
a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configura-
tion. This same property holds also for the non-monotonic
variants we propose.
4The condition that the words on the stack are not yet as-
signed a head is missing from (Goldberg and Nivre, 2012)
Changes due to non-monotonic Left-Arc:
? ?NML(RIGHTARC, c, t): The cost of Right-
Arc is decreased by 1 if the gold head of B0 is
on the buffer (because B0 can still acquire its
correct head later with a Left-Arc action). It
is increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w
is assigned a head already (in the monotonic
oracle, this cost was taken care of when the
word was assigned an incorrect head. In the
non-monotonic variant, this cost is delayed).
? ?NML(REDUCE, c, t): The cost of Reduce is
increased by 1 if the gold head of S0 is on the
buffer, because removing S0 from the stack
precludes it from acquiring its correct head
later on with a Left-Arc action. (This cost is
paid for in the monotonic version when S0
acquired its incorrect head).
? ?NML(LEFTARC, c, t): The cost of Left-
Arc is increased by 1 if S0 is already assigned
to its gold parent. (This situation is blocked
by a precondition in the monotonic case).
The cost is also increased if S0 is assigned
to a non-gold parent, and the gold parent is
in the buffer, but not B0. (As a future non-
monotonic Left-Arc is prevented from setting
the correct head.)
? ?NML(SHIFT, c, gold): The cost of Shift is
increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w is
assigned a head already. (As in Right-Arc, in
the monotonic oracle, this cost was taken care
of when w was assigned an incorrect head.)
Changes due to non-monotonic Reduce:
? ?NMD(SHIFT, c, gold): The cost of Shift is
decreased by 1 if the gold head of B0 is S0
(Because this arc can be added later on with
a non-monotonic Reduce action).
? ?NMD(LEFTARC, c, gold): The cost of
Left-Arc is increased by 1 if S0 is not as-
signed a head, and the gold head of S0 is
S1 (Because this precludes adding the correct
arc with a Reduce of S0 later).
? ?NMD(REDUCE, c, gold) = 0. While it may
seem that a change to the cost of a Reduce ac-
tion is required, in fact the costs of the mono-
tonic system hold here, as the head of S0 is
167
predetermined to be S1. The needed adjust-
ments are taken care of in Left-Arc and Shift
actions.5
? ?NMD(RIGHTARC, c, gold) = 0
6 Applying the Oracles in Training
Once the dynamic-oracles for the non-monotonic
system are defined, we could in principle just plug
them in the perceptron-based training procedure
described in Goldberg and Nivre (2012). How-
ever, a tacit assumption of the dynamic-oracles is
that all paths to recovering a given arc are treated
equally. This assumption may be sub-optimal
for the purpose of training a parser for a non-
monotonic system.
In Section 4, we explained why removing the
ambiguity between Shift and Right-Arcs alto-
gether was an inferior strategy. Failing to discrim-
inate between arcs reachable by monotonic and
non-monotonic paths does just that, so this oracle
did not perform well in preliminary experiments
on the development data.
Instead, we want to learn a model that will offer
its best prediction of Shift vs. Right-Arc, which
we expect to usually be correct. However, in those
cases where the model does make the wrong de-
cision, it should have the ability to later over-turn
that decision, by having an unconstrained choice
of Reduce vs. Left-Arc.
In order to correct for that, we don?t use the
non-monotonic oracles directly when training the
parser, but instead train the parser using both the
monotonic and non-monotonic oracles simultane-
ously by combining their judgements: while we
always prefer zero-cost non-monotonic actions to
monotonic-actions with non-zero cost, if the non-
monotonic oracle assigns several actions a zero-
cost, we prefer to follow those actions that are also
assigned a zero-cost by the monotonic oracle, as
these actions lead to the best outcome without re-
lying on a non-monotonic (repair) operation down
the road.
7 Experiments
We base our experiments on the parser described
by Goldberg and Nivre (2012). We began by im-
plementing their baseline system, a standard Arc-
Eager parser using an averaged Perceptron learner
and the extended feature set described by Zhang
5If using a labeled reduce transition, the label assignment
costs should be handled here.
Stanford MALT
W S W S
Unlabelled Attachment
Baseline (G&N-12) 91.2 42.0 90.9 39.7
NM L 91.4 43.1 91.0 40.1
NM D 91.4 42.8 91.1 41.2
NM L+D 91.6 43.3 91.3 41.5
Labelled Attachment
Baseline (G&N-12) 88.7 31.8 89.7 36.6
NM L 89.0 32.5 89.8 36.9
NM D 88.9 32.3 89.9 37.7
NM L+D 89.1 32.7 90.0 37.9
Table 1: Development results on WSJ 22. Both non-
monotonic transitions bring small improvements in per-token
(W) and whole sentence (S) accuracy, and the improvements
are additive.
and Nivre (2011). We follow Goldberg and Nivre
(2012) in training all models for 15 iterations, and
shuffling the sentences before each iteration.
Because the sentence ordering affects the
model?s accuracy, all results are averaged from
scores produced using 20 different random seeds.
The seed determines how the sentences are shuf-
fled before each iteration, as well as when to fol-
low an optimal action and when to follow a non-
optimal action during training. The Wilcoxon
signed-rank test was used for significance testing.
A train/dev/test split of 02-21/22/23 of the Penn
Treebank WSJ (Marcus et al, 1993) was used for
all models. The data was converted into Stan-
ford dependencies (de Marneffe et al, 2006) with
copula-as-head and the original PTB noun-phrase
bracketing. We also evaluate our models on de-
pendencies created by the PENN2MALT tool, to
assist comparison with previous results. Automat-
ically assigned POS tags were used during training,
to match the test data more closely. 6 We also eval-
uate the non-monotonic transitions on the CoNLL
2007 multi-lingual data.
8 Results and analysis
Table 1 shows the effect of the non-monotonic
transitions on labelled and unlabelled attachment
score on the development data. All results are av-
erages from 20 models trained with different ran-
dom seeds, as the ordering of the sentences at each
iteration of the Perceptron algorithm has an effect
on the system?s accuracy. The two non-monotonic
transitions each bring small but statistically signif-
icant improvements that are additive when com-
bined in the NM L+D system. The result is stable
6We thank Yue Zhang for supplying the POS-tagged files
used in the Zhang and Nivre (2011) experiments.
168
across both dependency encoding schemes.
Frequency analysis. Recall that there are two pop
moves available: Left-Arc and Reduce. The Left-
Arc is considered non-monotonic if the top of the
stack has a head specified, and the Reduce move
is considered non-monotonic if it does not. How
often does the parser select monotonic and non-
monotonic pop moves, and how often is its deci-
sion correct?
In Table 2, the True Positive column shows how
often non-monotonic transitions were used to add
gold standard dependencies. The False Positive
column shows how often they were used incor-
rectly. The False Negative column shows how
often the parser missed a correct non-monotonic
transition, and the True Negative column shows
how often the monotonic alternative was correctly
preferred (e.g. the parser correctly chose mono-
tonic Reduce in place of non-monotonic Left-
Arc). Punctuation dependencies were excluded.
The current system has high precision but low
recall for repair operations, as they are relatively
rare in the gold-standard. While we already
see improvements in accuracy, the upper bound
achievable by the non-monotonic operations is
higher, and we hope to approach it in the future
using improved learning techniques.
Linguistic analysis. To examine what construc-
tions were being corrected, we looked at the fre-
quencies of the labels being introduced by the
non-monotonic moves. We found that there were
two constructions being commonly repaired, and
a long-tail of miscellaneous cases.
The most frequent repair involved the mark la-
bel. This is assigned to conjunctions introducing
subordinate clauses. For instance, in the sentence
Results were released after the market closed, the
Stanford scheme attaches after to closed. The
parser is misled into greedily attaching after to re-
leased here, as that would be correct if after were a
preposition, as in Results were released after mid-
night. This construction was repaired 33 times, 13
where the initial decision was mark, and 21 times
the other way around. The other commonly re-
paired construction involved greedily attaching an
object that was actually the subject of a comple-
ment clause, e.g. NCNB corp. reported net income
doubled. These were repaired 19 times.
WSJ evaluation. Table 3 shows the final test
results. While still lagging behind search based
parsers, we push the boundaries of what can be
TP FP TN FN
Left-Arc 60 14 18,466 285
Reduce 52 26 14,066 250
Total 112 40 32,532 535
Table 2: True/False positive/negative rates for the predic-
tion of the non-monotonic transitions. The non-monotonic
transitions add correct dependencies 112 times, and produce
worse parses 40 times. 535 opportunities for non-monotonic
transitions were missed.
System O Stanford Penn2Malt
LAS UAS LAS UAS
K&C 10 n3 ? ? ? 93.00
Z&N 11 nk 91.9 93.5 91.8 92.9
G&N 12 n 88.72 90.96 ? ?
Baseline(G&N-12) n 88.7 90.9 88.7 90.6
NM L+D n 88.9 91.1 88.9 91.0
Table 3: WSJ 23 test results, with comparison against the
state-of-the-art systems from the literature of different run-
times. K&C 10=Koo and Collins (2010); Z&N 11=Zhang
and Nivre (2011); G&N 12=Goldberg and Nivre (2012).
achieved with a purely greedy system, with a sta-
tistically significant improvement over G&N 12.
CoNLL 2007 evaluation. Table 4 shows the ef-
fect of the non-monotonic transitions across the
ten languages in the CoNLL 2007 data sets. Statis-
tically significant improvements in accuracy were
observed for five of the ten languages. The accu-
racy improvement on Hungarian and Arabic did
not meet our significance threshold. The non-
monotonic transitions did not decrease accuracy
significantly on any of the languages.
9 Related Work
One can view our non-monotonic parsing system
as adding ?repair? operations to a greedy, deter-
ministic parser, allowing it to undo previous de-
cisions and thus mitigating the effect of incorrect
parsing decisions due to uncertain future, which
is inherent in greedy left-to-right transition-based
parsers. Several approaches have been taken to ad-
dress this problem, including:
Post-processing Repairs (Attardi and Ciaramita,
2007; Hall and Nova?k, 2005; Inokuchi and Ya-
maoka, 2012) Closely related to stacking, this line
of work attempts to train classifiers to repair at-
tachment mistakes after a parse is proposed by
a parser by changing head attachment decisions.
The present work differs from these by incorporat-
ing the repair process into the transition system.
Stacking (Nivre and McDonald, 2008; Martins
et al, 2008), in which a second-stage parser runs
over the sentence using the predictions of the first
parser as features. In contrast our parser works in
169
System AR BASQ CAT CHI CZ ENG GR HUN ITA TUR
Baseline 83.4 76.2 91.5 82.3 78.8 87.9 81.2 77.6 83.8 78.0
NM L+D 83.6 76.1 91.5 82.7 80.1 88.4 81.8 77.9 84.1 78.0
Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended
upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p > 0.4).
a single, left-to-right pass over the sentence.
Non-directional Parsing The EasyFirst parser
of Goldberg and Elhadad (2010) tackles similar
forms of ambiguities by dropping the Shift action
altogether, and processing the sentence in an easy-
to-hard bottom-up order instead of left-to-right,
resulting in a greedy but non-directional parser.
The indeterminate processing order increases the
parser?s runtime from O(n) to O(n log n). In con-
trast, our parser processes the sentence incremen-
tally, and runs in a linear time.
Beam Search An obvious approach to tackling
ambiguities is to forgo the greedy nature of the
parser and instead to adopt a beam search (Zhang
and Clark, 2008; Zhang and Nivre, 2011) or a
dynamic programming (Huang and Sagae, 2010;
Kuhlmann et al, 2011) approach. While these ap-
proaches are very successful in producing high-
accuracy parsers, we here explore what can be
achieved in a strictly deterministic system, which
results in much faster and incremental parsing al-
gorithms. The use of non-monotonic transitions in
beam-search parser is an interesting topic for fu-
ture work.
10 Conclusion and future work
We began this paper with the observation that
because the Arc-Eager transition system (Nivre
et al, 2004) attaches a word to its governor ei-
ther when the word is pushed onto the stack or
when it is popped off the stack, monotonicity (plus
the ?tree constraint? that a word has exactly one
governor) implies that a word?s push-move de-
termines its associated pop-move. In this paper
we suggest relaxing the monotonicity constraint
to permit the pop-move to alter existing attach-
ments if appropriate, thus breaking the 1-to-1 cor-
respondence between push-moves and pop-moves.
This permits the parser to correct some early in-
correct attachment decisions later in the parsing
process. Adding additional transitions means that
in general there are multiple transition sequences
that generate any given syntactic analysis, i.e., our
non-monotonic transition system generates spuri-
ous ambiguities (note that the Arc-Eager transition
system on its own generates spurious ambiguities).
As we explained in the paper, with the greedy de-
coding used here additional spurious ambiguity is
not necessarily a draw-back.
The conventional training procedure for
transition-based parsers uses a ?static? oracle
based on ?gold? parses that never predicts a
non-monotonic transition, so it is clearly not
appropriate here. Instead, we use the incremental
error-based training procedure involving a ?dy-
namic? oracle proposed by Goldberg and Nivre
(2012), where the parser is trained to predict the
transition that will produce the best-possible anal-
ysis from its current configuration. We explained
how to modify the Goldberg and Nivre oracle so
it predicts the optimal moves, either monotonic or
non-monotonic, from any configuration, and use
this to train an averaged perceptron model.
When evaluated on the standard WSJ training
and test sets we obtained a UAS of 91.1%, which
is a 0.2% improvement over the already state-of-
the-art baseline of 90.9% that is obtained with the
error-based training procedure of Goldberg and
Nivre (2012). On the CoNLL 2007 datasets, ac-
curacy improved significantly on 5/10 languages,
and did not decline significantly on any of them.
Looking to the future, we believe that it would
be interesting to investigate whether adding non-
monotonic transitions is beneficial in other parsing
systems as well, including systems that target for-
malisms other than dependency grammars. As we
observed in the paper, the spurious ambiguity that
non-monotonic moves introduce may well be an
advantage in a statistical parser with an enormous
state-space because it provides multiple pathways
to the correct analysis (of which we hope at least
one is navigable).
We investigated a very simple kind of non-
monotonic transition here, but of course it?s pos-
sible to design transition systems with many more
transitions, including transitions that are explicitly
designed to ?repair? characteristic parser errors. It
might even be possible to automatically identify
the most useful repair transitions and incorporate
them into the parser.
170
Acknowledgments
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
Stephen Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of pars-
ing strategies. Journal of Psycholinguistic Re-
search, 20(3):233?250.
Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency
parsing. In Human Language Technologies
2007: The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main Confer-
ence, pages 388?395. Association for Compu-
tational Linguistics, Rochester, New York.
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178?210.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association
for Computational Linguistics (NAACL HLT),
pages 742?750.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Keith Hall and Vaclav Nova?k. 2005. Corrective
modeling for non-projective dependency pars-
ing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT),
pages 42?52.
Liang Huang and Kenji Sagae. 2010. Dynamic
programming for linear-time incremental pars-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics (ACL), pages 1077?1086.
Akihiro Inokuchi and Ayumu Yamaoka. 2012.
Mining rules for rewriting states in a transition-
based dependency parser for English. In Pro-
ceedings of COLING 2012, pages 1275?1290.
The COLING 2012 Organizing Committee,
Mumbai, India.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?
11.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and
Giorgio Satta. 2011. Dynamic program-
ming algorithms for transition-based depen-
dency parsers. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 673?682. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?
330.
Andre? Filipe Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 157?166.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513?553.
Joakim Nivre, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing. In
171
Hwee Tou Ng and Ellen Riloff, editors, HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learn-
ing (CoNLL-2004), pages 49?56. Association
for Computational Linguistics, Boston, Mas-
sachusetts, USA.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based de-
pendency parsers. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 950?958.
Mark Steedman. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-
based and transition-based dependency parsing.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 562?571. Association for Computa-
tional Linguistics, Honolulu, Hawaii.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188?193. Association for Computational
Linguistics, Portland, Oregon, USA.
172
