Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 53?56, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseLearner: Word Sense Disambiguation
for All Words in Unrestricted Text
Rada Mihalcea and Andras Csomai
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu, ac0225@unt.edu
Abstract
This paper describes SENSELEARNER ? a
minimally supervised word sense disam-
biguation system that attempts to disam-
biguate all content words in a text using
WordNet senses. We evaluate the accu-
racy of SENSELEARNER on several stan-
dard sense-annotated data sets, and show
that it compares favorably with the best re-
sults reported during the recent SENSEVAL
evaluations.
1 Introduction
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a polyse-
mous word within a given context. Applications such
as machine translation, knowledge acquisition, com-
mon sense reasoning, and others, require knowledge
about word meanings, and word sense disambiguation
is considered essential for all these applications.
Most of the efforts in solving this problem were
concentrated so far toward targeted supervised learn-
ing, where each sense tagged occurrence of a particu-
lar word is transformed into a feature vector, which is
then used in an automatic learning process. The appli-
cability of such supervised algorithms is however lim-
ited only to those few words for which sense tagged
data is available, and their accuracy is strongly con-
nected to the amount of labeled data available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less attention.
While the performance of such methods is usually
exceeded by their supervised lexical-sample alterna-
tives, they have however the advantage of providing
larger coverage.
In this paper, we present a method for solving the
semantic ambiguity of all content words in a text. The
algorithm can be thought of as a minimally supervised
word sense disambiguation algorithm, in that it uses
a relatively small data set for training purposes, and
generalizes the concepts learned from the training data
to disambiguate the words in the test data set. As a
result, the algorithm does not need a separate classi-
fier for each word to be disambiguated, but instead it
learns global models for general word categories.
2 Background
For some natural language processing tasks, such as
part of speech tagging or named entity recognition,
regardless of the approach considered, there is a con-
sensus on what makes a successful algorithm. Instead,
no such consensus has been reached yet for the task
of word sense disambiguation, and previous work has
considered a range of knowledge sources, such as lo-
cal collocational clues, common membership in se-
mantically or topically related word classes, semantic
density, and others.
In recent SENSEVAL-3 evaluations, the most suc-
cessful approaches for all words word sense disam-
biguation relied on information drawn from annotated
corpora. The system developed by (Decadt et al,
2004) uses two cascaded memory-based classifiers,
combined with the use of a genetic algorithm for joint
parameter optimization and feature selection. A sep-
arate ?word expert? is learned for each ambiguous
word, using a concatenated corpus of English sense-
53
New raw
text
Feature vector
construction
(POS, NE, MWE)
Preprocessing 
Semantic model
learning
Sense?tagged
text
semantic models
SenseLearner
definitions
Word sense
disambiguation
Trained semantic
models
Sense?tagged
texts
Figure 1: Semantic model learning in SENSE-
LEARNER
tagged texts, including SemCor, SENSEVAL data sets,
and a corpus built from WordNet examples. The per-
formance of this system on the SENSEVAL-3 English
all words data set was evaluated at 65.2%.
Another top ranked system is the one developed by
(Yuret, 2004), which combines two Naive Bayes sta-
tistical models, one based on surrounding collocations
and another one based on a bag of words around the
target word. The statistical models are built based on
SemCor and WordNet, for an overall disambiguation
accuracy of 64.1%.
A different version of our own SENSELEARNER
system (Mihalcea and Faruque, 2004), using three of
the semantic models described in this paper, combined
with semantic generalizations based on syntactic de-
pendencies, achieved a performance of 64.6%.
3 SenseLearner
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm general
enough to be able to disambiguate as many content
words as possible in a text, and efficient enough so
that large amounts of text can be annotated in real
time. SENSELEARNER is attempting to learn general
semantic models for various word categories, starting
with a relatively small sense-annotated corpus. We
base our experiments on SemCor (Miller et al, 1993),
a balanced, semantically annotated dataset, with all
content words manually tagged by trained lexicogra-
phers.
The input to the disambiguation algorithm consists
of raw text. The output is a text with word meaning
annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-of-
speech tags; collocations are identified using a sliding
window approach, where a collocation is defined as
a sequence of words that forms a compound concept
defined in WordNet (Miller, 1995); named entities are
also identified at this stage1.
Next, a semantic model is learned for all predefined
word categories, which are defined as groups of words
that share some common syntactic or semantic prop-
erties. Word categories can be of various granulari-
ties. For instance, using the SENSELEARNER learn-
ing mechanism, a model can be defined and trained to
handle all the nouns in the test corpus. Similarly, us-
ing the same mechanism, a finer-grained model can be
defined to handle all the verbs for which at least one
of the meanings is of type <move>. Finally, small
coverage models that address one word at a time, for
example a model for the adjective small, can be also
defined within the same framework. Once defined and
trained, the models are used to annotate the ambigu-
ous words in the test corpus with their corresponding
meaning. Section 4 below provides details on the vari-
ous models that are currently implemented in SENSE-
LEARNER, and information on how new models can
be added to the SENSELEARNER framework.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared at
least once in the training corpus. The words that are
not covered by these models (typically about 10-15%
of the words in the test corpus) are assigned with the
most frequent sense in WordNet.
An alternative solution to this second step was sug-
gested in (Mihalcea and Faruque, 2004), using seman-
tic generalizations learned from dependencies identi-
fied between nodes in a conceptual network. Their
approach however, although slightly more accurate,
conflicted with our goal of creating an efficient WSD
system, and therefore we opted for the simpler back-
off method that employs WordNet sense frequencies.
1We only identify persons, locations, and groups, which are
the named entities specifically identified in SemCor.
54
4 Semantic Models
Different semantic models can be defined and trained
for the disambiguation of different word categories.
Although more general than models that are built in-
dividually for each word in a test corpus (Decadt et
al., 2004), the applicability of the semantic models
built as part of SENSELEARNER is still limited to
those words previously seen in the training corpus,
and therefore their overall coverage is not 100%.
Starting with an annotated corpus consisting of all
annotated files in SemCor, a separate training data set
is built for each model. There are seven models pro-
vided with the current SENSELEARNER distribution,
implementing the following features:
4.1 Noun Models
modelNN1: A contextual model that relies on the first
noun, verb, or adjective before the target noun, and
their corresponding part-of-speech tags.
modelNNColl: A collocation model that implements
collocation-like features based on the first word to the
left and the first word to the right of the target noun.
4.2 Verb Models
modelVB1 A contextual model that relies on the first
word before and the first word after the target verb,
and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to the
left and the first word to the right of the target verb.
4.3 Adjective Models
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the left
and the first word to the right of the target adjective.
4.4 Defining New Models
New models can be easily defined and trained fol-
lowing the same SENSELEARNER learning method-
ology. In fact, the current distribution of SENSE-
LEARNER includes a template for the subroutine re-
quired to define a new semantic model, which can be
easily adapted to handle new word categories.
4.5 Applying Semantic Models
In the training stage, a feature vector is constructed
for each sense-annotated word covered by a semantic
model. The features are model-specific, and feature
vectors are added to the training set pertaining to the
corresponding model. The label of each such feature
vector consists of the target word and the correspond-
ing sense, represented as word#sense. Table 1 shows
the number of feature vectors constructed in this learn-
ing stage for each semantic model.
To annotate new text, similar vectors are created for
all content-words in the raw text. Similar to the train-
ing stage, feature vectors are created and stored sepa-
rately for each semantic model.
Next, word sense predictions are made for all test
examples, with a separate learning process run for
each semantic model. For learning, we are using the
Timbl memory based learning algorithm (Daelemans
et al, 2001), which was previously found useful for
the task of word sense disambiguation (Hoste et al,
2002), (Mihalcea, 2002).
Following the learning stage, each vector in the test
data set is labeled with a predicted word and sense.
If several models are simultaneously used for a given
test instance, then all models have to agree in the la-
bel assigned, for a prediction to be made. If the word
predicted by the learning algorithm coincides with the
target word in the test feature vector, then the pre-
dicted sense is used to annotate the test instance. Oth-
erwise, if the predicted word is different than the tar-
get word, no annotation is produced, and the word is
left for annotation in a later stage.
5 Evaluation
The SENSELEARNER system was evaluated on the
SENSEVAL-2 and SENSEVAL-3 English all words
data sets, each data set consisting of three texts from
the Penn Treebank corpus annotated with WordNet
senses. The SENSEVAL-2 corpus includes a total of
2,473 annotated content words, and the SENSEVAL-
3 corpus includes annotations for an additional set
of 2,081 words. Table 1 shows precision and recall
figures obtained with each semantic model on these
two data sets. A baseline, computed using the most
frequent sense in WordNet, is also indicated. The
best results reported on these data sets are 69.0% on
SENSEVAL-2 data (Mihalcea and Moldovan, 2002),
55
Training SENSEVAL-2 SENSEVAL-3
Model size Precision Recall Precision Recall
modelNN1 88058 0.6910 0.3257 0.6624 0.3027
modelNNColl 88058 0.7130 0.3360 0.6813 0.3113
modelVB1 48328 0.4629 0.1037 0.5352 0.1931
modelVBColl 48328 0.4685 0.1049 0.5472 0.1975
modelJJ1 35664 0.6525 0.1215 0.6648 0.1162
modelJJ2 35664 0.6503 0.1211 0.6593 0.1153
modelJJColl 35664 0.6792 0.1265 0.6703 0.1172
model*1/2 207714 0.6481 0.6481 0.6184 0.6184
model*Coll 172050 0.6622 0.6622 0. 6328 0.6328
Baseline 63.8% 63.8% 60.9% 60.9%
Table 1: Precision and recall for the SENSELEARNER
semantic models, measured on the SENSEVAL-2 and
SENSEVAL-3 English all words data. Results for com-
binations of contextual (model*1/2) and collocational
(model*Coll) models are also included.
and 65.2% on SENSEVAL-3 data (Decadt et al, 2004).
Note however that both these systems rely on signifi-
cantly larger training data sets, and thus the results are
not directly comparable.
In addition, we also ran an experiment where a sep-
arate model was created for each individual word in
the test data, with a back-off method using the most
frequent sense in WordNet when no training exam-
ples were found in SEMCOR. This resulted into sig-
nificantly higher complexity, with a very large num-
ber of models (about 900?1000 models for each of
the SENSEVAL-2 and SENSEVAL-3 data sets), while
the performance did not exceed the one obtained with
the more general semantic models.
The average disambiguation precision obtained
with SENSELEARNER improves significantly over the
simple but competitive baseline that selects by de-
fault the ?most frequent sense? from WordNet. Not
surprisingly, the verbs seem to be the most difficult
word class, which is most likely explained by the large
number of senses defined in WordNet for this part of
speech.
6 Conclusion
In this paper, we described and evaluated an efficient
algorithm for minimally supervised word-sense dis-
ambiguation that attempts to disambiguate all content
words in a text using WordNet senses. The results ob-
tained on both SENSEVAL-2 and SENSEVAL-3 data
sets are found to significantly improve over the sim-
ple but competitive baseline that chooses by default
the most frequent sense, and are proved competitive
with the best published results on the same data sets.
SENSELEARNER is publicly available for download
at http://lit.csci.unt.edu/?senselearner.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation grant IIS-0336793.
References
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization of
memory-based wsd. In Senseval-3: Third International
Workshop on the Evaluation of Systems for the Semantic
Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-based
word-expert approach to unrestricted word sense dis-
ambiguation. In Proceedings of the ACL Workshop on
?Word Sense Disambiguatuion: Recent Successes and
Future Directions?, Philadelphia, July.
R. Mihalcea and E. Faruque. 2004. SenseLearner: Min-
imally supervised word sense disambiguation for all
words in open text. In Proceedings of ACL/SIGLEX
Senseval-3, Barcelona, Spain, July.
R. Mihalcea and D. Moldovan. 2002. Pattern learning
and active feature selection for word sense disambigua-
tion. In Senseval 2001, ACL Workshop, pages 127?130,
Toulouse, France, July.
R. Mihalcea. 2002. Instance based learning with automatic
feature selection applied to Word Sense Disambiguation.
In Proceedings of the 19th International Conference on
Computational Linguistics (COLING 2002), Taipei, Tai-
wan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
pages 303?308, Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Communi-
cation of the ACM, 38(11):39?41.
D. Yuret. 2004. Some experiments with a naive bayes wsd
system. In Senseval-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain, July.
56
Proceedings of ACL-08: HLT, pages 932?940,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Linguistically Motivated Features for Enhanced Back-of-the-Book Indexing
Andras Csomai and Rada Mihalcea
Department of Computer Science
University of North Texas
csomaia@unt.edu,rada@cs.unt.edu
Abstract
In this paper we present a supervised method
for back-of-the-book index construction. We
introduce a novel set of features that goes be-
yond the typical frequency-based analysis, in-
cluding features based on discourse compre-
hension, syntactic patterns, and information
drawn from an online encyclopedia. In exper-
iments carried out on a book collection, the
method was found to lead to an improvement
of roughly 140% as compared to an existing
state-of-the-art supervised method.
1 Introduction
Books represent one of the oldest forms of writ-
ten communication and have been used since thou-
sands of years ago as a means to store and trans-
mit information. Despite this fact, given that a
large fraction of the electronic documents avail-
able online and elsewhere consist of short texts
such as Web pages, news articles, scientific reports,
and others, the focus of natural language process-
ing techniques to date has been on the automa-
tion of methods targeting short documents. We
are witnessing however a change: more and more
books are becoming available in electronic for-
mat, in projects such as the Million Books project
(http://www.archive.org/details/millionbooks), the
Gutenberg project (http://www.gutenberg.org), or
Google Book Search (http://books.google.com).
Similarly, a large number of the books published
in recent years are often available ? for purchase
or through libraries ? in electronic format. This
means that the need for language processing tech-
niques able to handle very large documents such as
books is becoming increasingly important.
This paper addresses the problem of automatic
back-of-the-book index construction. A back-of-
the-book index typically consists of the most impor-
tant keywords addressed in a book, with pointers to
the relevant pages inside the book. The construc-
tion of such indexes is one of the few tasks related
to publishing that still requires extensive human la-
bor. Although there is a certain degree of computer
assistance, consisting of tools that help the profes-
sional indexer to organize and edit the index, there
are no methods that would allow for a complete or
nearly-complete automation.
In addition to helping professional indexers in
their task, an automatically generated back-of-the-
book index can also be useful for the automatic stor-
age and retrieval of a document; as a quick reference
to the content of a book for potential readers, re-
searchers, or students (Schutze, 1998); or as a start-
ing point for generating ontologies tailored to the
content of the book (Feng et al, 2006).
In this paper, we introduce a supervised method
for back-of-the-book index construction, using a
novel set of linguistically motivated features. The
algorithm learns to automatically identify important
keywords in a book based on an ensemble of syntac-
tic, discourse-based and information-theoretic prop-
erties of the candidate concepts. In experiments per-
formed on a collection of books and their indexes,
the method was found to exceed by a large margin
the performance of a previously proposed state-of-
the-art supervised system for keyword extraction.
2 Supervised Back-of-the-Book Indexing
We formulate the problem of back-of-the-book in-
dexing as a supervised keyword extraction task, by
making a binary yes/no classification decision at the
932
level of each candidate index entry. Starting with a
set of candidate entries, the algorithm automatically
decides which entries should be added to the back-
of-the-book index, based on a set of linguistic and
information theoretic features. We begin by iden-
tifying the set of candidate index entries, followed
by the construction of a feature vector for each such
candidate entry. In the training data set, these fea-
ture vectors are also assigned with a correct label,
based on the presence/absence of the entry in the
gold standard back-of-the-book index provided with
the data. Finally, a machine learning algorithm is
applied, which automatically classifies the candidate
entries in the test data for their likelihood to belong
to the back-of-the-book index.
The application of a supervised algorithm re-
quires three components: a data set, which is de-
scribed next; a set of features, which are described in
Section 3; and a machine learning algorithm, which
is presented in Section 4.
2.1 Data
We use a collection of books and monographs from
the eScholarship Editions collection of the Univer-
sity of California Press (UC Press),1 consisting of
289 books, each with a manually constructed back-
of-the-book index. The average length of the books
in this collection is 86053 words, and the average
length of the indexes is 820 entries. A collection
of 56 books was previously introduced in (Csomai
and Mihalcea, 2006); however, that collection is too
small to be split in training and test data to support
supervised keyword extraction experiments.
The UC Press collection was provided in a stan-
dardized XML format, following the Text Encoding
Initiative (TEI) recommendations, and thus it was
relatively easy to process the collection and separate
the index from the body of the text.
In order to use this corpus as a gold standard
collection for automatic index construction, we had
to eliminate the inversions, which are typical in
human-built indexes. Inversion is a method used by
professional indexers by which they break the order-
ing of the words in each index entry, and list the head
first, thereby making it easier to find entries in an
alphabetically ordered index. As an example, con-
sider the entry indexing of illustrations, which, fol-
lowing inversion, becomes illustrations, indexing of.
To eliminate inversion, we use an approach that gen-
1http://content.cdlib.org/escholarship/
erates each permutation of the composing words for
each index entry, looks up the frequency of that per-
mutation in the book, and then chooses the one with
the highest frequency as the correct reconstruction
of the entry. In this way, we identify the form of the
index entries as appearing in the book, which is the
form required for the evaluation of extraction meth-
ods. Entries that cannot be found in the book, which
were most likely generated by the human indexers,
are preserved in their original ordering.
For training and evaluation purposes, we used a
random split of the collection into 90% training and
10% test. This yields a training corpus of 259 docu-
ments and a test data set of 30 documents.
2.2 Candidate Index Entries
Every sequence of words in a book represents a po-
tential candidate for an entry in the back-of-the-book
index. Thus, we extract from the training and the test
data sets all the n-grams (up to the length of four),
not crossing sentence boundaries. These represent
the candidate index entries that will be used in the
classification algorithm. The training candidate en-
tries are then labeled as positive or negative, depend-
ing on whether the given n-gram was found in the
back-of-the-book index associated with the book.
Using a n-gram-based method to extract candidate
entries has the advantage of providing high cover-
age, but the unwanted effect of producing an ex-
tremely large number of entries. In fact, the result-
ing set is unmanageably large for any machine learn-
ing algorithm. Moreover, the set is extremely unbal-
anced, with a ratio of positive and negative exam-
ples of 1:675, which makes it unsuitable for most
machine learning algorithms. In order to address
this problem, we had to find ways to reduce the size
of the data set, possibly eliminating the training in-
stances that will have the least negative effect on the
usability of the data set.
The first step to reduce the size of the data set was
to use the candidate filtering techniques for unsuper-
vised back-of-the-book index construction that we
proposed in (Csomai and Mihalcea, 2007). Namely,
we use the commonword and comma filters, which
are applied to both the training and the test collec-
tions. These filters work by eliminating all the n-
grams that begin or end with a common word (we
use a list of 300 most frequent English words), as
well as those n-grams that cross a comma. This re-
sults in a significant reduction in the number of neg-
933
positive negative total positive:negative ratio
Training data
All (original) 71,853 48,499,870 48,571,723 1:674.98
Commonword/comma filters 66,349 11,496,661 11,563,010 1:173.27
10% undersampling 66,349 1,148,532 1,214,881 1:17.31
Test data
All (original) 7,764 6,157,034 6,164,798 1:793.02
Commonword/comma filters 7,225 1,472,820 1,480,045 1:203.85
Table 1: Number of training and test instances generated from the UC Press data set
ative examples, from 48 to 11 million instances, with
a loss in terms of positive examples of only 7.6%.
The second step is to use a technique for balanc-
ing the distribution of the positive and the negative
examples in the data sets. There are several meth-
ods proposed in the existing literature, focusing on
two main solutions: undersampling and oversam-
pling (Weiss and Provost, 2001). Undersampling
(Kubat and Matwin, 1997) means the elimination of
instances from the majority class (in our case nega-
tive examples), while oversampling focuses on in-
creasing the number of instances of the minority
class. Aside from the fact that oversampling has
hard to predict effects on classifier performance, it
also has the additional drawback of increasing the
size of the data set, which in our case is undesirable.
We thus adopted an undersampling solution, where
we randomly select 10% of the negative examples.
Evidently, the undersampling is applied only to the
training set.
Table 1 shows the number of positive and neg-
ative entries in the data set, for the different pre-
processing and balancing phases.
3 Features
An important step in the development of a super-
vised system is the choice of features used in the
learning process. Ideally, any property of a word or
a phrase indicating that it could be a good keyword
should be represented as a feature and included in
the training and test examples. We use a number
of features, including information-theoretic features
previously used in unsupervised keyword extraction,
as well as a novel set of features based on syntactic
and discourse properties of the text, or on informa-
tion extracted from external knowledge repositories.
3.1 Phraseness and Informativeness
We use the phraseness and informativeness features
that we previously proposed in (Csomai and Mihal-
cea, 2007). Phraseness refers to the degree to which
a sequence of words can be considered a phrase. We
use it as a measure of lexical cohesion of the com-
ponent terms and treat it as a collocation discovery
problem. Informativeness represents the degree to
which the keyphrase is representative for the docu-
ment at hand, and it correlates to the amount of in-
formation conveyed to the user.
To measure the informativeness of a keyphrase,
various methods can be used, some of which were
previously proposed in the keyword extraction liter-
ature:
? tf.idf, which is the traditional information re-
trieval metric (Salton and Buckley, 1997), em-
ployed in most existing keyword extraction ap-
plications. We measure inverse document fre-
quency using the article collection of the online
encyclopedia Wikipedia.
? ?2 independence test, which measures the de-
gree to which two events happen together more
often than by chance. In our work, we use the
?2 in a novel way. We measure the informa-
tiveness of a keyphrase by finding if a phrase
occurs in the document more frequently than
it would by chance. The information required
for the ?2 independence test can be typically
summed up in a contingency table (Manning
and Schutze, 1999):
count(phrase in count(all other phrases
document) in document)
count(phrase in other count(all other phrases
documents) in all other documents)
The independence score is calculated based on
the observed (O) and expected (E) counts:
?2 =
?
i,j
(Oi,j ? Ei,j)2
Ei,j
where i, j are the row and column indices of the
934
contingency table. The O counts are the cells of
the table. The E counts are calculated from the
marginal probabilities (the sum of the values of
a column or a row) converted into proportions
by dividing them with the total number of ob-
served events (N ):
N = O1,1 + O1,2 + O2,1 + O2,2
Then the expected count for seeing the phrase
in the document is:
E1,1 =
O1,1 + O1,2
N ?
O1,1 + O2,1
N ?N
To measure the phraseness of a candidate phrase
we use a technique based on the ?2 independence
test. We measure the independence of the events
of seeing the components of the phrase in the text.
This method was found to be one of the best per-
forming models in collocation discovery (Pecina and
Schlesinger, 2006). For n-grams where N > 2
we apply the ?2 independence test by splitting the
phrase in two (e.g. for a 4-gram, we measure the
independence of the composing bigrams).
3.2 Discourse Comprehension Features
Very few existing keyword extraction methods look
beyond word frequency. Except for (Turney and
Littman, 2003), who uses pointwise mutual infor-
mation to improve the coherence of the keyword set,
we are not aware of any other work that attempts
to use the semantics of the text to extract keywords.
The fact that most systems rely heavily on term fre-
quency properties poses serious difficulties, since
many index entries appear only once in the docu-
ment, and thus cannot be identified by features based
solely on word counts. For instance, as many as 52%
of the index entries in our training data set appeared
only once in the books they belong to. Moreover,
another aspect not typically covered by current key-
word extraction methods is the coherence of the key-
word set, which can also be addressed by discourse-
based properties.
In this section, we propose a novel feature for
keyword extraction inspired by work on discourse
comprehension. We use a construction integration
framework, which is the backbone used by many
discourse comprehension theories.
3.2.1 Discourse Comprehension
Discourse comprehension is a field in cognitive
science focusing on the modeling of mental pro-
cesses associated with reading and understanding
text. The most widely accepted theory for discourse
comprehension is the construction integration the-
ory (Kintsch, 1998). According to this theory,
the elementary units of comprehension are proposi-
tions, which are defined as instances of a predicate-
argument schema. As an example, consider the sen-
tence The hemoglobin carries oxygen, which gener-
ates the predicate CARRY[HEMOGLOBIN,OXIGEN].
The processing cycle of the construction integra-
tion model processes one proposition at a time, and
builds a local representation of the text in the work-
ing memory, called the propositional network.
During the construction phase, propositions are
extracted from a segment of the input text (typ-
ically a single sentence) using linguistic features.
The propositional network is represented as a graph,
with nodes consisting of propositions, and weighted
edges representing the semantic relations between
them. All the propositions generated from the in-
put text are inserted into the graph, as well as all the
propositions stored in the short term memory. The
short term memory contains the propositions that
compose the representation of the previous few sen-
tences. The second phase of the construction step
is the addition of past experiences (or background
knowledge), which is stored in the long term mem-
ory. This is accomplished by adding new elements
to the graph, usually consisting of the set of closely
related propositions from the long term memory.
After processing a sentence, the integration step
establishes the role of each proposition in the mean-
ing representation of the current sentence, through a
spreading activation applied on the propositions de-
rived from the original sentence. Once the weights
are stabilized, the set of propositions with the high-
est activation values give the mental representation
of the processed sentence. The propositions with
the highest activation values are added to the short
term memory, the working memory is cleared and
the process moves to the next sentence. Figure 3.2.1
shows the memory types used in the construction in-
tegration process and the main stages of the process.
3.2.2 Keyword Extraction using Discourse
Comprehension
The main purpose of the short term memory is to
ensure the coherence of the meaning representation
across sentences. By keeping the most important
propositions in the short term memory, the spreading
activation process transfers additional weight to se-
935
Semantic
Memory
Short-term
Memory
Add
Associates
AddPrevious
Propositions
Decay
Integration
Working
Memory
Next
Proposition
Figure 1: The construction integration process
mantically related propositions in the sentences that
follow. This also represents a way of alleviating one
of the main problems of statistical keyword extrac-
tion, namely the sole dependence on term frequency.
Even if a phrase appears only once, the construc-
tion integration process ensures the presence of the
phrase in the short term memory as long as it is rele-
vant to the current topic, thus being a good indicator
of the phrase importance.
The construction integration model is not directly
applicable to keyword extraction due to a number of
practical difficulties. The first implementation prob-
lem was the lack of a propositional parser. We solve
this problem by using a shallow parser to extract
noun phrase chunks from the original text (Munoz
et al, 1999). Second, since spreading activation is
a process difficult to control, with several parame-
ters that require fine tuning, we use instead a dif-
ferent graph centrality measure, namely PageRank
(Brin and Page, 1998).
Finally, to represent the relations inside the long
term semantic memory, we use a variant of latent
semantic analysis (LSA) (Landauer et al, 1998) as
implemented in the InfoMap package,2 trained on a
corpus consisting of the British National Corpus, the
English Wikipedia, and the books in our collection.
To alleviate the data sparsity problem, we also use
the pointwise mutual information (PMI) to calculate
the relatedness of the phrases based on the book be-
ing processed.
The final system works by iterating the following
steps: (1) Read the text sentence by sentence. For
each new sentence, a graph is constructed, consist-
ing of the noun phrase chunks extracted from the
original text. This set of nodes is augmented with
all the phrases from the short term memory. (2) A
2http://infomap.stanford.edu/
weighted edge is added between all the nodes, based
on the semantic relatedness measured between the
phrases by using LSA and PMI. We use a weighted
combination of these two measures, with a weight of
0.9 assigned to LSA and 0.1 to PMI. For the nodes
from the short term memory, we adjust the connec-
tion weights to account for memory decay, which is
a function of the distance from the last occurrence.
We implement decay by decreasing the weight of
both the outgoing and the incoming edges by n ? ?,
where n is the number of sentences since we last saw
the phrase and ? = 0.1. (3) Apply PageRank on
the resulting graph. (4) Select the 10 highest ranked
phrases and place them in the short term memory.
(5) Read the next sentence and go back to step (1).
Three different features are derived based on the
construction integration model:
? CI short term memory frequency (CI short-
term), which measures the number of iterations
that the phrase remains in the short term mem-
ory, which is seen as an indication of the phrase
importance.
? CI normalized short term memory fre-
quency (CI normalized), which is the same as
CI shortterm, except that it is normalized by the
frequency of the phrase. Through this normal-
ization, we hope to enhance the effect of the se-
mantic relatedness of the phrase to subsequent
sentences.
? CI maximum score (CI maxscore), which
measures the maximum centrality score the
phrase achieves across the entire book. This
can be thought of as a measure of the impor-
tance of the phrase in a smaller coherent seg-
ment of the document.
3.3 Syntactic Features
Previous work has pointed out the importance of
syntactic features for supervised keyword extraction
(Hulth, 2003). The construction integration model
described before is already making use of syntactic
patterns to some extent, through the use of a shal-
low parser to identify noun phrases. However, that
approach does not cover patterns other than noun
phrases. To address this limitation, we introduce a
new feature that captures the part-of-speech of the
words composing a candidate phrase.
936
There are multiple ways to represent such a fea-
ture. The simplest is to create a string feature con-
sisting of the concatenation of the part-of-speech
tags. However, this representation imposes limita-
tions on the machine learning algorithms that can
be used, since many learning systems cannot handle
string features. The second solution is to introduce
a binary feature for each part-of-speech tag pattern
found in the training and the test data sets. In our
case this is again unacceptable, given the size of the
documents we work with and the large number of
syntactic patterns that can be extracted. Instead, we
decided on a novel solution which, rather than us-
ing the part-of-speech pattern directly, determines
the probability of a phrase with a certain tag pattern
to be selected as a keyphrase. Formally:
P (pattern) = C(pattern, positive)C(pattern)
where C(pattern, positive) is the number of dis-
tinct phrases having the tag pattern pattern and be-
ing selected as keyword, and C(pattern) represents
the number of distinct phrases having the tag pattern
pattern. This probability is estimated based on the
training collection, and is used as a numeric feature.
We refer to this feature as part-of-speech pattern.
3.4 Encyclopedic Features
Recent work has suggested the use of domain
knowledge to improve the accuracy of keyword ex-
traction. This is typically done by consulting a vo-
cabulary of plausible keyphrases, usually in the form
of a list of subject headings or a domain specific
thesaurus. The use of a vocabulary has the addi-
tional benefit of eliminating the extraction of incom-
plete phrases (e.g. ?States of America?). In fact,
(Medelyan and Witten, 2006) reported an 110% F-
measure improvement in keyword extraction when
using a domain-specific thesaurus.
In our case, since the books can cover several do-
mains, the construction and use of domain-specific
thesauruses is not plausible, as the advantage of such
resources is offset by the time it usually takes to
build them. Instead, we decided to use encyclope-
dic information, as a way to ensure high coverage in
terms of domains and concepts.
We use Wikipedia, which is the largest and the
fastest growing encyclopedia available today, and
whose structure has the additional benefit of being
particularly useful for the task of keyword extrac-
tion. Wikipedia includes a rich set of links that con-
nect important phrases in an article to their corre-
sponding articles. These links are added manually
by the Wikipedia contributors, and follow the gen-
eral guidelines of annotation provided by Wikipedia.
The guidelines coincide with the goals of keyword
extraction, and thus the Wikipedia articles and their
link annotations can be treated as a vast keyword an-
notated corpus.
We make use of the Wikipedia annotations in two
ways. First, if a phrase is used as the title of a
Wikipedia article, or as the anchor text in a link,
this is a good indicator that the given phrase is well
formed. Second, we can also estimate the proba-
bility of a term W to be selected as a keyword in
a new document by counting the number of docu-
ments where the term was already selected as a key-
word (count(Dkey)) divided by the total number of
documents where the term appeared (count(DW )).
These counts are collected from the entire set of
Wikipedia articles.
P (keyword|W ) ? count(Dkey)count(DW )
(1)
This probability can be interpreted as ?the more
often a term was selected as a keyword among its
total number of occurrences, the more likely it is that
it will be selected again.? In the following, we will
refer to this feature as Wikipedia keyphraseness.
3.5 Other Features
In addition to the features described before, we add
several other features frequently used in keyword
extraction: the frequency of the phrase inside the
book (term frequency (tf)); the number of documents
that include the phrase (document frequency (df)); a
combination of the two (tf.idf); the within-document
frequency, which divides a book into ten equally-
sized segments, and counts the number of segments
that include the phrase (within document frequency);
the length of the phrase (length of phrase); and fi-
nally a binary feature indicating whether the given
phrase is a named entity, according to a simple
heuristic based on word capitalization.
4 Experiments and Evaluation
We integrate the features described in the previous
section in a machine learning framework. The sys-
tem is evaluated on the data set described in Sec-
tion 2.1, consisting of 289 books, randomly split into
937
90% training (259 books) and 10% test (30 books).
We experiment with three learning algorithms, se-
lected for the diversity of their learning strategy:
multilayer perceptron, SVM, and decision trees. For
all three algorithms, we use their implementation as
available in the Weka package.
For evaluation, we use the standard information
retrieval metrics: precision, recall, and F-measure.
We use two different mechanisms for selecting the
number of entries in the index. In the first evaluation
(ratio-based), we use a fixed ratio of 0.45% from the
number of words in the text; for instance, if a book
has 100,000 words, the index will consist of 450 en-
tries. This number was estimated based on previous
observations regarding the typical size of a back-of-
the-book index (Csomai and Mihalcea, 2006). In
order to match the required number of entries, we
sort all the candidates in reversed order of the confi-
dence score assigned by the machine learning algo-
rithm, and consequently select the top entries in this
ranking. In the second evaluation (decision-based),
we allow the machine learning algorithm to decide
on the number of keywords to extract. Thus, in this
evaluation, all the candidates labeled as keywords
by the learning algorithm will be added to the index.
Note that all the evaluations are run using a train-
ing data set with 10% undersampling of the negative
examples, as described before.
Table 2 shows the results of the evaluation. As
seen in the table, the multilayer perceptron and the
decision tree provide the best results, for an over-
all average F-measure of 27%. Interestingly, the re-
sults obtained when the number of keywords is auto-
matically selected by the learning method (decision-
based) are comparable to those when the number of
keywords is selected a-priori (ratio-based), indicat-
ing the ability of the machine learning algorithm to
correctly identify the correct keywords.
Additionally, we also ran an experiment to de-
termine the amount of training data required by the
system. While the learning curve continues to grow
with additional amounts of data, the steepest part of
the curve is observed for up to 10% of the training
data, which indicates that a relatively small amount
of data (about 25 books) is enough to sustain the sys-
tem.
It is worth noting that the task of creating back-
of-the-book indexes is highly subjective. In order
to put the performance figures in perspective, one
should also look at the inter-annotator agreement be-
tween human indexers as an upper bound of per-
formance. Although we are not aware of any com-
prehensive studies for inter-annotator agreement on
back-of-the-book indexing, we can look at the con-
sistency studies that have been carried out on the
MEDLINE corpus (Funk and Reid, 1983), where an
inter-annotator agreement of 48% was found on an
indexing task using a domain-specific controlled vo-
cabulary of subject headings.
4.1 Comparison with Other Systems
We compare the performance of our system with two
other methods for keyword extraction. One is the
tf.idf method, traditionally used in information re-
trieval as a mechanism to assign words in a text with
a weight reflecting their importance. This tf.idf base-
line system uses the same candidate extraction and
filtering techniques as our supervised systems. The
other baseline is the KEA keyword extraction system
(Frank et al, 1999), a state-of-the-art algorithm for
supervised keyword extraction. Very briefly, KEA is
a supervised system that uses a Na??ve Bayes learn-
ing algorithm and several features, including infor-
mation theoretic features such as tf.idf and positional
features reflecting the position of the words with re-
spect to the beginning of the text. The KEA system
was trained on the same training data set as used in
our experiments.
Table 3 shows the performance obtained by these
methods on the test data set. Since none of these
methods have the ability to automatically determine
the number of keywords to be extracted, the evalua-
tion of these methods is done under the ratio-based
setting, and thus for each method the top 0.45%
ranked keywords are extracted.
Algorithm P R F
tf.idf 8.09 8.63 8.35
KEA 11.18 11.48 11.32
Table 3: Baseline systems
4.2 Performance of Individual Features
We also carried out experiments to determine the
role played by each feature, by using the informa-
tion gain weight as assigned by the learning algo-
rithm. Note that ablation studies are not appropri-
ate in our case, since the features are not orthogonal
(e.g., there is high redundancy between the construc-
tion integration and the informativeness features),
and thus we cannot entirely eliminate a feature from
the system.
938
ratio-based decision-based
Algorithm P R F P R F
Multilayer perceptron 27.98 27.77 27.87 23.93 31.98 27.38
Decision tree 27.06 27.13 27.09 22.75 34.12 27.30
SVM 20.94 20.35 20.64 21.76 30.27 25.32
Table 2: Evaluation results
Feature Weight
part-of-speech pattern 0.1935
CI shortterm 0.1744
Wikipedia keyphraseness 0.1731
CI maxscore 0.1689
CI shortterm normalized 0.1379
ChiInformativeness 0.1122
document frequency (df) 0.1031
tf.idf 0.0870
ChiPhraseness 0.0660
length of phrase 0.0416
named entity heuristic 0.0279
within document frequency 0.0227
term frequency (tf) 0.0209
Table 4: Information gain feature weight
Table 4 shows the weight associated with each
feature. Perhaps not surprisingly, the features
with the highest weight are the linguistically moti-
vated features, including syntactic patterns and the
construction integration features. The Wikipedia
keyphraseness also has a high score. The smallest
weights belong to the information theoretic features,
including term and document frequency.
5 Related Work
With a few exceptions (Schutze, 1998; Csomai and
Mihalcea, 2007), very little work has been carried
out to date on methods for automatic back-of-the-
book index construction.
The task that is closest to ours is perhaps keyword
extraction, which targets the identification of the
most important words or phrases inside a document.
The state-of-the-art in keyword extraction is cur-
rently represented by supervised learning methods,
where a system is trained to recognize keywords in a
text, based on lexical and syntactic features. This ap-
proach was first suggested in (Turney, 1999), where
parameterized heuristic rules are combined with a
genetic algorithm into a system for keyphrase ex-
traction (GenEx) that automatically identifies key-
words in a document. A different learning algo-
rithm was used in (Frank et al, 1999), where a Naive
Bayes learning scheme is applied on the document
collection, with improved results observed on the
same data set as used in (Turney, 1999). Neither Tur-
ney nor Frank report on the recall of their systems,
but only on precision: a 29.0% precision is achieved
with GenEx (Turney, 1999) for five keyphrases ex-
tracted per document, and 18.3% precision achieved
with Kea (Frank et al, 1999) for fifteen keyphrases
per document. Finally, in recent work, (Hulth, 2003)
proposes a system for keyword extraction from ab-
stracts that uses supervised learning with lexical and
syntactic features, which proved to improve signifi-
cantly over previously published results.
6 Conclusions and Future Work
In this paper, we introduced a supervised method for
back-of-the-book indexing which relies on a novel
set of features, including features based on discourse
comprehension, syntactic patterns, and information
drawn from an online encyclopedia. According to
an information gain measure of feature importance,
the new features performed significantly better than
the traditional frequency-based techniques, leading
to a system with an F-measure of 27%. This rep-
resents an improvement of 140% with respect to a
state-of-the-art supervised method for keyword ex-
traction. Our system proved to be successful both
in ranking the phrases in terms of their suitability as
index entries, as well as in determining the optimal
number of entries to be included in the index. Fu-
ture work will focus on developing methodologies
for computer-assisted back-of-the-book indexing, as
well as on the use of the automatically extracted in-
dexes in improving the browsing of digital libraries.
Acknowledgments
We are grateful to Kirk Hastings from the Califor-
nia Digital Library for his help in obtaining the UC
Press corpus. This research has been partially sup-
ported by a grant from Google Inc. and a grant from
the Texas Advanced Research Program (#003594).
939
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
A. Csomai and R. Mihalcea. 2006. Creating a testbed
for the evaluation of automatically generated back-of-
the-book indexes. In Proceedings of the International
Conference on Computational Linguistics and Intelli-
gent Text Processing, pages 19?25, Mexico City.
A. Csomai and R. Mihalcea. 2007. Investigations in
unsupervised back-of-the-book indexing. In Proceed-
ings of the Florida Artificial Intelligence Research So-
ciety, Key West.
D. Feng, J. Kim, E. Shaw, and E. Hovy. 2006. Towards
modeling threaded discussions through ontology-
based analysis. In Proceedings of National Confer-
ence on Artificial Intelligence.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of the 16th In-
ternational Joint Conference on Artificial Intelligence.
M. E. Funk and C.A. Reid. 1983. Indexing consistency
in medline. Bulletin of the Medical Library Associa-
tion, 71(2).
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing, Japan, August.
W. Kintsch. 1998. Comprehension: A paradigm for cog-
nition. Cambridge Uniersity Press.
M. Kubat and S. Matwin. 1997. Addressing the curse
of imbalanced training sets: one-sided selection. In
Proceedings of the 14th International Conference on
Machine Learning.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
C. Manning and H. Schutze. 1999. Foundations of Natu-
ral Language Processing. MIT Press.
O. Medelyan and I. H. Witten. 2006. Thesaurus based
automatic keyphrase indexing. In Proceedings of the
Joint Conference on Digital Libraries.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing.
P. Pecina and P. Schlesinger. 2006. Combining asso-
ciation measures for collocation extraction. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 651?658, Sydney, Australia.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
H. Schutze. 1998. The hypertext concordance: a better
back-of-the-book index. In Proceedings of Comput-
erm, pages 101?104.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from as-
sociation. ACM Transactions on Information Systems,
4(21):315?346.
P. Turney. 1999. Learning to extract keyphrases from
text. Technical report, National Research Council, In-
stitute for Information Technology.
G. Weiss and F. Provost. 2001. The effect of class distri-
bution on classifier learning. Technical Report ML-TR
43, Rutgers University.
940
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406?409,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT-Yahoo: SuperSenseLearner: Combining SenseLearner with
SuperSense and other Coarse Semantic Features
Rada Mihalcea and Andras Csomai
University of North Texas
rada@cs.unt.edu,csomaia@unt.edu
Massimiliano Ciaramita
Yahoo! Research Barcelona
massi@yahoo-inc.com
Abstract
We describe the SUPERSENSELEARNER
system that participated in the English all-
words disambiguation task. The system re-
lies on automatically-learned semantic mod-
els using collocational features coupled with
features extracted from the annotations of
coarse-grained semantic categories gener-
ated by an HMM tagger.
1 Introduction
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a poly-
semous word within a given context. Applications
such as machine translation, knowledge acquisition,
common sense reasoning, and others, require knowl-
edge about word meanings, and word sense disam-
biguation is considered essential for all these tasks.
Most of the efforts in solving this problem
were concentrated so far toward targeted supervised
learning, where each sense tagged occurrence of a
particular word is transformed into a feature vector,
which is then used in an automatic learning process.
The applicability of such supervised algorithms is
however limited only to those few words for which
sense tagged data is available, and their accuracy
is strongly connected to the amount of labeled data
available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less atten-
tion. While the performance of such methods is usu-
ally exceeded by their supervised lexical-sample al-
ternatives, they have however the advantage of pro-
viding larger coverage.
In this paper, we describe SUPERSENSE-
LEARNER ? a system for solving the semantic am-
biguity of all words in unrestricted text. SUPER-
SENSELEARNER brings together under one system
the features previously used in the SENSELEARNER
(Mihalcea and Csomai, 2005) and the SUPERSENSE
(Ciaramita and Altun, 2006) all-words word sense
disambiguation systems. The system is using a rel-
atively small pre-existing sense-annotated data set
for training purposes, and it learns global semantic
models for general word categories.
2 Learning for All-Words Word Sense
Disambiguation
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm gen-
eral enough to be able to disambiguate as many
content words as possible in a text, and efficient
enough so that large amounts of text can be anno-
tated in real time. SUPERSENSELEARNER is at-
tempting to learn general semantic models for var-
ious word categories, starting with a relatively small
sense-annotated corpus. We base our experiments
on SemCor (Miller et al, 1993), a balanced, se-
mantically annotated dataset, with all content words
manually tagged by trained lexicographers.
The input to the disambiguation algorithm con-
sists of raw text. The output is a text with word
meaning annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-
406
of-speech tags; collocations are identified using a
sliding window approach, where a collocation is de-
fined as a sequence of words that forms a compound
concept defined in WordNet (Miller, 1995).
Next, a semantic model is learned for all pre-
defined word categories, where a word category is
defined as a group of words that share some com-
mon syntactic or semantic properties. Word cate-
gories can be of various granularities. For instance,
a model can be defined and trained to handle all the
nouns in the test corpus. Similarly, using the same
mechanism, a finer-grained model can be defined to
handle all the verbs for which at least one of the
meanings is of type e.g., ?<move>?. Finally, small
coverage models that address one word at a time, for
example a model for the adjective ?small,? can be
also defined within the same framework. Once de-
fined and trained, the models are used to annotate the
ambiguous words in the test corpus with their corre-
sponding meaning. Sections 3 and 4 below provide
details on the features implemented by the various
models.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared
at least once in the training corpus. The words that
are not covered by these models (typically about 10-
15% of the words in the test corpus) are assigned the
most frequent sense in WordNet.
3 SenseLearner Semantic Models
Different semantic models can be defined and
trained for the disambiguation of different word cat-
egories. Although more general than models that
are built individually for each word in a test corpus
(Decadt et al, 2004), the applicability of the seman-
tic models built as part of SENSELEARNER is still
limited to those words previously seen in the train-
ing corpus, and therefore their overall coverage is
not 100%.
Starting with an annotated corpus consisting of
all the annotated files in SemCor, augmented with
the SENSEVAL-2 and SENSEVAL-3 all-words data
sets, a separate training data set is built for each
model. There are seven models provided with the
current SENSELEARNER distribution, implementing
the following features:
3.1 Noun Models
modelNN1: A contextual model that relies on the
first noun, verb, or adjective before the target noun,
and their corresponding part-of-speech tags.
modelNNColl: A collocation model that imple-
ments collocation-like features based on the first
word to the left and the first word to the right of the
target noun.
3.2 Verb Models
modelVB1 A contextual model that relies on the
first word before and the first word after the target
verb, and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to
the left and the first word to the right of the target
verb.
3.3 Adjective Models
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the
left and the first word to the right of the target adjec-
tive.
Based on previous performance in the
SENSEVAL-2 and SENSEVAL-3 evaluations,
we selected the noun and verb collocational models
for inclusion in the SUPERSENSELEARNER system
participating in the SEMEVAL all-words task.
4 SuperSenses and other Coarse-Grained
Semantic Features
A great deal of work has focused in recent years
on shallow semantic annotation tasks such as named
entity recognition and semantic role labeling. In the
former task, systems analyze text to detect mentions
of instances of coarse-grained semantic categories
such as ?person?, ?organization? and ?location?. It
seems natural to ask if this type of shallow seman-
tic information can be leveraged to improve lexical
disambiguation. Particularly, since the best perform-
ing taggers typically implement sequential decoding
schemes, e.g., Viterbi decoding, which have linear
407
complexity and can be performed quite efficiently.
In practice thus, this type of pre-processing resem-
bles POS-tagging and could provide the WSD sys-
tem with useful additional evidence.
4.1 Tagsets
We use three different tagsets. The first is the set of
WordNet supersenses (Ciaramita and Altun, 2006):
a mapping of WordNet?s synsets to 45 broad lexi-
cographers categories, 26 for nouns, 15 for verbs,
3 for adjectives and 1 for adverbs. The second
tagset is based on the ACE 2007 English data for
entity mention detection (EMD) (ACE, 2007). This
tagset defines seven entity types: Facility, Geo-
Political Entity, Location, Organization, Person, Ve-
hicle, Weapon; further subdivided in 44 subtypes.
The third tagset is derived from the BBN Entity
Corpus (BBN, 2005) which complements the Wall
Street Journal Penn Treebank with annotations of a
large set of entities: 12 named entity types (Person,
Facility, Organization, GPE, Location, Nationality,
Product, Event, Work of Art, Law, Language, and
Contact-Info), nine nominal entity types (Person,
Facility, Organization, GPE, Product, Plant, Animal,
Substance, Disease and Game), and seven numeric
types (Date, Time, Percent, Money, Quantity, Ordi-
nal and Cardinal). Several of these types are further
divided into subtypes, for a total of 105 classes.1
4.2 Taggers
We annotate the training and evaluation data using
three sequential taggers, one for each tagset. The
tagger is a Hidden Markov Model trained with the
perceptron algorithm introduced in (Collins, 2002),
which applies Viterbi decoding and is regularized
using averaging. Label to label dependencies are
limited to the previous tag (first order HMM). We
use a generic feature set for NER based on words,
lemmas, POS tags, and word shape features, in addi-
tion we use as a feature of each token the supersense
of a first (super)sense baseline. A detailed descrip-
tion of the features used and the tagger can be found
in (Ciaramita and Altun, 2006). The supersense tag-
ger is trained on the Brown sections one and two of
SemCor. The BBN tagger is trained on sections 2-
21 of the BBN corpus. The ACE tagger is trained
1BBN Corpus documentation.
on the 599 ACE 2007 training files. The accuracy
of the tagger is, approximately, 78% F-score for su-
persenses and ACE, and 87% F-score for the BBN
corpus.
4.3 Features
The taggers disregard the lemmatization of the eval-
uation data. In practice, this means that multiword
lemmas such as ?take off?, are split into their ba-
sic components. In fact, the goal of the tagger is
to guess the elements of the instances of semantic
categories by means of the usual BIO encoding. In
other words, the tagger predicts a labeled bracket-
ing of the tokens in each sentence. As an exam-
ple, the supersense tagger annotates the tokens in the
phrase ?substance abuse? as ?substanceB?noun.act?
and ?abuseI?noun.act?, although the gold standard
segmentation of the data does not identify the phrase
as one lemma. We use the labels generated in this
way as features of each token to disambiguate.
5 Feature Combination
For the final system we create a combined feature set
for each target word, consisting of the lemma, the
part of speech, the collocational SENSELEARNER
features, and the three coarse grained semantic tags
of the target word. Note that the semantic fea-
tures are represented as lemma TAG to avoid over-
generalization.
In the training stage, a feature vector is con-
structed for each sense-annotated word covered by
a semantic model. The features are model-specific,
and feature vectors are added to the training set
pertaining to the corresponding model. The label
of each such feature vector consists of the target
word and the corresponding sense, represented as
word#sense. Table 1 shows the number of feature
vectors constructed in this learning stage for each
semantic model. To annotate new text, similar vec-
tors are created for all the content-words in the raw
text. Similar to the training stage, feature vectors
are created and stored separately for each semantic
model.
Next, word sense predictions are made for all the
test examples, with a separate learning process run
for each semantic model. For learning, we are using
the Timbl memory based learning algorithm (Daele-
408
Training RESULTS
mode size Precision Recall
noun 89052 0.658 0.228
verb 48936 0.539 0.353
all 137988 0.583 0.583
Table 1: Precision and recall for the SUPERSENSE-
LEARNER semantic models.
Training RESULTS
mode size Precision Recall
noun 89052 0.666 0.233
verb 48936 0.554 0.360
all 137988 0.593 0.593
Table 2: Precision and recall for the SUPERSENSE-
LEARNER semantic models - without U labels.
mans et al, 2001), which was previously found use-
ful for the task of word sense disambiguation (Hoste
et al, 2002; Mihalcea, 2002).
Following the learning stage, each vector in the
test data set is labeled with a predicted word and
sense. If the word predicted by the learning algo-
rithm coincides with the target word in the test fea-
ture vector, then the predicted sense is used to an-
notate the test instance. Otherwise, if the predicted
word is different from the target word, no annota-
tion is produced, and the word is left for annotation
in a later stage (e.g., using the most frequent sense
back-off method).
6 Results
The SUPERSENSELEARNER system participated in
the SEMEVAL all-words word sense disambigua-
tion task. Table 1 shows the results obtained for
each part-of-speech (nouns and verbs), as well as
the overall results. We have also ran a separate
evaluation excluding the U (unknown) tag, which
is shown in Table 2. SUPERSENSELEARNER was
ranked the third among the fourteen participating
systems, proving the validity of the approach.
Acknowledgments
We would like to thank Mihai Surdeanu for provid-
ing a pre-processed version of the ACE data.
References
2007. Automatic content extraction workshop.
http://www.nist.gov/speech/tests/ace/ace07/index.htm.
2005. BBN pronoun coreference and entity type cor-
pus. Linguistic Data Consortium (LDC) catalog num-
ber LDC2005T33.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, July. Association for
Computational Linguistics.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization
of memory-based wsd. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-
based word-expert approach to unrestricted word sense
disambiguation. In Proceedings of the ACL Workshop
on ?Word Sense Disambiguatuion: Recent Successes
and Future Directions?, Philadelphia, July.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
R. Mihalcea. 2002. Instance based learning with auto-
matic feature selection applied to Word Sense Disam-
biguation. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, Taiwan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Commu-
nication of the ACM, 38(11):39?41.
409
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?
Department of Computer Science and Engineering
University of North Texas
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
Abstract
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
1 Introduction
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
?on the fly? for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
2 A system for lexical substitution
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
?Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
3 Candidate Extraction
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
410
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
4 Candidate Ranking
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators? agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
P (c|g) = P (c, g)/P (g) ? Count(c, g) (1)
where c represents a possible candidate and g rep-
resents the context. The probability P (g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P (c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P (c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
Pn(c|g) ?
n
?
i=1
Count(ci, g) (2)
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(ci g). The Google N-gram corpus is a
411
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N ? 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al, 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University?s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al, 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
score (ci) =
?
m?rankings
?m
1
rmci
To determine the weight ? of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
5 Results and Discussion
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A ?mode? evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the ?mode?) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
412
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
Table 1: Weights of the individual ranking methods
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
Table 3: OOT results
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the ?mode? evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the ?official? submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
6 Conclusions
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the ?mode? eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
Acknowledgments
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
References
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
413
UBBNBC WSD System Description 
Csomai ANDR?S 
Department of Computer Science 
Babes-Bolyai University 
Cluj-Napoca, Romania 
csomaia@personal.ro 
 
 
Abstract 
The Na?ve Bayes classification proves to 
be a good performing tool in word sense 
disambiguation, although it has not yet 
been applied to the Romanian language. 
The aim of this paper is to present our 
WSD system, based on the NBC algorithm, 
that performed quite well in Senseval 3. 
1 Introduction 
According to the literature, the NBC algorithm is 
very efficient, in many cases it outperforms more 
sophisticated methods (Pedersen 1998). Therefore, 
this is the approach we used in our research. The  
word sense disambiguating process has three major 
steps, therefore, the application has three main 
components as follows: 
Stemming ? removal of suffixes, and the filter-
ing out of the irrelevant information from 
the corpora. A simple dictionary based ap-
proach. 
Learning ? the training of the classifier, based 
on the sense tagged corpora. A database 
containing the number of co-occurrences is 
built. 
Disambiguating ?on the basis of the database, 
the correct sense of a word in a given con-
text is estimated. 
In the followings the previously mentioned three 
steps are described in detail. 
 
 
2 Stemming 
The preprocessing of the corpora is one of the most 
result-influential steps. The preprocessing consists 
of the removal of suffixes and the elimination of 
the irrelevant data. The removal of suffixes is per-
formed trough a simple dictionary based method. 
For every wi word the possible wj candidates are 
selected from the dictionary containing the word 
stems. Then a similarity score is calculated be-
tween the word to be stemmed and the candidates, 
as follows: 
li, lj is the length of word i, respectively j.  
 
scorei=
ji
i
ll
l
+
2
if li ? lj   and 
scorej=0, otherwise. 
 
The result is the candidate with the highest score if 
its score is above a certain threshold, otherwise the 
word is leaved untouched. 
In the preprocessing phase we also erase the pro-
nouns and prepositions from the examined context. 
This exclusion was made upon a list of stop words. 
3 Learning 
The training is conducted according to the NBC 
algorithm. First a database is built, with the follow-
ing tables: 
words ? contains all the words found in the cor-
pora. Its role is to assign a sense id to every 
word. 
wordsenses ? contains all the tagged words in 
the corpora linked with their possible senses. 
One entry for a given sense and word. 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
nosenses - number of tagged contexts, with a 
given sense 
nocontexts  - number of tagged contexts of a 
given word 
occurrences ? number of co-occurrences of a 
given word with a given sense 
 
 
Figure1: The tables of the database 
 
The training of the system is nothing but filling up 
the tables of the database. 
fill NoSenses 
fill NoContexts 
fill Wordsenses 
scan corpora 
   cakt=actual entry in corpora (a context) 
   w=actual word in entry (the ambiguous word) 
   sk=actual sense of entry 
 scan cakt 
     vj=actual word in entry 
     if vj<>w then 
  if vj in words then 
   vi=wordid from words where w=vj 
  else 
   add words vj 
  endif 
     if (exists entry in occurrences where  
         wordid=vi and senseid=sk) then 
  increment C(wordid,senseid) in occurrences,  
                     where wordid=vi and senseid=sk 
     else 
  add occurrences(wordid, senseid, 1)  
          endif 
         step to next word 
        endscan 
step to next entry 
endscan corpora 
As it is obvious, the database is filled up (so the 
system is trained) only upon the training corpus 
provided for the Senseval3 Romanian Lexical 
Sample task. 
4 Disambiguation 
The basic assumption of the Na?ve Bayes method 
is that the contextual features are not dependent on 
each other. In this particular case, we assume that 
the probability of co-occurrence of a word vi with 
the ambiguous word w of sense s is not dependent 
on other co-occurrences.  
The goal is to find the correct sense s? , of  the 
word w, for a given context. This s? sense maxi-
mizes the following equation. 
)()|(maxarg    
)(
)(
)|(maxarg    
)|(maxarg
kks
k
k
s
ks
sPscP
sPcP
scP
csPs
k
k
k
=
=
=?
  
At this point we make the simplifying ?na?ve? as-
sumption: 
 ?
?
=
cv
kjk
j
svPscP )|()|(  
The algorithm (T?tar, 2003) for estimating the cor-
rect sense of word w according to its c context is 
the following: 
for every sk sense of w do 
 score(sk)=P(sk) 
 for every vj from context c do 
  score(sk)= score(sk)*P(vj  | sk) 
s?= ))((maxarg ks sscorek  
where s? is the estimated sense, vj is the j-th word 
of the context, sk is the k-th possible sense for word 
w. 
P(sk) and P(vj  | sk) are calculated as follows: 
 
where  C(w) is the number of contexts for word w,  
C(vj , sk) is the number of occurrences of word vj in 
)(
)C(s)P(s kk wC=
)s(
)s,C(v
)s|P(v
k
kj
kj C=
contexts tagged with sense sk , and C(sk) is the 
number of contexts tagged with sense sk 
 
The values are obtained from the database, as fol-
lows:  
C(w)- from nocontexts,  
C(vj , sk)- from occurrences,  
C(sk)- from nosenses.  
wordsenses is being used to determine the possible 
senses of a given word.  
 
5 Evaluation 
The described system was evaluated at Senseval 3. 
The output was not weighted, therefore for every 
ambiguous word, at most 1 solution (estimated 
sense) was provided. The results achieved, are the 
followings: 
 
 score correct/attempted
precision 0.710 2415 correct of 
3403 attempted 
recall 0.682 2415 correct of 
3541 in total 
attempted 96.10% 3403 attempted 
of 3541 in total 
Figure2: Fine-grained score 
 
 score correct/attempted
precision 0.750 2551 correct of 
3403 attempted 
recall 0.720 2551 correct of 
3541 in total 
attempted 96.10% 3403 attempted 
of 3541 in total 
Figure2: Coarse-grained score 
 
A simple test was made, before the Senseval 3 
evaluation. The system was trained on 90% of the 
Romanian Lexical Sample training corpus, and 
tested on the remaining 10%. The selection was 
random, with a uniform distribution.  A coarse 
grained score was computed and compared to the 
baseline score. A baseline method consists of  de-
termining the most frequent sense for every word 
(based upon the training corpus) and in the evalua-
tion phase always this sense is assigned. 
 
 
 UBBNBC  Baseline 
recall 0.66 0.56 
precision 0.69 0.56 
Figure3: baseline UBBNBC comparison 
References 
Ted Pedersen. 1998. Na?ve Bayes as a Satisficing 
Model. Working Notes of the AAAI Spring Sympo-
sium on Satisficing Models, Palo Alto, CA 
Doina T?tar. 2003. Inteligen?? artificial? - Aplica?ii ?n 
prelucrarea limbajului natural. Editura Albastra, 
Cluj-Napoca, Romania. 
Manning, C. D., Sch?tze, H. 1999. Foundations of sta-
tistical natural language processing. MIT Press, 
Cambridge, Massachusetts. 
