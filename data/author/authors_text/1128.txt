Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 380?389, Prague, June 2007. c?2007 Association for Computational Linguistics
Explorations in Automatic Book Summarization
Rada Mihalcea and Hakan Ceylan
Department of Computer Science
University of North Texas
rada@cs.unt.edu, hakan@unt.edu
Abstract
Most of the text summarization research car-
ried out to date has been concerned with
the summarization of short documents (e.g.,
news stories, technical reports), and very lit-
tle work if any has been done on the sum-
marization of very long documents. In this
paper, we try to address this gap and ex-
plore the problem of book summarization.
We introduce a new data set specifically de-
signed for the evaluation of systems for book
summarization, and describe summarization
techniques that explicitly account for the
length of the documents.
1 Introduction
Books represent one of the oldest forms of written
communication and have been used since thousands
of years ago as a means to store and transmit
information. Despite this fact, given that a large
fraction of the electronic documents available online
and elsewhere consist of short texts such as Web
pages, news articles, scientific reports, and others,
the focus of natural language processing techniques
to date has been on the automation of methods tar-
geting short documents. We are witnessing however
a change: an increasingly larger number of books
become available in electronic format, in projects
such as Gutenberg (http://www.gutenberg.org),
Google Book Search (http://books.google.com),
or the Million Books project
(http://www.archive.org/details/millionbooks).
Similarly, a large number of the books published in
recent years are often available ? for purchase or
through libraries ? in electronic format. This means
that the need for language processing techniques
able to handle very large documents such as books
is becoming increasingly important.
In this paper, we address the problem of book
summarization. While there is a significant body
of research that has been carried out on the task
of text summarization, most of this work has been
concerned with the summarization of short doc-
uments, with a particular focus on news stories.
However, books are different in both length and
genre, and consequently different summarization
techniques are required. In fact, the straight-forward
application of a current state-of-the-art summariza-
tion tool leads to poor results ? a mere 0.348 F-
measure compared to the baseline of 0.325 (see the
following sections for details). This is not surprising
since these systems were developed specifically for
the summarization of short news documents.
The paper makes two contributions. First, we
introduce a new data set specifically designed for
the evaluation of book summaries. We describe
the characteristics of a new benchmark consisting
of books with manually constructed summaries, and
we calculate and provide lower and upper perfor-
mance bounds on this data set. Second, after briefly
describing a summarization system that has been
successfully used for the summarization of short
documents, we show how techniques that take into
account the length of the documents can be used to
significantly improve the performance of this sys-
tem.
2 Related Work
Automatic summarization has received a lot of atten-
tion from the natural language processing commu-
380
nity, ever since the early approaches to automatic ab-
straction that laid the foundations of the current text
summarization techniques (Luhn, 1958; Edmunson,
1969). The literature typically distinguishes be-
tween extraction, concerned with the identification
of the information that is important in the input text;
and abstraction, which involves a generation step to
add fluency to a previously compressed text (Hovy
and Lin, 1997). Most of the efforts to date have been
concentrated on the extraction step, which is perhaps
the most critical component of a successful summa-
rization algorithm, and this is the focus of our cur-
rent work as well.
To our knowledge, no research work to date was
specifically concerned with the automatic summa-
rization of books. There is, however, a large and
growing body of work concerned with the summa-
rization of short documents, with evaluations typ-
ically focusing on news articles. In particular, a
significant number of summarization systems have
been proposed during the recent Document Under-
standing Conference exercises (DUC) ? annual eval-
uations that usually draw the participation of 20?30
teams every year.
There are two main trends that can be identified
in the summarization literature: supervised systems,
that rely on machine learning algorithms trained on
pre-existing document-summary pairs, and unsuper-
vised techniques, based on properties and heuristics
derived from the text.
Among the unsupervised techniques, typical sum-
marization methods account for both the weight of
the words in sentences, as well as the sentence posi-
tion inside a document. These techniques have been
successfully implemented in the centroid approach
(Radev et al, 2004), which extends the idea of tf.idf
weighting (Salton and Buckley, 1997) by introduc-
ing word centroids, as well as integrating other fea-
tures such as position, first-sentence overlap and
sentence length. More recently, graph-based meth-
ods that rely on sentence connectivity have also been
found successful, using algorithms such as node de-
gree (Salton et al, 1997) or eigenvector centrality
(Mihalcea and Tarau, 2004; Erkan and Radev, 2004;
Wolf and Gibson, 2004).
In addition to unsupervised methods, supervised
machine learning techniques have also been used
with considerable success. Assuming the avail-
ability of a collection of documents and their cor-
responding manually constructed summaries, these
methods attempt to identify the key properties of a
good summary, such as the presence of named enti-
ties, positional scores, or the location of key phrases.
Such supervised techniques have been successfully
used in the systems proposed by e.g. (Teufel and
Moens, 1997; Hirao et al, 2002; Zhou and Hovy,
2003; D?Avanzo and Magnini, 2005).
In addition to short news documents, which have
been the focus of most of the summarization systems
proposed to date, work has been also carried out on
the summarization of other types of documents. This
includes systems addressing the summarization of e-
mail threads (Wan and McKeown, 2004), online dis-
cussions (Zhou and Hovy, 2005), spoken dialogue
(Galley, 2006), product reviews (Hu and Liu, 2004),
movie reviews (Zhuang et al, 2006), or short literary
fiction stories (Kazantseva and Szpakowicz, 2006).
As mentioned before, we are not aware of any work
addressing the task of automatic book summariza-
tion.
3 A Data Set for the Evaluation of
Book Summarization
A first challenge we encountered when we started
working on the task of book summarization was the
lack of a suitable data set, designed specifically for
the evaluation of summaries of long documents. Un-
like the summarization of short documents, which
benefits from the data sets made available through
the annual DUC evaluations, we are not aware of
any publicly available data sets that can be used for
the evaluation of methods for book summarization.
The lack of such data sets is perhaps not sur-
prising since even for humans the summarization of
books is more difficult and time consuming than the
summarization of short news documents. Moreover,
books are often available in printed format and are
typically protected by copyright laws that do not al-
low their reproduction in electronic format, which
consequently prohibits their public distribution.
We constructed a data set starting from the ob-
servation that several English and literature courses
make use of books that are sometimes also avail-
able in the form of abstracts ? meant to ease the
access of students to the content of the books. In
381
particular, we have identified two main publish-
ers that make summaries available online for books
studied in the U.S. high-school and college sys-
tems: Grade Saver (http://www.gradesaver.com) and
Cliff?s Notes (http://www.cliffsnotes.com/). Fortu-
nately, many of these books are classics that are al-
ready in the public domain, and thus for most of
them we were able to find the online electronic ver-
sion of the books on sites such as Gutenberg or On-
line Literature (http://www.online-literature.com).
For instance, the following is an example drawn
from Cliff?s Notes summary of Bleak House by
Charles Dickens.
On a raw November afternoon, London is en-
shrouded in heavy fog made harsher by chimney
smoke. The fog seems thickest in the vicinity of
the High Court of Chancery. The court, now in ses-
sion, is hearing an aspect of the case of Jarndyce
and Jarndyce. A ?little mad old woman? is, as al-
ways, one of the spectators. Two ruined men, one
a ?sallow prisoner,? the other a man from Shrop-
shire, appear before the court ? to no avail. Toward
the end of the sitting, the Lord High Chancellor an-
nounces that in the morning he will meet with ?the
two young people? and decide about making them
wards of their cousin....
Starting with the set of books that had a sum-
mary available from Cliff?s Notes, we removed all
the books that did not have an online version, and
further eliminated those that did not have a summary
available from Grade Saver. This left us with a ?gold
standard? data set of 50 books, each of them with
two manually created summaries.
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 0  100000  200000  300000  400000
Su
m
m
ar
y 
le
ng
th
Book length
Figure 1: Summary and book lengths for 50 books
The books in this collection have an average
length of 92,000 words, with summaries with an
average length of 6,500 words (Cliff?s Notes) and
7,500 words (Grade Saver). Figure 1 plots the length
of the summaries (averaged over the two manual
summaries) with respect to the length of the books.
As seen in the plot, most of the books have a length
of 50,000-150,000 words, with a summary of 2,000?
6,000 words, corresponding to a compression rate of
about 5-15%. There are also a few very long books,
with more than 150,000 words, for which the sum-
maries tend to become correspondingly longer.
3.1 Evaluation Metrics
For the evaluation, we use the ROUGE evaluation
toolkit. ROUGE is a method based on Ngram statis-
tics, found to be highly correlated with human eval-
uations (Lin and Hovy, 2003).1 Throughout the pa-
per, the evaluations are reported using the ROUGE-
1 setting, which seeks unigram matches between
the generated and the reference summaries, and
which was found to have high correlation with hu-
man judgments at a 95% confidence level. Addi-
tionally, the final system is also evaluated using the
ROUGE-2 (bigram matches) and ROUGE-SU4 (non-
contiguous bigrams) settings, which have been fre-
quently used in the DUC evaluations.
In most of the previous summarization evalua-
tions, the data sets were constructed specifically for
the purpose of enabling system evaluations, and thus
the length of the reference and the generated sum-
maries was established prior to building the data set
and prior to the evaluations. For instance, some
of the previous DUC evaluations provided refer-
ence summaries of 100-word each, and required the
participating systems to generate summaries of the
same length.
However, in our case we have to deal with
pre-existing summaries, with large summary-length
variations across the 50 books and across the two
reference summaries. To address this problem, we
decided to keep one manual summary as the main
reference (Grade Saver), and use the other summary
(Cliff?s Notes) as a way to decide on the length of
the generated summaries. This means that for a
given book, the Cliff?s Notes summary and all the
1ROUGE is available at http://haydn.isi.edu/ROUGE/
382
automatically generated summaries have the same
length, and they are all evaluated against the (pos-
sibly with a different length) Grade Saver summary.
This way, we can also calculate an upper bound by
comparing the two manual summaries against each
other, and at the same time ensure a fair comparison
between the automatically generated summaries and
this upper bound.2
3.2 Lower and Upper Bounds
To determine the difficulty of the task on the 50 book
data set, we calculate and report lower and upper
bounds. The lower bound is determined by using a
baseline summary constructed by including the first
sentences in the book (also known in the literature
as the lead baseline).3 As mentioned in the previ-
ous section, all the generated summaries ? includ-
ing this baseline ? have a length equal to the Cliff?s
Notes manual summary. The upper bound is calcu-
lated by evaluating Cliff?s Notes manual summary
against the reference Grade Saver summary. Table
1 shows the precision (P), recall (R), and F-measure
(F) for these lower and upper bounds, calculated as
average across the 50 books.
P R F
Lower bound (lead baseline) 0.380 0.284 0.325
Upper bound (manual summary) 0.569 0.493 0.528
Table 1: Lower and upper bounds for the book sum-
marization task, calculated on the 50 book data set
An automatic system evaluated on this data set is
therefore expected to have an F-measure higher than
the lower bound of 0.325, and it is unlikely to exceed
the upper bound of 0.528 obtained with a human-
generated summary.
4 An Initial Summarization System
Our first book summarization experiment was done
using a re-implementation of an existing state-of-
the-art summarization system. We decided to use the
2An alternative solution would be to determine the length
of the generated summaries using a predefined compression
rate (e.g., 10%). However, this again implies great variations
across the lengths of the generated versus the manual sum-
maries, which can result in large and difficult to interpret varia-
tions across the ROUGE scores.
3A second baseline that accounts for text segments is also
calculated and reported in section 6.
centroid-based method implemented in the MEAD
system (Radev et al, 2004), for three main reasons.
First, MEAD was shown to lead to good perfor-
mance in several DUC evaluations, e.g., (Radev et
al., 2003; Li et al, 2005). Second, it is an unsuper-
vised method which, unlike supervised approaches,
does not require training data (not available in our
case). Finally, the centroid-based techniques imple-
mented in MEAD can be optimized and made very
efficient, which is an important aspect in the sum-
marization of very long documents such as books.
The latest version of MEAD4 uses features, clas-
sifiers and re-rankers to determine the sentences to
include in the summary. The default features are
centroid, position and sentence length. The centroid
value of a sentence is the sum of the centroid val-
ues of the words in the sentence. The centroid value
of a word is calculated by multiplying the term fre-
quency (tf) of a word by the word?s inverse docu-
ment frequency (idf) obtained from the Topic Detec-
tion and Tracking (TDT) corpus. The tf of a word
is calculated by dividing the frequency of a word in
a document cluster by the number of documents in
the cluster. The positional value Pi of a sentence is
calculated using the formula (Radev et al, 2004):
Pi =
n? i+ 1
n ? Cmax (1)
where n represents the number of sentences in the
document, i represents the position of the sentence
inside the text, and Cmax is the score of the sentence
that has the maximum centroid value.
The summarizer combines these features to give
a score to each sentence. The default setting con-
sists of a linear combination of features that assigns
equal weights to the centroid and the positional val-
ues, and only scores sentences that have more than
nine words. After the sentences are scored, the re-
rankers are used to modify the scores of a sentence
depending on its relation with other sentences. The
default re-ranker implemented in MEAD first ranks
the sentences by their scores in descending order
and iteratively adds the top ranked sentence if the
sentence is not too similar to the already added sen-
tences. This similarity is computed as a cosine sim-
ilarity and by default the sentences that exhibit a co-
sine similarity higher than 0.7 are not added to the
4MEAD 3.11, http://www.summarization.com/mead/
383
summary. Note that although the MEAD distribution
also includes an optional feature calculated using the
LexRank graph-based algorithm (Erkan and Radev,
2004), this feature could not be used since it takes
days to compute for very long documents such as
ours, and thus its application was not tractable.
Although the MEAD system is publicly available
for download, in order to be able to make continu-
ous modifications easily and efficiently to the system
as we develop new methods, we decided to write
our own implementation. Our implementation dif-
fers from the original one in certain aspects. First,
we determine document frequency counts using the
British National Corpus (BNC) rather than the TDT
corpus. Second, we normalize the sentence scores
by dividing the score of a sentence by the length of
the sentence, and instead we eliminate the sentence
length feature used by MEAD. Note also that we do
not take stop words into account when calculating
the length of a sentence. Finally, since we are not
doing multi-document summarization, we do not use
a re-ranker in our implementation.
P R F
MEAD (original download) 0.423 0.296 0.348
MEAD (our implementation) 0.435 0.323 0.369
Table 2: Summarization results using the MEAD
system
Table 2 shows the results obtained on the 50 book
data set using the original MEAD implementation,
as well as our implementation. Although the per-
formance of this system is clearly better than the
baseline (see Table 1), it is nonetheless far below the
upper bound. In the following section, we explore
techniques for improving the quality of the gener-
ated summaries by accounting for the length of the
documents.
5 Techniques for Book Summarization
We decided to make several changes to our initial
system, in order to account for the specifics of the
data set we work with. In particular, our data set
consists of very large documents, and correspond-
ingly the summarization of such documents requires
techniques that account for their length.
5.1 Sentence Position In Very Large
Documents
The general belief in the text summarization litera-
ture (Edmunson, 1969; Mani, 2001) is that the posi-
tion of sentences in a text represents one of the most
important sources of information for a summariza-
tion system. In fact, a summary constructed using
the lead sentences was often found to be a compet-
itive baseline, with only few systems exceeding this
baseline during the recent DUC summarization eval-
uations.
Although the position of sentences in a document
seems like a pertinent heuristic for the summariza-
tion of short documents, and in particular for the
newswire genre as used in the DUC evaluations, our
hypothesis is that this heuristic may not hold for
the summarization of very long documents such as
books. The style and topic may change several times
throughout a book, and thus the leading sentences
will not necessarily overlap with the essence of the
document.
To test this hypothesis, we modified our initial
system so that it does not account for the position
of the sentences inside a document, but it only ac-
counts for the weight of the constituent words. Cor-
respondingly, the score of a sentence is determined
only as a function of the word centroids, and ex-
cludes the positional score. Table 3 shows the av-
erage ROUGE scores obtained using the summariza-
tion system with and without the position scores.
P R F
With positional scores 0.435 0.323 0.369
Without positional scores 0.459 0.329 0.383
Table 3: Summarization results with and without po-
sitional scores
As suspected, removing the position scores leads
to a better overall performance, with an increase ob-
served in both the precision and the recall of the
system. Although the position in a document is a
heuristic that helps the summarization of news sto-
ries and other short documents, it appears that the
sentences located toward the beginning of a book are
not necessarily useful for building the summary of a
book.
384
5.2 Text Segmentation
A major difference between short and long docu-
ments stands in the frequent topic shifts typically
observed in the later. While short stories are usu-
ally concerned with one topic at a time, long doc-
uments such as books often cover more than one
topic. Thus, the intuition is that a summary should
include content covering the important aspects of
all the topics in the document, as opposed to only
generic aspects relevant to the document as a whole.
A system for the summarization of long documents
should therefore extract key concepts from all the
topics in the document, and this task is better per-
formed when the topic boundaries are known prior
to the summarization step.
To accomplish this, we augment our system with
a text segmentation module that attempts to deter-
mine the topic shifts, and correspondingly splits
the document into smaller segments. Note that al-
though chapter boundaries are available in some of
the books in our data set, this is not always the case
as there are also books for which the chapters are not
explicitly identified. To ensure an uniform treatment
of the entire data set, we decided not to use chap-
ter boundaries, and instead apply an automatic text
segmentation algorithm.
While several text segmentation systems have
been proposed to date, we decided to use a graph-
based segmentation algorithm using normalized-
cuts (Malioutov and Barzilay, 2006), shown to ex-
ceed the performance of alternative segmentation
methods. Briefly, the segmentation algorithm starts
by modeling the text as a graph, where sentences
are represented as nodes in the graph, and inter-
sentential similarities are used to draw weighted
edges. The similarity between sentences is calcu-
lated using cosine similarity, with a smoothing fac-
tor that adds the counts of the words in the neighbor
sentences. Words are weighted using an adaptation
of the tf.idf metric, where a document is uniformly
split into chunks that are used for the tf.idf computa-
tion. There are two parameters that have to be set in
this algorithm: (1) the length in words of the blocks
approximating sentences; and (2) the cut-off value
for drawing edges between nodes. Since the method
was originally developed for spoken lecture segmen-
tation, we were not able to use the same parameters
as suggested in (Malioutov and Barzilay, 2006). In-
stead, we used a development set of three books, and
determined the optimal sentence word-length as 20
and the optimal cut-off value as 25, and these are the
values used throughout our experiments.
Once the text is divided into segments, we gener-
ate a separate summary for each segment, and con-
sequently create a final summary by collecting sen-
tences from the individual segment summaries in
a round-robin fashion. That is, starting with the
ranked list of sentences generated by the summa-
rization algorithm for each segment, we pick one
sentence at a time from each segment summary until
we reach the desired book-summary length.
A useful property of the normalized-cut segmen-
tation algorithm is that one can decide apriori the
number of segments to be generated, and so we can
evaluate the summarization algorithm for different
segmentation granularities. Figure 2 shows the av-
erage ROUGE-1 F-measure score obtained for sum-
maries generated using one to 50 segments.
 0.37
 0.38
 0.39
 0.4
 0  5  10  15  20  25  30  35  40  45  50
R
ou
ge
-1
 F
-m
ea
su
re
Number of segments
Figure 2: Summarization results for different seg-
mentation granularities.
As seen in the figure, segmenting the text helps
the summarization process. The average ROUGE-1
F-measure score raises to more than 0.39 F-measure
for increasingly larger number of segments, with a
plateau reached at approximately 15?25 segments,
followed by a decrease when more than 30 segments
are used.
In all the following evaluations, we segment each
book into a constant number of 15 segments; in fu-
ture work, we plan to consider more sophisticated
methods for finding the optimal number of segments
individually for each book.
385
5.3 Modified Term Weighting
An interesting characteristic of documents with
topic shifts is that words do not have an uniform dis-
tribution across the entire document. Instead, their
distribution can vary with the topic, and thus the
weight of the words should change accordingly.
To account for the distribution of the words in-
side the entire book, as well as inside the individual
topics (segments), we devised a weighting scheme
that accounts for four factors: the segment term
frequency (stf), calculated as the number of occur-
rences of a word inside a segment; the book term
frequency (tf), determined as the number of occur-
rences of a word inside a book; the inverse segment
frequency (isf), measured as the inverse of the num-
ber of segments containing the word; and finally, the
inverse document frequency (idf), which takes into
account the distribution of a word in a large exter-
nal corpus (as before, we use the BNC corpus). A
word weight is consequently determined by multi-
plying the book term frequency with the segment
term frequency, and the result is then multiplied with
the inverse segment frequency and the inverse docu-
ment frequency. We refer to this weighting scheme
as tf.stf.idf.isf.
Using this weighting scheme, we prevent a word
from having the same score across the entire book,
and instead we give a higher weight to its occur-
rences in segments where the word has a high fre-
quency. For instance, the word doctor occurs 30
times in one of the books in our data set, which leads
to a constant tf.idf score of 36.76 across the entire
book. Observing that from these 30 occurrences, 19
appear in just one segment, the tf.stf.idf.isf weight-
ing scheme will lead to a weight of 698.49 for that
segment, much higher than e.g. the weight of 36
calculated for other segments that have only a few
occurrences of this word.
P R F
tf.idf weighting 0.463 0.339 0.391
tf.stf.idf.isf weighting 0.464 0.349 0.398
Table 4: Summarization results using a weighting
scheme accounting for the distribution of words in-
side and across segments
Table 4 shows the summarization results obtained
for the new weighting scheme (recall that all the re-
sults are calculated for a text segmentation into 15
segments).
5.4 Combining Summarization Methods
The next improvement we made was to bring an
additional source of knowledge into the system, by
combining the summarization provided by our cur-
rent system with the summarization obtained from a
different method.
We implemented a variation of a centrality graph-
based algorithm for unsupervised summarization,
which was successfully used in the past for the
summarization of short documents. Very briefly,
the TextRank system (Mihalcea and Tarau, 2004)
? similar in spirit with the concurrently proposed
LexRank method (Erkan and Radev, 2004) ? works
by building a graph representation of the text, where
sentences are represented as nodes, and weighted
edges are drawn using inter-sentential word overlap.
An eigenvector centrality algorithm is then applied
on the graph (e.g., PageRank), leading to a rank-
ing over the sentences in the document. An imped-
iment we encountered was the size of the graphs,
which become intractably large and dense for very
large documents such as books. In our implemen-
tation we decided to use a cut-off value for drawing
edges between nodes, and consequently removed all
the edges between nodes that are farther apart than
a given threshold. We use a threshold value of 75,
found to work best using the same development set
of three books used before.
P R F
Our system 0.464 0.349 0.398
TextRank 0.449 0.356 0.397
COMBINED 0.464 0.363 0.407
Table 5: Summarization results for individual and
combined summarization algorithms
Using the same segmentation as before (15 seg-
ments), the TextRank method by itself did not lead to
improvements over our current centroid-based sys-
tem. Instead, since we noticed that the summaries
generated with our system and with TextRank cov-
ered different sentences, we implemented a method
that combines the top ranked sentences from the
two methods. Specifically, the combination method
picks one sentence at a time from the summary gen-
erated by our system for each segment, followed by
386
one sentence selected from the summary generated
by the TextRank method, and so on. The combi-
nation method also specifically avoids redundancy.
Table 5 shows the results obtained with our current
centroid-based system the TextRank method, as well
as the combined method.
5.5 Segment Ranking
In the current system, all the segments identified in
a book have equal weight. However, this might not
always be the case, as there are sometimes topics
inside the book that have higher importance, and
which consequently should be more heavily repre-
sented in the generated summaries.
To account for this intuition, we implemented a
segment ranking method that assigns to each seg-
ment a score reflecting its importance inside the
book. The ranking is performed with a method sim-
ilar to TextRank, using a random-walk model over
a graph representing segments and segment simi-
larities. The resulting segment scores are multi-
plied with the sentence scores obtained from the
combined method described before, normalized over
each segment, resulting in a new set of scores. The
top ranked sentences over the entire book are then
selected for inclusion in the summary. Table 6 shows
the results obtained by using segment ranking.
P R F
COMBINED 0.464 0.363 0.407
COMBINED + Segment Ranking 0.472 0.366 0.412
Table 6: Summarization results using segment rank-
ing
6 Discussion
In addition to the ROUGE-1 metric, the quality of the
summaries generated with our final summarization
system was also evaluated using the ROUGE-2 and
the ROUGE-SU4 metrics, which are frequently used
in the DUC evaluations. Table 7 shows the figures
obtained with ROUGE-1, ROUGE-2 and ROUGE-
SU4 for our final system, for the original MEAD
download, as well as for the lower and upper bounds.
The table also shows an additional baseline deter-
mined by selecting the first sentences in each seg-
ment, using the segmentation into 15 segments as
determined before. As it can be seen from the F-
P R F
ROUGE-1
Lower bound 0.380 0.284 0.325 [0.306,0.343]
Segment baseline 0.402 0.301 0.344 [0.328,0.366]
MEAD 0.423 0.296 0.348 [0.329,0.368]
Our system 0.472 0.366 0.412 [0.394,0.428]
Upper bound 0.569 0.493 0.528 [0.507,0.548]
ROUGE-2
Lower bound 0.035 0.027 0.031 [0.027,0.035]
Segment baseline 0.040 0.031 0.035 [0.031,0.038]
MEAD 0.039 0.029 0.033 [0.028,0.037]
Our system 0.069 0.054 0.061 [0.055,0.067]
Upper bound 0.112 0.097 0.104 [0.096,0.111]
ROUGE-SU4
Lower bound 0.096 0.073 0.083 [0.076,0.090]
Segment baseline 0.102 0.079 0.089 [0.082,0.093]
MEAD 0.106 0.076 0.088 [0.081,0.095]
Our system 0.148 0.115 0.129 [0.121,0.138]
Upper bound 0.210 0.182 0.195 [0.183,0.206]
Table 7: Evaluation of our final book summariza-
tion system using different ROUGE metrics. The ta-
ble also shows: the lower bound (first sentences in
the book); the segment baseline (first sentences in
each segment); MEAD (original system download);
the upper bound (manual summary). Confidence in-
tervals for F-measure are also included.
measure confidence intervals also shown in the ta-
ble, the improvements obtained by our system with
respect to both baselines and with respect to the
MEAD system are statistically significant (as the
confidence intervals do not overlap).
Additionally, to determine the robustness of the
results with respect to the number of reference sum-
maries, we ran a separate evaluation where both the
Grade Saver and the Cliff?s Notes summaries were
used as reference. As before, the length of the gener-
ated summaries was determined based on the Cliff?s
Notes summary. The F-measure figures obtained
in this case using our summarization system were
0.402, 0.057 and 0.127 using ROUGE-1, ROUGE-2
and ROUGE-SU4 respectively. The F-measure fig-
ures calculated for the baseline using the first sen-
tences in each segment were 0.340, 0.033 and 0.085.
These figures are very close to those listed in Table
7 where only one summary was used as a reference,
suggesting that the use of more than one reference
summary does not influence the results.
Regardless of the evaluation metric used, the per-
formance of our book summarization system is sig-
nificantly higher than the one of an existing summa-
rization system that has been designed for the sum-
387
marization of short documents (MEAD). In fact, if
we account for the upper bound of 0.528, the rela-
tive error rate reduction for the ROUGE-1 F-measure
score obtained by our system with respect to MEAD
is a significant 34.44%.
The performance of our system is mainly due to
features that account for the length of the document:
exclusion of positional scores, text segmentation and
segment ranking, and a segment-based weighting
scheme. An additional improvement is obtained by
combining two different summarization methods. It
is also worth noting that our system is efficient, tak-
ing about 200 seconds to apply the segmentation al-
gorithm, plus an additional 65 seconds to generate
the summary of one book.5
To assess the usefulness of our system with re-
spect to the length of the documents, we analyzed
the individual results obtained for books of different
sizes. Averaging the results obtained for the shorter
books in our collection, i.e., 17 books with a length
between 20,000 and 50,000 words, the lead base-
line gives a ROUGE-1 F-measure score of 0.337,
our system leads to 0.378, and the upper bound is
measured at 0.498, indicating a relative error rate
reduction of 25.46% obtained by our system with
respect to the lead baseline (accounting for the max-
imum achievable score given by the upper bound).
Instead, when we consider only the books with a
length over 100,000 words (16 books in our data set
fall under this category), the lead baseline is deter-
mined as 0.347, our system leads to 0.418, and the
upper bound is calculated as 0.552, which results in
a higher 34.64% relative error rate reduction. This
suggests that our system is even more effective for
longer books, due perhaps to the features that specif-
ically take into account the length of the books.
There are also cases where our system does not
improve over the baseline. For instance, for the sum-
marization of Candide by Franc?ois Voltaire, our sys-
tem achieves a ROUGE-1 F-measure of 0.361, which
is slightly worse than the lead baseline of 0.368. In
other cases however, the performance of our system
comes close to the upper bound, as it is the case with
the summarization of The House of the Seven Gables
by Nathaniel Hawthorne, which has a lead baseline
5Running times measured on a Pentium IV 3GHz, 2GB
RAM.
of 0.296, an upper bound of 0.457, and our system
obtains 0.404. This indicates that a possible avenue
for future research is to account for the characteris-
tics of a book, and devise summarization methods
that can adapt to the specifics of a given book such
as length, genre, and others.
7 Conclusions
Although there is a significant body of work that has
been carried out on the task of text summarization,
most of the research to date has been concerned with
the summarization of short documents. In this paper,
we tried to address this gap and tackled the problem
of book summarization.
We believe this paper made two important con-
tributions. First, it introduced a new summariza-
tion benchmark, specifically targeting the evalua-
tion of systems for book summarization.6 Second,
it showed that systems developed for the summa-
rization of short documents do not fare well when
applied to very long documents such as books, and
instead a better performance can be achieved with
a system that accounts for the length of the docu-
ments. In particular, the book summarization sys-
tem we developed was found to lead to more than
30% relative error rate reduction with respect to an
existing state-of-the-art summarization tool.
Given the increasingly large number of books
available in electronic format, and correspondingly
the growing need for tools for book summarization,
we believe that the topic of automatic book sum-
marization will become increasingly important. We
hope that this paper will encourage and facilitate the
development of an active line of research concerned
with book summarization.
Acknowledgments
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for useful discussions and feed-
back, and in particular to Carmen Banea for sug-
gesting Cliff?s Notes as a source of book summaries.
This work was supported in part by a research grant
from Google Inc. and by a grant from the Texas Ad-
vanced Research Program (#003594).
6The data set is publicly available and can be downloaded
from http://lit.csci.unt.edu/index.php/Downloads
388
References
E. D?Avanzo and B. Magnini. 2005. A keyphrase-based
approach to summarization: The Lake system at DUC
2005. In Proceedings of the Document Understanding
Conference (DUC 2005).
H.P. Edmunson. 1969. New methods in automatic ex-
tracting. Journal of the ACM, 16(2):264?285.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2004), Barcelona,
Spain, July.
M. Galley. 2006. Automatic summarization of conversa-
tional multi-party speech. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI
2006), AAAI/SIGART Doctoral Consortium, Boston.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
NTT?s text summarization system for DUC-2002. In
Proceedings of the Document Understanding Confer-
ence 2002 (DUC 2002).
E. Hovy and C. Lin, 1997. Automated text summarization
in SUMMARIST. Cambridge Univ. Press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, Seattle, Washington.
A. Kazantseva and S. Szpakowicz. 2006. Challenges in
evaluating summaries of short stories. In Proceedings
of the Workshop on Task-Focused Summarization and
Question Answering, Sydney, Australia.
W. Li, W. Li, B. Li, Q. Chen, and M. Wu. 2005. The
Hong Kong Polytechnic University at DUC 2005. In
Proceedings of the Document Understanding Confer-
ence (DUC 2005), Vancouver, Canada.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development,
2(2):159?165.
I. Malioutov and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL 2006), pages 9?16.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
D. Radev, J. Otterbacher, H. Qi, and D. Tam. 2003.
MEAD ReDUCs: Michigan at DUC 2003. In Pro-
ceedings of the Document Understanding Conference
(DUC 2003).
D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-
based summarization of multiple documents. Informa-
tion Processing and Management, 40.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In ACL/EACL workshop on intelli-
gent and scalable text summarization, Madrid, Spain.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of the 20th International Conference on
Computational Linguistics, Geneva, Switzerland.
F. Wolf and E. Gibson. 2004. Paragraph-, word-, and
coherence-based approaches to sentence ranking: A
comparison of algorithm and human performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2004), Barcelona,
Spain, July.
L. Zhou and E. Hovy. 2003. A Web-trained extrac-
tion summarization system. In Proceedings of Hu-
man Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada, May.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet re-
lay chats. In Proceedings of Association for Computa-
tional Linguistics (ACL 2005), Ann Arbor.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management (CIKM 2006), Arlington, Vir-
ginia.
389
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1066?1074,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Language Identification of Search Engine Queries
Hakan Ceylan
Department of Computer Science
University of North Texas
Denton, TX, 76203
hakan@unt.edu
Yookyung Kim
Yahoo! Inc.
2821 Mission College Blvd.
Santa Clara, CA, 95054
ykim@yahoo-inc.com
Abstract
We consider the language identification
problem for search engine queries. First,
we propose a method to automatically
generate a data set, which uses click-
through logs of the Yahoo! Search En-
gine to derive the language of a query indi-
rectly from the language of the documents
clicked by the users. Next, we use this
data set to train two decision tree classi-
fiers; one that only uses linguistic features
and is aimed for textual language identi-
fication, and one that additionally uses a
non-linguistic feature, and is geared to-
wards the identification of the language
intended by the users of the search en-
gine. Our results show that our method
produces a highly reliable data set very ef-
ficiently, and our decision tree classifier
outperforms some of the best methods that
have been proposed for the task of written
language identification on the domain of
search engine queries.
1 Introduction
The language identification problem refers to the
task of deciding in which natural language a given
text is written. Although the problem is heav-
ily studied by the Natural Language Processing
community, most of the research carried out to
date has been concerned with relatively long texts
such as articles or web pages which usually con-
tain enough text for the systems built for this task
to reach almost perfect accuracy. Figure 1 shows
the performance of 6 different language identifi-
cation methods on written texts of 10 European
languages that use the Roman Alphabet. It can
be seen that the methods reach a very high ac-
curacy when the text has 100 or more characters.
However, search engine queries are very short in
length; they have about 2 to 3 words on average,
Figure 1: Performance of six Language Identifica-
tion methods on varying text size. Adapted from
(Poutsma, 2001).
which requires a reconsideration of the existing
methods built for this problem.
Correct identification of the language of the
queries is of critical importance to search engines.
Major search engines such as Yahoo! Search
(www.yahoo.com), or Google (www.google.com)
crawl billions of web pages in more than 50 lan-
guages, and about a quarter of their queries are in
languages other than English. Therefore a correct
identification of the language of a query is needed
in order to aid the search engine towards more ac-
curate results. Moreover, it also helps further pro-
cessing of the queries, such as stemming or spell
checking of the query terms.
One of the challenges in this problem is the lack
of any standard or publicly available data set. Fur-
thermore, creating such a data set is expensive as
it requires an extensive amount of work by hu-
man annotators. In this paper, we introduce a new
method to overcome this bottleneck by automat-
ically generating a data set of queries with lan-
guage annotations. We show that the data gener-
ated this way is highly reliable and can be used to
train a machine learning algorithm.
We also distinguish the problem of identifying
the textual language vs. the language intended by
the users for the search engine queries. For search
engines, there are cases where a correct identifi-
1066
cation of the language does not necessarily im-
ply that the user wants to see the results in the
same language. For example, although the textual
identification of the language for the query ?homo
sapiens? is Latin, a user entering this query from
Spain, would most probably want to see Spanish
web pages, rather than web pages in Latin. We ad-
dress this issue by adding a non-linguistic feature
to our system.
We organize the rest of the paper as follows:
First, we provide an overview of the previous re-
search in this area. Second, we present our method
to automatically generate a data set, and evaluate
the effectiveness of this technique. As a result of
this evaluation, we obtain a human-annotated data
set which we use to evaluate the systems imple-
mented in the following sections. In Section 4, we
implement some of the existing models and com-
pare their performance on our test set. We then
use the results from these models to build a deci-
sion tree system. Next, we consider identifying the
language intended by the user for the results of the
query, and describe a system geared towards this
task. Finally, we conclude our study and discuss
the future directions for the problem.
2 Related Work
Most of the work carried out to date on the writ-
ten language identification problem consists of su-
pervised approaches that are trained on a list of
words or n-gram models for each reference lan-
guage. The word based approaches use a list of
short words, common words, or a complete vocab-
ulary which are extracted from a corpus for each
language. The short words approach uses a list of
words with at most four or five characters; such as
determiners, prepositions, and conjunctions, and
is used in (Ingle, 1976; Grefenstette, 1995). The
common words method is a generalization over
the short words one which, in addition, includes
other frequently occuring words without limiting
them to a specific length, and is used in (Souter et
al., 1994; Cowie et al, 1999). For classification,
the word-based approaches sort the list of words in
descending order of their frequency in the corpus
from which they are extracted. Then the likelihood
of each word in a given text can be calculated by
using rank-order statistics or by transforming the
frequencies into probabilities.
The n-gram based approaches are based on the
counts of character or byte n-grams, which are se-
quences of n characters or bytes, extracted from
a corpus for each reference language. Different
classification models that use the n-gram features
have been proposed. (Cavnar and Trenkle, 1994)
used an out-of-place rank order statistic to mea-
sure the distance of a given text to the n-gram
profile of each language. (Dunning, 1994) pro-
posed a system that uses Markov Chains of byte n-
grams with Bayesian Decision Rules to minimize
the probability error. (Grefenstette, 1995) simply
used trigram counts that are transformed into prob-
abilities, and found this superior to the short words
technique. (Sibun and Reynar, 1996) used Rela-
tive Entropy by first generating n-gram probabil-
ity distributions for both training and test data, and
then measuring the distance between the two prob-
ability distributions by using the Kullback-Liebler
Distance. (Poutsma, 2001) developed a system
based on Monte Carlo Sampling.
Linguini, a system proposed by (Prager, 1999),
combines the word-based and n-gram models us-
ing a vector-space based model and examines the
effectiveness of the combined model and the in-
dividual features on varying text size. Similarly,
(Lena Grothe and Nrnberger, 2008) combines both
models using the ad-hoc method of (Cavnar and
Trenkle, 1994), and also presents a comparison
study. The work most closely related to ours is
presented very recently in (Hammarstro?m, 2007),
which proposes a model that uses a frequency dic-
tionary together with affix information in order to
identify the language of texts as short as one word.
Other systems that use methods aside from
the ones discussed above have also been pro-
posed. (Takci and Sogukpinar, 2004) used letter
frequency features in a centroid based classifica-
tion model. (Kruengkrai et al, 2005) proposed a
feature based on alignment of string kernels us-
ing suffix trees, and used it in two different clas-
sifiers. Finally, (Biemann and Teresniak, 2005)
presented an unsupervised system that clusters the
words based on sentence co-occurence.
Recently, (Hughes et al, 2006) surveyed the
previous work in this area and suggested that the
problem of language identification for written re-
sources, although well studied, has too many open
challenges which requires a more systematic and
collaborative study.
3 Data Generation
We start the construction of our data set by re-
trieving the queries, together with the clicked urls,
from the Yahoo! Search Engine for a three months
time period. For each language desired in our data
set, we retrieve the queries from the corresponding
1067
Yahoo! web site in which the default language is
the same as the one sought.1 Then we preprocess
the queries by getting rid of the ones that have any
numbers or special characters in them, removing
extra spaces between query terms, and lowercas-
ing all the letters of the queries2. Next, we ag-
gregate the queries that are exactly the same, by
calculating the frequencies of the urls clicked for
each query.
As we pointed out in Section 1, and illustrated
in Figure 1, the language identification methods
give almost perfect accuracy when the text has 100
or more characters. Furthermore, it is suggested in
(Levering and Cutler, 2006) that the average tex-
tual content in a web page is 474 words. Thus we
assume that it is a fairly trivial task to identify the
language for an average web page using one of the
existing methods.3 In our case, this task gets al-
ready accomplished by the crawler for all the web
pages crawled by the search engine.
Thus we can summarize our information in two
separate tables; T1 and T2. For Table T1, we have
a set of queries Q, and each q ? Q maps to a
set of url-frequency pairs. Each mapping is of the
form (q, u, fu), where u is a url clicked for q, and
fu is the frequency of u. Table T2, on the other
hand, contains the urls of all the web pages known
to the search engine and has only two columns;
(u, l), where u is a unique url, and l is the language
identified for u. Since we do not consider multi-
lingual web pages, every url in T2 is unique and
has only one language associated with it.
Next, we combine the tables T1 and T2 using
an inner join operation on the url columns. After
the join, we group the results by the language and
query columns, during which we also count the
number of distinct urls per query, and sum their
frequencies. We illustrate this operation with a
SQL query in Algorithm 1. As a result of these
operations, we have, for each query q ? Q, a set of
triplets (l, fl, cu,l) where l is a language, fl is the
count of clicks for l (which we obtained through
the urls in language l), and cu,l is the count of
unique urls in language l.
The resulting table T3 associates queries with
languages, but also contains a lot of noise. First,
1We do not make a distinction between the different di-
alects of the same languge. For English, Spanish and Por-
tuguese we gather queries from the web sites of United States,
Mexico, and Brazil respectively.
2In this study, we only considered languages that use the
Roman alphabet.
3Although not done in this study, the urls of web pages
that have less than a defined number of words, such as 100,
can be discarded to ensure a higher confidence.
Input: Tables T1:[q, u, fu], T2:[u, l]
Output: Table T3:[q, l, fl, cu,l]
CREATE VIEW T3 AS
SELECT
T1.q, T2.l, COUNT(T1.u) AS cu,l, SUM(T1.fu) AS fl
FROM T1
INNER JOIN T2
ON T1.u = T2.u
GROUP BY q, l;
Algorithm 1: Join Tables T1 and T2, group by
query and language, aggregate distinct url and fre-
quency counts.
we have queries that map to more than one lan-
guage, which suggests that the users clicked on the
urls in different languages for the same query. To
quantify the strength of each of these mappings,
we calculate a weight wq,l for each mapping of a
query q to a language l as:
wq,l = fl/Fq
where Fq, the total frequency of a query q, is de-
fined as:
Fq =
?
l?Lq
fl
where Lq is the set of languages for which q has a
mapping. Having computed a weight wq,l for each
mapping, we introduce our first threshold param-
eter, W . We eliminate all the queries in our data
set, which have weights, wq,l, below the threshold
W .
Second, even though some of the queries map to
only one language, this mapping cannot be trusted
due to the high frequency of the queries together
with too few distinct urls. This case suggests that
the query is most likely navigational. The intent
of navigational queries, such as ?ACL 2009?, is to
find a particular web site. Therefore they usually
consist of proper names, or acronyms that would
not be of much use to our language identification
problem. Hence we would like to get rid of the
navigational queries in our data set by using some
of the features proposed for the task of automatic
taxonomy of search engine queries. For a more
detailed discussion of this task, we refer the reader
to (Broder, 2002; Rose and Levinson, 2004; Lee et
al., 2005; Liu et al, 2006; Jansen et al, 2008).
Two of the features used in (Liu et al, 2006)
in identification of the navigational queries from
click-through data, are the number of Clicks Satis-
fied (nCS) and number of Results Satisfied (nRS).
In our problem, we substitute nCS with Fq, the to-
tal click frequency of the query q, and nRS with
1068
Uq, the number of distinct urls clicked for q. Thus
we eliminate the queries that have a total click fre-
quency above a given frequency threshold F , and,
that have less than a given distinct number of urls,
U . Thus, we have three parameters that help us in
eliminating the noise from the inital data; W , F ,
and U . We show the usage of these parameters in
SQL queries, in Algorithm 2.
Input: Tables T1:[q, u, fu], T2:[u, l], T3:[q, l, fl, cu,l]
Parameters W , F , and U
Output: Table D:[q, l]
CREATE VIEW T4 AS
SELECT T1.q, COUNT(T1.u) AS cu, SUM(T1.fu) AS Fq
FROM T1
INNER JOIN T2 ON T1.u = T2.u
GROUP BY q;
CREATE VIEW D AS
SELECT T3.q, T3.l, T3.fl / T4.Fq AS wq,l
FROM T1
INNER JOIN T4 ON T3.q = T4.q 10
WHERE
T4.Fq < F AND
wq,l >= W AND
T4.cu,l >= U ;
Algorithm 2: Construction of the final data set
D, by eliminating queries from T3 based on the
parameters W , F , and U .
The parameters F , U , and W are actually de-
pendent on the size of the data set under consid-
eration, and the study in (Silverstein et al, 1999)
suggests that we can get enough click-through data
for our analysis by retrieving a large sample of
queries. Since we retrieve the queries that are sub-
mitted within a three months period, for each lan-
guage, we have millions of unique queries in our
data set. Investigating a held-out development set
of queries retrieved from the United States web
site (www.yahoo.com), we empirically decided
the following values for the parameters, W = 1,
F = 50, and U = 5. In other words, we only
accepted the queries for which the contents of the
urls agree on the same language, that are submit-
ted less than 50 times, and at least have 5 unique
urls clicked.
The filtering process leaves us with 5-10% of
the queries due to the conservative choice of the
parameters. From the resulting set, we randomly
picked 500 queries and asked a native speaker to
annotate them. For each query, the annotator was
to classify the query into one of three categories:
? Category-1: If the query does not contain
any foreign terms.
Language Category-1 Category-1+2 Category-3
English 90.6% 94.2% 5.8%
French 84.6% 93.4% 6.6%
Portuguese 85.2% 93.4% 6.6%
Spanish 86.6% 97.4% 2.6%
Italian 82.4% 96.6% 3.4%
German 76.8% 87.2% 12.8%
Dutch 81.0% 92.0% 8.0%
Danish 82.4% 93.2% 6.8%
Finnish 87.2% 94.0% 6.0%
Swedish 86.6% 95.4% 4.6%
Average 84.3% 93.7% 6.3%
Table 1: Annotation of 500 sample queries drawn
from the automatically generated data.
? Category-2: If there exists some foreign
terms but the query would still be expected
to bring web pages in the same language.
? Category-3: If the query belongs to other
languages, or all the terms are foreign to the
annotator.4
90.6% of the queries in our data set were anno-
tated as Category-1, and 94.2% as Category-1 and
Category-2 combined. Having successful results
for the United States data set, we applied the same
parameters to the data sets retrieved for other lan-
guages as well, and had the native speakers of each
language annotate the queries in the same way. We
list these results in Table 1.
The results for English have the highest accu-
racy for Category-1, mostly due to the fact that we
tuned our parameters using the United States data.
The scores for German on the other hand, are the
lowest. We attribute this fact to the highly multi-
linguality of the Yahoo! Germany website, which
receives a high number of non-German queries.
In order to see how much of this multi-linguality
our parameter selection successfully eliminate, we
randomly picked 500 queries from the aggregated
but unfiltered queries of the Yahoo! Germany
website, and had them annotated as before.
As suspected, the second annotation results
showed that, only 47.6% of the queries were an-
notated as Category-1 and 60.2% are annotated
as Category-1 and Category-2 combined. Our
method was indeed successful and achieved 29.2%
improvement over Category-1, and 27% improve-
ment over Category-1 and Category-2 queries
combined.
Another interesting fact to note is the absolute
differences between Category-1 and Category-1+2
scores. While this number is very low, 3.8%,
for English, it is much higher for the other lan-
4We do not expect the annotators to know the etymology
of the words or have the knowledge of all the acronyms.
1069
Language MinC MaxC ?C MinW MaxW ?W
English 7 46 21.8 1 6 3.35
French 6 74 22.6 1 10 3.38
Portug. 3 87 22.5 1 14 3.55
Spanish 5 57 23.5 1 9 3.51
Italian 4 51 21.9 1 8 3.09
German 3 53 18.1 1 6 2.05
Dutch 5 43 16.3 1 6 2.11
Danish 3 40 14.3 1 6 1.93
Finnish 3 34 13.3 1 5 1.49
Swedish 3 42 13.7 1 8 1.80
Average 4.2 52.7 18.8 1 7.8 2.63
Table 2: Properties of the test set formed by taking
350 Category-1 queries from each language.
guages. Through an investigation of Category-2
non-English queries, we find out that this is mostly
due to the usage of some common internet or
computer terms such as ?download?, ?software?,
?flash player?, among other native language query
terms.
4 Language Identification
We start this section with the implementation of
three models each of which use a different exist-
ing feature. We categorize these models as statis-
tical, knowledge based, and morphological. We
then combine all three models in a machine learn-
ing framework using a novel approach. Finally, we
extend this framework by adding a non-linguistic
feature in order to identify the language intended
by the search engine user.
To train each model implemented, we used the
EuroParl Corpora, (Koehn, 2005), and the same 10
languages in Section 3. EuroParl Corpora is well
balanced, so we would not have any bias towards
a particular language resulting from our choice of
the corpora.
We tested all the systems in this section on a
test set of 3500 human annotated queries, which
is formed by taking 350 Category-1 queries from
each language. All the queries in the test set are
obtained from the evaluation results in Section
3. In Table 2, we give the properties of this test
set. We list the minimum, maximum, and average
number of characters and words (MinC, MaxC,
?C , MinW, MaxW, and ?W respectively).
As can be seen in Table 2, the queries in our test
set have 18.8 characters on average, which is much
lower than the threshold suggested by the existing
systems to achieve a good accuracy. Another in-
teresting fact about the test set is that, languages
which are in the bottom half of Table 2 (German,
Dutch, Danish, Finnish, and Swedish) have lower
number of characters and words on average com-
pared to the languages in the upper half. This
is due to the characteristics of those languages,
which allow the construction of composite words
from multiple words, or have a richer morphology.
Thus, the concepts can be expressed in less num-
ber of words or characters.
4.1 Models for Language Identification
We implement a statistical model using a charac-
ter based n-gram feature. For each language, we
collect the n-gram counts (for n = 1 to n = 7
also using the word beginning and ending spaces)
from the vocabulary of the training corpus, and
then generate a probability distribution from these
counts. We implemented this model using the
SRILM Toolkit (Stolcke, 2002) with the mod-
ified Kneser-Ney Discounting and interpolation
options. For comparison purposes, we also imple-
mented the Rank-Order method using the parame-
ters described in (Cavnar and Trenkle, 1994).
For the knowledge based method, we used the
vocabulary of each language obtained from the
training corpora, together with the word counts.
From these counts, we obtained a probability dis-
tribution for all the words in our vocabulary. In
other words, this time we used a word-based n-
gram method, only with n = 1. It should be noted
that increasing the size of n, which might help in
language identification of other types of written
texts, will not be helpful in this task due to the
unique nature of the search engine queries.
For the morphological feature; we gathered the
affix information for each language from the cor-
pora in an unsupervised fashion as described in
(Hammarstro?m, 2006). This method basically
considers each possible morphological segmenta-
tion of the words in the training corpora by as-
suming a high frequency of occurence of salient
affixes, and also assuming that words are made up
of random characters. Each possible affix is as-
signed a score based on its frequency, random ad-
justment, and curve-drop probabilities, which re-
spectively indicate the probability of the affix be-
ing a random sequence, and the probability of be-
ing a valid morphological segment based on the in-
formation of the preceding or the succeding char-
acter. In Table 3, we present the top 10 results of
the probability distributions obtained from the vo-
cabulary of English, Finnish, and German corpora.
We give the performance of each model on
our test set in Table 4. The character based n-
gram model outperforms all the other models with
the exception of French, Spanish, and Italian on
which the word-based unigram model is better.
1070
English Finnish German
-nts 0.133 erityis- 0.216 -ungen 0.172
-ity 0.119 ihmisoikeus- 0.050 -en 0.066
-ised 0.079 -inen 0.038 gesamt- 0.066
-ated 0.075 -iksi 0.037 gemeinschafts- 0.051
-ing 0.069 -iseksi 0.030 verhandlugs- 0.040
-tions 0.069 -ssaan 0.028 agrar- 0.024
-ted 0.048 maatalous- 0.028 su?d- 0.018
-ed 0.047 -aisesta 0.024 menschenrechts- 0.018
-ically 0.041 -iseen 0.023 umwelt- 0.017
-ly 0.040 -amme 0.023 -ches 0.017
Table 3: Top 10 prefixes and suffixes together with
their probabilities, obtained for English, Finnish,
and German.
The word-based unigram model performs poorly
on languages that may have highly inflected or
composite words such as Finnish, Swedish, and
German. This result is expected as we cannot
make sure that the training corpus will include
all the possible inflections or compositions of the
words in the language. The Rank-Order method
performs poorly compared to the character based
n-gram model, which suggests that for shorter
texts, a well-defined probability distribution with a
proper discounting strategy is better than using an
ad-hoc ranking method. The success of the mor-
phological feature depends heavily on the prob-
ability distribution of affixes in each language,
which in turn depends on the corpus due to the un-
supervised affix extraction algorithm. As can be
seen in Table 3, English affixes have a more uni-
form distribution than both Finnish and German.
Each model implemented in the previous sec-
tion has both strengths and weaknesses. The sta-
tistical approach is more robust to noise, such as
misspellings, than the others, however it may fail
to identify short queries or single words because
of the lack of enough evidence, and it may confuse
two languages that are very similar. In such cases,
the knowledge-based model could be more useful,
as it can find those query terms in the vocabulary.
On the other hand, the knowledge-based model
would have a sparse vocabulary for languages that
can have heavily inflected words such as Turkish,
and Finnish. In such cases, the morphological fea-
ture could provide a strong clue for identification
from the affix information of the terms.
4.2 Decision Tree Classification
Noting the fact that each model can complement
the other(s) in certain cases, we combined them by
using a decision tree (DT) classifier. We trained
the classifier using the automatically annotated
data set, which we created in Section 3. Since
this set comes with a certain amount of noise, we
Language Stat. Knowl. Morph. Rank-Order
English 90.3% 83.4% 60.6% 78.0%
French 77.4% 82.0% 4.86% 56.0%
Portuguese 79.7% 75.7% 11.7% 70.3%
Spanish 73.1% 78.3% 2.86% 46.3%
Italian 85.4% 87.1% 43.4% 77.7%
German 78.0% 60.0% 26.6% 58.3%
Dutch 85.7% 64.9% 23.1% 65.1%
Danish 87.7% 67.4% 46.9% 61.7%
Finnish 87.4% 49.4% 38.0% 82.3%
Swedish 81.7% 55.1% 2.0% 56.6%
Average 82.7% 70.3% 26.0% 65.2%
Table 4: Evaluation of the models built from the
individual features, and the Rank-Order method
on the test set.
pruned the DT during the training phase to avoid
overfitting. This way, we built a robust machine
learning framework at a very low cost and without
any human labour.
As the features of our DT classifier, we use the
results of the models that are implemented in Sec-
tion 4.1, together with the confidence scores cal-
culated for each instance. To calculate a confi-
dence score for the models, we note that since
each model makes its selection based on the lan-
guage that gives the highest probability, a confi-
dence score should indicate the relative highness
of that probability compared to the probabilities
of other languages. To calculate this relative high-
ness, we use the Kurtosis measure, which indicates
how peaked or flat the probabilities in a distribu-
tion are compared to a normal distribution. To cal-
culate the Kurtosis value, ?, we use the equation
below.
? =
?
l?L(pl ? ?)
4
(N ? 1)?4
where L is the set of languages, N is the number
of languages in the set, pl is the probability for
language l ? L, and ? and ? are respectively the
mean and the the standard deviation values of P =
{pl|l ? L}.
We calculate a ? measure for the result of each
model, and then discretize it into one of three cat-
egories:
? HIGH: If ? ? (?? + ??)
? MEDIUM: If [? > (?????)?? < (??+??)]
? LOW: If ? ? (?? ? ??)
where ?? and ?? are the mean and the standard
deviation values respectively, for a set of confi-
dence scores calculated for a model on a small de-
velopment set of 25 annotated queries from each
language. For the statistical model, we found
?? = 4.47, and ?? = 1.96, for the knowledge
1071
Language 500 1,000 5,000 10,000
English 78.6% 81.1% 84.3% 85.4%
French 83.4% 85.7% 85.4% 86.6%
Portuguese 81.1% 79.1% 81.7% 81.1%
Spanish 77.4% 79.4% 81.4% 82.3%
Italian 90.6% 89.7% 90.6% 90.0%
German 81.1% 82.3% 83.1% 83.1%
Dutch 86.3% 87.1% 88.3% 87.4%
Danish 86.3% 87.7% 88.0% 88.0%
Finnish 88.3% 88.3% 89.4% 90.3%
Swedish 81.4% 81.4% 81.1% 81.7%
Average 83.5% 84.2% 85.3% 85.6%
Table 5: Evaluation of the Decision Tree Classifier
with varying sizes of training data.
based ?? = 4.69, and ?? = 3.31, and finally for
the morphological model we found ?? = 4.65, and
?? = 2.25.
Hence, for a given query, we calculate the iden-
tification result of each model together with the
model?s confidence score, and then discretize the
confidence score into one of the three categories
described above. Finally, in order to form an as-
sociation between the output of the model and
its confidence, we create a composite attribute by
appending the discretized confidence to the iden-
tified language. As an example, our statistical
model identifies the query ?the sovereign individ-
ual? as English (en), and reports a ? = 7.60,
which is greater than or equal to ??+ ?? = 4.47+
1.96 = 6.43. Therefore the resulting composite
attribute assigned to this query by the statistical
model is ?en-HIGH?.
We used the Weka Machine Learning Toolkit
(Witten and Frank, 2005) to implement our DT
classifier. We trained our system with 500, 1,000,
5,000, and 10,000 instances of the automatically
annotated data and evaluate it on the same test set
of 3500 human-annotated queries. We show the
results in Table 5.
The results in Table 5 show that our DT clas-
sifier, on average, outperforms all the models in
Table 4 for each size of the training data. Fur-
thermore, the performance of the system increases
with the increasing size of training data. In par-
ticular, the improvement that we get for Spanish,
French, and German queries are strikingly good.
This shows that our DT classifier can take ad-
vantage of the complementary features to make
a better classification. The classifier that uses
10,000 instances gets outperformed by the statis-
tical model (by 4.9%) only in the identification of
English queries.
In order to evaluate the significance of our im-
provement, we performed a paired t-test, with a
null hypothesis and ? = 0.01 on the outputs of
da de en es fi fr it nl sv pt
da 308 4 9 0 2 3 1 7 14 2
de 7 291 6 2 4 4 5 19 9 3
en 6 8 299 3 3 9 4 5 8 5
es 3 2 4 288 2 2 10 1 1 37
fi 0 5 3 4 316 1 7 4 7 3
fr 2 7 6 3 2 303 10 7 2 8
it 0 1 2 7 4 4 315 2 1 14
nl 5 8 8 4 6 4 4 306 4 1
sv 24 8 6 5 6 2 2 6 286 5
pt 0 1 3 41 1 4 13 2 1 284
Figure 2: Confusion Matrix for the Decision Tree
Classifier that uses 10,000 training instances.
the statistical model, and the DT classifier that
uses 10,000 training instances. The test resulted
in P = 1.12?10  ?, which strongly indicates
that the improvement of the DT classifier over the
statistical model is statistically significant.
In order to illustrate the errors made by our DT
classifier, we show the confusion matrixM in Fig-
ure 2. The matrix entry Mli,lj simply gives the
number of test instances that are in language li but
misclassified by the system as lj . From the figure,
we can infer that, Portuguese and Spanish are the
languages that are confused mostly by the system.
This is an expected result because of the high sim-
ilarity between the two languages.
4.3 Towards Identifying the Language Intent
As a final step in our study, we build another DT
classifier by introducing a non-linguistic feature
to our system, which is the language information
of the country from which the user entered the
query.5 Our intuition behind introducing this extra
feature is to help the search engine in guessing the
language in which the user wants to see the result-
ing web pages. Since the real purpose of a search
engine is to bring the expected results to its users,
we believe that a correct identification of the lan-
guage that the user intended for the results when
typing the query is an important first part of this
process.
To illustrate this with an example, we con-
sider the query, ?how to tape for plantar fasci-
itis?, which we selected among the 500 human-
annotated queries retrieved from the United States
web site. This query is labelled as Category-2 by
the human annotator. Our DT classifier, together
with the statistical and knowledge-based models,
classifies this query falsely as a Porteguese query,
which is most likely caused due to the presence of
the Latin phrase ?plantar fasciitis?.
In order to test the effectiveness of our new fea-
ture, we introduce all the Category-2 queries to our
5For countries, where the number of official languages is
more than one, we simply pick the first one listed in our table.
1072
Language New Feat. Classifier-1 Classifier-2
English 74.9% 82.8% 89.5%
French 77.0% 85.6% 93.7%
Portuguese 79.1% 78.1% 93.3%
Spanish 84.1% 80.7% 94.2%
Italian 90.6% 86.7% 96.3%
German 80.2% 80.7% 94.2%
Dutch 91.6% 85.8% 95.3%
Danish 88.6% 87.0% 94.9%
Finnish 94.0% 87.7% 97.9%
Swedish 87.9% 80.9% 95.3%
Average 85.0% 83.6% 94.5%
Table 6: Evaluation of the new feature and the two
decision tree classifiers on the new test set.
test set and increase its size to 430 queries for each
language.6 Then we run both classifiers, with and
without the new feature, using a training data size
of 10,000 instances, and display the results in Ta-
ble 6. We also show the contribution of the new
feature as a standalone classifier in the first col-
umn of Table 6. We labeled the DT classifier that
we implemented in Section 4.2 as ?Classifier-1?
and the new one as ?Classifier-2?.
Interestingly, the results in Table 6 tell us that a
search engine can achieve a better accuracy than
Classifier-1 on average, should it decide to bring
the results based only on the geographical infor-
mation of its users. However one can argue that
this would be a bad idea for the web sites that re-
ceive a lot of visitors from all over the world, and
also are visited very often. For example, if the
search engine?s United States web site, which is
considered as one of the most important markets
in the world, was to employ such an approach, it?d
only receive 74.9% accuracy by misclassifying the
English queries entered from countries for which
the default language is not English. On the other
hand, when this geographical information is used
as a feature in our decision tree framework, we get
a very high boost on the accuracy of the results
for all the languages. As can be seen in Table 6,
Classifier-2 gives the best results.
5 Conclusions and Future Work
In this paper, we considered the language identi-
fication problem for search engine queries. First,
we presented a completely automated method to
generate a reliable data set with language anno-
tations that can be used to train a decision tree
classifier. Second, we implemented three features
used in the existing language identification meth-
6We don?t have equal number of Category-2 queries in
each language. For example, English has only 18 of them
whereas Italian has 71. Hence the resulting data set won?t be
balanced in terms of this category.
ods, and compared their performance. Next, we
built a decision tree classifier that improves the re-
sults on average by combining the outputs of the
three models together with their confidence scores.
Finally, we considered the practical application of
this problem for search engines, and built a second
classifier that takes into account the geographical
information of the users.
Human annotations on 5000 automatically an-
notated queries showed that our data generation
method is highly accurate, achieving 84.3% accu-
racy on average for Category-1 queries, and 93.7%
accuracy for Category-1 and Category-2 queries
combined. Furthermore, the process is fast as we
can get a data set of size approximately 50,000
queries in a few hours by using only 15 computers
in a cluster.
The decision tree classifier that we built for the
textual language identification in Section 4.2 out-
performs all three models that we implemented in
Section 4.1, for all the languages except English,
for which the statistical model is better by 4.9%,
and Swedish, for which we get a tie. Introducing
the geographical information feature to our deci-
sion tree framework boosts the accuracy greatly
even in the case of a noisier test set. This sug-
gests that the search engines can do a better job in
presenting the results to their users by taking the
non-linguistic features into account in identifying
the intended language of the queries.
In future, we would like to improve the accu-
racy of our data generation system by considering
additional features proposed in the studies of au-
tomated query taxonomy, and doing a more care-
ful examination in the assignment of the parameter
values. We are also planning to extend the num-
ber of languages in our data set. Furthermore, we
would like to improve the accuracy of Classifier-
2 with additional non-linguistic features. Finally,
we will consider other alternatives to the decision
tree framework when combining the results of the
models with their confidence scores.
6 Acknowledgments
We are grateful to Romain Vinot, and Rada Mi-
halcea, for their comments on an earlier draft of
this paper. We also would like to thank Sriram
Cherukiri for his contributions during the course
of this project. Finally, many thanks to Murat Bir-
inci, and Sec?kin Kara, for their help on the data an-
notation process, and Cem So?zgen for his remarks
on the SQL formulations.
1073
References
C. Biemann and S. Teresniak. 2005. Disentangling
from babylonian confusion - unsupervised language
identification. In Proceedings of CICLing-2005,
Computational Linguistics and Intelligent Text Pro-
cessing, pages 762?773. Springer.
Andrei Broder. 2002. A taxonomy of web search. SI-
GIR Forum, 36(2):3?10.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, US.
J. Cowie, Y. Ludovic, and R. Zacharski. 1999. Lan-
guage recognition for mono- and multi-lingual docu-
ments. In Proceedings of Vextal Conference, Venice,
Italy.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab (CRL), New Mexico State Uni-
versity.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of JADT-95,
3rd International Conference on the Statistical Anal-
ysis of Textual Data, Rome, Italy.
Harald Hammarstro?m. 2006. A naive theory of affix-
ation and an algorithm for extraction. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology at HLT-NAACL 2006, pages 79?88, New
York City, USA, June. Association for Computa-
tional Linguistics.
Harald Hammarstro?m. 2007. A fine-grained model for
language identification. In F. Lazarinis, J. Vilares,
J. Tait (eds) Improving Non-English Web Searching
(iNEWS07) SIGIR07 Workshop, pages 14?20.
B. Hughes, T. Baldwin, S. G. Bird, J. Nicholson, and
A. Mackinlay. 2006. Reconsidering language iden-
tification for written language resources. In 5th In-
ternational Conference on Language Resources and
Evaluation (LREC2006), Genoa, Italy.
Norman C Ingle. 1976. A language identification ta-
ble. The Incorporated Linguist, 15(4):98?101.
Bernard J. Jansen, Danielle L. Booth, and Amanda
Spink. 2008. Determining the informational, navi-
gational, and transactional intent of web queries. Inf.
Process. Manage., 44(3):1251?1266.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, Phuket, Thai-
land, pages 79?86.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In
In Proceedings of the 5th International Symposium
on Communications and Information Technologies
(ISCIT-2005, pages 896?899.
Uichin Lee, Zhenyu Liu, and Junghoo Cho. 2005. Au-
tomatic identification of user goals in web search.
In WWW ?05: Proceedings of the 14th international
conference on World Wide Web, pages 391?400,
New York, NY, USA. ACM.
Ernesto William De Luca Lena Grothe and Andreas
Nrnberger. 2008. A comparative study on lan-
guage identification methods. In Proceedings of the
Sixth International Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco, May. Eu-
ropean Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Ryan Levering and Michal Cutler. 2006. The portrait
of a common html web page. In DocEng ?06: Pro-
ceedings of the 2006 ACM symposium on Document
engineering, pages 198?204, New York, NY, USA.
ACM Press.
Yiqun Liu, Min Zhang, Liyun Ru, and Shaoping Ma.
2006. Automatic query type identification based on
click through information. In AIRS, pages 593?600.
Arjen Poutsma. 2001. Applying monte carlo tech-
niques to language identification. In In Proceed-
ings of Computational Linguistics in the Nether-
lands (CLIN).
John M. Prager. 1999. Linguini: Language identifi-
cation for multilingual documents. In HICSS ?99:
Proceedings of the Thirty-Second Annual Hawaii In-
ternational Conference on System Sciences-Volume
2, page 2035, Washington, DC, USA. IEEE Com-
puter Society.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In WWW ?04:
Proceedings of the 13th international conference on
World Wide Web, pages 13?19, New York, NY, USA.
ACM.
Penelope Sibun and Jeffrey C. Reynar. 1996. Lan-
guage identification: Examining the issues. In
5th Symposium on Document Analysis and Informa-
tion Retrieval, pages 125?135, Las Vegas, Nevada,
U.S.A.
Craig Silverstein, Hannes Marais, Monika Henzinger,
and Michael Moricz. 1999. Analysis of a very
large web search engine query log. SIGIR Forum,
33(1):6?12.
C. Souter, G. Churcher, J. Hayes, and J. Hughes. 1994.
Natural language identification using corpus-based
models. Hermes Journal of Linguistics, 13:183?
203.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, volume 2, pages 901?904,
Denver, CO.
Hidayet Takci and Ibrahim Sogukpinar. 2004.
Centroid-based language identification using letter
feature set. In CICLing, pages 640?648.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
1074
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903?911,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Quantifying the Limits and Success of Extractive Summarization Systems
Across Domains
Hakan Ceylan and Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
{hakan,rada}@unt.edu
Umut ?Ozertem
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
umut@yahoo-inc.com
Elena Lloret and Manuel Palomar
Department of
Software and Computing Systems
University of Alicante
San Vicente del Raspeig
Alicante 03690, Spain
{elloret,mpalomar}@dlsi.ua.es
Abstract
This paper analyzes the topic identification
stage of single-document automatic text sum-
marization across four different domains, con-
sisting of newswire, literary, scientific and le-
gal documents. We present a study that ex-
plores the summary space of each domain
via an exhaustive search strategy, and finds
the probability density function (pdf) of the
ROUGE score distributions for each domain.
We then use this pdf to calculate the per-
centile rank of extractive summarization sys-
tems. Our results introduce a new way to
judge the success of automatic summarization
systems and bring quantified explanations to
questions such as why it was so hard for the
systems to date to have a statistically signifi-
cant improvement over the lead baseline in the
news domain.
1 Introduction
Topic identification is the first stage of the gener-
ally accepted three-phase model in automatic text
summarization, in which the goal is to identify the
most important units in a document, i.e., phrases,
sentences, or paragraphs (Hovy and Lin, 1999; Lin,
1999; Sparck-Jones, 1999). This stage is followed
by the topic interpretation and summary generation
steps where the identified units are further processed
to bring the summary into a coherent, human read-
able abstract form. The extractive summarization
systems, however, only employ the topic identifi-
cation stage, and simply output a ranked list of the
units according to a compression ratio criterion. In
general, for most systems sentences are the preferred
units in this stage, as they are the smallest grammat-
ical units that can express a statement.
Since the sentences in a document are reproduced
verbatim in extractive summaries, it is theoretically
possible to explore the search space of this problem
through an enumeration of all possible extracts for
a document. Such an exploration would not only
allow us to see how far we can go with extractive
summarization, but we would also be able to judge
the difficulty of the problem by looking at the dis-
tribution of the evaluation scores for the generated
extracts. Moreover, the high scoring extracts could
also be used to train a machine learning algorithm.
However, such an enumeration strategy has an
exponential complexity as it requires all possible
sentence combinations of a document to be gener-
ated, constrained by a given word or sentence length.
Thus the problem quickly becomes impractical as
the number of sentences in a document increases and
the compression ratio decreases. In this work, we try
to overcome this bottleneck by using a large cluster
of computers, and decomposing the task into smaller
problems by using the given section boundaries or a
linear text segmentation method. As a result of this
exploration, we generate a probability density func-
tion (pdf) of the ROUGE score (Lin, 2004) distri-
butions for four different domains, which shows the
distribution of the evaluation scores for the gener-
ated extracts, and allows us to assess the difficulty
of each domain for extractive summarization.
Furthermore, using these pdfs, we introduce a
new success measure for extractive summarization
systems. Namely, given a system?s average score
over a data set, we show how to calculate the per-
903
centile rank of this system from the corresponding
pdf of the data set. This allows us to see the true
improvement a system achieves over another, such
as a baseline, and provides a standardized scoring
scheme for systems performing on the same data set.
2 Related Work
Despite the large amount of work in automatic
text summarization, there are only a few studies
in the literature that employ an exhaustive search
strategy to create extracts, which is mainly due to
the prohibitively large search space of the prob-
lem. Furthermore, the research regarding the align-
ment of abstracts to original documents has shown
great variations across domains (Kupiec et al, 1995;
Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;
Ceylan and Mihalcea, 2009), which indicates that
the extractive summarization techniques are not ap-
plicable to all domains at the same level.
In order to automate the process of corpus
construction for automatic summarization systems,
(Marcu, 1999) used exhaustive search to generate
the best Extract from a given (Abstract, Text) tuple,
where the best Extract contains a set of clauses from
Text that have the highest similarity to the given Ab-
stract.
In addition, (Donaway et al, 2000) used exhaus-
tive search to create all the sentence extracts of
length three starting with 15 TREC Documents, in
order to judge the performance of several summary
evaluation measures suggested in their paper.
Finally, the study most similar to ours was done
by (Lin and Hovy, 2003), who used the articles with
less than 30 sentences from the DUC 2001 data set
to find oracle extracts of 100 and 150 (?5) words.
These extracts were compared against one summary
source, selected as the one that gave the highest
inter-human agreement. Although it was concluded
that a 10% improvement was possible for extrac-
tive summarization systems, which typically score
around the lead baseline, there was no report on how
difficult it would be to achieve this improvement,
which is the main objective of our paper.
3 Description of the Data Set
Our data set is composed of four different domains:
newswire, literary, scientific and legal. For all the
Domain ?Dw ?Sw ?R ?C ?Cw
Newswire 641 101 84% 1 641
Literary 4973 1148 77% 6 196
Scientific 1989 160 92% 9 221
Legal 3469 865 75% 18 192
Table 1: Statistical properties of the data set. ?Dw, and
?Sw represent the average number of words for each doc-
ument and summary respectively; ?R indicates the av-
erage compression ratio; and ?C and ?Cw represent the
average number of sections for each document, and the
average number of words for each section respectively.
domains we used 50 documents and only one sum-
mary for each document, except for newswire where
we used two summaries per document. For the
newswire domain, we selected the articles and their
summaries from the DUC 2002 data set,1. For the
literary domain, we obtained 10 novels that are lit-
erature classics, and available online in text format.
Further, we collected the corresponding summaries
for these novels from various websites such as
CliffsNotes (www.cliffsnotes.com) and SparkNotes
(www.sparknotes.com), which make available hu-
man generated abstracts for literary works. These
sources give a summary for each chapter of the
novel, so each chapter can be treated as a sepa-
rate document. Thus we evaluate 50 chapters in to-
tal. For the scientific domain, we selected the ar-
ticles from the medical journal Autoimmunity Re-
views2 were selected, and their abstracts are used
as summaries. Finally, for the legal domain, we
gathered 50 law documents and their corresponding
summaries from the European Legislation Website,3
which comprises four types of laws - Council Di-
rectives, Acts, Communications, and Decisions over
several topics, such as society, environment, educa-
tion, economics and employment.
Although all the summaries are human generated
abstracts for all the domains, it is worth mention-
ing that the documents and their corresponding sum-
maries exhibit a specific writing style for each do-
main, in terms of the vocabulary used and the length
of the sentences. We list some of the statistical prop-
erties of each domain in Table 1.
1http://www-nlpir.nist.gov/projects/duc/data.html
2http://www.elsevier.com/wps/product/cws home/622356
3http://eur-lex.europa.eu/en/legis/index.htm
904
4 Experimental Setup
As mentioned in Section 1, an exhaustive search
algorithm requires generating all possible sentence
combinations from a document, and evaluating each
one individually. For example, using the values from
Table 1, and assuming 20 words per sentence, we
find that the search space for the news domain con-
tains approximately
(32
5
)
? 50 = 10, 068, 800 sum-
maries. The same calculation method for the sci-
entific domain gives us
(99
8
)
? 50 = 8.56 ? 1012
summaries. Obviously the search space gets much
bigger for the legal and literary domains due to their
larger text size.
In order to be able to cope with such a huge
search space, the first thing we did was to modify
the ROUGE 1.5.54 Perl script by fixing the parame-
ters to those used in the DUC experiments,5 and also
by modifying the way it handles the input and output
to make it suitable for streaming on the cluster.
The resulting script evaluates around 25-30 sum-
maries per second on an Intel 2.33 GHz processor.
Next, we streamed the resulting ROUGE script for
each (document, summary) pair on a large cluster
of computers running on an Hadoop Map-Reduce
framework.6 Based on the size of the search space
for a (document, summary) pair, the number of com-
puters allocated in the cluster ranged from just a few
to more than one thousand.
Although the combination of a large cluster and a
faster ROUGE is enough to handle most of the doc-
uments in the news domain in just a few hours, a
simple calculation shows that the problem is still im-
practical for the other domains. Hence for the scien-
tific, legal, and literary domains, rather than consid-
ering each document as a whole, we divide them into
sections, and create extracts for each section such
that the length of the extract is proportional to the
length of the section in the original document. For
the legal and scientific domains, we use the given
section boundaries (without considering the subsec-
tions for scientific documents). For the novels, we
treat each chapter as a single document (since each
chapter has its own summary), which is further di-
vided into sections using a publicly available linear
4http://berouge.com
5
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0
6http://hadoop.apache.org/
text segmentation algorithm by (Utiyama and Isa-
hara, 2001).7 In all cases, we let the algorithm pick
the number of segments automatically.
To evaluate the sections, we modified ROUGE
further so that it applies the length constraint to the
extracts only, not to the model summaries. This is
due to the fact that we evaluate the extracts of each
section individually against the whole model sum-
mary, which is larger than the extract. This way,
we can get an overall ROUGE recall score for a
document extract, simply by summing up the re-
call scores of each section extracts. The precision
score for the entire document can also be found by
adding the weighted precision scores for each sec-
tion, where the weight is proportional to the length
of the section in the original document. In our study,
however, we only use recall scores.
Note that, since for the legal, scientific, and lit-
erary domains we consider each section of a doc-
ument independently, we are not performing a true
exhaustive search for these domains, but rather solv-
ing a suboptimal problem, as we divide the number
of words in the model summary to each section pro-
portional to the section?s length. However, we be-
lieve that this is a fair assumption, as it has been
shown repeatedly in the past that text segmentation
helps improving the performance of text summariza-
tion systems (yen Kan et al, 1998; Nakao, 2000;
Mihalcea and Ceylan, 2007).
5 Exhaustive Search Algorithm
Let Eik = Si1 , Si2 , ..., Sik be the ith extract that
has k sentences, and generated from a document
D with n sentences D = S1, S2, . . . , Sn. Further,
let len(Sj) give the number of words in sentence
Sj . We enforce that Eik satisfies the following con-
straints:
len(Eik) = len(Si1) + . . . + len(Sik) ? L
len(Eik?1) = len(Si1) + . . . + len(Sik?1) < L
where L is the length constraint on all the extracts
of document D. We note that for any Eik , the or-
der of the sentences in Eik?1 does not affect the
ROUGE scores, since only the last sentence may be
7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-
1.211.tar.gz
905
chopped off due to the length constraint.8 Hence, we
start generating sentence combinations
(n
r
)
in lexico-
graphic order, for r = 1...n, and for each combina-
tion Eik = Si1 , Si2 , ..., Sik where k > 1, we gener-
ate additional extracts E?ik by successfully swapping
Sij with Sik for j = 1, ..., k? 1 and checking to see
if the above constraints are still satisfied. Therefore
from a combination with k sentences that satisfies
the constraints, we might generate up to k ? 1 ad-
ditional extracts. Finally, we stop the process either
when r = n and the last combination is generated,
or we cannot find any extract that satisfies the con-
straints for r.
6 Generating pdfs
Once the extracts for a document are generated and
evaluated, we go through each result and assign its
recall score to a range, which we refer to as a bin.
We use 1, 000 equally spaced bins between 0 and
1. As an example, a recall score of 0.46873 would
be assigned to the bin [0.468, 0.469]. By keeping
a count for each bin, we are in fact building a his-
togram of scores for the document. Let this his-
togram be h, and h[j] be the value in the jth bin of
the histogram. We then define the normalized his-
togram h? as:
h?[j] =
N
?N
i=1 h[j]
h[j] (1)
where N = 1, 000 is the number of bins in the his-
togram. Note that since the width of each bin is 1N ,
the Riemann sum of the normalized histogram h? is
equal to 1, so h? can be used as an approximation
to the underlying pdf. As an example, we show the
histogram h? for the newswire document AP890323-
0218 in Figure 1.
We combine the normalized histograms of all the
documents in a domain in order to find the pdf for
that domain. This requires multiplying the value
of each bin in a document?s histogram, with all
the other possible combinations of bin values taken
from each of the remaining histograms, and assign-
ing the result to the average bin for each combina-
8Note that we do not take the coherence of extracts into ac-
count, i.e. the sentences in an extract do not need to be sorted
in order of their appearance in the original document. We also
do not change the position of the words in a sentence.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  100  200  300  400  500  600  700  800  900  1000
"AP890323-0218.dat"
Figure 1: The normalized histogram h? of ROUGE-1 re-
call scores for the newswire document AP890323-0218.
tion. This can be done iteratively by keeping a mov-
ing average. We illustrate this procedure in Algo-
rithm 1, where K represents the number of docu-
ments in a domain.
Algorithm 1 Combine h?i?s for i = 1, . . . ,K to cre-
ate hd, the histogram for domain d.
1: hd := {}
2: for i = 1 to N do
3: hd[i] := h?1[i]
4: end for
5: for i = 2 to K do
6: ht = {}
7: for j = 1 to N do
8: for k = 1 to N do
9: a = round(((k ? (i? 1)) + j)/i)
10: ht[a] = ht[a] + (hd[k] ? h?i[j])
11: end for
12: end for
13: hd := ht
14: end for
The resulting histogram hd, when normalized us-
ing Equation 1, is an approximation to the pdf for
domain d. Furthermore, we used the round() func-
tion in line 9, which rounds a number to the nearest
integer, as the bins are indexed by integers. Note
that this rounding introduces an error, which is dis-
tributed uniformly due to the nature of the round()
function. It is also possible to lower the affect of this
error with higher resolutions (i.e. larger number of
bins). In Figure 2, we show a sample hd, obtained
by combining 10 documents from the newswire do-
906
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
"newswire_10-ROUGE-1.dat"
Figure 2: An example pdf obtained by combining 10 doc-
ument histograms of ROUGE-1 recall scores from the
newswire domain. The x-axis is normalized to [0,1].
main.
Recall from Section 4 that the documents in
the literary, legal, and scientific domains are di-
vided into sections either by using the given section
boundaries or by applying a text segmentation al-
gorithm, and the extracts of each section are then
evaluated individually. Hence for these domains, we
first calculate the histogram of each section individ-
ually, and then combine them to find the histogram
of a document. The combination procedure for the
section histograms is similar to Algorithm 1, except
that in this case we do not keep a moving average,
but rather sum up the bins of the sections. Note
that when bin i and j are added, the resulting val-
ues should be expected to be half the times in bin
i + j, and half the times in i + j ? 1.
7 Calculating Percentile Ranks
Given a pdf for a domain, the success of a system
having a ROUGE recall score of S could be sim-
ply measured by finding the area bounded by S.
This gives us the percentile rank of the system in
the overall distribution. Assuming 0 ? S ? 1, let
S? = ?N ?S?, then the formula to calculate the per-
centile rank can be simply given as:
PR(S) =
100
N
bS?
i=1
h?d[i] (2)
ROUGE-1
Domain ? ? max min
Newswire 39.39 0.87 65.70 20.20
Literary 45.20 0.47 63.90 28.40
Scientific 45.99 0.68 71.90 24.20
Legal 72.82 0.28 82.40 62.80
ROUGE-2
Domain ? ? max min
Newswire 11.57 0.79 37.40 1.60
Literary 5.41 0.34 16.90 1.80
Scientific 10.98 0.60 33.30 1.30
Legal 28.74 0.29 40.90 19.60
ROUGE-SU4
Domain ? ? max min
Newswire 15.33 0.69 38.10 6.40
Literary 13.28 0.30 24.30 6.90
Scientific 16.13 0.50 35.80 6.20
Legal 35.63 0.25 45.70 28.70
Table 2: Statistical properties of the pdfs
8 Results
The ensemble distributions of ROUGE-1 recall
scores per document are shown in Figure 3. The
ensemble distributions tell us that the performance
of the extracts, especially for the news and the sci-
entific domains, are mostly uniform for each docu-
ment. This is due to the fact that documents in these
domains, and their corresponding summaries, are
written with a certain conventional style. There is
however a little scattering in the distributions of the
literary and the legal domains. This is an expected
result for the literary domain, as there is no specific
summarization style for these documents, but some-
how surprising for the legal domain, where the effect
is probably due to the different types of legal docu-
ments in the data set.
The pdf plots resulting from the ROUGE-1 recall
scores are shown in Figure 4.9 In order to analyze
the pdf plots, and better understand their differences,
Table 2 lists the mean (?) and the standard deviation
(?) measures of the pdfs, as well as the average min-
imum and maximum scores that an extractive sum-
marization system can get for each domain.
By looking at the pdf plots and the minimum and
maximum columns from Table 2, we notice that for
9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,
even if at a different scale.
907
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45  50
"Ensemble-Newswire-50-ROUGE-1.dat"
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Literary-50-Ensemble-ROUGE-1.dat"
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Medical-50-Ensemble-ROUGE-1.dat"
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Legal-50-Ensemble-ROUGE-1.dat"
Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,
respectively from left to right.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0.36  0.38  0.4  0.42  0.44
"Newswire-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Literary-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Medical-50-ROUGE-1.dat"
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.7  0.72  0.74  0.76  0.78  0.8
"Legal-50-ROUGE-1.dat"
Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and Legal
Domains, respectively from left to right. The resolution of the x-axis is increased to 0.1.
all the domains, the pdfs are long-tailed distribu-
tions. This immediately implies that most of the
extracts in a summary space are clustered around
the mean, which means that for automatic summa-
rization systems, it is very easy to get scores around
this range. Furthermore, we can judge the hardness
of each domain by looking at the standard devia-
tion values. A lower standard deviation indicates a
steeper curve, which implies that improving a sys-
tem would be harder. From the table, we can in-
fer that the legal domain is the hardest while the
newswire is the easiest.
Comparing Table 2 with the values in Table 1,
we also notice that the compression ratio affects the
performance differently for each domain. For ex-
ample, although the scientific domain has the high-
est compression ratio, it has a higher mean than
the literary and the newswire domains for ROUGE-
1 and ROUGE-SU4 recall scores. This implies
that although the abstracts of the medical journals
are highly compressed, they have a high overlap
with the document, probably caused by their writ-
ing style. This was in fact confirmed earlier by the
experiments in (Kupiec et al, 1995), where it was
found out that for a data set of 188 scientific arti-
cles, 79% of the sentences in the abstracts could be
perfectly matched with the sentences in the corre-
sponding documents.
Next, we confirm our experiments by testing three
different extractive summarization systems on our
data set. The first system that we implement is called
Random, and gives a random score between 1 and
100 to each sentence in a document, and then se-
lects the top scoring sentences. The second system,
Lead, implements the lead baseline method which
takes the first k sentences of a document until the
length limit is reached. Finally, the last system that
we implement is TextRank, which uses a variation of
the PageRank graph centrality algorithm in order to
identify the most important sentences in a document
(Page et al, 1999; Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). We selected TextRank as it has a
performance competitive with the top systems par-
ticipating in DUC ?02 (Mihalcea and Tarau, 2004).
We would also like to mention that for the literary,
scientific, and legal domains, the systems apply the
algorithms for each section and each section is eval-
uated independently, and their resulting recall scores
are summed up. This is needed in order to be con-
sistent with our exhaustive search experiments.
The ROUGE recall scores of the three systems are
shown in Table 3. As expected, for the literary and
legal domains, the Random, and the Lead systems
score around the mean. This is due to the fact that
the leading sentences for these two domains do not
indicate any significance, hence the Lead system just
behaves like Random. However for the scientific and
newswire domains, the leading sentences do have
908
ROUGE-1
Domain Random Lead TextRank
Newswire 39.13 45.63 44.43
Literary 45.39 45.36 46.12
Scientific 45.75 47.18 49.26
Legal 73.04 72.42 74.82
ROUGE-2
Domain Random Lead TextRank
Newswire 11.39 19.60 17.99
Literary 5.33 5.41 5.92
Scientific 10.73 12.07 12.76
Legal 28.56 28.92 31.06
ROUGE-SU4
Domain Random Lead TextRank
Newswire 15.07 21.58 20.46
Literary 13.21 13.28 13.81
Scientific 15.92 17.12 17.85
Legal 35.41 35.55 37.64
Table 3: ROUGE recall scores of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
importance so the Lead system consistently outper-
forms Random. Furthermore, although TextRank is
the best system for the literary, scientific, and legal
domains, it gets outperformed by the Lead system
on the newswire domain. This is also an expected re-
sult as none of the single-document summarization
systems were able to achieve a statistically signifi-
cant improvement over the lead baseline in the previ-
ous Document Understanding Conferences (DUC).
The ROUGE scoring scheme does not tell us how
much improvement a system achieved over another,
or how far it is from the upper bound. Since we now
have access to the pdf of each domain in our data set,
we can find this information simply by calculating
the percentile rank of each system using the formula
given in Equation 2.
The percentile ranks of all three systems for each
domain are shown in Table 4. Notice how different
the gap is between the scores of each system this
time, compared to the scores in Table 3. For ex-
ample, we see in Table 3 that TextRank on scientific
domain has only a 3.51 ROUGE-1 score improve-
ment over a system that randomly selects sentences
to include in the extract. However, Table 4 tells us
that this improvement is in fact 57.57%.
From Table 4, we see that both TextRank and
the Lead system are in the 99.99% percentile of
ROUGE-1
Domain Random Lead TextRank
Newswire %39.18 %99.99 %99.99
Literary %62.89 %62.89 %97.90
Scientific %42.30 %95.56 %99.87
Legal %79.47 %16.19 %99.99
ROUGE-2
Domain Random Lead TextRank
Newswire %39.57 %99.99 %99.99
Literary %42.20 %54.32 %94.34
Scientific %35.6 %96.03 %99.79
Legal %36.68 %75.38 %99.99
ROUGE-SU4
Domain Random Lead TextRank
Newswire %40.68 %99.99 %99.99
Literary %46.39 %46.39 %96.84
Scientific %36.37 %97.69 %99.94
Legal %23.53 %42.00 %99.99
Table 4: Percentile rankings of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
the newswire domain although the systems have
1.20, 1.61, and 1.12 difference in their ROUGE-1,
ROUGE-2, and ROUGE-SU4 scores respectively.
The high percentile for the Lead system explains
why it was so hard to improve over these baseline in
previous evaluations on newswire data (e.g., see the
evaluations from the Document Understanding Con-
ferences). Furthermore, we see from Table 2 that the
upper bounds corresponding to these scores are 65.7,
37.4, and 38.1 respectively, which are well above
both the TextRank and the Lead systems. There-
fore, the percentile rankings of the Lead and the Tex-
tRank systems for this domain do not seem to give
us clues about how the two systems compare to each
other, nor about their actual distance from the up-
per bounds. There are two reasons for this: First,
as we mentioned earlier, most of the summary space
consists of easy extracts, which make the distribu-
tion long-tailed.10 Therefore even though we have
quite a bit of systems achieving high scores, their
number is negligible compared to the millions of ex-
tracts that are clustered around the mean. Secondly,
we need a higher resolution (i.e. larger number of
bins) in constructing the pdfs in order to be able to
10This also accounts for the fact that even though we might
have two very close ROUGE scores that are not statistically sig-
nificant, their percentile rankings might differ quite a bit.
909
see the difference more clearly between the two sys-
tems. Finally, when comparing two successful sys-
tems using percentile ranks, we believe the use of
error reduction would be more beneficial.
As a final note, we also randomly sampled ex-
tracts from documents in the scientific and legal do-
mains, but this time without considering the section
boundaries and without performing any segmenta-
tion. We kept the number of samples for each doc-
ument equal to the number of extracts we generated
from the same document using a divide-and-conquer
approach. We evaluated the samples using ROUGE-
1 recall scores, and obtained pdfs for each domain
using the same strategy discussed earlier in the pa-
per. The resulting pdfs, although they exhibit simi-
lar characteristics, they have mean values (?) around
10% lower than the ones we listed in Table 2, which
supports the findings from earlier research that seg-
mentation is useful for text summarization.
9 Conclusions and Future Work
In this paper, we described a study that explores the
search space of extractive summaries across four dif-
ferent domains. For the news domain we generated
all possible extracts of the given documents, and
for the literary, scientific, and legal domains we fol-
lowed a divide-and-conquer approach by chunking
the documents into sections, handled each section
independently, and combined the resulting scores at
the end. We then used the distributions of the eval-
uations scores to generate the probability density
functions (pdfs) for each domain. Various statistical
properties of these pdfs helped us asses the difficulty
of each domain. Finally, we introduced a new scor-
ing scheme for automatic text summarization sys-
tems that can be derived from the pdfs. The new
scheme calculates a percentile rank of the ROUGE-
1 recall score of a system, which gives scores in the
range [0-100]. This lets us see how far each sys-
tem is from the upper bound, and thus make a better
comparison among the systems. The new scoring
system showed us that while there is a 20.1% gap
between the upper bound and the lead baseline for
the news domain, closing this gap is difficult, as the
percentile rank of the lead baseline system, 99.99%,
indicates that the system is already very close to the
upper bound.
Furthermore, except for the literary domain, the
percentile rank of the TextRank system is also very
close to the upperbound. This result does not sug-
gest that additional improvements cannot be made
in these domains, but that making further improve-
ments using only extractive summarization will be
considerably difficult. Moreover, in order to see
these future improvements, a higher resolution (i.e.
larger number of bins) will be needed when con-
structing the pdfs.
In all our experiments we used the ROUGE
(Lin, 2004) evaluation package and its ROUGE-
1, ROUGE-2, and ROUGE-SU4 recall scores. We
would like to note that since ROUGE performs its
evaluations based on the n-gram overlap between
the peer and the model summary, it does not take
other summary quality metrics such as coherence
and cohesion into account. However, our goal in this
paper was to analyze the topic-identification stage
only, which concentrates on selecting the right con-
tent from the document to include in the summary,
and the ROUGE scores were found to correlate well
with the human judgments on assessing the content
overlap of summaries.
In the future, we would like to apply a similar ex-
haustive search strategy, but this time with differ-
ent compression ratios, in order to see the impact
of compression ratios on the pdf of each domain.
Furthermore, we would also like to analyze the
high scoring extracts found by the exhaustive search,
in terms of coherence, position and other features.
Such an analysis would allow us to see whether these
extracts exhibit certain properties which could be
used in training machine learning systems.
Acknowledgments
The authors would like to thank the anonymous re-
viewers of NAACL-HLT 2010 for their feedback.
The work of the first author has been partly sup-
ported by an award from Google, Inc. The work of
the fourth and fifth authors has been supported by an
FPI grant (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation, under the project
TEXT-MESS (TIN2006-15265-C06-01) funded by
the Spanish Government, and the project PROME-
TEO Desarrollo de Tcnicas Inteligentes e Interacti-
vas de Minera de Textos (2009/119) from the Valen-
cian Government.
910
References
Hakan Ceylan and Rada Mihalcea. 2009. The decompo-
sition of human-written book summaries. In CICLing
?09: Proceedings of the 10th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 582?593, Berlin, Heidelberg. Springer-
Verlag.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In NAACL-
ANLP 2000 Workshop on Automatic summarization,
pages 69?78, Morristown, NJ, USA. Association for
Computational Linguistics.
G. Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22.
Eduard H. Hovy and Chin Yew Lin. 1999. Automated
text summarization in summarist. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?97. MIT Press.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28(4):527?543.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95: Pro-
ceedings of the 18th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 68?73, New York, NY, USA.
ACM.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In CIKM ?99: Proceedings of the eighth
international conference on Information and knowl-
edge management, pages 55?62, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 137?144, New
York, NY, USA. ACM.
Rada Mihalcea and Hakan Ceylan. 2007. Explorations in
automatic book summarization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 380?
389, Prague, Czech Republic, June. Association for
Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierarchy
detection. In ACL ?00: Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 302?309, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
InfoLab.
Karen Sparck-Jones. 1999. Automatic summarising:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?13. MIT Press.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scallable
Text Summarization, Madrid, Spain, July.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 491?498.
Min yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 1998. Linear segmentation and segment sig-
nificance. In In Proceedings of the 6th International
Workshop of Very Large Corpora, pages 197?205.
911
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 103?108,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An Efficient Indexer for Large N-Gram Corpora
Hakan Ceylan
Department of Computer Science
University of North Texas
Denton, TX 76203
hakan@unt.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Abstract
We introduce a new publicly available tool
that implements efficient indexing and re-
trieval of large N-gram datasets, such as the
Web1T 5-gram corpus. Our tool indexes the
entire Web1T dataset with an index size of
only 100 MB and performs a retrieval of any
N-gram with a single disk access. With an
increased index size of 420 MB and dupli-
cate data, it also allows users to issue wild
card queries provided that the wild cards in the
query are contiguous. Furthermore, we also
implement some of the smoothing algorithms
that are designed specifically for large datasets
and are shown to yield better language mod-
els than the traditional ones on the Web1T 5-
gram corpus (Yuret, 2008). We demonstrate
the effectiveness of our tool and the smooth-
ing algorithms on the English Lexical Substi-
tution task by a simple implementation that
gives considerable improvement over a basic
language model.
1 Introduction
The goal of statistical language modeling is to cap-
ture the properties of a language through a proba-
bility distribution so that the probabilities of word
sequences can be estimated. Since the probability
distribution is built from a corpus of the language
by computing the frequencies of the N-grams found
in the corpus, the data sparsity is always an issue
with the language models. Hence, as it is the case
with many statistical models used in Natural Lan-
guage Processing (NLP), the models give a much
better performance with larger data sets.
However the large data sets, such as the Web1T
5-Gram corpus of (Brants and Franz, 2006), present
a major challenge. The language models built from
these sets cannot fit in memory, hence efficient ac-
cessing of the N-gram frequencies becomes an is-
sue. Trivial methods such as linear or binary search
over the entire dataset in order to access a single
N-gram prove inefficient, as even a binary search
over a single file of 10,000,000 records, which is
the case of the Web1T corpus, requires in the worst
case dlog2(10, 000, 000)e = 24 accesses to the disk
drive.
Since the access to N-grams is costly for these
large data sets, the implementation of further im-
provements such as smoothing algorithms becomes
impractical. In this paper, we overcome this problem
by implementing a novel, publicly available tool1
that employs an indexing strategy that reduces the
access time to any N-gram in the Web1T corpus to a
single disk access. We also make a second contribu-
tion by implementing some of the smoothing models
that take into account the size of the dataset, and are
shown to yield up to 31% perplexity reduction on the
Brown corpus (Yuret, 2008). Our implementation is
space efficient, and provides a fast access to both the
N-gram frequencies, as well as their smoothed prob-
abilities.
2 Related Work
Language modeling toolkits are used extensively for
speech processing, machine translation, and many
other NLP applications. The two of the most pop-
ular toolkits that are also freely available are the
CMU Statistical Language Modeling (SLM) Toolkit
(Clarkson and Rosenfeld, 1997), and the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002). However,
1Our tool can be freely downloaded from the download sec-
tion under http://lit.csci.unt.edu
103
even though these tools represent a great resource
for building language models and applying them to
various problems, they are not designed for very
large corpora, such as the Web1T 5-gram corpus
(Brants and Franz, 2006), hence they do not provide
efficient implementations to access these data sets.
Furthermore, (Yuret, 2008) has recently shown
that the widely popular smoothing algorithms for
language models such as Kneser-Ney (Kneser and
Ney, 1995), Witten-Bell (Witten and Bell, 1991), or
Absolute Discounting do not realize the full poten-
tials of very large corpora, which often come with
missing counts. The reason for the missing counts
is due to the omission of low frequency N-grams in
the corpus. (Yuret, 2008) shows that with a modified
version of Kneser-Ney smoothing algorithm, named
as the Dirichlet-Kneser-Ney, a 31% reduction in per-
plexity can be obtained on the Brown corpus.
A tool similar to ours that uses a hashing tech-
nique in order to provide a fast access to the Web1T
corpus is presented in detail in (Hawker et al, 2007).
The tool provides access to queries with wild card
symbols, and the performance of the tool on 106
queries on a 2.66 GHz processor with 1.5 GBytes
of memory is given approximately as one hour. An-
other tool, Web1T5-Easy, described in (Evert, 2010),
provides indexing of the Web1T corpus via rela-
tional database tables implemented in an SQLite en-
gine. It allows interactive searches on the corpus as
well as collocation discovery. The indexing time of
this tool is reported to be two weeks, while the non-
cached retrieval time is given to be in order of a few
seconds. Other tools that implement a binary search
algorithm as a simpler, yet less efficient method are
also given in (Giuliano et al, 2007; Yuret, 2007).
3 The Web1T 5-gram Corpus
The Web1T 5-gram corpus (Brants and Franz, 2006)
consists of sequences of words (N-grams) and their
associated counts extracted from a Web corpus of
approximately one trillion words. The length of each
sequence, N , ranges from 1 to 5, and the size of the
entire corpus is approximately 88GB (25GB in com-
pressed form). The unigrams form the vocabulary
of the corpus and are stored in a single file which
includes around 13 million tokens and their associ-
ated counts. The remaining N-grams are stored sep-
arately across multiple files in lexicographic order.
For example, there are 977,069,902 distinct trigrams
in the dataset, and they are stored consecutively in
98 files in lexicographic order. Furthermore, each
N-gram file contains 10,000,000 N-grams except the
last one, which contains less. It is also important to
note that N-grams with counts less than 40 are ex-
cluded from the dataset for N = 2, 3, 4, 5, and the
tokens with less than 200 are excluded from the un-
igrams.
4 The Indexer
4.1 B+-trees
We used a B+-tree structure for indexing. A B+-
tree is essentially a balanced search tree where each
node has several children. Indexing large files us-
ing B+ trees is a popular technique implemented
by most database systems today as the underlying
structure for efficient range queries. Although many
variations of B+-trees exist, we use the definition for
primary indexing given in (Salzberg, 1988). There-
fore we assume that the data, which is composed of
records, is only stored in the leaves of the tree and
the internal nodes store only the keys.
The data in the leaves of a B+-tree is grouped
into buckets, where the size of a bucket is deter-
mined by a bucket factor parameter, bkfr. Therefore
at any given time, each bucket can hold a number of
records in the range [1, bkfr]. Similarly, the num-
ber of keys that each internal node can hold is deter-
mined by the order parameter, v. By definition, each
internal node except the root can have any number of
keys in the range [v, 2v], and the root must have at
least one key. Finally, an internal node with k keys
has k + 1 children.
4.2 Mapping Unigrams to Integer Keys
A key in a B+-tree is a lookup value for a record,
and a record in our case is an N-gram together with
its count. Therefore each line of an N-gram file in
the Web1T dataset makes up a record. Since each
N-gram is distinct, it is possible to use the N-gram
itself as a key. However in order to reduce the stor-
age requirements and make the comparisons faster
during a lookup, we map each unigram to an inte-
ger, and form the keys of the records using the inte-
ger values instead of the tokens themselves.2
To map unigrams to integers, we use the unigrams
sorted in lexicographic order and assign an integer
value to each unigram starting from 1. In other
words, if we let the m-tuple U = (t1, t2, ..., tm) rep-
resent all the unigrams sorted in lexicographic order,
2This method does not give optimal storage, for which one
should implement a compression Huffman coding scheme.
104
then for a unigram ti, i gives its key value. The key
of trigram ?ti tj tk? is simply given as ?i j k.? Thus,
the comparison of two keys can be done in a similar
fashion to the comparison of two N-grams; we first
compare the first integer of each key, and in case of
equality, we compare the second integers, and so on.
We stop the comparison as soon as an inequality is
found. If all the comparisons result in equality then
the two keys (N-grams) are equal.
4.3 Searching for a Record
We construct a B+-tree for each N-gram file in the
dataset for N = 2, 3, 4, 5, and keep the key of the
first N-gram for each file in memory. When a query
q is issued, we first find the file that contains q by
comparing the key of q to the keys in memory. Since
this is an in-memory operation, it can be simply
done by performing a binary search. Once the cor-
rect file is found, we then search the B+-tree con-
structed for that file for the N-gram q by using its
key.
As is the case with any binary search tree, a search
in a B+-tree starts at the root level and ends in the
leaves. If we let ri and pj represent a key and a
pointer to the child of an internal node respectively,
for i = 1, 2, ..., k and j = 1, 2, ..., k + 1, then to
search an internal node, including the root, for a key
q, we first find the key rm that satisfies one of the
following:
? (q < rm) ? (m = 1)
? (rm?1 ? q) ? (rm > q) for 1 < m ? k
? (q > rm) ? (m = k)
If one of the first two cases is satisfied, the search
continues on the child node found by following pm,
whereas if the last condition is satisfied, the pointer
pm+1 is followed. Since the keys in an internal node
are sorted, a binary search can be performed to find
rm. Finally, when a leaf node is reached, the entire
bucket is read into memory first, then a record with
a key value of q is searched.
4.4 Constructing a B+-tree
The construction of a B+-tree is performed through
successive record insertions.3 Given a record, we
3Note that this may cause efficiency issues for very large
files as memory might become full during the construction pro-
cess, hence in practice, the file is usually sorted prior to index-
ing.
first compute its key, find the leaf node it is supposed
to be in, and insert it if the bucket is not full. Other-
wise, the leaf node is split into two nodes, each con-
taining dbkfr/2e, and bbkfr/2c+1 records, and the
first key of the node containing the larger key values
is placed into the parent internal node together with
the node?s pointer. The insertion of a key to an in-
ternal node is similar, only this time both split nodes
contain v values, and the middle key value is sent up
to the parent node.
Note that not all the internal nodes of a B+-tree
have to be kept on the disk, and read from there each
time we do a search. In practice, all but the last two
levels of a B+-tree are placed in memory. The rea-
son for this is the high branching factor of the B+-
trees together with their effective storage utilization.
It has been shown in (Yao, 1978) that the nodes of a
high-order B+-tree are ln2 ? 69% full on average.
However, note that the tree will be fixed in our
case, i.e., once it is constructed we will not be in-
serting any other N-gram records. Therefore we do
not need to worry about the 69% space utilization,
but instead try to make each bucket, and each in-
ternal node full. Thus, with a bkfr = 1250, and
v = 100, an N-gram file with 10,000,000 records
would have 8,000 leaf nodes on level 3, 40 inter-
nal nodes on level 2, and the root node on level 1.
Furthermore, let us assume that integers, disk and
memory pointers all hold 8 bytes of space. There-
fore a 5-gram key would require 40 bytes, and a full
internal node in level 2 would require (200x40) +
(201x8) = 9, 608 bytes. Thus the level 2 would re-
quire 9, 608x40 ? 384 Kbytes, and level 1 would
require (40?40)+(41?8) = 1, 928 bytes. Hence, a
Web1T 5-gram file, which has an average size of 286
MB can be indexed with approximately 386 Kbytes.
There are 118 5-gram files in the Web1T dataset, so
we would need 386x118 ? 46 MBytes of memory
space in order to index all of them. A similar calcu-
lation for 4-grams, trigrams, and bigrams for which
the bucket factor values are selected as 1600, 2000,
and 2500 respectively, shows that the entire Web1T
corpus, except unigrams, can be indexed with ap-
proximately 100 MBytes, all of which can be kept
in memory, thereby reducing the disk access to only
one. As a final note, in order to compute a key
for a given N-gram quickly, we keep the unigrams
in memory, and use a hashing scheme for mapping
tokens to integers, which additionally require 178
Mbytes of memory space.
The choice of the bucket factor and the inter-
105
nal node order parameters depend on the hard-disk
speed, and the available memory.4. Recall that even
to fetch a single N-gram record from the disk, the en-
tire bucket needs to be read. Therefore as the bucket
factor parameter is reduced, the size of the index will
grow, but the access time would be faster as long as
the index could be entirely fit in memory. On the
other hand, with a too large bucket factor, although
the index can be made smaller, thereby reducing the
memory requirements, the access time may be un-
acceptable for the application. Note that a random
reading of a bucket of records from the hard-disk
requires the disk head to first go to the location of
the first record, and then do a sequential read.5 As-
suming a hard-disk having an average transfer rate
of 100 MBytes, once the disk head finds the correct
location, a 40 bytes N-gram record can be read in
4x10?7 seconds. Thus, assuming a seek time around
8-10 ms, even with a bucket factor of 1,000, it can be
seen that the seek time is still the dominating factor.
Therefore, as the bucket size gets smaller than 1,000,
even though the index size will grow, there would be
almost no speed up in the access time, which justi-
fies our parameter choices.
4.5 Handling Wild Card Queries
Having described the indexing scheme, and how to
search for a single N-gram record, we now turn our
attention to queries including one or more wild card
symbols, which in our case is the underscore char-
acter ? ?, as it does not exist among the unigram
tokens of the Web1T dataset. We manually add the
wild card symbol to our mapping of tokens to inte-
gers, and map it to the integer 0, so that a search for a
query with a wild card symbol would be unsuccess-
ful but would point to the first record in the file that
replaces the wild card symbol with a real token as
the key for the wild card symbol is guaranteed to be
the smallest. Having found the first record we per-
form a sequential read until the last read record does
not match the query. The reason this strategy works
is because the N-grams are sorted in lexicographic
order in the data set, and also when we map unigram
tokens to integers, we preserve their order, i.e., the
first token in the lexicographically sorted unigram
list is assigned the value 1, the second is assigned
4We used a 7200 RPM disk-drive with an average read seek
time of 8.5 ms, write seek time of 10.0 ms, and a data transfer
time up to 3 GBytes per second.
5A rotational latency should also be taken into account be-
fore the sequential reading can be done.
2, and so forth. For example, for a given query Our
Honorable , the record that would be pointed at the
end of search in the trigram file 3gm-0041 is the N-
gram Our Honorable Court 186, which is the first
N-gram in the data set that starts with the bigram
Our Honorable.
Note however that the methodology that is de-
scribed to handle the queries with wild card sym-
bols will only work if the wild card symbols are
the last tokens of the query and they are contigu-
ous. For example a query such as Our Court will
not work as N-grams satisfying this query are not
stored contiguously in the data set. Therefore in or-
der to handle such queries, we need to store addi-
tional copies of the N-grams sorted in different or-
ders. When the last occurrence of the contiguous
wild card symbols is in position p of a query N-gram
for p = 0, 1, ..., N ? 1, then the N-grams sorted lex-
icographically starting from position (p + 1)modN
needs to be searched. A lexicographical sort for a
position p, for 0 ? p ? (N ? 1) is performed by
moving all the tokens in positions 0...(p ? 1) to the
end for each N-gram in the data set. Thus, for all
the bigrams in the data set, we need one extra copy
sorted in position 1, for all the trigrams, we need
two extra copies; one sorted in position 1, and an-
other sorted in position 2, and so forth. Hence, in
order to handle the contiguous wild card queries in
any position, in addition to the 88 GBytes of origi-
nal Web1T data, we need an extra disk space of 265
GBytes. Furthermore, the indexing cost of the du-
plicate data is an additional 320 MBytes. Thus, the
total disk cost of the system will be approximately
353 GBytes plus the index size of 420 MBytes, and
since we keep the entire index in memory, the final
memory cost of the system will be 420 MBytes +
178 MBytes = 598 MBytes.
4.6 Performance
Given that today?s commodity hardware comes with
at least 4 GBytes of memory and 1 TBytes of hard-
disk space, the requirements of our tool are rea-
sonable. Furthermore, our tool is implemented in
a client-server architecture, and it allows multiple
clients to submit multiple queries to the server over
a network. The server can be queried with an N-
gram query either for its count in the corpus, or
its smoothed probability with a given smoothing
method. The queries with wild cards can ask for
the retrieval of all the N-grams satisfying a query, or
only for the total count so the network overhead can
106
be avoided depending on the application needs.
Our program requires about one day of offline
processing due to resorting the entire data a few
times. Note that some of the files in the corpus
need to be sorted as many as four times. For the
sorting process, the files are first individually sorted,
and then a k-way merge is performed. In our im-
plementation, we used a min heap structure for this
purpose, and k is always chosen as the number of
files for a given N. The index construction however
is relatively fast. It takes about an hour to construct
the index for the 5-grams. Once the offline process-
ing is done, it only takes a few minutes to start the
server, and from that point the online performance
of our tool is very fast. It takes about 1-2 seconds to
process 1000 randomly picked 5-gram queries (with
no wild card symbols), which may or may not exist
in the corpus. For the queries asking for the fre-
quencies only, our tool implements a small caching
mechanism that takes the temporal locality into ac-
count. The mechanism is very useful for wild card
queries involving stop words, such as ?the ?, and
?of the ? which occur frequently, and take a long
time to process due to the sequential read of a large
number of records from the data set.
5 Lexical Substitution
In this section we demonstrate the effectiveness of
our tool by using it on the the English Lexical Sub-
stitution task, which was first introduced in SemEval
2007 (McCarthy and Navigli, 2007). The task re-
quires both the human annotators and the participat-
ing systems to replace a target word in a given sen-
tence with the most appropriate alternatives. The de-
scription of the tasks, the data sets, the performance
of the participating systems as well as a post analy-
sis of the results is given in (McCarthy and Navigli,
2009).
Although the task includes three subtasks, in this
evaluation we are only concerned with one of them,
namely the best subtask. The best subtask asks the
systems and the annotators to provide only one sub-
stitute for the target words ? the most appropriate
one. Two separate datasets were provided with this
task: a trial dataset was first provided in order for
the participants to get familiar with the task and train
their systems. The trial data used a lexical sample of
30 words with 10 instances each. The systems were
then tested on a larger test data, which used a lexical
sample of 171 words each again having 10 instances.
Our methodology for this task is very simple; we
Model Precision Mod Precision
No Smoothing 10.13 14.78
Absolute Discounting 11.05 16.75
KN with Missing Counts 11.19 16.75
Dirichlet KN 10.98 15.76
Table 1: Results on the trial data
Model Precision Mod Precision
No Smoothing 9.01 14.15
Absolute Discounting 11.64 18.62
KN with Missing Counts 11.61 18.54
Dirichlet KN 11.03 17.48
Best Baseline 9.95 15.28
Best SEMEVAL System 12.90 20.65
Table 2: Results on the test data
replace the target word with an alternative from a list
of candidates, and find the probability of the context
with the new word using a language model. The can-
didate that gives the highest probability is provided
as the system?s best guess. The list of candidates is
obtained from two different lexical sources, Word-
Net (Fellbaum, 1998) and Roget?s Thesaurus (The-
saurus.com, 2007). We retrieve all the synonyms
for all the different senses of the word from both re-
sources and combine them. We did not consider any
lexical relations other than synonymy, and similarly
we did not consider any words at a further semantic
distance.
We start with a simple language model that cal-
culates the probability of the context of a word,
and then continue with three smoothing algorithms
discussed in (Yuret, 2008), namely Absolute Dis-
counting, Kneser-Ney with Missing Counts, and the
Dirichlet-Kneser-Ney Discounting. Note that all
three are interpolated models, i.e., they do not just
back-off to a lower order probability when an N-
gram is not found, but rather use the higher and
lower order probabilities all the time in a weighted
fashion.
The results on the trial dataset are shown in Ta-
ble 1, and the results on the test dataset are shown
in Table 2. In all the experiments we use the trigram
models, i.e., we keep N fixed to 3. Since our sys-
tem makes a guess for all the target words in the set,
our precision and recall scores, as well as the mod
precision and the mod recall scores are the same,
so only one from each is shown in the table. Note
that the highest achievable score for this task is not
100%, but is restricted by the frequency of the best
substitute, and it is given as 46.15%. The highest
scoring participating system achieved 12.9%, which
107
gave a 2.95% improvement over the baseline (Yuret,
2008; McCarthy and Navigli, 2009); the scores ob-
tained by the best SEMEVAL system as well as the
best baseline calculated using the synonyms for the
first synset in WordNet are also shown in Table 2.
On both the trial and the test data, we see that the
interpolated smoothing algorithms consistently im-
prove over the naive language modeling, which is
an encouraging result. Perhaps a surprising result
for us was the performance of the Dirichlet-Kneser-
Ney Smoothing Algorithm, which is shown to give
minimum perplexity on the Brown corpus out of the
given models. This might suggest that the parame-
ters of the smoothing algorithms need adjustments
for each task.
It is important to note that this evaluation is meant
as a simple proof of concept to demonstrate the use-
fulness of our indexing tool. We thus used a very
simple approach for lexical substitution, and did not
attempt to integrate several lexical resources and
more sophisticated algorithms, as some of the best
scoring systems did. Despite this, the performance
of our system exceeds the best baseline, and is better
than five out of the eight participating systems (see
(McCarthy and Navigli, 2007)).
6 Conclusions
In this paper we described a new publicly avail-
able tool that provides fast access to large N-gram
datasets with modest hardware requirements. In
addition to providing access to individual N-gram
records, our tool also handles queries with wild card
symbols, provided that the wild cards in the query
are contiguous. Furthermore, the tool also imple-
ments smoothing algorithms that try to overcome
the missing counts that are typical to N-gram cor-
pora due to the omission of low frequencies. We
tested our tool on the English Lexical Substitution
task, and showed that the smoothing algorithms give
an improvement over simple language modeling.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #0917170
and #1018613. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram corpus
version 1. Linguistic Data Consortium.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the cmu-cambridge toolkit. In Pro-
ceedings of ESCA Eurospeech, pages 2707?2710.
S. Evert. 2010. Google web 1t 5-grams made easy (but
not for the computer). In Proceedings of the NAACL
HLT 2010 Sixth Web as Corpus Workshop, WAC-6 ?10,
pages 32?40.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
C. Giuliano, A. Gliozzo, and C. Strapparava. 2007. Fbk-
irst: lexical substitution task exploiting domain and
syntagmatic coherence. In SemEval ?07: Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, pages 145?148.
T. Hawker, M. Gardiner, and A. Bennetts. 2007. Practi-
cal queries of a massive n-gram database. In Proceed-
ings of the Australasian Language Technology Work-
shop 2007, pages 40?48, Melbourne, Australia.
R. Kneser and H. Ney. 1995. Improved backing-off for
n-gram language modeling. In Acoustics, Speech, and
Signal Processing, 1995. ICASSP-95., 1995 Interna-
tional Conference on, volume 1, pages 181?184 vol.1.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In SemEval ?07:
Proceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 48?53.
D. McCarthy and R. Navigli. 2009. The english lexical
substitution task. Language Resources and Evalua-
tion, 43:139?159.
B. Salzberg. 1988. File structures: an analytic ap-
proach. Prentice-Hall, Inc., Upper Saddle River, NJ,
USA.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Thesaurus.com. 2007. Rogets new millennium the-
saurus, first edition (v1.3.1).
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
A. Chi-Chih Yao. 1978. On random 2-3 trees. Acta Inf.,
9:159?170.
D. Yuret. 2007. Ku: word sense disambiguation by sub-
stitution. In SemEval ?07: Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, pages
207?213.
D. Yuret. 2008. Smoothing a tera-word language model.
In HLT ?08: Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
on Human Language Technologies, pages 141?144.
108
