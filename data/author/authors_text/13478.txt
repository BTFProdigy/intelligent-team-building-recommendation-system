Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,
Beijing, August 2010
Semantic Role Labeling for News Tweets 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 3Zhongyang Xiong and 2Changning Huang 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
zyxiong@cqu.edu.cn 
v-cnh@microsoft.com 
 
Abstract 
News tweets that report what is happen-
ing have become an important real-time 
information source. We raise the prob-
lem of Semantic Role Labeling (SRL) 
for news tweets, which is meaningful for 
fine grained information extraction and 
retrieval. We present a self-supervised 
learning approach to train a domain spe-
cific SRL system to resolve the problem. 
A large volume of training data is auto-
matically labeled, by leveraging the ex-
isting SRL system on news domain and 
content similarity between news and 
news tweets. On a human annotated test 
set, our system achieves  state-of-the-art 
performance, outperforming the SRL 
system trained on news. 
1 Introduction 
Tweets are text messages up to 140 characters. 
Every day, more than 50 million tweets are gen-
erated by millions of Twitter users. According to 
the investigation by Pear Analytics (2009), about 
4% tweets are related to news1. 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
1 http://blog.twitter.com/2010/02/measuring-tweets.html 
We divide news related tweets into two cate-
gories: those excerpted from news articles and 
those not. The former kind of tweets, hereafter 
called news excerpt, is formally written while 
the latter, hereafter called news tweet, varies in 
style and often is not grammatically correct. To 
understand the proportion of news tweets, we 
randomly selected 1000 tweets related to news, 
and got 865 news tweets. Following is an exam-
ple of anews tweet, containing oh, yea, which 
usually appear in spoken language, and :-(, an 
emoticon. 
oh yea and Chile earthquake the earth off it's 
axis according to NASA and shorten the day 
by a wee second :-(                                     (S1) 
News tweets arean important information 
source because they keep reporting what is hap-
pening in real time. For example, the earthquake 
near Los Angeles that happened on Tuesday, 
July 29, 2008 was first reported through news 
tweets only seconds later than the outbreak of 
the quake. Official news did not emerge about 
this event until four minutes later. By then, 
"Earthquake" was trending on Twitter Search 
with thousands of updates2. 
However, it is a daunting task for people to 
find out information they are interested in from 
such a huge number of news tweets, thus moti-
vating us to conduct some kind of information 
                                                 
2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 
698
extraction such as event mining, where SRL 
plays a crucial  role (Surdeanu et al, 2003). 
Considering Sentence 1, suppose the agent 
earthquake and the patient day for the predicate 
shorten are identified. Then it is straightforward 
to output the event Chile earthquake shorten the 
day, which captures the essential information 
encoded in this tweet. 
Following M?rquez (2009), we define SRL 
for news tweets as the task of identifying the 
arguments of a given verb as predicate in a news 
tweet and assigning them semantic labels de-
scribing the roles they play for the predicate. To 
make our method applicable to general infor-
mation extraction tasks,  rather than only to 
some special scenarios such as arresting event 
extraction, we adopt general semantic roles, i.e., 
Agent(A0), Patient(A1), Location(AM-LOC), 
Temporal(AM-TMP),etc., instead of situation-
specific roles (Fillmore et al, 2004) such as 
Suspect, Authorities, and Offense in an arrest 
frame.  
Our first attempt is to directly apply the state-
of-art SRL system (Meza-Ruiz and Riedel, 2009) 
that trained on the CoNLL 08 shared task da-
taset(Surdeanu et al, 2008), hereafter called 
SRL-BS, to news tweets. Not surprisingly, we 
observe its F1 score drops sharply from 75.5% 
on news corpus to 43.3% on our human annotat-
ed news tweets, owing much to the informal 
written style of news tweets. 
Therefore, we have to build a domain specific 
SRL system for news tweets. Given the diversi-
fied styles of news tweets, building such a sys-
tem requires a larger number of annotated news 
tweets, which are not available, and are not af-
fordable for human labeling. We propose a novel 
method to automatically annotate news tweets, 
which leverages the existing resources of SRL 
for news domain, and content similarity between 
news and news tweets. We argue that the same 
event is likely to be reported by both news and 
news tweets, which results in  content similarity 
between the news and news tweet. Further, we 
argue that the news and news tweets reporting 
the same event tend to have similar predicate-
argument structures. We tested our assumptions 
on the event Chile earthquake that happened on 
Match 2nd, 2010. We got 261 news and 722 news 
tweets published on the same day that described 
this event.  Sentence 2 and 3 are two examples 
of the news excerpts and Sentence 1 is one ex-
ample of news tweets for this event.   
Chile Earthquake Shortened Earth Day    (S2) 
Chile Earthquake Shortened Day              (S3) 
Obviously Sentence 1, 2 and 3 all have predi-
FDWH ?shortened? with the same A0 and A1 ar-
guments. Our manually checking showed that in 
average each news tweet in those 993 samples 
had 2.4 news excerpts that had the same predi-
cate-argument structures.  
Our news tweet annotation approach consists 
of four steps. First, we submit hot queries to 
Twitter and for each query we obtain a list of 
tweets. Second, for each list of tweets, we single 
out news excerpts using heuristic rules and re-
move them from the list, conduct SRL on news 
excerpts using SRL-BS, and cluster them in 
terms of the similarity in content and predicate-
argument structures. Third, for each list of 
tweets, we try to merge every remaining tweet 
into one news excerpt cluster according to its 
content similarity to the cluster. Those that can 
be put into one news group are regarded as news 
tweet. Finally, semantic structures of news ex-
cerpts are passed to the news tweet in the same 
group through word alignment. 
Our domain specific SRL system is then 
trained on automatically constructed training 
data using the Conditional Random Field (CRF: 
Lafferty et al, 2001) learning framework. Our 
system is evaluated on a human labeled dataset, 
and achieves state-of-the-art performance, out-
performing the baseline SRL-BS.  
Our contributions can be summarized as fol-
lows: 
1) We propose to conduct SRL for news 
tweets for fine grained information ex-
traction and retrieval;  
2) We present a semi-supervised learning 
approach to train a domain specific SRL 
system for news tweets, which outper-
forms SRL-BS and achieves the state-of-
the-art performance on a human labeled 
dataset. 
The rest of this paper is organized as follows: 
In the next section, we review related work.  In 
Section 3 we detail key components of our ap-
proach. In Section 4, we setup experiments and 
evaluate the effectiveness of our method.  Final-
699
ly, Section 5 concludes and presents the future 
work. 
2 Related Work 
Our related work falls into two categories: SRL 
on news and domain adaption. 
As for SRL on news, most researchers used 
the pipelined approach, i.e., dividing the task 
into several phases such as argument identifica-
tion, argument classification, global inference, 
etc.,  and conquering them individually (Xue and 
Palmer, 2004; Koomen et al, 2005; Cohn and 
Blunsom, 2005; Punyakanok et al, 2008; 
Toutanova et al, 2005; Toutanova et al, 2008). 
Exceptions to the pipelined approach exist.  
M?rquez et al (2005) sequentially labeled the 
words according to their positions relative to an 
argument (i.e., inside, outside or at the beginning 
of it). Carreras et al (2004) and Surdeanu et al 
(2007) jointly labeled all the predicates. Vickrey 
and Koller(2008) simplified the input sentence 
by hand-written and machine learnt rules before 
conducting SRL. Some other approaches simul-
taneously resolved all the sub-tasks by integrat-
ing syntactic parsing and SRL into a single mod-
el (Musillo and Merlo, 2006; Merlo and Musillo, 
2008), or by using Markov Logic Networks 
(MLN, Richardson and Domingos, 2005) as the 
learning framework (Riedel and Meza-Ruiz, 
2008; Meza-Ruiz and Riedel, 2009). 
All the above approaches focus on sentences 
from news articles or other formal documents, 
and depend on human annotated corpus for 
training. To our knowledge, little study has been 
carried out on SRL for news tweets.  
As for domain adaption, some researchers re-
garded the out-of-GRPDLQ GDWD DV ?SULRU
NQRZOHGJH?DQGestimated the model parameters 
by maximizing the posterior under this prior dis-
tribution, and successfully applied their ap-
proach to language modeling (Bacchiani and 
Roark, 2003) and parsing (Roark and Bacchiani, 
2003). Daum? III and Marcu (2006) presented a 
QRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-
PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\
out-of-GRPDLQ?   
Unlike existing domain adaption approaches, 
our method is about adapting SRL system on 
news domain to the news tweets domain, two 
domains that differ in writing style but are linked 
through content similarity. 
3 Our Method 
Our method of SRL for news tweets is to train a 
domain specific SRL on automatically annotated 
training data as briefed in Section 1.  
In this section we present details of the five 
crucial components of our method, i.e., news 
excerpt identification, news excerpt clustering, 
news tweets identification, semantic structure 
mapping, and the domain specific SRL system 
constructing. 
3.1 News Excerpt Identification 
We use one heuristic rule to decide whether or 
not a tweet is news excerpt:  if a tweet has a link 
to a news article and its text content is included 
by the news article, it is news excerpt, otherwise 
not. 
Given a tweet, to apply this rule, we first ex-
tract the content link and expand it, if any, into 
the full link with the unshorten service3. This 
step is necessary because content link in tweet is 
usually shortened to reduce the total amount of 
characters. Next, we check if the full link points 
to any of the pre-defined news sites, which, in 
our experiments, are 57 English news websites. 
If yes, we download the web page and check if it 
exactly contains the text content of the input 
tweet. Figure 1 illustrates this process.  
Figure 1. An illustration of news excerpt identi-
fication. 
To test the precision of this approach, while 
preparing for the training data for the experi-
ments, we checked 100 tweets that were identi-
fied as news excerpt by this rule to find out they 
all are excerpted from news. 
                                                 
3 http://unshort.me 
700
3.2 News Excerpt Clustering 
Given as input a list of news excerpts concerning 
the same query and published in the same time 
scope, this component uses the hierarchical ag-
glomerative clustering algorithm (Manning et 
al., 2008) to divide news excerpts into groups in 
terms of the similarity in content and predicate-
argument structures.  
Before clustering, for every news excerpt, we 
remove the content link and other metadata such 
as author, retweet marks (starting with RT @), 
reply marks (starting with @ immediately after 
the author), hash tags (starting with #), etc., and 
keep only the text content; then it is further 
parsed into tokens, POS tags, chunks and syntac-
tic tree using the OpenNLP toolkit4.  After that,  
SRL is conducted with SRL-BS to get predicate-
argument structures. Finally, every news excerpt 
is represented as frequency a vector of terms, 
including tokens, POS tagger, chunks, predicate-
argument structures, etc. A news cluster is re-
garded as a ?macro? news excerpt and is also 
represented as a term frequency vector, i.e., the 
sum of all the term vectors in the cluster.  Noisy 
terms, such as numbers and predefined stop 
words are excluded from the frequency vector. 
To reduce data sparseness, words are stemmed 
by Porter stemmer (Martin F. Porter, 1980). 
The cosine similarity is used to measure the 
relevance between two clusters, as defined in 
Formula 1.  
   ,
'
'
'
CVCV
CVCV
CCCS
u
?               (1) 
Where C, &? denote two clusters, CV, CV? de-
note  the term frequency vectors of C and  &? 
respectively, and CS(C, &?) stands for the  co-
sine similarity between C and  &?. 
Initially, one news excerpt forms one cluster.  
Then the clustering process repeats merging the 
two most similar clusters into one till the simi-
larity between any pair of clusters is below a 
threshold, which is experimentally set to 0.7 in 
our experiments. 
During the training data preparation process, 
we randomly selected 100 clusters, each with 3.2 
pieces of news in average. For every pair of 
news excerpts in the same cluster, we checked if 
                                                 
4 http://opennlp.sourceforge.net/ 
they shared similar contents and semantic struc-
tures, and found out that 91.1% were the cases. 
3.3 News Tweets Identification 
After news excerpts are identified and removed 
from the list, every remaining tweet is checked if 
it is a news tweet. Here we group news excerpts 
and news tweets together in two steps because 1) 
news excerpts count for only a small proportion 
of all the tweets in the list, making our two-step 
clustering algorithm more efficient; and 2) one-
step clustering tends to output meaningless clus-
ters that include no news tweets. 
Intuitively, news tweet, more often than not, 
have news counterparts that report similar con-
tents. Thus we use the following rule to identify 
news tweets: if the content similarity between 
the tweet and any news excerpt cluster is greater 
than a threshold, which is experimentally set to 
0.7 in our experiments, the tweet is a news tweet, 
otherwise it is not. Furthermore, each news 
tweet is merged into the cluster with most simi-
lar content. Finally, we re-label any news tweet 
as news excerpt, which is then process by SRL-
BS, if its content similarity to the cluster exceeds 
a threshold, which is experimentally set to 0.9 in 
our experiments. 
Again, the cosine similarity is used to meas-
ure the content similarity between tweet and 
news excerpt cluster. Each tweet is repressed as 
a term frequency vector. Before extracting terms 
from tweet, tweet metadata is removed and a 
rule-based normalization process is conducted to 
restore abnormal strLQJVVD\?	DSRV?LQWRWKHLU
KXPDQ IULHQGO\ IRUP VD\ ?? ? 1H[W VWHPPLQJ
tools and OpenNLP are applied to get lemmas, 
POS tags, chunks, etc., and noisy terms are fil-
tered.  
We evaluated the performance of this ap-
proach when preparing for the training data. We 
randomly sampled 500 tweets that were identi-
fied as news tweets, to find that 93.8% were true 
news tweets. 
3.4 Semantic Structure Mapping 
Semantic structure mapping is formed as the 
task of word alignment from news excerpt to 
news tweet. A HMM alignment model is trained 
with GIZA++ (Franz and Hermann, 2000) on all 
(news excerpt, news tweet) pairs in the same 
cluster. After word alignment is done, semantic 
701
information attached to a word in a news excerpt 
is passed to the corresponding word in the news 
tweet as illustrated in Figure 2. 
 
Chile Earthquake Shortened Earth Day
A0 predicate A1
NASA and shorten the day by a wee second :-(
oh yea and Chile earthquake the earth off it's axis according to
 
Figure 2. An example of mapping semantic 
structures from news excerpts to news tweets. 
In Figure 2, shorten, earthquake and day in 
two sentences are aligned, respectively; and two 
predicate-argument structures in the first sen-
tence, i.e., (shortened, earthquake, A0), (short-
ened, day, A1), are passed to the second. 
News tweets may receive no semantic infor-
mation from related news excerpts after mapping, 
because of word alignment errors or no news 
excerpt in the cluster with similar semantic 
structures.  Such tweets are dropped. 
Mapping may also introduce cases that violate 
the following two structural constraints in SRL 
(Meza-Ruiz and Riedel, 2009): 1) one (predi-
cate, argument) pair has only one role label in 
one sentence; and 2) for each predicate, each of 
the proper arguments (A0~A5) can occur at most 
once. Those conflicts are largely owing to the 
noisy outputs of SRL trained on news and to the 
alignment errors. While preparing for the train-
ing data for our experiments, we found 38.9% of 
news tweets had such conflicts.  
A majority voting schema and the structural 
constrains are used to resolve the conflicts as 
described below.   
1) Step 1, for every cluster, each (predicate, 
argument, role) is weighted according to 
its frequency in the cluster; 
2) Step 2, for every cluster, detect conflicts 
using the structural constrains; if no con-
flicts exist, stop; otherwise go to Step 3;   
3) Step 3, for every cluster, keep the one 
with higher weight in each conflicting 
(predicate, argument, role) pair; if the 
weights are equal,  drop both; 
Here is an example to show the conflicting 
resolution process.  Consider the cluster includ-
ing Sentence 1, 2 and 3, where (shorten, earth-
quake, A0), (shorten, earthquake, A1), (shorten, 
axis, A0), and (shorten, day, A1) occur 6, 4, 1 
and 3 times, respectively.  This cluster includes 
three conflicting pairs:   
1) (shorten, earthquake, A0) vs. (shorten, 
earthquake, A1); 
2) (shorten, earthquake, A1) vs. (shorten, 
day, A1); 
3) (shorten, earthquake, A0) vs. (shorten, ax-
is, A0); 
The first pair is first resolved, causing (short-
en, earthquake, A0) to be kept and (shorten, 
earthquake, A1) removed, which leads to the 
second pair being resolved as well; then we pro-
cess the third pair resulting in (shorten, earth-
quake, A0) being kept and (shorten, axis, A0) 
dropped; finally (shorten, earthquake, A0) and 
(shorten, day, A1) stay in the cluster. 
The conflicting resolution algorithm is sensi-
tive to the order of conflict resolution in Step 3. 
Still consider the three conflicting pairs listed 
above. If the second pair is first processed, only 
(shorten, earthquake, A0) will be left. Our strat-
egy is to first handle the conflict resolving which 
leads to most conflicts resolved. 
We tested the performance of this semantic 
structure mapping strategy while preparing for 
the training data. We randomly selected 56 news 
tweets with conflicts and manually annotated 
them with SRL. After the conflict resolution 
method was done, we observed that 38 news 
tweets were resolved correctly, 9 resolved but 
incorrectly, and 9 remain unresolved, suggesting 
the high precision of this method, which fits our 
task.  We leave it to our future work to study 
more advanced approach for semantic structure 
mapping. 
3.5 SRL System for News Tweets 
Following M?rquez et al (2005), we regard SRL 
for tweets as a sequential labeling task, because 
of its joint inference ability and its openness to 
support other languages. 
We adopt conventional features for each token 
defined in M?rquez et al(2005),  such as the 
lemma/POS tag of the current/previous/next to-
ken, the lemma of predicate and its combination 
with the lemma/POS tag of the current token, the 
voice of the predicate (active/passive), the dis-
tance between the current token and the predi-
cate, the relative position of the current token to 
702
the predicate, and so on. We do not use features 
related to syntactic parsing trees, to allow our 
system not to rely on any syntactic parser, whose 
performance depends on style and language of 
text, which limits the generality of our system. 
Before extracting features, we perform a pre-
processing step to remove tweet metadata and 
normalize tweet text content, as described in 
Section 3.3. The OpenNLP toolkit is used for 
feature extraction, and the CRF++ toolkit 5  is 
used to train the model. 
4 Experiments 
In this section, we evaluate our SRL system on a 
gold-standard dataset consisting of 1,110 human 
annotated news tweets and show that our system 
achieves the state-of-the-art performance com-
pared with SRL-BS that is trained on news. Fur-
thermore, we study the contribution of automati-
cally generated training data. 
4.1 Evaluation Metric 
We adopt the widely used precision (Pre.), recall 
(Rec.) and F-score (F., the harmonic mean of 
precision and recall) as evaluation metrics.  
4.2 Baseline System 
We use SRL-BS as our baseline because of its 
state-of-art performance on news domain, and its 
readiness to use as well. 
4.3 Data Preparation 
We restrict to English news tweets to test our 
method. Our method can label news tweets of 
other languages, given that the related tools such 
as the SRL system on news domain, the word 
alignment tool, OpenNLP, etc., can support oth-
er languages.  
We build two corpora for our experiments: 
one is the training dataset of 10,000 news tweets 
with semantic roles automatically labeled; the 
other is the gold-standard dataset of 1,110 news 
tweets with semantic roles manually labeled. 
Training Dataset 
We randomly sample 80 queries from 300 
English queries extracted from the top stories of 
Bing news, Google news and Twitter trending 
topics from March 1, 2010 to March 4, 2010.  
                                                 
5 http://crfpp.sourceforge.net/ 
Submitting the 80 queries to Twitter search, 
we retrieve and download 512,000 tweets, from 
which we got 4,785 news excerpts and 11,427 
news tweets, which were automatically annotat-
ed using the method described in Section 3.   
Furthermore, 10,000 tweets are randomly se-
lected from the automatically annotated news 
tweets, forming the training dataset, while the 
other 1,427 news tweets are used to construct the 
gold-standard dataset. 
Gold-standard Dataset 
We ask two people to annotate the 1,427 news 
tweets, following the Annotation guidelines for 
PropBank6 with one exception: for phrasal ar-
guments, only the head word is labeled as the 
argument, because our system and SRL-BS con-
duct word level SRL. 
317 news tweets are dropped because of in-
consistent annotation, and the remaining 1,110 
news tweets form the gold-standard dataset.  
Quality of Training dataset 
Since the news tweets in the gold-standard da-
taset are randomly sampled from the automati-
cally labeled corpus and are labeled by both hu-
man and machine, we use them to estimate the 
quality of training data, i.e., to which degree the 
automatically generated results are similar to 
humans?.   
We find that our method achieves 75.6% F1 
score, much higher than the baseline, suggesting 
the relatively high quality of the training data. 
4.4 Result and Analysis 
Table 1 reports the experimental results of our 
system (SRL-TS) and the baseline on the gold-
standard dataset. 
 
 Precision Recall F-Score 
SRL-BS 36.0 % 54.5% 43.3% 
SRL-TS 78.0% 57.1% 66.0% 
Table 1. Performances of our system and the 
baseline on the gold-standard dataset. 
As shown in Table 1, our system performs 
much better than the baseline on the gold-
standard dataset in terms of all metrics. We ob-
serve two types of errors that are often made by 
                                                 
6 http://verbs.colorado.edu/~mpalmer/projects/ace/PB 
guidelines.pdf 
703
SRL-BS but not so often by our system, which 
largely explains the difference in performance.  
The first type of errors, which accounts for 
25.3% of the total errors made by SRL-BS, is 
caused by the informal written style, such as el-
lipsis, of news tweets. For instance, for the ex-
ample Sentence 1 listed in Section 1, the SRL-
BS incorrectly identify earth as the A0 argument 
of the predicate shorten. The other type of errors, 
which accounts for 10.2% of the total errors 
made by SRL-BS, is related to the discretionary 
combination of news snippets. For example, 
consider the following news tweet: 
The Chile earthquake shifted the earth's axis, 
"shortened the length of an Earth day by 1.26 
miliseconds".                                              (S4) 
We analyze the errors made by our system 
and find that 12.5% errors are attributed to the 
complex syntactic structures, suggesting that 
combining our system with systems on news 
domain is a promising direction. For example, 
our system cannot identify the A0 argument of 
the predicate shortened, because of its blindness 
of attributive clause; in contrast, SRL-BS works 
on this case.  
wow..the earthquake that caused the 2004 In-
dian Ocean tsunami shortened the day by al-
most 3 microseconds..what does that even 
mean?! HOW?                                           (S5) 
We also find that 32.3% of the errors made by 
our system are more or less related to the train-
ing data, which has noise and cannot fully repre-
sent the knowledge of SRL on news tweets. For 
instance, our system fails to label the following 
sentence, partially because the predicate strike 
does not occur in the training set. 
8.8-Magnitude-Earthquake-Strikes-Chile (S6) 
We further study how the size of automatical-
O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-
formance, as illustrated in Figure 3. We conduct 
two sets of experiments: in the first set, the train-
ing data is automatically labeled and the testing 
data is the gold-standard dataset; in the second 
set, half of the news tweets from the gold-
standard dataset are added to the training data, 
the remaining half forms the testing dataset. 
Curve 1 and 2 represent the experimental results 
of set 1 and 2, respectively. 
From Curve 1, we see that RXUV\VWHP?VSHr-
formance increases sharply when the training 
data size varies from 5,000 to 6,000; then in-
creases relatively slowly with more training data; 
and finally reaches the highest when all training 
data is used.  Curve 2 reveals a similar trend. 
 
 
Figure 3. Performance on training data of vary-
ing size. 
This phenomenon is largely due to the com-
peting between two forces: the noise in the train-
ing data, and the knowledge of SRL encoded in 
the training data.  
Interestingly, from Figure 3, we observe that 
the contribution of human labeled data is no 
longer significant after 6,000 automatically la-
beled training data is used, reaffirming the effec-
tiveness of the training data. 
5 Conclusions and Future Work 
We propose to conduct SRL on news tweets for 
fine grained information extraction and retrieval. 
We present a self-supervised learning approach 
to train a domain specific SRL system for news 
tweets. Leveraging the SRL system on news 
domain and content similarity between news and 
news tweets, our approach automatically labels a 
large volume of training data by mapping SRL-
BS generated results of news excerpts to news 
tweets. Experimental results show that our sys-
tem outperforms the baseline and achieves the 
state-of-the-art performance.  
In the future, we plan to enlarge training data 
size and test our system on a larger dataset; we 
also plan to further boost the performance of our 
system by incorporating tweets specific features 
such as hash tags, reply/re-tweet marks into our 
704
CRF model, and by combining our system with 
SRL systems trained on news.  
 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper-
vised language model adaptation. Proceedings of 
the 2003 International Conference on Acoustics, 
Speech and Signal Processing, volume 1, pages: 
224-227 
Carreras, Xavier, Llu?s M?rquez, and Grzegorz 
&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-
sitional arguments with Perceptrons. Proceedings 
of the Eighth Conference on Computational Natu-
ral Language Learning, pages: 106-109. 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Daum?, Hal III and Daniel Marcu. 2006. Domain 
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26(1), 101-126. 
Fillmore, Charles J., Josef Ruppenhofer, Collin F. 
Baker. 2004. FrameNet and Representing the Link 
between Semantic and Syntactic Relations. Com-
putational Linguistics and Beyond, Institute of 
Linguistics, Academia Sinica. 
Kelly, Ryan, ed. 2009. Twitter Study Reveals Inter-
esting Results About Usage. San Antonio, Texas: 
Pear Analytics. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
Lafferty, John D., Andrew McCallum, Fernando C. 
N. Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling 
Sequence Data. Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, 
pages: 282-289. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval. Cambridge University Press, Cam-
bridge, UK. 
M?rquez, Llu?s, Jesus Gim?nez Pere Comas and 
Neus Catal?. 2005. Semantic Role Labeling as Se-
quential Tagging. Proceedings of the Ninth Con-
ference on Computational Natural Language 
Learning, pages: 193-196. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages: 155-163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Och, Franz Josef, Hermann Ney. Improved Statistical 
Alignment Models. Proceedings of the 38th Annu-
al Meeting of the Association for Computational 
Linguistics, pages: 440-447. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3), 130-137. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and in-
ference in semantic role labeling. Journal of Com-
putational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with Markov Logic. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 193-
197. 
Roark, Brian and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel 
domains. Proceedings of the 2003 Conference of 
the North American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology, volume 1, pages: 126-133. 
Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics, volume 1, pages: 8-15. 
Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras and 
Pere R. Comas. 2007. Combination strategies for 
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29(1), 105-151. 
705
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Vickrey, David and Daphne Koller. 2008. Applying 
sentence simplification to the conll-2008 shared 
task. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, pag-
es: 268-272  
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
706
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068?1076,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
 
 
Abstract 
In this paper we develop an approach to tackle 
the problem of verb selection for learners of 
English as a second language (ESL) by using 
features from the output of Semantic Role La-
beling (SRL). Unlike existing approaches to 
verb selection that use local features such as 
n-grams, our approach exploits semantic fea-
tures which explicitly model the usage context 
of the verb. The verb choice highly depends 
on its usage context which is not consistently 
captured by local features. We then combine 
these semantic features with other local fea-
tures under the generalized perceptron learn-
ing framework. Experiments on both in-
domain and out-of-domain corpora show that 
our approach outperforms the baseline and 
achieves state-of-the-art performance.1 
1 Introduction 
Verbs in English convey actions or states of being. 
In addition, they also communicate sentiments and 
imply circumstances, e.g., in ?He got [gained] the 
scholarship after three interviews.?, the verb 
?gained? may indicate that the ?scholarship? was 
competitive and required the agent?s efforts; in 
contrast, ?got? sounds neutral and less descriptive. 
                                                          
* This work has been done while the author was visiting Mi-
crosoft Research Asia. 
Since verbs carry multiple important functions, 
misusing them can be misleading, e.g., the native 
speaker could be confused when reading ?I like 
looking [reading] books?. Unfortunately, accord-
ing to (Gui and Yang, 2002; Yi et al, 2008), more 
than 30% of the errors in the Chinese Learner Eng-
lish Corpus (CLEC) are verb choice errors. Hence, 
it is useful to develop an approach to automatically 
detect and correct verb selection errors made by 
ESL learners. 
However, verb selection is a challenging task 
because verbs often exhibit a variety of usages and 
each usage depends on a particular context, which 
can hardly be adequately described by convention-
al n-gram features. For instance, both ?made? and 
?received? can complete ?I have __ a telephone 
call.?, where the usage context can be represented 
as ?made/received a telephone call?; however, in 
?I have __ a telephone call from my boss?, the 
prepositional phrase ?from my boss? becomes a 
critical part of the context, which now cannot be 
described by n-gram features, resulting in only 
?received? being suitable. 
Some researchers (Tetreault and Chodorow, 
2008) exploited syntactic information and n-gram 
features to represent verb usage context. Yi et al 
(2008) introduced an unsupervised web-based 
proofing method for correcting verb-noun colloca-
tion errors. Brockett et al (2006) employed phrasal 
Statistical Machine Translation (SMT) techniques 
to correct countability errors. None of their meth-
ods incorporated semantic information. 
SRL-based Verb Selection for ESL 
1,2Xiaohua Liu, 3Bo Han*, 4Kuan Li*, 5Stephan Hyeonjun Stiller and 2Ming Zhou 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3Department of Computer Science and Software Engineering 
The University of Melbourne 
4College of Computer Science 
Chongqing University 
5Computer Science Department 
Stanford University 
{xiaoliu,  mingzhou, v-kuli}@microsoft.com 
 b.han@pgrad.unimelb.edu.au 
sstiller@stanford.edu 
1068
Unlike the other papers, we derive features from 
the output of an SRL (M?rquez, 2009) system to 
explicitly model verb usage context. SRL is gener-
ally understood as the task of identifying the argu-
ments of a given verb and assigning them semantic 
labels describing the roles they play. For example, 
given a sentence ?I want to watch TV tonight? and 
the target predicate ?watch?, the output of SRL 
will be something like ?I [A0] want to watch [tar-
get predicate] TV [A1] tonight [AM-TMP].?, 
meaning that the action ?watch? is conducted by 
the agent ?I?, on the patient ?TV?, and the action 
happens ?tonight?. 
We believe that SRL results are excellent fea-
tures for characterizing verb usage context for 
three reasons: (i) Intuitively, the predicate-
argument structures generated by SRL systems 
capture major relationships between a verb and its 
contextual participants and consequently largely 
determine whether or not the verb usage is proper. 
For example, in ?I want to watch a match tonight.?, 
?match? is the patient of ?watch?, and ?watch ? 
match? forms a collocation, suggesting ?watch? is 
appropriately used. (ii) Predicate-argument struc-
tures abstract away syntactic differences in sen-
tences with similar meanings, and therefore can 
potentially filter out lots of noise from the usage 
context. For example, consider ?I want to watch a 
football match on TV tonight?: if ?match? is suc-
cessfully identified as the agent of ?watch?, 
?watch ? football?, which is unrelated to the us-
age of ?watch? in this case, can be easily excluded 
from the usage context. (iii) Research on SRL has 
made great achievements, including human-
annotated training corpora and state-of-the-art sys-
tems, which can be directly leveraged. 
Taking an English sentence as input, our method 
first generates correction candidates by replacing 
each verb with verbs in its pre-defined confusion 
set; then for every candidate, it extracts SRL-
derived features; finally our method scores every 
candidate using a linear function trained by the 
generalized perceptron learning algorithm (Collins, 
2002) and selects the best candidate as output. 
Experimental results show that SRL-derived fea-
tures are effective in verb selection, but we also 
observe that noise in SRL output adversely in-
creases feature space dimensions and the number 
of false suggestions. To alleviate this issue, we use 
local features, e.g., n-gram-related features, and 
achieve state-of-the-art performance when all fea-
tures are integrated. 
Our contributions can be summarized as follows: 
1. We propose to exploit SRL-derived fea-
tures to explicitly model verb usage con-
text. 
2. We propose to use the generalized percep-
tron framework to integrate SRL-derived 
(and other) features  and achieve state-of-
the-art performance on both in-domain and 
out-of-domain test sets. 
Our paper is organized as follows: In the next 
section, we introduce related work. In Section 3, 
we describe our method. Experimental results and 
analysis on both in-domain and out-of-domain cor-
pora are presented in Section 4. Finally, we con-
clude our paper with a discussion of future work in 
Section 5. 
2 Related Work 
SRL results are used in various tasks. Moldovan et 
al. (2004) classify the semantic relations of noun 
phrases based on SRL. Ye and Baldwin (2006) 
apply semantic role?related information to verb 
sense disambiguation. Narayanan and Harabagiu 
(2004) use semantic role structures for question 
answering. Surdeanu et al (2003) employ predi-
cate-argument structures for information extrac-
tion. 
However, in the context of ESL error detection 
and correction, little study has been carried out on 
clearly exploiting semantic information. Brockett 
et al (2006) propose the use of the phrasal statisti-
cal machine translation (SMT) technique to identi-
fy and correct ESL errors. They devise several 
heuristic rules to generate synthetic data from a 
high-quality newswire corpus and then use the syn-
thetic data together with their original counterparts 
for SMT training. The SMT approach on the artifi-
cial data set achieves encouraging results for cor-
recting countability errors. Yi et al (2008) use web 
frequency counts to identify and correct determiner 
and verb-noun collocation errors. Compared with 
these methods, our approach explicitly models 
verb usage context by leveraging the SRL output. 
The SRL-based semantic features are integrated, 
along with the local features, into the generalized 
perceptron model. 
 
1069
3 Our Approach 
Our method can be regarded as a pipeline consist-
ing of three steps. Given as input an English sen-
tence written by ESL learners, the system first 
checks every verb and generates correction candi-
dates by replacing each verb with its confusion set. 
Then a feature vector that represents verb usage 
context is derived from the outputs of an SRL sys-
tem and then multiplied with the feature weight 
vector trained by the generalized perceptron. Final-
ly, the candidate with the highest score is selected 
as the output. 
3.1 Formulation 
We formulate the task as a process of generating 
and then selecting correction candidates: 
           
? ? ? ?sScores sGENs 'maxarg* ??
                     (1) 
Here 's  denotes the input sentence for proofing, 
? ?'sGEN  is the set of correction candidates, and 
? ?sScore  is the linear model trained by the percep-
tron learning algorithm, which will be discussed in 
section 3.4. 
We call every target verb in 's  a checkpoint. 
For example, ?sees? is a checkpoint in ?Jane sees 
TV every day.?. Correction candidates are generat-
ed by replacing each checkpoint with its confu-
sions. Table 1 shows a sentence with one 
checkpoint and the corresponding correction can-
didates. 
 
Input Jane sees TV every day. 
Candidates Jane watches TV every day. 
Jane looks TV every day. 
? 
Table 1. Correction candidate list. 
One state-of-the-art SRL system (Riedel and 
Meza-Ruiz, 2008) is then utilized to extract predi-
cate-argument structures for each verb in the input, 
as illustrated in Table 2. 
Semantic features are generated by combining 
the predicate with each of its arguments; e.g., 
?watches_A0_Jane?, ?sees_A0_Jane?, ?watch-
es_A1_TV? and ?sees_A1_TV? are semantic fea-
tures derived from the semantic roles listed in Ta-
ble 2. 
 
Sentence Semantic roles 
Jane sees TV every day Predicate: sees; 
A0: Jane; 
A1: TV; 
Jane watches TV every 
day 
Predicate: watches; 
A0: Jane; 
A1: TV; 
Table 2. Examples of SRL outputs. 
At the training stage, each sentence is labeled by 
the SRL system. Each correction candidate s  is 
represented as a feature vector dRs ?? )( , where 
d  is the total number of features. The feature 
weight vector is denoted as dRw?? , and ? ?sScore  
is computed as follows: 
             ? ? wssScore ???? )(                        (2) 
Finally, ? ?sScore  is applied to each candidate, 
and *s , the one with the highest score, is selected 
as the output, as shown in Table 3. 
 
 Correction candidate Score 
*s  Jane watches TV every day. 10.8 
 Jane looks TV every day. 0.8 
 Jane reads TV every day. 0.2 
 ? ? 
Table 3.  Correction candidate scoring. 
In the above framework, the basic idea is to 
generate correction candidates with the help of pre-
defined confusion sets and apply the global linear 
model to each candidate to compute the degree of 
its fitness to the usage context that is represented 
as features derived from SRL results. 
To make our idea practical, we need to solve the 
following three subtasks: (i) generating the confu-
sion set that includes possible replacements for a 
given verb; (ii) representing the context with se-
mantic features and other complementary features; 
and (iii) training the feature weight. We will de-
scribe our solutions to those subtasks in the rest of 
this section. 
1070
3.2 Generation of Verb Confusion Sets 
Verb confusion sets are used to generate correction 
candidates. Due to the great number of verbs and 
their diversified usages, manually collecting all 
verb confusions in all scenarios is prohibitively 
time-consuming. To focus on the study of the ef-
fectiveness of semantic role features, we restrict 
our research scope to correcting verb selection er-
rors made by Chinese ESL learners and select fifty 
representative verbs which are among the most 
frequent ones and account for more than 50% of 
ESL verb errors in the CLEC data set. For every 
selected verb we manually compile a confusion set 
using the following data sources: 
1. Encarta treasures. We extract all the syno-
nyms of verbs from the Microsoft Encarta Diction-
ary, and this forms the major source for our 
confusion sets. 
2. English-Chinese Dictionaries. ESL learners 
may get interference from their mother tongue (Liu 
et al, 2000). For example, some Chinese people 
mistakenly say ?see newspaper?, partially because 
the translation of ?see? co-occurs with ?newspa-
per? in Chinese. Therefore English verbs in the 
dictionary sharing more than two Chinese mean-
ings are collected. For example, ?see? and ?read? 
are in a confusion set because they share the mean-
ings of both ??? (?to see?, ?to read?) and ???? 
(?to grasp?) in Chinese. 
3. An SMT translation table. We extract para-
phrasing verb expressions from a phrasal SMT 
translation table learnt from parallel corpora (Och 
and Ney, 2004). This may help us use the implicit 
semantics of verbs that SMT can capture but a dic-
tionary cannot, such as the fact that the verb  
Note that verbs in any confusion set that we are 
not interested in are dropped, and that the verb it-
self is included in its own confusion set. We leave 
it to our future work to automatically construct 
verb confusions. 
3.3 Verb Usage Context Features 
The verb usage context1 refers to its surrounding 
text, which influences the way one understands the 
expression. Intuitively, verb usage context can take 
the form of a collocation, e.g., ?watch ? TV? in ?I 
saw [watched] TV yesterday.? ; it can also simply 
be idioms, e.g., we say ?kick one?s habit? instead 
of ?remove one?s habit?.  
We use features derived from the SRL output to 
represent verb usage context. The SRL system ac-
cepts a sentence as input and outputs all arguments 
and the semantic roles they play for every verb in 
the sentence. For instance, given the sentence ?I 
have opened an American bank account in Bos-
ton.? and the predicate ?opened?, the output of 
SRL is listed in Table 4, where A0 and A1 are two 
core roles, representing the agent and patient of an 
action, respectively, and other roles starting with 
?AM-?are adjunct roles, e.g., AM-LOC indicates 
the location of an action. Predicate-argument struc-
tures keep the key participants of a given verb 
while dropping other unrelated words from its us-
age context. For instance, in ?My teacher said Chi-
nese is not easy to learn.?, the SRL system 
recognizes that ?Chinese? is not the A1-argument 
of ?said?. So ?say _ Chinese?, which is irrelevant 
to the usage of said, is not extracted as a feature. 
The SRL system, however, may output 
erroneous predicate-argument structures, which 
negatively affect the performance of verb 
selection.  For instance,  for the sentence ?He 
hasn?t done anything but take [make] a lot of 
money?, ?lot? is incorrectly identified as the patient 
of ?take?, making it hard to select ?make? as the 
proper verb even though ?make money? forms a 
sound collocation. To tackle this issue, we use 
local textual features, namely features related to n-
gram, chunk and chunk headword, as shown in 
Table 5.  Back-off features are generated by 
replacing the word with its POS tag to alleviate 
data sparseness. 
 
                                                          
1 http://en.wikipedia.org/wiki/Context_(language_use) 
I have made[opened] an American bank account in Boston . 
[A0] 
 
[Predicate] 
 
 
 
[A1] [AM-LOC] 
 
 
Table 4. An example of SRL output. 
1071
Table 5. An example of feature set. 
3.4 Perceptron Learning 
We choose the generalized perceptron algorithm as 
our training method because of its easy implemen-
tation and its capability of incorporating various 
features. However, there are still two concerns 
about this perceptron learning approach: its inef-
fectiveness in dealing with inseparable samples 
and its ignorance of weight normalization that po-
tentially limits its ability to generalize. In section 
4.4 we show that the training error rate drops sig-
nificantly to a very low level after several rounds 
of training, suggesting that the correct candidates 
can almost be separated from others. We also ob-
serve that our method performs well on an out-of-
domain test corpus, indicating the good generaliza-
tion ability of this method. We leave it to our fu-
ture work to replace perceptron learning with other 
models like Support Vector Machines (Vapnik, 
1995). 
In Figure 1, 
is  is the ith correct sentence within 
the training data. T and N represent the number of 
training iterations and training examples, respec-
tively. )( isGEN  is the function that outputs all the 
possible corrections for the input sentence is  with 
each checkpoint substituted by one of its confu-
sions, as described in Section 3.1. We observe that 
the generated candidates sometimes contain rea-
sonable outputs for the verb selection task, which 
should be removed. For instance, in ?? reporters 
could not take [make] notes or tape the conversa-
tion?, both ?take? and ?make? are suitable verbs in 
this context. To fix this issue, we trained a trigram 
language model using SRILM (Stolcke, 2002) on 
LDC data21, and calculated the logarithms of the 
language model score for the original sentence and 
its artificial manipulations. We only kept manipu-
lations with a language model score that is t lower 
than that of the original sentence. We experimen-
tally set t = 5. 
 
Inputs: training examples is , i=1?N 
Initialization: 0?w?  
Algorithm: 
   For r= 1.. T, i= 1..N    
   Calculate 
wso isGens ???? ? )(maxarg )(
 
   If os i ?  
         )()( osww o ????? ??  
Outputs: w?  
Figure 1. The perceptron algorithm, adapted from Co-
lins (2002). 
?  in Figure 1 is the feature extraction function. 
)(o? and )( is? are vectors extracted from the out-
put and oracle, respectively. A vector field is filled 
with 1 if the corresponding feature exists, or 0 oth-
erwise; w?  is the feature weight vector, where posi-
tive elements suggest that the corresponding 
features support the hypothesis that the candidate 
is correct. 
The training process is to update w? , when the 
output differs from the oracle. For example, when 
o is ?I want to look TV? and is  is ?I want to watch 
TV?, w?  will be updated. 
We use the averaged Perceptron algorithm (Col-
lins, 2002) to alleviate overfitting on the training 
data. The averaged perceptron weight vector is 
defined as 
                 
?
??
?
TrN
riwTN ..1,..1i
,1 ???
                    (3) 
where riw ,? is the weight vector immediately af-
ter the ith sentence in the  rth iteration. 
 
                                                          
2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12 
Local: trigrams 
   have_opened 
   have_opened_a 
   opened_an_American 
   PRP_VBP_opened 
   VBP_opened_DT 
   opened_DT_JJ 
Local: chunk 
   have_opened 
   opened_an_American_investment_bank 
_account 
   PRP_opened 
   opened_NN 
Semantic: SRL derived features 
   A0_I_opened 
   opened_A1_account 
   opened_AM-LOC_in 
   ... 
1072
4 Experiments 
In this section, we compare our approach with the 
SMT-based approach. Furthermore, we study the 
contribution of predicate-argument-related 
features, and the performances on verbs with 
varying distance to their arguments. 
4.1 Experiment Preparation 
The training corpus for perceptron learning was 
taken from LDC2005T12. We randomly selected 
newswires containing target verbs from the New 
York Times as the training data. We then used the 
OpenNLP package31to extract sentences from the 
newswire text and to parse them into the corre-
sponding tokens, POS tags, and chunks. The SRL 
system is built according to Riedel and Meza-Ruiz 
(2008), using the CoNLL-2008 shared task data for 
training. We assume that the newswire data is of 
high quality and free of linguistic errors, and final-
ly we gathered 20000 sentences that contain any of 
the target verbs we were focusing on.  We experi-
mentally set the number of training rounds to T = 
50. 
We constructed two sets of testing data for in-
domain and out-of-domain test purposes, respec-
tively. To construct the in-domain test data, we 
first collected all the sentences that contain any of 
the verbs we were interested in from the previous 
unused LDC dataset; then we replaced any target 
verb in our list with a verb in its confusion set; 
next, we used the language-model-based pruning 
strategy described in 3.4 to drop possibly correct 
manipulations from the test data; and finally we 
randomly sampled 5000 sentences for testing. 
To build the out-of-domain test dataset, we 
gathered 186 samples that contained errors related 
to the verbs we were interested in from English 
blogs written by Chinese and from the CLEC cor-
pus, which were then corrected by an English na-
tive speaker. Furthermore, for every error 
involving the verbs in our target list, both the verb 
and the word that determines the error are marked 
by the English native speaker. 
4.2 Baseline 
We built up a phrasal SMT system with the word 
re-ordering feature disabled, since our task only 
concerns the substitution of the target verb. To 
                                                          
3 http://opennlp.sourceforge.net/ 
construct the training corpus, we followed the idea 
in Brockett et al (2006), and applied a similar 
strategy described in section 3.4 to the SRL sys-
tem?s training data to generate aligned pairs. 
4.3 Evaluation Metric 
We employed the following metrics adapted from 
(Yi et al, 2008): revised precision (RP), recall of 
the correction (RC) and false alarm (FA). 
         
 sCheckpoint All of #
Proofings Correct of #RP ?
                      (4)      
RP reflects how many outputs are correct usag-
es. The output is regarded as a correct suggestion if 
and only if it is exactly the same as the answer. 
Paraphrasing scenarios, for example, the case that 
the output is ?take notes? and the answer is ?make 
notes?, are counted as errors. 
Errors Total of# 
Proofings Modified Correct of# RC ?
                (5) 
RC indicates how many erroneous sentences are 
corrected among all the errors. It measures the sys-
tem?s coverage of verb selection errors. 
     
sCheckpoint All of# 
sCheckpoint Modified Incorrect of# FA ?
        (6) 
FA is related to the cases where a correct verb is 
mistakenly replaced by an inappropriate one. The-
se false suggestions are likely to disturb or even 
annoy users, and thus should be avoided as much 
as possible. 
4.4 Results and Analysis 
The training error curves of perceptron learning 
with different feature sets are shown in Figure 2. 
They drop to a low error rate and then stabilize 
after a few number of training rounds, indicating 
that most of the cases are linearly separable and 
that perceptron learning is applicable to the verb 
selection task. 
We conducted feature selection by dropping fea-
tures that occur less than N times. Here N was ex-
perimentally set to 5. We observe that, after feature 
selection, some useful features such as 
?watch_A1_TV? and ?see_A1_TV? were kept, but 
some noisy features like ?Jane_A0_sees? and 
?Jane_A0_watches? were removed, suggesting the 
effectiveness of this feature selection approach. 
 
1073
 Figure 2. Training error curves of the perceptron. 
We tested the baseline and our approach on the 
in-domain and out-of-domain corpora. The results 
are shown in Table 7 and 8, respectively. 
In the in-domain test, the SMT-based approach 
has the highest false alarm rate, though its output 
with word insertions or deletions is not considered 
wrong if the substituted verb is correct. Our ap-
proach, regardless of what feature sets are used, 
outperforms the SMT-based approach in terms of 
all metrics, showing the effectiveness of percep-
tron learning for the verb selection task. Under the 
perceptron learning framework, we can see that the 
system using only SRL-related features has higher 
revised precision and recall of correction, but also 
a slightly higher false alarm rate than the system 
based on only local features. When local features 
and SRL-derived features are integrated together, 
the state-of-the-art performance is achieved with a 
5% increase in recall, and minor changes in preci-
sion and false alarm. 
In the out-of-domain test, the SMT-based ap-
proach performs much better than in the in-domain 
test, especially in terms of false alarm rate, indicat-
ing the SMT-based approach may favor short sen-
tences. However, its recall drops greatly. We ob-
serve similar performance differences between the 
systems with different feature sets under the same 
perceptron learning framework, reaffirming the 
usefulness of the SRL-based features for verb se-
lection. 
We also conducted significance test. The results 
confirm that the improvements (SRL+Local vs. 
SMT-based) are statistically significant (p-value < 
0.001) for both the open-domain and the in-domain 
experiments. 
Furthermore, we studied the performance of our 
system on verbs with varying distance to their ar-
guments on the out-of-domain test corpus. 
 
Local d<=2 2<d<=4 d>4 
RP 64.3% 60.3% 59.4% 
RC 34.6% 33.1% 28.9% 
FA 3.0% 6.3% 5.0% 
SRL d<=2 2<d<=4 d>4 
RP 65.1% 60.1% 62.1% 
RC 40.3% 34.0% 36.9% 
FA 5.0% 6.7% 6.3% 
Table 9. Performance on verbs with different distance to 
their arguments on out-of-domain test data. 
Table 9 shows that the system with only SRL-
derived features performs significantly better than 
the system with only local features on the verb 
whose usage depends on a distant argument, i.e., 
one where the number of words between the predi-
cate and the argument is larger than 4. To under-
stand the reason, consider the following sentence: 
?It's raining outside. Please wear[take] the 
black raincoat with you.? 
 SMT-based Our method 
SRL Local SRL + Local 
RP 48.4% 64.5% 62.2% 66.4% 
RC 23.5% 40.2% 32.9% 46.4% 
FA 13.3% 5.6% 4.2% 6.8% 
Table 7. In-domain test results. 
 SMT-based Our method 
SRL Local SRL + Local 
RP 50.7% 64.0% 62.6% 65.5% 
RC 13.5% 39.0% 33.3% 44.0% 
FA 6.1% 5.5% 4.0% 6.5% 
Table 8. Out-of-domain test results. 
 
1074
Intuitively, ?wear? and ?take? seem to fill the 
blank well, since they both form a collocation with 
?raincoat?; however, when ?with [AM-MNR] you? 
is considered as part of the context, ?wear? no 
longer fits it and ?take? wins. In this case, the long-
distance feature devised from AM-MNR helps se-
lect the suitable verb, while the trigram features 
cannot because they cannot represent the long dis-
tance verb usage context. 
We also find some typical cases that are beyond 
the reach of the SRL-derived features. For instance, 
consider ?Everyone doubts [suspects] that Tom is 
a spy.?. Both of the verbs can be followed by a 
clause. However, the SRL system regards ?is?, the 
predicate of the clause, as the patient, resulting in 
features like ?doubt_A1_is? and ?suspect_A1_is?, 
which capture nothing about verb usage context. 
However, if we consider the whole clause ?sus-
pect_Tom is a spy? as the patient, this could result 
in a very sparse feature that would be filtered. In 
the future, we will combine word-level and phrase-
level SRL systems to address this problem. 
Besides its incapability of handling verb selec-
tion errors involving clauses, the SRL-derived fea-
tures fail to work when verb selection depends on 
deep meanings that cannot be captured by current 
shallow predicate-argument structures. For exam-
ple, in ?He was wandering in the park, spending 
[killing] his time watching the children playing.?, 
though ?spending? and ?killing? fit the syntactic 
structure and collocation agreement, and express 
the meaning ?to allocate some time doing some-
thing?, the word ?wandering? suggests that ?kill-
ing? may be more appropriate. Current SRL 
systems cannot represent the semantic connection 
between two predicates and thus are helpless for 
this case. We argue that the performance of our 
system can be improved along with the progress of 
SRL. 
5 Conclusions and Future Work 
Verb selection is challenging because verb usage 
highly depends on the usage context, which is hard 
to capture and represent. In this paper, we propose 
to utilize the output of an SRL system to explicitly 
model verb usage context. We also propose to use 
the generalized perceptron learning framework to 
integrate SRL-derived features with other features. 
Experimental results show that our method outper-
forms the SMT-based system and achieves state-
of-the-art performance when SRL-related features 
and other local features are integrated. We also 
show that, for cases where the particular verb us-
age mainly depends on its distant arguments, a sys-
tem with only SRL-derived features performs 
much better than the system with only local fea-
tures. 
In the future, we plan to automatically construct 
confusion sets, expand our approach to more verbs 
and test our approach on a larger size of real data. 
We will try to combine the outputs of several SRL 
systems to make our system more robust. We also 
plan to further validate the effectiveness of the 
SRL-derived features under other learning methods 
like SVMs. 
Acknowledgment 
We thank the anonymous reviewers for their valu-
able comments. We also thank Changning Huang, 
Yunbo Cao, Dongdong Zhang, Henry Li and Mu 
Li for helpful discussions. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. Proc. of the 15th conference on 
Computational Linguistics, pages 32-38. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. Proc. of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational 
Linguistics, pages 249-256. 
Michael Collins. 2002. Discriminative training methods 
for hidden Markov models: theory and experiments 
with perceptron algorithms. Proc. of the ACL-02 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1-8. 
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic 
Grammar Checking for Second Language Learners ? 
the Use of Prepositions. Proc. of NoDaliDa. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitrtiy Belen-
ko, and Lucy Vanderwende. 2008. Using Contextual 
Speller Techniques and Language Modeling for ESL 
Error Correction. Proc. of the International Joint 
Conference on Natural Language Processing. 
Shichun Gui and Huizhong Yang. 2002. Chinese Learn-
er English Corpus. Shanghai Foreign Languages Ed-
ucation Press, Shanghai, China. 
1075
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. Proc. of the 36th Annual Meeting 
of the Association for Computational Linguistics and 
17th International Conference on Computational 
Linguistics, pages 519-525. 
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. Proc. of the 46th Annual Meeting 
on Association for Computational Linguistics, pages 
174-182. 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 
Changning Huang. 2000. PENS: A Machine-aided 
English Writing System for Chinese Users. Proc. of 
the 38th Annual Meeting on Association for Compu-
tational Linguistics, pages 529-536. 
Llu?s M?rquez. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 2009.   
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Proc. of the 
HLT-NAACL Workshop on Computational Lexical 
Semantics, pages 60-67. 
Srini Narayanan and Sanda Harabagiu. 2004. Question 
answering based on semantic structures. Proc. of the 
20th International Conference on Computational 
Linguistics, pages 693-701. 
Franz J. Och and Hermann Ney. 2004. The Alignment 
Template Approach to Statistical Machine Transla-
tion. Journal of Computational Linguistics, 30(4), 
pages 417-449. 
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective 
semantic role labelling with Markov Logic. Proc. of 
the Twelfth Conference on Computational Natural 
Language Learning, pages 193-197. 
Andreas Stolcke. 2002. SRILM -- An Extensible Lan-
guage Modeling Toolkit. Proc. of International Con-
ference on Spoken Language Processing, pages: 901-
904. 
Mihai Surdeanu, Lluis M?rquez, Xavier Carreras, and 
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence 
Research, page 105-151. 
Mihai Surdeanu, Sanda Harabagiu, John Williams, and 
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. Proc. of the 41st 
Annual Meeting on Association for Computational 
Linguistics, pages 8-15. 
Joel R. Tetreault and Martin Chodorow. 2008. The ups 
and downs of preposition error detection in ESL writ-
ing. Proc. of the 22nd international Conference on 
Computational Linguistics, pages 865-872. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Patrick Ye and Timothy Baldwin. 2006. Verb Sense 
Disambiguation Using Selectional Preferences 
Extracted with a State-of-the-art Semantic Role 
Labeler. Proc. of the Australasian Language 
Technology Workshop, pages 141-148. 
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A 
Web-based English Proofing System for English as a 
Second Language Users. Proc. of International Joint 
Conference on Natural Language Processing, pages 
619-624. 
1076
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 421?432, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Automatically Constructing a Normalisation Dictionary for Microblogs
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
Microblog normalisation methods often utilise
complex models and struggle to differenti-
ate between correctly-spelled unknown words
and lexical variants of known words. In this
paper, we propose a method for construct-
ing a dictionary of lexical variants of known
words that facilitates lexical normalisation via
simple string substitution (e.g. tomorrow for
tmrw). We use context information to generate
possible variant and normalisation pairs and
then rank these by string similarity. Highly-
ranked pairs are selected to populate the dic-
tionary. We show that a dictionary-based ap-
proach achieves state-of-the-art performance
for both F-score and word error rate on a stan-
dard dataset. Compared with other methods,
this approach offers a fast, lightweight and
easy-to-use solution, and is thus suitable for
high-volume microblog pre-processing.
1 Lexical Normalisation
A staggering number of short text ?microblog? mes-
sages are produced every day through social me-
dia such as Twitter (Twitter, 2011). The immense
volume of real-time, user-generated microblogs that
flows through sites has been shown to have utility
in applications such as disaster detection (Sakaki et
al., 2010), sentiment analysis (Jiang et al2011;
Gonza?lez-Iba?n?ez et al2011), and event discovery
(Weng and Lee, 2011; Benson et al2011). How-
ever, due to the spontaneous nature of the posts,
microblogs are notoriously noisy, containing many
non-standard forms ? e.g., tmrw ?tomorrow? and
2day ?today? ? which degrade the performance of
natural language processing (NLP) tools (Ritter et
al., 2010; Han and Baldwin, 2011). To reduce this
effect, attempts have been made to adapt NLP tools
to microblog data (Gimpel et al2011; Foster et al
2011; Liu et al2011b; Ritter et al2011). An al-
ternative approach is to pre-normalise non-standard
lexical variants to their standard orthography (Liu et
al., 2011a; Han and Baldwin, 2011; Xue et al2011;
Gouws et al2011). For example, se u 2morw!!!
would be normalised to see you tomorrow! The nor-
malisation approach is especially attractive as a pre-
processing step for applications which rely on key-
word match or word frequency statistics. For ex-
ample, earthqu, eathquake, and earthquakeee ? all
attested in a Twitter corpus ? have the standard
form earthquake; by normalising these types to their
standard form, better coverage can be achieved for
keyword-based methods, and better word frequency
estimates can be obtained.
In this paper, we focus on the task of lexical nor-
malisation of English Twitter messages, in which
out-of-vocabulary (OOV) tokens are normalised to
their in-vocabulary (IV) standard form, i.e., a stan-
dard form that is in a dictionary. Following other re-
cent work on lexical normalisation (Liu et al2011a;
Han and Baldwin, 2011; Gouws et al2011; Liu et
al., 2012), we specifically focus on one-to-one nor-
malisation in which one OOV token is normalised to
one IV word.
Naturally, not all OOV words in microblogs are
lexical variants of IV words: named entities, e.g.,
are prevalent in microblogs, but not all named en-
tities are included in our dictionary. One chal-
lenge for lexical normalisation is therefore to dis-
421
tinguish those OOV tokens that require normalisa-
tion from those that are well-formed. Recent un-
supervised approaches have not attempted to distin-
guish such tokens from other types of OOV tokens
(Cook and Stevenson, 2009; Liu et al2011a), lim-
iting their applicability to real-world normalisation
tasks. Other approaches (Han and Baldwin, 2011;
Gouws et al2011) have followed a cascaded ap-
proach in which lexical variants are first identified,
and then normalised. However, such two-step ap-
proaches suffer from poor lexical variant identifica-
tion performance, which is propagated to the nor-
malisation step. Motivated by the observation that
most lexical variants have an unambiguous standard
form (especially for longer tokens), and that a lexi-
cal variant and its standard form typically occur in
similar contexts, in this paper we propose methods
for automatically constructing a lexical normalisa-
tion dictionary ? a dictionary whose entries consist
of (lexical variant, standard form) pairs ? that en-
ables type-based normalisation.
Despite the simplicity of this dictionary-based
normalisation method, we show it to outperform
previously-proposed approaches. This very fast,
lightweight solution is suitable for real-time pro-
cessing of the large volume of streaming microblog
data available from Twitter, and offers a simple solu-
tion to the lexical variant detection problem that hin-
ders other normalisation methods. Furthermore, this
dictionary-based method can be easily integrated
with other more-complex normalisation approaches
(Liu et al2011a; Han and Baldwin, 2011; Gouws
et al2011) to produce hybrid systems.
After discussing related work in Section 2, we
present an overview of our dictionary-based ap-
proach to normalisation in Section 3. In Sections 4
and 5 we experimentally select the optimised con-
text similarity parameters and string similarity re-
ranking method. We present experimental results on
the unseen test data in Section 6, and offer some con-
cluding remarks in Section 7.
2 Related Work
Given a token t, lexical normalisation is the task
of finding argmaxP (s|t) ? argmaxP (t|s)P (s),
where s is the standard form, i.e., an IV word. Stan-
dardly in lexical normalisation, t is assumed to be an
OOV token, relative to a fixed dictionary. In prac-
tice, not all OOV tokens should be normalised; i.e.,
only lexical variants (e.g., tmrw ?tomorrow?) should
be normalised and tokens that are OOV but other-
wise not lexical variants (e.g., iPad ?iPad?) should
be unchanged. Most work in this area focuses only
on the normalisation task itself, oftentimes assuming
that the task of lexical variant detection has already
been completed.
Various approaches have been proposed to esti-
mate the error model, P (t|s). For example, in work
on spell-checking, Brill and Moore (2000) improve
on a standard edit-distance approach by consider-
ing multi-character edit operations; Toutanova and
Moore (2002) build on this by incorporating phono-
logical information. Li et al2006) utilise distri-
butional similarity (Lin, 1998) to correct misspelled
search queries.
In text message normalisation, Choudhury et al
(2007) model the letter transformations and emis-
sions using a hidden Markov model (Rabiner, 1989).
Cook and Stevenson (2009) and Xue et al2011)
propose multiple simple error models, each of which
captures a particular way in which lexical variants
are formed, such as phonetic spelling (e.g., epik
?epic?) or clipping (e.g., walkin ?walking?). Never-
theless, optimally weighting the various error mod-
els in these approaches is challenging.
Without pre-categorising lexical variants into dif-
ferent types, Liu et al2011a) collect Google
search snippets from carefully-designed queries
from which they then extract noisy lexical variant?
standard form pairs. These pairs are used to train
a conditional random field (Lafferty et al2001) to
estimate P (t|s) at the character level. One short-
coming of querying a search engine to obtain train-
ing pairs is it tends to be costly in terms of time and
bandwidth. Here we exploit microblog data directly
to derive (lexical variant, standard form) pairs, in-
stead of relying on external resources. In more-
recent work, Liu et al2012) endeavour to improve
the accuracy of top-n normalisation candidates by
integrating human cognitive inference, character-
level transformations and spell checking in their nor-
malisation model. The encouraging results shift the
focus to reranking and promoting the correct nor-
malisation to the top-1 position. However, like much
previous work on lexical normalisation, this work
422
assumes perfect lexical variant detection.
Aw et al2006) and Kaufmann and Kalita (2010)
consider normalisation as a machine translation task
from lexical variants to standard forms using off-the-
shelf tools. These methods do not assume that lexi-
cal variants have been pre-identified; however, these
methods do rely on large quantities of labelled train-
ing data, which is not available for microblogs.
Recently, Han and Baldwin (2011) and Gouws
et al2011) propose two-step unsupervised ap-
proaches to normalisation, in which lexical vari-
ants are first identified, and then normalised. They
approach lexical variant detection by using a con-
text fitness classifier (Han and Baldwin, 2011) or
through dictionary lookup (Gouws et al2011).
However, the lexical variant detection of both meth-
ods is rather unreliable, indicating the challenge
of this aspect of normalisation. Both of these
approaches incorporate a relatively small normal-
isation dictionary to capture frequent lexical vari-
ants with high precision. In particular, Gouws et
al. (2011) produce a small normalisation lexicon
based on distributional similarity and string simi-
larity (Lodhi et al2002). Our method adopts a
similar strategy using distributional/string similarity,
but instead of constructing a small lexicon for pre-
processing, we build a much wider-coverage nor-
malisation dictionary and opt for a fully lexicon-
based end-to-end normalisation approach. In con-
trast to the normalisation dictionaries of Han and
Baldwin (2011) and Gouws et al2011) which fo-
cus on very frequent lexical variants, we focus on
moderate frequency lexical variants of a minimum
character length, which tend to have unambiguous
standard forms; our intention is to produce normali-
sation lexicons that are complementary to those cur-
rently available. Furthermore, we investigate the im-
pact of a variety of contextual and string similarity
measures on the quality of the resulting lexicons.
In summary, our dictionary-based normalisation ap-
proach is a lightweight end-to-end method which
performs both lexical variant detection and normal-
isation, and thus is suitable for practical online pre-
processing, despite its simplicity.
3 A Lexical Normalisation Dictionary
Before discussing our method for creating a normal-
isation dictionary, we first discuss the feasibility of
such an approach.
3.1 Feasibility
Dictionary lookup approaches to normalisation have
been shown to have high precision but low recall
(Han and Baldwin, 2011; Gouws et al2011). Fre-
quent (lexical variant, standard form) pairs such as
(u, you) are typically included in the dictionaries
used by such methods, while less-frequent items
such as (g0tta, gotta) are generally omitted. Be-
cause of the degree of lexical creativity and large
number of non-standard forms observed on Twitter,
a wide-coverage normalisation dictionary would be
expensive to construct manually. Based on the as-
sumption that lexical variants occur in similar con-
texts to their standard forms, however, it should
be possible to automatically construct a normalisa-
tion dictionary with wider coverage than is currently
available.
Dictionary lookup is a type-based approach to
normalisation, i.e., every token instance of a given
type will always be normalised in the same way.
However, lexical variants can be ambiguous, e.g., y
corresponds to ?you? in yeah, y r right! LOL but
?why? in AM CONFUSED!!! y you did that? Nev-
ertheless, the relative occurrence of ambiguous lex-
ical variants is small (Liu et al2011a), and it has
been observed that while shorter variants such as y
are often ambiguous, longer variants tend to be un-
ambiguous. For example bthday and 4eva are un-
likely to have standard forms other than ?birthday?
and ?forever?, respectively. Therefore, the normali-
sation lexicons we produce will only contain entries
for OOVs with character length greater than a spec-
ified threshold, which are likely to have an unam-
biguous standard form.
3.2 Overview of approach
Our method for constructing a normalisation dictio-
nary is as follows:
Input: Tokenised English tweets
1. Extract (OOV, IV) pairs based on distributional
similarity.
423
2. Re-rank the extracted pairs by string similarity.
Output: A list of (OOV, IV) pairs ordered by string
similarity; select the top-n pairs for inclusion in
the normalisation lexicon.
In Step 1, we leverage large volumes of Twitter
data to identify the most distributionally-similar IV
type for each OOV type. The result of this pro-
cess is a set of (OOV, IV) pairs, ranked by dis-
tributional similarity. The extracted pairs will in-
clude (lexical variant, standard form) pairs, such as
(tmrw, tomorrow), but will also contain false posi-
tives such as (Tusday, Sunday) ? Tusday is a lexical
variant, but its standard form is not ?Sunday? ? and
(Youtube,web) ? Youtube is an OOV named en-
tity, not a lexical variant. Nevertheless, lexical vari-
ants are typically formed from their standard forms
through regular processes (Thurlow, 2003) ? e.g.,
the omission of characters ? and from this per-
spective Sunday and web are not plausible standard
forms for Tusday and Youtube, respectively. In Step
2, we therefore capture this intuition to re-rank the
extracted pairs by string similarity. The top-n items
in this re-ranked list then form the normalisation lex-
icon, which is based only on development data.
Although computationally-expensive to build,
this dictionary can be created offline. Once built,
it then offers a very fast approach to normalisation.
We can only reliably compute distributional simi-
larity for types that are moderately frequent in a cor-
pus. Nevertheless, many lexical variants are suffi-
ciently frequent to be able to compute distributional
similarity, and can potentially make their way into
our normalisation lexicon. This approach is not suit-
able for normalising low-frequency lexical variants,
nor is it suitable for shorter lexical variant types
which ? as discussed in Section 3.1 ? are more
likely to have an ambiguous standard form. Never-
theless, previously-proposed normalisation methods
that can handle such phenomena also rely in part on
a normalisation lexicon. The normalisation lexicons
we create can therefore be easily integrated with pre-
vious approaches to form hybrid normalisation sys-
tems.
4 Contextually-similar Pair Generation
Our objective is to extract contextually-similar
(OOV, IV) pairs from a large-scale collection of mi-
croblog data. Fundamentally, the surrounding words
define the primary context, but there are different
ways of representing context and different similar-
ity measures we can use, which may influence the
quality of generated normalisation pairs.
In representing the context, we experimentally ex-
plore the following factors: (1) context window size
(from 1 to 3 tokens on both sides); (2) n-gram or-
der of the context tokens (unigram, bigram, trigram);
(3) whether context words are indexed for relative
position or not; and (4) whether we use all context
tokens, or only IV words. Because high-accuracy
linguistic processing tools for Twitter are still under
exploration (Liu et al2011b; Gimpel et al2011;
Ritter et al2011; Foster et al2011), we do not
consider richer representations of context, for exam-
ple, incorporating information about part-of-speech
tags or syntax. We also experiment with a number
of simple but widely-used geometric and informa-
tion theoretic distance/similarity measures. In par-
ticular, we use Kullback?Leibler (KL) divergence
(Kullback and Leibler, 1951), Jensen?Shannon (JS)
divergence (Lin, 1991), Euclidean distance and Co-
sine distance.
We use a corpus of 10 million English tweets to do
parameter tuning over, and a larger corpus of tweets
in the final candidate ranking. All tweets were col-
lected from September 2010 to January 2011 via
the Twitter API.1 From the raw data we extract
English tweets using a language identification tool
(Lui and Baldwin, 2011), and then apply a simpli-
fied Twitter tokeniser (adapted from O?Connor et al
(2010)). We use the Aspell dictionary (v6.06)2 to
determine whether a word is IV, and only include
in our normalisation dictionary OOV tokens with
at least 64 occurrences in the corpus and character
length ? 4, both of which were determined through
empirical observation. For each OOV word type in
the corpus, we select the most similar IV type to
form (OOV, IV) pairs. To further narrow the search
space, we only consider IV words which are mor-
phophonemically similar to the OOV type, follow-
ing settings in Han and Baldwin (2011).3
1https://dev.twitter.com/docs/
streaming-api/methods
2http://aspell.net/
3We only consider IV words within an edit distance of 2 or a
phonemic edit distance of 1 from the OOV type, and we further
424
In order to evaluate the generated pairs, we ran-
domly selected 1000 OOV words from the 10 mil-
lion tweet corpus. We set up an annotation task
on Amazon Mechanical Turk,4 presenting five in-
dependent annotators with each word type (with no
context) and asking for corrections where appropri-
ate. For instance, given tmrw, the annotators would
likely identify it as a non-standard variant of ?to-
morrow?. For correct OOV words like iPad, on the
other hand, we would expect them to leave the word
unchanged. If 3 or more of the 5 annotators make
the same suggestion (in the form of either a canoni-
cal spelling or leaving the word unchanged), we in-
clude this in our gold standard for evaluation. In
total, this resulted in 351 lexical variants and 282
correct OOV words, accounting for 63.3% of the
1000 OOV words. These 633 OOV words were used
as (OOV, IV) pairs for parameter tuning. The re-
mainder of the 1000 OOV words were ignored on
the grounds that there was not sufficient consensus
amongst the annotators.5
Contextually-similar pair generation aims to in-
clude as many correct normalisation pairs as pos-
sible. We evaluate the quality of the normalisation
pairs using ?Cumulative Gain? (CG):
CG =
N ??
i=1
rel?i
Suppose there are N ? correct generated pairs
(oovi, ivi), each of which is weighted by rel?i, the
frequency of oovi to indicate its relative importance;
for example, (thinkin, thinking) has a higher weight
than (g0tta, gotta) because thinkin is more frequent
than g0tta in our corpus. In this evaluation we don?t
consider the position of normalisation pairs, and nor
do we penalise incorrect pairs. Instead, we push dis-
tinguishing between correct and incorrect pairs into
the downstream re-ranking step in which we incor-
porate string similarity information.
Given the development data and CG, we run an
exhaustive search of parameter combinations over
only consider the top 30% most-frequent of these IV words.
4https://www.mturk.com/mturk/welcome
5Note that the objective of this annotation task is to identify
lexical variants that have agreed-upon standard forms irrespec-
tive of context, as a special case of the more general task of
lexical normalisation (where context may or may not play a sig-
nificant role in the determination of the normalisation).
our development corpus. The five best parameter
combinations are shown in Table 1. We notice the
CG is almost identical for the top combinations. As
a context window size of 3 incurs a heavy process-
ing and memory overhead over a size of 2, we use
the 3rd-best parameter combination for subsequent
experiments, namely: context window of?2 tokens,
token bigrams, positional index, and KL divergence
as our distance measure.
To better understand the sensitivity of the method
to each parameter, we perform a post-hoc parame-
ter analysis relative to a default setting (as under-
lined in Table 2), altering one parameter at a time.
The results in Table 2 show that bigrams outper-
form other n-gram orders by a large margin (note
that the evaluation is based on a log scale), and
information-theoretic measures are superior to the
geometric measures. Furthermore, it also indicates
using the positional indexing better captures context.
However, there is little to distinguish context mod-
elling with just IV words or all tokens. Similarly,
the context window size has relatively little impact
on the overall performance, supporting our earlier
observation from Table 1.
5 Pair Re-ranking by String Similarity
Once the contextually-similar (OOV, IV) pairs are
generated using the selected parameters in Section
4, we further re-rank this set of pairs in an at-
tempt to boost morphophonemically-similar pairs
like (bananaz, bananas), and penalise noisy pairs
like (paninis, beans).
Instead of using the small 10 million tweet cor-
pus, from this step onwards, we use a larger cor-
pus of 80 million English tweets (collected over the
same period as the development corpus) to develop
a larger-scale normalisation dictionary. This is be-
cause once pairs are generated, re-ranking based on
string comparison is much faster. We only include
in the dictionary OOV words with a token frequency
> 15 to include more OOV types than in Section 4,
and again apply a minimum length cutoff of 4 char-
acters.
To measure how well our re-ranking method pro-
motes correct pairs and demotes incorrect pairs (in-
cluding both OOV words that should not be nor-
malised, e.g. (Youtube,web), and incorrect normal-
425
Rank Window size n-gram Positional index? Lex. choice Sim/distance measure log(CG)
1 ?3 2 Yes All KL divergence 19.571
2 ?3 2 No All KL divergence 19.562
3 ?2 2 Yes All KL divergence 19.562
4 ?3 2 Yes IVs KL divergence 19.561
5 ?2 2 Yes IVs JS divergence 19.554
Table 1: The five best parameter combinations in the exhaustive search of parameter combinations
Window size n-gram Positional index? Lexical choice Similarity/distance measure
?1 19.325 1 19.328 Yes 19.328 IVs 19.335 KL divergence 19.328
?2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227
?3 19.328 3 19.324 JS divergence 19.311
Cosine 19.170
Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs. We tune one parameter at
a time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in each
case is indicated in bold.
isations for lexical variants, e.g. (bcuz, cause)), we
modify our evaluation metric from Section 4 to
evaluate the ranking at different points, using Dis-
counted Cumulative Gain (DCG@N : Jarvelin and
Kekalainen (2002)):
DCG@N = rel1 +
N?
i=2
reli
log2 (i)
where reli again represents the frequency of the
OOV, but it can be gain (a positive number) or loss
(a negative number), depending on whether the ith
pair is correct or incorrect. Because we also expect
correct pairs to be ranked higher than incorrect pairs,
DCG@N takes both factors into account.
Given the generated pairs and the evaluation met-
ric, we first consider three baselines: no re-ranking
(i.e., the final ranking is that of the contextual simi-
larity scores), and re-rankings of the pairs based on
the frequencies of the OOVs in the Twitter corpus,
and the IV unigram frequencies in the Google Web
1T corpus (Brants and Franz, 2006) to get less-noisy
frequency estimates. We also compared a variety of
re-rankings based on a number of string similarity
measures that have been previously considered in
normalisation work (reviewed in Section 2). We ex-
periment with standard edit distance (Levenshtein,
1966), edit distance over double metaphone codes
(phonetic edit distance: (Philips, 2000)), longest
common subsequence ratio over the consonant edit
distance of the paired words (hereafter, denoted as
consonant edit distance: (Contractor et al2010)),
and a string subsequence kernel (Lodhi et al2002).
In Figure 1, we present the DCG@N results for
each of our ranking methods at different rank cut-
offs. Ranking by OOV frequency is motivated by
the assumption that lexical variants are frequently
used by social media users. This is confirmed
by our findings that lexical pairs like (goin, going)
and (nite, night) are at the top of the ranking.
However, many proper nouns and named entities
are also used frequently and ranked at the top,
mixed with lexical variants like (Facebook, speech)
and (Youtube,web). In ranking by IV word fre-
quency, we assume the lexical variants are usually
derived from frequently-used IV equivalents, e.g.
(abou, about). However, many less-frequent lexical
variant types have high-frequency (IV) normalisa-
tions. For instance, the highest-frequency IV word
the has more than 40 OOV lexical variants, such as
tthe and thhe. These less-frequent types occupy the
top positions, reducing the cumulative gain. Com-
pared with these two baselines, ranking by default
contextual similarity scores delivers promising re-
sults. It successfully ranks many more intuitive nor-
malisation pairs at the top, such as (2day, today)
and (wknd,weekend), but also ranks some incorrect
pairs highly, such as (needa, gotta).
The string similarity-based methods perform bet-
ter than our baselines in general. Through man-
ual analysis, we found that standard edit dis-
426
tance ranking is fairly accurate for lexical vari-
ants with low edit distance to their standard forms,
but fails to identify heavily-altered variants like
(tmrw, tomorrow). Consonant edit distance is simi-
lar to standard edit distance, but places many longer
words at the top of the ranking. Edit distance
over double metaphone codes (phonetic edit dis-
tance) performs particularly well for lexical vari-
ants that include character repetitions ? commonly
used for emphasis on Twitter ? because such rep-
etitions do not typically alter the phonetic codes.
Compared with the other methods, the string subse-
quence kernel delivers encouraging results. It mea-
sures common character subsequences of length n
between (OOV, IV) pairs. Because it is computa-
tionally expensive to calculate similarity for larger
n, we choose n=2, following Gouws et al2011).
As N (the lexicon size cut-off) increases, the per-
formance drops more slowly than the other meth-
ods. Although this method fails to rank heavily-
altered variants such as (4get, forget) highly, it typi-
cally works well for longer words. Given that we fo-
cus on longer OOVs (specifically those longer than
4 characters), this ultimately isn?t a great handicap.
6 Evaluation
Given the re-ranked pairs from Section 5, here we
apply them to a token-level normalisation task us-
ing the normalisation dataset of Han and Baldwin
(2011).
6.1 Metrics
We evaluate using the standard evaluation metrics of
precision (P), recall (R) and F-score (F) as detailed
below. We also consider the false alarm rate (FA)
and word error rate (WER), also as shown below.
FA measures the negative effects of applying nor-
malisation; a good approach to normalisation should
not (incorrectly) normalise tokens that are already
in their standard form and do not require normalisa-
tion.6 WER, like F-score, shows the overall benefits
of normalisation, but unlike F-score, measures how
many token-level edits are required for the output to
be the same as the ground truth data. In general, dic-
tionaries with a high F-score/low WER and low FA
6FA + P ? 1 because some lexical variants might be incor-
rectly normalised.
are preferable.
P =
# correctly normalised tokens
# normalised tokens
R =
# correctly normalised tokens
# tokens requiring normalisation
F =
2PR
P +R
FA =
# incorrectly normalised tokens
# normalised tokens
WER =
# token edits needed after normalisation
# all tokens
6.2 Results
We select the three best re-ranking methods, and
best cut-off N for each method, based on the
highest DCG@N value for a given method over
the development data, as presented in Figure 1.
Namely, they are string subsequence kernel (S-dict,
N=40,000), double metaphone edit distance (DM-
dict, N=10,000) and default contextual similarity
without re-ranking (C-dict, N=10,000).7
We evaluate each of the learned dictionaries in Ta-
ble 3. We also compare each dictionary with the
performance of the manually-constructed Internet
slang dictionary (HB-dict) used by Han and Bald-
win (2011), the small automatically-derived dictio-
nary of Gouws et al2011) (GHM-dict), and com-
binations of the different dictionaries. In addition,
the contribution of these dictionaries in hybrid nor-
malisation approaches is also presented, in which we
first normalise OOVs using a given dictionary (com-
bined or otherwise), and then apply the normalisa-
tion method of Gouws et al2011) based on con-
sonant edit distance (GHM-norm), or the approach
of Han and Baldwin (2011) based on the summation
of many unsupervised approaches (HB-norm), to the
remaining OOVs. Results are shown in Table 3, and
discussed below.
6.2.1 Individual Dictionaries
Overall, the individual dictionaries derived by the
re-ranking methods (DM-dict, S-dict) perform bet-
7We also experimented with combining ranks using Mean
Reciprocal Rank. However, the combined rank didn?t improve
performance on the development data. We plan to explore other
ranking aggregation methods in future work.
427
N
 c
ut
?o
ffs
Discounted Cumulative Gain
10K
30K
50K
70K
90K
110K
130K
150K
170K
190K
?
60
K
?
40
K
?
20
K0
20
K
40
K
W
ith
ou
t r
er
a
n
k
O
OV
 fr
eq
ue
nc
y
IV
 fr
eq
ue
nc
y
Ed
it 
di
st
an
ce
Co
ns
on
an
t e
di
t d
ist
.
Ph
on
et
ic
 e
di
t d
ist
.
St
rin
g 
su
bs
eq
. k
e
rn
e
l
Figure 1: Re-ranking based on different string similarity methods.
ter than that based on contextual similarity (C-dict)
in terms of precision and false alarm rate, indicating
the importance of re-ranking. Even though C-dict
delivers higher recall ? indicating that many lexi-
cal variants are correctly normalised ? this is offset
by its high false alarm rate, which is particularly un-
desirable in normalisation. Because S-dict has better
performance than DM-dict in terms of both F-score
and WER, and a much lower false alarm rate than
C-dict, subsequent results are presented using S-dict
only.
Both HB-dict and GHM-dict achieve better than
90% precision with moderate recall. Compared to
these methods, S-dict is not competitive in terms of
either precision or recall. This result seems rather
discouraging. However, considering that S-dict is an
automatically-constructed dictionary targeting lexi-
cal variants of varying frequency, it is not surprising
that the precision is worse than that of HB-dict ?
which is manually-constructed ? and GHM-dict ?
which includes entries only for more-frequent OOVs
for which distributional similarity is more accurate.
Additionally, the recall of S-dict is hampered by the
restriction on lexical variant token length of 4 char-
acters.
6.2.2 Combined Dictionaries
Next we look to combining HB-dict, GHM-dict
and S-dict. In combining the dictionaries, a given
OOV word can be listed with different standard
forms in different dictionaries. In such cases we use
the following preferences for dictionaries ? moti-
vated by our confidence in the normalisation pairs
of the dictionaries ? to resolve conflicts: HB-dict
> GHM-dict > S-dict.
When we combine dictionaries in the second sec-
tion of Table 3, we find that they contain com-
plementary information: in each case the recall
and F-score are higher for the combined dictio-
nary than any of the individual dictionaries. The
combination of HB-dict+GHM-dict produces only
a small improvement in terms of F-score over HB-
dict (the better-performing dictionary) suggesting
that, as claimed, HB-dict and GHM-dict share many
frequent normalisation pairs. HB-dict+S-dict and
GHM-dict+S-dict, on the other hand, improve sub-
428
Method Precision Recall F-Score False Alarm Word Error Rate
C-dict 0.474 0.218 0.299 0.298 0.103
DM-dict 0.727 0.106 0.185 0.145 0.102
S-dict 0.700 0.179 0.285 0.162 0.097
HB-dict 0.915 0.435 0.590 0.048 0.066
GHM-dict 0.982 0.319 0.482 0.000 0.076
HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052
GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061
HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063
HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049
GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135
HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124
HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081
HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079
Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone ren-
dering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al2011) (GHM-dict), the
Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition,
we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined
unsupervised approach of Han and Baldwin (2011) (HB-norm).
stantially over HB-dict and GHM-dict, respectively,
indicating that S-dict contains markedly different
entries to both HB-dict and GHM-dict. The best F-
score and WER are obtained using the combination
of all three dictionaries, HB-dict+GHM-dict+S-dict.
Furthermore, the difference between the results us-
ing HB-dict+GHM-dict+S-dict and HB-dict+GHM-
dict is statistically significant (p < 0.01), based on
the computationally-intensive Monte Carlo method
of Yeh (2000), demonstrating the contribution of S-
dict.
6.2.3 Hybrid Approaches
The methods of Gouws et al2011) (i.e.
GHM-dict+GHM-norm) and Han and Baldwin
(2011) (i.e. HB-dict+HB-norm) have lower preci-
sion and higher false alarm rates than the dictionary-
based approaches; this is largely caused by lex-
ical variant detection errors.8 Using all dic-
tionaries in combination with these methods ?
HB-dict+GHM-dict+S-dict+GHM-norm and HB-
dict+GHM-dict+S-dict+HB-norm ? gives some
improvements, but the false alarm rates remain high.
Despite the limitations of a pure dictionary-based
approach to normalisation ? discussed in Section
3.1 ? the current best practical approach to normal-
8Here we report results that do not assume perfect detection
of lexical variants, unlike the original published results in each
case.
Error type OOV
Standard form
Dict. Gold
(a) plurals playe players player
(b) negation unlike like dislike
(c) possessives anyones anyone anyone?s
(d) correct OOVs iphone phone iphone
(e) test data errors durin during durin
(f) ambiguity siging signing singing
Table 4: Error types in the combined dictionary (HB-
dict+GHM-dict+S-dict)
isation is to use a lexicon, combining hand-built and
automatically-learned normalisation dictionaries.
6.3 Discussion and Error Analysis
We first manually analyse the errors in the combined
dictionary (HB-dict+GHM-dict+S-dict) and give ex-
amples of each error type in Table 4. The most fre-
quent word errors are caused by slight morphologi-
cal variations, including plural forms (a), negations
(b), possessive cases (c), and OOVs that are correct
and do not require normalisation (d). In addition, we
also notice some missing annotations where lexical
variants are skipped by human annotations but cap-
tured by our method (e). Ambiguity (f) definitely
exists in longer OOVs, however, these cases do not
appear to have a strong negative impact on the nor-
malisation performance. An example of a remain-
429
Length cut-off (N ) #Variants Precision Recall (? N ) Recall (all) False Alarm
?4 556 0.700 0.381 0.179 0.162
?5 382 0.814 0.471 0.152 0.122
?6 254 0.804 0.484 0.104 0.131
?7 138 0.793 0.471 0.055 0.122
Table 5: S-dict normalisation results broken down according to OOV token length. Recall is presented both over the
subset of instances of length ? N in the data (?Recall (? N )?), and over the entirety of the dataset (?Recall (all)?);
?#Variants? is the number of token instances of the indicated length in the test dataset.
ing miscellaneous error is bday ?birthday?, which is
mis-normalised as day.
To further study the influence of OOV word
length relative to the normalisation performance, we
conduct a fine-grained analysis of the performance
of the derived dictionary (S-dict) in Table 5, bro-
ken down across different OOV word lengths. The
results generally support our hypothesis that our
method works better for longer OOV words. The
derived dictionary is much more reliable for longer
tokens (length 5, 6, and 7 characters) in terms of pre-
cision and false alarm. Although the recall is rela-
tively modest, in the future we intend to improve re-
call by mining more normalisation pairs from larger
collections of microblog data.
7 Conclusions and Future Work
In this paper, we describe a method for automat-
ically constructing a normalisation dictionary that
supports normalisation of microblog text through di-
rect substitution of lexical variants with their stan-
dard forms. After investigating the impact of dif-
ferent distributional and string similarity methods
on the quality of the dictionary, we present ex-
perimental results on a standard dataset showing
that our proposed methods acquire high quality
(lexical variant, standard form) pairs, with reason-
able coverage, and achieve state-of-the-art end-to-
end lexical normalisation performance on a real-
world token-level task. Furthermore, this dictionary-
lookup method combines the detection and normali-
sation of lexical variants into a simple, lightweight
solution which is suitable for processing of high-
volume microblog feeds.
In the future, we intend to improve our dictionary
by leveraging the constantly-growing volume of mi-
croblog data, and considering alternative ways to
combine distributional and string similarity. In addi-
tion to direct evaluation, we also want to explore the
benefits of applying normalisation for downstream
social media text processing applications, e.g. event
detection.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their insightful comments, and Stephan
Gouws for kindly sharing his data and discussing his
work.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT centre of Excel-
lence programme.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING/ACL 2006, pages
33?40, Sydney, Australia.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT 2011), pages 389?398, Port-
land, Oregon, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 286?293,
Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157?174.
430
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
pages 189?196, Beijing, China.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ?09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78, Boulder, USA.
Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,
Joseph L. Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS Tagging and Parsing the Twitterverse.
In Analyzing Microtext: Papers from the 2011 AAAI
Workshop, volume WS-11-05 of AAAI Workshops,
pages 20?25, San Francisco, CA, USA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 42?47,
Portland, Oregon, USA.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in Twitter:
a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-HLT
2011), pages 581?586, Portland, Oregon, USA.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82?90, Edinburgh,
Scotland, UK.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 368?378,
Portland, Oregon, USA.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
151?160, Portland, Oregon, USA.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22:49?
86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of COL-
ING/ACL 2006, pages 1025?1032, Sydney, Australia.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 36th An-
nual Meeting of the ACL and 17th International Con-
ference on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Quebec, Canada.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
71?76, Portland, Oregon, USA.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 359?367,
Portland, Oregon, USA.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012), Jeju, Republic of Korea.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. J. Mach. Learn. Res., 2:419?
444.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
431
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for Twitter. In Proceedings of the 4th In-
ternational Conference on Weblogs and Social Media
(ICWSM 2010), pages 384?385, Washington, USA.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38?43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2010), pages 172?180, Los Angeles,
USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 1524?1534, Edinburgh,
Scotland, UK.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on the World Wide
Web (WWW 2010), pages 851?860, Raleigh, North
Carolina, USA.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the
ACL and 3rd Annual Meeting of the NAACL (ACL-02),
pages 144?151, Philadelphia, USA.
Official Blog Twitter. 2011. 200 million tweets per day.
Retrived at August 17th, 2011.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the 5th International
Conference on Weblogs and Social Media (ICWSM
2011), Barcelona, Spain.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74?79,
San Francisco, USA.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), pages 947?953,
Saarbru?cken, Germany.
432
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?72,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Support Platform for Event Detection using Social Intelligence
Timothy Baldwin, Paul Cook, Bo Han, Aaron Harwood,
Shanika Karunasekera and Masud Moshtaghi
Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
Abstract
This paper describes a system designed
to support event detection over Twitter.
The system operates by querying the data
stream with a user-specified set of key-
words, filtering out non-English messages,
and probabilistically geolocating each mes-
sage. The user can dynamically set a proba-
bility threshold over the geolocation predic-
tions, and also the time interval to present
data for.
1 Introduction
Social media and micro-blogs have entered the
mainstream of society as a means for individu-
als to stay in touch with friends, for companies
to market products and services, and for agen-
cies to make official announcements. The attrac-
tions of social media include their reach (either
targeted within a social network or broadly across
a large user base), ability to selectively pub-
lish/filter information (selecting to publish cer-
tain information publicly or privately to certain
groups, and selecting which users to follow),
and real-time nature (information ?push? happens
immediately at a scale unachievable with, e.g.,
email). The serendipitous takeoff in mobile de-
vices and widespread support for social media
across a range of devices, have been significant
contributors to the popularity and utility of social
media.
While much of the content on micro-blogs de-
scribes personal trivialities, there is also a vein of
high-value content ripe for mining. As such, or-
ganisations are increasingly targeting micro-blogs
for monitoring purposes, whether it is to gauge
product acceptance, detect events such as traffic
jams, or track complex unfolding events such as
natural disasters.
In this work, we present a system intended
to support real-time analysis and geolocation of
events based on Twitter. Our system consists of
the following steps: (1) user selection of key-
words for querying Twitter; (2) preprocessing of
the returned queries to rapidly filter out messages
not in a pre-selected set of languages, and option-
ally normalise language content; (3) probabilistic
geolocation of messages; and (4) rendering of the
data on a zoomable map via a purpose-built web
interface, with facility for rich user interaction.
Our starting in the development of this system
was the Ushahidi platform,1 which has high up-
take for social media surveillance and information
dissemination purposes across a range of organ-
isations. The reason for us choosing to imple-
ment our own platform was: (a) ease of integra-
tion of back-end processing modules; (b) extensi-
bility, e.g. to visualise probabilities of geolocation
predictions, and allow for dynamic thresholding;
(c) code maintainability; and (d) greater logging
facility, to better capture user interactions.
2 Example System Usage
A typical user session begins with the user spec-
ifying a disjunctive set of keywords, which are
used as the basis for a query to the Twitter
Streaming API.2 Messages which match the query
are dynamically rendered on an OpenStreetMap
mash-up, indexed based on (grid cell-based) loca-
tion. When the user clicks on a location marker,
they are presented with a pop-up list of messages
matching the location. The user can manipulate a
time slider to alter the time period over which to
present results (e.g. in the last 10 minutes, or over
1http://ushahidi.com/
2https://dev.twitter.com/docs/
streaming-api
69
Figure 1: A screenshot of the system, with a pop-up presentation of the messages at the indicated location.
the last hour), to gain a better sense of report re-
cency. The user can further adjust the threshold of
the prediction accuracy for the probabilistic mes-
sage locations to view a smaller number of mes-
sages with higher-confidence locations, or more
messages that have lower-confidence locations.
A screenshot of the system for the following
query is presented in Figure 1:
study studying exam ?end of semester?
examination test tests school exams uni-
versity pass fail ?end of term? snow
snowy snowdrift storm blizzard flurry
flurries ice icy cold chilly freeze freez-
ing frigid winter
3 System Details
The system is composed of a front-end, which
provides a GUI interface for query parameter in-
put, and a back-end, which computes a result for
each query. The front-end submits the query pa-
rameters to the back-end via a servlet. Since
the result for the query is time-dependent, the
back-end regularly re-evaluates the query, gener-
ating an up-to-date result at regular intervals. The
front-end regularly polls the back-end, via another
servlet, for the latest results that match its submit-
ted query. In this way, the front-end and back-end
are loosely coupled and asynchronous.
Below, we describe details of the various mod-
ules of the system.
3.1 Twitter Querying
When the user inputs a set of keywords, this is is-
sued as a disjunctive query to the Twitter Stream-
ing API, which returns a streamed set of results
in JSON format. The results are parsed, and
piped through to the language filtering, lexical
normalisation, and geolocation modules, and fi-
nally stored in a flat file, which the GUI interacts
with.
3.2 Language Filtering
For language identification, we use langid.py,
a language identification toolkit developed at
The University of Melbourne (Lui and Baldwin,
2011).3 langid.py combines a naive Bayes
classifier with cross-domain feature selection to
provide domain-independent language identifica-
tion. It is available under a FOSS license as
a stand-alone module pre-trained over 97 lan-
guages. langid.py has been developed specif-
ically to be able to keep pace with the speed
of messages through the Twitter ?garden hose?
feed on a single-CPU machine, making it par-
ticularly attractive for this project. Additionally,
in an in-house evaluation over three separate cor-
pora of Twitter data, we have found langid.py
to be overall more accurate than other state-of-
the-art language identification systems such as
3http://www.csse.unimelb.edu.au/
research/lt/resources/langid
70
lang-detect4 and the Compact Language De-
tector (CLD) from the Chrome browser.5
langid.py returns a monolingual prediction
of the language content of a given message, and is
used to filter out all non-English tweets.
3.3 Lexical Normalisation
The prevalence of noisy tokens in microblogs
(e.g. yr ?your? and soooo ?so?) potentially hin-
ders the readability of messages. Approaches
to lexical normalisation?i.e., replacing noisy to-
kens by their standard forms in messages (e.g.
replacing yr with your)?could potentially over-
come this problem. At present, lexical normali-
sation is an optional plug-in for post-processing
messages.
A further issue related to noisy tokens is that
it is possible that a relevant tweet might contain
a variant of a query term, but not that query term
itself. In future versions of the system we there-
fore aim to use query expansion to generate noisy
versions of query terms to retrieve additional rel-
evant tweets. We subsequently intend to perform
lexical normalisation to evaluate the precision of
the returned data.
The present lexical normalisation used by our
system is the dictionary lookup method of Han
and Baldwin (2011) which normalises noisy to-
kens only when the normalised form is known
with high confidence (e.g. you for u). Ultimately,
however, we are interested in performing context-
sensitive lexical normalisation, based on a reim-
plementation of the method of Han and Baldwin
(2011). This method will allow us to target a
wider variety of noisy tokens such as typos (e.g.
earthquak ?earthquake?), abbreviations (e.g. lv
?love?), phonetic substitutions (e.g. b4 ?before?)
and vowel lengthening (e.g. goooood ?good?).
3.4 Geolocation
A vital component of event detection is the de-
termination of where the event is happening, e.g.
to make sense of reports of traffic jams or floods.
While Twitter supports device-based geotagging
of messages, less than 1% of messages have geo-
tags (Cheng et al 2010). One alternative is to re-
turn the user-level registered location as the event
4http://code.google.com/p/
language-detection/
5http://code.google.com/p/
chromium-compact-language-detector/
location, based on the assumption that most users
report on events in their local domicile. However,
only about one quarter of users have registered lo-
cations (Cheng et al 2010), and even when there
is a registered location, there?s no guarantee of
its quality. A better solution would appear to be
the automatic prediction of the geolocation of the
message, along with a probabilistic indication of
the prediction quality.6
Geolocation prediction is based on the terms
used in a given message, based on the assumption
that it will contain explicit mentions of local place
names (e.g. London) or use locally-identifiable
language (e.g. jawn, which is characteristic of the
Philadelphia area). By including a probability
with the prediction, we can give the system user
control over what level of noise they are prepared
to see in the predictions, and hopefully filter out
messages where there is insufficient or conflicting
geolocating evidence.
We formulate the geolocation prediction prob-
lem as a multinomial naive Bayes classification
problem, based on its speed and accuracy over the
task. Given a message m, the task is to output the
most probable location locmax ? {loci}n1 for m.
User-level classification can be performed based
on a similar formulation, by combining the total
set of messages from a given user into a single
combined message.
Given a message m, the task is to find
argmaxi P (loci|m) where each loci is a grid cell
on the map. Based on Bayes? theorem and stan-
dard assumptions in the naive Bayes formulation,
this is transformed into:
argmax
i
P (loci)
v?
j
P (wj |loci)
To avoid zero probabilities, we only consider to-
kens that occur at least twice in the training data,
and ignore unseen words. A probability is calcu-
lated for the most-probable location by normalis-
ing over the scores for each loci.
We employ the method of Ritter et al(2011) to
tokenise messages, and use token unigrams as fea-
tures, including any hashtags, but ignoring twitter
mentions, URLs and purely numeric tokens. We
6Alternatively, we could consider a hybrid approach of
user- and message-level geolocation prediction, especially
for users where we have sufficient training data, which we
plan to incorporate into a future version of the system.
71
ll
l
l
l
l
l l l
l l l
10000 20000 30000 40000
0.15
0.20
0.25
0.30
0.35
0.40
Feature Number
Pre
dict
ion 
Acc
urac
y
Figure 2: Accuracy of geolocation prediction, for
varying numbers of features based on information gain
also experimented with included the named en-
tity predictions of the Ritter et al(2011) method
into our system, but found that it had no impact
on predictive accuracy. Finally, we apply feature
selection to the data, based on information gain
(Yang and Pedersen, 1997).
To evaluate the geolocation prediction mod-
ule, we use the user-level geolocation dataset
of Cheng et al(2010), based on the lower 48
states of the USA. The user-level accuracy of our
method over this dataset, for varying numbers of
features based on information gain, can be seen
in Figure 2. Based on these results, we select the
top 36,000 features in the deployed version of the
system.
In the deployed system, the geolocation pre-
diction model is trained over one million geo-
tagged messages collected over a 4 month pe-
riod from July 2011, resolved to 0.1-degree lat-
itude/longitude grid cells (covering the whole
globe, excepting grid locations where there were
less than 8 messages). For any geotagged mes-
sages in the test data, we preserve the geotag and
simply set the probability of the prediction to 1.0.
3.5 System Interface
The final output of the various pre-processing
modules is a list of tweets that match the query,
in the form of an 8-tuple as follows:
? the Twitter user ID
? the Twitter message ID
? the geo-coordinates of the message (either
provided with the message, or automatically
predicted)
? the probability of the predicated geolocation
? the text of the tweet
In addition to specifying a set of keywords for
a given session, system users can dynamically se-
lect regions on the map, either via the manual
specification of a bounding box, or zooming the
map in and out. They can additionally change
the time scale to display messages over, specify
the refresh interval and also adjust the threshold
on the geolocation predictions, to not render any
messages which have a predictive probability be-
low the threshold. The size of each place marker
on the map is rendered proportional to the num-
ber of messages at that location, and a square is
superimposed over the box to represent the max-
imum predictive probability for a single message
at that location (to provide user feedback on both
the volume of predictions and the relative confi-
dence of the system at a given location).
References
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based ap-
proach to geo-locating twitter users. In Proceedings
of the 19th ACM international conference on In-
formation and knowledge management, CIKM ?10,
pages 759?768, Toronto, ON, Canada. ACM.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011),
pages 368?378, Portland, USA.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 553?561, Chiang Mai, Thailand.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An
experimental study. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1524?1534, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
412?420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
72
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368?378,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter
Bo Han and Timothy Baldwin
NICTA Victoria Research Laboratory
Department of Computer Science and Software Engineering
The University of Melbourne
hanb@student.unimelb.edu.au tb@ldwin.net
Abstract
Twitter provides access to large volumes of
data in real time, but is notoriously noisy,
hampering its utility for NLP. In this paper, we
target out-of-vocabulary words in short text
messages and propose a method for identify-
ing and normalising ill-formed words. Our
method uses a classifier to detect ill-formed
words, and generates correction candidates
based on morphophonemic similarity. Both
word similarity and context are then exploited
to select the most probable correction can-
didate for the word. The proposed method
doesn?t require any annotations, and achieves
state-of-the-art performance over an SMS cor-
pus and a novel dataset based on Twitter.
1 Introduction
Twitter and other micro-blogging services are highly
attractive for information extraction and text mining
purposes, as they offer large volumes of real-time
data, with around 65 millions tweets posted on Twit-
ter per day in June 2010 (Twitter, 2010). The quality
of messages varies significantly, however, ranging
from high quality newswire-like text to meaningless
strings. Typos, ad hoc abbreviations, phonetic sub-
stitutions, ungrammatical structures and emoticons
abound in short text messages, causing grief for text
processing tools (Sproat et al, 2001; Ritter et al,
2010). For instance, presented with the input u must
be talkin bout the paper but I was thinkin movies
(?You must be talking about the paper but I was
thinking movies?),1 the Stanford parser (Klein and
1Throughout the paper, we will provide a normalised version
of examples as a gloss in double quotes.
Manning, 2003; de Marneffe et al, 2006) analyses
bout the paper and thinkin movies as a clause and
noun phrase, respectively, rather than a prepositional
phrase and verb phrase. If there were some way of
preprocessing the message to produce a more canon-
ical lexical rendering, we would expect the quality
of the parser to improve appreciably. Our aim in this
paper is this task of lexical normalisation of noisy
English text, with a particular focus on Twitter and
SMS messages. In this paper, we will collectively
refer to individual instances of typos, ad hoc abbre-
viations, unconventional spellings, phonetic substi-
tutions and other causes of lexical deviation as ?ill-
formed words?.
The message normalisation task is challenging.
It has similarities with spell checking (Peterson,
1980), but differs in that ill-formedness in text mes-
sages is often intentional, whether due to the desire
to save characters/keystrokes, for social identity, or
due to convention in this text sub-genre. We propose
to go beyond spell checkers, in performing deabbre-
viation when appropriate, and recovering the canon-
ical word form of commonplace shorthands like b4
?before?, which tend to be considered beyond the
remit of spell checking (Aw et al, 2006). The free
writing style of text messages makes the task even
more complex, e.g. with word lengthening such as
goooood being commonplace for emphasis. In ad-
dition, the detection of ill-formed words is difficult
due to noisy context.
Our objective is to restore ill-formed words to
their canonical lexical forms in standard English.
Through a pilot study, we compared OOV words in
Twitter and SMS data with other domain corpora,
368
revealing their characteristics in OOV word distri-
bution. We found Twitter data to have an unsur-
prisingly long tail of OOV words, suggesting that
conventional supervised learning will not perform
well due to data sparsity. Additionally, many ill-
formed words are ambiguous, and require context
to disambiguate. For example, Gooood may refer to
Good or God depending on context. This provides
the motivation to develop a method which does not
require annotated training data, but is able to lever-
age context for lexical normalisation. Our approach
first generates a list of candidate canonical lexical
forms, based on morphological and phonetic vari-
ation. Then, all candidates are ranked according
to a list of features generated from noisy context
and similarity between ill-formed words and can-
didates. Our proposed cascaded method is shown
to achieve state-of-the-art results on both SMS and
Twitter data.
Our contributions in this paper are as follows: (1)
we conduct a pilot study on the OOV word distri-
bution of Twitter and other text genres, and anal-
yse different sources of non-standard orthography in
Twitter; (2) we generate a text normalisation dataset
based on Twitter data; (3) we propose a novel nor-
malisation approach that exploits dictionary lookup,
word similarity and word context, without requir-
ing annotated data; and (4) we demonstrate that our
method achieves state-of-the-art accuracy over both
SMS and Twitter data.
2 Related work
The noisy channel model (Shannon, 1948) has tradi-
tionally been the primary approach to tackling text
normalisation. Suppose the ill-formed text is T
and its corresponding standard form is S, the ap-
proach aims to find argmaxP (S|T ) by comput-
ing argmaxP (T |S)P (S), in which P (S) is usu-
ally a language model and P (T |S) is an error model.
Brill and Moore (2000) characterise the error model
by computing the product of operation probabilities
on slice-by-slice string edits. Toutanova and Moore
(2002) improve the model by incorporating pronun-
ciation information. Choudhury et al (2007) model
the word-level text generation process for SMS mes-
sages, by considering graphemic/phonetic abbrevi-
ations and unintentional typos as hidden Markov
model (HMM) state transitions and emissions, re-
spectively (Rabiner, 1989). Cook and Stevenson
(2009) expand the error model by introducing infer-
ence from different erroneous formation processes,
according to the sampled error distribution. While
the noisy channel model is appropriate for text nor-
malisation, P (T |S), which encodes the underlying
error production process, is hard to approximate
accurately. Additionally, these methods make the
strong assumption that a token ti ? T only depends
on si ? S, ignoring the context around the token,
which could be utilised to help in resolving ambigu-
ity.
Statistical machine translation (SMT) has been
proposed as a means of context-sensitive text nor-
malisation, by treating the ill-formed text as the
source language, and the standard form as the target
language. For example, Aw et al (2006) propose a
phrase-level SMT SMS normalisation method with
bootstrapped phrase alignments. SMT approaches
tend to suffer from a critical lack of training data,
however. It is labor intensive to construct an anno-
tated corpus to sufficiently cover ill-formed words
and context-appropriate corrections. Furthermore,
it is hard to harness SMT for the lexical normali-
sation problem, as even if phrase-level re-ordering
is suppressed by constraints on phrase segmenta-
tion, word-level re-orderings within a phrase are still
prevalent.
Some researchers have also formulated text nor-
malisation as a speech recognition problem. For ex-
ample, Kobus et al (2008) firstly convert input text
tokens into phonetic tokens and then restore them to
words by phonetic dictionary lookup. Beaufort et al
(2010) use finite state methods to perform French
SMS normalisation, combining the advantages of
SMT and the noisy channel model. Kaufmann and
Kalita (2010) exploit a machine translation approach
with a preprocessor for syntactic (rather than lexical)
normalisation.
Predominantly, however, these methods require
large-scale annotated training data, limiting their
adaptability to new domains or languages. In con-
trast, our proposed method doesn?t require annotated
data. It builds on the work on SMS text normalisa-
tion, and adapts it to Twitter data, exploiting multi-
ple data sources for normalisation.
369
Figure 1: Out-of-vocabulary word distribution in English Gigaword (NYT), Twitter and SMS data
3 Scoping Text Normalisation
3.1 Task Definition of Lexical Normalisation
We define the task of text normalisation to be a map-
ping from ?ill-formed? OOV lexical items to their
standard lexical forms, focusing exclusively on En-
glish for the purposes of this paper. We define the
task as follows:
? only OOV words are considered for normalisa-
tion;
? normalisation must be to a single-token word,
meaning that we would normalise smokin to
smoking, but not imo to in my opinion; a side-
effect of this is to permit lower-register contrac-
tions such as gonna as the canonical form of
gunna (given that going to is out of scope as a
normalisation candidate, on the grounds of be-
ing multi-token).
Given this definition, our first step is to identify
candidate tokens for lexical normalisation, where
we examine all tokens that consist of alphanumeric
characters, and categorise them into in-vocabulary
(IV) and out-of-vocabulary (OOV) words, relative to
a dictionary. The OOV word definition is somewhat
rough, because it includes neologisms and proper
nouns like hopeable or WikiLeaks which have not
made their way into the dictionary. However, it
greatly simplifies the candidate identification task,
at the cost of pushing complexity downstream to
the word detection task, in that we need to explic-
itly distinguish between correct OOV words and ill-
formed OOV words such as typos (e.g. earthquak
?earthquake?), register-specific single-word abbre-
viations (e.g. lv ?love?), and phonetic substitutions
(e.g. 2morrow ?tomorrow?).
An immediate implication of our task definition is
that ill-formed words which happen to coincide with
an IV word (e.g. the misspelling of can?t as cant) are
outside the scope of this research. We also consider
that deabbreviation largely falls outside the scope of
text normalisation, as abbreviations can be formed
freely in standard English. Note that single-word
abbreviations such as govt ?government? are very
much within the scope of lexical normalisation, as
they are OOV and match to a single token in their
standard lexical form.
Throughout this paper, we use the GNU aspell
dictionary (v0.60.6)2 to determine whether a token
is OOV. In tokenising the text, hyphenanted tokens
and tokens containing apostrophes (e.g. take-off and
won?t, resp.) are treated as a single token. Twit-
ter mentions (e.g. @twitter), hashtags (e.g. #twitter)
and urls (e.g. twitter.com) are excluded from consid-
eration for normalisation, but left in situ for context
modelling purposes. Dictionary lookup of Internet
slang is performed relative to a dictionary of 5021
items collected from the Internet.3
3.2 OOV Word Distribution and Types
To get a sense of the relative need for lexical nor-
malisation, we perform analysis of the distribution
of OOV words in different text types. In particular,
we calculate the proportion of OOV tokens per mes-
sage (or sentence, in the case of edited text), bin the
messages according to the OOV token proportion,
and plot the probability mass contained in each bin
for a given text type. The three corpora we compare
2We remove all one character tokens, except a and I, and
treat RT as an IV word.
3http://www.noslang.com
370
are the New York Times (NYT),4 SMS,5 and Twit-
ter.6 The results are presented in Figure 1.
Both SMS and Twitter have a relatively flat distri-
bution, with Twitter having a particularly large tail:
around 15% of tweets have 50% or more OOV to-
kens. This has implications for any context mod-
elling, as we cannot rely on having only isolated oc-
currences of OOV words. In contrast, NYT shows a
more Zipfian distribution, despite the large number
of proper names it contains.
While this analysis confirms that Twitter and SMS
are similar in being heavily laden with OOV tokens,
it does not shed any light on the relative similarity in
the makeup of OOV tokens in each case. To further
analyse the two data sources, we extracted the set
of OOV terms found exclusively in SMS and Twit-
ter, and analysed each. Manual analysis of the two
sets revealed that most OOV words found only in
SMS were personal names. The Twitter-specific set,
on the other hand, contained a heterogeneous col-
lection of ill-formed words and proper nouns. This
suggests that Twitter is a richer/noisier data source,
and that text normalisation for Twitter needs to be
more nuanced than for SMS.
To further analyse the ill-formed words in Twit-
ter, we randomly selected 449 tweets and manu-
ally analysed the sources of lexical variation, to
determine the phenomena that lexical normalisa-
tion needs to deal with. We identified 254 to-
ken instances of lexical normalisation, and broke
them down into categories, as listed in Table 1.
?Letter? refers to instances where letters are miss-
ing or there are extraneous letters, but the lexi-
cal correspondence to the target word form is triv-
ially accessible (e.g. shuld ?should?). ?Number
Substitution? refers to instances of letter?number
substitution, where numbers have been substituted
for phonetically-similar sequences of letters (e.g. 4
?for?). ?Letter&Number? refers to instances which
have both extra/missing letters and number substitu-
tion (e.g. b4 ?before?). ?Slang? refers to instances
4Based on 44 million sentences from English Gigaword.
5Based on 12.6 thousand SMS messages from How and Kan
(2005) and Choudhury et al (2007).
6Based on 1.37 million tweets collected from the Twitter
streaming API from Aug to Oct 2010, and filtered for mono-
lingual English messages; see Section 5.1 for details of the lan-
guage filtering methodology.
Category Ratio
Letter&Number 2.36%
Letter 72.44%
Number Substitution 2.76%
Slang 12.20%
Other 10.24%
Table 1: Ill-formed word distribution
of Internet slang (e.g. lol ?laugh out loud?), as found
in a slang dictionary (see Section 3.1). ?Other? is
the remainder of the instances, which is predomi-
nantly made up of occurrences of spaces having be-
ing deleted between words (e.g. sucha ?such a?). If
a given instance belongs to multiple error categories
(e.g. ?Letter&Number? and it is also found in a slang
dictionary), we classify it into the higher-occurring
category in Table 1.
From Table 1, it is clear that ?Letter? accounts
for the majority of ill-formed words in Twitter, and
that most ill-formed words are based on morpho-
phonemic variations. This empirical finding assists
in shaping our strategy for lexical normalisation.
4 Lexical normalisation
Our proposed lexical normalisation strategy in-
volves three general steps: (1) confusion set gen-
eration, where we identify normalisation candidates
for a given word; (2) ill-formed word identification,
where we classify a word as being ill-formed or not,
relative to its confusion set; and (3) candidate selec-
tion, where we select the standard form for tokens
which have been classified as being ill formed. In
confusion set generation, we generate a set of IV
normalisation candidates for each OOV word type
based on morphophonemic variation. We call this
set the confusion set of that OOV word, and aim to
include all feasible normalisation candidates for the
word type in the confusion set. The confusion can-
didates are then filtered for each token occurrence of
a given OOV word, based on their local context fit
with a language model.
4.1 Confusion Set Generation
Revisiting our manual analysis from Section 3.2,
most ill-formed tokens in Twitter are morphophone-
mically derived. First, inspired by Kaufmann and
Kalita (2010), any repititions of more than 3 let-
ters are reduced back to 3 letters (e.g. cooool is re-
371
Criterion Recall AverageCandidates
Tc ? 1 40.4% 24
Tc ? 2 76.6% 240
Tp = 0 55.4% 65
Tp ? 1 83.4% 1248
Tp ? 2 91.0% 9694
Tc ? 2 ? Tp ? 1 88.8% 1269
Tc ? 2 ? Tp ? 2 92.7% 9515
Table 2: Recall and average number of candidates for dif-
ferent confusion set generation strategies
duced to coool). Second, IV words within a thresh-
old Tc character edit distance of the given OOV
word are calculated, as is widely used in spell check-
ers. Third, the double metaphone algorithm (Philips,
2000) is used to decode the pronunciation of all IV
words, and IV words within a threshold Tp edit dis-
tance of the given OOV word under phonemic tran-
scription, are included in the confusion set; this al-
lows us to capture OOV words such as earthquick
?earthquake?. In Table 2, we list the recall and av-
erage size of the confusion set generated by the fi-
nal two strategies with different threshold settings,
based on our evaluation dataset (see Section 5.1).
The recall for lexical edit distance with Tc ? 2 is
moderately high, but it is unable to detect the correct
candidate for about one quarter of words. The com-
bination of the lexical and phonemic strategies with
Tc ? 2?Tp ? 2 is more impressive, but the number
of candidates has also soared. Note that increasing
the edit distance further in both cases leads to an ex-
plosion in the average number of candidates, with
serious computational implications for downstream
processing. Thankfully, Tc ? 2?Tp ? 1 leads to an
extra increment in recall to 88.8%, with only a slight
increase in the average number of candidates. Based
on these results, we use Tc ? 2?Tp ? 1 as the basis
for confusion set generation.
Examples of ill-formed words where we are un-
able to generate the standard lexical form are clip-
pings such as fav ?favourite? and convo ?conversa-
tion?.
In addition to generating the confusion set, we
rank the candidates based on a trigram language
model trained over 1.5GB of clean Twitter data, i.e.
tweets which consist of all IV words: despite the
prevalence of OOV words in Twitter, the sheer vol-
ume of the data means that it is relatively easy to col-
lect large amounts of all-IV messages. To train the
language model, we used SRILM (Stolcke, 2002)
with the -<unk> option. If we truncate the ranking
to the top 10% of candidates, the recall drops back
to 84% with a 90% reduction in candidates.
4.2 Ill-formed Word Detection
The next step is to detect whether a given OOVword
in context is actually an ill-formed word or not, rel-
ative to its confusion set. To the best of our knowl-
edge, we are the first to target the task of ill-formed
word detection in the context of short text messages,
although related work exists for text with lower rel-
ative occurrences of OOV words (Izumi et al, 2003;
Sun et al, 2007). Due to the noisiness of the data, it
is impractical to use full-blown syntactic or seman-
tic features. The most direct source of evidence is
IV words around an OOV word. Inspired by work
on labelled sequential pattern extraction (Sun et al,
2007), we exploit large-scale edited corpus data to
construct dependency-based features.
First, we use the Stanford parser (Klein and Man-
ning, 2003; de Marneffe et al, 2006) to extract de-
pendencies from the NYT corpus (see Section 3.2).
For example, from a sentence such as One obvious
difference is the way they look, we would extract
dependencies such as rcmod(way-6,look-8)
and nsubj(look-8,they-7). We then trans-
form the dependencies into relational features for
each OOV word. Assuming that way were an OOV
word, e.g., we would extract dependencies of the
form (look,way,+2), indicating that look oc-
curs 2 words after way. We choose dependencies to
represent context because they are an effective way
of capturing key relationships between words, and
similar features can easily be extracted from tweets.
Note that we don?t record the dependency type here,
because we have no intention of dependency parsing
text messages, due to their noisiness and the volume
of the data. The counts of dependency forms are
combined together to derive a confidence score, and
the scored dependencies are stored in a dependency
bank.
Given the dependency-based features, a linear
kernel SVM classifier (Fan et al, 2008) is trained
on clean Twitter data, i.e. the subset of Twitter mes-
sages without OOV words. Each word is repre-
372
sented by its IV words within a context window
of three words to either side of the target word,
together with their relative positions in the form
of (word1,word2,position) tuples, and their
score in the dependency bank. These form the pos-
itive training exemplars. Negative exemplars are
automatically constructed by replacing target words
with highly-ranked candidates from their confusion
set. Note that the classifier does not require any hand
annotation, as all training exemplars are constructed
automatically.
To predict whether a given OOV word is
ill-formed, we form an exemplar for each
of its confusion candidates, and extract
(word1,word2,position) features. If
all its candidates are predicted to be negative by the
model, we mark it as correct; otherwise, we treat
it as ill-formed, and pass all candidates (not just
positively-classified candidates) on to the candidate
selection step. For example, given the message
way yu lookin shuld be a sin and the OOV word
lookin, we would generate context features for each
candidate word such as (way,looking,-2),
and classify each such candidate.
In training, it is possible for the exact same fea-
ture vector to occur as both positive and negative ex-
emplars. To prevent positive exemplars being con-
taminated from the automatic generation, we re-
move all negative instances in such cases. The
(word1,word2,position) features are sparse
and sometimes lead to conservative results in ill-
formed word detection. That is, without valid fea-
tures, the SVM classifier tends to label uncertain
cases as correct rather than ill-formed words. This
is arguably the right approach to normalisation, in
choosing to under- rather than over-normalise in
cases of uncertainty.
As the context for a target word often contains
OOV words which don?t occur in the dependency
bank, we expand the dependency features to include
context tokens up to a phonemic edit distance of 1
from context tokens in the dependency bank. In
this way, we generate dependency-based features
for context words such as seee ?see? in (seee,
flm, +2) (based on the target word flm in the
context of flm to seee). However, expanded depen-
dency features may introduce noise, and we there-
fore introduce expanded dependency weights wd ?
{0.0, 0.5, 1.0} to ameliorate the effects of noise: a
weight of wd = 0.0 means no expansion, while 1.0
means expanded dependencies are indistinguishable
from non-expanded (strict match) dependencies.
We separately introduce a threshold td ?
{1, 2, ..., 10} on the number of positive predictions
returned by the detection classifier over the set of
normalisation candidates for a given OOV token: the
token is considered to be ill-formed iff td or more
candidates are positively classified, i.e. predicted to
be correct candidates.
4.3 Candidate Selection
For OOV words which are predicted to be ill-
formed, we select the most likely candidate from the
confusion set as the basis of normalisation. The final
selection is based on the following features, in line
with previous work (Wong et al, 2006; Cook and
Stevenson, 2009).
Lexical edit distance, phonemic edit distance,
prefix substring, suffix substring, and the longest
common subsequence (LCS) are exploited to cap-
ture morphophonemic similarity. Both lexical and
phonemic edit distance (ED) are normalised by the
reciprocal of exp(ED). The prefix and suffix fea-
tures are intended to capture the fact that leading
and trailing characters are frequently dropped from
words, e.g. in cases such as ish and talkin. We cal-
culate the ratio of the LCS over the maximum string
length between ill-formed word and the candidate,
since the ill-formed word can be either longer or
shorter than (or the same size as) the standard form.
For example, mve can be restored to either me or
move, depending on context. We normalise these ra-
tios following Cook and Stevenson (2009).
For context inference, we employ both language
model- and dependency-based frequency features.
Ranking by language model score is intuitively ap-
pealing for candidate selection, but our trigram
model is trained only on clean Twitter data and ill-
formed words often don?t have sufficient context for
the language model to operate effectively, as in bt
?but? in say 2 sum1 bt nt gonna say ?say to some-
one but not going to say?. To consolidate the con-
text modelling, we obtain dependencies from the de-
pendency bank used in ill-formed word detection.
Although text messages are of a different genre to
edited newswire text, we assume they form similar
373
dependencies based on the common goal of getting
across the message effectively. The dependency fea-
tures can be used in noisy contexts and are robust
to the effects of other ill-formed words, as they do
not rely on contiguity. For example, uz ?use? in i
did #tt uz me and yu, dependencies can capture rela-
tionships like aux(use-4, do-2), which is be-
yond the capabilities of the language model due to
the hashtag being treated as a correct OOV word.
5 Experiments
5.1 Dataset and baselines
The aim of our experiments is to compare the effec-
tiveness of different methodologies over text mes-
sages, based on two datasets: (1) an SMS corpus
(Choudhury et al, 2007); and (2) a novel Twitter
dataset developed as part of this research, based on
a random sampling of 549 English tweets. The En-
glish tweets were annotated by three independent
annotators. All OOV words were pre-identified,
and the annotators were requested to determine: (a)
whether each OOV word was ill-formed or not; and
(b) what the standard form was for ill-formed words,
subject to the task definition outlined in Section 3.1.
The total number of ill-formed words contained in
the SMS and Twitter datasets were 3849 and 1184,
respectively.7
The language filtering of Twitter to automatically
identify English tweets was based on the language
identification method of Baldwin and Lui (2010),
using the EuroGOV dataset as training data, a mixed
unigram/bigram/trigram byte feature representation,
and a skew divergence nearest prototype classifier.
We reimplemented the state-of-art noisy channel
model of Cook and Stevenson (2009) and SMT ap-
proach of Aw et al (2006) as benchmark meth-
ods. We implement the SMT approach in Moses
(Koehn et al, 2007), with synthetic training and
tuning data of 90,000 and 1000 sentence pairs, re-
spectively. This data is randomly sampled from the
1.5GB of clean Twitter data, and errors are gener-
ated according to distribution of SMS corpus. The
10-fold cross-validated BLEU score (Papineni et al,
2002) over this data is 0.81.
7The Twitter dataset is available at http://www.
csse.unimelb.edu.au/research/lt/resources/
lexnorm/.
In addition to comparing our method with com-
petitor methods, we also study the contribution of
different feature groups. We separately compare dic-
tionary lookup over our Internet slang dictionary,
the contextual feature model, and the word similar-
ity feature model, as well as combinations of these
three.
5.2 Evaluation metrics
The evaluation of lexical normalisation consists of
two stages (Hirst and Budanitsky, 2005): (1) ill-
formed word detection, and (2) candidate selection.
In terms of detection, we want to make sense of how
well the system can identify ill-formed words and
leave correct OOV words untouched. This step is
crucial to further normalisation, because if correct
OOV words are identified as ill-formed, the candi-
date selection step can never be correct. Conversely,
if an ill-formed word is predicted to be correct, the
candidate selection will have no chance to normalise
it.
We evaluate detection performance by token-level
precision, recall and F-score (? = 1). Previous work
over the SMS corpus has assumed perfect ill-formed
word detection and focused only on the candidate
selection step, so we evaluate ill-formed word de-
tection for the Twitter data only.
For candidate selection, we once again evalu-
ate using token-level precision, recall and F-score.
Additionally, we evaluate using the BLEU score
over the normalised form of each message, as the
SMT method can lead to perturbations of the token
stream, vexing standard precision, recall and F-score
evaluation.
5.3 Results and Analysis
First, we test the impact of the wd and td values
on ill-formed word detection effectiveness, based on
dependencies from either the Spinn3r blog corpus
(Blog: Burton et al (2009)) or NYT. The results for
precision, recall and F-score are presented in Fig-
ure 2.
Some conclusions can be drawn from the graphs.
First, higher detection threshold values (td) give bet-
ter precision but lower recall. Generally, as td is
raised from 1 to 10, the precision improves slightly
but recall drops dramatically, with the net effect that
the F-score decreases monotonically. Thus, we use a
374
Figure 2: Ill-formed word detection precision, recall and
F-score
smaller threshold, i.e. td = 1. Second, there are dif-
ferences between the two corpora, with dependen-
cies from the Blog corpus producing slightly lower
precision but higher recall, compared with the NYT
corpus. The lower precision for the Blog corpus ap-
pears to be due to the text not being as clean as NYT,
introducing parser errors. Nevertheless, the differ-
ence in F-score between the two corpora is insignif-
icant. Third, we obtain the best results, especially
in terms of precision, for wd = 0.5, i.e. with ex-
panded dependencies, but penalised relative to non-
expanded dependencies.
Overall, the best F-score is 71.2%, with a preci-
sion of 61.1% and recall of 85.3%, obtained over
the Blog corpus with td = 1 and wd = 0.5. Clearly
there is significant room for immprovements in these
results. We leave the improvement of ill-formed
word detection for future work, and perform eval-
uation of candidate selection for Twitter assuming
perfect ill-formed word detection, as for the SMS
data.
From Table 3, we see that the general perfor-
mance of our proposed method on Twitter is better
than that on SMS. To better understand this trend,
we examined the annotations in the SMS corpus, and
found them to be looser than ours, because they have
different task specifications than our lexical normal-
isation. In our annotation, the annotators only nor-
malised ill-formed word if they had high confidence
of how to normalise, as with talkin ?talking?. For
ill-formed words where they couldn?t be certain of
the standard form, the tokens were left untouched.
However, in the SMS corpus, annotations such as
sammis ?same? are also included. This leads to a
performance drop for our method over the SMS cor-
pus.
The noisy channel method of Cook and Stevenson
(2009) shares similar features with word similarity
(?WS?), However, when word similarity and con-
text support are combined (?WS+CS?), our method
outperforms the noisy channel method by about 7%
and 12% in F-score over SMS and Twitter corpora,
respectively. This can be explained as follows. First,
the Cook and Stevenson (2009) method is type-
based, so all token instances of a given ill-formed
word will be normalised identically. In the Twit-
ter data, however, the same word can be normalised
differently depending on context, e.g. hw ?how? in
so hw many time remaining so I can calculate it?
vs. hw ?homework? in I need to finish my hw first.
Second, the noisy channel method was developed
specifically for SMS normalisation, in which clip-
ping is the most prevalent form of lexical variation,
while in the Twitter data, we commonly have in-
stances of word lengthening for emphasis, such as
moviiie ?movie?. Having said this, our method is
superior to the noisy channel method over both the
SMS and Twitter data.
The SMT approach is relatively stable on the two
datasets, but well below the performance of our
method. This is due to the limitations of the training
data: we obtain the ill-formed words and their stan-
dard forms from the SMS corpus, but the ill-formed
words in the SMS corpus are not sufficient to cover
those in the Twitter data (and we don?t have suffi-
cient Twitter data to train the SMT method directly).
Thus, novel ill-formed words are missed in normal-
isation. This shows the shortcoming of supervised
data-driven approaches that require annotated data
to cover all possibilities of ill-formed words in Twit-
ter.
The dictionary lookup method (?DL?) unsurpris-
ingly achieves the best precision, but the recall
on Twitter is not competitive. Consequently, the
Twitter normalisation cannot be tackled with dictio-
nary lookup alone, although it is an effective pre-
processing strategy when combined with more ro-
375
Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CS
SMS
Precision 0.465 ? 0.927 0.521 0.116 0.532 0.756
Recall 0.464 ? 0.597 0.520 0.116 0.531 0.754
F-score 0.464 ? 0.726 0.520 0.116 0.531 0.755
BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876
Twitter
Precision 0.452 ? 0.961 0.551 0.194 0.571 0.753
Recall 0.452 ? 0.460 0.551 0.194 0.571 0.753
F-score 0.452 ? 0.622 0.551 0.194 0.571 0.753
BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934
Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson,
2009); MT = SMT (Aw et al, 2006); DL = dictionary lookup; WS = word similarity; CS = context support)
bust techniques such as our proposed method, and
effective at capturing common abbreviations such as
gf ?girlfriend?.
Of the component methods proposed in this re-
search, word similarity (?WS?) achieves higher pre-
cision and recall than context support (?CS?), sig-
nifying that many of the ill-formed words emanate
from morphophonemic variations. However, when
combined with word similarity features, context
support improves over the basic method at a level of
statistical significance (based on randomised estima-
tion, p < 0.05: Yeh (2000)), indicating the comple-
mentarity of the two methods, especially on Twitter
data. The best F-score is achieved when combin-
ing dictionary lookup, word similarity and context
support (?DL+WS+CS?), in which ill-formed words
are first looked up in the slang dictionary, and only
if no match is found do we apply our normalisation
method.
We found several limitations in our proposed ap-
proach by analysing the output of our method. First,
not all ill-formed words offer useful context. Some
highly noisy tweets contain almost all misspellings
and unique symbols, and thus no context features
can be extracted. This also explains why ?CS? fea-
tures often fail. For such cases, the method falls back
to context-independent normalisation. We found
that only 32.6% ill-formed words have all IV words
in their context windows. Moreover, the IV words
may not occur in the dependency bank, further de-
creasing the effectiveness of context support fea-
tures. Second, the different features are linearly
combined, where a weighted combination is likely
to give better results, although it also requires a cer-
tain amount of well-sampled annotations for tuning.
6 Conclusion and Future Work
In this paper, we have proposed the task of lexi-
cal normalisation for short text messages, as found
in Twitter and SMS data. We found that most ill-
formed words are based on morphophonemic varia-
tion and proposed a cascaded method to detect and
normalise ill-formed words. Our ill-formed word
detector requires no explicit annotations, and the
dependency-based features were shown to be some-
what effective, however, there was still a lot of
room for improvement at ill-formed word detection.
In normalisation, we compared our method with
two benchmark methods from the literature, and
achieved that highest F-score and BLEU score by
integrating dictionary lookup, word similarity and
context support modelling.
In future work, we propose to pursue a number of
directions. First, we plan to improve our ill-formed
word detection classifier by introducing an OOV
word whitelist. Furthermore, we intend to allevi-
ate noisy contexts with a bootstrapping approach, in
which ill-formed words with high confidence and no
ambiguity will be replaced by their standard forms,
and fed into the normalisation model as new training
data.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of the 21st International Con-
376
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33?40, Sydney, Australia.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
HLT ?10: Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
229?237, Los Angeles, USA.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 770?779, Uppsala, Sweden.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286?293, Hong Kong.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia, San Jose, USA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ?09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78, Boulder, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87?111.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Human Computer Interfaces International
(HCII 05), Las Vegas, USA.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume 2,
pages 145?148, Sapporo, Japan.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS 2002), pages 3?10, Whistler,
Canada.
Catherine Kobus, Franois Yvon, and Graldine Damnati.
2008. Transcrire les SMS comme on reconnat la pa-
role. In Actes de la Confrence sur le Traitement Au-
tomatique des Langues (TALN?08), pages 128?138.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, USA.
James L. Peterson. 1980. Computer programs for de-
tecting and correcting spelling errors. Commun. ACM,
23:676?687, December.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38?43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In HLT
?10: Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
172?180, Los Angeles, USA.
Claude Elwood Shannon. 1948. A mathematical the-
ory of communication. Bell System Technical Journal,
27:379?423, 623?656.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287 ? 333.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
377
ken Language Processing, pages 901?904, Denver,
USA.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and
Ming Zhou. 2007. Mining sequential patterns and tree
patterns to detect erroneous sentences. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 81?88, Prague, Czech
Republic.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?02, pages
144?151, Philadelphia, USA.
Twitter. 2010. Big goals, big game, big records.
http://blog.twitter.com/2010/06/
big-goals-big-game-big-records.html.
Retrieved 4 August 2010.
Wilson Wong, Wei Liu, and Mohammed Bennamoun.
2006. Integrated scoring for spelling error correction,
abbreviation expansion and case restoration in dirty
text. In Proceedings of the Fifth Australasian Con-
ference on Data Mining and Analytics, pages 83?89,
Sydney, Australia.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics -
Volume 2, COLING ?00, pages 947?953, Saarbru?cken,
Germany.
378
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?12,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Stacking-based Approach to Twitter User Geolocation Prediction
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
We implement a city-level geolocation
prediction system for Twitter users. The
system infers a user?s location based on
both tweet text and user-declared metadata
using a stacking approach. We demon-
strate that the stacking method substan-
tially outperforms benchmark methods,
achieving 49% accuracy on a benchmark
dataset. We further evaluate our method
on a recent crawl of Twitter data to in-
vestigate the impact of temporal factors
on model generalisation. Our results sug-
gest that user-declared location metadata
is more sensitive to temporal change than
the text of Twitter messages. We also de-
scribe two ways of accessing/demoing our
system.
1 Introduction
In this paper, we present and evaluate a geoloca-
tion prediction method for Twitter users.1 Given
a user?s tweet data as input, the task of user level
geolocation prediction is to infer a primary loca-
tion (i.e., ?home location?: Mahmud et al (2012))
for the user from a discrete set of pre-defined loca-
tions (Cheng et al, 2010). For instance, President
Obama?s location might be predicted to be Wash-
ington D.C., USA, based on his public tweets and
profile metadata.
Geolocation information is essential to location-
based applications, like targeted advertising and
local event detection (Sakaki et al, 2010;
MacEachren et al, 2011). However, the means
to obtain such information are limited. Although
Twitter allows users to specify a plain text de-
scription of their location in their profile, these de-
scriptions tend to be ad hoc and unreliable (Cheng
1We only use public Twitter data for experiments and ex-
emplification in this study.
et al, 2010). Recently, user geolocation predic-
tion based on a user?s tweets has become popular
(Wing and Baldridge, 2011; Roller et al, 2012),
based on the assumption that tweets implicitly
contain locating information, and with appropri-
ate statistical modeling, the true location can be
inferred. For instance, if a user frequently men-
tions NYC, JFK and yankees, it is likely that they
are from New York City, USA.
In this paper, we discuss an implementation of
a global city-level geolocation prediction system
for English Twitter users. The system utilises both
tweet text and public profile metadata for model-
ing and inference. Specifically, we train multino-
mial Bayes classifiers based on location indica-
tive words (LIWs) in tweets (Han et al, 2012),
and user-declared location and time zone meta-
data. These base classifiers are further stacked
(Wolpert, 1992) using logistic regression as the
meta-classifier. The proposed stacking model is
compared with benchmarks on a public geolo-
cation dataset. Experimental results demonstrate
that our stacking model outperforms benchmark
methods by a large margin, achieving 49% accu-
racy on the test data. We further evaluate the stack-
ing model on a more recent crawl of public tweets.
This experiment tests the effectiveness of a geolo-
cation model trained on ?old? data when applied to
?new? data. The results reveal that user-declared
locations are more variable over time than tweet
text and time zone data.
2 Background and Related Work
Identifying the geolocation of objects has been
widely studied in the research literature over target
objects including webpages (Zong et al, 2005),
search queries (Backstrom et al, 2008), Flickr im-
ages (Crandall et al, 2009) and Wikipedia ed-
itors (Lieberman and Lin, 2009). Recently, a
considerable amount of work has been devoted
to extending geolocation prediction for Twitter
7
users (Cheng et al, 2010; Eisenstein et al, 2010).
The geolocations are usually represented by un-
ambiguous city names or a partitioning of the
earth?s surface (e.g., grid cells specified by lati-
tude/longitude). User geolocation is generally re-
lated to a ?home? location where a user regularly
resides, and user mobility is ignored. Twitter al-
lows users to declare their home locations in plain
text in their profile, however, this data has been
found to be unstructured and ad hoc in preliminary
research (Cheng et al, 2010; Hecht et al, 2011).
While popular for desktop machine geoloca-
tion, methods that map IP addresses to physical
locations (Buyukokkten et al, 1999) cannot be
applied to Twitter-based user geolocation, as IPs
are only known to the service provider and are
non-trivial to retrieve in a mobile Internet environ-
ment. Although social network information has
been proven effective in inferring user locations
(Backstrom et al, 2010; Sadilek et al, 2012; Rout
et al, 2013), we focus exclusively on message
and metadata information in this paper, as they are
more readily accessible.
Text data tends to contain salient geospatial ex-
pressions that are particular to specific regions.
Attempts to leverage this data directly have been
based on analysis of gazetted expressions (Leid-
ner and Lieberman, 2011) or the identification of
geographical entities (Quercini et al, 2010; Qin et
al., 2003). However these methods are limited in
their ability to capture informal geospatial expres-
sions (e.g. Brissie for Brisbane) and more non-
geospatial terms which are associated with partic-
ular locations (e.g. ferry for Seattle or Sydney).
Beyond identifying geographical references us-
ing off-the-shelf tools, more sophisticated meth-
ods have been introduced in the social media
realm. Cheng et al (2010) built a simple gen-
erative model based on tweet words, and fur-
ther added words which are local to particular re-
gions and applied smoothing to under-represented
locations. Kinsella et al (2011) applied differ-
ent similarity measures to the task, and investi-
gated the relative difficulty of geolocation predic-
tion at city, state, and country levels. Wing and
Baldridge (2011) introduced a grid-based repre-
sentation for geolocation modeling and inference
based on fixed latitude and longitude values, and
aggregated all tweets in a single cell. Their ap-
proach was then based on lexical similarity us-
ing KL-divergence. One drawback to the uniform-
sized cell representation is that it introduces class
imbalance: urban areas tend to contain far more
tweets than rural areas. Based on this observa-
tion, Roller et al (2012) introduced an adaptive
grid representation in which cells contain approx-
imately the same number of users, based on a KD-
tree partition. Given that most tweets are from
urban areas, Han et al (2012) consider a city-
based class division, and explore different feature
selection methods to extract ?location indicative
words?, which they show to improve prediction
accuracy. Additionally, time zone information has
been incorporated in a coarse-to-fine hierarchical
model by first determining the time zone, and then
disambiguating locations within it (Mahmud et al,
2012). Topic models have also been applied to the
task, in capturing regional linguistic differences
(Eisenstein et al, 2010; Yin et al, 2011; Hong et
al., 2012).
When designing a practical geolocation sys-
tem, simple models such as naive Bayes and near-
est prototype methods (e.g., based on KL diver-
gence) have clear advantages in terms of train-
ing and classification throughput, given the size of
the class set (often numbering in the thousands of
classes) and sheer volume of training data (poten-
tially in the terabytes of data). This is particularly
important for online systems and downstream ap-
plications that require timely predictions. As such,
we build off the text-based naive Bayes-based ge-
olocation system of Han et al (2012), which our
experiments have shown to have a good balance of
tractability and accuracy. By selecting a reduced
set of ?location indicative words?, prediction can
be further accelerated.
3 Methodology
In this study, we adopt the same city-based rep-
resentation and multinomial naive Bayes learner
as Han et al (2012). The city-based representa-
tion consists of 3,709 cities throughout the world,
and is obtained by aggregating smaller cities with
the largest nearby city. Han et al (2012) found
that using feature selection to identify ?location
indicative words? led to improvements in geoloca-
tion performance. We use the same feature selec-
tion technique that they did. Specifically, feature
selection is based on information gain ratio (IGR)
(Quinlan, 1993) over the city-based label set for
each word.
In the original research of Han et al (2012),
8
only the text of Twitter messages was used,
and training was based exclusively on geotagged
tweets, despite these accounting for only around
1% of the total public data on Twitter. In this
research, we include additional non-geotagged
tweets (e.g., posted from a non-GPS enabled de-
vice) for those users who have geotagged tweets
(allowing us to determine a home location for the
user).
In addition to including non-geotagged data in
modeling and inference, we further take advan-
tage of the text-based metadata embedded in a
user?s public profile (and included in the JSON ob-
ject for each tweet). This metadata is potentially
complementary to the tweet message and of bene-
fit for geolocation prediction, especially the user-
declared location and time zone, which we con-
sider here. Note that these are in free text rather
than a structured data format, and that while there
are certainly instances of formal place name de-
scriptions (e.g., Edinburgh, UK), they are often
informal (e.g., mel for Melbourne). As such, we
adopt a statistical approach to model each selected
metadata field, by capturing the text in the form
of character 4-grams, and training a multinomial
naive Bayes classifier for each field.
To combine together the tweet text and meta-
data fields, we use stacking (Wolpert, 1992). The
training of stacking consists of two steps. First,
a multinomial naive Bayes base classifier (L0) is
learned for each data type using 10-fold cross
validation. This is carried out for the tweet
text (TEXT), user-declared location (MB-LOC) and
user-declared time zone (MB-TZ). Next, a meta-
classifier (L1 classifier) is trained over the base
classifiers, using a logistic regression learner (Fan
et al, 2008).
4 Evaluation and Discussion
In this section, we compare our proposed stack-
ing approach with existing benchmarks on a public
dataset, and investigate the impact of time using a
recently collected dataset.
4.1 Evaluation Measures
In line with other work on user geolocation pre-
diction, we use three evaluation measures:
? Acc : The percentage of correct city-level
predictions.
? Acc@161 : The percentage of predicted lo-
cations which are within a 161km (100 mile)
Methods Acc Acc@161 Median
KL .117 .277 793
MB .126 .262 913
KL-NG .260 .487 181
MB-NG .280 .492 170
MB-LOC .405 .525 92
MB-TZ .064 .171 1330
STACKING .490 .665 9
Table 1: Results over WORLD
radius of the home location (Cheng et al,
2010), to capture near-misses (e.g., Edin-
burgh UK being predicted as Glasgow, UK).
? Median : The median distance from the pre-
dicted city to the home location (Eisenstein et
al., 2010).
4.2 Comparison with Benchmarks
We base our evaluation on the publicly-available
WORLD dataset of Han et al (2012). The dataset
contains 1.4M users whose tweets are primarily
identified as English based on the output of the
langid.py language identification tool (Lui and
Baldwin, 2012), and who have posted at least 10
geotagged tweets. The city-level home location
for a geotagged user is determined as follows.
First, each of a user?s geotagged tweets is mapped
to its nearest city (based on the same set of 3,709
cities used for the city-based location representa-
tion). Then, the most frequent city for a user is
selected as the home location.
To benchmark our method, we reimplement
two recently-published state-of-the-art methods:
(1) the KL-divergence nearest prototype method
of Roller et al (2012) based on KD-tree parti-
tioned grid cells, which we denote as KL; and
(2) the multinomial naive Bayes city-level geolo-
cation model of Han et al (2012), which we de-
note as MB. Because of the different class repre-
sentations, Acc numbers are not comparable be-
tween the benchmarks. To remedy this, we find
the closest city to the centroid of each grid cell in
the KD-tree representation, and map the classifi-
cation onto this city. We present results including
non-geotagged data for users with geotagged mes-
sages for the two methods, as KL-NG and MB-
NG, respectively. We also present results based
on the user-declared location (MB-LOC) and time
zone (MB-TZ), and finally the stacking method
(STACKING) which combines MB-NG, MB-LOC
and MB-TZ. The results are shown in Table 1.
9
The approximate doubling of Acc for KL-
NG and MB-NG over KL and MB, respectively,
demonstrates the high utility of non-geotagged
data in tweet text-based geolocation prediction.
Of the two original models, we can see that MB
is comparable to KL, in line with the findings of
Han et al (2012). The MB-LOC results are by far
the highest of all the base classifiers. Contrary to
the suggestion of Cheng et al (2010) that user-
declared locations are too unreliable to use for user
geolocation, we find evidence indicating that they
are indeed a valuable source of information for this
task. The best overall results are achieved for the
stacking approach (STACKING), assigning almost
half of the test users to the correct city-level lo-
cation, and improving more than four-fold on the
previous-best accuracy (i.e., MB). These results
also suggest that there is strong complementarity
between user metadata and tweet text.
4.3 Evaluation on Time-Heterogeneous Data
In addition to the original held-out test data
(WORLDtest) from WORLD, we also developed a
new geotagged evaluation dataset using the Twit-
ter Streaming API.2 This new LIVEtest dataset is
intended to evaluate the impact of time on predic-
tive accuracy. The training and test data in WORLD
are time-homogeneous as they are randomly sam-
pled from data collected in a relatively narrow time
window. In contrast, LIVEtest is much newer, col-
lected more than 1 year later than WORLD. Given
that Twitter users and topics change over time, an
essential question is whether the statistical model
learned from the ?old? training data is still effec-
tive over the ?new? test data?
The LIVEtest data was collected over 48 hours
from 2013/03/03 to 2013/03/05. By selecting
users with at least 10 geotagged tweets and a de-
clared language of English, 55k users were ob-
tained. For each user, their recent status updates
were aggregated, and non-English users were fil-
tered out based on the language predictions of
langid.py. For some users with geotagged
tweets from many cities, the most frequent city
might not be an appropriate representation of their
home location for evaluation. To improve the eval-
uation data quality, we therefore exclude users
who have less than 50% of their geotagged tweets
originating from a single city. After filtering, 32k
2https://dev.twitter.com/docs/api/1.1/
get/statuses/sample
LIVEtest Acc Acc@161 Median
MB-NG .268 (?.012) .510 (?.018) 151 ( ?19)
MB-LOC .326 (?.079) .465 (?.060) 306 (+214)
MB-TZ .065 (+.001) .160 (?.011) 1529 (+199)
STACKING .406 (?.084) .614 (?.051) 40 ( +31)
Table 2: Results over LIVEtest, and the absolute
fluctuation over the results for WORLDtest
users were obtained, forming the final LIVEtest
dataset. In the final LIVEtest, the smallest class
has only one test user, and the largest class has
569 users. The mean users per city is 27.76.
The results over LIVEtest, and the difference
in absolute score over WORLDtest, are shown in
Table 2. The stacked model accuracy numbers
drop 5?8% on LIVEtest, and the median error
distance increases moderately by 31km. Over-
all, the numbers suggest inference on WORLDtest,
which is time-homogenous with the training data
(taken from WORLD), is an easier classification
than LIVEtest, which is time-heterogeneous with
the training data. Training on ?old? data and test-
ing on ?new? data is certainly possible, however.
Looking over the results of the base classifiers, we
can see that the biggest hit is for MB-LOC clas-
sifier. In contrast, the accuracy for MB-NG and
MB-TZ is relatively stable (other than the sharp in-
crease in the median error distance for MB-TZ).
5 Architecture and Access
In this section, we describe the architecture of the
proposed geolocation system, as well as two ways
of accessing the live system.3 The core structure
of the system consists of two parts: (1) the inter-
face; (2) the back-end geolocation service.
We offer two interfaces to access the system: a
Twitter bot and a web interface. The Twitter bot
account is: @MELBLTFSD. A daemon process de-
tects any user mentions of the bot in tweets via
keyword matching through the Twitter search API.
The screen name of the tweet author is extracted
and sent to the back-end geolocation service, and
the predicted user geolocation is sent to the Twitter
user in a direct message, as shown in Figure 1.
Web access is via http://hum.csse.unimelb.
edu.au:9000/geo.html. Users can input a Twit-
ter user screen name through the web interface,
whereby a call is made to the back-end geoloca-
tion service to geolocate that user. The geoloca-
3The source code is available from https://github.
com/tq010or/acl2013
10
Figure 2: Web interface for user geolocation. The numbered green markers represent geotagged tweets.
These coordinates are utilised to validate our predictions, and are not used in the geolocation process.
The red marker is the predicted city-based user geolocation.
Figure 1: Twitter bot interface. When the Twit-
ter bot is mentioned in a tweet, that user is sent a
direct message with the predicted geolocation.
tion results are rendered on a map (along with any
geotagged tweets for the user) as in Figure 2.4
The back-end geolocation service crawls recent
tweets for a given user in real time,5 and word
and n-gram features are extracted from both the
text and the user metadata. These features are sent
to the L0 classifiers (TEXT, MB-LOC and MB-TZ),
and the L0 results are further fed into the L1 clas-
sifier for the final prediction.
6 Summary and Future Work
In this paper, we presented a city-level geoloca-
tion prediction system for Twitter users. Over a
public dataset, our stacking method ? exploiting
both tweet text and user metadata ? substantially
4Currently, only Google Chrome is supported. https:
//www.google.com/intl/en/chrome/
5Up to 200 tweets are crawled, the upper bound of mes-
sages returned per single request based on Twitter API v1.1.
outperformed benchmark methods. We further
evaluated model generalisation on a newer, time-
heterogeneous dataset. The overall results de-
creased by 5?8% in accuracy, compared with num-
bers on time-homogeneous data, primarily due to
the poor generalisation of the MB-LOC classifier.
In future work, we plan to further investigate
the cause of the MB-LOC classifier accuracy de-
crease on the new dataset. In addition, we?d like
to study differences in prediction accuracy across
cities. For cities with reliable predictions, the sys-
tem can be adapted as a preprocessing module for
downstream applications, e.g., local event detec-
tion based on users with reliable predictions.
Acknowledgements
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT centre
of Excellence programme.
References
Lars Backstrom, Jon Kleinberg, Ravi Kumar, and Jas-
mine Novak. 2008. Spatial variation in search en-
gine queries. In Proc. of WWW, pages 357?366,
Beijing, China.
Lars Backstrom, Eric Sun, and Cameron Marlow.
2010. Find me if you can: improving geographi-
cal prediction with social and spatial proximity. In
Proc. of WWW, pages 61?70, Raleigh, USA.
11
Orkut Buyukokkten, Junghoo Cho, Hector Garcia-
Molina, Luis Gravano, and Narayana Shivakumar.
1999. Exploiting geographical location informa-
tion of web pages. In ACM SIGMOD Workshop on
The Web and Databases, pages 91?96, Philadelphia,
USA.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based
approach to geo-locating twitter users. In Proc. of
CIKM, pages 759?768, Toronto, Canada.
David J. Crandall, Lars Backstrom, Daniel Hutten-
locher, and Jon Kleinberg. 2009. Mapping the
world?s photos. In Proc. of WWW, pages 761?770,
Madrid, Spain.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP,
pages 1277?1287, Cambridge, MA, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by find-
ing location indicative words. In Proc. of COLING,
pages 1045?1062, Mumbai, India.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from justin bieber?s heart:
the dynamics of the location field in user profiles.
In Proc. of SIGCHI, pages 237?246, Vancouver,
Canada.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the twit-
ter stream. In Proc. of WWW, pages 769?778, Lyon,
France.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in glasgow?: mod-
eling locations with tweets. In Proc. of the 3rd In-
ternational Workshop on Search and Mining User-
generated Contents, pages 61?68, Glasgow, UK.
Jochen L. Leidner and Michael D. Lieberman. 2011.
Detecting geographical references in the form of
place names and associated spatial natural language.
SIGSPATIAL Special, 3(2):5?11.
Michael D. Lieberman and Jimmy Lin. 2009. You
are where you edit: Locating wikipedia contributors
through edit histories. In ICWSM.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proc.
of the ACL, pages 25?30, Jeju Island, Korea.
Alan M. MacEachren, Anuj Jaiswal, Anthony C.
Robinson, Scott Pezanowski, Alexander Savelyev,
Prasenjit Mitra, Xiao Zhang, and Justine Blanford.
2011. Senseplace2: Geotwitter analytics support for
situational awareness. In IEEE Conference on Vi-
sual Analytics Science and Technology, pages 181?
190, Rhode Island, USA.
Jalal Mahmud, Jeffrey Nichols, and Clemens Drews.
2012. Where is this tweet from? inferring home lo-
cations of twitter users. In Proc. of ICWSM, Dublin,
Ireland.
Teng Qin, Rong Xiao, Lei Fang, Xing Xie, and Lei
Zhang. 2003. An efficient location extraction algo-
rithm by leveraging web contextual information. In
Proc. of SIGSPATIAL, pages 55?62, San Jose, USA.
Gianluca Quercini, Hanan Samet, Jagan Sankara-
narayanan, and Michael D. Lieberman. 2010. De-
termining the spatial reader scopes of news sources
using local lexicons. In Proc. of the 18th Interna-
tional Conference on Advances in Geographic In-
formation Systems, pages 43?52, San Jose, USA.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
USA.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language mod-
els on an adaptive grid. In Proc. of EMNLP, pages
1500?1510, Jeju Island, Korea.
Dominic Rout, Kalina Bontcheva, Daniel Preot?iuc-
Pietro, and Trevor Cohn. 2013. Where?s @wally?:
a classification approach to geolocating users based
on their social ties. In Proc. of the 24th ACM Con-
ference on Hypertext and Social Media, pages 11?
20, Paris, France.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. of WSDM, pages 723?732,
Seattle, USA.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proc. of WWW,
pages 851?860, Raleigh, USA.
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL, pages 955?964, Portland,
USA.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5(2):241?259.
Zhijun Yin, Liangliang Cao, Jiawei Han, Chengxiang
Zhai, and Thomas Huang. 2011. Geographical
topic discovery and comparison. In Proc. of WWW,
pages 247?256, Hyderabad, India.
Wenbo Zong, Dan Wu, Aixin Sun, Ee-Peng Lim,
and Dion Hoe-Lian Goh. 2005. On assigning
place names to geography related web pages. In
ACM/IEEE Joint Conference on Digital Libraries,
pages 354?362.
12
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 248?253, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Unsupervised Word Usage Similarity in Social Media Texts
Spandana Gella,? Paul Cook,? and Bo Han??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,
hanb@student.unimelb.edu.au
Abstract
We propose an unsupervised method for au-
tomatically calculating word usage similar-
ity in social media data based on topic mod-
elling, which we contrast with a baseline dis-
tributional method and Weighted Textual Ma-
trix Factorization. We evaluate these meth-
ods against a novel dataset made up of human
ratings over 550 Twitter message pairs anno-
tated for usage similarity for a set of 10 nouns.
The results show that our topic modelling ap-
proach outperforms the other two methods.
1 Introduction
In recent years, with the growing popularity of so-
cial media applications, there has been a steep rise
in the amount of ?post?-based user-generated text
(including microblog posts, status updates and com-
ments) (Bennett, 2012). This data has been iden-
tified as having potential for applications ranging
from trend analysis (Lau et al, 2012a) and event de-
tection (Osborne et al, 2012) to election outcome
prediction (O?Connor et al, 2010). However, given
that posts are generally very short, noisy and lack-
ing in context, traditional NLP approaches tend to
perform poorly over social media data (Hong and
Davison, 2010; Ritter et al, 2011; Han et al, 2012).
This is the first paper to address the task of lexi-
cal semantic interpretation in microblog data based
on word usage similarity. Word usage similar-
ity (USIM: Erk et al (2009)) is a relatively new
paradigm for capturing similarity in the usages of
a given word independently of any lexicon or sense
inventory. The task is to rate on an ordinal scale the
similarity in usage between two different usages of
the same word. In doing so, it avoids common issues
in conventional word sense disambiguation, relating
to sense underspecification, the appropriateness of a
static sense inventory to a given domain, and the in-
ability to capture similarities/overlaps between word
senses. As an example of USIM, consider the fol-
lowing pairing of Twitter posts containing the target
word paper:
1. Deportation of Afghan Asylum Seekers from
Australia : This paper aims to critically evalu-
ate a newly signed agree.
2. @USER has his number on a piece of paper
and I walkd off!
The task is to predict a real-valued number in the
range [1, 5] for the similarity in the respective us-
ages of paper, where 1 indicates the usages are com-
pletely different and 5 indicates they are identical.
In this paper we develop a new USIM dataset
based on Twitter data. In experiments on this dataset
we demonstrate that an LDA-based topic modelling
approach outperforms a baseline distributional se-
mantic approach and Weighted Textual Matrix Fac-
torization (WTMF: Guo and Diab (2012a)). We
further show that context expansion using a novel
hashtag-based strategy improves both the LDA-
based method and WTMF.
2 Related Work
Word sense disambiguation (WSD) is the task of
determining the particular sense of a word from a
given set of pre-defined senses (Navigli, 2009). It
248
contrasts with word sense induction (WSI), where
the senses of a given target word are induced from
an unannotated corpus of usages, and the induced
senses are then used to disambiguate each token us-
age of the word (Manandhar et al, 2010; Lau et
al., 2012b). WSD and WSI have been the predomi-
nant paradigms for capturing and evaluating lexical
semantics, and both assume that each usage corre-
sponds to exactly one of a set of discrete senses of
the target word, and that any prediction other than
the ?correct? sense is equally wrong.
Erk et al (2009) showed that, given a sense in-
ventory, there is a high likelihood of multiple senses
being compatible with a given usage, and proposed
USIM as a means of capturing the similarity in us-
age between a pairing of usages of a given word.
As part of their work, they released a dataset, which
Lui et al (2012) recently developed a topic mod-
elling approach over. Based on extensive experi-
mentation, they demonstrated the best results with
a single topic model for all target words based on
full document context. Our topic modelling-based
approach to USIM builds off the approach of Lui
et al (2012). Guo and Diab (2012a) observed that,
when applied to short texts, the effectiveness of la-
tent semantic approaches can be boosted by expand-
ing the text to include ?missing? words. Based on
this, they proposed Weighted Textual Matrix Factor-
ization (WTMF), based on weighted matrix factor-
ization (Srebro and Jaakkola, 2003). Here we ex-
periment with both LDA based topic modeling and
WTMF to estimate word similarities in twitter data.
LDA based topic modeling has been earlier studied
on Twitter data for tweet classification (Ramage et
al., 2010) and tweet clustering (Jin et al, 2011).
3 Data Preparation
This section describes the construction of the USIM-
tweet dataset based on microblog posts (?tweets?)
from Twitter. We describe the pre-processing steps
taken to sample the tweets in our datasets, outline
the annotation process, and then describe the back-
ground corpora used in our experiments.
3.1 Data preprocessing
Around half of Twitter is non-English (Hong et al,
2011), so our first step was to automatically identify
English tweets using langid.py (Lui and Bald-
win, 2012). We next performed lexical normaliza-
tion using the dictionary of Han et al (2012) to con-
vert lexical variants (e.g., tmrw) to their standard
forms (e.g., tomorrow) and reduce data sparseness.
As our target words, we chose the 10 nouns from
the original USIM dataset of Erk et al (2009) (bar,
charge, execution, field, figure, function, investiga-
tor, match, paper, post), and identified tweets con-
taining the target words as nouns using the CMU
Twitter POS tagger (Owoputi et al, 2012).
3.2 Annotation Settings and Data
To collect word usage similarity scores for Twitter
message pairs, we used a setup similar to that of
Erk et al (2009) using Amazon Mechanical Turk:
we asked the annotators to rate each sentence pair
with an integer score in the range [1, 5] using sim-
ilar annotation guidelines to Erk et al We ran-
domly sampled twitter messages from the TREC
2011 microblog dataset,1 and for each of our 10
nouns, we collected 55 pairs of messages satisfying
the preprocessing described in Section 3.1. These
55 pairs are chosen such that each tweet has at least
4 content words (nouns, verbs, adjectives and ad-
verbs) and at least 70+% of its post-normalized to-
kens in the Aspell dictionary (v6.06)2; these restric-
tions were included in an effort to ensure the tweets
would contain sufficient linguistic content to be in-
terpretable.3 We created 110 Mechanical Turk jobs
(referred to as HITs), with each HIT containing 5
randomly-selected message pairs. For this annota-
tion the tweets were presented in their original form,
i.e., without lexical normalisation applied. Each HIT
was completed by 10 ?turkers?, resulting in a total
of 5500 annotations. The annotation was restricted
to turkers based in the United States having had at
least 95% of their previous HITs accepted. In total,
the annotation was carried out by 68 turkers, each
completing between 1 and 100 HITs.
To detect outlier annotators, we calculated the av-
erage Spearman correlation score (?) of every an-
notator by correlating their annotation values with
every other annotator and taking the average. We
1http://trec.nist.gov/data/tweets/
2http://aspell.net/
3In future analyses we intend to explore the potential impact
of these restrictions on the resulting dataset.
249
Word Orig Exp Word Orig Exp
bar 180k 186k function 26k 27k
charge 41k 43k investigator 17k 19k
execution 28k 30k field 72k 75k
figure 28k 29k match 126k 133k
paper 210k 218k post 299k 310k
Table 1: The number of tweets for each word in each
background corpus (?Orig? = ORIGINAL; ?Exp?
= EXPANDED; RANDEXPANDED, not shown, con-
tains the same number of tweets as EXPANDED).
accepted all the annotations of annotators whose av-
erage ? is greater than 0.6; this corresponded to 95%
of the annotators. Two annotators had a negative
average ? and their annotations (only 4 HITs to-
tal) were discarded. For the other annotators (i.e.,
0 ? ? ? 0.6), we accepted each of their HITs on
a case by case basis; a HIT was accepted only if
at least 2 out of 5 of the annotations for that HIT
were within ?2.0 of the mean for that annotation
based on the judgments of the other turkers. (21
HITS were discarded using this heuristic.) We fur-
ther eliminated 7 HITS which have incomplete judg-
ments. In total only 32 HITs (of the 1100 HITs com-
pleted) were discarded through these heuristics. The
weighted average Spearman correlation over all an-
notators after this filtering is 0.681, which is some-
what higher than the inter-annotator agreement of
0.548 reported by Erk et al (2009). This dataset is
available for download.
3.3 Background Corpus
We created three background corpora based on data
from the Twitter Streaming API in February 2012
(only tweets satisfying the preprocessing steps in
Section 3.1 were chosen).
ORIGINAL: 1 million tweets which contain at least
one of the 10 target nouns;
EXPANDED: ORIGINAL plus an additional 40k
tweets containing at least 1 hashtag attested in
ORIGINAL with an average frequency of use of
10?35 times/hour (medium frequency);
RANDEXPANDED: ORIGINAL plus 40k randomly
sampled tweets containing the same target
nouns.
We select medium-frequency hashtags because low-
frequency hashtags tend to be ad hoc and non-
thematic in nature, while high-frequency hash-
tags are potentially too general to capture us-
age similarity. Statistics for ORIGINAL and EX-
PANDED/RANDEXPANDED are shown in Table 1.
RANDEXPANDED is sampled such that it has the
same number of tweets as EXPANDED.
4 Methodology
We propose an LDA topic modelling-based ap-
proach to the USIM task, which we contrast with
a baseline distributional model and WTMF. In all
these methods, the similarity between two word us-
ages is measured using cosine similarity between the
vector representation of each word usage.
4.1 Baseline
We represent each target word usage in a tweet as a
second-order co-occurrence vector (Schu?tze, 1998).
A second-order co-occurrence vector is built from
the centroid (summation) of all the first-order co-
occurrence vectors of the context words in the same
tweet as the target word.
The first-order co-occurrence vector for a given
target word represents the frequency with which that
word co-occurs in a tweet with other context words.
Each first-order vector is built from all tweets which
contain a context word and the target word catego-
rized as noun in the background corpus, thus sensi-
tizing the first-order vector to the target word. We
use the most frequent 10000 words (excluding stop-
words) in the background corpus as our first-order
vector dimensions/context words. Context words
(dimensions) in the first-order vectors are weighted
by mutual information.
Second-order co-occurrence is used as the context
representation to reduce the effects of data sparse-
ness in the tweets (which cannot be more than 140
codepoints in length).
4.2 Weighted Textual Matrix Factorization
WTMF (Guo and Diab, 2012b) addresses the data
sparsity problem suffered by many latent variable
250
Model ORIGINAL EXPANDED RANDEXPANDED
Baseline 0.09 0.08 0.09
WTMF 0.02 0.09 0.06
LDA 0.20 0.29 0.18
Table 2: Spearman rank correlation (?) for each
method based on each background corpus. The best
result for each corpus is shown in bold.
models by predicting ?missing? words on the ba-
sis of the message content, and including them in
the vector representation. Guo and Diab showed
WTMF to outperform LDA on the SemEval-2012
semantic textual similarity task (STS) (Agirre et al,
2012). The semantic space required for this model
as applied here is built from the background tweets
corresponding to the target word. We experimented
with the missing weight parameter wm of WTMF
in the range [0.05, 0.01, 0.005, 0.0005] and with di-
mensions K=100 and report the best results (wm =
0.0005).
4.3 Topic Modelling
Latent Dirichlet Allocation (LDA) (Blei et al, 2003)
is a generative model in which a document is mod-
eled as a finite mixture of topics, where each topic is
represented as a multinomial distribution of words.
We treat each tweet as a document. Topics sensi-
tive to each target word are generated from its corre-
sponding background tweets. We topic model each
target word individually,4 and create a topic vector
for each word usage based on the topic allocations of
the context words in that usage. We use Gibbs sam-
pling in Mallet (McCallum, 2002) for training and
inference of the LDA model. We experimented with
the number of topics T for each target word ranging
from 2 to 500. We optimized the hyper parameters
by choosing those which best fit the data every 20 it-
erations over a total of 800 iterations, following 200
burn-in iterations.
4Unlike Lui et al (2012) we found a single topic model for
all target words to perform very poorly.
l
l l
l
l l l
l
l l
l l l
l
l l l
2 3 5 8 10 20 30 50 100 150 200 250 300 350 400 450 500
T
?
0.1
0.0
0.1
0.2
0.3
Sp
ea
rm
an
 
co
rre
lati
on
 
? l Original
Expanded
RandExpanded
Figure 1: Spearman rank correlation (?) for LDA for
varying numbers of topics (T ) using different back-
ground corpora.
5 Results
We evaluate the above methods for word usage sim-
ilarity on the dataset constructed in Section 3.2. We
evaluate our models against the mean human ratings
using Spearman?s rank correlation. Table 2 presents
results for each method using each background cor-
pus. The results for LDA are for the optimal set-
ting for T (8, 5, and 20 for ORIGINAL, EXPANDED,
and RANDEXPANDED, respectively). LDA is su-
perior to both the baseline and WTMF using each
background corpus. The performance of LDA im-
proves for EXPANDED but not RANDEXPANDED,
over ORIGINAL, demonstrating the effectiveness of
our hashtag based corpus expansion strategy.
In Figure 1 we plot the rank correlation of LDA
across all words against the number of topics (T ).
As the number of topics increases beyond a certain
number, the rank correlation decreases. LDA trained
on EXPANDED consistently outperforms ORIGINAL
and RANDEXPANDED for lower values of T (i.e.,
T <= 20).
In Table 3, we show results for LDA over each tar-
get word, for ORIGINAL and EXPANDED. (Results
for RANDEXPANDED are not shown but are similar
to ORIGINAL.) Results are shown for the optimal
T for each lemma, and the optimal T over all lem-
mas. Optimizing T for each lemma gives an indica-
tion of the upperbound of the performance of LDA,
and unsurprisingly gives better performance than us-
251
Lemma
ORIGINAL EXPANDED
Per lemma Global Per lemma Global
? (T ) ? (T=8) ? (T ) ? (T=5)
bar 0.39 (10) 0.28 0.35 (50) 0.1
charge 0.27 (30) 0.04 0.33 (20) ?0.08
execution 0.43 (8) 0.43 0.58 (5) 0.58
field 0.46 (5) 0.33 0.53 (10) 0.32
figure 0.24 (150) 0.06 0.24 (250) 0.14
function 0.44 (8) 0.44 0.40 (10) 0.27
investigator 0.3 (30) 0.05 0.50 (5) 0.50
match 0.28 (5) 0.26 0.45 (5) 0.45
paper 0.29 (30) 0.20 0.32 (30) 0.22
post 0.1 (3) ?0.13 0.2 (30) ?0.01
Table 3: Spearman?s ? using LDA for the optimal T
for each lemma (Per lemma) and the best T over all
lemmas (Global) using ORIGINAL and EXPANDED.
? values that are significant at the 0.05 level are
shown in bold.
ing a fixed T for all lemmas. This suggests that ap-
proaches that learn an appropriate number of topics
(e.g., HDP, (Teh et al, 2006)) could give further im-
provements; however, given the size of the dataset,
the computational cost of HDP could be a limitation.
Contrasting our results with a fixed number of
topics to those of Lui et al (2012), our highest rank
correlation of 0.29 (T = 5 using EXPANDED) is
higher than the 0.11 they achieved over the origi-
nal USIM dataset (where the documents offer an or-
der of magnitude more context). The higher inter-
annotator agreement for USIM-tweet compared to
the original USIM dataset (Section 3.2), combined
with this finding, demonstrates that USIM over mi-
croblog data is indeed a viable task.
Returning to the performance of LDA relative
to WTMF in Table 2, the poor performance of
WTMF is somewhat surprising here given WTMF?s
encouraging performance on the somewhat similar
SemEval-2012 STS task. This difference is possi-
bly due to the differences in the tasks: usage simi-
larity measures the similarity of the usage of a tar-
get word while STS measures the similarity of two
texts. Differences in domain ? i.e., Twitter here
and more standard text for STS ? could also be a
factor. WTMF attempts to alleviate the data spar-
sity problem by adding information from ?missing?
words in a text by assigning a small weight to these
missing words. Because of the prevalence of lexical
variation on Twitter, some missing words might be
counted multiple times (e.g., coool, kool, and kewl
all meaning roughly cool) thus indirectly assigning
higher weights to the missing words leading to the
lower performance of WTMF compared to LDA.
6 Summary
We have analysed word usage similarity in mi-
croblog data. We developed a new dataset (USIM-
tweet) for usage similarity of nouns over Twitter.
We applied a topic modelling approach to this task,
and contrasted it with baseline and benchmark meth-
ods. Our results show that the LDA-based approach
outperforms the other methods over microblog data.
Moreover, our novel hashtag-based corpus expan-
sion strategy substantially improves the results.
In future work, we plan to expand our annotated
dataset, experiment with larger background corpora,
and explore alternative corpus expansion strategies.
We also intend to further analyse the difference in
performance LDA and WTMF on similar data.
Acknowledgements
We are very grateful to Timothy Baldwin for his
tremendous help with this work. We additionally
thank Diana McCarthy for her insightful comments
on this paper. We also acknowledge the European
Erasmus Mundus Masters Program in Language and
Communication Technologies from the European
Commission.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence programme.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montreal, Canada.
Shea Bennett. 2012. Twitter on track for
500 million total users by March, 250 mil-
lion active users by end of 2012. http:
//www.mediabistro.com/alltwitter/
twitter-active-total-users_b17655.
252
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of the Joint conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing (ACL-IJCNLP
2009), pages 10?18, Singapore.
Weiwei Guo and Mona Diab. 2012a. Modeling sen-
tences in the latent space. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864?872, Jeju, Republic of Korea.
Weiwei Guo and Mona Diab. 2012b. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 586?590, Montreal, Canada.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary for
microblogs. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning 2012,
pages 421?432, Jeju, Republic of Korea.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proc. of the First
Workshop on Social Media Analytics, pages 80?88.
Lichan Hong, Gregoria Convertino, and Ed H. Chi. 2011.
Language matters in Twitter: A large scale study. In
Proceedings of the 5th International Conference on
Weblogs and Social Media (ICWSM 2011), pages 518?
521, Barcelona, Spain.
Ou Jin, Nathan N Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Proc.
of the 20th ACM International Conference on Informa-
tion and Knowledge Management, pages 775?784.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012a. On-line trend analysis with topic models:
#twitter trends detection topic model online. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012b. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics (EACL
2012), pages 591?601, Avignon, France.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012) Demo Session,
pages 25?30, Jeju, Republic of Korea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages 33?
41, Dunedin, New Zealand.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
4th International Conference on Weblogs and Social
Media, pages 122?129, Washington, USA.
Miles Osborne, Sasa Petrovic?, Richard McCreadie, Craig
Macdonald, and Iadh Ounis. 2012. Bieber no more:
First story detection using Twitter and Wikipedia. In
Proceedings of the SIGIR 2012 Workshop on Time-
aware Information Access, Portland, USA.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, and Nathan Schneider. 2012. Part-of-speech
tagging for Twitter: Word clusters and other advances.
Technical Report CMU-ML-12-107, Carnegie Mellon
University.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, volume 5, pages 130?137.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1524?1534, Edinburgh, UK.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the
20th International Conference on Machine Learning,
Washington, USA.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
253
