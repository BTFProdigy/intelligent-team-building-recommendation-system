Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311?321,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Representations from Unlabeled Data with Co-training
for Chinese Word Segmentation
Longkai Zhang Houfeng Wang? Xu Sun Mairgup Mansur
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn, mairgup@gmail.com,
Abstract
Nowadays supervised sequence labeling
models can reach competitive performance
on the task of Chinese word segmenta-
tion. However, the ability of these mod-
els is restricted by the availability of an-
notated data and the design of features.
We propose a scalable semi-supervised fea-
ture engineering approach. In contrast
to previous works using pre-defined task-
specific features with fixed values, we dy-
namically extract representations of label
distributions from both an in-domain cor-
pus and an out-of-domain corpus. We
update the representation values with a
semi-supervised approach. Experiments
on the benchmark datasets show that our
approach achieve good results and reach
an f-score of 0.961. The feature engineer-
ing approach proposed here is a general
iterative semi-supervised method and not
limited to the word segmentation task.
1 Introduction
Chinese is a language without natural word
delimiters. Therefore, Chinese Word Segmen-
tation (CWS) is an essential task required by
further language processing. Previous research
shows that sequence labeling models trained on
labeled data can reach competitive accuracy on
the CWS task, and supervised models are more
accurate than unsupervised models (Xue, 2003;
Low et al, 2005). However, the resource of man-
ually labeled training corpora is limited. There-
fore, semi-supervised learning has become one
?Corresponding author
of the most natural forms of training for CWS.
Traditional semi-supervised methods focus on
adding new unlabeled instances to the training
set by a given criterion. The possible mislabeled
instances, which are introduced from the auto-
matically labeled raw data, can hurt the per-
formance and not easy to exclude by setting a
sound selecting criterion.
In this paper, we propose a simple and scal-
able semi-supervised strategy that works by pro-
viding semi-supervision at the level of represen-
tation. Previous works mainly assume that con-
text features are helpful to decide the potential
label of a character. However, when some of the
context features do not appear in the training
corpus, this assumption may fail. An example is
shown in table 1. Although the context of ???
and ??? is totally different, they share a homo-
geneous structure as ?verb-noun?. Therefore. A
much better way is to map the context informa-
tion to a kind of representation. More precisely,
the mapping should let the similar contexts map
to similar representations, while let the distinct
contexts map to distinct representations.
??? ???
Label B B
Character ? ? ? ? ? ?
Context C-1= ? C-1= ?
Features C0= ? C0= ?
C1= ? C1= ?
Table 1: Example of the context of ??? in ???
? (Eat fruits)? and the context of ??? in ????
(Play basketball)?
We use the label distribution information that
311
is extracted from the unlabeled corpus as this
representation to enhance the supervised model.
We add ?pseudo-labels? by tagging the unla-
beled data with the trained model on the train-
ing corpus. These ?pseudo-labels? are not accu-
rate enough. Therefore, we use the label distri-
bution, which is much more accurate.
To accurately calculate the precise label dis-
tribution, we use a framework similar to the co-
training algorithm to adjust the feature values
iteratively. Generally speaking, unlabeled data
can be classified as in-domain data and out-of-
domain data. In previous works these two kinds
of unlabeled data are used separately for differ-
ent purposes. In-domain data is mainly used to
solve the problem of data sparseness (Sun and
Xu, 2011). On the other hand, out-of domain
data is used for domain adaptation (Chang and
Han, 2010). In our work, we use in-domain and
out-of-domain data together to adjust the labels
of the unlabeled corpus.
We evaluate the performance of CWS on the
benchmark dataset of Peking University in the
second International Chinese Word Segmenta-
tion Bakeoff. Experiment results show that our
approach yields improvements compared with
the state-of-art systems. Even when the la-
beled data is insufficient, our methods can still
work better than traditional methods. Com-
pared to the baseline CWS model, which has
already achieved an f-score above 0.95, we fur-
ther reduce the error rate by 15%.
Our method is not limited to word segmen-
tation. It is also applicable to other problems
which can be solved by sequence labeling mod-
els. We also applied our method to the Chi-
nese Named Entity Recognition task, and also
achieved better results compared to traditional
methods.
The main contributions of our work are as fol-
lows:
? We proposed a general method to utilize
the label distribution given text contexts as
representations in a semi-supervised frame-
work. We let the co-training process ad-
just the representation values from label
distribution instead of using manually pre-
defined feature templates.
? Compared with previous work, our method
achieved a new state-of-art accuracy on the
CWS task as well as on the NER task.
The remaining part of this paper is organized
as follows. Section 2 describes the details of the
problem and our algorithm. Section 3 describes
the experiment and presents the results. Section
4 reviews the related work. Section 5 concludes
this paper.
2 System Architecture
2.1 Sequence Labeling
Nowadays the character-based sequence label-
ing approach is widely used for the Chinese word
segmentation problem. It was first proposed in
Xue (2003), which assigns each character a label
to indicate its position in the word. The most
prevalent tag set is the BMES tag set, which
uses 4 tags to carry word boundary information.
This tag set uses B, M, E and S to represent the
Beginning, the Middle, the End of a word and
a Single character forming a word respectively.
We use this tag set in our method. An example
of the ?BMES? representation is shown in table
2.
Character: ? ? ? ? ? ? ?
Tag: S S B E B M E
Table 2: An example for the ?BMES? representa-
tion. The sentence is ????????? (I love Bei-
jing Tian-an-men square), which consists of 4 Chi-
nese words: ??? (I), ??? (love), ???? (Beijing),
and ????? (Tian-an-men square).
2.2 Unlabeled Data
Unlabeled data can be divided into in-domain
data and out-of-domain data. In previous works,
these two kinds of unlabeled data are used sep-
arately for different purposes. In-domain data
only solves the problem of data sparseness (Sun
and Xu, 2011). Out-of domain data is used
only for domain adaptation (Chang and Han,
2010). These two functionalities are not contra-
dictory but complementary. Our study shows
312
that by correctly designing features and algo-
rithms, both in-domain unlabeled data and out-
of-domain unlabeled data can work together to
help enhancing the segmentation model. In our
algorithm, the dynamic features learned from
one corpus can be adjusted incrementally with
the dynamic features learned from the other cor-
pus.
As for the out-of-domain data, it will be even
better if the corpus is not limited to a specific
domain. We choose a Chinese encyclopedia cor-
pus which meets exactly this requirement. We
use the corpus to learn a large set of informative
features. In our experiment, two different views
of features on unlabeled data are considered:
Static Statistical Features (SSFs): These
features capture statistical information of char-
acters and character n-grams from the unlabeled
corpus. The values of these features are fixed
during the training process once the unlabeled
corpus is given.
Dynamic Statistical Features (DSFs):
These features capture label distribution infor-
mation from the unlabeled corpus given fixed
text contexts. As the training process proceeds,
the value of these features will change, since the
trained tagger at each training iteration may as-
sign different labels to the unlabeled data.
2.3 Framework
Suppose we have labeled data L, two unla-
beled corpora Ua and Ub (one is an in-domain
corpus and the other is an out-of-domain cor-
pus). Our algorithm is shown in Table 3.
During each iteration, we tag the unlabeled
corpus Ua using Tb to get pseudo-labels. Then
we extract features from the pseudo-labels. We
use the label distribution information as dy-
namic features. We add these features to the
training data to train a new tagger Ta. To adjust
the feature values, we extract features from one
corpus and then apply the statistics to the other
corpus. This is similar to the principle of co-
training (Yarowsky, 1995; Blum and Mitchell,
1998; Dasgupta et al, 2002). The difference is
that there are not different views of features, but
different kinds of unlabeled data. Detailed de-
scription of features is given in the next section.
Algorithm
Init:
Using baseline features only:
Train an initial tagger T0 based on L ()
Label Ua and Ub individually using T0
BEGIN LOOP:
Generate DSFs from tagged Ua
Augment L with DSFs to get La
Generate DSFs from tagged Ub
Augment L with DSFs to get Lb
Using baseline features, SSFs and DSFs:
Train new tagger Ta using La
Train new tagger Tb using Lb
Label Ua using Tb
Label Ub using Ta
LOOP until performance does not improve
RETURN the tagger which is trained with
in-domain features.
Table 3: Algorithm description
2.4 Features
2.4.1 Baseline Features
Our baseline feature templates include the
features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). These features are
widely used in the CWS task. To be convenient,
for a character ci with context . . . ci?1cici+1 . . .,
its baseline features are listed below:
? Character uni-grams: ck (i? 3 < k < i+3)
? Character bi-grams: ckck+1 (i ? 3 < k <
i+ 2)
? Whether ck and ck+1 are identical (i? 2 <
k < i + 2)
? Whether ck and ck+2 are identical (i? 4 <
k < i + 2)
The last two feature templates are designed to
detect character reduplication, which is a mor-
phological phenomenon in Chinese language.
An example is ?????? (Perfect), which is
a Chinese idiom with structure ?ABAC?.
313
2.4.2 Static statistical features
Statistical features are statistics that distilled
from the large unlabeled corpus. They are
proved useful in the Chinese word segmenta-
tion task. We define Static Statistical Features
(SSFs) as features whose value do not change
during the training process. The SSFs in our
approach includes Mutual information, Punctu-
ation information and Accessor variety. Previ-
ous works have already explored the functions
of the three static statistics in the Chinese word
segmentation task, e.g. Feng et al (2004); Sun
and Xu (2011). We mainly follow their defini-
tions while considering more details and giving
some modification.
Mutual information
Mutual information (MI) is a quantity that
measures the mutual dependence of two random
variables. Previous works showed that larger MI
of two strings claims higher probability that the
two strings should be combined. Therefore, MI
can show the tendency of two strings forming
one word. However, previous works mainly fo-
cused on the balanced case, i.e., the MI of strings
with the same length. In our study we find that,
in Chinese, there remains large amount of imbal-
anced cases, like a string with length 1 followed
by a string with length 2, and vice versa. We
further considered the MI of these string pairs
to capture more information.
Punctuation information
Punctuations can provide implicit labels for
the characters before and after them. The char-
acter after punctuations must be the first char-
acter of a word. The character before punctua-
tions must be the last character of a word. When
a string appears frequently after punctuations,
it tends to be the beginning of a word. The situ-
ation is similar when a string appears frequently
preceding punctuations. Besides, the probabil-
ity of a string appears in the corpus also affects
this tendency. Considering all these factors,
we propose ?punctuation rate? (PR) to capture
this information. For a string with length len
and probability p in the corpus, we define the
left punctuation rate LPRlen as the number of
times the string appears after punctuations, di-
vided by p. Similarly, the right punctuation
rate RPRlen is defines as the number of times
it appears preceding punctuations divided by its
probability p. The length of string we consider
is from 1 to 4.
Accessor variety
Accessor variety (AV) is also known as letter
successor variety (LSV) (Harris, 1955; Hafer and
Weiss, 1974). If a string appears after or pre-
ceding many different characters, this may pro-
vide some information of the string itself. Pre-
vious work of Feng et al (2004), Sun and Xu
(2011) used AV to represent this statistic. Sim-
ilar to punctuation rate, we also consider both
left AV and right AV. For a string s with length
l, we define the left accessor variety (LAV) as
the types of distinct characters preceding s in
the corpus, and the right accessor variety (RAV)
as the types of distinct characters after s in the
corpus. The length of string we consider is also
from 1 to 4.
2.4.3 Dynamic statistical features
The unlabeled corpus lacks precise labels. We
can use the trained tagger to give the unla-
beled data ?pseudo-labels?. These labels can-
not guarantee an acceptable precision. How-
ever, the label distribution will not be largely
affected by small mistakes. Using the label dis-
tribution information is more accurate than us-
ing the pseudo-labels directly.
Based on this assumption, we propose ?dy-
namic statistical features? (DSFs). The DSFs
are intended to capture label distribution infor-
mation given a text context. The word ?Dy-
namic? is in accordance with the fact that these
feature values will change during the training
process.
We give a formal description of DSFs. Sup-
pose there are K labels in our task. For example,
K = 4 if we take BMES labeling method. We
define the whole character sequence with length
n as X = (x1, x2 ? ? ?xj ? ? ?xn). Given a text con-
text Ci, where i is current character position,
the DSFs can be represented as a list,
DSF (Ci) = (DSF (Ci)1, ? ? ? , DSF (Ci)K)
314
Each element in the list represents the proba-
bility of the corresponding label in the distribu-
tion.
For convenience, we further define function
?count(condition)? as the total number of times
a ?condition? is true in the unlabeled corpus.
For example, count (current=?a?) represents the
times the current character equals ?a?, which is
exactly the number of times character ?a? ap-
pears in the unlabeled corpus.
According to different types of text context
Ci, we can divide DSFs into 3 types:
1.Basic DSF
For Basic DSF of Ci, we define D(Ci):
D(Ci) = (D(Ci)1, . . . , D(Ci)K)
We define Basic DSF with current character po-
sition i, text context Ci and label l (the lth di-
mension in the list) as:
D(Ci)l = P (y = l|Ci = xi)
= count(Ci = xi ? y = l)count(Ci = xi)
In this equation, the numerator counts the num-
ber of times current character is xi with label l.
The denominator counts the number of times
current character is xi.
We use the term ?Basic? because this kind of
DSFs only considers the character of position i
as its context. The text context refers to the cur-
rent character itself. This feature captures the
label distribution information given the charac-
ter itself.
2.BigramDSF
Basic DSF is simple and very easy to imple-
ment. The weakness is that it is less power-
ful to describe word-building features. Although
characters convey context information, charac-
ters themselves in Chinese is sometimes mean-
ingless. Character bi-grams can carry more con-
text information than uni-grams. We modify
Basic DSFs to bi-gram level and propose Bigram
DSFs.
For Bigram DSF of Ci, we define B(Ci):
B(Ci) = (B(Ci)1, . . . , B(Ci)K)
We define Bigram DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
B(Ci)l = P (y = l|Ci = xi?jxi?j+1)
= count(Ci = xi?jxi?j+1 ? y = l)count(Ci = xi?jxi?j+1)
j can take value 0 and 1.
In this equation, the numerator counts the
number of times current context is xi?jxi?j+1
with label l. The denominator counts the num-
ber of times current context is xi?jxi?j+1.
3.WindowDSF
Considering Basic DSF and Bigram DSF only
might cause the over-fitting problem, therefore
we introduce another kind of DSF. We call it
Window DSF, which considers the surrounding
context of a character and omits the character
itself.
For Window DSF, we define W (Ci):
W (Ci) = (W (Ci)1, . . . ,W (Ci)K)
We define Window DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
W (Ci)l = P (y = l|Ci = xi?1xi+1)
= count(Ci = xi?1xi+1 ? y = l)count(Ci = xi?1xi+1)
In this equation, the numerator counts the
number of times current context is xi?1xi+1
with label l. The denominator counts the num-
ber of times current context is xi?1xi+1.
2.4.4 Discrete features VS. Continuous
features
The statistical features may be expressed as
real values. A more natural way is to use dis-
crete values to incorporate them into the se-
quence labeling models . Previous works like
Sun and Xu (2011) solve this problem by set-
ting thresholds and converting the real value
into boolean values. We use a different method
to solve this, which does not need to consider
tuning thresholds. In our method, we process
static and dynamic statistical features using dif-
ferent strategies.
315
For static statistical value:
For mutual information, we round the real
value to their nearest integer. For punctuation
rate and accessor variety, as the values tend to
be large, we first get the log value of the feature
and then use the nearest integer as the corre-
sponding discrete value.
For dynamic statistical value:
Dynamic statistical features are distributions
of a label. The values of DSFs are all percentage
values. We can solve this by multiply the proba-
bility by an integer N and then take the integer
part as the final feature value. We set the value
of N by cross-validation..
2.5 Conditional Random Fields
Our algorithm is not necessarily limited to
a specific baseline tagger. For simplicity and
reliability, we use a simple Conditional Ran-
dom Field (CRF) tagger, although other se-
quence labeling models like Semi-Markov CRF
Gao et al (2007) and Latent-variable CRF Sun
et al (2009) may provide better results than
a single CRF. Detailed definition of CRF can
be found in Lafferty et al (2001); McCallum
(2002); Pinto et al (2003).
3 Experiment
3.1 Data and metrics
We used the benchmark datasets provided by
the second International Chinese Word Segmen-
tation Bakeoff1 to test our approach. We chose
the Peking University (PKU) data in our exper-
iment. Although the benchmark provides an-
other three data sets, two of them are data of
traditional Chinese, which is quite different from
simplified Chinese. Another is the data from Mi-
crosoft Research (MSR). We experimented on
this data and got 97.45% in f-score compared
to the state-of-art 97.4% reported in Sun et al
(2012). However, this corpus is much larger
than the PKU corpus. Using the labeled data
alone can get a relatively good tagger and the
unlabeled data contributes little to the perfor-
mance. For simplicity and efficiency, our further
1http://www.sighan.org/bakeoff2005/
experiments are all conducted on the PKU data.
Details of the PKU data are listed in table 4.
We also used two un-segmented corpora as
unlabeled data. The first one is Chinese Giga-
word2 corpus. It is a comprehensive archive of
newswire data. The second one is articles from
Baike3 of baidu.com. It is a Chinese encyclope-
dia similar to Wikipedia but contains more Chi-
nese items and their descriptions. In the exper-
iment we used about 5 million characters from
each corpus for efficiency. Details of unlabeled
data can be found in table 5.
In our experiment, we did not use any ex-
tra resources such as common surnames, part-
of-speech or other dictionaries.
F-score is used as the accuracy measure. We
define precision P as the percentage of words
in the output that are segmented correctly. We
define recall R as the percentage of the words
in reference that are correctly segmented. Then
F-score is as follows:
F = 2 ? P ?RP +R
The recall of out-of-vocabulary is also taken into
consideration, which measures the ability of the
model to correctly segment out of vocabulary
words.
3.2 Main Results
Table 6 summarizes the segmentation results
on test data with different feature combinations.
We performed incremental evaluation. In this
table, we first present the results of the tagger
only using baseline features. Then we show the
results of adding SSF and DSF individually. In
the end we compare the results of combining
SSF and DSF with baseline features.
Because the baseline features is strong to
reach a relative good result, it is not easy to
largely enhance the performance. Neverthe-
less, there are significant increases in f-score and
OOV-Recall when adding these features. From
table 6 we can see that by adding SSF and DSF
individually, the F-score is improved by +1.1%
2http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
3http://baike.baidu.com/
316
Identical words Total word Identical Character Total character
5.5 ? 104 1.1 ? 106 5 ? 103 1.8 ? 106
Table 4: Details of the PKU data
Corpus Character used
Gigaword 5000193
Baike 5000147
Table 5: Details of the unlabeled data.
P R F OOV
Baseline 0.950 0.943 0.946 0.676
+SSF 0.961 0.953 0.957 0.728
+DSF 0.958 0.953 0.955 0.678
+SSF+DSF 0.965 0.958 0.961 0.731
Table 6: Segmentation results on test data with
different feature combinations. The symbol ?+?
means this feature configuration contains features set
containing the baseline features and all features after
?+?. The size of unlabeled data is fixed as 5 million
characters.
and +0.9%. The OOV-Recall is also improved,
especially after adding SSFs. When considering
SSF and DSF together, the f-score is improved
by +1.5% while the OOV-Recall is improved by
+5.5%.
To compare the contribution of unlabeled
data, we conduct experiments of using differ-
ent sizes of unlabeled data. Note that the SSFs
are still calculated using all the unlabeled data.
However, each iteration in the algorithm uses
unlabeled data with different sizes.
Table 7 shows the results when changing the
size of unlabeled data. We experimented on
three different sizes: 0.5 million, 1 million and 5
million characters.
P R F OOV
DSF(0.5M) 0.962 0.954 0.958 0.727
DSF(1M) 0.963 0.955 0.959 0.728
DSF(5M) 0.965 0.958 0.961 0.731
Table 7: Comparison of results when changing the
size of unlabeled data. (0.5 million, 1 million and 5
million characters).
We further experimented on unlabeled corpus
with larger size (up to 100 million characters).
However the performance did not change signif-
icantly. Besides, because the number of features
in our method is very large, using too large un-
labeled corpus is intractable in real applications
due to the limitation of memory.
Our method can keep working well even when
the labeled data are insufficient. Table 8 shows
the comparison of f-scores when changing the
size of labeled data. We compared the results
of using all labeled data with 3 different situa-
tions: using 1/10, 1/2 and 1/4 of all the labeled
data. In fact, the best system on the Second In-
ternational Chinese Word Segmentation bakeoff
reached 0.95 in f-score by using all labeled data.
From table 8 we can see that our algorithm only
needs 1/4 of all labeled data to achieve the same
f-score.
Baseline +SSF+DSF Improve
1/10 0.934 0.943 +0.96%
1/4 0.946 0.951 +0.53%
1/2 0.952 0.956 +0.42%
All 0.957 0.961 +0.42%
Table 8: Comparison of f-scores when changing the
size of labeled data. (1/10, 1/4, 1/2 and all labeled
data. The size of unlabeled data is fixed as 5 million
characters.)
We also explored how the performance
changes as iteration increases. Figure 1 shows
the change of F-score during the first 10 itera-
tions. From figure 1 we find that f-score has a
fast improvement in the first few iterations, and
then stables at a fixed point. Besides, as the size
of labeled data increases, it converges faster.
Using an in-domain corpus and an out-of-
domain corpus is better than use one corpus
alone. We compared our approach with the
method which uses only one unlabeled corpus.
To use only one corpus, we modify our algorithm
to extract DSFs from the Chinese Giga word
corpus and apply the learned features to itself.
317
Figure 1: Learning curve of using different size of
labeled data
Table 9 shows the result. We can see that our
method outperforms by +0.2% in f-score and
+0.7% in OOV-Recall.
Finally, we compared our method with the
state-of-art systems reported in the previous pa-
pers. Table 10 listed the results. Best05 repre-
sents the best system reported on the Second In-
ternational Chinese Word Segmentation Bake-
off. CRF + Rule system represents a combina-
tion of CRF model and rule based model pre-
sented in Zhang et al (2006). Other three sys-
tems all represent the methods using their cor-
responding model in the corresponding papers.
Note that these state-of-art systems are either
using complicated models with semi-Markov re-
laxations or latent variables, or modifying mod-
els to fit special conditions. Our system uses a
single CRF model. As we can see in table 10,
our method achieved higher F-scores than the
previous best systems.
3.3 Results on NER task
Our method is not limited to the CWS prob-
lem. It is applicable to all sequence labeling
problems. We applied our method on the Chi-
nese NER task. We used the MSR corpus of
the sixth SIGHAN Workshop on Chinese Lan-
guage Processing. It is the only NER corpus
using simplified Chinese in that workshop. We
compared our method with the pure sequence la-
beling approach in He and Wang (2008). We re-
implemented their method to eliminate the dif-
ference of various CRFs implementations. Ex-
periment results are shown in table 11. We can
see that our methods works better, especially
when handling the out-of-vocabulary named en-
tities;
4 Related work
Recent studies show that character sequence
labeling is an effective method of Chinese word
segmentation for machine learning (Xue, 2003;
Low et al, 2005; Zhao et al, 2006a,b). These su-
pervised methods show good results. Unsuper-
vised word segmentation (Maosong et al, 1998;
Peng and Schuurmans, 2001; Feng et al, 2004;
Goldwater et al, 2006; Jin and Tanaka-Ishii,
2006) takes advantage of the huge amount of raw
text to solve Chinese word segmentation prob-
lems. These methods need no annotated corpus,
and most of them use statistics to help model
the problem. However, they usually are less ac-
curate than supervised ones.
Currently ?feature-engineering? methods
have been successfully applied into NLP ap-
plications. Miller et al (2004) applied this
method to named entity recognition. Koo et al
(2008) applied this method to dependency pars-
ing. Turian et al (2010) applied this method to
both named entity recognition and text chunk-
ing. These papers shared the same concept of
word clustering. However, we cannot simply
equal Chinese character to English word because
characters in Chinese carry much less informa-
tion than words in English and the clustering
results is less meaningful.
Features extracted from large unlabeled cor-
pus in previous works mainly focus on statisti-
cal information of characters. Feng et al (2004)
used the accessor variety criterion to extract
word types. Li and Sun (2009) used punctua-
tion information in Chinese word segmentation
by introducing extra labels ?L? and ?R?. Chang
and Han (2010), Sun and Xu (2011) used rich
statistical information as discrete features in
a sequence labeling framework. All these ap-
proaches can be viewed as using static statistics
features in a supervised approach. Our method
is different from theirs. For the static statistics
features in our approach, we not only consider
richer string pairs with the different lengths, but
also consider term frequency when processing
318
P R F OOV
Using one corpus 0.963 0.955 0.959 0.724
Our method 0.965 0.958 0.961 0.731
Table 9: Comparison of our approach with using only the Gigaword corpus
Method P R F-score
Best05 (Chen et al (2005)) 0.953 0.946 0.950
CRF + rule-system (Zhang et al (2006)) 0.947 0.955 0.951
Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945
Latent-variable CRF (Sun et al (2009)) 0.956 0.948 0.952
ADF-CRF (Sun et al (2012)) 0.958 0.949 0.954
Our method 0.965 0.958 0.961
Table 10: Comparison of our approach with the state-of-art systems
P R F OOV
Traditional 0.925 0.872 0.898 0.712
Our method 0.916 0.887 0.902 0.737
Table 11: Comparison of our approach with tradi-
tional NER systems
punctuation features.
There are previous works using features ex-
tracted from label distribution of unlabeled cor-
pus in NLP tasks. Schapire et al (2002) use a
set of features annotated with majority labels
to boost a logistic regression model. We are
different from their approach because there is
no pseudo-example labeling process in our ap-
proach. Qi et al (2009) investigated on large
set of distribution features and used these fea-
tures in a self-training way. They applied the
method on three tasks: named entity recogni-
tion, POS tagging and gene name recognition
and got relatively good results. Our approach is
different from theirs. Although we all consider
label distribution, the way we use features are
different. Besides, our approach uses two unla-
beled corpora which can mutually enhancing to
get better result.
5 Conclusion and Perspectives
In this paper, we presented a semi-supervised
method for Chinese word segmentation. Two
kinds of new features are used for the itera-
tive modeling: static statistical features and dy-
namic statistical features. The dynamic statis-
tical features use label distribution information
for text contexts, and can be adjusted automat-
ically during the co-training process. Experi-
mental results show that the new features can
improve the performance on the Chinese word
segmentation task. We further conducted exper-
iments to show that the performance is largely
improved, especially when the labeled data is
insufficient.
The proposed iterative semi-supervised
method is not limited to the Chinese word
segmentation task. It can be easily extended
to any sequence labeling task. For example, it
works well on the NER task as well. As our
future work, we plan to apply our method to
other natural language processing tasks, such
as text chunking.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Pro-
gram) (No. 2012AA011101) and National Natu-
ral Science Foundation of China (No.91024009).
We also thank Xu Sun and Qiuye Zhao for proof-
reading the paper.
319
References
Blum, A. and Mitchell, T. (1998). Combining
labeled and unlabeled data with co-training.
In Proceedings of the eleventh annual confer-
ence on Computational learning theory, pages
92?100. ACM.
Chang, B. and Han, D. (2010). Enhancing
domain portability of chinese segmentation
model using chi-square statistics and boot-
strapping. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 789?798. Association
for Computational Linguistics.
Chen, A., Zhou, Y., Zhang, A., and Sun, G.
(2005). Unigram language model for chinese
word segmentation. In Proceedings of the
4th SIGHAN Workshop on Chinese Language
Processing, pages 138?141. Association for
Computational Linguistics Jeju Island, Korea.
Dasgupta, S., Littman, M. L., and McAllester,
D. (2002). Pac generalization bounds for co-
training. Advances in neural information pro-
cessing systems, 1:375?382.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Gao, J., Andrew, G., Johnson, M., and
Toutanova, K. (2007). A comparative study
of parameter estimation methods for statisti-
cal natural language processing. In ANNUAL
MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page
824.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Compu-
tational Linguistics and the 44th annual meet-
ing of the Association for Computational Lin-
guistics, pages 673?680. Association for Com-
putational Linguistics.
Hafer, M. A. and Weiss, S. F. (1974). Word seg-
mentation by letter successor varieties. Infor-
mation storage and retrieval, 10(11):371?385.
Harris, Z. S. (1955). From phoneme to mor-
pheme. Language, 31(2):190?222.
He, J. and Wang, H. (2008). Chinese named en-
tity recognition and word segmentation based
on character. In Sixth SIGHAN Workshop on
Chinese Language Processing, page 128.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsu-
pervised segmentation of chinese text by use
of branching entropy. In Proceedings of the
COLING/ACL on Main conference poster
sessions, pages 428?435. Association for Com-
putational Linguistics.
Koo, T., Carreras, X., and Collins, M. (2008).
Simple semi-supervised dependency parsing.
Lafferty, J., McCallum, A., and Pereira, F.
(2001). Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data.
Li, Z. and Sun, M. (2009). Punctuation
as implicit annotations for chinese word
segmentation. Computational Linguistics,
35(4):505?512.
Low, J., Ng, H., and Guo, W. (2005). A
maximum entropy approach to chinese word
segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
McCallum, A. (2002). Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the Nineteenth Conference on Uncer-
tainty in Artificial Intelligence, pages 403?410.
Morgan Kaufmann Publishers Inc.
Miller, S., Guinness, J., and Zamanian, A.
(2004). Name tagging with word clusters
and discriminative training. In Proceedings of
HLT-NAACL, volume 4.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
320
vances in Intelligent Data Analysis, pages
238?247.
Pinto, D., McCallum, A., Wei, X., and Croft,
W. (2003). Table extraction using conditional
random fields. In Proceedings of the 26th an-
nual international ACM SIGIR conference on
Research and development in informaion re-
trieval, pages 235?242. ACM.
Qi, Y., Kuksa, P., Collobert, R., Sadamasa,
K., Kavukcuoglu, K., and Weston, J. (2009).
Semi-supervised sequence labeling with self-
learned features. In Data Mining, 2009.
ICDM?09. Ninth IEEE International Confer-
ence on, pages 428?437. IEEE.
Schapire, R., Rochery, M., Rahim, M., and
Gupta, N. (2002). Incorporating prior
knowledge into boosting. In MACHINE
LEARNING-INTERNATIONAL WORK-
SHOP THEN CONFERENCE-, pages
538?545.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka,
Y., and Tsujii, J. (2009). A discriminative
latent variable chinese segmenter with hybrid
word/character information. In Proceedings of
Human Language Technologies: The 2009 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 56?64. Association for Compu-
tational Linguistics.
Turian, J., Ratinov, L., and Bengio, Y. (2010).
Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguis-
tics, pages 384?394. Association for Compu-
tational Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Yarowsky, D. (1995). Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd annual meeting
on Association for Computational Linguistics,
pages 189?196. Association for Computational
Linguistics.
Zhang, R., Kikui, G., and Sumita, E. (2006).
Subword-based tagging by conditional ran-
dom fields for chinese word segmentation. In
Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196. Asso-
ciation for Computational Linguistics.
Zhang, Y. and Clark, S. (2007). Chi-
nese segmentation with a word-based percep-
tron algorithm. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL
LINGUISTICS, volume 45, page 840.
Zhao, H., Huang, C., and Li, M. (2006a). An
improved chinese word segmentation system
with conditional random field. In Proceed-
ings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing, volume 117. Syd-
ney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field mod-
eling. In Proceedings of PACLIC, volume 20,
pages 87?94.
321
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 266?277,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Go Climb a Dependency Tree and Correct the Grammatical Errors
Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
State-of-art systems for grammar error
correction often correct errors based on
word sequences or phrases. In this paper,
we describe a grammar error correction
system which corrects grammatical errors
at tree level directly. We cluster all error
into two groups and divide our system into
two modules correspondingly: the general
module and the special module. In the
general module, we propose a TreeNode
Language Model to correct errors related
to verbs and nouns. The TreeNode Lan-
guage Model is easy to train and the de-
coding is efficient. In the special module,
two extra classification models are trained
to correct errors related to determiners and
prepositions. Experiments show that our
system outperforms the state-of-art sys-
tems and improves the F
1
score.
1 Introduction
The task of grammar error correction is difficult
yet important. An automatic grammar error cor-
rection system can help second language (L2)
learners improve the quality of their writing. In re-
cent years, there are various competitions devoted
to grammar error correction, such as the HOO-
2011(Dale and Kilgarriff, 2011), HOO-2012(Dale
et al., 2012) and the CoNLL-2013 shared task (Ng
et al., 2013). There has been a lot of work ad-
dressing errors made by L2 learners. A significant
proportion of the systems for grammar error cor-
rection train individual statistical models to cor-
rect each special kind of error word by word and
ignore error interactions. These methods assume
no interactions between different kinds of gram-
matical errors. In real problem settings errors are
correlated, which makes grammar error correction
much more difficult.
Recent research begins to focus on the error
interaction problem. For example, Wu and Ng
(2013) decodes a global optimized result based
on the individual correction confidence of each
kind of errors. The individual correction confi-
dence is still based on the noisy context. Ro-
zovskaya and Roth (2013) uses a joint modeling
approach, which considers corrections in phrase
structures instead of words. For dependencies that
are not covered by the joint learning model, Ro-
zovskaya and Roth (2013) uses the results of Illi-
nois system in the joint inference. These results
are still at word level and are based on the noisy
context. These systems can consider error inter-
actions, however, the systems are complex and
inefficient. In both Wu and Ng (2013) and Ro-
zovskaya and Roth (2013), Integer Linear Pro-
gramming (ILP) is used for decoding a global op-
timized result. In the worst case, the time com-
plexity of ILP can be exponent.
In contrast, we think a better grammar error cor-
rection system should correct grammatical errors
at sentence level directly and efficiently. The sys-
tem should correct as many kinds of errors as pos-
sible in a generalized framework, while allowing
special models for some kinds of errors that we
need to take special care. We cluster all error into
two groups and correspondingly divide our sys-
tem into two modules: the general module and the
special module. In the general module, our sys-
tem views each parsed sentence as a dependency
tree. The system generates correction candidates
for each node on the dependency tree. The cor-
rection can be made on the dependency tree glob-
ally. In this module, nearly all replacement errors
related to verb form, noun form and subject-verb
agreement errors can be considered. In the spe-
cial module, two extra classification models are
used to correct the determiner errors and preposi-
tion errors . The classifiers are also trained at tree
node level. We take special care of these two kinds
266
of errors because these errors not only include re-
placement errors, but also include insertion and
deletion errors. A classification model is more
suitable for handling insertion and deletion errors.
Besides, they are the most common errors made
by English as a Second Language (ESL) learners
and are much easier to be incorporated into a clas-
sification framework.
We propose a TreeNode Language Model
(TNLM) to efficiently measure the correctness of
selecting a correction candidate of a node in the
general module. Similar to the existing statistical
language models which assign a probability to a
linear chain of words, our TNLM assigns correct-
ness scores directly on each node on the depen-
dency tree. We select candidates for each node
to maximize the global correctness score and use
these candidates to form the corrected sentence.
The global optimized inference can be tackled ef-
ficiently using dynamic programming. Because
the decoding is based on the whole sentence, error
interactions can be considered. Our TNLM only
needs to use context words related to each node
on the dependency tree. Training a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus. Experiments
show that our system can outperform the state-of-
art systems.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe the
task and give an overview of the system. In section
3 we describe the general module and in section 4
we describe the special module. Experiments are
described in section 5. In section 6 related works
are introduced, and the paper is concluded in the
last section.
2 Task and System Overview
2.1 Task Description
The task of grammar error correction aims to cor-
rect grammatical errors in sentences. There are
various competitions devoted to the grammar er-
ror correction task for L2 learners. The CoNLL-
2013 shared task is one of the most famous, which
focuses on correcting five types of errors that
are commonly made by non-native speakers of
English, including determiner, preposition, noun
number, subject-verb agreement and verb form er-
rors. The training data released by the task orga-
nizers come from the NUCLE corpus(Dahlmeier
et al., 2013). This corpus contains essays writ-
ten by ESL learners, which are then corrected by
English teachers. The test data are 50 student es-
says. Details of the corpus are described in Ng
et al. (2013).
2.2 System Architecture
In our system, lists of correction candidates are
first generated for each word. We generate can-
didates for nouns based on their plurality. We gen-
erate candidates for verbs based on their tenses.
Then we select the correction candidates that max-
imize the overall correctness score. An example
process of correcting figure 1(a) is shown in table
1.
Correcting grammatical errors using local sta-
tistical models on word sequence is insufficient.
The local models can only consider the contexts
in a fixed window. In the example of figure 1(a),
the context of the verb ?is? is ?that boy is on the?,
which sounds reasonable at first glance but is in-
correct when considering the whole sentence. The
limitation of local classifiers is that long distance
syntax information cannot be incorporated within
the local context. In order to effectively use the
syntax information to get a more accurate correct-
ing result, we think a better way is to tackle the
problem directly at tree level to view the sentence
as a whole. From figure 1(a) we can see that the
node ?is? has two children on the dependency tree:
?books? and ?on?. When we consider the node
?is?, its context is ?books is on?, which sounds in-
correct. Therefore, we can make better corrections
using such context information on nodes.
Therefore, our system corrects grammatical er-
rors on dependency trees directly. Because the
correlated of words are more linked on trees than
in a word sequence, the errors are more easier to
be corrected on the trees and the agreement of dif-
ferent error types is guaranteed by the edges. We
follow the strategy of treating different kinds of
errors differently, which is used by lots of gram-
mar error correction systems. We cluster the five
types of errors considered in CoNLL-2013 into
two groups and divide our system into two mod-
ules correspondingly.
? The general module, which is responsible
for the verb form errors, noun number errors
and subject-verb agreement errors. These er-
rors are all replacement errors, which can
be corrected by replacing the wrongly used
word with a reasonable candidate word.
267
Figure 1: Dependency parsing results of (a) the original sentence ?The books of that boy is on the desk
.? (b) the corrected sentence.
Position Original Correction Candidates Corrected
1 The The The
2 books books, book books
3 of of of
4 that that that
5 boy boy, boys boy
6 is is,are,am,was,were,be,being,been are
7 on on on
8 the the the
9 desk desk, desks desk
10 . . .
Table 1: An example of the ?correction candidate generation and candidate selection? framework.
? The special module, where two classifica-
tion models are used to correct the determiner
errors and preposition errors at tree level. We
take special care of these two kinds of errors
because these errors include both replace-
ment errors and insertion/deletion errors. Be-
sides, they are the most common errors made
by ESL learners and is much easier to be in-
corporated into a classification framework.
We should make it clear that we are not the first
to use tree level correction models on ungrammat-
ical sentences. Yoshimoto et al. (2013) uses a
Treelet Language model (Pauls and Klein, 2012)
to correct agreement errors. However, the perfor-
mance of Treelet language model is not that good
compared with the top-ranked system in CoNLL-
2013. The reason is that the production rules in the
Treelet language model are based on complex con-
texts, which will exacerbate the data sparseness
problem. The ?context? in Treelet language model
also include words ahead of treelets, which are
sometimes unrelated to the current node. In con-
trast, our TreeNode Language model only needs to
consider useful context words related to each node
on the dependency tree. To train a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus.
2.3 Data Preparation
Our system corrects grammatical errors on de-
pendency trees directly, therefore the sentences
in training and testing data should have been
parsed before being corrected. In our system, we
use the Stanford parser
1
to parse the New York
Times source of the Gigaword corpus
2
, and use the
parsed sentences as our training data. We use the
original training data provided by CoNLL-2013 as
the develop set to tune all parameters.
Some sentences in the news texts use a differ-
ent writing style against the sentences written by
ESL learners. For example, sentences written by
ESL learners seldom include dialogues between
people, while very often news texts include para-
graphs such as ??I am frightened!? cried Tom?. We
use heuristic rules to eliminate the sentences in the
Gigaword corpus that are less likely to appear in
the ESL writing. The heuristic rules include delet-
1
http://nlp.stanford.edu/software/lex-parser.shtml
2
https://catalog.ldc.upenn.edu/LDC2003T05
268
ing sentences that are too short or too long
3
, delet-
ing sentences that contains certain punctuations
such as quotation marks, or deleting sentences that
are not ended with a period.
In total we select and parse 5 million sen-
tences of the New York Times source of English
newswire in the Gigaword corpus. We build the
system and experiment based on these sentences.
3 The General Module
3.1 Overview
The general module aims to correct verb form er-
rors, noun number errors and subject-verb agree-
ment errors. Other replacement errors such as
spelling errors can also be incorporated into the
general module. Here we focus on verb form er-
rors, noun number errors and subject-verb agree-
ment errors only. Our general module views each
sentence as a dependency tree. All words in the
sentence form the nodes of the tree. Nodes are
linked through directed edges, annotated with the
dependency relations.
Before correcting the grammatical errors, the
general module should generate correction candi-
dates for each node first. For each node we use
the word itself as its first candidate. Because the
general module considers errors related to verbs
and nouns, we generate extra correction candi-
dates only for verbs and nouns. For verbs we use
all its verb forms as its extra candidates. For ex-
ample when considering the word ?speaks?, we
use itself and {speak, spoke, spoken, speaking}
as its correction candidates. For nouns we use
its singular form and plural form as its extra cor-
rection candidates. For example when consider-
ing the word ?dog?, we use itself and ?dogs? as
its correction candidate. If the system selects the
original word as the final correction, the sentence
remains unchanged. But for convenience we still
call the newly generated sentence ?the corrected
sentence?.
In a dependency tree, the whole sentence
s can be formulized as a list of production
rules r
1
, ..., r
L
of the form: [r = head ?
modifier
1
,modifier
2
...]. An example of all
production rules of figure 1(a) is shown in table
2. Because the production rules are made up of
words, selecting a different correction candidate
for only one node will result in a list of different
3
In our experiment, no less than 5 words and no more than
30 words.
production rules. For example, figure 1(b) selects
the correction candidate ?is? to replace the origi-
nal ?are?. Therefore the production rules of figure
1(b) include [are ? books, on], instead of [is ?
books, on] in figure 1(a).
books? The, of
of? boy
boy? that
is? books, on
on? desk
desk? the
Table 2: All the production rules in the example of
figure 1(a)
The overall correctness score of s, which
is score(s), can be further decomposed into
?
L
i=0
score(r
i
). A reasonable score function
should score the correct candidate higher than the
incorrect one. Consider the node ?is? in Figure
1(a), the production rule with head ?is? is [is ?
books, on]. Because the correction of ?is? is ?are?,
a reasonable scorer should have score([is ?
books, on]) < score([are? books, on]).
Given the formulation of sentence s =
[r
1
, ..., r
L
] and the candidates for each node, we
are faced with two problems:
1. Score Function. Given a fixed selection of
candidate for each node, how to compute
the overall score of the dependency tree, i.e.,
score(s). Because score(s) is decomposed
into
?
L
i=0
score(r
i
), the problem becomes
finding a score function to measure the cor-
rectness of each r given a fixed selection of
candidates.
2. Decoding. Given each node a list of correc-
tion candidates and a reasonable score func-
tion score(r) for the production rules, how to
find the selection of candidates that maximize
the overall score of the dependency tree.
For the first problem, we propose a TreeNode
Language Model as the correctness measure of a
fixed candidate selection. For the decoding prob-
lem, we use a dynamic programming method to
efficiently find the correction candidates that max-
imize the overall score. We will describe the de-
tails in the following sections.
One concern is whether the automatically
parsed trees are reliable for grammar error cor-
rection. We define ?reliable? as follows. If we
269
change some words in original sentence into their
reasonable correction candidates (e.g. change ?is?
to ?are?) but the structure of the dependency tree
does not change (except the replaced word and
its corresponding POS tag, which are definitely
changed), then we say the dependency tree is reli-
able for this sentence. To verify this we randomly
selected 1000 sentences parsed by the Stanford
Parser. We randomly select the verbs and nouns
and replace them with a wrong form. We parsed
the modified sentences again and asked 2 annota-
tors to examine whether the dependency trees are
reliable for grammar error correction. We find that
99% of the dependency trees are reliable. There-
fore we can see that the dependency tree can be
used as the structure for grammar error correction
directly.
3.2 TreeNode Language Model
In our system we use the score of TreeNode Lan-
guage Model (TNLM) as the scoring function.
Consider a node n on a dependency tree and as-
sume n has K modifiers C
1
, ..., C
K
as its child
nodes. We define Seq(n) = [C
1
, ..., n, ..., C
K
]
as an ordered sub-sequence of nodes that includes
the node n itself and all its child nodes. The or-
der of the sub-sequence in Seq(n) is sorted based
on their position in the sentence. In this formula-
tion, we can score the correctness of a production
rule r by scoring the correctness of Seq(n). Be-
cause Seq(n) is a word sequence, we can use a
language model to measure its correctness. The
sub-sequences are not identical to the original
text. Therefore instead of using ordinary language
models, we should train special language models
using the sub-sequences to measure the correct-
ness of a production rule.
Take the sentence in figure 2 as an example.
When considering the node ?is? in the word se-
quence, it is likely to be corrected into ?are? be-
cause it appear directly after the plural noun ?par-
ents?. However, by the definition above, the sub-
sequence corresponding to the node ?damaged? is
?car is damaged by ?. In such context, the word
?is? is less likely to be changed to ?are?. From
the example we can see that the sub-sequence is
suitable to be used to measure the correctness of
a production rule. From this example we can also
find that the sub-sequences are different with or-
dinary sentences, because ordinary sentences are
less likely to end with ?by?.
Table 3 shows all the sub-sequences in the ex-
ample of figure 2. If we collect all the sub-
sequences in the corpus to form a new sub-
sequence corpus, we can train a language model
based on the new sub-sequence corpus. This is
our TreeNode Language Model. One advantage
of TLM is that once we have generated the sub-
sequences, we can train the TLM in the same
way as we train ordinary language models. Be-
sides, the TLM is not limited to a fixed smoothing
method. Any smoothing methods for ordinary lan-
guage models are applicable for TLM.
Node Sub-sentence
The The
car The car of
of of parents
my my
parents my parents
is is
damaged car is damaged by
by by storm
the the
storm the storm
Table 3: All the sub-sentences in the example of
figure 2
In our system we train the TLM using the same
way as training tri-gram language model. For a
sub-sequence S = w
0
...w
L
, we calculate P (S) =
?
L
i=0
P (w
i
|w
i?1
w
i?2
). The smoothing method
we use is interpolation, which assumes the final
P
?
(w
i
|w
i?1
w
i?2
) of the tri-gram language model
follows the following decomposition:
P
?
(w
i
|w
i?1
w
i?2
) =?
1
P (w
i
|w
i?1
w
i?2
)
+?
2
P (w
i
|w
i?1
)
+?
3
P (w
i
)
(1)
where ?
1
, ?
2
and ?
3
are parameters sum to
1. The parameters ?
1
, ?
2
and ?
3
are estimated
through EM algorithm(Baum et al., 1970; Demp-
ster et al., 1977; Jelinek, 1980).
3.3 Decoding
The decoding problem is to select one correction
candidate for each node that maximizes the over-
all score of the corrected sentence. When the sen-
tence is long and contains many verbs and nouns,
enumerating all possible candidate selections is
time-consuming. We use a bottom-up dynamic
270
Figure 2: A illustrative sentence for TreeNode Language Model.
programming approach to find the maximized cor-
rections within polynomial time complexity.
For a node n with L correction candidates
n
1
, ...n
L
and K child nodes C
1
, ..., C
K
, we define
n.scores[i] as the maximum score if we choose
the ith candidate n
i
for n. Because we decode
from leaves to the root, C
1
.scores, ..., C
K
.scores
have already been calculated before we calculate
n.scores.
We assume the sub-sequence Seq(n
i
) =
[C
1
, ..., C
M
, n
i
, C
M+1
, ..., C
K
] without loss of
generality, where C
1
, .., C
M
are the nodes before
n
i
and C
M+1
, ..., C
K
are the nodes after n
i
.
We define c
i,j
as the jth correction can-
didate of child node C
i
. Given a se-
lection of candidates for each child node
seq = [c
1,j
1
, ..., c
M,j
M
, n
i
, c
M+1,j
M+1
, ..., c
K,j
K
],
we can calculate score(seq) as:
score(seq) = TNLM(seq)
K
?
i=1
C
i
.scores[j
i
]
(2)
where TNLM(seq) is the TreeNode Language
Model score of seq. Then, n.scores[i] is calcu-
lated as:
n.scores[i] = max
?seq
score(seq) (3)
Because seq is a word sequence, the maxi-
mization can be efficiently calculated using Viterbi
algorithm (Forney Jr, 1973). To be specific,
the Viterbi algorithm uses the transition scores
and emission scores as its input. The transition
scores in our model are the tri-gram probabilities
from our tri-gram TNLM. The emission scores
in our model are the candidate scores of each
child: C
1
.scores, ..., C
K
.scores, which have al-
ready been calculated.
After the bottom-up calculation, we only need
to look into the ?ROOT? node to find the maxi-
mum score of the whole tree. Similar to the Viterbi
algorithm, back pointers should be kept to find
which candidate is selected for the final corrected
sentence. Detailed decoding algorithm is shown in
table 4.
Function decode(Node n)
if n is leaf
set n.scores uniformly
return
for each child c of n
decode(c)
calculating n.scores using Viterbi
End Function
BEGIN
decode(ROOT )
find the maximum score for the tree and back-
track all candidates
END
Table 4: The Decoding algorithm
In the real world implementations, we add a
controlling parameter for the confidence of the
correctness of the inputs. We multiply ? on
P (w
0
|w
?2
w
?1
) of the tri-gram TNLM if the cor-
recting candidate w
0
is the same word in the orig-
inal input. ? is larger than 1 to ?emphasis? the
confidence of the original word because the most
of the words in the inputs are correct. The value of
? can be set using the development data.
3.4 The Special Module
The special module is designed for determiner er-
rors and preposition errors. We take special care
of these two kinds of errors because these errors
include insertion and deletion errors, which can-
not be corrected in the general module. Because
there is a fixed number of prepositions and deter-
miners, these two kinds of errors are much easier
to be incorporated into a classification framework.
Besides, they are the most common errors made by
ESL learners and there are lots of previous works
that leave valuable guidance for us to follow.
Similar to many previous state-of-art systems,
we treat the correction of determiner errors and
preposition errors as a classification problem. Al-
though some previous works (e.g. Rozovskaya
et al. (2013)) use NPs and the head of NPs as
271
features, they are basically local classifiers mak-
ing predictions on word sequences. Difference to
the local classifier approaches, we make predic-
tions on the nodes of the dependency tree directly.
In our system we correct determiner errors and
preposition errors separately.
For the determiner errors, we consider the in-
sertion, deletion and replacement of articles (i.e.
?a?, ?an? and ?the?). Because the articles are used
to modify nouns in the dependency trees, we can
classify based on noun nodes. We give each noun
node (node whose POS tag is noun) a label to in-
dicate which article it should take. We use left po-
sition (LP) and right position (RP) to specify the
position of the article. The article therefore lo-
cates between LP and RP. If a noun node already
has an article as its modifier, then LP will be the
position directly ahead of the article. In this case,
RP = LP + 2. If an insertion is needed, the RP
is the position of the first child node of the noun
node. In this case LP = RP ? 1. With this no-
tation, detailed feature templates we use to correct
determiner errors are listed in table 5. In our model
we use 3 labels: ?a?, ?the? and ???. We use ?a?, ?the?
to represent a noun node should be modified with
?a? or ??the? correspondingly. We use ???? to in-
dicate that no article is needed for the noun node.
We use rule-based method to distinguish between
?a? and ?an? as a post-process.
For the preposition errors, we only consider
deletion and replacement of an existing preposi-
tion. The classification framework is similar to
determiner errors. We consider classification on
preposition nodes (nodes whose POS tag is prepo-
sition). We use prepositions as labels to indicate
which preposition should be used. and use ???
to denote that the preposition should be deleted.
We use the same definition of LP and RP as the
correction of determiner errors. Detailed feature
templates we use to correct preposition errors are
listed in table 6. Similar to the previous work(Xing
et al., 2013), we find that adding more preposi-
tions will not improve the performance in our ex-
periments. Thus we only consider a fixed set of
prepositions: {in, for, to, of, on}.
Previous works such as Rozovskaya et al.
(2013) show that Naive Bayes model and averaged
perceptron model show better results than other
classification models. These classifiers can give a
reasonably good performance when there are lim-
ited amount of training data. In our system, we use
large amount of automatically generated training
data based on the parsed Gigaword corpus instead
of the limited training data provided by CoNLL-
2013.
Take generating training data for determiner er-
rors as an example. We generate training data
based on the parsed Gigaword corpus C described
in section 2. Each sentence S in C is a depen-
dency tree T . We use each noun node N on T as
one training instance. If N is modified by ?the?,
its label will be ?THE?. If N is modified by ?a?
or ?an?, its label will be ?A?. Otherwise its label
will be ?NULL?. Then we just omit the determiner
modifier and generate features based on table 5.
Generating training data for preposition errors is
the same, except we use preposition nodes instead
of noun nodes.
By generating training instances in this way, we
can get large amount of training data. Therefore
we think it is a good time to try different classifi-
cation models with enough training data. We ex-
periment on Naive Bayes, Averaged Perceptron,
SVM and Maximum Entropy models (ME) in a 5-
fold cross validation on the training data. We find
ME achieves the highest accuracy. Therefore we
use ME as the classification model in our system.
4 Experiment
4.1 Experiment Settings
In the experiments, we use our parsed Gigaword
corpus as the training data, use the training data
provided by CoNLL-2013 as the develop data, and
use the test data of CoNLL-2013 as test data di-
rectly. In the general module, the training data
is used for the training of TreeNode Language
Model. In the special module, the training data is
used for training individual classification models.
We use the M2 scorer (Dahlmeier and Ng,
2012b) provided by the organizer of CoNLL-2013
for the evaluation of our system. The M2 scorer
is widely used as a standard scorer in previous
systems. Because we make comparison with the
state-of-art systems on the CoNLL-2013 corpus,
we use the same evaluation metric F
1
score of M2
scorer as the evaluation metric.
In reality, some sentences may have more than
one kind of possible correction. As the example in
?The books of that boy is on the desk.?, the cor-
responding correction can be either ?The books of
that boy are on the desk.? or ?The book of that boy
is on the desk.?. The gold test data can only con-
272
Word Features w
LP
, w
LP?1
, w
LP?2
, w
RP
, w
RP+1
, w
RP+2
, w
LP?2
w
LP?1
, w
LP?1
w
LP
,
w
LP
w
RP
, w
RP
w
RP+1
, w
RP+1
w
RP+2
, w
LP?2
w
LP?1
w
LP
, w
LP?1
w
LP
w
RP
,
w
LP
w
RP
w
RP+1
, w
RP
w
RP+1
w
RP+2
Noun Node
Features
NN , w
LP
NN , w
LP?1
w
LP
NN , w
LP?2
w
LP?1
w
LP
NN
Father/Child
Node Features
Fa, w
RP
Fa, w
RP
w
RP+1
Fa, w
RP
w
RP+1
w
RP+2
Fa, Fa&Ch
Table 5: Feature templates for the determiner errors. w
i
is the word at the ith position. NN is the current
noun node. Fa is the father node of the current noun node. Ch is a child node of the current noun node.
Word Features w
LP
, w
LP?1
, w
LP?2
, w
RP
, w
RP+1
, w
RP+2
, w
LP?2
w
LP?1
, w
LP?1
w
LP
,
w
LP
w
RP
, w
RP
w
RP+1
, w
RP+1
w
RP+2
, w
LP?2
w
LP?1
w
LP
, w
LP?1
w
LP
w
RP
,
w
LP
w
RP
w
RP+1
, w
RP
w
RP+1
w
RP+2
Father/Child
Node Features
Fa, w
RP
Fa, w
RP
w
RP+1
Fa, w
RP
w
RP+1
w
RP+2
Fa, Fa&Ch
Table 6: Feature templates for preposition errors. w
i
is the word at the ith position. Fa is the father node
of the current preposition node. Ch is a child node of the current preposition node.
sider a small portion of possible answers. To re-
lieve this, the CoNLL-2013 shared task allows all
participating teams to provide alterative answers
if they believe their system outputs are also cor-
rect. These alterative answers form the ?Revised
Data? in the shared task, which indeed help evalu-
ate the outputs of the participating systems. How-
ever, the revised data only include alterative cor-
rections from the participating teams. Therefore
the evaluation is not that fair for future systems. In
our experiment we only use the original test data
as the evaluation dataset.
4.2 Experiment Results
We first show the performance of each stage of our
system. In our system, the general module and
the special module correct grammar errors conse-
quently. Therefore in table 7 we show the perfor-
mance when each component is added to the sys-
tem.
Method P R F1 score
TNLM 33.96% 17.71% 23.28%
+Det 32.83% 38.28% 35.35%
+Prep 32.64% 39.20% 35.62%
Table 7: Results of each stage in our system.
TNLM is the general module. ?+Det? is the sys-
tem containing the general module and determiner
part of special module.?+Prep? is the final system
We evaluate the effect of using TreeNode lan-
guage model for the general module. We compare
the TNLM with ordinary tri-gram language model.
We use the same amount of training data and the
same smoothing strategy (i.e. interpolation) for
both of them. Table 8 shows the comparison. The
TNLM can improve the F
1
by +2.1%.
Method P R F1 score
Ordinary LM 29.27% 16.68% 21.27%
Our TNLM 33.96% 17.71% 23.28%
Table 8: Comparison for the general module
between TNLM and ordinary tri-gram language
model on the test data.
Based on the result of the general module using
TNLM, we compare our tree level special mod-
ule against the local classification approach. The
special module of our system makes predictions
on the dependency tree directly, while local clas-
sification approaches make predictions on linear
chain of words and decide the article of a noun
Phrase or the preposition of a preposition phrase.
We use the same word level features for the two
approaches except for the local classifiers we do
not add tree level features. Table 9 shows the com-
parison.
When using the parsed Gigaword texts as train-
ing data, the quality of the sentences we select
will influence the result. For comparison, we ran-
domly select the same amount of sentences from
the same source of Gigaword and parse them as
a alterative training set. Table 10 shows the com-
parison between random chosen training data and
273
Method P R F1 score
Local Classifier 26.38% 39.14% 31.51%
Our Tree-based 32.64% 39.20% 35.62%
Table 9: Comparison for the special module on the
test data. The input of the special module is the
sentences corrected by the TNLM in the general
module.
the selected training data of our system. We can
see that the data selection (cleaning) procedure is
important for the improvement of system F1.
Method P R F1 score
Random 31.89% 35.85% 33.75%
Selected 32.64% 39.20% 35.62%
Table 10: Comparison of training using random
chosen sentences and selected sentences.
Method F1 score
Rozovskaya et al. (2013) 31.20%
Kao et al. (2013) 25.01%
Yoshimoto et al. (2013) 22.17%
Rozovskaya and Roth (2013) 35.20%
Our method 35.62%
Table 11: Comparison of F1 of different systems
on the test data .
4.3 Comparison With Other Systems
We also compare our system with the state-of-
art systems. The first two are the top-2 systems
at CoNLL-2013 shared task : Rozovskaya et al.
(2013) and Kao et al. (2013). The third one is
the Treelet Language Model in Yoshimoto et al.
(2013). The fourth one is Rozovskaya and Roth
(2013), which until now shows the best perfor-
mance. The comparison on the test data is shown
in table 11.
In CoNLL-2013 only 5 kinds of errors are con-
sidered. Our system can be slightly modified to
handle the case where other errors such as spelling
errors should be considered. In that case, we can
modify the candidate generation of the general
module. We only need to let the generate cor-
rection candidates be any possible words that are
similar to the original word, and run the same de-
coding algorithm to get the corrected sentence. As
a comparison, the ILP systems should add extra
scoring system to score extra kind of errors.
5 Related Works
Early grammatical error correction systems use
the knowledge engineering approach (Murata and
Nagao, 1994; Bond et al., 1996; Bond and Ikehara,
1996; Heine, 1998). However, manually designed
rules usually have exceptions. Therefore, the ma-
chine learning approach has become the dominant
approach recently. Previous machine learning ap-
proaches typically formulates the task as a clas-
sification problem. Of all the errors, determiner
and preposition errors are the two main research
topics (Knight and Chander, 1994; AEHAN et al.,
2006; Tetreault and Chodorow, 2008; Dahlmeier
and Ng, 2011). Features used in the classifica-
tion models include the context words, POS tags,
language model scores (Gamon, 2010), and tree
level features (Tetreault et al., 2010). Models used
include maximum entropy (AEHAN et al., 2006;
Tetreault and Chodorow, 2008), averaged percep-
tron, Naive Bayes (Rozovskaya and Roth, 2011),
etc. Other errors such as verb form and noun num-
ber errors also attract some attention recently (Liu
et al., 2010; Tajiri et al., 2012).
Recent research efforts have started to deal with
correcting different errors jointly (Gamon, 2011;
Park and Levy, 2011; Dahlmeier and Ng, 2012a;
Wu and Ng, 2013; Rozovskaya and Roth, 2013).
Gamon (2011) uses a high-order sequential label-
ing model to detect various errors. Park and Levy
(2011) models grammatical error correction using
a noisy channel model. Dahlmeier and Ng (2012a)
uses a beam search decoder, which iteratively cor-
rects to produce the best corrected output. Wu and
Ng (2013) and Rozovskaya and Roth (2013) use
ILP to decode a global optimized result. The joint
learning and joint inference are still at word/phrase
level and are based on the noisy context. In the
worst case, the time complexity of ILP can reach
exponent. In contrast, our system corrects gram-
mar errors at tree level directly, and the decoding
is finished with polynomial time complexity.
6 Conclusion and Future work
In this paper we describe our grammar error cor-
rection system which corrects errors at tree level
directly. We propose a TreeNode Language Model
and use it in the general module to correct errors
related to verbs and nouns. The TNLM is easy to
train and the decoding of corrected sentence is ef-
ficient. In the special module, two extra classifica-
tion models are trained to correct errors related to
274
determiners and prepositions at tree level directly.
Because our current method depends on an auto-
matically parsed corpus, future work may include
applying some additional filtering (e.g. Mejer and
Crammer (2012)) of the extended training set ac-
cording to some confidence measure of parsing ac-
curacy.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018), Major National Social
Science Fund of China (No.12&ZD227) and Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101). The contact author of this pa-
per, according to the meaning given to this role
by Key Laboratory of Computational Linguistics,
Ministry of Education, School of Electronics En-
gineering and Computer Science, Peking Univer-
sity, is Houfeng Wang. We thank Longyue Wang
and the reviewers for their comments and sugges-
tions.
References
AEHAN, N., Chodorow, M., and LEACOCK,
C. L. (2006). Detecting errors in english arti-
cle usage by non-native speakers.
Baum, L. E., Petrie, T., Soules, G., and Weiss, N.
(1970). A maximization technique occurring in
the statistical analysis of probabilistic functions
of markov chains. The annals of mathematical
statistics, pages 164?171.
Bond, F. and Ikehara, S. (1996). When and how to
disambiguate??countability in machine trans-
lation?. In International Seminar on Multi-
modal Interactive Disambiguation: MIDDIM-
96, pages 29?40. Citeseer.
Bond, F., Ogura, K., and Kawaoka, T. (1996).
Noun phrase reference in japanese-to-english
machine translation. arXiv preprint cmp-
lg/9601008.
Dahlmeier, D. and Ng, H. T. (2011). Grammatical
error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 915?923. Association for
Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012a). A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 568?578. Associa-
tion for Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012b). Better eval-
uation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 568?572. Association for Com-
putational Linguistics.
Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).
Building a large annotated corpus of learner en-
glish: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 22?31.
Dale, R., Anisimoff, I., and Narroway, G. (2012).
Hoo 2012: A report on the preposition and de-
terminer error correction shared task. In Pro-
ceedings of the Seventh Workshop on Build-
ing Educational Applications Using NLP, pages
54?62. Association for Computational Linguis-
tics.
Dale, R. and Kilgarriff, A. (2011). Helping our
own: The hoo 2011 pilot shared task. In Pro-
ceedings of the 13th European Workshop on
Natural Language Generation, pages 242?249.
Association for Computational Linguistics.
Dempster, A. P., Laird, N. M., Rubin, D. B., et al.
(1977). Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal
statistical Society, 39(1):1?38.
Forney Jr, G. D. (1973). The viterbi algorithm.
Proceedings of the IEEE, 61(3):268?278.
Gamon, M. (2010). Using mostly native data
to correct errors in learners? writing: a meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 163?171. As-
sociation for Computational Linguistics.
Gamon, M. (2011). High-order sequence model-
ing for language learner error detection. In Pro-
ceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications,
275
pages 180?189. Association for Computational
Linguistics.
Heine, J. E. (1998). Definiteness predictions
for japanese noun phrases. In Proceedings
of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th
International Conference on Computational
Linguistics-Volume 1, pages 519?525. Associ-
ation for Computational Linguistics.
Jelinek, F. (1980). Interpolated estimation of
markov source parameters from sparse data.
Pattern recognition in practice.
Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,
Boisson, J., Wu, J.-c., and Chang, J. S. (2013).
Conll-2013 shared task: Grammatical error cor-
rection nthu system description. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 20?25, Sofia, Bulgaria. Association
for Computational Linguistics.
Knight, K. and Chander, I. (1994). Automated
postediting of documents. In AAAI, volume 94,
pages 779?784.
Liu, X., Han, B., Li, K., Stiller, S. H., and Zhou,
M. (2010). Srl-based verb selection for esl.
In Proceedings of the 2010 conference on em-
pirical methods in natural language process-
ing, pages 1068?1076. Association for Compu-
tational Linguistics.
Mejer, A. and Crammer, K. (2012). Are you
sure? confidence in prediction of dependency
tree edges. In Proceedings of the 2012 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 573?576,
Montr?eal, Canada. Association for Computa-
tional Linguistics.
Murata, M. and Nagao, M. (1994). Determination
of referential property and number of nouns in
japanese sentences for machine translation into
english. arXiv preprint cmp-lg/9405019.
Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and
Tetreault, J. (2013). The conll-2013 shared task
on grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria. Association
for Computational Linguistics.
Park, Y. A. and Levy, R. (2011). Automated
whole sentence grammar correction using a
noisy channel model. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies-Volume 1, pages 934?944. Asso-
ciation for Computational Linguistics.
Pauls, A. and Klein, D. (2012). Large-scale syn-
tactic language modeling with treelets. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long
Papers-Volume 1, pages 959?968. Association
for Computational Linguistics.
Rozovskaya, A., Chang, K.-W., Sammons, M.,
and Roth, D. (2013). The university of illi-
nois system in the conll-2013 shared task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13?19, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2011). Algorithm
selection and model adaptation for esl correc-
tion tasks. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 924?933. Association for
Computational Linguistics.
Rozovskaya, A. and Roth, D. (2013). Joint learn-
ing and inference for grammatical error correc-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 791?802, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Tajiri, T., Komachi, M., and Matsumoto, Y.
(2012). Tense and aspect error correction for
esl learners using global context. In Proceed-
ings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short
Papers-Volume 2, pages 198?202. Association
for Computational Linguistics.
Tetreault, J., Foster, J., and Chodorow, M. (2010).
Using parse features for preposition selection
and error detection. In Proceedings of the acl
2010 conference short papers, pages 353?358.
Association for Computational Linguistics.
Tetreault, J. R. and Chodorow, M. (2008). The
ups and downs of preposition error detec-
tion in esl writing. In Proceedings of the
22nd International Conference on Computa-
276
tional Linguistics-Volume 1, pages 865?872.
Association for Computational Linguistics.
Wu, Y. and Ng, H. T. (2013). Grammatical error
correction using integer linear programming. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1456?1465, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Xing, J., Wang, L., Wong, D. F., Chao, L. S., and
Zeng, X. (2013). Um-checker: A hybrid sys-
tem for english grammatical error correction.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 34?42, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-
aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-
machi, M., and Matsumoto, Y. (2013). Naist at
2013 conll grammatical error correction shared
task. CoNLL-2013, 26.
277
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405?1414,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Chinese Abbreviations with Minimum Semantic Unit and
Global Constraints
Longkai Zhang Li Li Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, {li.l,wanghf,xusun}@pku.edu.cn
Abstract
We propose a new Chinese abbreviation
prediction method which can incorporate
rich local information while generating the
abbreviation globally. Different to previ-
ous character tagging methods, we intro-
duce the minimum semantic unit, which is
more fine-grained than character but more
coarse-grained than word, to capture word
level information in the sequence labeling
framework. To solve the ?character dupli-
cation? problem in Chinese abbreviation
prediction, we also use a substring tagging
strategy to generate local substring tagging
candidates. We use an integer linear pro-
gramming (ILP) formulation with various
constraints to globally decode the final ab-
breviation from the generated candidates.
Experiments show that our method outper-
forms the state-of-the-art systems, without
using any extra resource.
1 Introduction
Abbreviation is defined as a shortened description
of the original fully expanded form. For example,
?NLP? is the abbreviation for the corresponding
full form ?Natural Language Processing?. The ex-
istence of abbreviations makes it difficult to iden-
tify the terms conveying the same concept in the
information retrieval (IR) systems and machine
translation (MT) systems. Therefore, it is impor-
tant to maintain a dictionary of the prevalent orig-
inal full forms and the corresponding abbrevia-
tions.
Previous works on Chinese abbreviation gen-
eration focus on the sequence labeling method,
which give each character in the full form an extra
label to indicate whether it is kept in the abbre-
viation. One drawback of the character tagging
strategy is that Chinese characters only contain
limited amount of information. Using character-
based method alone is not enough for Chinese ab-
breviation generation. Intuitively we can think of a
word as the basic tagging unit to incorporate more
information. However, if the basic tagging unit
is word, we need to design lots of tags to repre-
sent which characters are kept for each unit. For a
word with n characters, we should design at least
2
n
labels to cover all possible situations. This re-
duces the generalization ability of the proposed
model. Besides, the Chinese word segmentation
errors may also hurt the performance. Therefore
we propose the idea of ?Minimum Semantic Unit?
(MSU) which is the minimum semantic unit in
Chinese language. Some of the MSUs are words,
while others are more fine-grained than words.
The task of selecting representative characters in
the full form can be further broken down into se-
lecting representative characters in the MSUs. We
model this using the MSU-based tagging method,
which can both utilize semantic information while
keeping the tag set small.
Meanwhile, the sequence labeling method per-
forms badly when the ?character duplication? phe-
nomenon exists. Many Chinese long phrases con-
tain duplicated characters, which we refer to as
the ?character duplication? phenomenon. There is
no sound criterion for the character tagging mod-
els to decide which of the duplicated character
should be kept in the abbreviation and which one
to be skipped. An example is ????????
??(Beijing University of Aeronautics and Astro-
nautics) whose abbreviation is ????. The char-
acter ??? appears twice in the full form and only
one is kept in the abbreviation. In these cases, we
can break the long phase into local substrings. We
can find the representative characters in the sub-
strings instead of the long full form and let the de-
coding phase to integrate useful information glob-
ally. We utilize this sub-string based approach and
obtain this local tagging information by labeling
1405
on the sub-string of the full character sequence.
Given the MSU-based and substring-based
methods mentioned above, we can get a list of
potential abbreviation candidates. Some of these
candidates may not agree on keeping or skipping
of some specific characters. To integrate their ad-
vantages while considering the consistency, we
further propose a global decoding strategy using
Integer Linear Programming(ILP). The constraints
in ILP can naturally incorporate ?non-local? infor-
mation in contrast to probabilistic constraints that
are estimated from training examples. We can also
use linguistic constraints like ?adjacent identical
characters is not allowed? to decode the correct
abbreviation in examples like the previous ????
example.
Experiments show that our Chinese abbrevia-
tion prediction system outperforms the state-of-
the-art systems. In order to reduce the size of
the search space, we further propose pruning con-
straints that are learnt from the training corpus.
Experiment shows that the average number of con-
straints is reduced by about 30%, while the top-1
accuracy is not affected.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
our method, including the MSUs, the substring-
based tagging strategy and the ILP decoding pro-
cess. Experiments are described in section 3. We
also give a detailed analysis of the results in sec-
tion 3. In section 4 related works are introduced,
and the paper is concluded in the last section.
2 System Architecture
2.1 Chinese Abbreviation Prediction
Chinese abbreviations are generated by selecting
representative characters from the full forms. For
example, the abbreviation of ?????? (Peking
University) is ???? which is generated by se-
lecting the first and third characters, see TABLE
1. This can be tackled from the sequence labeling
point of view.
Full form ? ? ? ?
Status Keep Skip Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Peking University)
From TABLE 1 we can see that Chinese abbre-
viation prediction is a problem of selecting repre-
sentative characters from the original full form
1
.
Based on this assumption, previous works mainly
focus on this character tagging schema. In these
methods, the basic tagging unit is the Chinese
character. Each character in the full form is la-
beled as ?K? or ?S?, where ?K? means the current
character should be kept in abbreviation and ?S?
means the current character should be skipped.
However, a Chinese character can only contain
limited amount of information. Using character-
based method alone is not enough for Chinese
abbreviation generation. We introduce an MSU-
based method, which models the process of se-
lecting representative characters given local MSU
information.
2.2 MSU Based Tagging
2.2.1 Minimum Semantic Unit
Because using the character-based method is not
enough for Chinese abbreviation generation, we
may think of word as the basic tagging unit to in-
corporate more information intuitively. In English,
the abbreviations (similar to acronyms) are usually
formed by concatenating initial letters or parts of a
series of words. In other words, English abbrevia-
tion generation is based on words in the full form.
However, in Chinese, word is not the most suit-
able abbreviating unit. Firstly, there is no natural
boundary between Chinese words. Errors from the
Chinese word segmentation tools will accumulate
to harm the performance of abbreviation predic-
tion. Second, it is hard to design a reasonable tag
set when the length of a possible Chinese word is
very long. The second column of TABLE 2 shows
different ways of selecting representative charac-
ters of Chinese words with length 3. For a Chi-
nese compound word with 3 characters, there are 6
possible ways to select characters. In this case we
should have at least 6 kinds of tags to cover all pos-
sible situations. The case is even worse for words
with more complicated structures. A suitable ab-
breviating unit should be smaller than word.
We propose the ?Minimum Semantic Unit
(MSU)? as the basic tagging unit. We define MSU
as follows:
1. A word whose length is less or equal to 2 is
an MSU.
1
A small portion of Chinese abbreviations are not gener-
ated from the full form. For example, the abbreviation of ??
??(Shan Dong Province) is ???. However, we can use a
look-up table to get this kind of abbreviations.
1406
Full form SK Label MSUs
???(nursery) ?/K?/S?/S ??+?
???(allowance) ?/S?/K?/S ??+?
???(Credit card) ?/S?/S?/K ??+?
???(Hydropower Station) ?/K?/K?/S ?+?+?
???(Senate) ?/K?/S?/K ?+?+?
???(Music group) ?/S?/K?/K ??+?
Table 2: Representing characters of Chinese words with length 3 (K for keep and S for skip) and the
corresponding MSUs
2. A word whose length is larger than 2, but
does not contain any MSUs with length equal
to 2. For example, ?????(Railway Sta-
tion) is not an MSU because the first two
characters ????(Train) can form an MSU.
By this definition, all 6 strings in TABLE 2 are
often thought as a word, but they are not MSUs
in our view. Their corresponding MSU forms are
shown in TABLE 2.
We collect all the MSUs from the benchmark
datasets provided by the second International Chi-
nese Word Segmentation Bakeoff
2
. We choose the
Peking University (PKU) data because it is more
fine-grained than all other corpora. Suppose we
represent the segmented data as L (In our case L
is the PKU word segmentation data), the MSU se-
lecting algorithm is shown in TABLE 3.
For a given full form, we first segment it us-
ing a standard word segmenter to get a coarse-
grained segmentation result. Here we use the Stan-
ford Chinese Word Segmenter
3
. Then we use the
MSU set to segment each word using the strategy
of ?Maximum Forward Matching?
4
to get the fine-
grained MSU segmentation result.
2.2.2 Labeling strategy
For MSU-based tagging, we use a labeling method
which uses four tags, ?KSFL?. ?K? stands for
?Keep the whole unit?, ?S? stands for ?Skip the
whole unit?, ?F? stands for ?keep the First charac-
ter of the unit?, and Label ?L? stands for ?keep the
Last character of the unit?. An example is shown
in TABLE 4.
The ?KSFL? tag set is also applicable for MSUs
whose length is greater than 2 (an example is ??
??/chocolate?). By examining the corpus we
find that such MSUs are either kept of skipped in
2
http://www.sighan.org/bakeoff2005/
3
http://nlp.stanford.edu/software/
segmenter.shtml
4
In Chinese, ?Forward? means from left to right.
????????????? (The ab-
breviation is ??????)
KSFL ??/K ??/F ??/S ??/S ?
?/F?/S
Table 4: The abbreviation ?????? of ????
??????? (National Linguistics Work Com-
mittee) based on MSU tagging.
the final abbreviations. Therefore, the labels of
these long MSUs are either ?K? or ?S?. Empirically,
this assumption holds for MSUs, but does not hold
for words
5
.
2.2.3 Feature templates
The feature templates we use are as follows. See
TABLE 5.
1. Word X
i
(?2 ? i ? 2)
2. POS tag of word X
i
(?2 ? i ? 2)
3. Word Bigrams (X
i
, X
i+1
) (?2 ? i ? 1)
4. Type of word X
i
(?2 ? i ? 2)
5. Length of word X
i
(?2 ? i ? 2)
Table 5: Feature templates for unit tagging. X
represents the MSU sequence of the full form. X
i
represents the ith MSU in the sequence.
Templates 1, 2 and 3 express word uni-grams
and bi-grams. In MSU-based tagging, we can uti-
lize the POS information, which we get from the
Stanford Chinese POS Tagger
6
. In template 4, the
type of word refers to whether it is a number, an
English word or a Chinese word. Because the ba-
sic tagging unit is MSU, which carries word infor-
mation, we can use many features that are infeasi-
ble in character-based tagging.
5
In table 2, all examples are partly kept.
6
http://nlp.stanford.edu/software/
tagger.shtml
1407
Init:
Let MSUSet = empty set
For each word w in L:
If Length(w) ? 2
Add w to MSUSet
End if
End for
For each word w in L:
If Length(w) > 2 and no word x in MSUSet is a substring of w
Add w to MSUSet
End if
End for
Return MSUSet
Table 3: Algorithm for collecting MSUs from the PKU corpus
2.2.4 Sequence Labeling Model
The MSU-based method gives each MSU an ex-
tra indicative label. Therefore any sequence label-
ing model is appropriate for the method. Previous
works showed that Conditional Random Fields
(CRFs) can outperform other sequence labeling
models like MEMMs in abbreviation generation
tasks (Sun et al., 2009; Tsuruoka et al., 2005). For
this reason we choose CRFs model in our system.
For a given full form?s MSU list, many can-
didate abbreviations are generated by choosing
the k-best results of the CRFs. We can use the
forward-backward algorithm to calculate the prob-
ability of a specified tagging result. To reduce the
searching complexity in the ILP decoding process,
we delete those candidate tagged sequences with
low probability.
2.3 Substring Based Tagging
As mentioned in the introduction, the sequence
labeling method, no matter character-based or
MSU-based, perform badly when the ?character
duplication? phenomenon exists. When the full
form contains duplicated characters, there is no
sound criterion for the sequence tagging strategy
to decide which of the duplicated character should
be kept in the abbreviation and which one to be
skipped. On the other hand, we can tag the sub-
strings of the full form to find the local represen-
tative characters in the substrings of the long full
form. Therefore, we propose the sub-string based
approach to given labeling results on sub-strings.
These results can be integrated into a more accu-
rate result using ILP constraints, which we will de-
scribe in the next section.
Another reason for using the sub-string based
methods is that long full forms contain more char-
acters and are much easier to make mistakes dur-
ing the sequence labeling phase. Zhang et al.
(2012) shows that if the full form contains less
than 5 characters, a simple tagger can reach an ac-
curacy of 70%. Zhang et al. (2012) also shows that
if the full form is longer than 10 characters, the
average accuracy is less than 30%. The numerous
potential candidates make it hard for the tagger to
choose the correct one. For the long full forms,
although the whole sequence is not correctly la-
beled, we find that if we only consider its short
substrings, we may find the correct representative
characters. This information can be integrated into
the decoding model to adjust the final result.
We use the MSU-based tagging method in the
sub-string tagging. The labeling strategy and fea-
ture templates are the same to the MSU-based tag-
ging method. In practice, enumerating all sub-
sequences of a given full form is infeasible if the
full form is very long. For a given full form,
we use the boundary MSUs to reduce the pos-
sible sub-sequence set. For example, ????
???(Chinese Academy of Science) has 5 sub-
sequences: ????, ??????, ????, ???
?? and ???.
2.4 ILP Formulation of Decoding
Given the MSU-based and sub-sequence-based
methods mentioned above as well as the preva-
lent character-based methods, we can get a list
of potential abbreviation candidates and abbrevi-
ated substrings. We should integrate their advan-
tages while keeping the consistency between each
1408
candidate. Therefore we further propose a global
decoding strategy using Integer Linear Program-
ming(ILP). The constraints in ILP can naturally
incorporate ?non-local? information in contrast to
probabilistic constraints that are estimated from
training examples. We can also use linguistic con-
straints like ?adjacent identical characters is not
allowed? to decode the correct abbreviation in ex-
amples like the ???? example in section 1.
Formally, given the character sequence of the
full form c = c
1
...c
l
, we keep Q top-ranked
MSU-based tagging results T=(T
1
, ..., T
Q
) and M
tagged substrings S=(S
1
, ..., S
M
) using the meth-
ods described in previous sections. We also
use N top-ranked character-based tagging results
R=(R
1
, ..., R
N
) based on the previous character-
based works. We also define the setU = S?R?T
as the union of all candidate sequences. Our goal
is to find an optimal binary variable vector solution
~v = ~x~y~z = (x
1
, ..., x
M
, y
1
, ..., y
N
, z
1
, ..., z
Q
) that
maximizes the object function:
?
1
M
?
i=1
score(S
i
) ? x
i
+ ?
2
N
?
i=1
score(R
i
) ? y
i
+?
3
Q
?
i=1
score(T
i
) ? z
i
subject to constrains in TABLE 6. The parame-
ters ?
1
, ?
2
, ?
3
controls the preference of the three
parts, and can be decided using cross-validation.
Constraint 1 indicates that x
i
, y
i
, z
i
are all
boolean variables. They are used as indicator vari-
ables to show whether the corresponding tagged
sequence is in accordance with the final result.
Constraint 2 is used to guarantee that at most
one candidate from the character-based tagging is
preserved. We relax the constraint to allow the
sum to be zero in case that none of the top-ranked
candidate is suitable to be the final result. If the
sum equals zero, then the sub-sequence based tag-
ging method will generate a more suitable result.
Constrain 3 has the same utility for the MSU-
based tagging.
Constraint 4, 5, 6 are inter-method constraints.
We use them to guarantee that the labels of the
preserved sequences of different tagging methods
do not conflict with each other. Constraint 7 is
used to guarantee that the labels of the preserved
sub-strings do not conflict with each other.
Constraint 8 is used to solve the ?character du-
plicate? problem. When two identical characters
are kept adjacently, only one of them will be kept.
Which one will be kept depends on the global de-
coding score. This is the advantage of ILP against
traditional sequence labeling methods.
2.5 Pruning Constraints
The efficiency of solving the ILP decoding prob-
lem depends on the number of candidate tagging
sequences N and Q, as well as the number of sub-
sequences M. Usually, N and Q is less than 10 in
our experiment. Therefore, M influences the time
complexity the most. Because we use the bound-
ary of MSUs instead of enumerating all possible
subsequences, the value of M can be largely re-
duced.
Some characters are always labeled as ?S? or
?K? once the context is given. We can use this
phenomenon to reduce the search space of decod-
ing. Let c
i
denote the i
th
character relative to the
current character c
0
and t
i
denote the tag of c
i
. The
context templates we use are listed in TABLE 7.
Uni-gram Contexts c
0
, c
?1
, c
1
Bi-gram Contexts c
?1
c0, c
?1
c
1
, c
0
c
1
Table 7: Context templates used in pruning
With respect to a training corpus, if a context
C relative to c
0
always assigns a certain tag t to
c
0
, then we can use this constraint in pruning. We
judge the degree of ?always? by checking whether
count(C?t
0
=t)
count(C)
> threshold. The threshold is a
non-negative real number under 1.0.
3 Experiments
3.1 Data and Evaluation Metric
We use the abbreviation corpus provided by Insti-
tute of Computational Linguistics (ICL) of Peking
University in our experiments. The corpus is sim-
ilar to the corpus used in Sun et al. (2008, 2009);
Zhang et al. (2012). It contains 8, 015 Chinese ab-
breviations, including noun phrases, organization
names and some other types. Some examples are
presented in TABLE 8. We use 80% abbreviations
as training data and the rest as testing data. In
some cases, a long phrase may contain more than
one abbreviation. For these cases, the corpus just
keeps their most commonly used abbreviation for
each full form.
The evaluation metric used in our experiment
is the top-K accuracy, which is also used by
Tsuruoka et al. (2005), Sun et al. (2009) and
1409
1. x
i
? {0, 1}, y
i
? {0, 1}, z
i
? {0, 1}
2.
?
N
i=1
y
i
? 1
3.
?
Q
i=1
z
i
? 1
4. ?R
i
? R, S
j
? S, if R
i
and S
j
have a same position but the position gets different labels,
then y
i
+ x
j
? 1
5. ?T
i
? T , S
j
? S, if T
i
and S
j
have a same position but the position gets different labels,
then z
i
+ x
j
? 1
6. ?R
i
? R, T
j
? T , if R
i
and T
j
have a same position but the position gets different labels,
then x
i
+ z
j
? 1
7. ?S
i
, S
j
? S if S
i
and S
j
have a same position but the position gets different labels, then
z
i
+ z
j
? 1
8. ?S
i
, S
j
? S if the last character S
i
keeps is the same as the first character S
j
keeps, then
z
i
+ z
j
? 1
Table 6: Constraints for ILP
Type Full form Abbreviation
Noun Phrase ????(Excellent articles) ??
Organization ????(Writers? Association) ??
Coordinate phrase ????(Injuries and deaths) ??
Proper noun ????(Media) ??
Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)
Zhang et al. (2012). The top-K accuracy measures
what percentage of the reference abbreviations are
found if we take the top N candidate abbreviations
from all the results. In our experiment, top-10 can-
didates are considered in re-ranking phrase and the
measurement used is top-1 accuracy (which is the
accuracy we usually refer to) because the final aim
of the algorithm is to detect the exact abbreviation.
CRF++
7
, an open source linear chain CRF tool,
is used in the sequence labeling part. For ILP part,
we use lpsolve
8
, which is also an open source tool.
The parameters of these tools are tuned through
cross-validation on the training data.
3.2 Results
TABLE 9 shows the top-K accuracy of the
character-based and MSU-based method. We can
see that the MSU-based tagging method can uti-
lize word information, which can get better perfor-
mance than the character-based method. We can
also figure out that the top-5 candidates include the
reference abbreviation for most full forms. There-
fore reasonable decoding by considering all possi-
ble labeling of sequences may improve the perfor-
mance. Although the MSU-based methods only
outperforms character-based methods by 0.75%
7
http://crfpp.sourceforge.net/
8
http://lpsolve.sourceforge.net/5.5/
for top-1 accuracy, it is much better when consid-
ering top-2 to top-5 accuracy (+2.5%). We further
select the top-ranked candidates for ILP decod-
ing. Therefore the MSU-based method can further
improve the performance in the global decoding
phase.
K char-based MSU-based
1 0.5714 0.5789
2 0.6879 0.7155
3 0.7681 0.7819
4 0.8070 0.8283
5 0.8333 0.8583
Table 9: Top-K (K ? 5) results of character-based
tagging and MSU-based tagging
We then use the top-5 candidates of character-
based method and MSU-based method, as well
as the top-2 results of sub-sequence labeling in
the ILP decoding phase. Then we select the top-
ranked candidate as the final abbreviation of each
instance. TABLE 10 shows the results. We can see
that the accuracy of our method is 61.0%, which
improved by +3.89% compared to the character-
based method, and +3.14% compared to the MSU-
based method.
We find that the ILP decoding phase do play
an important role in generating the right an-
1410
Method Top-1 Accuracy
Char-based 0.5714
MSU-based 0.5789
ILP Result 0.6103
Table 10: Top-1 Accuracy after ILP decoding
swer. Some reference abbreviations which are not
picked out by either tagging method can be found
out after decoding. TABLE 11 shows the exam-
ple of the organization name ?????????
???? (Higher Education Admissions Office).
Neither the character-based method nor the MSU-
based method finds the correct answer ?????,
while after ILP decoding, ????? becomes the
final result. TABLE 12 and TABLE 13 give two
more examples.
True Result ???
Char-based ??
MSU-based ???
ILP Decoding ???
Table 11: Top-1 result of ??????????
??? (Higher Education Admissions Office)
True Result ??
Char-based ??
MSU-based ???
ILP Decoding ??
Table 12: Top-1 result of ?????? (Articles
exceed the value)
True Result ????
Char-based ???
MSU-based ???
ILP Decoding ????
Table 13: Top-1 result of ??????????
(Visual effects of sound and lights)
3.3 Improvements Considering Length
Full forms that are longer than five characters are
long terms. Long terms contain more characters,
which is much easier to make mistakes. Figure
1 shows the top-1 accuracy respect to the term
length using different tagging methods and using
ILP decoding. The x-axis represents the length of
the full form. The y-axis represents top-1 accu-
racy. We find that our method works especially
better than pure character-based or MSU-based
approach when the full form is long. By decod-
ing using ILP, both local and global information
are incorporated. Therefore many of these errors
can be eliminated.
Figure 1: Top-1 accuracy of different methods
considering length
3.4 Effect of pruning
As discussed in previous sections, if we are able
to pre-determine that some characters in a certain
context should be kept or skipped, then the num-
ber of possible boolean variable x can be reduced.
TABLE 14 shows the differences. To guarantee
a high accuracy, we set the threshold to be 0.99.
When the original full form is partially tagged by
the pruning constraints, the number of boolean
variables per full form is reduced from 34.4 to
25.5. By doing this, we can improve the predic-
tion speed over taking the raw input.
From TABLE 14 we can also see that the top-
1 accuracy is not affected by these pruning con-
straints. This is obvious, because CRF itself has
a strong modeling ability. The pruning constraints
cannot improve the model accuracy. But they can
help eliminate those false candidates to make the
ILP decoding faster.
Accuracy Average length Time(s)
raw 0.6103 34.4 12.5
pruned 0.6103 25.5 7.1
Table 14: Comparison of testing time of raw input
and pruned input
3.5 Compare with the State-of-the-art
Systems
We also compare our method with previous meth-
ods, including Sun et al. (2009) and Zhang et al.
(2012). Because we use a different corpus, we
re-implement the system Sun et al. (2009), Zhang
1411
et al. (2012) and Sun et al. (2013), and experi-
ment on our corpus. The first two are CRF+GI
and DPLVM+GI in Sun et al. (2009), which are
reported to outperform the methods in Tsuruoka
et al. (2005) and Sun et al. (2008). For DPLVM
we use the same model in Sun et al. (2009) and
experiment on our own data. We also compare
our approach with the method in Zhang et al.
(2012). However, Zhang et al. (2012) uses dif-
ferent sources of search engine result information
to re-rank the original candidates. We do not use
any extra web resources. Because Zhang et al.
(2012) uses web information only in its second
stage, we use ?BIEP?(the tag set used by Zhang
et al. (2012)) to denote the first stage of Zhang
et al. (2012), which also uses no web information.
TABLE 15 shows the results of the comparisons.
We can see that our method outperforms all other
methods which use no extra resource. Because
Zhang et al. (2012) uses extra web resource, the
top-1 accuracy of Zhang et al. (2012) is slightly
better than ours.
Method Top-1 Accuracy
CRF+GI 0.5850
DPLVM+GI 0.5990
BIEP 0.5812
Zhang et al. (2012) 0.6205
Our Result 0.6103
Table 15: Comparison with the state-of-the-art
systems
4 Related Work
Previous research mainly focuses on ?abbrevia-
tion disambiguation?, and machine learning ap-
proaches are commonly used (Park and Byrd,
2001; HaCohen-Kerner et al., 2008; Yu et al.,
2006; Ao and Takagi, 2005). These ways of link-
ing abbreviation pairs are effective, however, they
cannot solve our problem directly. In many cases
the full form is definite while we don?t know the
corresponding abbreviation.
To solve this problem, some approaches main-
tain a database of abbreviations and their corre-
sponding ?full form? pairs. The major problem
of pure database-building approach is obvious. It
is impossible to cover all abbreviations, and the
building process is quit laborious. To find these
pairs automatically, a powerful approach is to find
the reference for a full form given the context,
which is referred to as ?abbreviation generation?.
There is research on heuristic rules for gen-
erating abbreviations Barrett and Grems (1960);
Bourne and Ford (1961); Taghva and Gilbreth
(1999); Park and Byrd (2001); Wren et al. (2002);
Hearst (2003). Most of them achieved high per-
formance. However, hand-crafted rules are time
consuming to create, and it is not easy to transfer
the knowledge of rules from one language to an-
other.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed a supervised
learning approach by using SVM model. Tsu-
ruoka et al. (2005); Sun et al. (2009) formal-
ized the process of abbreviation generation as a
sequence labeling problem. In Tsuruoka et al.
(2005) each character in the full form is associated
with a binary value label y, which takes the value
S (Skip) if the character is not in the abbreviation,
and value P (Preserve) if the character is in the ab-
breviation. Then a MEMM model is used to model
the generating process. Sun et al. (2009) followed
this schema but used DPLVM model to incor-
porate both local and global information, which
yields better results. Sun et al. (2013) also uses
machine learning based methods, but focuses on
the negative full form problem, which is a little
different from our work.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations.Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method uses no extra resource, but reaches
comparable results.
ILP shows good results in many NLP tasks.
Punyakanok et al. (2004); Roth and Yih (2005)
used it in semantic role labeling (SRL). Martins
et al. (2009) used it in dependency parsing. (Zhao
and Marcus, 2012) used it in Chinese word seg-
mentation. (Riedel and Clarke, 2006) used ILP
1412
in dependency parsing. However, previous works
mainly focus on the constraints of avoiding bound-
ary confliction. For example, in SRL, two argu-
ment of cannot overlap. In CWS, two Chinese
words cannot share a same character. Different to
their methods, we investigate on the conflict of la-
bels of character sub-sequences.
5 Conclusion and Future work
We propose a new Chinese abbreviation predic-
tion method which can incorporate rich local in-
formation while generating the abbreviation glob-
ally. We propose the MSU, which is more coarse-
grained than character but more fine-grained than
word, to capture word information in the se-
quence labeling framework. Besides the MSU-
based method, we use a substring tagging strategy
to generate local substring tagging candidates. We
use an ILP formulation with various constraints
to globally decode the final abbreviation from the
generated candidates. Experiments show that our
method outperforms the state-of-the-art systems,
without using any extra resource. This method
is not limited to Chinese abbreviation generation,
it can also be applied to similar languages like
Japanese.
The results are promising and outperform the
baseline methods. The accuracy can still be im-
proved. Potential future works may include using
semi-supervised methods to incorporate unlabeled
data and design reasonable features from large cor-
pora. We are going to study on these issues in the
future.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Martins, A. F., Smith, N. A., and Xing, E. P.
(2009). Concise integer linear programming
formulations for dependency parsing. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1,
pages 342?350. Association for Computational
Linguistics.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Punyakanok, V., Roth, D., Yih, W.-t., and Zimak,
D. (2004). Semantic role labeling via integer
linear programming inference. In Proceedings
of the 20th international conference on Compu-
1413
tational Linguistics, page 1346. Association for
Computational Linguistics.
Riedel, S. and Clarke, J. (2006). Incremental in-
teger linear programming for non-projective de-
pendency parsing. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 129?137. Associ-
ation for Computational Linguistics.
Roth, D. and Yih, W.-t. (2005). Integer linear
programming inference for conditional random
fields. In Proceedings of the 22nd international
conference on Machine learning, pages 736?
743. ACM.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhao, Q. and Marcus, M. (2012). Exploring deter-
ministic constraints: from a constrained english
pos tagger to an efficient ilp solution to chinese
word segmentation. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1054?1062, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
1414
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1816?1821,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Muli-label Text Categorization with Hidden Components
Li Li Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
li.l@pku.edu.cn, zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
Multi-label text categorization (MTC) is
supervised learning, where a documen-
t may be assigned with multiple categories
(labels) simultaneously. The labels in the
MTC are correlated and the correlation re-
sults in some hidden components, which
represent the ?share? variance of correlat-
ed labels. In this paper, we propose a
method with hidden components for MTC.
The proposed method employs PCA to
capture the hidden components, and incor-
porates them into a joint learning frame-
work to improve the performance. Experi-
ments with real-world data sets and evalu-
ation metrics validate the effectiveness of
the proposed method.
1 Introduction
Many real-world text categorization applications
are multi-label text categorization (Srivastava and
Zane-Ulman, 2005; Katakis et al., 2008; Rubin
et al., 2012; Nam et al., 2013), where a docu-
ments is usually assigned with multiple labels si-
multaneously. For example, as figure 1 shows,
a newspaper article concerning global warming
can be classified into two categories, Environmen-
t, and Science simultaneously. Let X = R
d
be the documents corpus, and Y = {0, 1}
m
be
the label space with m labels. We denote by
{(x
1
, y
1
), (x
2
, y
2
), ..., (x
n
, y
n
)} the training set of
n documents. Each document is denoted by a vec-
tor x
i
= [x
i,1
, x
i,2
, ..., x
i,d
] of d dimensions. The
labeling of the i-th document is denoted by vector
y
i
= [y
i,1
, y
i,2
, ..., y
i,m
], where y
il
is 1 when the
i-th document has the l-th label and 0 otherwise.
The goal is to learn a function f : X ? Y . Gener-
ally, we can assume f consists of m functions, one
for a label.
f = [f
1
, f
2
, ..., f
m
]
Figure 1: A newspaper article concerning global
warming can be classified into two categories, En-
vironment, and Science.
The labels in the MLC are correlated. For ex-
ample, a ?politics? document is likely to be an ?e-
conomic? document simultaneously, but likely not
to be a ?literature? document. According to the
latent variable model (Tabachnick et al., 2001),
the labels with correlation result in some hidden
components, which represent the ?share? variance
of correlated labels. Intuitively, if we can capture
and utilize these hidden components in MTC, the
performance will be improved. To implement this
idea, we propose a multi- label text categorization
method with hidden components, which employ
PCA to capture the hidden components, and then
incorporates these hidden components into a joint
learning framework. Experiments with various da-
ta sets and evaluation metrics validate the values
of our method. The research close to our work is
ML-LOC (Multi-Label learning using LOcal Cor-
relation) in (Huang and Zhou, 2012). The differ-
1816
ences between ours and ML-LOC is that ML-LOC
employs the cluster method to gain the local cor-
relation, but we employ the PCA to obtain the hid-
den code. Meanwhile, ML-LOC uses the linear
programming in learning the local code, but we
employ the gradient descent method since we add
non-linear function to the hidden code.
The rest of this paper is organized as follows.
Section 2 presents the proposed method. We con-
duct experiments to demonstrate the effectiveness
of the proposed method in section 3. Section 4
concludes this paper.
2 Methodology
2.1 Capturing Hidden Component via
Principle Component Analysis
The first step of the proposed method is to capture
hidden components of training instances. Here we
employ Principal component analysis (PCA). This
is because PCA is a well-known statistical tool that
converts a set of observations of possibly correlat-
ed variables into a set of values of linearly uncorre-
lated variables called principle components. These
principle components represent the inner structure
of the correlated variables.
In this paper, we directly employ PCA to con-
vert labels of training instances into their principle
components, and take these principle components
as hidden components of training instances. We
denote by h
i
the hidden components of the i-th in-
stance captured by PCA.
2.2 Joint Learning Framework
We expand the original feature representation of
the instance x
i
by its hidden component code vec-
tor c
i
. For simplicity, we use logistic regression as
the motivating example. Let w
l
denote weights in
the l-th function f
l
, consisting of two parts: 1)w
x
l
is the part involving the instance features. 2) w
c
l
is the part involving the hidden component codes.
Hence f
l
is:
f
l
(x,c) =
1
1 + exp(?x
T
w
x
l
? c
T
w
c
l
)
(1)
where C is the code vectors set of all training in-
stances.
The natural choice of the code vector c is h.
However, when testing an instance, the labeling is
unknown (exactly what we try to predict), conse-
quently we can not capture h with PCA to replace
the code vector c in the prediction function Eq.(1).
Therefore, we assume a linear transformation M
from the training instances to their independent
components, and use Mx as the approximate in-
dependent component. For numerical stability, we
add a non-linear function (e.g., the tanh function)
toMx. This is formulated as follows.
c = tanh(Mx) (2)
Aiming to the discrimination fitting and the in-
dependent components encoding, we optimize the
following optimization problem.
min
W ,C
n
?
i=1
m
?
l=1
`(x
i
, c
i
, y
il
, f
l
) + ?
1
?(f )
+?
2
Z(C) (3)
The first term of Eq.(3) is the loss function. `
is the loss function defined on the training data,
and W denotes all weights in the our model, i.e.,
w
1
, ...,w
l
, ...,w
m
. Since we utilize the logistic re-
gression in our model, the loss function is defined
as follows.
`(x,c, y, f)
= ?ylnf(x,c)? (1? y)ln(1? f(x,c)) (4)
The second term of Eq.(3) ? is to punish the
model complexity, which we use the `
2
regular-
ization term.
?(f ) =
m
?
l=1
||w
l
||
2
. (5)
The third term of Eq.(3) Z is to enforce the code
vector close to the independent component vector.
To obtain the goal, we use the least square error
between the code vector and the independent com-
ponent vector as the third regularized term.
Z(C) =
n
?
i=1
||c
i
? h
i
||
2
. (6)
By substituting the Eq.(5) and Eq.(6) into Eq.(3)
and changing c to tanh(Mx) (Eq.(2)), we obtain
the following optimization problem.
min
W ,M
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f )
+?
1
m
?
l=1
||w
l
||
2
+ ?
2
n
?
i=1
||Mx
i
? h
i
||
2
(7)
1817
2.3 Alternative Optimization method
We solve the optimization problem in Eq.(7) by
the alternative optimization method, which opti-
mize one group of the two parameters with the
other fixed. When the M fixed, the third term of
Eq.(7) is a constant and thus can be ignored, then
Eq.(7) can be rewritten as follows.
min
W
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
)
+?
1
m
?
l=1
||w
l
||
2
(8)
By decomposing Eq.(8) based on the label, the e-
quation Eq.(8) can be simplified to:
min
w
l
n
?
i=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
) + ?
1
||w
l
||
2
(9)
Eq.(9) is the standard logistic regression, which
has many efficient optimization algorithms.
When W fixed, the second term is constan-
t and can be omitted, then Ep.(7) can rewritten
to Eq.(10). We can apply the gradient descen-
t method to optimize this problem.
min
M
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
)
+?
2
n
?
i=1
||Mx
i
? h
i
||
2
(10)
3 Experiments
3.1 Evaluation Metrics
Compared with the single-label classification, the
multi-label setting introduces the additional de-
grees of freedom, so that various multi-label eval-
uation metrics are requisite. We use three differen-
t multi-label evaluation metrics, include the ham-
ming loss evaluation metric.
The hamming loss is defined as the percentage
of the wrong labels to the total number of labels.
Hammingloss =
1
m
|h(x)?y| (11)
where ? denotes the symmetric difference of two
sets, equivalent to XOR operator in Boolean logic.
m denotes the label number.
The multi-label 0/1 loss, also known as subset
accuracy, is the exact match measure as it requires
any predicted set of labels h(x) to match the true
set of labels S exactly. The 0/1 loss is defined as
follows:
0/1loss = I(h(x) 6= y) (12)
Let a
j
and r
j
denote the precision and recall for
the j-th label. The macro-averaged F is a harmon-
ic mean between precision and recall, defined as
follows:
F =
1
m
m
?
i=j
2 ? a
j
? r
j
a
j
+ r
j
(13)
3.2 Datasets
We perform experiments on three MTC data sets:
1) the first data set is slashdot (Read et al., 2011).
The slashdot data set is concerned about science
and technology news categorization, which pre-
dicts multiply labels given article titles and partial
blurbs mined from Slashdot.org. 2) the second da-
ta set is medical (Pestian et al., 2007). This data set
involves the assignment of ICD-9-CM codes to ra-
diology reports. 3) the third data set is tmc2007 (S-
rivastava and Zane-Ulman, 2005). It is concerned
about safety report categorization, which is to la-
bel aviation safety reports with respect to what
types of problems they describe. The characteris-
tics of them are shown in Table 1, where n denotes
the size of the data set, d denotes the dimension of
the document instance, and m denotes the number
of labels.
dataset n d m Lcard
slashdot 3782 1079 22 1.18
medical 978 1449 45 1.245
tmc2007 28596 500 22 2.16
Table 1: Multi-label data sets and associated statis-
tics
The measure label cardinality Lcard, which
is one of the standard measures of ?multi-label-
ness?, defined as follows, introduced in (T-
soumakas and Katakis, 2007).
Lcard(D) =
?
n
i=1
?
m
j=1
y
i
j
n
where D denotes the data set, l
i
j
denotes the j-th
label of the i-th instance in the data set.
1818
3.3 Compared to Baselines
To examine the values of the joint learning frame-
work, we compare our method to two baselines.
The baseline 1 eliminates the PCA, which just
adds an extra set of non-linear features. To im-
plement this baseline, we only need to set ?
2
= 0.
The baseline 2 eliminates the joint learning frame-
work. This baseline captures the hidden compo-
nent codes with PCA, trains a linear regression
model to fit the hidden component codes, and u-
tilizes the outputs of the linear regression model
as features.
For the proposed method, we set ?
1
= 0.001
and ?
2
= 0.1. For the baseline 2, we employ l-
ogistic regression with 0.001 `2 regularization as
the base classifier. Evaluations are done in ten-
fold cross validation. Note that all of them pro-
duce real-valued predictions. A threshold t needs
to be used to determine the final multi-label set y
such that l
j
? y where p
j
? t. We select threshold
t, which makes the Lcard measure of predictions
for the training set is closest to the Lcard mea-
sure of the training set (Read et al., 2011). The
threshold t is determined as follows, where D
t
is
the training set and a multi-label model H
t
pre-
dicts for the training set under threshold t.
t = argmin
t?[0,1]
|Lcard(D
t
)? Lcard(H
t
(D
t
))|
(14)
Table 2 reports our method wins over the base-
lines in terms of different evaluation metrics,
which shows the values of PCA and our join-
t learning framework. The hidden component code
only fits the hidden component in the baseline
method. The hidden component code obtains bal-
ance of fitting hidden component and fitting the
training data in the joint learning framework.
3.4 Compared to Other Methods
We compare the proposed method to BR, C-
C (Read et al., 2011), RAKEL (Tsoumakas and
Vlahavas, 2007) and ML-KNN (Zhang and Zhou,
2007). entropy. ML-kNN is an adaption of kNN
algorithm for multilabel classification. methods.
Binary Revelance (BR) is a simple but effective
method that trains binary classifiers for each label
independently. BR has a low time complexity but
makes an arbitrary assumption that the labels are
independent from each other. CC organizes the
classifiers along a chain and take predictions pro-
duced by the former classifiers as features of the
latter classifiers. ML-kNN uses kNN algorithms
independently for each label with considering pri-
or probabilities. The Label Powerset (LP) method
models independencies among labels by treating
each label combination as a new class. LP con-
sumes too much time, since there are 2
m
label
combinations with m labels. RAndom K labEL
(RAKEL) is an ensemble method of LP. RAKEL
learns several LP models with random subsets of
size k from all labels, and then uses a vote process
to determine the final predictions.
For our proposed method, we employ the set-
up in subsection 3.3. We utilize logistic regression
with 0.001 `2 regularization as the base classifier
for BR, CC and RAKEL. For RAKEL, the num-
ber of ensemble is set to the number of label and
the size of the label subset is set to 3. For MLKN-
N, the number of neighbors used in the k-nearest
neighbor algorithm is set to 10 and the smooth pa-
rameter is set to 1. Evaluations are done in ten-
fold cross validation. We employ the threshold-
selection strategy introduced in subsection 3.3
Table 2 also reports the detailed results in terms
of different evaluation metrics. The mean metric
value and the standard deviation of each method
are listed for each data set. We see our proposed
method shows majorities of wining over the other
state-of-the-art methods nearly at all data sets un-
der hamming loss, 0/1 loss and macro f score. E-
specially, under the macro f score, the advantages
of our proposed method over the other methods are
very clear.
4 CONCLUSION
Many real-world text categorization applications
are multi-label text categorization (MTC), where a
documents is usually assigned with multiple labels
simultaneously. The key challenge of MTC is the
label correlations among labels. In this paper, we
propose a MTC method via hidden components
to capture the label correlations. The proposed
method obtains hidden components via PCA and
incorporates them into a joint learning framework.
Experiments with various data sets and evaluation
metrics validate the effectiveness of the proposed
method.
Acknowledge
We thank anonymous reviewers for their help-
ful comments and suggestions. This research
was partly supported by National High Tech-
1819
hamming?. Lower is better.
Dataset slashdot medical tmc2007
Proposed 0.044? 0.004 0.010? 0.002 0.056? 0.002
Baseline1 0.046? 0.003? 0.010? 0.002 0.056? 0.001
Baseline2 0.047? 0.003? 0.011? 0.001 0.059? 0.001?
BR 0.058? 0.003? 0.010? 0.001 0.060? 0.001?
CC 0.049? 0.003? 0.010? 0.001 0.058? 0.001?
RAKEL 0.039? 0.002? 0.011? 0.002 0.057? 0.001
MLKNN 0.067? 0.003? 0.016? 0.003? 0.070? 0.002?
0/1 loss?. Lower is better.
Dataset slashdot medical tmc2007
Proposed 0.600? 0.042 0.316? 0.071 0.672? 0.010
Baseline1 0.615? 0.034? 0.324? 0.058? 0.672? 0.008
Baseline2 0.669? 0.039? 0.354? 0.062? 0.698? 0.007?
BR 0.803? 0.018? 0.337? 0.063? 0.701? 0.008?
CC 0.657? 0.025? 0.337? 0.064? 0.687? 0.010?
RAKEL 0.686? 0.024? 0.363? 0.064? 0.682? 0.009?
MLKNN 0.776? 0.020? 0.491? 0.083? 0.746? 0.003?
F score?. Larger is better.
Dataset slashdot medical tmc2007
Proposed 0.429? 0.026 0.575? 0.067 0.587? 0.010
Baseline1 0.413? 0.032? 0.547? 0.056? 0.577? 0.011
Baseline2 0.398? 0.032? 0.561? 0.052? 0.506? 0.011?
BR 0.204? 0.011? 0.501? 0.058? 0.453? 0.011?
CC 0.303? 0.022? 0.510? 0.052? 0.505? 0.011?
RAKEL 0.349? 0.023? 0.589? 0.063? 0.555? 0.011?
MLKNN 0.297? 0.031? 0.410? 0.064? 0.431? 0.014?
Table 2: Performance (mean?std.) of our method and baseline in terms of different evaluation metrics.
?/? indicates whether the proposed method is statistically superior/inferior to baseline (pairwise t-test at
5% significance level).
nology Research and Development Program of
China (863 Program) (No.2012AA011101), Na-
tional Natural Science Foundation of China
(No.91024009), Major National Social Science
Fund of China (No. 12&ZD227). The contac-
t author of this paper, according to the meaning
given to this role by Key Laboratory of Computa-
tional Linguistics, Ministry of Education, School
of Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang
References
Sheng-Jun Huang and Zhi-Hua Zhou. 2012. Multi-
label learning by exploiting label correlations local-
ly. In AAAI.
Ioannis Katakis, Grigorios Tsoumakas, and Ioannis
Vlahavas. 2008. Multilabel text classification for
automated tag suggestion. In Proceedings of the
ECML/PKDD.
Jinseok Nam, Jungi Kim, Iryna Gurevych, and Jo-
hannes F?urnkranz. 2013. Large-scale multi-label
text classification-revisiting neural networks. arXiv
preprint arXiv:1312.5419.
John P Pestian, Christopher Brew, Pawe? Matykiewicz,
DJ Hovermale, Neil Johnson, K Bretonnel Cohen,
and W?odzis?aw Duch. 2007. A shared task involv-
ing multi-label classification of clinical free text. In
Proceedings of the Workshop on BioNLP 2007: Bio-
logical, Translational, and Clinical Language Pro-
cessing, pages 97?104. Association for Computa-
tional Linguistics.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2011. Classifier chains for multi-label
classification. Machine learning, 85(3):333?359.
Timothy N Rubin, America Chambers, Padhraic S-
myth, and Mark Steyvers. 2012. Statistical topic
models for multi-label document classification. Ma-
chine Learning, 88(1-2):157?208.
Ashok N Srivastava and Brett Zane-Ulman. 2005. Dis-
covering recurring anomalies in text reports regard-
1820
ing complex space systems. In Aerospace Confer-
ence, 2005 IEEE, pages 3853?3862. IEEE.
Barbara G Tabachnick, Linda S Fidell, et al. 2001. Us-
ing multivariate statistics.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Interna-
tional Journal of Data Warehousing and Mining (I-
JDWM), 3(3):1?13.
Grigorios Tsoumakas and Ioannis Vlahavas. 2007.
Random k-labelsets: An ensemble method for mul-
tilabel classification. Machine Learning: ECML
2007, pages 406?417.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038?2048.
1821
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1881?1890,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Coarse-grained Candidate Generation and Fine-grained Re-ranking for
Chinese Abbreviation Prediction
Longkai Zhang Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn
Abstract
Correctly predicting abbreviations given
the full forms is important in many natu-
ral language processing systems. In this
paper we propose a two-stage method to
find the corresponding abbreviation given
its full form. We first use the contextual
information given a large corpus to get ab-
breviation candidates for each full form
and get a coarse-grained ranking through
graph random walk. This coarse-grained
rank list fixes the search space inside the
top-ranked candidates. Then we use a sim-
ilarity sensitive re-ranking strategy which
can utilize the features of the candidates
to give a fine-grained re-ranking and se-
lect the final result. Our method achieves
good results and outperforms the state-of-
the-art systems. One advantage of our
method is that it only needs weak super-
vision and can get competitive results with
fewer training data. The candidate genera-
tion and coarse-grained ranking is totally
unsupervised. The re-ranking phase can
use a very small amount of training data
to get a reasonably good result.
1 Introduction
Abbreviation Prediction is defined as finding the
meaningful short subsequence of characters given
the original fully expanded form. As an example,
?HMM? is the abbreviation for the correspond-
ing full form ?Hidden Markov Model?. While
the existence of abbreviations is a common lin-
guistic phenomenon, it causes many problems like
spelling variation (Nenadi?c et al., 2002). The dif-
ferent writing manners make it difficult to identify
the terms conveying the same concept, which will
hurt the performance of many applications, such
as information retrieval (IR) systems and machine
translation (MT) systems.
Previous works mainly treat the Chinese ab-
breviation generation task as a sequence labeling
problem, which gives each character a label to in-
dicate whether the given character in the full form
should be kept in the abbreviation or not. These
methods show acceptable results. However they
rely heavily on the character-based features, which
means it needs lots of training data to learn the
weights of these context features. The perfor-
mance is good on some test sets that are similar to
the training data, however, when it moves to an un-
seen context, this method may fail. This is always
true in real application contexts like the social me-
dia where there are tremendous new abbreviations
burst out every day.
A more intuitive way is to find the full-
abbreviation pairs directly from a large text cor-
pus. A good source of texts is the news texts. In
a news text, the full forms are often mentioned
first. Then in the rest of the news its corresponding
abbreviation is mentioned as an alternative. The
co-occurrence of the full form and the abbrevia-
tion makes it easier for us to mine the abbreviation
pairs from the large amount of news texts. There-
fore, given a long full form, we can generate its
abbreviation candidates from the given corpus, in-
stead of doing the character tagging job.
For the abbreviation prediction task, the candi-
date abbreviation must be a sub-sequence of the
given full form. An intuitive way is to select
all the sub-sequences in the corpus as the can-
didates. This will generate large numbers of ir-
relevant candidates. Instead, we use a contextual
graph random walk method, which can utilize the
contextual information through the graph, to select
a coarse grained list of candidates given the full
form. We only select the top-ranked candidates to
reduce the search space. On the other hand, the
candidate generation process can only use limited
contextual information to give a coarse-grained
ranked list of candidates. During generation, can-
1881
didate level features cannot be included. There-
fore we propose a similarity sensitive re-ranking
method to give a fine-grained ranked list. We then
select the final result based on the rank of each
candidate.
The contribution of our work is two folds.
Firstly we propose an improved method for abbre-
viation generation. Compared to previous work,
our method can perform well with less training
data. This is an advantage in the context of so-
cial media. Secondly, we build a new abbreviation
corpus and make it publicly available for future re-
search on this topic.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the abbreviation task. In section 3 we describe
the candidate generation part and in section 4 we
describe the re-ranking part. Experiments are de-
scribed in section 5. We also give a detailed anal-
ysis of the results in section 5. In section 6 related
works are introduced, and the paper is concluded
in the last section.
2 Chinese Abbreviation Prediction
System
Chinese Abbreviation Prediction is the task of
selecting representative characters from the long
full form
1
. Previous works mainly use the se-
quence labeling strategies, which views the full
form as a character sequence and give each char-
acter an extra label ?Keep? or ?Skip? to indicate
whether the current character should be kept in
the abbreviation. An example is shown in Table
1. The sequence labeling method assumes that
the character context information is crucial to de-
cide the keep or skip of a character. However,
we can give many counterexamples. An exam-
ple is ??????(Peking University) and ??
????(Tsinghua University), whose abbrevia-
tions correspond to ???? and ???? respec-
tively. Although sharing a similar character con-
text, the third character ??? is kept in the first case
and is skipped in the second case.
We believe that a better way is to extract these
abbreviation-full pairs from a natural text corpus
where the full form and its abbreviation co-exist.
Therefore we propose a two stage method. The
first stage generates a list of candidates given a
large corpus. To reduce the search space, we adopt
1
Details of the difference between English and Chinese
abbreviation prediction can be found in Zhang et al. (2012).
Full form ? ? ? ?
Status Skip Keep Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Hong Kong University)
graph random walk to give a coarse-grained rank-
ing and select the top-ranked ones as the can-
didates. Then we use a similarity sensitive re-
ranking method to decide the final result. Detailed
description of the two parts is shown in the follow-
ing sections.
3 Candidate Generation through Graph
Random Walk
3.1 Candidate Generation and Graph
Representation
Chinese abbreviations are sub-sequences of the
full form. We use a brute force method to select
all strings in a given news article that is the sub-
sequence of the full form. The brute force method
is not time consuming compared to using more
complex data structures like trie tree, because in
a given news article there are a limited number of
sub-strings which meet the sub-sequence criteria
for abbreviations. When generating abbreviation
candidates for a given full form, we require the
full form should appear in the given news article
at least once. This is a coarse filter to indicate that
the given news article is related to the full form and
therefore the candidates generated are potentially
meaningful.
The main motivation of the candidate genera-
tion stage in our approach is that the full form and
its abbreviation tend to share similar context in a
given corpus. To be more detailed, given a word
context window w, the words that appear in the
context window of the full form tend to be sim-
ilar to those words in the context window of the
abbreviations.
We use a bipartite graph G(V
word
, V
context
, E)
to represent this phenomena. We build bipartite
graphs for each full form individually. For a given
full form v
full
, we first extract all its candidate
abbreviations V
C
. We have two kinds of nodes
in the bipartite graph: the word nodes and the
context nodes. We construct the word nodes as
V
word
= V
C
? {v
full
}, which is the node set of
the full form and all the candidates. We construct
the context nodes V
context
as the words that appear
1882
in a fixed window of V
word
. To reduce the size of
the graph, we make two extra assumptions: 1) We
only consider the nouns and verbs in the context
and 2) context words should appear in the vocab-
ulary for more than a predefined threshold (i.e. 5
times). Because G is bipartite graph, the edges E
only connect word node and context nodes. We
use the number of co-occurrence of the candidate
and the context word as the weight of each edge
and then form the weight matrix W . Details of the
bipartite graph construction algorithm are shown
in Table 2. An example bipartite graph is shown
in figure 1.
Figure 1: An example of the bipartite graph rep-
resentation. The full form is ??????(Hong
Kong University), which is the first node on the
left. The three candidates are ????, ????,
????, which are the nodes on the left. The
context words in this example are ?????(Tsui
Lap-chee, the headmaster of Hong Kong Uni-
versity), ????(Enrollment), ????(Hold), ??
??(Enact), ????(Subway), which are the nodes
on the right. The edge weight is the co-occurrence
of the left word and the right word.
3.2 Coarse-grained Ranking Using Random
Walks
We perform Markov Random Walk on the con-
structed bipartite graph to give a coarse-grained
ranked list of all candidates. In random walk, a
walker starts from the full form source node S
(in later steps, v
i
) and randomly walks to another
node v
j
with a transition probability p
ij
. In ran-
dom walk we assume the walker do the walking n
times and finally stops at a final node. When the
walking is done, we can get the probability of each
node that the walker stops in the end. Because
the destination of each step is selected based on
transition probabilities, the word node that shares
more similar contexts are more likely to be the fi-
nal stop. The random walk method we use is sim-
ilar to those defined in Norris (1998); Zhu et al.
(2003); Sproat et al. (2006); Hassan and Menezes
(2013); Li et al. (2013).
The transition probability p
ij
is calculated us-
ing the weights in the weight matrix W and then
normalized with respect to the source node v
i
with
the formula p
ij
=
w
ij?
l
w
il
. When the graph ran-
dom walk is done, we get a list of coarse-ranked
candidates, each with a confidence score derived
from the context information. By performing the
graph random walk, we reduce the search space
from exponential to the top-ranked ones. Now we
only need to select the final result from the candi-
dates, which we will describe in the next section.
4 Candidate Re-ranking
Although the coarse-grained ranked list can serve
as a basic reference, it can only use limited in-
formation like co-occurrence. We still need a re-
ranking process to decide the final result. The rea-
son is that we cannot get any candidate-specific
features when the candidate is not fully gener-
ated. Features such as the length of a candidate are
proved to be useful to rank the candidates by pre-
vious work. In this section we describe our second
stage for abbreviation generation, which we use a
similarity sensitive re-ranking method to find the
final result.
4.1 Similarity Sensitive Re-ranking
The basic idea behind our similarity sensitive re-
ranking model is that we penalize the mistakes
based on the similarity of the candidate and the
reference. If the model wrongly selects a less sim-
ilar candidate as the result, then we will attach a
large penalty to this mistake. If the model wrongly
chooses a candidate but the candidate is similar to
the reference, we slightly penalize this mistake.
The similarity between a candidate and the ref-
erence is measured through character similarity,
which we will describe later.
1883
Input: the full form v
full
, news corpus U
Output: bipartite graph G(V
word
, V
context
, E)
Candidate vector V
c
= ?, V
context
= ?
for each document d in U
if d contains v
full
add all words w in the window of v
full
into V
context
for each n-gram s in d
if s is a sub-sequence of v
full
add s into V
c
add all word w in the window of s into V
context
end if
end for
end if
end for
V
word
= V
c
? {v
full
}
for each word v
i
in V
word
for each word v
j
in V
context
calculate edge weight in E based on co-occurrence
end for
end for
Return G(V
word
, V
context
, E)
Table 2: Algorithm for constructing bipartite graphs
We first give some notation of the re-ranking
phase.
1. f(x, y) is a scoring function for a given com-
bination of x and y, where x is the original full
form and y is an abbreviation candidate. For a
given full form x
i
with K candidates, we assume
its corresponding K candidates are y
1
i
,y
2
i
,...,y
K
i
.
2. evaluation function s(x, y) is used to mea-
sure the similarity of the candidate to the refer-
ence, where x is the original full form and y is one
abbreviation candidate. We require that s(x, y)
should be in [0, 1] and s(x, y) = 1 if and only if y
is the reference.
One choice for s(x, y) may be the indicator
function. However, indicator function returns zero
for all false candidates. In the abbreviation predic-
tion task, some false candidates are much closer to
the reference than the rest. Considering this, we
use a Longest Common Subsequence(LCS) based
criterion to calculate s(x, y). Suppose the length
of a candidate is a, the length of the reference is b
and the length of their LCS is c, then we can define
precision P and recall R as:
P =
c
a
,
R =
c
b
,
F =
2 ? P ?R
P +R
(1)
It is easy to see that F is a suitable s(x, y).
Therefore we can use the F-score as the value for
s(x, y).
3. ?(x, y) is a feature function which returns a
m dimension feature vector. m is the number of
features in the re-ranking.
4. ~w is a weight vector with dimension m.
~w
T
?(x, y) is the score after re-ranking. The candi-
date with the highest score will be our final result.
Given these notations, we can now describe our
re-ranking algorithm. Suppose we have the train-
ing set X = {x
1
, x
2
, ..., x
n
}. We should find the
weight vector ~w that can minimize the loss func-
tion:
Loss(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
(2)
1884
I(x) is the indicator function. It equals to 1
if and only if x ? 0. I(j) = 1 means that the
candidate which is less ?similar? to the reference
is ranked higher than the reference. Intuitively,
Loss(~w) is the weighted sum of all the wrongly
ranked candidates.
It is difficult to optimize Loss(~w) because
Loss(~w) is discontinuous. We make a relaxation
here
2
:
L(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
?
1
1 + e
?~w
T
(?(x
i
,y
j
i
)??(x
i
,y
1
i
))
)
?
1
2
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
=
1
2
Loss(~w)
(3)
From the equations above we can see that
2L(~w) is the upper bound of our loss function
Loss(~w). Therefore we can optimize L(~w) to ap-
proximate Loss(~w).
We can use optimization methods like gradient
descent to get the ~w that minimize the loss func-
tion. Because L is not convex, it may go into a lo-
cal minimum. In our experiment we held out 10%
data as the develop set and try random initializa-
tion to decide the initial ~w.
4.2 Features for Re-ranking
One advantage of the re-ranking phase is that it
can now use features related to candidates. There-
fore, we can use a variety of features. We list them
as follows.
1. The coarse-grained ranking score from the
graph random walk phase. From the de-
scription of the previous section we know that
this score is the probability a ?walker? ?walk?
from the full form node to the current candi-
date. This is a coarse-grained score because
it can only use the information of words in-
side the window. However, it is still informa-
tive because in the re-ranking phase we can-
not collect this information directly.
2
To prove this we need the following two inequalities: 1)
when x ? 0, I(x) ?
2
1+e
?x
and 2) s(x
i
, y
1
i
) ? s(x
i
, y
j
i
) ?
0.
2. The character uni-grams and bi-grams in
the candidate. This kind of feature cannot
be used in the traditional character tagging
methods.
3. The language model score of the candi-
date. In our experiment, we train a bi-gram
language model using Laplace smoothing on
the Chinese Gigaword Data
3
.
4. The length of the candidate. Intuitively,
abbreviations tend to be short. Therefore
length can be an important feature for the re-
ranking.
5. The degree of ambiguity of the candidate.
We first define the degree of ambiguity d
i
of a
character c
i
as the number of identical words
that contain the character. We then define the
degree of ambiguity of the candidate as the
sum of all d
i
in the candidates. We need a dic-
tionary to extract this feature. We collect all
words in the PKU data of the second Interna-
tional Chinese Word Segmentation Bakeoff
4
.
6. Whether the candidate is in a word dictio-
nary. We use the PKU dictionary in feature
5.
7. Whether all bi-grams are in a word dictio-
nary. We use the PKU dictionary in feature
5.
8. Adjacent Variety(AV) of the candidate. We
define the left AV of the candidate as the
probability that in a corpus the character in
front of the candidate is a character in the
full form. For example if we consider the full
form ??????(Peking University) and the
candidate ????, then the left AV of ????
is the probability that the character preced-
ing ???? is ??? or ??? or ??? or ??? in
a corpus. We can similarly define the right
AV, with respect to characters follow the can-
didate.
The AV feature is very useful because in some
cases a substring of the full form may have a con-
fusingly high frequency. In the example of ???
???(Peking University), an article in the corpus
may mention ??????(Peking University) and
3
http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
4
http://www.sighan.org/bakeoff2005/
1885
??????(Tokyo University) at the same time.
Then the substring ????? may be included in
the candidate generation phase for ??????
with a high frequency. Because the left AV of ??
??? is high, the re-ranker can easily detect this
false candidate.
In practice, all the features need to be scaled in
order to speed up training. There are many ways
to scale features. We use the most intuitive scal-
ing method. For a feature value x, we scale it as
(x?mean)/(max?min). Note that for language
model score and the score of random walk phase,
we scale based on their log value.
5 Experiments
5.1 Dataset and Evaluation metrics
For the dataset, we collect 3210 abbreviation pairs
from the Chinese Gigaword corpus. The abbre-
viation pairs include noun phrases, organization
names and some other types. The Chinese Gi-
gaword corpus contains news texts from the year
1992 to 2007. We only collect those pairs whose
full form and corresponding abbreviation appear
in the same article for at least one time. For full
forms with more than one reasonable reference,
we keep the most frequently used one as its refer-
ence. We use 80% abbreviation pairs as the train-
ing data and the rest as the testing data.
We use the top-K accuracy as the evaluation
metrics. The top-K accuracy is widely used as the
measurement in previous work (Tsuruoka et al.,
2005; Sun et al., 2008, 2009; Zhang et al., 2012). It
measures what percentage of the reference abbre-
viations are found if we take the top k candidate
abbreviations from all the results. In our experi-
ment, we compare the top-5 accuracy with base-
lines. We choose the top-10 candidates from the
graph random walk are considered in re-ranking
phase and the measurement used is top-1 accuracy
because the final aim of the algorithm is to detect
the exact abbreviation, rather than a list of candi-
dates.
5.2 Candidate List
Table 3 shows examples of the candidates. In our
algorithm we further reduce the search space to
only incorporate 10 candidates from the candidate
generation phase.
K Top-K Accuracy
1 6.84%
2 19.35%
3 49.01%
4 63.70%
5 73.60%
Table 4: Top-5 accuracy of the candidate genera-
tion phase
5.3 Comparison with baselines
We first show the top-5 accuracy of the candidate
generation phase Table 4. We can see that, just
like the case of using other feature alone, using
the score of random walk alone is far from enough.
However, the first 5 candidates contain most of the
correct answers. We use the top-5 candidates plus
another 5 candidates in the re-ranking phase.
We choose the character tagging method as the
baseline method. The character tagging strategy
is widely used in the abbreviation generation task
(Tsuruoka et al., 2005; Sun et al., 2008, 2009;
Zhang et al., 2012). We choose the ?SK? labeling
strategy which is used in Sun et al. (2009); Zhang
et al. (2012). The ?SK? labeling strategy gives each
character a label in the character sequence, with
?S? represents ?Skip? and ?K? represents ?Keep?.
Same with Zhang et al. (2012), we use the Con-
ditional Random Fields (CRFs) model in the se-
quence labeling process.
The baseline method mainly uses the charac-
ter context information to generate the candidate
abbreviation. To be fair we use the same fea-
ture set in Sun et al. (2009); Zhang et al. (2012).
One drawback of the sequence labeling method is
that it relies heavily on the character context in
the full form. With the number of new abbrevi-
ations grows rapidly (especially in social media
like Facebook or twitter), it is impossible to let the
model ?remember? all the character contexts. Our
method is different from theirs, we use a more in-
tuitive way which finds the list of candidates di-
rectly from a natural corpus.
Table 5 shows the comparison of the top-5 accu-
racy. We can see that our method outperforms the
baseline methods. The baseline model performs
well when using character features (Column 3).
However, it performs poorly without the charac-
ter features (Column 2). In contrast, without the
character features, our method (Column 4) works
much better than the sequence labeling method.
1886
Full form Reference Generated Candidates #Enum #Now
????? (Depart-
ment of International
Politics)
??? ???,???,????,??
?,??,??,??
30 7
?????? (Non-
nuclear Countries)
??? ??,??,??,???,??
?,???,????,???
?,????,?????,??
?,??,??
62 13
???? (Drug traf-
ficking)
?? ???,???,??,??,?? 14 5
?????????
????? (Yangtze
Joint River Economic
Development Inc.)
???? ??,??,????,???
?,????,????,???
?,????,??????,?
?????,??????,??
????,????????,?
?????,????,??,?
?,??,??,??
16382 20
Table 3: Generated Candidates. #Enum is the number of candidates generated by enumerating all possi-
ble candidates. #Now is the number of candidates generated by our method.
When we add character features, our method (Col-
umn 5) still outperforms the sequence labeling
method.
K CRF-char Our-char CRF Our
1 38.00% 48.60% 53.27% 55.61%
2 38.16% 70.87% 65.89% 73.10%
3 39.41% 81.78% 72.43% 81.96%
4 55.30% 87.54% 78.97% 87.57%
5 62.31% 89.25% 81.78% 89.27%
Table 5: Comparison of the baseline method and
our method. CRF-char (?-? means minus) is the
baseline method without character features. CRF
is the baseline method. Our-char is our method
without character features. We define character
features as the features that consider the charac-
ters from the original full form as their parts.
5.4 Performance with less training data
One advantage of our method is that it only
requires weak supervision. The baseline
method needs plenty of manually collected
full-abbreviation pairs to learn a good model.
In our method, the candidate generation and
coarse-grained ranking is totally unsupervised.
The re-ranking phase needs training instances
to decide the parameters. However we can use
a very small amount of training data to get a
reasonably good model. Figure 2 shows the result
of using different size of training data. We can
see that the performance of the baseline methods
drops rapidly when there are less training data.
In contrast, when using less training data, our
method does not suffer that much.
Figure 2: Top-1 accuracy when changing the size
of training data. For example, ?50%? means ?us-
ing 50% of all the training data?.
5.5 Comparison with previous work
We compare our method with the method in the
previous work DPLVM+GI in Sun et al. (2009),
which outperforms Tsuruoka et al. (2005); Sun
et al. (2008). We also compare our method with
the web-based method CRF+WEB in Zhang et al.
(2012). Because the comparison is performed on
different corpora, we run the two methods on our
data. Table 6 shows the top-1 accuracy. We
can see that our method outperforms the previous
1887
methods.
System Top-K Accuracy
DPLVM+GI 53.29%
CRF+WEB 54.02%
Our method 55.61%
Table 6: Comparison with previous work. The
search results of CRF+WEB is based on March 9,
2014 version of the Baidu search engine.
5.6 Error Analysis
We perform cross-validation to find the errors and
list the two major errors below:
1. Some full forms may correspond to more
than one acceptable abbreviation. In this
case, our method may choose the one that is
indeed used as the full form?s abbreviation in
news texts, but not the same as the standard
reference abbreviations. The reason for this
phenomenon may lie in the fact that the veri-
fication data we use is news text, which tends
to be formal. Therefore when a reference is
often used colloquially, our method may miss
it. We can relieve this by changing the corpus
we use.
2. Our method may provide biased information
when handling location sensitive phrases.
Not only our system, the system of Sun et al.
(2009); Zhang et al. (2012) also shows this
phenomenon. An example is the case of ??
?????? (Democracy League of Hong
Kong). Because most of the news is about
news in mainland China, it is hard for the
model to tell the difference between the ref-
erence ????? and a false candidate ??
??(Democracy League of China).
Another ambiguity is ??????(Tsinghua
University), which has two abbreviations ??
?? and ????. This happens because the
full form itself is ambiguous. Word sense dis-
ambiguation can be performed first to handle
this kind of problem.
6 Related Work
Abbreviation generation has been studied during
recent years. At first, some approaches maintain
a database of abbreviations and their correspond-
ing ?full form? pairs. The major problem of pure
database-building approach is obvious. It is im-
possible to cover all abbreviations, and the build-
ing process is quite laborious. To find these pairs
automatically, a powerful approach is to find the
reference for a full form given the context, which
is referred to as ?abbreviation generation?.
There is research on using heuristic rules
for generating abbreviations Barrett and Grems
(1960); Bourne and Ford (1961); Taghva and
Gilbreth (1999); Park and Byrd (2001); Wren et al.
(2002); Hearst (2003). Most of them achieved
high performance. However, hand-crafted rules
are time consuming to create, and it is not easy to
transfer the knowledge of rules from one language
to another.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed an SVM ap-
proach. Tsuruoka et al. (2005); Sun et al. (2009)
formalized the process of abbreviation generation
as a sequence labeling problem. The drawback of
the sequence labeling strategies is that they rely
heavily on the character features. This kind of
method cannot fit the need for abbreviation gen-
eration in social media texts where the amount of
abbreviations grows fast.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations. Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method only uses a fixed corpus, instead of us-
ing collective information, which varies from time
to time.
Some of the previous work that relate to ab-
breviations focuses on the task of ?abbreviation
disambiguation?, which aims to find the correct
abbreviation-full pairs. In these works, machine
learning approaches are commonly used (Park and
Byrd, 2001; HaCohen-Kerner et al., 2008; Yu
et al., 2006; Ao and Takagi, 2005). We focus on
another aspect. We want to find the abbreviation
1888
given the full form. Besides, Sun et al. (2013) also
works on abbreviation prediction but focuses on
the negative full form problem, which is a little
different from our work.
One related research field is text normalization,
with many outstanding works (Sproat et al., 2001;
Aw et al., 2006; Hassan and Menezes, 2013; Ling
et al., 2013; Yang and Eisenstein, 2013). While
the two tasks share similarities, abbreviation pre-
diction has its identical characteristics, like the
sub-sequence assumption. This results in different
methods to tackle the two different problems.
7 Conclusion
In this paper, we propose a unified framework for
Chinese abbreviation generation. Our approach
contains two stages: candidate generation and
re-ranking. Given a long term, we first gener-
ate a list of abbreviation candidates using the co-
occurrence information. We give a coarse-grained
rank using graph random walk to reduce the search
space. After we get the candidate lists, we can use
the features related to the candidates. We use a
similarity sensitive re-rank method to get the final
abbreviation. Experiments show that our method
outperforms the previous systems.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Aw, A., Zhang, M., Xiao, J., and Su, J. (2006). A
phrase-based statistical model for sms text nor-
malization. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 33?
40. Association for Computational Linguistics.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hassan, H. and Menezes, A. (2013). Social text
normalization using contextual graph random
walks. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1577?
1586, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Li, J., Ott, M., and Cardie, C. (2013). Identify-
ing manipulated offerings on review portals. In
EMNLP, pages 1933?1942.
Ling, W., Dyer, C., Black, A. W., and Trancoso, I.
(2013). Paraphrasing 4 microblog normaliza-
tion. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language
Processing, pages 73?84, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Nenadi?c, G., Spasi?c, I., and Ananiadou, S. (2002).
Automatic acronym acquisition and term varia-
tion management within domain-specific texts.
1889
In Third International Conference on Language
Resources and Evaluation (LREC2002), pages
2155?2162.
Norris, J. R. (1998). Markov chains. Number
2008. Cambridge university press.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Sproat, R., Black, A. W., Chen, S., Kumar, S.,
Ostendorf, M., and Richards, C. (2001). Nor-
malization of non-standard words. Computer
Speech & Language, 15(3):287?333.
Sproat, R., Tao, T., and Zhai, C. (2006). Named
entity transliteration with comparable corpora.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 73?80. Association for
Computational Linguistics.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yang, Y. and Eisenstein, J. (2013). A log-linear
model for unsupervised text normalization. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing,
pages 61?72, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhu, X., Ghahramani, Z., Lafferty, J., et al. (2003).
Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, volume 3,
pages 912?919.
1890
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30?34,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Entity Representation for Entity Disambiguation
Zhengyan He? Shujie Liu? Mu Li? Ming Zhou? Longkai Zhang? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
zhlongk@qq.com wanghf@pku.edu.cn
Abstract
We propose a novel entity disambigua-
tion model, based on Deep Neural Net-
work (DNN). Instead of utilizing simple
similarity measures and their disjoint com-
binations, our method directly optimizes
document and entity representations for a
given similarity measure. Stacked Denois-
ing Auto-encoders are first employed to
learn an initial document representation in
an unsupervised pre-training stage. A su-
pervised fine-tuning stage follows to opti-
mize the representation towards the simi-
larity measure. Experiment results show
that our method achieves state-of-the-art
performance on two public datasets with-
out any manually designed features, even
beating complex collective approaches.
1 Introduction
Entity linking or disambiguation has recently re-
ceived much attention in natural language process-
ing community (Bunescu and Pasca, 2006; Han
et al, 2011; Kataria et al, 2011; Sen, 2012). It is
an essential first step for succeeding sub-tasks in
knowledge base construction (Ji and Grishman,
2011) like populating attribute to entities. Given
a sentence with four mentions, ?The [[Python]] of
[[Delphi]] was a creature with the body of a snake.
This creature dwelled on [[Mount Parnassus]], in
central [[Greece]].? How can we determine that
Python is an earth-dragon in Greece mythology
and not the popular programming language, Del-
phi is not the auto parts supplier, and Mount Par-
nassus is in Greece, not in Colorado?
A most straightforward method is to compare
the context of the mention and the definition of
candidate entities. Previous work has explored
many ways of measuring the relatedness of context
?Corresponding author
d and entity e, such as dot product, cosine similar-
ity, Kullback-Leibler divergence, Jaccard distance,
or more complicated ones (Zheng et al, 2010;
Kulkarni et al, 2009; Hoffart et al, 2011; Bunescu
and Pasca, 2006; Cucerzan, 2007; Zhang et al,
2011). However, these measures are often dupli-
cate or over-specified, because they are disjointly
combined and their atomic nature determines that
they have no internal structure.
Another line of work focuses on collective dis-
ambiguation (Kulkarni et al, 2009; Han et al,
2011; Ratinov et al, 2011; Hoffart et al, 2011).
Ambiguous mentions within the same context are
resolved simultaneously based on the coherence
among decisions. Collective approaches often un-
dergo a non-trivial decision process. In fact, (Rati-
nov et al, 2011) show that even though global ap-
proaches can be improved, local methods based on
only similarity sim(d, e) of context d and entity e
are hard to beat. This somehow reveals the impor-
tance of a good modeling of sim(d, e).
Rather than learning context entity associa-
tion at word level, topic model based approaches
(Kataria et al, 2011; Sen, 2012) can learn it in
the semantic space. However, the one-topic-per-
entity assumption makes it impossible to scale to
large knowledge base, as every entity has a sepa-
rate word distribution P (w|e); besides, the train-
ing objective does not directly correspond with
disambiguation performances.
To overcome disadvantages of previous ap-
proaches, we propose a novel method to learn con-
text entity association enriched with deep architec-
ture. Deep neural networks (Hinton et al, 2006;
Bengio et al, 2007) are built in a hierarchical man-
ner, and allow us to compare context and entity
at some higher level abstraction; while at lower
levels, general concepts are shared across entities,
resulting in compact models. Moreover, to make
our model highly correlated with disambiguation
performance, our method directly optimizes doc-
30
ument and entity representations for a fixed simi-
larity measure. In fact, the underlying representa-
tions for computing similarity measure add inter-
nal structure to the given similarity measure. Fea-
tures are learned leveraging large scale annotation
of Wikipedia, without any manual design efforts.
Furthermore, the learned model is compact com-
pared with topic model based approaches, and can
be trained discriminatively without relying on ex-
pensive sampling strategy. Despite its simplicity,
it beats all complex collective approaches in our
experiments. The learned similarity measure can
be readily incorporated into any existing collective
approaches, which further boosts performance.
2 Learning Representation for
Contextual Document
Given a mention string m with its context docu-
ment d, a list of candidate entities C(m) are gen-
erated form, for each candidate entity ei ? C(m),
we compute a ranking score sim(dm, ei) indicat-
ing how likely m refers to ei. The linking result is
e = argmaxei sim(dm, ei).
Our algorithm consists of two stages. In the pre-
training stage, Stacked Denoising Auto-encoders
are built in an unsupervised layer-wise fashion to
discover general concepts encoding d and e. In the
supervised fine-tuning stage, the entire network
weights are fine-tuned to optimize the similarity
score sim(d, e).
2.1 Greedy Layer-wise Pre-training
Stacked Auto-encoders (Bengio et al, 2007) is
one of the building blocks of deep learning. As-
sume the input is a vector x, an auto-encoder con-
sists of an encoding process h(x) and a decod-
ing process g(h(x)). The goal is to minimize the
reconstruction error L(x, g(h(x))), thus retaining
maximum information. By repeatedly stacking
new auto-encoder on top of previously learned
h(x), stacked auto-encoders are obtained. This
way we learn multiple levels of representation of
input x.
One problem of auto-encoder is that it treats all
words equally, no matter it is a function word or
a content word. Denoising Auto-encoder (DA)
(Vincent et al, 2008) seeks to reconstruct x given
a random corruption x? of x. DA can capture global
structure while ignoring noise as the author shows
in image processing. In our case, we input each
document as a binary bag-of-words vector (Fig.
1). DA will capture general concepts and ignore
noise like function words. By applying masking
noise (randomly mask 1 with 0), the model also
exhibits a fill-in-the-blank property (Vincent et
al., 2010): the missing components must be re-
covered from partial input. Take ?greece? for ex-
ample, the model must learn to predict it with
?python? ?mount?, through some hidden unit. The
hidden unit may somehow express the concept of
Greece mythology.
h(x)
g(h(x))
pythondragon delphicoding ... greecemountsnake phd
reconstruct input
reconstruct randomzero nodenot reconstruct
inactive
active, but mask out
active
Figure 1: DA and reconstruction sampling.
In order to distinguish between a large num-
ber of entities, the vocabulary size must be large
enough. This adds considerable computational
overhead because the reconstruction process in-
volves expensive dense matrix multiplication. Re-
construction sampling keeps the sparse property
of matrix multiplication by reconstructing a small
subset of original input, with no loss of quality of
the learned representation (Dauphin et al, 2011).
2.2 Supervised Fine-tuning
This stage we optimize the learned representation
(?hidden layer n? in Fig. 2) towards the ranking
score sim(d, e), with large scale Wikipedia an-
notation as supervision. We collect hyperlinks in
Wikipedia as our training set {(di, ei,mi)}, where
mi is the mention string for candidate generation.
The network weights below ?hidden layer n? are
initialized with the pre-training stage.
Next, we stack another layer on top of the
learned representation. The whole network is
tuned by the final supervised objective. The reason
to stack another layer on top of the learned rep-
resentation, is to capture problem specific struc-
tures. Denote the encoding of d and e as d? and
e? respectively, after stacking the problem-specific
layer, the representation for d is given as f(d) =
sigmoid(W ? d? + b), where W and b are weight
and bias term respectively. f(e) follows the same
31
encoding process.
The similarity score of (d, e) pair is defined as
the dot product of f(d) and f(e) (Fig. 2):
sim(d, e) = Dot(f(d), f(e)) (1)
<.,.>
f(d) f(e)
hidden layer n
stacked auto-encoder
sim(d,e)
Figure 2: Network structure of fine-tuning stage.
Our goal is to rank the correct entity higher
than the rest candidates relative to the context of
the mention. For each training instance (d, e), we
contrast it with one of its negative candidate pair
(d, e?). This gives the pairwise ranking criterion:
L(d, e) = max{0, 1? sim(d, e) + sim(d, e?)}
(2)
Alternatively, we can contrast with all its candi-
date pairs (d, ei). That is, we raise the similarity
score of true pair sim(d, e) and penalize all the
rest sim(d, ei). The loss function is defined as
negative log of softmax function:
L(d, e) = ? log exp sim(d, e)?
ei?C(m) exp sim(d, ei)
(3)
Finally, we seek to minimize the following train-
ing objective across all training instances:
L =
?
d,e
L(d, e) (4)
The loss function is closely related to con-
trastive estimation (Smith and Eisner, 2005),
which defines where the positive example takes
probability mass from. We find that by penaliz-
ing more negative examples, convergence speed
can be greatly accelerated. In our experiments, the
softmax loss function consistently outperforms
pairwise ranking loss function, which is taken as
our default setting.
However, the softmax training criterion adds
additional computational overhead when per-
forming mini-batch Stochastic Gradient Descent
(SGD). Although we can use a plain SGD (i.e.
mini-batch size is 1), mini-batch SGD is faster to
converge and more stable. Assume the mini-batch
size ism and the number of candidates is n, a total
of m ? n forward-backward passes over the net-
work are performed to compute a similarity ma-
trix (Fig. 3), while pairwise ranking criterion only
needs 2?m. We address this problem by grouping
training pairs with same mentionm into one mini-
batch {(d, ei)|ei ? C(m)}. Observe that if candi-
date entities overlap, they share the same forward-
backward path. Only m + n forward-backward
passes are needed for each mini-batch now.
Python (programming language)PythonidaePython (mythology)
... ...
... ...
... ...
d0
d1 ...dm ... =sim(d,e)
e0 e1 e2 en
Figure 3: Sharing path within mini-batch.
The re-organization of mini-batch is similar
in spirit to Backpropagation Through Structure
(BTS) (Goller and Kuchler, 1996). BTS is a vari-
ant of the general backpropagation algorithm for
structured neural network. In BTS, parent node
is computed with its child nodes at the forward
pass stage; child node receives gradient as the sum
of derivatives from all its parents. Here (Fig. 2),
parent node is the score node sim(d, e) and child
nodes are f(d) and f(e). In Figure 3, each row
shares forward path of f(d) while each column
shares forward path of f(e). At backpropagation
stage, gradient is summed over each row of score
nodes for f(d) and over each column for f(e).
Till now, our input simply consists of bag-of-
words binary vector. We can incorporate any
handcrafted feature f(d, e) as:
sim(d, e) = Dot(f(d), f(e)) + ~?~f(d, e) (5)
In fact, we find that with only Dot(f(d), f(e))
as ranking score, the performance is sufficiently
good. So we leave this as our future work.
32
3 Experiments and Analysis
Training settings: In pre-training stage, input
layer has 100,000 units, all hidden layers have
1,000 units with rectifier functionmax(0, x). Fol-
lowing (Glorot et al, 2011), for the first recon-
struction layer, we use sigmoid activation func-
tion and cross-entropy error function. For higher
reconstruction layers, we use softplus (log(1 +
exp(x))) as activation function and squared loss
as error function. For corruption process, we use a
masking noise probability in {0.1,0.4,0.7} for the
first layer, a Gaussian noise with standard devi-
ation of 0.1 for higher layers. For reconstruction
sampling, we set the reconstruction rate to 0.01. In
fine-tuning stage, the final layer has 200 units with
sigmoid activation function. The learning rate is
set to 1e-3. The mini-batch size is set to 20.
We run all our experiments on a Linux ma-
chine with 72GB memory 6 core Xeon CPU. The
model is implemented in Python with C exten-
sions, numpy configured with Openblas library.
Thanks to reconstruction sampling and refined
mini-batch arrangement, it takes about 1 day to
converge for pre-training and 3 days for fine-
tuning, which is fast given our training set size.
Datasets: We use half of Wikipedia 1 plain text
(?1.5M articles split into sections) for pre-training.
We collect a total of 40M hyperlinks grouped by
name string m for fine-tuning stage. We holdout
a subset of hyperlinks for model selection, and we
find that 3 layers network with a higher masking
noise rate (0.7) always gives best performance.
We select TAC-KBP 2010 (Ji and Grishman,
2011) dataset for non-collective approaches, and
AIDA 2 dataset for collective approaches. For both
datasets, we evaluate the non-NIL queries. The
TAC-KBP and AIDA testb dataset contains 1020
and 4485 non-NIL queries respectively.
For candidate generation, mention-to-entity dic-
tionary is built by mining Wikipedia structures,
following (Cucerzan, 2007). We keep top 30 can-
didates by prominence P (e|m) for speed consid-
eration. The candidate generation recall are 94.0%
and 98.5% for TAC and AIDA respectively.
Analysis: Table 1 shows evaluation results
across several best performing systems. (Han et
al., 2011) is a collective approach, using Person-
alized PageRank to propagate evidence between
1available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
2available at http://www.mpi-inf.mpg.de/yago-naga/aida/
different decisions. To our surprise, our method
with only local evidence even beats several com-
plex collective methods with simple word similar-
ity. This reveals the importance of context model-
ing in semantic space. Collective approaches can
improve performance only when local evidence is
not confident enough. When embedding our sim-
ilarity measure sim(d, e) into (Han et al, 2011),
we achieve the best results on AIDA.
A close error analysis shows some typical er-
rors due to the lack of prominence feature and
name matching feature. Some queries acciden-
tally link to rare candidates and some link to en-
tities with completely different names. We will
add these features as mentioned in Eq. 5 in future.
We will also add NIL-detection module, which is
required by more realistic application scenarios.
A first thought is to construct pseudo-NIL with
Wikipedia annotations and automatically learn the
threshold and feature weight as in (Bunescu and
Pasca, 2006; Kulkarni et al, 2009).
Methods micro
P@1
macro
P@1
TAC 2010 eval
Lcc (2010) (top1, noweb) 79.22 -
Siel 2010 (top2, noweb) 71.57 -
our best 80.97 -
AIDA dataset (collective approaches)
AIDA (2011) 82.29 82.02
Shirakawa et al (2011) 81.40 83.57
Kulkarni et al (2009) 72.87 76.74
wordsim (cosine) 48.38 37.30
Han (2011) +wordsim 78.97 75.77
our best (non-collective) 84.82 83.37
Han (2011) + our best 85.62 83.95
Table 1: Evaluation on TAC and AIDA dataset.
4 Conclusion
We propose a deep learning approach that auto-
matically learns context-entity similarity measure
for entity disambiguation. The intermediate rep-
resentations are learned leveraging large scale an-
notations of Wikipedia, without any manual effort
of designing features. The learned representation
of entity is compact and can scale to very large
knowledge base. Furthermore, experiment reveals
the importance of context modeling in this field.
By incorporating our learned measure into collec-
tive approach, performance is further improved.
33
Acknowledgments
We thank Nan Yang, Jie Liu and Fei Wang for helpful discus-
sions. This research was partly supported by National High
Technology Research and Development Program of China
(863 Program) (No. 2012AA011101), National Natural Sci-
ence Foundation of China (No.91024009) and Major Na-
tional Social Science Fund of China(No. 12&ZD227).
References
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
2007. Greedy layer-wise training of deep networks.
Advances in neural information processing systems,
19:153.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 6, pages 708?716.
Y. Dauphin, X. Glorot, and Y. Bengio. 2011.
Large-scale learning of embeddings with recon-
struction sampling. In Proceedings of the Twenty-
eighth International Conference on Machine Learn-
ing (ICML11).
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In Proceedings of the 28th
International Conference on Machine Learning.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
X. Han, L. Sun, and J. Zhao. 2011. Collective en-
tity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765?774. ACM.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of
named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782?792. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1148?
1158, Portland, Oregon, USA, June. Association for
Computational Linguistics.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hier-
archical topic models. In Proceedings of KDD.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 457?
466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and
Y. Shi. 2010. Lcc approaches to knowledge base
population at tac 2010. In Proc. TAC 2010 Work-
shop.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
P. Sen. 2012. Collective context-aware topic mod-
els for entity disambiguation. In Proceedings of the
21st international conference on World Wide Web,
pages 729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011. Entity
disambiguation based on a probabilistic taxonomy.
Technical report, Technical Report MSR-TR-2011-
125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 354?
362. Association for Computational Linguistics.
P. Vincent, H. Larochelle, Y. Bengio, and P.A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25th international conference on Ma-
chine learning, pages 1096?1103. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371?3408.
W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity
linking with effective acronym expansion, instance
selection and topic modeling. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 483?491, Los Ange-
les, California, June. Association for Computational
Linguistics.
34
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177?182,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving Chinese Word Segmentation on Micro-blog Using Rich
Punctuations
Longkai Zhang Li Li Zhengyan He Houfeng Wang? Ni Sun
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,
wanghf@pku.edu.cn,sunny.forwork@gmail.com
Abstract
Micro-blog is a new kind of medium
which is short and informal. While no
segmented corpus of micro-blogs is avail-
able to train Chinese word segmentation
model, existing Chinese word segmenta-
tion tools cannot perform equally well
as in ordinary news texts. In this pa-
per we present an effective yet simple ap-
proach to Chinese word segmentation of
micro-blog. In our approach, we incor-
porate punctuation information of unla-
beled micro-blog data by introducing char-
acters behind or ahead of punctuations,
for they indicate the beginning or end of
words. Meanwhile a self-training frame-
work to incorporate confident instances is
also used, which prove to be helpful. Ex-
periments on micro-blog data show that
our approach improves performance, espe-
cially in OOV-recall.
1 INTRODUCTION
Micro-blog (also known as tweets in English) is
a new kind of broadcast medium in the form of
blogging. A micro-blog differs from a traditional
blog in that it is typically smaller in size. Further-
more, texts in micro-blogs tend to be informal and
new words occur more frequently. These new fea-
tures of micro-blogs make the Chinese Word Seg-
mentation (CWS) models trained on the source do-
main, such as news corpus, fail to perform equally
well when transferred to texts from micro-blogs.
For example, the most widely used Chinese seg-
menter ?ICTCLAS? yields 0.95 f-score in news
corpus, only gets 0.82 f-score on micro-blog data.
The poor segmentation results will hurt subse-
quent analysis on micro-blog text.
?Corresponding author
Manually labeling the texts of micro-blog is
time consuming. Luckily, punctuations provide
useful information because they are used as indi-
cators of the end of previous sentence and the be-
ginning of the next one, which also indicate the
start and the end of a word. These ?natural bound-
aries? appear so frequently in micro-blog texts that
we can easily make good use of them. TABLE 1
shows some statistics of the news corpus vs. the
micro-blogs. Besides, English letters and digits
are also more than those in news corpus. They
all are natural delimiters of Chinese characters and
we treat them just the same as punctuations.
We propose a method to enlarge the training
corpus by using punctuation information. We
build a semi-supervised learning (SSL) framework
which can iteratively incorporate newly labeled in-
stances from unlabeled micro-blog data during the
training process. We test our method on micro-
blog texts and experiments show good results.
This paper is organized as follows. In section 1
we introduce the problem. Section 2 gives detailed
description of our approach. We show the experi-
ment and analyze the results in section 3. Section
4 gives the related works and in section 5 we con-
clude the whole work.
2 Our method
2.1 Punctuations
Chinese word segmentation problem might be
treated as a character labeling problem which
gives each character a label indicating its position
in one word. To be simple, one can use label ?B?
to indicate a character is the beginning of a word,
and use ?N? to indicate a character is not the be-
ginning of a word. We also use the 2-tag in our
work. Other tag sets like the ?BIES? tag set are not
suiteable because the puctuation information can-
not decide whether a character after punctuation
should be labeled as ?B? or ?S?(word with Single
177
Chinese English Number Punctuation
News 85.7% 0.6% 0.7% 13.0%
micro-blog 66.3% 11.8% 2.6% 19.3%
Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.
character).
Punctuations can serve as implicit labels for the
characters before and after them. The character
right after punctuations must be the first character
of a word, meanwhile the character right before
punctuations must be the last character of a word.
An example is given in TABLE 2.
2.2 Algorithm
Our algorithm ?ADD-N? is shown in TABLE 3.
The initially selected character instances are those
right after punctuations. By definition they are all
labeled with ?B?. In this case, the number of train-
ing instances with label ?B? is increased while the
number with label ?N? remains unchanged. Be-
cause of this, the model trained on this unbal-
anced corpus tends to be biased. This problem can
become even worse when there is inexhaustible
supply of texts from the target domain. We as-
sume that labeled corpus of the source domain can
be treated as a balanced reflection of different la-
bels. Therefore we choose to estimate the bal-
anced point by counting characters labeling ?B?
and ?N? and calculate the ratio which we denote
as ?. We assume the enlarged corpus is also bal-
anced if and only if the ratio of ?B? to ?N? is just
the same to? of the source domain.
Our algorithm uses data from source domain to
make the labels balanced. When enlarging corpus
using characters behind punctuations from texts
in target domain, only characters labeling ?B? are
added. We randomly reuse some characters label-
ing ?N? from labeled data until ratio? is reached.
We do not use characters ahead of punctuations,
because the single-character words ahead of punc-
tuations take the label of ?B? instead of ?N?. In
summary our algorithm tackles the problem by du-
plicating labeled data in source domain. We de-
note our algorithm as ?ADD-N?.
We also use baseline feature templates include
the features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). Our algorithm is not
necessarily limited to a specific tagger. For sim-
plicity and reliability, we use a simple Maximum-
Entropy tagger.
3 Experiment
3.1 Data set
We evaluate our method using the data from
weibo.com, which is the biggest micro-blog ser-
vice in China. We use the API provided by
weibo.com1 to crawl 500,000 micro-blog texts of
weibo.com, which contains 24,243,772 charac-
ters. To keep the experiment tractable, we first ran-
domly choose 50,000 of all the texts as unlabeled
data, which contain 2,420,037 characters. We
manually segment 2038 randomly selected micro-
blogs.We follow the segmentation standard as the
PKU corpus.
In micro-blog texts, the user names and URLs
have fixed format. User names start with ?@?, fol-
lowed by Chinese characters, English letters, num-
bers and ? ?, and terminated when meeting punc-
tuations or blanks. URLs also match fixed pat-
terns, which are shortened using ?http://t.
cn/? plus six random English letters or numbers.
Thus user names and URLs can be pre-processed
separately. We follow this principle in following
experiments.
We use the benchmark datasets provided by the
second International Chinese Word Segmentation
Bakeoff2 as the labeled data. We choose the PKU
data in our experiment because our baseline meth-
ods use the same segmentation standard.
We compare our method with three baseline
methods. The first two are both famous Chinese
word segmentation tools: ICTCLAS3 and Stan-
ford Chinese word segmenter4, which are widely
used in NLP related to word segmentation. Stan-
ford Chinese word segmenter is a CRF-based seg-
mentation tool and its segmentation standard is
chosen as the PKU standard, which is the same
to ours. ICTCLAS, on the other hand, is a HMM-
based Chinese word segmenter. Another baseline
is Li and Sun (2009), which also uses punctua-
tion in their semi-supervised framework. F-score
1http://open.weibo.com/wiki
2http://www.sighan.org/bakeoff2005/
3http://ictclas.org/
4http://nlp.stanford.edu/projects/
chinese-nlp.shtml\#cws
178
? ? ? ? ? ? ? ? ? ? ? ?
B - - - - - B - - - - -
B N B B N B B N B B N B
Table 2: The first line represents the original text. The second line indicates whether each character is
the Beginning of sentence. The third line is the tag sequence using ?BN? tag set.
ADD-N algorithm
Input: labeled data {(xi, yi)li?1}, unlabeled data {xj}l+uj=l+1.
1. Initially, let L = {(xi, yi)li?1} and U = {xj}l+uj=l+1.2. Label instances behind punctuations in U as ?B? and add them into
L.
3. Calculate ?B?, ?N? ratio ? in labeled data.
4. Randomly duplicate characters whose labels are ?N? in L to make
?B?/?N?= ?
5. Repeat:
5.1 Train a classifier f from L using supervised learning.
5.2 Apply f to tag the unlabeled instances in U .
5.3 Add confident instances from U to L.
Table 3: ADD-N algorithm.
is used as the accuracy measure. The recall of
out-of-vocabulary is also taken into consideration,
which measures the ability of the model to cor-
rectly segment out of vocabulary words.
3.2 Main results
Method P R F OOV-R
Stanford 0.861 0.853 0.857 0.639
ICTCLAS 0.812 0.861 0.836 0.602
Li-Sun 0.707 0.820 0.760 0.734
Maxent 0.868 0.844 0.856 0.760
No-punc 0.865 0.829 0.846 0.760
No-balance 0.869 0.877 0.873 0.757
Our method 0.875 0.875 0.875 0.773
Table 4: Segmentation performance with different
methods on the development data.
TABLE 4 summarizes the segmentation results.
In TABLE 4, Li-Sun is the method in Li and
Sun (2009). Maxent only uses the PKU data for
training, with neither punctuation information nor
self-training framework incorporated. The next 4
methods all require a 100 iteration of self-training.
No-punc is the method that only uses self-training
while no punctuation information is added. No-
balance is similar to ADD N. The only difference
between No-balance and ADD-N is that the for-
mer does not balance label ?B? and label ?N?.
The comparison of Maxent and No-punctuation
shows that naively adding confident unlabeled in-
stances does not guarantee to improve perfor-
mance. The writing style and word formation of
the source domain is different from target domain.
When segmenting texts of the target domain using
models trained on source domain, the performance
will be hurt with more false segmented instances
added into the training set.
The comparison of Maxent, No-balance and
ADD-N shows that considering punctuation as
well as self-training does improve performance.
Both the f-score and OOV-recall increase. By
comparing No-balance and ADD-N alone we can
find that we achieve relatively high f-score if we
ignore tag balance issue, while slightly hurt the
OOV-Recall. However, considering it will im-
prove OOV-Recall by about +1.6% and the f-
score +0.2%.
We also experimented on different size of un-
labeled data to evaluate the performance when
adding unlabeled target domain data. TABLE 5
shows different f-scores and OOV-Recalls on dif-
ferent unlabeled data set.
We note that when the number of texts changes
from 0 to 50,000, the f-score and OOV both are
improved. However, when unlabeled data changes
to 200,000, the performance is a bit decreased,
while still better than not using unlabeled data.
This result comes from the fact that the method
?ADD-N? only uses characters behind punctua-
179
Size P R F OOV-R
0 0.864 0.846 0.855 0.754
10000 0.872 0.869 0.871 0.765
50000 0.875 0.875 0.875 0.773
100000 0.874 0.879 0.876 0.772
200000 0.865 0.865 0.865 0.759
Table 5: Segmentation performance with different
size of unlabeled data
tions from target domain. Taking more texts into
consideration means selecting more characters la-
beling ?N? from source domain to simulate those
in target domain. If too many ?N?s are introduced,
the training data will be biased against the true dis-
tribution of target domain.
3.3 Characters ahead of punctuations
In the ?BN? tagging method mentioned above,
we incorporate characters after punctuations from
texts in micro-blog to enlarge training set.We also
try an opposite approach, ?EN? tag, which uses
?E? to represent ?End of word?, and ?N? to rep-
resent ?Not the end of word?. In this contrasting
method, we only use characters just ahead of punc-
tuations. We find that the two methods show sim-
ilar results. Experiment results with ADD-N are
shown in TABLE 6 .
Unlabeled ?BN? tag ?EN? tag
Data size F OOV-R F OOV-R
50000 0.875 0.773 0.870 0.763
Table 6: Comparison of BN and EN.
4 Related Work
Recent studies show that character sequence la-
beling is an effective formulation of Chinese
word segmentation (Low et al, 2005; Zhao et al,
2006a,b; Chen et al, 2006; Xue, 2003). These
supervised methods show good results, however,
are unable to incorporate information from new
domain, where OOV problem is a big challenge
for the research community. On the other hand
unsupervised word segmentation Peng and Schu-
urmans (2001); Goldwater et al (2006); Jin and
Tanaka-Ishii (2006); Feng et al (2004); Maosong
et al (1998) takes advantage of the huge amount
of raw text to solve Chinese word segmentation
problems. However, they usually are less accurate
and more complicated than supervised ones.
Meanwhile semi-supervised methods have been
applied into NLP applications. Bickel et al (2007)
learns a scaling factor from data of source domain
and use the distribution to resemble target do-
main distribution. Wu et al (2009) uses a Domain
adaptive bootstrapping (DAB) framework, which
shows good results on Named Entity Recognition.
Similar semi-supervised applications include Shen
et al (2004); Daume? III and Marcu (2006); Jiang
and Zhai (2007); Weinberger et al (2006). Be-
sides, Sun and Xu (2011) uses a sequence labeling
framework, while unsupervised statistics are used
as discrete features in their model, which prove to
be effective in Chinese word segmentation.
There are previous works using punctuations as
implicit annotations. Riley (1989) uses it in sen-
tence boundary detection. Li and Sun (2009) pro-
posed a compromising solution to by using a clas-
sifier to select the most confident characters. We
do not follow this approach because the initial er-
rors will dramatically harm the performance. In-
stead, we only add the characters after punctua-
tions which are sure to be the beginning of words
(which means labeling ?B?) into our training set.
Sun and Xu (2011) uses punctuation information
as discrete feature in a sequence labeling frame-
work, which shows improvement compared to the
pure sequence labeling approach. Our method
is different from theirs. We use characters after
punctuations directly.
5 Conclusion
In this paper we have presented an effective yet
simple approach to Chinese word segmentation on
micro-blog texts. In our approach, punctuation in-
formation of unlabeled micro-blog data is used,
as well as a self-training framework to incorpo-
rate confident instances. Experiments show that
our approach improves performance, especially in
OOV-recall. Both the punctuation information and
the self-training phase contribute to this improve-
ment.
Acknowledgments
This research was partly supported by Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101), National Natural Science Foun-
dation of China (No.91024009) and Major
National Social Science Fund of China(No.
12&ZD227).
180
References
Bickel, S., Bru?ckner, M., and Scheffer, T. (2007).
Discriminative learning for differing training
and test distributions. In Proceedings of the 24th
international conference on Machine learning,
pages 81?88. ACM.
Chen, W., Zhang, Y., and Isahara, H. (2006). Chi-
nese named entity recognition with conditional
random fields. In 5th SIGHAN Workshop on
Chinese Language Processing, Australia.
Daume? III, H. and Marcu, D. (2006). Domain
adaptation for statistical classifiers. Journal of
Artificial Intelligence Research, 26(1):101?126.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 673?680. Association for Computa-
tional Linguistics.
Jiang, J. and Zhai, C. (2007). Instance weight-
ing for domain adaptation in nlp. In Annual
Meeting-Association For Computational Lin-
guistics, volume 45, page 264.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsuper-
vised segmentation of chinese text by use of
branching entropy. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 428?435. Association for Computational
Linguistics.
Li, Z. and Sun, M. (2009). Punctuation as im-
plicit annotations for chinese word segmenta-
tion. Computational Linguistics, 35(4):505?
512.
Low, J., Ng, H., and Guo, W. (2005). A maximum
entropy approach to chinese word segmenta-
tion. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
Pan, S. and Yang, Q. (2010). A survey on trans-
fer learning. Knowledge and Data Engineering,
IEEE Transactions on, 22(10):1345?1359.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
vances in Intelligent Data Analysis, pages 238?
247.
Riley, M. (1989). Some applications of tree-based
modelling to speech and language. In Pro-
ceedings of the workshop on Speech and Nat-
ural Language, pages 339?352. Association for
Computational Linguistics.
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,
C. (2004). Multi-criteria-based active learning
for named entity recognition. In Proceedings
of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589. Associa-
tion for Computational Linguistics.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Association
for Computational Linguistics.
Weinberger, K., Blitzer, J., and Saul, L. (2006).
Distance metric learning for large margin near-
est neighbor classification. In In NIPS. Citeseer.
Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).
Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009
Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume
3, pages 1523?1532. Association for Computa-
tional Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Zhao, H., Huang, C., and Li, M. (2006a). An im-
proved chinese word segmentation system with
181
conditional random field. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language
Processing, volume 117. Sydney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20,
pages 87?94.
182
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96?102,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
A Unified Framework for Grammar Error Correction
Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
In this paper we describe the PKU system
for the CoNLL-2014 grammar error cor-
rection shared task. We propose a unified
framework for correcting all types of er-
rors. We use unlabeled news texts instead
of large amount of human annotated texts
as training data. Based on these data, a
tri-gram language model is used to cor-
rect the replacement errors while two extra
classification models are trained to correct
errors related to determiners and preposi-
tions. Our system achieves 25.32% in f
0.5
on the original test data and 29.10% on the
revised test data.
1 Introduction
The task of grammar error correction is diffi-
cult yet important. An automatic grammar error
correction system can help second language(L2)
learners improve the quality of their writing. Pre-
vious shared tasks for grammar error correction,
such as the HOO shared task of 2012 (HOO-2012)
and the CoNLL-2013 shared task(CoNLL-2013),
focus on limited types of errors. For example,
HOO-2012 only considers errors related to de-
terminers and prepositions. CoNLL-2013 further
considers errors that are related to noun number,
verb form and subject-object agreement. In the
CoNLL-2014 shared task, all systems should con-
sider all the 28 kinds of errors, including errors
such as spelling errors which cannot be corrected
using a single classifier.
Most of the top-ranked systems in the CoNLL-
2013 shared task(Ng et al., 2013) train individ-
ual classifiers or language models for each kind
of errors independently. Although later systems
such as Wu and Ng (2013); Rozovskaya and Roth
(2013) use Integer Linear Programming (ILP) to
decode a global optimized result, the input scores
for ILP still come from the individual classifica-
tion confidence of each kind of errors. It is hard
to adapt these methods directly into the CoNLL-
2014 shared task. It will be both time-consuming
and impossible to train individual classifiers for all
the 28 kinds of errors.
Besides the classifier and language model
based methods, some systems(Dahlmeier and Ng,
2012a; Yoshimoto et al., 2013; Yuan and Felice,
2013) also use the machine translation approach.
Because there are a limited amount of training
data, this kind of approaches often need to use
other corpora of L2 learners, such as the Cam-
bridge Learner Corpus. Because these corpora use
different annotation criteria, the correction sys-
tems should figure out ways to map the error types
from one corpus to another. Even with these ad-
ditions and transformations, there are still too few
training data available to train a good translation
model.
In contrast, we think the grammar error correc-
tion system should 1) correct most kinds of er-
rors in a unified framework and 2) use as much
unlabeled data as possible instead of using large
amount of human annotated data. To be specific,
our system do not need to train individual clas-
sifiers for each kind of errors, nor do we need
to use manually corrected texts. Following the
observation that a correction can either replace a
wrong word or delete/insert a word, our system
is divided into two parts. Firstly, we use a Lan-
guage Model(LM) to correct errors with respect to
the wrongly used words. The LM only uses the
statistics from a large corpus. All errors related to
wrongly used words can be examined in this uni-
fied model instead of designing individual systems
for each kind of errors. Secondly, we train extra
classifiers for determiner errors and preposition er-
rors. We further consider these two kinds of errors
because many of the deletion and insertion errors
belongs to determiner or preposition errors. The
96
training data of the two classification models also
come from a large unlabeled news corpus there-
fore no human annotation is needed.
Although we try to use a unified framework to
get better performance in the grammar error cor-
rection task, there are still a small portion of errors
we do not consider. The insertion and deletion of
words are not considered if the word is neither a
determiner nor a preposition. Our system is also
incapable of replacing a word sequence into an-
other word sequence. We do not consider these
kinds of errors because we find some of them are
hard to generate correction candidates without fur-
ther understanding of the context, and are not easy
to be corrected even by human beings.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the task. In section 3 we describe our algorithm.
Experiments are described in section 4. We also
give a detailed analysis of the results in section 4.
In section 5 related works are introduced, and the
paper is concluded in the last section.
2 Task Description
The CoNLL-2014 shared task focuses on correct-
ing all errors that are commonly made by L2 learn-
ers of English. The training data released by
the task organizers come from the NUCLE cor-
pus(Dahlmeier et al., 2013). This corpus contains
essays written by L2 learners of English. These
essays are then corrected by English teachers. De-
tails of the CoNLL-2014 shared task can be found
in Ng et al. (2014).
3 System Overview
3.1 Overview
It is time-consuming to train individual models for
each kind of errors. We believe a better way is to
correct errors in a unified framework. We assume
that each word in the sentence may be involved in
some kinds of errors. We generate a list of cor-
rection candidates for each word. Then a Lan-
guage Model (LM) is used to find the most proba-
ble word sequences based on the original sentence
and the correction candidates for each word. An
illustrative example is shown in figure 1.
Because the LM is designed for the replace-
ment errors rather than insertion and deletion er-
rors, we train two extra classifiers for determiners
and prepositions. The determiner model and the
preposition model can improve the performance in
our experiment.
3.2 Correction Candidate Generation
The correction candidate generation phase aims to
generate a list of correction candidates for each
word in the original sentence. We generate cor-
rection candidates based on the following rules:
1. Words with the same stem
2. Similar words based on edit distance
The first rule includes the words with the
same stem as candidates. These candidates
can be used later to correct the errors re-
lated to word form. For example, candidates
for the word ?time? in the original sentence
?This is a timely rain indeed.? may include
?timed?,?time?,?timed?,?times?,?timings?,?timely?,
?timees? and ?timing?, which all have the stem
?time?. The correct candidate ?timely? is also
included in the candidate list and can be detected
through further processing.
The candidate generated by the second rule are
mainly used for spelling correction. For exam-
ple, a such candidate for ?beleive? may be ?belive?
or ?believe?. To generate meaningful candidates
while guarantee accuracy, we require that the can-
didate and the original word should have the same
initial character. By examining the training data
we experimentally find that very few L2 learn-
ers make spelling errors on the initial characters.
For example, they may spell ?believe? as ?belive?.
However, very few of them may spell ?believe? as
?pelieve? or ?delieve?.
In our system, we generate 10 candidates for
each word. To keep the decoding of the best word
sequence controllable, we do not generate candi-
dates for every word in the original sentence. We
only generate the edit distance based candidates
for the following words:
1. Words that never appear in the English giga-
word corpus
1
2. Words that appear in the gigaword corpus but
with frequency below a threshold (we use 10
in the experiment)
Besides, we do not generate candidates for the
words whose POS tags are ?NNP? or ?NNPS?.
1
http://catalog.ldc.upenn.edu/
LDC2003T05
97
Figure 1: Correction of the original sentence ?Thera is no spaces for Tom?. We use red nodes to represent
the original words in the sentence, and use blue nodes below each word to represent the candidate list of
each word. We use arrows to show the final corrected word sequence with the highest probability.
These words are proper nouns. The correction
of this kind of words should depend on more
contextual information. For the stemming tools
we use the snowball stemmer
2
. To generate
candidates based on edit distance, we use the
org.apache.lucene.search.spell.SpellChecker in
Lucene
3
. Note that unlike other context based
spell checkers such as the one in Microsoft Office,
the SpellChecker class in Lucene is actually not
a spell checker. For an input word w, it can only
suggest words that are similar to w given a pre-
defined dictionary. We build the dictionary using
all words collected from the English Gigaword
corpus.
3.3 Language Model for Candidate Selection
After given each word a list of candidates, we can
now find the word sequence which is most likely to
be the correct sentence. The model we use is the
language model. The probability P (s) of a sen-
tence s = w
0
w
1
...w
n?1
is calculated as:
P (s) =
n?1
?
i=0
P (w
i
|w
0
, ..., w
i?1
) (1)
The transition probability P (w
i
|w
0
, ..., w
i?1
)
is calculated based on language model. In
our system we use a tri-gram language
model trained on the gigaword corpus.
2
http://snowball.tartarus.org/
3
https://lucene.apache.org/
Therefore, P (w
i
|w
0
, ..., w
i?1
) is reduced to
P (w
i
|w
i?2
, w
i?1
). We do not use a fixed smooth-
ing method. We just set the probability of an
unseen string to be a positive decimal which is
very close to zero.
The decoding of the word sequence that max-
imize p(s) can be tackled through dynamic
programming using Viterbi algorithm(Forney Jr,
1973). One useful trick is that to multiply
p(w
i
|w
i?2
, w
i?1
) with a coefficient (4 in our sys-
tem) if w
i?2
, w
i?1
and w
i
are all words in the orig-
inal sentence. This is because most of the original
word sequences are correct. If the system needs to
make a correction, the corrected sequence should
have a much higher score than the original one.
We do not generate candidates for determin-
ers and prepositions. Firstly, they are all frequent
words that are excluded by the rules we men-
tioned in this section. Secondly, the determiner
and preposition errors are the main kinds of errors
made by L2 learners. Some of the errors are re-
lated to the wrong deletions or insertions. There-
fore we choose to take special care of determiners
and prepositions to correct all their replacement,
deletion and insertion errors instead of generating
candidates for them in this stage.
3.4 Determiner Correction
After using LM, the spelling errors as well as ordi-
nary word form errors such as noun numbers, verb
98
forms are supposed to be corrected. As we men-
tioned in the introduction, we should now handle
the deletion and insertion errors. We choose to use
special models for determiner and prepositions be-
cause many of the deletion and insertion errors are
related to determiner errors or preposition errors.
Also, these two kinds of errors have been consid-
ered in HOO-2012 and CoNLL2013. Therefore
it?s easier to make meaningful comparison with
previous works. We use Maximum Entropy (ME)
classifiers to correct the determiner and preposi-
tion errors. In this section we consider the deter-
miner errors. The preposition errors will be con-
sidered in the next section. For both of the two
parts, we use the open source tool MaxEnt
4
as the
implementation of ME.
We consider the determiner correction task as a
multi-class classification task. The input instances
for classification are the space between words. We
consider whether the space should keep empty, or
insert ?a? or ?the?. Therefore, 3 labels are con-
sidered to indicate ?a?, ?the? and ?NULL?. We use
??NULL? to denote that the correct space does not
need an article. We leave the clarification between
?a? and ?an? as a post-process by manually de-
signed rules. We do not consider other determiners
such as ?this? or ??these? because further informa-
tion such as the coreference resolution results is
needed.
Instead of considering all spaces in a sen-
tence, some previous works(AEHAN et al., 2006;
Rozovskaya and Roth, 2010; Rozovskaya et al.,
2013) only consider spaces at the beginning of
noun phrases. Compared to these methods, our
system do not need a POS tagger or a phrase chun-
ker (which is sometimes not accurate enough) to
filter the positions. All the operations are done on
the word level. We list the features we use in ta-
ble 1. Note that for 3-grams and 4-grams we do
not use all combinations of characters because it
will generate more sparse features while the per-
formance is not improved.
Because there are limited amount of training
data, we choose to use the English Gigaword cor-
pus to generate training instances instead of us-
ing the training data of CoNLL-2014. Because the
texts in the Gigaword corpus are all news texts,
most of them are well written by native speakers
and are proofread by the editors. Therefore they
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html
1-gram w
?3
, w
?2
, w
?1
, w
1
, w
2
, w
3
2-gram all combinations of w
i
w
j
where
i, j ? {?3,?2,?1, 1, 2, 3}
3-gram w
?3
w
?2
w
?1
,w
?2
w
?1
w
1
,
w
?1
w
1
w
2
, w
1
w
2
w
3
4-gram w
?3
w
?2
w
?1
w
1
,
w
?2
w
?1
w
1
w
2
,w
?1
w
1
w
2
w
3
Table 1: The features used in our system. For a
given blank(space), w
i
means the next ith word
and w
?i
means the previous ith word. For the
example of ?I do not play balls .?, if the current
considered instance is the space between ?play?
and ?balls?, then w
?2
means ?not? and w
1
means
?balls?.
can serve as implicit gold annotations. We gener-
ate the training instances from the sentences in the
Gigaword corpus with the following rules:
1. for each space between words, we treat it as
an instance with label ?NULL?, which means
no article is needed. We use the 3 words be-
fore the space as w
?3
, w
?2
, w
?1
and the 3
words after the space as w
1
, w
2
, w
3
to gener-
ate features. We name this kind of instances
?Space Instance? to indicate we operate on
a space. This kind of training instances can
convey the information that in this context no
article is needed.
2. for each word that is an article, we assume it
as an instance, with the label ?a? or ?the? de-
pending on itself. We use the 3 words before
it as w
?3
, w
?2
, w
?1
and the 3 words after
is as w
1
, w
2
, w
3
. In this case we do not use
the article itself as the context. We name this
kind of instances ?article Instance? to indicate
we operate on an article. This kind of train-
ing instances can convey the information that
in this context a particular article should be
added.
The testing instance are also generated follow-
ing the previously mentioned rules. The decoding
process is as follows. If an instance is a ?space
instance? and is predicted as ?a? or ?the?, we then
add ?a? or ?the? in this space. If an instance is an
?article instance?, the situation is a bit complex. If
it is predicted as another article, we replace it with
the predicted one. If it is predicted as ?NULL?, we
should delete the article to make it a space.
99
To guarantee a certain level of precision, we re-
quire the decoding should only be based on confi-
dent predictions. We use the probability calculated
by the classifier as the confidence score and re-
quire the probability of the considered predictions
should exceed a threshold.
3.5 Preposition Correction
The preposition model is similar to the article
model. We use the same set of features as in ta-
ble 1. The training and testing instance generation
is similar except now we consider prepositions in-
stead of articles. The decoding phase is also iden-
tical to the determiner model.
3.6 Post Processing
The post processing in our system is listed as fol-
lows:
1. Distinguish between ?a? and ?an?. We use
rule based method for this issue.
2. Splitting words. If a word is not in the dic-
tionary but one of its splitting results has a
high frequency, we will split the word into
two words. For example, ?dailylife? is an
out of vocabulary word and the splitting re-
sult ?daily life? is common in English. Then
we split ?dailylife? into ?daily life?.
3. We capitalize the first character of each sen-
tence.
4 Experiment and Analysis
We experiment on the CoNLL-2014 test data. We
evaluate our system based on the M2 scorer which
is provided by the organizers. Details of the M2
scorer can be found in Dahlmeier and Ng (2012b).
We tune the additional parameters like all the
thresholds on the CoNLL-2014 official training
data. We use all the text in the Gigaword corpus to
train the language model. We use 2.5 million sen-
tences in the Gigaword corpus to train the extra
two classifier.
Results of our system are shown in table 2. LM
refers to using language model alone. LM+det
refers to using a determiner classifier after using
a language model. LM+prep refers to using a
preposition classifier after using a language model.
LM+det+preposition refers to using a preposition
classifier after LM+det, which is the method used
in our final system.
Model P R F0.5
LM 29.89% 10.04% 21.42%
LM+det 32.23% 13.64% 25.33%
LM+prep 29.73% 10.04% 21.35%
LM+det+prep(all) 32.21% 13.65% 25.32%
Table 2: The experimental results of our system in
the CoNLL-2014 shared task. The threshold for
determiner model and preposition model is 0.99
and 0.99. Parameters are tuned on the CoNLL-
2014 training data.
Model P R F0.5
LM+det+prep(all) 36.64% 15.96% 29.10%
Table 3: The experimental results of our system
in the CoNLL-2014 shared task on the revised an-
notations. The threshold for determiner model and
preposition model is 0.99 and 0.99. Parameters are
tuned on the CoNLL-2014 training data.
From the results we can see that the main con-
tribution comes from the LM model and deter-
miner model. The preposition model can correct
part of the errors while introduce new errors. The
preposition model may harm the overall perfor-
mance. But considering the fact that the grammar
error correction systems are always used for rec-
ommending errors, we still keep the preposition
model in real applications and suggest the errors
predicted by the preposition model.
One limitation of our system is that we only
use a tri-gram based language model as well as up
to 4-gram features for limited instances. Previous
works(Rozovskaya et al., 2013; Kao et al., 2013)
have shown that other resources like the Google 5-
gram statistics can help improve performance. For
the determiner and preposition models, we exper-
iment on different size of training data, from near
zero to the upper bound of our server?s memory
limit (about 72GB). We find that under this lim-
itation, the performance is still improving when
adding more training instances. We believe the
performance can be further improved.
Scores based on the revised annotations is
shown in table 3.
For the convenience of future meaningful com-
parison, we report the result of our system on the
CoNLL-2013 data set in table 4. We tune the ad-
ditional parameters like all the thresholds on the
CoNLL-2013 official training data. Note that in
CoNLL-2013 the scorer considers F1 score in-
100
Model P R F1
CoNLL13 1st 23.49% 46.45% 31.20 %
CoNLL13 2nd 26.35% 23.80% 25.01 %
LM 18.92% 14.55% 16.45%
LM+det 23.76% 36.15% 28.67%
LM+prep 18.89% 14.55% 16.44%
LM+det+prep 23.74% 36.15% 28.66%
Table 4: The experimental results of our system
on the CoNLL-2013 shared task data. The thresh-
old for determiner model and preposition model
is 0.75 and 0.99. Parameters are tuned on the
CoNLL-2013 training data. CoNLL13 1st is Ro-
zovskaya et al. (2013) and the 2nd is Kao et al.
(2013)
stead of F0.5. Therefore some of the thresholds are
different with the ones in the CoNLL-2014 sys-
tem. Because the CoNLL-2013 shared task only
considers 5 types of errors, it will be much easier
to design components specially for each kind of
errors. Therefore our system is a bit less accurate
than the best system. In this system, we restrict the
candidates to be either noun or verb, and omit the
spell checking model. We also omit some post-
processings like deciding whether a word should
be split into two words, because these kinds of er-
rors are not included.
5 Conclusion
In this paper we describe the PKU system for
the CoNLL-2014 grammar error correction shared
task. We propose a unified framework for correct-
ing all types of errors. A tri-gram language model
is used to correct the replacement errors while two
extra classification models are trained to correct
errors related to determiners and prepositions. Our
system achieves 25.32% in f
0.5
on the original test
data and 29.10% on the revised test data.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117, No.61333018), National High
Technology Research and Development Program
of China (863 Program) (No.2012AA011101)
and Major National Social Science Fund of
China(No.12&ZD227).
References
AEHAN, N., Chodorow, M., and LEACOCK,
C. L. (2006). Detecting errors in english arti-
cle usage by non-native speakers.
Dahlmeier, D. and Ng, H. T. (2012a). A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 568?578. Associa-
tion for Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012b). Better eval-
uation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 568?572. Association for Com-
putational Linguistics.
Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).
Building a large annotated corpus of learner en-
glish: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 22?31.
Forney Jr, G. D. (1973). The viterbi algorithm.
Proceedings of the IEEE, 61(3):268?278.
Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,
Boisson, J., Wu, J.-c., and Chang, J. S. (2013).
Conll-2013 shared task: Grammatical error cor-
rection nthu system description. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 20?25, Sofia, Bulgaria. Association
for Computational Linguistics.
Ng, H. T., Wu, S. M., Briscoe, T., Hadiwinoto,
C., Susanto, R. H., and Bryant, C. (2014). The
conll-2014 shared task on grammatical error
correction. In Proceedings of the Eighteenth
Conference on Computational Natural Lan-
guage Learning: Shared Task (CoNLL-2014
Shared Task), pages 1?12, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and
Tetreault, J. (2013). The conll-2013 shared task
on grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
101
Task, pages 1?12, Sofia, Bulgaria. Association
for Computational Linguistics.
Rozovskaya, A., Chang, K.-W., Sammons, M.,
and Roth, D. (2013). The university of illi-
nois system in the conll-2013 shared task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13?19, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2010). Train-
ing paradigms for correcting errors in grammar
and usage. In Human language technologies:
The 2010 annual conference of the north amer-
ican chapter of the association for computa-
tional linguistics, pages 154?162. Association
for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2013). Joint learn-
ing and inference for grammatical error correc-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 791?802, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Wu, Y. and Ng, H. T. (2013). Grammatical error
correction using integer linear programming. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1456?1465, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-
aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-
machi, M., and Matsumoto, Y. (2013). Naist at
2013 conll grammatical error correction shared
task. CoNLL-2013, 26.
Yuan, Z. and Felice, M. (2013). Constrained gram-
matical error correction using statistical ma-
chine translation. CoNLL-2013, page 52.
102
