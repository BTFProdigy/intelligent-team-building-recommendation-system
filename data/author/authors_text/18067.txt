Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 29?38,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Lexical Tightness and Text Complexity
Michael Flor Beata Beigman Klebanov Kathleen M. Sheehan
Educational Testing Service
Princeton, NJ, 08541, USA
{mflor,bbeigmanklebanov,ksheehan}@ets.org
Abstract
We present a computational notion of Lexical 
Tightness that measures global cohesion of con-
tent words in a text. Lexical tightness represents 
the degree to which a text tends to use words 
that are highly inter-associated in the language. 
We demonstrate the utility of this measure for 
estimating text complexity as measured by US 
school grade level designations of texts. Lexical 
tightness strongly correlates with grade level in 
a collection of expertly rated reading materials. 
Lexical  tightness  captures  aspects  of  prose 
complexity that are not covered by classic read-
ability indexes, especially for literary texts. We 
also present initial findings on the utility of this 
measure for automated estimation of complex-
ity for poetry.
1 Introduction
Adequate estimation of text complexity has a long 
and rich history.  Various readability metrics have 
been designed in the last 100 years (DuBay, 2004). 
Recent work on computational  estimation of text 
complexity for school- and college-level texts in-
cludes (Vajjala and Meurers 2012; Graesser et al, 
2011;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009; Heilman et al, 2006). Several commer-
cial  systems were recently evaluated in the Race 
To The Top competition (Nelson et al,  2012) in 
relation to the US Common Core State Standards 
for instruction (CCSSI, 2010). 
A variety of factors influence text  complexity, 
including vocabulary, sentence structure, academic 
orientation,  narrativity,  cohesion,  etc.  (Hiebert, 
2011)  and  corresponding  features  are  utilized  in 
automated  systems  of  complexity  evaluation
(Vajjala and Meurers, 2012; Graesser et al, 2011; 
Sheehan et al, 2010).
We focus on text complexity levels expressed as 
US school grade level equivalents1. Our interest is 
in  quantifying  the  differences  among  texts  (es-
say-length  reading  passages)  at  different  grade 
levels, for the purposes of automatically evaluating 
text complexity.  The work described in this paper 
is part of an ongoing project that investigates novel 
features indicative of text complexity.
The paper is organized as follows. Section 2.1 
presents our methodology for building word asso-
ciation profiles  for  texts.  Section 2.2 defines  the 
measure of lexical tightness (LT). Section 2.3 de-
scribes the datasets used in this study. Sections 3.1 
and  3.2  present  our  study  of  the  relationship 
between LT and text complexity.  Section 3.3 de-
scribes application to poetry. Section 3.4 evaluates 
an improved measure (LTR). Section 4 reviews re-
lated work.
2 Methodology
2.1 Word-Association Profile
We define WAPT ? a word association profile of a 
text T ? as the distribution of association values for 
all pairs of content words of text T, where the asso-
ciation values are estimated from a very large cor-
pus of texts. In this work, WAP is purely illustrat-
ive, and sets the stage for lexical tightness.
1 For age equivalents of grade levels see 
http://en.wikipedia.org/wiki/Educational_stage 
29
There exists an extensive literature on the use of 
word-association measures for NLP, especially for 
detection  of  collocations  (Pecina,  2010;  Evert, 
2008).  The  use  of  pointwise  mutual  information 
(PMI) with word-space models is noted in (Zhang 
et al, 2012; Baroni and Lenci, 2010; Mitchell and 
Lapata, 2008; Turney, 2001). We begin with PMI, 
and provide a modified measure in later sections.
To obtain comprehensive information about co-
occurrence behavior of words in English, we build 
a  first-order  co-occurrence  word-space  model 
(Turney  and  Pantel,  2010;  Baroni  and  Lenci, 
2010). The model was generated from a corpus of 
texts  of  about  2.5  billion  word  tokens,  counting 
non-directed co-occurrence in  a  paragraph,  using 
no  distance  coefficients  (Bullinaria  and  Levy, 
2007). About 2 billion word tokens come from the 
Gigaword  2003  corpus  (Graff  and  Cieri,  2003). 
Additional 500 million word tokens come from an 
in-house corpus containing texts from the genres of 
fiction and popular science. The matrix of 2.1x2.1 
million  word  types  and  their  co-occurrence  fre-
quencies, as well as single-word frequencies, is ef-
ficiently compressed using the TrendStream tech-
nology (Flor, 2013), resulting in a database file of 
4.7GB.  The  same  toolkit  allows  fast  retrieval  of 
word  probabilities  and  statistical  associations  for 
pairs of words.2 
In this study we use all content word tokens of a 
text.  We use the OpenNLP tagger3 to POS-tag a 
text and only take into account nouns, verbs, ad-
jective and adverbs.  We further  apply a stop-list 
(see Appendix A) to filter out auxiliary verbs.
To illustrate why WAP is an interesting notion, 
consider  this  toy  example:  The  texts  ?The  dog 
barked and wagged its tail? vs. ?Green ideas sleep  
furiously?. Their matrices of pairwise word associ-
ations are presented in Table 1. For the first text, 
all  the  six  content  word  pairs  score  above 
PMI=5.5.  On  the  other  hand,  for  ?Green  ideas 
sleep  furiously?,  all  the  six  content  word  pairs 
score below PMI=2.2. The first text puts together 
words that often go together in English, and this 
might be one of the reasons it seems easier to un-
derstand than the second text.
We use histograms to illustrate word-association 
profiles  for  real  texts,  containing  hundreds  of 
2 The distributional word-space model includes counts for 2.1 
million words and 1279 million word pairs (types). Associ-
ation measures are computed on the fly. 
3 http://opennlp.apache.org  
words.  For  a 60-bin histrogram spanning all  ob-
tained PMI values,  the  lowest  bin contains  pairs 
with PMI??5, the highest bin contains pairs with 
PMI>4.83, while the rest of the bins contain word 
pairs  (a,b)  with  -5<PMI(a,b)?4.83.  Figure  1 
presents  WAP  histograms  for  two  real  text 
samples, one for grade level 3 (age 8-9) and one 
for grade level 11 (age 16-17). We observe that the 
shape of distribution is normal-like. The distribu-
tion of GL3 text is shifted to the right ? it contains 
more highly associated word-pairs than the text of 
GL11.  In  a  separate  study  we  investigated  the 
properties of WAP distribution (Beigman-Kleban-
ov and Flor,  2013).  The normal-like  shape turns 
out to be stable across a variety of texts.
The dog barked and wagged its tail:
dog barked wagged tail
dog 7.02 7.64 5.57
barked 9.18 5.95
wagged 9.45
tail
Green ideas sleep furiously:
green ideas sleep furiously
green 0.44 1.47 2.05
ideas 1.01 0.94
sleep 2.18
furiously
Table 1. Word association matrices (PMI values) for 
two illustrative examples.
-5 -4 -3 -2 -1 0 1 2 3 4 5
0
1
2
3
4
5
6
7
8
9
10
TextGL11 TextGL3 PMI
Pe
rce
nta
ge
 of
 pa
irs
 of
 wo
rd 
tok
en
s
Figure  1.  Word  Association  Profiles  for  two  sample 
texts,  showing 60-bin histograms with smoothed lines 
instead of bars. The last bin of the histogram contains 
all pairs with PMI>4.83, hence the uptick at PMI=5.
30
2.2 Lexical Tightness
In this section we consider how to derive a single 
measure to represent each text for further analyses. 
Given the stable  normal-like  shape of  WAP,  we 
use average (mean) value per text for further in-
vestigations. We experimented with several associ-
ation measures.
Point-wise mutual information is defined as fol-
lows (Church and Hanks, 1990): 
PMI = log2 p ?a ,b ?p ?a? p ?b?
Normalized PMI (Bouma, 2009):
NPMI = 2 2( , )log log ( , )( ) ( )
p a b p a bp a p b
? ?
?? ?? ?
Unlike the standard PMI (Manning and Sch?tze, 
1999), NPMI has the property that its values are 
mostly constrained in the range {-1,1}, it is less in-
fluenced by rare extreme values, which is conveni-
ent  for  summing  values  over  multiple  pairs  of 
words.  Additional  experiments  on  our  data  have 
shown that ignoring negative NPMI values4.  works 
best.  Thus,  we  define  Positive  Normalized  PMI 
(PNPMI) for a pair of words  a and b as follows:
PNPMI(a,b) 
=  NPMI(a,b)  if NPMI(a,b)>0
=  0  if NPMI(a,b)?0
or if database has no data for 
co-occurrence of a and b.5
We define Lexical Tightness (LT) of a text as 
the mean value of PNPMI for all pairs of content-
word tokens in a text. Thus, if a text has N words, 
and after filtering we remain with K content words, 
the total number of pairs is K*(K-1)/2. 
Lexical tightness represents the degree to which 
a text tends to use words that are highly inter-asso-
ciated in the language. We conjecture that lexically 
tight texts (with higher values of LT) are easier to 
read  and  would  thus  correspond  to  lower  grade 
levels.
4 Ignoring negative values is described by Bullinaria and Levy 
(2007), also Mohammad and Hirst (2006).
5In our text collection, the average percentage of word-pairs 
not found in database is 5.5% per text.
2.3 Datasets
Our data consists of two sets of passages. The first 
set consists of 1012 passages (636K words) ? read-
ing materials that were used in various tests in state 
and national assessment  frameworks in the USA. 
Part of this set is taken from Sheehan et al (2007) 
(from testing programs and US state departments 
of education), and part was taken from the Standar-
dized State Test Passages set of the Race To The 
Top (RTT)  competition  (Nelson et  al.,  2012).  A 
distinguishing feature of this dataset is that the ex-
act grade level specification was available for each 
text. Table 2 provides the breakdown by grade and 
genre.  Text length in this set ranged between 27 
and 2848 words, with average 629 words. Average 
text length in the literary subset was 689 words and 
in the informational subset 560 words.
Grade
Level
Genre TotalInf Lit Other
1 2 4 1 7
2 2 4 3 9
3 49 63 10 122
4 54 77 8 139
5 47 48 15 110
6 44 43 6 93
7 39 61 6 106
8 73 66 19 158
9 25 25 3 53
10 29 52 2 83
11 18 25 0 43
12 47 20 22 89
Total 429 488 95 1012
Table 2. Counts of texts by grade level and genre, set #1 
Grade
Band GL
Genre TotalInf Lit Other
2?3 2.5 6 10 4 20
4?5 4.5 16 10 4 30
6?8 7 12 16 13 41
9?10 9.5 12 10 17 39
11+ ' 11.5 8 10 20 38
Total 54 56 58 168
Table  3. Counts of texts by grade band and genre, for 
dataset #2. GL specifies our grade level designation.
The second dataset comprises 168 texts (80.8K 
word  tokens)  from Appendix  B of  the  Common 
Core State Standards (CCSSI, 2010)6, not includ-
6 www.corestandards.org/assets/Appendix_B.pdf 
31
ing  poetry  items.  Exact  grade  level  designations 
are not  available for this set,  rather the texts are 
classified into grade bands, as established by ex-
pert  instructors  (Nelson  et  al.,  2012).  Table  3 
provides the breakdown by grade and genre. Text 
length  in  this  set  ranged  between  99  and  2073 
words,  with  average  481  words.  Average  text 
length in the literary subset was 455 words and in 
the informational subset 373 words.
Our  collection  is  not  very  large  in  terms  of 
typical datasets used in NLP research. However, it 
has two unique facets: grading and genres. Rather 
than having grade-ranges, set #1 has exact grade 
designations  for each text.  Moreover,  these  were 
rated by educational experts and used in state and 
nationwide testing programs. 
Previous research has emphasized the importan-
ce of genre effects for predicting readability and 
complexity (Sheehan et al, 2008) and for text ad-
aptation (Fountas and Pinnell, 2001). For all texts 
in our collection, genre designations (information-
al, literary, or 'other') were provided by expert hu-
man  judges  (we  used  the  designations  that  were 
prepared for the RTT competition,  Nelson et  al., 
2012). The 'other' category included texts that were 
somewhere in between literary and informational 
(e.g. biographies), as well as speeches, schedules, 
and manuals.
3 Results 
3.1 Lexical Tightness and Grade Level
Correlations of lexical tightness with grade level 
are shown in Table 4, for sets 1 and 2, the com-
bined set and for literary and informational subsets.
Our first finding is that lexical tightness has con-
siderable  and  statistically  significant  correlation 
with grade level, in each dataset, in the combined 
dataset  and  for  the  specific  subsets.  Notably the 
correlation  between  lexical  tightness  and  grade 
level is negative. Texts of higher grade levels are 
lexically less tight, as predicted.  
Although in these datasets grade level is mode-
rately correlated with text length, lexical tightness 
remains  considerably and significantly correlated 
with grade level even after removing the influence 
of correlations with text length.
Our second finding is that lexical tightness has a 
stronger correlation with grade level for the subset 
of literary texts (r=-0.610) than for informational 
texts (r=-0.499) in set #1. A similar pattern exists 
for set #2.
Figure 2 shows the average LT for each grade 
level,  for  texts  of  set  #1.  As the grade level  in-
creases,  average lexical tightness values decrease 
consistently, especially for informational and liter-
ary  texts.  There  are  two  'outliers'.  Informational 
texts for grade 12 show a sudden increase in lexic-
al tightness. Also, for genre 'other', grades 9,10,11 
are underepresented (see Table 2).
Subset N Correlation GL&length
Correlation 
GL&LT
Partial 
Correlation 
GL&LT
  Set #1
All 1012 0.362 -0.546 -0.472
Inf 429 0.396 -0.499 -0.404
Lit 488 0.408 -0.610 -0.549
  Set #2 (Common Core)
All 168 0.360 -0.441 -0.373
Inf 54 0.406 -0.313 -0.347
Lit 56 0.251 -0.546 -0.505
  Combined set
All 1180 0.339 -0.528 -0.462
Inf 483 0.386 -0.472 -0.369
Lit 544 0.374 -0.601 -0.545
Table  4.  Correlations  of  grade  level  (GL)  with  text 
length  and  lexical  tightness  (LT).  Partial  correlation 
GL&LT  controls  for  text  length.  All  correlations  are 
significant with p<0.04.
Figure 3 shows the average LT for each grade 
band, for texts of set #2. Here as well, decrease of 
lexical tightness is evident with increase of grade 
3 4 5 6 7 8 9 10 11 12
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 2. Lexical tighness by grade level and genre, 
for texts of grades 3-12 in dataset #1.
32
level. In this small set, informational texts show a 
relatively  smooth  decrease  of  LT,  while  literary 
texts  show a  sharp  decrease  of  LT in  transition 
from grade band 4-5 (4.5) to grade band 6-8 (7). 
Texts labelled as 'other' genre in set #2 are gener-
ally less 'tight' than literary or informational. Also 
for 'other' genre, bands 7-8, 9-10 and 11-12 have 
equal lexical tighness.
3.2 Grade Level and Readability Indexes
We have also calculated readability indexes for 
each passage in sets 1 and 2. We used well known 
readability formulae: Flesch-Kincaid Grade Level 
(FKGL: Kincaid et al, 1975), Flesch Reading Ease 
(FRE:  Flesch,  1948),  Gunning-Fog  Index  (GFI: 
Gunning, 19527), Coleman Liau Index (CLI: Cole-
man and Liau, 1975) and Automated Readability 
Index (ARI: Senter and Smith, 1967). All of them 
are based on measuring the length of words (in let-
ters  or  syllables)  and  length  of  sentences  (mean 
number  of  words).  For  our  collection,  we  also 
computed the average sentence length (avgSL, as 
word count),  average word frequency8 (avgWF ? 
over all  words),  and average word frequency for 
only  content  words  (avgWFCW).  Results  are 
shown in Table 5. 
Word frequency has quite low correlation with 
grade  level  in  both  datasets.  Readability  indexes 
7 Using the modern formula, as referenced at http://en.wikipe-
dia.org/wiki/Fog_Index 
8 For word frequency we use the unigrams data from the 
Google Web1T collection (Brants and Franz, 2006).
have a strong and consistent correlation with grade 
level.  For  dataset  #1,  readability  indexes  have 
much stronger correlation with grade level for in-
formational  texts  (|r| between  0.7  and  0.81)  as 
compared  to  literary  texts  (|r| between 0.53  and 
0.68), and a similar pattern is seen for dataset #2, 
with overall lower correlation.
The correlation of Flesch-Kincaid (FKGL) val-
ues with LT are  r=-0.444 for set #1,  r=-0.499 for 
the informational subset and  r=-0.541 for literary 
subset. The correlation is r=-0.182 in set #2. 
All Inf Lit
                  Set #1
N (texts): 1012 429 488
FKGL 0.705 0.807 0.673
FRE -0.658 -0.797 -0.629
GFI 0.701 0.810 0.673
CLI 0.537 0.722 0.537
ARI 0.670 0.784 0.653
avgSL 0.667 0.705 0.630
avgWF 0.205 0.128 0.249
avgWFCW 0.039 -0.039 0.095
                    Set #2 (Common Core)
N (texts): 168 54 56
FKGL 0.487 0.670 0.312
FRE9 -0.503 -0.586 -0.398
GFI 0.493 0.622 0.356
CLI 0.430 0.457 0.440
ARI 0.458 0.658 0.298
avgSL 0.407 0.701 0.203
avgWF 0.100 0.234 -0.109
avgWFCW 0.156 -0.053 -0.038
Table 5. Correlations of grade level with readability 
formulae and word frequency. All correlations apart 
from the italicized ones are significant with p<0.05. 
Abbreviations are explained in the text.
3.3 Lexical Tightness and Readability Indexes
To  evaluate  the  usefulness  of  LT  in  predicting 
grade level of passages, we estimate, using dataset 
#1, a linear regression model where the grade level 
is a dependent variable and Flesch-Kincaid score 
and lexical tightness are the two independent vari-
ables (features). First, we checked whether regres-
sion model improves over FKGL in the training set 
(#1). Then, we tested the regression model estim-
ated on 1012 texts of set #1, on 168 texts of set #2.
The  results  of  the  regression  model  on  1012 
texts  of  set  #1  (R2=0.565,  F(2,1009)=655.85, 
9 Flesch Reading Ease formula is inversely related to grade 
level, hence the negative correlations.
2.5 4.5 7 9.5 11.5
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 3. Lexical tighness by grade band and genre, 
for texts in dataset #2 (CommonCore).
33
p<0.0001)  indicate  that  the  amount  of  explained 
variance in the grade levels, as measured by the ad-
justed R2 of the model, improved from 0.497 (with 
FKGL alone,  multiple  r=0.705)  to  0.564 (FKGL 
with LT, r=0.752), that is an absolute improvement 
of 6.7%, and a relative improvement of 13.5%.
A separate regression model  was estimated on 
the  informational  texts  of  dataset  #1.  The  result 
(R2=0.664, F(2,426)=420.3, p<0.0001) reveals that 
adjusted  R2 of  the  model  improved  from  0.651 
(with FKGL alone, r=0.807) to 0.663 (FKGL with 
LT,  r=0.815).  Similarly,  a  regression  model  was 
estimated on the literary texts of set #1. The result 
(R2=0.522, F(2,485)=264.6, p<0.0001) reveals that 
adjusted R2 of the model improved from .453 (with 
FKGL alone,  r=0.673) to 0.520 (FKGL with LT, 
r=0.722). We observe that Flesch-Kincaid formula 
works well on informational texts, better than on 
literary  texts;  while  lexical  tightness  correlates 
with grade level in the literary texts better than it 
does in the informational texts. Thus, for informa-
tional texts, adding LT to FKGL provides a small 
(1.2%) but statistically significant improvement for 
predicting  GL.  For  literary  texts,  LT  provides  a 
considerable  improvement  (explaining  additional 
6.3% in the variance).
We use the regression model (FKGL & LT) es-
timated on the 1012 texts of set #1 and test it on 
168 texts of set #2. In dataset #2, FKGL alone cor-
relates with grade level with  r=0.487, and the es-
timated regression equation achieves correlation of 
r=0.574 (the difference between correlation coeffi-
cients  is  statistically  significant10,  p<0.001).  The 
amount of explained variance rises from 23.7% to 
33%,  an  almost  10%  improvement  in  absolute 
scores, and 39% relative improvement over FKGL 
readability index alone.
3.4 Analyzing Poetry
Since poetry is often included in school curricula, 
automated estimation of poem complexity can be 
useful. Poetry is notoriously hard to analyze com-
putationally. Many poems do not adhere to stand-
ard  punctuation  conventions,  have  peculiar  sen-
tence structure  (if  sentence boundaries are  indic-
ated at all). However, poems can be tackled with 
bag-of-words approaches. 
We have collected 66 poems from Appendix B 
of  the  Common  Core  State  Standards  (CCSSI, 
10Non-independent correlations test, McNemar (1955), p.148.
2010). Just as other materials from that source, the 
poems  are  classified  into  grade  bands,  as  estab-
lished by expert instructors. Table 6 provides the 
breakdown by grade band. Text length in this set 
ranges between 21 and 1100 words, the average is 
182, total word count is 12,030.
Grade Band GL N (texts)
K-1 1 12
2?3 2.5 15
4?5 4.5 9
6?8 7 11
9?10 9.5 7
11+ ' 11.5 12
Total 66
Table 6. Counts of poems by grade band, 
from Common Core Appendix B. 
GL specifies our grade level designation.
We computed lexical tightness for all 66 poems 
using the same procedure as for the two larger text 
collections. For computing correlations, texts from 
each grade band where assigned grade level as lis-
ted in Table 6. For the poetry dataset, LT has rather 
low  correlation  with  grade  level,  r=-0.271 
(p<0.002).  Text  length  correlation  with  GL  is 
r=0.218  (p<0.04).  Correlation  of  LT  and  text 
length is  r=-0.261 (p<0.02). Partial correlation of 
LT and GL, controlling for text length, is r=-0.227 
and only almost significant (p=0.069). In this data-
set,  the  correlation  of  Flesch-Kincaid  index 
(FKGL) with GL is r=0.291 (p<0.003) and Flesch 
Reading Ease (FRE)  has  a  stronger  correlation,  
r=-0.335 (p<0.003).
On examining some of the poems, we noted that 
the LT measure does not assign enough importance 
to recurrence of words within a text. For example, 
PNPMI(voice,  voice)  is  0.208,  while  the  ceiling 
value is 1.0. We modify the LT measure in the fol-
lowing way. Revised Association Score (RAS) for 
two words a and b:
=1.0   if a=b (token repetition)
RAS(a,b) =0.9  if a and b are inflectional variants of same lemma
= PNPMI(a,b)  otherwise
Revised Lexical Tightness (LTR) for  a text  is 
average of RAS scores for all accepted word pairs 
in the text (same filtering as before).
34
For the set of 66 poems, LTR moderately correl-
ates with grade level r=-0.353 (p<0.002). LTR cor-
relates  with  text  length  r=0.28  (p<0.02).  Partial 
correlation  of  LTR and  GL,  controlling  for  text 
length,  is  r=-0.312 (p<0.012).  This  suggests  that 
the revised measure captures some aspect of com-
plexity of the poems. 
We  re-estimated  the  regression  model,  using 
FRE readability and LTR, on all 1012 texts of set 
#1. We then applied this model  for prediction of 
grade levels  in  the  set  of  66  poems.  The model 
achieves  a  solid  correlation  with  grade  level, 
r=0.447 (p<0.0001). 
3.5 Revisiting Prose
We revisit the analysis of our two main datasets, 
set #1 and #2, using the revised lexical tightness 
measure  LTR.  Table  7  presents  correlations  of 
grade level with LT and LTR measures. Evidently, 
in each case LTR achieves better correlations. 
Subset N Correlation GL&LT
Correlation 
GL&LTR
  Set #1
All 1012 -0.546 -0.605
Inf 429 -0.499 -0.561
Lit 488 -0.610 -0.659
  Set #2 (Common Core)
All 168 -0.441 -0.492
Inf 54 -0.310 -0.336
Lit 56 -0.546 -0.662
  Combined set
All 1180 -0.528 -0.587
Inf 483 -0.472 -0.531
Lit 544 -0.601 -0.655
Table 7. Pearson correlations of grade level (GL) with 
lexical tightness (LT) and revised lexical tightness 
(LTR). All correlations are significant with p<0.04.
We re-estimated a linear regression model using 
the grade level as a dependent variable and Flesch-
Kincaid score (FKGL) and LTR as the two inde-
pendent variables. The results of regression model 
on  1012  texts  of  dataset  #1,  R2=0.583, 
F(2,1009)=706.07,  p<0.0001,  indicate  that  the 
amount of explained variance in the grade levels, 
as measured by the adjusted R2 of the model, im-
proved from 0.497 (with FKGL alone, r=0.705) to 
0.582 (FKGL with LTR, r=0.764), that is absolute 
improvement of 8.5%. For comparison, the regres-
sion model  with LT explained 0.564 of the vari-
ance, with 6.7% improvement over FKGL alone.
We re-estimated separate regression models for 
informational and literary subsets of set #1. For in-
formational  texts,  the  model  has  R2=0.667, 
F(2,426)=426.8,  p<0.0001,  R2 improving  from 
0.651 (with FKGL alone,  r=0.807) to adjusted R2 
0.666  (FKGL  with  LTR,  r=0.817).  Regression 
model with LT brought an improvement of 1.2%, 
the model with LTR provides 1.5%.
A regression model was estimated on the literary 
texts  of  dataset  #1.  The  result  (R2=0.560, 
F(2,485)=308.5, p<0.0001) reveals that adjusted R2 
of the  model  rose from .453 (with FKGL alone, 
r=0.673) to 0.558 (FKGL with LT,  r=0.748), that 
is 10.5% absolute improvement.  For comparison, 
LT brought 6.3% improvement. As with the origin-
al LT measure, LTR provides the bulk of improve-
ment for evaluation of literary texts.
The  regression  model  (FKGL  with  LTR), 
estimated on all 1012 texts of set #1, is tested on 
168  texts  of  set  #2.  In  set  #2,  FKGL  alone 
correlates with grade level with  r=0.487, and the 
prediction formula achieves correlation of r=0.585 
(the difference between correlation coefficients is 
statistically significant,  p<0.001).  The amount  of 
explained variance rises from 23.7% to 34.3%, that 
is 10.6% absolute improvement. Even better result 
of predicting grade level in set #2 is achieved using 
a  regression  model  of  Flesch  Readability  Ease 
(FRE) and LTR, estimated on all 1012 texts of set 
#1.  This  model  achieves  correlation  of r=0.616 
(p<0.0001) on the 168 texts of set #2, explaining 
37.9% of the variance. 
For  complexity  estimation,  in  both  proze  and 
poetry, LTR is more effective than simple LT.
4 Related Work 
Traditional readability formulae use a small num-
ber of surface features,  such as the average sen-
tence length (a proxy for syntactic complexity) and 
the average word length in syllables or characters 
(a  proxy to  vocabulary difficulty).  Such features 
are considered linguistically shallow, but they are 
surprisingly  effective  and  are  still  widely  used 
(DuBay, 2004;  ?tajner et al, 2012). The formulae 
or their features are incorporated in modern read-
ability classification systems (Vajjala and Meurers, 
2012;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009).
Developments  in  computational  linguistics  en-
abled inclusion of multiple features for capturing 
35
various  manifestations  of  text-related  readability. 
Peterson and Ostendorf (2009) compute a variety 
of features: vocabulary/lexical (including the clas-
sic 'syllables per word'), parse features, including 
average parse-tree height, noun-phrase count, verb-
phrase  count  and  average  count  of  subordinated 
clauses. They use machine learning to train classi-
fiers  for  direct  prediction of  grade level.  Vajjala 
and  Meurers  (2012)  also  use  machine  learning, 
with a wide variety of features, including classic 
features,  parse  features,  and  features  motivated 
from studies on second language acquisition, such 
as Lexical  Density and Type-Token Ratio.  Word 
frequency and its derivations, such as proportion of 
rare words, are utilized in many models of com-
plexity (Graesser et al, 2011; Sheehan et al 2010; 
Stenner et al, 2006; Collins-Thompson and Callan, 
2004).
Inspired by psycholinguistic research, two sys-
tems have explicitly set to measure textual cohe-
sion for estimations of readability and complexity: 
Coh-Metrix  (Graesser  et  al.,  2011)  and  Sour-
ceRater (Sheehan et al, 2010). One notion of cohe-
sion involved in those two systems is lexical cohe-
sion ? the amount of lexically/semantically related 
words in a text. Some amount of local lexical cohe-
sion can be measured via stem overlap of adjacent 
sentences, with averaging of such metric per text 
(McNamara et al, 2010). However, Sheehan et al 
(submitted) demonstrated that such measure is not 
well correlated with grade levels.
Perhaps closest to our present study is work re-
ported in Foltz et al (1998) and McNamara et al 
(2010). These studies used Latent Semantic Ana-
lysis,  which  reflects  second  order  co-occurrence 
associative relations, to characterize levels of lex-
ical similarity for pairs of adjacent sentences with-
in  paragraphs,  and  for  all  possible  pairs  of  sen-
tences  within  paragraphs.  McNamara  et  al.  have 
shown success in distinguishing lower and higher 
cohesion versions of the same text,  but  have not 
shown  whether  that  approach  systematically  ap-
plies for different texts and across grade levels.
Our study is a first demonstration that a measure 
of  lexical  cohesion  based  on  word-associations, 
and computed globally for the whole text, is an in-
dicative  feature  that  varies  systematically  across 
grade levels.
In the theoretical tradition, our work is closest in 
spirit to Michael Hoey?s theory of lexical priming 
(Hoey, 2005, 1991), positing that users of language 
internalize patterns of word co-occurrence and use 
them in reading, as well as when creating their own 
texts. We suggest that such patterns become richer 
with age and education, beginning with the most 
tight patterns at early age.
5 Conclusions 
In  this  paper  we  defined  a  novel  computational 
measure, lexical tightness. It represents the degree 
to which a text tends to use words that are highly 
inter-associated  in  the  language.  We  interpret 
lexical tightness as a measure of intra-text global 
cohesion.
This  study  presented  the  relationship  between 
lexical  tightness  and  text  complexity,  using  two 
datasets of reading materials (1180 texts in total), 
with  expert-assigned  grade  levels.  Lexical  tight-
ness has a significant correlation with grade levels: 
about  -0.6  overall.  The  correlation  is  negative: 
texts for lower grades are lexically tight, they use a 
higher  proportion  of  mildly  and  strongly  inter-
associated words; texts for higher grades are less 
tight, they use a lesser amount of inter-associated 
words.  The  correlation  of  lexical  tightness  with 
grade level is stronger for texts of the literary genre 
(fiction and stories) than for text belonging to in-
formational genre (expositional).
While lexical tightness is moderately correlated 
with  readability  indexes,  it  also  captures  some 
aspects of prose complexity that are not covered by 
classic  readability  indexes,  especially for  literary 
texts.  Regression analyses  on a  training set  have 
shown  that  lexical  tightness  adds  between  6.7% 
and 8.5% of explained grade level variance on top 
of  the  best  readability  formula.  The  utility  of 
lexical  tightness  was  confirmed  by  testing  the 
regression formula on a held out set of texts. 
Lexical  tightness  is  also moderately correlated 
with grade level (-0.353) in a small set of poems. 
In the same set,  Flesch Reading Ease readability 
formula  correlates  with  grade  level  at  -0.335.  A 
regression  model  using  that  formula  and  lexical 
tightness achieves correlation of  0.447 with grade 
level.  Thus we have shown that  lexical  tightness 
has good potential for analysis of poetry.
In future work, we intend to a) evaluate on lar-
ger datasets, and b) integrate lexical tightness with 
other  features  used  for  estimation  of  readability. 
We also intend to use this or a related measure for 
evaluation of writing quality.
36
References 
Baroni M. and Lenci A. 2010. Distributional Memory: 
A General Framework for Corpus-Based Semantics. 
Computational Linguistics, 36(4):673-721.
Beigman-Klebanov  B.  and  Flor  M.  2013.  Word 
Association  Profiles  and  their  Use  for  Automated 
Scoring of Essays. To appear in  Proceedings of the  
51th  Annual  Meeting  of  the  Association  for  
Computational Linguistics, ACL 2013.
Bouma  G.  2009.  Normalized  (Pointwise)  Mutual 
Information in Collocation Extraction. In:  Chiarcos, 
Eckart  de  Castilho  &  Stede  (eds), From  Form  to  
Meaning:  Processing  Texts  Automatically,  
Proceedings of the Biennial GSCL Conference 2009, 
31?40, Gunter Narr Verlag: T?bingen.
Brants T. and Franz A. 2006. ?Web 1T 5-gram Version 
1?.  LDC2006T13.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Bullinaria  J.  and  Levy  J.  2007.  Extracting  semantic 
representations from word co-occurrence statistics: A 
computational  study.  Behavior  Research  Methods, 
39:510?526.
Church K. and Hanks P. 1990. Word association norms, 
mutual information and lexicography. Computational  
Linguistics, 16(1):22?29.
Coleman,  M.  and  Liau,  T.  L.  1975.  A  computer 
readability  formula  designed  for  machine  scoring. 
Journal of Applied Psychology, 60:283-284.
Collins-Thompson K. and Callan J. 2004. A language 
modeling approach  to  predicting reading  difficulty. 
Proceedings of HLT / NAACL 2004, Boston, USA.
Common Core State Standards Initiative (CCSSI) 2010. 
Common core state standards for English language 
arts & literacy in history/social studies, science and 
technical subjects. Washington, DC: CCSSO & 
National Governors Association. 
http://www.corestandards.org/ELA-Literacy    
DuBay W.H. 2004. The principles of readability. Impact 
Information:  Costa Mesa,  CA.  http://www.impact-
information.com/impactinfo/readability02.pdf    
Evert S. 2008. Corpora and collocations. In A. L?deling 
and  M.  Kyt?  (eds.),  Corpus  Linguistics:  An  
International  Handbook,  article  58.  Mouton  de 
Gruyter: Berlin.
Flesch R. 1948. A new readability yardstick. Journal of  
Applied Psychology, 32:221-233.
Flor M. 2013. A fast and flexible architecture for very 
large word n-gram datasets. Natural Language 
Engineering, 19(1):61-93.
Foltz P.W., Kintsch W., and Landauer T.K. 1998. The 
measurement of textual coherence with Latent 
Semantic Analysis. Discourse Processes, 25:285-
307.
Fountas I. and Pinnell G.S. 2001. Guiding Readers and 
Writers, Grades 3?6. Heinemann, Portsmouth, NH.
Graesser, A.C., McNamara, D.S., and Kulikowich, J.M. 
Coh-Metrix: Providing Multilevel Analyses of Text 
Characteristics.  Educational  Researcher,  40(5): 
223?234.
Graff,  D.  and  Cieri,  C.  2003.  English  Gigaword.  
LDC2003T05.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Gunning  R.  1952.  The  technique  of  clear  writing. 
McGraw-Hill: New York.
Heilman,  M.,  Collins-Thompson,  K.,  Callan,  J.  and 
Eskenazi,  M.  2006.  Classroom  success  of  an 
intelligent  tutoring  system  for  lexical  practice  and 
reading comprehension. In  Proceedings of the Ninth  
International  Conference  on  Spoken  Language  
Processing, Pittsburgh, PA.
Hiebert,  E.H.  2011.  Using  multiple  sources  of  
information in establishing text complexity. Reading 
Research Report 11.03. TextProject Inc., Santa Cruz, 
CA.
Hoey  M.  1991.  Patterns  of  Lexis  in  Text.  Oxford 
University Press.
Hoey M. 2005. Lexical Priming: A new theory of words  
and language. Routledge, London.
Kincaid  J.P.,  Fishburne  R.P.  Jr,  Rogers  R.L.,  and 
Chissom B.S.  1975.  Derivation  of  new readability  
formulas   for  Navy  enlisted  personnel.  Research 
Branch  Report  8-75,  Millington,  TN:  Naval 
Technical  Training,  U.S.  Naval  Air  Station, 
Memphis, TN.
Manning,  C.  and  Sch?tze  H.  1999.  Foundations  of  
Statistical Natural Language Processing. MIT Press, 
Cambridge, MA.
McNamara,  D.S.,  Louwerse,  M.M.,  McCarthy,  P.M. 
and  Graesser  A.C.  2010.  Coh-metrix:  Capturing 
linguistic features of cohesion.  Discourse Processes, 
47:292-330.
McNemar,  Q.  1955.  Psychological  Statistics.  New 
York, John Wiley & Sons.
Mitchell J. and Lapata M.  2008. Vector-based models 
of semantic composition. In Proceedings of the 46th 
Annual  Meeting  of  the  Association  for  
Computational Linguistics, 236?244, Columbus, OH.
Mohammad  S.  and  Hirst  G.  2006.  Distributional 
Measures  of  Concept-Distance:  A  Task-oriented 
Evaluation. In  Proceedings of the 2006 Conference  
on  Empirical  Methods  in  Natural  Language  
Processing (EMNLP 2006), 35?43.
Nelson  J.,  Perfetti  C.,  Liben  D.,  and Liben  M. 2012. 
Measures of Text Difficulty: Testing their Predictive 
Value  for  Grade  Levels  and  Student  Performance. 
Student  Achievement  Partners.  Available  from 
http://www.ccsso.org/Documents/2012/Measures
%20ofText%20Difficulty_final.2012.pd  f   
Pecina  P.  2010.  Lexical  association  measures  and 
collocation  extraction.  Language  Resources  & 
Evaluation, 44:137?158.
37
Petersen  S.E.  and  Ostendorf  M.  2009.  A  machine 
learning  approach  to  reading  level  assessment. 
Computer Speech and Language, 23: 89?109.
Senter  R.J.  and  Smith  E.A.  1967.  Automated 
Readability Index. Report AMRL-TR-6620. Wright-
Patterson Air Force Base, USA.
Sheehan K.M.,  Kostin I.,  Napolitano D.,  and Flor  M. 
TextEvaluator:  Helping  Teachers  and  Test 
Developers  Select  Texts for  Use in Instruction and 
Assessment.  Submitted  to  The  Elementary  School  
Journal (Special Issue: Text Complexity).  
Sheehan K.M., Kostin I., Futagi Y., and Flor M. 2010. 
Generating automated text complexity classifications 
that  are  aligned  with  targeted  text  complexity 
standards. (ETS RR-10-28). ETS, Princeton, NJ.
Sheehan K.M., Kostin I., and Futagi Y. 2008. When do 
standard  approaches  for  measuring  vocabulary 
difficulty,  syntactic  complexity  and  referential 
cohesion yield biased estimates of text difficulty? In 
B.C.  Love,  K.  McRae,  &  V.M.  Sloutsky  (eds.), 
Proceedings  of  the 30th Annual  Conference  of  the  
Cognitive Science Society, Washington DC.
Sheehan  K.M.,  Kostin  I.,  and  Futagi  Y.  2007. 
SourceFinder:  A  construct-driven  approach  for 
locating  appropriately  targeted  reading 
comprehension  source  texts.  In  Proceedings  of  the  
2007  workshop  of  the  International  Speech  
Communication Association,  Special  Interest  Group 
on Speech and Language Technology in Education, 
Farmington, PA.
?tajner S., Evans R., Or?san C., and Mitkov R. 2012. 
What  Can  Readability  Measures  Really  Tell  Us 
About Text Complexity? In proceedings of workshop 
on   Natural  Language  Processing  for  Improving  
Textual Accessibility (NLP4ITA 2012), 14-22.
Stenner A.J.,  Burdick H., Sanford E., and Burdick D. 
2006.  How  accurate  are  Lexile  text  measures? 
Journal of Applied Measurement, 7(3):307-322.
Turney  P.D.  2001.  Mining  the  Web  for  Synonyms: 
PMI-IR versus  LSA on TOEFL.  In  proceedings  of 
European  Conference  on  Machine  Learning,  491?
502, Freiburg, Germany.
Turney  P.D.  and  Pantel  P.  2010.  From Frequency  to 
Meaning:  Vector  Space  Models  of  Semantics. 
Journal  of  Artificial  Intelligence  Research,  37:141-
188.
Vajjala  S.  and  Meurers  D.  2012.  On  Improving  the 
Accuracy of Readability Classification using Insights 
from Second Language Acquisition. In  proceedings 
of  The 7th Workshop on the Innovative Use of NLP  
for  Building  Educational  Applications,  (BEA-7), 
163?173, ACL.
Zhang  Z.,  Gentile  A.L.,  Ciravegna  F.  2012.  Recent 
advances in methods of lexical semantic relatedness 
?  a  survey.  Natural  Language  Engineering,  DOI: 
http://dx.doi.org/10.1017/S1351324912000125   
Appendix A
The list of stopwords utilized in this study:
a, an, the, at, as, by, for, from, in, on, of, off, up,  
to, out, over, if, then, than, with, have, had, has,  
can,  could,  do,  did,  does,  be,  am,  are,  is,  was,  
were, would, will,  it,  this,  that,  no, not,  yes, but,  
all,  and,  or,  any,  so,  every,  we,  us,  you,  also,  s
Note that most of these words would be excluded 
by POS filtering. However, the full  stop list  was 
applied anyway.
38
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 49?58,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
A Two-Stage Approach for Generating Unbiased                                                   
Estimates of Text Complexity 
 
 
Kathleen M. Sheehan Michael Flor Diane Napolitano 
   
Educational Testing Service 
Princeton, NJ, USA 
{ksheehan, mflor, dnapolitano}@ets.org 
 
 
 
 
 
 
Abstract 
Many existing approaches for measuring text 
complexity tend to overestimate the complexi-
ty levels of informational texts while simulta-
neously underestimating the complexity levels 
of literary texts. We present a two-stage esti-
mation technique that successfully addresses 
this problem.  At Stage 1, each text is classi-
fied into one or another of three possible ge-
nres:  informational, literary or mixed.  Next, 
at Stage 2, a complexity score is generated for 
each text by applying one or another of three 
possible prediction models:  one optimized for 
application to informational texts, one opti-
mized for application to literary texts, and one 
optimized for application to mixed texts.  
Each model combines lexical, syntactic and 
discourse features, as appropriate, to best rep-
licate human complexity judgments. We dem-
onstrate that resulting text complexity 
predictions are both unbiased, and highly cor-
related with classifications provided by expe-
rienced educators. 
1 Introduction 
Automated text analysis systems, such as reada-
bility metrics, are frequently used to assess the 
probability that texts with varying combinations of 
linguistic features will be more or less accessible to 
readers with varying levels of reading comprehen-
sion skill (Stajner, Evans, Orasan and Mitkov, 
2012).  This paper introduces TextEvaluator, a ful-
ly-automated text analysis system designed to faci-
litate such work.1
 Our approach for addressing these differences 
can be summarized as follows.  First, a large set of 
lexical, syntactic and discourse features is ex-
tracted from each text.  Next, either human raters, 
or an automated genre classifier is used to classify 
each text into one or another of three possible ge-
nre categories: informational, literary, or mixed.  
Finally, a complexity score is generated for each 
text by applying one or another of three possible 
prediction models: one optimized for application to 
informational texts, one optimized for application 
to literary texts, and one optimized for application 
to mixed texts. We demonstrate that resulting 
complexity measures are both unbiased, and highly 
correlated with text grade level (GL) classifications 
provided by experienced educators.  
 TextEvaluator successfully 
addresses an important limitation of many existing 
readability metrics:  the tendency to over-predict 
the complexity levels of informational texts, while 
simultaneously under-predicting the complexity 
levels of literary texts (Sheehan, Kostin & Futagi, 
2008; Sheehan, Kostin, Futagi & Flor, 2010). We 
illustrate this phenomenon, and argue that it results 
from two fundamental differences between infor-
mational and literary texts:  (a) differences in the 
way that common every-day words are used and 
combined; and (b) differences in the rate at which 
rare words are repeated.  
                                                          
1 TextEvaluator was previously called SourceRater. 
49
Our paper is organized as follows.  Section 2 
summarizes related work on readability assess-
ment. Section 3 describes the two corpora assem-
bled for use in this study, and outlines how genre 
and GL classifications were assigned.  Section 4 
illustrates the problem of genre bias by considering 
the specific biases detected in two widely-used 
readability metrics.  Section 5 describes the Text- 
Evaluator features, methods and results. Section 6 
presents a summary and discussion. 
2    Related Work  
Despite the large numbers of text features that may 
potentially contribute to the ease or difficulty of 
comprehending complex text, many widely-used 
readability metrics are based on extremely limited 
feature sets.  For example, the Flesch-Kincaid GL 
score (Kincaid, et al, 1975), the FOG Index (Gun-
ning, 1952), and the Lexile Framework (Stenner, et 
al., 2006) each consider just two features: a single 
measure of syntactic complexity (average sentence 
length) and a single measure of lexical difficulty 
(either average word length in syllables, average 
frequency of multi-syllable words, or average word 
familiarity estimated via a word frequency, WF, 
index).  
Recently, more computationally sophisticated 
modeling techniques such as Statistical Language 
Models (Si and Callan, 2001; Collins-Thompson 
and Callan, 2004, Heilman, et al, 2007, Pitler and 
Nenkova, 2008), Support Vector Machines 
(Schwarm and Ostendorf, 2005), Principal Com-
ponents Analyses (Sheehan, et al, 2010) and Mul-
ti-Layer Perceptron classifiers (Vajjala and 
Meurers, 2012) have enabled researchers to inves-
tigate a broader range of potentially useful fea-
tures.  For example: Schwarm and Ostendorf 
(2005) demonstrated that vocabulary measures 
based on trigrams were effective at distinguishing 
articles targeted at younger and older readers; Pit-
ler and Nenkova (2008) reported improved validity 
for measures based on the likelihood of vocabulary 
and the likelihood of discourse relations; and Vaj-
jala and Meurers (2012) demonstrated that features 
inspired by Second Language Acquisition research 
also contributed to validity improvements.  Impor-
tantly, however, while this research has contributed 
to our understanding of the types of text features 
that may cause texts to be more or less compre-
hensible, evaluations focused on the presence and 
degree of genre bias have not been reported. 
3   Corpora   
Two text collections are considered in this re-
search.  Our training corpus includes 934 passages 
selected from a set of previously administered 
standardized assessments constructed to provide 
valid and reliable feedback about the types of ver-
bal reasoning skills described in U.S. state and na-
tional assessment frameworks. Human judgments 
of genre (informational, literary or mixed) and GL 
(grades 3-12) were available for all texts.  Genre 
classifications were based on established guide-
lines which place texts structured to inform or per-
suade (e.g., newspaper text, excerpts from science 
or social studies textbooks) in the informational 
category, and texts structured to provide a reward-
ing literary experience (e.g., folk tales, short sto-
ries, excerpts from novels) in the literary category 
(see American Institutes for Research, 2008). We 
added a Mixed category to accommodate texts 
classified as incorporating both informational and 
literary elements.  Nelson, Perfetti, Liben and Li-
ben (2012) describe an earlier, somewhat smaller 
version of this dataset.  We added additional pas-
sages downloaded from State Department of Edu-
cation web sites, and from the National 
Assessment of Educational Progress (NAEP).  In 
each case, GL classifications reflected the GLs at 
which passages were administered to students.  
Thus, all passages classified at Grade 3 appeared 
on high-stakes assessments constructed to provide 
evidence of student performance relative to Grade 
3 reading standards.  
Two important characteristics of this dataset 
should be noted.  First, unlike many previous cor-
pora, (e.g., Stenner, et al, 2006; Zeno, et al, 2005) 
accurate paragraph markings are included for all 
texts. Second, while many of the datasets consi-
dered in previous readability research were com-
prised entirely of informational text (e.g., Pitler 
and Nenkova, 2008; Schwarm and Ostendorf, 
2005;  Vajjala and Meurers, 2012) the current da-
taset covers the full range of text types considered 
by teachers and students in U.S. classrooms.   
Table 1 shows the numbers of informational, li-
terary and mixed training passages at each targeted 
GL.  Passage lengths ranged from 112 words at 
Grade 3, to more than 2000 words at Grade 12. 
50
Average passage lengths were 569 words and 695 
words in the informational and literary subsets, 
respectively.  
 
Grade 
Level 
Genre  
Total Inf. Lit. Mixed 
3 46 60 8 114 
4 51 74 7 132 
5 44 46 12 102 
6 41 40 6 87 
7 36 58 6 100 
8 70 63 18 151 
9 23 23 2 48 
10 26 49 2 77 
11 15 24 0 39 
12 47 15 22 84 
Total 399 452 83 934 
 
Table 1.  Numbers of passages in the model develop-
ment/training dataset, by grade level and genre.  
 
A validation dataset was also constructed.  It in-
cludes the 168 texts that were published as Appen-
dix B of the new Common Core State Standards 
(CCSSI, 2010), a new standards document that has 
now been adopted in 46 U.S. states. Individual 
texts were contributed by teachers, librarians, cur-
riculum experts, and reading researchers.  GL clas-
sifications are designed to illustrate the ?staircase 
of increasing complexity? that teachers and test 
developers are being encouraged to replicate when 
selecting texts for use in K-12 instruction and as-
sessment in the U.S.  The staircase is specified in 
terms of five grade bands:  Grades 2-3, Grades 4-5, 
Grades 6-8, Grades 9-10 or Grades 11+.  Table 2 
shows the numbers of informational, literary and 
?Other? texts (includes both Mixed and speeches) 
included at each grade band.   
 
Grade 
Band 
Genre  
Total Inf. Lit. Other 
2-3 6 10 4 20 
4-5 16 10 4 30 
6-8 12 16 13 41 
9-10 12 10 17 39 
11+ 8 10 20 38 
Total 54 56 58 168 
 
Table 2.  Numbers of passages in the validation dataset, 
by grade band and genre. 
4   Genre Bias 
   
This section examines the root causes of genre bi-
as. We focus on two fundamental differences be-
tween informational and literary texts: differences 
in the types of vocabularies employed, and differ-
ences in the rate at which rare words are repeated.  
These differences have been examined in several 
previous studies.  For example, Lee (2001) docu-
mented differences in the use of ?core? vocabulary 
within a corpus of informational and literary texts 
that included over one million words downloaded 
from the British National Corpus.  Core vocabulary 
was defined in terms of a list of 2000 common 
words classified as appropriate for use in the dic-
tionary definitions presented in the Longman Dic-
tionary of Contemporary English.  The analyses 
demonstrated that core vocabulary usage was high-
er in literary texts than in informational texts.  For 
example, when literary texts such as fiction, poetry 
and drama were considered, the percent of total 
words classified as ?core? vocabulary ranged from 
81% to 84%.  By contrast, when informational 
texts such as science and social studies texts were 
considered, the percent of total words classified as 
?core? vocabulary ranged from 66% to 71%.  In 
interpreting these results Lee suggested that the 
creativity and imaginativeness typically associated 
with literary writing may be less closely tied to the 
type or level of vocabulary employed and more 
closely tied to the way that core words are used 
and combined.  Note that this implies that an indi-
vidual word detected in a literary text may not be 
indicative of the same level of processing chal-
lenge as that same word detected in an informa-
tional text. 
Differences in the vocabularies employed within 
informational and literary texts, and subsequent 
impacts on readability metrics, are also discussed 
in Appendix A of the Common Core State Stan-
dards (CCSSI, 2010).  The tendency of many exist-
ing readability metrics to underestimate the 
complexity levels of literary texts is described as 
follows: ?The Lexile Framework, like traditional 
formulas, may underestimate the difficulty of texts 
that use simple, familiar language to convey so-
phisticated ideas, as is true of much high-quality 
fiction written for adults and appropriate for older 
students? (p. 7).  
Genre bias may also result from genre-specific 
differences in word repetition rates.  Hiebert and 
51
Mesmer (2013, p.46) describe this phenomenon as 
follows:  ?Content area texts often receive inflated 
readability scores since key concept words that are 
rare (e.g., photosynthesis, inflation) are often re-
peated which increases vocabulary load, even 
though repetition of content words can support 
student learning (Cohen & Steinberg, 1983)?.  
Table 3 provides empirical evidence of these 
trends.  The table presents mean GL classifications 
estimated conditional on mean WF scores, for the 
informational (n = 399) and literary (n = 452) pas-
sages in our training dataset.  WF scores were gen-
erated via an in-house WF index constructed from 
a corpus of more than 400 million word tokens.  
The corpus includes more than 17,000 complete 
books, including both fiction and nonfiction titles.   
 
 
Avg. WF 
Informational Literary 
N GL SD N GL SD 
51.0?52.5 2 12.0 0.0 0 -- -- 
52.5?54.0 16 10.8 1.9 0 -- -- 
54.0?55.5 68 9.6 2.0 1 10.0 -- 
55.5?57.0 89 7.8 2.7 18 9.9 1.9 
57.0?58.5 96 6.6 2.3 46 9.2 2.0 
58.5?60.0 78 5.3 1.8 92 7.6 2.4 
60.0?61.5 44 4.6 1.8 142 6.2 2.4 
61.5?63.0 6 3.7 0.8 119 5.5 2.1 
63.0?64.5 0 -- -- 31 4.5 1.9 
64.5?66.0 0 -- -- 3 4.0 1.7 
Total 399 57.4 2.1 452 60.6 1.9 
 
Table 3.  Mean GL classifications, by Average WF 
score, for informational and literary passages targeted at 
readers in grades 3 through 12.   
 
The results in Table 3 confirm that, consistent 
with expectations, texts with lower average WF 
scores are more likely to appear on assessments 
targeted at older readers, while texts with higher 
average WF scores are more likely to appear on 
assessments targeted at younger readers.  But note 
that large genre differences are also present. Figure 
1 provides a graphical representation of these 
trends.  Results for informational texts are plotted 
with a solid line; those for literary texts are plotted 
with a dashed line. Note that the literary curve ap-
pears above the informational curve throughout the 
entire observed range of the data. This suggests 
that a given value of the Average WF measure is 
indicative of a higher GL classification if the text 
in question is a literary text, and a lower GL classi-
fication if the text in question is an informational 
text. Since a readability measure that includes this 
feature (or a feature similar to this feature) without 
also accounting for genre effects will tend to yield 
predictions that fall between the two curves, result-
ing GL predictions will tend to be too high for in-
formational texts (positive bias) and too low for 
literary texts (negative bias).   
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Figure 1.  Mean text GL plotted conditional on average 
WF score. (One literary mean score based on evidence 
from a single text is not plotted.) 
 
Figure 2 confirms that this evidence-based pre-
diction holds true for two widely-used readability 
metrics: the Flesch-Kincaid GL score and the Lex-
ile Framework2
                                                          
2 All Lexile scores were obtained via the Lexile Analyzer 
available at www.lexile.com. Scores are only available for a 
subset of texts since our training corpus included just 548 
passages at the time that these data were collected. Corres-
ponding human GL classifications were approximately evenly 
distributed across grades 3 through 12. 
. Each individual plot compares 
Flesch-Kincaid GL scores (top row), or Lexile 
scores (bottom row) to the human GL classifica-
tions stored in our training dataset, i.e., classifica-
tions that were developed and reviewed by 
experienced educators, and were subsequently used 
to make high-stakes decisions about students and 
teachers, e.g., requiring students to repeat a grade 
rather than advancing to the next GL. The plots 
confirm that, in each case, the predicted pattern of 
over- and under-estimation is present. That is, on 
average, both Flesch-Kincaid scores and Lexile 
scores tend to be slightly too high for informational 
texts, and slightly too low for literary texts, thereby 
calling into doubt any cross-genre comparisons. 
Average ETS Word Frequency
M
ea
n 
G
ra
de
 L
ev
el
52 54 56 58 60 62 64 66
4
6
8
10
12 Literary
Informational
 
52
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Informational (n = 243)
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Literary (n = 305)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Passage complexity scores generated via the 
Flesch-Kincaid GL score (top) and the Lexile Frame-
work (bottom) compared to GL classifications provided 
by experienced educators. 
5  Features, Components and Results 
5.1 Features 
 
The TextEvaluator feature set is designed to 
measure the ease or difficulty of implementing 
four types of processes believed to be critically 
involved in comprehending complex text: (1) 
processes involved in word recognition and decod-
ing, (2) processes associated with using relevant 
syntactic knowledge to assemble words into mea-
ningful propositions, (3) processes associated with 
inferring connections across propositions or larger 
sections of text, and (4) processes associated with 
using relevant prior knowledge and experience to 
develop a more complete, more integrated mental 
representation of a text. (See Kintsch, 1998). 
A total of 43 candidate features were developed. 
Since many of these were expected to be moderate-
ly inter-correlated, a Principal Components Analy-
sis (PCA) was used to locate clusters of features 
that exhibited high within-cluster correlation and 
low between-cluster correlation.  Linear combina-
tions defined in terms of the resulting feature clus-
ters provided the independent variables considered 
in subsequent investigations.  Biber and his col-
leagues (2004) justify this approach by noting that, 
because many important aspects of text variation 
are not well captured by individual linguistic fea-
tures, investigation of such characteristics requires 
a focus on ?constellations of co-occurring linguis-
tic features? as opposed to individual features (p. 
45). 
The PCA suggested that more than 60% of the 
variation captured by the full set of 43 features 
could be accounted for via a set of eight compo-
nent scores, where each component is estimated as 
a linear combination of multiple correlated fea-
tures, and only 3 of the 43 features had moderately 
high loadings on more than one component, and 
most loadings exceeded 0.70.  The individual fea-
tures comprising each component are described 
below.  
Component #1:  Academic Vocabulary.  Ten 
features loaded heavily on this component.  Two 
are based on the Academic Word List described in 
Coxhead (2000). These include:  the frequency per 
thousand words of all words on the Academic 
Word List, and the ratio of listed words to total 
words.  In a previous study, Vajjala and Meurers 
(2012)  demonstrated that the ratio of listed words 
to total wards was very effective at distinguishing 
texts at lower and higher levels in the Weekly 
Reader corpus. Two additional features focus on 
the frequency of nominalizations, including one 
estimated from token counts and one estimated 
from type counts. Four additional features are 
based on word lists developed by Biber and his 
colleagues.  These include the frequency per thou-
sand words of academic verbs, abstract nouns, top-
ical adjectives and cognitive process nouns (see 
Biber, 1986, 1988; and Biber, et al, 2004). Two 
measures of word length also loaded on this di-
mension:  average word length measured in syl-
lables, and the frequency per thousand words of 
words containing more than 8 characters.  
Component #2:  Syntactic Complexity. Seven 
features loaded heavily on this component. These 
include features determined from the output of the 
Stanford  Parser (Klein and Manning, 2003), as 
well as more easily computed measures such as 
average sentence length, average frequency of long 
sentences (>= 25 words), and average number of 
53
words between punctuation marks (commas, semi-
colons, etc.).  Parse-based features include average 
number of dependent clauses, and an automated 
version of the word ?depth? measure introduced by 
Yngve (1960). This last feature, called Average 
Maximum Yngve Depth, is designed to capture 
variation in the memory load imposed by sentences 
with varying syntactic structures. It is estimated by 
first assigning a depth classification to each word 
in the text, then determining the maximum depth 
represented within each sentence, and then averag-
ing over resulting sentence-level estimates to ob-
tain a passage-level estimate.  Several studies of 
this word depth measure have been reported. For 
example, Bormuth (1964) reported a correlation of 
-0.78 between mean word depth scores and cloze 
fill-in rates provided by Japanese EFL learners.  
Component #3:  Concreteness. Words that are 
more concrete are more likely to evoke meaningful 
mental images, a response that has been shown to 
facilitate comprehension (Coltheart, 1981). Alder-
son (2000) argued that the level of concreteness 
present in a text is a useful feature to consider 
when evaluating passages for use on reading as-
sessments targeted at L2 readers. A total of five 
concreteness and imageability measures loaded 
heavily on this dimension.  All five measures are 
based on concreteness and imageability ratings 
downloaded from the MRC psycholinguistic data-
base (Coltheart, 1981).  Ratings are expressed on a 
7 point scale with 1 indicating least concrete, or 
least imageable, and 7 indicating most concrete or 
most imageable.   
Component #4:  Word Unfamiliarity. This com-
ponent summarizes variation detected via six dif-
ferent features.  Two features are measures of 
average word familiarity: one estimated via our in-
house WF Index, and one estimated via the TASA 
WF Index (see Zeno, et al, 1995).  Both features 
have negative loadings, suggesting that the com-
ponent is measuring vocabulary difficulty as op-
posed to vocabulary easiness. The other features 
with high loadings on this component are all meas-
ures of rare word frequency. These all have posi-
tive loadings since texts with large numbers of rare 
words are expected to be more difficult. Two types 
of rare word indices are included: indices based on 
token counts and indices based on type counts. 
Vocabulary measures based on token counts view 
each new word as an independent comprehension 
challenge, even when the same word occurs re-
peatedly throughout the text. By contrast, vocabu-
lary measures based on type counts assume that a 
passage containing five different unfamiliar words 
may be more challenging than a passage contain-
ing the same unfamiliar word repeated five times. 
This difference is consistent with the notion that 
each repetition of an unknown word provides an 
additional opportunity to connect to prior know-
ledge (Cohen & Steinberg, 1983).  
Component #5:  Interactive/Conversational 
Style.  This component includes the frequency per 
thousand words of:  conversation verbs, fiction 
verbs, communication verbs, 1st person plural pro-
nouns, contractions, and words enclosed in quotes.  
Verb types were determined from one or more of 
the following studies: Biber (1986),  Biber (1988), 
and Biber, et al (2004).   
Component #6:  Degree of Narrativity. Three 
features had high positive loadings on this dimen-
sion:  Frequency of past perfect aspect verbs, fre-
quency of past tense verbs and frequency of 3rd 
person singular pronouns.  All three features have 
previously been classified as providing positive 
evidence of the degree of narrativity exhibited in a 
text (see Biber, 1986 and Biber, 1988). 
Component #7:  Cohesion. Cohesion is that 
property of a text that enables it to be interpreted as 
a ?coherent message? rather than a collection of 
unrelated clauses and sentences.  Halliday and Ha-
san (1976) argued that readers are more likely to 
interpret a text as a ?coherent message?  when cer-
tain observable features are present.  These include 
repeated content words and explicit connectives.  
The seventh component extracted in the PCA in-
cludes three different types of cohesion features.  
The first two features measure the frequency of 
content word repetition across adjacent sentences 
within paragraphs. These measures differ from the 
cohesion measures discussed in Graesser et al 
(2004) and in Pitler and Nenkova (2008) in that a 
psychometric linking procedure is used to ensure 
that results for different texts are reported on com-
parable scales (See Sheehan, in press).  The fre-
quency of causal conjuncts (therefore, 
consequently, etc.) also loads on this dimension. 
Component #8:  Argumentation.  Two features 
have high loadings on this dimension:  the fre-
quency of concessive and adversative conjuncts 
(although, though, alternatively, in contrast, etc.), 
and the frequency of negations (no, neither, etc.), 
Just and Carpenter, (1987).  
54
5.2  An Automated Genre Classifier 
 
A preliminary automated genre classifier was 
developed by training a logistic regression model 
to predict the probability that a text is classified as 
informational  as opposed to literary.  A signifi-
cant positive coefficient was obtained for the Aca-
demic Vocabulary component defined above, 
suggesting that a high score on this component 
may be interpreted as an indication that the text is 
more likely to be informational.  Significant nega-
tive coefficients were obtained for Narrativity, In-
teractive/Conversational Style, and Syntactic 
Complexity, indicating that a high score on any of 
these components may be interpreted as an indica-
tion that the text is more likely to be literary.  Two 
individual features that were not included in the 
PCA were also significant:  the proportion of adja-
cent sentences containing at least one overlapping 
stemmed content word, and the frequency of 1st 
person singular pronouns.  These features were not 
included in the PCA because they are not reliably 
indicative of differences in text complexity (See 
Sheehan, in press; Pitler and Nenkova, 2008.) Re-
sults confirmed, however, that these features are 
useful for predicting a text?s genre classification.  
Alternative decision rules based on this model 
were investigated. Table 4 summarizes the levels 
of precision (P), recall (R) and F1 = 2RP/(R+P) 
obtained for the selected decision rule which was 
defined as follows: Classify as informational if 
P(Inf) >= 0.52, classify as literary if P(inf) < 0.48, 
else classify as mixed. This decision rule is defined 
such that few texts are classified into the mixed 
category since, at present, the training dataset in-
cludes very few mixed texts. The table shows de-
creased precision in the Validation dataset since 
many more mixed texts are included, and the ma-
jority of these were classified as informational. 
 
Dataset Genre N R P F1 
Training Inf 399 .84 .79 .81 
Training Lit 452 .88 .79 .83 
Training Mixed 83 .01 .09 .01 
Validation Inf 67 .91 .56 .69 
Validation Lit 56 .80 .80 .80 
Validation Mixed 45 .07 1.0 .13 
 
Table 4.  Levels of Precision, Recall and F1 obtained for 
1, 089 texts in the training and validation datasets.  
Speeches are not included in this summary. 
5.3  Prediction Equations 
 
We use separate genre-specific regression mod-
els to generate GL predictions for texts classified 
as informational, literary, or mixed. The coeffi-
cients estimated for informational and literary texts 
are shown in Table 5. Note that each component is 
significant in one or both models.  The table also 
highlights key genre differences. For example, note 
that the Interactive/Conv. Style score is significant 
in the Inf. model but not in the Literary model.  
This reflects the fact that, while literary texts at all 
GLs tend to exhibit relatively high interactivity, 
similarly high interactivity among inf. texts tends 
to only be present at the lowest GLs.  Thus, a high 
Interactivity is an indication of low complexity if 
the text in question is an informational text, but 
provides no statistically significant evidence about 
complexity if the text in question is a literary text.  
 
Component Informational Literary 
Academic Voc. 1.126* .824* 
Word Unfamiliarity .802* .793* 
Word Concreteness -.610* -.483* 
Syn. Complexity .983* 1.404* 
Lexical Cohesion -.266* -.440* 
Interactive/Conv. Style -.518* ns 
Degree of Narrativity ns -.361* 
Argumentation .431* ns 
 
Table 5. Regression coefficients estimated from training 
texts.  *p < .01, ns = not significant. 
 
5.4  Validity Evidence 
 
Two aspects of system validity are of interest: 
(a) whether genre bias is present, and (b) whether 
complexity scores correlate well with judgments 
provided by professional educators, i.e., the educa-
tors involved in selecting texts for use on high-
stakes state reading assessments. The issue of ge-
nre bias is addressed in Figure 3. Each plot com-
pares GL predictions generated via TextEvaluator 
to GL predictions provided by experienced educa-
tors.  Note that no evidence of a systematic tenden-
cy to under-predict the complexity levels of 
literary texts is present. This suggests that our 
strategy of developing distinct prediction models 
for informational and literary texts has succeeded 
in overcoming the genre biases present among 
many key features.  
55
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3.  TextEvaluator GL predictions compared to 
human GL classifications for informational and literary 
texts. 
 
TestEvaluator performance relative to the goal of 
predicting the human grade band classifications in 
the validation dataset was also examined. Results 
are summarized in Table 6 along with correspond-
ing results for the Lexile Framework (Stenner, et 
al., 2006) and the REAP system (Heilman, et al, 
2007).  All results are reprinted, with permission, 
from Nelson, et al, (2012).  In each case, perfor-
mance is summarized in terms of the Spearman 
rank order correlation between the readability 
scores generated for each text, and corresponding 
human grade band classifications.  95% confidence 
limits estimated via the Fisher r to z transformation 
are also listed.   
 
 
 
System 
Lower 
95% 
Bound 
 
Correlation 
Coefficient 
Upper 
95% 
Bound 
TextEvaluator 0.683 0.76 0.814 
REAP 0.427 0.54 0.641 
Lexile 0.380 0.50 0.607 
 
Table 6. Correlation  between readability scores and 
human grade band classifications for the 168 Common 
Core texts in the validation dataset.   
 
The comparison suggests that, relative to the task 
of  predicting the human grade band classifications 
assigned to the informational, literary and mixed 
texts in Appendix B of the new Common Core 
State Standards, TextEvaluator is significantly 
more effective than both the Lexile Framework 
and the REAP system. 
6  Summary and Discussion 
 
In many recent studies, proposed readability me-
trics have been trained and validated on text collec-
tions composed entirely of informational text, e.g., 
Wall Street Journal articles (Pitler and Nenkova, 
2008), Encyclopedia Britannica articles (Schwarm 
and Ostendorf, 2005) and Weekly Reader articles 
(Vajjala and Meurers, 2012). This paper considers 
the more challenging task of predicting human-
assigned GL classifications in a corpus of texts 
constructed to be representative of the broad range 
of reading materials considered by teachers and 
students in U.S. classrooms.   
Two approaches for modeling the complexity 
characteristics of these passages were compared.  
In Approach #1, a single, non-genre specific pre-
diction equation is estimated, and that equation is 
then applied to texts in all genres.  Two measures 
developed via this approach were evaluated:  the 
Lexile Framework and the REAP system.    
Approach #2 differs from Approach #1 in that 
genre-specific prediction equations are used, there-
by ensuring that important genre effects are ac-
commodated.  This approach is currently only 
available via the TextEvaluator system.  
Measures developed via each approach were 
evaluated on a held-out sample.  Results confirmed 
that complexity classifications obtained via                       
TextEvaluator are significantly more highly corre-
lated with the human grade band classifications in 
the held-out sample than are classifications ob-
tained via the Lexile Framework or REAP system.  
This study also demonstrated that, when genre 
effects are ignored, readability scores for informa-
tional texts tend to be overestimated, while those 
for literary texts tend to be underestimated. Note 
that this finding significantly complicates the 
process of using readability metrics to generate 
valid cross-genre comparisons. For example, 
Stajner, et al (2012) conclude that SimpleWiki 
may not serve as a ?gold standard? of high acces-
sibility because comparisons based on readability 
metrics suggest that it is more complex than Fic-
tion. We intend to further investigate this finding 
using TextEvaluator since conclusions that are not 
impacted by genre bias can then be reported. Addi-
tional planned work involves investigating addi-
tional measures of genre, and incorporating these 
into our genre classifier.    
56
References  
 
Alderson, J. C. (2000). Assessing reading. Cam-
bridge: Cambridge University Press. 
American Institutes for Research (2008). Reading 
framework for the 2009 National Assessment of 
Educational Progress.  Washington, DC: Na-
tional Assessment Governing Board. 
Biber, D. (1986).  Spoken and written textual di-
mension in English: Resolving the contradictory 
findings.  Language, 62: 394-414. 
Biber, D. (1988).  Variation across Speech and 
Writing.  Cambridge: Cambridge University 
Press. 
Biber, D., Conrad, S., Reppen, R., Byrd, P., Helt, 
M., Clark, V., et al, (2004).  Representing lan-
guage use in the university:  Analysis of the 
TOEFL 2000 Spoken and Written Academic 
Language Corpus.  TOEFL Monograph Series, 
MS-25, January 2004.  Princeton, NJ: Educa-
tional Testing Service.  
Bormuth, J.R. (1964).  Mean word depth as a pre-
dictor of comprehension difficulty.  California 
Journal of Educational Research, 15, 226-231. 
Cohen, S. A. & Steinberg, J. E. (1983). Effects of 
three types of vocabulary on readability of in-
termediate grade science textbooks:  An applica-
tion of Finn?s transfer feature theory.  Reading 
Research Quarterly, 19(1), 86-101. 
Collins-Thompson, K. and Callan, J. (2004). A 
language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 
2004, Boston, USA. 
Coltheart, M. (1981).  The MRC psycholinguistic 
database, Quarterly Journal of Experimental 
Psychology, 33A, 497-505. 
Common Core State Standards Initiative (2010).  
Common core state standards for English lan-
guage arts & literacy in history/social studies, 
science and technical subjects.  Washington, 
DC: CCSSO & National Governors Association. 
Coxhead, A. (2000)  A new academic word list.  
TESOL Quarterly, 34(2), 213-238.  
Gunning, R. (1952).  The technique of clear writ-
ing. McGraw-Hill: New York. 
Graesser, A.C., McNamara, D. S., Louwerse, 
M.W. and Cai, Z. (2004).  Coh-Metrix:  Analy-
sis of text on cohesion and language. Behavior 
Research Methods, Instruments & Computers, 
36(2), 193-202.  
Halliday, M. A.K. & Hasan, R. (1976) Cohesion in 
English. Longman, London. 
Hiebert, E. H. & Mesmer, H. A. E. (2013).  Upping 
the ante of text complexity in the Common Core 
State Standards: Examining its potential impact 
on young readers. Educational Researcher, 
42(1), 44-51. 
Heilman, M., Collins-Thompson, K., Callan, J. & 
Eskenazi, M. (2007). Combining lexical and 
grammatical features to improve readability 
measures for first and second language texts. In 
Human Language Technologies 2007: The Con-
ference of the North American Chapter of the 
Association for Computational Linguistics 
(HLT-NAACL?07), 460-467. 
Just, M. A. & Carpenter, P. A. (1987). The psy-
chology of reading and language comprehen-
sion. Boston: Allyn & Bacon. 
Kincaid, J.P., Fishburne, R.P, Rogers, R.L. & 
Chissom, B.S. (1975). Derivation of new reada-
bility formulas (automated readability index, 
Fog count and Flesch reading ease formula) for 
navy enlisted personnel. Research Branch Re-
port 8-75. Naval Air Station, Memphis, TN. 
Kintsch, W. (1998). Comprehension: A paradigm 
for cognition. Cambridge, UK: Cambridge Uni-
versity Press. 
Klein, D. & Manning, C. D. (2003). Accurate Un-
lexicalized Parsing. In Proceedings of the 41st 
Meeting of the Association for Computational 
Linguistics, 423-430. 
Lee, D. Y. W. (2001)  Defining core vocabulary 
and tracking its distribution across spoken and 
written genres.  Journal of English Linguistics.  
29, 250-278. 
Nelson, J., Perfetti, C., Liben, D. and Liben, M. 
(2012). Measures of text difficulty: Testing their 
predictive value for grade levels and student 
performance. Technical Report, The Council of 
Chief State School Officers. 
57
Pitler, E. & Nenkova, A (2008). Revisiting reada-
bility:  A unified framework for predicting text 
quality. In Proceedings of the 2008 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational Lin-
guistics, 186-195. 
Schwarm, S. & Ostendorf, M. (2005). Reading 
level assessment using support vector machines 
and statistical language models.  In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL?05), 523-530. 
Sheehan, K.M. (in press).  Measuring cohesion: An 
approach that accounts for differences in the de-
gree of integration challenge presented by dif-
ferent types of sentences.  Educational 
Measurement: Issues and Practice. 
Sheehan, K.M., Kostin, I & Futagi, Y. (2008). 
When do standard approaches for measuring 
vocabulary difficulty, syntactic complexity and 
referential cohesion yield biased estimates of 
text difficulty?  In B.C. Love, K. McRae, & 
V.M. Sloutsky (Eds.), Proceedings of the 30th 
Annual Conference of the Cognitive Science 
Society, Washington D.C. 
 
Sheehan, K.M., Kostin, I., Futagi, Y. & Flor, M.  
(2010). Generating automated text complexity 
classifications that are aligned with targeted 
text complexity standards. (ETS RR-10-28). 
Princeton, NJ: ETS. 
 
Si, L. & Callan, J. (2001). A statistical model for 
scientific readability. In Proceedings of the 10th 
International Conference on Information and 
Knowledge Management (CIKM), 574-576. 
?tajner, S., Evans, R., Orasan, C., & Mitkov, R. 
(2012). What Can Readability Measures Really 
Tell Us About Text Complexity?. In Natural 
Language Processing for Improving Textual Ac-
cessibility (NLP4ITA) Workshop Programme                   
(p. 14). 
 
Stenner, A. J., Burdick, H., Sanford, E. & Burdick, 
D. (2006). How accurate are Lexile text meas-
ures?  Journal of Applied Measurement, 7(3), 
307-322. 
 
Vajjala, S. & Meurers, D. (2012). On improving 
the accuracy of readability classification using 
insights from second language acquisition. In 
Proceedings of the 7th Workshop on the Innova-
tive Use of NLP for Building Educational Appli-
cations, 163-173. 
Yngve, V.H. (1960).  A model and an hypothesis 
for language structure.  Proceedings of the 
American Philosophical Society, 104, 444-466. 
Zeno, S. M., Ivens, S. H., Millard, R. T., Duvvuri, 
R. (1995). The educator?s word frequency 
guide. Brewster, NY: Touchstone Applied 
Science Associates.  
  
58
