An NP-Cluster Based Approach to Coreference Resolution
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Traditionally, coreference resolution is done
by mining the reference relationships be-
tween NP pairs. However, an individual NP
usually lacks adequate description informa-
tion of its referred entity. In this paper,
we propose a supervised learning-based ap-
proach which does coreference resolution by
exploring the relationships between NPs and
coreferential clusters. Compared with indi-
vidual NPs, coreferential clusters could pro-
vide richer information of the entities for bet-
ter rules learning and reference determina-
tion. The evaluation done on MEDLINE
data set shows that our approach outper-
forms the baseline NP-NP based approach
in both recall and precision.
1 Introduction
Coreference resolution is the process of linking
as a cluster1 multiple expressions which refer
to the same entities in a document. In recent
years, supervised machine learning approaches
have been applied to this problem and achieved
considerable success (e.g. Aone and Bennett
(1995); McCarthy and Lehnert (1995); Soon et
al. (2001); Ng and Cardie (2002b)). The main
idea of most supervised learning approaches is to
recast this task as a binary classification prob-
lem. Specifically, a classifier is learned and then
used to determine whether or not two NPs in a
document are co-referring. Clusters are formed
by linking coreferential NP pairs according to a
certain selection strategy. In this way, the identi-
fication of coreferential clusters in text is reduced
to the identification of coreferential NP pairs.
One problem of such reduction, however,
is that the individual NP usually lacks ade-
quate descriptive information of its referred en-
tity. Consequently, it is often difficult to judge
whether or not two NPs are talking about the
1In this paper the term ?cluster? can be interchange-
ably used as ?chain?, while the former better emphasizes
the equivalence property of coreference relationship.
same entity simply from the properties of the
pair alone. As an example, consider the pair of a
non-pronoun and its pronominal antecedent can-
didate. The pronoun itself gives few clues for the
reference determination. Using such NP pairs
would have a negative influence for rules learn-
ing and subsequent resolution. So far, several
efforts (Harabagiu et al, 2001; Ng and Cardie,
2002a; Ng and Cardie, 2002b) have attempted to
address this problem by discarding the ?hard?
pairs and select only those confident ones from
the NP-pair pool. Nevertheless, this eliminat-
ing strategy still can not guarantee that the NPs
in ?confident? pairs bear necessary description
information of their referents.
In this paper, we present a supervised
learning-based approach to coreference resolu-
tion. Rather than attempting to mine the ref-
erence relationships between NP pairs, our ap-
proach does resolution by determining the links
of NPs to the existing coreferential clusters. In
our approach, a classifier is trained on the in-
stances formed by an NP and one of its possi-
ble antecedent clusters, and then applied dur-
ing resolution to select the proper cluster for an
encountered NP to be linked. As a coreferen-
tial cluster offers richer information to describe
an entity than a single NP in the cluster, we
could expect that such an NP-Cluster framework
would enhance the resolution capability of the
system. Our experiments were done on the the
MEDLINE data set. Compared with the base-
line approach based on NP-NP framework, our
approach yields a recall improvement by 4.6%,
with still a precision gain by 1.3%. These results
indicate that the NP-Cluster based approach is
effective for the coreference resolution task.
The remainder of this paper is organized as
follows. Section 2 introduces as the baseline the
NP-NP based approach, while Section 3 presents
in details our NP-Cluster based approach. Sec-
tion 4 reports and discusses the experimental re-
sults. Section 5 describes related research work.
Finally, conclusion is given in Section 6.
2 Baseline: the NP-NP based
approach
2.1 Framework description
We built a baseline coreference resolution sys-
tem, which adopts the common NP-NP based
learning framework as employed in (Soon et al,
2001).
Each instance in this approach takes the form
of i{NPj , NPi}, which is associated with a fea-
ture vector consisting of 18 features (f1 ? f18) as
described in Table 2. Most of the features come
from Soon et al (2001)?s system. Inspired by the
work of (Strube et al, 2002) and (Yang et al,
2004), we use two features, StrSim1 (f17) and
StrSim2 (f18), to measure the string-matching
degree of NPj and NPi. Given the following sim-
ilarity function:
Str Simlarity(Str1, Str2) = 100? |Str1 ? Str2|Str1
StrSim1 and StrSim2 are computed
using Str Similarity(SNPj , SNPi) and
Str Similarity(SNPi , SNPj ), respectively. Here
SNP is the token list of NP, which is obtained
by applying word stemming, stopword removal
and acronym expansion to the original string as
described in Yang et al (2004)?s work.
During training, for each anaphor NPj in a
given text, a positive instance is generated by
pairing NPj with its closest antecedent. A set
of negative instances is also formed by NPj and
each NP occurring between NPj and NPi.
When the training instances are ready, a clas-
sifier is learned by C5.0 algorithm (Quinlan,
1993). During resolution, each encountered noun
phrase, NPj , is paired in turn with each preced-
ing noun phrase, NPi. For each pair, a test-
ing instance is created as during training, and
then presented to the decision tree, which re-
turns a confidence value (CF)2 indicating the
likelihood that NPi is coreferential to NPj . In
our study, two antecedent selection strategies,
Most Recent First (MRF) and Best First (BF),
are tried to link NPj to its a proper antecedent
with CF above a threshold (0.5). MRF (Soon
et al, 2001) selects the candidate closest to the
anaphor, while BF (Aone and Bennett, 1995; Ng
2The confidence value is obtained by using the
smoothed ratio p+1t+2 , where p is the number of positiveinstances and t is the total number of instances contained
in the corresponding leaf node.
and Cardie, 2002b) selects the candidate with
the maximal CF.
2.2 Limitation of the approach
Nevertheless, the problem of the NP-NP based
approach is that the individual NP usually lacks
adequate description information about its re-
ferred entity. Consequently, it is often difficult
to determine whether or not two NPs refer to
the same entity simply from the properties of
the pair. See the the text segment in Table 1,
for example,
[1 A mutant of [2 KBF1/p50] ], unable to
bind to DNA but able to form homo- or [3 het-
erodimers] , has been constructed.
[4 This protein] reduces or abolishes the DNA
binding activity of wild-type proteins of [5 the
same family ([6 KBF1/p50] , c- and v-rel)].
[7 This mutant] also functions in vivo as a
transacting dominant negative regulator:. . .
Table 1: An Example from the data set
In the above text, [1 A mutant of KBF1/p50],
[4 This protein] and [7 This mutant] are anno-
tated in the same coreferential cluster. Accord-
ing to the above framework, NP7 and its closest
antecedent, NP4, will form a positive instance.
Nevertheless, such an instance is not informa-
tive in that NP4 bears little information related
to the entity and thus provides few clues to ex-
plain its coreference relationship with NP7.
In fact, this relationship would be clear if [1 A
mutant of KBF1/p50], the antecedent of NP4,
is taken into consideration. NP1 gives a de-
tailed description of the entity. By comparing
the string of NP7 with this description, it is ap-
parent that NP7 belongs to the cluster of NP1,
and thus should be coreferential to NP4. This
suggests that we use the coreferential cluster,
instead of its single element, to resolve an NP
correctly. In our study, we propose an approach
which adopts an NP-Cluster based framework to
do resolution. The details of the approach are
given in the next section.
3 The NP-Cluster based approach
Similar to the baseline approach, our approach
also recasts coreference resolution as a binary
classification problem. The difference, however,
is that our approach aims to learn a classifier
which would select the most preferred cluster,
instead of the most preferred antecedent, for an
encountered NP in text. We will give the frame-
work of the approach, including the instance rep-
Features describing the relationships between NPj and NPi
1. DefNp 1 1 if NPj is a definite NP; else 0
2. DemoNP 1 1 if NPj starts with a demonstrative; else 0
3. IndefNP 1 1 if NPj is an indefinite NP; else 0
4. Pron 1 1 if NPj is a pronoun; else 0
5. ProperNP 1 1 if NPj is a proper NP; else 0
6. DefNP 2 1 if NPi is a definite NP; else 0
7. DemoNP 2 1 if NPi starts with a demonstrative; else 0
8. IndefNP 2 1 if NPi is an indefinite NP; else 0
9. Pron 2 1 if NPi is a pronoun; else 0
10. ProperNP 2 1 if NPi is a proper NP; else 0
11. Appositive 1 if NPi and NPj are in an appositive structure; else 0
12. NameAlias 1 if NPi and NPj are in an alias of the other; else 0
13. GenderAgree 1 if NPi and NPj agree in gender; else 0
14. NumAgree 1 if NPi and NPj agree in number; else 0
15. SemanticAgree 1 if NPi and NPj agree in semantic class; else 0
16. HeadStrMatch 1 if NPi and NPj contain the same head string; else 0
17. StrSim 1 The string similarity of NPj against NPi
18. StrSim 2 The string similarity of NPi against NPj
Features describing the relationships between NPj and cluster Ck
19. Cluster NumAgree 1 if Ck and NPj agree in number; else 0
20. Cluster GenAgree 1 if Ck and NPj agree in gender; else 0
21. Cluster SemAgree 1 if Ck and NPj agree in semantic class; else 0
22. Cluster Length The number of elements contained in Ck
23. Cluster StrSim The string similarity of NPj against Ck
24. Cluster StrLNPSim The string similarity of NPj against the longest NP in Ck
Table 2: The features in our coreference resolution system (Features 1 ? 18 are also used in the
baseline system using NP-NP based approach)
resentation, the training and the resolution pro-
cedures, in the following subsections.
3.1 Instance representation
An instance in our approach is composed of three
elements like below:
i{NPj , Ck, NPi}
where NPj , like the definition in the baseline,
is the noun phrase under consideration, while Ck
is an existing coreferential cluster. Each cluster
could be referred by a reference noun phrase NPi,
a certain element of the cluster. A cluster would
probably contain more than one reference NPs
and thus may have multiple associated instances.
For a training instance, the label is positive if
NPj is annotated as belonging to Ck, or negative
if otherwise.
In our system, each instance is represented as
a set of 24 features as shown in Table 2. The
features are supposed to capture the properties
of NPj and Ck as well as their relationships. In
the table we divide the features into two groups,
one describing NPj and NPi and the other de-
scribing NPj and Ck. For the former group, we
just use the same features set as in the baseline
system, while for the latter, we introduce 6 more
features:
Cluster NumAgree, Cluster GenAgree
and Cluster SemAgree: These three fea-
tures mark the compatibility of NPj and Ck
in number, gender and semantic agreement,
respectively. If NPj mismatches the agreement
with any element in Ck, the corresponding
feature is set to 0.
Cluster Length: The number of NPs in the
cluster Ck. This feature reflects the global
salience of an entity in the sense that the more
frequently an entity is mentioned, the more im-
portant it would probably be in text.
Cluster StrSim: This feature marks the string
similarity between NPj and Ck. Suppose
SNPj is the token set of NPj , we compute
the feature value using the similarity function
Str Similarity(SNPj , SCk), where
SCk =
?
NPi?Ck
SNPi
Cluster StrLNPSim: It marks the string
matching degree of NPj and the noun phrase
in Ck with the most number of tokens. The
intuition here is that the NP with the longest
string would probably bear richer description in-
formation of the referent than other elements in
the cluster. The feature is calculated using the
similarity function Str Similarity(SNPj , SNPk),
where
NPk = arg maxNPi?Ck |SNPi |
3.2 Training procedure
Given an annotated training document, we pro-
cess the noun phrases from beginning to end.
For each anaphoric noun phrase NPj , we consider
its preceding coreferential clusters from right to
left3. For each cluster, we create only one in-
stance by taking the last NP in the cluster as
the reference NP. The process will not terminate
until the cluster to which NPj belongs is found.
To make it clear, consider the example in Ta-
ble 1 again. For the noun phrase [7 This mu-
tant], the annotated preceding coreferential clus-
ters are:
C1: { . . . , NP2, NP6 }
C2: { . . . , NP5 }
C3: { NP1, NP4 }
C4: { . . . , NP3 }
Thus three training instances are generated:
i{ NP7, C1, NP6 }
i{ NP7, C2, NP5 }
i{ NP7, C3, NP4 }
Among them, the first two instances are la-
belled as negative while the last one is positive.
After the training instances are ready, we use
C5.0 learning algorithm to learn a decision tree
classifier as in the baseline approach.
3.3 Resolution procedure
The resolution procedure is the counterpart of
the training procedure. Given a testing docu-
ment, for each encountered noun phrase, NPj ,
we create a set of instances by pairing NPj with
each cluster found previously. The instances are
presented to the learned decision tree to judge
the likelihood that NPj is linked to a cluster.
The resolution algorithm is given in Figure 1.
As described in the algorithm, for each clus-
ter under consideration, we create multiple in-
stances by using every NP in the cluster as the
reference NP. The confidence value of the cluster
3We define the position of a cluster as the position of
the last NP in the cluster.
algorithm RESOLVE (a testing document d)
ClusterSet = ?;
//suppose d has N markable NPs;
for j = 1 to N
foreach cluster in ClusterSet
CFcluster = maxNPi?clusterCFi(NPj ,cluster,NPi)
select a proper cluster, BestCluster, according
to a ceterin cluster selection strategy;
if BestCluster != NULL
BestCluster = BestCluster ? {NPj};
else
//create a new cluster
NewCluster = { NPj };
ClusterSet = ClusterSet ? {NewCluster};
Figure 1: The clusters identification algorithm
is the maximal confidence value of its instances.
Similar to the baseline system, two cluster selec-
tion strategies, i.e. MRF and BF, could be ap-
plied to link NPj to a proper cluster. For MRF
strategy, NPj is linked to the closest cluster with
confidence value above 0.5, while for BF, it is
linked to the cluster with the maximal confidence
value (above 0.5).
3.4 Comparison of NP-NP and
NP-Cluster based approaches
As noted above, the idea of the NP-Cluster based
approach is different from the NP-NP based ap-
proach. However, due to the fact that in our
approach a cluster is processed based on its refer-
ence NPs, the framework of our approach could
be reduced to the NP-NP based framework if
the cluster-related features were removed. From
this point of view, this approach could be con-
sidered as an extension of the baseline approach
by applying additional cluster features as the
properties of NPi. These features provide richer
description information of the entity, and thus
make the coreference relationship between two
NPs more apparent. In this way, both rules
learning and coreference determination capabili-
ties of the original approach could be enhanced.
4 Evaluation
4.1 Data collection
Our coreference resolution system is a compo-
nent of our information extraction system in
biomedical domain. For this purpose, an anno-
tated coreference corpus have been built 4, which
4The annotation scheme and samples are avail-
able in http://nlp.i2r.a-star.edu.sg/resources/GENIA-
coreference
MRF BF
Experiments R P F R P F
Baseline 80.2 77.4 78.8 80.3 77.5 78.9
AllAnte 84.4 70.2 76.6 85.7 71.4 77.9
Our Approach 84.4 78.2 81.2 84.9 78.8 81.7
Table 3: The performance of different coreference resolution systems
consists of totally 228 MEDLINE abstracts se-
lected from the GENIA data set. The aver-
age length of the documents in collection is 244
words. One characteristic of the bio-literature
is that pronouns only occupy about 3% among
all the NPs. This ratio is quite low compared
to that in newswire domain (e.g. above 10% for
MUC data set).
A pipeline of NLP components is applied to
pre-process an input raw text. Among them,
NE recognition, part-of-speech tagging and text
chunking adopt the same HMM based engine
with error-driven learning capability (Zhou and
Su, 2002). The NE recognition component
trained on GENIA (Shen et al, 2003) can
recognize up to 23 common biomedical entity
types with an overall performance of 66.1 F-
measure (P=66.5% R=65.7%). In addition, to
remove the apparent non-anaphors (e.g., em-
bedded proper nouns) in advance, a heuristic-
based non-anaphoricity identification module is
applied, which successfully removes 50.0% non-
anaphors with a precision of 83.5% for our data
set.
4.2 Experiments and discussions
Our experiments were done on first 100 docu-
ments from the annotated corpus, among them
70 for training and the other 30 for testing.
Throughout these experiments, default learning
parameters were applied in the C5.0 algorithm.
The recall and precision were calculated auto-
matically according to the scoring scheme pro-
posed by Vilain et al (1995).
In Table 3 we compared the performance of
different coreference resolution systems. The
first line summarizes the results of the baseline
system using traditional NP-NP based approach
as described in Section 2. Using BF strategy,
Baseline obtains 80.3% recall and 77.5% preci-
sion. These results are better than the work by
Castano et al (2002) and Yang et al (2004),
which were also tested on the MEDLINE data
set and reported a F-measure of about 74% and
69%, respectively.
In the experiments, we evaluated another NP-
NP based system, AllAnte. It adopts a similar
learning framework as Baseline except that dur-
ing training it generates the positive instances by
paring an NP with all its antecedents instead of
only the closest one. The system attempts to use
such an instance selection strategy to incorpo-
rate the information from coreferential clusters.
But the results are nevertheless disappointing:
although this strategy boosts the recall by 5.4%,
the precision drops considerably by above 6% at
the same time. The overall F-measure is even
lower than the baseline systems.
The last line of Table 3 demonstrates the re-
sults of our NP-Cluster based approach. For BF
strategy, the system achieves 84.9% recall and
78.8% precision. As opposed to the baseline sys-
tem, the recall rises by 4.6% while the precision
still gains slightly by 1.3%. Overall, we observe
the increase of F-measure by 2.8%.
The results in Table 3 also indicate that the
BF strategy is superior to the MRF strategy.
A similar finding was also reported by Ng and
Cardie (2002b) in the MUC data set.
To gain insight into the difference in the per-
formance between our NP-Cluster based system
and the NP-NP based system, we compared the
decision trees generated in the two systems in
Figure 2. In both trees, the string-similarity
features occur on the top portion, which sup-
ports the arguments by (Strube et al, 2002)
and (Yang et al, 2004) that string-matching is a
crucial factor for NP coreference resolution. As
shown in the figure, the feature StrSim 1 in left
tree is completely replaced by the Cluster StrSim
and Cluster StrLNPSim in the right tree, which
means that matching the tokens with a cluster
is more reliable than with a single NP. More-
over, the cluster length will also be checked when
the NP under consideration has low similarity
against a cluster. These evidences prove that
the information from clusters is quite important
for the coreference resolution on the data set.
The decision tree visualizes the importance of
the features for a data set. However, the tree is
learned from the documents where coreferential
clusters are correctly annotated. During resolu-
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
: NameAlias = 0:
: :...Appositive = 0: 0 (13095/265)
: Appositive = 1: 1 (15/4)
HeadMatch = 1:
:...StrSim_1 > 71:
:...DemoNP_1 = 0: 1 (615/29)
: DemoNP_1 = 1:
: :...NumAgree = 0: 0 (5)
: NumAgree = 1: 1 (26)
StrSim_1 <= 71:
:...DemoNP_2 = 1: 1 (12/2)
DemoNP_2 = 0:
:...StrSim_2 <= 77: 0 (144/17)
StrSim_2 > 77:
:...StrSim_1 <= 33: 0 (42/11)
StrSim_1 > 33: 1 (38/11)
HeadMatch = 1:
:...Cluster_StrSim > 66: 1 (663/36)
: Cluster_StrSim <= 66:
: :...StrSim_2 <= 85: 0 (140/14)
: StrSim_2 > 85:
: :...Cluster_StrLNPSim > 50: 1 (16/1)
: Cluster_StrLNPSim <= 50:
: :...Cluster_Length <= 5: 0 (59/17)
: Cluster_Length > 5: 1 (4)
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
NameAlias = 0:
:...Appositive = 1: 1 (15/4)
Appositive = 0:
:...StrSim_2 <= 54:
:..
StrSim_2 > 54:
:..
Figure 2: The resulting decision trees for the NP-NP and NP-Cluster based approaches
Features R P F
f1?21 80.3 77.5 78.9
f1?21, f22 84.1 74.4 79.0
f1?21, f23 84.7 78.8 81.6
f1?21, f24 84.3 78.0 81.0
f1?21, f23, f22 84.9 78.6 81.6
f1?21, f23, f24 84.9 78.9 81.8
f1?21, f23, f24, f22 84.9 78.8 81.7
Table 4: Performance using combined features
(fi refers to the i(th) feature listed in Table 2)
tion, unfortunately, the found clusters are usu-
ally not completely correct, and as a result the
features important in training data may not be
also helpful for testing data. Therefore, in the
experiments we were concerned about which fea-
tures really matter for the real coreference res-
olution. For this purpose, we tested our system
using different features and evaluated their per-
formance in Table 4. Here we just considered fea-
ture Cluster Length (f22), Cluster StrSim (f23)
and Cluster StrLNPSim (f24), as Figure 2 has
indicated that among the cluster-related features
only these three are possibly effective for resolu-
tion. Throughout the experiment, the Best-First
strategy was applied.
As illustrated in the table, we could observe
that:
1. Without the three features, the system is
equivalent to the baseline system in terms
of the same recall and precision.
2. Cluster StrSim (f23) is the most effective
as it contributes most to the system per-
formance. Simply using this feature boosts
the F-measure by 2.7%.
3. Cluster StrLNPSim (f24) is also effective by
improving the F-measure by 2.1% alone.
When combined with f23, it leads to the
best F-measure.
4. Cluster Length (f22) only brings 0.1% F-
measure improvement. It could barely
increase, or even worse, reduces the F-
measure when used together with the the
other two features.
5 Related work
To our knowledge, our work is the first
supervised-learning based attempt to do coref-
erence resolution by exploring the relationship
between an NP and coreferential clusters. In the
heuristic salience-based algorithm for pronoun
resolution, Lappin and Leass (1994) introduce
a procedure for identifying anaphorically linked
NP as a cluster for which a global salience value
is computed as the sum of the salience values of
its elements. Cardie and Wagstaff (1999) have
proposed an unsupervised approach which also
incorporates cluster information into considera-
tion. Their approach uses hard constraints to
preclude the link of an NP to a cluster mismatch-
ing the number, gender or semantic agreements,
while our approach takes these agreements to-
gether with other features (e.g. cluster-length,
string-matching degree,etc) as preference factors
for cluster selection. Besides, the idea of cluster-
ing can be seen in the research of cross-document
coreference, where NPs with high context simi-
larity would be chained together based on certain
clustering methods (Bagga and Biermann, 1998;
Gooi and Allan, 2004).
6 Conclusion
In this paper we have proposed a supervised
learning-based approach to coreference resolu-
tion. Rather than mining the coreferential re-
lationship between NP pairs as in conventional
approaches, our approach does resolution by ex-
ploring the relationships between an NP and the
coreferential clusters. Compared to individual
NPs, coreferential clusters provide more infor-
mation for rules learning and reference determi-
nation. In the paper, we first introduced the con-
ventional NP-NP based approach and analyzed
its limitation. Then we described in details the
framework of our NP-Cluster based approach,
including the instance representation, training
and resolution procedures. We evaluated our ap-
proach in the biomedical domain, and the experi-
mental results showed that our approach outper-
forms the NP-NP based approach in both recall
(4.6%) and precision (1.3%).
While our approach achieves better perfor-
mance, there is still room for further improve-
ment. For example, the approach just resolves
an NP using the cluster information available so
far. Nevertheless, the text after the NP would
probably give important supplementary infor-
mation of the clusters. The ignorance of such
information may affect the correct resolution of
the NP. In the future work, we plan to work out
more robust clustering algorithm to link an NP
to a globally best cluster.
References
C. Aone and S. W. Bennett. 1995. Evaluating
automated and manual acquistion of anaphora
resolution strategies. In Proceedings of the
33rd Annual Meeting of the Association for
Compuational Linguistics, pages 122?129.
A. Bagga and A. Biermann. 1998. Entity-based
cross document coreferencing using the vector
space model. In Proceedings of the 36th An-
nual Meeting of the Association for Computa-
tional Linguisticsthe 17th International Con-
ference on Computational Linguistics, pages
79?85.
C. Cardie and K. Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of
the Joint Conference on Empirical Methods in
NLP and Very Large Corpora.
J. Castano, J. Zhang, and J. Pustejovsky. 2002.
Anaphora resolution in biomedical literature.
In International Symposium on Reference Res-
olution, Alicante, Spain.
C. Gooi and J. Allan. 2004. Cross-document
coreference on a large scale corpus. In Pro-
ceedings of 2004 Human Language Technology
conference / North American chapter of the
Association for Computational Linguistics an-
nual meeting.
S. Harabagiu, R. Bunescu, and S. Maiorano.
2001. Text knowledge mining for coreference
resolution. In Proceedings of the 2nd An-
nual Meeting of the North America Chapter of
the Association for Compuational Linguistics,
pages 55?62.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):525?561.
J. McCarthy and Q. Lehnert. 1995. Using de-
cision trees for coreference resolution. In Pro-
ceedings of the 14th International Conference
on Artificial Intelligences, pages 1050?1055.
V. Ng and C. Cardie. 2002a. Combining sam-
ple selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceed-
ings of the conference on Empirical Methods
in Natural Language Processing, pages 55?62,
Philadelphia.
V. Ng and C. Cardie. 2002b. Improving ma-
chine learning approaches to coreference res-
olution. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publishers,
San Francisco, CA.
D. Shen, J. Zhang, G. Zhou, J. Su, and
C. Tan. 2003. Effective adaptation of hid-
den markov model-based named-entity recog-
nizer for biomedical domain. In Proceedings of
ACL03 Workshop on Natural Language Pro-
cessing in Biomedicine, Japan.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference resolu-
tion of noun phrases. Computational Linguis-
tics, 27(4):521?544.
M. Strube, S. Rapp, and C. Muller. 2002. The
influence of minimum edit distance on refer-
ence resolution. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 312?319, Philadel-
phia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan. 2004. Im-
proving noun phrase coreference resolution by
matching strings. In Proceedings of the 1st In-
ternational Joint Conference on Natural Lan-
guage Processing, Hainan.
G. Zhou and J. Su. 2002. Named Entity recog-
nition using a HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, Philadelphia.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 121?128
Manchester, August 2008
 
 
Other-Anaphora Resolution in Biomedical Texts with Automatically 
Mined Patterns 
 
Chen Bin#, Yang Xiaofeng$, Su Jian^ and Tan Chew Lim* 
#*School of Computing, National University of Singapore  
$^Institute for Infocomm Research, A-STAR, Singapore 
{#chenbin, *tancl}@comp.nus.edu.sg 
{$xiaofengy, ^sujian}@i2r.a-star.edu.sg 
? Abstract 
This paper proposes an other-anaphora 
resolution approach in bio-medical texts. 
It utilizes automatically mined patterns to 
discover the semantic relation between an 
anaphor and a candidate antecedent. The 
knowledge from lexical patterns is incor-
porated in a machine learning framework 
to perform anaphora resolution. The ex-
periments show that machine learning 
approach combined with the auto-mined 
knowledge is effective for other-
anaphora resolution in the biomedical 
domain. Our system with auto-mined pat-
terns gives an accuracy of 56.5%., yield-
ing 16.2% improvement against the base-
line system without pattern features, and 
9% improvement against the system us-
ing manually designed patterns.  
1 Introduction 
The last decade has seen an explosive growth in 
the amount of textual information in biomedi-
cine. There is a need for an effective and effi-
cient text-mining system to gather and utilize the 
knowledge encoded in the biomedical literature. 
For a correct discourse analysis, a text-mining 
system should have the capability of understand-
ing the reference relations among different ex-
pressions in texts. Hence, anaphor resolution, the 
task of resolving a given text expression to its 
referred expression in prior texts, is important for 
an intelligent text processing system. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
In linguistics, an expression that points back 
to a previously mentioned expression is called an 
anaphor, and the expression being referred to by 
the anaphor is called its antecedent. Most pre-
vious work on anaphora resolution aims at identi-
ty-anaphora in which both an anaphor and its 
antecedent are mentions of the same entity. 
In this paper, we focus on a special type of 
anaphora resolution, namely, other-anaphora 
resolution, in which an anaphor to be resolved 
has a prefix modifier ?other? or ?another?. The 
antecedent of an other-anaphor is a complement 
expression to the anaphor in a super set. In other 
words, an other-anaphor is a set of elements ex-
cluding the element(s) specified by the antece-
dent. If the modifier ?other? or ?another? is re-
moved, an anaphor becomes the super set includ-
ing the antecedent. Thus, other-anaphora in fact 
represents a ?part-whole? relation. Consider the 
following text  
 ?IL-10 inhibits nuclear stimulation of nuclear 
factor kappa B (NF kappa B).  
Several other transcription factors including NF- 
IL-6, AP-1, AP-2, GR, CREB, Oct-1, and Sp-1 
are not affected by IL-10.?  
Here, the expression ?other transcription fac-
tors? is an other-anaphor, while the ?NF kappa 
B? is its antecedent. The anaphor refers to any 
transcription factors except the antecedent.  By 
removing the lexical modifier ?other?, we can 
get a supper set ?transcription factors? that in-
cludes the antecedent. The anaphor and antece-
dent thus have a ?part-whole? relation1.  
Other-anaphora resolution is an important 
sub-task in information extraction for biomedical 
                                                 
1 Other-anaphora could be also held between ex-
pressions that have subset-set or member-collection 
relations. In this paper, we treat them in a uniform 
way by using the patterned-based method. 
121
  
domain. It also contributes to biomedical ontolo-
gy building as it targeted at a ?part-whole? rela-
tion which is in the same hierarchical orders as in 
ontology. Furthermore, other-anaphora resolu-
tion is a first-step exploration in the resolution of 
bridging anaphora. Furthermore, other-anaphora 
resolution is a first-step exploration in the resolu-
tion of bridging, a special anaphora phenomenon 
in which the semantic relation between an ana-
phor and its antecedent is more complex (e.g. 
part-whole) than co-reference. 
Previous work on other-anaphora resolution 
relies on knowledge resources, for example, on-
tology like WordNet to determine the ?part-
whole? relation. However, in the biomedical do-
main, a document is full of technical terms which 
are usually missing in a general-purpose ontolo-
gy. To deal with this problem, pattern-based ap-
proaches have been widely employed, in which a 
pattern that represents the ?part-whole? relation 
is designed. Two expressions are connected with 
the specific pattern and form a query. The query 
is searched in a large corpus for the occurrence 
frequency which would indicate how likely the 
two given expressions have the part-whole rela-
tion. The solution can avoid the efforts of con-
structing the ontology knowledge for the "part-
whole" relation. However, the pattern is designed 
in an ad-hoc method, usually from linguistic in-
tuition and its effectiveness for other-anaphora 
resolution is not guaranteed. 
In this paper, we propose a method to auto-
matically mine effective patterns for other-
anaphora resolution in biomedical texts. Our me-
thod runs on a small collection of seed word 
pairs. It searches a large corpus (e.g., PubMed 
abstracts as in our system) for the texts where the 
seed pairs co-occur, and collects the surrounding 
words as the surface patterns. The automatically 
found patterns will be used in a machine learning 
framework for other-anaphora resolution. To our 
knowledge, our work is the first effort of apply-
ing the pattern-base technique to other-anaphora 
resolution in biomedical texts. 
The rest of this paper is organized as follows. 
Section 2 introduces previous related work. Sec-
tion 3 describes the machine learning framework 
for other-anaphora resolution. Section 4 presents 
in detail our method for automatically pattern 
mining. Section 5 gives experiment results and 
has some discussions. Finally, Section 6 con-
cludes the paper and shows some future work. 
2 Related Work 
Previous work on other-anaphora resolution 
commonly depends on human engineered know-
ledge and/or deep semantic knowledge for the 
?part-whole? relation, and mostly works only in 
the news domain. 
Markert et al, (2003) presented a pattern-
based algorithm for other-anaphor resolution. 
They used a manually designed pattern ?ANTE-
CEDENT and/or other ANAPHOR ?. Given two 
expression to be resolved, a query is formed by 
instantiating the pattern with the two given ex-
pressions. The query is searched in the Web. The 
higher the hit number returned, the more likely 
that the anaphor and the antecedent candidate 
have the ?part-whole? relation. The anaphor is 
resolved to the candidate with the highest hit 
number. Their work was tested on 120 other-
anaphora cases extracted from Wall Street Jour-
nal. The final accuracy was 52.5%. 
Modjeska et al, (2003) also presented a simi-
lar pattern-based method for other-anaphora res-
olution, using the same pattern ?ANTECEDENT 
and/or other ANAPHOR?. The hit number re-
turned from the Web is used as a feature for a 
Na?ve Bayesian Classifier to resolve other-
anaphors. Other features include surface words, 
substring matching, distance, gender/number 
agreement, and semantic tag of the NP. They 
evaluated their method with 500 other-anaphor 
cases extracted from Wall Street Journal, and 
reported a result of 60.8% precision and 53.4% 
recall. 
Markert and Nissim (2005) compared three 
systems for other-anaphora resolution, using the 
same data set as in (Modjeska et al, 2003). 
The first system consults WordNet for the 
part-whole relation. The WordNet provides in-
formation on meronym/holonym (part-of rela-
tion) and hypernym/ hyponym (type-of relation). 
Their system achieves a performance of 56.8% 
for precision and 37.0% for recall. 
The second and third systems employ the pat-
tern based approach, employing the same manual 
pattern ?ANTECEDENT and/or other ANA-
PHOR?. The second system did search in British 
Nation Corpus, giving 62.6% precision and 
26.2% recall. The third system did search in the 
Web as in (Markert et al, 2003), giving 53.8% 
precision and 51.7% recall. 
122
  
3 Anaphora Resolution System 
3.1 Corpus 
In our study, we used the GENIA corpus2 for our 
other-anaphora resolution in biomedical texts. 
The corpus consists of 2000 MEDLINE abstracts 
(around 440,000 words). From the GENIA cor-
pus, we extracted 598 other-anaphora cases. The 
598 cases do not contain compound prepositions 
or idiomatic uses of ?other?, like ?on the other 
hand? and ?other than?. And all these anaphors 
have their antecedents found in the current and 
previous two sentences of the other-anaphor. On 
average, there are 15.33 candidate antecedents 
for each anaphor to be resolved. 
To conduct other-anaphora resolution, an in-
put document is preprocessed through a pipeline 
of NLP components, including tokenization, sen-
tence boundary detection, part-of-speech (POS) 
tagging, noun phrase (NP) chunking, and named-
entity recognition (NER). These preprocessing 
modules are aimed to determine the boundaries 
of each NP in a text, and to provide necessary 
information of an NP for subsequent processing. 
In our system, we employed the tool-kits built by 
our group for these components. The POS tagger 
was trained and tested on the GENIA corpus 
(version 2.1) and achieved an accuracy of 97.4%. 
The NP-chunking module, evaluated on UPEN 
WSJ TreeBank, produced 94% F-measure. The 
NER module, trained on GENIA corpus (version 
3.0), achieved 71.2% F-measure covering 22 ent-
ity types (e.g., Virus, Protein, Cell, DNA, etc). 
3.2 Learning Framework 
Our other-anaphora resolution system adopts the 
common learning-based model for identity-
anaphora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002). 
In the learning framework, a training or test-
ing instance has the form of ?? ?????? ,???  
where ??????  is the ?
th candidates of the antece-
dent of anaphor ???. An instance is labelled as 
positive if ??????  is the antecedent of  ??? , or 
negative if ??????  is not the antecedent of  ???. 
An instance is associated with a feature vector 
which records different properties and relations 
between ???  and ?????? . The features used in 
our system will be discussed later in the paper. 
During training, for each other-anaphor, we 
consider as the candidate antecedents the preced-
ing NPs in its current and previous two sentences. 
                                                 
2 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
A positive instance is formed by pairing the ana-
phor and the correct antecedent. And a set of 
negative instances is formed by pairing the ana-
phor and each of the other candidates.  
Based on these generated training instances, 
we can train a binary classifier using any dis-
criminative learning algorithm. In our work, we 
employed support vector machine (SVM) due to 
its good performance in high dimensional feature 
vector spaces. 
During the resolution process, for each other-
anaphor encountered, all of the preceding NPs in 
a three-sentence window are considered. A test 
instance is created for each of the candidate ante-
cedents. The feature vector is presented to the 
trained classifier to determine the other-
anaphoric relation. The candidate with highest 
SVM outcome value is selected as the antecedent.  
3.3 Baseline Features 
Knowledge is usually represented as features for 
machine learning. In our system, we used the 
following groups of features for other-anaphora 
resolution 
 
? Word Distance Indicator 
This feature measures the word distance between 
an anaphor and a candidate antecedent, with the 
assumption that the candidate closer to the ana-
phor has a higher preference to be the antecedent. 
? Same Sentence Indicator 
This feature is either 0 or 1 indicating whether an 
anaphor and a candidate antecedent are in the 
same sentence. Here, the assumption is that the 
candidate in the same sentence as the anaphor is 
preferred for the antecedent. 
? Semantic Group Indicators 
A named-entity can be classified to a semantic 
category such as ?DNA?, ?RNA?, ?Protein? and 
so on3. Thus we use a set of features to record the 
category pair of an anaphor and a candidate ante-
cedent. For example, ?DNA-DNA? is generated 
for the case when both anaphor and candidate are 
DNAs. And ?DNA-Protein? is generated if an 
anaphor is a DNA and a candidate is a protein. 
These features indicate whether a semantic group 
can refer to another.  
Note that an anaphor and its antecedent may 
possibly belong to different semantic categories. 
For example, in the GENIA corpus we found that 
                                                 
3 In our study, we followed the semantic categories defined 
in the annotation scheme of the GENIA corpus.  
123
  
in some cases an expression of a protein name 
actually denotes the gene that encodes the pro-
tein. Thus for a given anaphor and a candidate 
under consideration, it is necessary to record the 
pair-wise semantic groups, instead of using a 
single feature indicating whether two expressions 
are of the same group. 
The semantic group for a named entity is giv-
en by our preprocessing NER. For the common 
NPs produced from the NP chunker, we classify 
the semantic group by looking for the words in-
side NPs. For example, an NP ending with 
?cells? is classified to ?Cell? group while an NP 
ending with ?gene? or ?allele? is classified to 
?DNA? group. 
? Lexical Pattern Indicators 
In some cases, the surrounding words of an ana-
phor and a candidate antecedent strongly indicate 
the ?part-whole? relation. For example, in 
?...asthma and other hypereosinophilic diseas-
es?, the reference between ?other hypereosino-
philic diseases? and ?asthma? is clear if the in-
between words ?and other? are taken into con-
sideration. Another example of such a hint pat-
tern is ?one? the other ?? The feature is 1 if the 
specific patterns are present for the current ana-
phor and candidate pair. A candidate with such a 
feature is preferred to be the antecedent. 
? Hierarchical Name Indicator  
This feature indicates whether an antecedent 
candidate is a substring of an anaphor or vice 
versa. This feature is used to capture cases like 
?Jun? and ?JunB? (?Jun? is a family of protein 
while ?JunB? is a member of this family). In 
many cases, an expression that is a super set 
comes with certain postfix words, for example, 
?family members? in  
?Fludarabine caused a specific depletion of 
STAT1 protein (and mRNA) but not of other 
STAT family members.?  
This kind of phenomenon is more common in 
bio-medical texts than in news articles. 
3.4 SVM Training and Classification 
In our system, we utilized the open-source soft-
ware SVM-Light4 for the classifier training and 
testing.  SVM is a robust statistical model which 
has been applied to many NLP tasks. SVM tries 
to learn a separating line to separate the positive 
instances from negative instances. Kernel trans-
formations are applied for non-linear separable 
                                                 
4 http://svmlight.joachims.org/ 
cases (Vapnik, 1995). In our study, we just used 
the default learning parameters provided by 
SVM-Light with the linear kernel. A more so-
phisticated kernel may further improve the per-
formance. 
4 Using Auto-mined Pattern Features 
The baseline features listed in Section 3.3 only 
rely on shallow lexical, position and semantic 
information about an anaphor and a candidate 
antecedent. It could not, nevertheless, disclose 
the ?part-whole? relation between two given ex-
pressions. In section 2, we have shown some ex-
isting pattern-based solutions that mine the ?part-
whole? relation in a large corpus with some pat-
terns that can represent the relation. However, 
these manually designed patterns are usually se-
lected by heuristics, which may not necessarily 
lead to a high coverage with a good accuracy in 
different domains. To overcome this shortcom-
ing, we would like to use an automatic method to 
mine effective patterns from a large data set. 
First, we create a set of seed pairs of the ?part-
whole? relation. And then, we use the seed pairs 
to discover the patterns that encode the ?part-
whole? relation from a large data set (PubMed as 
in our system). Such a solution is supposed to 
improve the coverage of lexical patterns, while 
still retain the desired ?part-whole? relation for 
other-anaphora resolution. 
The overview of our system with the automat-
ic mined patterns is illustrated in figure 1. 
Seed Pairs 
Generation
Pattern Mining
SVM
GENIA 
Corpus
Seed 
Pairs
Lexical 
Patterns
GENIA
T st 
Cas s
PubMED 
Corpus
 
Figure 1: System Overview 
There are three major parts in our system, 
namely, seed-pairs generation, pattern mining 
and SVM learning and classification. In the sub-
sequent subsections, we will discuss each of the 
three parts in details. 
124
  
4.1 Seed Pairs Preparation 
A seed pair is a pair of phrases/words following 
?part-whole? order, for example,  
?integrin alpha? - ?adhesion molecules? 
where ?integrin alpha? is a kind of ?adhesion 
molecules?.  
We extracted the seed pairs automatically 
from the GENIA corpus. The auto-extracting 
procedure makes uses of some lexical clues like 
?A, such as B, C and D?, ?A (e.g. B and C)?, ?A 
including B? and etc. The capital letter A, B, C 
and D refer to a noun phrase such as ?integrin 
alpha? and ?adhesion molecules?. For each oc-
currence of ?A such as B, C and D?, the program 
will generate seed pairs ?B-A?, ?C-A? and ?D-
A?. 
Consider the following example, 
?Mouse thymoma line EL-4 cells produce cyto-
kines such as interleukin (IL) -2, IL-3, IL-4, IL-
10, and granulocyte-macrophage colony-
stimulating factor in response to phorbol 12-
myristate 13-acetate (PMA).? 
We can extract the following seed pairs, 
?interleukin (IL) -2? ? ?cytokines? 
?IL -3? ? ?cytokines? 
?IL -4? ? ?cytokines? 
?IL -10? ? ?cytokines? 
?granulocyte-macrophage colony-stimulating 
factor? ? ?cytokines?  
A similar action is taken for other lexical 
clues. Totally, we got 909 distinct seed pairs ex-
tracted from the GENIA corpus. 
After the seed pairs have been extracted, an 
automatic verification of the seed pairs is per-
formed. The first purpose of the verification is to 
correct chunking errors. For example, ?HLA 
Class II Gene? may likely be wrongly split into 
?HLA Class? and ?II Gene?. This kind of errors 
is repaired by several simple syntactic rules. The 
second purpose of the verification is to remove 
the inappropriate seed pairs. In our system, we 
abandoned the seed pairs containing pronouns 
like ?those?, ?they?, or nouns like ?element?, 
?member? and ?agent?. Such seed pairs may ei-
ther find no patterns, or lead to meaningless pat-
terns because ?those? or ?elements? have no spe-
cific semantics and could refer to anything. 
4.2 Pattern Mining 
Having obtained the set of seed pairs, we will use 
them to mine patterns for the ?part-whole? rela-
tion. For each seed pair ?antecedent - anaphor? 
(anaphor represents the NP for the ?whole?, 
while antecedent represents the NP for the 
?part?), our system will search in a large data set 
for two queries: ?antecedent * anaphor? and 
?anaphor * antecedent? where the ?*? denotes 
any sequence of words or symbols. For a re-
turned search results, the text in between ?ante-
cedent? and ?anaphora? is extracted as a pattern. 
In our study, we used PubMed 2007 data set 
for the pattern mining. The data set contains 
about 52,000 abstracts with around 9,400,000 
words, and is an ideal large-scale resource for 
pattern mining. 
Consider, as an example, a seed pair ?NK 
kappa B ? ? ?transcription factor?. Suppose that 
a returned sentence for the query ?NK kappa B * 
transcription factor? is  
?...NK kappa B family transcription factors...? 
And a returned sentence for the query ?transcrip-
tion factor * NK kappa B? is 
?...transcription factors, including NF kappa 
B...? 
We can extract a pattern, 
?ANTECEDENT family ANAPHOR? from the 
first sentence and a pattern 
?ANAPHOR, including ANTECEDENT? from 
the second sentence.  
We restrict the patterns so that no pattern span 
across two or more sentences. In other words, the 
pattern shall not contain the symbol ?.?. The vi-
olated patterns will be removed. 
The count that a pattern occurs in the PubMed 
for a seed pair is recorded. As a pattern could be 
reduced by different seed pairs, we define the 
occurrence frequency of a pattern as the sum of 
the counts of the pattern for all the seed pairs, 
using following formula: 
???? ? =  ???(???? , ?? )
????
                           ??(1)   
where ???? ?  is the frequency of pattern ???? ; ??  is 
a seed pair; ?  is the set of all seed pairs. 
???(???? , ?? ) is the count of the pattern ????  for 
?? . 
All the mined patterns are sorted according to 
its frequency as defined in ??(1). 
4.3 Pattern Application 
For classifier training and testing, the patterns 
with high frequency are used as features. In our 
system, we used the top 40 patterns, while we 
also examined the influence the number of the 
patterns on the performance. (See Section 5.2) 
Given an instance ??(?????? , ???) and a pat-
tern feature ????  , a query is constructed by in-
125
  
stantiating with ??????  and ??? . For example, 
for an instance ??("?? ????? ?", "???????-  
?????? ???????")  and a pattern feature ?ANA-
PHOR, including ANTECEDENT?, we can get 
a query ?transcription factors, including NF 
kappa B?. The query is searched in the PubMed 
data set. The count of the query is recorded. The 
value of the pattern feature of a candidate is cal-
culated by normalizing the occurrence frequency 
among all the candidates of the anaphor. 
For demonstration, suppose we have an ana-
phor ?other transcription factors? with two ante-
cedent candidates ?IL-10? and ?NF kappa B?. 
Given a pattern feature ?ANAPHOR, including 
ANTECEDENT?, the count of the query ?tran-
scription factors, including IL-10? is 100 while 
that for ?transcription factors, including NF-
Kappa B? is 300. Then the values of the pattern 
feature for ?IL-10? and ?NF kappa B? are 0.25 
(
100
100+300
) and 0.75 (
300
100+300
), respectively. 
The value of a pattern feature can be inter-
preted as a degree of belief that an anaphor and a 
candidate antecedent have the ?part-whole? rela-
tion, with regard to the specific pattern. Since the 
value of a pattern feature is normalized among 
all the candidates, it could indicate the preference 
of a candidate against other competing candi-
dates. 
5 Experiment Results 
5.1 Experiments Setup 
In our experiments, we conducted a 3-fold cross 
validation to evaluate the performances. The total 
598 other-anaphora cases were divided into 3 
sets of size 200, 199 and 199 respectively. For 
each experiment, two sets were used for training 
while the other set was used for testing.  
For evaluation, we used the accuracy as the 
performance metric, which is defined as the cor-
rectly resolved other-anaphors divided by all the 
testing other-anaphors, that is, 
 
???????? =
# of correctly resolved anaphors
 # of total anaphors
 
5.2 Experiments Results 
Table 1 shows the performance of different 
other-anaphora resolution systems. The first line 
is for the baseline system with only the normal 
features as described in Section 3.3. From the 
table, we can find that the baseline system only 
achieves around 40% accuracy. A performance is 
lower than a similar system in news domain by 
Modjeska et al, (2003) where they reported  
51.6 % precision with 40.6% recall. This differ-
ence is probably because they utilized more se-
mantic knowledge such as hypernymy and mero-
nymy acquired from WordNet. Such knowledge, 
nevertheless, is not easily available in the bio-
medical domain. 
 
Sys Fold-1 Fold-2 Fold-3 Overall 
Baseline 
No Pattern 
42.0 % 
84/200 
38.2 % 
76/199 
40.7 % 
81/199 
40.3 % 
241/598 
Manual 
Pattern 
49.0 % 
98/200 
45.7 % 
91/199 
47.7 % 
95/199 
47.5 % 
284/598 
Auto-
mined 
Pattern 
59.0 % 
118/200 
53.8 % 
107/199 
56.8 % 
113/199 
56.5 % 
338/598 
Table 1: Performance Comparisons 
In our experiments, we tested the system with 
manually designed pattern features. We tried 10 
patterns that can represent the ?part-whole? rela-
tion. Table 2 summaries the patterns used in the 
system. Among them, the pattern ?Anaphor such 
as Antecedent? and ?Antecedent and other Ana-
phor? are commonly used in previous pattern 
based approaches (Markert et al, 2003; Mod-
jeska et al, 2003). 
 
Pattern 
ANTECEDENT is a kind of ANAPHOR 
ANTECEDENT is a type of ANAPHOR 
ANTECEDENT is a member of ANAPHOR 
ANTECEDENT is a part of ANAPHOR 
ANAPHOR such as ANTECEDENT 
ANTECEDENT and other ANAPHOR 
ANTECEDENT within ANAPHOR 
ANTECEDENT is a component of ANAPHOR 
ANTECEDENT is a sort of ANAPHOR 
ANTECEDENT belongs to ANAPHOR 
Table 2: Manually Selected Patterns 
 
The second line of Table 1 shows the results 
of the system with the manual pattern features. 
We can find that adding these pattern features 
produces an overall accuracy of 47%, yielding an 
increase of 7% accuracy against the baseline sys-
tem without the pattern features.  
The improvement in accuracy is consistent 
with previous work using the pattern-based ap-
proaches in the news domain (Modjeska et al, 
2003). However, we found the performance in 
the biomedical domain is worse than that in the 
news domain. For example, Modjeska et al 
(2003) reported a precision around 53%. This 
difference of performance suggests that the ma-
126
  
nually designed patterns may not necessarily 
work equally well in different domains.  
The last system we examined in the experi-
ment is the one with the automatically mined 
pattern features. Table 3 summarizes the top 
mined patterns ranked based on their occurrence 
frequency. Some of the patterns are intuitively 
good representation of the ?part-whole? relation. 
For example, ?ANAPHOR, including ANTE-
CEDENT?. ?ANAPHOR, such as ANTECE-
DENT? and ?ANAPHOR and other ANTECE-
DENT? which are in the manually designed pat-
tern list, are generated.  
The last line of Table 1 lists the result of the 
system with automatically mined pattern fea-
tures. It outperforms the baseline system (up to 
16% accuracy), and the system with manually 
selected patterns (9% accuracy). These results 
prove that our pattern features are effective for 
the other-anaphora resolution.  
 
Pattern Freq 
ANAPHOR, including ANTECEDENT 1213 
ANAPHOR including ANTECEDENT 726 
ANTECEDENT family ANAPHOR 583 
ANAPHOR such as ANTECEDENT 542 
ANTECEDENT transcription ANAPHOR 439 
ANAPHOR, such as ANTECEDENT 295 
ANTECEDENT and other ANAPHOR 270 
ANAPHOR and ANTECEDENT 250 
ANTECEDENT, dendritic ANAPHOR 246 
ANTECEDENT and ANAPHOR 238 
ANTECEDENT human ANAPHOR 223 
ANAPHOR (e.g., ANTECEDENT  213 
ANTECEDENT/rel ANAPHOR 188 
ANTECEDENT-like ANAPHOR 188 
ANAPHOR against ANTECEDENT  163 
Table 3: Auto-Mined Patterns 
To further compare the manually designed 
patterns and the automatically discovered pat-
terns. We examined the coverage rate of the two 
pattern sets. The coverage rate measures the ca-
pability that a set of patterns could lead to posi-
tive anaphor-antecedent pairs. An other-anaphor 
is said to be covered by a pattern set, if the ana-
phor and its antecedent could be hit (i.e., the cor-
responding query has a non-zero hit number) by 
at least one pattern in the list. Thus the coverage 
rate could be defined as 
????????(?)  
=   
#anaphors covered by the pattern set P
# total anaphors
 
The coverage rates of the two pattern sets are 
tabulated in table 4. It is apparent that the auto-
mined patterns have a significantly higher cover-
age (more than twice) than the manually de-
signed patterns. 
 
Patterns Coverage Rate 
Manually Designed 36.0 % 
Auto-Mined 92.1 % 
Table 4: Coverage Comparison 
In our experiments we were also concerned 
about the usefulness of each individual pattern. 
For this purpose, we examined the loss of the 
accuracy when withdrawing a pattern feature 
from the feature list. The top 10 patterns with the 
largest accuracy loss are summarized in table 5. 
 
Pattern 
Acc 
Loss 
ANAPHOR, including ANTECEDENT 4.18% 
ANAPHOR including ANTECEDENT 3.18% 
ANAPHOR such as ANTECEDENT 2.84% 
ANTECEDENT transcription ANAPHOR 2.17% 
ANTECEDENT and other ANAPHOR 2.01% 
ANAPHOR, such as ANTECEDENT 1.84% 
ANTECEDENT family ANAPHOR 1.84% 
ANAPHOR (e.g., ANTECEDENT 1.51% 
ANTECEDENT-like ANAPHOR 1.17% 
ANTECEDENT/rel ANAPHOR 1.17% 
Table 5: Usefulness of Each Pattern 
The process of automatic pattern mining 
would generate numerous surface patterns. It is 
not reasonable to use all the patterns as features. 
As mentioned in section 4.3, we rank the pattern 
based on their occurrence frequency and select 
the top ones as the features. It would be interest-
ing to see how the number of patterns influences 
the performance of anaphora resolution. In figure 
2, we plot the accuracy under different number 
top pattern features. We can find by using more 
patterns, the coverage keeps increasing. The ac-
curacy also increases, but it reaches the peak 
with around 40 patterns. With more patterns, the 
accuracy remains at the same level. This is be-
cause the low frequency patterns usually are not 
that indicative of the ?part-whole? relation. In-
cluding these pattern features would bring noises 
but not help the performance. The flat curve after 
the peak point suggests that the machine learning 
algorithm can effectively identify the importance 
of the pattern features for the resolution decision, 
and therefore including non-indicative patterns 
would not damage the performance. 
In our experiment, we also interested to com-
pare the utility of PubMed with other general 
data sets. Thus, we tested pattern mining by us-
127
  
ing the Google-5-grams corpus5 which lists the 
hit number of all the queries of five words or less 
in the Web. Unfortunately, we found that the per-
formance is worse than using PubMed. The pat-
terns mined from the Web corpus only gives an 
accuracy of around 41%, almost the same as the 
baseline system without using any pattern fea-
tures. The bad performance is due to the fact that 
most of bio-medical names are quite long (2~4 
words) and occur infrequently in the non-
technique data set. Consequently, a query formed 
by a biomedical seed pair usually cannot be 
found in the Web corpus (We found the coverage 
of the auto-mined patterns mined from the corpus 
is only about 20%). 
 
Figure 2: Performance of Various No. of Patterns 
6 Conclusion & Future Works 
In this paper, we have presented how to automat-
ically mined pattern features for learning-based 
other-anaphora resolution in bio-medical texts. 
The patterns that represent the ?part-whole? rela-
tions are automatically mined from a large data 
set. They are used as features for a SVM-based 
classifier learning and testing. The results of our 
experiments show a reasonably good perfor-
mance with 56.5% accuracy). It outperforms 
(16% in accuracy) the baseline system without 
the pattern features, and also beats (9%) the sys-
tem with manually designed pattern features. 
There are several directions for future work. 
We would like to employ a pattern pruning 
process to remove those less indicative patterns 
such as ?ANAPHOR, ANTECEDENT?. And we 
also plan to perform pattern normalization which 
integrates two similar or literally identical pat-
                                                 
5 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?  
   catalogId=LDC2006T13 
terns into a single one. By doing so, the useful 
patterns may come to the top of the pattern list. 
Also we would like to explore ontology re-
sources like MESH and Genes Ontology, which 
can provide enriched hierarchies of bio-medical 
terms and thus would benefit other-anaphora res-
olution. 
Acknowledgements  
This study on co-reference resolution is partially supported 
by a Specific Targeted Research Project (STREP) of the 
European Union's 6th Framework Programme within IST 
call 4, Bootstrapping of Ontologies and Terminologies 
STrategic REsearch Project (BOOTStrep). 
References 
Castano J, Zhang J and Pustejovsky J. Anaphora Resolution 
in Biomedical Literature. Submitted to International Sym-
posium on Reference Resolution 2002, Alicante, Spain 
Clark H. Bridging. In Thinking. Readings in Cognitive 
Science. Johnson-Laird and Wason edition. Cambridge. 
Cambridge University Press; 1977.411?420 
Gasperin C and Vieira R. Using Word Similarity Lists for 
Resolving Indirect Anaphora. In Proceedings of ACL 
Workshop on Reference Resolution and Its Application. 
30 June 2004; Barcelona. 2004.40-46 
Girju R, Badulescu A and Moldovan D. Automatic Discov-
ery of Part-Whole Relations. Computational Linguistics, 
2006, 32(2):83-135 
Bernauer J.. Analysis of Part-Whole Relation and Subsump-
tion in Medical Domain. Data Knowledge Enginnering 
1996, 20:405-415 
Markert K. and Nissim M. Comparing Knowledge Sources 
for Nominal Anaphora Resolution. Computational Lin-
guistics, 2005, 31(3):367-402 
Markert K, Modjeska N and Nissim M. Using the Web for 
Nominal Anaphora Resolution. In Proceedings of EACL 
Workshop on the Computational Treatment of Anaphora. 
14 April 2003; Budapest. 2003.39-46 
Mitokov R. Anaphor Resolution. The State of The Art. 
Working Paper, University of Wolverhampton, UK, 1999 
Modjeska N, Markert K and Nissim M. Using the Web in 
Machine Learning for Other-anaphor Resolution. In Pro-
ceedings of the 2003 Conference on Empirical Methods in 
Natural Language Processing. July2003,Sapporo.176-183 
Soon WM, Ng HT and Lim CY. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases. Com-
putational Linguistics, 2001, 27(4).521-544 
Vapnik, V. Chapter 5 Methods of Pattern Recognition. In 
The Nature of Statistical Learning Theory. New York. 
Springer-Verlag, 1995.123-167 
Varzi C.  Parts, Wholes, and Part-whole Relation. The Pros-
pects of the Mereotopology. Data & Knowledge Engi-
neering, 1996, 20.259-286 
Vieira R, Bick E, Coelho J, Muller V, Collovini S, Souza J 
and Rino L. Semantic Tagging for Resolution of Indirect 
Anaphora. In Proceedings of 7th SIGdial Workshop on 
Discourse and Dialogue. July 2006; Sydney.76-79 
Burges C. A Tutorial on Supporting Vector Machines for 
Pattern Recognition. Data Mining and Knowledge Dis-
covery 1998, 2:121-167 
Ng V. and Cardie C. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of An-
nual Conference for Association of Computational Lin-
guistics 2002, Philadelphia.104-111 
128
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1037?1045,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Fast Translation Rule Matching for Syntax-based Statistical 
Machine Translation 
 
 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Chew Lim Tan2    
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
 
 
 
 
Abstract 
In a linguistically-motivated syntax-based trans-
lation system, the entire translation process is 
normally carried out in two steps, translation 
rule matching and target sentence decoding us-
ing the matched rules. Both steps are very time-
consuming due to the tremendous number of 
translation rules, the exhaustive search in trans-
lation rule matching and the complex nature of 
the translation task itself. In this paper, we pro-
pose a hyper-tree-based fast algorithm for trans-
lation rule matching. Experimental results on 
the NIST MT-2003 Chinese-English translation 
task show that our algorithm is at least 19 times 
faster in rule matching and is able to help to 
save 57% of overall translation time over previ-
ous methods when using large fragment transla-
tion rules. 
1 Introduction 
Recently linguistically-motivated syntax-based 
translation method has achieved great success in 
statistical machine translation (SMT) (Galley et al, 
2004; Liu et al, 2006, 2007; Zhang et al, 2007, 
2008a; Mi et al, 2008; Mi and Huang 2008; 
Zhang et al, 2009). It translates a source sentence 
to its target one in two steps by using structured 
translation rules. In the first step, which is called 
translation rule matching step, all the applicable1 
translation rules are extracted from the entire rule 
set by matching the source parse tree/forest. The 
second step is to decode the source sentence into 
its target one using the extracted translation rules. 
Both of the two steps are very time-consuming 
due to the exponential number of translation rules 
and the complex nature of machine translation as 
                                                           
1 Given a source structure (either a parse tree or a parse 
forest), a translation rule is applicable if and only if the 
left hand side of the translation rule exactly matches a 
tree fragment of the given source structure. 
an NP-hard search problem (Knight, 1999). In the 
SMT research community, the second step has 
been well studied and many methods have been 
proposed to speed up the decoding process, such 
as node-based or span-based beam search with 
different pruning strategies (Liu et al, 2006; 
Zhang et al, 2008a, 2008b) and cube pruning 
(Huang and Chiang, 2007; Mi et al, 2008). How-
ever, the first step attracts less attention. The pre-
vious solution to this problem is to do exhaustive 
searching with heuristics on each tree/forest node 
or on each source span. This solution becomes 
computationally infeasible when it is applied to 
packed forests with loose pruning threshold or rule 
sets with large tree fragments of large rule height 
and width. This not only overloads the translation 
process but also compromises the translation per-
formance since as shown in our experiments the 
large tree fragment rules are also very useful.  
To solve the above issue, in this paper, we pro-
pose a hyper-tree-based fast algorithm for transla-
tion rule matching. Our solution includes two 
steps. In the first step, all the translation rules are 
re-organized using our proposed hyper-tree struc-
ture, which is a compact representation of the en-
tire translation rule set, in order to make the com-
mon parts of translation rules shared as much as 
possible. This enables the common parts of differ-
ent translation rules to be visited only once in rule 
matching. Please note that the first step can be 
easily done off-line very fast. As a result, it does 
not consume real translation time. In the second 
step, we design a recursive algorithm to traverse 
the hyper-tree structure and the input source forest 
in a top-down manner to do the rule matching be-
tween them. As we will show later, the hyper-tree 
structure and the recursive algorithm significantly 
improve the speed of the rule matching and the 
entire translation process compared with previous 
methods. 
With the proposed algorithm, we are able to 
carry out experiments with very loose pruning 
1037
thresholds and larger tree fragment rules effi-
ciently. Experimental results on the NIST MT-
2003 Chinese-English translation task shows that 
our algorithm is 19 times faster in rule matching 
and is able to save 57% of overall translation time 
over previous methods when using large fragment 
translation rules with height up to 5. It also shows 
that the larger rules with height of up to 5 signifi-
cantly outperforms the rules with height of up to 3 
by around 1 BLEU score. 
The rest of this paper is organized as follows. 
Section 2 introduces the syntax-based translation 
system that we are working on. Section 3 reviews 
the previous work. Section 4 explains our solution 
while section 5 reports the experimental results. 
Section 6 concludes the paper. 
2 Syntax-based Translation 
This section briefly introduces the forest/tree-
based tree-to-string translation model which 
serves as the translation platform in this paper. 
2.1 Tree-to-string model 
   
                                                    
 
                                                      
XNA declaration is related to some regulation 
 
Figure 1. A tree-to-string translation process. 
 
The tree-to-string model (Galley et al 2004; Liu et 
al. 2006) views the translation as a structure map-
ping process, which first breaks the source syntax 
tree into many tree fragments and then maps each 
tree fragment into its corresponding target transla-
tion using translation rules, finally combines these 
target translations into a complete sentence. Fig. 1 
illustrates this process. In real translation, the 
number of possible tree fragment segmentations 
for a given input tree is exponential in the number 
of tree nodes.  
2.2 Forest-based translation 
To overcome parse error for SMT, Mi and Huang 
(2008) propose forest-based translation by using a 
packed forest instead of a single syntax tree as the 
translation input. A packed forest (Tomita 1987; 
Klein and Manning, 2001; Huang and Chiang, 
2005) is a compact representation of many possi-
ble parse trees of a sentence, which can be for-
mally described as a triple , where V is 
the set of non-terminal nodes, E is the set of hy-
per-edges and S is a sentence represented as an 
ordered word sequence. A hyper-edge in a packed 
forest is a group of edges in a tree which connects 
a father node to all its children nodes, representing 
a CFG-based parse rule. Fig. 2 is a packed forest 
incorporating two parse trees T1 and T2 of a sen-
tence as shown in Fig. 3 and Fig. 4. Given a hy-
per-edge e, let h be its father node, then we say 
that e is attached to h. 
A non-terminal node in a packed forest can be 
represented as ?label [start, stop]?, where ?label? 
is its syntax category and ?[start, stop]? is the 
range of words it covers. For example, the node in 
Fig. 5 pointed by the dark arrow is labelled as 
?NP[3,4]?, where NP is its label and [3,4] means 
that it covers the span from the 3rd word to the 4th  
word. In forest-based translation, rule matching is 
much more complicated than the tree-based one.  
 
 
 
Figure 2. A packed forest 
 
Zhang et al (2009) reduce the tree sequence 
problem into tree problem by introducing virtual 
node and related forest conversion algorithms, so 
1038
the algorithm proposed in this paper is also appli-
cable to the tree sequence-based models. 
 
     
 
Figure 3. Tree 1 (T1)      Figure 4. Tree 2 (T2) 
3 Matching Methods in Previous Work  
In this section, we discuss the two typical rule 
matching algorithms used in previous work. 
3.1 Exhaustive search by tree fragments 
This method generates all possible tree fragments 
rooted by each node in the source parse tree or 
forest, and then matches all the generated tree 
fragments against the source parts (left hand side) 
of translation rules to extract the useful rules 
(Zhang et al, 2008a).  
 
 
 
Figure 5. Node NP[3,4] in packed forest 
 
 
 
Figure 6. Candidate fragments on NP[3,4] 
For example, if we want to extract useful rules 
for node NP[3,4] in Fig 5, we have to generate all 
the tree fragments rooted at node NP[3,4] as 
shown in Fig 6, and then query each fragment in 
the rule set. Let  be a node in the packed forest, 
 represents the number of possible tree frag-
ments rooted at node , then we have: 
 
 
?
?? ?? ??? 
??? ????????
 ???? ?? ?
? ?? ? ?????????? 
???????? ?? ?
 
 
 
The above equation shows that the number of 
tree fragments is exponential to the span size, the 
height and the number of hyper-edges it covers. In 
a real system, one can use heuristics, e.g. the max-
imum number of nodes and the maximum height 
of fragment, to limit the number of possible frag-
ments. However, these heuristics are very subjec-
tive and hard to optimize. In addition, they may 
filter out some ?good? fragments.  
3.2 Exhaustive search by rules 
This method does not generate any source tree 
fragments. Instead, it does top-down recursive 
matching from each node one-by-one with each 
translation rule in the rule set (Mi and Huang 
2008). 
For example, given a translation rule with its 
left hand side as shown in Fig. 7, the rule match-
ing between the given rule and the node IP[1,4] in 
Fig. 2 can be done as follows.  
1. Decompose the left hand side of the transla-
tion rule as shown in Fig. 7 into a sequence of hy-
per-edges in top-down, left-to-right order as fol-
lows: 
IP => NP VP;  NP => NP NP;  NP => NN; 
NN => ?? 
 
 
 
Figure 7. The left hand side of a rule 
 
2. Pattern match these hyper-edges(rule) one-
by-one in top-down left-to-right order from node 
IP[1,4]. If there is a continuous path in the forest 
matching all of these hyper-edges in order, then 
we can say that the rule is useful and matchable 
1039
with the tree fragment covered by the continuous 
path. The following illustrates the matching steps: 
1. Match hyper-edge ?IP => NP VP? with node 
IP[1,4]. There are two hyper-edges in the forest 
matching it: ?IP[1,4] => NP[1,1] VP[2,4]? and 
?IP[1,4] => NP[1,2] VP [3,4]?, which generates 
two candidate paths. 
2. Since hyper-edge ?NP => NP NP? fails to 
match NP[1,1], the path initiated with ?IP[1,4] => 
NP[1,1] VP[2,4]? is pruned out. 
3.  Since there is a hyper-edge ?NP[1,2] => 
NP[1,1] NP[2,2]? matching ?NP => NP NP? on 
NP[1,2], then continue for further matching. 
4. Since ?NP=>NN? on NP[2,2] matches 
?NP[2,2] => NN[2,2]?, then continue for further 
matching. 
5. ?NN=>??? on NN[2,2] matches ?NN[2,2] 
=>??? and it is the last hyper-edge in the input 
rules. Finally, there is one continuous path suc-
cessfully matching the left hand side of the input 
rule.  
This method is able to avoid the exponential 
problem of the first method as described in the 
previous subsection. However, it has to do one-by-
one pattern matching for each rule on each node. 
When the rule set is very large (indeed it is very 
large in the forest-based model even with a small 
training set), it becomes very slow, and even much 
slower than the first method. 
4 The Proposed Hyper-tree-based Rule 
Matching Algorithm 
In this section, we first explain the motivation why 
we re-organize the translation rule sets, and then 
elaborate how to re-organize the translation rules 
using our proposed hyper-tree structure. Finally 
we discuss the top-down rule matching algorithm 
between forest and hyper-tree.  
4.1 Motivation 
 
 
              Figure 8.  Two rules? left hand side 
 
 
Figure 9. Common part of the two rules? left hand  
sides in Figure 8 
 
Fig. 9 shows the common part of the left hand 
sides of two translation rules as shown in Fig. 8. 
In previous rule matching algorithm, the common 
parts are matched as many times as they appear in 
the rule set, which reduces the rule matching 
speed significantly. This motivates us to propose 
the hyper-tree structure and the rule matching al-
gorithm to make the common parts shared by mul-
tiple translation rules to be visited only once in the 
entire rule matching process. 
4.2 Hyper-node, hyper-path and hyper-tree 
A hyper-tree is a compact representation of a 
group of tree translation rules with common parts 
shared. It consists of a set of hyper-nodes with 
edges connecting different hyper-nodes into a big 
tree. A hyper-tree is constructed from the transla-
tion rule sets in two steps: 
1) Convert each tree translation rule into a hy-
per-path; 
2) Construct the hyper-tree by incrementally 
adding each individual hyper-path into the 
hyper-tree. 
A tree rule can be converted into a hyper-path 
without losing information. Fig. 10 demonstrates 
the conversion process:  
1) We first fill the rule tree with virtual nodes  
to make all its leaves have the same depth 
to the root; 
2) We then group all the nodes in the same 
tree level to form a single hyper-node, 
where we use a comma as a delimiter to 
separate the tree nodes with different father 
nodes; 
3) A hyper-path is a set of hyper-nodes linked 
in a top-down manner. 
The commas and virtual nodes  are introduced 
to help to recover the original tree from the hyper-
path. Given a tree node in a hyper-node, if there 
are n commas before it, then its father node is the 
(n+1)th tree node in the father hyper-node. If we 
could find father node for each node in hyper-
nodes, then it is straightforward to recover the 
original tree from the hyper-path by just adding 
the edges between original father and children 
nodes except the virtual node .  
1040
After converting each tree rule into a hyper-
path, we can organize the entire rule set into a big 
hyper-tree as shown in Figure 11. The concept of 
hyper-path and hyper-tree could be viewed as an 
extension of the "prefix merging" ideas for CFG 
rules (Klein and Manning 2001). 
 
         
 
 
 
 
Figure 10. Convert tree to hyper-path 
 
 
 
Figure 11. A hyper-tree example 
 
Algorithm 1 shows how to organize the rule set 
into a big hyper-tree. The general process is that 
for each rule we convert it into a hyper-path and 
then add the hyper-path into a hyper-tree incre-
mentally. However, there are many different hy-
per-trees generated given a big rule set. We then 
introduce a TOP label as the root node to link all 
the individual hyper-trees to a single big hyper-
tree. Algorithm 2 shows the process of adding a 
hyper-path into a hyper-tree. Given a hyper-path, 
we do a top-down matching between the hyper-
tree and the input hyper-path from root hyper-
node until a leaf hyper-node is reached or there is 
no matching hyper-node at some level found. 
Then we add the remaining unmatchable part of 
the input hyper-path as the descendants of the last 
matchable hyper-node. 
Please note that in Fig. 10 and Fig. 11, we ig-
nore the target side (right hand side) of translation 
rules for easy discussion. Indeed, we can easily 
represent all the complete translation rules (not 
only left hand side) in Fig. 11 by simply adding 
the corresponding rule target sides into each hy-
per-node as done by line 5 of Algorithm 1.  
Any hyper-path from the root to any hyper-
node (not necessarily be a leaf of the hyper-tree) 
in a hyper-tree can represent a tree fragment. As a 
result, the hyper-tree in Fig. 11 can represent up to 
6 candidate tree fragments. It is easy to understand 
that the maximum number of tree fragments that a 
hyper-tree can represent is equal to the number of 
hyper-nodes in it except the root. It is worth not-
ing that a hyper-node in a hyper-tree without any 
target side rule attached means there is no transla-
tion rule corresponding to the tree fragment repre-
sented by the hyper-path from the root to the cur-
rent hyper-node. The compact representation of 
the rule set by hyper-tree enables a fast algorithm 
to do translation rule matching. 
 
Algorithm 1. Compile rule set into hyper-tree 
Input: rule set 
Output: hyper-tree 
 
1.  Initialize hyper-tree as a TOP node  
2.  for  each rule in rule set  do 
3.          Convert the left hand side tree to a hyper-path p 
4.          Add hyper-path p into hyper-tree 
5. Add rule?s right hand side to the leaf hyper-node of  
a hyper-path in the hyper-tree  
6. end for 
 
Algorithm  2. Add hyper-path into hyper-tree 
Input: hyper-path p and hyper-tree t 
Notation:  
   h: the height of hyper-path p 
   p(i) : the hyper-node of ith level (top-down) of p 
   TN: the hyper-node in hyper-tree  
Output: updated hyper-tree t  
 
1. Initialize TN as TOP 
2. for  i := 1 to h  do 
3.       if there is a child c of TN has the same label as p(i)    
              then 
4.             TN := c 
5.       else  
6.             Add a child c to TN, label c as p(i) 
7.             TN := c 
4.3 Translation rule matching between forest 
and hyper-tree 
Given the source parse forest and the translation 
rules represented in the hyper-tree structure, here 
we present a fast matching algorithm to extract so-
called useful translation rules from the entire rule 
set in a top-down manner for each node of the for-
est.  
As shown in Algorithm 3, the general process 
of the matching algorithm is as follows: 
 
1041
Algorithm 3. Rule matching on one node  
Input: hyper-tree T, forest F, and node n 
Notation:   
      FP: a pair <FNS, TN>, FNS is the frontier nodes of      
             matched tree fragment,  
             TN is the hyper-tree node matching it 
      SFP: the queue of FP 
Output: Available rules on node n 
 
1. if there is no child c of TOP having the same label as n      
   then 
2.        Return failure. 
3. else  
4.      Initialize FP as <{n},c> and put it into SFP 
5.      for each FP in SFP do 
6.                 SFP ? PropagateNextLevel(FP.FNS, FP.TN)  
7.      for each FP in SFP do 
8.          if the rule set attached to FP.TN is not empty   
         then 
9.               Add FP to result 
 
Algorithm 4. PropagateNextLevel  
Input: Frontier node sequence FNS, hyper-tree node TN 
Notation: 
           CT: a child node of TN 
                  the number of node sequence (separated by  
                  comma, see Fig 11) in CT is equal to the number  
                  of node in TN.   
           CT(i) : the ith node sequence in hyper-node CT 
           FNS(i): the ith node in FNS 
           TFNS: the temporary set of frontier node sequence 
           RFNS: the result set of frontier node sequence  
           FP:  a pair of frontier node sequence  
                   and hyper-tree node 
           RFP: the result set of FP 
Output: RFP  
 
1. for each child hyper-node CT of TN do 
2.        for i:= 1 to the number of node sequence in CT do 
3.              empty TFNS 
4.              if CT(i) ==  then 
5.                      Add FNS(i) to TFNS. 
6.              else 
7.                   for each hyper-edge e attached to FNS(i) do 
8.                         if e.children match CT(i) then 
9.                                Add e.children to TFNS 
10.              if TFNS is empty then 
11.                      empty RFNS 
12.                      break 
13.              else if i == 1 then  
14.                       RFNS := TFNS 
15.              else  
16.                       RFNS := RFNS  TFNS 
17.        for each FNS in RFNS do 
18.                add <FNS, CT > into RFP 
 
1) For each node n of the source forest if no 
child node of TOP in hyper-tree has the same label 
with it, it means that no rule matches any tree 
fragments rooted at the node n (i.e., no useful 
rules to be used for the node n) (line 1-2) 
2) Otherwise, we match the sub-forest starting 
from the node n against a sub-hyper-tree starting 
from the matchable child node of TOP layer by 
layer in a top-down manner. There may be many 
possible tree fragments rooted at node n and each 
of them may have multiple useful translation rules. 
In our implementation, we maintain a data struc-
ture of FP = <FNS, TN> to record the currently 
matched tree fragment of forest and its corres-
ponding hyper-tree node in the rule set, where 
FNS is the frontier node set of the current tree 
fragment and TN is the hyper-tree node. The data 
structure FP is used to help extract useful transla-
tion rules and is also used for further matching of 
larger tree fragments. Finally, all the FPs for the 
node n are kept in a queue. During the search, the 
queue size is dynamically increased. The matching 
algorithm terminates when all the FPs have been 
visited (line 5-6 and Algorithm 4). 
3) In the final queue, each element (FP) of the 
queue contains the frontier node sequence of the 
matched tree fragment and its corresponding hy-
per-tree node. If the target side of a rule in the hy-
per-tree node is not empty, we just output the 
frontier nodes of the matched tree fragment, its 
root node n and all the useful translation rules for 
later translation process. 
Algorithm 4 describes the detailed process of 
how to propagate the matching process down to 
the next level.  <FNS, TN> is the current level 
frontier node sequence and hyper-tree node. Given 
a child hyper-node CT of TN (line 1), we try to 
find the group of next level frontier node sequence 
to match it (line 2-18). As shown in Fig 11, a hy-
per-node consists of a sequence of node sequence 
with comma as delimiter. For the ith node se-
quence CT(i) in CT, If CT(i) is , that means 
FNS(i) is a leaf/frontier node in the matched tree 
fragment and thus no need to propagate to the next 
level (line 4-5). Otherwise, we try each hyper-
edge e of FNS(i) to see whether its children match 
CT(i), and put the children of the matched hyper-
edge into a temp set TFNS (line 7-9). If the temp 
set is empty, that means the current matching fails 
and no further expansion needs (line 10-12). Oth-
erwise, we integrate current matched children into 
the final group of frontier node sequence (line 13-
16) by Descartes Product ( ). Finally, we con-
struct all the <FNS, TN> pair for next level 
matching (line 17-18). 
It would be interesting to study the time com-
plexity of our Algorithm 3 and 4. Suppose the 
maximum number of children of each hyper-node 
in hyper-tree is N (line 1), the maximum number 
of node sequence in CT is M (line 2), the maxi-
mum number of hyper-edge in each node in 
packed forest is K (line 7), the maximum number 
of hyper-edge with same children representation 
in each node in packed forest is C (i.e. the maxi-
mum size of TFNS in line 16, and the maximum 
complexity of the Descartes Product in line 16 
1042
would be CM), then the time complexity upper-
bound of Algorithm 4 is O(NM(K+CM)). For Al-
gorithm 3, its time complexity is O(RNM(K+CM)), 
where R is the maximum number of tree fragment 
matched in each node.  
5 Experiment 
5.1 Experimental settings 
We carry out experiment on Chinese-English 
NIST evaluation tasks. We use FBIS corpus 
(250K sentence pairs) as training data with the 
source side parsed by a modified Charniak parser 
(Charniak 2000) which can output a packed forest. 
The Charniak Parser is trained on CTB5, tuned on 
301-325 portion, with F1 score of 80.85% on 271-
300 portion. We use GIZA++ (Och and Ney, 2003) 
to do m-to-n word-alignment and adopt heuristic 
?grow-diag-final-and? to do refinement. A 4-gram 
language model is trained on Gigaword 3 Xinhua 
portion by SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing. We use NIST 2002 as 
development set and NIST 2003 as test set. The 
feature weights are tuned by the modified Koehn?s 
MER (Och, 2003, Koehn, 2007) trainer. We use 
case-sensitive BLEU-4 (Papineni et al, 2002) to 
measure the quality of translation result. Zhang et 
al. 2004?s implementation is used to do significant 
test. 
Following (Mi and Huang 2008), we use viterbi 
algorithm to prune the forest. Instead of using a 
static pruning threshold (Mi and Huang 2008), we 
set the threshold as the distance of the probabili-
ties of the nth best tree and the 1st best tree. It 
means the pruned forest is able to at least keep all 
the top n best trees. However, because of the shar-
ing nature of the packed forest, it may still contain 
a large number of additional trees. Our statistic 
shows that when we set the threshold as the 100th 
best tree, the average number of all possible trees 
in the forest is 1.2*105 after pruning. 
In our experiments, we compare our algorithm 
with the two traditional algorithms as discussed in 
section 3. For the ?Exhaustive search by tree? al-
gorithm, we use a bottom-up dynamic program-
ming algorithm to generate all the candidate tree 
fragments rooted at each node. For the ?Exhaus-
tive search by rule? algorithm, we group all rules 
with the same left hand side in order to remove the 
duplicated matching for the same left hand side 
rules. All these settings aim for fair comparison. 
5.2 Accuracy, speed vs. rule heights 
We first compare the three algorithms? perfor-
mance by setting the maximum rule height from 1 
to 5. We set the forest pruning threshold to the 
100th best parse tree.  
Table 1 compares the speed of the three algo-
rithms. It clearly shows that the speed of both of 
the two traditional algorithms increases dramati-
cally while the speed of our hyper-tree based algo-
rithm is almost linear to the tree height. In the case 
of rule height of 5, the hyper-tree algorithm is at 
least 19 times (9.329/0.486) faster than the two 
traditional algorithms and saves 8.843(9.329 - 
0.486) seconds in rule matching for each sentence 
on average, which contributes 57% (8.843/(9.329 
+ 6.21)) speed improvement to the overall transla-
tion.  
 
H 
Rule Matching 
D Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree-
based 
1 0.043 0.077 0.083   2.96 
2 0.047 0.920 0.173   3.56 
3 0.237 9.572 0.358   4.02 
4 2.300 48.90 0.450   5.27 
5 9.329 90.80 0.486   6.21 
 
Table 1. Speed in seconds per sentence vs. rule 
height; ?H? is rule height, ?D? represents the de-
coding time after rule matching 
 
 
Height BLEU 
1 0.1646 
2 0.2498 
3 0.2824 
4 0.2874 
5 0.2925 
Moses 0.2625 
 
Table 2. BLEU vs. rule height 
 
Table 2 reports the BLEU score with different 
rule heights, where Moses, a state-of-the-art 
phrase-based SMT system, serves as the baseline 
system.  It shows the BLEU score consistently 
improves as the rule height increases. In addition, 
one can see that the rules with maximum height of 
5 are able to outperform the rules with maximum 
height of 3 by 1 BLEU score (p<0.05) and signifi-
cantly outperforms Moses by 3 BLEU score 
(p<0.01). To our knowledge, this is the first time 
to report the performance of rules up to height of 5 
for forest-based translation model.  
1043
We also study the distribution of the rules used 
in the 1-best translation output. The results are 
shown in Table 3; we could see something inter-
esting that is as the rule height increases, the total 
number of rules with that height decreases, while 
the percentage of partial-lexicalized increases 
dramatically. And one thing needs to note is the 
percentage of partial-lexicalized rules with height 
of 1 is 0, since there is no partial-lexicalized rule 
with height of 1 in the rule set (the father node of 
a word is a pos tag node).  
 
H Total 
Rule Type Percentage (%) 
F P U 
1 9814   76.58     0 23.42 
2 5289   44.99     46.40 8.60 
3 3925   18.39     77.25 4.35 
4 1810   7.90      87.68 4.41 
5 511    6.46 90.50 3.04 
 
Table 3. statistics of rules used in the 1-best trans-
lation output, ?F? means full-lexicalized, ?P? 
means partial-lexicalized, ?U? means unlexiclaizd. 
5.3 Speed vs. forest pruning threshold 
This section studies the impact of the forest prun-
ing threshold on the rule matching speed when 
setting the maximum rule height to 5. 
 
Threshold 
Rule Matching  
Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree- 
based 
1 1.2 23.66 0.171 
10 3.1 36.42 0.234 
50 5.7 66.20 0.405 
100 9.3 90.80 0.486 
200 27.3 104.86 0.598 
500 133.6 148.54 0.873 
 
Table 4. Speed in seconds per sentence vs. for-
est  pruning threshold 
 
In Table 4, we can see that our hyper-tree based 
algorithm is the fastest among the three algorithms 
in all pruning threshold settings and even 150 
times faster than both of the two traditional algo-
rithms with threshold of 500th best. Table 5 shows 
the average number of parse trees embedded in a 
packed forest with different pruning thresholds per 
sentence. We can see that the number of trees in-
creases exponentially when the pruning threshold 
increases linearly. When the threshold is 500th best, 
the average number of trees per sentence is 
1.49*109. However, even in this extreme case, the 
hyper-tree based algorithm is still capable of com-
pleting rule matching within 1 second.  
 
Threshold Number of Trees  
1 1 
10 32 
50 5922 
100 128860 
200 2.75*106 
500 1.49*109 
 
Table 5. Average number of trees in packed 
forest with different pruning threshold. 
5.4 Hyper-tree compression rate 
As we describe in section 4.2, theoretically the 
number of tree fragments that a hyper-tree can 
represent is equal to the number of hyper-nodes in 
it. However, in real rule set, there is no guarantee 
that each tree fragment in the hyper-tree has cor-
responding translation rules. To gain insights into 
how effective the compact representation of the 
hyper-tree and how many hyper-nodes without 
translation rules, we define the compression rate 
as follows.  
 
 
 
 
Table 6 reports the different statistics on the 
rule sets with different maximum rule heights 
ranging from 1 to 5. The reported statistics are the 
number of rules, the number of unique left hand 
side (since there may be more than one rules hav-
ing the same left hand side), the number of hyper-
nodes and the compression rate.  
 
H n_rules n_LHS n_nodes c_rate 
1 21588 10779 10779 100% 
2 141632 51807 51903 99.8% 
3 1.73*106 491268 494919 99.2% 
4 8.65*106 2052731 2083296 98.5% 
5 1.89*107 3966742 4043824 98.1% 
 
Table 6. Statistics of rule set and hyper-tree. ?H? 
is rule height, ?n_rules? is the number of rules, 
?n_LHS? is the number of unique left hand side, 
?n_nodes? is the number of hyper-nodes in hyper-
tree and ?c_rate? is the compression rate. 
 
Table 6 shows that in all the five cases, the 
compression rates of hyper-tree are all more than 
1044
98%. It means that almost all the tree fragments 
embedded in the hyper-tree have corresponding 
translation rules. As a result, we are able to use 
almost only one hyper-edge (i.e. only the frontier 
nodes of a tree fragment without any internal 
nodes) to represent all the rules with the same left 
hand side. This suggests that our hyper-tree is par-
ticularly effective in representing the tree transla-
tion rules compactly. It also shows that there are a 
lot of common parts among different translation 
rules. 
All the experiments reported in this section 
convincingly demonstrate the effectiveness of our 
proposed hyper-tree representation of translation 
rules and the hyper-tree-based rule matching algo-
rithm. 
6 Conclusion   
In this paper2, we propose the concept of hyper-
tree for compact rule representation and a hyper-
tree-based fast algorithm for translation rule 
matching in a forest-based translation system. We 
compare our algorithm with two previous widely-
used rule matching algorithms.  Experimental re-
sults on the NIST Chinese-English MT 2003 eval-
uation data set show the rules with maximum rule 
height of 5 outperform those with height 3 by 1.0 
BLEU and outperform MOSES by 3.0 BLEU. In 
the same test cases, our algorithm is at least 19 
times faster than the two traditional algorithms, 
and contributes 57% speed improvement to the 
overall translation. We also show that in a more 
challenging setting (forest containing 1.49*109 
trees on average) our algorithm is 150 times faster 
than the two traditional algorithms. Finally, we 
show that the hyper-tree structure has more than 
98% compression rate. It means the compact re-
presentation by the hyper-tree is very effective for 
translation rules. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04.  
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. ACL-07. 144?151 
                                                           
The corresponding authors of this paper are Hui Zhang 
(zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Dan Klein and Christopher D. Manning. 2001. Parsing 
with Treebank Grammars: Empirical Bounds, Theo-
retical Models, and the Structure of the Penn Tree-
bank. ACL - 2001. 338-345. 
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. CL: J99-4005. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1) 19-51  
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02.311-318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational 
Linguistics 13(1-2): 31-46. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009. Forest-based Tree Sequence to 
String Translation Model. ACL-IJCNLP-09. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008a. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. ACL-
HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng 
Li. 2008b. Grammar Comparison Study for Transla-
tional Equivalence Modeling and Statistical Ma-
chine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04 
1045
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552?1560,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
K-Best Combination of Syntactic Parsers  
 
Hui Zhang1, 2   Min Zhang1   Chew Lim Tan2   Haizhou Li1   
1Institute for Infocomm Research                 2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
Abstract 
In this paper, we propose a linear model-based 
general framework to combine k-best parse 
outputs from multiple parsers. The proposed 
framework leverages on the strengths of pre-
vious system combination and re-ranking 
techniques in parsing by integrating them into 
a linear model. As a result, it is able to fully 
utilize both the logarithm of the probability of 
each k-best parse tree from each individual 
parser and any additional useful features. For 
feature weight tuning, we compare the simu-
lated-annealing algorithm and the perceptron 
algorithm. Our experiments are carried out on 
both the Chinese and English Penn Treebank 
syntactic parsing task by combining two state-
of-the-art parsing models, a head-driven lexi-
calized model and a latent-annotation-based 
un-lexicalized model. Experimental results 
show that our F-Scores of 85.45 on Chinese 
and 92.62 on English outperform the previ-
ously best-reported systems by 1.21 and 0.52, 
respectively. 
1 Introduction 
Statistical models have achieved great success in 
language parsing and obtained the state-of-the-
art results in a variety of languages. In general, 
they can be divided into two major categories, 
namely lexicalized models (Collins 1997, 1999; 
Charniak 1997, 2000) and un-lexicalized models 
(Klein and Manning 2003; Matsuzaki et al 2005; 
Petrov et al 2006; Petrov and Klein 2007). In 
lexicalized models, word information play a key 
role in modeling grammar rule generation, while 
un-lexicalized models usually utilize latent in-
formation derived from the parse structure diver-
sity. Although the two models are different from 
each other in essence, both have achieved state-
of-the-art results in a variety of languages and 
are complementary to each other (this will be 
empirically verified later in this paper). There-
fore, it is natural to combine the two models for 
better parsing performance.  
Besides individual parsing models, many sys-
tem combination methods for parsing have been 
proposed (Henderson and Brill 1999; Zeman and 
?abokrtsk? 2005; Sagae and Lavie 2006) and 
promising performance improvements have been 
reported. In addition, parsing re-ranking (Collins 
2000; Riezler et al 2002; Charniak and Johnson 
2005; Huang 2008) has also been shown to be 
another effective technique to improve parsing 
performance. This technique utilizes a bunch of 
linguistic features to re-rank the k-best (Huang 
and Chiang 2005) output on the forest level or 
tree level. In prior work, system combination 
was applied on multiple parsers while re-ranking 
was applied on the k-best outputs of individual 
parsers. 
In this paper, we propose a linear model-based 
general framework for multiple parsers combina-
tion. The proposed framework leverages on the 
strengths of previous system combination and re-
ranking methods and is open to any type of fea-
tures. In particular, it is capable of utilizing the 
logarithm of the parse tree probability from each 
individual parser while previous combination 
methods are unable to use this feature since the 
probabilities from different parsers are not com-
parable. In addition, we experiment on k-best 
combination while previous methods are only 
verified on 1-best combination. Finally, we apply 
our method in combining outputs from both the 
lexicalized and un-lexicalized parsers while pre-
vious methods only carry out experiments on 
multiple lexicalized parsers. We also compare 
two learning algorithms in tuning the feature 
weights for the linear model. 
We perform extensive experiments on the 
Chinese and English Penn Treebank corpus. Ex-
perimental results show that our final results, an 
F-Score of 92.62 on English and 85.45 on Chi-
nese, outperform the previously best-reported 
systems by 0.52 point and 1.21 point, respec-
tively. This convincingly demonstrates the effec-
tiveness of our proposed framework. Our study 
also shows that the simulated-annealing algo-
rithm (Kirkpatrick et al 1983) is more effective 
1552
than the perceptron algorithm (Collins 2002) for 
feature weight tuning. 
The rest of this paper is organized as follows. 
Section 2 briefly reviews related work. Section 3 
discusses our method while section 4 presents 
the feature weight tuning algorithm. In Section 5, 
we report our experimental results and then con-
clude in Section 6. 
2 Related Work  
As discussed in the previous section, system 
combination and re-ranking are two techniques 
to improve parsing performance by post-
processing parsers? k-best outputs.  
Regarding the system combination study, 
Henderson and Brill (1999) propose two parser 
combination schemes, one that selects an entire 
tree from one of the parsers, and one that builds a 
new tree by selecting constituents suggested by 
the initial trees. According to the second scheme, 
it breaks each parse tree into constituents, calcu-
lates the count of each constituent, then applies 
the majority voting to decide which constituent 
would appear in the final tree. Sagae and Lavie 
(2006) improve this second scheme by introduc-
ing a threshold for the constituent count, and 
search for the tree with the largest number of 
count from all the possible constituent combina-
tion. Zeman and ?abokrtsk? (2005) study four 
combination techniques, including voting, stack-
ing, unbalanced combining and switching, for 
constituent selection on Czech dependency pars-
ing. Promising results have been reported in all 
the above three prior work. Henderson and Brill 
(1999) combine three parsers and obtained an F1 
score of 90.6, which is better than the score of 
88.6 obtained by the best individual parser as 
reported in their paper. Sagae and Lavie (2006) 
combine 5 parsers to obtain a score of 92.1, 
while they report a score of 91.0 for the best sin-
gle parser in their paper. Finally, Zeman and 
?abokrtsk? (2005) reports great improvements 
over each individual parsers and show that a 
parser with very low accuracy can also help to 
improve the performance of a highly accurate 
parser. However, there are two major limitations 
in these prior works. First, only one-best output 
from each individual parsers are utilized. Second, 
none of these works uses the parse probability of 
each parse tree output from the individual parser.  
Regarding the parser re-ranking, Collins (2000) 
proposes a dozen of feature types to re-rank k-
best outputs of a single head-driven parser. He 
uses these feature types to extract around half a 
million different features on the training set, and 
then examine two loss functions, MRF and 
Boosting, to do feature selection. Charniak and 
Johnson (2005) generate a more accurate k-best 
output and adopt MaxEnt method to estimate the 
feature weights for more than one million fea-
tures extracted from the training set. Huang 
(2008) further improves the re-ranking work of 
Charniak and Johnson (2005) by re-ranking on 
packed forest, which could potentially incorpo-
rate exponential number of k-best list. The re-
ranking techniques also achieve great improve-
ment over the original individual parser. Collins 
(2002) improves the F1 score from 88.2% to 
89.7%, while Charniak and Johnson (2005) im-
prove from 90.3% to 91.4%. This latter work 
was then further improved by Huang (2008) to 
91.7%, by utilizing the benefit of forest structure. 
However, one of the limitations of these tech-
niques is the huge number of features which 
makes the training very expensive and inefficient 
in space and memory usage.  
3 K-best Combination of Lexicalized 
and Un-Lexicalized Parsers with 
Model Probabilities 
In this section, we first introduce our proposed k-
best combination framework. Then we apply this 
framework to the combination of two state-of-
the-art lexicalized and un-lexicalized parsers 
with an additional feature inspired by traditional 
combination techniques. 
3.1 K-best Combination Framework 
Our proposed framework consists of the follow-
ing steps: 
1) Given an input sentence and N different 
parsers, each parser generates K-best parse 
trees. 
2) We combine the N*K output trees and 
remove any duplicates to obtain M unique 
tress. 
3) For each of the M unique trees, we re-
evaluate it with all the N models which are 
used by the N parsers. It is worth noting 
that this is the key point (i.e. one of the 
major advantages) of our method since 
some parse trees are only generated from 
one or I (I<N) parsers. For example, if a 
tree is only generated from head-driven 
lexicalized model, then it only has the 
head-driven model score. Now we re-
evaluate it with the latent-annotation un-
lexicalized model to reflect the latent-
1553
annotation model?s confidence for this 
tree. This enables our method to effec-
tively utilize the confidence measure of all 
the individual models without any bias. 
Without this re-evaluation step, the previ-
ous combination methods are unable to 
utilize the various model scores. 
4) Besides model scores, we also compute 
some additional feature scores for each 
tree, such as the widely-used ?constituent 
count? feature. 
5) Then we adopt the linear model to balance 
and combine these feature scores and gen-
erate an overall score for each parse tree.  
6) Finally we re-rank the M best trees and 
output the one with the highest score. 
 
 
? ? ? ? ? ?
? ?  
 
The above is the linear function used in our 
method, where t is the tree to be evaluated,  to 
 are the model confidence scores (in this paper, 
we use logarithm of the parse tree probability) 
from the N models,  to  are their weights, 
?  to ?  are the L additional features, ?  to ?  
are their weights.  
In this paper, we employ two individual pars-
ing model scores and only one additional feature. 
Let  be the head-driven model score,  be the 
latent-annotation model score, ?  be the consti-
tuent count feature and ?  is the weight of fea-
ture ? .  
3.2 Confidences of Lexicalized and Un-
lexicalized Model 
The term ?confidence? was used in prior parser 
combination studies to refer to the accuracy of 
each individual parser. This reflects how much 
we can trust the parse output of each parser. In 
this paper, we use the term ?confidence? to refer 
to the logarithm of the tree probability computed 
by each model, which is a direct measurement of 
the model?s confidence on the target tree being 
the best or correct parse output. In fact, the fea-
ture weight ? in our linear model functions simi-
larly as the traditional ?confidence?. However, 
we do not directly use parser?s accuracy as its 
value. Instead we tune it automatically on devel-
opment set to optimize it against the parsing per-
formance directly. In the following, we introduce 
the state-of-the-art head-driven lexicalized and 
latent-annotation un-lexicalized models (which 
are used as two individual models in this paper), 
and describe how they compute the tree probabil-
ity briefly. 
Head-driven model is one of the most repre-
sentative lexicalized models. It attaches the head 
word to each non-terminal and views the genera-
tion of each rule as a Markov process first from 
father to head child, and then to the head child?s 
left and right siblings. 
Take following rule r as example,  
 
 
 
 is the rule?s left hand side (i.e. father label), 
 is the head child,  is M?s left sibling and  
is M?s right sibling. Let h be M?s head word, the 
probability of this rule is 
 
 
 
The probability of a tree is just the product of the 
probabilities of all the rules in it. The above is 
the general framework of head-driven model. For 
a specific model, there may be some additional 
features and modification. For example, the 
model2 in Collins (1999) introduces sub-
categorization and model3 introduces gap as ad-
ditional features. Charniak (2000)?s model intro-
duces pre-terminal as additional features. 
The latent-annotation model (Matsuzaki et al 
2005; Petrov et al 2006) is one of the most ef-
fective un-lexicalized models. Briefly speaking, 
latent-annotation model views each non-terminal 
in the Treebank as a non-terminal followed by a 
set of latent variables, and uses EM algorithms to 
automatically learn the latent variables? probabil-
ity functions to maximize the probability of the 
given training data. Take the following binarized 
rule as example, 
 
 
 
could be viewed as the set of rules  
 
 
 
The process of computing the probability of a 
normal tree is to first binarized all the rules in it, 
and then replace each rule to the corresponding 
set of rules with latent variables. Now the pre-
vious tree becomes a packed forest (Klein and 
Manning 2001; Petrov et al 2007) in the latent-
annotation model, and its probability is the inside 
probability of the root node. This model is quite 
different from the head-driven model in which 
1554
the probability of a tree is just the product all the 
rules? probability. 
3.3 Constituent Counts 
Besides the two model scores, we also adopt 
constituent count as an additional feature in-
spired by (Henderson and Brill 1999) and (Sagae 
and Lavie 2006). A constituent is a non-terminal 
node covering a special span. For example, 
?NP[2,4]? means a constituent labelled as ?NP? 
which covers the span from the second word to 
the fourth word. If we have 100 trees and NP[2,4] 
appears in 60 of them, then its constituent count 
is 60. For each tree, its constituent count is the 
sum of all the counts of its constituent. However, 
as suggested in (Sagae and Lavie 2006), this fea-
ture favours precision over recall. To solve this 
issue, Sagae and Lavie (2006) use a threshold to 
balance them. For any constituent, we calculate 
its count if and only if it appears more than X 
times in the k-best trees; otherwise we set it as 0. 
In this paper, we normalize this feature by divid-
ing the constituent count by the number of k-best. 
Note that the threshold value and the additional 
feature value are not independent. Once the 
threshold changes, the feature value has to be re-
calculated. 
In conclusion, we have four parameters to es-
timate: two model score weights, one additional 
feature weight and a threshold for the additional 
feature.  
4 Parameter Estimation  
We adopt the minimum error rate principle to 
tune the feature weights by minimizing the error 
rate (i.e. maximizing the F1 score) on the devel-
opment set. In our study, we implement and 
compare two algorithms, the simulated-annealing 
style algorithm and the average perceptron algo-
rithm. 
4.1 Simulated Annealing 
Simulated-annealing algorithm has been proved 
to be a powerful and efficient algorithm in solv-
ing NP problem (?ern? 1985). Fig 1 is the pseu-
do code of the simulated-annealing algorithm 
that we apply.   
In a single iteration (line 4-11), the simulated 
algorithm selects some random points (the Mar-
kov link) for hill climbing. However, it accepts 
some bad points with a threshold probability 
controlled by the annealing temperature (line 7-
10). The hill climbing nature gives this algorithm 
the ability of converging at local maximal point 
and the random nature offers it the chance to 
jump from some local maximal points to global 
maximal point. We do a slight modification to 
save the best parameter so far across all the fi-
nished iterations and let it be the initial point for 
upcoming iterations (line 12-17). 
RandomNeighbour(p) is the function to gener-
ate a random neighbor for the p (the four-tuple 
parameter to be estimated). F1(p) is the function 
to calculate the F1 score over the entire test set. 
Given a fixed parameter p, it selects the candi-
date tree with best score for each sentence and 
computes the F1 score with the PARSEVAL me-
trics. 
 
Pseudo code 1. Simulated-annealing algorithm 
Input: k-best trees combined from two model output 
Notation:  
   p: the current parameter value 
   F1(p): the F1 score with the parameter value p 
   TMF: the max F1 score of each iteration 
   TMp: the optimal parameter value during iteration 
   MaxF1: the max F1 score on dev set 
   Rp: the parameter value which maximizes the F1 score 
of the dev set 
   T: annealing temperature 
   L: length of Markov link 
Output: Rp 
 
1. MaxF1:= 0, Rp:= (0,0,0,0), T:=1, L=100 // initialize 
2. Repeat                                                       // iteration 
3.      TMp :=Rp 
4.      for  i := 1 to L  do 
5.            p := RandomNeighbour(TMp) 
6.            d= F1(p)- TMF 
7.            if d>0 or exp(d/T) > random[0,1) then  
8.                  TMF:=F1(p) 
9.                  TMp:=p 
10.            end if 
11.      end for 
12.      if TMF > MaxF1 then 
13.            MaxF:=TMF 
14.            Rp:=TMp 
15.      else  
16.            TMp:=Rp 
17.      end if 
18.      T=T*0.9 
19. Until convergence 
 
Fig 1. Simulated Annealing Algorithm 
4.2 Averaged Perceptron 
Another algorithm we apply is the averaged per-
ceptron algorithm. Fig 2 is the pseudo code of 
this algorithm. Averaged perceptron is an online 
algorithm. It iterates through each instance. In 
each instance, it selects the candidate answer 
with the maximum function score. Then it up-
dates the weight by the margin of feature value 
between the select answer and the oracle answer 
(line 5-9). After each iteration, it does average to 
generate a new weight (line 10). The averaged 
1555
perceptron has a solid theoretical fundamental 
and was proved to be effective across a variety of 
NLP tasks (Collins 2002). 
However, it needs a slightly modification to 
adapt to our problem. Since the threshold and the 
constituent count are not independent, they are 
not linear separable. In this case, the perceptron 
algorithm cannot be guaranteed to converge. To 
solve this issue, we introduce an outer loop (line 
2) to iterate through the value range of threshold 
with a fixed step length and in the inner loop we 
use perceptron to estimate the other three para-
meters. Finally we select the final parameter 
which has maximum F1 score across all the itera-
tion (line 14-17). 
 
Pseudo code 2. Averaged perceptron algorithm 
Input: k-best trees combined from two model output 
Notation:  
   MaxF1, Rp: already defined in pseudo code 1 
   T: the max number of iterations 
   I: the number of instances 
   Threshold: the threshold for constituent count 
   w: the three feature weights other than threshold 
   ?: the candidate tree with max function score given a 
fixed weight w 
   ?: the candidate tree with the max F1 score (since the 
oracle tree may not appeared in our candidate set, 
we choose this one as the pseudo orcale tree) 
   : the set of candidate tree for ith sentence 
Output: Rp 
 
1. MaxF1:=0, T=30 
2. for  Threshold :=0 to 1 with step 0.01 do  
3.     Initialize w 
4.     for iter : 1 to T do 
5.           for  i := 1 to I  do 
6.               ? ?????????  
7.               ? ?  
8.               ?:= w 
9.           end for  
10.           ? ??I???I  
11.           if converged  then break 
12.     end for 
13.     p := (Threshold, w) 
14.     if F1(p) > MaxF1 then 
15.         MaxF1 := F1(p) 
16.         Rp:=p 
17.     end if 
18. end for 
 
Fig 2. Averaged Perceptron Algorithm 
5 Experiments 
We evaluate our method on both Chinese and 
English syntactic parsing task with the standard 
division on Chinese Penn Treebank Version 5.0 
and WSJ English Treebank 3.0 (Marcus et al 
1993) as shown in Table 1.  
We use Satoshi Sekine and Michael Collins? 
EVALB script modified by David Ellis for accu-
racy evaluation. We use Charniak?s parser 
(Charniak 2000) and Berkeley?s parser (Petrov 
and Klein 2007) as the two individual parsers, 
where Charniak?s parser represents the best per-
formance of the lexicalized model and the Berke-
ley?s parser represents the best performance of 
the un-lexicalized model. We retrain both of 
them according to the division in Table. 1. The 
number of EM iteration process for Berkeley?s 
parser is set to 5 on English and 6 on Chinese. 
Both the Charniak?s parser and Berkeley?s parser 
provide function to evaluate an input parse tree?s 
probability and output the logarithm of the prob-
ability. 
 
        Div. 
Lang. Train Dev Test 
English Sec.02-21 Sec. 22 Sec. 23 
 
Chinese 
Art. 
001-270, 
400-1151 
Art. 
301-325 
Art. 
271-300 
 
          Table 1. Data division 
5.1 Effectiveness of our Combination Me-
thod 
This sub-section examines the effectiveness of 
our proposed methods. The experiment is set up 
as follows: 1) for each sentence in the dev and 
test sets, we generate 50-best from Charniak?s 
parser (Charniak 2000) and Berkeley?s parser 
(Petrov and Klein 2007), respectively; 2) the two 
50-best trees are merged together and duplication 
was removed; 3) we tune the parameters on the 
dev set and test on the test set. (Without specific 
statement, we use simulated-annealing as default 
weight tuning algorithm.)  
The results are shown in Table 2 and Table 3. 
?P? means precision, ?R? means recall and ?F? is 
the F1-measure (all is in % percentage metrics); 
?Charniak? represents the parser of (Charniak 
2000), ?Berkeley? represents the parser of (Pe-
trov and Klein 2007), ?Comb.? represents the 
combination of the two parsers. 
 
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 85.20 86.65 90.44 
R 83.70 84.18 85.96 
F 84.44 85.40 88.15 
All 
P 82.07 84.63 87.76 
R 79.66 81.69 83.27 
F 80.85 83.13 85.45 
 
Table 2. Results on Chinese 
1556
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 90.45 90.27 92.36 
R 90.14 89.76 91.42 
F 90.30 90.02 91.89 
All 
P 89.86 89.77 91.89 
R 89.53 89.26 90.97 
F 89.70 89.51 91.43 
 
Table 3. Results on English 
 
From Table 2 and Table 3, we can see our me-
thod outperforms the single systems in all test 
cases with all the three evaluation metrics. Using 
the entire Chinese test set, our method improves 
the performance by 2.3 (85.45-83.13) point in 
F1-Score, representing 13.8% error rate reduc-
tion. Using the entire English test set, our method 
improves the performance by 1.7 (91.43-89.70) 
point in F1-Score, representing 16.5% error rate 
reduction. These improvements convincingly 
demonstrate the effectiveness of our method. 
5.2 Effectiveness of K 
Fig 3 and Fig. 4 show the relationship between 
F1 score and the number of K-best used when 
doing combination on Chinese and English re-
spectively.  
From Fig 3 and Fig. 4, we could see that the 
F1 score first increases with the increasing of K 
(there are some vibration points, this may due to 
statistical noise) and reach the peak when K is 
around 30-50, then it starts to drop.  It shows that 
k-best list did provide more information than 
one-best and thus can help improve the accuracy; 
however more k-best list may also contain more 
noises and these noises may hurt the final com-
bination quality. 
 
 
 
       Fig 3. F1-measure vs. K on Chinese 
 
 
 
       Fig 4. F1-measure vs. K on English 
5.3 Diversity on the K-best Output of the 
Head-driven and Latent-annotation-
driven Model  
In this subsection, we examine how different of 
the 50-best trees generated from Charnriak?s 
parser (head-driven model) (Charnriak, 2000) 
and Berkeley?s parser (latent-annotation model) 
(Petrov and Klein, 2007).   
Table 4 reports the statistics on the 50-best 
output for Chinese and English test set. Since for 
some short sentences the parser cannot generate 
up to 50 best trees, the average number of trees is 
less than 50 for each sentence. Each cell reports 
the total number of trees generated over the en-
tire test set followed by the average count for 
each sentence in bracket. ?Total? means simply 
combine the number of trees from the two pars-
ers while ?Unique? means the number after re-
moving the duplicated trees for each sentence. In 
the last row, we report the averaged redundant 
rate for each sentence, which is derived by divid-
ing the figures in the row ?Duplicated? by those 
in the row ?Total?. 
 
 Chinese English 
Charniak 14577 (41.9) 120438 (49.9) 
Berkeley 14524 (41.7) 114299 (47.3) 
Total 29101 (83.6) 234737 (97.2) 
Unique 27747 (79.7) 221633 (91.7) 
Duplicated 1354 (3.9) 13104 (5.4) 
Redundant rate 4.65% 5.58% 
 
          Table 4. The statistics on the 50-best out-
put for Chinese and English test set.  
 
The small redundant rate clearly suggests that 
the two parsing models are quite different and 
are complementary to each other.  
1557
         parser 
Oracle Charniak Berkeley Comb. 
Chinese 
P 88.95 90.07 92.45 
R 86.51 87.12 89.67 
F 87.71 88.57 91.03 
English 
P 97.06 95.86 98.10 
R 96.57 95.53 97.68 
F 96.82 95.70 97.89 
 
Table 5. The oracle over 50-best output for in-
dividual parser and our method 
 
The k-best oracle score is the upper bound of 
the quality of the k-best trees. Table 5 reports the 
oracle score for the 50-best of the two individual 
parsers and our method.  Similar to Table 4, Ta-
ble 5 shows again that the two models are com-
plementary to each other and our method is able 
to take the strength of the two models. 
5.4 Effectiveness of Model Confidence 
One of the advantages of our method that we 
claim is that we can utilize the feature of the 
model confidence score (logarithm of the parse 
tree probability). 
Table 6 shows that all the three features con-
tribute to the final accuracy improvement. Even 
if we only use the ?B+C? confidence scores, it 
also outperforms the baseline individual parser 
(as reported in Table 2 and Table 3) greatly. All 
these together clearly verify the effective of the 
model confidence feature and our method can 
effectively utilize this feature. 
 
         Feat.  
Lang    I B+C B+C+I 
Chinese 82.34 84.67 85.45 
English 90.20 91.02 91.43 
 
Table 6. F1 score on 50-best combination with 
different feature configuration. ?I? means the 
constituent count, ?B? means Berkeley parser 
confidence score and ?C? means Charniak parser 
confidence score. 
5.5 Comparison of the Weight Tuning Al-
gorithms 
In this sub-section, we compare the two weight 
tuning algorithms on 50-best combination tasks 
on both Chinese and English. Dan Bikel?s ran-
domized parsing evaluation comparator (Bikel 
2004) was used to do significant test on precision 
and recall metrics. The results are shown in Ta-
ble 7.  
We can see, simulated annealing outperforms 
the averaged perceptron significantly in both 
precision (p<0.005) and recall (p<0.05) metrics 
of Chinese task and precision (p<0.005) metric 
of English task. Though averaged perceptron got 
slightly better recall score on English task, it is 
not significant according to the p-value (p>0.2). 
From table 8, we could see the simulated an-
nealing algorithm is around 2-4 times slower 
than averaged perceptron algorithm. 
 
         Algo. 
Lang SA. AP. P-value 
Chinese 
P 87.76 86.85 0.003 
R 83.27 82.90 0.030 
English 
P 91.89 91.72 0.004 
R 90.97 91.02 0.205 
 
Table 7. Precision and Recall score on 50-best 
combination by the two parameter estimation 
algorithms with significant test; ?SA.? is simu-
lated annealing, ?AP.? is averaged perceptron, 
?P-value? is the significant test p-value. 
 
           Algo. 
Lang 
Simulated 
Annealing 
Averaged 
Perceptron 
Chinese 2.3 0.6 
English 12 6 
  
   Table 8. Time taken (in minutes) on 50-best 
combination of the two parameter estimation 
algorithms 
5.6 Performance-Enhanced Individual  
Parsers on English  
For Charniak?s lexicalized parser, there are two 
techniques to improve its performance. One is re-
ranking as explained in section 2. The other is 
the self-training (McClosky et al 2006) which 
first parses and reranks the NANC corpus, and 
then use them as additional training data to re-
train the model. In this sub-section, we apply our 
method to combine the Berkeley parser and the 
enhanced Charniak parser by using the new 
model confidence score output from the en-
hanced Charniak parser.  
Table 9 and Table 10 show that the Charniak 
parser enhanced by re-ranking and self-training 
is able to help to further improve the perfor-
mance of our method. This is because that the 
enhanced Charniak parser provides more accu-
rate model confidence score.  
 
1558
         parser 
accuracy reranking Comb. baseline 
<=40 
words 
P 92.34 93.41 92.36 
R 91.61 92.15 91.42 
F 91.97 92.77 91.89 
All 
P 91.78 92.92 91.89 
R 91.03 91.70 90.97 
F 91.40 92.30 91.43 
 
Table 9. Performance with Charniak parser 
enhanced by re-ranking; ?baseline? is the per-
formance of the combination of Table 3. 
 
         parser 
accuracy 
self-train+ 
reranking Comb. baseline 
<=40 
words 
P 92.87 93.69 92.36 
R 92.12 92.44 91.42 
F 92.49 93.06 91.89 
All 
P 92.41 93.25 91.89 
R 91.64 92.00 90.97 
F 92.02 92.62 91.43 
 
 Table 10. Performance with Charniak parser 
enhanced by re-ranking plus self-training 
5.7 Comparison with Other State-of-the-art 
Results 
Table 11 and table 12 compare our method with 
the other state-of-the-art methods; we use I, B, R, 
S and C to denote individual model (Charniak 
2000; Collins 2000; Bod 2003; Petrov and Klein 
2007), bilingual-constrained model (Burkett and 
Klein 2008)1, re-ranking model (Charniak and 
Johnson 2005, Huang 2008), self-training model 
(David McClosky 2006) and combination model 
(Sagae and Lavie 2006) respectively. The two 
tables clearly show that our method advance the 
state-of-the-art results on both Chinese and Eng-
lish syntax parsing. 
 
System  F1-Measure 
I 
Charniak (2000) 80.85 
Petrov and Klein (2007) 83.13 
B Burkett and Klein (2008)1 84.24 
C Our method 85.45 
 
Table 11. Accuracy comparison on Chinese 
 
                                                           
1 Burkett and Klein (2008) use the additional know-
ledge from Chinese-English parallel Treebank to im-
prove Chinese parsing accuracy. 
System  F1-Measure 
I 
Petrov and Klein (2007) 89.5 
Charniak (2000) 89.7 
Bod (2003) 90.7 
R 
Collins (2000) 89.7 
Charniak and Johnson (2005) 91.4 
Huang (2008) 91.7 
S David McClosky (2006) 92.1 
C 
Sagae and Lavie (2006) 92.1 
Our method 92.6 
 
  Table 12. Accuracy comparison on English. 
6 Conclusions   
In this paper2, we propose a linear model-based 
general framework for multiple parser combina-
tion. Compared with previous methods, our me-
thod is able to use diverse features, including 
logarithm of the parse tree probability calculated 
by the individual systems. We verify our method 
by combining the two representative parsing 
models, lexicalized model and un-lexicalized 
model, on both Chinese and English. Experimen-
tal results show our method is very effective and 
advance the state-of-the-art results on both Chi-
nese and English syntax parsing. In the future, 
we will explore more features and study the for-
est-based combination methods for syntactic 
parsing. 
Acknowledgement  
We would like to thank Prof. Hwee Tou Ng for 
his help and support; Prof. Charniak for his sug-
gestion on doing the experiments with the self-
trained parser and David McCloksy for his help 
on the self-trained model; Yee Seng Chan and 
the anonymous reviewers for their valuable 
comments. 
References  
Dan  Bikel. 2004. On the Parameter Space of Genera-
tive Lexicalized Statistical Parsing Models. Ph.D. 
Thesis, University of Pennsylvania 2004. 
Rens Bod. 2003. An efficient implementation of a new 
DOP model. EACL-04. 
David Burkett and Dan Klein. 2008. Two Languages 
are Better than One (for Syntactic Parsing). 
EMNLP-08. 
                                                           
The corresponding authors of this paper are Hui 
Zhang (zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
1559
V ?ern? 1985. Thermodynamical approach to the 
travelling salesman problem: an efficient simula-
tion algorithm. Journal of Optimization Theory and 
Applications, 45:41-51.1985. 
Eugene Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. AAAI-
97, pages 598-603. 
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. NAACL-2000. 
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative 
reranking. ACL-05, Pages 173-180. 
Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. ACL-97, pages 16-
23.  
Michael Collins.1999. Head-drivenstatistical models 
for natural language parsing. Doctoral Disserta-
tion, Dept. of Computer and Information Science, 
University of Pennsylvania, Philadelphia 1999. 
Michael Collins. 2000. Discriminative reranking for 
natural language parsing. ICML-00, pages 175-
182. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. EMNLP-02. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08, 
pages 586-594. 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
S. Kirkpatrick, C. D. Gelatt, Jr. and M. P. Vecchi. 
1983. Optimization by Simulated Annealing. 
Science. New Series 220 (4598): 671-680. 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-01. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. ACL-03, pages 423-
430. 
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: combining 
parsers. EMNLP-99. 
Mitchell P. Marcus, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313-330. 
Takuya Matsuzaki. Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. 
ACL-05, pages 75-82. 
David McClosky, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. 
NAACL-06, pages 152-159. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. COLING-ACL-06, 
pages 443-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. HLT-NAACL-07, pages 
401-411. 
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, 
Richard Crouch, John T. III Maxwell and Mark 
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discrimina-
tive estimation techniques. ACL-02, pages 271?
278.  
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. HLT-NAACL-06, pages 129-
132. 
Daniel Zeman and Zden?k ?abokrtsk?. Improving 
Parsing Accuracy by Combining Diverse Depen-
dency Parsers. IWPT-05. 
 
1560
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 907?914, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Semi-Supervised Feature Clustering Algorithm
with Application to Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
In this paper we investigate an applica-
tion of feature clustering for word sense
disambiguation, and propose a semi-
supervised feature clustering algorithm.
Compared with other feature clustering
methods (ex. supervised feature cluster-
ing), it can infer the distribution of class
labels over (unseen) features unavailable
in training data (labeled data) by the use of
the distribution of class labels over (seen)
features available in training data. Thus,
it can deal with both seen and unseen fea-
tures in feature clustering process. Our ex-
perimental results show that feature clus-
tering can aggressively reduce the dimen-
sionality of feature space, while still main-
taining state of the art sense disambigua-
tion accuracy. Furthermore, when com-
bined with a semi-supervised WSD algo-
rithm, semi-supervised feature clustering
outperforms other dimensionality reduc-
tion techniques, which indicates that using
unlabeled data in learning process helps to
improve the performance of feature clus-
tering and sense disambiguation.
1 Introduction
This paper deals with word sense disambiguation
(WSD) problem, which is to assign an appropriate
sense to an occurrence of a word in a given context.
Many corpus based statistical methods have been
proposed to solve this problem, including supervised
learning algorithms (Leacock et al, 1998; Towel and
Voorheest, 1998), weakly supervised learning algo-
rithms (Dagan and Itai, 1994; Li and Li, 2004; Mi-
halcea, 2004; Niu et al, 2005; Park et al, 2000;
Yarowsky, 1995), unsupervised learning algorithms
(or word sense discrimination) (Pedersen and Bruce,
1997; Schu?tze, 1998), and knowledge based algo-
rithms (Lesk, 1986; McCarthy et al, 2004).
In general, the most common approaches start by
evaluating the co-occurrence matrix of features ver-
sus contexts of instances of ambiguous word, given
sense-tagged training data for this target word. As
a result, contexts are usually represented in a high-
dimensional sparse feature space, which is far from
optimal for many classification algorithms. Further-
more, processing data lying in high-dimensional fea-
ture space requires large amount of memory and
CPU time, which limits the scalability of WSD
model to very large datasets or incorporation of
WSD model into natural language processing sys-
tems.
Standard dimentionality reduction techniques in-
clude (1) supervised feature selection and super-
vised feature clustering when given labeled data, (2)
unsupervised feature selection, latent semantic in-
dexing, and unsupervised feature clustering when
only unlabeled data is available. Supervised fea-
ture selection improves the performance of an ex-
amplar based learning algorithm over SENSEVAL-
2 data (Mihalcea, 2002), Naive Bayes and deci-
sion tree over SENSEVAL-1 and SENSEVAL-2 data
(Lee and Ng, 2002), but feature selection does not
improve SVM and Adaboost over SENSEVAL-1
and SENSEVAL-2 data (Lee and Ng, 2002) for
word sense disambiguation. Latent semantic in-
dexing (LSI) studied in (Schu?tze, 1998) improves
the performance of sense discrimination, while un-
supervised feature selection also improves the per-
formance of word sense discrimination (Niu et al,
2004). But little work is done on using feature clus-
tering to conduct dimensionality reduction for WSD.
This paper will describe an application of feature
907
clustering technique to WSD task.
Feature clustering has been extensively studied
for the benefit of text categorization and document
clustering. In the context of text categorization, su-
pervised feature clustering algorithms (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
and Tishby, 2001) usually cluster words into groups
based on the distribution of class labels over fea-
tures, which can compress the feature space much
more aggressively while still maintaining state of
the art classification accuracy. In the context of
document clustering, unsupervised feature cluster-
ing algorithms (Dhillon, 2001; Dhillon et al, 2002;
Dhillon et al, 2003; El-Yaniv and Souroujon, 2001;
Slonim and Tishby, 2000) perform word clustering
by the use of word-document co-occurrence matrix,
which can improve the performance of document
clustering by clustering documents over word clus-
ters.
Supervised feature clustering algorithm groups
features into clusters based on the distribution of
class labels over features. But it can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
On the other hand, while given labeled data, un-
supervised feature clustering method can not uti-
lize class label information to guide feature cluster-
ing procedure. While, as a promising classification
strategy, semi-supervised learning methods (Zhou et
al., 2003; Zhu and Ghahramani, 2002; Zhu et al,
2003) usually utilize all the features occurring in la-
beled data and unlabeled data. So in this paper we
propose a semi-supervised feature clustering algo-
rithm to overcome this problem. Firstly, we try to
induce class labels for unseen features based on the
similarity among seen features and unseen features.
Then all the features (including seen features and
unseen features) are clustered based on the distrib-
ution of class labels over them.
This paper is organized as follows. First, we
will formulate a feature clustering based WSD prob-
lem in section 2. Then in section 3 we will de-
scribe a semi-supervised feature clustering algo-
rithm. Section 4 will provide experimental results
of various dimensionality reduction techniques with
combination of state of the art WSD algorithms on
SENSEVAL-3 data. Section 5 will provide a review
of related work on feature clustering. Finally we will
conclude our work and suggest possible improve-
ment in section 6.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
We use F? to represent feature clustering result
into NF? clusters when F is a set of features. After
feature clustering, any context xi in X can be repre-
sented as a vector over feature clusters F? . Then we
can use supervised methods (ex. SVM) (Lee and
Ng, 2002) or semi-supervised methods (ex. label
propagation algorithm) (Niu et al, 2005) to perform
sense disambiguation on unlabeled instances of tar-
get word.
3 Semi-Supervised Feature Clustering
Algorithm
In supervised feature clustering process, F consists
of features occurring in the first l labeled examples,
which can be denoted as FL. But in the setting of
transductive learning, semi-supervised learning al-
gorithms will utilize not only the features in labeled
examples (FL), but also unseen features in unlabeled
examples (denoted as FL). FL consists of the fea-
tures that occur in unlabeled data, but never appear
in labeled data.
Supervised feature clustering algorithm usually
performs clustering analysis over feature-class ma-
trix, where each entry (i, j) in this matrix is the num-
ber of times of the i-th feature co-occurring with the
j-th class. Therefore it can not group features in FL
into meaningful clusters since there are no class la-
bels associated with these features. We overcome
this problem by firstly inducing class labels for un-
seen features based on the similarity among features
in FL and FL, then clustering all the features (in-
cluding FL and FL) based on the distribution of class
908
labels over them.
This semi-supervised feature clustering algorithm
is defined as follows:
Input:
Feature set F = FL
?FL (the first |FL| features
in F belong to FL, and the remaining |FL| features
belong to FL), context set X , the label information
of xg(1 ? g ? l), NF? (the number of clusters in F? );
Output:
Clustering solution F? ;
Algorithm:
1. Construct |F | ? |X| feature-example matrix
MF,X , where entry MF,Xi,j is the number of times of
fi co-occurring with example xj (1 ? j ? n).
2. Form |F | ? |F | affinity matrix W defined by
Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0 (1 ?
i, j ? |F |), where dij is the distance (ex. Euclid-
ean distance) between fi (the i-th row in MF,X ) and
fj (the j-th row in MF,X ), and ? is used to control
the weight Wij .
3. Construct |FL| ? |S| feature-class matrix
Y FL,S , where the entry Y FL,Si,j is the number of
times of feature fi (fi ? FL) co-occurring with
sense sj .
4. Obtain hard label matrix for features in FL
(denoted as Y FL,Shard ) based on Y FL,S , where entry
Y F,Shard i,j = 1 if the hard label of fi is sj , otherwise
zero. Obtain hard labels for features in FL using
a classifier based on W and Y FL,Shard . In this paper
we use label propagation (LP) algorithm (Zhu and
Ghahramani, 2002) to get hard labels for FL.
5. Construct |F | ? |S| feature-class matrix Y F,Shard,
where entry Y F,Shard i,j = 1 if the hard label of fi is
sj , otherwise zero.
6. Construct the matrix L = D?1/2WD?1/2 in
which D is a diagonal matrix with its (i, i)-element
equal to the sum of the i-th row of W .
7. Label each feature in F as soft label Y? F,Si , the
i-th row of Y? F,S , where Y? F,S = (I ? ?L)?1Y F,Shard.
8. Obtain the feature clustering solution F? by
clustering the rows of Y? F,Si into NF? groups. In
this paper we use sequential information bottleneck
(sIB) algorithm (Slonim and Tishby, 2000) to per-
form clustering analysis.
End
Step 3 ? 5 are the process to obtain hard la-
bels for features in F , while the operation in step 6
and 7 is a local and global consistency based semi-
supervised learning (LGC) algorithm (Zhou et al,
2003) that smooth the classification result of LP al-
gorithm to acquire a soft label for each feature.
At first sight, this semi-supervised feature cluster-
ing algorithm seems to make little sense. Since we
run feature clustering in step 8, why not use LP algo-
rithm to obtain soft label matrix Y FL,S for features
in FL by the use of Y FL,S and W , then just apply
sIB directly to soft label matrix Y? F,S (constructed
by catenating Y FL,S and Y FL,S)?
The reason for using LGC algorithm to acquire
soft labels for features in F is that in the context
of transductive learning, the size of labeled data is
rather small, which is much less than that of un-
labeled data. This makes it difficult to obtain re-
liable estimation of class label?s distribution over
features from only labeled data. This motivates
us to use raw information (hard labels of features
in FL) from labeled data to estimate hard labels
of features in FL. Then LGC algorithm is used
to smooth the classification result of LP algorithm
based on the assumption that a good classification
should change slowly on the coherent structure ag-
gregated by a large amount of unlabeled data. This
operation makes our algorithm more robust to the
noise in feature-class matrix Y FL,S that is estimated
from labeled data.
In this paper, ? is set as the average distance be-
tween labeled examples from different classes, and
NF? = |F |/10. Latent semantic indexing technique
(LSI) is used to perform factor analysis in MF,X be-
fore calculating the distance between features in step
2.
4 Experiments and Results
4.1 Experiment Design
For empirical study of dimensionality reduction
techniques on WSD task, we evaluated five dimen-
sionality reduction algorithms on the data in English
lexical sample (ELS) task of SENSEVAL-3 (Mihal-
cea et al, 2004)(including all the 57 English words
) 1: supervised feature clustering (SuFC) (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
1Available at http://www.senseval.org/senseval3
909
and Tishby, 2001), iterative double clustering (IDC)
(El-Yaniv and Souroujon, 2001), semi-supervised
feature clustering (SemiFC) (our algorithm), super-
vised feature selection (SuFS) (Forman, 2003), and
latent semantic indexing (LSI) (Deerwester et. al.,
1990) 2.
We used sIB algorithm 3 to cluster features in
FL into groups based on the distribution of class la-
bels associated with each feature. This procedure
can be considered as our re-implementation of su-
pervised feature clustering. After feature clustering,
examples can be represented as vectors over feature
clusters.
IDC is an extension of double clustering method
(DC) (Slonim and Tishby, 2000), which performs it-
erations of DC. In the transductive version of IDC,
they cluster features in F as distributions over class
labels (given by the labeled data) during the first
stage of the IDC first iteration. This phase results in
feature clusters F? . Then they continue as usual; that
is, in the second phase of the first IDC iteration they
group X into NX? clusters, where X is represented
as distribution over F? . Subsequent IDC iterations
use all the unlabeled data. This IDC algorithm can
result in two clustering solutions: F? and X? . Follow-
ing (El-Yaniv and Souroujon, 2001), the number of
iterations is set as 15, and NX? = |S| (the number of
senses of target word) in our re-implementation of
IDC. After performing IDC, examples can be repre-
sented as vectors over feature clusters F? .
Supervised feature selection has been extensively
studied for text categorization task (Forman, 2003).
Information gain (IG) is one of state of the art cri-
teria for feature selection, which measures the de-
crease in entropy when the feature is given vs. ab-
sent. In this paper, we calculate IG score for each
feature in FL, then select top |F |/10 features with
highest scores to form reduced feature set. Then
examples can be represented as vectors over the re-
duced feature set.
LSI is an unsupervised factor analysis technique
based on Singular Value Decomposition of a |X| ?
|F | example-feature matrix. The underlying tech-
nique for LSI is to find an orthogonal basis for the
2Following (Baker and McCallum, 1998), we use LSI as a
representative method for unsupervised dimensionality reduc-
tion.
3Available at http://www.cs.huji.ac.il/?noamm/
feature-example space for which the axes lie along
the dimensions of maximum variance. After using
LSI on the example-feature matrix, we can get vec-
tor representation for each example in X in reduced
feature space.
For each ambiguous word in ELS task of
SENSEVAL-3, we used three types of features to
capture contextual information: part-of-speech of
neighboring words with position information, un-
ordered single words in topical context, and local
collocations (as same as the feature set used in (Lee
and Ng, 2002) except that we did not use syntactic
relations). We removed the features with occurrence
frequency (counted in both training set and test set)
less than 3 times.
We ran these five algorithms for each ambiguous
word to reduce the dimensionality of feature space
from |F | to |F |/10 no matter which training data is
used (ex. full SENSEVAL-3 training data or sam-
pled SENSEVAL-3 training data). Then we can ob-
tain new vector representation of X in new feature
space acquired by SuFC, IDC, SemiFC, and LSI or
reduced feature set by SuFS.
Then we used SVM 4 and LP algorithm to per-
form sense disambiguation on vectors in dimension-
ality reduced feature space. SVM and LP were eval-
uated using accuracy 5 (fine-grained score) on test
set of SENSEVAL-3. For LP algorithm, the test set
in SENSEVAL-3 data was also used as unlabeled
data in tranductive learning process.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991). Cosine similarity measures the angle
between two feature vectors, while JS divergence
measures the distance between two probability dis-
tributions if each feature vector is considered as
probability distribution over features.
For sense disambiguation on SENSEVAL-3 data,
we constructed connected graphs for LP algorithm
following (Niu et al, 2005): two instances u, v will
be connected by an edge if u is among v?s k nearest
neighbors, or if v is among u?s k nearest neighbors
4We used SV M light with linear kernel function, available
at http://svmlight.joachims.org/.
5If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
910
as measured by cosine or JS distance measure. k is
5 in later experiments.
4.2 Experiments on Full SENSEVAL-3 Data
In this experiment, we took the training set in
SENSEVAL-3 as labeled data, and the test set as un-
labeled data. In other words, all of dimensionality
reduction methods and classifiers can use the label
information in training set, but can not access the
label information in test set. We evaluated differ-
ent sense disambiguation processes using test set in
SENSEVAL-3.
We use features with occurrence frequency no less
than 3 in training set and test set as feature set F for
each ambiguous word. F consists of two disjoint
subsets: FL and FL. FL consists of features occur-
ring in training set of target word in SENSEVAL-3,
while FL consists of features that occur in test set,
but never appear in training set.
Table 1 lists accuracies of SVM and LP
without or with dimensionality reduction on full
SENSEVAL-3 data. From this table, we have some
findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 70.3%
(LPJS), while if using dimensionality reduction,
the best two systems can achieve 69.8% (SuFS +
LPJS) and 69.0% (SemiFC + LPJS) accuracies.
It seems that feature selection and feature clustering
can significantly reduce the dimensionality of fea-
ture space while losing only about 1.0% accuracy.
(2) Furthermore, LPJS algorithm performs bet-
ter than SVM when combined with the same dimen-
sionality reduction technique (except IDC). Notice
that LP algorithm uses unlabelled data during its dis-
ambiguation phase while SVM doesn?t. This indi-
cates that using unlabeled data helps to improve the
performance of sense disambiguation.
(3) When using LP algorithm for sense disam-
biguation, SemiFC performs better than other fea-
ture clustering algorithms, such as SuFC, IDC.
This indicates that clustering seen and unseen fea-
tures can satisfy the requirement of semi-supervised
learning algorithm, which does help the classifica-
tion process.
(4) When using SuFC, IDC, SuFS, or SemiFC for
dimensionality reduction, the performance of sense
disambiguation is always better than that using LSI
as dimensionality reduction method. SuFC, IDC,
SuFS, and SemiFC use label information to guide
feature clustering or feature selection, while LSI is
an unsupervised factor analysis method that can con-
duct dimensionality reduction without the use of la-
bel information from labeled data. This indicates
that using label information in dimensionality re-
duction procedure can cluster features into better
groups or select better feature subsets, which results
in better representation of contexts in reduced fea-
ture space.
4.3 Additional Experiments on Sampled
SENSEVAL-3 Data
For investigating the performance of various dimen-
sionality reduction techniques with very small train-
ing data, we ran them with only lw examples from
training set of each word in SENSEVAL-3 as la-
beled data. The remaining training examples and
all the test examples were used as unlabeled data
for SemiFC or LP algorithm. Finally we evaluated
different sense disambiguation processes using test
set in SENSEVAL-3. For each labeled set size lw,
we performed 20 trials. In each trial, we randomly
sampled lw labeled examples for each word from
training set. If any sense was absent from the sam-
pled labeled set, we redid the sampling. lw is set as
Nw,train ? 10%, where Nw,train is the number of
examples in training set of word w. Other settings
of this experiment is as same as that of previous one
in section 4.2.
In this experiment, feature set F is as same as that
in section 4.2. FL consists of features occurring in
sampled training set of target word in SENSEVAL-
3, while FL consists of features that occur in unla-
beled data (including unselected training data and all
the test set), but never appear in labeled data (sam-
pled training set).
Table 2 lists accuracies of SVM and LP with-
out or with dimensionality reduction on sampled
SENSEVAL-3 training data 6. From this table, we
have some findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 54.9%
(LPJS), while if using dimensionality reduction, the
6We can not obtain the results of IDC over 20 trials since it
costs about 50 hours for each trial (Pentium 1.4 GHz CPU/1.0
GB memory).
911
Table 1: This table lists the accuracies of SVM and LP without or with dimensionality reduction on full
SENSEVAL-3 data. There is no result for LSI + LPJS , since the vectors obtained by LSI may contain
negative values, which prohibits the application of JS divergence for measuring the distance between these
vectors.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC IDC SuFS LSI SemiFC
SVM 69.7% 66.4% 65.1% 65.2% 59.1% 64.0%
LPcosine 68.4% 66.7% 64.9% 66.0% 60.7% 67.6%
LPJS 70.3% 67.2% 64.0% 69.8% - 69.0%
Table 2: This table lists the accuracies of SVM and LP without or with dimensionality reduction on sam-
pled SENSEVAL-3 training data. For each classifier, we performed paired t-test between the system using
SemiFC for dimensionality reduction and any other system with or without dimensionality reduction. ? (or
?) means p-value ? 0.01, while > (or <) means p-value falling into (0.01, 0.05]. Both ? (or ?) and >
(or <) indicate that the performance of current WSD system is significantly better (or worse) than that using
SemiFC for dimensionality reduction, when given same classifier.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC SuFS LSI SemiFC
SVM 53.4?1.1% (?) 50.4?1.1% (?) 52.2?1.2% (>) 49.8?0.8% (?) 51.5?1.0%
LPcosine 54.4?1.2% (?) 49.5?1.1% (?) 51.1?1.0% (?) 49.8?1.0% (?) 52.9?1.0%
LPJS 54.9?1.1% (?) 52.0?0.9% (?) 52.5?1.0% (?) - 54.1?1.2%
best performance of sense disambiguation is 54.1%
(SemiFC + LPJS). Feature clustering can signif-
icantly reduce the dimensionality of feature space
while losing only 0.8% accuracy.
(2) LPJS algorithm performs better than SVM
when combined with most of dimensionality reduc-
tion techniques. This result confirmed our previous
conclusion that using unlabeled data can improve
the sense disambiguation process. Furthermore,
SemiFC performs significantly better than SuFC and
SuFS when using LP as the classifier for sense dis-
ambiguation. The reason is that when given very
few labeled examples, the distribution of class labels
over features can not be reliably estimated, which
deteriorates the performance of SuFC or SuFS. But
SemiFC uses only raw label information (hard label
of each feature) estimated from labeled data, which
makes it robust to the noise in very small labeled
data.
(3) SuFC, SuFS and SemiFC perform better than
LSI no matter which classifier is used for sense dis-
ambiguation. This observation confirmed our previ-
ous conclusion that using label information to guide
dimensionality reduction process can result in bet-
ter representation of contexts in feature subspace,
which further improves the results of sense disam-
biguation.
5 Related Work
Feature clustering has been extensively studied for
the benefit of text categorization and document clus-
tering, which can be categorized as supervised fea-
ture clustering, semi-supervised feature clustering,
and unsupervised feature clustering.
Supervised feature clustering algorithms (Baker
and McCallum, 1998; Bekkerman et al, 2003;
Slonim and Tishby, 2001) usually cluster words into
groups based on the distribution of class labels over
features. Baker and McCallum (1998) apply super-
vised feature clustering based on distributional clus-
tering for text categorization, which can compress
the feature space much more aggressively while still
912
maintaining state of the art classification accuracy.
Slonim and Tishby (2001) and Bekkerman et. al.
(2003) apply information bottleneck method to find
word clusters. They present similar results with the
work by Baker and McCallum (1998). Slonim and
Tishby (2001) goes further to show that when the
training sample is small, word clusters can yield sig-
nificant improvement in classification accuracy.
Unsupervised feature clustering algorithms
(Dhillon, 2001; Dhillon et al, 2002; Dhillon et al,
2003; El-Yaniv and Souroujon, 2001; Slonim and
Tishby, 2000) perform word clustering by the use
of word-document co-occurrence matrix, which do
not utilize class labels to guide clustering process.
Slonim and Tishby (2000), El-Yaniv and Souroujon
(2001) and Dhillon et. al. (2003) show that word
clusters can improve the performance of document
clustering.
El-Yaniv and Souroujon (2001) present an itera-
tive double clustering (IDC) algorithm, which per-
forms iterations of double clustering (Slonim and
Tishby, 2000). Furthermore, they extend IDC algo-
rithm for semi-supervised learning when given both
labeled and unlabeled data.
Our algorithm belongs to the family of semi-
supervised feature clustering techniques, which can
utilize both labeled and unlabeled data to perform
feature clustering.
Supervised feature clustering can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
Our algorithm can overcome this problem by induc-
ing class labels for unseen features based on the sim-
ilarity among seen features and unseen features, then
clustering all the features (including both seen fea-
tures and unseen features) based on the distribution
of class labels over them.
Compared with the semi-supervised version of
IDC algorithm, our algorithm is more efficient, since
we perform feature clustering without iterations.
The difference between our algorithm and unsu-
pervised feature clustering is that our algorithm de-
pends on both labeled and unlabeled data, but unsu-
pervised feature clustering requires only unlabeled
data.
O?Hara et. al. (2004) use semantic class-
based collocations to augment traditional word-
based collocations for supervised WSD. Three sep-
arate sources of word relatedness are used for
these collocations: 1) WordNet hypernym rela-
tions; 2) cluster-based word similarity classes; and
3) dictionary definition analysis. Their system
achieved 56.6% fine-grained score on ELS task of
SENSEVAL-3. In contrast with their work, our data-
driven method for feature clustering based WSD
does not require external knowledge resource. Fur-
thermore, our SemiFC+LPJS method can achieve
69.0% fine-grained score on the same dataset, which
shows the effectiveness of our method.
6 Conclusion
In this paper we have investigated feature clustering
techniques for WSD, which usually group features
into clusters based on the distribution of class labels
over features. We propose a semi-supervised fea-
ture clustering algorithm to satisfy the requirement
of semi-supervised classification algorithms for di-
mensionality reduction in feature space. Our ex-
perimental results on SENSEVAL-3 data show that
feature clustering can aggressively reduce the di-
mensionality of feature space while still maintaining
state of the art sense disambiguation accuracy. Fur-
thermore, when combined with a semi-supervised
WSD algorithm, semi-supervised feature cluster-
ing outperforms supervised feature clustering and
other dimensionality reduction techniques. Our ad-
ditional experiments on sampled SENSEVAL-3 data
indicate that our semi-supervised feature clustering
method is robust to the noise in small labeled data,
which achieves better performance than supervised
feature clustering.
In the future, we may extend our work by using
more datasets to empirically evaluate this feature
clustering algorithm. This semi-supervised feature
clustering framework is quite general, which can be
applied to other NLP tasks, ex. text categorization.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Baker L. & McCallum A.. 1998. Distributional Clus-
tering of Words for Text Classification. ACM SIGIR
913
1998.
Bekkerman, R., El-Yaniv, R., Tishby, N., & Winter, Y..
2003. Distributional Word Clusters vs. Words for
Text Categorization. Journal of Machine Learning Re-
search, Vol. 3: 1183-1208.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Corpus.
Computational Linguistics, Vol. 20(4), pp. 563-596.
Deerwester, S.C., Dumais, S.T., Landauer, T.K., Furnas,
G.W., & Harshman, R.A.. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
of Information Science, Vol. 41(6), pp. 391-407.
Dhillon I.. 2001. Co-Clustering Documents and Words
Using Bipartite Spectral Graph Partitioning. ACM
SIGKDD 2001.
Dhillon I., Mallela S., & Kumar R.. 2002. Enhanced
Word Clustering for Hierarchical Text Classification.
ACM SIGKDD 2002.
Dhillon I., Mallela S., & Modha, D.. 2003. Information-
Theoretic Co-Clustering. ACM SIGKDD 2003.
El-Yaniv, R., & Souroujon, O.. 2001. Iterative Dou-
ble Clustering for Unsupervised and Semi-Supervised
Learning. NIPS 2001.
Forman, G.. 2003. An Extensive Empirical Study of Fea-
ture Selection Metrics for Text Classification. Journal
of Machine Learning Research 3(Mar):1289?1305.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. EMNLP 2002, (pp. 41-
48).
Lesk M.. 1986. Automated Word Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. ACM SIGDOC
1986.
Li, H. & Li, C.. 2004. Word Translation Disambiguation
Using Bilingual Bootstrapping. Computational Lin-
guistics, 30(1), 1-22.
Lin, J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J..
2004. Finding Predominant Word Senses in Untagged
Text. ACL 2004.
Mihalcea R.. 2002. Instance Based Learning with Au-
tomatic Feature Selection Applied to Word Sense Dis-
ambiguation. COLING 2002.
Mihalcea R.. 2004. Co-Training and Self-Training for
Word Sense Disambiguation. CoNLL 2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The
SENSEVAL-3 English Lexical Sample Task. SENSE-
VAL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2004. Learning Word
Senses With Feature Selection and Order Identification
Capabilities. ACL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. ACL 2005.
O?Hara, T., Bruce, R., Donner, J., & Wiebe, J..
2004. Class-Based Collocations for Word-Sense Dis-
ambiguation. SENSEVAL 2004.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word
Sense Disambiguation by Learning from Unlabeled
Data. ACL 2000.
Pedersen. T., & Bruce, R.. 1997. Distinguishing Word
Senses in Untagged Text. EMNLP 1997.
Schu?tze, H.. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Slonim, N. & Tishby, N.. 2000. Document Clustering
Using Word Clusters via the Information Bottleneck
Method. ACM SIGIR 2000.
Slonim, N. & Tishby, N.. 2001. The Power of Word
Clusters for Text Classification. The 23rd European
Colloquium on Information Retrieval Research.
Towel, G. & Voorheest, E.M.. 1998. Disambiguating
Highly Ambiguous Words. Computational Linguis-
tics, 24:1, 125?145.
Yarowsky, D.. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. ACL 1995,
pp. 189-196.
Zhou D., Bousquet, O., Lal, T.N., Weston, J., &
Scho?lkopf, B.. 2003. Learning with Local and Global
Consistency. NIPS 16,pp. 321-328.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. ICML 2003.
914
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 ? 389, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Discovering Relations Between Named Entities  
from a Large Raw Corpus Using Tree  
Similarity-Based Clustering 
Min Zhang1, Jian Su1, Danmei Wang1,2, Guodong Zhou1, and Chew Lim Tan2 
1 Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, sujian, stuwang, zhougd}@i2r.a-star.edu.sg 
2 Department of Computer Science,  
National University of Singapore, 
Singapore, 117543 
tancl@comp.nus.edu.sg 
Abstract. We propose a tree-similarity-based unsupervised learning method to 
extract relations between Named Entities from a large raw corpus. Our method 
regards relation extraction as a clustering problem on shallow parse trees. First, 
we modify previous tree kernels on relation extraction to estimate the similarity 
between parse trees more efficiently. Then, the similarity between parse trees is 
used in a hierarchical clustering algorithm to group entity pairs into different 
clusters. Finally, each cluster is labeled by an indicative word and unreliable 
clusters are pruned out. Evaluation on the New York Times (1995) corpus 
shows that our method outperforms the only previous work by 5 in F-measure. 
It also shows that our method performs well on both high-frequent and less-
frequent entity pairs. To the best of our knowledge, this is the first work to use a 
tree similarity metric in relation clustering. 
1   Introduction  
The relation extraction task identifies various semantic relations such as location, 
affiliation, revival and so on between entities from text. For example, the sentence 
?George Bush is the president of the United States.? conveys the semantic relation 
?President?, between the entities ?George Bush? (PERSON) and ?the United States? 
(GPE1). The task of relation extraction was first introduced as part of the Template 
Element task in MUC6 and formulated as the Template Relation task in MUC7 [1]. 
Most work at MUC [1] was rule-based, which tried to use syntactic and semantic 
patterns to capture the corresponding relations by means of manually written linguis-
tic rules. The major drawback of this method is the poor adaptability and the poor 
robustness in handling large-scale or new domain data due to two reasons. First, rules 
have to be rewritten for different tasks or when porting to different domains. Second, 
generating rules manually is quite labor- and time-consuming. 
                                                          
1
 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity 
--- an entity with land and a government. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 379 
Since then, various supervised learning approaches [2,3,4,5] have been explored ex-
tensively in relation extraction. These approaches automatically learn relation patterns 
or models from a large annotated corpus. To decrease the corpus annotation require-
ment, some researchers turned to weakly supervised learning approaches [6,7], which 
rely on a small set of initial seeds instead of a large annotated corpus. However, there is 
no systematic way in selecting initial seeds and deciding an ?optimal? number of them. 
Alternatively, Hasegawa et al [8] proposed a cosine similarity-based unsupervised 
learning approach for extracting relations from a large raw corpus. The context words 
in between the same entity pairs in different sentences are used to form word vectors, 
which are then clustered according to the cosine similarity. This approach does not 
rely on any annotated corpus and works effectively on high-frequent entity pairs [8]. 
However, there are two problems in this approach: 
? The assumption that the same entity pairs in different sentences have the same 
relation. 
? The cosine similarity measure between the flat feature vectors, which only con-
sider the words between entities. 
In this paper, we propose a tree similarity-based unsupervised learning approach 
for relation extraction. In order to resolve the above two problems in Hasegawa et al 
[8], we assume that the same entity pairs in different sentences can have different 
relation types. Moreover, rather than the cosine similarity measure, a similarity func-
tion over parse trees is proposed to capture much larger feature spaces instead of the 
simple word features. 
The rest of the paper is organized as follows. In Section 2, we discuss the proposed 
tree-similarity-based clustering algorithm. Section 3 shows the experimental result. 
Section 4 compares our work with the previous work. We conclude our work with a 
summary and an outline of the future direction in Section 5. 
2   Tree Similarity-Based Unsupervised Learning 
We use the shallow parse tree as the representation of relation instances, and regard 
relation extraction as a clustering problem on shallow parse trees. Our method con-
sists of three steps: 
1) Calculating the similarity between two parse trees using a tree similarity func-
tion; 
2) Clustering relation instances based on the similarities using a hierarchical 
clustering algorithm; 
3) Labeling each cluster using indicative words as its relation type, and pruning 
out unreliable clusters.  
In this section, we introduce the parse tree representation for a relation instance, 
define the tree similarity function, and describe the clustering algorithm. 
2.1   Parse Tree Representation for Relation Instance 
A parse tree T is a set of node {p1?pn}, which are connected hierarchically. Here, a 
node pi includes a set of features { f1,?, f4} as follows: 
380 M. Zhang et al 
? Head Word ( 1f ): for a leaf (or terminal) node, it is the word itself of the leaf 
node; for a non-terminal node, it is a ?Head Word? propagated from a leaf 
node. This feature defines the main meaning of the phrase or the sub-tree rooted 
by the current node. 
? Node Tag ( 2f ): for a leaf node, it is the part-of-speech of this node; for a non-
terminal node, it is a phrase name, such as Noun Phrase (NP), Verb Phrase (VP). 
This feature defines the linguistic category of this node. 
? Entity Type ( 3f )2:it indicates the entity type which can be PER, COM or GPE 
if the current node refers to a Named Entity.  
? Relation Order ( 4f ): it is used to differentiate asymmetric relations, e.g., ?A 
belongs to B? or ?B belongs to A?. 
These features are widely-adopted in Relation Extraction task.  In the parse tree repre-
sentation, we denote by .
i jfp  the j
th
 feature of node ip , by [ ]ip j   the j
th
 child of 
node ip , and by [ ]ip C  the set of all children of node ip , i.e., [ ] [ ]i ip j p? C .   
2.2   Tree Similarity Function  
Inspired by the special property of kernel-based methods3, we extend the tree kernels 
in Zelenko et al [3] to a novel tree similarity measure function, and apply the above 
tree similarity function to unsupervised learning for relation extraction. Mostly, in 
previous work, kernels are used in supervised learning algorithms such as SVM, Per-
ceptron and PCA (Collins and Duffy, 2001). In our approach, the hierarchical cluster-
ing algorithm is adopted, this allows us to explore more robust and powerful similar-
ity functions, other than a proper kernel function4.  
A similarity function returns a normalized, symmetric similarity score in the range 
[0, 1]. Especially, our tree similarity function 1 2( , )K T T over two trees 1T  and 2T , with 
the root nodes 1r  and 2r , is defined as follows: 
1 2 1 2 1 2 1 2
( , ) ( , ) * ( , ) ( [ ], [ ]){ }
C
K T T m s Kr r r r r r= + c c                                   (1) 
where,  
                                                          
2
  For the features of ?Entity Type?, please refer to the literature ACE [22] for details. 
3
  As an alternative to the feature-based method [5], the advantage of kernels [9] is that they can 
replace any dot product between input points in a high dimensional feature space. Compared 
with the feature-based method, the kernel method displays several unique characteristics, 
such as implicitly mapping the feature space from low-dimension to high-dimension, and ef-
fectively modeling structure data. A few kernels over structured data have been proposed in 
NLP study [10-16]. Zelenko et al [3] and Culotta et al [4] explored tree kernels with SVM 
[9] for relation extraction. We study the tree kernels from similarity measure viewpoints. 
4
  A function is a kernel function if and only if the function is symmetric and positive semi-
definite [3, 9].  
 Discovering Relations Between Named Entities from a Large Raw Corpus 381 
? , )( i jm p p is a matching function over the features of two tree nodes ip  and jp . 
In this paper, only the node tag feature ( 2f ) is considered:  
2 2  
, )  1      if . .(
 0     otherwise
 ji
i j
f fp p
m p p
?? =???
=                                                (2) 
The binary function (1) means that two nodes are matched only if they share 
the same Node Tag. 
? 1 2( , )p ps  is a similarity function between two nodes ip  and jp : 
1 1
3 3
1 1
  
 &   
if 
, else if
other features match
no match
 
 
       
. .
1         
. .
( ) 0.5       . .
0.25    
0    
i j
i j
jii j
p f p f
p f p f
p p f fp ps
? =??
=??
????
= =?????
                                 (3) 
where the values of the weights are assigned empirically according to the discrimina-
tive ability of the feature types. Function (3) measures the similarity between two 
nodes according to the weights of matched features. 
? CK  is the similarity function over the two children node sequences 1[ ]p c  
and 2[ ]p c :  
 
1 2 1 2
, , ( ) ( )
( [ ], [ ]) ( [ ], [ ])argmax { }
C l l
K p p K p p
=
=
a b a b
c c a b                              (4)  
  
1 2 21
( )
1
( [ ], [ ]) ( [ ], [ ])
l
i
K p p K p p
=
= ? i i
a
a ba b                                             (5) 
where a and b are two index sequences, i.e., a is a sequence  1 10 ... [ ]ma a p< < ? | |C  and 
l(a) is the length of sequence a, and likewise for b. The node set 
1 1 1
[ ] { [ ], ..., [ ]}p p p= 1 ma a a  is the subset of 1[ ]p c , 1 1[ ] [ ]p p?a c , 1[ ]p ai is the i
th
 
node of
1
[ ]p a , and likewise for 2p . 
We define 1 2( , )K T T in terms of the similarity function 1 2( , )r rs  between the par-
ent nodes and the similarity function CK  over the two children node sequences 1[ ]r c  
and 2[ ]r c . Formula (5) defines the similarity between two node sequences by sum-
ming up the similarity of each corresponding node pair. CK  in Formula (4) searches 
out such two children node subsequences 1[ ]p a and 2[ ]p b , which has the maximum 
node sequence similarity among all the possible combining pairs of node subse-
quences. Given the similarity scores of all children node pairs, Formula (4) can be 
382 M. Zhang et al 
easily resolved by the dynamic programming (DP) algorithm5. By traversing the two 
trees from top to down and applying the DP algorithm layer by layer, the parse tree 
similarity 1 2( , )K T T defined by Formula (1) is obtained. Due to the DP algorithm, the 
defined tree similarity function is computable in O(mn), where m and n are the num-
ber of nodes in the two trees, respectively. The matching function , )( i jm p p in For-
mula (2) can narrow down the search space during similarity calculation, since the 
sub-trees with unmatched root nodes are unnecessary to be further explored.  
 
Fig. 1.  Sub-structure with maximum similarity 
From the above discussion, we can see that our defined tree similarity function is 
trying to detect the two trees? maximum isomorphic sub-structures. The similarity 
score between the maximum isomorphic sub-structures is returned as the value of the 
similarity function. Fig. 1 illustrates the sub-structures with the maximum similarity 
between two trees. Among the all matched sub-structures, only the sub-structures 
circled by the dashed lines are the isomorphic sub-structures with the maximum simi-
larity. The similarity score between the sub-structures is obtained by summing up the 
similarity score between the corresponding matched nodes.  
Finally, since the size of the input parse tree is not constant, the similarity score is 
normalized as follows: 
1 2
1 1 2 2
1 2
( , )
( , )* ( , )
? ( , ) K T T
K T T K T T
K T T =                (6) 
The value of 1 2? ( , )K T T ranges from 0 to 1. In particular, 1 2? ( , )K T T =1 if and only 
if 1 2T T= . For example, given two parse trees A and B, and A is a subtree of B, then 
under Formula (1), K(A, B) = K(A, A). However, after the normalization through 
                                                          
5
  A well-known application of Dynamic Programming is to compute the edit distance between 
two character strings. Let us regard a node as a character and a node sequence as a character 
string. Then given the similarity  score between nodes, Formula (4) can be resolved using DP 
algorithm in the same way as that of strings. Due to space limitation, the implementation 
deatils are not discussed here. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 383 
Equation (6), we can get ? ?
  ( , ) ( , ) 1K A B K A A< = . In this way, we can differentiate such 
two cases.  
According to the Formula (1) to (5), the similarity function 1 2( , )K T T  over the two 
trees in Fig. 1 is computed as follows: 
1 2 ([NP, VP], [NP, VP])
([bought, NP], [sold, NP, yesterday])
1 bought sold (NP, NP)
= 1+0.25+0.25+ ([a, red, car]
( , ) (S,S) *{ (S,S) }
0.25 (NP, NP) (VP, VP)
0.25 0.25 (Paul, Smith) 0.25
  
( , )
c
c
c
K T T m s K
K K
K
K
K K
K
= +
= +
= + +
= + + +
+
+
, [the, flat])
1.5 a the car flat( , ) ( , )
2
K K= + +
=
 
The above similarity score is more than one. This is because we did not normalize 
the score using Formula (6). 
2.3   Tree Similarity Based Unsupervised Learning 
Our method consists of five steps: 
1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE 
types provide more effective information for relation discovery. Here we use Sekine?s 
NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are 
defined [21]. This NE tagger has also been adopted by Hasegawa et al [8]. Besides, 
Collin?s parser [18] is adopted to generate shallow parse trees.  
2) Similarity calculation: The similarity between two relation instances is defined 
between two parse trees. However, the state-of-the-art of parser is always error-prone. 
Therefore, we only use the minimum span parse tree including the NE pairs when 
calculating the similarity function [4]. Please note that the two entities may not be the 
leftmost or rightmost node in the sub-tree. 
3) NE pairs clustering: Clustering of NE pairs is based on the similarity score gener-
ated by the tree similarity function. Rather than k-means [17], we used a bottom-up 
hierarchical clustering method so that there is no need to determine the number of 
clusters in advance. This means that we are not restricted to the limited types of rela-
tions defined in MUC [1] or ACE [22]. Therefore, more substantial existing relations 
can be discovered. We adopt the group-average clustering algorithm [17] since it 
produces the best performance compared with the complete-link and single-link algo-
rithms in our study.  
 
4) Cluster labeling: In our study, we label each cluster by the most frequent ?Head 
Word? in this cluster. As indicated in subsection 2.1, the ?Head Word? of root node 
defines the main meaning of a parse tree. This way, the ?Head Word? of the root 
384 M. Zhang et al 
node of the minimum span tree naturally characterizes the relation between this NE 
pair in this tree. Thus, we simply count the frequency of the ?Head Word? of the root 
node in the cluster, and then chose the most frequent ?Head Word? as the relation 
type of the cluster.  
 
5) Cluster pruning: Unreliable clusters may be generated due to various reasons 
such as divergent relation type distributions and the fact that most of the entity pairs 
inside this cluster are totally unrelated. Therefore, pruning is necessary and done in 
our approach using two criteria. Firstly, if the most frequent ?Head Word? occurs 
less than a predefined percentage in this cluster, which means that the relation type 
defined by this ?Head Word? is not significant statistically, the cluster is pruned out. 
Secondly, we prune out the clusters whose NE pair number is below a predefined 
threshold because such clusters may not be representative enough for this relation.  
3   Experiments 
3.1   Experimental Setting 
To verify our proposed method and establish proper comparison with Hasegawa et al 
[8], we use the same corpus ?The New York Times (1995)?, and evaluate our work on 
the same two kinds of NE pairs: COMPANY-COMPANY (COM-COM) and 
PERSON-GPE (PER-GPE) as Hasegawa et al in [8]. First, we iterate over all pairs of 
Named Entities occurring in the same sentence to generate potential relation in-
stances. Then, according to the co-occurrence frequency of NE pairs, all the relation 
instances are grouped into three categories: 
1) High frequent instances with the co-occurrence frequency not less than 30. In 
this category, only the relation instances, which satisfy the all criteria of Ha-
segawa et al [8]6, are kept for final experiment. By doing so, this category 
data is the same as the entire experimental set used by Hasegawa et al [8]. 
2) Intermediate frequent instances with the co-occurrence frequency between 5 
and 30. In this category, only two distinct NE pairs are randomly picked at 
each frequency for final evaluation due to the large number of such NE pairs. 
3) Less frequent instances with the co-occurrence frequency not more than 5. In 
this category, twenty distinct NE pairs are randomly picked at each fre-
quency for final evaluation due to the similar reason as 2). 
Table 1 reports the statistics of the entire evaluation corpus7 which is manually 
tagged. Table 2 reports the percentage of the NE pairs which carry more than one 
relation types when occurring at different relation instances. The numbers inside pa-
rentheses in Table 1 and Table 2 correspond to the statistical values of the NE pair 
?PER-GPE?, while the numbers outside parentheses are related to the NE pair ?COM-
COM?. Table 2 shows that at least 9.88% of distinct NE pairs have more than one 
                                                          
6
 To discover reliable relations, Hasegawa et al [8] sets five conditions to generate relation 
instance set. NE pair co-occurrence more than 30 times is one of the five conditions. 
7
  Due to the parsing errors and NE tagging errors, the actual number of relation instances is 
less than the theory number that we should pick up. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 385 
relation types in the test corpus. Thus it is reasonable and necessary to assume that 
each occurrence of NE pairs forms one individual relation instance. 
Table 1. Statistics on the manually annotated evaluation data 
Category by 
frequency 
# of instances # of distinct NE pairs    # of relation  
types 
High 8931 (13205) 65 (177) 10 (38) 
Intermediate 672  (783) 38 (41) 6 (7) 
Less 276  (215) 76 (81) 5 (8) 
Table 2. % of distinct NE pairs with more than one relation types on the evaluation data 
Category by frequency % of NE pairs have more than one relations 
    High  15.4   (12.99) 
    Intermediate 28.9   (24.4) 
    Less 11.8   (9.88) 
3.2   Evaluation Measures 
All the experiments are carried out against the manually annotated evaluation corpus. 
We adopt the same criteria as Hasegawa et al [8] to evaluate the performance of our 
method. Grouping and labeling are evaluated separately. For grouping evaluation, all 
the single NE pair clusters are labeled as non-relation while all the other clusters are 
labeled as the most frequent relation type counted in this cluster. For each individual 
relation instance, if the manually assigned relation type is the same as its cluster label, 
the grouping of this relation instance is counted as correct, otherwise, are counted as 
incorrect. Recall (R), Precision (P) and F-measure (F) are adopted as the main per-
formance measure for grouping [8]. For labeling evaluation, a cluster is labeled cor-
rectly only if the labeling relation type, represented by most frequent ?Head Word? 
of the root node of the minimal-span subtree, is the same as the cluster label gotten in 
the grouping process. 
3.3   Experimental Results 
Like other applications using clustering algorithms, the performance of the proposed 
method also depends on the threshold of the clustering similarity. Here this threshold 
is used to truncate the hierarchical tree, so that the different clusters are generated. 
When the threshold is set to 1, then each individual relation instance forms one unique 
group; when the threshold is set to 0, then the all relation instance form one big group. 
Table 3 reports the evaluation results of grouping, where the best F-measures and the 
corresponding similarity thresholds are listed. We can see that our method not only 
achieves good performance on the high-frequent data, but also performs well on the 
386 M. Zhang et al 
intermediate and less-frequent data. The higher frequency, the higher performance. 
Since the best thresholds of the two NE cases are the almost same, we just fix the 
universal threshold as the one used in ?PER-GPE? case in each category.  
Table 3.  Performance evaluation of Grouping phase, the numbers inside parentheses corre-
spond to the evaluation score of ?PER-GPE? while the numbers outside parentheses are related 
to ?COM-COM?. 
Performance Category by fre-
quency 
  F P (%) R (%) 
Threshold 
High 80 (87) 82 (90) 78 (84) 0.28 (0.29) 
Intermediate 74 (76) 87 (84) 64 (69)  0.32 (0.30) 
Less   62 (65) 75 (77) 53 (56)  0.36 (0.35) 
Table 4. Best performance comparison in the high-frequent data (F) 
 Our approach  Hasegawa et al [8] 
PER-GPE 87 82 
COM-COM 80 77 
Table 4 compares the performances of the proposed method and Hasegawa et al 
[8], where the best F-measures on the same high-frequent data are reported. Table 4 
shows that our method outperforms the previous approach by 5 and 3 F-measures in 
clustering NE pairs of ?PER-GPE? and ?COM-COM?, respectively.  
An interesting phenomenon is that the best threshold is set to be just above 0 for 
the cosine similarity in Hasegawa et al [8]. This means that each word feature vector 
of each combination of NE pairs in the same cluster shares at least one word in com-
mon --- and most of these common words were pertinent to the relations [8]. This also 
prevents them from working well on less-frequent data [8]. In contrast, for the simi-
larity function in our approach, the best threshold is much greater than 0. The differ-
ence between the two thresholds implies that the similarity function over the parse 
trees can capture more common structured features than the word feature vectors can. 
This is also the reason why our method is effective on both high and less- 
frequent data. 
It is not surprising that we do have that a few identical NE pairs, occurring in dif-
ferent relation instances, are grouped into different relation sets. For example, the NE 
pairs ?General Electric Co. and NBC?, in one sentence ?General Electric Co., which 
bought NBC in 1986, will announce a new marketing plan.?, is grouped into the rela-
tion set ?M&A?, but in another sentence ?Prime Star Partners and General Electric 
Co., parent of NBC, has signed up 430,000 subscribers.?, is grouped into another 
relation set ?parent?. Among all the NE pairs that carry more than one relation types, 
41.8% of them are grouped correctly using our tree similarity function.  
 Discovering Relations Between Named Entities from a Large Raw Corpus 387 
The performance of grouping is the upper bound of the performances of labeling 
and pruning. In the final, there are 146 PER-GPE clusters and 95 COM-COM clusters 
are generated after grouping. Out of which, only 57 PER-GPE clusters and 42 COM-
COM clusters are labeled correctly before pruning. This is because that a large por-
tion of the non-relation clusters are labeled as one kind of true relations. After prun-
ing, 117 PER-GPE clusters and 84 COM-COM clusters are labeled correctly. This is 
because lots of the non-relation clusters are labeled correctly by the pruning process, 
so we can say that pruning is a non-relation labeling process, which greatly improves 
the performance of labeling.  
The experimental results discussed above suggest that our proposed method is an 
effective solution for discovering relation from a large raw corpus. 
4   Discussions 
It would be interesting to review and summarize how the proposed method deals with 
the relation extraction issue differently from other related works. Table 5 in the next 
page summarizes the differences between our method and Hasegawa et al [8]. 
Table 5. The differences between our method and Hasegawa et al [8] 
 Our approach  Hasegawa et al [8] 
Similarity  
Measure 
tree similarity over parse 
tree structures 
cosine similarity between the 
context word feature vectors 
Assumption No Yes (The same entity pairs in 
different sentences have the 
same relation) 
Labeling the most frequent ?Head 
Word? of the root node of 
sub-tree 
the most frequent context 
word 
Pruning Yes (We present two prun-
ing criterion) 
No 
Data Frequency effective on both high and 
less-frequent data 
effective only on high-
frequent data 
In addition, since our tree similarity function has benefited from the relation tree 
kernels of Zelenko et al [3], let us compare our similarity measure function with their 
relation kernel function [3] from the viewpoint of computational efficiency. Zelenko 
et al [3] defined the first parse tree kernels for relation extraction, and then this  
relation tree kernels were extended to dependency tree kernels by Culotta et al [4].  
Their tree kernels sum up the similarity scores among all possible subsequences of 
children nodes with matching parents, and give a penalty to longer sequences. Their 
388 M. Zhang et al 
tree kernels are closely related to the convolution kernels [12]. But, by doing so, lots 
of sub-trees will be considered again and again. An extreme case occurs when two 
tree structures are identical. In that situation all the sub-trees will be considered 
exhaustedly, even if the sub-tree is a part of other bigger sub-trees. We use the maxi-
mum score in Formula (4) instead of the summation in our approach. With our ap-
proach, the entire tree is only considered once. The replacement of summation with 
maximization reduces the computational time greatly. 
5   Conclusions and Future Directions 
We modified the relation tree kernels [3] to be a tree similarity measure function by 
replacing the summation over all possible subsequences of children nodes with 
maximization, and used it in clustering for relation extraction. The experimental result 
showed much improvement over the previous best result [8] on the same test corpus. 
It also showed that our method is high effective on both high-frequent and less-
frequent data. Our work demonstrated the effectiveness of combining the tree similar-
ity measure with unsupervised learning for relation extraction. 
Although our method shows good performance, there are still other aspects of the 
proposed method worth discussing here. Without additional knowledge, relation de-
tecting and relation labeling are still not easy to be resolved, especially in less-
frequent data. We expect that using additional easily-acquired knowledge can im-
prove the performance of the proposed method. For example, we can introduce the 
WordNet [19] thesaurus information into Formula (3) to obtain more accurate node 
similarities and resolve data sparse problem. We can also use the same resource to 
improve the labeling scheme and find more abstract relation types like the definitions 
used in ACE program [22].  
References  
1. MUC. 1987-1998. The nist MUC website:  http://www.itl.nist.gov/iaui/894.02/related_ 
projects/muc/ 
2. Miller, S., Fox, H., Ramshaw, L. and Weischedel, R. 2000. A novel use of statistical pars-
ing to extract information from text. Proceedings of NAACL-00 
3. Zelenko, D., Aone, C. and Richardella, A. 2003. Kernel Methods for Relation Extraction. 
Journal of Machine Learning Research. 2003(2):1083-1106 
4. Culotta, A. and Sorensen, J. 2004. Dependency Tree Kernel for Relation Extraction. Pro-
ceeding of ACL-04 
5. Kambhatla, N. 2004. Combining Lexical, Syntactic, and Semantic Features with Maxi-
mum Entropy Models for Extracting Relations. Proceeding of ACL-04, Poster paper. 
6. Agichtein, E. and Gravano, L. 2000. Snow-ball: Extracting Relations from Large Plain-
text Collections. Proceedings of the Fifth ACM International Conference on Digital Li-
braries. 
7. Stevenson, M. 2004. An Unsupervised WordNet-based Algorithm for Relation Extraction. 
Proceedings of the 4th LREC workshop "Beyond Named Entity: Semantic Labeling for 
NLP tasks" 
 Discovering Relations Between Named Entities from a Large Raw Corpus 389 
8. Hasegawa, T., Sekine, S. and Grishman, R. 2004. Discovering Relations among Named 
Entities from Large Corpora. Proceeding of ACL-04 
9. Vapnik, V. 1998. Statistical Learning Theory. John Wiley 
10. Collins, M. and Duffy, N. 2001. Convolution Kernels for Natural Language. Proceeding of 
NIPS-01 
11. Collins, M. and Duffy, N. 2002. New Ranking Algorithm for Parsing and Tagging: Kernel 
over Discrete Structure, and the Voted Perceptron. Proceeding of ACL-02. 
12. Haussler, D. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-
CRL-99-10, University of California 
13. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N. and Watkins, C. 2002. Text clas-
sification using string kernel. Journal of Machine Learning Research, 2002(2):419-444 
14. Suzuki, J., Hirao, T., Sasaki Y. and Maeda, E. 2003. Hierarchical Directed Acyclic Graph 
Kernel: Methods for Structured Natural Language Data. Proceedings of ACL-03 
15. Suzuki, J., Isozaki, H. and Maeda, E. 2003. Convolution Kernels with Feature Selection 
for Natural Language Processing Tasks. Proceedings of ACL-04 
16. Moschitti, A. 2004. A study on Convolution Kernels for Shallow Semantic Parsing. Pro-
ceedings of ACL-04 
17. Manning, C. and Schutze, H. 1999. Foundations of Statistical Natural Language Process-
ing. The MIT Press: 500-527 
18. Collins, M. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. 
Thesis. University of Pennsylvania 
19. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database and some of its Applica-
tions. Cambridge, MA: MIT Press. 
20. Sekine, S. 2001. OAK System (English Sentence Analysis). Http://nlp.cs.nyu.edu/oak 
21. Sekine, S., Sudo, K. and Nobata, C. 2002. Extended named entity hierarchy. Proceedings 
of LREC-02 
22. ACE. 2004. The Automatic Content Extraction (ACE) Projects. http://www.ldc.upenn.edu/ 
Projects/ACE/ 
  	
   	

	 	
	  	 	

A Twin-Candidate Model of Coreference
Resolution with Non-Anaphor
Identification Capability
Xiaofeng Yang1,2, Jian Su1, and Chew Lim Tan2
1 Institute for Infocomm Research,
21, Heng Mui Keng Terrace, Singapore, 119613
{xiaofengy, sujian}@i2r.a-star.edu.sg
2 Department of Computer Science,
National University of Singapore, Singapore, 117543
{yangxiao, tancl}@comp.nus.edu.sg
Abstract. Although effective for antecedent determination, the tradi-
tional twin-candidate model can not prevent the invalid resolution of
non-anaphors without additional measures. In this paper we propose a
modified learning framework for the twin-candidate model. In the new
framework, we make use of non-anaphors to create a special class of
training instances, which leads to a classifier capable of identifying the
cases of non-anaphors during resolution. In this way, the twin-candidate
model itself could avoid the resolution of non-anaphors, and thus could
be directly deployed to coreference resolution. The evaluation done on
newswire domain shows that the twin-candidate based system with our
modified framework achieves better and more reliable performance than
those with other solutions.
1 Introduction
In recent years supervised learning approaches have been widely used in corefer-
ence resolution task and achieved considerable success [1,2,3,4,5]. Most of these
approaches adopt the single-candidate learning model, in which coreference rela-
tion is determined between a possible anaphor and one individual candidate at a
time [1,3,4]. However, it has been claimed that the reference between an anaphor
and its candidate is often subject to the other competing candidates [5]. Such
information is nevertheless difficult to be captured in the single-candidate model.
As an alternative, several researchers proposed a twin-candidate model [2,5,6].
Instead of directly determining coreference relations, this model would judge the
preference between candidates and then select the most preferred one as the an-
tecedent. The previous work has reported that such a model can effectively help
antecedent determination for anaphors [5,6].
However, one problem exits with the twin-candidate model. For every encoun-
tered NP during resolution, the model would always pick out a ?best? candidate
as the antecedent, even if the current NP is not an anaphor. The twin-candidate
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 719?730, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
720 X. Yang, J. Su, and C.L. Tan
model itself could not identify and block such invalid resolution of non-anaphors.
Therefore, to apply such a model to coreference resolution, some additional ef-
forts have to be required, e.g., using an anaphoricity determination module to
eliminate non-anaphors in advance [5], or using threshold to prevent the selection
of a candidate if the confidence it wins other competitors is low [6].
In this paper, we explore how to effectively apply the twin-candidate model
to the coreference resolution task. We propose a modified learning framework
with the capability of processing non-anaphors. In the framework, we make use of
non-anaphors to create training instances. This special class of instances would
enable the learned classifier to identify the test instances formed by non-anaphors
during resolution. Thus, the resulting model could avoid resolving a non-anaphor
to a non-existent antecedent by itself, without specifying a threshold or using an
additional anaphoricity determination module. Our experiments on MUC data
set systematically evaluated effectiveness of our modified learning framework.
We found that with this new framework, the twin-candidate based system could
not only outperform the single-candidate based one, but also achieve better and
more reliable results than those twin-candidate based systems using the two
mentioned solutions.
The rest of the paper is organized as follows. Section 2 describes the original
framework of the twin-candidate model. Section 3 presents in details the modified
framework, including the training and resolution procedures. Section 4 reports
and discusses the experimental results and finally Section 6 gives the conclusions.
2 The Original Framework of the Twin-Candidate Model
The basic idea of the twin-candidate model is to learn a binary classifier which
could judge the preference between candidates of an anaphor. In this section we
will describe a general framework of such a model.
2.1 Instance Representation
In the twin-candidate model, an instance takes a form like i{C1, C2, M }, where
M is a possible anaphor and C1 and C2 are two of its antecedent candidates.
We stipulate that C2 should be closer to M than C1 in distance. An instance is
labelled as ?10? if C1 is preferred to C2 to be the antecedent, or ?01? if otherwise.
A feature vector would be specified for an instance. The features may describe
the lexical, syntactic, semantic and positional relationships between M and each
one of the candidates, C1 or C2. In addition, inter-candidate features could
be used to represent the relationships between the pair of candidates, e.g. the
distance between C1 and C2 in position.
2.2 Training Procedure
For each anaphor Mana in a given training text, its closet antecedent, Cante,
would be selected as the anchor candidate to compare with other candidates.
A Twin-Candidate Model of Coreference Resolution 721
A set of ?10? instances, i{Cante, Cp, Mana}, is generated by pairing Mana and
Cante, as well as each of the interning candidates Cp. Also a set of ?01? instances,
i{Ca, Cante, Mana}, is created by pairing Cante and each non-antecedental can-
didate Ca before Cante.
Table 1. An example text
[1 Globalstar] still needs to raise [2 $600 million], and [3
Schwartz] said [4 that company] would try to raise [5 the money]
in [6 the debt market] .
Consider the example in Table 1. In the text segment, [4 that company] and
[5 the money] are two anaphors with [1 Globalstar] and [2 $600 million] being
their antecedents respectively. Thus the training instances to be created for this
text would be:
i{[1 Globalstar], [2 $600 million], [4 that company]} : 10
i{[1 Globalstar], [3 Schwartz], [4 that company]} : 10
i{[1 Globalstar], [2 $600 million], [5 the money]} : 01
i{[2 $600 million], [3 Schwartz], [5 the money]} : 10
i{[2 $600 million], [4 that company], [5 the money]} : 10
Based on the training instances, a classifier is trained using a certain machine
learning algorithm. Given the feature vector of a test instance, the classifier
would return ?10? or ?01? indicating which one of the two candidates under
consideration is preferred.
2.3 Resolution
After the classifier is ready, it could be employed to select the antecedent for
an encountered anaphor. The resolution algorithm is shown in Figure 1. In the
algorithm, a round-robin model is employed, in which each candidate is compared
with every other candidate and the final winner is determined by the won-lost
records. The round-robin model would be fair for each competitor and the result
is reliable to represent the rank of the candidates.
As described in the algorithm, after each match between two candidates,
the record of the winning candidate (i.e., the one judged as preferred by the
classifier) will increase and that of the loser will decrease. The algorithm simply
uses a unit of one as the increment and decrement. Therefore, the final record of
a candidate is its won-lost difference in the round-robin matches. Alternatively,
we can use the confidence value returned by the classifier as the in(de)crement,
while we found no much performance difference between these two recording
strategies in experiments.
722 X. Yang, J. Su, and C.L. Tan
algorithm ANTE-SEL
input:
M : the anaphor to be resolved
candidate set: the set of antecedent candidates of M,
{C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
/*CR returns the classification result*/
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
return CSelectedIdx
Fig. 1. The original antecedent selection algorithm
3 Modified Framework for Coreference Resolution Task
3.1 Non-anaphor Processing
In the task of coreference resolution, it is often that an encountered NP is non-
anaphoric, that is, no antecedent exists among its possible candidates. However,
the resolution algorithm described in the previous section would always try to
pick out a ?best? candidate as the antecedent for each given NP, and thus could
not be applied for coreference resolution directly.
One natural solution to this is to use an anaphoricity determination (AD)
module to identify the non-anaphoric NPs in advance (e.g. [5]). If an NP is judged
as anaphoric, then we deploy the resolution algorithm to find its antecedent.
Otherwise we just leave the NP unresolved. This solution, however, would heavily
rely on the performance of the AD module. Unfortunately, the accuracy that
most state-of-the-art AD systems could provide is still not high enough (around
80% as reported in [7]) for our coreference resolution task.
Another possible solution is to set a threshold to avoid selecting a candidate
that wins with low confidence (e.g. [6]). Specifically, for two candidates in a
match, we update their match records only if the confidence returned from the
classifier is above the specified threshold. If no candidate has a positive record in
the end, we deem the NP in question as non-anaphoric and leave it unresolved.
In other words, a NP would be resolved to a candidate only if the candidate won
at least one competitor with confidence above the threshold.
The assumption under this solution is that the classifier would return low con-
fidence for the test instances formed by non-anaphors. Although it may be true,
A Twin-Candidate Model of Coreference Resolution 723
there exist other cases for which the classifier would also assign low confidence
values, for example, when the two candidates of an anaphoric NP both have
strong or weak preference. The solution of using threshold could not discrimi-
nate these different cases and thus may not be reliable for coreference resolution.
In fact, the above problem could be addressed if we could teach the classi-
fier to explicitly identify the cases of non-anaphors, instead of using threshold
implicitly. To do this, we need to provide a special set of instances formed by
the non-anaphors to train the classifier. Given a test instance formed by a non-
anaphor, the newly learned classifier is supposed to give a class label different
from the instances formed by anaphors. This special label would indicate that
the current NP is a non-anaphor, and no preference relationship is held be-
tween the two candidates under consideration. In this way, the twin-candidate
model could do the anaphoricity determination by itself, without any additional
pre-possessing module. We will describe the modified training and resolution
procedures in the subsequent subsections.
3.2 Training
In the modified learning framework, an instance also takes a form like i{C1,
C2, M }. During training, for an encountered anaphor, we create ?01? or ?10?
training instances in the same way as in the original learning framework, while
for a non-anaphor Mnon ana, we
? From the candidate set, randomly select a candidate Crand as the anchor
candidate.
? Create an instance by pairing Mnon ana, Crand, and each of the candidates
other than Crand.
The above instances formed by non-anaphors would be labelled as ?00?. Note that
an instance may have a form like i{Ca, Crand, Mnon ana} if candidate Ca is pre-
ceding Crand, or like i{Crand, Cp, Mnon ana} if candidate Cp is following Crand.
Consider the text in Table 1 again. For the non-anaphors [3 Schwartz] and
[6 the debt market], supposing the selected anchor candidates are [1 Globalstar]
and [2 $600 million], respectively. The ?00? instances generated for the text are:
i{[1 Globalstar], [2 $600 million], [3 Schwartz]} : 00
i{[1 Globalstar], [2 $600 million], [6 the debt market]} : 00
i{[2 $600 million], [3 Schwartz], [6 the debt market]} : 00
i{[2 $600 million], [4 that company], [6 the debt market]} : 00
i{[2 $600 million], [5 the money], [6 the debt market]} : 00
3.3 Resolution
The ?00? training instances are used together with the ?01? and ?10? ones to
train a classifier. The resolution procedure is described in Figure 2. Like in the
original algorithm, each candidate is compared with every other candidate. The
724 X. Yang, J. Su, and C.L. Tan
difference is that, if two candidates are judged as ?00? in a match, both candi-
dates would receive a penalty of ?1 in their respective record; If no candidate
has a positive final score, then the NP would be deemed as non-anaphoric and
left unresolved. Otherwise, it would be resolved to the candidate with highest
score as usual. In the case when an NP has only one antecedent candidate, a
pseudo-instance is created by paring the candidate with itself. The NP would be
resolved to the candidate if the return label is not ?00?.
Note that in the algorithm a threshold could still be used, for example, to
update the match record only if the classification confidence is high enough.
algorithm ANTE-SEL
input:
M : the new NP to be resolved
candidate set: the candidates set of M, {C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
if CR(i{Ci, Cj, M}) ) = = 00 then
Score[ i ]??;
Score[ j ]??;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
if (Score[SelectedIdx] <= 0)
return nil;
return CSelectedIdx;
Fig. 2. The new antecedent selection algorithm
4 Evaluation and Discussion
4.1 Experiment Setup
The experiments were done on the newswire domain, using MUC coreference
data set (Wall Street Journal articles). For MUC-6 [8] and MUC-7 [9], 30 ?dry-
run? documents were used for training as well as 20-30 documents for testing.
In addition, another 100 annotated documents from MUC-6 corpus were also
prepared for the purpose of deeper system analysis. Throughout the experiments,
C5 was used as the learning algorithm [10]. The recall and precision rates of
the coreference resolution systems were calculated based on the scoring scheme
proposed by Vilain et al [11].
A Twin-Candidate Model of Coreference Resolution 725
Table 2. Features for coreference resolution using the twin-candidate model
Features describing the new markable M :
1. M DefNP 1 if M is a definite NP; else 0
2. M IndefNP 1 if M is an indefinite NP; else 0
3. M ProperNP 1 if M is a proper noun; else 0
4. M Pronoun 1 if M is a pronoun; else 0
Features describing the candidate, C1 or C2, of M
5. candi DefNp 1(2) 1 if C1 (C2) is a definite NP; else 0
6. candi IndefNp 1(2) 1 if C1 (C2) is an indefinite NP; else 0
7. candi ProperNp 1(2) 1 if C1 (C2) is a proper noun; else 0
8. candi Pronoun 1(2) 1 if C1 (C2) is a pronoun; else 0
Features describing the relationships between C1(C2) and M :
9. Appositive 1(2) 1 if C1 (C2) and M are in an appositive structure; else 0
10. NameAlias 1(2) 1 if C1 (C2) and M are in an alias of the other; else 0
11. GenderAgree 1(2) 1 if C1 (C2) and M agree in gender; else 0 if disagree; -1
if unknown
12. NumAgree 1(2) 1 if C1 (C2) and M agree in number; else 0 if disagree;
-1 if unknown
13. SentDist 1(2) Distance between C1 (C2) in sentences
14. HeadStrMatch 1(2) 1 if C1 (C2) and M match in head string; else 0
15. NPStrMatch 1(2) 1 if C1 (C2) and M match in full strings; else 0
16. StrSim 1(2) The ratio of the common strings between C1 (C2) and
M , over the strings of C1 (C2)
17. SemSim 1(2) The semantic agreement of C1 (C2) against M in Word-
Net
Features describing the relationships between C1 and C2
18. inter SentDist Distance between C1 and C2 in sentences
19. inter StrSim 0, 1, 2 if StrSim 1(C1, M) is equal to, larger or less than
StrSim 1(C2, M)
20. inter SemSim 0, 1, 2 if SemSim 1(C1, M) is equal to, larger or less than
SemSim 1(C2, M)
The candidates of a markable to be resolved were selected as follows. During
training, for each encountered markable, the preceding markables in the current
and previous four sentences were taken as the candidates. During resolution, for
a non-pronoun, all the preceding markables were included into the candidate
set, while for a pronoun, only the markables in the previous four sentences were
used, as the antecedent of a pronoun usually occurs in a short distance.
For MUC-6 and MUC-7, our modified framework generated 207k training
training instances, three times larger than the single-candidate based system
by Soon et al[3]. Among them, the ratio of ?00?,?01? and ?10? instances was
around 8:2:1. The distribution of the class labels was more balanced than in Soon
et al?s system, where only 5% training instances were positive while others were
all negative.
In our study we only considered domain-independent features that could be
obtained with low computational cost but with high reliability. Table 2 summa-
rizes the features with their respective possible values. Features f1-f17 record
726 X. Yang, J. Su, and C.L. Tan
the properties of a new markable and its two candidates, as well as their relation-
ships. Most of these features could be found in previous systems on coreference
resolution (e.g. [3], [4]). In addition, three inter-candidate features, f18-f20,
mark the relationship between the two candidates. The first one, inter SentDist,
records the distance between the two candidates in sentences, while the latter
two, inter StrSim and inter SemSim compare the similarity scores of the two
candidates, in string-matching and semantics respectively.
To provide necessary information of feature computation, an input raw text
was preprocessed automatically by a pipeline of NLP components. Among them,
the chunking component was trained and tested for the shared task for CoNLL-
2000 and achieved 92% F-score. The HMM based NE recognition component
was capable of recognizing the MUC-style NEs with F-scores of 96.9% (MUC-6)
and 94.3% (MUC-7).
4.2 Results and Discussion
In the experiment we compared four systems:
SC. The system based on the single-candidate model. It was a duplicate of the
system by Soon et al [3]. The feature set used in the baseline system was
similar to those listed in Table 2, except that no inter-candidate feature
would be used and only one set of features related to the single candidate
was required.
TC AD. The system based on the twin-candidate mode with the original learn-
ing framework, in which non-anaphors were eliminated by an anaphoricity
determination module in advance. We built a supervised learning based AD
module similar to the system proposed by Ng and Cardie [7]. We trained
the AD classifier on the additional 100 MUC-6 documents. By adjusting the
misclassification cost parameter of C5, we obtained a set of classifiers capable
of identifying ?positive? anaphors with variant recall and precision rates.
TC THRESH. The system based on the twin-candidate mode with the origi-
nal learning framework, using threshold to discard the low-confidenced com-
parison results between candidates.
TC NEW. The system based on the twin-candidate mode, with our modified
learning framework.
The results of the four systems on MUC-6 and MUC-7 are summarized in
Table 3. In these experiments, five-fold cross-evaluation was performed on the
training data to select the resolution parameters, for example, the threshold for
systems TC THRESH and TC NEW, and final AD classifier for TC AD.
As shown in the table, the baseline system SC achieves 66.1% and 65.9%
F-measure for MUC-6 and MUC-7 data sets. This performance is better than
that reported by Soon et al [3], and is comparable to that of the state-of-the-art
systems on the same data sets.
From the table we could find system TC AD achieves a comparatively high pre-
cision but a low recall, resulting in a F-measure worse than that of SC. The analysis
A Twin-Candidate Model of Coreference Resolution 727
Table 3. The performance of different coreference resolution systems
30 Docs 100 Docs
MUC-6 MUC-7 MUC-6 MUC-7
Experiments R P F R P F R P F R P F
SC 70.4 62.4 66.1 69.8 62.5 65.9 67.9 62.1 64.9 69.8 62.5 65.9
TC AD 62.6 66.4 64.4 60.8 64.7 62.7 61.6 65.4 63.4 60.8 64.6 62.7
TC THRESH 70.7 59.1 64.4 70.0 61.7 65.6 71.0 60.7 65.4 70.6 60.9 65.4
TC NEW 64.8 70.1 67.3 66.0 68.6 67.2 67.0 70.2 68.5 67.0 69.2 68.1
of the AD classifier reveals that it successfully identifies 79.3% anaphors (79.48%
precision) for MUC-6, and 70.9% anaphors (76.3% precision) for MUC-6. That
means, although the pre-processing AD module could partly avoid the wrong res-
olution of a non-anaphor, it eliminates many anaphors at the same, which leads to
the low recall for coreference resolution. Although in resolution different AD clas-
sifiers could be applied, we only observe the tradeoff between recall and precision,
with no effective resolution improvement in F-measure.
In contrast to TC AD, system TC THRESH yields large gains in recall. The
recall, up to above 70%, is higher than all the other three systems. However, the
precision at the same time is unfortunately the lowest. Such a pattern of high
recall and low precision indicates that using threshold could reduce, to some
degree, the risk of eliminating true anaphors, but it would be too lenient to
effectively block the resolution of non-anaphors.
Compared with TC AD and TC THRESH, TC NEW produces large gains in
the precision rates, which rank the highest among all the four systems. Although
the recall also drops at the same time, the increase in the precision could compen-
sate it well; we observe a F-measure of 67.3% for MUC-6 and 67.2% for MUC-7,
significantly better (p ? 0.05, by a sign test) than the other twin-candidate
based systems. These results suggest that with our modified framework, the
twin-candidate model could effectively identify non-anaphors and block their in-
valid resolution, without affecting the accuracy of the antecedent determination
for anaphors.
In our experiment we were interested to evaluate the resolution performance
of TC NEW under different sizes of training data. For this purpose, we used the
additional 100 annotated documents for training, and plotted the learning curve
in Figure 3. The curve indicates that the system could perform well with a small
number of training data, while the performance would get further improved with
more training data (the best performance is obtained on 90 documents).
In Table 3, we also summarized the results of different systems trained on 100
documents. In contrast to TC NEW, we find for system SC, there is no much
performance difference between using 30 and 100 training documents. This is
consistent with the report by Soon et al [3] that the single-candidate model
would achieve the peak performance with a moderate size of data. In the table
we could also find that the performance improvement of TC NEW against the
other three systems is apparently larger on 100 training documents than on
30 documents.
728 X. Yang, J. Su, and C.L. Tan
0 10 20 30 40 50 60 70 80 90 100
60
61
62
63
64
65
66
67
68
69
70
Number of Documents Trained On
F?
m
ea
su
re
Fig. 3. Learning curve of system TC NEW on MUC-7
40 50 60 70 80 90 100
30
35
40
45
50
55
60
65
70
75
80
Recall
Pr
ec
is
io
n
TC_THRESH
TC_AD
TC_NEW
Fig. 4. Recall and precision results for the twin-candidate based systems
In Figure 4, we plotted the variant recall and precision scores that the three
twin-candidate based systems were capable of producing when trained on 100
documents. (Here we only showed the results for MUC-7. Similar results could
be obtained for MUC-6). In line with the results in Table 3, system TC AD
tends to obtain a high precision but low recall, while system TC THRESH
tends to obtain a high recall but low precision. Comparatively, system TC NEW
produces even recall and precision. For the range of recall within which the
three systems coincide, TC NEW yields higher precision than the other two
systems. This figure further proves the effectiveness of our modified
learning framework.
As mentioned, in systems TC THRESH and TC NEW the threshold pa-
rameter could be adjusted. It would be interesting to evaluate the influence of
different thresholds on the resolution performance. In Figure 5 we compared the
recall and precision of two systems, with thresholds ranging from 65 to 100.
In TC THRESH, when the threshold is low, the recall is almost 100% while
the precision is quite low. In such a case, all the markables, regardless anaphors or
A Twin-Candidate Model of Coreference Resolution 729
65 70 75 80 85 90 95 100
30
40
50
60
70
80
90
100
Threshold
R
es
ul
ts
TC_THRESH
Recall
Precision
65 70 75 80 85 90 95 100
0
10
20
30
40
50
60
70
80
90
Threshold
R
es
ul
ts
TC_NEW
Recall
Precision
Fig. 5. Performance of TC THRESH and TC NEW under different thresholds
non-anaphors, will be resolved. As a consequence, all the occurring markables in a
document tends to be linked together. In fact, the effective range of the threshold
that leads to an acceptable performance is quite short. The threshold would only
work when it is considerably high (above 95). Before that, the precision remains
very low (less than 40%) while the recall keeps going down with the increase of
the threshold.
By contrast, in TC NEW, both the recall and precision vary little unless the
threshold is extremely high. That means, the threshold would not impose much
influence on the resolution performance of TC NEW. This should be because in
the modified framework, the cases of non-anaphors are determined by the special
class label ?00?, instead of the threshold as in TC THRESH. The purpose of
using threshold in TC NEW is not to identify the non-anaphors, but to improve
the accuracy of class labelling. Indeed, we could obtain a good result without
using any threshold in TC NEW. These further confirm our claims that the
modified learning framework could perform more reliably than the solution of
using threshold.
5 Conclusions
In this paper we aimed to find an effective way to apply the twin-candidate model
into coreference resolution task. We proposed a modified learning framework in
which non-anaphors were utilized to create a special class of training instances.
With such instances, the resulting classifier could avoid the invalid resolution of
non-anaphors, which enables the twin-candidate model to be directly deployed to
coreference resolution, without using an additional anaphoricity determination
module or using a pre-defined threshold.
In the paper we evaluated the effectiveness of our modified framework on
the MUC data set. The results show that the system with the new framework
outperforms the single-candidate based system, as well as the twin-candidate
730 X. Yang, J. Su, and C.L. Tan
based systems using other solutions. Especially, the analysis of the results indi-
cates that our modified framework could lead to more reliable performance than
the solution of using threshold. All these suggest that the twin-candidate model
with the new framework is effective for coreference resolution.
References
1. McCarthy, J., Lehnert, Q.: Using decision trees for coreference resolution. In:
Proceedings of the 14th International Conference on Artificial Intelligences. (1995)
1050?1055
2. Connolly, D., Burger, J., Day, D. New Methods in Language Processing. In: A
machine learning approach to anaphoric reference. (1997) 133?144
3. Soon, W., Ng, H., Lim, D.: A machine learning approach to coreference resolution
of noun phrases. Computational Linguistics 27 (2001) 521?544
4. Ng, V., Cardie, C.: Improving machine learning approaches to coreference resolu-
tion. In: Proceedings of the 40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia (2002) 104?111
5. Yang, X., Zhou, G., Su, J., Tan, C.: Coreference resolution using competition
learning approach. In: Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics, Japan (2003)
6. Iida, R., Inui, K., Takamura, H., Matsumoto, Y.: Incorporating contextual cues in
trainable models for coreference resolution. In: Proceedings of the 10th Conference
of EACL, Workshop ?The Computational Treatment of Anaphora?. (2003)
7. Ng, V., Cardie, C.: Identifying anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In: Proceedings of the 19th International Conference
on Computational Linguistics (COLING02). (2002)
8. MUC-6: Proceedings of the Sixth Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1995)
9. MUC-7: Proceedings of the Seventh Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1998)
10. Quinlan, J.R.: C4.5: Programs for machine learning. Morgan Kaufmann Publishers,
San Francisco, CA (1993)
11. Vilain, M., Burger, J., Aberdeen, J., Connolly, D., Hirschman, L.: A model-
theoretic coreference scoring scheme. In: Proceedings of the Sixth Message under-
standing Conference (MUC-6), San Francisco, CA, Morgan Kaufmann Publishers
(1995) 45?52
Unsupervised Feature Selection for Relation Extraction
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised re-
lation extraction algorithm, which in-
duces relations between entity pairs by
grouping them into a ?natural? num-
ber of clusters based on the similarity
of their contexts. Stability-based crite-
rion is used to automatically estimate
the number of clusters. For removing
noisy feature words in clustering proce-
dure, feature selection is conducted by
optimizing a trace based criterion sub-
ject to some constraint in an unsuper-
vised manner. After relation clustering
procedure, we employ a discriminative
category matching (DCM) to find typi-
cal and discriminative words to repre-
sent different relations. Experimental
results show the effectiveness of our al-
gorithm.
1 Introduction
Relation extraction is the task of finding rela-
tionships between two entities from text contents.
There has been considerable work on supervised
learning of relation patterns, using corpora which
have been annotated to indicate the information to
be extracted (e.g. (Califf and Mooney, 1999; Ze-
lenko et al, 2002)). A range of extraction mod-
els have been used, including both symbolic rules
and statistical rules such as HMMs or Kernels.
These methods have been particularly success-
ful in some specific domains. However, manu-
ally tagging of large amounts of training data is
very time-consuming; furthermore, it is difficult
for one extraction system to be ported across dif-
ferent domains.
Due to the limitation of supervised methods,
some weakly supervised (or semi-supervised) ap-
proaches have been suggested (Brin, 1998; Eu-
gene and Luis, 2000; Sudo et al, 2003). One
common characteristic of these algorithms is that
they need to pre-define some initial seeds for any
particular relation, then bootstrap from the seeds
to acquire the relation. However, it is not easy
to select representative seeds for obtaining good
results.
Hasegawa, et al put forward an unsuper-
vised approach for relation extraction from large
text corpora (Hasegawa et al, 2004). First, they
adopted a hierarchical clustering method to clus-
ter the contexts of entity pairs. Second, after con-
text clustering, they selected the most frequent
words in the contexts to represent the relation
that holds between the entities. However, the ap-
proach exists its limitation. Firstly, the similar-
ity threshold for the clusters, like the appropriate
number of clusters, is somewhat difficult to pre-
defined. Secondly, the representative words se-
lected by frequency tends to obscure the clusters.
For solving the above problems, we present a
novel unsupervised method based on model or-
der selection and discriminative label identifica-
tion. For achieving model order identification,
stability-based criterion is used to automatically
estimate the number of clusters. For removing
noisy feature words in clustering procedure, fea-
ture selection is conducted by optimizing a trace
based criterion subject to some constraint in an
262
unsupervised manner. Furthermore, after relation
clustering, we employ a discriminative category
matching (DCM) to find typical and discrimina-
tive words to represent different relations types.
2 Proposed Method
Feature selection for relation extraction is the task
of finding important contextual words which will
help to discriminate relation types. Unlike su-
pervised learning, where class labels can guide
feature search, in unsupervised learning, it is ex-
pected to define a criterion to assess the impor-
tance of the feature subsets. Due to the interplay
between feature selection and clustering solution,
we should define an objective function to evaluate
both feature subset and model order.
In this paper, the model selection capability is
achieved by resampling based stability analysis,
which has been successfully applied to several un-
supervised learning problems (e.g. (Levine and
Domany, 2001), (Lange et al, 2002), (Roth and
Lange et al, 2003), (Niu et al, 2004)). We extend
the cluster validation strategy further to address
both feature selection and model order identifica-
tion.
Table 1 presents our model selection algorithm.
The objective function MFk,k is relevant with
both feature subset and model order. Clustering
solution that is stable against resampling will give
rise to a local optimum of MFk,k, which indicates
both important feature subset and the true cluster
number.
2.1 Entropy-based Feature Ranking
Let P = {p1, p2, ...pN} be a set of local context
vectors of co-occurrences of entity pair E1 and
E2. Here, the context includes the words occur-
ring between, before and after the entity pair. Let
W = {w1, w2, ..., wM} represent all the words
occurred in P . To select a subset of important
features from W , words are first ranked accord-
ing to their importance on clustering. The im-
portance can be assessed by the entropy criterion.
Entropy-based feature ranking is based on the as-
sumption that a feature is irrelevant if the presence
of it obscures the separability of data set(Dash et
al., 2000).
We assume pn, 1 ? n ? N , lies in feature
space W , and the dimension of feature space is
Table 1: Model Selection Algorithm for Relation Extrac-
tion
Input: Corpus D tagged with Entities(E1, E2);
Output: Feature subset and Model Order (number of
relation types);
1. Collect the contexts of all entity pairs in the document
corpus D, namely P ;
2. Rank features using entropy-based method described
in section 2.1;
3. Set the range (Kl,Kh) for the possible number of
relation clusters;
4. Set estimated model order k = Kl;
5. Conduct feature selection using the algorithm pre-
sented in section 2.2;
6. Record F?k,k and the score of the merit of both of
them, namely MF,k;
7. If k < Kh, k = k + 1, go to step 5; otherwise, go to
Step 7;
8. Select k and feature subset F?k which maximizes the
score of the merit MF,k;
M . Then the similarity between i-th data point
pi and j-th data point pj is given by the equa-
tion: Si,j = exp(?? ? Di,j), where Di,j is the
Euclidean distance between pi and pj , and ? is a
positive constant, its value is ? ln 0.5D , where D is
the average distance among the data points. Then
the entropy of data set P with N data points is
defined as:
E = ?
N?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log(1? Si,j))
(1)
For ranking of features, the importance of each
word I(wk) is defined as entropy of the data af-
ter discarding feature wk. It is calculated in this
way: remove each word in turn from the feature
space and calculate E of the data in the new fea-
ture space using the Equation 1. Based on the
observation that a feature is the least important if
the removal of it results in minimum E, we can
obtain the rankings of the features.
2.2 Feature Subset Selection and Model
Order Identification
In this paper, for each specified cluster number,
firstly we perform K-means clustering analysis on
each feature subset and adopts a scattering cri-
terion ?Invariant Criterion? to select an optimal
feature subset F from the feature subset space.
Here, trace(P?1W PB) is used to compare the clus-
ter quality for different feature subsets 1, which
1trace(P?1W PB) is trace of a matrix which is the sum
of its diagonal elements. PW is the within-cluster scatter
263
Table 2: Unsupervised Algorithm for Evaluation of Fea-
ture Subset and Model Order
Function: criterion(F, k, P, q)
Input: feature subset F , cluster number k, entity pairs
set P , and sampling frequency q;
Output: the score of the merit of F and k;
1. With the cluster number k as input, perform k-means
clustering analysis on pairs set PF ;
2. Construct connectivity matrix CF,k based on above
clustering solution on full pairs set PF ;
3. Use a random predictor ?k to assign uniformly drawn
labels to each entity pair in PF ;
4. Construct connectivity matrix CF,?k based on above
clustering solution on full pairs set PF ;
5. Construct q sub sets of the full pairs set, by randomly
selecting ?N of the N original pairs, 0 ? ? ? 1;
6. For each sub set, perform the clustering analysis in
Step 2, 3, 4, and result C?F,k, C?F,?k ;
7. Compute MF,k to evaluate the merit of k using Equa-
tion 3;
8. Return MF,k;
measures the ratio of between-cluster to within-
cluster scatter. The higher the trace(P?1W PB), the
higher the cluster quality.
To improve searching efficiency, features are
first ranked according to their importance. As-
sume Wr = {f1, ..., fM} is the sorted feature list.
The task of searching can be seen in the feature
subset space: {(f1, ..., fk),1 ? k ? M}.
Then the selected feature subset F is eval-
uated with the cluster number using the ob-
jective function, which can be formulated as:
F?k = argmaxF?Wr{criterion(F, k)}, subject
to coverage(P, F ) ? ? 2. Here, F?k is the opti-
mal feature subset, F and k are the feature subset
and the value of cluster number under evaluation,
and the criterion is set up based on resampling-
based stability, as Table 2 shows.
Let P? be a subset sampled from full entity
pairs set P with size ?|P | (? set as 0.9 in this
paper.), C(C?) be |P | ? |P |(|P?| ? |P?|) con-
nectivity matrix based on the clustering results on
P (P?). Each entry cij(c?ij) of C(C?) is calculated
in the following: if the entity pair pi ? P (P?),
pj ? P (P?) belong to the same cluster, then
cij(c?ij) equals 1, else 0. Then the stability is de-
matrix as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xj ? mj)
t
and PB is the between-cluster scatter matrix as: PB =?c
j=1(mj ?m)(mj ?m)t, where m is the total mean vec-
tor and mj is the mean vector for jth cluster and (Xj?mj)t
is the matrix transpose of the column vector (Xj ?mj).
2let coverage(P, F ) be the coverage rate of the feature
set F with respect to P . In practice, we set ? = 0.9.
fined in Equation 2:
M(C?, C) =
?
i,j 1{C?i,j = Ci,j = 1, pi ? P?, pj ? P?}?
i,j 1{Ci,j = 1, pi ? P?, pj ? P?}
(2)
Intuitively, M(C?, C) denotes the consistency
between the clustering results on C? and C. The
assumption is that if the cluster number k is actu-
ally the ?natural? number of relation types, then
clustering results on subsets P? generated by
sampling should be similar to the clustering re-
sult on full entity pair set P . Obviously, the above
function satisfies 0 ? M ? 1.
It is noticed that M(C?, C) tends to decrease
when increasing the value of k. Therefore for
avoiding the bias that small value of k is to be
selected as cluster number, we use the cluster
validity of a random predictor ?k to normalize
M(C?, C). The random predictor ?k achieved
the stability value by assigning uniformly drawn
labels to objects, that is, splitting the data into k
clusters randomly. Furthermore, for each k, we
tried q times. So, in the step 7 of the algorithm
of Table 2, the objective function M(C?F,k, CF,k)
can be normalized as equations 3:
MnormF,k = 1q
q?
i=1
M(C?iF,k, CF,k)?
1
q
q?
i=1
M(C?iF,?k , CF,?k )
(3)
Normalizing M(C?, C) by the stability of the
random predictor can yield values independent of
k.
After the number of optimal clusters and the
feature subset has been chosen, we adopted the
K-means algorithm for the clustering phase. The
output of context clustering is a set of context
clusters, each of them is supposed to denote one
relation type.
2.3 Discriminative Feature identification
For labelling each relation type, we use DCM
(discriminative category matching) scheme to
identify discriminative label, which is also used
in document classification (Gabriel et al, 2002)
and weights the importance of a feature based on
their distribution. In this scheme, a feature is not
important if the feature appears in many clusters
and is evenly distributed in these clusters, other-
wise it will be assigned higher importance.
To weight a feature fi within a category, we
take into account the following information:
264
Table 3: Three domains of entity pairs: frequency distribution for different relation types
PER-ORG # of pairs:786 ORG-GPE # of pairs:262 ORG-ORG # of pairs:580
Relation types Percentage Relation types Percentage Relation types Percentage
Management 36.39% Based-In 46.56% Member 27.76%
General-staff 29.90% Located 35.11% Subsidiary 19.83%
Member 19.34% Member 11.07% Part-Of 18.79%
Owner 4.45% Affiliate-Partner 3.44% Affiliate-Partner 17.93%
Located 3.28% Part-Of 2.29% Owner 8.79%
Client 1.91% Owner 1.53% Client 2.59%
Other 1.91% Management 2.59%
Affiliate-Partner 1.53% Other 1.21%
Founder 0.76% Other 0.52%
? The relative importance of fi within a cluster is de-
fined as: WCi,k = log2(pfi,k+1)log2(Nk+1) , where pfi,k is the
number of those entity pairs which contain feature fi
in cluster k. Nk is the total number of term pairs in
cluster k.
? The relative importance of fi across clusters is given
by: CCi = log N?maxk?Ci{WCi,k}?N
k=1 WCi,k
? 1logN , where Ci
is the set of clusters which contain feature fi. N is the
total number of clusters.
Here, WCi,k and CCi are designed to capture
both local information within a cluster and global
information about the feature distribution across
clusters respectively. Combining both WCi,k and
CCi we define the weight Wi,k of fi in cluster k
as: Wi,k = WC
2
i,k?CC2i?
WC2i,k+CC2i
? ?2, 0 ? Wi,k ? 1.
3 Experiments and Results
3.1 Data
We constructed three subsets for domains PER-
ORG, ORG-GPE and ORG-ORG respectively
from ACE corpus3 The details of these subsets
are given in Table 3, which are broken down by
different relation types. To verify our proposed
method, we only extracted those pairs of entity
mentions which have been tagged relation types.
And the relation type tags were used as ground
truth classes to evaluate.
3.2 Evaluation method for clustering result
Since there was no relation type tags for each
cluster in our clustering results, we adopted a
permutation procedure to assign different rela-
tion type tags to only min(|EC|,|TC|) clusters,
where |EC| is the estimated number of clusters,
and |TC| is the number of ground truth classes
3http://www.ldc.upenn.edu/Projects/ACE/
(relation types). This procedure aims to find an
one-to-one mapping function ? from the TC to
EC. To perform the mapping, we construct a
contingency table T , where each entry ti,j gives
the number of the instances that belong to both
the i-th cluster and j-th ground truth class. Then
the mapping procedure can be formulated as:?? =
argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the index
of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we
can define the evaluation measure as follows:
Accuracy(P ) =
?
j t??(j),j?
i,j ti,j
. Intuitively, it reflects
the accuracy of the clustering result.
3.3 Evaluation method for relation labelling
For evaluation of the relation labeling, we need
to explore the relatedness between the identified
labels and the pre-defined relation names. To do
this, we use one information-content based mea-
sure (Lin, 1997), which is provided in Wordnet-
Similarity package (Pedersen et al, 2004) to eval-
uate the similarity between two concepts in Word-
net. Intuitively, the relatedness between two con-
cepts in Wordnet is captured by the information
content of their lowest common subsumer (lcs)
and the information content of the two concepts
themselves , which can be formalized as follows:
Relatednesslin(c1, c2) = 2?IC(lcs(c1,c2))IC(c1)+IC(c2) . This
measure depends upon the corpus to estimate in-
formation content. We carried out the experi-
ments using the British National Corpus (BNC)
as the source of information content.
3.4 Experiments and Results
For comparison of the effect of the outer and
within contexts of entity pairs, we used five dif-
265
Table 4: Automatically determined the number of relation types using different feature ranking methods.
Domain Context
Window
Size
# of real
relation
types
Model Or-
der Base-
line
Model
Order with
?2
Model
Order with
Freq
Model Or-
der with
Entropy
PER-ORG 0-5-0 9 7 7 7 7
2-5-2 9 8 6 7 8
0-10-0 9 8 6 8 8
2-10-2 9 6 7 6 8
5-10-5 9 5 5 6 7
ORG-GPE 0-5-0 6 3 3 3 4
2-5-2 6 2 3 4 4
0-10-0 6 6 4 5 6
2-10-2 6 4 3 4 5
5-10-5 6 2 3 3 3
ORG-ORG 0-5-0 9 7 7 7 7
2-5-2 9 7 5 6 7
0-10-0 9 9 8 9 9
2-10-2 9 6 6 6 7
5-10-5 9 8 5 7 9
ferent settings of context window size (WINpre-
WINmid-WINpost) for each domain.
Table 4 shows the results of model order iden-
tification without feature selection (Baseline) and
with feature selection based on different feature
ranking criterion( ?2 , Frequency and Entropy).
The results show that the model order identifica-
tion algorithm with feature selection based on en-
tropy achieve best results: estimate cluster num-
bers which are very close to the true values. In ad-
dition, we can find that with the context setting, 0-
10-0, the estimated number of the clusters is equal
or close to the ground truth value. It demonstrates
that the intervening words less than 10 are appro-
priate features to reflect the structure behind the
contexts, while the intervening words less than 5
are not enough to infer the structure. For the con-
textual words beyond (before or after) the enti-
ties, they tend to be noisy features for the relation
estimation, as can be seen that the performance
deteriorates when taking them into consideration,
especially for the case without feature selection.
Table 5 gives a comparison of the aver-
age accuracy over five different context win-
dow size settings for different clustering settings.
For each domain, we conducted five cluster-
ing procedures: Hasegawa?s method, RLBaseline,
RLFS?2 , RLFSFreq and RLFSEntropy. For
Hasegawa?s method (Hasegawa et al, 2004), we
set the cluster number to be identical with the
number of ground truth classes. For RLBaseline,
we use the estimated cluster number to clus-
ter contexts without feature selection. For
RLFS?2 ,RLFSFreq and RLFSEntropy, we use
the selected feature subset and the estimated clus-
ter number to cluster the contexts, where the fea-
ture subset comes from ?2, frequency and entropy
criterion respectively. Comparing the average ac-
curacy of these clustering methods, we can find
that the performance of feature selection methods
is better than or comparable with the baseline sys-
tem without feature selection. Furthermore, it is
noted that RLFSEntropy achieves the highest av-
erage accuracy in three domains, which indicates
that entropy based feature pre-ranking provides
useful heuristic information for the selection of
important feature subset.
Table 6 gives the automatically estimated labels
for relation types for the domain PER-ORG. We
select two features as labels of each relation type
according to their DCM scores and calculate the
average (and maximum) relatedness between our
selected labels (E) and the predefined labels (H).
Following the same strategy, we also extracted re-
lation labels (T) from the ground truth classes and
provided the relatedness between T and H. From
the column of relatedness (E-H), we can see that it
is not easy to find the hand-tagged relation labels
exactly, furthermore, the identified labels from the
ground-truth classes are either not always compa-
rable to the pre-defined labels in most cases (T-
H). The reason may be that the pre-defined rela-
tion names tend to be some abstract labels over
the features, e.g., ?management? vs. ?president?,
266
Table 5: Performance of the clustering algorithms over three domains: the average accuracy over 5 different context window
size.
Domain Hasegawa?s
method
RLBaseline RLFS?2 RLFSFreq RLFSEntropy
PER-ORG 32.4% 34.3% 33.9% 36.6% 41.3%
ORG-GPE 43.7% 47.4% 47.1% 48.4% 50.6%
ORG-ORG 26.5% 36.2% 36.0% 38.7% 42.4%
Table 6: Relation Labelling using DCM strategy for the domain PER-ORG. Here, (T) denotes the identified relation labels
from ground truth classes. (E) is the identified relation labels from our estimated clusters. ?Ave (T-H)? denotes the average
relatedness between (T) and (H). ?Max (T-H)? denotes the maximum relatedness between (T) and (H).
Hand-tagged La-
bel (H)
Identified Label
(T)
Identified Label
(E)
Ave
(T-H)
Max
(T-H)
Ave
(E-H)
Max
(E-H)
Ave
(E-T)
Max
(E-T)
management head,president president,control 0.3703 0.4515 0.3148 0.3406 0.7443 1.0000
general-staff work,fire work,charge 0.6254 0.7823 0.6411 0.7823 0.6900 1.0000
member join,communist become,join 0.394 0.4519 0.1681 0.3360 0.3366 1.0000
owner bond,bought belong,house 0.1351 0.2702 0.0804 0.1608 0.2489 0.4978
located appear,include lobby,appear 0.0000 0.0000 0.1606 0.3213 0.2500 1.0000
client hire,reader bought,consult 0.4378 0.8755 0.0000 0.0000 0.1417 0.5666
affiliate-partner affiliate,associate assist,affiliate 0.9118 1.0000 0.5000 1.0000 0.5000 1.0000
founder form,found invest,set 0.1516 0.3048 0.3437 0.6875 0.4376 0.6932
?head? or ?control?; ?member? vs. ?join?, ?be-
come?, etc., while the abstract words and the fea-
tures are located far away in Wordnet. Table 6
also lists the relatedness between (E) and (T). We
can see that the labels are comparable by their
maximum relatedness(E-T).
4 Conclusion and Future work
In this paper, we presented an unsupervised ap-
proach for relation extraction from corpus. The
advantages of the proposed approach includes
that it doesn?t need any manual labelling of the re-
lation instances, it can identify an important fea-
ture subset and the number of the context clusters
automatically, and it can avoid extracting those
common words as characterization of relations.
References
Mary Elaine Califf and Raymond J.Mooney. 1999. Rela-
tional Learning of Pattern-Match Rules for Information
Extraction, AAAI99.
Sergey Brin. 1998. Extracting patterns and relations from
world wide web. In Proc. of WebDB?98. pages 172-183.
Kiyoshi Sudo, Satoshi Sekine and Ralph Grishman. 2003.
An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition. Proceedings of ACL
2003; Sapporo, Japan.
Eugene Agichtein and Luis Gravano. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections, In
Proc. of the 5th ACM International Conference on Digi-
tal Libraries (ACMDL?00).
Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman.
2004. Discovering Relations among Named Entities from
Large Corpora, ACL2004. Barcelona, Spain.
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella.
2002. Kernel Methods for Relation Extraction,
EMNLP2002. Philadelphia.
Lange,T., Braun,M.,Roth, V., and Buhmann,J.M.. 2002.
Stability-Based Model Selection, Advances in Neural In-
formation Processing Systems 15.
Levine,E. and Domany,E.. 2001. Resampling Method
for Unsupervised Estimation of Cluster Calidity, Neural
Computation, Vol.13, 2573-2593.
Zhengyu Niu, Donghong Ji and Chew Lim Tan. 2004. Doc-
ument Clustering Based on Cluster Validation, CIKM?04.
November 8-13, 2004, Washington, DC, USA.
Volker Roth and Tilman Lange. 2003. Feature Selection in
Clustering Problems, NIPS2003 workshop.
Manoranjan Dash and Huan Liu. 2000. Feature Selection
for Clustering, Proceedings of Pacific-Asia Conference
on Knowledge Discovery and Data Mining.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun
Lu. 2002. Discriminative Category Matching: Effi-
cient Text Classification for Huge Document Collections,
ICDM2002. December 09-12, 2002, Japan.
D.Lin. 1997. Using syntactic dependency as a local context
to resolve word sense ambiguity. In Proceedings of the
35th Annual Meeting of ACL,. Madrid, July 1997.
Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi.
2004. WordNet::Similarity-Measuring the Relatedness of
Concepts, AAAI2004.
267
  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
A Twin-Candidate Model for Learning-Based
Anaphora Resolution
Xiaofeng Yang?
Institute for Infocomm Research
Jian Su??
Institute for Infocomm Research
Chew Lim Tan?
School of Computing,
National University of Singapore
The traditional single-candidate learning model for anaphora resolution considers the antecedent
candidates of an anaphor in isolation, and thus cannot effectively capture the preference relation-
ships between competing candidates for its learning and resolution. To deal with this problem,
we propose a twin-candidate model for anaphora resolution. The main idea behind the model
is to recast anaphora resolution as a preference classification problem. Specifically, the model
learns a classifier that determines the preference between competing candidates, and, during
resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates. We
present in detail the framework of the twin-candidate model for anaphora resolution. Further,
we explore how to deploy the model in the more complicated coreference resolution task. We
evaluate the twin-candidate model in different domains using the Automatic Content Extraction
data sets. The experimental results indicate that our twin-candidate model is superior to the
single-candidate model for the task of pronominal anaphora resolution. For the task of coreference
resolution, it also performs equally well, or better.
1. Introduction
Anaphora is reference to an entity that has been previously introduced into the dis-
course (Jurafsky and Martin 2000). The referring expression used is called the anaphor
and the expression being referred to is its antecedent. The anaphor is usually used
to refer to the same entity as the antecedent; hence, they are coreferential with each
other. The process of determining the antecedent of an anaphor is called anaphora
resolution. As a key problem in discourse and language understanding, anaphora
resolution is crucial inmany natural language applications, such asmachine translation,
text summarization, question answering, information extraction, and so on. In recent
? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: xiaofengy@i2r.a-star.edu.sg.
?? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: sujian@i2r.a-star.edu.sg.
? 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg.
Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication:
5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
years, supervised learning approaches have been widely applied to anaphora resolu-
tion, and they have achieved considerable success (Aone and Bennett 1995; McCarthy
and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak
1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube andMueller 2003; Luo et al
2004; Ng et al 2005).
The strength of learning-based anaphora resolution is that resolution regularities
can be automatically learned from annotated data. Traditionally, learning-based ap-
proaches to anaphora resolution adopt the single-candidate model, in which the po-
tential antecedents (i.e., antecedent candidates) are considered in isolation for both
learning and resolution. In such a model, the purpose of classification is to determine if
a candidate is the antecedent of a given anaphor. A training or testing instance is formed
by an anaphor and each of its candidates, with features describing the properties of the
anaphor and the individual candidate. During resolution, the antecedent of an anaphor
is selected based on the classification results for each candidate.
One assumption behind the single-candidate model is that whether a candidate
is the antecedent of an anaphor is completely independent of the other competing
candidates. However, anaphora resolution can be more accurately represented as a
ranking problem in which candidates are ordered based on their preference and the best
one is the antecedent of the anaphor (Jurafsky and Martin 2000). The single-candidate
model, which only considers the candidates of an anaphor in isolation, is incapable
of effectively capturing the preference relationship between candidates for its training.
Consequently, the learned classifier cannot produce reliable results for preference deter-
mination during resolution.
To deal with this problem, we propose a twin-candidate learning model for
anaphora resolution. Themain idea behind themodel is to recast anaphora resolution as
a preference classification problem. The purpose of the classification is to determine the
preference between two competing candidates for the antecedent of a given anaphor. In
the model, an instance is formed by an anaphor and two of its antecedent candidates,
with features used to describe their properties and relationships. The antecedent is
selected based on the judged preference among the candidates.
In the article we focus on two issues about the twin-candidate model. In the first
part, we will introduce the framework of the twin-candidate model for anaphora reso-
lution, including detailed training procedures and resolution schemes. In the second
part, we will further explore how to deploy the twin-candidate model in the more
complicated task of coreference resolution. We will present an empirical evaluation of
the twin-candidate model in different domains, using the Automatic Content Extraction
(ACE) data sets. The experimental results indicate that the twin-candidate model is
superior to the single-candidate model for the task of pronominal anaphora resolution.
For the coreference resolution task, it also performs equally well, or better.
2. Related Work
To our knowledge, the first work on the twin-candidate model for anaphora resolution
was proposed by Connolly, Burger, and Day (1997). Their work relied on a set of features
that included lexical type, grammatical role, recency, and number/gender/semantic
agreement, and employed a simple linear search scheme to choose the most preferred
candidate. Their system produced a relatively low accuracy rate for pronoun reso-
lution (55.3%) and definite NP resolution (37.4%) on a set of selected news articles.
Iida et al (2003) used the twin-candidate model (called the tournament model in their
work) to perform Japanese zero-anaphora resolution. They utilized the same linear
328
Yang, Su, and Tan A Twin-Candidate Model for AR
scheme to search for antecedents. Compared with Connolly, Burger, and Day (1997),
they adopted richer features in which centering information was incorporated to cap-
ture contextual knowledge. Their system achieved an accuracy of around 70% on a
data set drawn from a corpus of newspaper articles. Both of these studies were carried
out on uncommon data sets, which makes it difficult to compare their results with
other baseline systems. In contrast to the previous work, we will explore the twin-
candidate model comprehensively by describing the model in more detail, trying more
effective resolution schemes, deploying the model in the more complicated coreference
resolution task, performing more extensive experiments, and evaluating the model in
more depth.
Denis and Baldridge (2007) proposed a pronoun resolution system that directly
used a ranking learning algorithm (based on Maximal Entropy) to train a preference
classifier for antecedent selection. They reported an accuracy of around 72?76% for
the different domains in the ACE data set. In our study, we will also investigate the
solution of using a general ranking learner (e.g., Ranking-SVM). By comparison, the
twin-candidate model is applicable to any discriminative learning algorithm, no matter
whether it is capable of ranking learning or not. Moreover, as the model is trained and
tested on pairwise candidates, it can effectively capture various relationships between
candidates for better preference learning and determination.
Ng (2005) presented a ranking model for coreference resolution. The model focused
on the preference between the potential partitions of NPs, instead of the potential
antecedents of an NP as in our work. Given an input document, the model first em-
ployed n pre-selected coreference resolution systems to generate n candidate partitions
of NPs. The model learned a preference classifier (trained using Ranking-SVM) that
could distinguish good and bad partitions during testing. The best rank partition would
be selected as the resolution output of the current text. The author evaluated the model
on the ACE data set and reported an F-measure of 55?69% for the different domains.
Although ranking-based, Ng?s model is quite different from ours as it operates at the
cluster-level whereas ours operates at the mention-level. In fact, the result of our twin-
candidate system can be used as an input to his model.
3. The Twin-Candidate Model for Anaphora Resolution
3.1 The Single-Candidate Model
Learning-based anaphora resolution uses a machine learning method to obtain p(ante
(Ck)|ana,C1,C2, . . . ,Cn), the probability that a candidate Ck is the antecedent of the
anaphor ana in the context of its antecedent candidates, C1,C2, . . . ,Cn. The single-
candidate model assumes that the probability that Ck is the antecedent is only de-
pendent on the anaphor ana and Ck, and independent of all the other candidates.
That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn) = p (ante(Ck) | ana,Ck) (1)
Thus, the probability of a candidate Ck being the antecedent can be approximated using
the classification result on the instance describing the anaphor and Ck alone.
The single-candidate model is widely used in most anaphora resolution sys-
tems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and
Mueller 2003; Kehler et al 2004; Ng et al 2005). In our study, we also build as the
329
Computational Linguistics Volume 34, Number 3
Table 1
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government] can
stick with [6 them], [7 it] will be able to halve this year?s 120 billion
ruble (US $193 billion) deficit.
Table 2
Training instances generated under the single-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them] , [1 Those figures]} 1
[6 them] i{[6 them] , [2 the government]} 0
i{[6 them] , [3 legislators]} 0
i{[6 them] , [4 September]} 0
i{[6 them] , [5 the government]} 0
i{[7 it] , [1 Those figures]} 0
i{[7 it] , [3 legislators]} 0
[7 it] i{[7 it] , [4 September]} 0
i{[7 it] , [5 the government]} 1
i{[7 it] , [6 them]} 0
baseline a system for pronominal anaphora resolution based on the single-candidate
model.
In the single-candidate model, an instance has the form of i{ana, candi}, where ana
is an anaphor and candi is an antecedent candidate.1 For training, instances are created
for each anaphor occurring in an annotated text. Specifically, given an anaphor ana and
its antecedent candidates, a set of negative instances (labeled ?0?) is formed by pairing
ana and each of the candidates that is not coreferential with ana. In addition, a single
positive instance (labeled ?1?) is formed by pairing ana and the closest antecedent, that
is, the closest candidate that is coreferential with ana.2 Note that it is possible that an
anaphor has two or more antecedents, but we only create one positive instance for the
closest antecedent as its reference relationship with the anaphor is usually the most
direct and thus the most confident.
As an example, consider the text in Table 1.
Here, [6 them] and [7 it] are two anaphors. [1 Those figures] and [5 the government] are
their closest antecedents, respectively. Supposing that the antecedent candidates of the
two anaphors are just all their preceding NPs in the current text, the training instances
to be created for the text segment are listed in Table 2.
1 In our study, we only consider anaphors whose antecedents are noun phrases. Typically, all the NPs
preceding an anaphor can be taken as the initial antecedent candidates. For better learning and
resolution, however, candidates can be filtered so that only those ?confident? NPs, which occur in the
specified search scope and meet constraints such as number/gender agreement, are considered. The
details of candidate selection in our system will be discussed later in the section on experiments.
2 We assume that at least one antecedent exists in the candidate set of an anaphor. However, for real
resolution, if none of the antecedents of an anaphor occur in the candidate set, we simply discard the
anaphor and do not create any training instance for it.
330
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 3
Feature set for pronominal anaphora resolution.
ana Reflexive whether the anaphor is a reflexive pronoun
ana PronType type of the anaphor if it is a pronoun (he, she, it or they?)
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite NP
candi Name whether the candidate is a named entity
candi Pron whether the candidate is a pronoun
candi FirstNP whether the candidate is the first mentioned NP in the sentence
candi Subject whether the candidate is the subject of a sentence, the subject of a clause, or not.
candi Oject whether the candidate is the object of a verb, the object of a preposition,
or not
candi ParallelStruct whether the candidate has an identical collocation pattern with the anaphor
candi SentDist the sentence distance between the candidate and the anaphor
candi NearestNP whether the candidate is the candidate closest to the anaphor in position
Note that for [7 it], we do not use [2 the government] to create a positive training
instance as it is not the closest candidate that is coreferential with the anaphor.
A vector of features is specified for each training instance. The featuresmay describe
the characteristics of the anaphor and the candidate, as well as their relationships from
lexical, syntactic, semantic, and positional aspects. Table 3 lists the features used in our
study. All these features can be computed with high reliability, and have been proven
effective for pronoun resolution in previous work.
Based on the generated feature vectors, a classifier is trained using a certain learning
algorithm. During resolution, given a newly encountered anaphor, a test instance is
formed for each of the antecedent candidates. The instance is passed to the classifier,
which then returns a confidence value indicating the likelihood that the candidate is the
antecedent of the anaphor. The candidate with the highest confidence is selected as the
antecedent. For example, suppose [7 it] is an anaphor to be resolved. Six test instances
will be created for its six antecedent candidates, as listed in Table 4. The learned classifier
is supposed to give the highest confidence to i{[7 it] , [5 the government]}, indicating the
candidate [5 the government] is the antecedent of [7 it].
3.2 A Problem with the Single-Candidate Model
As described, the assumption behind the single-candidate model is that the probability
of a candidate being the antecedent of a given anaphor is completely independent of
Table 4
Test instances generated under the single-candidate model for anaphora resolution.
Anaphor Test Instance
i{[7 it] , [1 Those figures]}
i{[7 it] , [2 the government]}
i{[7 it] , [3 legislators]}
[7 it] i{[7 it] , [4 September]}
i{[7 it] , [5 the government]}
i{[7 it] , [6 them]}
331
Computational Linguistics Volume 34, Number 3
the other competing candidates. However, for an anaphor, the determination of the
antecedent is often subject to preference among the candidates (Jurafsky and Martin
2000). Whether a candidate is the antecedent depends on whether it is the ?best? among
the candidate set, that is, whether there exists no other candidate that is preferred over
it. Hence, simply considering one candidate individually is an indirect and unreliable
way to select the correct antecedent.
The idea of preference is common in linguistic theories on anaphora. Garnham
(2001) summarizes different factors that influence the interpretation of anaphoric
expressions. Some factors such as morphology (gender, number, animacy, and case)
or syntax (e.g., the role of binding and commanding relations [Chomsky 1981]) are
?eliminating,? forbidding certain NPs from being antecedents. However, many others
are ?preferential,? giving more preference to certain candidates over others; examples
include:
 Sentence-based factors: Pronouns in one clause prefer to refer to the
NP that is the subject of the previous clause (Crawley, Stevenson, and
Kleinman 1990). Also, the NP that is the first-mentioned expression is
preferred regardless of the syntactic and semantic role played by the
referring expression (Gernsbacher and Hargreaves 1988).
 Stylistic factors: Pronouns preferentially take parallel antecedents that play
the same role as the anaphor in their respective clauses (Grober, Beardsley,
and Caramazza 1978; Stevenson, Nelson, and Stenning 1995).
 Discourse-based factors: Items currently in focus are the prime candidates
for providing antecedents for anaphoric expressions. According to
centering theory (Grosz, Joshi, and Weinstein 1995), each utterance has a
set of forward-looking centers that have higher preference to be referred to
in later utterances. The forward-looking centers can be ranked based
on grammatical roles or other factors.
 Distance-based factors: Pronouns prefer candidates in the previous
sentence compared with those two or more sentences back (Clark
and Sengul 1979).
As a matter of fact, ?eliminating? factors could also be considered ?preferential? if
we think of the act of eliminating candidates as giving them low preference.
Preference-based strategies are also widely seen in earlier manual approaches to
pronominal anaphora resolution. For example, the SHRDLU system byWinograd (1972)
prefers antecedent candidates in the subject position over those in the object position.
The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with
the anaphor. Hobbs?s algorithm (Hobbs 1978) prefers candidates that are closer to the
anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers
candidates that have a high salience value computed by aggregating the weights of
different factors.
During resolution, the single-candidate model does select an antecedent based on
preference by using classification confidence for candidates; that is, the higher con-
fidence value the classifier returns, the more likely the candidate is preferred as the
antecedent. Nevertheless, as the model considers only one candidate at a time during
training, it cannot effectively capture the preference between candidates for classifier
learning. For example, consider an anaphor and a candidate Ci. If there are no ?better?
332
Yang, Su, and Tan A Twin-Candidate Model for AR
candidates in the candidate set, Ci is the antecedent and forms a positive instance.
Otherwise, Ci is not selected as the antecedent and thus forms a negative instance.
Simply looking at a candidate alone cannot explain this, and may possibly result in
inconsistent training instances (i.e., the same feature vector but different class labels).
Consequently, the confidence values returned by the learned classifier cannot reliably
reflect the preference relationship between candidates.
3.3 The Twin-Candidate Model
To address the problem with the single-candidate model, we propose a twin-candidate
model to handle anaphora resolution. As opposed to the single-candidate model, the
model explicitly learns a preference classifier to determine the preference relationship
between candidates. Formally, the model considers the probability that a candidate
is the antecedent as the probability that the candidate is preferred over all the other
competing candidates. That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn)
= p (Ck  {C1, . . . ,Ck?1,Ck+1, . . .Cn} | ana,C1,C2, . . . ,Cn) (2)
= p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
Assuming that the preference between Ck and Ci is independent of the preference
between Ck and the candidates other than Ci, we have:
p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
p(Ck  Ci | ana,Ck,Ci) (3)
Thus:
ln p (ante(Ck) | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
ln p(Ck  Ci | ana,Ck,Ci) (4)
This suggests that the probability that a candidate Ck is the antecedent can be esti-
mated using the classification results on the set of instances describing Ck and each of
the other competing candidates. To do this, we learn a classifier that, given any two can-
didates of a given anaphor, can determine which one is preferred to be the antecedent
of the anaphor. The final antecedent is identified based on the classified preference
relationships among the candidates. This is the main idea of the twin-candidate model.
In such a model, each instance consists of three elements: i{ana, Ci, Cj}, where ana
is an anaphor, and Ci and Cj are two of its antecedent candidates. The class label of
an instance represents the preference between the two candidates for the antecedent,
for example, ?01? indicating Cj is preferred over Ci and ?10? indicating Ci is preferred.
Being trained with instances built based on this principle, the classifier is capable of de-
termining the preference between any two candidates of a given anaphor by returning
333
Computational Linguistics Volume 34, Number 3
Table 5
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government]
can stick with [6 them], [7 it] will be able to halve this year?s
120 billion ruble (US $193 billion) deficit.
a class label, either ?01? or ?10?, accordingly. In the next section, we will introduce in
detail a system based on the twin-candidate model for anaphora resolution.
3.4 Framework of the Twin-Candidate Model
3.4.1 Instance Representation. In the twin-candidate model, an instance takes the form
i{ana,Ci,Cj}, where ana is an anaphor andCi andCj are two of its antecedent candidates.
We stipulate that Cj should be closer to ana than Ci in position (i.e., i < j). An instance is
labeled ?10? if Ci is preferred over Cj as the antecedent, or ?01? if otherwise.
A feature vector is associated with an instance, and it describes different properties
and relationships between ana and each of the candidates, Ci or Cj. In our study, the
system with the twin-candidate model adopts the same feature set as the baseline
system with the single-candidate model (shown in Table 3). The difference is that a
feature for the single candidate, candi X, has to be replaced by a pair of features for
the twin candidates, candi1 X and candi2 X. For example, feature candi Pron, which
describes whether a candidate is a pronoun, will be replaced by two features candi1 Pron
and candi2 Pron, which describe whether Ci and Cj are pronouns, respectively.
3.4.2 Training Instances Creation. To learn a preference classifier, a training instance for an
anaphor should be composed of two candidates with an explicit preference relationship,
for example, one being an antecedent and the other being a non-antecedent. A pair
of candidates that are both antecedents or both non-antecedents are not suitable for
instance creation because their preference cannot be explicitly represented for training,
although it does exist.
Based on this idea, during training, for an encountered anaphor ana, we take the
closest antecedent, Cante, as the anchor candidate.
3 Cante is paired with each of the
candidates Cnc that is not coreferential with ana. If Cante is closer to ana than Cnc, an
instance i{ana, Cnc, Cante} is created and labeled ?01?. Otherwise, if Cnc is closer, an
instance i{ana, Cante, Cnc} is created and labeled ?10? instead.
Consider again the sample text given in Table 1, which is repeated in Table 5. For the
anaphor [7 it], the closest antecedent, [5 the government] (denoted as NP5), is chosen as
the anchor candidate. It is paired with the four non-coreferential candidates (i.e., NP1,
NP3,NP4, andNP6) to create four training instances. Among them, the instances formed
withNP1,NP3 orNP4 are labeled ?01? and the one withNP6 is labeled ?10?. Table 6 lists
all the training instances to be generated for the text.
3.4.3 Classifier Generation. Based on the feature vectors for the generated training in-
stances, a classifier can be trained using a discriminative learning algorithm. Given a
test instance i{ana, Ci, Cj} (i < j), the classifier is supposed to return a class label of ?10?,
3 If no antecedent is found in the candidate set, we do not generate any training instance for the anaphor.
334
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 6
Training instances generated under the twin-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [3 legislators], [5 the government]} 01
[7 it] i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [5 the government], [6 them]} 10
indicating that Ci is preferred over Cj for the antecedent of ana, or ?01?, indicating that
Cj is preferred.
3.4.4 Antecedent Identification. After training, the preference classifier can be used to
resolve anaphors. The process of determining the antecedent of a given anaphor, called
antecedent identification, could be thought of as a tournament, a competition in which
many participants play against each other in individual matches. The candidates are like
players in a tournament. A series of matches between candidates is held to determine
the champion of the tournament, that is, the final antecedent of the anaphor under con-
sideration. Here, the preference classifier is like the referee who judges which candidate
wins or loses in a match.
If an anaphor has only one antecedent candidate, it is resolved to the candidate
directly. For anaphors that have more than one candidate, two possible schemes can be
employed to find the antecedent.
Tournament Elimination Tournament Elimination is a type of tournament where
the loser in a match is immediately eliminated. Such a scheme is also applicable to
antecedent identification. In the scheme, candidates are compared linearly from the
beginning to the end. Specifically, the first candidate is compared with the second one,
forming a test instance, which is then passed to the classifier to determine the prefer-
ence. The ?losing? candidate that is judged less preferred by the classifier is eliminated
and never considered. The winner, that is, the preferred candidate, is compared with
the third candidate. The process continues until all the candidates are compared, and
the candidate that wins in the last comparison is selected as the antecedent.
For demonstration, we use the text in Table 5 as a test example. Suppose we have a
?perfect? classifier that can correctly determine the preference between candidates. That
is, the candidates that are coreferential with the anaphor will be classified as preferred
over those that are not. (If the two candidates are both coreferential or both non-
coreferential with the anaphor, the one closer to the anaphor in position is preferred.)
To resolve the anaphor [7 it], the candidate NP1 is first compared with NP2. The formed
instance is classified as ?01?, indicating NP2 is preferred. Thus, NP1 is eliminated and
NP2 continues to compete with NP3 and NP4 until it fails in the comparison with NP5.
Finally, NP5 beats NP6 in the last match and is selected as the antecedent. All the test
instances to be generated in sequence for the resolution of [6 them] and [7 it] are listed in
Table 7.
The Tournament Elimination scheme has a computational complexity of O(N),
where N is the number of the candidates. Thus, it enables a relatively large number
335
Computational Linguistics Volume 34, Number 3
Table 7
Test instances generated under the twin-candidate model with the Tournament Elimination
scheme.
Anaphor Test Instance Result
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it ], [1 Those figures], [2 the government]} 01
i{[7 it ], [2 the government], [3 legislators]} 10
[7 it ] i{[7 it ], [2 the government], [4 September]} 10
i{[7 it ], [2 the government], [5 the government]} 01
i{[7 it ], [5 the government], [6 them]} 10
of candidates to be processed. However, as our twin-candidate model imposes no
constraints that enforce transitivity of the preference relation, the preference classifier
would likely output C1  C2, C2  C3, and C3  C1. Hence, it is unreliable to eliminate
a candidate once it happens to lose in one comparison, without considering all of its
winning/losing results against the other candidates.
Round Robin In Section 3.3, we have shown that the probability that a candidate is
the antecedent can be calculated using the preference classification results between the
candidate and its opponents. The candidate with the highest preference is selected as
the antecedent, that is:
Antecedent(ana) = argimax p (ante(Ci) | ana,C1,C2, . . . ,Cn)
? argimax
?
j =i
CF(i{ana,Ci,Cj},Ci) (5)
where CF(i{ana, Ci, Cj}, Ci) is the confidence with which the classifier determines Ci to
be preferred over Cj as the antecedent of ana. If we define the score of Ci as:
Score(Ci) =
?
j =i
CF(i{ana,Ci,Cj},Ci) (6)
Then, the most preferred candidate is the candidate that has the maximum score. If we
simply use 1 to denote the result that Ci is classified as preferred over Cj, and ?1 if Cj is
preferred otherwise, then:
Score(Ci) = |{Cj|Ci  Cj}| ? |{Cj|Cj  Ci}| (7)
That is, the score of a candidate is the number of the opponents to which it is preferred,
less the number of the opponents to which it is less preferred. To obtain the scores, the
antecedent candidates are comparedwith each other. For each candidate, its comparison
336
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 8
Test instances generated under the twin-candidate model with the Round Robin scheme.
Anaphor Test Instance Result
i{[7 it], [1 Those figures], [2 the government]} 01
i{[7 it], [1 Those figures], [3 legislators]} 01
i{[7 it], [1 Those figures], [4 September]} 01
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [1 Those figures], [6 them]} 01
i{[7 it], [2 the government], [3 legislators]} 10
i{[7 it], [2 the government], [4 September]} 10
[7 it] i{[7 it], [2 the government], [5 the government]} 01
i{[7 it], [2 the government], [6 them]} 10
i{[7 it], [3 legislators], [4 September]} 01
i{[7 it], [3 legislators], [5 the government]} 01
i{[7 it], [3 legislators], [6 them]} 01
i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [4 September], [6 them]} 01
i{[7 it], [5 the government], [6 them]} 10
result against every other candidate is recorded. Its score increases by one if it wins a
match, or decreases by one if it loses. The candidate with the highest score is selected as
the antecedent.
Antecedent identification carried out in such a way corresponds to a type of tourna-
ment called Round Robin in which each participant plays every other participant once,
and the final champion is selected based on the winning?losing records of the players.
In contrast to the Elimination scheme, the Round Robin scheme is more reliable in
that the preference of a candidate is determined by overall comparisons with the other
competing candidates. The computational complexity of the scheme is O(N2), where N
is the number of the candidates.
To illustrate this, consider the example in Table 5 again. The test instances to be
generated for resolving the anaphor [7 it] are listed in Table 8. As shown, each of
the candidates is compared with every other competing candidate. The scores of the
candidates are summarized in Table 9. Here, the candidate NP5 beats all the opponents
in the comparisons and obtains the maximum score of five. Thus it will be selected as
the antecedent.
An extension of the above Round Robin scheme is called the Weighted Round
Robin scheme. In the weighted version, the confidence values returned by the classifier,
Table 9
Scores for the candidates under the Round Robin scheme.
NP1 NP2 NP3 NP4 NP5 NP6 Score
NP1 ?1 ?1 ?1 ?1 ?1 ?5
NP2 +1 +1 +1 ?1 +1 +3
NP3 +1 ?1 ?1 ?1 ?1 ?3
NP4 +1 ?1 +1 ?1 ?1 ?1
NP5 +1 +1 +1 +1 +1 +5
NP6 +1 ?1 +1 +1 ?1 +1
337
Computational Linguistics Volume 34, Number 3
Table 10
Statistics for the training and testing data sets.
NWire NPaper BNews
# Tokens 85k 72k 67kTrain
# Files 130 76 216
# Tokens 20k 18k 18kTest
# Files 29 17 51
instead of the simple 0 and 1, are employed to calculate the score of a candidate based
on the formula
Score(Ci) =
?
CiCj
CF(Ci  Cj)?
?
CkCi
CF(Ck  Ci) (8)
Here, CF is the confidence value that the classifier returns for the corresponding
instance.
3.5 Evaluation
3.5.1 Experimental Setup.We used the ACE (Automatic Content Extraction)4 coreference
data set for evaluation. All the experiments were done on the ACE-2 V1.0 corpus. It
contains two data sets, training and devtest, which were used for training and testing,
respectively. Each of these sets is further divided into three domains: newswire (NWire),
newspaper (NPaper), and broadcast news (BNews). Statistics for the data sets are sum-
marized in Table 10.
For both training and resolution, a raw input document was processed by a
pipeline of NLP modules including a Tokenizer, Part-of-Speech tagger, NP chunker,
Named-Entity (NE) Recognizer, and so on. These preprocessing modules were meant to
determine the boundary of each NP in a text, and to provide the necessary information
about an NP for subsequent processing. Trained and tested on the UPENWSJ TreeBank,
the POS tagger (Zhou and Su 2000) could obtain an accuracy of 97% and the NP
chunker (Zhou and Su 2000) could produce an F-measure above 94%. Evaluated for
the MUC-6 and MUC-7 Named-Entity task, the NER module (Zhou and Su 2002) could
provide an F-measure of 96.6% (MUC-6) and 94.1% (MUC-7).
In our experiments, we focused on the resolution of the third-personal pronominal
anaphors, including she, he, it, they as well as their morphologic variants (such as her, his,
him, its, itself, them, etc.). For both training and testing, we considered all the pronouns
that had at least one preceding NP in their respective annotated coreferential chains. We
used the accuracy rate as the evaluation metric, and defined it as follows:
Accuracy =
number of anaphors being correctly resolved
total number of anaphors to be resolved
(9)
Here, an anaphor is deemed ?correctly resolved? if the found antecedent is in the co-
referential chain of the anaphor.
4 See http://www.itl.nist.gov/iad/894.01/tests/ace for a detailed description of the ACE program.
338
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 11
Statistics of the training instances generated for the pronominal anaphora resolution task.
NWire NPaper BNews
0 instances 8,200 11,648 6,037
Single-Candidate 1 instances 1,241 1,466 1,291
01 instances 6,899 9,861 5,004
Twin-Candidate 10 instances 1,301 1,787 1,033
For pronoun resolution, the distance between the closest antecedent and the
anaphor is usually short, predominantly (98% for the current data set) limited to only
one or two sentences (McEnery, Tanaka, and Botley 1997). For this reason, given an
anaphor, we only took the NPs occurring within the current and previous two sentences
as initial antecedent candidates. The candidates with mismatched number and gender
agreement were filtered automatically from the candidate set. Also, pronouns or NEs
that disagreed in person with the anaphor were removed in advance. For training, there
were 1,241 (NWire), 1,466 (NPaper), and 1,291 (BNews) anaphors found with at least
one antecedent in the candidate set. For testing, the numbers were 313 (NWire), 399
(NPaper), and 271 (BNews). On average, an anaphor had nine antecedent candidates.
Table 11 summarizes the statistics of the training instances as well as the class
distribution. Note that for the single-candidate model, the number of ?1? instances
was identical to the number of anaphors in the training data, because we only used
the closest antecedents of anaphors to create the positive instances. The number of ?0?
instances was equal to the total number of ?01? and ?10? training instances for the twin-
candidate model.
We examined three different learning algorithms: C5 (Quinlan 1993), Maximum
Entropy (Berger, Della Pietra, and Della Pietra 1996), and SVM (linear kernel) (Vapnik
1995),5 using the software See5,6 OpenNlp.MaxEnt,7 and SVM-light,8 respectively. All
the classifiers were learned with the default learning parameters set in the respective
learning software.
3.5.2 Results and Discussions. Table 12 lists the performance of the different anaphora
resolution systems with the single-candidate (SC) and the twin-candidate (TC) models.
For the TC model, two antecedent identification schemes, Tournament Elimination and
Round Robin, were compared.
From the table, we can see that our baseline systemwith the single-candidate model
can obtain accuracy of up to 72.9% (NWire), 77.1% (NPaper), and 74.9% (BNews).
5 As MaxEnt learns a probability model, we used the returned probability as the confidence of a candidate
being the antecedent. For C5, the confidence value of a candidate was estimated based on the following
smoothed ratio:
CF =
p+ 1
t+ 2
where cwas the number of positive instances and twas the total number of instances stored in the
corresponding leaf node. For SVM, the returned value was used as the confidence value: the lower
(maybe negative) the less confident.
6 http://www.rulequest.com/see5-info.html
7 http://MaxEnt.sourceforge.net/
8 http://svmlight.joachims.org/
339
Computational Linguistics Volume 34, Number 3
Table 12
Accuracy in percent for the pronominal anaphora resolution.
NWire NPaper BNews Average
C5 SC 71.6 75.6 69.5 72.7
TC
- Elimination 71.6 81.3 74.5 76.4
- Round Robin 72.9 81.3 74.9 76.9
- Weighted Round Robin 72.9 80.5 75.6 76.7
MaxEnt SC 72.9 77.1 74.9 75.2
TC
- Elimination 75.1 79.1 77.5 77.4
- Round Robin 75.1 79.1 77.5 77.4
- Weighted Round Robin 75.7 78.6 77.1 77.3
SVM SC 72.9 77.3 74.2 75.1
TC
- Elimination 73.5 82.0 78.9 78.5
- Round Robin 74.4 82.0 78.9 78.7
- Weighted Round Robin 74.6 79.3 78.2 77.5
Rank SVM 73.5 79.3 76.4 76.7
The average accuracy is comparable to that reported by Kehler et al (2004) (around
75%), who also used the single-candidate model to do pronoun resolution with similar
features (using MaxEnt) on the ACE data sets. By contrast, the systems with the twin-
candidate model are able to achieve accuracy of up to 75.7% (NWire), 82.0% (NPaper),
and 78.9% (BNews). The average accuracy is 76.9% for C5, 77.4% for MaxEnt, and 78.7%
for SVM,which is statistically significantly9 better than the results of the baselines (4.2%,
2.2%, and 3.6% in accuracy). These results confirm our claim that the twin-candidate
model is more effective than the single-candidate model for the task of pronominal
anaphora resolution.
We see no significant difference between the accuracy rates (less than 1.0% accuracy)
produced by the two antecedent identification schemes, Tournament Elimination and
Round Robin. This is in contrast to our belief that the Round Robin scheme, which is
more reliable than the Tournament Elimination, should lead to much better results. One
possible reason could be that the classifier in our systems can make a correct preference
judgement (with accuracy above 92% as in our test) in the cases where one candidate is
the antecedent and the other is not. As a consequence, the simple linear search can find
the final antecedent as well as the Round Robin method. These results suggest that we
can use the Elimination scheme in a practical system to make antecedent identification
more efficient. (Recall that the Elimination scheme requires complexity ofO(N), instead
of O(N2) as in Round Robin.)
Ranking-SVM In our experiments, we were particularly interested in comparing
the results using the twin-candidate model and those directly using a preference learn-
ing algorithm. For this purpose, we built a system based on Ranking-SVM (Joachims
2002), an extension of SVM capable of preference learning.
9 Throughout our experiments, the significance was examined by using the paired t-test, with p < 0.05.
340
Yang, Su, and Tan A Twin-Candidate Model for AR
The system uses a similar framework to the single-candidate-based system. For
training, given an anaphor, a set of instances is created for each of the antecedent candi-
dates. To learn the preference between competing candidates, a ?query-ID? is specified
for each training instance in such a way that the instances formed by the candidates of
the same anaphor bear the same query-ID. The label of an instance represents the rank of
the candidate in the candidate set; here, ?1? for the instances formed by the candidates
that are the antecedents, and ?0? for the instances formed by the others. The training
instances are associated with features as defined in Table 3, to which the Ranking-
SVM algorithm is then applied to generate a preference classifier. During resolution, for
each candidate of a given anaphor, a test instance is formed and passed to the learned
classifier, which in turn returns a value to represent the rank of the candidate among all
the candidates. The anaphor is resolved to the one with the highest value.
In fact, if we look into the learning mechanism of Ranking-SVM, we can find
that the algorithm will, in the background, pair any two instances that have the same
query-ID but different rank labels. This is quite similar to the twin-candidate model,
which creates an instance by putting together two candidates with different preferences.
However, one advantage of the twin-candidate model is that it can explicitly record
various relationships between two competing candidates, for example, ?which one
of the two candidates is closer to the anaphor in position/syntax/semantics??10 Such
inter-candidate information can make the preference between candidates clearer, and
thus facilitate both preference learning and determination. In contrast, Ranking-SVM,
which constructs instances in the single-candidate form, cannot effectively capture this
kind of information.
The last line of Table 12 shows the results from such a system based on Ranking-
SVM. We can see that the system achieves an average accuracy of 76.7%, statistically
significantly better than the baseline system with the single-candidate model by 1.6%
(0.4% for NWire, 2.0% for NPaper, and 2.2% for BNews). The results lend support to our
claim that the preference relationships between candidates, if taken into consideration
for classifier training, can lead to better resolution performance. Still, we observe that
our twin-candidate model beats Ranking-SVM in average accuracy by 1.8% (Elimina-
tion scheme) and 2.0% (Round Robin).
Decision Tree One advantage of the C5 learning algorithm is that the generated
classifier can be easily interpreted by humans, and the importance of the features
can be visually illustrated. In Figures 1 and 2, we show the decision trees (top four
levels) output by C5 for the NWire domain, based on the single-candidate and the
twin-candidate models, respectively. As the twin-candidate model uses a larger pool
of features, the tree for the twin-candidate model is more complicated (180 nodes) than
the one for the single-candidate model (36 nodes).
From the two trees, we can see that bothmodels rely on similar features such as lexi-
cal, positional, and grammatical properties for pronoun resolution. However, we can see
that the preferential factors (e.g., subject preference, parallelism preference, and distance
preference as discussed in Section 3.2) are more clearly presented in the twin-candidate-
based tree. For example, if two candidates are both pronouns, the twin-candidate-based
tree will suggest that the one closer to the anaphor has a higher preference to be the
antecedent. By contrast, such a preference relationship has to be implicitly represented
10 In the current work, we only consider the positional relationship between candidates by stipulating
that i < j for an instance i{ana, Ci, Cj}. In our future work, we will explore more inter-candidate
relationships that are helpful for preference determination.
341
Computational Linguistics Volume 34, Number 3
Figure 1
Decision tree generated for pronoun resolution under the single-candidate model. For feature
ana Type, the values PRON SHE,PRON SHE,PRON SHE, and PRON THEY represent whether
the anaphor is a pronoun such as she, he, it, and they, respectively. For candi Subject, the values
SUBJ MAIN, SUBJ CLAUSE and NO represent whether the candidate is the subject of a main
sentence, or the subject of a clause, or not. For candi Object, the values OBJ VERB, OBJ PREP, and
NO represent whether the candidate is the object of a verb, a preposition, or not, respectively.
For other features, 0 and 1 represent yes/no.
in the single-candidate-based tree, with different confidence values being assigned to
the candidates in different sentences.
Learning Curve In our experiments, we were also concerned about how training
data size might influence anaphora resolution performance. For this purpose, we di-
vided the anaphors in the training documents into 10 batches, and then performed
resolution using the classifiers trained with 1, 2, . . . , 10 batches of anaphors. Figure 3
plots the learning curves of the systems with the single-candidate model and the twin-
candidate model (Round Robin scheme) for the NPaper domain. Each accuracy rate
shown in the figure is the average of the results from three trials trained on different
anaphors.
From the figure we can see that both the single-candidate model and the twin-
candidate model reach their peak performance with around six batches (around
880 anaphors). As shown, the twin-candidate model is not apparently superior to
the single-candidate model when the size of the training data is small (below two
batches, 290 anaphors). This is due to the fact that the number of features in the twin-
candidate model is nearly double that in the single-candidate model. As a result, the
twin-candidate model requires more training data than the single-candidate model to
avoid the data sparseness problem. Nevertheless, it does not need too much training
data to beat the latter; it can produce the accuracy rates consistently higher than the
342
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 2
Decision tree generated for pronoun resolution under the twin-candidate model.
Figure 3
Learning curves of different models for pronominal anaphora resolution in the NPaper Domain
(120 anaphors per batch).
343
Computational Linguistics Volume 34, Number 3
Table 13
A sample text for coreference resolution.
[1 Globalstar] still needs to raise [2 $600 million], and
[3 Schwartz] said [4 that company] would try to raise [5 the
money] in [6 the debt market].
single-candidate model when trained with more than two batches of anaphors. This
figure further demonstrates that the twin-candidate model is reliable and effective for
the pronominal anaphora resolution task.
4. Deploying the Twin-Candidate Model to Coreference Resolution
One task that is closely related to anaphora resolution is coreference resolution, the
process of identifying all the coreferential expressions in texts.11 Coreference resolution
is different from anaphor resolution. The latter focuses on how an anaphor can be suc-
cessfully resolved, and the resolution is done on given anaphors. The former, in contrast,
focuses on how the NPs that are coreferential with each other can be found correctly
and completely, and the resolution is done on all possible NPs. In a text, many NPs,
especially the non-pronouns, are non-anaphors that have no antecedent to be found
in the previous text. Hence, the task of coreference resolution is a more complicated
challenge than anaphora resolution, as a solution should not only be able to resolve
an anaphor to the correct antecedent, but should also refrain from resolving a non-
anaphor. In this section, we will explore how to deploy the learning models for anaphor
resolution in the coreference resolution task. As pronouns are usually anaphors, we will
focus mainly on the resolution of non-pronouns.
4.1 Coreference Resolution Based on the Single-Candidate Model
In practice, the single-candidate model can be applied to coreference resolution directly,
using the similar training and testing procedures to those used in anaphora resolution
(described in Section 2).
For training, we create ?0? and ?1? training instances for each encountered anaphor,
that is, the NP that is coreferential with at least one preceding NP. Specifically, given an
anaphor and its antecedent candidates, a positive instance is generated for the closest
antecedent and a set of negative instances is generated for each of the candidates that is
not coreferential with the anaphor.12
Consider the text in Table 13 as an example. In the text, [4 that company] and [5 the
money] are two anaphors, with [1 Globalstar] and [2 $600 million] being their antecedents,
respectively. Table 14 lists the training instances to be created for this text.
11 In our study, we only consider within-document noun phrase coreference resolution.
12 In some coreference resolution systems (Soon, Ng, and Lim 2001; Ng and Cardie 2002b), only the
non-coreferential candidates occurring between the closest antecedent and the anaphor are used to create
negative instances. In the experiments, we found that these sampling strategies for negative instances
led to a trade-off between recall and precision, but no significant difference in the overall F-measure.
344
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 14
Training instances generated under the single-candidate model for coreference resolution.
Anaphor Training Instance Label
i{[4 that company] , [1 Globalstar]} 1
[4 that company] i{[4 that company] , [2 $600 million]} 0
i{[4 that company] , [3 Schwartz]} 0
i{[5 the money] , [1 Globalstar]} 0
[5 the money] i{[5 the money] , [2 $600 million]} 1
i{[5 the money] , [3 Schwartz]} 0
i{[5 the money] , [4 that company]} 0
Table 15
Feature set for coreference resolution.
ana Def whether the possible anaphor is a definite description
ana Indef whether the possible anaphor is an indefinite NP
ana Name whether the possible anaphor is a named entity
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite description
candi Name whether the candidate is a named-entity
candi SentDist the sentence distance between the possible anaphor and the candidate
candi NameAlias whether the candidate and the candidate are aliases for each other
candi Appositive whether the possible anaphor and the candidate are in an appositive
structure
candi NumberAgree whether the possible anaphor and the candidate agree in number
candi GenderAgree whether the possible anaphor and the candidate agree in gender
candi HeadStrMatch whether the possible anaphor and the candidate have the same head
string
candi FullStrMatch whether the possible anaphor and the candidate contain the same
strings (excluding the determiners)
candi SemAgree whether the possible anaphor and the candidate belong to the same
semantic category in WordNet
In Table 15, we list the features used in our study for coreference resolution, which
are similar to those proposed in Soon, Ng, and Lim?s (2001) system.13 All these features
are domain independent and the values can be computed with low cost but high
reliability.
After training, the learned classifier can be directly used for coreference resolution.
Given an NP to be resolved, a test instance is generated for each of its antecedent
candidates. The classifier, being given the instance, will determine the likelihood that
the candidate is the antecedent of the possible anaphor. If the confidence is below
a pre-specified threshold, the candidate is discarded. In the case where none of the
candidates have a confidence higher than the threshold, the current NP is deemed a
13 As we focus on coreference resolution for non-pronouns, we do not use the feature that describes
whether or not the NP to be resolved is a pronoun. Also, we do not use the feature that describes
whether or not a candidate is a pronoun, because, as will be discussed together with the experiments,
a pronoun is not taken as an antecedent candidate for a non-pronoun to be resolved.
345
Computational Linguistics Volume 34, Number 3
non-anaphor and left unresolved. Otherwise, it is resolved to the candidate with the
highest confidence.14
4.2 Coreference Resolution Based on the Twin-Candidate Model
The twin-candidate model presented in the previous section focuses on the preference
between candidates. The model will always select a ?best? candidate as the antecedent,
even if the current NP is a non-anaphor. To deal with this problem, we will teach the
preference classifier how to identify non-anaphors, by incorporating non-anaphors to
create a special class of training instances. For resolution, if the newly learned classifier
returns the special class label, wewill know that the current NP is a non-anaphor, and no
preference relationship holds between the two candidates under consideration. In this
way, the twin-candidate model is capable of carrying out both antecedent identification
and anaphoricity determination by itself, and thus can be deployed for coreference
resolution directly. In this section, we will describe the modified training and resolution
procedures of the twin-candidate model.
4.2.1 Training. As with anaphora resolution, an instance of the twin-candidate model
for coreference resolution takes the form i{ana, Ci, Cj}, where ana is a possible anaphor,
and Cj and Cj are two of its antecedent candidates (i < j). The feature set is similar to
that for the single-candidate model as defined in Table 15, except that a candi X feature
is replaced by a pair of features, cand1 x and candi2 x, for the two competing candi-
dates, respectively.
During training, if an encountered NP is an anaphor, we create ?01? or ?10? training
instances in the same way as in the original learning framework. If the NP is a non-
anaphor, we do the following:
 From the antecedent candidates,15 randomly select one as the anchor
candidate.
 Create a set of instances by pairing the anchor candidate and each of the
other non-coreferential candidates.
The instances formed by the non-anaphors are labeled ?00.?
Consider the sample text in Table 13. For the two anaphors [4 that company] and
[5 the money], we create the ?01? and ?10? instances as usual. For the non-anaphors
[3 Schwartz] and [6 the debt market], we generate two sets of ?00? instances. Table 16 lists
all the training instances for the text (supposing [1 Globalstar] and [2 $600 million] are the
anchor candidates for [3 Schwartz] and [6 the debt market], respectively).
The ?00? training instances are used together with the ?01? and ?10? ones to train
a classifier. Given a test instance i{ana, Ci, Cj} (i < j), the newly learned classifier is
supposed to return ?01? (or ?10?), indicating ana is an anaphor and Ci (or Cj) is preferred
as its antecedent, or return ?00?, indicating ana is a non-anaphor and no preference
exists between Ci and Cj.
14 Other clustering strategies are also available, for example, ?closest-first? where a possible anaphor is
resolved to the closest candidate with the confidence above the specified threshold, if any (Soon, Ng,
and Lim 2001).
15 For a non-anaphor, we also take the preceding NPs as its antecedent candidates. We will discuss this
issue later together with the experimental setup.
346
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 16
Training instances generated under the twin-candidate model for coreference resolution.
Possible Anaphor Training Instance Label
i{[4 that company], [1 Globalstar], [2 $600 million]} 10
[4 that company] i{[4 that company], [1 Globalstar], [3 Schwartz]} 10
i{[5 the money], [1 Globalstar], [2 $600 million]} 01
[5 the money] i{[5 the money], [2 $600 million], [3 Schwartz]} 10
i{[5 the money], [2 $600 million], [4 that company]} 10
[3 Schwartz] i{[3 Schwartz], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [2 $600 million], [3 Schwartz]} 00
[6 the debt market] i{[6 the debt market], [2 $600 million], [4 that company]} 00
i{[6 the debt market], [2 $600 million], [5 the money]} 00
4.2.2 Antecedent Identification. Accordingly, we make a modification to the original Tour-
nament Elimination and the Round Robin schemes:
Tournament Elimination Scheme As with anaphora resolution, given an NP to be
resolved, candidates are compared linearly from the beginning to the end. If an instance
for two competing candidates is classified as ?01? or ?10?, the preferred candidate will
be compared with subsequent competitors while the loser is eliminated immediately.
If the instance is classified as ?00?, both the two candidates are discarded and the
comparison restarts with the next two candidates.16 The process continues until all the
candidates have been compared. If both of the candidates in the last match are judged to
be ?00?, the current NP is left unresolved. Otherwise, the NPwill be resolved to the final
winner, on the condition that the highest confidence that the winner has ever obtained
is above a pre-specified threshold.
Round Robin Scheme In the Round Robin scheme, each candidate is compared
with every other candidate. If two candidates are labeled ?00? in a match, both candi-
dates receive a penalty of ?1 in their respective scores. If no candidate has a positive
final score, then the NP is considered non-anaphoric and left unresolved. Otherwise,
it is resolved to the candidate with the highest score as usual. Here, we can also use a
threshold. That is, we will update the scores of the two candidates in a match if and
only if the preference confidence returned by the classifier is higher than a pre-specified
threshold.
In rare cases where an NP to be resolved has only one antecedent candidate, a
pseudo-instance is created by pairing the candidate with itself. The NP will be resolved
to the candidate unless the instance is labeled ?00?.
4.3 Evaluation
4.3.1 Experimental Setup. We used the same ACE data sets for coreference resolution
evaluation, as described in the previous section for anaphora resolution. A raw input
document was processed in advance by the same pipeline of NLP modules including
16 If only one candidate remains, it will be compared with the candidate eliminated last.
347
Computational Linguistics Volume 34, Number 3
Table 17
Statistics of the training instances generated for coreference resolution (non-pronoun).
NWire NPaper BNews
0 instances 78,191 105,152 33,748Single-Candidate 1 instances 3,197 3,792 2,094
00 instances 296,000 331,957 159,752
Twin-Candidate 01 instances 50,499 70,433 21,170
10 instances 27,692 34,719 12,578
POS-tagger, NP chunker, NE recognizer, and so on, to obtain all possible NPs and
related information (see Section 3.5.1).
For evaluation, we adopted Vilain et al?s (1995) scoring algorithm in which recall
and precision17 were computed by comparing the key chains (i.e., the annotated ?stan-
dard? coreferential chains) and the response chains (i.e., the chains generated by the
coreference resolution system).
As already mentioned, the twin-candidate model described in this section is mainly
meant for non-pronouns that are often not anaphoric. To better examine the utility
of the model in our experiments, we first focused on coreference resolution for non-
pronominal NPs. The recall and precision to be reported were computed based on the
response chains and the key chains from which all the pronouns are removed. We will
later show the results of overall coreference resolution for whole NPs by combining the
resolution of pronouns and non-pronouns.
In non-pronoun resolution, an anaphor and its antecedent do not often occur a short
distance apart as they do in pronoun resolution. For this reason, during training, we
took as antecedent candidates all the preceding non-pronominal NPs18 in the current
and previous four sentences; while during testing, we used all the preceding non-
pronouns, regardless of distance, as candidates.19 The statistics of the training instances
for each data set are summarized in Table 17.
Again, we examined the three learning algorithms: C5, MaxEnt, and SVM.20 As
both the single-candidate and the twin-candidate models used a threshold to block low-
confidence coreferential pairs, we performed three-fold cross-evaluation on the training
data to determine the thresholds for the coreference resolution systems.
4.3.2 Results and Discussions. Table 18 lists the results for the different systems on the
non-pronominal NP coreference resolution. We used as the baseline the system with the
single-candidate model described in Section 4.1. As mentioned, the system was trained
17 The overall F-measure was defined as
2 ? Recall ? Precision
Recall+ Precision
18 As suggested in Ng and Cardie (2002b), we did not include pronouns in the candidate set of a
non-pronoun, because a pronoun is usually anaphoric and cannot give much information about
the entity to which it refers.
19 Unlike in the case of pronoun resolution, we did not filter candidates that had mismatched
number/gender agreement as these constraints are not reliable for non-pronoun resolution (e.g.,
in our data set, around 15% of coreferential pairs do not agree in number). Instead, we took these
factors as features (see Table 15) and let the learning algorithm make the preference decision.
20 For SVM, we employed the one-against-all aggregation method for the 3-class learning and testing.
348
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 18
Recall (R), Precision (P), and F-measure (F) in percent for coreference resolution (non-pronoun).
NWire NPaper BNews
R P F R P F R P F
C5 SC
- baseline 63.3 48.1 54.7 63.8 42.2 50.8 63.5 53.7 58.2
- with non-anaphors 40.9 81.5 54.4 39.8 81.4 53.4 35.1 76.8 48.2
TC
- Elimination 50.8 63.0 56.2 56.6 60.1 58.3 44.6 71.2 54.9
- Round Robin 58.7 57.9 58.3 56.5 60.5 58.4 49.0 70.1 57.7
MaxEnt SC
- baseline 62.1 52.3 56.8 56.4 58.8 57.6 61.8 54.1 57.7
- with non-anaphors 59.6 54.0 56.7 54.2 62.6 58.1 53.8 58.4 56.0
TC
- Elimination 59.1 55.4 57.2 52.2 69.0 59.5 53.5 61.9 57.4
- Round Robin 58.7 55.9 57.2 53.4 65.9 59.0 54.3 62.8 58.3
SVM SC
- baseline 64.1 49.0 55.5 65.5 42.1 51.3 63.5 53.7 58.2
- with non-anaphors 42.3 70.0 52.7 40.0 76.6 52.5 35.7 77.0 48.8
TC
- Elimination 57.8 53.2 55.4 51.7 56.5 54.0 63.3 53.8 58.2
- Round Robin 54.3 56.9 55.6 56.1 58.1 57.1 63.7 53.8 58.3
on the instances formed by anaphors. For better comparison with the twin-candidate
model, we built another single-candidate-based system inwhich the non-anaphors were
also incorporated for training. Specifically, for each encountered non-anaphor during
training, we created a set of ?0? instances by pairing the non-anaphor with each of the
candidates. These instances were added to the original instances formed by anaphors
to learn a classifier,21 which was then applied for the resolution as usual.
The results for the two single-candidate based systems are listed in Table 18. When
trained with the instances formed only by anaphors, the system could achieve recall
above 60% and precision of around 50% for the three domains. When trained with the
instances formed by both anaphors and non-anaphors, the system yielded a significant
improvement in precision. In the case of using C5 and SVM, the system is capable of
producing precision rates of up to 80%. The increase in precision is reasonable since the
classifier tends to be stricter in blocking non-anaphors. Unfortunately, however, at the
same time recall drops significantly, and no apparent improvement can be observed in
the resulting overall F-measure.
When trained with non-anaphors incorporated, the systems with the twin-
candidate model, described in Section 4.2, are capable of yielding higher precision
against the baseline. Although recall also drops at the same time, the increase in
precision can compensate it well: We observe that in most cases, the system with the
twin-candidate model can achieve a better F-measure than the baseline system with
the single-candidate model. Also, the improvement is statistically significant (t-test,
p < 0.05) in the NWire domain when C5 is used (3.6%), and in the NPaper domain
21 The statistics of the ?0? instances shown in Table 17 become 392,646, 455,167, and 207,667 for NWire,
NPaper, and BNews, respectively.
349
Computational Linguistics Volume 34, Number 3
when any of the three learning algorithms, C5 (5.0%), MaxEnt (1.4%), and SVM (4.6%),
is used. These results suggest that our twin-candidate model can effectively identify
non-anaphors and block their invalid resolution, without affecting the accuracy of
determining antecedents for anaphors.
Compared with the pronoun resolution described in the previous section, here
we find that for non-pronoun resolution the superiority of the twin-candidate model
against the single-candidate model is not apparent. In some domains such as BNews,
the difference between the two models is not statistically significant. One possible
explanation is that for non-pronoun resolution, the features that really matter are quite
limited, that is, NameAlias, String-Matching, and Appositive (we will later show this
in the decision trees). A candidate that has any one of these features is most likely the
antecedent, regardless of the other competing candidates. In this situation, the single-
candidate model, which considers candidates in isolation, does as well as the twin-
candidate model. Still, the results suggest that the twin-candidate model is suitable for
both resolution tasks, no matter whether the features involved are strongly indicative
(as with non-pronoun resolution) or not (as with pronoun resolution).
As with anaphora resolution, we do not observe any apparent performance differ-
ence between the two twin-candidate identification schemes, Tournament Elimination
and Round Robin. The Round Robin scheme performs better than Elimination when
trained using C5 and SVM, by up to 2.8% and 2.9% in F-measure, respectively. However,
the Elimination scheme, when trained using MaxEnt, is capable of performing equally
well or slightly better (0.5% F-measure) than the Round Robin scheme.
Recall vs. Precision As discussed, the results in Table 18 show different recall and
precision patterns for different systems. The baseline system with the single-candidate
model tends to yield higher recall while the system with the twin-candidate model
tends to produce higher precision. Thus, a fairer comparison of the two systems is to
examine the precision rates that these systems achieve under the same recall rates. For
this purpose, in Figure 4, we plot the variant recall and precision rates that the two
systems are capable of obtaining (tested using MaxEnt, Round Robin scheme, for the
NPaper domain), focusing on precision rates above 50% and recall rates above 40%.
From the figure, we find that the systemwith the twin-candidate model achieves higher
precision for recall rates ranging from 40% and 55%, and performs equally well for recall
rates above 55%, which further proves the reliability of our twin-candidate model for
coreference resolution.
Decision Trees In Figures 5 and 6, we show the two decision trees (NWire domain)
generated by the systems with the single-candidate model and the twin-candidate
model, respectively. The tree from the single-candidate model contains only 13 nodes,
considerably smaller than that from the twin-candidate model, which contains around
1.2k nodes. From the figure, we can see that both models heavily rely on string-
matching, name-alias, and appositive features to perform non-pronoun resolution, in
contrast to pronoun resolution where lexical and positional features seem more impor-
tant (as shown in Figures 1 and 2).
Learning Curves In our experiments, we were also interested in evaluating the
resolution performance of the two learning models on different quantities of training
data. Figure 7 plots the learning curves for the systems using the single-candidate model
and the system using the twin-candidate model (NPaper domain). The F-measure is
averaged over three random trials trained on 5, 10, 15, . . . documents. Consistent with
the curves for the anaphora resolution task as depicted in Figure 3, the system with
the twin-candidate model outperforms the one with the single-candidate model on a
small amount of training data (less than five documents). When more data is available,
350
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 4
Various recall (%) and precision (%) of different models for non-pronoun resolution.
Figure 5
Decision tree generated for non-pronoun resolution under the single-candidate model.
the twin-candidate model also yields a consistently better F-measure than the single-
candidate model.
Overall Coreference Resolution Having demonstrated the performance of the
twin-candidate model on coreference resolution for non-pronouns, we now further
examine overall coreference resolution for whole NPs, combining both pronoun
resolution and non-pronoun resolution. Specifically, given an input test document,
we check each encountered NP from beginning to end. If it is a pronoun,22 we use
22 We identify the pleonastic use of it in advance (79.2% accuracy) using a set of predefined pattern rules
based on regular expressions. The first-person and second-person pronouns are heuristically resolved
to the closest pronoun of the same type or a speaker nearby, if any, with an average 61.8% recall and
79.5% precision.
351
Computational Linguistics Volume 34, Number 3
Figure 6
Decision tree generated for non-pronoun resolution under the twin-candidate model.
Figure 7
Learning curves of different models for non-pronoun resolution.
the pronominal anaphora resolution systems, as described in the previous section, to
resolve it to an antecedent. Otherwise, we use the non-pronoun coreference resolution
systems described in this section to resolve the NP to an antecedent, if any is found. All
the coreferential pairs are put together in a coreferential chain. The recall and precision
rates are computed by comparing the standard key chains and generated response
chains using Vilain et al?s (1995) algorithm.
352
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 19
Recall (R), Precision (P), and F-Measure (F) in percent for coreference resolution.
NWire NPaper BNews
R P F R P F R P F
C5 SC 62.2 52.6 57.0 64.9 50.6 56.9 62.9 58.5 60.6
TC
- Elimination 53.8 65.9 59.2 61.2 64.4 62.8 53.1 70.9 60.7
- Round Robin 59.0 61.2 60.1 62.0 64.3 63.1 56.0 69.9 62.2
MaxEnt SC 60.7 56.0 58.3 60.8 62.2 61.5 63.8 60.6 62.2
TC
- Elimination 59.5 59.2 59.3 58.6 67.8 62.9 59.3 66.4 62.7
- Round Robin 60.6 57.9 59.2 59.4 69.2 63.3 61.7 64.5 63.0
SVM SC 62.3 53.3 57.5 66.2 50.5 57.3 64.7 60.1 62.3
TC
- Elimination 57.6 57.0 57.3 58.5 62.6 60.5 65.0 60.6 62.7
- Round Robin 56.0 60.6 58.2 60.4 63.6 62.0 65.4 60.7 63.0
Table 19 lists the coreference resolution results of the systemswith different learning
models. We observe that the results for overall coreference resolution are better than
those of non-pronoun coreference resolution as shown in Table 18, which is due to the
comparatively high accuracy of the resolution of pronouns.
In line with the previous results for pronoun resolution and non-pronoun resolu-
tion, the twin-candidate model outperforms the single-candidate model in coreference
resolution for whole NPs. Consider the system trained with MaxEnt as an example.
The single-candidate-based system obtains F-measures of 58.3%, 61.5%, and 62.2% for
the NWire, NPaper, and BNews domains.23 By comparison, the twin-candidate-based
system (Round Robin scheme) can achieve F-measures of 59.2%, 63.3%, and 63.0% for
the three domains. The improvement over the single-candidate model in F-measure
(0.9%, 1.8%, and 0.8%) is larger than that for non-pronoun resolution (0.4%, 1.4%,
and 0.6% as shown in Table 18), owing to the higher gains obtained from pronoun
resolution. For the systems trained using C5 and SVM, similar patterns of performance
improvement may be observed.
5. Conclusion
In this article, we have presented a twin-candidate model for learning-based anaphora
resolution. The traditional single-candidate model considers candidates in isolation,
and thus cannot accurately capture the preference relationships between competing
candidates to provide reliable resolution. To deal with this problem, our proposed twin-
candidate model recasts anaphora resolution as a preference classification problem.
It learns a classifier that can explicitly determine the preference between competing
candidates, and then during resolution, choose the antecedent of an anaphor based on
the ranking of the candidates.
23 The results are comparable to the baseline system by Ng (2005), which also uses the single-candidate
model and is capable of F-measures of 50.1%, 62.1%, and 57.5% for the three domains, respectively.
353
Computational Linguistics Volume 34, Number 3
We have introduced in detail the framework of the twin-candidate model for
anaphora resolution, including instance representation, training procedure, and the an-
tecedent identification scheme. The efficacy of the twin-candidate model for pronominal
anaphora resolution has been evaluated in different domains, using ACE data sets. The
experimental results show that the model yields statistically significantly higher accu-
racy rates than the traditional single-candidate model (up to 4.2% in average accuracy
rate), suggesting that the twin-candidate model is superior to the latter for pronominal
anaphora resolution.
We have further investigated the deployment of the twin-candidate model in the
more complicated coreference resolution task, where not all the encountered NPs are
anaphoric. We have modified the model to make it directly applicable for coreference
resolution. The experimental results for non-pronoun resolution indicate that the twin-
candidate-based system performs equally well, and, in some domains, statistically
significantly better than the single-candidate based systems. When combined with
the results for pronoun resolution, the twin-candidate based system achieves further
improvement against the single-candidate-based systems in all the domains.
A number of further contributions can be made by extending this work in new
directions. Currently, we only adopt simple domain-independent features for learning.
Our recent work (Yang, Su, and Tan 2005) suggests that more complicated features, such
as statistics-based semantic compatibility, can be effectively incorporated in the twin-
candidate model for pronoun resolution. In future work, we intend to provide a more
in-depth investigation into the various kinds of knowledge that are suitable for the twin-
candidate model. Furthermore, in our current work for coreference resolution, all the
NPs preceding an anaphor are used as antecedent candidates, and all encountered non-
anaphors in texts are incorporated without filtering into training instance creation. For
more balanced training data and better classifier learning, we intend to explore some
instance-sampling techniques, such as those proposed by Ng and Cardie (2002a), to
remove in advance low-confidence candidates and the less informative non-anaphors.
We hope that these efforts can further improve the performance of the twin-candidate
model in both anaphora resolution and coreference resolution.
Acknowledgments
We would like to thank Guodong Zhou,
Alexia Leong, Stanley Wai Keong Yong,
and three anonymous reviewers for their
helpful comments and suggestions.
References
Aone, Ghinatsu and Scott W. Bennett.
1995. Evaluating automated and
manual acquisition of anaphora
resolution strategies. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 122?129, Cambridge, Massachusetts.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Foris,
Dordrecht, The Netherlands.
Clark, Herber H. and C. J. Sengul. 1979.
In search of referents for noun phrases
and pronouns.Memory and Cognition,
7:35?41.
Connolly, Dennis, John D. Burger, and
David S. Day, 1997. A machine learning
approach to anaphoric reference. In
New Methods in Language Processing,
pages 133?144, Taylor and Francis,
Bristol, Pennsylvania.
Crawley, Rosalind A., Rosemary J.
Stevenson, and David Kleinman. 1990.
The use of heuristic strategies in the
interpretation of pronouns. Journal of
Psycholinguistic Research, 19:245?264.
Denis, Pascal and Jason Baldridge. 2007. A
ranking approach to pronoun resolution.
In Proceedings of the 20th International Joint
354
Yang, Su, and Tan A Twin-Candidate Model for AR
Conference on Artificial Intelligence (IJCAI),
pages 1588?1593, Hyderabad, India.
Garnham, Alan, 2001.Mental Models and the
Interpretation of Anaphora. Psychology
Press Ltd., Hove, East Sussex, UK.
Ge, Niyu, John Hale, and Eugene Charniak.
1998. A statistical approach to anaphora
resolution. In Proceedings of the 6th
Workshop on Very Large Corpora,
pages 161?171, Montreal, Quebec, Canada.
Gernsbacher, Morton A. and David
Hargreaves. 1988. Accessing sentence
participants: The advantage of first
mention. Journal of Memory and Language,
27:699?717.
Grober, Ellen H., William Beardsley, and
Alfonso Caramazza. 1978. Parallel
function in pronoun assignment.
Cognition, 6:117?133.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hobbs, Jerry. 1978. Resolving pronoun
references. Lingua, 44:339?352.
Iida, Ryu, Kentaro Inui, Hiroya Takamura,
and Yuji Matsumoto. 2003. Incorporating
contextual cues in trainable models for
coreference resolution. In Proceedings of the
10th Conference of EACL, Workshop ?The
Computational Treatment of Anaphora,?
pages 23?30, Budapest, Hungary.
Joachims, Thorsten. 2002. Optimizing search
engines using clickthrough data. In
Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining
(KDD), pages 133?142, Edmonton,
Alberta, Canada.
Jurafsky, Daniel and James H. Martin.
2000. Speech and Language Processing:
An Introduction to Natural Language
Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall,
Upper Saddle River, New Jersey.
Kehler, Andrew. 1997. Probabilistic
coreference in information extraction.
In Proceedings of the 2nd Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 163?173,
Providence, Rhode Island.
Kehler, Andrew, Douglas Appelt,
Lara Taylor, and Aleksandr Simma. 2004.
The (non)utility of predicate-argument
frequencies for pronoun interpretation.
In Proceedings of the North American
Chapter of the Association for Computational
Linguistics annual meeting (NAACL),
pages 289?296, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):525?561.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm
based on the bell tree. In Proceedings
of the 42nd Annual Meeting of the
Association for Computational
Linguistics (ACL), pages 135?142,
Barcelona, Spain.
McCarthy, Joseph F. and Wendy G. Lehnert.
1995. Using decision trees for coreference
resolution. In Proceedings of the 14th
International Conference on Artificial
Intelligences (IJCAI), pages 1050?1055,
Montreal, Quebec, Canada.
McEnery, A., I. Tanaka, and S. Botley.
1997. Corpus annotation and reference
resolution. In Proceedings of the ACL
Workshop on Operational Factors in
Practical Robust Anaphora Resolution
for Unrestricted Texts, pages 67?74,
Madrid, Spain.
Ng, Hwee Tou, Yu Zhou, Robert Dale,
and Mary Gardiner. 2005. Machine
learning approach to identification
and resolution of one-anaphora. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1105?1110, Edinburgh,
Scotland.
Ng, Vincent. 2005. Machine learning for
coreference resolution: From local
classification to global ranking. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 157?164,
Ann Arbor, Michigan.
Ng, Vincent and Claire Cardie. 2002a.
Combining sample selection and
error-driven pruning for machine
learning of coreference rules. In
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 55?62,
Philadelphia, PA.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 104?111, Philadelphia, PA.
Preiss, Judita. 2001. Machine learning for
anaphora resolution. Technical Report
CS-01-10, University of Sheffield,
Sheffield, England.
355
Computational Linguistics Volume 34, Number 3
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann
Publishers, San Francisco, CA.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
Stevenson, Rosemary J., Alexander W. R.
Nelson, and Keith Stenning. 1995. The role
of parallelism in strategies of pronoun
comprehension. Language and Speech,
29:393?418.
Strube, Michael and Christoph Mueller.
2003. A machine learning approach to
pronoun resolution in spoken dialogue.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 168?175,
Sapporo, Japan.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York, NY.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 45?52, San Francisco, CA.
Wilks, Yorick. 1973. Preference Semantics.
Stanford AI Laboratory Memo AIM-206.
Stanford University.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, New York.
Yang, Xiaofeng, Jian Su, and Chew Lim Tan.
2005. Improving pronoun resolution using
statistics-based semantic compatibility
information. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 165?172, Ann Arbor, MI.
Zhou, Guodong and Jian Su. 2000.
Error-driven HMM-based chunk tagger
with context-dependent lexicon. In
Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora,
pages 71?79, Hong Kong.
Zhou, Guodong and Jian Su. 2002. Named
Entity recognition using a HMM-based
chunk tagger. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 473?480, Philadelphia, PA.
356
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?28,
New York, June 2006. c?2006 Association for Computational Linguistics
Semi-supervised Relation Extraction with Label Propagation
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
To overcome the problem of not hav-
ing enough manually labeled relation in-
stances for supervised relation extraction
methods, in this paper we propose a label
propagation (LP) based semi-supervised
learning algorithm for relation extraction
task to learn from both labeled and unla-
beled data. Evaluation on the ACE corpus
showed when only a few labeled examples
are available, our LP based relation extrac-
tion can achieve better performance than
SVM and another bootstrapping method.
1 Introduction
Relation extraction is the task of finding relation-
ships between two entities from text. For the task,
many machine learning methods have been pro-
posed, including supervised methods (Miller et al,
2000; Zelenko et al, 2002; Culotta and Soresen,
2004; Kambhatla, 2004; Zhou et al, 2005), semi-
supervised methods (Brin, 1998; Agichtein and Gra-
vano, 2000; Zhang, 2004), and unsupervised method
(Hasegawa et al, 2004).
Supervised relation extraction achieves good per-
formance, but it requires a large amount of manu-
ally labeled relation instances. Unsupervised meth-
ods do not need the definition of relation types and
manually labeled data, but it is difficult to evaluate
the clustering result since there is no relation type
label for each instance in clusters. Therefore, semi-
supervised learning has received attention, which
can minimize corpus annotation requirement.
Current works on semi-supervised resolution for
relation extraction task mostly use the bootstrap-
ping algorithm, which is based on a local consis-
tency assumption: examples close to labeled ex-
amples within the same class will have the same
labels. Such methods ignore considering the simi-
larity between unlabeled examples and do not per-
form classification from a global consistency view-
point, which may fail to exploit appropriate mani-
fold structure in data when training data is limited.
The objective of this paper is to present a label
propagation based semi-supervised learning algo-
rithm (LP algorithm) (Zhu and Ghahramani, 2002)
for Relation Extraction task. This algorithm works
by representing labeled and unlabeled examples as
vertices in a connected graph, then propagating the
label information from any vertex to nearby vertices
through weighted edges iteratively, finally inferring
the labels of unlabeled examples after the propaga-
tion process converges. Through the label propaga-
tion process, our method can make the best of the
information of labeled and unlabeled examples to re-
alize a global consistency assumption: similar ex-
amples should have similar labels. In other words,
the labels of unlabeled examples are determined by
considering not only the similarity between labeled
and unlabeled examples, but also the similarity be-
tween unlabeled examples.
2 The Proposed Method
2.1 Problem Definition
Let X = {xi}ni=1 be a set of contexts of occurrences
of all entity pairs, where xi represents the contexts
of the i-th occurrence, and n is the total number of
occurrences of all entity pairs. The first l examples
are labeled as yg ( yg ? {rj}Rj=1, rj denotes relation
type and R is the total number of relation types).
And the remaining u(u = n? l) examples are unla-
beled.
Intuitively, if two occurrences of entity pairs have
25
the similar contexts, they tend to hold the same re-
lation type. Based on this assumption, we create a
graph where the vertices are all the occurrences of
entity pairs, both labeled and unlabeled. The edge
between vertices represents their similarity. Then
the task of relation extraction can be formulated as
a form of propagation on a graph, where a vertex?s
label propagates to neighboring vertices according
to their proximity. Here, the graph is connected with
the weights: Wij = exp(? s
2
ij
?2 ), where sij is the sim-
ilarity between xi and xj calculated by some simi-
larity measures. In this paper,two similarity mea-
sures are investigated, i.e. Cosine similarity measure
and Jensen-Shannon (JS) divergence (Lin, 1991).
And we set ? as the average similarity between la-
beled examples from different classes.
2.2 Label Propagation Algorithm
Given such a graph with labeled and unlabeled ver-
tices, we investigate the label propagation algorithm
(Zhu and Ghahramani, 2002) to help us propagate
the label information of any vertex in the graph
to nearby vertices through weighted edges until a
global stable stage is achieved.
Define a n ? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
, where Tij is the prob-
ability to jump from vertex xj to vertex xi. Also de-
fine a n?R label matrix Y , where Yij representing
the probabilities of vertex yi to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1: Initialization Firstly, set the iteration in-
dex t = 0. Then let Y 0 be the initial soft labels at-
tached to each vertex and Y 0L be the top l rows of Y 0,
which is consistent with the labeling in labeled data
(Y 0ij = 1 if yi is label rj and 0 otherwise ). Let Y 0U
be the remaining u rows corresponding to unlabeled
data points and its initialization can be arbitrary.
Step 2: Propagate the label by Y t+1 = TY t,
where T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the class
probability interpretation.
Step 3: Clamp the labeled data, i.e., replace the
top l row of Y t+1 with Y 0L . In this step, the labeled
data is clamped to replenish the label sources from
these labeled data. Thus the labeled data act like
sources to push out labels through unlabeled data.
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
Step 4: Repeat from step 2 until Y converges.
Step 5: Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
3 Experiments and Results
3.1 Data
Our proposed graph-based method is evaluated on
the ACE corpus 1, which contains 519 files from
sources including broadcast, newswire, and news-
paper. A break-down of the tagged data by different
relation subtypes is given in Table 1.
3.2 Features
We extract the following lexical and syntactic fea-
tures from two entity mentions, and the contexts be-
fore, between and after the entity pairs. Especially,
we set the mid-context window as everything be-
tween the two entities and the pre- and post- context
as up to two words before and after the correspond-
ing entity. Most of these features are computed from
the parse trees derived from Charniak Parser (Char-
niak, 1999) and the Chunklink script 2 written by
Sabine Buchholz from Tilburg University.
1 http://www.ldc.upenn.edu/Projects/ACE/
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
26
Table 2: Performance of Relation Detection: SVM and LP algorithm with different size of labeled data. The LP algorithm is
performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: Performance of Relation Classification on Relation Subtype: SVM and LP algorithm with different size of labeled data.
The LP algorithm is performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
Words: Surface tokens of the two entities and
three context windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZATION,
FACILITY, LOCATION and GPE.
POS: Part-Of-Speech tags corresponding to all
tokens in the two entities and three context windows.
Chunking features: Chunk tag information and
Grammatical function of the two entities and three
context windows. IOB-chains of the heads of the
two entities are also considered. IOB-chain notes
the syntactic categories of all the constituents on the
path from the root node to this leaf node of tree.
We combine the above features with their position
information in the context to form the context vec-
tor. Before that, we filter out low frequency features
which appeared only once in the entire set.
3.3 Experimental Evaluation
3.3.1 Relation Detection
We collect all entity mention pairs which co-occur
in the same sentence from the training and devtest
corpus into two set C1 and C2 respectively. The set
C1 includes annotated training data AC1 and un-
related data UC1. We randomly sample l examples
from AC1 as labeled data and add a ?NONE? class
into labeled data for the case where the two entity
mentions are not related. The data of the ?NONE?
Table 4: Comparison of performance on individual relation
type of Zhang (2004)?s method and our method. For Zhang
(2004)?s method, feature sampling probability is set to 0.3 and
agreement threshold is set to 9 out of 10.
Bootstrapping LPJS
Rel-Type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR undef 0 undef 13.7 12.5 13.0
class is resulted by sampling l examples from UC1.
Moreover, we combine the rest examples of C1 and
the whole set C2 as unlabeled data.
Given labeled and unlabeled data,we can perform
LP algorithm to detect possible relations, which
are those entity pairs that are not classified to the
?NONE? class but to the other 24 subtype classes.
In addition,we conduct experiments with different
sampling set size l, including 1% ? Ntrain,10% ?
Ntrain,25%?Ntrain,50%?Ntrain,75%?Ntrain,
100% ? Ntrain (Ntrain = |AC1|). If any major
subtype was absent from the sampled labeled set,we
redo the sampling. For each size,we perform 20 tri-
als and calculate an average of 20 random trials.
3.3.2 SVM vs. LP
Table 2 reports the performance of relation detec-
tion by using SVM and LP with different sizes of
27
labled data. For SVM, we use LIBSVM tool with
linear kernel function 3. And the same sampled la-
beled data used in LP is used to train SVM mod-
els. From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Especially,
with small labeled dataset (percentage of labeled
data ? 25%), this merit is more distinct. When
the percentage of labeled data increases from 50%
to 100%, LPCosine is still comparable to SVM in F-
measure while LPJS achieves better F-measure than
SVM. On the other hand, LPJS consistently outper-
forms LPCosine.
Table 3 reports the performance of relation classi-
fication, where the performance describes the aver-
age values over major relation subtypes. From Table
3, we see that LPCosine and LPJS outperform SVM
by F-measure in almost all settings of labeled data,
which is due to the increase of Recall. With smaller
labeled dataset, the gap between LP and SVM is
larger. On the other hand, LPJS divergence consis-
tently outperforms LPCosine.
3.3.3 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance on individual rela-
tion type. We can find that LP algorithm achieves
6.8% performance improvement compared with the
(Zhang, 2004)?s bootstrapped SVM algorithm aver-
age on all five relation types. Notice that perfor-
mance reported on relation type ?NEAR? is low, be-
cause it occurs rarely in both training and test data.
4 Conclusion and Future work
This paper approaches the task of semi-supervised
relation extraction on Label Propagation algorithm.
Our results demonstrate that, when only very few
labeled examples are available, this manifold learn-
ing based algorithm can achieve better performance
than supervised learning method (SVM) and boot-
strapping based method, which can contribute to
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
minimize corpus annotation requirement. In the fu-
ture we would like to investigate how to select more
useful feature stream and whether feature selection
method can improve the performance of our graph-
based semi-supervised relation extraction.
References
Agichtein E. and Gravano L. 2000. Snowball: Extracting Rela-
tions from large Plain-Text Collections, In Proceeding of the
5th ACM International Conference on Digital Libraries.
Brin Sergey. 1998. Extracting patterns and relations from
world wide web. In Proceeding of WebDB Workshop at 6th
International Conference on Extending Database Technol-
ogy. pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser. Tech-
nical Report CS-99-12. Computer Science Department,
Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels for
relation extraction, In Proceedings of 42th ACL conference.
Hasegawa T., Sekine S. and Grishman R. 2004. Discover-
ing Relations among Named Entities from Large Corpora,
In Proceeding of Conference ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and semantic
features with Maximum Entropy Models for extracting rela-
tions, In Proceedings of 42th ACL conference. Spain.
Lin,J. 1991. Divergence Measures Based on the Shannon En-
tropy. IEEE Transactions on Information Theory. 37:1,145-
150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000. A novel
use of statistical parsing to extract information from text.
In Proceedings of 6th Applied Natural Language Processing
Conference 29 April-4 may 2000, Seattle USA.
Yarowsky D. 1995. Unsupervised Word Sense Disambiguation
Rivaling Supervised Methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational Linguis-
tics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Kernel Meth-
ods for Relation Extraction, In Proceedings of the EMNLP
Conference. Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classification for
Information Extraction, In proceedings of ACM 13th con-
ference on Information and Knowledge Management. 8-13
Nov 2004. Washington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min. 2005.
Combining lexical, syntactic and semantic features with
Maximum Entropy Models for extracting relations, In pro-
ceedings of 43th ACL conference. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning from
Labeled and Unlabeled Data with Label Propagation. CMU
CALD tech report CMU-CALD-02-107.
28
Coreference Resolution Using Competition Learning Approach 
Xiaofeng Yang*+ Guodong Zhou*  Jian Su* Chew Lim Tan + 
*Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, 
Singapore 119613 
+Department of Computer Science, 
National University of Singapore,  
Singapore 117543  
*{xiaofengy,zhougd,sujian}@ 
i2r.a-star.edu.sg 
+(yangxiao,tancl)@comp.nus.edu.sg
  
Abstract 
In this paper we propose a competition 
learning approach to coreference resolu-
tion. Traditionally, supervised machine 
learning approaches adopt the single-
candidate model. Nevertheless the prefer-
ence relationship between the antecedent 
candidates cannot be determined accu-
rately in this model. By contrast, our ap-
proach adopts a twin-candidate learning 
model. Such a model can present the 
competition criterion for antecedent can-
didates reliably, and ensure that the most 
preferred candidate is selected. Further-
more, our approach applies a candidate 
filter to reduce the computational cost and 
data noises during training and resolution. 
The experimental results on MUC-6 and 
MUC-7 data set show that our approach 
can outperform those based on the single-
candidate model.  
1 Introduction 
Coreference resolution is the process of linking 
together multiple expressions of a given entity. The 
key to solve this problem is to determine the ante-
cedent for each referring expression in a document.  
In coreference resolution, it is common that two 
or more candidates compete to be the antecedent of 
an anaphor (Mitkov, 1999). Whether a candidate is 
coreferential to an anaphor is often determined by 
the competition among all the candidates. So far, 
various algorithms have been proposed to deter-
mine the preference relationship between two can-
didates. Mitkov?s knowledge-poor pronoun 
resolution method (Mitkov, 1998), for example, 
uses the scores from a set of antecedent indicators 
to rank the candidates. And centering algorithms 
(Brennan et al, 1987; Strube, 1998; Tetreault, 
2001), sort the antecedent candidates based on the 
ranking of the forward-looking or backward-
looking centers. 
In recent years, supervised machine learning 
approaches have been widely used in coreference 
resolution (Aone and Bennett, 1995; McCarthy, 
1996; Soon et al, 2001; Ng and Cardie, 2002a), 
and have achieved significant success. Normally, 
these approaches adopt a single-candidate model in 
which the classifier judges whether an antecedent 
candidate is coreferential to an anaphor with a con-
fidence value. The confidence values are generally 
used as the competition criterion for the antecedent 
candidates. For example, the ?Best-First? selection 
algorithms (Aone and Bennett, 1995; Ng and 
Cardie, 2002a) link the anaphor to the candidate 
with the maximal confidence value (above 0.5). 
One problem of the single-candidate model, 
however, is that it only takes into account the rela-
tionships between an anaphor and one individual 
candidate at a time, and overlooks the preference 
relationship between candidates. Consequently, the 
confidence values cannot accurately represent the 
true competition criterion for the candidates. 
In this paper, we present a competition learning 
approach to coreference resolution. Motivated by 
the research work by Connolly et al (1997), our 
approach adopts a twin-candidate model to directly 
learn the competition criterion for the antecedent 
candidates. In such a model, a classifier is trained 
based on the instances formed by an anaphor and a 
pair of its antecedent candidates. The classifier is 
then used to determine the preference between any 
two candidates of an anaphor encountered in a new 
document. The candidate that wins the most com-
parisons is selected as the antecedent. In order to 
reduce the computational cost and data noises, our 
approach also employs a candidate filter to elimi-
nate the invalid or irrelevant candidates.  
The layout of this paper is as follows. Section 2 
briefly describes the single-candidate model and 
analyzes its limitation. Section 3 proposes in de-
tails the twin-candidate model and Section 4 pre-
sents our coreference resolution approach based on 
this model. Section 5 reports and discusses the ex-
perimental results. Section 6 describes related re-
search work. Finally, conclusion is given in 
Section 7. 
2 The Single-Candidate Model 
The main idea of the single-candidate model for 
coreference resolution is to recast the resolution as 
a binary classification problem. 
During training, a set of training instances is 
generated for each anaphor in an annotated text. 
An instance is formed by the anaphor and one of 
its antecedent candidates. It is labeled as positive 
or negative based on whether or not the candidate 
is tagged in the same coreferential chain of the 
anaphor. 
After training, a classifier is ready to resolve the 
NPs1 encountered in a new document. For each NP 
under consideration, every one of its antecedent 
candidates is paired with it to form a test instance. 
The classifier returns a number between 0 and 1 
that indicates the likelihood that the candidate is 
coreferential to the NP. 
The returned confidence value is commonly 
used as the competition criterion to rank the candi-
date. Normally, the candidates with confidences 
less than a selection threshold (e.g. 0.5) are dis-
carded. Then some algorithms are applied to 
choose one of the remaining candidates, if any, as 
the antecedent. For example, ?Closest-First? (Soon 
et al, 2001) selects the candidate closest to the 
anaphor, while ?Best-First? (Aone and Bennett, 
1995; Ng and Cardie, 2002a) selects the candidate 
with the maximal confidence value.  
One limitation of this model, however, is that it 
only considers the relationships between a NP en-
countered and one of its candidates at a time dur-
ing its training and testing procedures. The 
confidence value reflects the probability that the 
candidate is coreferential to the NP in the overall 
                                                          
1 In this paper a NP corresponds to a Markable in MUC 
coreference resolution tasks. 
distribution 2 , but not the conditional probability 
when the candidate is concurrent with other com-
petitors. Consequently, the confidence values are 
unreliable to represent the true competition crite-
rion for the candidates.  
To illustrate this problem, just suppose a data 
set where an instance could be described with four 
exclusive features: F1, F2, F3 and F4. The ranking 
of candidates obeys the following rule: 
CSF1 >> CSF2 >> CSF3 >> CSF4 
Here CSFi ( 41 ?? i ) is the set of antecedent can-
didates with the feature Fi on. The mark of ?>>? 
denotes the preference relationship, that is, the 
candidates in CSF1 is preferred to those in CSF2, and 
to those in CSF3 and CSF4.  
Let CF2 and CF3 denote the class value of a leaf 
node ?F2 = 1? and ?F3 = 1?, respectively. It is pos-
sible that CF2 < CF3, if the anaphors whose candi-
dates all belong to CSF3 or CSF4 take the majority in 
the training data set.  In this case, a candidate in 
CSF3 would be assigned a larger confidence value 
than a candidate in CSF2. This nevertheless contra-
dicts the ranking rules. If during resolution, the 
candidates of an anaphor all come from CSF2 or 
CSF3, the anaphor may be wrongly linked to a can-
didate in CSF3 rather than in CSF2. 
3 The Twin-Candidate Model 
Different from the single-candidate model, the 
twin-candidate model aims to learn the competition 
criterion for candidates. In this section, we will 
introduce the structure of the model in details. 
3.1 Training Instances Creation 
Consider an anaphor ana and its candidate set can-
didate_set, {C1, C2, ?, Ck}, where Cj is closer to 
ana than Ci if j > i. Suppose positive_set is the set 
of candidates that occur in the coreferential chain 
of ana, and negative_set is the set of candidates not 
in the chain, that is, negative_set = candidate_set  
- positive_set. The set of training instances based 
on ana, inst_set, is defined as follows:  
                                                          
2 Suppose we use C4.5 algorithm and the class value takes the 
smoothed ration, 
2
1
+
+
t
p , where p is the number of positive 
instances and t is the total number of instances contained in 
the corresponding leaf node. 
} _  C  , _Cj,i |{
  } _  C  ,_ C j,i |{
_
ji),,(
ji),,(
setpositvesetnegativeinst
setnegativesetpositveinst
setinst
anaCjCi
anaCjCi
??>
??>
=
U
 
From the above definition, an instance is 
formed by an anaphor, one positive candidate and 
one negative candidate. For each instance, 
)ana,cj,ci(inst , the candidate at the first position, Ci, 
is closer to the anaphor than the candidate at the 
second position, Cj.  
A training instance )ana,cj,ci(inst is labeled as 
positive if Ci ?  positive-set and Cj ?  negative-set; 
or negative if Ci ?  negative-set and Cj ?  positive-
set.  
See the following example:  
 
Any design to link China's accession to the WTO 
with the missile tests1 was doomed to failure.  
 ?If some countries2 try to block China TO acces-
sion, that will not be popular and will fail to win the 
support of other countries3? she said.  
Although no governments4 have suggested formal 
sanctions5 on China over the missile tests6, the United 
States has called them7 ?provocative and reckless? and 
other countries said they could threaten Asian stability.  
 
In the above text segment, the antecedent can-
didate set of the pronoun ?them7? consists of six 
candidates highlighted in Italics. Among the can-
didates, Candidate 1 and 6 are in the coreferential 
chain of ?them7?, while Candidate 2, 3, 4, 5 are not. 
Thus, eight instances are formed for ?them7?:  
 
(2,1,7)  (3,1,7)  (4,1,7)  (5,1,7) 
(6,5,7)  (6,4,7)  (6,3,7)  (6,2,7) 
 
Here the instances in the first line are negative, 
while those in the second line are all positive.  
3.2 Features Definition 
A feature vector is specified for each training or 
testing instance. Similar to those in the single-
candidate model, the features may describe the 
lexical, syntactic, semantic and positional relation-
ships of an anaphor and any one of its candidates. 
Besides, the feature set may also contain inter-
candidate features characterizing the relationships 
between the pair of candidates, e.g. the distance 
between the candidates in the number distances or 
paragraphs. 
3.3 Classifier Generation 
Based on the feature vectors generated for each 
anaphor encountered in the training data set, a 
classifier can be trained using a certain machine 
learning algorithm, such as C4.5, RIPPER, etc. 
Given the feature vector of a test instance 
)ana,cj,ci(inst  (i > j), the classifier returns the posi-
tive class indicating that Ci is preferred to Cj as the 
antecedent of ana; or negative indicating that Cj is 
preferred.  
3.4 Antecedent Identification 
Let CR( )ana,cj,ci(inst ) denote the classification re-
sult for an instance )ana,cj,ci(inst . The antecedent of 
an anaphor is identified using the algorithm shown 
in Figure 1.  
 
Algorithm ANTE-SEL 
Input: ana: the anaphor under consideration  
candidate_set: the set of antecedent can-
didates of ana, {C1, C2,?,Ck} 
 
for i = 1 to K do 
   Score[ i ] = 0; 
for  i = K downto 2 do 
for j = i ? 1 downto 1 do 
  if  CR( )ana,cj,ci(inst ) = = positive then  
Score[ i ]++; 
else  
Score[ j ] ++; 
  endif 
SelectedIdx= ][maxarg
_
iScore
setcandidateCii ?
 
return CselectedIdx; 
Figure 1:The antecedent identification algorithm
 
Algorithm ANTE-SEL takes as input an ana-
phor and its candidate set candidate_set, and re-
turns one candidate as its antecedent. In the 
algorithm, each candidate is compared against any 
other candidate. The classifier acts as a judge dur-
ing each comparison. The score of each candidate 
increases by one every time when it wins. In this 
way, the final score of a candidate records the total 
times it wins. The candidate with the maximal 
score is singled out as the antecedent.  
If two or more candidates have the same maxi-
mal score, the one closest to the anaphor would be 
selected. 
3.5 Single-Candidate Model: A Special Case 
of Twin-Candidate Model? 
While the realization and the structure of the twin-
candidate model are significantly different from 
the single-candidate model, the single-candidate 
model in fact can be regarded as a special case of 
the twin-candidate model.  
To illustrate this, just consider a virtual ?blank? 
candidate C0 such that we could convert an in-
stance )ana,ci(inst in the single-candidate model to 
an instance )ana,c,ci( 0inst in the twin-candidate 
model. Let )ana,c,ci( 0inst have the same class label 
as )ana,ci(inst , that is, )ana,c,ci( 0inst is positive if Ci is 
the antecedent of ana; or negative if not.  
Apparently, the classifier trained on the in-
stance set { )ana,ci(inst }, T1, is equivalent to that 
trained on { )ana,c,ci( 0inst }, T2.  T1 and T2 would 
assign the same class label for the test instances 
)ana,ci(inst  and )ana,c,ci( 0inst , respectively. That is to 
say, determining whether Ci is coreferential to ana 
by T1 in the single-candidate model equals to 
determining whether Ci is better than C0 w.r.t ana 
by T2 in the twin-candidate model. Here we could 
take C0 as a ?standard candidate?. 
While the classification in the single-candidate 
model can find its interpretation in the twin-
candidate model, it is not true vice versa. Conse-
quently, we can safely draw the conclusion that the 
twin-candidate model is more powerful than the 
single-candidate model in characterizing the rela-
tionships among an anaphor and its candidates. 
4 The Competition Learning Approach 
Our competition learning approach adopts the 
twin-candidate model introduced in the Section 3. 
The main process of the approach is as follows: 
1. The raw input documents are preprocessed to 
obtain most, if not all, of the possible NPs.  
2. During training, for each anaphoric NP, we 
create a set of candidates, and then generate 
the training instances as described in Section 3.  
3. Based on the training instances, we make use 
of the C5.0 learning algorithm (Quinlan, 1993) 
to train a classifier. 
4. During resolution, for each NP encountered, 
we also construct a candidate set. If the set is 
empty, we left this NP unresolved; otherwise 
we apply the antecedent identification algo-
rithm to choose the antecedent and then link 
the NP to it.  
4.1 Preprocessing 
To determine the boundary of the noun phrases, a 
pipeline of Nature Language Processing compo-
nents are applied to an input raw text: 
z Tokenization and sentence segmentation 
z Named entity recognition 
z Part-of-speech tagging 
z Noun phrase chunking 
Among them, named entity recognition, part-of-
speech tagging and text chunking apply the same 
Hidden Markov Model (HMM) based engine with 
error-driven learning capability (Zhou and Su, 
2000 & 2002). The named entity recognition 
component recognizes various types of MUC-style 
named entities, i.e., organization, location, person, 
date, time, money and percentage.  
4.2 Features Selection 
For our study, in this paper we only select those 
features that can be obtained with low annotation 
cost and high reliability. All features are listed in 
Table 1 together with their respective possible val-
ues.  
4.3 Candidates Filtering 
For a NP under consideration, all of its preceding 
NPs could be the antecedent candidates. Neverthe-
less, since in the twin-candidate model the number 
of instances for a given anaphor is about the square 
of the number of its antecedent candidates, the 
computational cost would be prohibitively large if 
we include all the NPs in the candidate set. More-
over, many of the preceding NPs are irrelevant or 
even invalid with regard to the anaphor. These data 
noises may hamper the training of a good-
performanced classifier, and also damage the accu-
racy of the antecedent selection: too many com-
parisons are made between incorrect candidates. 
Therefore, in order to reduce the computational 
cost and data noises, an effective candidate filter-
ing strategy must be applied in our approach. 
During training, we create the candidate set for 
each anaphor with the following filtering algorithm: 
1. If the anaphor is a pronoun,  
(a) Add to the initial candidate set al the pre-
ceding NPs in the current and the previous 
two sentences. 
(b) Remove from the candidate set those that 
disagree in number, gender, and person. 
(c) If the candidate set is empty, add the NPs in 
an earlier sentence and go to 1(b). 
2. If the anaphor is a non-pronoun, 
(a) Add all the non-pronominal antecedents to 
the initial candidate set. 
(b) For each candidate added in 2(a), add the 
non-pronouns in the current, the previous 
and the next sentences into the candidate set. 
During resolution, we filter the candidates for 
each encountered pronoun in the same way as dur-
ing training. That is, we only consider the NPs in 
the current and the preceding 2 sentences. Such a 
context window is reasonable as the distance be-
tween a pronominal anaphor and its antecedent is 
generally short. In the MUC-6 data set, for exam-
ple, the immediate antecedents of 95% pronominal 
anaphors can be found within the above distance. 
Comparatively, candidate filtering for non-
pronouns during resolution is complicated. A po-
tential problem is that for each non-pronoun under 
consideration, the twin-candidate model always 
chooses a candidate as the antecedent, even though 
all of the candidates are ?low-qualified?, that is, 
unlikely to be coreferential to the non-pronoun un-
der consideration.  
In fact, the twin-candidate model in itself can 
identify the qualification of a candidate. We can 
compare every candidate with a virtual ?standard 
candidate?, C0. Only those better than C0 are 
deemed qualified and allowed to enter the ?round 
robin?, whereas the losers are eliminated. As we 
have discussed in Section 3.5, the classifier on the 
pairs of a candidate and C0 is just a single-
candidate classifier. Thus, we can safely adopt the 
single-candidate classifier as our candidate filter.  
The candidate filtering algorithm during resolu-
tion is as follows:  
Features describing the candidate: 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10 
ante_DefNp_1(2) 
ante_IndefNP_1(2) 
ante_Pron_1(2) 
ante_ProperNP_1(2) 
ante_M_ProperNP_1(2) 
ante_ProperNP_APPOS_1(2) 
ante_Appositive_1(2) 
ante_NearestNP_1(2) 
ante_Embeded_1(2) 
ante_Title_1(2) 
1 if Ci (Cj) is a definite NP; else 0 
1 if Ci (Cj) is an indefinite NP; else 0 
1 if Ci (Cj) is a pronoun; else 0 
1 if Ci (Cj) is a proper NP; else 0  
1 if Ci (Cj) is a mentioned proper NP; else 0 
1 if Ci (Cj) is a proper NP modified by an appositive; else 0 
1 if Ci (Cj) is in a apposition structure; else 0 
1 if Ci (Cj) is the nearest candidate to the anaphor; else 0 
1 if Ci (Cj) is in an embedded NP; else 0 
1 if Ci (Cj) is in a title; else 0 
Features describing the anaphor: 
11. 
12. 
13. 
14. 
15. 
 
16. 
ana_DefNP 
ana_IndefNP 
ana_Pron 
ana_ProperNP 
ana_PronType 
 
ana_FlexiblePron 
1 if ana is a definite NP; else 0 
1 if ana is an indefinite NP; else 0 
1 if ana is a pronoun; else 0 
1 if ana is a proper NP; else 0 
1 if ana is a third person pronoun; 2 if a single neuter pro-
noun; 3 if a plural neuter pronoun; 4 if other types 
1 if ana is a flexible pronoun; else 0 
Features describing the candidate and the anaphor: 
17. 
18. 
 
18. 
 
20. 
21. 
ante_ana_StringMatch_1(2) 
ante_ana_GenderAgree_1(2) 
 
ante_ana_NumAgree_1(2) 
 
ante_ana_Appositive_1(2) 
ante_ana_Alias_1(2) 
1 if Ci (Cj) and ana match in string; else 0 
1 if Ci (Cj) and ana agree in gender; else 0 if disagree; -1 if 
unknown 
1 if Ci (Cj) and ana agree in number; 0 if disagree; -1 if un-
known 
1 if Ci (Cj) and ana are in an appositive structure; else 0 
1 if Ci (Cj) and ana are in an alias of the other; else 0 
Features describing the two candidates 
22. 
23. 
inter_SDistance 
inter_Pdistance 
Distance between Ci and Cj in sentences 
Distance between Ci and Cj in paragraphs 
Table 1:  Feature set for coreference resolution (Feature 22, 23 and features involving Cj are not 
used in the single-candidate model) 
1. If the current NP is a pronoun, construct the 
candidate set in the same way as during training.  
2. If the current NP is a non-pronoun,  
(a) Add all the preceding non-pronouns to the ini-
tial candidate set. 
(b) Calculate the confidence value for each candi-
date using the single-candidate classifier. 
(c) Remove the candidates with confidence value 
less than 0.5. 
5 Evaluation and Discussion 
Our coreference resolution approach is evaluated 
on the standard MUC-6 (1995) and MUC-7 (1998) 
data set. For MUC-6, 30 ?dry-run? documents an-
notated with coreference information could be used 
as training data. There are also 30 annotated train-
ing documents from MUC-7. For testing, we util-
ize the 30 standard test documents from MUC-6 
and the 20 standard test documents from MUC-7. 
5.1 Baseline Systems 
In the experiment we compared our approach with 
the following research works:  
1. Strube?s S-list algorithm for pronoun resolu-
tion (Stube, 1998).  
2. Ng and Cardie?s machine learning approach to 
coreference resolution (Ng and Cardie, 2002a).  
3. Connolly et al?s machine learning approach to 
anaphora resolution (Connolly et al, 1997).  
Among them, S-List, a version of centering 
algorithm, uses well-defined heuristic rules to rank 
the antecedent candidates; Ng and Cardie?s ap-
proach employs the standard single-candidate 
model and ?Best-First? rule to select the antece-
dent; Connolly et al?s approach also adopts the 
twin-candidate model, but their approach lacks of 
candidate filtering strategy and uses greedy linear 
search to select the antecedent (See ?Related 
work? for details). 
We constructed three baseline systems based on 
the above three approaches, respectively. For com-
parison, in the baseline system 2 and 3, we used 
the similar feature set as in our system (see table 1).  
5.2 Results and Discussion 
Table 2 and 3 show the performance of different 
approaches in the pronoun and non-pronoun reso-
lution, respectively. In these tables we focus on the 
abilities of different approaches in resolving an 
anaphor to its antecedent correctly. The recall 
measures the number of correctly resolved ana-
phors over the total anaphors in the MUC test data 
set, and the precision measures the number of cor-
rect anaphors over the total resolved anaphors. The 
F-measure F=2*RP/(R+P) is the harmonic mean of 
precision and recall. 
The experimental result demonstrates that our 
competition learning approach achieves a better 
performance than the baseline approaches in re-
solving pronominal anaphors. As shown in Table 2, 
our approach outperforms Ng and Cardie?s single-
candidate based approach by 3.7 and 5.4 in F-
measure for MUC-6 and MUC-7, respectively. 
Besides, compared with Strube?s S-list algorithm, 
our approach also achieves gains in the F-measure 
by 3.2 (MUC-6), and 1.6 (MUC-7). In particular, 
our approach obtains significant improvement 
(21.1 for MUC-6, and 13.1 for MUC-7) over Con-
nolly et al?s twin-candidate based approach. 
 
MUC-6 MUC-7  
 R P F R P F 
Strube (1998)  76.1 74.3 75.1 62.9 60.3 61.6 
Ng and Cardie (2002a) 75.4 73.8 74.6 58.9 56.8 57.8 
Connolly et al (1997) 57.2 57.2 57.2 50.1 50.1 50.1 
Our approach 79.3 77.5 78.3 64.4 62.1 63.2 
Table 2:  Results for the pronoun resolution  
 
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 51.0 89.9 65.0 39.1 86.4 53.8 
Connolly et al (1997) 52.2 52.2 52.2 43.7 43.7 43.7 
Our approach  51.3 90.4 65.4 39.7 87.6 54.6 
Table 3:  Results for the non-pronoun resolution  
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 62.2 78.8 69.4 48.4 74.6 58.7 
Our approach 64.0 80.5 71.3 50.1 75.4 60.2 
Table 4: Results for the coreference resolution  
 
Compared with the gains in pronoun resolution, 
the improvement in non-pronoun resolution is 
slight. As shown in Table 3, our approach resolves 
non-pronominal anaphors with the recall of 51.3 
(39.7) and the precision of 90.4 (87.6) for MUC-6 
(MUC-7). In contrast to Ng and Cardie?s approach, 
the performance of our approach improves only 0.3 
(0.6) in recall and 0.5 (1.2) in precision. The rea-
son may be that in non-pronoun resolution, the 
coreference of an anaphor and its candidate is usu-
ally determined only by some strongly indicative 
features such as alias, apposition, string-matching, 
etc (this explains why we obtain a high precision 
but a low recall in non-pronoun resolution). There-
fore, most of the positive candidates are coreferen-
tial to the anaphors even though they are not the 
?best?. As a result, we can only see comparatively 
slight difference between the performances of the 
two approaches.  
Although Connolly et al?s approach also adopts 
the twin-candidate model, it achieves a poor per-
formance for both pronoun resolution and non-
pronoun resolution. The main reason is the absence 
of candidate filtering strategy in their approach 
(this is why the recall equals to the precision in the 
tables). Without candidate filtering, the recall may 
rise as the correct antecedents would not be elimi-
nated wrongly. Nevertheless, the precision drops 
largely due to the numerous invalid NPs in the 
candidate set. As a result, a significantly low F-
measure is obtained in their approach. 
Table 4 summarizes the overall performance of 
different approaches to coreference resolution. Dif-
ferent from Table 2 and 3, here we focus on 
whether a coreferential chain could be correctly 
identified. For this purpose, we obtain the recall, 
the precision and the F-measure using the standard 
MUC scoring program (Vilain et al 1995) for the 
coreference resolution task. Here the recall means 
the correct resolved chains over the whole 
coreferential chains in the data set, and precision 
means the correct resolved chains over the whole 
resolved chains.  
In line with the previous experiments, we see 
reasonable improvement in the performance of the 
coreference resolution: compared with the baseline 
approach based on the single-candidate model, the 
F-measure of approach increases from 69.4 to 71.3 
for MUC-6, and from 58.7 to 60.2 for MUC-7.  
6 Related Work 
A similar twin-candidate model was adopted in the 
anaphoric resolution system by Connolly et al 
(1997). The differences between our approach and 
theirs are: 
(1) In Connolly et al?s approach, all the preceding 
NPs of an anaphor are taken as the antecedent 
candidates, whereas in our approach we use 
candidate filters to eliminate invalid or irrele-
vant candidates.  
(2) The antecedent identification in Connolly et 
al.?s approach is to apply the classifier to 
successive pairs of candidates, each time 
retaining the better candidate. However, due to 
the lack of strong assumption of transitivity, 
the selection procedure is in fact a greedy 
search. By contrast, our approach evaluates a 
candidate according to the times it wins over 
the other competitors. Comparatively this 
algorithm could lead to a better solution. 
(3) Our approach makes use of more indicative 
features, such as Appositive, Name Alias, 
String-matching, etc. These features are effec-
tive especially for non-pronoun resolution. 
7 Conclusion 
In this paper we have proposed a competition 
learning approach to coreference resolution. We 
started with the introduction of the single-
candidate model adopted by most supervised ma-
chine learning approaches. We argued that the con-
fidence values returned by the single-candidate 
classifier are not reliable to be used as ranking cri-
terion for antecedent candidates. Alternatively, we 
presented a twin-candidate model that learns the 
competition criterion for antecedent candidates 
directly. We introduced how to adopt the twin-
candidate model in our competition learning ap-
proach to resolve the coreference problem. Particu-
larly, we proposed a candidate filtering algorithm 
that can effectively reduce the computational cost 
and data noises.  
The experimental results have proved the effec-
tiveness of our approach. Compared with the base-
line approach using the single-candidate model, the 
F-measure increases by 1.9 and 1.5 for MUC-6 and 
MUC-7 data set, respectively. The gains in the 
pronoun resolution contribute most to the overall 
improvement of coreference resolution. 
Currently, we employ the single-candidate clas-
sifier to filter the candidate set during resolution. 
While the filter guarantees the qualification of the 
candidates, it removes too many positive candi-
dates, and thus the recall suffers. In our future 
work, we intend to adopt a looser filter together 
with an anaphoricity determination module (Bean 
and Riloff, 1999; Ng and Cardie, 2002b). Only if 
an encountered NP is determined as an anaphor, 
we will select an antecedent from the candidate set 
generated by the looser filter. Furthermore, we 
would like to incorporate more syntactic features 
into our feature set, such as grammatical role or 
syntactic parallelism. These features may be help-
ful to improve the performance of pronoun resolu-
tion.  
References 
Chinatsu Aone and Scott W.Bennett. 1995. Evaluating 
automated and manual acquisition of anaphora reso-
lution strategies. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational Lin-
guistics, Pages 122-129. 
D.Bean and E.Riloff. 1999. Corpus-Based identification 
of non-anaphoric noun phrases. In Proceedings of the 
37th Annual Meeting of the Association for Computa-
tional Linguistics, Pages 373-380. 
Brennan, S, E., M. W. Friedman and C. J. Pollard. 1987. 
A Centering approach to pronouns. In Proceedings of 
the 25th Annual Meeting of The Association for Com-
putational Linguistics, Page 155-162. 
Dennis Connolly, John D. Burger and David S. Day. 
1997. A machine learning approach to anaphoric ref-
erence. New Methods in Language Processing, Page 
133-144.  
Joseph F. McCarthy. 1996. A trainable approach to 
coreference resolution for Information Extraction. 
Ph.D. thesis. University of Massachusetts. 
Ruslan Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proceedings of the 17th Int. 
Conference on Computational Linguistics (COLING-
ACL'98), Page 869-875. 
Ruslan Mitkov. 1999. Anaphora resolution: The state of 
the art. Technical report. University of Wolverhamp-
ton, Wolverhampton. 
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA. 
Vincent Ng and Claire Cardie. 2002a. Improving ma-
chine learning approaches to coreference resolution. 
In Proceedings of the 40rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages 104-
111. 
Vincent Ng and Claire Cardie. 2002b. Identifying ana-
phoric and non-anaphoric noun phrases to improve 
coreference resolution. In Proceedings of 19th Inter-
national Conference on Computational Linguistics 
(COLING-2002). 
J R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA. 
Wee Meng Soon, Hwee Tou Ng and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4), Page 521-544. 
Michael Strube. Never look back: An alternative to 
Centering. 1998. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics and 36th An-
nual Meeting of ACL, Page 1251-1257 
Joel R. Tetreault. 2001. A Corpus-Based evaluation of 
Centering and pronoun resolution. Computational 
Linguistics, 27(4), Page 507-520. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and 
L.Hirschman. 1995. A model-theoretic coreference 
scoring scheme. In Proceedings of the Sixth Message 
understanding Conference (MUC-6), Pages 42-52. 
GD Zhou and J. Su, 2000. Error-driven HMM-based 
chunk tagger with context-dependent lexicon. In 
Proceedings of the Joint Conference on Empirical 
Methods on Natural Language Processing and Very 
Large Corpus (EMNLP/ VLC'2000).  
GD Zhou and J. Su. 2002. Named Entity recognition 
using a HMM-based chunk tagger. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, P473-478. 
Improving Pronoun Resolution by Incorporating Coreferential
Information of Candidates
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Coreferential information of a candidate, such
as the properties of its antecedents, is important
for pronoun resolution because it reflects the
salience of the candidate in the local discourse.
Such information, however, is usually ignored in
previous learning-based systems. In this paper
we present a trainable model which incorporates
coreferential information of candidates into pro-
noun resolution. Preliminary experiments show
that our model will boost the resolution perfor-
mance given the right antecedents of the can-
didates. We further discuss how to apply our
model in real resolution where the antecedents
of the candidate are found by a separate noun
phrase resolution module. The experimental re-
sults show that our model still achieves better
performance than the baseline.
1 Introduction
In recent years, supervised machine learning ap-
proaches have been widely explored in refer-
ence resolution and achieved considerable suc-
cess (Ge et al, 1998; Soon et al, 2001; Ng and
Cardie, 2002; Strube and Muller, 2003; Yang et
al., 2003). Most learning-based pronoun res-
olution systems determine the reference rela-
tionship between an anaphor and its antecedent
candidate only from the properties of the pair.
The knowledge about the context of anaphor
and antecedent is nevertheless ignored. How-
ever, research in centering theory (Sidner, 1981;
Grosz et al, 1983; Grosz et al, 1995; Tetreault,
2001) has revealed that the local focusing (or
centering) also has a great effect on the pro-
cessing of pronominal expressions. The choices
of the antecedents of pronouns usually depend
on the center of attention throughout the local
discourse segment (Mitkov, 1999).
To determine the salience of a candidate
in the local context, we may need to check
the coreferential information of the candidate,
such as the existence and properties of its an-
tecedents. In fact, such information has been
used for pronoun resolution in many heuristic-
based systems. The S-List model (Strube,
1998), for example, assumes that a co-referring
candidate is a hearer-old discourse entity and
is preferred to other hearer-new candidates.
In the algorithms based on the centering the-
ory (Brennan et al, 1987; Grosz et al, 1995), if
a candidate and its antecedent are the backward-
looking centers of two subsequent utterances re-
spectively, the candidate would be the most pre-
ferred since the CONTINUE transition is al-
ways ranked higher than SHIFT or RETAIN.
In this paper, we present a supervised
learning-based pronoun resolution system which
incorporates coreferential information of candi-
dates in a trainable model. For each candi-
date, we take into consideration the properties
of its antecedents in terms of features (hence-
forth backward features), and use the supervised
learning method to explore their influences on
pronoun resolution. In the study, we start our
exploration on the capability of the model by
applying it in an ideal environment where the
antecedents of the candidates are correctly iden-
tified and the backward features are optimally
set. The experiments on MUC-6 (1995) and
MUC-7 (1998) corpora show that incorporating
coreferential information of candidates boosts
the system performance significantly. Further,
we apply our model in the real resolution where
the antecedents of the candidates are provided
by separate noun phrase resolution modules.
The experimental results show that our model
still outperforms the baseline, even with the low
recall of the non-pronoun resolution module.
The remaining of this paper is organized as
follows. Section 2 discusses the importance of
the coreferential information for candidate eval-
uation. Section 3 introduces the baseline learn-
ing framework. Section 4 presents and evaluates
the learning model which uses backward fea-
tures to capture coreferential information, while
Section 5 proposes how to apply the model in
real resolution. Section 6 describes related re-
search work. Finally, conclusion is given in Sec-
tion 7.
2 The Impact of Coreferential
Information on Pronoun
Resolution
In pronoun resolution, the center of attention
throughout the discourse segment is a very im-
portant factor for antecedent selection (Mitkov,
1999). If a candidate is the focus (or center)
of the local discourse, it would be selected as
the antecedent with a high possibility. See the
following example,
<s> Gitano1 has pulled off a clever illusion2
with its3 advertising4. <s>
<s> The campaign5 gives its6 clothes a
youthful and trendy image to lure consumers
into the store. <s>
Table 1: A text segment from MUC-6 data set
In the above text, the pronoun ?its6? has
several antecedent candidates, i.e., ?Gitano1?,
?a clever illusion2?, ?its3?, ?its advertising4?
and ?The campaign5?. Without looking back,
?The campaign5? would be probably selected
because of its syntactic role (Subject) and its
distance to the anaphor. However, given the
knowledge that the company Gitano is the fo-
cus of the local context and ?its3? refers to
?Gitano1?, it would be clear that the pronoun
?its6? should be resolved to ?its3? and thus
?Gitano1?, rather than other competitors.
To determine whether a candidate is the ?fo-
cus? entity, we should check how the status (e.g.
grammatical functions) of the entity alternates
in the local context. Therefore, it is necessary
to track the NPs in the coreferential chain of
the candidate. For example, the syntactic roles
(i.e., subject) of the antecedents of ?its3? would
indicate that ?its3? refers to the most salient
entity in the discourse segment.
In our study, we keep the properties of the an-
tecedents as features of the candidates, and use
the supervised learning method to explore their
influence on pronoun resolution. Actually, to
determine the local focus, we only need to check
the entities in a short discourse segment. That
is, for a candidate, the number of its adjacent
antecedents to be checked is limited. Therefore,
we could evaluate the salience of a candidate
by looking back only its closest antecedent in-
stead of each element in its coreferential chain,
with the assumption that the closest antecedent
is able to provide sufficient information for the
evaluation.
3 The Baseline Learning Framework
Our baseline system adopts the common
learning-based framework employed in the sys-
tem by Soon et al (2001).
In the learning framework, each training or
testing instance takes the form of i{ana, candi},
where ana is the possible anaphor and candi is
its antecedent candidate1. An instance is associ-
ated with a feature vector to describe their rela-
tionships. As listed in Table 2, we only consider
those knowledge-poor and domain-independent
features which, although superficial, have been
proved efficient for pronoun resolution in many
previous systems.
During training, for each anaphor in a given
text, a positive instance is created by paring
the anaphor and its closest antecedent. Also a
set of negative instances is formed by paring the
anaphor and each of the intervening candidates.
Based on the training instances, a binary classi-
fier is generated using C5.0 learning algorithm
(Quinlan, 1993). During resolution, each possi-
ble anaphor ana, is paired in turn with each pre-
ceding antecedent candidate, candi, from right
to left to form a testing instance. This instance
is presented to the classifier, which will then
return a positive or negative result indicating
whether or not they are co-referent. The pro-
cess terminates once an instance i{ana, candi}
is labelled as positive, and ana will be resolved
to candi in that case.
4 The Learning Model Incorporating
Coreferential Information
The learning procedure in our model is similar
to the above baseline method, except that for
each candidate, we take into consideration its
closest antecedent, if possible.
4.1 Instance Structure
During both training and testing, we adopt the
same instance selection strategy as in the base-
line model. The only difference, however, is the
structure of the training or testing instances.
Specifically, each instance in our model is com-
posed of three elements like below:
1In our study candidates are filtered by checking the
gender, number and animacy agreements in advance.
Features describing the candidate (candi)
1. candi DefNp 1 if candi is a definite NP; else 0
2. candi DemoNP 1 if candi is an indefinite NP; else 0
3. candi Pron 1 if candi is a pronoun; else 0
4. candi ProperNP 1 if candi is a proper name; else 0
5. candi NE Type 1 if candi is an ?organization? named-entity; 2 if ?person?, 3 if
other types, 0 if not a NE
6. candi Human the likelihood (0-100) that candi is a human entity (obtained
from WordNet)
7. candi FirstNPInSent 1 if candi is the first NP in the sentence where it occurs
8. candi Nearest 1 if candi is the candidate nearest to the anaphor; else 0
9. candi SubjNP 1 if candi is the subject of the sentence it occurs; else 0
Features describing the anaphor (ana):
10. ana Reflexive 1 if ana is a reflexive pronoun; else 0
11. ana Type 1 if ana is a third-person pronoun (he, she,. . . ); 2 if a single
neuter pronoun (it,. . . ); 3 if a plural neuter pronoun (they,. . . );
4 if other types
Features describing the relationships between candi and ana:
12. SentDist Distance between candi and ana in sentences
13. ParaDist Distance between candi and ana in paragraphs
14. CollPattern 1 if candi has an identical collocation pattern with ana; else 0
Table 2: Feature set for the baseline pronoun resolution system
i{ana, candi, ante-of-candi}
where ana and candi, similar to the defini-
tion in the baseline model, are the anaphor and
one of its candidates, respectively. The new
added element in the instance definition, ante-
of-candi, is the possible closest antecedent of
candi in its coreferential chain. The ante-of-
candi is set to NIL in the case when candi has
no antecedent.
Consider the example in Table 1 again. For
the pronoun ?it6?, three training instances will
be generated, namely, i{its6, The compaign5,
NIL}, i{its6, its advertising4, NIL}, and
i{its6, its3, Gitano1}.
4.2 Backward Features
In addition to the features adopted in the base-
line system, we introduce a set of backward fea-
tures to describe the element ante-of-candi. The
ten features (15-24) are listed in Table 3 with
their respective possible values.
Like feature 1-9, features 15-22 describe the
lexical, grammatical and semantic properties of
ante-of-candi. The inclusion of the two features
Apposition (23) and candi NoAntecedent (24) is
inspired by the work of Strube (1998). The
feature Apposition marks whether or not candi
and ante-of-candi occur in the same appositive
structure. The underlying purpose of this fea-
ture is to capture the pattern that proper names
are accompanied by an appositive. The entity
with such a pattern may often be related to the
hearers? knowledge and has low preference. The
feature candi NoAntecedent marks whether or
not a candidate has a valid antecedent in the
preceding text. As stipulated in Strube?s work,
co-referring expressions belong to hearer-old en-
tities and therefore have higher preference than
other candidates. When the feature is assigned
value 1, all the other backward features (15-23)
are set to 0.
4.3 Results and Discussions
In our study we used the standard MUC-
6 and MUC-7 coreference corpora. In each
data set, 30 ?dry-run? documents were anno-
tated for training as well as 20-30 documents
for testing. The raw documents were prepro-
cessed by a pipeline of automatic NLP com-
ponents (e.g. NP chunker, part-of-speech tag-
ger, named-entity recognizer) to determine the
boundary of the NPs, and to provide necessary
information for feature calculation.
In an attempt to investigate the capability of
our model, we evaluated the model in an opti-
mal environment where the closest antecedent
of each candidate is correctly identified. MUC-
6 and MUC-7 can serve this purpose quite well;
the annotated coreference information in the
data sets enables us to obtain the correct closest
Features describing the antecedent of the candidate (ante-of-candi):
15. ante-candi DefNp 1 if ante-of-candi is a definite NP; else 0
16. ante-candi IndefNp 1 if ante-of-candi is an indefinite NP; else 0
17. ante-candi Pron 1 if ante-of-candi is a pronoun; else 0
18. ante-candi Proper 1 if ante-of-candi is a proper name; else 0
19. ante-candi NE Type 1 if ante-of-candi is an ?organization? named-entity; 2 if ?per-
son?, 3 if other types, 0 if not a NE
20. ante-candi Human the likelihood (0-100) that ante-of-candi is a human entity
21. ante-candi FirstNPInSent 1 if ante-of-candi is the first NP in the sentence where it occurs
22. ante-candi SubjNP 1 if ante-of-candi is the subject of the sentence where it occurs
Features describing the relationships between the candidate (candi) and ante-of-candi :
23. Apposition 1 if ante-of-candi and candi are in an appositive structure
Features describing the candidate (candi):
24. candi NoAntecedent 1 if candi has no antecedent available; else 0
Table 3: Backward features used to capture the coreferential information of a candidate
antecedent for each candidate and accordingly
generate the training and testing instances. In
the next section we will further discuss how to
apply our model into the real resolution.
Table 4 shows the performance of different
systems for resolving the pronominal anaphors 2
in MUC-6 and MUC-7. Default learning param-
eters for C5.0 were used throughout the exper-
iments. In this table we evaluated the perfor-
mance based on two kinds of measurements:
? ?Recall-and-Precision?:
Recall = #positive instances classified correctly#positive instances
Precision = #positive instances classified correctly#instances classified as positive
The above metrics evaluate the capability
of the learned classifier in identifying posi-
tive instances3. F-measure is the harmonic
mean of the two measurements.
? ?Success?:
Success = #anaphors resolved correctly#total anaphors
The metric4 directly reflects the pronoun
resolution capability.
The first and second lines of Table 4 compare
the performance of the baseline system (Base-
2The first and second person pronouns are discarded
in our study.
3The testing instances are collected in the same ways
as the training instances.
4In the experiments, an anaphor is considered cor-
rectly resolved only if the found antecedent is in the same
coreferential chain of the anaphor.
ante-candi_SubjNP = 1: 1 (49/5)
ante-candi_SubjNP = 0:
:..candi_SubjNP = 1:
:..SentDist = 2: 0 (3)
: SentDist = 0:
: :..candi_Human > 0: 1 (39/2)
: : candi_Human <= 0:
: : :..candi_NoAntecedent = 0: 1 (8/3)
: : candi_NoAntecedent = 1: 0 (3)
: SentDist = 1:
: :..ante-candi_Human <= 50 : 0 (4)
: ante-candi_Human > 50 : 1 (10/2)
:
candi_SubjNP = 0:
:..candi_Pron = 1: 1 (32/7)
candi_Pron = 0:
:..candi_NoAntecedent = 1:
:..candi_FirstNPInSent = 1: 1 (6/2)
: candi_FirstNPInSent = 0: ...
candi_NoAntecedent = 0: ...
Figure 1: Top portion of the decision tree
learned on MUC-6 with the backward features
line) and our system (Optimal), where DTpron
and DTpron?opt are the classifiers learned in
the two systems, respectively. The results in-
dicate that our system outperforms the base-
line system significantly. Compared with Base-
line, Optimal achieves gains in both recall (6.4%
for MUC-6 and 4.1% for MUC-7) and precision
(1.3% for MUC-6 and 9.0% for MUC-7). For
Success, we also observe an apparent improve-
ment by 4.7% (MUC-6) and 3.5% (MUC-7).
Figure 1 shows the portion of the pruned deci-
sion tree learned for MUC-6 data set. It visual-
izes the importance of the backward features for
the pronoun resolution on the data set. From
Testing Backward feature MUC-6 MUC-7
Experiments classifier assigner* R P F S R P F S
Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0
Optimal DTpron?opt (Annotated) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5
RealResolve-1 DTpron?opt DTpron?opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8
RealResolve-2 DTpron?opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9
RealResolve-3 DT?pron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
RealResolve-4 DT?pron DT
?
pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7
(*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to
RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon?pron.)
the tree we could find that:
1.) Feature ante-candi SubjNP is of the most
importance as the root feature of the tree.
The decision tree would first examine the
syntactic role of a candidate?s antecedent,
followed by that of the candidate. This
nicely proves our assumption that the prop-
erties of the antecedents of the candidates
provide very important information for the
candidate evaluation.
2.) Both features ante-candi SubjNP and
candi SubjNP rank top in the decision tree.
That is, for the reference determination,
the subject roles of the candidate?s referent
within a discourse segment will be checked
in the first place. This finding supports well
the suggestion in centering theory that the
grammatical relations should be used as the
key criteria to rank forward-looking centers
in the process of focus tracking (Brennan
et al, 1987; Grosz et al, 1995).
3.) candi Pron and candi NoAntecedent are
to be examined in the cases when the
subject-role checking fails, which confirms
the hypothesis in the S-List model by
Strube (1998) that co-refereing candidates
would have higher preference than other
candidates in the pronoun resolution.
5 Applying the Model in Real
Resolution
In Section 4 we explored the effectiveness of
the backward feature for pronoun resolution. In
those experiments our model was tested in an
ideal environment where the closest antecedent
of a candidate can be identified correctly when
generating the feature vector. However, during
real resolution such coreferential information is
not available, and thus a separate module has
algorithm PRON-RESOLVE
input:
DTnon?pron: classifier for resolving non-pronouns
DTpron: classifier for resolving pronouns
begin:
M1..n:= the valid markables in the given docu-
ment
Ante[1..n] := 0
for i = 1 to N
for j = i - 1 downto 0
if (Mi is a non-pron and
DTnon?pron(i{Mi,Mj}) == + )
or
(Mi is a pron and
DTpron(i{Mi,Mj , Ante[j]}) == +)
then
Ante[i] := Mj
break
return Ante
Figure 2: The pronoun resolution algorithm by
incorporating coreferential information of can-
didates
to be employed to obtain the closest antecedent
for a candidate. We describe the algorithm in
Figure 2.
The algorithm takes as input two classifiers,
one for the non-pronoun resolution and the
other for pronoun resolution. Given a testing
document, the antecedent of each NP is identi-
fied using one of these two classifiers, depending
on the type of NP. Although a separate non-
pronoun resolution module is required for the
pronoun resolution task, this is usually not a
big problem as these two modules are often in-
tegrated in coreference resolution systems. We
just use the results of the one module to improve
the performance of the other.
5.1 New Training and Testing
Procedures
For a pronominal candidate, its antecedent can
be obtained by simply using DTpron?opt. For
Training Procedure:
T1. Train a non-pronoun resolution clas-
sifier DTnon?pron and a pronoun resolution
classifier DTpron, using the baseline learning
framework (without backward features).
T2. Apply DTnon?pron and DTpron to iden-
tify the antecedent of each non-pronominal
and pronominal markable, respectively, in a
given document.
T3. Go through the document again. Gen-
erate instances with backward features as-
signed using the antecedent information ob-
tained in T2.
T4. Train a new pronoun resolution classifier
DT?pron on the instances generated in T3.
Testing Procedure:
R1. For each given document, do T2?T3.
R2. Resolve pronouns by applying DT?pron.
Table 5: New training and testing procedures
a non-pronominal candidate, we built a non-
pronoun resolution module to identify its an-
tecedent. The module is a duplicate of the
NP coreference resolution system by Soon et
al. (2001)5 , which uses the similar learn-
ing framework as described in Section 3. In
this way, we could do pronoun resolution
just by running PRON-RESOLVE(DTnon?pron,
DTpron?opt), where DTnon?pron is the classifier
of the non-pronoun resolution module.
One problem, however, is that DTpron?opt is
trained on the instances whose backward fea-
tures are correctly assigned. During real resolu-
tion, the antecedent of a candidate is found by
DTnon?pron or DTpron?opt, and the backward
feature values are not always correct. Indeed,
for most noun phrase resolution systems, the
recall is not very high. The antecedent some-
times can not be found, or is not the closest
one in the preceding coreferential chain. Con-
sequently, the classifier trained on the ?perfect?
feature vectors would probably fail to output
anticipated results on the noisy data during real
resolution.
Thus we modify the training and testing pro-
cedures of the system. For both training and
testing instances, we assign the backward fea-
ture values based on the results from separate
NP resolution modules. The detailed proce-
dures are described in Table 5.
5Details of the features can be found in Soon et al
(2001)
algorithm REFINE-CLASSIFIER
begin:
DT1pron := DT
?
pron
for i = 1 to ?
Use DTipron to update the antecedents of
pronominal candidates and the correspond-
ing backward features;
Train DTi+1pron based on the updated training
instances;
if DTi+1pron is not better than DTipron then
break;
return DTipron
Figure 3: The classifier refining algorithm
The idea behind our approach is to train
and test the pronoun resolution classifier on
instances with feature values set in a consis-
tent way. Here the purpose of DTpron and
DTnon?pron is to provide backward feature val-
ues for training and testing instances. From this
point of view, the two modules could be thought
of as a preprocessing component of our pronoun
resolution system.
5.2 Classifier Refining
If the classifier DT?pron outperforms DTpron
as expected, we can employ DT?pron in place
of DTpron to generate backward features for
pronominal candidates, and then train a clas-
sifier DT??pron based on the updated training in-
stances. Since DT?pron produces more correct
feature values than DTpron, we could expect
that DT??pron will not be worse, if not better,
than DT?pron. Such a process could be repeated
to refine the pronoun resolution classifier. The
algorithm is described in Figure 3.
In algorithm REFINE-CLASSIFIER, the it-
eration terminates when the new trained clas-
sifier DTi+1pron provides no further improvement
than DTipron. In this case, we can replace
DTi+1pron by DTipron during the i+1(th) testing
procedure. That means, by simply running
PRON-RESOLVE(DTnon?pron,DTipron), we can
use for both backward feature computation and
instance classification tasks, rather than apply-
ing DTpron and DT?pron subsequently.
5.3 Results and Discussions
In the experiments we evaluated the perfor-
mance of our model in real pronoun resolution.
The performance of our model depends on the
performance of the non-pronoun resolution clas-
sifier, DTnon?pron. Hence we first examined the
coreference resolution capability of DTnon?pron
based on the standard scoring scheme by Vi-
lain et al (1995). For MUC-6, the module ob-
tains 62.2% recall and 78.8% precision, while for
MUC-7, it obtains 50.1% recall and 75.4% pre-
cision. The poor recall and comparatively high
precision reflect the capability of the state-of-
the-art learning-based NP resolution systems.
The third block of Table 4 summarizes the
performance of the classifier DTpron?opt in real
resolution. In the systems RealResolve-1 and
RealResolve-2, the antecedents of pronominal
candidates are found by DTpron?opt and DTpron
respectively, while in both systems the an-
tecedents of non-pronominal candidates are by
DTnon?pron. As shown in the table, compared
with the Optimal where the backward features
of testing instances are optimally assigned, the
recall rates of two systems drop largely by 7.8%
for MUC-6 and by about 14% for MUC-7. The
scores of recall are even lower than those of
Baseline. As a result, in comparison with Op-
timal, we see the degrade of the F-measure and
the success rate, which confirms our hypothesis
that the classifier learned on perfect training in-
stances would probably not perform well on the
noisy testing instances.
The system RealResolve-3 listed in the fifth
line of the table uses the classifier trained
and tested on instances whose backward fea-
tures are assigned according to the results from
DTnon?pron and DTpron. From the table we can
find that: (1) Compared with Baseline, the sys-
tem produces gains in recall (2.1% for MUC-6
and 2.8% for MUC-7) with no significant loss
in precision. Overall, we observe the increase in
F-measure for both data sets. If measured by
Success, the improvement is more apparent by
4.7% (MUC-6) and 1.8% (MUC-7). (2) Com-
pared with RealResolve-1(2), the performance
decrease of RealResolve-3 against Optimal is
not so large. Especially for MUC-6, the system
obtains a success rate as high as Optimal.
The above results show that our model can
be successfully applied in the real pronoun res-
olution task, even given the low recall of the
current non-pronoun resolution module. This
should be owed to the fact that for a candidate,
its adjacent antecedents, even not the closest
one, could give clues to reflect its salience in
the local discourse. That is, the model prefers a
high precision to a high recall, which copes well
with the capability of the existing non-pronoun
resolution module.
In our experiments we also tested the clas-
sifier refining algorithm described in Figure 3.
We found that for both MUC-6 and MUC-7
data set, the algorithm terminated in the second
round. The comparison of DT2pron and DT1pron
(i.e. DT?pron) showed that these two trees were
exactly the same. The algorithm converges fast
probably because in the data set, most of the
antecedent candidates are non-pronouns (89.1%
for MUC-6 and 83.7% for MUC-7). Conse-
quently, the ratio of the training instances with
backward features changed may be not substan-
tial enough to affect the classifier generation.
Although the algorithm provided no further
refinement for DT?pron, we can use DT
?
pron, as
suggested in Section 5.2, to calculate back-
ward features and classify instances by running
PRON-RESOLVE(DTnon?pron, DT?pron). The
results of such a system, RealResolve-4, are
listed in the last line of Table 4. For both MUC-
6 and MUC-7, RealResolve-4 obtains exactly
the same performance as RealResolve-3.
6 Related Work
To our knowledge, our work is the first ef-
fort that systematically explores the influence of
coreferential information of candidates on pro-
noun resolution in learning-based ways. Iida et
al. (2003) also take into consideration the con-
textual clues in their coreference resolution sys-
tem, by using two features to reflect the ranking
order of a candidate in Salience Reference List
(SRL). However, similar to common centering
models, in their system the ranking of entities
in SRL is also heuristic-based.
The coreferential chain length of a candidate,
or its variants such as occurrence frequency and
TFIDF, has been used as a salience factor in
some learning-based reference resolution sys-
tems (Iida et al, 2003; Mitkov, 1998; Paul et
al., 1999; Strube and Muller, 2003). However,
for an entity, the coreferential length only re-
flects its global salience in the whole text(s), in-
stead of the local salience in a discourse segment
which is nevertheless more informative for pro-
noun resolution. Moreover, during resolution,
the found coreferential length of an entity is of-
ten incomplete, and thus the obtained length
value is usually inaccurate for the salience eval-
uation.
7 Conclusion and Future Work
In this paper we have proposed a model which
incorporates coreferential information of candi-
dates to improve pronoun resolution. When
evaluating a candidate, the model considers its
adjacent antecedent by describing its properties
in terms of backward features. We first exam-
ined the effectiveness of the model by applying
it in an optimal environment where the clos-
est antecedent of a candidate is obtained cor-
rectly. The experiments show that it boosts
the success rate of the baseline system for both
MUC-6 (4.7%) and MUC-7 (3.5%). Then we
proposed how to apply our model in the real res-
olution where the antecedent of a non-pronoun
is found by an additional non-pronoun resolu-
tion module. Our model can still produce Suc-
cess improvement (4.7% for MUC-6 and 1.8%
for MUC-7) against the baseline system, de-
spite the low recall of the non-pronoun resolu-
tion module.
In the current work we restrict our study only
to pronoun resolution. In fact, the coreferential
information of candidates is expected to be also
helpful for non-pronoun resolution. We would
like to investigate the influence of the coreferen-
tial factors on general NP reference resolution in
our future work.
References
S. Brennan, M. Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of
the Association for Compuational Linguis-
tics, pages 155?162.
N. Ge, J. Hale, and E. Charniak. 1998. A
statistical approach to anaphora resolution.
In Proceedings of the 6th Workshop on Very
Large Corpora.
B. Grosz, A. Joshi, and S. Weinstein. 1983.
Providing a unified account of definite noun
phrases in discourse. In Proceedings of the
21st Annual meeting of the Association for
Computational Linguistics, pages 44?50.
B. Grosz, A. Joshi, and S. Weinstein. 1995.
Centering: a framework for modeling the
local coherence of discourse. Computational
Linguistics, 21(2):203?225.
R. Iida, K. Inui, H. Takamura, and Y. Mat-
sumoto. 2003. Incorporating contextual cues
in trainable models for coreference resolu-
tion. In Proceedings of the 10th Confer-
ence of EACL, Workshop ?The Computa-
tional Treatment of Anaphora?.
R. Mitkov. 1998. Robust pronoun resolution
with limited knowledge. In Proceedings of the
17th Int. Conference on Computational Lin-
guistics, pages 869?875.
R. Mitkov. 1999. Anaphora resolution: The
state of the art. Technical report, University
of Wolverhampton.
MUC-6. 1995. Proceedings of the Sixth Message
Understanding Conference. Morgan Kauf-
mann Publishers, San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh
Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA.
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, pages 104?111, Philadelphia.
M. Paul, K. Yamamoto, and E. Sumita. 1999.
Corpus-based anaphora resolution towards
antecedent preference. In Proceedings of
the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop ?Coreference and It?s Applications?,
pages 47?52.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publish-
ers, San Francisco, CA.
C. Sidner. 1981. Focusing for interpretation
of pronouns. American Journal of Computa-
tional Linguistics, 7(4):217?231.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference reso-
lution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine
learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st
Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alterna-
tive to centering. In Proceedings of the 17th
Int. Conference on Computational Linguis-
tics and 36th Annual Meeting of ACL, pages
1251?1257.
J. R. Tetreault. 2001. A corpus-based eval-
uation of centering and pronoun resolution.
Computational Linguistics, 27(4):507?520.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan.
2003. Coreference resolution using competi-
tion learning approach. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, Japan.
Multi-Criteria-based Active Learning for Named Entity Recognition 
Dan Shen??1 Jie Zhang?? Jian Su? Guodong Zhou? Chew-Lim Tan? 
? Institute for Infocomm Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg 
{shendan,zhangjie,tancl}@comp.nus.edu.sg 
                                                                 
1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germany 
dshen@coli.uni-sb.de 
 
 
 
 
Abstract 
In this paper, we propose a multi-criteria -
based active learning approach and effec-
tively apply it to named entity recognition. 
Active learning targets to minimize the 
human annotation efforts by selecting ex-
amples for labeling.  To maximize the con-
tribution of the selected examples, we 
consider the multiple criteria: informative-
ness, representativeness and diversity  and 
propose measures to quantify them.  More 
comprehensively, we incorporate all the 
criteria using two selection strategies, both 
of which result in less labeling cost than 
single-criterion-based method.  The results 
of the named entity recognition in both 
MUC-6 and GENIA show that the labeling 
cost can be reduced by at least 80% with-
out degrading the performance. 
1 Introduction 
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally 
trained on large annotated corpus.  However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an 
existing model to a new domain.  In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP 
applications such as POS tagging (Engelson and 
Dagan 1999), information extraction (Thompson et 
al. 1999), text classif ication (Lewis and Catlett 
1994; McCallum and Nigam 1998; Schohn and 
Cohn 2000; Tong and Koller 2000; Brinker 2003), 
statistical parsing (Thompson et al 1999; Tang et 
al. 2002; Steedman et al 2003), noun phrase 
chunking (Ngai and Yarowsky 2000), etc. 
Active learning is based on the assumption that 
a small number of annotated examples and a large 
number of unannotated examples are available.  
This assumption is valid in most NLP tasks.  Dif-
ferent from supervised learning in which the entire 
corpus are labeled manually, active learning is to 
select the most useful example for labeling and add 
the labeled example  to training set to retrain model.  
This procedure is repeated until the model achieves 
a certain level of performance.  Practically, a batch 
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994) 
since it is time consuming to retrain the model if 
only one new example is added to the training set.  
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et 
al. 1999; Tang et al 2002; Schohn and Cohn 2000; 
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000) to select the most informative examples for 
which the current model are most uncertain. 
Being the first piece of work on active learning 
for name entity recognition (NER) task, we target 
to minimize the human annotation efforts yet still 
reaching the same level of performance as a super-
vised learning approach.  For this purpose, we 
make a more comprehensive consideration on the 
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch 
based on three criteria : informativeness, represen-
tativeness and diversity. 
First, we propose three scoring functions to 
quantify the informativeness of an example , which 
can be used to select the most uncertain examples.  
Second, the representativeness measure is further 
proposed to choose the examples representing the 
majority.  Third, we propose two diversity consid-
erations (global and local) to avoid repetition 
among the examples of a batch.  Finally, two com-
bination strategies with the above three criteria are 
proposed to reach the maximum effectiveness on 
active learning for NER. 
We build our NER model using Support Vec-
tor Machines (SVM).  The experiment shows that 
our active learning methods achieve a promising 
result in this NER task.  The results in both MUC-
6 and GENIA show that the amount of the labeled 
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer.  The contributions not only come from the 
above measures, but also the two sample selection 
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.  
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.  
Furthermore, such measures and strategies can be 
easily adapted to other active learning tasks as well.  
 
2 Multi-criteria for NER Active Learning 
Support Vector Machines (SVM) is a powerful 
machine learning method, which has been applied 
successfully in NER tasks, such as (Kazama et al 
2002; Lee et al 2003).  In this paper, we apply ac-
tive learning methods to a simple  and effective 
SVM model to recognize one class of names at a 
time, such as protein names, person names, etc.  In 
NER, SVM is to classify a word into positive class 
?1? indicating that the word is a part of an entity, 
or negative class ?-1? indicating that the word is 
not a part of an entity.  Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic 
features, POS feature and semantic trigger features 
(Shen et al 2003).  The semantic trigger features 
consist of some special head nouns for an entity 
class which is supplied by users.  Furthermore, a 
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.   
However, for active learning in NER, it is not 
reasonable to select a single word without context 
for human to label.  Even if we require human to 
label a single word, he has to make an addition 
effort to refer to the context of the word.  In our 
active learning process, we select a word sequence 
which consists of a machine-annotated named en-
tity and its context rather than a single word.  
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further 
study how to extend the measures for words to 
named entities.  Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classif ication 
on which most SVM active learning works are 
conducted (Schohn and Cohn 2000; Tong and 
Koller 2000; Brinker 2003).  In the next part, we 
will introduce informativeness, representativeness 
and diversity measures for the SVM-based NER. 
2.1 Informativeness 
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods, 
which have been used in many previous works.  In 
our task, we use a distance-based measure to 
evaluate the informativeness of a word and extend 
it to the measure of an entity using three scoring 
functions.  We prefer the examples with high in-
formative degree for which the current model are 
most uncertain. 
2.1.1 Informativeness Measure for Word 
In the simplest linear form, training SVM is to find 
a hyperplane that can separate the posit ive and 
negative examples in training set with maximum 
margin.  The margin is defined by the distance of 
the hyperplane to the nearest of the positive and 
negative examples.  The training examples which 
are closest to the hyperplane are called support 
vectors.  In SVM, only the support vectors are use-
ful for the classification, which is different from 
statistical models.  SVM training is to get these 
support vectors and their weights from training set 
by solving quadratic programming problem.  The 
support vectors can later be used to classify the test 
data. 
Intuitively, we consider the informativeness of 
an example  as how it can make effect on the sup-
port vectors by adding it to training set.  An exam-
ple may be informative for the learner if the 
distance of its feature vector to the hyperplane is 
less than that of the support vectors to the hyper-
plane (equal to 1).  This intuition is also justified 
by (Schohn and Cohn 2000; Tong and Koller 2000) 
based on a version space analysis.  They state that 
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution.  In our task, we use the distance to measure 
the informativeness of an example. 
The distance of a word?s feature vector to the 
hyperplane is computed as follows: 
1
( ) ( , )
N
i i i
i
Dist y k ba
=
= +?w s w  
where w is the feature vector of the word, ai, yi, si 
corresponds to the weight, the class and the feature 
vector of the ith support vector respectively.  N is 
the number of the support vectors in current model. 
We select the example with minimal Dist, 
which indicates that it comes closest to the hyper-
plane in feature space.  This example is considered 
most informative for current model. 
2.1.2 Informativeness Measure for Named 
Entity 
Based on the above informativeness measure for a 
word, we compute the overall informativeness de-
gree of a named entity NE.  In this paper, we pro-
pose three scoring functions as follows. Let NE = 
w1?wN in which wi is the feature vector of the ith 
word of NE. 
? Info_Avg: The informativeness of NE is 
scored by the average distance of the words in 
NE to the hyperplane.  
 ( ) 1 ( )
i
i
N E
Info NE Dist
?
= - ?
w
w  
 where, wi is the feature vector of the ith word in 
NE. 
? Info_Min: The informativeness of NE is 
scored by the minimal distance of the words in 
NE. 
 ( ) 1 { ( )}
i
iNE
Info NE Min Dist
?
= -
w
w  
? Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our 
task), the word is considered with short dis-
tance.  Then, we compute the proportion of the 
number of words with short distance to the to-
tal number of words in the named entity and 
use this proportion to quantify the informa-
tiveness of the named entity.  
 
( ( ) )
( ) i
i
N E
NUM Dist
Info NE
N
a
?
<
= w
w
 
In Section 4.3, we will evaluate the effective-
ness of these scoring functions. 
2.2 Representativeness 
In addition to the most informative example, we 
also prefer the most representative example.  The 
representativeness of an example can be evaluated 
based on how many examples there are similar or 
near to it.  So, the examples with high representa-
tive degree are less likely to be an outlier.  Adding 
them to the training set will have effect on a large 
number of unlabeled examples.  There are only a 
few works considering this selection criterion 
(McCallum and Nigam 1998; Tang et al 2002) and 
both of them are specific to their tasks, viz. text 
classification and statistical parsing.  In this section, 
we compute the simila rity between words using a 
general vector-based measure, extend this measure 
to named entity level using dynamic time warping 
algorithm and quantify the representativeness of a 
named entity by its density. 
2.2.1 Similarity Measure  between Words 
In general vector space model, the similarity be-
tween two vectors may be measured by computing 
the cosine value of the angle between them.  The 
smaller the angle is, the more similar between the 
vectors are.  This measure, called cosine-similarity 
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).    
In our task, we also use it to quantify the similarity 
between two words.  Particularly, the calculation in 
SVM need be projected to a higher dimensional 
space by using a certain kernel function ( , )i jK w w .  
Therefore, we adapt the cosine-similarity measure 
to SVM as follows: 
( , )
( , )
( , ) ( , )
i j
i j
i i j j
k
Sim
k k
=
w w
w w
w w w w
 
where, wi and wj are the feature vectors of the 
words i and j.  This calculation is also supported by 
(Brinker 2003)?s work.  Furthermore, if we use the 
linear kernel ( , )i j i jk = ?w w w w , the measure is 
the same as the traditional cosine similarity meas-
ure cos i j
i j
q
?
=
?
w w
w w
 and may be regarded as a 
general vector-based similarity measure. 
2.2.2 Similarity Meas ure between Named En-
tities 
In this part, we compute the similarity between two 
machine-annotated named entities given the simi-
larities between words.  Regarding an entity as a 
word sequence, this work is analogous to the 
alignment of two sequences.  We employ the dy-
namic time warping (DTW) algorithm (Rabiner et 
al. 1978) to find an optimal alignment between the 
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.  
Here, we adapt it to our task.  A sketch of the 
modified algorithm is as follows. 
Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) and 
NE2 = w21w22?w2m?w2M, (m = 1,?, M) denote two 
word sequences to be matched.  NE1 and NE2 con-
sist of M and N words respectively.  NE1(n) = w1n 
and NE2(m) = w2m.  A similarity value Sim(w1n ,w2m) 
has been known for every pair of words (w1n,w2m) 
within NE1 and NE2.  The goal of DTW is to find a 
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity 
Sim* along the path is maximized. 
1 2
{ ( )} 1
* { ( ( ), ( ( ))}
N
m a p n n
Sim M a x Sim N E n N E m a p n
=
= ?  
A dynamic programming method is used to deter-
mine the optimum path map(n).  The accumulated 
similarity SimA to any grid point (n, m) can be re-
cursively calculated as 
1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -
Finally, * ( , )ASim Sim N M=  
Certainly, the overall similarity measure Sim* 
has to be normalized as longer sequences normally 
give higher similarity value.  So, the similarity be-
tween two sequences NE1 and NE2 is calculated as 
1 2
*( , )
( , )
SimSim NE NE
Max N M
=  
2.2.3 Representativeness Measure for Named 
Entity 
Given a set of machine-annotated named entities 
NESet = {NE1, ? , NEN}, the representativeness of 
a named entity NEi in NESet is quantified by its 
density.  The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows. 
( , )
( )
1
i j
j i
i
Sim NE NE
Density N E
N
?=
-
?
 
If NEi has the largest density among all the entities 
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in 
NESet. 
2.3 Diversity 
Diversity criterion is to maximize the training util-
ity of a batch.  We prefer the batch in which the 
examples have high variance to each other.  For 
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time.  To our 
knowledge, there is only one work (Brinker 2003) 
exploring this criterion.  In our task, we propose 
two methods: local and global, to make the exam-
ples diverse enough in a batch.   
2.3.1 Global Consideration 
For a global consideration, we cluster all named 
entities in NESet based on the similarity measure 
proposed in Section 2.2.2.  The named entities in 
the same cluster may be considered similar to each 
other, so we will select the named entities from 
different clusters at one time.  We employ a K-
means clustering algorithm (Jelinek 1997), which 
is shown in Figure 1. 
Given: 
NESet = {NE1, ? , NEN} 
Suppose: 
The number of clusters is K 
Initialization: 
Randomly equally partition {NE1, ? , NEN} into K 
initial clusters Cj (j = 1, ? , K). 
Loop until the number of changes for the centroids of 
all clusters is less than a threshold 
? Find the centroid of each cluster Cj (j = 1, ? , K). 
 arg ( ( , ))
j i j
j i
NE C NE C
NECent max Sim NE NE
? ?
= ?  
? Repartition {NE1, ? , NEN} into K clusters.  NEi 
will be assigned to Cluster Cj if 
 
( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j? ?  
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm 
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster.  And then, we need to compute 
the similarities between each example and all cen-
troids to repartition the examples.  So, the algo-
rithm is time-consuming.  Based on the assumption 
that N examples are uniformly distributed between 
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al 2002).  In 
one of our experiments, the size of the NESet (N) is 
around 17000 and K is equal to 50, so the time 
complexity is about O(106).  For efficiency, we 
may filter the entities in NESet before clustering 
them, which will be further discussed in Section 3.  
2.3.2 Local Consideration 
When selecting a machine-annotated named entity, 
we compare it with all previously selected named 
entities in the current batch.  If the similarity be-
tween them is above a threshold ?, this example 
cannot be allowed to add into the batch.  The order 
of selecting examples is based on some measure, 
such as informativeness measure, representative-
ness measure or their combination.  This local se-
lection method is shown in Figure 2.  In this way, 
we avoid selecting too similar examples (similarity 
value ?  ?) in a batch.  The threshold ? may be the 
average similarity between the examples in NESet. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = empty 
Loop until BatchSet is full 
? Select NEi based on some measure from NESet. 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 2: Local Consideration for Diversity 
 
This consideration only requires O(NK+K2) 
computational time.  In one of our experiments (N 
 ?17000 and K = 50), the time complexity is about 
O(105).  It is more efficient than clustering algo-
rithm described in Section 2.3.1.  
 
3 Sample Selection strategies 
In this section, we will study how to combine and 
strike a proper balance between these criteria, viz. 
informativeness, representativeness and diversity, 
to reach the maximum effectiveness on NER active 
learning.  We build two strategies to combine the 
measures proposed above.  These strategies are 
based on the varying priorities of the criteria and 
the varying degrees to satisfy the criteria. 
? Strategy 1: We first consider the informative-
ness criterion.  We choose m examples with the 
most informativeness score from NESet to an in-
termediate set called INTERSet.  By this pre-
selecting, we make the selection process faster in 
the later steps since the size of INTERSet is much 
smaller than that of NESet.  Then we cluster the 
examples in INTERSet and choose the centroid of 
each cluster into a batch called BatchSet.  The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.  
Furthermore, the examples in different clusters 
may be considered diverse to each other.  By this 
means, we consider representativeness and diver-
sity criteria at the same time.  This strategy is 
shown in Figure 3.  One limitation of this strategy 
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster 
on INTERSet for efficiency.  The other is that since 
the representativeness of an example is only evalu-
ated on a cluster.  If the cluster size is too small, 
the most representative example in this cluster may 
not be representative in the whole sample space. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
INTERSet with the maximal size M 
Steps :  
? BatchSet  = ?  
? INTERSet = ?  
? Select M entities with most Info score from NESet 
to INTERSet. 
? Cluster the entities in INTERSet into K clusters 
? Add the centroid entity of each cluster to BatchSet 
Figure 3: Sample Selection Strategy 1 
 
? Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria  using 
the functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , in 
which the Info and Density  value of NEi are nor-
malized first.  The individual importance of each 
criterion in this function is adjusted by the trade-
off parameter l ( 0 1l? ? ) (set to 0.6 in our 
experiment).  First, we select a candidate example 
NEi with the maximum value of this function from 
NESet.  Second, we consider diversity criterion 
using the local method in Section 3.3.2.  We add 
the candidate example NEi to a batch only if NEi is 
different enough from any previously selected ex-
ample in the batch.  The threshold ? is set to the 
average pair-wise similarity of the entities in NE-
Set. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = ?  
Loop until BatchSet is full 
? Select NEi which have the maximum value for the 
combination function between Info score and Den-
sity socre from NESet. 
arg ( ( ) (1 ) ( ))
i
i i i
N E NESet
N E Max Info NE Density NEl l
?
= + -
 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 4: Sample Selection Strategy 2 
 
4 Experimental Results and Analysis 
4.1 Experiment Settings  
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein 
(PRT) names in biomedical domain using GENIA 
corpus V1.1 (Ohta et al 2002) and person (PER), 
location (LOC), organization (ORG) names in 
newswire domain using MUC-6 corpus.  First, we 
randomly split the whole corpus into three parts: an 
initial training set to build an in itial model, a test 
set to evaluate the performance of the model and 
an unlabeled set to select examples.  The size of 
each data set is shown in Table 1.  Then, iteratively, 
we select a batch of examples following the selec-
tion strategies proposed, require human experts to 
label them and add them into the training set.  The 
batch size K = 50 in GENIA and 10 in MUC-6.  
Each example is defined as a machine-recognized 
named entity and its context words (previous 3 
words and next 3 words). 
Domain Class Corpus Initial Training Set Test Set Unlabeled Set 
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words) 
PER 5 sent. (131 words) 7809 sent. (157K words) 
LOC 5 sent. (130 words) 7809 sent. (157K words) 
 
Newswire 
ORG 
 
MUC-6 
 5 sent. (113 words) 
 
602 sent. (14K words) 
 7809 sent. (157K words) 
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG) 
The goal of our work is to minimize the human 
annotation effort to learn a named entity recognizer 
with the same performance level as supervised 
learning.  The performance of our model is evalu-
ated using ?precision/recall/F-measure?. 
4.2 Overall Result in GENIA and MUC-6 
In this section, we evaluate our selection strategies 
by comparing them with a random selection 
method, in which a batch of examples is randomly 
selected iteratively, on GENIA and MUC-6 corpus.  
Table 2 shows the amount of training data needed 
to achieve the performance of supervised learning 
using various selection methods, viz. Random, 
Strategy1 and Strategy2.  In GENIA, we find: 
? The model achieves 63.3 F-measure using 223K  
words in the supervised learning. 
? The best performer is Strategy2 (31K words), 
requiring less than 40% of the training data that 
Random (83K words) does and 14% of the train-
ing data that the supervised learning does. 
? Strategy1 (40K words) performs slightly worse 
than Strategy2, requiring 9K more words.  It is 
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small. 
? Random (83K words) requires about 37% of the 
training data that the supervised learning does.  It 
indicates that only the words in and around a 
named entity are useful for classification and the 
words far from the named entity may not be 
helpful. 
 
Class Supervised Random Strategy1 Strategy2 
PRT 223K (F=63.3) 83K 40K 31K 
PER 157K (F=90.4) 11.5K 4.2K 3.5K 
LOC 157K (F=73.5) 13.6K 3.5K 2.1K 
ORG 157K (F=86.0) 20.2K 9.5K 7.8K 
Table 2: Overall Result in GENIA and MUC-6 
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and 
Strategy2 show a more promising result by com-
paring with the supervised learning and Random, 
as shown in Table 2.  On average, about 95% of 
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.  
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical 
domain (Shen et al 2003) and named entities are 
less and distributed much sparser in the newswire 
texts than in the biomedical texts. 
 
4.3 Effectiveness of Informativeness-based 
Selection Method 
In this section, we investigate the effectiveness of 
informativeness criterion in NER task.  Figure 5 
shows a plot of training data size versus F-measure 
achieved by the informativeness-based measures in 
Section 3.1.2: Info_Avg, Info_Min  and Info_S/N as 
well as Random.  We make the comparisons in 
GENIA corpus.  In Figure 5, the horizontal line is 
the performance level (63.3 F-measure) achieved 
by supervised learning (223K words).  We find 
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom.  Table 3 highlights the various data sizes to 
achieve the peak performance using these selection 
methods.  We find that Random (83K words) on 
average requires over 1.5 times as much as data to 
achieve the same performance as the informative-
ness-based selection methods (52K words). 
 
0.5
0.55
0.6
0.65
0 20 40 60 80K words
F
Supervised
Random
Info_Min
Info_S/N
Info_Avg
 
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the 
Random selection. 
Supervised Random Info_Avg Info_Min Info_ S/N 
223K 83K 52.0K 51.9K 52.3K 
Table 3: Training data sizes for various selection methods to 
achieve the same performance level as the supervised learning 
 
4.4 Effectiveness of Two Sample Selection 
Strategies 
In addition to the informativeness criterion, we 
further incorporate representativeness and diversity 
criteria into active learning using two strategies 
described in Section 3.  Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min , we are to jus-
tify that representativeness and diversity are also 
important factors for active learning.  Figure 6 
shows the learning curves for the various methods: 
Strategy1, Strategy2 and Info_Min.  In the begin-
ning iterations (F-measure < 60), the three methods 
performed similarly.  But with the larger training 
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident.  Table 4 highlights the final re-
sult of the three methods.  In order to reach the 
performance of supervised learning, Strategy1 
(40K words) and Strategyy2 (31K words) require 
about 80% and 60% of the data that Info_Min 
(51.9K) does.  So we believe the effective combi-
nations of informativeness, representativeness and 
diversity will help to learn the NER model more 
quickly and cost less in annotation. 
0.5
0.55
0.6
0.65
0 20 40 60 K words
F
Supervised
Info_Min
Strategy1
Strategy2
 
Figure 6: Active learning curves: effectiveness of the two 
multi-criteria-based selection strategies comparing with the 
informativeness-criterion-based selection (Info_Min). 
Info_Min Strategy1 Strategy2 
51.9K 40K 31K 
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning. 
 
5 Related Work 
Since there is no study on active learning for NER 
task previously, we only introduce general active 
learning methods here.  Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al 1999; 
Schohn and Cohn 2000; Tong and Koller 2000; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000).  Our informativeness-based measure is 
similar to these works.  However these works just 
follow a single criterion.  (McCallum and Nigam 
1998; Tang et al 2002) are the only two works 
considering the representativeness criterion in ac-
tive learning.  (Tang et al 2002) use the density 
information to weight the selected examples while 
we use it to select examples.  Moreover, the repre-
sentativeness measure we use is relatively general 
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text 
chunking, POS tagging, etc.  On the other hand, 
(Brinker 2003) first incorporate diversity in active 
learning for text classification.  Their work is simi-
lar to our local consideration in Section 2.3.2.  
However, he didn?t further explore how to avoid 
selecting outliers to a batch.  So far, we haven?t 
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether. 
 
6 Conclusion and Future Work 
In this paper, we study the active learning in a 
more complex NLP task, named entity recognition.  
We propose a multi-criteria -based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are 
incorporated all together by two strategies (local 
and global).  Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion 
(informativeness).  The labeling cost can be sig-
nificantly reduced by at least 80% comparing with 
the supervised learning.  To our best knowledge, 
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the 
first work to incorporate the three criteria all to-
gether for selecting examples. 
Although the current experiment results are 
very promising, some parameters in our experi-
ment, such as the batch size K and the l in the 
function of strategy 2, are decided by our experi-
ence in the domain.  In practical application, the 
optimal value of these parameters should be de-
cided automatically based on the training process.  
Furthermore, we will study how to overcome the 
limitation of the strategy 1 discussed in Section 3 
by using more effective clustering algorithm.  An-
other interesting work is to study when to stop ac-
tive learning.  
 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X. 
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In 
Proceedings of ICML, 2003. 
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research. 
F. Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings 
of the ACL2002 Workshop on NLP in Biomedi-
cine. 
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs.  In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning. 
In Proceedings of ICML, 1994. 
A. McCallum and K. Nigam. 1998. Employing EM 
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998. 
G. Ngai and D. Yarowsky. 2000. Rule Writing or 
Annotation: Cost-efficient Resource Usage for 
Base Noun Phrase Chunking. In Proceedings of 
ACL, 2000. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002. 
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 
1978. Considerations in Dynamic Time Warping 
Algorithms for Discrete Word Recognition.  In 
Proceedings of IEEE Transactions on acoustics, 
speech and signal processing. Vol. ASSP-26, 
NO.6. 
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In 
Proceedings of the 17th International Confer-
ence on Machine Learning. 
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 
2003. Effective Adaptation of a Hidden Markov 
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the 
ACL2003 Workshop on NLP in Biomedicine. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. 
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and 
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003. 
M. Tang, X. Luo and S. Roukos. 2002. Active 
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002. 
C. A. Thompson, M. E. Califf and R. J. Mooney. 
1999. Active Learning for Natural Language 
Parsing and Information Extraction. In Proceed-
ings of ICML 1999. 
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search. 
V. Vapnik. 1998. Statistical learning theory. 
N.Y.:John Wiley. 
 
Learning Word Senses With Feature Selection and Order Identification
Capabilities
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised word sense
learning algorithm, which induces senses of target
word by grouping its occurrences into a ?natural?
number of clusters based on the similarity of their
contexts. For removing noisy words in feature set,
feature selection is conducted by optimizing a clus-
ter validation criterion subject to some constraint in
an unsupervised manner. Gaussian mixture model
and Minimum Description Length criterion are used
to estimate cluster structure and cluster number.
Experimental results show that our algorithm can
find important feature subset, estimate model or-
der (cluster number) and achieve better performance
than another algorithm which requires cluster num-
ber to be provided.
1 Introduction
Sense disambiguation is essential for many lan-
guage applications such as machine translation, in-
formation retrieval, and speech processing (Ide and
Ve?ronis, 1998). Almost all of sense disambigua-
tion methods are heavily dependant on manually
compiled lexical resources. However these lexical
resources often miss domain specific word senses,
even many new words are not included inside.
Learning word senses from free text will help us
dispense of outside knowledge source for defining
sense by only discriminating senses of words. An-
other application of word sense learning is to help
enriching or even constructing semantic lexicons
(Widdows, 2003).
The solution of word sense learning is closely re-
lated to the interpretation of word senses. Different
interpretations of word senses result in different so-
lutions to word sense learning.
One interpretation strategy is to treat a word sense
as a set of synonyms like synset in WordNet. The
committee based word sense discovery algorithm
(Pantel and Lin, 2002) followed this strategy, which
treated senses as clusters of words occurring in sim-
ilar contexts. Their algorithm initially discovered
tight clusters called committees by grouping top
n words similar with target word using average-
link clustering. Then the target word was assigned
to committees if the similarity between them was
above a given threshold. Each committee that the
target word belonged to was interpreted as one of
its senses.
There are two difficulties with this committee
based sense learning. The first difficulty is about
derivation of feature vectors. A feature for target
word here consists of a contextual content word and
its grammatical relationship with target word. Ac-
quisition of grammatical relationship depends on
the output of a syntactic parser. But for some lan-
guages, ex. Chinese, the performance of syntactic
parsing is still a problem. The second difficulty with
this solution is that two parameters are required to
be provided, which control the number of commit-
tees and the number of senses of target word.
Another interpretation strategy is to treat a word
sense as a group of similar contexts of target word.
The context group discrimination (CGD) algorithm
presented in (Schu?tze, 1998) adopted this strategy.
Firstly, their algorithm selected important contex-
tual words using ?2 or local frequency criterion.
With the ?2 based criterion, those contextual words
whose occurrence depended on whether the am-
biguous word occurred were chosen as features.
When using local frequency criterion, their algo-
rithm selected top n most frequent contextual words
as features. Then each context of occurrences of
target word was represented by second order co-
occurrence based context vector. Singular value de-
composition (SVD) was conducted to reduce the di-
mensionality of context vectors. Then the reduced
context vectors were grouped into a pre-defined
number of clusters whose centroids corresponded to
senses of target word.
Some observations can be made about their fea-
ture selection and clustering procedure. One ob-
servation is that their feature selection uses only
first order information although the second order co-
occurrence data is available. The other observation
is about their clustering procedure. Similar with
committee based sense discovery algorithm, their
clustering procedure also requires the predefinition
of cluster number. Their method can capture both
coarse-gained and fine-grained sense distinction as
the predefined cluster number varies. But from a
point of statistical view, there should exist a parti-
tioning of data at which the most reliable, ?natural?
sense clusters appear.
In this paper, we follow the second order repre-
sentation method for contexts of target word, since
it is supposed to be less sparse and more robust than
first order information (Schu?tze, 1998). We intro-
duce a cluster validation based unsupervised fea-
ture wrapper to remove noises in contextual words,
which works by measuring the consistency between
cluster structures estimated from disjoint data sub-
sets in selected feature space. It is based on the
assumption that if selected feature subset is impor-
tant and complete, cluster structure estimated from
data subset in this feature space should be stable
and robust against random sampling. After deter-
mination of important contextual words, we use a
Gaussian mixture model (GMM) based clustering
algorithm (Bouman et al, 1998) to estimate cluster
structure and cluster number by minimizing Min-
imum Description Length (MDL) criterion (Ris-
sanen, 1978). We construct several subsets from
widely used benchmark corpus as test data. Experi-
mental results show that our algorithm (FSGMM )
can find important feature subset, estimate cluster
number and achieve better performance compared
with CGD algorithm.
This paper is organized as follows. In section
2 we will introduce our word sense learning al-
gorithm, which incorporates unsupervised feature
selection and model order identification technique.
Then we will give out the experimental results of
our algorithm and discuss some findings from these
results in section 3. Section 4 will be devoted to
a brief review of related efforts on word sense dis-
crimination. In section 5 we will conclude our work
and suggest some possible improvements.
2 Learning Procedure
2.1 Feature selection
Feature selection for word sense learning is to find
important contextual words which help to discrim-
inate senses of target word without using class la-
bels in data set. This problem can be generalized
as selecting important feature subset in an unsuper-
vised manner. Many unsupervised feature selection
algorithms have been presented, which can be cate-
gorized as feature filter (Dash et al, 2002; Talav-
era, 1999) and feature wrapper (Dy and Brodley,
2000; Law et al, 2002; Mitra et al, 2002; Modha
and Spangler, 2003).
In this paper we propose a cluster valida-
tion based unsupervised feature subset evaluation
method. Cluster validation has been used to solve
model order identification problem (Lange et al,
2002; Levine and Domany, 2001). Table 1 gives
out our feature subset evaluation algorithm. If some
features in feature subset are noises, the estimated
cluster structure on data subset in selected feature
space is not stable, which is more likely to be the
artifact of random splitting. Then the consistency
between cluster structures estimated from disjoint
data subsets will be lower. Otherwise the estimated
cluster structures should be more consistent. Here
we assume that splitting does not eliminate some of
the underlying modes in data set.
For comparison of different clustering structures,
predictors are constructed based on these clustering
solutions, then we use these predictors to classify
the same data subset. The agreement between class
memberships computed by different predictors can
be used as the measure of consistency between clus-
ter structures. We use the stability measure (Lange
et al, 2002) (given in Table 1) to assess the agree-
ment between class memberships.
For each occurrence, one strategy is to construct
its second order context vector by summing the vec-
tors of contextual words, then let the feature selec-
tion procedure start to work on these second order
contextual vectors to select features. However, since
the sense associated with a word?s occurrence is al-
ways determined by very few feature words in its
contexts, it is always the case that there exist more
noisy words than the real features in the contexts.
So, simply summing the contextual word?s vectors
together may result in noise-dominated second or-
der context vectors.
To deal with this problem, we extend the feature
selection procedure further to the construction of
second order context vectors: to select better feature
words in contexts to construct better second order
context vectors enabling better feature selection.
Since the sense associated with a word?s occur-
rence is always determined by some feature words
in its contexts, it is reasonable to suppose that the
selected features should cover most of occurrences.
Formally, let coverage(D,T ) be the coverage rate
of the feature set T with respect to a set of con-
texts D, i.e., the ratio of the number of the occur-
rences with at least one feature in their local con-
texts against the total number of occurrences, then
we assume that coverage(D,T ) ? ? . In practice,
we set ? = 0.9.
This assumption also helps to avoid the bias to-
ward the selection of fewer features, since with
fewer features, there are more occurrences without
features in contexts, and their context vectors will
be zero valued, which tends to result in more stable
cluster structure.
Let D be a set of local contexts of occurrences of
target word, then D = {di}Ni=1, where di represents
local context of the i-th occurrence, and N is the
total number of this word?s occurrences.
W is used to denote bag of words occurring in
context set D, then W = {wi}Mi=1, where wi de-
notes a word occurring in D, and M is the total
number of different contextual words.
Let V denote a M ? M second-order co-
occurrence symmetric matrix. Suppose that the i-th
, 1 ? i ? M , row in the second order matrix corre-
sponds to word wi and the j-th , 1 ? j ? M , col-
umn corresponds to word wj , then the entry speci-
fied by i-th row and j-th column records the number
of times that word wi occurs close to wj in corpus.
We use v(wi) to represent the word vector of con-
textual word wi, which is the i-th row in matrix V .
HT is a weight matrix of contextual word subset
T , T ? W . Then each entry hi,j represents the
weight of word wj in di, wj ? T , 1 ? i ? N . We
use binary term weighting method to derive context
vectors: hi,j = 1 if word wj occurs in di, otherwise
zero.
Let CT = {cTi }Ni=1 be a set of context vectors in
feature space T , where cTi is the context vector of
the i-th occurrence. cTi is defined as:
cTi =
?
j
(hi,jv(wj)), wj ? T, 1 ? i ? N. (1)
The feature subset selection in word set W can be
formulated as:
T? = argmax
T
{criterion(T,H, V, q)}, T ? W, (2)
subject to coverage(D,T ) ? ? , where T? is the op-
timal feature subset, criterion is the cluster valida-
tion based evaluation function (the function in Ta-
ble 1), q is the resampling frequency for estimate
of stability, and coverage(D,T ) is the proportion
of contexts with occurrences of features in T . This
constrained optimization results in a solution which
maximizes the criterion and meets the given con-
straint at the same time. In this paper we use se-
quential greedy forward floating search (Pudil et al,
1994) in sorted word list based on ?2 or local fre-
quency criterion. We set l = 1, m = 1, where l is
plus step, and m is take-away step.
2.2 Clustering with order identification
After feature selection, we employ a Gaussian mix-
ture modelling algorithm, Cluster (Bouman et al,
Table 1: Unsupervised Feature Subset Evaluation Algorithm.
Intuitively, for a given feature subset T , we iteratively split data
set into disjoint halves, and compute the agreement of cluster-
ing solutions estimated from these sets using stability measure.
The average of stability over q resampling is the estimation of
the score of T .
Function criterion(T , H , V , q)
Input parameter: feature subset T , weight matrix H ,
second order co-occurrence matrix V , resampling
frequency q;
(1) ST = 0;
(2) For i = 1 to q do
(2.1) Randomly split CT into disjoint halves, denoted
as CTA and CTB ;
(2.2) Estimate GMM parameter and cluster number on CTA
using Cluster, and the parameter set is denoted as ??A;
The solution ??A can be used to construct a predictor
?A;
(2.3) Estimate GMM parameter and cluster number on CTB
using Cluster, and the parameter set is denoted as ??B ,
The solution ??B can be used to construct a predictor
?B ;
(2.4) Classify CTB using ?A and ?B ;
The class labels assigned by ?A and ?B are denoted
as LA and LB ;
(2.5) ST+ = maxpi 1|CTB |
?
i 1{pi(LA(cTBi)) = LB(cTBi)},
where pi denotes possible permutation relating indices
between LA and LB , and cTBi ? CTB ;
(3) ST = 1qST ;
(4) Return ST ;
1998), to estimate cluster structure and cluster num-
ber. Let Y = {yn}Nn=1 be a set of M dimen-
sional vectors to be modelled by GMM. Assuming
that this model has K subclasses, let pik denote the
prior probability of subclass k, ?k denote the M di-
mensional mean vector for subclass k, Rk denote
the M ?M dimensional covariance matrix for sub-
class k, 1 ? k ? K. The subclass label for pixel
yn is represented by xn. MDL criterion is used
for GMM parameter estimation and order identifi-
cation, which is given by:
MDL(K, ?) = ?
N?
n=1
log (pyn|xn(yn|?)) +
1
2L log (NM),
(3)
pyn|xn(yn|?) =
K?
k=1
pyn|xn(yn|k, ?)pik, (4)
L = K(1 +M + (M + 1)M2 )? 1, (5)
The log likelihood measures the goodness of fit of
a model to data sample, while the second term pe-
nalizes complex model. This estimator works by at-
tempting to find a model order with minimum code
length to describe the data sample Y and parameter
set ?.
If the cluster number is fixed, the estimation of
GMM parameter can be solved using EM algorithm
to address this type of incomplete data problem
(Dempster et al, 1977). The initialization of mix-
ture parameter ?(1) is given by:
pi(1)k =
1
Ko (6)
?(1)k = yn, where n = b(k? 1)(N ? 1)/(Ko? 1)c+1 (7)
R(1)k =
1
N ?
N
n=1ynytn (8)
Ko is a given initial subclass number.
Then EM algorithm is used to estimate model pa-
rameters by minimizing MDL:
E-step: re-estimate the expectations based on pre-
vious iteration:
pxn|yn(k|yn, ?(i)) =
pyn|xn(yn|k, ?(i))pik?K
l=1(pyn|xn(yn|l, ?(i))pil)
, (9)
M-step: estimate the model parameter ?(i) to
maximize the log-likelihood in MDL:
Nk =
N?
n=1
pxn|yn(k|yn, ?(i)) (10)
pik = NkN (11)
?k =
1
Nk
N?
n=1
ynpxn|yn(k|yn, ?(i)) (12)
Rk = 1Nk
N?
n=1
(yn ? ?k)(yn ? ?k)tpxn|yn(k|yn, ?(i))
(13)
pyn|xn(yn|k, ?(i)) =
1
(2pi)M/2 |Rk|
?1/2 exp{?} (14)
? = ?12(yn ? ?k)
tR?1k (yn ? ?k) (15)
The EM iteration is terminated when the change
of MDL(K, ?) is less than ?:
? = 1100(1 +M +
(M + 1)M
2 )log(NM) (16)
For inferring the cluster number, EM algorithm
is applied for each value of K, 1 ? K ? Ko, and
the value K? which minimizes the value of MDL
is chosen as the correct cluster number. To make
this process more efficient, two cluster pair l and m
are selected to minimize the change in MDL crite-
ria when reducing K to K ? 1. These two clusters
l and m are then merged. The resulting parameter
set is chosen as an initial condition for EM iteration
with K ? 1 subclasses. This operation will avoid a
complete minimization with respect to pi, ?, and R
for each value of K.
Table 2: Four ambiguous words, their senses and frequency
distribution of each sense.
Word Sense Percentage
hard not easy (difficult) 82.8%
(adjective) not soft (metaphoric) 9.6%
not soft (physical) 7.6%
interest money paid for the use of money 52.4%
a share in a company or business 20.4%
readiness to give attention 14%
advantage, advancement or favor 9.4%
activity that one gives attention to 3.6%
causing attention to be given to 0.2%
line product 56%
(noun) telephone connection 10.6%
written or spoken text 9.8%
cord 8.6%
division 8.2%
formation 6.8%
serve supply with food 42.6%
(verb) hold an office 33.6%
function as something 16%
provide a service 7.8%
3 Experiments and Evaluation
3.1 Test data
We constructed four datasets from hand-tagged cor-
pus 1 by randomly selecting 500 instances for each
ambiguous word - ?hard?, ?interest?, ?line?, and
?serve?. The details of these datasets are given in
Table 2. Our preprocessing included lowering the
upper case characters, ignoring all words that con-
tain digits or non alpha-numeric characters, remov-
ing words from a stop word list, and filtering out
low frequency words which appeared only once in
entire set. We did not use stemming procedure.
The sense tags were removed when they were used
by FSGMM and CGD. In evaluation procedure,
these sense tags were used as ground truth classes.
A second order co-occurrence matrix for English
words was constructed using English version of
Xinhua News (Jan. 1998-Dec. 1999). The win-
dow size for counting second order co-occurrence
was 50 words.
3.2 Evaluation method for feature selection
For evaluation of feature selection, we used mutual
information between feature subset and class label
set to assess the importance of selected feature sub-
set. Our assessment measure is defined as:
M(T ) = 1|T |
?
w?T
?
l?L
p(w, l)log p(w, l)p(w)p(l) , (17)
where T is the feature subset to be evaluated, T ?
W , L is class label set, p(w, l) is the joint distri-
bution of two variables w and l, p(w) and p(l) are
marginal probabilities. p(w, l) is estimated based
1http://www.d.umn.edu/?tpederse/data.html
on contingency table of contextual word set W and
class label set L. Intuitively, if M(T1) > M(T2),
T1 is more important than T2 since T1 contains more
information about L.
3.3 Evaluation method for clustering result
When assessing the agreement between clustering
result and hand-tagged senses (ground truth classes)
in benchmark data, we encountered the difficulty
that there was no sense tag for each cluster.
In (Lange et al, 2002), they defined a permu-
tation procedure for calculating the agreement be-
tween two cluster memberships assigned by differ-
ent unsupervised learners. In this paper, we applied
their method to assign different sense tags to only
min(|U |, |C|) clusters by maximizing the accuracy,
where |U | is the number of clusters, and |C| is the
number of ground truth classes. The underlying as-
sumption here is that each cluster is considered as
a class, and for any two clusters, they do not share
same class labels. At most |C| clusters are assigned
sense tags, since there are only |C| classes in bench-
mark data.
Given the contingency table Q between clusters
and ground truth classes, each entry Qi,j gives the
number of occurrences which fall into both the i-
th cluster and the j-th ground truth class. If |U | <
|C|, we constructed empty clusters so that |U | =
|C|. Let ? represent a one-to-one mapping function
from C to U . It means that ?(j1) 6= ?(j2) if j1 6=
j2 and vice versa, 1 ? j1, j2 ? |C|. Then ?(j)
is the index of the cluster associated with the j-th
class. Searching a mapping function to maximize
the accuracy of U can be formulated as:
?? = argmax
?
|C|?
j=1
Q?(j),j . (18)
Then the accuracy of solution U is given by
Accuracy(U) =
?
j Q??(j),j?
i,j Qi,j
. (19)
In fact,
?
i,j Qi,j is equal to N , the number of
occurrences of target word in test set.
3.4 Experiments and results
For each dataset, we tested following procedures:
CGDterm:We implemented the context group
discrimination algorithm. Top max(|W | ?
20%, 100) words in contextual word list was se-
lected as features using frequency or ?2 based rank-
ing. Then k-means clustering2 was performed on
context vector matrix using normalized Euclidean
distance. K-means clustering was repeated 5 times
2We used k-means function in statistics toolbox of Matlab.
and the partition with best quality was chosen as fi-
nal result. The number of clusters used by k-means
was set to be identical with the number of ground
truth classes. We tested CGDterm using various
word vector weighting methods when deriving con-
text vectors, ex. binary, idf , tf ? idf .
CGDSV D: The context vector matrix was de-
rived using same method in CGDterm. Then k-
means clustering was conducted on latent seman-
tic space transformed from context vector matrix,
using normalized Euclidean distance. Specifically,
context vectors were reduced to 100 dimensions us-
ing SVD. If the dimension of context vector was
less than 100, all of latent semantic vectors with
non-zero eigenvalue were used for subsequent clus-
tering. We also tested it using different weighting
methods, ex. binary, idf , tf ? idf .
FSGMM : We performed cluster validation
based feature selection in feature set used by CGD.
Then Cluster algorithm was used to group target
word?s instances using Euclidean distance measure.
? was set as 0.90 in feature subset search procedure.
The random splitting frequency is set as 10 for es-
timation of the score of feature subset. The initial
subclass number was 20 and full covariance matrix
was used for parameter estimation of each subclass.
For investigating the effect of different context
window size on the performance of three proce-
dures, we tested these procedures using various con-
text window sizes: ?1, ?5, ?15, ?25, and all of
contextual words. The average length of sentences
in 4 datasets is 32 words before preprocessing. Per-
formance on each dataset was assessed by equation
19.
The scores of feature subsets selected by
FSGMM and CGD are listed in Table 3 and
4. The average accuracy of three procedures with
different feature ranking and weighting method is
given in Table 5. Each figure is the average over 5
different context window size and 4 datasets. We
give out the detailed results of these three proce-
dures in Figure 1. Several results should be noted
specifically:
From Table 3 and 4, we can find that FSGMM
achieved better score on mutual information (MI)
measure than CGD over 35 out of total 40 cases.
This is the evidence that our feature selection pro-
cedure can remove noise and retain important fea-
tures.
As it was shown in Table 5, with both ?2 and
freq based feature ranking, FSGMM algorithm
performed better than CGDterm and CGDSV D if
we used average accuracy to evaluate their per-
formance. Specifically, with ?2 based feature
ranking, FSGMM attained 55.4% average accu-
racy, while the best average accuracy of CGDterm
and CGDSV D were 40.9% and 51.3% respec-
tively. With freq based feature ranking, FSGMM
achieved 51.2% average accuracy, while the best av-
erage accuracy of CGDterm and CGDSV D were
45.1% and 50.2%.
The automatically estimated cluster numbers by
FSGMM over 4 datasets are given in Table 6.
The estimated cluster number was 2 ? 4 for ?hard?,
3 ? 6 for ?interest?, 3 ? 6 for ?line?, and 2 ? 4
for ?serve?. It is noted that the estimated cluster
number was less than the number of ground truth
classes in most cases. There are some reasons for
this phenomenon. First, the data is not balanced,
which may lead to that some important features can-
not be retrieved. For example, the fourth sense of
?serve?, and the sixth sense of ?line?, their corre-
sponding features are not up to the selection criteria.
Second, some senses can not be distinguished using
only bag-of-words information, and their difference
lies in syntactic information held by features. For
example, the third sense and the sixth sense of ?in-
terest? may be distinguished by syntactic relation of
feature words, while the bag of feature words occur-
ring in their context are similar. Third, some senses
are determined by global topics, rather than local
contexts. For example, according to global topics, it
may be easier to distinguish the first and the second
sense of ?interest?.
Figure 2 shows the average accuracy over three
procedures in Figure 1 as a function of context
window size for 4 datasets. For ?hard?, the per-
formance dropped as window size increased, and
the best accuracy(77.0%) was achieved at win-
dow size 1. For ?interest?, sense discrimination
did not benefit from large window size and the
best accuracy(40.1%) was achieved at window size
5. For ?line?, accuracy dropped when increas-
ing window size and the best accuracy(50.2%) was
achieved at window size 1. For ?serve?, the per-
formance benefitted from large window size and the
best accuracy(46.8%) was achieved at window size
15.
In (Leacock et al, 1998), they used Bayesian ap-
proach for sense disambiguation of three ambiguous
words, ?hard?, ?line?, and ?serve?, based on cues
from topical and local context. They observed that
local context was more reliable than topical context
as an indicator of senses for this verb and adjective,
but slightly less reliable for this noun. Compared
with their conclusion, we can find that our result
is consistent with it for ?hard?. But there is some
differences for verb ?serve? and noun ?line?. For
Table 3: Mutual information between feature subset and class
label with ?2 based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4018 80 0.4300
15 100 0.1362 80 0.1416
25 133 0.0997 102 0.1003
all 145 0.0937 107 0.0890
interest 1 64 1.9697 55 2.0639
5 100 0.3234 89 0.3355
15 157 0.1558 124 0.1531
25 190 0.1230 138 0.1267
all 200 0.1163 140 0.1191
line 1 39 4.2089 32 4.6456
5 100 0.4628 84 0.4871
15 183 0.1488 128 0.1429
25 263 0.1016 163 0.0962
all 351 0.0730 192 0.0743
serve 1 22 6.8169 20 6.7043
5 100 0.5057 85 0.5227
15 188 0.2078 164 0.2094
25 255 0.1503 225 0.1536
all 320 0.1149 244 0.1260
Table 4: Mutual information between feature subset and class
label with freq based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4194 80 0.4832
15 100 0.1647 80 0.1774
25 133 0.1150 102 0.1259
all 145 0.1064 107 0.1269
interest 1 64 1.9697 55 2.7051
5 100 0.6015 89 0.8309
15 157 0.2526 124 0.3495
25 190 0.1928 138 0.2982
all 200 0.1811 140 0.2699
line 1 39 4.2089 32 4.4606
5 100 0.6895 84 0.7816
15 183 0.2301 128 0.2929
25 263 0.1498 163 0.2181
all 351 0.1059 192 0.1630
serve 1 22 6.8169 20 7.0021
5 100 0.7045 85 0.8422
15 188 0.2763 164 0.3418
25 255 0.1901 225 0.2734
all 320 0.1490 244 0.2309
?serve?, the possible reason is that we do not use
position of local word and part of speech informa-
tion, which may deteriorate the performance when
local context(? 5 words) is used. For ?line?, the
reason might come from the feature subset, which
is not good enough to provide improvement when
Table 5: Average accuracy of three procedures with various
settings over 4 datasets.
Algorithm Feature Feature Average
ranking weighting accuracy
method method
FSGMM ?2 binary 0.554
CGDterm ?2 binary 0.404
CGDterm ?2 idf 0.407
CGDterm ?2 tf ? idf 0.409
CGDSVD ?2 binary 0.513
CGDSVD ?2 idf 0.512
CGDSVD ?2 tf ? idf 0.508
FSGMM freq binary 0.512
CGDterm freq binary 0.451
CGDterm freq idf 0.437
CGDterm freq tf ? idf 0.447
CGDSVD freq binary 0.502
CGDSVD freq idf 0.498
CGDSVD freq tf ? idf 0.485
Table 6: Automatically determined mixture component num-
ber.
Word Context Model Model
window order order
size with ?2 with freq
hard 1 3 4
5 2 2
15 2 3
25 2 3
all 2 3
interest 1 5 4
5 3 4
15 4 6
25 4 6
all 3 4
line 1 5 6
5 4 3
15 5 4
25 5 4
all 3 4
serve 1 3 3
5 3 4
15 3 3
25 3 3
all 2 4
context window size is no less than 5.
4 Related Work
Besides the two works (Pantel and Lin, 2002;
Schu?tze, 1998), there are other related efforts on
word sense discrimination (Dorow and Widdows,
2003; Fukumoto and Suzuki, 1999; Pedersen and
Bruce, 1997).
In (Pedersen and Bruce, 1997), they described an
experimental comparison of three clustering algo-
rithms for word sense discrimination. Their feature
sets included morphology of target word, part of
speech of contextual words, absence or presence of
particular contextual words, and collocation of fre-
0 1 5 15 25 all0.4
0.5
0.6
0.7
0.8
0.9
Hard dataset
Acc
ura
cy
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
Acc
ura
cy
Interest dataset
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
0.7
Line dataset
Acc
ura
cy
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
Serve dataset
Acc
ura
cy
Figure 1: Results for three procedures over 4 datases. The
horizontal axis corresponds to the context window size. Solid
line represents the result of FSGMM + binary, dashed line
denotes the result of CGDSVD + idf , and dotted line is the
result of CGDterm + idf . Square marker denotes ?2 based
feature ranking, while cross marker denotes freq based feature
ranking.
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Ave
rage
 Acc
urac
y
Hard datasetInterest datasetLine datasetServe dataset
Figure 2: Average accuracy over three procedures in Figure
1 as a function of context window size (horizontal axis) for 4
datasets.
quent words. Then occurrences of target word were
grouped into a pre-defined number of clusters. Sim-
ilar with many other algorithms, their algorithm also
required the cluster number to be provided.
In (Fukumoto and Suzuki, 1999), a term weight
learning algorithm was proposed for verb sense dis-
ambiguation, which can automatically extract nouns
co-occurring with verbs and identify the number of
senses of an ambiguous verb. The weakness of their
method is to assume that nouns co-occurring with
verbs are disambiguated in advance and the number
of senses of target verb is no less than two.
The algorithm in (Dorow and Widdows, 2003)
represented target noun word, its neighbors and
their relationships using a graph in which each node
denoted a noun and two nodes had an edge between
them if they co-occurred with more than a given
number of times. Then senses of target word were
iteratively learned by clustering the local graph of
similar words around target word. Their algorithm
required a threshold as input, which controlled the
number of senses.
5 Conclusion and Future Work
Our word sense learning algorithm combined two
novel ingredients: feature selection and order iden-
tification. Feature selection was formalized as a
constrained optimization problem, the output of
which was a set of important features to determine
word senses. Both cluster structure and cluster num-
ber were estimated by minimizing a MDL crite-
rion. Experimental results showed that our algo-
rithm can retrieve important features, estimate clus-
ter number automatically, and achieve better per-
formance in terms of average accuracy than CGD
algorithm which required cluster number as input.
Our word sense learning algorithm is unsupervised
in two folds: no requirement of sense tagged data,
and no requirement of predefinition of sense num-
ber, which enables the automatic discovery of word
senses from free text.
In our algorithm, we treat bag of words in lo-
cal contexts as features. It has been shown that
local collocations and morphology of target word
play important roles in word sense disambiguation
or discrimination (Leacock et al, 1998; Widdows,
2003). It is necessary to incorporate these more
structural information to improve the performance
of word sense learning.
References
Bouman, C. A., Shapiro, M., Cook, G. W., Atkins,
C. B., & Cheng, H. (1998) Cluster: An
Unsupervsied Algorithm for Modeling Gaus-
sian Mixtures. http://dynamo.ecn.purdue.edu/
?bouman/software/cluster/.
Dash, M., Choi, K., Scheuermann, P., & Liu, H. (2002)
Feature Selection for Clustering - A Filter Solution.
Proc. of IEEE Int. Conf. on Data Mining(pp. 115?
122).
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977)
Maximum likelihood from incomplete data using the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(B).
Dorow, B, & Widdows, D. (2003) Discovering Corpus-
Specific Word Senses. Proc. of the 10th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics, Conference Companion (research
notes and demos)(pp.79?82).
Dy, J. G., & Brodley, C. E. (2000) Feature Subset Selec-
tion and Order Identification for Unsupervised Learn-
ing. Proc. of the 17th Int. Conf. on Machine Learn-
ing(pp. 247?254).
Fukumoto, F., & Suzuki, Y. (1999) Word Sense Disam-
biguation in Untagged Text Based on Term Weight
Learning. Proc. of the 9th Conf. of European Chapter
of the Association for Computational Linguistics(pp.
209?216).
Ide, N., & Ve?ronis, J. (1998) Word Sense Disambigua-
tion: The State of the Art. Computational Linguistics,
24:1, 1?41.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. (2002)
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Law, M. H., Figueiredo, M., & Jain, A. K. (2002) Fea-
ture Selection in Mixture-Based Clustering. Advances
in Neural Information Processing Systems 15.
Leacock, C., Chodorow, M., & Miller A. G. (1998) Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Levine, E., & Domany, E. (2001) Resampling Method
for Unsupervised Estimation of Cluster Validity. Neu-
ral Computation, Vol. 13, 2573?2593.
Mitra, P., Murthy, A. C., & Pal, K. S. (2002) Unsu-
pervised Feature Selection Using Feature Similarity.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24:4, 301?312.
Modha, D. S., & Spangler, W. S. (2003) Feature Weight-
ing in k-Means Clustering. Machine Learning, 52:3,
217?237.
Pantel, P. & Lin, D. K. (2002) Discovering Word Senses
from Text. Proc. of ACM SIGKDD Conf. on Knowl-
edge Discovery and Data Mining(pp. 613-619).
Pedersen, T., & Bruce, R. (1997) Distinguishing Word
Senses in Untagged Text. Proceedings of the 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing(pp. 197?207).
Pudil, P., Novovicova, J., & Kittler, J. (1994) Floating
Search Methods in Feature Selection. Pattern Recog-
nigion Letters, Vol. 15, 1119-1125.
Rissanen, J. (1978) Modeling by Shortest Data Descrip-
tion. Automatica, Vol. 14, 465?471.
Schu?tze, H. (1998) Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Talavera, L. (1999) Feature Selection as a Preprocessing
Step for Hierarchical Clustering. Proc. of the 16th Int.
Conf. on Machine Learning(pp. 389?397).
Widdows, D. (2003) Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. Proc. of the Human Language Tech-
nology / Conference of the North American Chapter
of the Association for Computational Linguistics(pp.
276?283).
Proceedings of the 43rd Annual Meeting of the ACL, pages 165?172,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Improving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information
Xiaofeng Yang?? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
In this paper we focus on how to improve
pronoun resolution using the statistics-
based semantic compatibility information.
We investigate two unexplored issues that
influence the effectiveness of such in-
formation: statistics source and learning
framework. Specifically, we for the first
time propose to utilize the web and the
twin-candidate model, in addition to the
previous combination of the corpus and
the single-candidate model, to compute
and apply the semantic information. Our
study shows that the semantic compatibil-
ity obtained from the web can be effec-
tively incorporated in the twin-candidate
learning model and significantly improve
the resolution of neutral pronouns.
1 Introduction
Semantic compatibility is an important factor for
pronoun resolution. Since pronouns, especially neu-
tral pronouns, carry little semantics of their own,
the compatibility between an anaphor and its an-
tecedent candidate is commonly evaluated by ex-
amining the relationships between the candidate and
the anaphor?s context, based on the statistics that the
corresponding predicate-argument tuples occur in a
particular large corpus. Consider the example given
in the work of Dagan and Itai (1990):
(1) They know full well that companies held tax
money aside for collection later on the basis
that the government said it1 was going to col-
lect it2.
For anaphor it1, the candidate government should
have higher semantic compatibility than money be-
cause government collect is supposed to occur more
frequently than money collect in a large corpus. A
similar pattern could also be observed for it2.
So far, the corpus-based semantic knowledge has
been successfully employed in several anaphora res-
olution systems. Dagan and Itai (1990) proposed
a heuristics-based approach to pronoun resolu-
tion. It determined the preference of candidates
based on predicate-argument frequencies. Recently,
Bean and Riloff (2004) presented an unsupervised
approach to coreference resolution, which mined
the co-referring NP pairs with similar predicate-
arguments from a large corpus using a bootstrapping
method.
However, the utility of the corpus-based se-
mantics for pronoun resolution is often argued.
Kehler et al (2004), for example, explored the
usage of the corpus-based statistics in supervised
learning based systems, and found that such infor-
mation did not produce apparent improvement for
the overall pronoun resolution. Indeed, existing
learning-based approaches to anaphor resolution
have performed reasonably well using limited
and shallow knowledge (e.g., Mitkov (1998),
Soon et al (2001), Strube and Muller (2003)).
Could the relatively noisy semantic knowledge give
us further system improvement?
In this paper we focus on improving pronominal
anaphora resolution using automatically computed
semantic compatibility information. We propose to
enhance the utility of the statistics-based knowledge
from two aspects:
Statistics source. Corpus-based knowledge usu-
ally suffers from data sparseness problem. That is,
many predicate-argument tuples would be unseen
even in a large corpus. A possible solution is the
165
web. It is believed that the size of the web is thou-
sands of times larger than normal large corpora, and
the counts obtained from the web are highly corre-
lated with the counts from large balanced corpora
for predicate-argument bi-grams (Keller and Lapata,
2003). So far the web has been utilized in nominal
anaphora resolution (Modjeska et al, 2003; Poesio
et al, 2004) to determine the semantic relation be-
tween an anaphor and candidate pair. However, to
our knowledge, using the web to help pronoun reso-
lution still remains unexplored.
Learning framework. Commonly, the predicate-
argument statistics is incorporated into anaphora res-
olution systems as a feature. What kind of learn-
ing framework is suitable for this feature? Previous
approaches to anaphora resolution adopt the single-
candidate model, in which the resolution is done on
an anaphor and one candidate at a time (Soon et al,
2001; Ng and Cardie, 2002). However, as the pur-
pose of the predicate-argument statistics is to eval-
uate the preference of the candidates in semantics,
it is possible that the statistics-based semantic fea-
ture could be more effectively applied in the twin-
candidate (Yang et al, 2003) that focusses on the
preference relationships among candidates.
In our work we explore the acquisition of the se-
mantic compatibility information from the corpus
and the web, and the incorporation of such semantic
information in the single-candidate model and the
twin-candidate model. We systematically evaluate
the combinations of different statistics sources and
learning frameworks in terms of their effectiveness
in helping the resolution. Results on the MUC data
set show that for neutral pronoun resolution in which
an anaphor has no specific semantic category, the
web-based semantic information would be the most
effective when applied in the twin-candidate model:
Not only could such a system significantly improve
the baseline without the semantic feature, it also out-
performs the system with the combination of the cor-
pus and the single-candidate model (by 11.5% suc-
cess).
The rest of this paper is organized as follows. Sec-
tion 2 describes the acquisition of the semantic com-
patibility information from the corpus and the web.
Section 3 discusses the application of the statistics
in the single-candidate and twin-candidate learning
models. Section 4 gives the experimental results,
and finally, Section 5 gives the conclusion.
2 Computing the Statistics-based Semantic
Compatibility
In this section, we introduce in detail how to com-
pute the semantic compatibility, using the predicate-
argument statistics obtained from the corpus or the
web.
2.1 Corpus-Based Semantic Compatibility
Three relationships, possessive-noun, subject-verb
and verb-object, are considered in our work. Be-
fore resolution a large corpus is prepared. Doc-
uments in the corpus are processed by a shallow
parser that could generate predicate-argument tuples
of the above three relationships1.
To reduce data sparseness, the following steps are
applied in each resulting tuple, automatically:
? Only the nominal or verbal heads are retained.
? Each Named-Entity (NE) is replaced by a com-
mon noun which corresponds to the seman-
tic category of the NE (e.g. ?IBM? ? ?com-
pany?) 2.
? All words are changed to their base morpho-
logic forms (e.g. ?companies ? company?).
During resolution, for an encountered anaphor,
each of its antecedent candidates is substituted with
the anaphor . According to the role and type of the
anaphor in its context, a predicate-argument tuple is
extracted and the above three steps for data-sparse
reduction are applied. Consider the sentence (1),
for example. The anaphors ?it1? and ?it2? indicate
a subject verb and verb object relationship, respec-
tively. Thus, the predicate-argument tuples for the
two candidates ?government? and ?money? would
be (collect (subject government)) and (collect (sub-
ject money)) for ?it1?, and (collect (object govern-
ment)) and (collect (object money)) for ?it2?.
Each extracted tuple is searched in the prepared
tuples set of the corpus, and the times the tuple oc-
curs are calculated. For each candidate, its semantic
1The possessive-noun relationship involves the forms like
?NP2 of NP1? and ?NP1?s NP2?.
2In our study, the semantic category of a NE is identified
automatically by the pre-processing NE recognition component.
166
compatibility with the anaphor could be represented
simply in terms of frequency
StatSem(candi, ana) = count(candi, ana) (1)
where count(candi, ana) is the count of the tuple
formed by candi and ana, or alternatively, in terms
of conditional probability (P (candi, ana|candi)),
where the count of the tuple is divided by the count
of the single candidate in the corpus. That is
StatSem(candi, ana) = count(candi, ana)count(candi) (2)
In this way, the statistics would not bias candidates
that occur frequently in isolation.
2.2 Web-Based Semantic Compatibility
Unlike documents in normal corpora, web pages
could not be preprocessed to generate the predicate-
argument reserve. Instead, the predicate-argument
statistics has to be obtained via a web search engine
like Google and Altavista. For the three types of
predicate-argument relationships, queries are con-
structed in the forms of ?NPcandi VP? (for subject-
verb), ?VP NPcandi? (for verb-object), and ?NPcandi
?s NP? or ?NP of NPcandi? (for possessive-noun).
Consider the following sentence:
(2) Several experts suggested that IBM?s account-
ing grew much more liberal since the mid 1980s
as its business turned sour.
For the pronoun ?its? and the candidate ?IBM?, the
two generated queries are ?business of IBM? and
?IBM?s business?.
To reduce data sparseness, in an initial query only
the nominal or verbal heads are retained. Also, each
NE is replaced by the corresponding common noun.
(e.g, ?IBM?s business? ? ?company?s business? and
?business of IBM? ? ?business of company?).
A set of inflected queries is generated by ex-
panding a term into all its possible morphologi-
cal forms. For example, in Sentence (1), ?collect
money? becomes ?collected|collecting|... money?,
and in (2) ?business of company? becomes ?business
of company|companies?). Besides, determiners are
inserted for every noun. If the noun is the candidate
under consideration, only the definite article the is
inserted. For other nouns, instead, a/an, the and the
empty determiners (for bare plurals) would be added
(e.g., ?the|a business of the company|companies?).
Queries are submitted to a particular web search
engine (Google in our study). All queries are per-
formed as exact matching. Similar to the corpus-
based statistics, the compatibility for each candidate
and anaphor pair could be represented using either
frequency (Eq. 1) or probability (Eq. 2) metric. In
such a situation, count(candi, ana) is the hit num-
ber of the inflected queries returned by the search
engine, while count(candi) is the hit number of the
query formed with only the head of the candidate
(i.e.,?the + candi?).
3 Applying the Semantic Compatibility
In this section, we discuss how to incorporate the
statistics-based semantic compatibility for pronoun
resolution, in a machine learning framework.
3.1 The Single-Candidate Model
One way to utilize the semantic compatibility is to
take it as a feature under the single-candidate learn-
ing model as employed by Ng and Cardie (2002).
In such a learning model, each training or testing
instance takes the form of i{C, ana}, where ana is
the possible anaphor and C is its antecedent candi-
date. An instance is associated with a feature vector
to describe their relationships.
During training, for each anaphor in a given text,
a positive instance is created by pairing the anaphor
and its closest antecedent. Also a set of negative in-
stances is formed by pairing the anaphor and each
of the intervening candidates. Based on the train-
ing instances, a binary classifier is generated using a
certain learning algorithm, like C5 (Quinlan, 1993)
in our work.
During resolution, given a new anaphor, a test in-
stance is created for each candidate. This instance is
presented to the classifier, which then returns a pos-
itive or negative result with a confidence value indi-
cating the likelihood that they are co-referent. The
candidate with the highest confidence value would
be selected as the antecedent.
3.2 Features
In our study we only consider those domain-
independent features that could be obtained with low
167
Feature Description
DefNp 1 if the candidate is a definite NP; else 0
Pron 1 if the candidate is a pronoun; else 0
NE 1 if the candidate is a named entity; else 0
SameSent 1 if the candidate and the anaphor is in the same sentence; else 0
NearestNP 1 if the candidate is nearest to the anaphor; else 0
ParalStuct 1 if the candidate has an parallel structure with ana; else 0
FirstNP 1 if the candidate is the first NP in a sentence; else 0
Reflexive 1 if the anaphor is a reflexive pronoun; else 0
Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2:
Male personal pronoun; 3: Female personal pronoun)
StatSem? the statistics-base semantic compatibility of the candidate
SemMag?? the semantic compatibility difference between two competing candidates
Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model
while **ed feature is only for the twin-candidate mode)
computational cost but with high reliability. Table 1
summarizes the features with their respective possi-
ble values. The first three features represent the lex-
ical properties of a candidate. The POS properties
could indicate whether a candidate refers to a hearer-
old entity that would have a higher preference to be
selected as the antecedent (Strube, 1998). SameSent
and NearestNP mark the distance relationships be-
tween an anaphor and the candidate, which would
significantly affect the candidate selection (Hobbs,
1978). FirstNP aims to capture the salience of the
candidate in the local discourse segment. ParalStuct
marks whether a candidate and an anaphor have sim-
ilar surrounding words, which is also a salience fac-
tor for the candidate evaluation (Mitkov, 1998).
Feature StatSem records the statistics-based se-
mantic compatibility computed, from the corpus or
the web, by either frequency or probability metric,
as described in the previous section. If a candidate
is a pronoun, this feature value would be set to that
of its closest nominal antecedent.
As described, the semantic compatibility of a can-
didate is computed under the context of the cur-
rent anaphor. Consider two occurrences of anaphors
?. . . it1 collected . . . ? and ?. . . it2 said . . . ?. As ?NP
collected? should occur less frequently than ?NP
said?, the candidates of it1 would generally have
predicate-argument statistics lower than those of it2.
That is, a positive instance for it1 might bear a lower
semantic feature value than a negative instance for
it2. The consequence is that the learning algorithm
would think such a feature is not that ?indicative?
and reduce its salience in the resulting classifier.
One way to tackle this problem is to normalize the
feature by the frequencies of the anaphor?s context,
e.g., ?count(collected)? and ?count(said)?. This,
however, would require extra calculation. In fact,
as candidates of a specific anaphor share the same
anaphor context, we can just normalize the semantic
feature of a candidate by that of its competitor:
StatSemN (C, ana) = StatSem(C, ana)max
ci?candi set(ana)
StatSem(ci, ana)
The value (0 ? 1) represents the rank of the
semantic compatibility of the candidate C among
candi set(ana), the current candidates of ana.
3.3 The Twin-Candidate Model
Yang et al (2003) proposed an alternative twin-
candidate model for anaphora resolution task. The
strength of such a model is that unlike the single-
candidate model, it could capture the preference re-
lationships between competing candidates. In the
model, candidates for an anaphor are paired and
features from two competing candidates are put to-
gether for consideration. This property could nicely
deal with the above mentioned training problem of
different anaphor contexts, because the semantic
feature would be considered under the current can-
didate set only. In fact, as semantic compatibility is
168
a preference-based factor for anaphor resolution, it
would be incorporated in the twin-candidate model
more naturally.
In the twin-candidate model, an instance takes a
form like i{C1, C2, ana}, where C1 and C2 are two
candidates. We stipulate that C2 should be closer to
ana than C1 in distance. The instance is labelled as
?10? if C1 the antecedent, or ?01? if C2 is.
During training, for each anaphor, we find its
closest antecedent, Cante. A set of ?10? instances,
i{Cante, C, ana}, is generated by pairing Cante and
each of the interning candidates C. Also a set of ?01?
instances, i{C, Cante, ana}, is created by pairing
Cante with each candidate before Cante until another
antecedent, if any, is reached.
The resulting pairwise classifier would return
?10? or ?01? indicating which candidate is preferred
to the other. During resolution, candidates are paired
one by one. The score of a candidate is the total
number of the competitors that the candidate wins
over. The candidate with the highest score would be
selected as the antecedent.
Features The features for the twin-candidate
model are similar to those for the single-candidate
model except that a duplicate set of features has to
be prepared for the additional candidate. Besides,
a new feature, SemMag, is used in place of Stat-
Sem to represent the difference magnitude between
the semantic compatibility of two candidates. Let
mag = StatSem(C1, ana)/StatSem(C2, ana), feature
SemMag is defined as follows,
SemMag(C1, C2, ana) =
{
mag ? 1 : mag >= 1
1?mag?1 : mag < 1
The positive or negative value marks the times that
the statistics of C1 is larger or smaller than C2.
4 Evaluation and Discussion
4.1 Experiment Setup
In our study we were only concerned about the third-
person pronoun resolution. With an attempt to ex-
amine the effectiveness of the semantic feature on
different types of pronouns, the whole resolution
was divided into neutral pronoun (it & they) reso-
lution and personal pronoun (he & she) resolution.
The experiments were done on the newswire do-
main, using MUC corpus (Wall Street Journal ar-
ticles). The training was done on 150 documents
from MUC-6 coreference data set, while the testing
was on the 50 formal-test documents of MUC-6 (30)
and MUC-7 (20). Throughout the experiments, de-
fault learning parameters were applied to the C5 al-
gorithm. The performance was evaluated based on
success, the ratio of the number of correctly resolved
anaphors over the total number of anaphors.
An input raw text was preprocessed automati-
cally by a pipeline of NLP components. The noun
phrase identification and the predicate-argument ex-
traction were done based on the results of a chunk
tagger, which was trained for the shared task of
CoNLL-2000 and achieved 92% accuracy (Zhou et
al., 2000). The recognition of NEs as well as their
semantic categories was done by a HMM based
NER, which was trained for the MUC NE task
and obtained high F-scores of 96.9% (MUC-6) and
94.3% (MUC-7) (Zhou and Su, 2002).
For each anaphor, the markables occurring within
the current and previous two sentences were taken
as the initial candidates. Those with mismatched
number and gender agreements were filtered from
the candidate set. Also, pronouns or NEs that dis-
agreed in person with the anaphor were removed in
advance. For the training set, there are totally 645
neutral pronouns and 385 personal pronouns with
non-empty candidate set, while for the testing set,
the number is 245 and 197.
4.2 The Corpus and the Web
The corpus for the predicate-argument statistics
computation was from the TIPSTER?s Text Re-
search Collection (v1994). Consisting of 173,252
Wall Street Journal articles from the year 1988 to
1992, the data set contained about 76 million words.
The documents were preprocessed using the same
POS tagging and NE-recognition components as in
the pronoun resolution task. Cass (Abney, 1996), a
robust chunker parser was then applied to generate
the shallow parse trees, which resulted in 353,085
possessive-noun tuples, 759,997 verb-object tuples
and 1,090,121 subject-verb tuples.
We examined the capacity of the web and the
corpus in terms of zero-count ratio and count num-
ber. On average, among the predicate-argument tu-
ples that have non-zero corpus-counts, above 93%
have also non-zero web-counts. But the ratio is only
around 40% contrariwise. And for the predicate-
169
Neutral Pron Personal Pron Overall
Learning Model System Corpus Web Corpus Web Corpus Web
baseline 65.7 86.8 75.1
+frequency 67.3 69.9 86.8 86.8 76.0 76.9
Single-Candidate +normalized frequency 66.9 67.8 86.8 86.8 75.8 76.2
+probability 65.7 65.7 86.8 86.8 75.1 75.1
+normalized probability 67.7 70.6 86.8 86.8 76.2 77.8
baseline 73.9 91.9 81.9
Twin-Candidate +frequency 76.7 79.2 91.4 91.9 83.3 84.8
+probability 75.9 78.0 91.4 92.4 82.8 84.4
Table 2: The performance of different resolution systems
Relationship N-Pron P-Pron
Possessive-Noun 0.508 0.517
Verb-Object 0.503 0.526
Subject-Verb 0.619 0.676
Table 3: Correlation between web and corpus counts
on the seen predicate-argument tuples
argument tuples that could be seen in both data
sources, the count from the web is above 2000 times
larger than that from the corpus.
Although much less sparse, the web counts are
significantly noisier than the corpus count since no
tagging, chunking and parsing could be carried out
on the web pages. However, previous study (Keller
and Lapata, 2003) reveals that the large amount of
data available for the web counts could outweigh the
noisy problems. In our study we also carried out a
correlation analysis3 to examine whether the counts
from the web and the corpus are linearly related,
on the predicate-argument tuples that can be seen
in both data sources. From the results listed in Ta-
ble 3, we observe moderately high correlation, with
coefficients ranging from 0.5 to 0.7 around, between
the counts from the web and the corpus, for both
neutral pronoun (N-Pron) and personal pronoun (P-
Pron) resolution tasks.
4.3 System Evaluation
Table 2 summarizes the performance of the systems
with different combinations of statistics sources and
learning frameworks. The systems without the se-
3All the counts were log-transformed and the correlation co-
efficients were evaluated based on Pearsons? r.
mantic feature were used as the baseline. Under the
single-candidate (SC) model, the baseline system
obtains a success of 65.7% and 86.8% for neutral
pronoun and personal pronoun resolution, respec-
tively. By contrast, the twin-candidate (TC) model
achieves a significantly (p ? 0.05, by two-tailed t-
test) higher success of 73.9% and 91.9%, respec-
tively. Overall, for the whole pronoun resolution,
the baseline system under the TC model yields a
success 81.9%, 6.8% higher than SC does4. The
performance is comparable to most state-of-the-art
pronoun resolution systems on the same data set.
Web-based feature vs. Corpus-based feature
The third column of the table lists the results us-
ing the web-based compatibility feature for neutral
pronouns. Under both SC and TC models, incorpo-
ration of the web-based feature significantly boosts
the performance of the baseline: For the best sys-
tem in the SC model and the TC model, the success
rate is improved significantly by around 4.9% and
5.3%, respectively. A similar pattern of improve-
ment could be seen for the corpus-based semantic
feature. However, the increase is not as large as
using the web-based feature: Under the two learn-
ing models, the success rate of the best system with
the corpus-based feature rises by up to 2.0% and
2.8% respectively, about 2.9% and 2.5% less than
that of the counterpart systems with the web-based
feature. The larger size and the better counts of the
web against the corpus, as reported in Section 4.2,
4The improvement against SC is higher than that reported
in (Yang et al, 2003). It should be because we now used 150
training documents rather than 30 ones as in the previous work.
The TC model would benefit from larger training data set as it
uses more features (more than double) than SC.
170
should contribute to the better performance.
Single-candidate model vs. Twin-Candidate
model The difference between the SC and the TC
model is obvious from the table. For the N-Pron
and P-Pron resolution, the systems under TC could
outperform the counterpart systems under SC by
above 5% and 8% success, respectively. In addition,
the utility of the statistics-based semantic feature is
more salient under TC than under SC for N-Pron res-
olution: the best gains using the corpus-based and
the web-based semantic features under TC are 2.9%
and 5.3% respectively, higher than those under the
SC model using either un-normalized semantic fea-
tures (1.6% and 3.3%), or normalized semantic fea-
tures (2.0% and 4.9%). Although under SC, the nor-
malized semantic feature could result in a gain close
to under TC, its utility is not stable: with metric fre-
quency, using the normalized feature performs even
worse than using the un-normalized one. These re-
sults not only affirm the claim by Yang et al (2003)
that the TC model is superior to the SC model for
pronoun resolution, but also indicate that TC is more
reliable than SC in applying the statistics-based se-
mantic feature, for N-Pron resolution.
Web+TC vs. Other combinations The above
analysis has exhibited the superiority of the web
over the corpus, and the TC model over the
SC model. The experimental results also re-
veal that using the the web-based semantic fea-
ture together with the TC model is able to further
boost the resolution performance for neutral pro-
nouns. The system with such a Web+TC combi-
nation could achieve a high success of 79.2%, de-
feating all the other possible combinations. Es-
pecially, it considerably outperforms (up to 11.5%
success) the system with the Corpus+SC combina-
tion, which is commonly adopted in previous work
(e.g., Kehler et al (2004)).
Personal pronoun resolution vs. Neutral pro-
noun resolution Interestingly, the statistics-based
semantic feature has no effect on the resolution of
personal pronouns, as shown in the table 2. We
found in the learned decision trees such a feature
did not occur (SC) or only occurred in bottom nodes
(TC). This should be because personal pronouns
have strong restriction on the semantic category (i.e.,
human) of the candidates. A non-human candidate,
even with a high predicate-argument statistics, could
Feature Group Isolated Combined
SemMag (Web-based) 61.2 61.2
Type+Reflexive 53.1 61.2
ParaStruct 53.1 61.2
Pron+DefNP+InDefNP+NE 57.1 67.8
NearestNP+SameSent 53.1 70.2
FirstNP 65.3 79.2
Table 4: Results of different feature groups under
the TC model for N-pron resolution
SameSent_1 = 0:
:..SemMag > 0:
: :..Pron_2 = 0: 10 (200/23)
: : Pron_2 = 1: ...
: SemMag <= 0:
: :..Pron_2 = 1: 01 (75/1)
: Pron_2 = 0:
: :..SemMag <= -28: 01 (110/19)
: SemMag > -28: ...
SameSent_1 = 1:
:..SameSent_2 = 0: 01 (1655/49)
SameSent_2 = 1:
:..FirstNP_2 = 1: 01 (104/1)
FirstNP_2 = 0:
:..ParaStruct_2 = 1: 01 (3)
ParaStruct_2 = 0:
:..SemMag <= -151: 01 (27/2)
SemMag > -151:...
Figure 1: Top portion of the decision tree learned
under TC model for N-pron resolution (features ended
with ? 1? are for the first candidate C1 and those with ? 2? are
for C2.)
not be used as the antecedent (e.g. company said in
the sentence ?. . . the company . . . he said . . . ?). In
fact, our analysis of the current data set reveals that
most P-Prons refer back to a P-Pron or NE candidate
whose semantic category (human) has been deter-
mined. That is, simply using features NE and Pron
is sufficient to guarantee a high success, and thus the
relatively weak semantic feature would not be taken
in the learned decision tree for resolution.
4.4 Feature Analysis
In our experiment we were also concerned about the
importance of the web-based compatibility feature
(using frequency metric) among the feature set. For
this purpose, we divided the features into groups,
and then trained and tested on one group at a time.
Table 4 lists the feature groups and their respective
results for N-Pron resolution under the TC model.
171
The second column is for the systems with only the
current feature group, while the third column is with
the features combined with the existing feature set.
We see that used in isolation, the semantic compati-
bility feature is able to achieve a success up to 61%
around, just 4% lower than the best indicative fea-
ture FirstNP. In combination with other features, the
performance could be improved by as large as 18%
as opposed to being used alone.
Figure 1 shows the top portion of the pruned deci-
sion tree for N-Pron resolution under the TC model.
We could find that: (i) When comparing two can-
didates which occur in the same sentence as the
anaphor, the web-based semantic feature would be
examined in the first place, followed by the lexi-
cal property of the candidates. (ii) When two non-
pronominal candidates are both in previous sen-
tences before the anaphor, the web-based semantic
feature is still required to be examined after FirstNP
and ParaStruct. The decision tree further indicates
that the web-based feature plays an important role in
N-Pron resolution.
5 Conclusion
Our research focussed on improving pronoun reso-
lution using the statistics-based semantic compati-
bility information. We explored two issues that af-
fect the utility of the semantic information: statis-
tics source and learning framework. Specifically, we
proposed to utilize the web and the twin-candidate
model, in addition to the common combination of
the corpus and single-candidate model, to compute
and apply the semantic information.
Our experiments systematically evaluated differ-
ent combinations of statistics sources and learn-
ing models. The results on the newswire domain
showed that the web-based semantic compatibility
could be the most effectively incorporated in the
twin-candidate model for the neutral pronoun res-
olution. While the utility is not obvious for per-
sonal pronoun resolution, we can still see the im-
provement on the overall performance. We believe
that the semantic information under such a config-
uration would be even more effective on technical
domains where neutral pronouns take the majority
in the pronominal anaphors. Our future work would
have a deep exploration on such domains.
References
S. Abney. 1996. Partial parsing via finite-state cascades. In
Workshop on Robust Parsing, 8th European Summer School
in Logic, Language and Information, pages 8?15.
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of 2004 North American chapter of the Association for
Computational Linguistics annual meeting.
I. Dagan and A. Itai. 1990. Automatic processing of large cor-
pora for the resolution of anahora references. In Proceedings
of the 13th International Conference on Computational Lin-
guistics, pages 330?332.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:339?352.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proceedings of 2004 North American
chapter of the Association for Computational Linguistics an-
nual meeting.
F. Keller and M. Lapata. 2003. Using the web to obtain
freqencies for unseen bigrams. Computational Linguistics,
29(3):459?484.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of the 17th Int. Conference on
Computational Linguistics, pages 869?875.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the web
in machine learning for other-anaphora resolution. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
42th Annual Meeting of the Association for Computational
Linguistics.
J. R. Quinlan. 1993. C4.5: Programs for machine learning.
Morgan Kaufmann Publishers, San Francisco, CA.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine learning approach
to pronoun resolution in spoken dialogue. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alternative to centering.
In Proceedings of the 17th Int. Conference on Computational
Linguistics and 36th Annual Meeting of ACL, pages 1251?
1257.
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference reso-
lution using competition learning approach. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, Japan.
G. Zhou and J. Su. 2002. Named Entity recognition using a
HMM-based chunk tagger. In Proceedings of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics, Philadelphia.
G. Zhou, J. Su, and T. Tey. 2000. Hybrid text chunking. In
Proceedings of the 4th Conference on Computational Natu-
ral Language Learning, pages 163?166, Lisbon, Portugal.
172
Proceedings of the 43rd Annual Meeting of the ACL, pages 395?402,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation Using Label Propagation Based
Semi-Supervised Learning
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Shortage of manually sense-tagged data is
an obstacle to supervised word sense dis-
ambiguation methods. In this paper we in-
vestigate a label propagation based semi-
supervised learning algorithm for WSD,
which combines labeled and unlabeled
data in learning process to fully realize
a global consistency assumption: simi-
lar examples should have similar labels.
Our experimental results on benchmark
corpora indicate that it consistently out-
performs SVM when only very few la-
beled examples are available, and its per-
formance is also better than monolingual
bootstrapping, and comparable to bilin-
gual bootstrapping.
1 Introduction
In this paper, we address the problem of word sense
disambiguation (WSD), which is to assign an appro-
priate sense to an occurrence of a word in a given
context. Many methods have been proposed to deal
with this problem, including supervised learning al-
gorithms (Leacock et al, 1998), semi-supervised
learning algorithms (Yarowsky, 1995), and unsuper-
vised learning algorithms (Schu?tze, 1998).
Supervised sense disambiguation has been very
successful, but it requires a lot of manually sense-
tagged data and can not utilize raw unannotated data
that can be cheaply acquired. Fully unsupervised
methods do not need the definition of senses and
manually sense-tagged data, but their sense cluster-
ing results can not be directly used in many NLP
tasks since there is no sense tag for each instance in
clusters. Considering both the availability of a large
amount of unlabelled data and direct use of word
senses, semi-supervised learning methods have re-
ceived great attention recently.
Semi-supervised methods for WSD are character-
ized in terms of exploiting unlabeled data in learning
procedure with the requirement of predefined sense
inventory for target words. They roughly fall into
three categories according to what is used for su-
pervision in learning process: (1) using external re-
sources, e.g., thesaurus or lexicons, to disambiguate
word senses or automatically generate sense-tagged
corpus, (Lesk, 1986; Lin, 1997; McCarthy et al,
2004; Seo et al, 2004; Yarowsky, 1992), (2) exploit-
ing the differences between mapping of words to
senses in different languages by the use of bilingual
corpora (e.g. parallel corpora or untagged monolin-
gual corpora in two languages) (Brown et al, 1991;
Dagan and Itai, 1994; Diab and Resnik, 2002; Li and
Li, 2004; Ng et al, 2003), (3) bootstrapping sense-
tagged seed examples to overcome the bottleneck of
acquisition of large sense-tagged data (Hearst, 1991;
Karov and Edelman, 1998; Mihalcea, 2004; Park et
al., 2000; Yarowsky, 1995).
As a commonly used semi-supervised learning
method for WSD, bootstrapping algorithm works
by iteratively classifying unlabeled examples and
adding confidently classified examples into labeled
dataset using a model learned from augmented la-
beled dataset in previous iteration. It can be found
that the affinity information among unlabeled ex-
amples is not fully explored in this bootstrapping
process. Bootstrapping is based on a local consis-
tency assumption: examples close to labeled exam-
ples within same class will have same labels, which
is also the assumption underlying many supervised
learning algorithms, such as kNN.
Recently a promising family of semi-supervised
learning algorithms are introduced, which can ef-
fectively combine unlabeled data with labeled data
395
in learning process by exploiting cluster structure
in data (Belkin and Niyogi, 2002; Blum et al,
2004; Chapelle et al, 1991; Szummer and Jaakkola,
2001; Zhu and Ghahramani, 2002; Zhu et al, 2003).
Here we investigate a label propagation based semi-
supervised learning algorithm (LP algorithm) (Zhu
and Ghahramani, 2002) for WSD, which works by
representing labeled and unlabeled examples as ver-
tices in a connected graph, then iteratively propagat-
ing label information from any vertex to nearby ver-
tices through weighted edges, finally inferring the
labels of unlabeled examples after this propagation
process converges.
Compared with bootstrapping, LP algorithm is
based on a global consistency assumption. Intu-
itively, if there is at least one labeled example in each
cluster that consists of similar examples, then unla-
beled examples will have the same labels as labeled
examples in the same cluster by propagating the la-
bel information of any example to nearby examples
according to their proximity.
This paper is organized as follows. First, we will
formulate WSD problem in the context of semi-
supervised learning in section 2. Then in section
3 we will describe LP algorithm and discuss the
difference between a supervised learning algorithm
(SVM), bootstrapping algorithm and LP algorithm.
Section 4 will provide experimental results of LP al-
gorithm on widely used benchmark corpora. Finally
we will conclude our work and suggest possible im-
provement in section 5.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
The cluster structure in X can be represented as a
connected graph, where each vertex corresponds to
an example, and the edge between any two examples
xi and xj is weighted so that the closer the vertices
in some distance measure, the larger the weight as-
sociated with this edge. The weights are defined as
follows: Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0
(1 ? i, j ? n), where dij is the distance (ex. Euclid-
ean distance) between xi and xj , and ? is used to
control the weight Wij .
3 Semi-supervised Learning Method
3.1 Label Propagation Algorithm
In LP algorithm (Zhu and Ghahramani, 2002), label
information of any vertex in a graph is propagated
to nearby vertices through weighted edges until a
global stable stage is achieved. Larger edge weights
allow labels to travel through easier. Thus the closer
the examples, more likely they have similar labels
(the global consistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
396
?2 ?1 0 1 2 3 4
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
labeled +1
unlabeled
labeled ?1
(a) Dataset with Two?Moon Pattern (b) SVM 
(c) Bootstrapping (d) Ideal Classification 
A 8
A 9
B8 
B9
A10 
B10 
A0 
B0 
Figure 1: Classification result on two-moon pattern dataset.
(a) Two-moon pattern dataset with two labeled points, (b) clas-
sification result by SVM, (c) labeling procedure of bootstrap-
ping algorithm, (d) ideal classification.
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
3.2 Comparison between SVM, Bootstrapping
and LP
For WSD, SVM is one of the state of the art super-
vised learning algorithms (Mihalcea et al, 2004),
while bootstrapping is one of the state of the art
semi-supervised learning algorithms (Li and Li,
2004; Yarowsky, 1995). For comparing LP with
SVM and bootstrapping, let us consider a dataset
with two-moon pattern shown in Figure 1(a). The
upper moon consists of 9 points, while the lower
moon consists of 13 points. There is only one la-
beled point in each moon, and other 20 points are un-
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
(a) Minimum Spanning Tree (b) t=1 
(c) t=7 (d) t=10
(e) t=12 (f) t=100
B 
A 
C 
Figure 2: Classification result of LP on two-moon pattern
dataset. (a) Minimum spanning tree of this dataset. The conver-
gence process of LP algorithm with t varying from 1 to 100 is
shown from (b) to (f).
labeled. The distance metric is Euclidian distance.
We can see that the points in one moon should be
more similar to each other than the points across the
moons.
Figure 1(b) shows the classification result of
SVM. Vertical line denotes classification hyper-
plane, which has the maximum separating margin
with respect to the labeled points in two classes. We
can see that SVM does not work well when labeled
data can not reveal the structure (two moon pattern)
in each class. The reason is that the classification
hyperplane was learned only from labeled data. In
other words, the coherent structure (two-moon pat-
tern) in unlabeled data was not explored when infer-
ring class boundary.
Figure 1(c) shows bootstrapping procedure using
kNN (k=1) as base classifier with user-specified pa-
rameter b = 1 (the number of added examples from
unlabeled data into classified data for each class in
each iteration). Termination condition is that the dis-
tance between labeled and unlabeled points is more
than inter-class distance (the distance between A0
and B0). Each arrow in Figure 1(c) represents
one classification operation in each iteration for each
class. After eight iterations, A1 ? A8 were tagged
397
as +1, and B1 ? B8 were tagged as ?1, while
A9 ? A10 and B9 ? B10 were still untagged. Then
at the ninth iteration, A9 was tagged as +1 since the
label of A9 was determined only by labeled points in
kNN model: A9 is closer to any point in {A0 ? A8}
than to any point in {B0 ? B8}, regardless of the
intrinsic structure in data: A9 ? A10 and B9 ? B10
are closer to points in lower moon than to points in
upper moon. In other words, bootstrapping method
uses the unlabeled data under a local consistency
based strategy. This is the reason that two points A9
and A10 are misclassified (shown in Figure 1(c)).
From above analysis we see that both SVM and
bootstrapping are based on a local consistency as-
sumption.
Finally we ran LP on a connected graph-minimum
spanning tree generated for this dataset, shown in
Figure 2(a). A, B, C represent three points, and
the edge A ? B connects the two moons. Figure
2(b)- 2(f) shows the convergence process of LP with
t increasing from 1 to 100. When t = 1, label in-
formation of labeled data was pushed to only nearby
points. After seven iteration steps (t = 7), point B
in upper moon was misclassified as ?1 since it first
received label information from point A through the
edge connecting two moons. After another three it-
eration steps (t=10), this misclassified point was re-
tagged as +1. The reason of this self-correcting be-
havior is that with the push of label information from
nearby points, the value of YB,+1 became higher
than YB,?1. In other words, the weight of edge
B ? C is larger than that of edge B ? A, which
makes it easier for +1 label of point C to travel to
point B. Finally, when t ? 12 LP converged to a
fixed point, which achieved the ideal classification
result.
4 Experiments and Results
4.1 Experiment Design
For empirical comparison with SVM and bootstrap-
ping, we evaluated LP on widely used benchmark
corpora - ?interest?, ?line? 1 and the data in English
lexical sample task of SENSEVAL-3 (including all
57 English words ) 2.
1Available at http://www.d.umn.edu/?tpederse/data.html
2Available at http://www.senseval.org/senseval3
Table 1: The upper two tables summarize accuracies (aver-
aged over 20 trials) and paired t-test results of SVM and LP on
SENSEVAL-3 corpus with percentage of training set increasing
from 1% to 100%. The lower table lists the official result of
baseline (using most frequent sense heuristics) and top 3 sys-
tems in ELS task of SENSEVAL-3.
Percentage SVM LPcosine LPJS
1% 24.9?2.7% 27.5?1.1% 28.1?1.1%
10% 53.4?1.1% 54.4?1.2% 54.9?1.1%
25% 62.3?0.7% 62.3?0.7% 63.3?0.9%
50% 66.6?0.5% 65.7?0.5% 66.9?0.6%
75% 68.7?0.4% 67.3?0.4% 68.7?0.3%
100% 69.7% 68.4% 70.3%
Percentage SVM vs. LPcosine SVM vs. LPJS
p-value Sign. p-value Sign.
1% 8.7e-004 ? 8.5e-005 ?
10% 1.9e-006 ? 1.0e-008 ?
25% 9.2e-001 ? 3.0e-006 ?
50% 1.9e-006 ? 6.2e-002 ?
75% 7.4e-013 ? 7.1e-001 ?
100% - - - -
Systems Baseline htsa3 IRST-Kernels nusels
Accuracy 55.2% 72.9% 72.6% 72.4%
We used three types of features to capture con-
textual information: part-of-speech of neighboring
words with position information, unordered sin-
gle words in topical context, and local collocations
(as same as the feature set used in (Lee and Ng,
2002) except that we did not use syntactic relations).
For SVM, we did not perform feature selection on
SENSEVAL-3 data since feature selection deterio-
rates its performance (Lee and Ng, 2002). When
running LP on the three datasets, we removed the
features with occurrence frequency (counted in both
training set and test set) less than 3 times.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991).
For the three datasets, we constructed connected
graphs following (Zhu et al, 2003): two instances
u, v will be connected by an edge if u is among v?s
k nearest neighbors, or if v is among u?s k nearest
neighbors as measured by cosine or JS distance mea-
sure. For ?interest? and ?line? corpora, k is 10 (fol-
lowing (Zhu et al, 2003)), while for SENSEVAL-3
data, k is 5 since the size of dataset for each word
in SENSEVAL-3 is much less than that of ?interest?
and ?line? datasets.
398
4.2 Experiment 1: LP vs. SVM
In this experiment, we evaluated LP and SVM
3 on the data of English lexical sample task in
SENSEVAL-3. We used l examples from training
set as labeled data, and the remaining training ex-
amples and all the test examples as unlabeled data.
For each labeled set size l, we performed 20 trials.
In each trial, we randomly sampled l labeled exam-
ples for each word from training set. If any sense
was absent from the sampled labeled set, we redid
the sampling. We conducted experiments with dif-
ferent values of l, including 1%?Nw,train, 10%?
Nw,train, 25%?Nw,train, 50%?Nw,train, 75%?
Nw,train, 100%?Nw,train (Nw,train is the number
of examples in training set of word w). SVM and LP
were evaluated using accuracy 4 (fine-grained score)
on test set of SENSEVAL-3.
We conducted paired t-test on the accuracy fig-
ures for each value of l. Paired t-test is not run when
percentage= 100%, since there is only one paired
accuracy figure. Paired t-test is usually used to esti-
mate the difference in means between normal pop-
ulations based on a set of random paired observa-
tions. {?, ?}, {<, >}, and ? correspond to p-
value ? 0.01, (0.01, 0.05], and > 0.05 respectively.
? (or ?) means that the performance of LP is sig-
nificantly better (or significantly worse) than SVM.
< (or >) means that the performance of LP is better
(or worse) than SVM.?means that the performance
of LP is almost as same as SVM.
Table 1 reports the average accuracies and paired
t-test results of SVM and LP with different sizes
of labled data. It also lists the official results of
baseline method and top 3 systems in ELS task of
SENSEVAL-3.
From Table 1, we see that with small labeled
dataset (percentage of labeled data ? 10%), LP per-
forms significantly better than SVM. When the per-
centage of labeled data increases from 50% to 75%,
the performance of LPJS and SVM become almost
same, while LPcosine performs significantly worse
than SVM.
3we used linear SVM light, available at
http://svmlight.joachims.org/.
4If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
Table 2: Accuracies from (Li and Li, 2004) and average ac-
curacies of LP with c ? b labeled examples on ?interest? and
?line? corpora. Major is a baseline method in which they al-
ways choose the most frequent sense. MB-D denotes monolin-
gual bootstrapping with decision list as base classifier, MB-B
represents monolingual bootstrapping with ensemble of Naive
Bayes as base classifier, and BB is bilingual bootstrapping with
ensemble of Naive Bayes as base classifier.
Ambiguous Accuracies from (Li and Li, 2004)
words Major MB-D MB-B BB
interest 54.6% 54.7% 69.3% 75.5%
line 53.5% 55.6% 54.1% 62.7%
Ambiguous Our results
words #labeled examples LPcosine LPJS
interest 4?15=60 80.2?2.0% 79.8?2.0%
line 6?15=90 60.3?4.5% 59.4?3.9%
4.3 Experiment 2: LP vs. Bootstrapping
Li and Li (2004) used ?interest? and ?line? corpora
as test data. For the word ?interest?, they used its
four major senses. For comparison with their re-
sults, we took reduced ?interest? corpus (constructed
by retaining four major senses) and complete ?line?
corpus as evaluation data. In their algorithm, c is
the number of senses of ambiguous word, and b
(b = 15) is the number of examples added into clas-
sified data for each class in each iteration of boot-
strapping. c ? b can be considered as the size of
initial labeled data in their bootstrapping algorithm.
We ran LP with 20 trials on reduced ?interest? cor-
pus and complete ?line? corpus. In each trial, we
randomly sampled b labeled examples for each sense
of ?interest? or ?line? as labeled data. The rest
served as both unlabeled data and test data.
Table 2 summarizes the average accuracies of LP
on the two corpora. It also lists the accuracies of
monolingual bootstrapping algorithm (MB), bilin-
gual bootstrapping algorithm (BB) on ?interest? and
?line? corpora. We can see that LP performs much
better than MB-D and MB-B on both ?interest? and
?line? corpora, while the performance of LP is com-
parable to BB on these two corpora.
4.4 An Example: Word ?use?
For investigating the reason for LP to outperform
SVM and monolingual bootstrapping, we used the
data of word ?use? in English lexical sample task of
SENSEVAL-3 as an example (totally 26 examples
in training set and 14 examples in test set). For data
399
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
(a) Initial Setting (b) Ground?truth
(c) SVM (d) Bootstrapping
(e) Bootstrapping (f) LP
B A 
C 
Figure 3: Comparison of sense disambiguation results be-
tween SVM, monolingual bootstrapping and LP on word ?use?.
(a) only one labeled example for each sense of word ?use?
as training data before sense disambiguation (? and ? denote
the unlabeled examples in SENSEVAL-3 training set and test
set respectively, and other five symbols (+, ?, ?, ?, and ?)
represent the labeled examples with different sense tags sam-
pled from SENSEVAL-3 training set.), (b) ground-truth re-
sult, (c) classification result on SENSEVAL-3 test set by SVM
(accuracy= 314 = 21.4%), (d) classified data after bootstrap-
ping, (e) classification result on SENSEVAL-3 training set and
test set by 1NN (accuracy= 614 = 42.9% ), (f) classifica-
tion result on SENSEVAL-3 training set and test set by LP
(accuracy= 1014 = 71.4% ).
visualization, we conducted unsupervised nonlinear
dimensionality reduction5 on these 40 feature vec-
tors with 210 dimensions. Figure 3 (a) shows the
dimensionality reduced vectors in two-dimensional
space. We randomly sampled only one labeled ex-
ample for each sense of word ?use? as labeled data.
The remaining data in training set and test set served
as unlabeled data for bootstrapping and LP. All of
these three algorithms are evaluated using accuracy
on test set.
From Figure 3 (c) we can see that SVM misclassi-
5We used Isomap to perform dimensionality reduction by
computing two-dimensional, 39-nearest-neighbor-preserving
embedding of 210-dimensional input. Isomap is available at
http://isomap.stanford.edu/.
fied many examples from class + into class ? since
using only features occurring in training set can not
reveal the intrinsic structure in full dataset.
For comparison, we implemented monolingual
bootstrapping with kNN (k=1) as base classifier.
The parameter b is set as 1. Only b unlabeled ex-
amples nearest to labeled examples and with the
distance less than dinter?class (the minimum dis-
tance between labeled examples with different sense
tags) will be added into classified data in each itera-
tion till no such unlabeled examples can be found.
Firstly we ran this monolingual bootstrapping on
this dataset to augment initial labeled data. The re-
sulting classified data is shown in Figure 3 (d). Then
a 1NN model was learned on this classified data and
we used this model to perform classification on the
remaining unlabeled data. Figure 3 (e) reports the
final classification result by this 1NN model. We can
see that bootstrapping does not perform well since it
is susceptible to small noise in dataset. For example,
in Figure 3 (d), the unlabeled example B 6 happened
to be closest to labeled example A, then 1NN model
tagged example B with label ?. But the correct label
of B should be + as shown in Figure 3 (b). This
error caused misclassification of other unlabeled ex-
amples that should have label +.
In LP, the label information of example C can
travel to B through unlabeled data. Then example A
will compete with C and other unlabeled examples
around B when determining the label of B. In other
words, the labels of unlabeled examples are deter-
mined not only by nearby labeled examples, but also
by nearby unlabeled examples. Using this classifi-
cation strategy achieves better performance than the
local consistency based strategy adopted by SVM
and bootstrapping.
4.5 Experiment 3: LPcosine vs. LPJS
Table 3 summarizes the performance comparison
between LPcosine and LPJS on three datasets. We
can see that on SENSEVAL-3 corpus, LPJS per-
6In the two-dimensional space, example B is not the closest
example to A. The reason is that: (1) A is not close to most
of nearby examples around B, and B is not close to most of
nearby examples around A; (2) we used Isomap to maximally
preserve the neighborhood information between any example
and all other examples, which caused the loss of neighborhood
information between a few example pairs for obtaining a glob-
ally optimal solution.
400
Table 3: Performance comparison between LPcosine and
LPJS and the results of three model selection criteria are re-
ported in following two tables. In the lower table, < (or >)
means that the average value of function H(Qcosine) is lower
(or higher) than H(QJS), and it will result in selecting cosine
(or JS) as distance measure. Qcosine (or QJS) represents a ma-
trix using cosine similarity (or JS divergence). ? and ? denote
correct and wrong prediction results respectively, while ?means
that any prediction is acceptable.
LPcosine vs. LPJS
Data p-value Significance
SENSEVAL-3 (1%) 1.1e-003 ?
SENSEVAL-3 (10%) 8.9e-005 ?
SENSEVAL-3 (25%) 9.0e-009 ?
SENSEVAL-3 (50%) 3.2e-010 ?
SENSEVAL-3 (75%) 7.7e-013 ?
SENSEVAL-3 (100%) - -
interest 3.3e-002 >
line 8.1e-002 ?
H(D) H(W ) H(YU )
Data cos. vs. JS cos. vs. JS cos. vs. JS
SENSEVAL-3 (1%) > (?) > (?) < (?)
SENSEVAL-3 (10%) < (?) > (?) < (?)
SENSEVAL-3 (25%) < (?) > (?) < (?)
SENSEVAL-3 (50%) > (?) > (?) > (?)
SENSEVAL-3 (75%) > (?) > (?) > (?)
SENSEVAL-3 (100%) < (?) > (?) < (?)
interest < (?) > (?) < (?)
line > (?) > (?) > (?)
forms significantly better than LPcosine, but their
performance is almost comparable on ?interest? and
?line? corpora. This observation motivates us to au-
tomatically select a distance measure that will boost
the performance of LP on a given dataset.
Cross-validation on labeled data is not feasi-
ble due to the setting of semi-supervised learning
(l ? u). In (Zhu and Ghahramani, 2002; Zhu et
al., 2003), they suggested a label entropy criterion
H(YU ) for model selection, where Y is the label
matrix learned by their semi-supervised algorithms.
The intuition behind their method is that good para-
meters should result in confident labeling. Entropy
on matrix W (H(W )) is a commonly used measure
for unsupervised feature selection (Dash and Liu,
2000), which can be considered here. Another pos-
sible criterion for model selection is to measure the
entropy of c ? c inter-class distance matrix D cal-
culated on labeled data (denoted as H(D)), where
Di,j represents the average distance between the i-
th class and the j-th class. We will investigate three
criteria, H(D), H(W ) and H(YU ), for model se-
lection. The distance measure can be automatically
selected by minimizing the average value of function
H(D), H(W ) or H(YU ) over 20 trials.
Let Q be the M ?N matrix. Function H(Q) can
measure the entropy of matrix Q, which is defined
as (Dash and Liu, 2000):
Si,j = exp (?? ?Qi,j), (1)
H(Q) = ?
M?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log (1? Si,j)),
(2)
where ? is positive constant. The possible value of ?
is? ln 0.5I? , where I? =
1
MN
?
i,j Qi,j . S is introduced
for normalization of matrix Q. For SENSEVAL-
3 data, we calculated an overall average score of
H(Q) by ?w
Nw,test?
w Nw,test
H(Qw). Nw,test is the
number of examples in test set of word w. H(D),
H(W ) and H(YU ) can be obtained by replacing Q
with D, W and YU respectively.
Table 3 reports the automatic prediction results
of these three criteria.
From Table 3, we can see that using H(W )
can consistently select the optimal distance measure
when the performance gap between LPcosine and
LPJS is very large (denoted by? or?). But H(D)
and H(YU ) fail to find the optimal distance measure
when only very few labeled examples are available
(percentage of labeled data ? 10%).
H(W ) measures the separability of matrix W .
Higher value of H(W ) means that distance mea-
sure decreases the separability of examples in full
dataset. Then the boundary between clusters is ob-
scured, which makes it difficult for LP to locate this
boundary. Therefore higher value of H(W ) results
in worse performance of LP.
When labeled dataset is small, the distances be-
tween classes can not be reliably estimated, which
results in unreliable indication of the separability
of examples in full dataset. This is the reason that
H(D) performs poorly on SENSEVAL-3 corpus
when the percentage of labeled data is less than 25%.
For H(YU ), small labeled dataset can not reveal
intrinsic structure in data, which may bias the esti-
mation of YU . Then labeling confidence (H(YU ))
can not properly indicate the performance of LP.
This may interpret the poor performance of H(YU )
on SENSEVAL-3 data when percentage ? 25%.
401
5 Conclusion
In this paper we have investigated a label propaga-
tion based semi-supervised learning algorithm for
WSD, which fully realizes a global consistency as-
sumption: similar examples should have similar la-
bels. In learning process, the labels of unlabeled ex-
amples are determined not only by nearby labeled
examples, but also by nearby unlabeled examples.
Compared with semi-supervised WSD methods in
the first and second categories, our corpus based
method does not need external resources, includ-
ing WordNet, bilingual lexicon, aligned parallel cor-
pora. Our analysis and experimental results demon-
strate the potential of this cluster assumption based
algorithm. It achieves better performance than SVM
when only very few labeled examples are avail-
able, and its performance is also better than mono-
lingual bootstrapping and comparable to bilingual
bootstrapping. Finally we suggest an entropy based
method to automatically identify a distance measure
that can boost the performance of LP algorithm on a
given dataset.
It has been shown that one sense per discourse
property can improve the performance of bootstrap-
ping algorithm (Li and Li, 2004; Yarowsky, 1995).
This heuristics can be integrated into LP algorithm
by setting weight Wi,j = 1 if the i-th and j-th in-
stances are in the same discourse.
In the future we may extend the evaluation of LP
algorithm and related cluster assumption based al-
gorithms using more benchmark data for WSD. An-
other direction is to use feature clustering technique
to deal with data sparseness and noisy feature prob-
lem.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Belkin, M., & Niyogi, P.. 2002. Using Manifold Structure for Partially Labeled
Classification. NIPS 15.
Blum, A., Lafferty, J., Rwebangira, R., & Reddy, R.. 2004. Semi-Supervised
Learning Using Randomized Mincuts. ICML-2004.
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mercer.. 1991. Word Sense
Disambiguation Using Statistical Methods. ACL-1991.
Chapelle, O., Weston, J., & Scho?lkopf, B. 2002. Cluster Kernels for Semi-
supervised Learning. NIPS 15.
Dagan, I. & Itai A.. 1994. Word Sense Disambiguation Using A Second Lan-
guage Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Dash, M., & Liu, H.. 2000. Feature Selection for Clustering. PAKDD(pp. 110?
121).
Diab, M., & Resnik. P.. 2002. An Unsupervised Method for Word Sense Tagging
Using Parallel Corpora. ACL-2002(pp. 255?262).
Hearst, M.. 1991. Noun Homograph Disambiguation using Local Context in
Large Text Corpora. Proceedings of the 7th Annual Conference of the UW
Centre for the New OED and Text Research: Using Corpora, 24:1, 1?41.
Karov, Y. & Edelman, S.. 1998. Similarity-Based Word Sense Disambiguation.
Computational Linguistics, 24(1): 41-59.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Using Corpus Statistics and
WordNet Relations for Sense Identification. Computational Linguistics, 24:1,
147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and
Learning Algorithms for Word Sense Disambiguation. EMNLP-2002, (pp.
41-48).
Lesk M.. 1986. Automated Word Sense Disambiguation Using Machine Read-
able Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Pro-
ceedings of the ACM SIGDOC Conference.
Li, H. & Li, C.. 2004. Word Translation Disambiguation Using Bilingual Boot-
strapping. Computational Linguistics, 30(1), 1-22.
Lin, D.K.. 1997. Using Syntactic Dependency as Local Context to Resolve Word
Sense Ambiguity. ACL-1997.
Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Trans-
actions on Information Theory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J.. 2004. Finding Predominant
Word Senses in Untagged Text. ACL-2004.
Mihalcea R.. 2004. Co-training and Self-training for Word Sense Disambigua-
tion. CoNLL-2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The SENSEVAL-3 English
Lexical Sample Task. SENSEVAL-2004.
Ng, H.T., Wang, B., & Chan, Y.S.. 2003. Exploiting Parallel Texts for Word
Sense Disambiguation: An Empirical Study. ACL-2003, pp. 455-462.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word Sense Disambiguation by
Learning from Unlabeled Data. ACL-2000.
Schu?tze, H.. 1998. Automatic Word Sense Discrimination. Computational Lin-
guistics, 24:1, 97?123.
Seo, H.C., Chung, H.J., Rim, H.C., Myaeng. S.H., & Kim, S.H.. 2004. Unsu-
pervised Word Sense Disambiguation Using WordNet Relatives. Computer,
Speech and Language, 18:3, 253?273.
Szummer, M., & Jaakkola, T.. 2001. Partially Labeled Classification with Markov
Random Walks. NIPS 14.
Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Super-
vised Methods. ACL-1995, pp. 189-196.
Yarowsky, D.. 1992. Word Sense Disambiguation Using Statistical Models of
Roget?s Categories Trained on Large Corpora. COLING-1992, pp. 454-460.
Zhu, X. & Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data
with Label Propagation. CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. ICML-2003.
402
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge
Xiaofeng Yang? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
tancl@comp.nus.edu.sg
Abstract
Syntactic knowledge is important for pro-
noun resolution. Traditionally, the syntac-
tic information for pronoun resolution is
represented in terms of features that have
to be selected and defined heuristically.
In the paper, we propose a kernel-based
method that can automatically mine the
syntactic information from the parse trees
for pronoun resolution. Specifically, we
utilize the parse trees directly as a struc-
tured feature and apply kernel functions to
this feature, as well as other normal fea-
tures, to learn the resolution classifier. In
this way, our approach avoids the efforts
of decoding the parse trees into the set of
flat syntactic features. The experimental
results show that our approach can bring
significant performance improvement and
is reliably effective for the pronoun reso-
lution task.
1 Introduction
Pronoun resolution is the task of finding the cor-
rect antecedent for a given pronominal anaphor
in a document. Prior studies have suggested that
syntactic knowledge plays an important role in
pronoun resolution. For a practical pronoun res-
olution system, the syntactic knowledge usually
comes from the parse trees of the text. The is-
sue that arises is how to effectively incorporate the
syntactic information embedded in the parse trees
to help resolution. One common solution seen in
previous work is to define a set of features that rep-
resent particular syntactic knowledge, such as the
grammatical role of the antecedent candidates, the
governing relations between the candidate and the
pronoun, and so on. These features are calculated
by mining the parse trees, and then could be used
for resolution by using manually designed rules
(Lappin and Leass, 1994; Kennedy and Boguraev,
1996; Mitkov, 1998), or using machine-learning
methods (Aone and Bennett, 1995; Yang et al,
2004; Luo and Zitouni, 2005).
However, such a solution has its limitation. The
syntactic features have to be selected and defined
manually, usually by linguistic intuition. Unfor-
tunately, what kinds of syntactic information are
effective for pronoun resolution still remains an
open question in this research community. The
heuristically selected feature set may be insuffi-
cient to represent all the information necessary for
pronoun resolution contained in the parse trees.
In this paper we will explore how to utilize the
syntactic parse trees to help learning-based pro-
noun resolution. Specifically, we directly utilize
the parse trees as a structured feature, and then use
a kernel-based method to automatically mine the
knowledge embedded in the parse trees. The struc-
tured syntactic feature, together with other nor-
mal features, is incorporated in a trainable model
based on Support Vector Machine (SVM) (Vapnik,
1995) to learn the decision classifier for resolution.
Indeed, using kernel methods to mine structural
knowledge has shown success in some NLP ap-
plications like parsing (Collins and Duffy, 2002;
Moschitti, 2004) and relation extraction (Zelenko
et al, 2003; Zhao and Grishman, 2005). However,
to our knowledge, the application of such a tech-
nique to the pronoun resolution task still remains
unexplored.
Compared with previous work, our approach
has several advantages: (1) The approach uti-
lizes the parse trees as a structured feature, which
avoids the efforts of decoding the parse trees into
a set of syntactic features in a heuristic manner.
(2) The approach is able to put together the struc-
tured feature and the normal flat features in a
trainable model, which allows different types of
41
information to be considered in combination for
both learning and resolution. (3) The approach
is applicable for practical pronoun resolution as
the syntactic information can be automatically ob-
tained from machine-generated parse trees. And
our study shows that the approach works well un-
der the commonly available parsers.
We evaluate our approach on the ACE data set.
The experimental results over the different do-
mains indicate that the structured syntactic fea-
ture incorporated with kernels can significantly
improve the resolution performance (by 5%?8%
in the success rates), and is reliably effective for
the pronoun resolution task.
The remainder of the paper is organized as fol-
lows. Section 2 gives some related work that uti-
lizes the structured syntactic knowledge to do pro-
noun resolution. Section 3 introduces the frame-
work for the pronoun resolution, as well as the
baseline feature space and the SVM classifier.
Section 4 presents in detail the structured feature
and the kernel functions to incorporate such a fea-
ture in the resolution. Section 5 shows the exper-
imental results and has some discussion. Finally,
Section 6 concludes the paper.
2 Related Work
One of the early work on pronoun resolution rely-
ing on parse trees was proposed by Hobbs (1978).
For a pronoun to be resolved, Hobbs? algorithm
works by searching the parse trees of the current
text. Specifically, the algorithm processes one sen-
tence at a time, using a left-to-right breadth-first
searching strategy. It first checks the current sen-
tence where the pronoun occurs. The first NP
that satisfies constraints, like number and gender
agreements, would be selected as the antecedent.
If the antecedent is not found in the current sen-
tence, the algorithm would traverse the trees of
previous sentences in the text. As the searching
processing is completely done on the parse trees,
the performance of the algorithm would rely heav-
ily on the accuracy of the parsing results.
Lappin and Leass (1994) reported a pronoun
resolution algorithm which uses the syntactic rep-
resentation output by McCord?s Slot Grammar
parser. A set of salience measures (e.g. Sub-
ject, Object or Accusative emphasis) is derived
from the syntactic structure. The candidate with
the highest salience score would be selected as
the antecedent. In their algorithm, the weights of
Category: whether the candidate is a definite noun phrase,
indefinite noun phrase, pronoun, named-entity or others.
Reflexiveness: whether the pronominal anaphor is a reflex-
ive pronoun.
Type: whether the pronominal anaphor is a male-person
pronoun (like he), female-person pronoun (like she), sin-
gle gender-neuter pronoun (like it), or plural gender-neuter
pronoun (like they)
Subject: whether the candidate is a subject of a sentence, a
subject of a clause, or not.
Object: whether the candidate is an object of a verb, an
object of a preposition, or not.
Distance: the sentence distance between the candidate and
the pronominal anaphor.
Closeness: whether the candidate is the candidate closest
to the pronominal anaphor.
FirstNP: whether the candidate is the first noun phrase in
the current sentence.
Parallelism: whether the candidate has an identical collo-
cation pattern with the pronominal anaphor.
Table 1: Feature set for the baseline pronoun res-
olution system
salience measures have to be assigned manually.
Luo and Zitouni (2005) proposed a coreference
resolution approach which also explores the infor-
mation from the syntactic parse trees. Different
from Lappin and Leass (1994)?s algorithm, they
employed a maximum entropy based model to au-
tomatically compute the importance (in terms of
weights) of the features extracted from the trees.
In their work, the selection of their features is
mainly inspired by the government and binding
theory, aiming to capture the c-command relation-
ships between the pronoun and its antecedent can-
didate. By contrast, our approach simply utilizes
the parse trees as a structured feature, and lets the
learning algorithm discover all possible embedded
information that is necessary for pronoun resolu-
tion.
3 The Resolution Framework
Our pronoun resolution system adopts the com-
mon learning-based framework similar to those
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance is formed by a pronoun and one of its
antecedent candidate. During training, for each
pronominal anaphor encountered, a positive in-
stance is created by paring the anaphor and its
closest antecedent. Also a set of negative instances
is formed by paring the anaphor with each of the
42
non-coreferential candidates. Based on the train-
ing instances, a binary classifier is generated using
a particular learning algorithm. During resolution,
a pronominal anaphor to be resolved is paired in
turn with each preceding antecedent candidate to
form a testing instance. This instance is presented
to the classifier which then returns a class label
with a confidence value indicating the likelihood
that the candidate is the antecedent. The candidate
with the highest confidence value will be selected
as the antecedent of the pronominal anaphor.
3.1 Feature Space
As with many other learning-based approaches,
the knowledge for the reference determination is
represented as a set of features associated with
the training or test instances. In our baseline sys-
tem, the features adopted include lexical property,
morphologic type, distance, salience, parallelism,
grammatical role and so on. Listed in Table 1, all
these features have been proved effective for pro-
noun resolution in previous work.
3.2 Support Vector Machine
In theory, any discriminative learning algorithm is
applicable to learn the classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels to
incorporate the structured feature.
Suppose the training set S consists of labelled
vectors {(xi, yi)}, where xi is the feature vector
of a training instance and yi is its class label. The
classifier learned by SVM is
f(x) = sgn(
?
i=1
yiaix ? xi + b) (1)
where ai is the learned parameter for a support
vector xi. An instance x is classified as positive
(negative) if f(x) > 0 (f(x) < 0)1.
One advantage of SVM is that we can use ker-
nel methods to map a feature space to a particu-
lar high-dimension space, in case that the current
problem could not be separated in a linear way.
Thus the dot-product x1 ? x2 is replaced by a ker-
nel function (or kernel) between two vectors, that
is K(x1, x2). For the learning with the normal
features listed in Table 1, we can just employ the
well-known polynomial or radial basis kernels that
can be computed efficiently. In the next section we
1For our task, the result of f(x) is used as the confidence
value of the candidate to be the antecedent of the pronoun
described by x.
will discuss how to use kernels to incorporate the
more complex structured feature.
4 Incorporating Structured Syntactic
Information
4.1 Main Idea
A parse tree that covers a pronoun and its an-
tecedent candidate could provide us much syntac-
tic information related to the pair. The commonly
used syntactic knowledge for pronoun resolution,
such as grammatical roles or the governing rela-
tions, can be directly described by the tree struc-
ture. Other syntactic knowledge that may be help-
ful for resolution could also be implicitly repre-
sented in the tree. Therefore, by comparing the
common substructures between two trees we can
find out to what degree two trees contain similar
syntactic information, which can be done using a
convolution tree kernel.
The value returned from the tree kernel reflects
the similarity between two instances in syntax.
Such syntactic similarity can be further combined
with other knowledge to compute the overall simi-
larity between two instances, through a composite
kernel. And thus a SVM classifier can be learned
and then used for resolution. This is just the main
idea of our approach.
4.2 Structured Syntactic Feature
Normally, parsing is done on the sentence level.
However, in many cases a pronoun and an an-
tecedent candidate do not occur in the same sen-
tence. To present their syntactic properties and
relations in a single tree structure, we construct a
syntax tree for an entire text, by attaching the parse
trees of all its sentences to an upper node.
Having obtained the parse tree of a text, we shall
consider how to select the appropriate portion of
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun
and a candidate, the structured feature at least
should be able to cover both of these two expres-
sions. Generally, the more substructure of the tree
is included, the more syntactic information would
be provided, but at the same time the more noisy
information that comes from parsing errors would
likely be introduced. In our study, we examine
three possible structured features that contain dif-
ferent substructures of the parse tree:
Min-Expansion This feature records the mini-
mal structure covering both the pronoun and
43
Min-Expansion Simple-Expansion Full-Expansion
Figure 1: structured-features for the instance i{?him?, ?the man?}
the candidate in the parse tree. It only in-
cludes the nodes occurring in the shortest
path connecting the pronoun and the candi-
date, via the nearest commonly commanding
node. For example, considering the sentence
?The man in the room saw him.?, the struc-
tured feature for the instance i{?him?,?the
man?} is circled with dash lines as shown in
the leftmost picture of Figure 1.
Simple-Expansion Min-Expansion could, to
some degree, describe the syntactic relation-
ships between the candidate and pronoun.
However, it is incapable of capturing the
syntactic properties of the candidate or
the pronoun, because the tree structure
surrounding the expression is not taken into
consideration. To incorporate such infor-
mation, feature Simple-Expansion not only
contains all the nodes in Min-Expansion, but
also includes the first-level children of these
nodes2. The middle of Figure 1 shows such a
feature for i{?him?, ?the man?}. We can see
that the nodes ?PP? (for ?in the room?) and
?VB? (for ?saw?) are included in the feature,
which provides clues that the candidate is
modified by a prepositional phrase and the
pronoun is the object of a verb.
Full-Expansion This feature focusses on the
whole tree structure between the candidate
and pronoun. It not only includes all the
nodes in Simple-Expansion, but also the
nodes (beneath the nearest commanding par-
ent) that cover the words between the candi-
date and the pronoun3. Such a feature keeps
the most information related to the pronoun
2If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences
before the candidate or after the pronoun.
3We will not expand the nodes denoting the sentences
other than where the pronoun and the candidate occur.
and candidate pair. The rightmost picture of
Figure 1 shows the structure for feature Full-
Expansion of i{?him?, ?the man?}. As illus-
trated, different from in Simple-Expansion,
the subtree of ?PP? (for ?in the room?) is
fully expanded and all its children nodes are
included in Full-Expansion.
Note that to distinguish from other words, we
explicitly mark up in the structured feature the
pronoun and the antecedent candidate under con-
sideration, by appending a string tag ?ANA? and
?CANDI? in their respective nodes (e.g.,?NN-
CANDI? for ?man? and ?PRP-ANA? for ?him? as
shown in Figure 1).
4.3 Structural Kernel and Composite Kernel
To calculate the similarity between two structured
features, we use the convolution tree kernel that is
defined by Collins and Duffy (2002) and Moschitti
(2004). Given two trees, the kernel will enumerate
all their subtrees and use the number of common
subtrees as the measure of the similarity between
the trees. As has been proved, the convolution
kernel can be efficiently computed in polynomial
time.
The above tree kernel only aims for the struc-
tured feature. We also need a composite kernel
to combine together the structured feature and the
normal features described in Section 3.1. In our
study we define the composite kernel as follows:
Kc(x1, x2) = Kn(x1, x2)|Kn(x1, x2)| ?
Kt(x1, x2)
|Kt(x1, x2)|(2)
where Kt is the convolution tree kernel defined
for the structured feature, and Kn is the kernel
applied on the normal features. Both kernels are
divided by their respective length4 for normaliza-
tion. The new composite kernel Kc, defined as the
4The length of a kernel K is defined as |K(x1, x2)| =?
K(x1, x1) ?K(x2, x2)
44
multiplier of normalized Kt and Kn, will return a
value close to 1 only if both the structured features
and the normal features from the two vectors have
high similarity under their respective kernels.
5 Experiments and Discussions
5.1 Experimental Setup
In our study we focussed on the third-person
pronominal anaphora resolution. All the exper-
iments were done on the ACE-2 V1.0 corpus
(NIST, 2003), which contain two data sets, train-
ing and devtest, used for training and testing re-
spectively. Each of these sets is further divided
into three domains: newswire (NWire), newspa-
per (NPaper), and broadcast news (BNews).
An input raw text was preprocessed automati-
cally by a pipeline of NLP components, including
sentence boundary detection, POS-tagging, Text
Chunking and Named-Entity Recognition. The
texts were parsed using the maximum-entropy-
based Charniak parser (Charniak, 2000), based on
which the structured features were computed au-
tomatically. For learning, the SVM-Light soft-
ware (Joachims, 1999) was employed with the
convolution tree kernel implemented by Moschitti
(2004). All classifiers were trained with default
learning parameters.
The performance was evaluated based on the
metric success, the ratio of the number of cor-
rectly resolved5 anaphor over the number of all
anaphors. For each anaphor, the NPs occurring
within the current and previous two sentences
were taken as the initial antecedent candidates.
Those with mismatched number and gender agree-
ments were filtered from the candidate set. Also,
pronouns or NEs that disagreed in person with the
anaphor were removed in advance. For training,
there were 1207, 1440, and 1260 pronouns with
non-empty candidate set found pronouns in the
three domains respectively, while for testing, the
number was 313, 399 and 271. On average, a
pronoun anaphor had 6?9 antecedent candidates
ahead. Totally, we got around 10k, 13k and 8k
training instances for the three domains.
5.2 Baseline Systems
Table 2 lists the performance of different systems.
We first tested Hobbs? algorithm (Hobbs, 1978).
5An anaphor was deemed correctly resolved if the found
antecedent is in the same coreference chain of the anaphor.
NWire NPaper BNews
Hobbs (1978) 66.1 66.4 72.7
NORM 74.4 77.4 74.2
NORM MaxEnt 72.8 77.9 75.3
NORM C5 71.9 75.9 71.6
S Min 76.4 81.0 76.8
S Simple 73.2 82.7 82.3
S Full 73.2 80.5 79.0
NORM+S Min 77.6 82.5 82.3
NORM+S Simple 79.2 82.7 82.3
NORM+S Full 81.5 83.2 81.5
Table 2: Results of the syntactic structured fea-
tures
Described in Section 2, the algorithm uses heuris-
tic rules to search the parse tree for the antecedent,
and will act as a good baseline to compare with the
learned-based approach with the structured fea-
ture. As shown in the first line of Table 2, Hobbs?
algorithm obtains 66%?72% success rates on the
three domains.
The second block of Table 2 shows the baseline
system (NORM) that uses only the normal features
listed in Table 1. Throughout our experiments, we
applied the polynomial kernel on the normal fea-
tures to learn the SVM classifiers. In the table we
also compared the SVM-based results with those
using other learning algorithms, i.e., Maximum
Entropy (Maxent) and C5 decision tree, which are
more commonly used in the anaphora resolution
task.
As shown in the table, the system with normal
features (NORM) obtains 74%?77% success rates
for the three domains. The performance is simi-
lar to other published results like those by Keller
and Lapata (2003), who adopted a similar fea-
ture set and reported around 75% success rates
on the ACE data set. The comparison between
different learning algorithms indicates that SVM
can work as well as or even better than Maxent
(NORM MaxEnt) or C5 (NORM C5).
5.3 Systems with Structured Features
The last two blocks of Table 2 summarize the re-
sults using the three syntactic structured features,
i.e, Min Expansion (S MIN), Simple Expansion
(S SIMPLE) and Full Expansion (S FULL). Be-
tween them, the third block is for the systems us-
ing the individual structured feature alone. We
can see that all the three structured features per-
45
NWire NPaper BNews
Sentence Distance 0 1 2 0 1 2 0 1 2
(Number of Prons) (192) (102) (19) (237) (147) (15) (175) (82) (14)
NORM 80.2 72.5 26.3 81.4 75.5 33.3 80.0 65.9 50.0
S Simple 79.7 70.6 21.1 87.3 81.0 26.7 89.7 70.7 57.1
NORM+S Simple 85.4 76.5 31.6 87.3 79.6 40.0 88.6 74.4 50.0
Table 3: The resolution results for pronouns with antecedent in different sentences apart
NWire NPaper BNews
Type person neuter person neuter person neuter
(Number of Prons) (171) (142) (250) (149) (153) (118)
NORM 81.9 65.5 80.0 73.2 74.5 73.7
S Simple 81.9 62.7 83.2 81.9 82.4 82.2
NORM+S Simple 87.1 69.7 83.6 81.2 86.9 76.3
Table 4: The resolution results for different types of pronouns
form better than the normal features for NPaper
(up to 5.3% success) and BNews (up to 8.1% suc-
cess), or equally well (?1 ? 2% in success) for
NWire. When used together with the normal fea-
tures, as shown in the last block, the three struc-
tured features all outperform the baselines. Es-
pecially, the combinations of NORM+S SIMPLE
and NORM+S FULL can achieve significantly6
better results than NORM, with the success rate
increasing by (4.8%, 5.3% and 8.1%) and (7.1%,
5.8%, 7.2%) respectively. All these results prove
that the structured syntactic feature is effective for
pronoun resolution.
We further compare the performance of the
three different structured features. As shown in
Table 2, when used together with the normal
features, Full Expansion gives the highest suc-
cess rates in NWire and NPaper, but neverthe-
less the lowest in BNews. This should be be-
cause feature Full-Expansion captures a larger
portion of the parse trees, and thus can provide
more syntactic information than Min Expansion
or Simple Expansion. However, if the texts are
less-formally structured as those in BNews, Full-
Expansion would inevitably involve more noises
and thus adversely affect the resolution perfor-
mance. By contrast, feature Simple Expansion
would achieve balance between the information
and the noises to be introduced: from Table 2 we
can find that compared with the other two features,
Simple Expansion is capable of producing aver-
age results for all the three domains. And for this
6p < 0.05 by a 2-tailed t test.
reason, our subsequent reports will focus on Sim-
ple Expansion, unless otherwise specified.
As described, to compute the structured fea-
ture, parse trees for different sentences are con-
nected to form a large tree for the text. It would
be interesting to find how the structured feature
works for pronouns whose antecedents reside in
different sentences. For this purpose we tested
the success rates for the pronouns with the clos-
est antecedent occurring in the same sentence,
one-sentence apart, and two-sentence apart. Ta-
ble 3 compares the learning systems with/without
the structured feature present. From the table,
for all the systems, the success rates drop with
the increase of the distances between the pro-
noun and the antecedent. However, in most cases,
adding the structured feature would bring consis-
tent improvement against the baselines regardless
of the number of sentence distance. This observa-
tion suggests that the structured syntactic informa-
tion is helpful for both intra-sentential and inter-
sentential pronoun resolution.
We were also concerned about how the struc-
tured feature works for different types of pro-
nouns. Table 4 lists the resolution results for two
types of pronouns: person pronouns (i.e., ?he?,
?she?) and neuter-gender pronouns (i.e., ?it? and
?they?). As shown, with the structured feature in-
corporated, the system NORM+S Simple can sig-
nificantly boost the performance of the baseline
(NORM), for both personal pronoun and neuter-
gender pronoun resolution.
46
1 2 3 4 5 6 7 8 9 100.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 2 4 6 8 10 120.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 1 2 3 4 5 6 7 80.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple
NWire NPaper BNews
Figure 2: Learning curves of systems with different features
5.4 Learning Curves
Figure 2 plots the learning curves for the sys-
tems with three feature sets, i.e, normal features
(NORM), structured feature alone (S Simple),
and combined features (NORM+S Simple). We
trained each system with different number of in-
stances from 1k, 2k, 3k, . . . , till the full size. Each
point in the figures was the average over two trails
with instances selected forwards and backwards
respectively. From the figures we can find that
(1) Used in combination (NORM+S Simple), the
structured feature shows superiority over NORM,
achieving results consistently better than the nor-
mal features (NORM) do in all the three domains.
(2) With training instances above 3k, the struc-
tured feature, used either in isolation (S Simple)
or in combination (NORM+S Simple), leads to
steady increase in the success rates and exhibit
smoother learning curves than the normal features
(NORM). These observations further prove the re-
liability of the structured feature in pronoun reso-
lution.
5.5 Feature Analysis
In our experiment we were also interested to com-
pare the structured feature with the normal flat
features extracted from the parse tree, like fea-
ture Subject and Object. For this purpose we
took out these two grammatical features from the
normal feature set, and then trained the systems
again. As shown in Table 5, the two grammatical-
role features are important for the pronoun resolu-
tion: removing these features results in up to 5.7%
(NWire) decrease in success. However, when the
structured feature is included, the loss in success
reduces to 1.9% and 1.1% for NWire and BNews,
and a slight improvement can even be achieved for
NPaper. This indicates that the structured feature
can effectively provide the syntactic information
NWire NPaper BNews
NORM 74.4 77.4 74.2
NORM - subj/obj 68.7 76.2 72.7
NORM + S Simple 79.2 82.7 82.3
NORM + S Simple - subj/obj 77.3 83.0 81.2
NORM + Luo05 75.7 77.9 74.9
Table 5: Comparison of the structured feature and
the flat features extracted from parse trees
Feature Parser NWire NPaper BNews
Charniak00 73.2 82.7 82.3
S Simple Collins99 75.1 83.2 80.4
NORM+ Charniak00 79.2 82.7 82.3
S Simple Collins99 80.8 81.5 82.3
Table 6: Results using different parsers
important for pronoun resolution.
We also tested the flat syntactic feature set pro-
posed in Luo and Zitouni (2005)?s work. As de-
scribed in Section 2, the feature set is inspired
the binding theory, including those features like
whether the candidate is c commanding the pro-
noun, and the counts of ?NP?, ?VP?, ?S? nodes
in the commanding path. The last line of Table 5
shows the results by adding these features into the
normal feature set. In line with the reports in (Luo
and Zitouni, 2005) we do observe the performance
improvement against the baseline (NORM) for all
the domains. However, the increase in the success
rates (up to 1.3%) is not so large as by adding the
structured feature (NORM+S Simple) instead.
5.6 Comparison with Different Parsers
As mentioned, the above reported results were
based on Charniak (2000)?s parser. It would be
interesting to examine the influence of different
parsers on the resolution performance. For this
purpose, we also tried the parser by Collins (1999)
47
(Mode II)7, and the results are shown in Table 6.
We can see that Charniak (2000)?s parser leads to
higher success rates for NPaper and BNews, while
Collins (1999)?s achieves better results for NWire.
However, the difference between the results of the
two parsers is not significant (less than 2% suc-
cess) for the three domains, no matter whether the
structured feature is used alone or in combination.
6 Conclusion
The purpose of this paper is to explore how to
make use of the structured syntactic knowledge to
do pronoun resolution. Traditionally, syntactic in-
formation from parse trees is represented as a set
of flat features. However, the features are usu-
ally selected and defined by heuristics and may
not necessarily capture all the syntactic informa-
tion provided by the parse trees. In the paper, we
propose a kernel-based method to incorporate the
information from parse trees. Specifically, we di-
rectly utilize the syntactic parse tree as a struc-
tured feature, and then apply kernels to such a fea-
ture, together with other normal features, to learn
the decision classifier and do the resolution. Our
experimental results on ACE data set show that
the system with the structured feature included
can achieve significant increase in the success rate
by around 5%?8%, for all the different domains.
The deeper analysis on various factors like training
size, feature set or parsers further proves that the
structured feature incorporated with our kernel-
based method is reliably effective for the pronoun
resolution task.
References
C. Aone and S. W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Compuational Lin-
guistics, pages 122?129.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of North American chapter
of the Association for Computational Linguistics an-
nual meeting, pages 132?139.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: kernels over discrete
structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association
7As in their pulic reports on Section 23 of WSJ TreeBank,
Charniak (2000)?s parser achieves 89.6% recall and 89.5%
precision with 0.88 crossing brackets (words ? 100), against
Collins (1999)?s 88.1% recall and 88.3% precision with 1.06
crossing brackets.
for Computational Linguistics (ACL?02), pages 263?
270.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44:339?352.
T. Joachims. 1999. Making large-scale svm learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain freqencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
C. Kennedy and B. Boguraev. 1996. Anaphora
for everyone: pronominal anaphra resolution with-
out a parser. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 113?118, Copenhagen, Denmark.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):525?561.
X. Luo and I. Zitouni. 2005. Milti-lingual coreference
resolution with syntactic features. In Proceedings of
Human Language Techonology conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 660?667.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics, pages 869?
875.
A. Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04), pages 335?342.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 104?111,
Philadelphia.
W. Soon, H. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?
544.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improv-
ing pronoun resolution by incorporating coreferen-
tial information of candidates. In Proceedings of
42th Annual Meeting of the Association for Compu-
tational Linguistics, pages 127?134, Barcelona.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3(6):1083 ? 1106.
S. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel meth-
ods. In Proceedings of 43rd Annual Meeting of the
Association for Computational Linguistics (ACL05),
pages 419?426.
48
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 129?136,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relation Extraction Using Label Propagation Based Semi-supervised
Learning
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Shortage of manually labeled data is an
obstacle to supervised relation extraction
methods. In this paper we investigate a
graph based semi-supervised learning al-
gorithm, a label propagation (LP) algo-
rithm, for relation extraction. It represents
labeled and unlabeled examples and their
distances as the nodes and the weights of
edges of a graph, and tries to obtain a la-
beling function to satisfy two constraints:
1) it should be fixed on the labeled nodes,
2) it should be smooth on the whole graph.
Experiment results on the ACE corpus
showed that this LP algorithm achieves
better performance than SVM when only
very few labeled examples are available,
and it also performs better than bootstrap-
ping for the relation extraction task.
1 Introduction
Relation extraction is the task of detecting and
classifying relationships between two entities from
text. Many machine learning methods have been
proposed to address this problem, e.g., supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithms (Hasegawa et al, 2004).
Supervised methods for relation extraction per-
form well on the ACE Data, but they require a large
amount of manually labeled relation instances. Un-
supervised methods do not need the definition of
relation types and manually labeled data, but they
cannot detect relations between entity pairs and its
result cannot be directly used in many NLP tasks
since there is no relation type label attached to
each instance in clustering result. Considering both
the availability of a large amount of untagged cor-
pora and direct usage of extracted relations, semi-
supervised learning methods has received great at-
tention.
DIPRE (Dual Iterative Pattern Relation Expan-
sion) (Brin, 1998) is a bootstrapping-based sys-
tem that used a pattern matching system as clas-
sifier to exploit the duality between sets of pat-
terns and relations. Snowball (Agichtein and Gra-
vano, 2000) is another system that used bootstrap-
ping techniques for extracting relations from un-
structured text. Snowball shares much in common
with DIPRE, including the employment of the boot-
strapping framework as well as the use of pattern
matching to extract new candidate relations. The
third system approaches relation classification prob-
lem with bootstrapping on top of SVM, proposed by
Zhang (2004). This system focuses on the ACE sub-
problem, RDC, and extracts various lexical and syn-
tactic features for the classification task. However,
Zhang (2004)?s method doesn?t actually ?detect? re-
laitons but only performs relation classification be-
tween two entities given that they are known to be
related.
Bootstrapping works by iteratively classifying un-
labeled examples and adding confidently classified
examples into labeled data using a model learned
from augmented labeled data in previous iteration. It
129
can be found that the affinity information among un-
labeled examples is not fully explored in this boot-
strapping process.
Recently a promising family of semi-supervised
learning algorithm is introduced, which can effec-
tively combine unlabeled data with labeled data in
learning process by exploiting manifold structure
(cluster structure) in data (Belkin and Niyogi, 2002;
Blum and Chawla, 2001; Blum et al, 2004; Zhu
and Ghahramani, 2002; Zhu et al, 2003). These
graph-based semi-supervised methods usually de-
fine a graph where the nodes represent labeled and
unlabeled examples in a dataset, and edges (may be
weighted) reflect the similarity of examples. Then
one wants a labeling function to satisfy two con-
straints at the same time: 1) it should be close to the
given labels on the labeled nodes, and 2) it should be
smooth on the whole graph. This can be expressed
in a regularization framework where the first term
is a loss function, and the second term is a regu-
larizer. These methods differ from traditional semi-
supervised learning methods in that they use graph
structure to smooth the labeling function.
To the best of our knowledge, no work has been
done on using graph based semi-supervised learning
algorithms for relation extraction. Here we inves-
tigate a label propagation algorithm (LP) (Zhu and
Ghahramani, 2002) for relation extraction task. This
algorithm works by representing labeled and unla-
beled examples as vertices in a connected graph,
then propagating the label information from any ver-
tex to nearby vertices through weighted edges itera-
tively, finally inferring the labels of unlabeled exam-
ples after the propagation process converges. In this
paper we focus on the ACE RDC task1.
The rest of this paper is organized as follows. Sec-
tion 2 presents related work. Section 3 formulates
relation extraction problem in the context of semi-
supervised learning and describes our proposed ap-
proach. Then we provide experimental results of our
proposed method and compare with a popular su-
pervised learning algorithm (SVM) and bootstrap-
ping algorithm in Section 4. Finally we conclude
our work in section 5.
1 http://www.ldc.upenn.edu/Projects/ACE/, Three tasks of
ACE program: Entity Detection and Tracking (EDT), Rela-
tion Detection and Characterization (RDC), and Event Detec-
tion and Characterization (EDC)
2 The Proposed Method
2.1 Problem Definition
The problem of relation extraction is to assign an ap-
propriate relation type to an occurrence of two entity
pairs in a given context. It can be represented as fol-
lows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 denote the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs. In this pa-
per, we set the mid-context window as the words be-
tween the two entity mentions and the pre- and post-
context as up to two words before and after the cor-
responding entity mention.
Let X = {xi}ni=1 be a set of contexts of occur-
rences of all the entity mention pairs, where xi rep-
resents the contexts of the i-th occurrence, and n is
the total number of occurrences. The first l exam-
ples (or contexts) are labeled as yg ( yg ? {rj}Rj=1,
rj denotes relation type and R is the total number of
relation types). The remaining u(u = n ? l) exam-
ples are unlabeled.
Intuitively, if two occurrences of entity mention
pairs have the similarity context, they tend to hold
the same relation type. Based on the assumption, we
define a graph where the vertices represent the con-
texts of labeled and unlabeled occurrences of entity
mention pairs, and the edge between any two ver-
tices xi and xj is weighted so that the closer the ver-
tices in some distance measure, the larger the weight
associated with this edge. Hence, the weights are de-
fined as follows:
Wij = exp(?
s2ij
?2 ) (2)
where sij is the similarity between xi and xj calcu-
lated by some similarity measures, e.g., cosine sim-
ilarity, and ? is used to scale the weights. In this
paper, we set ? as the average similarity between la-
beled examples from different classes.
2.2 A Label Propagation Algorithm
In the LP algorithm, the label information of any
vertex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage is
achieved. Larger edge weights allow labels to travel
130
through easier. Thus the closer the examples are, the
more likely they have similar labels.
We define soft label as a vector that is a proba-
bilistic distribution over all the classes. In the la-
bel propagation process, the soft label of each initial
labeled example is clamped in each iteration to re-
plenish label sources from these labeled data. Thus
the labeled data act like sources to push out labels
through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. Hopefully, the val-
ues of Wij across different classes would be as small
as possible and the values of Wij within the same
class would be as large as possible. This will make
label propagation to stay within the same class. This
label propagation process will make the labeling
function smooth on the graph.
Define an n? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
(3)
where Tij is the probability to jump from vertex xj
to vertex xi. We define a n ? R label matrix Y ,
where Yij representing the probabilities of vertex yi
to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1 : Initialization
? Set the iteration index t = 0;
? Let Y 0 be the initial soft labels attached to
each vertex, where Y 0ij = 1 if yi is label rj
and 0 otherwise.
? Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent
with the labeling in labeled data and the
initialization of Y 0U can be arbitrary.
Step 2 : Propagate the labels of any vertex to
nearby vertices by Y t+1 = TY t , where
T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the
class probability interpretation.
Step 3 : Clamp the labeled data, that is, replace the
top l row of Y t+1 with Y 0L .
Step 4 : Repeat from step 2 until Y converges.
Step 5 : Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
The above algorithm can ensure that the labeled
data YL never changes since it is clamped in Step 3.
Actually we are interested in only YU . This algo-
rithm has been shown to converge to a unique solu-
tion Y?U = limt?? Y tU = (I ? T?uu)?1T?ulY 0L (Zhu
and Ghahramani, 2002). Here, T?uu and T?ul are ac-
quired by splitting matrix T? after the l-th row and
the l-th column into 4 sub-matrices. And I is u? u
identity matrix. We can see that the initialization of
Y 0U in this solution is not important, since Y 0U does
not affect the estimation of Y?U .
3 Experiments and Results
3.1 Feature Set
Following (Zhang, 2004), we used lexical and syn-
tactic features in the contexts of entity pairs, which
are extracted and computed from the parse trees de-
rived from Charniak Parser (Charniak, 1999) and the
Chunklink script 2 written by Sabine Buchholz from
Tilburg University.
Words: Surface tokens of the two entities and
words in the three contexts.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all tokens in the two entities and words in
the three contexts.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and words in the three contexts. The
?0? tag means that the word is not in any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two enti-
ties and words in the three contexts. The
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
131
last word in each chunk is its head, and
the function of the head is the function of
the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The
other words in a chunk that are not the
head have ?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
The position information is also specified in the
description of each feature above. For example,
word features with position information include:
1) WE1 (WE2): all words in e1 (e2)
2) WHE1 (WHE2): head word of e1 (e2)
3) WMNULL: no words in Cmid
4) WMFL: the only word in Cmid
5) WMF, WML, WM2, WM3, ...: first word, last
word, second word, third word, ...in Cmid when at
least two words in Cmid
6) WEL1, WEL2, ...: first word, second word, ...
before e1
7) WER1, WER2, ...: first word, second word, ...
after e2
We combine the above lexical and syntactic features
with their position information in the contexts to
form context vectors. Before that, we filter out low
frequency features which appeared only once in the
dataset.
3.2 Similarity Measures
The similarity sij between two occurrences of entity
pairs is important to the performance of the LP al-
gorithm. In this paper, we investigated two similar-
ity measures, cosine similarity measure and Jensen-
Shannon (JS) divergence (Lin, 1991). Cosine sim-
ilarity is commonly used semantic distance, which
measures the angle between two feature vectors. JS
divergence has ever been used as distance measure
for document clustering, which outperforms cosine
similarity based document clustering (Slonim et al,
2002). JS divergence measures the distance between
two probability distributions if feature vector is con-
sidered as probability distribution over features. JS
divergence is defined as follows:
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
JS(q, r) = 12 [DKL(q?p?) +DKL(r?p?)] (4)
DKL(q?p?) =
?
y
q(y)(log q(y)p?(y) ) (5)
DKL(r?p?) =
?
y
r(y)(log r(y)p?(y) ) (6)
where p? = 12(q + r) and JS(q, r) represents JS
divergence between probability distribution q(y) and
r(y) (y is a random variable), which is defined in
terms of KL-divergence.
3.3 Experimental Evaluation
3.3.1 Experiment Setup
We evaluated this label propagation based rela-
tion extraction method for relation subtype detection
and characterization task on the official ACE 2003
corpus. It contains 519 files from sources including
broadcast, newswire, and newspaper. We dealt with
only intra-sentence explicit relations and assumed
that all entities have been detected beforehand in the
EDT sub-task of ACE. Table 1 lists the types and
subtypes of relations for the ACE Relation Detection
and Characterization (RDC) task, along with their
132
Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes.
The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification
on relation subtypes. The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
frequency of occurrence in the ACE training set and
test set. We constructed labeled data by randomly
sampling some examples from ACE training data
and additionally sampling examples with the same
size from the pool of unrelated entity pairs for the
?NONE? class. We used the remaining examples in
the ACE training set and the whole ACE test set as
unlabeled data. The testing set was used for final
evaluation.
3.3.2 LP vs. SVM
Support Vector Machine (SVM) is a state of the
art technique for relation extraction task. In this ex-
periment, we use LIBSVM tool 3 with linear kernel
function.
For comparison between SVM and LP, we ran
SVM and LP with different sizes of labeled data
and evaluate their performance on unlabeled data
using precision, recall and F-measure. Firstly, we
ran SVM or LP algorithm to detect possible rela-
tions from unlabeled data. If an entity mention pair
is classified not to the ?NONE? class but to the other
24 subtype classes, then it has a relation. Then con-
struct labeled datasets with different sampling set
size l, including 1%?Ntrain, 10%?Ntrain, 25%?
Ntrain, 50%?Ntrain, 75%?Ntrain, 100%?Ntrain
(Ntrain is the number of examples in the ACE train-
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
ing set). If any relation subtype was absent from the
sampled labeled set, we redid the sampling. For each
size, we performed 20 trials and calculated average
scores on test set over these 20 random trials.
Table 2 reports the performance of SVM and LP
with different sizes of labled data for relation detec-
tion task. We used the same sampled labeled data in
LP as the training data for SVM model.
From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Specifically,
with small labeled dataset (percentage of labeled
data ? 25%), the performance improvement by LP
is significant. When the percentage of labeled data
increases from 50% to 100%, LPCosine is still com-
parable to SVM in F-measure while LPJS achieves
slightly better F-measure than SVM. On the other
hand, LPJS consistently outperforms LPCosine.
Table 3 reports the performance of relation clas-
sification by using SVM and LP with different sizes
of labled data. And the performance describes the
average values of Precision, Recall and F-measure
over major relation subtypes.
From Table 3, we see that LPCosine and LPJS out-
perform SVM by F-measure in almost all settings
of labeled data, which is due to the increase of Re-
call. With smaller labeled dataset (percentage of la-
beled data ? 50%), the gap between LP and SVM
is larger. When the percentage of labeled data in-
133
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
1% 10% 25% 50% 75% 100%
Percentage of Labeled Examples
F-m
ea
su
re SVM
LP_Cosine
LP_JS
 
Figure 1: Comparison of the performance of SVM
and LP with different sizes of labeled data
creases from 75% to 100%, the performance of LP
algorithm is still comparable to SVM. On the other
hand, the LP algorithm based on JS divergence con-
sistently outperforms the LP algorithm based on Co-
sine similarity. Figure 1 visualizes the accuracy of
three algorithms.
As shown in Figure 1, the gap between SVM
curve and LPJS curves is large when the percentage
of labeled data is relatively low.
3.3.3 An Example
In Figure 2, we selected 25 instances in train-
ing set and 15 instances in test set from the ACE
corpus,which covered five relation types. Using
Isomap tool 4, the 40 instances with 229 feature di-
mensions are visualized in a two-dimensional space
as the figure. We randomly sampled only one la-
beled example for each relation type from the 25
training examples as labeled data. Figure 2(a) and
2(b) show the initial state and ground truth result re-
spectively. Figure 2(c) reports the classification re-
sult on test set by SVM (accuracy = 415 = 26.7%),
and Figure 2(d) gives the classification result on both
training set and test set by LP (accuracy = 1115 =
73.3%).
Comparing Figure 2(b) and Figure 2(c), we find
that many examples are misclassified from class ?
to other class symbols. This may be caused that
SVMs method ignores the intrinsic structure in data.
For Figure 2(d), the labels of unlabeled examples
are determined not only by nearby labeled examples,
but also by nearby unlabeled examples, so using LP
4The tool is available at http://isomap.stanford.edu/.
     




	
      




	

     




	
      




	

Figure 2: An example: comparison of SVM and LP
algorithm on a data set from ACE corpus. ? and
4 denote the unlabeled examples in training set and
test set respectively, and other symbols (?,?,2,+
and 5) represent the labeled examples with respec-
tive relation type sampled from training set.
strategy achieves better performance than the local
consistency based SVM strategy when the size of
labeled data is quite small.
3.3.4 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance of the bootstrapped
SVM method from (Zhang, 2004) and LP method
with 100 seed labeled examples for relation type
classification task. We can see that LP algorithm
outperforms the bootstrapped SVM algorithm on
four relation type classification tasks, and perform
comparably on the relation ?SOC? classification
task.
4 Discussion
In this paper,we have investigated a graph-based
semi-supervised learning approach for relation ex-
traction problem. Experimental results showed that
the LP algorithm performs better than SVM and
134
Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed
labeled examples for relation type classification task.
Bootstrapping LPJS
Relation type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR ? ? ? 13.7 12.5 13.0
Table 5: Comparison of the performance of previous methods on ACE RDC task.
Relation Dectection Relation Detection and Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
bootstrapping. We have some findings from these
results:
The LP based relation extraction method can use
the graph structure to smooth the labels of unlabeled
examples. Therefore, the labels of unlabeled exam-
ples are determined not only by the nearby labeled
examples, but also by nearby unlabeled examples.
For supervised methods, e.g., SVM, very few la-
beled examples are not enough to reveal the struc-
ture of each class. Therefore they can not perform
well, since the classification hyperplane was learned
only from few labeled data and the coherent struc-
ture in unlabeled data was not explored when in-
ferring class boundary. Hence, our LP-based semi-
supervised method achieves better performance on
both relation detection and classification when only
few labeled data is available. Bootstrapping
Currently most of works on the RDC task of
ACE focused on supervised learning methods Cu-
lotta and Soresen (2004; Kambhatla (2004; Zhou
et al (2005). Table 5 lists a comparison on re-
lation detection and classification of these meth-
ods. Zhou et al (2005) reported the best result as
63.1%/49.5%/55.5% in Precision/Recall/F-measure
on the relation subtype classification using feature
based method, which outperforms tree kernel based
method by Culotta and Soresen (2004). Compared
with Zhou et al?s method, the performance of LP al-
gorithm is slightly lower. It may be due to that we
used a much simpler feature set. Our work in this
paper focuses on the investigation of a graph based
semi-supervised learning algorithm for relation ex-
traction. In the future, we would like to use more ef-
fective feature sets Zhou et al (2005) or kernel based
similarity measure with LP for relation extraction.
5 Conclusion and Future Work
This paper approaches the problem of semi-
supervised relation extraction using a label propaga-
tion algorithm. It represents labeled and unlabeled
examples and their distances as the nodes and the
weights of edges of a graph, and tries to obtain a
labeling function to satisfy two constraints: 1) it
should be fixed on the labeled nodes, 2) it should
be smooth on the whole graph. In the classifica-
tion process, the labels of unlabeled examples are
determined not only by nearby labeled examples,
but also by nearby unlabeled examples. Our exper-
imental results demonstrated that this graph based
algorithm can achieve better performance than SVM
when only very few labeled examples are available,
and also outperforms the bootstrapping method for
relation extraction task.
In the future, we would like to investigate more
effective feature set or use feature selection to im-
prove the performance of this graph-based semi-
supervised relation extraction method.
135
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries (ACMDL?00).
Belkin M. and Niyogi P.. 2002. Using Manifold Struc-
ture for Partially Labeled Classification. Advances in
Neural Infomation Processing Systems 15.
Blum A. and Chawla S. 2001. Learning from Labeled
and Unlabeled Data Using Graph Mincuts. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning.
Blum A., Lafferty J., Rwebangira R. and Reddy R. 2004.
Semi-Supervised Learning Using Randomized Min-
cuts. In Proceedings of the 21th International Confer-
ence on Machine Learning..
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB Work-
shop at 6th International Conference on Extending
Database Technology (WebDB?98). pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Hasegawa T., Sekine S. and Grishman R. 2004. Dis-
covering Relations among Named Entities from Large
Corpora, In Proceeding of Conference ACL2004.
Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics.. 21-26 July 2004. Barcelona, Spain.
Lin J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory. Vol 37, No.1, 145-150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In Proceedings of 6th Applied Natural Lan-
guage Processing Conference 29 April-4 may 2000,
Seattle USA.
Slonim, N., Friedman, N., and Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. In Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Yarowsky D. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In Proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction. In Proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning
from Labeled and Unlabeled Data with Label Propa-
gation. CMU CALD tech report CMU-CALD-02-107.
Zhu Xiaojin, Ghahramani Zoubin, and Lafferty J. 2003.
Semi-Supervised Learning Using Gaussian Fields and
Harmonic Functions. In Proceedings of the 20th Inter-
national Conference on Machine Learning.
136
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 89?96,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation Using Spectral Clustering
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised learn-
ing approach to disambiguate various rela-
tions between name entities by use of vari-
ous lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Laplacian
to recover a submanifold of data from a
high dimensionality space and then per-
forming cluster number estimation on the
eigenvectors. Experiment results on ACE
corpora show that this spectral cluster-
ing based approach outperforms the other
clustering methods.
1 Introduction
In this paper, we address the task of relation extrac-
tion, which is to find relationships between name en-
tities in a given context. Many methods have been
proposed to deal with this task, including supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithm (Hasegawa et al, 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
89
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity men-
tion pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
Words: Words in the two entities and three context
windows.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Entity Type: the entity type of both entities, which
can be PERSON, ORGANIZATION, FACIL-
ITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and three context windows. The ?0?
tag means that the word is outside of any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two entities
and three context windows. The last word
in each chunk is its head, and the function
of the head is the function of the whole
chunk. ?NP-SBJ? means a NP chunk as
the subject of the sentence. The other
words in a chunk that are not the head have
?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
90
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
91
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
2 In this paper, the sharpness parameter ? is set to 0.2
92
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE 2003 corpus, which contains 519
files from sources including broadcast, newswire,
and newspaper. We only deal with intra-sentence
explicit relations and assumed that all entities have
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with other two
unsupervised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
93
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with other unsupervised methods:
((Hasegawa et al, 2004))?s clustering method and K-means
clustering.
Precision Recall F-measure
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with other Unsupervised
methods
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
94
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our proposed
method comparing with the other two unsupervised
methods. Table 5 shows our proposed spectral based
method clearly outperforms the other two unsuper-
vised methods by 12.5% and 9.5% in F-measure re-
spectively. Moreover, the incorporation of various
lexical and syntactic features into Hasegawa et al
(2004)?s method2 makes it outperform Hasegawa et
al. (2004)?s method1 which only uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of entity
pairs, performed well on the unsupervised relation
extraction problem. Our experiments show that by
the choice of the distance parameter ?2, we can esti-
mate the cluster number which provides the tightest
clusters. We notice that the estimated cluster num-
ber is less than the number of ground truth classes
in most cases. The reason for this phenomenon may
be that some relation types can not be easily distin-
guished using the context information only. For ex-
ample, the relation subtypes ?Located?, ?Based-In?
and ?Residence? are difficult to disambiguate even
for human experts to differentiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the character-
istics of entity pairs. This perhaps dues to the fact
that two entity mentions are close to each other in
most of relations defined in ACE. Another observa-
tion from the result is that extending the outer con-
text window of entity mention pairs too much may
not improve the performance since the process may
incorporate more noise information and affect the
clustering result.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Generally, from the point of view of unsu-
pervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation ex-
traction problem by using spectral-based clustering
technique with diverse lexical and syntactic features
derived from context. The advantage of our method
is that it doesn?t need any manually labeled relation
instances, and pre-definition the number of the con-
text clusters. Experiment results on the ACE corpus
show that our method achieves better performance
than other unsupervised methods, i.e.Hasegawa et
al. (2004)?s method and Kmeans-based method.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
95
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
96
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
Proceedings of ACL-08: HLT, pages 559?567,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Tree Sequence Alignment-based Tree-to-Tree Translation Model 
 
 
Min Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li2
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg 
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn  
hli@i2r.a-star.edu.sg   
 
  
Abstract 
This paper presents a translation model that is 
based on tree sequence alignment, where a tree 
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages 
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with 
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous 
models, it not only captures non-syntactic 
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree 
typology with larger span. This gives our 
model stronger expressive power than other re-
ported models. Experimental results on the 
NIST MT-2005 Chinese-English translation 
task show that our method statistically signifi-
cantly outperforms the baseline systems.  
1 Introduction 
Phrase-based modeling method (Koehn et al, 
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can 
model local reorderings and translations of multi-
word expressions well. However, it cannot handle 
long-distance reorderings properly and does not 
exploit discontinuous phrases and linguistically 
syntactic structure features (Quirk and Menezes, 
2006). Recently, many syntax-based models have 
been proposed to address the above deficiencies 
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and 
Palmer, 2005; Quirk et al 2005; Cowan et al, 
2006; Zhang et al, 2007; Bod, 2007; Yamada and 
Knight, 2001; Liu et al, 2006; Liu et al, 2007; 
Gildea, 2003; Poutsma, 2000; Hearne and Way, 
2003). Although good progress has been reported, 
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase 
modeling, are still worth well studying. 
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of 
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence 1  as the basic 
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore, 
it not only describes non-syntactic phrases with 
syntactic structure information, but also supports 
multi-level tree structure reordering in larger span. 
These give our model much more expressive 
power and flexibility than those previous models. 
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al, 2007), 
a state-of-the-art phrase-based SMT system, and 
other linguistically syntax-based methods, such as 
SCFG-based and STSG-based methods (Zhang et 
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our 
model are very useful for performance improve-
ment while discontinuous phrase rules have less 
contribution and 2) tree sequence rules are able to 
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the 
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no 
contribution.  
The rest of this paper is organized as follows: 
Section 2 reviews previous work. Section 3 elabo-
                                                          
1 A tree sequence refers to an ordered sub-tree sequence that 
covers a phrase or a consecutive tree fragment in a parse tree. 
It is the same as the concept ?forest? used in Liu et al(2007).  
559
rates the modelling process while Sections 4 and 5 
discuss the training and decoding algorithms. The 
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7. 
2 Related Work 
Many techniques on linguistically syntax-based 
SMT have been proposed in literature. Yamada 
and Knight (2001) use noisy-channel model to 
transfer a target parse tree into a source sentence. 
Eisner (2003) studies how to learn non-isomorphic 
tree-to-tree/string mappings using a STSG. Ding 
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous 
dependency insertion grammar. Quirk et al (2005) 
propose a dependency treelet-based translation 
model. Cowan et al (2006) propose a feature-
based discriminative model for target language 
syntactic structures prediction, given a source 
parse tree. Huang et al (2006) study a TSG-based 
tree-to-string alignment model. Liu et al (2006) 
propose a tree-to-string model. Zhang et al 
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs 
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the 
translation process. However, most of them fail to 
utilize non-syntactic phrases well that are proven 
useful in the phrase-based methods (Koehn et al, 
2003). 
The formally syntax-based model for SMT was 
first advocated by Wu (1997). Xiong et al (2006) 
propose a MaxEnt-based reordering model for 
BTG (Wu, 1997) while Setiawan et al (2007) pro-
pose a function word-based reordering model for 
BTG. Chiang (2005)?s hierarchal phrase-based 
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive 
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005). 
In the last two years, many research efforts were 
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following, 
we review four representatives of them.   
1) Hassan et al (2007) integrate supertags (a 
kind of lexicalized syntactic description) into the 
target side of translation model and language mod-
el under the phrase-based translation framework, 
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor 
reordering model is further explored.  
2) Galley et al (2006) handle non-syntactic 
phrasal translations by traversing the tree upwards 
until a node that subsumes the phrase is reached. 
This solution requires larger applicability contexts 
(Marcu et al, 2006). However, phrases are utilized 
independently in the phrase-based method without 
depending on any contexts.  
3) Addressing the issues in Galley et al (2006), 
Marcu et al (2006) create an xRS rule headed by a 
pseudo, non-syntactic non-terminal symbol that 
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS 
rule that explains how the pseudo symbol can be 
combined with other genuine non-terminals for 
acquiring the genuine parse trees. The name of the 
pseudo non-terminal is designed to reflect the full 
realization of the corresponding rule. The problem 
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more 
complicated phenomena (Liu et al, 2007).  
4) Liu et al (2006) treat all bilingual phrases as 
lexicalized tree-to-string rules, including those 
non-syntactic phrases in training corpus. Although 
the solution shows effective empirically, it only 
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the 
translation probabilities of the bilingual phrases 
and other tree-to-string rules are not compatible 
since they are estimated independently, thus hav-
ing different parameter spaces. To address the 
above problems, Liu et al (2007) propose to use 
forest-to-string rules to enhance the expressive 
power of their tree-to-string model. As is inherent 
in a tree-to-string framework, Liu et al?s method 
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are 
not described by probabilities since they are con-
structed during decoding, rather than learned from 
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature 
counting the number of auxiliary rules used in a 
derivation for penalizing the use of forest-to-string 
and auxiliary rules. 
In this paper, an alternative solution is presented 
to combine the strengths of phrase-based and syn-
560
1( )
IT e
1( )
JT f
A
 
 
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation  
 
 
 
Figure 2: Two Examples of tree sequences 
 
 
 
Figure 3: Two examples of translation rules 
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts 
(Galley et al, 2006), nor depends on pseudo nodes 
(Marcu et al, 2006) or auxiliary rules (Liu et al, 
2007). We go beyond the single sub-tree mapping 
model to propose a tree sequence alignment-based 
translation model. To the best of our knowledge, 
this is the first attempt to empirically explore the 
tree sequence alignment based model in SMT.  
3 Tree Sequence Alignment Model 
3.1 Tree Sequence Translation Rule   
The leaf nodes of a sub-tree in a tree sequence can 
be either non-terminal symbols (grammar tags) or 
terminal symbols (lexical words). Given a pair of 
source and target parse trees 1( )
JT f and 1( )
IT e  in 
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree 
sequence translation rule r  is a pair of aligned tree 
sequences r =< 2
1
( )jjTS f , 21( )
i
iTS e , A%  >, where: 
z 2
1
( )jjTS f is a source tree sequence, covering 
the span [ 1 2,j j ] in 1( )
JT f , and 
z 2
1
( )iiTS e is a target one, covering the span 
[ 1 2,i i ] in 1( )
IT e , and 
z A% are the alignments between leaf nodes of 
two tree sequences, satisfying the following 
condition: 1 2 1 2( , ) :i j A i i i j j j? ? ? ? ? ? ?% . 
Fig. 3 shows two rules extracted from the tree pair 
shown in Fig. 1, where r1 is a tree-to-tree rule and 
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful 
than phrases or tree rules as they can capture all 
phrases (including both syntactic and non-syntactic 
phrases) with syntactic structure information and 
allow any tree node operations in a longer span. 
We expect that these properties can well address 
the issues of non-isomorphic structure alignments, 
structure reordering, non-syntactic phrases and 
discontinuous phrases translations. 
3.2 Tree Sequence Translation Model 
Given the source and target sentences 1
Jf and 1
Ie  
and their parse trees 1( )
JT f and 1( )
IT e , the tree 
sequence-to-tree sequence translation model is 
formulated as: 
1 1
1 1
1 1 1 1 1 1
( ), ( )
1 1
( ), ( )
1 1 1
1 1 1 1
( | ) ( , ( ), ( ) | )
( ( ( ) | )
( ( ) | ( ), )
( | ( ), ( ), ))
                
                      
                      
J I
J I
I J I I J J
T f T e
J J
T f T e
I J J
I I J J
r r
r
r
r
P e f P e T e T f f
P T f f
P T e T f f
P e T e T f f
=
=
?
?
?
? (1) 
In our implementation, we have: 
561
1) 1 1( ( ) | ) 1
J JrP T f f ? since we only use the best 
source and target parse tree pairs in training. 
2) 1 1 1 1( | ( ), ( ), ) 1
I I J JrP e T e T f f ? since we just 
output the leaf nodes of 1( )
IT e to generate 1
Ie  
regardless of source side information. 
Since 1( )
JT f contains the information of 1
Jf , 
now we have: 
1 1 1 1 1
1 1
( | ) ( ( ) | ( ), )
                 ( ( ) | ( ))
I J I J J
I J
r r
r
P e f P T e T f f
P T e T f
=
=
           (2) 
By Eq. (2), translation becomes a tree structure 
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source 
parse tree 1( )
JT f , there are multiple derivations 
that could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is obtained 
by summing over the probabilities of all deriva-
tions. The probability of each derivation? is given 
as the product of the probabilities of all the rules 
( )ip r  used in the derivation (here we assume that 
a rule is applied independently in a derivation). 
2 2
1 1
1 1 1 1( | ) ( ( ) | ( ))
     = ( : ( ), ( ), )
i
I J I J
i j
i i j
r
r rP e f P T e T f
p r TS e TS f A
? ??
=
< >?? %    (3) 
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how 
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the 
source parse tree is detached into two source tree 
sequences (the left hand side of rules in Fig. 3). 
Then the two rules in Fig. 3 are used to map the 
two source tree sequences to two target tree se-
quences, which are then combined to generate a 
target parse tree. Finally, a target translation is 
yielded from the target tree.  
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the commonly 
used features in phrase-based systems (Koehn, 
2004): 1) bidirectional rule mapping probabilities; 
2) bidirectional lexical rule translation probabilities; 
3) the target language model; 4) the number of 
rules used and 5) the number of target words. In 
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model?s 
preference for lexicalized rules over un-lexicalized 
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules. 
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we 
let these rules compete directly with each other. 
4 Rule Extraction 
Rules are extracted from word-aligned, bi-parsed 
sentence pairs 1 1( ), ( ),
J IT f T e A< > , which are 
classified into two categories: 
z initial rule, if all leaf nodes of the rule are 
terminals (i.e. lexical word), and 
z abstract rule, otherwise, i.e. at least one leaf 
node is a non-terminal (POS or phrase tag). 
Given an initial rule 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% , 
its sub initial rule is defined as a triple 
4 4
3 3
?( ), ( ),j ij iTS f TS e A< >  if and only if: 
z 4 4
3 3
?( ), ( ),j ij iTS f TS e A< > is an initial rule. 
z 3 4 3 4( , ) :i j A i i i j j j? ? ? ? ? ? ?% , i.e. 
A? A? %  
z 4
3
( )jjTS f is a sub-graph of 21( )
j
jTS f while  
4
3
( )iiTS e  is a sub-graph of 21( )
i
iTS e . 
Rules are extracted in two steps: 
1) Extracting initial rules first. 
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules. 
It is straightforward to extract initial rules. We 
first generate all fully lexicalized source and target 
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and 
target tree sequence pairs 2 2
1 1
( ), ( )j ij iTS f TS e< > . If 
the condition ? ( , )i j? 1 2 1 2:A i i i j j j? ? ? ? ? ? ? 
is satisfied, the triple 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% is 
an initial rule, where A%  are alignments between 
leaf nodes of 2
1
( )jjTS f  and 21( )
i
iTS e . We then de-
rive abstract rules from initial rules by removing 
one or more of its sub initial rules. The abstract 
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space 
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints 
for both finally extracted initial and abstract rules:  
1) The depth of a tree in a rule is not greater 
562
than h . 
2) The number of non-terminals as leaf nodes is 
not greater than c . 
3) The tree number in a rule is not greater than d. 
In addition, we limit initial rules to have at most 
seven lexical words as leaf nodes on either side. 
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with 
more than seven lexical words for abstract rules 
extraction only (not used in decoding). This makes 
our abstract rules more powerful in handling 
global structure reordering. Moreover, by configur-
ing these parameters we can implement other 
translation models easily: 1) STSG-based model  
when 1d = ; 2) SCFG-based model when 1d =  
and 2h = ; 3) phrase-based translation model only 
(no reordering model) when 0c =  and 1h = . 
 
Algorithm 1: abstract rules extraction 
Input: initial rule set inir  
Output: abstract rule set absr  
1: for each i inir r? , do 
2:    put all sub initial rules of ir  into a set subiniir
3:    for each subset subiniir? ? do 
4:          if there are spans overlapping between 
any two rules in the subset ?  then 
5:                    continue   //go to line 3 
6:           end if  
7:           generate an abstract rule by removing 
the portions covered by ?  from ir  and 
co-indexing the pairs of non-terminals 
that rooting the removed source and 
target parts 
8:           add them into the abstract rule set absr  
9:     end do 
10: end do  
 
5 Decoding 
Given 1( )
JT f , the decoder is to find the best deri-
vation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (4) 
Algorithm 2: Tree Sequence-based Decoder 
 Input: 1( )
JT f   Output: 1( )
IT e  
 Data structures: 
1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j  
1: for s = 0 to J -1 do      // s: span length 
2:     for 1j = 1 to J - s , 2j = 1j + s  do  
3:          for each rule r spanning 1 2[ , ]j j  do  
4:               if r  is an initial rule then 
5:                    insert r into 1 2[ , ]h j j  
6:               else 
7:      generate new translations from 
r by replacing non-terminal leaf 
nodes of r with their correspond-
ing spans? translations that are al-
ready translated in previous steps 
8:      insert them into 1 2[ , ]h j j  
9:  end if 
10: end for 
11: end for 
12: end for 
13: output the hypothesis with the highest score  
in [1, ]h J  as the final best translation 
 
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates 
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).  
This strategy can guarantee that when translating 
the current span, all spans smaller than the current 
one have already been translated before if they are 
translatable (line 7). When translating a span, if the 
usable rule is an initial rule, then the tree sequence 
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule 
with their corresponding spans? translations that 
are already translated in previous steps (line 7). To 
speed up the decoder, we use several thresholds to 
limit search beams for each span:  
1) ? , the maximal number of rules used 
2) ? , the minimal log probability of rules 
3) ? , the maximal number of translations yield  
It is worth noting that the decoder does not force 
a complete target parse tree to be generated. If no 
rules can be used to generate a complete target 
parse tree, the decoder just outputs whatever have 
563
been translated so far monotonically as one hy-
pothesis. 
6 Experiments 
6.1 Experimental Settings 
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the 
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing. We used sentences with 
less than 50 characters from the NIST MT-2002 
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford 
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The 
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al, 2002). We used GIZA++ (Och and Ney, 
2004) and the heuristics ?grow-diag-final? to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn?s MER 
trainer (Koehn, 2004) for our tree sequence-based 
system. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). 
We set three baseline systems: Moses (Koehn et 
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al, 2007). For 
Moses, we used its default settings. For the 
SCFG/STSG and our proposed model, we used the 
same settings except for the parameters d and h  
( 1d = and 2h = for the SCFG; 1d = and 6h = for 
the STSG; 4d =  and 6h = for our model). We 
optimized these parameters on the training and de-
velopment sets: c =3, ? =20, ? =-100 and ? =100. 
6.2 Experimental Results   
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based 
translation model. In this subsection, we first re-
port the rule distributions and compare our model 
with the three baseline systems. Then we study the 
model?s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase 
rules, structure reordering rules and discontinuous 
phrase rules2. Finally, we investigate the impact of 
maximal sub-tree number and sub-tree depth in our 
model. All of the following discussions are held on 
the training and test data. 
 
 
Rule 
 Initial Rules  Abstract Rules  
L P U Total 
BP 322,965 0 0  322,965
TR 443,010 144,459 24,871  612,340
TSR 225,570 103,932 714  330,216
 
Table 1: # of rules used in the testing ( 4d = , h =  6) 
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fully 
lexicalized, P: partially lexicalized, U: unlexicalized) 
 
Table 1 reports the statistics of rules used in the 
experiments. It shows that:  
1) We verify that the BPs are fully covered by 
the initial rules (i.e. lexicalized rules), in which the 
lexicalized TSRs model all non-syntactic phrase 
pairs with rich syntactic information. In addition, 
we find that the number of initial rules is greater 
than that of bilingual phrases. This is because one 
bilingual phrase can be covered by more than one 
initial rule which having different sub-tree struc-
tures. 
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The 
number of the abstract rule is far less than that of 
the initial rules. This is because leaf nodes of an 
abstract rule can be non-terminals that can 
represent any sub-trees using the non-terminals as 
roots.   
Fig. 4 compares the performance of different 
models. It illustrates that: 
1) Our tree sequence-based model significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method. 
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that: 
z The linguistically motivated structure features 
are very useful for SMT, which can be cap-
                                                          
2 To be precise, we examine the contributions of strict tree 
sequence rules and single tree rules separately in this section. 
Therefore, unless specified, the term ?tree sequence rules? 
used in this section only refers to the strict tree sequence rules, 
which must contain at least two sub-trees on the source side. 
564
tured by the two syntax-based models through 
tree node operations. 
z Our model is much more effective in utilizing 
linguistic structures than STSG since it uses 
tree sequence as basic translation unit. This 
allows our model not only to handle structure 
reordering by tree node operations in a larger 
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic 
constraints, thus giving our model more ex-
pressive power. 
3) The linguistically motivated SCFG shows 
much lower performance. This is largely because 
SCFG only allows sibling nodes reordering and fails 
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure transfer 
between Chinese and English when using Penn 
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG 
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005). 
This is because the formal syntax is learned from 
phrases directly without relying on any linguistic 
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and 
non-isomorphic structure alignment.  
24.71
26.07
23.86
22.72
21.5
22.5
23.5
24.5
25.5
26.5
SCFG Moses STSG Ours
BL
EU
(%
)
 
Figure 4: Performance comparison of different methods 
 
Rule  
Type 
TR 
(STSG) 
TR 
+TSR_L 
TR+TSR_L
+TSR_P 
TR 
+TSR 
BLEU(%) 24.71 25.72 25.93 26.07 
 
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table) 
 
Table 2 measures the contributions of different 
kinds of tree sequence rules. It suggests that: 
1) All the three kinds of TSRs contribute to the 
performance improvement and their combination 
further improves the performance. It suggests that 
they are complementary to each other since the 
lexicalized TSRs are used to model non-syntactic 
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases. 
2)  The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic 
phrases with syntactic structure features. 
 
Rule Type BLEU (%) 
TR+TSR 26.07 
(TR+TSR) w/o SRR 24.62 
(TR+TSR) w/o DPR 25.78 
 
Table 3: Effect of Structure Reordering Rules (SRR: 
refers to the structure reordering rules that have at least 
two non-terminal leaf nodes with inverted order in the 
source and target sides, which are usually not captured 
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not 
considered here) and Discontinuous Phrase Rules (DPR: 
refers to these rules having at least one non-terminal 
leaf node between two lexicalized leaf nodes) in our 
tree sequence-based model ( 4d =  and 6h = ) 
 
Rule Type # of rules # of rules overlapped 
(Intersection) 
SRR 68,217 18,379 (26.9%) 
DPR 57,244 18,379 (32.1%) 
 
Table 4: numbers of SRR and DPR rules 
 
Table 3 shows the contributions of SRR and 
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score. 
However, DPRs have less impact on performance 
in our tree sequence-based model. This seems in 
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation 
units. Thereby, our model can capture all phrases. 
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
                                                          
3 Wellington et al (2006) reports that discontinuities are very 
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree 
constraints while they are almost of no use if under word 
alignment constraints only. Bod (2007) finds that discontinues 
phrase rules make significant performance improvement in 
linguistically STSG-based SMT models. 
565
es (Wellington et al, 2006). Our additional expe-
riments also verify that discontinuous phrase rules 
are complementary to syntactic phrase rules (Bod, 
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase 
rules. Table 4 reports the numbers of these two 
kinds of rules. It shows that around 30% rules are 
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf 
nodes plus two terminal leaf nodes, which implies 
that longer rules do not affect performance too 
much. 
 
22.07
25.28
26.1425.94 26.02 26.07
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5 6
BL
EU
(%
)
 
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when 4d = ) 
 
22.72
24.71
26.0526.03 26.07
25.74
25.2925.2825.2624.78
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5
B
LE
U
(%
)
 
Figure 6: Accuracy changing with the different maximal 
number of trees in a tree sequence ( d =1 to 5), the upper 
line is for 6h =  while the lower line is for 2h = .  
 
Fig. 5 studies the impact when setting different 
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:  
1) Significant performance improvement is 
achieved when the value of h  is increased from 1 
to 2. This can be easily explained by the fact that 
when h = 1, only monotonic search is conducted, 
while h = 2 allows non-terminals to be leaf nodes, 
thus introducing preliminary structure features to 
the search and allowing non-monotonic search. 
2) Internal structures and large span (due to h  
increasing) are also useful as attested by the gain 
of 0.86 (26.14-25.28) Blue score when the value of 
h  increases from 2 to 4. 
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It 
further indicates that: 
1) Tree sequence rules (d >1) are useful and 
even more helpful if we limit the tree depth to no 
more than two (lower line, h=2). However, tree 
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data 
sparseness issue when d >3. 
2) Even if only two-layer sub-trees (lower line) 
are allowed, our method still outperforms STSG 
and Moses when d>1. This further validates the 
effectiveness of our design philosophy of using 
multi-sub-trees as basic translation unit in SMT. 
7 Conclusions and Future Work 
In this paper, we present a tree sequence align-
ment-based translation model to combine the 
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate 
the effectiveness of the proposed model. Our study 
also finds that in our model the tree sequence rules 
are very useful since they can model non-syntactic 
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree 
sequence rules with more than three sub-trees have 
less impact on performance. 
There are many interesting research topics on 
the tree sequence-based translation model worth 
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus, 
effective rule optimization and pruning algorithms 
are highly desirable. Ideally, a linguistically and 
empirically motivated theory can be worked out, 
suggesting what kinds of rules should be extracted 
given an input phrase pair. For example, most 
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We 
will study direct structure alignments to reduce the 
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al, 2007). Finally, we 
would also like to study unsupervised learning-
based bilingual parsing for SMT.  
566
 References  
Rens Bod. 2007. Unsupervised Syntax-Based Machine 
Translation: The Contribution of Discontinuous 
Phrases. MT-Summmit-07. 51-56. 
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? HLT-
NAACL-04. 
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
Daniel Gildea. 2003. Loosely Tree-Based Alignment for 
Machine Translation. ACL-03. 80-87. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. HLT-NAACL-2004. 105-112. 
Mary Hearne and Andy Way. 2003. Seeing the wood for 
the trees: data-oriented translation. MT Summit IX, 
165-172. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. AMTA-04, 115-124 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and Hermann Ney. 2004a. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk and Arul Menezes. 2006. Do we need 
phrases? Challenging the conventional wisdom in 
SMT. COLING-ACL-06. 9-16. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06. 
248-255. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. 
Ordering Phrases with Function Words. ACL-7. 
712-719. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521? 528. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04. 2051-
2054. 
567
Proceedings of ACL-08: HLT, pages 843?851,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
1Institute for Infocomm Research
{xiaofengy,sujian}@i2r.a-star.edu.sg
2Harbin Institute of Technology
{bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
3National University of Singapore,
tancl@comp.nus.edu.sg
Abstract
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
1 Introduction
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al, 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al, 2004; Culotta
et al, 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, ?whether the entity has any men-
tion that is a name alias of the active mention?? or
?whether most of the mentions in the entity have the
same head word as the active mention?? These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
843
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
2 Related Work
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a ?best-
first? linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an ?Any-X? strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al, 2004), the entity-
level features are computed using a ?Most-X? strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al, 2003), WSD (Specia et al, 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
3 Modelling Coreference Resolution
Suppose we have a document containing n mentions
{mj : 1 < j < n}, in which mj is the jth mention
occurring in the document. Let ei be the ith entity in
the document. We define
P (L|ei,mj), (1)
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of ei.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
P (L|ei,mj)
? P (L|{mk ? ei, 1 ? k ? j ? 1},mj) (2)
? max
mk?ei,1?k?j?1
P (L|mk,mj) (3)
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
844
[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23
[ yesterday ]34. [ The company ]15 said [ he ]26 will . . .
Table 1: A sample text
pair score. Both (2) and (1) can be approximated
with a machine learning method, leading to the tra-
ditional mention-pair model and the entity-mention
model for coreference resolution, respectively.
The two models will be described in the next sub-
sections, with the sample text in Table 1 used for
demonstration. In the table, a mention m is high-
lighted as [ m ]eidmid, where mid and eid are the IDs
for the mention and the entity to which it belongs,
respectively. Three entity chains can be found in the
text, that is,
e1 : Microsoft Corp. - its - The company
e2 : its new CEO - he
e3 : yesterday
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun ?he?, three
instances are generated: i(?The company?,?he?),
i(?yesterday?,?he?), and i(?its new CEO?,?he?).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
encountered mention mj , a test instance is formed
for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
3.2 Entity-Mention Model
The mention-based solution has a limitation that in-
formation beyond a mention pair cannot be captured.
As an individual mention usually lacks complete de-
scription about the referred entity, the coreference
relationship between two mentions may be not clear,
which would affect classifier learning. Consider
a document with three coreferential mentions ?Mr.
Powell?, ?he?, and ?Powell?, appearing in that or-
der. The positive training instance i(?he?, ?Powell?)
is not informative, as the pronoun ?he? itself dis-
closes nothing but the gender. However, if the whole
entity is considered instead of only one mention, we
can know that ?he? refers to a male person named
?Powell?. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
?Mr. Powell?, ?Powell?, and ?she? in a document.
The model tends to link ?she? with ?Powell? be-
cause of their proximity. This error can be avoided,
if we know ?Powell? belongs to the entity starting
with ?Mr. Powell?, and therefore refers to a male
person and cannot co-refer with ?she?.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj . During train-
ing, given each anaphoric mention mj , one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj .
See the sample in Table 1 again. For the pronoun
?he?, the following three instances are generated for
845
Features describing an active mention, mj
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is a definite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is a pronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
Features describing the relationships between mk and mj
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0
Table 2: Feature set for coreference resolution
entity e1, e3 and e2:
i({?Microsoft Corp.?, ?its?, ?The company?},?he?),
i({?yesterday?},?he?),
i({?its new CEO?},?he?).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj , a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
4 Entity-mention Model with ILP
4.1 Motivation
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk ? ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
846
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ ? E?, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E?, i.e., K ? h |= E+ and K ? h 6|= E?.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
4.2 Apply ILP to coreference resolution
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
?he?). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?the
company?}, while e1 5 denotes the part of e1 be-
fore m5 (?The company?), i.e., {?Microsoft Corp.?,
?its?}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j , mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun ?he? are represented as follows:
link(e1 6,m6).
link(e3 6,m6).
link(e2 6,m6).
The first two predicates are put into E?, while the
last one is put to E+.
The background knowledge for an instance
link(ei j , mj) is also represented with predicates,
which are divided into the following types:
1. Predicates describing the information related to
ei j and mj . The properties of mj are pre-
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
sented with predicates like f (m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun ?he? can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6, 1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f (e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj . v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj , e.g,
entNumAgree(e1 6,m6, 1).
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
has mention(e1 6,m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
3. Predicates describing the information related to
mj and each mention mk in ei j . The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (?Microsoft
Corp.) and m6 (?he), the following predicates
can be applied:
nameNP(m1, 1).
pron(m1, 0).
. . .
nameAlias(m1,m6, 0).
sentDist(m1,m6, 1).
. . .
the last two predicates represent that m1 and
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
847
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the ?global? information about the entity, as well as
the ?local? information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, . . . , has mention(A,C), . . . , prediN.
which corresponds to first-order logic
?A,B(predi1 ? predi2 ? . . .?
?C(has mention(A,C) ? . . . ? prediN)
? link(A,B))
Consider an example rule produced in our system:
link(A,B) :-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
Table 3: statistics of entities (length > 1) and contained
mentions
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
5 Experiments and Results
5.1 Experimental Setup
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al (1995)?s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
5.2 Results and Discussions
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
848
NWire NPaper BNews
R P F R P F R P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
Table 4: Results of different systems for coreference resolution
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
?Any-X? based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
?0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al
(2004) that the entity-mention model with the ?Any-
X? first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the ?Most-X? strategy for the first-order features as
in (Culotta et al, 2007), but got similar results with-
out much difference (?0.5% F-measure) in perfor-
4http://www.rulequest.com/see5-info.html
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the ?All-
X? strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the ?Any-X? strategy.
As described, given an active mention mj , the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (<1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
849
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
...
Figure 1: Examples of rules produced by ILP (entity-
mention model)
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p <
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al, 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
6 Conclusions
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al, 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union?s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
850
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122?129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493?525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL), pages 81?88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236?243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135?142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79?86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050?1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3?24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104?111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157?164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536?543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41?48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528?535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127?134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219?225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71?79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473?480, Philadel-
phia.
851
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172?180,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Forest-based Tree Sequence to String Translation Model 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Aiti Aw1   Chew Lim Tan2 
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg  
 
 
 
 
Abstract 
This paper proposes a forest-based tree se-
quence to string translation model for syntax- 
based statistical machine translation, which 
automatically learns tree sequence to string 
translation rules from word-aligned source-
side-parsed bilingual texts. The proposed 
model leverages on the strengths of both tree 
sequence-based and forest-based translation 
models. Therefore, it can not only utilize forest 
structure that compactly encodes exponential 
number of parse trees but also capture non-
syntactic translation equivalences with linguis-
tically structured information through tree se-
quence. This makes our model potentially 
more robust to parse errors and structure di-
vergence. Experimental results on the NIST 
MT-2003 Chinese-English translation task 
show that our method statistically significantly 
outperforms the four baseline systems. 
1 Introduction 
Recently syntax-based statistical machine trans-
lation (SMT) methods have achieved very prom-
ising results and attracted more and more inter-
ests in the SMT research community. Fundamen-
tally, syntax-based SMT views translation as a 
structural transformation process. Therefore, 
structure divergence and parse errors are two of 
the major issues that may largely compromise 
the performance of syntax-based SMT (Zhang et 
al., 2008a; Mi et al, 2008).  
Many solutions have been proposed to address 
the above two issues. Among these advances, 
forest-based modeling (Mi et al, 2008; Mi and 
Huang, 2008) and tree sequence-based modeling 
(Liu et al, 2007; Zhang et al, 2008a) are two 
interesting modeling methods with promising 
results reported. Forest-based modeling aims to 
improve translation accuracy through digging the 
potential better parses from n-bests (i.e. forest) 
while tree sequence-based modeling aims to 
model non-syntactic translations with structured 
syntactic knowledge. In nature, the two methods 
would be complementary to each other since 
they manage to solve the negative impacts of 
monolingual parse errors and cross-lingual struc-
ture divergence on translation results from dif-
ferent viewpoints. Therefore, one natural way is 
to combine the strengths of the two modeling 
methods for better performance of syntax-based 
SMT. However, there are many challenges in 
combining the two methods into a single model 
from both theoretical and implementation engi-
neering viewpoints. In theory, one may worry 
about whether the advantage of tree sequence has 
already been covered by forest because forest 
encodes implicitly a huge number of parse trees 
and these parse trees may generate many differ-
ent phrases and structure segmentations given a 
source sentence. In system implementation, the 
exponential combinations of tree sequences with 
forest structures make the rule extraction and 
decoding tasks much more complicated than that 
of the two individual methods.  
In this paper, we propose a forest-based tree 
sequence to string model, which is designed to 
integrate the strengths of the forest-based and the 
tree sequence-based modeling methods. We pre-
sent our solutions that are able to extract transla-
tion rules and decode translation results for our 
model very efficiently. A general, configurable 
platform was designed for our model. With this 
platform, we can easily implement our method 
and many previous syntax-based methods by 
simple parameter setting. We evaluate our 
method on the NIST MT-2003 Chinese-English 
translation tasks. Experimental results show that 
our method significantly outperforms the two 
individual methods and other baseline methods. 
Our study shows that the proposed method is 
able to effectively combine the strengths of the 
forest-based and tree sequence-based methods, 
and thus having great potential to address the 
issues of parse errors and non-syntactic transla-
172
tions resulting from structure divergence. It also 
indicates that tree sequence and forest play dif-
ferent roles and make contributions to our model 
in different ways. 
The remainder of the paper is organized as fol-
lows. Section 2 describes related work while sec-
tion 3 defines our translation model. In section 4 
and section 5, the key rule extraction and decod-
ing algorithms are elaborated. Experimental re-
sults are reported in section 6 and the paper is 
concluded in section 7. 
2 Related work  
As discussed in section 1, two of the major chal-
lenges to syntax-based SMT are structure diver-
gence and parse errors. Many techniques have 
been proposed to address the structure diver-
gence issue while only fewer studies are reported 
in addressing the parse errors in the SMT re-
search community. 
To address structure divergence issue, many 
researchers (Eisner, 2003; Zhang et al, 2007) 
propose using the Synchronous Tree Substitution 
Grammar (STSG) grammar in syntax-based 
SMT since the STSG uses larger tree fragment as 
translation unit. Although promising results have 
been reported, STSG only uses one single sub-
tree as translation unit which is still committed to 
the syntax strictly. Motivated by the fact that 
non-syntactic phrases make non-trivial contribu-
tion to phrase-based SMT, the tree sequence-
based translation model is proposed (Liu et al, 
2007; Zhang et al, 2008a) that uses tree se-
quence as the basic translation unit, rather than 
using single sub-tree as in the STSG. Here, a tree 
sequence refers to a sequence of consecutive 
sub-trees that are embedded in a full parse tree. 
For any given phrase in a sentence, there is at 
least one tree sequence covering it. Thus the tree 
sequence-based model has great potential to ad-
dress the structure divergence issue by using tree 
sequence-based non-syntactic translation rules. 
Liu et al (2007) propose the tree sequence con-
cept and design a tree sequence to string transla-
tion model. Zhang et al (2008a) propose a tree 
sequence-based tree to tree translation model and 
Zhang et al (2008b) demonstrate that the tree 
sequence-based modelling method can well ad-
dress the structure divergence issue for syntax-
based SMT. 
To overcome the parse errors for SMT, Mi et 
al. (2008) propose a forest-based translation 
method that uses forest instead of one best tree as 
translation input, where a forest is a compact rep-
resentation of exponentially number of n-best 
parse trees. Mi and Huang (2008) propose a for-
est-based rule extraction algorithm, which learn 
tree to string rules from source forest and target 
string. By using forest in rule extraction and de-
coding, their methods are able to well address the 
parse error issue. 
From the above discussion, we can see that 
traditional tree sequence-based method uses sin-
gle tree as translation input while the forest-
based model uses single sub-tree as the basic 
translation unit that can only learn tree-to-string 
(Galley et al 2004; Liu et al, 2006) rules. There-
fore, the two methods display different strengths, 
and which would be complementary to each 
other. To integrate their strengths, in this paper, 
we propose a forest-based tree sequence to string 
translation model.  
3 Forest-based tree sequence to string 
model  
In this section, we first explain what a packed 
forest is and then define the concept of the tree 
sequence in the context of forest followed by the 
discussion on our proposed model. 
3.1 Packed Forest 
A packed forest (forest in short) is a special kind 
of hyper-graph (Klein and Manning, 2001; 
Huang and Chiang, 2005), which is used to rep-
resent all derivations (i.e. parse trees) for a given 
sentence under a context free grammar (CFG). A 
forest F is defined as a triple ? ?, ?, ? ?, where 
? is non-terminal node set, ?  is hyper-edge set 
and ? is leaf node set (i.e. all sentence words). A 
forest F satisfies the following two conditions: 
 
1) Each node ?  in ?  should cover a phrase, 
which is a continuous word sub-sequence in ?. 
2) Each hyper-edge ?  in ?  is defined as 
?? ? ?? ??? ? ??, ??? ? ?? ? ??, ?? ? ?? , 
where ?? ? ?? ???  covers a sequence of conti-
nuous and non-overlap phrases, ??  is the father 
node of the children sequence ?? ??? ???. The 
phrase covered by ??  is just the sum of all the 
phrases covered by each child node ??. 
 
We here introduce another concept that is used 
in our subsequent discussions. A complete forest 
CF is a general forest with one additional condi-
tion that there is only one root node N in CF, i.e., 
all nodes except the root N in a CF must have at 
least one father node. 
Fig. 1 is a complete forest while Fig. 7 is a 
non-complete forest due to the virtual node 
?VV+VV? introduced in Fig. 7. Fig. 2 is a hyper-
edge (IP => NP VP) of Fig. 1, where NP covers 
173
the phrase ?Xinhuashe?, VP covers the phrase 
?shengming youguan guiding? and IP covers the 
entire sentence. In Fig.1, only root IP has no fa-
ther node, so it is a complete forest. The two 
parse trees T1 and T2 encoded in Fig. 1 are 
shown separately in Fig. 3 and Fig. 41.  
Different parse tree represents different deri-
vations and explanations for a given sentence. 
For example, for the same input sentence in Fig. 
1, T1 interprets it as ?XNA (Xinhua News 
Agency) declares some regulations.? while T2 
interprets it as ?XNA declaration is related to 
some regulations.?.  
 
 
 
Figure 1. A packed forest for sentence ????
/Xinhuashe ?? /shengming ?? /youguan ??
/guiding? 
             
Figure 2.  A hyper-edge used in Fig. 1 
 
       
 
Figure 3. Tree 1 (T1)            Figure 4. Tree 2 (T2) 
3.2 Tree sequence in packed forest 
Similar to the definition of tree sequence used in 
a single parse tree defined in Liu et al (2007) 
and Zhang et al (2008a), a tree sequence in a 
forest also refers to an ordered sub-tree sequence 
that covers a continuous phrase without overlap-
ping. However, the major difference between 
                                                          
1 Please note that a single tree (as T1 and T2 shown in Fig. 
3 and Fig. 4) is represented by edges instead of hyper-edges. 
A hyper-edge is a group of edges satisfying the 2nd condi-
tion as shown in the forest definition. 
them lies in that the sub-trees of a tree sequence 
in forest may belongs to different single parse 
trees while, in a single parse tree-based model, 
all the sub-trees in a tree sequence are committed 
to the same parse tree.  
The forest-based tree sequence enables our 
model to have the potential of exploring addi-
tional parse trees that may be wrongly pruned out 
by the parser and thus are not encoded in the for-
est. This is because that a tree sequence in a for-
est allows its sub-trees coming from different 
parse trees, where these sub-trees may not be 
merged finally to form a complete parse tree in 
the forest. Take the forest in Fig. 1 as an exam-
ple, where ((VV shengming) (JJ youguan)) is a 
tree sequence that all sub-trees appear in T1 
while ((VV shengming) (VV youguan)) is a tree 
sequence whose sub-trees do not belong to any 
single tree in the forest. But, indeed the two sub-
trees (VV shengming) and (VV youguan) can be 
merged together and further lead to a complete 
single parse tree which may offer a correct inter-
pretation to the input sentence (as shown in Fig. 
5). In addition, please note that, on the other 
hand, more parse trees may introduce more noisy 
structures. In this paper, we leave this problem to 
our model and let the model decide which sub-
structures are noisy features. 
 
          
 
 Figure 5. A parse tree that was wrongly 
pruned out 
 
            
    Figure 6. A tree sequence to string rule 
 
174
A tree-sequence to string translation rule in a 
forest is a triple <L, R, A>, where L is the tree 
sequence in source language, R is the string con-
taining words and variables in target language, 
and A is the alignment between the leaf nodes of 
L and R. This definition is similar to that of (Liu 
et al 2007, Zhang et al 2008a) except our tree-
sequence is defined in forest. The shaded area of 
Fig. 6 exemplifies a tree sequence to string trans-
lation rule in the forest.  
3.3 Forest-based tree-sequence to string 
translation model 
Given a source forest F and target translation TS 
as well as word alignment A, our translation 
model is formulated as: 
  
 Pr??, ??, ?? ? ? ? ????????????? ?,???????, ??,??  
 
By the above Eq., translation becomes a tree 
sequence structure to string mapping issue. Giv-
en the F, TS and A, there are multiple derivations 
that could map F to TS under the constraint A. 
The mapping probability Pr??, ??, ??  in our 
study is obtained by summing over the probabili-
ties of all derivations ?. The probability of each 
derivation ?? is given as the product of the prob-
abilities of all the rules ( )ip r  used in the deriva-
tion (here we assume that each rule is applied 
independently in a derivation). 
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the common-
ly used features in phrase-based systems (Koehn, 
2003): 1) bidirectional rule mapping probabilities, 
2) bidirectional lexical rule translation probabili-
ties, 3) target language model, 4) number of rules 
used and 5) number of target words. In addition, 
we define two new features: 1) number of leaf 
nodes in auxiliary rules (the auxiliary rule will be 
explained later in this paper) and 2) product of 
the probabilities of all hyper-edges of the tree 
sequences in forest. 
4 Training  
This section discusses how to extract our transla-
tion rules given a triple ? ?, ??, ? ? . As we 
know, the traditional tree-to-string rules can be 
easily extracted from ? ?, ??, ? ? using the algo-
rithm of Mi and Huang (2008)2. We would like 
                                                          
2 Mi and Huang (2008) extend the tree-based rule extraction 
algorithm (Galley et al, 2004) to forest-based by introduc-
ing non-deterministic mechanism. Their algorithm consists 
of two steps, minimal rule extraction and composed rule 
generation. 
to leverage on their algorithm in our study. Un-
fortunately, their algorithm is not directly appli-
cable to our problem because tree rules have only 
one root while tree sequence rules have multiple 
roots. This makes the tree sequence rule extrac-
tion very complex due to its interaction with for-
est structure. To address this issue, we introduce 
the concepts of virtual node and virtual hyper-
edge to convert a complete parse forest ?  to a 
non-complete forest ? which is designed to en-
code all the tree sequences that we want. There-
fore, by doing so, the tree sequence rules can be 
extracted from a forest in the following two 
steps: 
1) Convert the complete parse forest ? into a 
non-complete forest ?  in order to cover those 
tree sequences that cannot be covered by a single 
tree node. 
2) Employ the forest-based tree rule extraction 
algorithm (Mi and Huang, 2008) to extract our 
rules from the non-complete forest. 
To facilitate our discussion, here we introduce 
two notations:  
? Alignable: A consecutive source phrase is 
an alignable phrase if and only if it can be 
aligned with at least one consecutive target 
phrase under the word-alignment con-
straint. The covered source span is called 
alignable span. 
? Node sequence: a sequence of nodes (ei-
ther leaf or internal nodes) in a forest cov-
ering a consecutive span. 
Algorithm 1 illustrates the first step of our rule 
extraction algorithm, which is a CKY-style Dy-
namic Programming (DP) algorithm to add vir-
tual nodes into forest. It includes the following 
steps: 
1) We traverse the forest to visit each span in 
bottom-up fashion (line 1-2), 
1.1) for each span [u,v] that is covered by 
single tree nodes3, we put these tree 
nodes into the set NSS(u,v) and go 
back to step 1 (line 4-6). 
1.2) otherwise we concatenate the tree se-
quences of sub-spans to generate the 
set of tree sequences covering the cur-
rent larger span (line 8-13). Then, we 
prune the set of node sequences (line 
14). If this span is alignable, we 
create virtual father nodes and corres-
ponding virtual hyper-edges to link 
the node sequences with the virtual 
father nodes (line 15-20). 
                                                          
3 Note that in a forest, there would be multiple single tree 
nodes covering the same span as shown Fig.1.  
175
2) Finally we obtain a forest with each align-
able span covered by either original tree 
nodes or the newly-created tree sequence 
virtual nodes. 
Theoretically, there is exponential number of 
node sequences in a forest. Take Fig. 7 as an ex-
ample. The NSS of span [1,2] only contains ?NP? 
since it is alignable and covered by the single 
tree node NP. However, span [2,3] cannot be 
covered by any single tree node, so we have to 
create the NSS of span[2,3] by concatenating the 
NSSs of span [2,2] and span [3,3]. Since NSS of 
span [2,2] contains 4 element {?NN?, ?NP?, 
?VV?, ?VP?} and NSS of span [3, 3] also con-
tains 4 element {?VV?, ?VP?, ?JJ?, ?ADJP?}, 
NSS of span [2,3] contains 16=4*4 elements. To 
make the NSS manageable, we prune it with the 
following thresholds: 
? each node sequence should contain less 
than n nodes 
? each node sequence set should contain less 
than m node sequences 
? sort node sequences according to their 
lengths and only keep the k shortest ones 
Each virtual node is simply labeled by the 
concatenation of all its children?s labels as 
shown in Fig. 7. 
 
Algorithm 1. add virtual nodes into forest 
Input: packed forest F, alignment A 
Notation:  
   L: length of source sentence 
   NSS(u,v): the set of node sequences covering span [u,v] 
  VN(ns): virtual father node for node sequence ns. 
Output: modified forest F with virtual nodes 
 
 
1. for length := 0 to L - 1 do 
2.      for start := 1 to L - length do 
3.          stop := start + length 
4.          if span[start, stop] covered by tree nodes then 
5.                for each node n of span [start, stop] do 
6.                    add n into NSS(start, stop) 
7.          else  
8.                for pivot := start to stop - 1 
9.                     for each ns1 in NSS(start, pivot) do 
10.                          for each ns2 in NSS(pivot+1, stop) do 
11.                               create ?? ?? ?1? ?  ?2?  
12.                                if ns is not in NSS(start, stop) then 
13.                                      add ns into NSS(start, stop) 
14.                do pruning on NSS(start, stop) 
15.                if the span[start, stop] is alignable then 
16.                    for each ns of NSS(start, stop) do 
17.                   if node VN(ns) is not in F then 
18.                                add node VN(ns) into F 
19.                          add a hyper-edge h into F,  
20.                          let lhs(h) := VN(ns), rhs(h) := ns 
 
Algorithm 1 outputs a non-complete forest CF 
with each alignable span covered by either tree 
nodes or virtual nodes. Then we can easily ex-
tract our rules from the CF using the tree rule 
extraction algorithm (Mi and Huang, 2008). 
Finally, to calculate rule feature probabilities 
for our model, we need to calculate the fractional 
counts (it is a kind of probability defined in Mi 
and Huang, 2008) of each translation rule in a 
parse forest. In the tree case, we can use the in-
side-outside-based methods (Mi and Huang 
2008) to do it. In the tree sequence case, since 
the previous method cannot be used directly, we 
provide another solution by making an indepen-
dent assumption that each tree in a tree sequence 
is independent to each other. With this assump-
tion, the fractional counts of both tree and tree 
sequence can be calculated as follows: 
 
???? ? ?????????????????   
 
???????? ? ? ????
????????????
? ? ????
??????
? ? ????
??????????????
 
 
where ???? is the fractional counts to be calcu-
lated for rule r, a frag is either lhs(r) (excluding 
virtual nodes and virtual hyper-edges) or any tree 
node in a forest, TOP is the root of the forest, 
??. ? and ??.) are the outside and inside probabil-
ities of nodes, ?????. ? returns the root nodes of a 
tree sequence fragment, ???????. ?  returns the 
leaf nodes of a tree sequence fragment, ???? is 
the hyper-edge probability. 
 
 
 
              Figure 7. A virtual node in forest 
5 Decoding  
We benefit from the same strategy as used in our 
rule extraction algorithm in designing our decod-
ing algorithm, recasting the forest-based tree se-
quence-to-string decoding problem as a forest-
based tree-to-string decoding problem. Our de-
coding algorithm consists of four steps: 
1) Convert the complete parse forest to a non-
complete one by introducing virtual nodes. 
176
2) Convert the non-complete parse forest into 
a translation forest4 ?? by using the translation 
rules and the pattern-matching algorithm pre-
sented in Mi et al (2008). 
3) Prune out redundant nodes and add auxil-
iary hyper-edge into the translation forest for 
those nodes that have either no child or no father. 
By this step, the translation forest ?? becomes a 
complete forest.  
4) Decode the translation forest using our 
translation model and a dynamic search algo-
rithm. 
The process of step 1 is similar to Algorithm 1 
except no alignment constraint used here. This 
may generate a large number of additional virtual 
nodes; however, all redundant nodes will be fil-
tered out in step 3. In step 2, we employ the tree-
to-string pattern match algorithm (Mi et al, 
2008) to convert a parse forest to a translation 
forest. In step 3, all those nodes not covered by 
any translation rules are removed. In addition, 
please note that the translation forest is already 
not a complete forest due to the virtual nodes and 
the pruning of rule-unmatchable nodes. We, 
therefore, propose Algorithm 2 to add auxiliary 
hyper-edges to make the translation forest com-
plete.  
In Algorithm 2, we travel the forest in bottom-
up fashion (line 4-5). For each span, we do: 
1) generate all the NSS for this span (line 7-12)  
2) filter the NSS to a manageable size (line 13) 
3) add auxiliary hyper-edges for the current 
span (line 15-19) if it can be covered by at least 
one single tree node, otherwise go to step 1 . This 
is the key step in our Algorithm 2. For each tree 
node and each node sequences covering the same 
span (stored in the current NSS), if the tree node 
has no children or at least one node in the node 
sequence has no father, we add an auxiliary hy-
per-edge to connect the tree node as father node 
with the node sequence as children. Since Algo-
rithm 2 is DP-based and traverses the forest in a 
bottom-up way, all the nodes in a node sequence 
should already have children node after the lower 
level process in a small span. Finally, we re-build 
the NSS of current span for upper level NSS 
combination use (line 20-22). 
 
 In Fig. 8, the hyper-edge ?IP=>NP VV+VV 
NP? is an auxiliary hyper-edge introduced by 
Algorithm 2. By Algorithm 2, we convert the 
translation forest into a complete translation for-
est. We then use a bottom-up node-based search 
                                                          
4 The concept of translation forest is proposed in Mi et 
al. (2008). It is a forest that consists of only the hyper-
edges induced from translation rules. 
algorithm to do decoding on the complete trans-
lation forest. We also use Cube Pruning algo-
rithm (Huang and Chiang 2007) to speed up the 
translation process. 
 
 
 
Figure 8. Auxiliary hyper-edge in a translation 
forest 
 
Algorithm 2. add auxiliary hyper-edges into mt forest F 
Input:  mt forest F 
Output: complete forest F with auxiliary hyper-edges 
 
1. for i := 1 to L do 
2.      for each node n of span [i, i] do 
3.          add n into NSS(i, i) 
4. for length := 1 to L - 1 do 
5.      for start := 1 to L - length do 
6.          stop := start + length 
7.          for pivot := start to stop-1 do 
8.               for each ns1 in NSS (start, pivot) do 
9.                    for each ns2 in NSS (pivot+1,stop) do 
10.                 create ?? ?? ?1? ?  ?2? 
11.                          if ns is not in NSS(start, stop) then 
12.                                add ns into NSS (start, stop) 
13.           do pruning on NSS(start, stop) 
14.           if there is tree node cover span [start, stop] then 
15.         for each tree node n of span [start,stop] do 
16.                      for each ns of NSS(start, stop) do 
17.                     if node n have no children or  
there is node in ns with no father  
then 
18.                                add auxiliary hyper-edge h into F 
19.                                let lhs(h) := n, rhs(h) := ns 
20.          empty NSS(start, stop) 
21.          for each node n of span [start, stop] do 
22.                 add n into NSS(start, stop) 
6 Experiment 
6.1 Experimental Settings 
We evaluate our method on Chinese-English 
translation task. We use the FBIS corpus as train-
ing set, the NIST MT-2002 test set as develop-
ment (dev) set and the NIST MT-2003 test set as 
test set. We train Charniak?s parser (Charniak 
2000) on CTB5 to do Chinese parsing, and modi-
fy it to output packed forest. We tune the parser 
on section 301-325 and test it on section 271-
300. The F-measure on all sentences is 80.85%. 
A 3-gram language model is trained on the Xin-
177
hua portion of the English Gigaword3 corpus and 
the target side of the FBIS corpus using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing (Kenser and Ney, 1995). 
GIZA++ (Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-
n word alignments. For the MER training (Och, 
2003), Koehn?s MER trainer (Koehn, 2007) is 
modified for our system. For significance test, 
we use Zhang et al?s implementation (Zhang et 
al, 2004). Our evaluation metrics is case-
sensitive BLEU-4 (Papineni et al, 2002). 
For parse forest pruning (Mi et al, 2008), we 
utilize the Margin-based pruning algorithm pre-
sented in (Huang, 2008). Different from Mi et al 
(2008) that use a static pruning threshold, our 
threshold is sentence-depended. For each sen-
tence, we compute the Margin between the n-th 
best and the top 1 parse tree, then use the Mar-
gin-based pruning algorithm presented in 
(Huang, 2008) to do pruning. By doing so, we 
can guarantee to use at least all the top n best 
parse trees in the forest. However, please note 
that even after pruning there is still exponential 
number of additional trees embedded in the for-
est because of the sharing structure of forest. 
Other parameters are set as follows: maximum 
number of roots in a tree sequence is 3, maxi-
mum height of a translation rule is 3, maximum 
number of leaf nodes is 7, maximum number of 
node sequences on each span is 10, and maxi-
mum number of rules extracted from one node is 
10000. 
6.2 Experimental Results 
We implement our proposed methods as a gen-
eral, configurable platform for syntax-based 
SMT study. Based on this platform, we are able 
to easily implement most of the state-of-the-art 
syntax-based x-to-string SMT methods via sim-
ple parameter setting. For training, we set forest 
pruning threshold to 1 best for tree-based me-
thods and 100 best for forest-based methods. For 
decoding, we set: 
1) TT2S: tree-based tree-to-string model by 
setting the forest pruning threshold to 1 best and 
the number of sub-trees in a tree sequence to 1. 
2) TTS2S: tree-based tree-sequence to string 
system by setting the forest pruning threshold to 
1 best and the maximum number of sub-trees in a 
tree sequence to 3. 
3) FT2S: forest-based tree-to-string system by 
setting the forest pruning threshold to 500 best, 
the number of sub-trees in a tree sequence to 1. 
4) FTS2S: forest-based tree-sequence to string 
system by setting the forest pruning threshold to 
500 best and the maximum number of sub-trees 
in a tree sequence to 3. 
 
Model BLEU(%) 
Moses 25.68 
TT2S 26.08 
TTS2S 26.95 
FT2S 27.66 
FTS2S 28.83 
 
Table 1. Performance Comparison 
 
We use the first three syntax-based systems 
(TT2S, TTS2S, FT2S) and Moses (Koehn et al, 
2007), the state-of-the-art phrase-based system, 
as our baseline systems. Table 1 compares the 
performance of the five methods, all of which are 
fine-tuned.  It shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S. This shows that tree sequence is very use-
ful to forest-based model. Although a forest can 
cover much more phrases than a single tree does, 
there are still many non-syntactic phrases that 
cannot be captured by a forest due to structure 
divergence issue. On the other hand, tree se-
quence is a good solution to non-syntactic trans-
lation equivalence modeling. This is mainly be-
cause tree sequence rules are only sensitive to 
word alignment while tree rules, even extracted 
from a forest (like in FT2S), are also limited by 
syntax according to grammar parsing rules. 
2) FTS2S shows significant performance im-
provement (p<0.05) over TTS2S due to the con-
tribution of forest. This is mainly due to the fact 
that forest can offer very large number of parse 
trees for rule extraction and decoder. 
3) Our model statistically significantly outper-
forms all the baselines system. This clearly de-
monstrates the effectiveness of our proposed 
model for syntax-based SMT. It also shows that 
the forest-based method and tree sequence-based 
method are complementary to each other and our 
proposed method is able to effectively integrate 
their strengths. 
4) All the four syntax-based systems show bet-
ter performance than Moses and three of them 
significantly outperforms (p<0.05) Moses. This 
suggests that syntax is very useful to SMT and 
translation can be viewed as a structure mapping 
issue as done in the four syntax-based systems. 
Table 2 and Table 3 report the distribution of 
different kinds of translation rules in our model 
(training forest pruning threshold is set to 100 
best) and in our decoding (decoding forest prun-
ing threshold is set to 500 best) for one best 
translation generation. From the two tables, we 
can find that: 
178
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 4,854,406 20,526,674 
P 37,360,684 58,826,261 
U 3,297,302 3,775,734 
All 45,512,392 83,128,669 
 
Table 2. # of rules extracted from training cor-
pus. L means fully lexicalized, P means partially 
lexicalized, U means unlexicalized. 
 
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 10,592 1,161 
P 7,132 742 
U 4,874 278 
All 22,598 2,181 
 
Table 3. # of rules used to generate one-best 
translation result in testing 
 
1) In Table 2, the number of tree sequence 
rules is much larger than that of tree rules al-
though our rule extraction algorithm only ex-
tracts those tree sequence rules over the spans 
that tree rules cannot cover. This suggests that 
the non-syntactic structure mapping is still a big 
challenge to syntax-based SMT. 
2) Table 3 shows that the tree sequence rules 
is around 9% of the tree rules when generating 
the one-best translation. This suggests that 
around 9% of translation equivalences in the test 
set can be better modeled by tree sequence to 
string rules than by tree to string rules. The 9% 
tree sequence rules contribute 1.17 BLEU score 
improvement (28.83-27.66 in Table 1) to FTS2S 
over FT2S.  
3) In Table 3, the fully-lexicalized rules are 
the major part (around 60%), followed by the 
partially-lexicalized (around 35%) and un-
lexicalized (around 15%). However, in Table 2, 
partially-lexicalized rules extracted from training 
corpus are the major part (more than 70%). This 
suggests that most partially-lexicalized rules are 
less effective in our model. This clearly directs 
our future work in model optimization. 
 
BLEU (%)    
N-best \ model FT2S FTS2S 
100 Best 27.40 28.61 
500 Best  27.66 28.83 
2500 Best  27.66 28.96 
5000 Best  27.79 28.89 
 
Table 4. Impact of the forest pruning  
 
Forest pruning is a key step for forest-based 
method. Table 4 reports the performance of the 
two forest-based models using different values of 
the forest pruning threshold for decoding. It 
shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S consistently in all test cases. This again 
demonstrates the effectiveness of our proposed 
model. Even if in the 5000 Best case, tree se-
quence is still able to contribute 1.1 BLEU score 
improvement (28.89-27.79). It indicates the ad-
vantage of tree sequence cannot be covered by 
forest even if we utilize a very large forest.  
2) The BLEU scores are very similar to each 
other when we increase the forest pruning thre-
shold. Moreover, in one case the performance 
even drops. This suggests that although more 
parse trees in a forest can offer more structure 
information, they may also introduce more noise 
that may confuse the decoder. 
7 Conclusion   
In this paper, we propose a forest-based tree-
sequence to string translation model to combine 
the strengths of forest-based methods and tree-
sequence based methods. This enables our model 
to have the great potential to address the issues 
of structure divergence and parse errors for syn-
tax-based SMT. We convert our forest-based tree 
sequence rule extraction and decoding issues to 
tree-based by introducing virtual nodes, virtual 
hyper-edges and auxiliary rules (hyper-edges). In 
our system implementation, we design a general 
and configurable platform for our method, based 
on which we can easily realize many previous 
syntax-based methods. Finally, we examine our 
methods on the FBIS corpus and the NIST MT-
2003 Chinese-English translation task. Experi-
mental results show that our model greatly out-
performs the four baseline systems. Our study 
demonstrates that forest-based method and tree 
sequence-based method are complementary to 
each other and our proposed method is able to 
effectively combine the strengths of the two in-
dividual methods for syntax-based SMT. 
Acknowledgement  
We would like to thank Huang Yun for preparing 
the pictures in this paper; Run Yan for providing 
the java version modified MERT program and 
discussion on the details of MOSES; Mi Haitao 
for his help and discussion on re-implementing 
the FT2S model; Sun Jun and Xiong Deyi for 
their valuable suggestions. 
179
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. ACL-07. 144?151 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06. (poster) 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95. 181-184 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, ToddWard and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. ACL-02. 311-
318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, Sheng Li. 2008a. A Tree Sequence 
Alignment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
 
180
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 914?922,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A non-contiguous Tree Sequence Alignment-based Model for 
Statistical Machine Translation 
 
                     Jun Sun1,2                            Min Zhang1                    Chew Lim Tan2 
1 Institute for Infocomm Research    2School of Computing, National University of Singapore 
         sunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sg  
 
 
Abstract 
The tree sequence based translation model al-
lows the violation of syntactic boundaries in a 
rule to capture non-syntactic phrases, where a 
tree sequence is a contiguous sequence of sub-
trees. This paper goes further to present a trans-
lation model based on non-contiguous tree se-
quence alignment, where a non-contiguous tree 
sequence is a sequence of sub-trees and gaps. 
Compared with the contiguous tree sequence-
based model, the proposed model can well han-
dle non-contiguous phrases with any large gaps 
by means of non-contiguous tree sequence 
alignment. An algorithm targeting the non-
contiguous constituent decoding is also proposed. 
Experimental results on the NIST MT-05 Chi-
nese-English translation task show that the pro-
posed model statistically significantly outper-
forms the baseline systems. 
1 Introduction 
Current research in statistical machine translation 
(SMT) mostly settles itself in the domain of either 
phrase-based or syntax-based. Between them, the 
phrase-based approach (Marcu and Wong, 2002; 
Koehn et al 2003; Och and Ney, 2004) allows lo-
cal reordering and contiguous phrase translation. 
However, it is hard for phrase-based models to 
learn global reorderings and to deal with non-
contiguous phrases. To address this issue, many 
syntax-based approaches (Yamada and Knight, 
2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 
2005; Quirk et al 2005; Zhang et al 2007, 2008a; 
Bod, 2007; Liu et al 2006, 2007; Hearne and Way, 
2003) tend to integrate more syntactic information 
to enhance the non-contiguous phrase modeling. In 
general, most of them achieve this goal by intro-
ducing syntactic non-terminals as translational 
equivalent placeholders in both source and target 
sides. Nevertheless, the generated rules are strictly 
required to be derived from the contiguous transla-
tional equivalences (Galley et al 2006; Marcu et al 
2006; Zhang et al 2007, 2008a, 2008b; Liu et al 
2006, 2007). Among them, Zhang et al (2008a) 
acquire the non-contiguous phrasal rules from the 
contiguous tree sequence pairs1, and find them use-
less via real syntax-based translation systems. 
However, Wellington et al (2006) statistically re-
port that discontinuities are very useful for transla-
tional equivalence analysis using binary branching 
structures under word alignment and parse tree 
constraints. Bod (2007) also finds that discontinues 
phrasal rules make significant improvement in lin-
guistically motivated STSG-based translation 
model. The above observations are conflicting to 
each other. In our opinion, the non-contiguous 
phrasal rules themselves may not play a trivial role, 
as reported in Zhang et al (2008a). We believe that 
the effectiveness of non-contiguous phrasal rules 
highly depends on how to extract and utilize them.   
To verify the above assumption, suppose there is 
only one tree pair in the training data with its 
alignment information illustrated as Fig. 1(a) 2. A 
test sentence is given in Fig. 1(b): the source sen-
tence with its syntactic tree structure as the upper 
tree and the expected target output with its syntac-
tic structure as the lower tree. In the tree sequence 
alignment based model, in addition to the entire 
tree pair, it is capable to acquire the contiguous 
tree sequence pairs: TSP (1~4) 3  in Fig. 1. By 
means of the rules derived from these contiguous 
tree sequence pairs, it is easy to translate the conti-
guous phrase ?
 
/he  /show up  /?s?. As for the 
non-contiguous phrase ?  /at, ***,  /time?, the 
only related rule is r1 derived from TSP4 and the 
entire tree pair. However, the source side of r1 does 
not match the source tree structure of the test sen-
tence. Therefore, we can only partially translate the 
illustrated test sentence with this training sample. 
                                                 
1 A tree sequence pair in this context is a kind of translational 
equivalence comprised of a pair of tree sequences. 
2 We illustrate the rule extraction with an example from the 
tree-to-tree translation model based on tree sequence align-
ment (Zhang et al 2008a) without losing of generality to most 
syntactic tree based models. 
3 We only list the contiguous tree sequence pairs with one 
single sub-tree in both sides without losing of generality. 
914
As discussed above, the problem lies in that the 
non-contiguous phrases derived from the conti-
guous tree sequence pairs demand greater reliance 
on the context. Consequently, when applying those 
rules to unseen data, it may suffer from the data 
sparseness problem. The expressiveness of the 
model also slacks due to their weak ability of gene-
ralization.  
To address this issue, we propose a syntactic 
translation model based on non-contiguous tree 
sequence alignment. This model extracts the 
translation rules not only from the contiguous tree 
sequence pairs but also from the non-contiguous 
tree sequence pairs where a non-contiguous tree 
sequence is a sequence of sub-trees and gaps. With 
the help of the non-contiguous tree sequence, the 
proposed model can well capture the non-
contiguous phrases in avoidance of the constraints 
of large applicability of context and enhance the 
non-contiguous constituent modeling. As for the 
above example, the proposed model enables the 
non-contiguous tree sequence pair indexed as 
TSP5 in Fig. 1 and is allowed to further derive r2 
from TSP5. By means of r2 and the same 
processing to the contiguous phrase ?
 
/he  
/show up  /?s? as the contiguous tree sequence 
based model, we can successfully translate the en-
tire source sentence in Fig. 1(b). 
We define a synchronous grammar, named Syn-
chronous non-contiguous Tree Sequence Substitu-
tion Grammar (SncTSSG), extended from syn-
chronous tree substitution grammar (STSG: 
Chiang, 2006) to illustrate our model. The pro-
posed synchronous grammar is able to cover the 
previous proposed grammar based on tree (STSG, 
Eisner, 2003; Zhang et al 2007) and tree sequence 
(STSSG, Zhang et al 2008a) alignment. Besides, 
we modify the traditional parsing based decoding 
algorithm for syntax-based SMT to facilitate the 
non-contiguous constituent decoding for our model. 
To the best of our knowledge, this is the first 
attempt to acquire the translation rules with rich 
syntactic structures from the non-contiguous 
Translational Equivalences (non-contiguous tree 
sequence pairs in this context). 
The rest of this paper is organized as follows: 
Section 2 presents a formal definition of our model 
with detailed parameterization. Sections 3 and 4 
elaborate the extraction of the non-contiguous tree 
sequence pairs and the decoding algorithm respec-
tively. The experiments we conduct to assess the 
effectiveness of the proposed method are reported 
in Section 5. We finally conclude this work in Sec-
tion 6. 
2 Non-Contiguous Tree sequence Align-
ment-based Model  
In this section, we give a formal definition of 
SncTSSG and accordingly we propose the align-
ment based translation model. The details of prob-
abilistic parameterization are elaborated based on 
the log-linear framework. 
2.1 Synchronous non-contiguous TSSG 
(SncTSSG) 
Extended from STSG (Shiever, 2004), SncTSSG 
can be formalized as a quintuple G = <  , , , 
, R>, where: 
x and  are source and target terminal 
alphabets (words) respectively, and 
x  and  are source and target non-
terminal alphabets (linguistically syntactic 
tags, i.e. NP, VP) respectively; as well as the 
non-terminal  to denote a gap,  
VP
NP
ASVV PN
IP
CP
NNDECVV
 
	


SBAR
VP
S
RPVBZPRPWRB
upshowshewhen
TSP1:  PN( )   PRP(he)
r1: VP(VV( ),AS(  ),NP(CP[0],NN(  ))) 
SBAR(WRB(when),S[0])
TSP5:  VV(  ), *** ,NN(  )   WRB(when)
TSP3:  IP(PN(  ),VV(  ))   
S((PRP(he), VP(VBZ(shows), RP(up))))
TSP2:  VV(  )   VP(VBZ(shows),RP(up))
r2: VV( ), *** ,NN(  )  WRB(when) 
TSP4:  CP(IP(PN( ),VV(  )),DEC(  ))   
S((PRP(he), VP(VBZ(shows), RP(up))))
(at) (NULL) (he) (show up) (?s) (time)
VP
NP
VV PN
IP
CP
NNDECVV
 
	
SBAR
VP
S
RPVBZPRPWRB
upshowshewhen
(at) (he) (show up) (?s) (time)
 
                      (a)                                                (b) 
 
Figure 1: Rule extraction of tree-to-tree model based on tree sequence pairs 
915
 can represent any syntactic or non-
syntactic tree sequences, and 
x R is a production rule set consisting of rules 
derived from corresponding contiguous or 
non-contiguous tree sequence pairs, where a 
rule is a pair of contiguous or non-
contiguous tree sequence with alignment re-
lation between leaf nodes across the tree se-
quence pair. 
A non-contiguous tree sequence translation rule 
r  R can be further defined as a triple 
, where: 
x is a non-contiguous source tree 
sequence, covering the span set 
 in , where 
  which means each subspan has non-
zero width and  which means there 
is a non-zero gap between each pair of 
consecutive intervals. A gap of interval 
[ ] is denoted as , and  
x is a non-contiguous target tree 
sequence, covering the span set 
 in , where   
which means each subspan has non-zero 
width and  which means there is a 
non-zero gap between each pair of 
consecutive intervals. A gap of interval 
[ ] is denoted as , and 
x  are the alignments between leaf nodes of 
the source and target non-contiguous tree 
sequences, satisfying the following 
conditions : 
,  
where   and  
In SncTSSG, the leaf nodes in a non-contiguous 
tree sequence rule can be either non-terminal 
symbols (grammar tags) or terminal symbols 
(lexical words) and the non-terminal symbols with 
the same index which are subsumed 
simultaneously are not required to be contiguous. 
Fig. 4 shows two examples of non-contiguous tree 
sequence rules (?non-contiguous rule? for short in 
the following context) derived from the non-
contiguous tree sequence pair (in Fig. 3) which is 
extracted from the bilingual tree pair in Fig. 2. 
Between them, ncTSr1 is a tree rule with internal 
nodes non-contiguously subsumed from a 
contiguous tree sequence pair (dashed in Fig. 2) 
while ncTSr2 is a non-contiguous rule with a 
contiguous source side and a non-contiguous target 
side. Obviously, the non-contiguous tree sequence 
rule ncTSr2 is more flexible by neglecting the 
context among the gaps of the tree sequence pair 
while capturing all aligned counterparts with the 
corresponding syntactic structure information. We 
 
 
Figure 2: A word-aligned parse tree pair 
 
 
 
Figure 3: A non-contiguous tree sequence pair 
 
 
 
Figure 4: Two examples of non-contiguous  
tree sequence translation rules 
916
expect these properties can well address the issues 
of non-contiguous phrase modeling. 
2.2 SncTSSG based Translation Model 
Given the source and target sentence and , as 
well as the corresponding parse trees   
and , our approach directly approximates the 
posterior probability  based on 
the log-linear framework: 
 
???  
 
In this model, the feature function hm is log-
linearly combined by the corresponding parameter 
(Och and Ney, 2002). The following features 
are utilized in our model: 
1) The bi-phrasal translation probabilities 
2) The bi-lexical translation probabilities 
3) The target language model 
4) The # of words in the target sentence 
5) The  # of rules utilized 
6) The average tree depth in the source side 
of the rules adopted 
7) The # of non-contiguous rules utilized 
8) The # of reordering times caused by the 
utilization of the non-contiguous rules 
Feature 1~6 can be applied to either STSSG or 
SncTSSG based models, while the last two targets 
SncTSSG only. 
3 Tree Sequence Pair Extraction  
In training, other than the contiguous tree sequence 
pairs, we extract the non-contiguous ones as well. 
Nevertheless, compared with the contiguous tree 
sequence pairs, the non-contiguous ones suffer 
more from the tree sequence pair redundancy 
problem that one non-contiguous tree sequence 
pair can be comprised of two or more unrelated 
and nonadjacent contiguous ones. To model the 
contiguous phrases, this problem is actually trivial, 
since the contiguous phrases stay adjacently and 
share the related syntactic constraints; however, as 
for non-contiguous phrase modeling, the cohesion 
of syntactically and semantically unrelated tree 
sequence pairs is more likely to generate noisy 
rules which do not benefit at all. In order to minim-
ize the number of redundant tree sequence pairs, 
we limit the # of gaps of non-contiguous tree se-
quence pairs to be 0 in either source or target side. 
In other words, we only allow one side to be non-
contiguous (either source or target side) to partially 
reserve its syntactic and semantic cohesion4. We 
further design a two-phase algorithm to extract the 
tree sequence pairs as described in Algorithm 1.  
For the first phase (line 1-11), we extract the 
contiguous tree sequence pairs (line 3-5) and the 
non-contiguous ones with contiguous tree se-
quence in the source side (line 6-9). In the second 
phase (line 12-19), the ones with contiguous tree 
sequence in the target side and non-contiguous tree 
sequence on the source side are extracted.  
                                                 
4 Wellington et al (2006) also reports that allowing gaps in 
one side only is enough to eliminate the hierarchical alignment 
failure with word alignment and one side parse tree constraints. 
This is a particular case of our definition of non-contiguous 
tree sequence pair since a non-contiguous tree sequence can be 
considered to overcome the structural constraint by neglecting 
the structural information in the gaps. 
Algorithm 1: Tree Sequence Pair Extraction
Input: source tree and target tree  
Output: the set of tree sequence pairs 
Data structure:  
p[j1, j2] to store tree sequence pairs covering source 
span[j1, j2] 
1: foreach source span [j1, j2], do 
2:    find a target span [i1,i2] with minimal length cov-
ering all the target words aligned to [j1, j2] 
3:   if all the target words in [i1,i2] are aligned with 
source words only in [j1, j2], then 
4:       Pair each source tree sequence covering [j1, j2] 
with those in target covering [i1,i2] as a conti-
guous tree sequence pair 
5:         Insert them into p[j1, j2] 
6:     else 
7:       create sub-span set s([i1,i2]) to cover all the tar-
get words aligned to [j1, j2] 
8:       Pair each source tree sequence covering [j1, j2] 
with each target tree sequence covering 
s([i1,i2]) as a non-contiguous tree sequence pair 
9:         Insert them into p[j1, j2] 
10:   end if 
11:end do 
12: foreach target span [i1,i2], do 
13:   find a source span [j1, j2] with minimal length 
covering all the source words aligned to [i1,i2] 
14:    if any source word in [j1, j2] is aligned with tar-
get words outside [i1,i2], then 
15:       create sub-span set s([j1, j2]) to cover all the 
source words aligned to [i1,i2] 
16:         Pair each source tree sequence covering s([j1, 
j2]) with each target tree sequence covering 
[i1,i2] as a non-contiguous tree sequence pair 
17:          Insert them into p[j1, j2] 
18:     end if 
19: end do
 
917
The extracted tree sequence pairs are then uti-
lized to derive the translation rules. In fact, both 
the contiguous and non-contiguous tree sequence 
pairs themselves are applicable translation rules; 
we denote these rules as Initial rules. By means of 
the Initial rules, we derive the Abstract rules simi-
larly as in Zhang et al (2008a). 
Additionally, we develop a few constraints to 
limit the number of Abstract rules. The depth of a 
tree in a rule is no greater than h. The number of 
non-terminals as leaf nodes is no greater than c. 
The tree number is no greater than d. Besides, the 
number of lexical words at leaf nodes in an Initial 
rule is no greater than l. The maximal number of 
gaps for a non-contiguous rule is no greater than . 
4 The Pisces decoder 
We implement our decoder Pisces by simulating 
the span based CYK parser constrained by the 
rules of SncTSSG. The decoder translates each 
span iteratively in a bottom up manner which guar-
antees that when translating a source span, any of 
its sub-spans is already translated. 
For each source span [j1, j2], we perform a three-
phase decoding process. In the first phase, the 
source side contiguous translation rules are utilized 
as described in Algorithm 2. When translating us-
ing a source side contiguous rule, the target tree 
sequence of the rule whether contiguous or non-
contiguous is directly considered as a candidate 
translation for this span (line 3), if the rule is an 
Initial rule; otherwise, the non-terminal leaf nodes 
are replaced with the corresponding sub-spans? 
translations (line 5). 
In the second phase, the source side non-
contiguous rules5 for [j1, j2] are processed. As for 
                                                 
5 A source side non-contiguous translation rules which cover a 
list of n non-contiguous spans s([ , ], i=1,?,n) is consi-
dered to cover the source span [j1, j2] if and only if = j1 and 
= j2. 
the ones with non-terminal leaf nodes, the re-
placement with corresponding spans? translations 
is initially performed in the same way as with the 
contiguous rules in the first phase. After that, an 
operation specified for the source side non-
contiguous rules named ?Source gap insertion? is 
performed. As illustrated in Fig. 5, to use the non-
contiguous rule r1, which covers the source span 
set ([0,0], [4,4]), the target portion ?IN(in)? is first 
attained, then the translations to the gap span [1,3] 
is acquired from the previous steps and is inserted 
either to the right or to the left of ?IN(in)?. The 
insertion is rather cohesion based but leaves a gap 
<***> for further ?Target tree sequence reordering? 
in the next phase if necessary. 
 In the third phase, we carry out the other non-
contiguous rule specific operation named ?Target 
tree sequence reordering?. Algorithm 3 gives an 
overview of this operation. For each source span, 
we first binarize the span into the left one and the 
right one. The translation hypothesis for this span 
is generated by firstly inserting the candidate trans-
lations of the right span to each gap in the ones of 
the left span respectively (line 2-9) and then re-
peating in the alternative direction (line10-17). The 
gaps for the insertion of the tree sequences in the 
target side are generated from either the inherit-
 
 
Figure 5: Illustration of ?Source gap insertion?  
 
 
Algorithm 2: Contiguous rule processing 
Data structure:  
h[j1, j2]to store translations covering source span[j1, j2]
1: foreach rule r contiguous in source span [j1, j2], do 
2:     if r is an Initial rule, then 
3:         insert r into h[j1, j2] 
4:     else //Abstract rule 
5:   generate translations by replacing the non-
terminal leaf nodes of r with their correspond-
ing spans? translation 
6:         insert the new translation into h[j1, j2] 
7:     end if 
8: end do 
 
918
ance of the target side non-contiguous tree se-
quence pairs or the production of the previous op-
erations of ?Source gap insertion?. Therefore, the 
insertion for target gaps helps search for a better 
order of the non-contiguous constituents in the tar-
get side. On the other hand, the non-contiguous 
tree sequences with rich syntactic information are 
reordered, nevertheless, without much considera-
tion of the constraints of the syntactic structure. 
Consequently, this distortional operation, like 
phrase-based models, is much more flexible in the 
order of the target constituents than the traditional 
syntax-based models which are limited by the syn-
tactic structure. As a result, ?Target tree sequence 
reordering? enhances the reordering ability of the 
model. 
To speed up the decoder, we use several thre-
sholds to limit the searching space for each span. 
The maximal number of the rules in a source span 
is no greater than . The maximal number of trans-
lation candidates for a source span is no greater 
than . On the other hand, to simplify the compu-
tation of language model, we only compute for 
source side contiguous translational hypothesis, 
while neglecting gaps in the target side if any. 
5 Experiments 
5.1 Experimental Settings  
In the experiments, we train the translation model 
on FBIS corpus (7.2M (Chinese) + 9.2M (English) 
words) and train a 4-gram language model on the 
Xinhua portion of the English Gigaword corpus 
(181M words) using the SRILM Toolkits (Stolcke, 
2002). We use these sentences with less than 50 
characters from the NIST MT-2002 test set as the 
development set and the NIST MT-2005 test set as 
our test set. We use the Stanford parser (Klein and 
Manning, 2003) to parse bilingual sentences on the 
training set and Chinese sentences on the devel-
opment and test set. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002). We base 
on the m-to-n word alignments dumped by GI-
ZA++ to extract the tree sequence pairs. For the 
MER training, we modify Koehn?s version (Koehn, 
2004). We use Zhang et als implementation 
(Zhang et al 2004) for 95% confidence intervals 
significant test. 
 We compare the SncTSSG based model against 
two baseline models: the phrase-based and the 
STSSG-based models. For the phrase-based model, 
we use Moses (Koehn et al 2007) with its default 
settings; for the STSSG and SncTSSG based mod-
els we use our decoder Pisces by setting the fol-
lowing parameters: , , , , 
, . Additionally, for STSSG we set 
, and for SncTSSG, we set . 
5.2 Experimental Results  
Table 1 compares the performance of different 
models across the two systems. The proposed 
SncTSSG based model significantly outperforms 
(p < 0.05) the two baseline models. Since the 
SncTSSG based model covers the STSSG based 
model in its modeling ability and obtains a superset 
in rules, the improvement empirically verifies the 
effectiveness of the additional non-contiguous 
rules.  
 
System Model BLEU 
Moses cBP 23.86 
 
Pisces STSSG 25.92 
SncTSSG 26.53 
 
Table 1: Translation results of different models (cBP 
refers to contiguous bilingual phrases without syntactic 
structural information, as used in Moses) 
 
Table 2 measures the contribution of different 
combination of rules. cR refers to the rules derived 
from contiguous tree sequence pairs (i.e., all 
STSSG rules); ncPR refers to non-contiguous 
phrasal rules derived from contiguous tree se-
quence pairs with at least one non-terminal leaf 
node between two lexicalized leaf nodes (i.e., all 
non-contiguous rules in STSSG defined as in 
Zhang et al (2008a)); srcncR refers to source side 
non-contiguous rules (SncTSSG rules only, not 
STSSG rules); tgtncR refers to target side non-
contiguous rules (SncTSSG rules only, not STSSG 
rules) and src&tgtncR refers non-contiguous rules 
Algorithm 3: Target tree sequence reordering
Data structure:  
h[j1, j2]to store translations covering source span[j1, 
j2] 
1: foreach k [j1, j2), do                       
2:     foreach translation  h[j1, k], do 
3:         foreach gap  in , do 
4:             foreach translation  h[k+1, j2], do 
5:                  insert  into the position of  
6:                   insert the new translation into h[j1, j2] 
7:             end do 
8:         end do 
9:      end do 
10:    foreach translation  h[k+1, j2], do 
11:        foreach gap  in , do 
12:            foreach translation  h[j1, k], do 
13:                 insert  into the position of  
14:                  insert the new translation into h[j1, j2] 
15:            end do 
16:        end do 
17:    end do 
18:end do 
 
919
with gaps in either side (srcncR+ tgtncR). The last 
three kinds of rules are all derived from non-
contiguous tree sequence pairs. 
1) From Exp 1 and 2 in Table 2, we find that 
non-contiguous phrasal rules (ncPR) derived from 
contiguous tree sequence pairs make little impact 
on the translation performance which is consistent 
with the discovery of Zhang et al (2008a). How-
ever, if we append the non-contiguous phrasal 
rules derived from non-contiguous tree sequence 
pairs, no matter whether non-contiguous in source 
or in target, the performance statistically signifi-
cantly (p < 0.05) improves (as presented in Exp 
2~5), which validates our prediction that the non-
contiguous rules derived from non-contiguous tree 
sequence pairs contribute more to the performance 
than those acquired from contiguous tree sequence 
pairs.  
2) Not only that, after comparing Exp 6,7,8 
against Exp 3,4,5 respectively, we find that the 
ability of rules derived from non-contiguous tree 
sequence pairs generally covers that of the rules 
derived from the contiguous tree sequence pairs, 
due to the slight change in BLEU score.  
3) The further comparison of the non-
contiguous rules from non-contiguous spans in Exp. 
6&7 as well as Exp 3&4, shows that non-
contiguity in the target side in Chinese-English 
translation task is not so useful as that in the source 
side when constructing the non-contiguous phrasal 
rules. This also validates the findings in Welling-
ton et al (2006) that varying the gaps on the Eng-
lish side (the target side in this context) seldom 
reduce the hierarchical alignment failures.  
Table 3 explores the contribution of the non-
contiguous translational equivalence to phrase-
based models (all the rules in Table 3 has no 
grammar tags, but a gap <***> is allowed in the 
last three rows). tgtncBP refers to the bilingual 
phrases with gaps in the target side; srcncBP refers 
to the bilingual phrases with gaps in the source 
side; src&tgtncBP refers to the bilingual phrases 
with gaps in either side. 
 
System Rule Set BLEU
Moses cBP 23.86 
 
 
Pisces 
cBP 22.63 
cBP + tgtncBP 23.74 
cBP + srcncBP 23.93 
cBP + src&tgtncBP 24.24 
 
Table 3: Performance of bilingual phrasal rules 
 
1) As presented in Table 3, the effectiveness 
of the bilingual phrases derived from non-
contiguous tree sequence pairs is clearly indicated. 
Models adopting both tgtncBP and srcncBP sig-
nificantly (p < 0.05) outperform the model adopt-
ing cBP only. 
2) Pisces underperforms Moses when utiliz-
ing cBPs only, since Pisces can only perform mo-
notonic search with cBPs. 
3) The bilingual phrase model with both 
tgtncBP and srcncBP even outperforms Moses. 
Compared with Moses, we only utilize plain fea-
tures in Pisces for the bilingual phrase model (Fea-
ture 1~5 for all phrases and additional 7, 8 only for 
non-contiguous bilingual phrases as stated in Sec-
tion 2.2; None of the complex reordering features 
or distortion features are employed by Pisces while 
Moses uses them), which suggests the effective-
ness of the non-contiguous rules and the advantag-
es of the proposed decoding algorithm. 
Table 4 studies the impact on performance when 
setting different maximal gaps allowed for either 
side in a tree sequence pair (parameter ) and the 
relation with the quantity of rule set.  
Significant improvement is achieved when al-
lowing at least one gap on either side compared 
with when only allowing contiguous tree sequence 
pairs. However, the further increment of gaps does 
not benefit much. The result exhibits the accor-
dance with the growing amplitude of the rule set 
filtered for the test set, in which the rule size in-
creases more slowly as the maximal number of 
gaps increments. As a result, this slow increase 
against the increment of gaps can be probably at-
tributed to the small augmentation of the effective 
ID Rule Set BLEU 
1 cR (STSSG) 25.92 
2 cR w/o ncPR 25.87 
3 cR w/o ncPR + tgtncR 26.14 
4 cR w/o ncPR + srcncR 26.50 
5 cR w/o ncPR + src&tgtncR 26.51 
6 cR + tgtncR 26.11 
7 cR + srcncR 26.56 
8 cR+src&tgtncR(SncTSSG) 26.53 
 
Table 2: Performance of different rule combination 
Max gaps allowed Rule # BLEU 
source target 
0 0 1,661,045 25.92 
1 1 +841,263 26.53 
2 2 +447,161 26.55 
3 3 +17,782 26.56 
 ? +8,223 26.57 
 
Table 4: Performance and rule size changing with 
different maximal number of gaps 
920
non-contiguous rules. 
In order to facilitate a better intuition to the abil-
ity of the SncTSSG based model against the 
STSSG based model, we present in Table 5, two 
translation outputs produced by both models.  
In the first example, GIZA++ wrongly aligns the 
idiom word ?  /confront at court? to a non-
contiguous phrase ?confront other countries at 
court*** leisurely manner? in training, in which 
only the first constituent ?confront other countries 
at court? is reasonable, indicated from the key 
rules of SncTSSG leant from the training set.  The 
STSSG or any contiguous translational equiva-
lence based model is unable to attain the corres-
ponding target output for this idiom word via the 
non-contiguous word alignment and consider it as 
an out-of-vocabulary (OOV). On the contrary, the 
SncTSSG based model can capture the non-
contiguous tree sequence pair consistent with the 
word alignment and further provide a reasonable 
target translation. It suggests that SncTSSG can 
easily capture the non-contiguous translational 
candidates while STSSG cannot. Besides, 
SncTSSG is less sensitive to the error of word 
alignment when extracting the translation candi-
dates than the contiguous translational equivalence 
based models.  
In the second example, ?  /in 
	 /recent  /?s 

/survey  /middle? is correctly translated into ?in 
the recent surveys? by both the STSSG and 
SncTSSG based models. This suggests that the 
short non-contiguous phrase ?  /in ***  /middle? 
is well handled by both models. Nevertheless, as 
for the one with a larger gap, ?  /will *** 
/continue? is correctly translated and well reorder-
ing into ?will continue? by SncTSSG but failed by 
STSSG. Although the STSSG is theoretically able 
to capture this phrase from the contiguous tree se-
quence pair, the richer context in the gap as in this 
example, the more difficult STSSG can correctly 
translate the non-contiguous phrases. This exhibits 
the flexibility of SncTSSG to the rich context 
among the non-contiguous constituents. 
6 Conclusions and Future Work 
In this paper, we present a non-contiguous tree se-
quence alignment model based on SncTSSG to 
enhance the ability of non-contiguous phrase mod-
eling and the reordering caused by non-contiguous 
constituents with large gaps. A three-phase decod-
ing algorithm is developed to facilitate the usage of 
non-contiguous translational equivalences (tree 
sequence pairs in this work) which provides much 
flexibility for the reordering of the non-contiguous 
constituents with rich syntactic structural informa-
tion. The experimental results show that our model 
outperforms the baseline models and verify the 
effectiveness of non-contiguous translational equi-
valences to non-contiguous phrase modeling in 
both syntax-based and phrase-based systems. We 
also find that in Chinese-English translation task, 
gaps are more effective in Chinese side than in the 
English side. 
Although the characteristic of more sensitive-
ness to word alignment error enables SncTSSG to 
capture the additional non-contiguous language 
phenomenon, it also induces many redundant non-
contiguous rules. Therefore, further work of our 
studies includes the optimization of the large rule 
set of the SncTSSG based model. 
 
 Output & References 
Source  /only  /pass  /null  /five years   ffMachine Learning Methods for 
Chinese Web Page Categorization 
J i  He  1, Ah-Hwee Tan 2 and Chew-L i ra  Tan  1 
1School of Computing, National University of Singapore 
10 Kent Ridge Crescent, Singapore 119260 
(heji,tancl}@comp.nus.edu.sg 
2Kent Ridge Digital Labs 
21 Heng Mui Keng Terrace, Singapore 119613 
ahhwee@krdl.org.sg 
Abst rac t  
This paper eports our evaluation of 
k Nearest Neighbor (kNN), Support 
Vector Machines (SVM), and Adap- 
tive Resonance Associative Map 
(ARAM) on Chinese web page clas- 
sification. Benchmark experiments 
based on a Chinese web corpus 
showed that their predictive per- 
formance were roughly comparable 
although ARAM and kNN slightly 
outperformed SVM in small cate- 
gories. In addition, inserting rules 
into ARAM helped to improve per- 
formance, especially for small well- 
defined categories. 
1 In t roduct ion  
Text categorization refers to the task of au- 
tomatically assigning one or multiple pre- 
defined category labels to free text docu- 
ments. Whereas an extensive range of meth- 
ods has been applied to English text cate- 
gorization, relatively few have been bench- 
marked for Chinese text categorization. Typi- 
cal approaches toChinese text categorization, 
such as Naive Bayes (NB) (Zhu, 1987), Vector 
Space Model (VSM) (Zou et al, 1998; Zou et 
al., 1999) and Linear List Square Fit (LLSF) 
(Cao et al, 1999; Yang, 1994), have well stud- 
ied theoretical basis derived from the informa- 
tion retrieval research, but are not known to 
be the best classifiers (Yang and Liu, 1999; 
Yang, 1999). In addition, there is a lack of 
publicly available Chinese corpus for evaluat- 
ing Chinese text categorization systems. 
This paper reports our applications of 
three statistical machine learning methods, 
namely k Nearest Neighbor system (kNN) 
(Dasarathy, 1991), Support Vector Machines 
(SVM) (Cortes and Vapnik, 1995), and Adap- 
tive Resonance Associative Map (ARAM) 
(Tan, 1995) to Chinese web page categoriza- 
tion. kNN and SVM have been reported as 
the top performing methods for English text 
categorization (Yang and Liu, 1999). ARAM 
belongs to a popularly known family of pre- 
dictive self-organizing neural networks which 
until recently has not been used for docu- 
ment classification. The trio has been eval- 
uated based on a Chinese corpus consisting 
of news articles extracted from People's Daily 
(He et al, 2000). This article reports the ex- 
periments of a much more challenging task in 
classifying Chinese web pages. The Chinese 
web corpus was created by downloading from 
various Chinese web sites covering awide vari- 
ety of topics. There is a great diversity among 
the web pages in terms of document length, 
style, and content. The objectives of our ex- 
periments are two-folded. First, we examine 
and compare the capabilities of these meth- 
ods in learning categorization k owledge from 
real-fife web docllments. Second, we investi- 
gate if incorporating domain knowledge de- 
rived from the category description can en- 
hance ARAM's predictive performance. 
The rest of this article is organized as fol- 
lows. Section 2describes our choice of the fea- 
ture selection and extraction methods. Sec- 
tion 3 gives a sllrnrnary of the kNN and SVM, 
and presents the less familiar ARAM algo- 
rithm in more details. Section 4 presents our 
evaluation paradigm and reports the experi- 
93 
mental results. 
2 Feature  Se lec t ion  and  Ext rac t ion  
A pre-requisite of text categorization is to ex- 
tract a suitable feature representation f the 
documents. Typically, word stems are sug- 
gested as the representation units by infor- 
mation retrieval research. However, unlike 
English and other Indo-European languages, 
Chinese text does not have a natural delim- 
iter between words. As a consequence, word 
segmentation is a major issue in Chinese doc- 
ument processing. Chinese word segmenta- 
tion methods have been extensively discussed 
in the literature. Unfortunately perfect preci- 
sion and disambiguation cannot be reached. 
As a result, the inherent errors caused by 
word segmentation always remains as a prob- 
lem in Chinese information processing. 
In our experiments, a word-class bi-gram 
model is adopted to segment each training 
document into a set of tokens. The lexi- 
con used by the segmentation model contains 
64,000 words in 1,006 classes. High precision 
segmentation is not the focus of our work. In- 
stead we aim to compare different classifier's 
performance on noisy document set as long as 
the errors caused by word segmentation are 
reasonably low. 
To select keyword features for classifica- 
tion, X (CHI) statistics is adopted as the 
ranking metric in our experiments. A prior 
study on several well-known corpora in- 
cluding Reuters-21578 and OHSUMED has 
proven that CHI statistics generally outper- 
forms other feature ranking measures, such 
as term strength (TS), document frequency 
(DF), mutual information (MI), and informa- 
tion gain (IG) (Yang and J.P, 1997). 
During keyword extraction, the document 
is first segmented and converted into a 
keyword frequency vector (t f l ,  t f2 , . . . ,  t.f M ) , 
where t f i  is the in-document term frequency 
of keyword wi, and M is the number of the 
keyword features selected. A term weight- 
ing method based on inverse document .fre- 
quency (IDF) (Salton, 1988) and the L1- 
norm~llzation are then applied on the fre- 
quency vector to produce the keyword feature 
vector 
(X l ,  X2 ,  ? - ? , XM)  
x = max{xi} ' (i) 
in which xi is computed by 
zi = (1 + log 2 tf i)  log2 ~ (2) 
n i  ' 
where n is the number of documents in the 
whole training set, and ni is the number of 
training documents in which the keyword wi 
occurs at least once. 
3 The  C lass i f iers  
3.1 k Nearest Neighbor 
k Nearest Neighbor (kNN) is a tradi- 
tional statistical pattern recognition algo- 
rithm (Dasarathy, 1991). It has been studied 
extensively for text categorization (Yang and 
Liu, 1999). In essence, kNN makes the predic- 
tion based on the k training patterns that are 
closest to the unseen (test) pattern, accord- 
ing to a distance metric. The distance metric 
that measures the similarity between two nor- 
malized patterns can be either a simple LI- 
distance function or a L2-distance function, 
such as the plain Euclidean distance defined 
by 
D(a ,b)=~s~.  (a~-bi)2. (3) 
The class assignment to the test pattern is 
based on the class assignment of the closest k 
training patterns. A commonly used method 
is to label the test pattern with the class that 
has the most instances among the k nearest 
neighbors. Specifically, the class index y(x) 
assigned to the test pattern x is given by 
yCx) ..-.. arg'max, {n(dj, )ld.:j kNN}, (4) 
where n(dj,  ~) is the number of training pat- 
tern dj in the k nearest neighbor set that are 
associated with class c4. 
The drawback of kNN is the difficulty in 
deciding a optimal k value. Typically it has 
to be determined through conducting a series 
of experiments using different values. 
94 
3.2 Support  Vector  Machines 
Support Vector Machines (SVM) is a rela- 
tively new class of machine learning tech- 
niques first introduced by Vapnik (Cortes 
and Vapnik, 1995). Based on the structural 
risk minimization principle from the compu- 
tational learning theory, SVM seeks a decision 
surface to separate the tralning data points 
into two classes and makes decisions based on 
the support vectors that are selected as the 
only effective lements from the training set. 
Given a set of linearly separable points 
s = {x  Rnli = 1,2 , . . . ,N},  each point xi 
belongs to one of the two classes, labeled as 
y iE{-1,+l}.  A separating hyper-plane di- 
vides S into two sides, each side containing 
points with the same class label only. The 
separating hyper-plane can be identified by 
the pair (w,b) that satisfies 
w-x+b=0 
and y i (w'x i  + b)>l (5) 
for i = 1, 2 , . . . ,  N; where the dot product op- 
eration ? is defined by 
w.  x ---- ~ wixi (6) 
for vectors w and x. Thus the goal of the 
SVM learning is to find the optimal separat- 
ing hyper-plane (OSH) that has the maximal 
margin to both sides. This can be formula- 
rized as: 
minimize ?w. w 
subject o yi(w.x i  + b)>l (7) 
The points that are closest to the OSH are 
termed support vectors (Fig. 1). 
The SVM problem can be extended to lin- 
early non-separable case and non-linear case. 
Various quadratic programming algorithms 
have been proposed and extensively studied 
to solve the SVM problem (Cortes and Vap- 
nik, 1995; Joachims, 1998; Joacbims, 1999). 
During classification, SVM makes decision 
based on the OSH instead of the whole 
training set. It simply finds out on which 
side of the OSH the test pattern is located. 
0 0 O0 
o o , 
J / / / - - . .  
Figure 1: Separating hyperplanes (the set 
of solid lines), optimal separating hyperpIane 
(the bold solid line), and support vectors (data 
points on the dashed lines). The dashed lines 
identify the max margin. 
This property makes SVM highly compet- 
itive, compared with other traditional pat- 
tern recognition methods, in terms of com- 
putational efficiency and predictive accuracy 
(Yang and Liu, 1999). 
In recent years, Joachims has done much re- 
search on the application of SVM to text cat- 
egorization (Joachims, 1998). His SVM zight 
system published via http://www-ai.cs.uni- 
dortmund.de/FORSCHUNG/VERFAHREN/ 
SVM_LIGHT/svm_light.eng.html is used in 
our benchmark experiments. 
3.3 Adapt ive  Resonance Associat ive 
Map 
Adaptive Resonance Associative Map 
(ARAM) is a class of predictive serf- 
organizing neural networks that performs 
incremental supervised learning of recog- 
nition categories (pattern classes) and 
multidimensional maps of patterns. An 
ARAM system can be visualized as two 
overlapping Adaptive Resonance Theory 
(ART) modules consisting of two input fields 
F~ and F1 b with an F2 category field (Tan, 
1995; Tan, 1997) (Fig. 2). For classification 
problems, the F~ field serves as the input 
field containing the document feature vector 
and the F1 b field serves as the output field 
containing the class prediction vector. The 
F2 field contains the activities of the recogni- 
tion categories that are used to encode the 
patterns. 
95 
.. I ?/? I 
x !.'1 x, 
ARTa A B ARTb 
Figure 2: The Adaptive Resonance Associa- 
tive Map architecture 
When performing classification tasks, 
ARAM formulates recognition categories of 
input patterns, and associates each cate- 
gory with its respective prediction. During 
learning, given an input pattern (document 
feature) presented at the F~ input layer 
and an output pattern (known class label) 
presented at the Fib output field, the category 
field F2 selects a winner that receives the 
largest overall input. The winning node se- 
lected in F2 then triggers a top-down priming 
on F~ and F~, monitored by separate reset 
mechanisms. Code stabilization is ensured 
by restricting encoding to states where 
resonance are reached in both modules. 
By synchronizing the un.qupervised catego- 
rization of two pattern sets, ARAM learns 
supervised mapping between the pattern sets. 
Due to the code stabilization mechanism, 
fast learning in a real-time environment is
feasible. 
The knowledge that ARAM discovers dur- 
ing learning is compatible with IF-THEN 
rule-based presentation. Specifically, each 
node in the FF2 field represents a recognition 
category associating the F~ patterns with the 
F1 b output vectors. Learned weight vectors, 
one for each F2 node, constitute a set of rules 
that link antecedents to consequences. At any 
point during the incremental learning process, 
the system architecture can be translated into 
a compact set of rules. Similarly, domain 
knowledge in the form of IF-THEN rules can 
be inserted into ARAM architecture. 
The ART modules used in ARAM can be 
ART 1, which categorizes binary patterns, or 
analog ART modules such as ART  2, ART  2- 
A, and fuzzy ART, which categorize both bi- 
nary and analog patterns. The fuzzy ARAM 
(Tan, 1995) algorithm based on fuzzy ART 
(Carpenter et al, 1991) is introduced below. 
Parameters: Fuzzy ARAM dynamics are 
determined by the choice parameters aa > 0 
and ab > 0; the learning rates ~a E \[0, 1\] and 
~b E \[0, 1\]; the vigilance parameters Pa E \[0, 1\] 
and Pb E \[0, 1\]; and the contribution parame- 
ter '7 E \[0, 1\]. 
Weight vectors: Each F2 category node j
is associated with two adaptive weight tem- 
plates w~ and w~. Initially, all category nodes 
are uncommitted and all weights equal ones. 
After a category node is selected for encoding, 
it becomes committed. 
Category choice: Given the F~ and F1 b in- 
put vectors A and B, for each F2 node j, the 
choice function Tj is defined by 
IA Aw~l IB A w~l 
= ~a~ + Iw~'l + (1 --~)~b + Iw~l' (S) 
where the fuzzy AND operation A is defined 
by 
(p A q)i --~ min(pi, qi), (9) 
and where the norm I-I is defined by 
IPl -= ~P i  (10) 
i 
for vectors p and q. 
The system is said to make a choice when 
at most one F2 node can become active. The 
choice is indexed at J where 
Tj  = ma,x{Tj : for all F2 node j } .  (11) 
When a category choice is made at node J, 
yj = 1; andyj =0 for all j ~ J. 
Resonance or reset: Resonance occurs if 
the match .functions, m~ and m~, meet the 
vigilance criteria in their respective modules: 
IA A w~l 
m~ = \[AI _> pa (12) 
and 
m~ = IB A w~l> Pb. (13) 
IBI - 
96 
Learning then ensues, as defined below. If 
any of the vigilance constraints is violated, 
mismatch reset occurs in which the value of 
the choice function Tj  is set to 0 for the du- 
ration of the i.nput presentation. The search 
process repeats to select another new index J 
until resonance is achieved. 
Learn ing:  Once the search ends, the weight 
vectors w~ and w~ are updated according to 
the equations 
W~ (new) - -  (1 ,~ iRa(o ld )  - - . . , , , j  +&(A^w3 
(14) 
and 
wb, cnew)~ (i ~ ~_ b(old) = - ,bJWj + ~b(B A wbj (Old)) 
(15) 
respectively. Fast learning corresponds to set- 
ting/~a =/~b = 1 for committed nodes. 
Classification: During classification, using 
the choice rule, only the F2 node J that re- 
ceives maximal F~ ~ F2 input Tj predicts 
ARTb output. In simulations, 
1 if j = J where T j  > Tk 
y j  = for all k ? J (16) 
0 otherwise. 
The F1 b activity vector x b is given by 
J 
The output prediction vector B is then given 
by 
B ~ (bl, b2,. . ,  bN)  = X b (18) 
where bi indicates the likelihood or confidence 
of assigning a pattern to category i.
Ru le  insert ion:  Rule insertion proceeds in 
two phases. The first phase parses the rules 
for keyword features. When a new keyword is 
encountered, it is added to a keyword feature 
table containing keywords obtained through 
automatic feature selection from training 
documents. Based on the keyword feature 
table, the second phase of rule insertion 
translates each rule into a M-dimensional 
vector a and a N-dimensional vector b, where 
M is the total number of features in the 
keyword feature table and N is the number 
of categories. Given a rule of  the following 
format, 
IF Xl ,  X2 ,  - ? ? , Xm 
THEN Yl, Y2,.-., Yn 
where xt , . . . ,  xm are antecedents and 
Yt , . . .  ,Yn are consequences, the algorithm 
derives a pair of vectors a and b such that 
? for each index i = 1, . . . ,  M, 
1 ifwi = x j  for some j 6 {1 , . . . ,m} 
ai = 0 otherwise 
(19) 
where wi is the i th entry in the keyword fea- 
ture table; and for each index i = 1, . . . ,  N, 
1 ifwi = y j  for some j E {1 , . . . ,n}  
bi = 0 otherwise 
(20) 
where wi  is the class label of the category i.
The vector pairs derived from the rules are 
then used as training patterns to initialize a 
ARAM network. During rule insertion, the 
vigilance parameters Pa and Pb are each set 
to 1 to ensure that only identical attribute 
vectors are grouped into one recognition cat- 
egory. Contradictory symbolic rules are de- 
tected during rule insertion when identical in- 
put attribute vectors are associated with dis- 
tinct output attribute vectors. 
4 Empi r i ca l  Eva luat ion  
4.1 The Chinese Web Corpus 
The Chinese web corpus, colleeted in-house, 
consists of web pages downloaded from vari- 
ous Chinese web sites covering a wide variety 
of topics. Our experiments are based on a 
subset of the corpus consisting of 8 top-level 
categories and over 6,000 documents. For 
each category, we conduct binary classifica- 
tion experiments in which we tag the cur- 
rent category as the positive category and the 
other seven categories as the negative cate- 
gories. The corpus is further partitioned into 
training and testing data such that the num- 
ber of the training documents i at least 2.5 
times of that of the testing documents (Table 
1). 
97 
Table 1: The eight top-level categories in the 
Chinese web corpus, and the training and test 
samples by category. 
Category Description Train Test Art 
Art Topic regarding 325 102 Belief 
literature, art 
Belief Philosophy and 131 40 B/z 
religious beliefs 
Biz Business 2647 727 Edu 
Edu Education 205 77 
IT  Computer and 1085 309 /T 
internet informatics 
Joy Online fresh, 636 216 
interesting info Joy 
Med Medical care 155 57 Meal 
related web sites 
Sci Various kinds 119 39 Sci 
of science 
Table 2: A sample set of 19 rules generated 
based on the accompanied description of the 
Chinese web categories. 
:- ~ (Chinese painting) 
:- ~"  (.pray) ~.~r~ (rabbi) 
:- {~ (promotion) ~ (rrcal estate) 
~:P (cli~O 
:- ~-~ (undergradua~) - -~ (supervisor) 
~2N (campus) 
:- ~:~k (version) ~ (virus) 
g/~k~ (ffirewan) ~ (program) 
:- ~ '~ (lantern riddle) 
:- ~ (health cam) ~J~ (pmscriplion) 
\ [~  (medical jurisprudence) 
:- ~fl ~ ~ (supernaturalism) 
~ (high technology) 
4.2 Exper iment  Parad igm 
kNN experiments used the plain Euclidean 
distance defined by equation (3) as the simi- 
laxity measure. On each pattern set contain- 
ing a varying number of documents, different 
values of k ranging ~om 1 to 29 were tested 
and the best results were recorded. Only odd 
k were used to ensure that a prediction can 
always be made. 
SVM experiments used the default built-in 
inductive SVM parameter set in VM tight, 
which is described in detail on the web site 
and elsewhere (Joachims, 1999). 
ARAM experiments employed a standard 
set of parameter values of fuzzy ARAM. In 
addition, using a voting strategy, 5 ARAM 
systems were trained using the same set of 
patterns in different orders of presentation 
and were combined to yield a final prediction 
vector. 
To derive domain theory on web page clas- 
sification, a varying number (ranging from 10 
to 30) of trainiug documents from each cate- 
gory were reviewed. A set of domain knowl- 
edge consists of 56 rules with about one to 10 
rules for each category was generated. Only 
positive rules that link keyword antecedents 
to positive category consequences were in- 
cluded (Table 2). 
4.3 Per fo rmance  Measures  
Our experiments adopt the most commonly 
used performance measures, including the re- 
call, precision, and F1 measures. Recall (R) is 
the percentage of the documents for a given 
category that are classified correctly. Preci- 
sion (P) is the percentage of the predicted 
documents for a given category that are clas- 
sifted correctly. Ft rating is one of the com- 
monly used measures to combine R and P into 
a single rating, defined as 
2RP 
Ft = (R + P)" (21) 
These scores are calculated for a series of bi- 
nary classification experiments, one for each 
category. Micro-averaged scores and macro- 
averaged scores on the whole corpus are 
then produced across the experiments. With 
micro-averaging, the performance measures 
are produced across the documents by adding 
up all the documents counts across the differ- 
ent tests, and calculating using these summed 
values. With macro-averaging, each category 
is assigned with the same weight and per- 
formance measures are calculated across the 
categories. It is understandable that micro- 
averaged scores and macro-averaged scores re- 
flect a classifier's performance on large cate- 
gories and small categories respectively (Yang 
and Liu, 1999). 
98 
Table 3: 
classifiers on the Chinese web 
kNN 
Category 
Art 
Belief 
Biz 
Edu 
IT 
Joy 
Med 
$ci 
Predictive performance of the four 
P R 
corpus. 
! SVM 
F1 ~P R 
.440 .398 .402 
.548 .556 .500 
.706 .692 .703 
.365 .602 .074 
.321 .394 .307 
.291 .462 .255 
.494 .330 .544 
.213! .137 .179 
.795 .304 
.773 .425 
.724 .689 
.380 .351 
.309 .333 
.381 .236 
.833 .351 
.625 .128 
F~ 
.400 
.526 
.698 
.180 
.345 
.328 
.411 
.156 
Micro-ave. .584 .482 .528 .523 .521 .522 
Macro-ave. .600 .352 .422 .384 .454 .380 
ARAM ARAMw/rules 
Category P R Fx P R F1 
Art 
Belief 
Biz 
Edu 
IT 
Joy 
Med 
Sei 
.653 .461 .540 
.750 .750 .750 
.742 .622 .677 
.421 .312 .358 
.444 .259 .327 
.600 .208 .309 
.421 .421 .421 
.292 .179 .222 
.706 .471 .565 
.714 .750 .732 
.745 .604 .667 
.420 .273 .331 
.437 .291 .350 
.618 .194 .296 
.448 .456 .452 
.409 .231 .295 
Micro-ave. .619 .453 .523 .628 .450 .524 
Macro-ave. .540 .402 .451 .562 .409 .461 
4.4 Resu l ts  and  Discuss ions  
Table 3 summarizes the three classifier's per- 
formances on the test corpus in terms of pre- 
cision, recall, and F1 measures. The micro- 
averaged scores produced by the trio, which 
were predominantly determined by the clas- 
sifters' performance on the large categories 
(such as Biz, IT, and Joy), were roughly com- 
parable. Among the three, kNN seemed to 
be marginally better than SVM and ARAM. 
Inserting rules into ARAM did not have a 
significant impact. This showed that do- 
main knowledge was not very useful for cat- 
egories that already have a large number of 
training examples. The differences in the 
macro-averaged scores produced by the three 
classifiers, however, were much more signifi- 
cant. The macro-averaged F1 score obtained 
by ARAM was noticeably better than that of 
kNN, which in turn was higher than that of 
SVM. This indicates that ARAM (and kNN) 
tends to outperform SVM in small categories 
that have a smaller number of training pat- 
terns. 
We are particularly interested in the classi- 
fier's learning ability on small categories. In 
certain applications, such as personalized con- 
tent delivery, a large pre-labeled training cor- 
pus may not be available. Therefore, a classi- 
f iefs ability of learning from a small training 
pattern set is a major concern. The different 
approaches adopted by these three classifiers 
in learning categorization knowledge are best 
? seen in the light of the distinct learning pe- 
culiarities they exhibit on the small training 
sets. 
kNN is a lazy learning method in the sense 
that it does not carry out any off-line learning 
to generate a particular category knowledge 
representation. Instead, kNN performs on- 
line scoring to find the training patterns that 
are nearest o a test pattern and makes the 
decision based on the statistical presumption 
that patterns in the same category have simi- 
lar feature representations. The presumption 
is basically true to most pattern instances. 
Thus kNN exhibits a relatively stable perfor- 
manee across small and large categories. 
SVM identifies optimal separating hyper- 
plane (OSH) across the training data points 
and makes classification decisions based on 
the representative data instances (known as 
support vectors). Compared with kNN, SVM 
is more computationally efficient during clas- 
sification for large-scale training sets. How- 
ever, the OSH generated using small train- 
ing sets may not be very representative, spe- 
cially when the training patterns are sparsely 
distributed and there is a relatively narrow 
margin between the positive and negative pat- 
terns. In our experiments on small train- 
ing sets including Art, Belief, Edu, and Sci, 
SVM's performance were generally lower than 
those of kNN and ARAM. 
ARAM generates recognition categories 
from the input training patterns. The incre- 
mentally learned rules abstract he major rep- 
resentations of the training patterns and elim- 
inate minor inconsistencies in the data pat- 
terns. During classifying, it works in a sim- 
ilar fashion as kNN. The major difference is 
that AI:tAM uses the learned recognition cat- 
egories as the similarity-scoring unit whereas 
kNN uses the raw in-processed training pat- 
terns as the distance-scoring unit. It follows 
99 
that ARAM is notably more scalable than 
kNN by its pattern abstraction capability and 
therefore is more suitable for handling very 
large data sets. 
The overall improvement in predictive per- 
formance obtained by inserting rules into 
ARAM is also of particular interest to us. 
ARAM's performance was more likely to be 
improved by rule insertion in categories that 
are well defined and have relatively fewer 
numbers of training patterns. As long as a 
user is able to abstract he category knowl- 
edge into certain specific rule representa- 
tion, domain knowledge could complement 
the limited knowledge acquired through a 
small training set quite effectively. 
Acknowledgements  
We would like to thank our colleagues, Jian 
Su and Guo-Dong Zhou, for providing the 
Chinese segmentation software and Fon-Lin 
Lai for his valuable suggestions in designing 
the experiment system. In addition, we thank 
T. Joachims at the University of Dortmund 
for making SVM light available. 
Re ferences  
Suqing Cao, Fuhu Zeng, and Huanguang Cao. 
1999. A mathematical model for automatic 
Chinese text categorization. Journal of the 
China Society for Scientific and Technical In- 
formation \[in Chinese\], 1999(1). 
G.A. Carpenter, S. Grossberg, and D.B. Rosen. 
1991. Fuzzy ART: Fast stable learning and cat- 
egorization of analog patterns by an adaptive 
resonance system. Neural Networks, 4:759-771. 
C. Cortes and V. Vapnik. 1995. Support vector 
networks. Machine learning, 20:273-297. 
Belur V. Dasarathy. 1991. Nearest Neighbor (NN) 
Norms: NN Pattern Classification Techniques. 
IEEE Computer Society Press, Las Alamitos, 
California. 
Ji He, A.-H. Tan, and Chew-Lira Tan. 2000. 
A comparative study on Chinese text catego- 
rization methods. In PRICAI'2000 Interna- 
tional Workshop on Text and Web Mining, Mel- 
bourne, August. 
T. Joachims. 1998. Text categorization with sup- 
port vector machines: Learning with many rel- 
evant features. In Proceedings of the European 
Conference on Machine Learning, Springer. 
T. Joachims. 1999. Making large-Scales SVM 
learning Pracical. Advances in Kernel Methods 
- Support Vector Learning. B. Scholkopf, C. 
Burges and A. Smola (ed.), MIT Press. 
Salton. 1988. Term weighting approaches in au- 
tomatic text retrieval. Information Processing 
and Management, 24(5):513-523. 
A.-H. Tan. 1995. Adaptive resonance associative 
map. Neural Networks, 8(3):437--446. 
A.-H. Tan. 1997. Cascade ARTMAP: Integrat- 
ing neural computation and symbolic knowl- 
edge processing. IEEE Transactions on Neural 
Networks, 8(2):237-235. 
Y. Yang and Pedersen J.P. 1997. A comparative 
study on feature selection in text categoriza- 
tion. In the Fourteenth International Confer- 
ence on Machine Learning (ICML'97), pages 
412-420. 
Y. Yang and X. Liu. 1999. A re-examination 
of text categorization methods. In ~nd An- 
nual International ACM SIGIR Conference on 
Research and Development in Information Re- 
trieval (SIGIR'99), pages 42-49. 
Y. Yang. 1994. Expert network: Effective and ef- 
ficient learning from human decisions in text 
categorization and retrieval. In 17th Annual 
International ACM SIGIR Conference on Re- 
search and Development in Information Re- 
trieval (SIGIR '94). 
Y. Yang. 1999. An evaluation of statistical ap- 
proaches to text categorization. Journal of In- 
formation Retrieval, 1(1/2):67-88. 
Lanjuan Zhu. 1987. The theory and experiments 
on automatic Chinese documents classification. 
Journal of the China Society for Scientific and 
Technical Information \[in Chinese\], 1987(6). 
Tao Zou, Ji-Cheng Wang, Yuan Huang, and Fu- 
Yan Zhang. 1998. The design and implementa- 
tion of an automatic Chinese documents classi- 
fication system. Journal for Chinese Informa- 
tion \[in Chinese\], 1998(2). 
Tao Zou, Yuan Huang, and Fuyan Zhang. 1999. 
Technology of information mining on WWW. 
Journal of the China Society for Scientific and 
Technical Information \[in Chinese\], 1999(4). 
100 
Effective Adaptation of a Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain 
Dan Shen?? Jie Zhang?? Guodong Zhou? Jian Su? Chew-Lim Tan?
? Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,zhougd,sujian}@i2r.a-star.edu.sg 
{tancl}@comp.nus.edu.sg 
 
 
Abstract 
In this paper, we explore how to adapt a 
general Hidden Markov Model-based 
named entity recognizer effectively to 
biomedical domain.  We integrate various 
features, including simple deterministic 
features, morphological features, POS 
features and semantic trigger features, to 
capture various evidences especially for 
biomedical named entity and evaluate 
their contributions.  We also present a 
simple algorithm to solve the abbreviation 
problem and a rule-based method to deal 
with the cascaded phenomena in biomedi-
cal domain.  Our experiments on GENIA 
V3.0 and GENIA V1.1 achieve the 66.1 
and 62.5 F-measure respectively, which 
outperform the previous best published 
results by 8.1 F-measure when using the 
same training and testing data.  
1 Introduction 
As the research in biomedical domain has grown 
rapidly in recent years, a huge amount of nature 
language resources have been developed and be-
come a rich knowledge base.  The technique of 
named entity (NE) recognition (NER) is strongly 
demanded to be applied in biomedical domain.  
Since in previous work, many NER systems have 
been applied successfully in newswire domain 
(Zhou and Su 2002; Bikel et al 1999; Borthwich et 
al. 1999), more and more explorations have been 
done to port existing NER system into biomedical 
domain (Kazama et al 2002; Takeuchi et al 2002; 
Nobata et al 1999 and 2000; Collier et al 2000; 
Gaizauskas et al 2000; Fukuda et al 1998; Proux 
et al 1998).  However, compared with those in 
newswire domain, these systems haven?t got high 
performance.  It is probably because of the follow-
ing factors of biomedical NE (Zhang et al 2003): 
1. Some modifiers are often before basic NEs, 
e.g. activated B cell lines, and sometimes biomedi-
cal NEs are very long, e.g. 47 kDa sterol regula-
tory element binding factor.  This kind of factor 
highlights the difficulty for identifying the bound-
ary of NE. 
2. Two or more NEs share one head noun by 
using conjunction or disjunction construction, e.g. 
91 and 84 kDa proteins.  It is hard to identify these 
NEs respectively. 
3. An entity may be found with various spelling 
forms, e.g. N-acetylcysteine, N-acetyl-cysteine, 
NAcetylCysteine, etc.  Since the use of capitaliza-
tion is casual, the capitalization information may 
not be so evidential in this domain. 
4. NE may be cascaded.  One NE may be em-
bedded in another NE, e.g. <PROTEIN><DNA> 
kappa 3</DNA> binding factor </PROTEIN>.  
More effort must be made to identify this kind of 
NE. 
5. Abbreviations are frequently used in bio-
medical domain, e.g. TCEd, IFN, TPA, etc.  Since 
abbreviations don?t have many evidences for cer-
tain NE class, it is difficult to classify them cor-
rectly. 
These factors above make NER in biomedical 
domain difficult.  Therefore, it is necessary to ex-
plore more evidential features and more effective 
methods to cope with such difficulties. 
In this paper, we will study how to adapt a gen-
eral Hidden Markov Model (HMM)-based NE rec-
ognizer (Zhou and Su 2002) to biomedical domain.  
We specially explore various evidences for bio-
medical NE and propose methods to cope with ab-
breviations and cascaded phenomena.  As a result, 
features (simple deterministic features, morpho-
logical features, part-of-speech features and head 
noun trigger features) and methods (abbreviation 
recognition algorithm and rule-based cascaded 
phenomena resolution) are integrated in our system.  
The experiment shows that system outperforms the 
best published system by 8.1 F-measure. 
In Section 2, we will introduce the HMM-
based NE recognizer briefly.  In Section 3, we will 
focus on the features that we have used.  The 
methods and the adaptations of different features 
will be discussed in detail.  In Section 5 and 6, we 
will present the solutions of abbreviation and cas-
caded phenomena. Finally, our experiment results 
will be presented and the contributions of different 
features will be analyzed in Section 7. 
2 
3 
3.1 
HMM-based Named Entity Recognizer 
Our system is adapted from a HMM-based NE 
recognizer, which has been proved very effective 
in MUC (Zhou and Su 2002). 
The purpose of HMM is to find the most likely 
tag sequence T for a given sequence 
of tokens G  that maxi-
mizes . 
n
n ttt ???= 211
n gg ?= 211
)1
nG
ng??
|( 1
nTP
In token sequence G , the token g  is defined 
as , where w is the word and is 
the feature set related with the word . 
n
1 i
i
>=< iii wfg , i if
w
In tag sequenceT , each tag consists of three 
parts: 1. Boundary category, which denotes the 
position of the current word in NE.  2. Entity cate-
gory, which indicates the NE class.  3. Feature set, 
which will be discussed in Section 3. 
n
1 it
When we incorporate a plentiful feature set in 
HMM, we will encounter data sparseness problem.  
An alternative back-off modeling approach by 
means of constraint relaxation is applied in our 
model (Zhou and Su 2002).  It enables the decod-
ing process effectively find a near optimal fre-
quently occurred pattern entry in determining the 
NE tag probability distribution of current word. 
Finally, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence 
in the state space of the possible tag distribution 
based on the state transition probabilities.  Fur-
thermore, some constraints on the boundary cate-
gory and entity category between two consecutive 
tags are applied to filter the invalid NE tags (Zhou 
and Su 2002). 
Feature Set 
Simple Deterministic Features (Fsd) 
The purpose of simple deterministic features is to 
capture the capitalization, digitalization and word 
formation information.  This kind of features have 
been widely used in both newswire NER system, 
such as (Zhou and Su 2002), and biomedical NER 
system, such as (Nobata et al 1999; Gaizauskas et 
al. 2000; Collier et al 2000; Takeuchi and Collier 
2002; Kazama et al 2002).  Based on the charac-
teristics of biomedical NEs, we designed simple 
deterministic features manually.  Table 1 shows the 
simple deterministic features with descending or-
der of priority. 
 
Fsd Name Example 
Comma , 
Dot . 
LRB ( 
RRB ) 
LSB [ 
RSB ] 
RomanDigit II 
GreekLetter Beta 
StopWord in, at 
ATCGsequence AACAAAG 
OneDigit 5 
AllDigits 60 
DigitCommaDigit 1,25 
DigitDotDigit 0.5 
OneCap T 
AllCaps CSF 
CapLowAlpha All 
CapMixAlpha IgM 
LowMixAlpha kDa 
AlphaDigitAlpha H2A 
AlphaDigit T4 
DigitAlphaDigit 6C2 
DigitAlpha 19D 
Table 1: Simple deterministic features 
From Table 1, we can find that: 
1. Features such as comma, dot, StopWord, etc. 
are designed intuitively to provide information to 
detect the boundary of NE. 
2. Features Parenthesis is often used to indicate 
the definition of abbreviation in biomedical docu-
ments. 
3. Features GreekLetter and RomanDigit are 
specially designed to capture the symbols 
frequently occurred in biomedical NE. 
4. Feature ATCG sequence identify the similar-
ity of words according to their word formations, 
e.g. AACAAAG, CTCAGGA, etc. 
5. Features dealing with mixed alphabets and 
digits such as AlphaDigitAlpha, CapMixAlpha, etc. 
are beneficial for biomedical abbreviations. 
Furthermore, we evaluate these features and 
compare with those used in MUC (Zhou and Su, 
2002).  The reported result of the simple determi-
nistic features used in MUC can achieve F-
measure of 74.1 (Zhou and Su 2002), but when 
they are used in biomedical domain, they only get 
F-measure of 24.3.  By contrast, using the simple 
deterministic features we designed for biomedical 
NER, the system achieves F-measure of 29.4.  Ac-
cording to the comparison, some findings may be 
concluded as follows: 
1) Simple deterministic features are domain de-
pendent, which suggests that it is necessary to de-
sign special features for biomedical NER. 
2) Simple deterministic features have weaker 
predictive power for NE classes in biomedical do-
main than in newswire domain. 
3.2 Morphological Feature (Fm) 
Morphological information, such as prefix/suffix, 
is considered as an important cue for terminology 
identification.  In our system, we get most frequent 
100 prefixes and suffixes from training data as 
candidates.  Then, each of these candidates is 
evaluated according to formula f1.  ( )
i
ii
i N
OUTIN
Wt
## ?=   (f1) 
in which, #INi is the number that prefix/suffix i 
occurs within NEs; #OUTi is the number that pre-
fix/suffix i occurs out of NEs; Ni is the total num-
ber of prefix/suffix i. 
The formula assumes that the particular pre-
fix/suffix, which is most likely inside NEs and 
least likely outside NEs, may be thought as a good 
evidence for distinguishing the NEs.  The candi-
dates with Wt above a certain threshold (0.7 in ex-
periment) are chosen.  Then, we calculated the 
frequency of each prefix/suffix in each NE class 
and group the prefixes/suffixes with the similar 
distribution among NE classes into one feature.  
This is because prefixes/suffixes with the similar 
distribution have the similar contribution, and it 
will avoid suffering from the data sparseness prob-
lem.  Some of morphological features were listed 
in Table 2. 
 
Fm Name Prefix/Suffix Example 
sOOC ~cin actinomycin 
 ~mide Cycloheximide 
 ~zole Sulphamethoxazole 
sLPD ~lipid Phospholipids 
 ~rogen Estrogen 
 ~vitamin dihydroxyvitamin 
sCTP ~blast erythroblast 
 ~cyte thymocyte 
 ~phil eosinophil 
sPEPT ~peptide neuropeptide 
sMA ~ma hybridoma 
sVIR ~virus cytomegalovirus 
Table 2: Examples of morphological features 
 
From Table 2, the suffixes ~cin, ~mide, ~zole 
have been grouped into one feature sOOC because 
they all have the high frequency in the NE class 
OtherOrganicCompound and relatively low fre-
quencies in the other NE classes.   In our system, 
totally 37 prefixes and suffixes were selected and 
grouped to 23 features. 
3.3 Part-of-Speech Features (Fpos) 
In the previous NER research in newswire domain, 
part-of-speech (POS) features were stated not use-
ful, as POS features may affect the use of some 
important capitalization information (Zhou and Su 
2002).  However, since more and more words with 
lower case are included in NEs, capitalization in-
formation in biomedical domain is not as eviden-
tial as it in newswire domain (Zhang et al 2003).  
Moreover, since many biomedical NEs are descrip-
tive and long, identifying NE boundary is not a 
trivial task.  POS tagging can provide the evidence 
of noun phrase region based on word syntactic in-
formation and the noun phrases are most likely to 
be NE.  Therefore, we reconsidered the POS tag-
ging.   
In previous research, (Kazama et al 2002) 
make use of POS information and conclude that it 
only slightly improves performance.  Moreover, 
(Collier et al 2000; Nobata et al 2000; Takeuchi 
and Collier. 2002) don?t incorporate POS informa-
tion in their systems.  The probable reason ex-
plained by them is that since POS tagger they used 
is trained on newswire articles, the assigned POS 
tags are often incorrect in biomedical documents.  
On the whole, it can be concluded that POS infor-
mation hasn?t been well used in previous work. 
In our experiment, a POS tagger was trained us-
ing 80% of GENIA V2.1 corpus (536 abstracts, 
123K words) and evaluated on the rest 20% (134 
abstracts, 29K words).  We use GENIA corpus to 
train the POS tagger in order to let it be adapted for 
biomedical domain.  As for comparison, we also 
trained the POS tagger on Wall Street Journal arti-
cles (2500 articles, 756K words) and tested on the 
20% of GENIA corpus.  The results are shown in 
Table 3. 
 
Training set Testing set Precision 
2500 WSJ articles 84.31 
536 GENIA abstracts 
134 GENIA 
abstracts 97.37 
Table 3: Comparison of POS tagger using dif-
ferent training data  
 
From Table 3, it can be found that POS tagger 
trained on the biomedical documents performs 
much better on the biomedical testing documents 
than that trained on WSJ articles.  This is consis-
tent with earlier explanation for why POS features 
are not so useful in biomedical NER (Nobata et al 
2000; Takeuchi and Collier 2002).   
3.4 Semantic Trigger Features 
Semantic trigger features are collected to capture 
the evidence of certain NE class based on the se-
mantic information of some key words.  Initially, 
we design two types of semantic triggers: head 
noun triggers and special verb triggers. 
3.4.1 Head Noun Triggers (Fhnt) 
Head noun means the main noun or noun phrase of 
some compound words and describes the function 
or the property, e.g. ?B cells? is the head noun for 
the NE ?activated human B cells?.  Compared with 
the other words in NE, head noun is a much more 
decisive factor for distinguishing NE classes.  For 
instance, 
<OtherName>IFN-gamma treatment</OtherName> 
<DNA>IFN-gamma activation sequence</DNA> 
In our work, we extract uni-gram and bi-grams 
of head nouns automatically from training data, 
and rank them by frequency.  According to the ex-
periment, we selected 60% top ranked head nouns 
as trigger features for each NE class.  Some exam-
ples are shown in Table 4. 
In the future application, we may also extract 
the head nouns from some public resources to en-
hance the triggers. 
 
1-gram 2-grams 
PROTEIN 
interleukin activator protein 
interferon binding protein 
kinase cell receptor 
ligand gene product 
CELL TYPE 
lymphocyte blast cell 
astrocyte blood lymphocyte 
eosinophil killer cell 
fibroblast peripheral monocyte 
DNA 
DNA X chromosome 
breakpoint alpha promoter 
cDNA binding motif 
chromosome promoter element 
Table 4: Examples of head noun triggers 
3.4.2 Special Verb Triggers (Fsvt) 
Besides collecting the triggers, such as head noun 
triggers, from the NEs themselves, we also extract 
the triggers from the local contexts of the NEs.  
Recently, some frequently occurred verbs in bio-
medical document have been proved useful for 
extracting the interaction between entities (Thomas 
et al 2000; Sekimizu et al 1998).  In biomedical 
NER, we have the intuition that particular verbs 
may also provide the evidence for boundary and 
NE class.  For instance, the verb bind is often used 
to indicate the interaction between proteins. 
In our system, we selected 20 most frequent 
verbs which occur adjacent to NE from training 
data automatically as the verb trigger features, 
which is shown in Table 5.   
 
 
Special Verb Triggers 
activate express 
bind induce 
inhibit interact 
regulate stimulate 
Table 5: Examples of special verb triggers 
4 Method for Abbreviation Recognition 
Abbreviations are widely used in biomedical do-
main.  Identifying the class of them constitutes an 
important and difficult problem (Zhang et al 2003). 
In our current system, we incorporate a method 
to classify abbreviation by mapping the abbrevia-
tion to its full form. This approach is based on the 
assumption that it is easier to classify the full form 
than abbreviation.  In most cases, this assumption 
is valid because the full form has more evidences 
than its abbreviation to capture the NE class.  
Moreover, if we can map the abbreviation to its 
full form in the current document, the recognized 
abbreviation is still helpful for classifying the same 
forthcoming abbreviations in the same document, 
as in (Zhou and Su 2002). 
In practice, abbreviation and its full form often 
occur simultaneously with parenthesis when first 
appear in biomedical documents.  There are two 
cases: 
1. full form (abbreviation) 
2. abbreviation (full form) 
Most patterns conform to the first case and if 
the content inside the parenthesis includes more 
than two words, the second case is assumed 
(Schwartz and Hearst 2003).   
In these two cases, the use of parenthesis is 
both evidential and confusing.  On one hand, it is 
evidential because it can provide the indication to 
map the abbreviation to its full form.  On the other 
hand, it is confusing because it makes the annota-
tion of NE more complicated.  Sometimes, the ab-
breviation and its full form are annotated 
separately, such as  
<CellType>human mononulear leuko-
cytes</CellType>(<CellType>hMNL</CellType>), 
and sometimes, they are all embedded in the whole 
entity, such as 
<OtherName>leukotriene B4 (LTB4) genera-
tion</OtherName>.   
Therefore, parenthesis needs to be treated specially.  
We develop an abbreviation recognition algorithm 
described in Figure 1. 
In preprocessing stage, we remove the abbre-
viations and parentheses from the sentence, when 
the abbreviation is first defined.  This measure will 
make the annotation simpler and the NE recognizer 
more effective.  The main work in this stage is to 
judge which case the current pattern belongs to and 
record the original positions of the abbreviation 
and parenthesis. 
After applying the HMM-based NE recognizer 
to the sentence, we restore the abbreviation and 
parenthesis to the original position in the sentence.  
Next, the abbreviation is classified.  There are two 
priorities of the class (from high to low): the class 
of its full form identified by the recognizer, and the 
class of the abbreviation itself identified by the 
recognizer.  At last, the same abbreviation occur-
ring in the rest sentences of the current document 
are assigned the same NE class.   
 
for each sentence Si in the document{ 
if exist parenthesis{ 
judge the case of { 
?full form (abbr.)?; 
?abbr. (full form)?; 
} 
store the abbr. A and position Pa  to a list; 
record the parenthesis position Pp; 
remove A and parenthesis from sentence; 
apply HMM-based NE recognizer to Si; 
restore A and parenthesis into Pa, Pp; 
if Pp within an identified NE E with the class CE 
parenthesis is included in E; 
else{ 
parenthesis is not included; 
   classify A to CE; 
   classify A in the rest part of document to CE; 
} 
} 
else apply HMM-based NE recognizer to Si; 
} 
Figure 1: Abbreviation recognition algorithm 
5 Solution of Cascaded Phenomena 
In (Zhang et al 2003), they state that 16.57% of 
NEs in GENIA V3.0 have cascaded annotations, 
such as  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Currently, we only consider the longest NE and 
ignore the embedded NEs.   
Based on the features described in section 3, 
our system counters some problems when dealing 
with cascaded NEs.  The probable reason is that 
the features we used are not so effective for this 
kind of NEs.   
For instance, POS is based on the assumption 
that NE is most likely to be a noun phrase.  For 
cascaded NE, this assumption may not always be 
valid because one NE may consist of two or more 
noun phrases connected by some special words, 
such as TSH receptor specific T cell lines. 
Moreover, in section 3.4.1, we have shown that 
head noun is the significant clue for distinguishing 
NE classes.  Even for cascaded NEs, head noun 
features are still effective to some extent, such as 
IL-2 mRNA.  However, cascaded NEs sometimes 
contain two or more head nouns, which belong to 
different NE classes.  For example, <DNA>IgG Fc 
receptor type IC gene</DNA>, in which receptor 
is the head noun of protein and gene is the head 
noun of DNA.  In general, the latter head noun will 
be more important.  Unfortunately, it seems that 
sometimes the shorter NE is more possible to be 
identified, such as <protein>IgG Fc recep-
tor</protein> type IC gene.   
On the whole, we have to explore an additional 
method to cope with the cascaded phenomena 
separately.  In our experiment, we attempt to solve 
this problem based on some rules. 
In GENIA corpus, we find that there are four 
basic types of cascaded NEs: 
1. < <NE> head noun >  
2. < modifier <NE> > 
3. < <NE1> <NE2> > 
4. < <NE1> word <NE2> > 
Moreover, these cascaded NEs may be generated 
iteratively.  For instance, 
5. < modifier <NE> head noun > 
6. < <NE1> <NE2> head noun > 
The rules are constructed automatically from 
the cascaded NEs in training data.  Corresponding 
to the four basic types of cascaded NEs mentioned 
before, we propose four patterns and apply them 
iteratively in each sentence: 
1. <entity1> head noun ? <entity2>  
e.g. <Protein> binding motif ? <DNA> 
2. <entity1> <entity2> ? <entity3> 
e.g. <Lipid> <Protein> ? <Protein> 
3. modifier <entity1> ? <entity2> 
e.g. anti <Protein> ? <Protein> 
4. <entity1> word <entity2> ? <entity3> 
e.g. <Virus> infected <Multicell> ? <Multicell> 
In our system, 102 rules are incorporated to 
classify the cascaded NEs. 
6 
6.1 
6.2 
Experiments 
GENIA Corpus 
GENIA corpus is the largest annotated corpus in 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiment, three ver-
sions are used: 
? GENIA Version 1.1 (V1.1) -- It contains 670 
MEDLINE abstracts.  Since a lot of previous re-
lated work used this version, we use it to compare 
our result with others?. 
? GENIA Version 2.1 (V2.1) -- It contains the 
same 670 abstracts as V1.1 and POS tagging.  We 
use it to train and evaluate our POS tagger. 
? GENIA Version 3.0 (V3.0) -- It contains 2000 
abstracts, which is the superset of V1.1.  We use it 
to get the latest result and find out the effect of 
training data size. 
The annotation of NE is based on the GENIA 
ontology.  In our task, we use 23 distinct NE 
classes.  As for the conjunctive and disjunctive 
NEs, we ignore such cases and take the whole con-
struction as one entity.  In addition, for the cas-
caded annotations in V3.0, currently, we only 
consider the longest one level of the annotations. 
Experimental Results 
The system is evaluated using standard ?preci-
sion/recall/F-measure?, in which ?F-measure? is 
defined as F-measure = (2PR) / (P+R). 
We evaluate our NER system on both V3.0 and 
V1.1, each of which has been split into a training 
set and a testing set.  As for V1.1, we divide the 
corpus into 590 abstracts (136K words) as training 
set and the rest 80 abstracts (17K words) as testing 
set.  As for V3.0, we use the same testing set as 
V1.1 and the rest 1920 abstracts (447K words) as 
training set. 
 
Corpus P R F 
Our system on V3.0 66.5 65.7 66.1 
Our system on V1.1 63.8 61.3 62.5 
Kazama?s on V1.1 56.2 52.8 54.4 
Table 6: Comparison of overall performance 
 
Table 6 shows the overall performance of our 
system on V3.0 and V1.1, and the best reported 
system on V1.1 described in (Kazama et al 2002).  
On V1.1, we use the same training and testing data 
and capture the same NE classes as (Kazama et al 
2002).  Our system (62.5 F-measure) outperforms 
Kazama?s (54.4 F-measure) by 8.1 F-measure.  
This probably benefits from the various evidential 
features and the effective methods we proposed.  
Furthermore, as our expectation, the performance 
achieved on V3.0 (66.1 F-measure) is better than 
that on V1.1 (62.5 F-measure), which indicate that 
our system still has some room for improvement 
with the larger training data set. 
 
 
Figure 2: Performance of each NE class 
 
In addition, Figure 2 shows the detailed per-
formance chart of each NE class on V3.0.  In the 
figure, the numbers in the parenthesis are the num-
ber that NEs of that class occur in training/testing 
data.  It can be found that the performances vary a 
lot among the NE classes.  Some NE classes that 
have very few training data, such as Carbohydrate 
and Organism, get extremely low performance.  
In order to evaluate the contributions of differ-
ent features, we evaluate our system using different 
combinations of features (Table 7). 
From Table 7, several findings are concluded:  
1) With only Fsd, our system achieves a basic 
level F-measure of 29.4. 
2) Fm shows the positive effect with 2.4 F-
measure improvement based on the basic level.  
However, it only can slightly improve the perform-
ance (+1.2 F-measure) based on Fsd, Fpos and Fhnt.  
The probable reason is that the evidences included 
in Fm have already been captured by Fhnt.  More-
over, the evidences captured by Fhnt are more accu-
rate than that captured by Fm.  The contribution 
made by Fm may come from where there is no indi-
cation of Fhnt. 
 
Fsd Fm Fpos Fhnt Fsvt P R F 
?     42.4 22.5 29.4 
? ?    44.8 24.6 31.8 
? ? ?   58.3 50.9 54.3 
?  ? ?  62.0 61.6 61.8 
? ? ? ?  64.4 61.7 63.0 
? ? ? ? ? 60.6 59.3 60.0 
Table 7: Effects of different features on V3.0 
 
3) Fpos is proved very beneficial as it makes 
great increase on F-measure (+22.5) based on Fsd 
and Fm.   
4) Fhnt leads to an improvement of 8.7 F-
measure based on Fsd, Fm and Fpos. 
5) Out of our expectation, the use of Fsvt de-
creases both precision and recall, which may be 
explained as the present and past participles of 
some special verbs often play the adjective-like 
roles inside biomedical NEs, such as IL10-
inhibited lymphocytes.  
 
 P R F 
Fsd+Fm+Fpos+Fhnt 64.4 61.7 63.0 
+abbr. recog. algorithm 64.6 62.5 63.5 
+rule-based casc. method 66.2 65.8 66.0 
+both 66.5 65.7 66.1 
Table 8: Effects of solution for abbr. and casc. 
 
From Table 8, it can be found that the abbrevia-
tion recognition method slightly improves the per-
formance by 0.5 F-measure.  The probable reason 
is that the recognition of abbreviation relies too 
much on the recognition of its full form.  Once the 
full form is wrongly classified, the abbreviation 
and the forthcoming ones throughout the document 
are wrong altogether.  In the near future, the pre-
defined abbreviation dictionary may be incorpo-
rated to enhance the decision of NE class. 
Moreover, it can be found that the rule-based 
method effectively solves the problem of cascaded 
phenomena and shows prominent improvement 
(+3.0 F-measure) based on the performance of 
?Fsd+Fm+Fpos+Fhnt?. 
7 Conclusion 
In the paper, we describe our exploration on how 
to adapt a general HMM-based named entity rec-
ognizer to biomedical domain.  We integrate vari-
ous evidences for biomedical NER, including 
lexical, morphological, syntactic and semantic in-
formation.  Furthermore, we present a simple algo-
rithm to solve the abbreviation problem and a rule-
based method to deal with the cascaded phenom-
ena. Based on such evidences and methods, our 
system is successfully adapted to biomedical do-
main and achieves significantly better performance 
than the best published system.  In the near future, 
more effective abbreviation recognition algorithm 
and some pre-defined NE lists for some classes 
may be incorporated to enhance our system. 
Acknowledgements 
We would like to thank Mr. Tan Soon Heng for his 
support of biomedical knowledge.   
References 
M. Bikel Danie, R.Schwartz and M. Weischedel Ralph. 
1999.  An Algorithm that Learns What's in a Name. 
In Proc. of Machine Learning (Special Issue on NLP). 
A. Borthwick.  1999.  A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis. New 
York University. 
N. Collier, C. Nobata, and J. Tsujii.  2000.  Extracting 
the names of genes and gene products with a hidden 
Markov model.  In Proc. of COLING 2000, pages 
201-207. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of the 
Pacific Symposium on Biocomputing?98 (PSB?98), 
pages 707-718, January. 
R. Gaizauskas, G. Demetriou and K. Humphreys.  Term 
Recognition and Classification in Biological Science 
Journal Articles.  2000.  In Proc. of the Computional 
Terminology for Medical and Biological Applications 
Workshop of the 2nd International Conference on 
NLP, pages 37-44. 
J. Kazama, T. Makino, Y.Ohta, and J. Tsujii.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the Work-
shop on Natural Language Processing in the Bio-
medical Domain (at ACL?2002), pages 1-8. 
C. Nobata, N. Collier, and J. Tsujii.  1999.  Automatic 
term identification and classification in biology texts.  
In Proc. of the 5th NLPRS, pages 369-374. 
C. Nobata, N. Collier, and J. Tsujii.  2000.  Comparison 
between tagged corpora for the named entity task.  In 
Proc. of the Workshop on Comparing Corpora (at 
ACL?2000), pages 20-27. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
D. Proux, F. Rechenmann, L. Julliard, V. Pillet and B. 
Jacq.  1998.  Detecting Gene Symbols and Names in 
Biological Texts: A First Step toward Pertinent In-
formation Extraction.  In Proc. of Genome Inform 
Ser Workshop Genome Inform, pages 72-80. 
A.S. Schwartz and M.A. Hearst.  2003.  A Simple Algo-
rithm for Identifying Abbreviation Definitions in 
Biomedical Text.  In Proc. of the Pacific Symposium 
on Biocomputing (PSB 2003) Kauai. 
T. Sekimizu, H. Park, and J. Tsujii.  1998.  Identifying 
the interaction between genes and gene products 
based on frequently seen verbs in medline abstracts.  
In Proc. of Genome Informatics, Universal Academy 
Press, Inc.  
K. Takeuchi and N. Collier.  2002.  Use of Support Vec-
tor Machines in Extended Named Entity Recognition.  
In Proc. of the Sixth Conference on Natural Lan-
guage Learning (CONLL 2002), pages 119-125. 
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. 
Carroll.  2000.  Automatic extraction of protein inter-
actions from scientific abstracts.  In Proc. of the Pa-
cific Symposium on Biocomputing?2000 (PSB?2000), 
pages 541-551, Hawaii, January. 
A. J. Viterbi.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding algo-
rithm.  In Proc. of IEEE Transactions on Information 
Theory, pages 260-269. 
J. Zhang, D. Shen, G. Zhou, J. Su and C. Tan. 2003.  
Exploring Various Evidences for Recognition of 
Named Entities in Biomedical Domain.  Submitted to 
EMNLP 2003. 
G. Zhou and J. Su.  2002.  Named Entity Recognition 
using an HMM-based Chunk Tagger.  In Proc. of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473-480. 
Optimizing Feature Set for Chinese Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This article describes the implementation of I2R
word sense disambiguation system (I2R ?WSD)
that participated in one senseval3 task: Chinese lex-
ical sample task. Our core algorithm is a supervised
Naive Bayes classifier. This classifier utilizes an op-
timal feature set, which is determined by maximiz-
ing the cross validated accuracy of NB classifier on
training data. The optimal feature set includes part-
of-speech with position information in local con-
text, and bag of words in topical context.
1 Introduction
Word sense disambiguation (WSD) is to assign ap-
propriate meaning to a given ambiguous word in
a text. Corpus based method is one of the suc-
cessful lines of research on WSD. Many supervised
learning algorithms have been applied for WSD,
ex. Bayesian learning (Leacock et al, 1998), ex-
emplar based learning (Ng and Lee, 1996), decision
list (Yarowsky, 2000), neural network (Towel and
Voorheest, 1998), maximum entropy method (Dang
et al, 2002), etc.. In this paper, we employ Naive
Bayes classifier to perform WSD.
Resolving the ambiguity of words usually relies
on the contexts of their occurrences. The feature
set used for context representation consists of lo-
cal and topical features. Local features include part
of speech tags of words within local context, mor-
phological information of target word, local collo-
cations, and syntactic relations between contextual
words and target word, etc.. Topical features are
bag of words occurred within topical context. Con-
textual features play an important role in providing
discrimination information for classifiers in WSD.
In other words, an informative feature set will help
classifiers to accurately disambiguate word senses,
but an uninformative feature set will deteriorate the
performance of classifiers. In this paper, we opti-
mize feature set by maximizing the cross validated
accuracy of Naive Bayes classifier on sense tagged
training data.
2 Naive Bayes Classifier
Let C = {c1, c2, ..., cL} represent class labels,
F = {f1, f2, ..., fM} be a set of features. The
value of fj , 1 ? j ? M , is 1 if fj is present in
the context of target word, otherwise 0. In classi-
fication process, the Naive Bayes classifier tries to
find the class that maximizes P (ci|F ), the proba-
bility of class ci given feature set F , 1 ? i ? L.
Assuming the independence between features, the
classification procedure can be formulated as:
i? = arg max
1?i?L
p(ci)
?M
j=1 p(fj |ci)?M
j=1 p(fj)
, (1)
where p(ci), p(fj |ci) and p(fj) are estimated using
maximum likelihood method. To avoid the effects
of zero counts when estimating p(fj |ci), the zero
counts of p(fj |ci) are replaced with p(ci)/N , where
N is the number of training examples.
3 Feature Set
For Chinese WSD, there are two strategies to extract
contextual information. One is based on Chinese
characters, the other is to utilize Chinese words and
related morphological or syntactic information. In
our system, context representation is based on Chi-
nese words, since words are less ambiguous than
characters.
We use two types of features for Chinese WSD:
local features and topical features. All of these fea-
tures are acquired from data at senseval3 without
utilization of any other knowledge resource.
3.1 Local features
Two sets of local features are investigated, which
are represented by LocalA and LocalB. Let nl de-
note the local context window size.
LocalA contains only part of speech tags
with position information: POS?nl , ...,
POS?1, POS0, POS+1, ..., POS+nl , where
POS?i (POS+i) is the part of speech (POS) of the
i-th words to the left (right) of target word w, and
POS0 is the POS of w.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
LocalB enriches the local context by including
the following features: local words with position in-
formation (W?nl , ..., W?1, W+1, ..., W+nl), bigram
templates ((W?nl , W?(nl?1)), ..., (W?1, W+1),
..., (W+(nl?1), W+nl)), local words with POS tags(W POS) (position information is not considered),
and part of speech tags with position information.
All of these POS tags, words, and bigrams are
gathered and each of them contributed as one fea-
ture. For a training or test example, the value of
some feature is 1 if it occurred in local context, oth-
erwise it is 0. In this paper, we investigate two val-
ues of nl for LocalA and LocalB, 1 and 2, which
results in four feature sets.
3.2 Topical features
We consider all Chinese words within a context
window size nt as topical features. For each training
or test example, senseval3 data provides one sen-
tence as the context of ambiguous word. In sense-
val3 Chinese training data, all contextual sentences
are segmented into words and tagged with part of
speech.
Words which contain non-Chinese character are
removed, and remaining words occurred within
context window size nt are gathered. Each remain-
ing word is considered as one feature. The value of
topical feature is 1 if it occurred within window size
nt, otherwise it is 0.
In later experiment, we set different values for nt,
ex. 1, 2, 3, 4, 5, 10, 20, 30, 40, 50. Our experimen-
tal result indicated that the accuracy of sense dis-
ambiguation is related to the value of nt. For differ-
ent ambiguous words, the value of nt which yields
best disambiguation accuracy is different. It is de-
sirable to determine an optimal value, n?t, for each
ambiguous word by maximizing the cross validated
accuracy.
4 Data Set
In Chinese lexical sample task, training data con-
sists of 793 sense-tagged examples for 20 ambigu-
ous Chinese words. Test data consists of 380 un-
tagged examples for the same 20 target words. Ta-
ble 1 shows the details of training data and test data.
5 Criterion for Evaluation of Feature Sets
In this paper, five fold cross validation method was
employed to estimate the accuracy of our classi-
fier, which was the criterion for evaluation of fea-
ture sets. All of the sense tagged examples of some
target word in senseval3 training data were shuf-
fled and divided into five equal folds. We used four
folds as training set and the remaining fold as test
set. This procedure was repeated five times under
different division between training set and test set.
The average accuracy over five runs is defined as the
accuracy of our classifier.
6 Evaluation of Feature Sets
Four feature sets were investigated:
FEATUREA1: LocalA with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREA2: LocalA with nl = 2, and topical
feature within optimal context window size n?t;
FEATUREB1: LocalB with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREB2: LocalB with nl = 2, and topical
feature within optimal context window size n?t.
We performed training and test procedure using
exactly same training and test set for each feature
set. For each word, the optimal value of topical con-
text window size n?t was determined by selecting a
minimal value of nt which maximized the cross val-
idated accuracy.
Table 2 summarizes the results of Naive Bayes
classifier using four feature sets evaluated on sen-
seval3 Chinese training data. Figure 1 shows the
accuracy of Naive Bayes classifier as a function of
topical context window size on four nouns and three
verbs. Several results should be noted specifically:
If overall accuracy over 20 Chinese charac-
ters is used as evaluation criterion for feature
set, the four feature sets can be sorted as fol-
lows: FEATUREA1 > FEATUREA2 ?
FEATUREB1 > FEATUREB2. This indi-
cated that simply increasing local window size or
enriching feature set by incorporating bigram tem-
plates, local word with position information, and lo-
cal words with POS tags did not improve the perfor-
mance of sense disambiguation.
In table 2, it showed that with FEATUREA1, the
optimal topical context window size was less than
10 words for 13 out of 20 target words. Figure
1 showed that for most of nouns and verbs, Naive
Bayes classifier achieved best disambiguation accu-
racy with small topical context window size (<10
words). This gives the evidence that for most of
Chinese words, including nouns and verbs, the near
distance context is more important than the long dis-
tance context for sense disambiguation.
7 Experimental Result
The empirical study in section 6 showed that FEA-
TUREA1 performed best among all the feature sets.
A Naive Bayes classifier with FEATUREA1 as fea-
ture set was learned from all the senseval3 Chinese
training data for each target word. Then we used
Table 1: Details of training data and test data in Chinese lexical sample task.
POS occurred # senses occurred
Ambiguous word in training data # training examples in training data # test examples
ba3wo4 n v vn 31 4 15
bao1 n nr q v 76 8 36
cai2liao4 n 20 2 10
chong1ji1 v vn 28 3 13
chuan1 v 28 3 14
di4fang1 b n 36 4 17
fen1zi3 n 36 2 16
huo2dong4 a v vn 36 5 16
lao3 Ng a an d j 57 6 26
lu4 n nr q 57 6 28
mei2you3 d v 30 3 15
qi3lai2 v 40 4 20
qian2 n nr 40 4 20
ri4zi5 n 48 3 21
shao3 Ng a ad j v 42 5 20
tu1chu1 a ad v 30 3 15
yan2jiu1 n v vn 30 3 15
yun4dong4 n nz v vn 54 3 27
zou3 v vn 49 5 24
zuo4 v 25 3 12
this classifier to determine the senses of occurrences
of target words in test data. The official result of
I2R?WSD system in Chinese lexical sample task
is listed below:
Precision: 60.40% (229.00 correct of 379.00 at-
tempted).
Recall: 60.40% (229.00 correct of 379.00 in to-
tal).
Attempted: 100.00% (379.00 attempted of
379.00 in total).
8 Conclusion
In this paper, we described the implementation of
I2R ? WSD system that participated in one sen-
seval3 task: Chinese lexical sample task. An op-
timal feature set was selected by maximizing the
cross validated accuracy of supervised Naive Bayes
classifier on sense-tagged data. The senses of occur-
rences of target words in test data were determined
using Naive Bayes classifier with optimal feature
set learned from training data. Our system achieved
60.40% precision and recall in Chinese lexical sam-
ple task.
References
Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D.
(2002) Simple Features for Chinese Word Sense
Disambiguation. In Proc. of COLING.
Leacock, C., Chodorow, M., & Miller G. A. (1998)
Using Corpus Statistics and WordNet Relations
for Sense Identification. Computational Linguis-
tics, 24:1, 147?165.
Mooney, R. J. (1996) Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proc.
of EMNLP, pp. 82-91, Philadelphia, PA.
Ng, H. T., & Lee H. B. (1996) Integrating Multi-
ple Knowledge Sources to Disambiguate Word
Sense: An Exemplar-Based Approach. In Proc.
of ACL, pp. 40-47.
Pedersen, T. (2001) A Decision Tree of Bigrams is
an Accurate Predictor of Word Sense. In Proc. of
NAACL.
Towel, G., & Voorheest, E. M. (1998) Disambiguat-
ing Highly Ambiguous Words. Computational
Linguistics, 24:1, 125?146.
Yarowsky, D. (2000) Hierarchical Decision Lists
for Word Sense Disambiguation. Computers and
the Humanities, 34(1-2), 179?186.
Table 2: Accuracy of Naive Bayes classifier with different feature sets on Senseval3 Chinese training data.
FEATUREA1 FEATUREA2 FEATUREB1 FEATUREB2
Ambiguous word n?t Accuracy n?t Accuracy n?t Accuracy n?t Accuracy
ba3wo4 5 30.0 4 23.3 4 30.0 3 30.0
bao1 2 30.7 20 34.0 2 33.3 20 32.0
cai2liao4 2 85.0 2 80.0 2 75.0 2 60.0
chong1ji1 20 40.0 3 40.0 30 36.0 1 28.0
chuan1 3 72.0 5 68.0 3 56.0 5 64.0
di4fang1 2 74.3 1 62.9 1 71.4 1 65.7
fen1zi3 20 91.4 50 91.4 20 88.6 20 85.7
huo2dong4 5 40.0 20 51.4 10 42.9 4 40.0
lao3 3 49.1 4 47.3 3 52.7 20 52.7
lu4 1 83.6 2 78.2 2 81.8 1 76.4
mei2you3 20 50.0 20 47.9 4 43.3 3 50.0
qi3lai2 4 75.0 1 75.0 1 80.0 1 77.5
qian2 3 57.5 4 57.5 3 60.0 5 57.5
ri4zi5 4 62.2 4 57.8 10 55.6 4 55.6
shao3 4 45.0 3 50.0 10 42.5 20 50.0
tu1chu1 10 83.3 10 80.0 10 80.0 10 76.7
yan2jiu1 20 43.3 20 46.7 10 50.0 20 36.7
yun4dong4 10 64.0 10 66.0 10 62.0 10 58.0
zou3 5 44.4 5 44.4 4 51.1 4 51.1
zuo4 20 64.0 30 60.0 20 64.0 20 64.0
Overall 57.7 56.9 57.0 55.1
0 1 2 3 4 5 10 20 30 40 500.4
0.5
0.6
0.7
0.8
0.9
1
nt
Ac
cu
rac
y
0 1 2 3 4 5 10 20 30 40 500.3
0.4
0.5
0.6
0.7
0.8
nt
Ac
cu
rac
y
chuan1 
qi3lai2
zuo4   
cai2liao4
fen1zi3  
qian2    
ri4zi5   
Figure 1: Accuracy of Naive Bayes classifier with the optimal feature set FEATUREA1 on four nouns (top
figure) and three verbs (bottom figure). The horizontal axis represents the topical context window size.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 415?422,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Supervised Sense Disambiguation by Learning Sense Number
from Tagged and Untagged Corpora
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Supervised and semi-supervised sense dis-
ambiguation methods will mis-tag the in-
stances of a target word if the senses of
these instances are not defined in sense in-
ventories or there are no tagged instances
for these senses in training data. Here we
used a model order identification method
to avoid the misclassification of the in-
stances with undefined senses by discov-
ering new senses from mixed data (tagged
and untagged corpora). This algorithm
tries to obtain a natural partition of the
mixed data by maximizing a stability cri-
terion defined on the classification result
from an extended label propagation al-
gorithm over all the possible values of
the number of senses (or sense number,
model order). Experimental results on
SENSEVAL-3 data indicate that it outper-
forms SVM, a one-class partially super-
vised classification algorithm, and a clus-
tering based model order identification al-
gorithm when the tagged data is incom-
plete.
1 Introduction
In this paper, we address the problem of partially
supervised word sense disambiguation, which is
to disambiguate the senses of occurrences of a tar-
get word in untagged texts when given incomplete
tagged corpus 1.
Word sense disambiguation can be defined as
associating a target word in a text or discourse
1?incomplete tagged corpus? means that tagged corpus
does not include the instances of some senses for the target
word, while these senses may occur in untagged texts.
with a definition or meaning. Many corpus based
methods have been proposed to deal with the sense
disambiguation problem when given definition for
each possible sense of a target word or a tagged
corpus with the instances of each possible sense,
e.g., supervised sense disambiguation (Leacock et
al., 1998), and semi-supervised sense disambigua-
tion (Yarowsky, 1995).
Supervised methods usually rely on the infor-
mation from previously sense tagged corpora to
determine the senses of words in unseen texts.
Semi-supervised methods for WSD are charac-
terized in terms of exploiting unlabeled data in
the learning procedure with the need of prede-
fined sense inventories for target words. The in-
formation for semi-supervised sense disambigua-
tion is usually obtained from bilingual corpora
(e.g. parallel corpora or untagged monolingual
corpora in two languages) (Brown et al, 1991; Da-
gan and Itai, 1994), or sense-tagged seed examples
(Yarowsky, 1995).
Some observations can be made on the previous
supervised and semi-supervised methods. They
always rely on hand-crafted lexicons (e.g., Word-
Net) as sense inventories. But these resources may
miss domain-specific senses, which leads to in-
complete sense tagged corpus. Therefore, sense
taggers trained on the incomplete tagged corpus
will misclassify some instances if the senses of
these instances are not defined in sense invento-
ries. For example, one performs WSD in informa-
tion technology related texts using WordNet 2 as
sense inventory. When disambiguating the word
?boot? in the phrase ?boot sector?, the sense tag-
ger will assign this instance with one of the senses
of ?boot? listed in WordNet. But the correct sense
2Online version of WordNet is available at
http://wordnet.princeton.edu/cgi-bin/webwn2.0
415
?loading operating system into memory? is not in-
cluded in WordNet. Therefore, this instance will
be associated with an incorrect sense.
So, in this work, we would like to study the
problem of partially supervised sense disambigua-
tion with an incomplete sense tagged corpus.
Specifically, given an incomplete sense-tagged
corpus and a large amount of untagged examples
for a target word 3, we are interested in (1) label-
ing the instances in the untagged corpus with sense
tags occurring in the tagged corpus; (2) trying to
find undefined senses (or new senses) of the target
word 4 from the untagged corpus, which will be
represented by instances from the untagged cor-
pus.
We propose an automatic method to estimate
the number of senses (or sense number, model or-
der) of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification result over all
the possible values of sense number. At the same
time, we can obtain a classification of the mixed
data with the optimal number of groups. If the es-
timated sense number in the mixed data is equal
to the sense number of the target word in tagged
corpus, then there is no new sense in untagged
corpus. Otherwise new senses will be represented
by groups in which there is no instance from the
tagged corpus.
This partially supervised sense disambiguation
algorithm may help enriching manually compiled
lexicons by inducing new senses from untagged
corpora.
This paper is organized as follows. First, a
model order identification algorithm will be pre-
sented for partially supervised sense disambigua-
tion in section 2. Section 3 will provide experi-
mental results of this algorithm for sense disam-
biguation on SENSEVAL-3 data. Then related
work on partially supervised classification will be
summarized in section 4. Finally we will conclude
our work and suggest possible improvements in
section 5.
2 Partially Supervised Word Sense
Disambiguation
The partially supervised sense disambiguation
problem can be generalized as a model order iden-
3Untagged data usually includes the occurrences of all the
possible senses of the target word
4?undefined senses? are the senses that do not appear in
tagged corpus.
tification problem. We try to estimate the sense
number of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification results over all
the possible values of sense number. If the esti-
mated sense number in the mixed data is equal to
the sense number in the tagged corpus, then there
is no new sense in the untagged corpus. Other-
wise new senses will be represented by clusters in
which there is no instance from the tagged corpus.
The stability criterion assesses the agreement be-
tween classification results on full mixed data and
sampled mixed data. A partially supervised clas-
sification algorithm is used to classify the full or
sampled mixed data into a given number of classes
before the stability assessment, which will be pre-
sented in section 2.1. Then we will provide the
details of the model order identification procedure
in section 2.2.
2.1 An Extended Label Propagation
Algorithm
Table 1: Extended label propagation algorithm.
Function: ELP(DL, DU , k, Y 0DL+DU )Input: labeled examples DL, unlabeled
examples DU , model order k, initial
labeling matrix Y 0DL+DU ;Output: the labeling matrix YDU on DU ;
1 If k < kXL then
YDU =NULL;
2 Else if k = kXL then
Run plain label propagation algorithm
on DU with YDU as output;
3 Else then
3.1 Estimate the size of tagged data set
of new classes;
3.2 Generate tagged examples from DU
for (kXL + 1)-th to k-th new classes;
3.3 Run plain label propagation algorithm
on DU with augmented tagged dataset
as labeled data;
3.4 YDU is the output from plain label
propagation algorithm;
End if
4 Return YDU ;
Let XL+U = {xi}ni=1 be a set of contexts of
occurrences of an ambiguous word w, where xi
represents the context of the i-th occurrence, and n
is the total number of this word?s occurrences. Let
416
SL = {sj}cj=1 denote the sense tag set of w in XL,
where XL denotes the first l examples xg(1 ? g ?
l) that are labeled as yg (yg ? SL). Let XU denote
other u (l + u = n) examples xh(l + 1 ? h ? n)
that are unlabeled.
Let Y 0XL+U ? N |XL+U |?|SL| represent initialsoft labels attached to tagged instances, where
Y 0XL+U ,ij = 1 if yi is sj and 0 otherwise. Let Y 0XL
be the top l rows of Y 0XL+U and Y 0XU be the remain-
ing u rows. Y 0XL is consistent with the labeling inlabeled data, and the initialization of Y 0XU can bearbitrary.
Let k denote the possible value of the number
of senses in mixed data XL+U , and kXL be the
number of senses in initial tagged data XL. Note
that kXL = |SL|, and k ? kXL .
The classification algorithm in the order identi-
fication process should be able to accept labeled
data DL 5, unlabeled data DU 6 and model order k
as input, and assign a class label or a cluster index
to each instance in DU as output. Previous super-
vised or semi-supervised algorithms (e.g. SVM,
label propagation algorithm (Zhu and Ghahra-
mani, 2002)) cannot classify the examples in DU
into k groups if k > kXL . The semi-supervised k-
means clustering algorithm (Wagstaff et al, 2001)
may be used to perform clustering analysis on
mixed data, but its efficiency is a problem for clus-
tering analysis on a very large dataset since multi-
ple restarts are usually required to avoid local op-
tima and multiple iterations will be run in each
clustering process for optimizing a clustering so-
lution.
In this work, we propose an alternative method,
an extended label propagation algorithm (ELP),
which can classify the examples in DU into k
groups. If the value of k is equal to kXL , then
ELP is identical with the plain label propagation
algorithm (LP) (Zhu and Ghahramani, 2002). Oth-
erwise, if the value of k is greater than kXL , we
perform classification by the following steps:
(1) estimate the dataset size of each new class as
sizenew class by identifying the examples of new
classes using the ?Spy? technique 7 and assuming
5DL may be the dataset XL or a subset sampled from XL.
6DU may be the dataset XU or a subset sampled from
XU .
7The ?Spy? technique was proposed in (Liu et al, 2003).
Our re-implementation of this technique consists of three
steps: (1) sample a small subset DsL with the size 15%?|DL|from DL; (2) train a classifier with tagged data DL ? DsL;(3) classify DU and DsL, and then select some examples from
DU as the dataset of new classes, which have the classifica-
that new classes are equally distributed;
(2) D?L = DL, D?U = DU ;
(3) remove tagged examples of the m-th new
class (kXL + 1 ? m ? k) from D?L 8 and train a
classifier on this labeled dataset without the m-th
class;
(4) the classifier is then used to classify the ex-
amples in D?U ;
(5) the least confidently unlabeled point
xclass m ? D
?
U , together with its label m, is added
to the labeled data D?L = D?L + xclass m, and
D?U = D
?
U ? xclass m;
(6) steps (3) to (5) are repeated for each new
class till the augmented tagged data set is large
enough (here we try to select sizenew class/4 ex-
amples with their sense tags as tagged data for
each new class);
(7) use plain LP algorithm to classify remaining
unlabeled data D?U with D?L as labeled data.
Table 1 shows this extended label propagation
algorithm.
Next we will provide the details of the plain la-
bel propagation algorithm.
Define Wij = exp(?d
2
ij
?2 ) if i 6= j and Wii = 0(1 ? i, j ? |DL + DU |), where dij is the distance
(e.g., Euclidean distance) between the example xi
and xj , and ? is used to control the weight Wij .
Define |DL + DU | ? |DL + DU | probability
transition matrix Tij = P (j ? i) = Wij?n
k=1 Wkj
,
where Tij is the probability to jump from example
xj to example xi.
Compute the row-normalized matrix T by
T ij = Tij/
?n
k=1 Tik.
The classification solution is obtained by
YDU = (I ? T uu)?1T ulY 0DL . I is |DU | ? |DU |
identity matrix. T uu and T ul are acquired by split-
ting matrix T after the |DL|-th row and the |DL|-th
column into 4 sub-matrices.
2.2 Model Order Identification Procedure
For achieving the model order identification (or
sense number estimation) ability, we use a clus-
ter validation based criterion (Levine and Domany,
2001) to infer the optimal number of senses of w
in XL+U .
tion confidence less than the average of that in DsL. Classifi-cation confidence of the example xi is defined as the absolute
value of the difference between two maximum values from
the i-th row in labeling matrix.
8Initially there are no tagged examples for the m-th class
in D?L. Therefore we do not need to remove tagged examples
for this new class, and then directly train a classifier with D?L.
417
Table 2: Model order evaluation algorithm.
Function: CV(XL+U , k, q, Y 0XL+U )
Input: data set XL+U , model order k,
and sampling frequency q;
Output: the score of the merit of k;
1 Run the extended label propagation
algorithm with XL, XU , k and Y 0XL+U ;
2 Construct connectivity matrix Ck based
on above classification solution on XU ;
3 Use a random predictor ?k to assign
uniformly drawn labels to each vector
in XU ;
4 Construct connectivity matrix C?k using
above classification solution on XU ;
5 For ? = 1 to q do
5.1 Randomly sample a subset X?L+U with
the size ?|XL+U | from XL+U , 0 < ? < 1;
5.2 Run the extended label propagation
algorithm with X?L, X?U , k and Y 0?;
5.3 Construct connectivity matrix C?k using
above classification solution on X?U ;
5.4 Use ?k to assign uniformly drawn labels
to each vector in X?U ;
5.5 Construct connectivity matrix C??k usingabove classification solution on X?U ;
Endfor
6 Evaluate the merit of k using following
formula:
Mk = 1q
?
?(M(C?k , Ck) ? M(C??k , C?k)),
where M(C?, C) is given by equation (2);
7 Return Mk;
Then this model order identification procedure
can be formulated as:
k?XL+U = argmaxKmin?k?Kmax{CV (XL+U , k, q, Y 0XL+U )}.(1)
k?XL+U is the estimated sense number in XL+U ,
Kmin (or Kmax) is the minimum (or maximum)
value of sense number, and k is the possible value
of sense number in XL+U . Note that k ? kXL .
Then we set Kmin = kXL . Kmax may be set as a
value greater than the possible ground-truth value.
CV is a cluster validation based evaluation func-
tion. Table 2 shows the details of this function.
We set q, the resampling frequency for estimation
of stability score, as 20. ? is set as 0.90. The ran-
dom predictor assigns uniformly distributed class
labels to each instance in a given dataset. We
run this CV procedure for each value of k. The
value of k that maximizes this function will be se-
lected as the estimation of sense number. At the
same time, we can obtain a partition of XL+U with
k?XL+U groups.
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, xi, xj ? X?U}
?
i,j 1{Ci,j = 1, xi, xj ? X
?
U}
,
(2)
where X?U is the untagged data in X?L+U , X?L+U
is a subset with the size ?|XL+U | (0 < ? < 1)
sampled from XL+U , C or C? is |XU | ? |XU | or
|X?U | ? |X
?
U | connectivity matrix based on classi-
fication solutions computed on XU or X?U respec-
tively. The connectivity matrix C is defined as:
Ci,j = 1 if xi and xj belong to the same cluster,
otherwise Ci,j = 0. C? is calculated in the same
way.
M(C?, C) measures the proportion of example
pairs in each group computed on XU that are also
assigned into the same group by the classification
solution on X?U . Clearly, 0 ? M ? 1. Intu-
itively, if the value of k is identical with the true
value of sense number, then classification results
on the different subsets generated by sampling
should be similar with that on the full dataset. In
the other words, the classification solution with the
true model order as parameter is robust against re-
sampling, which gives rise to a local optimum of
M(C?, C).
In this algorithm, we normalize M(C?k , Ck) by
the equation in step 6 of Table 2, which makes
our objective function different from the figure of
merit (equation ( 2)) proposed in (Levine and Do-
many, 2001). The reason to normalize M(C?k , Ck)
is that M(C?k , Ck) tends to decrease when increas-
ing the value of k (Lange et al, 2002). Therefore
for avoiding the bias that the smaller value of k
is to be selected as the model order, we use the
cluster validity of a random predictor to normalize
M(C?k , Ck).
If k?XL+U is equal to kXL , then there is no new
sense in XU . Otherwise (k?XL+U > kXL) new
senses of w may be represented by the groups in
which there is no instance from XL.
3 Experiments and Results
3.1 Experiment Design
We evaluated the ELP based model order iden-
tification algorithm on the data in English lexi-
cal sample task of SENSEVAL-3 (including all
418
Table 3: Description of The percentage of official
training data used as tagged data when instances
with different sense sets are removed from official
training data.
The percentage of official
training data used as tagged data
Ssubset = {s1} 42.8%
Ssubset = {s2} 76.7%
Ssubset = {s3} 89.1%
Ssubset = {s1, s2} 19.6%
Ssubset = {s1, s3} 32.0%
Ssubset = {s2, s3} 65.9%
the 57 English words ) 9, and further empirically
compared it with other state of the art classifi-
cation methods, including SVM 10 (the state of
the art method for supervised word sense disam-
biguation (Mihalcea et al, 2004)), a one-class par-
tially supervised classification algorithm (Liu et
al., 2003) 11, and a semi-supervised k-means clus-
tering based model order identification algorithm.
The data for English lexical samples task in
SENSEVAL-3 consists of 7860 examples as offi-
cial training data, and 3944 examples as official
test data for 57 English words. The number of
senses of each English word varies from 3 to 11.
We evaluated these four algorithms with differ-
ent sizes of incomplete tagged data. Given offi-
cial training data of the word w, we constructed
incomplete tagged data XL by removing the all
the tagged instances from official training data that
have sense tags from Ssubset, where Ssubset is a
subset of the ground-truth sense set S for w, and S
consists of the sense tags in official training set for
w. The removed training data and official test data
of w were used as XU . Note that SL = S?Ssubset.
Then we ran these four algorithm for each target
word w with XL as tagged data and XU as un-
tagged data, and evaluated their performance us-
ing the accuracy on official test data of all the 57
words. We conducted six experiments for each tar-
get word w by setting Ssubset as {s1}, {s2}, {s3},
{s1, s2}, {s1, s3}, or {s2, s3}, where si is the i-th
most frequent sense of w. Ssubset cannot be set as
{s4} since some words have only three senses. Ta-
ble 3 lists the percentage of official training data
used as tagged data (the number of examples in in-
9Available at http://www.senseval.org/senseval3
10we used a linear SV M light, available at
http://svmlight.joachims.org/.
11Available at http://www.cs.uic.edu/?liub/LPU/LPU-
download.html
complete tagged data divided by the number of ex-
amples in official training data) when we removed
the instances with sense tags from Ssubset for all
the 57 words. If Ssubset = {s3}, then most of
sense tagged examples are still included in tagged
data. If Ssubset = {s1, s2}, then there are very few
tagged examples in tagged data. If no instances are
removed from official training data, then the value
of percentage is 100%.
Given an incomplete tagged corpus for a target
word, SVM does not have the ability to find the
new senses from untagged corpus. Therefore it la-
bels all the instances in the untagged corpus with
sense tags from SL.
Given a set of positive examples for a class and
a set of unlabeled examples, the one-class partially
supervised classification algorithm, LPU (Learn-
ing from Positive and Unlabeled examples) (Liu
et al, 2003), learns a classifier in four steps:
Step 1: Identify a small set of reliable negative
examples from unlabeled examples by the use of a
classifier.
Step 2: Build a classifier using positive ex-
amples and automatically selected negative exam-
ples.
Step 3: Iteratively run previous two steps until
no unlabeled examples are classified as negative
ones or the unlabeled set is null.
Step 4: Select a good classifier from the set of
classifiers constructed above.
For comparison, LPU 12 was run to perform
classification on XU for each class in XL. The
label of each instance in XU was determined by
maximizing the classification score from LPU out-
put for each class. If the maximum score of an
instance is negative, then this instance will be la-
beled as a new class. Note that LPU classifies
XL+U into kXL + 1 groups in most of cases.
The clustering based partially supervised sense
disambiguation algorithm was implemented by re-
placing ELP with a semi-supervised k-means clus-
tering algorithm (Wagstaff et al, 2001) in the
model order identification procedure. The label
information in labeled data was used to guide the
semi-supervised clustering on XL+U . Firstly, the
labeled data may be used to determine initial clus-
ter centroids. If the cluster number is greater
12The three parameters in LPU were set as follows: ?-s1
spy -s2 svm -c 1?. It means that we used the spy technique for
step 1 in LPU, the SVM algorithm for step 2, and selected the
first or the last classifier as the final classifier. It is identical
with the algorithm ?Spy+SVM IS? in Liu et al (2003).
419
than kXL , the initial centroids of clusters for new
classes will be assigned as randomly selected in-
stances. Secondly, in the clustering process, the
instances with the same class label will stay in
the same cluster, while the instances with different
class labels will belong to different clusters. For
better clustering solution, this clustering process
will be restarted three times. Clustering process
will be terminated when clustering solution con-
verges or the number of iteration steps is more than
30. Kmin = kXL = |SL|, Kmax = Kmin + m. m
is set as 4.
We used Jensen-Shannon (JS) divergence (Lin,
1991) as distance measure for semi-supervised
clustering and ELP, since plain LP with JS diver-
gence achieves better performance than that with
cosine similarity on SENSEVAL-3 data (Niu et al,
2005).
For the LP process in ELP algorithm, we con-
structed connected graphs as follows: two in-
stances u, v will be connected by an edge if u is
among v?s 10 nearest neighbors, or if v is among
u?s 10 nearest neighbors as measured by cosine or
JS distance measure (following (Zhu and Ghahra-
mani, 2002)).
We used three types of features to capture the
information in all the contextual sentences of tar-
get words in SENSEVAL-3 data for all the four
algorithms: part-of-speech of neighboring words
with position information, words in topical con-
text without position information (after removing
stop words), and local collocations (as same as the
feature set used in (Lee and Ng, 2002) except that
we did not use syntactic relations). We removed
the features with occurrence frequency (counted
in both training set and test set) less than 3 times.
If the estimated sense number is more than the
sense number in the initial tagged corpus XL, then
the results from order identification based meth-
ods will consist of the instances from clusters of
unknown classes. When assessing the agreement
between these classification results and the known
results on official test set, we will encounter the
problem that there is no sense tag for each instance
in unknown classes. Slonim and Tishby (2000)
proposed to assign documents in each cluster with
the most dominant class label in that cluster, and
then conducted evaluation on these labeled docu-
ments. Here we will follow their method for as-
signing sense tags to unknown classes from LPU,
clustering based order identification process, and
ELP based order identification process. We as-
signed the instances from unknown classes with
the dominant sense tag in that cluster. The result
from LPU always includes only one cluster of the
unknown class. We also assigned the instances
from the unknown class with the dominant sense
tag in that cluster. When all instances have their
sense tags, we evaluated the their results using the
accuracy on official test set.
3.2 Results on Sense Disambiguation
Table 4 summarizes the accuracy of SVM, LPU,
the semi-supervised k-means clustering algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input using various incomplete
tagged data. The last row in Table 4 lists the av-
erage accuracy of each algorithm over the six ex-
perimental settings. Using |S| as input means that
we do not perform order identification procedure,
while using k?XL+U as input is to perform order
identification and obtain the classification results
on XU at the same time.
We can see that ELP based method outperforms
clustering based method in terms of average accu-
racy under the same experiment setting, and these
two methods outperforms SVM and LPU. More-
over, using the correct sense number as input helps
to improve the overall performance of both clus-
tering based method and ELP based method.
Comparing the performance of the same sys-
tem with different sizes of tagged data (from the
first experiment to the third experiment, and from
the fourth experiment to the sixth experiment), we
can see that the performance was improved when
given more labeled data. Furthermore, ELP based
method outperforms other methods in terms of ac-
curacy when rare senses (e.g. s3) are missing in
the tagged data. It seems that ELP based method
has the ability to find rare senses with the use of
tagged and untagged corpora.
LPU algorithm can deal with only one-class
classification problem. Therefore the labeled data
of other classes cannot be used when determining
the positive labeled data for current class. ELP
can use the labeled data of all the known classes to
determine the seeds of unknown classes. It may
explain why LPU?s performance is worse than
ELP based sense disambiguation although LPU
can correctly estimate the sense number in XL+U
420
Table 4: This table summarizes the accuracy of SVM, LPU, the semi-supervised k-means clustering al-
gorithm with correct sense number |S| or estimated sense number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense number k?XL+U as input on the official test data of ELS
task in SENSEVAL-3 when given various incomplete tagged corpora.
Clustering algorithm ELP algorithm Clustering algorithm ELP algorithm
SVM LPU with |S| as input with |S| as input with k?XL+U as input with k?XL+U as input
Ssubset =
{s1} 30.6% 22.3% 43.9% 47.8% 40.0% 38.7%
Ssubset =
{s2} 59.7% 54.6% 44.0% 62.4% 48.5% 62.6%
Ssubset =
{s3} 67.0% 53.4% 48.7% 67.2% 52.4% 69.1%
Ssubset =
{s1, s2} 14.6% 13.1% 44.4% 40.2% 35.6% 33.0%
Ssubset =
{s1, s3} 25.7% 21.1% 48.5% 37.9% 39.8% 31.0%
Ssubset =
{s2, s3} 56.2% 53.1% 47.3% 59.4% 46.6% 58.7%
Average accuracy 42.3% 36.3% 46.1% 52.5% 43.8% 48.9%
Table 5: These two tables provide the mean and
standard deviation of absolute values of the differ-
ence between ground-truth results |S| and sense
numbers estimated by clustering or ELP based or-
der identification procedure respectively.
Clustering based method ELP based method
Ssubset =
{s1} 1.3?1.1 2.2?1.1
Ssubset =
{s2} 2.4?0.9 2.4?0.9
Ssubset =
{s3} 2.6?0.7 2.6?0.7
Ssubset =
{s1, s2} 1.2?0.6 1.6?0.5
Ssubset =
{s1, s3} 1.4?0.6 1.8?0.4
Ssubset =
{s2, s3} 1.8?0.5 1.8?0.5
when only one sense is missing in XL.
When very few labeled examples are avail-
able, the noise in labeled data makes it difficult
to learn the classification score (each entry in
YDU ). Therefore using the classification confi-
dence criterion may lead to poor performance of
seed selection for unknown classes if the classifi-
cation score is not accurate. It may explain why
ELP based method does not outperform cluster-
ing based method with small labeled data (e.g.,
Ssubset = {s1}).
3.3 Results on Sense Number Estimation
Table 5 provides the mean and standard devia-
tion of absolute difference values between ground-
truth results |S| and sense numbers estimated by
clustering or ELP based order identification pro-
cedures respectively. For example, if the ground
truth sense number of the word w is kw, and the es-
timated value is k?w, then the absolute value of the
difference between these two values is |kw ? k?w|.
Therefore we can have this value for each word.
Then we calculated the mean and deviation on this
array of absolute values. LPU does not have the
order identification capability since it always as-
sumes that there is at least one new class in un-
labeled data, and does not further differentiate the
instances from these new classes. Therefore we do
not provide the order identification results of LPU.
From the results in Table 5, we can see that esti-
mated sense numbers are closer to ground truth re-
sults when given less labeled data for clustering or
ELP based methods. Moreover, clustering based
method performs better than ELP based method in
terms of order identification when given less la-
beled data (e.g., Ssubset = {s1}). It seems that
ELP is not robust to the noise in small labeled data,
compared with the semi-supervised k-means clus-
tering algorithm.
4 Related Work
The work closest to ours is partially supervised
classification or building classifiers using positive
examples and unlabeled examples, which has been
studied in machine learning community (Denis et
al., 2002; Liu et al, 2003; Manevitz and Yousef,
2001; Yu et al, 2002). However, they cannot
421
group negative examples into meaningful clusters.
In contrast, our algorithm can find the occurrence
of negative examples and further group these neg-
ative examples into a ?natural? number of clusters.
Semi-supervised clustering (Wagstaff et al, 2001)
may be used to perform classification by the use
of labeled and unlabeled examples, but it encoun-
ters the same problem of partially supervised clas-
sification that model order cannot be automatically
estimated.
Levine and Domany (2001) and Lange et al
(2002) proposed cluster validation based criteria
for cluster number estimation. However, they
showed the application of the cluster validation
method only for unsupervised learning. Our work
can be considered as an extension of their methods
in the setting of partially supervised learning.
In natural language processing community, the
work that is closely related to ours is word sense
discrimination which can induce senses by group-
ing occurrences of a word into clusters (Schu?tze,
1998). If it is considered as unsupervised meth-
ods to solve sense disambiguation problem, then
our method employs partially supervised learning
technique to deal with sense disambiguation prob-
lem by use of tagged and untagged texts.
5 Conclusions
In this paper, we present an order identification
based partially supervised classification algorithm
and investigate its application to partially super-
vised word sense disambiguation problem. Exper-
imental results on SENSEVAL-3 data indicate that
our ELP based model order identification algo-
rithm achieves better performance than other state
of the art classification algorithms, e.g., SVM,
a one-class partially supervised algorithm (LPU),
and a semi-supervised k-means clustering based
model order identification algorithm.
References
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mer-
cer.. 1991. Word Sense Disambiguation Using Sta-
tistical Methods. Proceedings of ACL.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Cor-
pus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Denis, F., Gilleron, R., & Tommasi, M.. 2002. Text
Classification from Positive and Unlabeled Exam-
ples. Proceedings of the 9th International Confer-
ence on Information Processing and Management of
Uncertainty in Knowledge-Based Systems.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M.
2002. Stability-Based Model Selection. NIPS 15.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24:1, 147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Eval-
uation of Knowledge Sources and Learning Algo-
rithms for Word Sense Disambiguation. Proceed-
ings of EMNLP, (pp. 41-48).
Levine, E., & Domany, E. 2001. Resampling Method
for Unsupervised Estimation of Cluster Validity.
Neural Computation, Vol. 13, 2573?2593.
Lin, J. 1991. Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on Informa-
tion Theory, 37:1, 145?150.
Liu, B., Dai, Y., Li, X., Lee, W.S., & Yu, P.. 2003.
Building Text Classifiers Using Positive and Unla-
beled Examples. Proceedings of IEEE ICDM.
Manevitz, L.M., & Yousef, M.. 2001. One Class
SVMs for Document Classification. Journal of Ma-
chine Learning, 2, 139-154.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based
Semi-Supervised Learning. Proceedings of ACL.
Schu?tze, H.. 1998. Automatic Word Sense Discrimi-
nation. Computational Linguistics, 24:1, 97?123.
Wagstaff, K., Cardie, C., Rogers, S., & Schroedl, S..
2001. Constrained K-Means Clustering with Back-
ground Knowledge. Proceedings of ICML.
Yarowsky, D.. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Pro-
ceedings of ACL.
Yu, H., Han, J., & Chang, K. C.-C.. 2002. PEBL: Pos-
itive example based learning for web page classifi-
cation using SVM. Proceedings of ACM SIGKDD.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
422
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 568?575,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation with Order Identification
Capabilities
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
We present an unsupervised learning ap-
proach to disambiguate various relations
between name entities by use of various
lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Lapla-
cian to recover a submanifold of data
from a high dimensionality space and
then performing cluster number estima-
tion on the eigenvectors. This method
can address two difficulties encoutered
in Hasegawa et al (2004)?s hierarchical
clustering: no consideration of manifold
structure in data, and requirement to pro-
vide cluster number by users. Experiment
results on ACE corpora show that this
spectral clustering based approach outper-
forms Hasegawa et al (2004)?s hierarchi-
cal clustering method and a plain k-means
clustering method.
1 Introduction
The task of relation extraction is to identify vari-
ous semantic relations between name entities from
text. Prior work on automatic relation extraction
come in three kinds: supervised learning algorithms
(Miller et al, 2000; Zelenko et al, 2002; Culotta
and Soresen, 2004; Kambhatla, 2004; Zhou et al,
2005), semi-supervised learning algorithms (Brin,
1998; Agichtein and Gravano, 2000; Zhang, 2004),
and unsupervised learning algorithm (Hasegawa et
al., 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
568
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Words: Words in the two entities and three context
windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two entities and
three context windows. The ?0? tag means that
the word is outside of any chunk. The ?I-XP? tag
means that this word is inside an XP chunk. The
?B-XP? by default means that the word is at the be-
ginning of an XP chunk.
? Grammatical function of the two entities and
three context windows. The last word in each chunk
is its head, and the function of the head is the func-
tion of the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The other
words in a chunk that are not the head have ?NO-
FUNC? as their function.
? IOB-chains of the heads of the two entities. So-
called IOB-chain, noting the syntactic categories of
all the constituents on the path from the root node
to this leaf node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
569
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
570
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE corpus, which contains 519 files
from sources including broadcast, newswire, and
newspaper. We only deal with intra-sentence ex-
plicit relations and assumed that all entities have
2 In this paper, the sharpness parameter ? is set to 0.2
571
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with super-
vised method based on SVM and other two unsuper-
vised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
572
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with supervised method (SVM) and
unsupervised methods((Hasegawa et al, 2004))?s method and
K-means clustering.
Precision Recall F-measure
SVM 61.2% 49.6% 54.8%
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with Supervised methods
and other Unsupervised methods
To explore the effectiveness of our unsupervised
method compared to supervised method, we perform
SVM technique with the same feature set defined in
our proposed method. The LIBSVM tool is used in
this test 4. The kernel function we used is linear
and SVM models are trained using the training set
of ACE corpus.
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
4 LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. It
supports multi-class classification.
573
Table 6: Comparison of the existing efforts on ACE RDC task.
Relation Dectection Relation Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our pro-
posed method comparing with SVM-based super-
vised method and the other two unsupervised meth-
ods. As the result shows, SVM-based method by us-
ing the same feature set in our proposed method can
achieve 61.2%/49.6%/54.8% in Precision/Recall/F-
measure. Table 5 also shows our proposed spec-
tral based method clearly outperforms the other
two unsupervised methods by 12.5% and 9.5% in
F-measure respectively. Moreover, the incorpora-
tion of various lexical and syntactic features into
Hasegawa et al (2004)?s method2 makes it outper-
form Hasegawa et al (2004)?s method1 which only
uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of en-
tity pairs, performed well on the unsupervised re-
lation disambiguation problem. Our experiments
show that by the choice of the distance parameter
?2, we can estimate the cluster number which pro-
vides the tightest clusters. We notice that the es-
timated cluster number is less than the number of
ground truth classes in most cases. The reason for
this phenomenon may be that some relation types
can not be easily distinguished using the context in-
formation only. For example, the relation subtypes
?Located?, ?Based-In? and ?Residence? are difficult
to disambiguate even for human experts to differen-
tiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the charac-
teristics of entity pairs. Another observation from
the result is that extending the outer context window
of entity mention pairs too much may not improve
the performance since the process may incorporate
more noise information and affect the clustering re-
sult.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Currently most of works on the RDC task of ACE
focused on supervised learning methods. Table 6
lists a comparison of these methods on relation de-
tection and relation classification. Zhou et al (2005)
reported the best result as 63.1%/49.5%/55.5% in
Precision/Recall/F-measure on the extraction of
ACE relation subtypes using feature based method,
which outperforms tree kernel based method by
Culotta and Soresen (2004). Although our unsu-
pervised method still can not outperform these su-
574
pervised methods, from the point of view of un-
supervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation dis-
ambiguation problem by using spectral-based clus-
tering technique with diverse lexical and syntactic
features derived from context. The advantage of our
method is that it doesn?t need any manually labeled
relation instances, and pre-definition the number of
the context clusters. Experiment results on the ACE
corpus show that our method achieves better perfor-
mance than other unsupervised methods.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
575
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177?182,
Prague, June 2007. c?2007 Association for Computational Linguistics
I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense
Disambiguation, and English Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
niu zy@hotmail.com
dhji@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper describes the implementation
of our three systems at SemEval-2007, for
task 2 (word sense discrimination), task 5
(Chinese word sense disambiguation), and
the first subtask in task 17 (English word
sense disambiguation). For task 2, we ap-
plied a cluster validation method to esti-
mate the number of senses of a target word
in untagged data, and then grouped the in-
stances of this target word into the esti-
mated number of clusters. For both task 5
and task 17, We used the label propagation
algorithm as the classifier for sense disam-
biguation. Our system at task 2 achieved
63.9% F-score under unsupervised evalua-
tion, and 71.9% supervised recall with su-
pervised evaluation. For task 5, our sys-
tem obtained 71.2% micro-average preci-
sion and 74.7% macro-average precision.
For the lexical sample subtask for task
17, our system achieved 86.4% coarse-
grained precision and recall.
1 Introduction
SemEval-2007 launches totally 18 tasks for evalua-
tion exercise, covering word sense disambiguation,
word sense discrimination, semantic role labeling,
and sense disambiguation for information retrieval,
and other topics in NLP. We participated three tasks
in SemEval-2007, which are task 2 (Evaluating
Word Sense Induction and Discrimination Systems),
task 5 (Multilingual Chinese-English Lexical Sam-
ple Task) and the first subtask at task 17 (English
Lexical Sample, English Semantic Role Labeling
and English All-Words Tasks).
The goal for SemEval-2007 task 2 (Evaluat-
ing Word Sense Induction and Discrimination Sys-
tems)(Agirre and Soroa, 2007) is to automatically
discriminate the senses of English target words by
the use of only untagged data. Here we address this
word sense discrimination problem by (1) estimat-
ing the number of word senses of a target word in
untagged data using a stability criterion, and then (2)
grouping the instances of this target word into the
estimated number of clusters according to the simi-
larity of contexts of the instances. No sense-tagged
data is used to help the clustering process.
The goal of task 5 (Chinese Word Sense Disam-
biguation) is to create a framework for the evaluation
of word sense disambiguation in Chinese-English
machine translation systems. Each participates of
this task will be provided with sense tagged train-
ing data and untagged test data for 40 Chinese pol-
ysemous words. The ?sense tags? for the ambigu-
ous Chinese target words are given in the form of
their English translations. Here we used a semi-
supervised classification algorithm (label propaga-
tion algorithm) (Niu, et al, 2005) to address this
Chinese word sense disambiguation problem.
The lexical sample subtask of task 17 (English
Word Sense Disambiguation) provides sense-tagged
training data and untagged test data for 35 nouns and
65 verbs. This data includes, for each target word:
OntoNotes sense tags (these are groupings of Word-
Net senses that are more coarse-grained than tradi-
177
tional WN entries), as well as the sense inventory for
these lemmas. Here we used only the training data
supplied in this subtask for sense disambiguation in
test set. The label propagation algorithm (Niu, et al,
2005) was used to perform sense disambiguation by
the use of both training data and test data.
This paper will be organized as follows. First, we
will provide the feature set used for task 2, task 5
and task 17 in section 2. Secondly, we will present
the word sense discrimination method used for task
2 in section 3. Then, we will give the label propa-
gation algorithm for task 5 and task 17 in section 4.
Section 5 will provide the description of data sets at
task 2, task 5 and task 17. Then, we will present the
experimental results of our systems at the three tasks
in section 6. Finally we will give a conclusion of our
work in section 7.
2 Feature Set
In task 2, task 5 and task 17, we used three types of
features to capture contextual information: part-of-
speech of neighboring words (no more than three-
word distance) with position information, unordered
single words in topical context (all the contextual
sentences), and local collocations (including 11 col-
locations). The feature set used here is as same as
the feature set used in (Lee and Ng, 2002) except
that we did not use syntactic relations.
3 The Word Sense Discrimination Method
for Task 2
Word sense discrimination is to automatically dis-
criminate the senses of target words by the use of
only untagged data. So we can employ clustering
algorithms to address this problem. Another prob-
lem is that there is no sense inventories for target
words. So the clustering algorithms should have the
ability to automatically estimate the sense number
of a target word.
Here we used the sequential Information Bottle-
neck algorithm (sIB) (Slonim, et al, 2002) to esti-
mate cluster structure, which measures the similarity
of contexts of instances of target words according to
the similarity of their contextual feature conditional
distribution. But sIB requires the number of clus-
ters as input. So we used a cluster validation method
to automatically estimate the sense number of a tar-
Table 1: Sense number estimation procedure for
word sense discrimination.
1 Set lower bound Kmin and upper bound Kmax
for sense number k;
2 Set k = Kmin;
3 Conduct the cluster validation process
presented in Table 2 to evaluate the merit of k;
4 Record k and the value of Mk;
5 Set k = k + 1. If k ? Kmax, go to step 3,
otherwise go to step 6;
6 Choose the value k? that maximizes Mk,
where k? is the estimated sense number.
get word before clustering analysis. Cluster valida-
tion (or stability based approach)is a commonly used
method to the problem of model order identification
(or cluster number estimation) (Lange, et al, 2002;
Levine and Domany, 2001). The assumption of this
method is that if the model order is identical with the
true value, then the cluster structure estimated from
the data is stable against resampling, otherwise, it is
more likely to be the artifact of sampled data.
3.1 The Sense Number Estimation Procedure
Table 1 presents the sense number estimation pro-
cedure. Kmin was set as 2, and Kmax was set as 5 in
our system. The evaluation function Mk (described
in Table 2) is relevant with the sense number k. q
is set as 20 here. Clustering solution which is stable
against resampling will give rise to a local optimum
of Mk, which indicates the true value of sense num-
ber. In the cluster validation procedure, we used the
sIB algorithm to perform clustering analysis (de-
scribed in section 3.2).
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, di ? D?, dj ? D?}?
i,j 1{Ci,j = 1, di ? D?, dj ? D?}
,
(1)
where D? is a subset with size ?|D| sampled from
full data set D, C and C? are |D|? |D| connectivity
matrixes based on clustering solutions computed on
D and D? respectively, and 0 ? ? ? 1. The con-
nectivity matrix C is defined as: Ci,j = 1 if di and
dj belong to the same cluster, otherwise Ci,j = 0.
C? is calculated in the same way. ? is set as 0.90 in
this paper.
178
Table 2: The cluster validation method for evalua-
tion of values of sense number k.
Function: Cluster Validation(k, D, q)
Input: cluster number k, data set D,
and sampling frequency q;
Output: the score of the merit of k;
1 Perform clustering analysis using sIB on
data set D with k as input;
2 Construct connectivity matrix Ck based on
above clustering solution on D;
3 Use a random predictor ?k to assign
uniformly drawn labels to instances in D;
4 Construct connectivity matrix C?k
using above clustering solution on D;
5 For ? = 1 to q do
5.1 Randomly sample a subset (D?) with size
?|D| from D, 0 ? ? ? 1;
5.2 Perform clustering analysis using sIB on
(D?) with k as input;
5.3 Construct connectivity matrix C?k using
above clustering solution on (D?);
5.4 Use ?k to assign uniformly drawn labels
to instances in (D?);
5.5 Construct connectivity matrix C??kusing above clustering solution on (D?);
Endfor
6 Evaluate the merit of k using following
objective function:
Mk = 1q
?
? M(C?k , Ck) ? 1q
?
? M(C??k , C?k),
where M(C?, C) is given by equation (1);
7 Return Mk;
M(C?, C) measures the proportion of document
pairs in each cluster computed on D that are also as-
signed into the same cluster by clustering solution
on D?. Clearly, 0 ? M ? 1. Intuitively, if clus-
ter number k is identical with the true value, then
clustering results on different subsets generated by
sampling should be similar with that on full data set,
which gives rise to a local optimum of M(C?, C).
In our algorithm, we normalize M(C?F,k, CF,k)using the equation in step 6 of Table 2, which
makes our objective function different from the fig-
ure of merit (equation ( 1)) proposed in (Levine
and Domany, 2001). The reason to normalize
M(C?F,k, CF,k) is that M(C?F,k, CF,k) tends to de-
crease when increasing the value of k. Therefore for
avoiding the bias that smaller value of k is to be se-
lected as cluster number, we use the cluster validity
of a random predictor to normalize M(C?F,k, CF,k).
3.2 The sIB Clustering Algorithm
Here we used the sIB algorithm (Slonim, et al,
2002) to estimate cluster structure, which measures
the similarity of contexts of instances according to
the similarity of their feature conditional distribu-
tion. sIB is a simplified ?hard? variant of informa-
tion bottleneck method (Tishby, et al, 1999).
Let d represent a document, and w represent a fea-
ture word, d ? D, w ? F . Given the joint distri-
bution p(d,w), the document clustering problem is
formulated as looking for a compact representation
T for D, which preserves as much information as
possible about F . T is the document clustering so-
lution. For solving this optimization problem, sIB
algorithm was proposed in (Slonim, et al, 2002),
which found a local maximum of I(T, F ) by: given
an initial partition T , iteratively drawing a d ? D
out of its cluster t(d), t ? T , and merging it into
tnew such that tnew = argmaxt?Td(d, t). d(d, t) is
the change of I(T, F ) due to merging d into cluster
tnew, which is given by
d(d, t) = (p(d) + p(t))JS(p(w|d), p(w|t)). (2)
JS(p, q) is the Jensen-Shannon divergence, which
is defined as
JS(p, q) = pipDKL(p?p) + piqDKL(q?p), (3)
DKL(p?p) =
?
y
plog pp, (4)
DKL(q?p) =
?
y
qlog qp, (5)
{p, q} ? {p(w|d), p(w|t)}, (6)
{pip, piq} ? {
p(d)
p(d) + p(t) ,
p(t)
p(d) + p(t)}, (7)
p = pipp(w|d) + piqp(w|t). (8)
179
4 The Label Propagation Algorithm for
Task 5 and Task 17
In the label propagation algorithm (LP) (Zhu and
Ghahramani, 2002), label information of any ver-
tex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage
is achieved. Larger edge weights allow labels to
travel through easier. Thus the closer the examples,
more likely they have similar labels (the global con-
sistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
For task 5 and 17, we constructed connected
graphs as follows: two instances u, v will be con-
nected by an edge if u is among v?s k nearest neigh-
bors, or if v is among u?s k nearest neighbors as mea-
sured by cosine or JS distance measure. k is set 10
in our system implementation.
5 Data Sets of Task 2, Task 5 and Task 17
The test data for task 2 includes totally 27132 un-
tagged instances for 100 ambiguous English words.
There is no training data for task 2.
There are 40 ambiguous Chinese words in task
5. The training data for this task consists of 2686
instances, while the test data includes 935 instances.
There are 100 ambiguous English words in the
first subtask of task 17. The training data for this
task consists of 22281 instances, while the test data
includes 4851 instances.
6 Experimental Results of Our Systems at
Task 2, Task 5 and Task 17
Table 3: The best/worst/average F-score of all the
systems at task 2 and the F-score of our system at
task 2 for all target words, nouns and verbs with un-
supervised evaluation.
All words Nouns Verbs
Best 78.7% 80.8% 76.3%
Worst 56.1% 65.8% 45.1%
Average 65.4% 69.0% 61.4%
Our system 63.9% 68.0% 59.3%
Table 3 lists the best/worst/average F-score of all
the systems at task 2 and the F-score of our system
at task 2 for all target words, nouns and verbs with
180
Table 4: The best/worst/average supervised recall of
all the systems at task 2 and the supervised recall of
our system at task 2 for all target words, nouns and
verbs with supervised evaluation.
All words Nouns Verbs
Best 81.6% 86.8% 75.7%
Worst 78.5% 81.4% 75.2%
Average 79.6% 83.0% 75.7%
Our system 81.6% 86.8% 75.7%
Table 5: The best/worst/average micro-average pre-
cision and macro-average precision of all the sys-
tems at task 5 and the micro-average precision and
macro-average precision of our system at task 5.
Micro-average Macro-average
Best 71.7% 74.9%
Worst 33.7% 39.6%
Average 58.5% 62.7%
Our system 71.2% 74.7%
unsupervised evaluation. Our system obtained the
fourth place among six systems with unsupervised
evaluation. Table 4 shows the best/worst/average
supervised recall of all the systems at task 2 and the
supervised recall of our system at task 2 for all tar-
get words, nouns and verbs with supervised evalu-
ation. Our system is ranked as the first among six
systems with supervised evaluation. Table 7 lists
the estimated sense numbers by our system for all
the words at task 2. The average of all the estimated
sense numbers is 3.1, while the average of all the
ground-truth sense numbers is 3.6 if we consider the
sense inventories provided in task 17 as the answer.
It seems that our estimated sense numbers are close
to the ground-truth ones.
Table 5 provides the best/worst/average micro-
average precision and macro-average precision of all
the systems at task 5 and the micro-average preci-
sion and macro-average precision of our system at
task 5. Our system obtained the second place among
six systems for task 5.
Table 6 shows the best/worst/average coarse-
grained score (precision) of all the systems the lexi-
cal sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sample
Table 6: The best/worst/average coarse-grained
score (precision) of all the systems at the lexical
sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sam-
ple subtask of task 17.
Coarse-grained score (precision)
Best 88.7%
Worst 52.1%
Average 70.0%
Our system 86.4%
subtask of task 17. The attempted rate of all the sys-
tems is 100%. So the precision value is equal to the
recall value for all the systems. Here we listed only
the precision for the 13 systems at this subtask. Our
system is ranked as the third one among 13 systems.
7 Conclusion
In this paper, we described the implementation of
our I2R systems that participated in task 2, task 5,
and task 17 at SemEval-2007. Our systems achieved
63.9% F-score and 81.6% supervised recall for task
2, 71.2% micro-average precision and 74.7% macro-
average precision for task 5, and 86.4% coarse-
grained precision and recall for the lexical sample
subtask of task 17. The performance of our system
is very good under supervised evaluation. It may
be explained by that our system has the ability to
find some minor senses so that it can outperforms
the baseline system that always uses the most fre-
quent sense as the answer.
References
Agirre E. , & Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimination
Systems. Proceedings of SemEval-2007, Association
for Computational Linguistics.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002.
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Lee, Y.K., & Ng, H.T. 2002. An Empirical Evalua-
tion of Knowledge Sources and Learning Algorithms
for Word Sense Disambiguation. Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, (pp. 41-48).
181
Levine, E., & Domany, E. 2001. Resampling Method for
Unsupervised Estimation of Cluster Validity. Neural
Computation, Vol. 13, 2573?2593.
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics.
Slonim, N., Friedman, N., & Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Tishby, N., Pereira, F., & Bialek, W. (1999) The Infor-
mation Bottleneck Method. Proc. of the 37th Allerton
Conference on Communication, Control and Comput-
ing.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Table 7: The estimated sense numbers by our system
for all the words at task 2.
explain 2 move 3
position 3 express 4
buy 2 begin 2
hope 3 prepare 3
feel 5 policy 2
hold 2 attempt 2
work 5 recall 3
people 4 find 2
system 2 join 2
bill 2 build 2
hour 5 base 3
value 4 management 2
job 5 turn 4
rush 2 kill 2
ask 2 area 5
approve 4 affect 4
capital 4 keep 5
purchase 2 improve 2
propose 2 do 2
see 3 drug 5
president 3 come 5
power 3 disclose 4
effect 2 avoid 3
part 5 plant 2
exchange 4 share 2
state 2 carrier 2
care 5 complete 2
promise 3 maintain 3
estimate 2 development 4
rate 2 space 5
say 2 raise 3
remove 5 future 3
grant 4 network 3
remember 3 announce 5
cause 2 start 3
point 5 order 2
occur 4 defense 5
authority 3 set 3
regard 2 chance 2
go 3 produce 2
allow 4 negotiate 2
describe 2 enjoy 4
prove 3 exist 4
claim 4 replace 3
fix 2 examine 3
end 5 lead 3
receive 3 source 2
complain 3 report 2
need 2 believe 2
condition 2 contribute 3
182
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183?191,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Text Classification by a Sense Spectrum Approach to Term
Expansion
Peter Wittek
Department of Computer Science
National University of Singapore
Computing 1, Law Link
Singapore 117590
wittek@comp.nus.edu.sg
Sa?ndor Dara?nyi
Swedish School of Library
and Information Science
Go?teborg University &
University of Bora?s
Alle?gatan 1
50190 Bora?s, Sweden
sandor.daranyi@hb.se
Chew Lim Tan
Department of Computer Science
National University of Singapore
Computing 1, Law Link
Singapore 117590
tancl@comp.nus.edu.sg
Abstract
Experimenting with different mathematical
objects for text representation is an important
step of building text classification models. In
order to be efficient, such objects of a for-
mal model, like vectors, have to reasonably re-
produce language-related phenomena such as
word meaning inherent in index terms. We in-
troduce an algorithm for sense-based seman-
tic ordering of index terms which approxi-
mates Cruse?s description of a sense spectrum.
Following semantic ordering, text classifica-
tion by support vector machines can benefit
from semantic smoothing kernels that regard
semantic relations among index terms while
computing document similarity. Adding ex-
pansion terms to the vector representation can
also improve effectiveness. This paper pro-
poses a new kernel which discounts less im-
portant expansion terms based on lexical re-
latedness.
1 Introduction
Generally, building an automated text classification
system consists of two key subtasks. The first task
is text representation which converts the content of
documents into compact format so that they can be
further processed by the text classifiers. Another
task is to learn the model of a text classifier which
is used to classify the unlabeled documents. This
paper proposes a substantially new model for text
representation to improve effectiveness of text clas-
sification by semantic ordering.
Our motivation for the research presented here
came from (Dorrer et al, 2001) who demonstrated
the viability of database searching by visible light
using a quantum algorithm, albeit on meaningless
items. The question was, what kind of document
representation would be necessary to extend their
in-principle results to include semantics, one that
has been leading us to test both periodic and non-
periodic functions for this purpose. Since represen-
tation and retrieval by colors was implied in their
method, we speculated that the following compo-
nents could be useful in a rephrased model: (a)
a metaphorically presented spectral expression of
lexical semantic phenomena, (b) a ranked one-
dimensional condensate of multidimensional sense
structure, and (c) representation of documents and
queries by functions in L2 space with a similarity
measure. Our anticipation was that by matching
these components, a new model could demonstrate
new capacities in general, and contribute to comput-
ing meaning by waves in particular.
Semantic ordering (component b) is an approxi-
mation of what (Cruse, 1986) referred to as a sense
spectrum, i.e. a series of points - called local senses
and constituting lexical units -, in a one-dimensional
semantic continuum (component a). Apart from dif-
ferentiating between the conceptual content of the
same word in terms of its senses in word pairs, i.e.
their semantic relatedness, it also compresses the
result in spectral form. The scalar values of this
spectrum have the double potential of being a con-
densed measure for semantic weighting, and, ten-
tatively, they can play the role of mass in experi-
ments where gravity is called in as a metaphor for
text categorization and information retrieval (Paij-
mans, 1997; Shi et al, 2005; Wittek et al, 2009).
183
This paper addresses text categorization by means
of non-periodical functions only.
In support of Cruse?s point, recently it has been
demonstrated by measurements that sense classifi-
cation errors made by their maximum entropy based
word sense disambiguation system were partly
remedied once instead of a fine-grained view, a more
coarse-grained view of senses was adopted (Palmer
et al, 2006). Improvement of sense classification ac-
curacy linked with ?zooming out? in terms of obser-
vation granularity indicates, in our eyes, the ?fluid?,
perhaps spectral nature of sense inasmuch as it is
impossible to precisely distinguish between the bor-
derlines and some fuzziness is implied both in the
phenomenon and its perception. This ?fluidity of
language?, as Palmer et al call it, is in accord
with the theory of shared semantic representations in
psycholinguistics (Rodd et al, 2002), according to
which related senses share a portion of their mean-
ing representation in the mental lexicon; it also sup-
ports an earlier observation of two of the present au-
thors based on the same methodology as outlined in
this paper, namely that using continuous functions
for information retrieval leads to content representa-
tion without exact term or document locations, one
which is regional in its nature and subject to a math-
ematical uncertainty principle (Wittek and Dara?nyi,
2007).
We approach our problem in three steps: (1)
whether distributional semantics alone is enough for
the representation of word meaning, (2) whether se-
mantic relatedness between word pairs can be ex-
pressed in an ordered form while preserving lexical
field structure, and if (3) the uniqueness of entries in
such an order can be expressed by functions rather
than scalars such as distance. As we will show, this
line of thought leads to performance improvement
in text classification by using kernel-based feature
weighting.
Since the early days of the vector space model,
it has been debated whether it is a proper carrier
of meaning of texts (Raghavan and Wong, 1986),
arguing if distributional similarity is an adequate
proxy for lexical semantic relatedness (Budanitsky
and Hirst, 2006). We argue for the need to enrich
distributional semantics-based text representation by
other components because with the statistical, i.e.
devoid of word semantics approaches there is gen-
erally no way to improve both precision and recall
at the same time, increasing one is done at the ex-
pense of the other. For example, casting a wider net
of search terms to improve recall of relevant items
will also bring in an even greater proportion of ir-
relevant items, lowering precision. In the mean-
time, practical approaches have been proliferating,
especially with developments in kernel methods in
the last decade (Joachims, 1998; Cristianini et al,
2002). Some researchers suggested a more general
mathematical framework to accommodate the needs
that the vector space model cannot satisfy (van Rijs-
bergen, 2004). This paper explores the opportunities
of this representation in the domain of text classifi-
cation by introducing it as a new nonlinear semantic
kernel.
Another aspect of the same problem is term ex-
pansion for document classification and retrieval.
By automatically selecting expansion terms for a
text classification system to expand a document vec-
tor by adding terms that are related to the terms
already in the document, performance can be im-
proved (Hu et al, 2008). Such new terms can ei-
ther be statistically related to the original terms or
chosen from lexical resources such as thesauri, con-
trolled vocabularies, ontologies and the like.
However, in doing so the fundamental question
often overlooked is whether the expansion terms ex-
tracted are equally related to the document and are
useful for text classification. In what follows we
propose a form of term expansion with decreasing
importance of those terms that are less related, as
contrasted with rigid term expansion. This can be
carried out by a combination of semantic ordering
and using function space for classification.
This paper is organized as follows. Section 2
overviews text classification by support vector ma-
chines, expanding on traditional text similarity mea-
sures (Section 2.1), semantic smoothing kernels
(Section 2.2), term expansion strategies (Section
2.3), and finally introduces our semantic kernels in
the L2 space (Section 2.4). Section 3 discusses ex-
perimental results and Section 4 concludes the pa-
per.
184
2 Text Classification with Support Vector
Machines
Text categorization is the task of assigning unlabeled
documents into predefined categories. Given a col-
lection of {d1, d2, . . . , dN} documents, and a C =
{c1, c2, ..., c|C|} set of predefined categories, the
task is, for each document dj (j ? {1, 2, . . . , N}),
to assign a decision to file dj under ci or a deci-
sion not to file dj under ci (ci ? C) by virtue of
a function ?, where the function ? is also referred
to as the classifier, or model, or hypothesis, or rule.
Supervised text classification is a machine learning
technique for creating the function ? from training
data. The training data consist of pairs of input doc-
uments, and desired outputs (i.e., classes).
Support vector machines have been found the
most effective by several authors (Joachims, 1998).
The proposed semantic text classification method is
grounded in the kernel methods underlying support
vector machines.
A support vector machine is a kind of supervised
learning algorithm. In its simplest, linear form, a
support vector machine is a hyperplane that sepa-
rates a set of positive examples from a set of nega-
tive examples with maximum margin (Shawe-Taylor
and Cristianini, 2004). The strength of kernel meth-
ods is that they allow a mapping ?(.) of x to a higher
dimensional space. In the dual formulation of the
mathematical programming problem, only the ker-
nel matrix K(xi,xj) = ?(xi)??(xi) is needed in
the calculations.
2.1 Traditional Text Similarity Measure
Intuitively, if a text fragment of two documents ad-
dress similar topics, it is highly possible that they
share lots of substantive terms. After having re-
moved the stopwords and stemmed the rest, the
stemmed terms construct a vector representation for
each text document. Let aj be a document vector in
the vector space model, that is, aj = ?Mk=1 akjek,
where M is the number of index terms, akj is some
weighting (e.g., term frequency), and ek is a basis
vector of the M -dimensional Euclidean space. This
representation is also referred to as the bag-of-words
(BOW) model.
Given this representation, semantic relatedness of
a pair of text fragments is computed as the cosine
similarity of their corresponding term vectors which
is defined as:
S(ai,aj) = aiaj|ai||a|j . (1)
2.2 Linear Semantic Kernels
One enrichment strategy is to use a semantic
smoothing kernel while calculating the similarity
between two documents. Any linear kernel for texts
is characterized by K(ai,aj) = a?iS?Saj , where
S is an appropriately shaped matrix commonly re-
ferred to as semantic smoothing matrix (Siolas and
d?Alche? Buc, 2000; Shawe-Taylor and Cristianini,
2004; Basili et al, 2005; Mavroeidis et al, 2005;
Bloehdorn et al, 2006). The presence of S changes
the orthogonality of the vector space model, as this
mapping should introduce term dependence. A re-
cent attempt tried to manually construct S with the
help of a lexical resource (Siolas and d?Alche? Buc,
2000). The entries in the symmetric matrix S ex-
press the semantic similarity between the terms i and
j. Entries in this matrix are inversely proportional
to the length of the WordNet hierarchy path linking
the two terms. The performance, measured over the
20NewsGroups corpus, showed an improvement of
2 % over the the basic vector space method. More-
over, the semantic matrix S is almost fully dense,
hence computational issues arise. In a similar con-
struction, (Bloehdorn et al, 2006) defined the ma-
trix entries as weights of superconcepts of the two
terms in the WordNet hierarchy. Focusing on special
subcategories of Reuters-21578 and on the TREC
Question Answering Dataset, they showed consis-
tent improvement over the baseline. As (Mavroei-
dis et al, 2005) pointed out, polysemy will remain
a problem in semantic smoothing kernels. A more
complex way of calculating the semantic similarity
as the matrix entries was also proposed (Basili et al,
2005). For a more general discussion on semantic
similarity see Section 2.4.1.
An early attempt to overcome the untenable or-
thogonality assumption of the vector space model
was proposed under the name of generalized vec-
tor space model (Wong et al, 1985). The article
which proposed the model did not provide empiri-
cal results, and since then the model has been re-
garded of large theoretical importance with less im-
pact on actual applications. The model takes a distri-
185
butional approach, focusing on term co-occurrences.
The underlying assumption is that term correlations
are captured by the co-occurrence information. That
is, two terms are semantically related if they co-
occur often in the same documents. By eliminat-
ing orthogonality, documents can be seen as similar
even if they do not share any terms. The term co-
occurrence matrix is AA?, hence the model takes A?
as the semantic similarity matrix S. A major draw-
back of the generalized vector space model is that it
replaces the orthogonality assumption with another
questionable assumption. The computational needs
are tremendous too, if the dimensions of A are con-
sidered. Moreover, the co-occurrence matrix is not
sparse anymore.
Latent semantic indexing (or latent semantic anal-
ysis) was another attempt to bring more linguis-
tic and psychological aspects to language process-
ing via a kernel. Conceptually, latent semantic in-
dexing is similar to the generalized vector space
model, it measures semantic information through
co-occurrence analysis in the corpus. From the al-
gorithmic perspective it is an enormous problem that
textual data have a large number of relevant fea-
tures. This results in huge computational needs and
the classification models may overfit the data. The
number of features can be reduced by multivariate
feature extraction methods. In latent semantic in-
dexing, the dimension of the vector space is reduced
by singular value decomposition (Deerwester et al,
1990).
Using rank reduction, terms that occur together
very often in the same documents are merged into
a single dimension of the feature space. The di-
mensions of the reduced space correspond to the
axes of greatest variance. For latent semantic in-
dexing, by dual representation the kernel matrix is
K = V ?2kV ?, where ?k is a diagonal matrix con-
taining the k largest singular values of the singu-
lar value decomposition of the vector space, and V
holds the right singular vectors of the decomposi-
tion. The new kernel matrix can be obtained directly
from K by applying an eigenvalue decomposition
of K (Cristianini et al, 2002). The computational
complexity of performing an eigenvalue decompo-
sition on the kernel matrix is a major drawback of
latent semantic indexing.
2.3 Text Representation Enrichment Strategies
by Term Expansion
In order to eliminate the bottleneck of the traditional
BOW representation, previous approaches in term
expansion enriched this convention by external lexi-
cal resources such as WordNet.
As a first step, these methods generate new fea-
tures for each document in the dataset. These new
features can be synonyms or homonyms of docu-
ment terms as in (Hotho et al, 2003; Rodriguez
and Hidalgo, 1997), or expanded features for terms,
sentences and documents as in (Gabrilovich and
Markovitch, 2005), or term context information for
word sense disambiguation such as topic signatures
(Agirre and De Lacalle, 2003; Agirre et al, 2004).
Then, the generated new features replace the old
ones or are appended to the document representa-
tion, and construct a new vector representation a?i
for each text document. The similarity measure of
document pairs is defined as:
S(a?i, a?j) = a?ia?j|a?i||a?j | . (2)
2.4 Our Framework
The basic assumption of our framework is that terms
can be arranged in an order such that consecutive
terms are semantically related. Hence each term ac-
quires a unique position, and this position ties the
term to its semantically related neighbors. However,
given a BOW representation with a cosine similarity
measure, this position would not improve classifica-
tion performance. Therefore we suggest to associate
a mathematical function with each term, thus map-
ping terms and documents to theL2 space, and using
the inner product of this space to express similar-
ity. The choice of function will determine to which
extent neighboring terms, i.e., the enriching terms,
are considered in calculating the similarity between
two documents. This section first introduces an al-
gorithm that produces the aforementioned semantic
order, then the semantic kernels in the L2 space are
discussed.
2.4.1 An Algorithm for a Semantic Ordering of
Terms
The proposed kernels assume that there is a se-
mantic order between terms. Let V denote a set of
186
terms {t1, t2, . . . , tn} and let d(ti, tj) denote the se-
mantic distance between the terms ti and tj . The
initial order of the terms is not relevant, though it is
assumed to be alphabetic. Let G = (V,E) denote
a weighted undirected graph, where the weights in
the set E are defined by the distances between the
terms.
Various lexical resource-based (Budanitsky and
Hirst, 2006) and distributional measures (Moham-
mad and Hirst, 2005) have been proposed to mea-
sure semantic relatedness and distance between
terms. Terms can be corpus- or genre-specific. Man-
ually constructed general-purpose lexical resources
include many usages that are infrequent in a partic-
ular corpus or genre of documents. For example,
one of the 8 senses of company in WordNet is a
visitor/visitant, which is a hyponym of person (Lin,
1998). This sense of the term is practically never
used in newspaper articles, hence distributional at-
tributes should be taken into consideration. Com-
posite measures that combine the advantages of both
approaches have also been developed (Resnik, 1995;
Jiang and Conrath, 1997). This paper relies on the
Jiang-Conrath composite measure (Jiang and Con-
rath, 1997), which has been shown to be superior
to other measures (Budanitsky and Hirst, 2006), and
we also found that this measure works the best for
the purpose. The Jiang-Conrath metric measures
the distance between two senses by using the hier-
archy of WordNet. By denoting the lowest super-
ordinate of two senses s1 and s2 in the hierarchy
with LSuper(s1,s2), the metric is calculated as fol-
lows:
d(s1, s2) = IC(s1)+IC(s2)?2IC(LSuper(s1, s2)),
where IC(s) is the information content of a sense
s based on a corpus. Distance between two terms
is calculated according to the following equation:
d(t1, t2) = maxs1?sen(t1),s2?sen(t2) d(s1, s2), where
t1 and t2 are two terms, and sen(ti) is the set of
senses of ti. The distance between two terms is
usually defined as the minimum of the sense dis-
tances. We chose maximum because it ensures that
only closely related terms will be placed to adjacent
positions by the algorithm below.
Finding a semantic ordering of terms can be trans-
lated to a graph problem: a minimum-weight Hamil-
tonian path G? of G gives the ordering by reading
the nodes from one of the paths to the other. G is
a complete graph, therefore such a path always ex-
ists, but finding it is an NP-complete problem. The
following greedy algorithm is similar to the nearest
neighbor heuristic for the solution of the traveling
salesman problem. It creates a graph G? = (V ?, E?),
where V ? = V and E? ? E. This G? graph is a
spanning tree of G in which the maximum degree of
a node is two, that is, the minimum spanning tree is
a path between two nodes.
Step 1 Find the term at the highest stage of the hi-
erarchy in a lexical resource.
ts = argminti?V depth(ti).
This seed term is the first element of V ?, V ? =
{ts}. Remove it from the set V :
V := V \{ts}.
Using WordNet, this seed term is entity, if the
vocabulary of the text collection contains it.
Step 2 Let tl denote the leftmost term of the order-
ing and tr the rightmost one. Find the next two
elements of the ordering:
t?l = argminti?V d(ti, tl),
t?r = argminti?V \{t?l}d(ti, tr).
Step 3 If d(tl, t?l) < d(tr, t?r) then add t?l to V ?,
E? := E? ? {e(tl, t?l)}, and V := V \{t?l}.
Else add t?r to V ?, E? := E? ? {e(tr, t?r)} and
V := V \{t?r}.
Step 4 Repeat from Step 2 until V = ?.
The above algorithm can be thought of as a modi-
fied Prim?s algorithm, but it does not find the optimal
minimum-weight spanning tree.
2.4.2 Semantic Kernels in the L2 Space
The L2 space shares resemblance with a real
vector space. Real-valued vectors are replaced by
square-integrable functions, and the dot product is
replaced by the following inner product: (fi, fj) =?
fifjdx, for some fi, fj in the given L2 space.
Lately, Hoenkamp has also pointed out that the
L2 space can be used for information retrieval when
187
he introduced a Haar basis for the document space
(Hoenkamp, 2003). He utilized a signal processing
framework within the context of latent semantic in-
dexing. In order to apply an L2 representation for
text classification, the problem is approached from a
different angle than by Hoenkamp, taking discount-
ing expansion terms as our point of departure.
Assigning a function w(x ? k) to the term in the
kth position in a semantic order, a document j can
be expressed as follows:
fj(x) =
M?
k=1
akjw(x? k), (3)
where x is in [1,M ], and it is the variable of inte-
gration in calculating the inner product of the L2;
x can be regarded as a ?dummy? variable carrying
no meaning in itself. The above formula will be re-
ferred to as a document function. In the experiments,
the function exp(?bx2) was used as w(x), with b as
a free parameter reflecting the width of the function
expressing how many neighboring expansion terms
are considered.
The inner product of theL2[1,M ] space is applied
to express similarity between two documents in sim-
ilar vein as the dot product does in a real-valued vec-
tor space:
(fi, fj) =
?
[1,M ]
fi(x)fj(x)dx, (4)
where fi and fj are the representations of the docu-
ments in the L2 space (fi, fj ? L2([1,M ])).
0.5
1
1.5
2
bran
d bran
d
name trade name
Figure 1: Two documents with matching term brand
name. Dotted line: Document-1. Dashed line:
Document-2. Solid line: Their product.
With the above formula, a matching term in two
documents will be counted to its full term frequency
or tfidf score, while semantically related terms will
be counted less and less according their semantic
similarity to the matching term. Assuming that the
terms brand, brand name, and trade name follow
each other in the semantic order, consider the fol-
lowing example. The first document has the term
brand name, and so does the second document. In
Figure 1, it can be seen brand name is counted the
same way as it would be in a BOW model with its
full term frequency score, brand and trade name are
counted to a lesser extent, while other related terms
are considered even less.
0.5
1
1.5
2
bran
d bran
d
name trade name
Figure 2: Two documents with no matching term but
with related terms brand and trade name. Dotted line:
Document-1. Dashed line: Document-2. Solid line:
Their product.
Now if the two documents do not share the exact
term, only related terms occur, for instance, trade
name and brand, respectively, then the term brand
name, placed between trade name and brand in the
s semantic order, will be considered only to some
extent for the calculation of similarity (see Figure
2).
3 Experimental Results
The most widely used benchmark corpus is the
Reuters-21578 collection. For benchmarking pur-
poses, the ModApte split was adopted. 9603 doc-
uments were used as the training set and 3299 as the
test set in the experiments. Only those ninety text
categories which had at least one positive example
in the training set were included in the benchmark.
Another benchmark data corpus we used was the 20
188
Newsgroups corpus, which is a collection of approx-
imate 20,000 newsgroup documents nearly evenly
divided among 20 discussion groups and each doc-
ument is labeled as one of the 20 categories corre-
sponding to the name of the newsgroup that the doc-
ument was posted to.
In preparing the index terms, we restricted the vo-
cabulary to the terms of WordNet 3.0 in order to be
able to calculate the similarity score between any
two terms. Stop words were removed in advance.
Multiple word expressions were used to fully utilize
WordNet. We used the built-in stemmer of WordNet,
which is able to distinguish between different parts-
of-speeches if the form of the word is unambiguous.
For example, {accommodates, accommodated, ac-
commodation} was stemmed to {accommodate, ac-
commodate, accommodation}. We used term fre-
quency as term weighting.
Prior to the semantic ordering, terms were as-
sumed to be in alphabetic order. Measuring the
Jiang-Conrath distance between adjacent terms, the
average distance was 1.68 on the Reuters corpus.
Note that the Jiang-Conrath distance was normal-
ized to the interval [0, 2]. There were few terms with
zero or little distance between them. This is due to
terms which are related and start with the same word
or stem. For example, account, account executive,
account for, accountable.
The same average distance after reordering the
terms with the proposed algorithm and the Jiang-
Conrath distance was 0.56 on the same corpus.
About one third of the terms had very little distance
between each other. Nevertheless, over 10 % of the
total terms still had the maximum distance. This is
due to the non-optimal nature of the proposed term-
ordering algorithm. These terms add noise to the
classification. The noisy terms occur typically at the
two sides of the scale, that is, the leftmost terms and
the rightmost terms. While it is easy to find close
terms in the beginning, as the algorithm proceeds,
fewer terms remain in the pool to be chosen. For in-
stance, brand, brand name, trade name, label are in
the 33rd, 34th, 35th and 36th position on the left side
counting from the seed respectively, while windy,
widespread, willingly, whatsoever, worried, worth-
while close the left side, apparently sharing little in
common. The noise can be reduced by the appropri-
ate choice of the parameter b in exp(?bx2), so that
Kernel Reuters Reuters 20News 20News
Micro Macro Micro Macro
Linear 0.900 0.826 0.801 0.791
Poly 0.903 0.824 0.796 0.788
L2 0.911 0.835 0.813 0.799
Table 1: Micro- and macro-average F1 results
the impact of adjacent but distantly related terms can
be minimized.
Table 1 shows the results on the two benchmark
corpora with the baseline linear kernel. Precision
and recall with regard to a class ck, the F1 score
shown is their average. For all the kernels, the results
with the best parameter settings are shown. Polyno-
mial kernels were benchmarked between degrees 2
and 5. L2 kernels were benchmarked with width b
between 1 and 8, the performance peaking at 4 in
all cases. The model is able to outperform the base-
line kernels, and the differences in micro-averaged
results are statistically significant. In all cases of the
L2 kernel, the increase of F1 was due the increase in
both precision and recall.
4 Conclusions
Information systems are in great need of automated
intelligent tools, but existing algorithms and meth-
ods cannot be pushed much further. Most tech-
niques in current use are impaired by the semanti-
cally poor but widespread representation of informa-
tion and knowledge. For this reason, we propose a
new formalism that combines Cruse?s idea about a
sense spectrum, approximated by semantic ordering,
and its calculation by functions.
The suggested model combines term expansion
with the semantic relations and semantic relatedness
used in semantic smoothing kernels. This slightly
unusual approach needs to transform the real vector
representation to the L2 space, and the experimen-
tal results show that this new representation can im-
prove text classification effectiveness.
Our new model also blends insights from differ-
ent approaches to lexical semantics theory at its dif-
ferent levels. First, during the semantic ordering
of terms the distributional hypothesis meets hand-
crafted lexical resources of word meaning that relate
to term occurrences as if they were their referents,
189
a component external to term context. While high-
quality lexical resources enable such an ordering in
themselves, the procedure can benefit from data de-
rived from the specific corpora in study ? seman-
tic relatedness measures such as the Jiang-Conrath
similarity operate this way. Secondly, once the or-
dering is done and a sense spectrum is constructed,
weights expressing statistical relationships between
terms and documents are borrowed from the vector
space model to form the basis for constructing hypo-
thetical signals of content, documents as continuous
functions.
5 Future research
Figure 3: A hypothetical spectrum of terms.
As we have shown, a spectral interpretation of
sense granularity can lead to improved text catego-
rization results by utilizing L2 space for informa-
tion representation. Whether non-periodic functions
other than the variant tested in this paper can be ap-
plied to the same end needs to be explored.
Turning back to the use of the spectrum of visi-
ble light for representing meaning, this raises new
research questions. On the one hand, translat-
ing one-dimensional semantic ordering into colors
is straightforward. Consider the following map-
ping. Assume that a language has a finite N num-
ber of terms, so the 1-dimensional result is an or-
dered list o1, o2, . . . , oN . Calculate the following:
? = ?N?1i=1 d(oi, oi+1), where ? is the sum of
distances between consecutive words. Further let
F : [0,?] ? [400, 700] be the following map-
ping: F (x) = 400 + x300? . The visible spectrumis between 400 and 700 nm, F maps the cumulative
distances of terms from [0,?] to the visible spec-
trum congruently, i.e. without distorting the dis-
tances. With this bijective (one-to-one) mapping,
each term is assigned a physical wavelength and fre-
quency. Figure 3 shows an example of such a term
spectrum.
On the other hand, we have only begun to test the
applicability of periodic functions in L2 space (Wit-
tek and Dara?nyi, 2007), hence a well-established
link to semantic computing by waves is missing for
the time being. A possible compromise between the
non-periodic vs. periodic approaches can be to ap-
ply wavelets instead of waves, a direction where our
ongoing research shows promising results. These
will be reported elsewhere. In a broader frame of
thought, we are also working on the optical equiv-
alents of the vector space model and the general-
ized vector space model as a first step toward coding
more semantics in mathematical objects, and putting
them to work in novel computing environments.
6 Acknowledgments
The authors are grateful to Martha Palmer (Univer-
sity of Colorado Boulder) for her inspiring sugges-
tions and advice on sense granularity.
References
E. Agirre and O.L. De Lacalle. 2003. Clustering Word-
Net word senses. In Proceedings of RANLP-03, 4th
International Conference on Recent Advances in Nat-
ural Language Processing, pages 121?130.
E. Agirre, E. Alfonseca, and O.L. de Lacalle. 2004. Ap-
proximating hierarchy-based similarity for WordNet
nominal synsets using topic signatures. In Proceed-
ings of GWC-04, 2nd Global WordNet Conference,
pages 15?22.
R. Basili, M. Cammisa, and A. Moschitti. 2005. Effec-
tive use of WordNet semantics via kernel-based learn-
ing. In Proceedings of CoNLL-05, 9th Conference on
Computational Natural Language Learning, pages 1?
8.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. Proceed-
ings of ICDM-06, 6th IEEE International Conference
on Data Mining.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13?47.
190
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18(2):127?152.
D.A. Cruse. 1986. Lexical semantics.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
C. Dorrer, P. Londero, M. Anderson, S. Wallentowitz, and
IA Walmsley. 2001. Computing with interference:
all-optical single-query 50-element database search.
In Proceedings of QELS-01, Quantum Electronics and
Laser Science Conference, pages 149?150.
E. Gabrilovich and S. Markovitch. 2005. Feature gen-
eration for text categorization using world knowledge.
In Proceedings of IJCAI-05, 19th International Joint
Conference on Artificial Intelligence, volume 19.
E. Hoenkamp. 2003. Unitary operators on the document
space. Journal of the American Society for Informa-
tion Science and Technology, 54(4):314?320.
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet
improves text document clustering. In Proceedings of
SIGIR-03, 26th ACM International Conference on Re-
search and Development in Information Retrieval.
J. Hu, L. Fang, Y. Cao, H.J. Zeng, H. Li, Q. Yang, and
Z. Chen. 2008. Enhancing text clustering by lever-
aging Wikipedia semantics. In Proceedings of SIGIR-
08, 31st ACM International Conference on Research
and Development in Information Retrieval, pages 179?
186.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, pages 19?33.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of ECML-98, 10th European Confer-
ence on Machine Learning, pages 137?142.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL Workshop
on Usage of WordNet in Natural Language Processing
Systems, volume 98, pages 768?773.
D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,
M. Theobald, and G. Weikum. 2005. Word sense
disambiguation for exploiting hierarchical thesauri
in text classification. Proceedings of PKDD-05,
9th European Conference on the Principles of Data
Mining and Knowledge Discovery, pages 181?192.
S. Mohammad and G. Hirst. 2005. Distributional mea-
sures as proxies for semantic relatedness.
H. Paijmans. 1997. Gravity wells of meaning: detecting
information-rich passages in scientific texts. Journal
of Documentation, 53(5):520?536.
M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language
Engineering, 13(02):137?163.
V.V. Raghavan and S.K.M. Wong. 1986. A critical anal-
ysis of vector space model for information retrieval.
Journal of the American Society for Information Sci-
ence, 37(5):279?287.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
IJCAI-95, 14th International Joint Conference on Ar-
tificial Intelligence, volume 1, pages 448?453.
J. Rodd, G. Gaskell, and W. Marslen-Wilson. 2002.
Making sense of semantic ambiguity: Semantic com-
petition in lexical access. Journal of Memory and Lan-
guage, 46(2):245?266.
M.D.E.B. Rodriguez and J.M.G. Hidalgo. 1997. Using
WordNet to complement training information in text
categorisation. In Procedings of RANLP-97, 2nd In-
ternational Conference on Recent Advances in Natural
Language Processing.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis.
S. Shi, J.R. Wen, Q. Yu, R. Song, and W.Y. Ma. 2005.
Gravitation-based model for information retrieval. In
Proceedings of SIGIR-05, 28th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 488?495. ACM New York, NY,
USA.
G. Siolas and F. d?Alche? Buc. 2000. Support vector ma-
chines based on a semantic kernel for text categoriza-
tion. In Proceedings of IJCNN-00, IEEE International
Joint Conference on Neural Networks.
C. J. van Rijsbergen. 2004. The Geometry of Information
Retrieval.
P. Wittek and S. Dara?nyi. 2007. Representing word
semantics for IR by continuous functions. In S. Do-
minich and F. Kiss, editors, Proceedings of ICTIR-07,
1st International Conference of the Theory of Informa-
tion Retrieval, pages 149?155.
P. Wittek, C.L. Tan, and S. Dara?nyi. 2009. An or-
dering of terms based on semantic relatedness. In
H. Bunt, editor, Proceedings of IWCS-09, 8th Inter-
national Conference on Computational Semantics.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of SIGIR-85, 8th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 18?25.
191
Proceedings of the 8th International Conference on Computational Semantics, pages 235?247,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
An Ordering of Terms Based on Semantic
Relatedness
Peter Wittek
Department of Computer Science
National University of Singapore
Sa?ndor Dara?nyi
Swedish School of Library and Information Science
Go?teborg University
Sandor.Daranyi@hb.se
Chew Lim Tan
Department of Computer Science
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Term selection methods typically employ a statistical measure to
filter or weight terms. Term expansion for IR may also depend on
statistics, or use some other, non-metric method based on a lexical
resource. At the same time, a wide range of semantic similarity mea-
sures have been developed to support natural language processing tasks
such as word sense disambiguation. This paper combines the two ap-
proaches and proposes an algorithm that provides a semantic order of
terms based on a semantic relatedness measure. This semantic order
can be exploited by term weighting and term expansion methods.
1 Introduction
Since the early days of the vector space model, it has been debated whether
it is a proper carrier of meaning of texts [23], arguing if distributional sim-
ilarity is an adequate proxy for lexical semantic relatedness [3]. With the
statistical, i.e. devoid of word semantics approaches there is generally no
way to improve both precision and recall at the same time, increasing one
is done at the expense of the other. For example, casting a wider net of
235
search terms to improve recall of relevant items will also bring in an even
greater proportion of irrelevant items, lowering precision. In the meantime,
practical applications in information retrieval and text classification have
been proliferating, especially with developments in kernel methods in the
last decade [9, 4].
Ordering of terms based on semantic relatedness seeks an answer to the
simple question, can statistical term weighting be eclipsed? Namely, vari-
ants of weighting schemes based on term occurrences and co-occurrences
dominate the information retrieval and text classification scenes. However,
they also have a number of limitations. The connection between statistics
and word semantics is in general not understood very well. In other words,
a systematic discussion of mappings between theories of word meaning and
modeling them by mathematical objects is missing for the time being. Fur-
ther, enriching weighting schemes by importing their sense content from
lexical resources such as WordNet lacks a theoretical interpretation in terms
of lexical semantics. Combining co-occurrence and lexical resource-based
approaches for term weighting and term expansion may offer further theo-
retical insights, as well as performance benefits.
Using vectors in the vector space model as such mathematical objects
for the representation of term, document or query meaning necessarily ex-
presses content mapped on form as a set of coordinates. These coordinates,
at least in the case of the tfidf scheme, are corpus-specific, i.e. term weights
are neither constant over time nor database independent. Introducing a se-
mantic ordering of terms, hence loading a coordinate with semantic content,
reduces the dependence on a specific corpus.
In what follows, we will argue that:
? By assigning specific scalar values to terms in an ontology, terms rep-
resented by sets of geometric coordinates can be outdone;
? Such values result from a one-dimensional ordering based on the idea
of a sense-preserving distance between terms in a conceptual hierarchy;
? Sense-preserving distances mapped onto a line condense lexical rela-
tions and express them as a kind of within-language referential mean-
ing pertinent to individual terms, quasi charging their occurrences
independent of their occurrence rates, i.e. from the outside;
? This linear order can be used to assist term expansion and term weight-
ing.
236
This paper is organized as follows. Section 2 discusses the most impor-
tant measures for semantic relatedness with regard to the major linguistic
theories. Section 3 introduces an algorithm that creates a linear semantic
order of terms of a corpus, and Section 4 both offers first results in text
classification and discusses some implications. Finally, Section 5 concludes
the paper.
2 Measuring Semantic Relatedness
Several methods have been proposed for measuring similarity. One of such
early proposals was the semantic differential which analyzes the affective
meaning of terms into a range of different dimensions with the opposed
adjectives at both ends, and locates the terms within semantic space [20].
Semantic similarity as proposed by Miller and Charles is a continuous
variable that describes the degree of synonymy between two terms [16]. They
argue that native speakers can order pairs of terms by semantic similarity,
for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house,
ship-dog, ship-sun. This concept may be extended to quantify relations
between non-synonymous but closely related terms, for example airplane-
wing. Semantic distance is the inverse of semantic similarity [17].
Semantic relatedness is defined between senses of terms. Given a relat-
edness formula rel(s
1
, s
2
) between two senses s
1
and s
2
, term relatedness
between two terms t
1
and t
2
can be calculated as
rel(t
1
, t
2
) = max
s
1
?sen(t
1
),s
2
?sen(t
2
)
rel(s
1
, s
2
),
where sen(t) is a set of senses of term t [3].
Automated systems assign a score of semantic relatedness to a given pair
of terms calculated from a relatedness measure. The absolute score itself
is typically irrelevant on its own, what is important is that the measure
assigns a higher score to term pairs which humans think are more related
and comparatively lower scores to term pairs that are less related [17].
The best known theories of word semantics fall in three major groups:
1. ?Meaning is use? [30]: habitual usage provides indirect contextual in-
terpretation of any term. In accord with Carnap, frequency of use
expresses aspects of a conceptual hierarchy. In terms of logical seman-
tics, one regards document groups as value extensions (classes) and
index terms as value intensions (properties) of a (semantic) function
237
?f?. Extensions and intensions are inverse proportional: the more prop-
erties defined, the less entities they apply to - there are more flowers
in general than tulips in particular, for instance.
2. ?Meaning is change?: the stimulus-response theory by Bloomfield and
the biological theory of meaning by von Uexku?ll both stress that the
meaning of any action is its consequences.
3. ?Meaning is equivalence?: referential or ostensional theories of mean-
ing suggest that ?X = Y for/as long as Z? [22].
Point 2 refers to theories which assign a temporal structure to word
meaning, they are not discussed here. Measures that rely on distributional
measures (Point 1) and those that use knowledge-rich resources (Point 3)
both exist, and they have been individually shown to good quantifiers of
term similarity each [17], These theories have been individually shown to be
good, therefore their combination must be a valid research alternative.
A lexical resource in computer science is a structure that captures se-
mantic relations among terms. Such a resource necessarily entails some sort
of world view with respect to a given domain. This is often conceived as a set
of concepts, their definitions and their inter-relationships; this is referred to
as a conceptualization. The following types of resources are commonly used
in measuring semantic similarity between terms: dictionary [12], semantic
networks, such as WordNet [5], thesauri modeled on Roget?s Thesaurus [19].
All approaches to measuring semantic relatedness that use a lexical re-
source regard the resource as a network or a directed graph, making use of
the structural information embedded in the graph [8, 3].
Distributional similarity, as studied by language technology, covers an
important kind of theories of word meaning and can be hence seen as con-
tributing to semantic document indexing and retrieval. Its predecessors go
back a long way, building on the notion of term dependence and structures
derived therefrom [2, 18]. Also called the contextual theory of meaning (see
[15] for the historical development of the concept), the underlying distri-
butional hypothesis is often cited for explaining how word meaning enters
information processing [10], and basically equals the claim ?meaning is use?
in language philosophy. Before attempts to utilize lexical resources for the
same purpose, this used to be the sole source of word semantics in informa-
tion retrieval, inherent in the exploitation of term occurrences (tfidf) and
term co-occurrences [7, 21, 27], including multiple-level term co-occurrences
[11].
238
Statistical techniques typically suffer from the sparse data problem: they
perform poorly when the terms are relatively rare, due to the scarcity of data.
Hybrid approaches attempt to address this problem by supplementing sparse
data with information from a lexical database [24, 8]. In a semantic network,
to differentiate between the weights of edges connecting a node and all its
child nodes, one needs to consider the link strength of each specific child
link. This is a situation in which corpus statistics can contribute. Ideally
the method chosen should be both theoretically sound and computationally
efficient [8].
Following the notation in information theory, the information content
(IC) of a concept c can be quantified as follows.
IC(c) =
1
logP (c)
.
where P (c) is the probability of encountering an instance of concept c. In the
case of the hierarchical structure, where a concept in the hierarchy subsumes
those ones below it, this implies that P (c) is monotonic as one moves up in
the hierarchy. As the node?s probability increases, its information content or
its informativeness decreases. If there is a unique top node in the hierarchy,
then its probability is 1, hence its information content is 0. Given the
monotonic feature of the information content value, the similarity of two
concepts can be formally defined as follows.
sim(c
1
, c
2
) = max
c?Sup(c
1
,c
2
)
IC(c) = max
c?Sup(c
1
,c
2
)
? log p(c)
where Sup(c
1
, c
2
) is the set of concepts that subsume both c
1
and c
2
. To
maximize the representativeness, the similarity value is the information con-
tent value of the node whose IC value is the largest among those higher order
classes.
The information content method requires less information on the detailed
structure of a lexical resource and it is insensitive to varying link types [24].
On the other hand, it does not differentiate between the similarity values of
any pair of concepts in a sub-hierarchy as long as their lowest super-ordinate
class is the same. Moreover, in the calculation of information content, a
polysemous term will have a large content value if only term frequency data
are used.
The distance function between two terms can be written as follows:
d(t
1
, t
2
) = IC(c
1
) + IC(c
2
)? 2IC(LSuper(c
1
, c
2
)),
239
where LSuper(c1, c2) denotes the lowest super-ordinate of c
1
and c
2
in a
lexical resource. This distance measure also satisfies the properties of a
metric [8].
3 A Semantic Ordering of Terms
Traditional distributional term clustering methods do not provide signifi-
cantly improved text representation [13]. Distributional clustering has also
been employed to compress the feature space while compromising document
classification accuracy [1]. Applying the information bottleneck method to
find term clusters that preserve the information about document categories
has been shown to increase text classification accuracy in certain cases [28].
On the other hand, term expansion has been widely researched, with
varying results [21]. These methods generate new features for each docu-
ment in the data set. These new features can be synonyms or homonyms of
document terms [26], or expanded features for terms, sentences and docu-
ments as in [6]. Several distributional criteria have been used to select terms
related to the query. For instance, [25] proposed the principle that the se-
lected terms should have a higher probability in the relevant documents than
in the irrelevant documents. Others examined the impact of determining ex-
pansion terms using a minimum spanning tree and some simple linguistic
analysis [29].
This section proposes an algorithm that connects term clustering and
term expansion. It employs a pairwise comparison between the terms to
find a linear order, instead of finding clusters. In this order, the transition
from a term to an adjacent one is ?smooth? if the semantic distance between
two neighboring terms is small. The dimension of the feature space is not
compressed, yet, groups of adjacent terms can be regarded as semantic clus-
ters. Hence, following the idea of term expansion, adjacent terms can help
to improve the effectiveness of any vector space-based language technology.
Let V denote a set of terms {t
1
, t
2
, . . . , t
n
} and let d(t
i
, t
j
) denote the
semantic distance between the terms t
i
and t
j
.
Let G = (V,E) denote a weighted undirected graph, where the weights
on the set E are defined by the distances between the terms.
Finding a semantic ordering of terms can be translated to a graph prob-
lem: a minimum-weight Hamiltonian path S of G gives the ordering by
reading the nodes from one end of the path to the other. G is a complete
graph, therefore such a path always exists, but finding it is an NP-complete
problem. The following greedy algorithm is similar to the nearest neighbor
240
heuristic for the solution of the traveling salesman problem. It creates a
graph G
?
= (S, T ), where S = V and T ? E. This G
?
graph is a span-
ning tree of G in which the maximum degree of a node is two, that is, the
minimum spanning tree is a path between two nodes.
Step 1 Find the term at the highest stage of the hierarchy in a lexical
resource.
t
s
= argmin
t
i
?V
depth(t
i
).
This seed term is the first element of V
?
, V
?
= {t
s
}. Remove it from
the set V :
V := V \{t
s
}.
Step 2 Let t
l
denote the leftmost term of the ordering and t
r
the rightmost
one. Find the next two elements of the ordering:
t
?
l
= argmin
t
i
?V
d(t
i
, t
l
),
t
?
r
= argmin
t
i
?V \{t
?
l
}
d(t
i
, t
r
).
Step 3 If d(t
l
, t
?
l
) < d(t
r
, t
?
r
) then add t
?
l
to V
?
, E
?
:= E
?
? {e(t
l
, t
?
l
)}, and
V := V \{t
?
l
}. Else add t
?
r
to V
?
, E
?
:= E
?
?{e(t
r
, t
?
r
)} and V := V \{t
?
r
}.
Step 4 Repeat from Step 2 until V = ?.
The computational cost of the algorithm is O(n
2
). The above algorithm
can be thought of as a modified Prim?s algorithm, but it does not find the
optimal minimum-weight spanning tree.
The validity of the ordering algorithm is discussed as follows.
1. The ordering is possible. Starting from the seed term, the candidate
sets will always contain elements, which either share the same hyper-
nym or are hypernyms of each other.
2. The ordering is good enough. The quality will also depend on the
lexical resource in question. Further, the complexity of human lan-
guages makes the creation of even a near perfect semantic network of
its concepts impossible. Thus in many ways the lexical resource-based
measures are as good as the networks on which they are based.
3. The distance between adjacent terms is uniform. By the construction
of the ordering, it is obvious that the distances will not be uniform.
241
4 Discussion
We were interested in how the distances of consecutive index terms change if
we apply the semantic ordering. We indexed the ModApte split of Reuters-
21578 benchmark corpus with a WordNet-based stemmer. The indexing
found 12643 individual terms. Prior to the semantic ordering, terms were
assumed to be in an arbitrary order. Measuring the Jiang-Conrath distance
between the arbitrarily ordered terms, the average distance was 1.68. Note
that the Jiang-Conrath distance was normalized to the interval [0, 2]. Fig-
ure 1 shows the distribution of distances. The histogram has a high peak
at the maximum distance, indicating that the original arrangment had little
to do with semantic distance. However, there were few terms with zero or
little distance between them. This is due to terms which are related and
start with the same word or stem. For example, account, account execu-
tive, account for, accountable, accountant, accounting principle, accounting
standard, accounting system, accounts payable, accounts receivable.
Figure 1: Distribution of Distances Between Adjacent Terms in an Arbitrary
Order
After the semantic ordering of the term by the proposed algorithm, both
the average distance and the Jiang-Conrath distance were 0.56. About one
third of the terms had very little distance between each other (see Figure 2).
Nevertheless, over 10 % of the total terms still had the maximum distance.
This is due to the non-optimal nature of the proposed term-ordering algo-
rithm. These terms add noise to the classification. The noisy terms occur
242
typically at the two sides of the scale, being the leftmost and the rightmost
ones. While it is easy to find terms close to each other in the beginning, as
the algorithm proceeds, fewer terms remain in the pool to be chosen. For
instance, brand, brand name, trade name, label are in the 33rd, 34th, 35th
and 36th position on the left side counting from the seed respectively, while
windy, widespread, willingly, whatsoever, worried, worthwhile close the left
side, apparently sharing little in common.
Figure 2: Distribution of Distances Between Adjacent Terms in a Semantic
Order Based on Jiang-Conrath Distance
We conducted experiments on the ten most common categories of the
ModApte split of Reuters-21578. We trained support vector machines with
a linear kernel to compare the micro- and macro-average F
1
measures for
different methods. Table 1 summarizes the results. The baseline vector
space model has zero expansion terms. Neighboring terms of the semantic
order were chosen as expansion terms. We found that increasing the number
of expansion terms also increases the effectiveness of classification, however,
effectiveness decreases after 4 expansions for micro-F1 and after 6 expansions
for macro-F1.
5 Conclusions
Terms can be corpus- or genre-specific. Manually constructed general-purpose
lexical resources include many usages that are infrequent in a particular cor-
243
Number of
Expansion Micro-F
1
Macro-F
1
Terms
0 0.900 0.826
2 0.901 0.826
4 0.905 0.828
6 0.898 0.830
8 0.896 0.827
Table 1: Micro-Average and Macro F
1
-measure, Reuters-21578
pus or genre of documents, and therefore of little use. For example, one of
the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym
of person [14]. This usage of the term is practically never used in newspaper
articles, hence distributional attributes should be taken into consideration
when creating a linear ordering of terms.
Integrating lexical resources into an upgraded semantic weighting scheme
that could augment statistical term weighting is a prospect that cannot be
overlooked in information retrieval and text categorization. Our first results
with such a scheme in text categorization. At the same time, the results
also raise the question, does assigning specific scalar values to terms in an
ontology, this far represented by their geometric coordinates only, turn them
metaphorically into band lines of elements in a conceptual spectrum. We
anticipate that applying other types of kernels to the task may bring a new
set of challenging results.
References
[1] L.D. Baker and A.K. McCallum. Distributional clustering of words
for text classification. In Proceedings of SIGIR-98, 21st ACM Inter-
national Conference on Research and Development in Information Re-
trieval, pages 96?103, Melbourne, Australia, August 1998. ACM Press,
New York, NY, USA.
[2] M.A. Ba?rtschi. Term dependence in information retrieval models. Mas-
ter?s thesis, Swiss Federal Institute of Technology, 1984.
244
[3] A. Budanitsky and G. Hirst. Evaluating WordNet-based measures of
lexical semantic relatedness. Computational Linguistics, 32(1):13?47,
2006.
[4] N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent semantic kernels.
Journal of Intelligent Information Systems, 18(2):127?152, 2002.
[5] C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press,
Cambridge, MA, USA, 1998.
[6] E. Gabrilovich and S. Markovitch. Feature generation for text cate-
gorization using world knowledge. In Proceedings of IJCAI-05, 19th
International Joint Conference on Artificial Intelligence, volume 19,
Edinburgh, UK, 2005. Lawrence Erlbaum Associates Ltd.
[7] S. I. Gallant. A practical approach for representing context and for
performing word sense disambiguation using neural networks. Neural
Computation, 3:293?309, 1991.
[8] J.J. Jiang and D.W. Conrath. Semantic similarity based on corpus
statistics and lexical taxonomy. In Proceedings of the International Con-
ference on Research in Computational Linguistics, pages 19?33, Taipei,
Taiwan, 1997.
[9] T. Joachims. Text categorization with support vector machines: Learn-
ing with many relevant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning, pages 137?142, Chemnitz,
Germany, April 1998. Springer-Verlag, London, UK.
[10] J. Karlgren and M. Sahlgren. From words to understanding. Founda-
tions of Real-World Intelligence, pages 294?308, 2001.
[11] A. Kontostathis and W.M. Pottenger. A framework for understanding
latent semantic indexing (LSI) performance. Information Processing
and Management, 42(1):56?73, 2006.
[12] M. Lesk. Automatic sense disambiguation using machine readable dic-
tionaries: How to tell a pine cone from an ice cream cone? In Proceed-
ings of SIGDOC-86, 5th Annual International Conference on Systems
Documentation, pages 24?26, New York, NY, USA, 1986. ACM Press.
[13] D.D. Lewis. An evaluation of phrasal and clustered representations
on a text categorization task. In Proceedings of SIGIR-92, 15th ACM
245
International Conference on Research and Development in Information
Retrieval, pages 37?50, Copenhagen, Denmark, June 1992. ACM Press,
New York, NY, USA.
[14] D. Lin. Automatic retrieval and clustering of similar words. In Pro-
ceedings of COLING-ACL Workshop on Usage of WordNet in Natu-
ral Language Processing Systems, volume 98, pages 768?773, Montre?al,
Que?bec, Canada, August 1998. ACL, Morristown, NJ, USA.
[15] J. Lyons. Semantics. Cambridge University Press, New York, NY,
USA, 1977.
[16] G. Miller and W. Charles. Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28, 1991.
[17] S. Mohammad and G. Hirst. Distributional measures as proxies for
semantic relatedness. Submitted for publication, 2005.
[18] J. Morris, C. Beghtol, and G. Hirst. Term relationships and their contri-
bution to text semantics and information literacy through lexical cohe-
sion. In Proceedings of the 31st Annual Conference of the Canadian As-
sociation for Information Science, Halifax, Nova Scotia, Canada, May
2003.
[19] J. Morris and G. Hirst. Lexical cohesion computed by thesaural rela-
tions as an indicator of the structure of text. Computational Linguistics,
17(1):21?48, 1991.
[20] C.E. Osgood. The nature and measurement of meaning. Psychological
Bulletin, 49(3):197?237, 1952.
[21] H.J. Peat and P. Willett. The limitations of term co-occurrence data for
query expansion in document retrieval systems. Journal of the Ameri-
can Society for Information Science, 42(5):378?383, 1991.
[22] C.S. Peirce. Logic as semiotic: The theory of signs. Philosophical
Writings of Peirce, pages 98?119, 1955.
[23] V.V. Raghavan and S.K.M. Wong. A critical analysis of vector space
model for information retrieval. Journal of the American Society for
Information Science, 37(5):279?287, 1986.
246
[24] P. Resnik. Using information content to evaluate semantic similarity in
a taxonomy. In Proceedings of IJCAI-95, 14th International Joint Con-
ference on Artificial Intelligence, volume 1, pages 448?453, Montre?al,
Que?bec, Canada, August 1995.
[25] S.E. Robertson. On term selection for query expansion. Journal of
Documentation, 46(4):359?364, 1990.
[26] M.D.E.B. Rodriguez and J.M.G. Hidalgo. Using WordNet to com-
plement training information in text categorisation. In Procedings of
RANLP-97, 2nd International Conference on Recent Advances in Natu-
ral Language Processing. John Benjamins Publishing, Amsterdam, The
Netherlands, 1997.
[27] H. Schutze and T. Pedersen. A co-occurrence-based thesaurus and
two applications to information retrieval. Information Processing and
Management, 3(33):307?318, 1997.
[28] N. Slonim and N. Tishby. The power of word clusters for text clas-
sification. In Proceedings of ECIR-01, 23rd European Colloquium on
Information Retrieval Research, Darmstadt, Germany, 2001.
[29] A.F. Smeaton and C.J. van Rijsbergen. The retrieval effects of query
expansion on a feedback document retrieval system. The Computer
Journal, 26(3):239?246, 1983.
[30] L. Wittgenstein. Philosophical Investigations. Blackwell Publishing,
Oxford, UK, 1967.
247
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188?196,
Beijing, August 2010
A Twin-Candidate Based Approach for Event Pronoun Resolution us-
ing Composite Kernel  
Chen Bin1 Su Jian2 Tan Chew Lim1 
1National University of Singapore 2Institute for Inforcomm Research, A-STAR 
{chenbin,tancl}@comp.nus.edu.sg sujian@i2r.a-star.edu.sg 
 
Abstract 
Event Anaphora Resolution is an important 
task for cascaded event template extraction 
and other NLP study. In this paper, we provide 
a first systematic study of resolving pronouns 
to their event verb antecedents for general 
purpose. First, we explore various positional, 
lexical and syntactic features useful for the 
event pronoun resolution. We further explore 
tree kernel to model structural information 
embedded in syntactic parses. A composite 
kernel is then used to combine the above di-
verse information. In addition, we employed a 
twin-candidate based preferences learning 
model to capture the pair wise candidates? pre-
ference knowledge. Besides we also look into 
the incorporation of the negative training in-
stances with anaphoric pronouns whose ante-
cedents are not verbs. Although these negative 
training instances are not used in previous 
study on anaphora resolution, our study shows 
that they are very useful for the final resolu-
tion through random sampling strategy. Our 
experiments demonstrate that it?s meaningful 
to keep certain training data as development 
data to help SVM select a more accurate hyper 
plane which provides significant improvement 
over the default setting with all training data. 
1 Introduction 
Anaphora resolution, the task of resolving a giv-
en text expression to its referred expression in 
prior texts, is important for intelligent text 
processing systems. Most previous works on 
anaphora resolution mainly aims at object ana-
phora in which both the anaphor and its antece-
dent are mentions of the same real world objects 
In contrast, an event anaphora as first defined 
in (Asher, 1993) is an anaphoric reference to an 
event, fact, and proposition which is representa-
tive of eventuality and abstract entity. Consider 
the following example: 
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The anaphor [It]2 in the above example refers 
back to an event, ?all-white and all-Christian city 
of Postville is diluted by different ethnic groups.? 
Here, we take the main verb of the event, [in-
vaded]1 as the representation of this event and 
the antecedent for pronoun [It]2.  
According to (Asher, 1993), antecedents of 
event pronoun include both gerunds (e.g. de-
struction) and inflectional verbs (e.g. destroying). 
In our study, we focus on the inflectional verb 
representation, as the gerund representation is 
studied in the conventional anaphora resolution. 
For the rest of this paper, ?event pronouns? are 
pronouns whose antecedents are event verbs 
while ?non-event anaphoric pronouns? are those 
with antecedents other than event verbs. 
 Entity anaphora resolution provides critical 
links for cascaded event template extraction. It 
also provides useful information for further infe-
rence needed in other natural language 
processing tasks such as discourse relation and 
entailment. Event anaphora (both pronouns and 
noun phrases) contributes a significant propor-
tion in anaphora corpora, such as OntoNotes. 
19.97% of its total number of entity chains con-
tains event verb mentions. 
In (Asher, 1993) chapter 6, a method to re-
solve references to abstract entities using dis-
course representation theory is discussed. How-
ever, no computation system was proposed for 
entity anaphora resolution. (Byron, 2002) pro-
posed semantic filtering as a complement to sa-
lience calculations to resolve event pronoun tar-
geted by us. This knowledge deep approach only 
188
works for much focused domain like trains spo-
ken dialogue with handcraft knowledge of rele-
vant events for only limited number of verbs in-
volved.  Clearly, this approach is not suitable for 
general event pronoun resolution say in news 
articles. Besides, there?s also no specific perfor-
mance report on event pronoun resolution, thus 
it?s not clear how effective their approach is. 
(M?ller, 2007) proposed pronoun resolution sys-
tem using a set of hand-crafted constraints such 
as ?argumenthood? and ?right-frontier condition? 
together with logistic regression model based on 
corpus counts. The event pronouns are resolved 
together with object pronouns. This explorative 
work produced an 11.94% F-score for event pro-
noun resolution which demonstrated the difficul-
ty of event anaphora resolution. In (Pradhan, 
et.al, 2007), a general anaphora resolution sys-
tem is applied to OntoNotes corpus. However, 
their set of features is designed for object ana-
phora resolution. There is no specific perfor-
mance reported on event anaphora. We suspect 
the event pronouns are not correctly resolved in 
general as most of these features are irrelevant to 
event pronoun resolution.  
In this paper, we provide the first systematic 
study on pronominal references to event antece-
dents. First, we explore various positional, lexi-
cal and syntactic features useful for event pro-
noun resolution, which turns out quite different 
from conventional pronoun resolution except 
sentence distance information. These have been 
used together with syntactic structural informa-
tion using a composite kernel. Furthermore, we 
also consider candidates? preferences informa-
tion using twin-candidate model. 
Besides we further look into the incorporation 
of negative instances from non-event anaphoric 
pronoun, although these instances are not used in 
previous study on co-reference or anaphora reso-
lution as they make training instances extremely 
unbalanced. Our study shows that they can be 
very useful for the final resolution after random 
sampling strategy.  
We further demonstrate that it?s meaningful to 
keep certain training data as development data to 
help SVM select a more accurate hyper-plane 
which provide significant improvement over the 
default setting with all training data.  
The rest of this paper is organized as follows.  
Section 2 introduces the framework for event 
pronoun resolution, the considerations on train-
ing instance, the various features useful for event 
pronoun resolution and SVM classifier with ad-
justment of hyper-plane. Twin-candidate model 
is further introduced to capture the preferences 
among candidates. Section 3 presents in details 
the structural syntactic feature and the kernel 
functions to incorporate such a feature in the res-
olution. Section 4 presents the experiment results 
and some discussion. Section 5 concludes the 
paper. 
2 The Resolution Framework 
Our event-anaphora resolution system adopts the 
common learning-based model for object ana-
phora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002a). 
2.1 Training and Testing instance 
In the learning framework, training or testing 
instance of the resolution system has a form of 
               where        is the i
th candi-
date of the antecedent of anaphor    . An in-
stance is labeled as positive if        is the ante-
cedent of      , or negative if        is not the 
antecedent of     . An instance is associated 
with a feature vector which records different 
properties and relations between     and       . 
The features used in our system will be discussed 
later in this paper.  
During training, for each event pronoun, we 
consider the preceding verbs in its current and 
previous two sentences as its antecedent candi-
dates. A positive instance is formed by pairing an 
anaphor with its correct antecedent. And a set of 
negative instances is formed by pairing an ana-
phor with its candidates other than the correct 
antecedent. In addition, more negative instances 
are generated from non-event anaphoric pro-
nouns. Such an instance is created by pairing up 
a non-event anaphoric pronoun with each of the 
verbs within the pronoun?s sentence and previous 
two sentences. This set of instances from non-
event anaphoric pronouns is employed to provide 
extra power on ruling out non-event anaphoric 
pronouns during resolution. This is inspired by 
the fact that event pronouns are only 14.7% of all 
the pronouns in the OntoNotes corpus. Based on 
these generated training instances, we can train a 
binary classifier using any discriminative learn-
ing algorithm. 
189
The natural distribution of textual data is of-
ten imbalanced. Classes with fewer examples are 
under-represented and classifiers often perform 
far below satisfactory. In our study, this becomes 
a significant issue as positive class (event ana-
phoric) is the minority class in pronoun resolu-
tion task. Thus we utilize a random down sam-
pling method to reduce majority class samples to 
an equivalent level with the minority class sam-
ples which is described in (Kubat and Matwin, 
1997) and (Estabrooks et al 2004). In (Ng and 
Cardie, 2002b), they proposed a negative sample 
selection scheme which included only negative 
instances found in between an anaphor and its 
antecedent. However, in our event pronoun reso-
lution, we are distinguishing the event-anaphoric 
from non-event anaphoric which is different 
from (Ng and Cardie, 2002b). 
2.2 Feature Space 
In a conventional pronoun resolution, a set of 
syntactic and semantic knowledge has been re-
ported as in (Strube and M?ller, 2003; Yang et al 
2004;2005a;2006). These features include num-
ber agreement, gender agreement and many oth-
ers. However, most of these features are not use-
ful for our task, as our antecedents are inflection-
al verbs instead of noun phrases. Thus we have 
conducted a study on effectiveness of potential 
positional, lexical and syntactic features. The 
lexical knowledge is mainly collected from cor-
pus statistics. The syntactic features are mainly 
from intuitions. These features are purposely en-
gineered to be highly correlated with positive 
instances. Therefore such kind of features will 
contribute to a high precision classifier.  
? Sentence Distance 
This feature measures the sentence distance be-
tween an anaphor and its antecedent candidate 
under the assumptions that a candidate in the 
closer sentence to the anaphor is preferred to be 
the antecedent. 
? Word Distance  
This feature measures the word distance between 
an anaphor and its antecedent candidate. It is 
mainly to distinguish verbs from the same sen-
tence. 
? Surrounding Words and POS Tags 
The intuition behind this set of features is to find 
potential surface words that occur most frequent-
ly with the positive instances. Since most of 
verbs occurred in front of pronoun, we have built 
a frequency table from the preceding 5 words of 
the verb to succeeding 5 surface words of the 
pronoun. After the frequency table is built, we 
select those words with confidence1  > 70% as 
features. Similar to Surrounding Words, we have 
built a frequency table to select indicative sur-
rounding POS tags which occurs most frequently 
with positive instances. 
? Co-occurrences of Surrounding Words 
The intuition behind this set of features is to cap-
ture potential surface patterns such as ?It 
caused?? and ?It leads to?. These patterns are 
associated with strong indication that pronoun 
?it? is an event pronoun. The range for the co-
occurrences is from preceding 5 words to suc-
ceeding 5 words. All possible combinations of 
word positions are used for a co-occurrence 
words pattern. For example ?it leads to? will 
generate a pattern as ?S1_S2_lead_to? where S1 
and S2 mean succeeding position 1 and 2. Simi-
lar to previous surrounding words, we will con-
duct corpus statistics analysis and select co-
occurrence patterns with a confidence greater 
than 70%. Following the same process, we have 
examined co-occurrence patterns for surrounding 
POS tags.  
? Subject/Object Features 
This set of features aims to capture the relative 
position of the pronoun in a sentence. It denotes 
the preference of pronoun?s position at the clause 
level. There are 4 features in this category as 
listed below. 
Subject of Main Clause 
This feature indicates whether a pronoun is at the 
subject position of a main clause. 
Subject of Sub-clause 
This feature indicates whether a pronoun is at the 
subject position of a sub-clause. 
Object of Main Clause 
This feature indicates whether a pronoun is at the 
object position of a main clause. 
Object of Sub-clause 
This feature indicates whether a pronoun is at the 
object position of a sub-clause. 
? Verb of Main/Sub Clause 
Similar to the Subject/Object features of pro-
noun, the following two features capture the rela-
                                                 
1               
                                        
                    
 
190
tive position of a verb in a sentence. It encodes 
the preference of verb position between main 
verbs in main/sub clauses. 
Main Verb in Main Clause 
This feature indicates whether a verb is a main 
verb in a main clause. 
Main Verb in Sub-clause 
This feature indicates whether a verb is a main 
verb in a sub-clause. 
2.3 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn a classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels 
to incorporate the structure feature. One advan-
tage of SVM is that we can use tree kernel ap-
proach to capture syntactic parse tree information 
in a particular high-dimension space. 
Suppose a training set   consists of labeled 
vectors          , where    is the feature vector 
of a training instance and    is its class label. The 
classifier learned by SVM is: 
                     
   
  
where    is the learned parameter for a support 
vector   . An instance   is classified as positive 
if       . Otherwise,   is negative. 
? Adjust Hyper-plane with Development Data 
Previous works on pronoun resolution such as 
(Yang et al 2006) used the default setting for 
hyper-plane which sets       . And an in-
stance is positive if        and negative oth-
erwise. In our study, we look into a method of 
adjusting the hyper-plane?s position using devel-
opment data to improve the classifier?s perfor-
mance.  
Considering a default model setting for SVM 
as shown in Figure 2(for illustration purpose, we 
use a 2-D example). 
 
Figure 2: 2-D SVM Illustration 
The objective of SVM learning process is to find 
a set of weight vector   which maximizes the 
margin (defined as  
   
) with constraints defined 
by support vectors. The separating hyper-plane is 
given by         as bold line in the center. 
The margin is the region between the two dotted 
lines (bounded by         and     
    ). The margin is a space without any in-
formation from training instances. The actual 
hyper-plane may fall in any place within the 
margin. It does not necessarily occur in the. 
However, the hyper-plane is used to separate 
positive and negative instances during classifica-
tion process without consideration of the margin. 
Thus if an instance falls in the margin, SVM can 
only decide class label from hyper-plane which 
may cause misclassification in the margin. 
 Based on the previous discussion, we propose 
an adjustment of the hyper-plane using develop-
ment data. For simplicity, we adjust the hyper-
plane function value instead of modeling the 
function itself. The hyper-plane function value 
will be further referred as a threshold  . The fol-
lowing is a modified version of a learned SVM 
classifier. 
        
                          
   
   
                         
   
   
  
where   is the threshold,    is the learned para-
meter for a feature    and    is its class label. A 
set of development data is used to adjust the hy-
per-plane function threshold   in order to max-
imize the accuracy of the learned SVM classifier 
on development data. The adjustment of hyper-
plane is defined as: 
                            
   
  
where        is an indicator function which out-
put 1 if       is same sign as   and 0 otherwise. 
Thereafter, the learned threshold    is applied to 
the testing set. 
3 Incorporating Structural Syntactic In-
formation 
A parse tree that covers a pronoun and its ante-
cedent candidate could provide us much syntac-
tic information related to the pair which is expli-
citly or implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to what degree 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. The value returned from tree kernel reflects 
similarity between two instances in syntax. Such 
191
syntactic similarity can be further combined with 
other knowledge to compute overall similarity 
between two instances, through a composite ker-
nel. Normally, parsing is done at sentence level. 
However, in many cases a pronoun and its ante-
cedent candidate do not occur in the same sen-
tence. To present their syntactic properties and 
relations in a single tree structure, we construct a 
syntax tree for an entire text, by attaching the 
parse trees of all its sentences to an upper node. 
Having obtained the parse tree of a text, we shall 
consider how to select the appropriate portion of 
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun 
and a candidate, the structured feature at least 
should be able to cover both of these two expres-
sions. 
3.1 Structural Syntactic Feature 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information that comes from parsing errors 
would likely be introduced. In our study, we ex-
amine three possible structured features that con-
tain different substructures of the parse tree: 
 
? Minimum Expansion Tree 
This feature records the minimal structure cover-
ing both pronoun and its candidate in parse tree. 
It only includes the nodes occurring in the short-
est path connecting the pronoun and its candidate, 
via the nearest commonly commanding node.  
When the pronoun and candidate are from differ-
ent sentences, we will find a path through pseudo 
?TOP? node which links all the parse trees. Con-
sidering the example given in section 1,  
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The minimum expansion structural feature of the 
instance {invaded, it} is annotated with bold 
lines and shaded nodes in figure 1.  
? Simple Expansion Tree 
Minimum-Expansion could, to some degree, de-
scribe the syntactic relationships between the 
candidate and pronoun. However, it is incapable 
of capturing the syntactic properties of the can-
didate or the pronoun, because the tree structure 
surrounding the expression is not taken into con-
sideration. To incorporate such information, fea-
ture Simple-Expansion not only contains all the 
nodes in Minimum-Expansion, but also includes 
the first-level children of these nodes2 except the 
punctuations. The simple-expansion structural 
feature of instance {invaded, it} is annotated in 
figure 2. In the left sentence?s tree, the node ?NP? 
for ?perhaps different groups? is terminated to 
provide a clue that we have a noun phrase at the 
object position of the candidate verb. 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 1: Minimum-Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 2: Simple Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 3: Full-Expansion Tree 
? Full Expansion Tree 
This feature focuses on the whole tree structure 
between the candidate and pronoun. It not only 
includes all the nodes in Simple-Expansion, but 
also the nodes (beneath the nearest commanding 
parent) that cover the words between the candi-
                                                 
2 If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences 
before the candidate or after the pronoun. 
192
date and the pronoun3. Such a feature keeps the 
most information related to the pronoun and can-
didate pair. Figure 3 shows the structure for fea-
ture full-expansion for instance {invaded, it}. As 
illustrated, the ?NP? node for ?perhaps different 
groups? is further expanded to the POS level. All 
its child nodes are included in the full-expansion 
tree except the surface words. 
3.2 Convolution Parse Tree Kernel and Com-
posite Kernel 
To calculate the similarity between two struc-
tured features, we use the convolution tree kernel 
that is defined by Collins and Duffy (2002) and 
Moschitti (2004). Given two trees, the kernel 
will enumerate all their sub-trees and use the 
number of common sub-trees as the measure of 
similarity between two trees. The above tree ker-
nel only aims for the structured feature. We also 
need a composite kernel to combine the struc-
tured feature and the flat features from section 
2.2. In our study we define the composite kernel 
as follows: 
             
            
              
 
            
              
 
where       is the convolution tree kernel de-
fined for the structured feature, and       is the 
kernel applied on the flat features. Both kernels 
are divided by their respective length4 for norma-
lization. The new composite kernel      , de-
fined as the sum of normalized       and      , 
will return a value close to 1 only if both the 
structured features and the flat features have high 
similarity under their respective kernels. 
3.3 Twin-Candidate Framework using Rank-
ing SVM Model 
In a ranking SVM kernel as described in (Mo-
schitti et al 2006) for Semantic Role Labeling, 
two argument annotations (as argument trees) are 
presented to the ranking SVM model to decide 
which one is better.  In our case, we present two 
syntactic trees from two candidates to the rank-
ing SVM model. The idea is inspired by (Yang, 
et.al, 2005b;2008). The intuition behind the 
twin-candidate model is to capture the informa-
tion of how much one candidate is more pre-
                                                 
3 We will not expand the nodes denoting the sentences other 
than where the pronoun and the candidate occur. 
4  The length of a kernel   is defined as            
                   
ferred than another. The candidate wins most of 
the pair wise comparisons is selected as antece-
dent. 
The feature vector for each training instance 
has a form of                    . An in-
stance is positive if       is a better antecedent 
choice than       . Otherwise, it is a negative 
instance. For each feature vector, both tree struc-
tural features and flat features are used.  Thus 
each feature vector has a form of    
              where    and    are trees of candi-
date i and j respectively,    and    are flat feature 
vectors of candidate i and j respectively.  
In the training instances generation, we only 
generate those instances with one candidate is 
the correct antecedent. This follows the same 
strategy used in (Yang et al 2008) for object 
anaphora resolution. 
In the resolution process, a list of m candi-
dates is extracted from a three sentences window. 
A total of  
 
 
  instances are generated by pairing-
up the m candidates pair-wisely. We used a 
Round-Robin scoring scheme for antecedent se-
lection. Suppose a SVM output for an instance 
                   is 1, we will give a score 
1 for        and -1 for        and vice versa. At 
last, the candidate with the highest score is se-
lected as antecedent. In order to handle a non-
event anaphoric pronoun, we have set a threshold 
to distinguish event anaphoric from non-event 
anaphoric. A pronoun is considered as event 
anaphoric if its score is above the threshold. In 
our experiments, we kept a set of development 
data to find out the threshold in an empirical way. 
4 Experiments and Discussions 
4.1 Experimental Setup 
OntoNotes Release 2.0 English corpus as in 
(Hovy et al 2006) is used in our study, which 
contains 300k words of English newswire data 
(from the Wall Street Journal) and 200k words of 
English broadcast news data (from ABC, CNN, 
NBC, Public Radio International and Voice of 
America).  Table 1 shows the distribution of var-
ious entities. We focused on the resolution of 
502 event pronouns encountered in the corpus. 
The resolution system has to handle both the 
event pronoun identification and antecedent se-
lection tasks. To illustrate the difficulty of event 
pronoun resolution, 14.7% of all pronoun men-
tions are event anaphoric and only 31.5% of 
193
event pronoun can be resolved using ?most re-
cent verb? heuristics. Therefore a most-recent-
verb baseline will yield an f-score 4.63%. 
To conduct event pronoun resolution, an input 
raw text was preprocessed automatically by a 
pipeline of NLP components. The noun phrase 
identification and the predicate-argument extrac-
tion were done based on Stanford Parser (Klein 
and Manning, 2003a;b) with F-score of 86.32% 
on Penn Treebank corpus.  
Non-Event Anaphora:        4952   80.03% 
Event  
Anaphora: 
1235  
19.97% 
Event NP:        733   59.35% 
Event  
Pronoun: 
502   40.65% 
It:       29.0% 
This:   16.9% 
That:  54.1% 
Table 1: The distribution of various types of 6187 
anaphora in OntoNotes 2.0 
For each pronoun encountered during resolu-
tion, all the inflectional verbs within the current 
and previous two sentences are taken as candi-
dates. For the current sentence, we take only 
those verbs in front of the pronoun. On average, 
each event pronoun has 6.93 candidates. Non-
event anaphoric pronouns will generate 7.3 nega-
tive instances on average.  
4.2 Experiment Results and Discussion 
In this section, we will present our experimental 
results with discussions. The performance meas-
ures we used are precision, recall and F-score. 
All the experiments are done with a 10-folds 
cross validation. In each fold of experiments, the 
whole corpus is divided into 10 equal sized por-
tions. One of them is selected as testing corpus 
while the remaining 9 are used for training. In 
experiments with development data, 1 of the 9 
training portions is kept for development purpose. 
In case of statistical significance test for differ-
ences is needed, a two-tailed, paired-sample Stu-
dent?s t-Test is performed at 0.05 level of signi-
ficance. 
In the first set of experiments, we are aiming 
to investigate the effectiveness of each single 
knowledge source. Table 2 reports the perfor-
mance of each individual experiment. The flat 
feature set yields a baseline system with 40.6% f-
score. By using each tree structure along, we can 
only achieve a performance of 44.4% f-score 
using the minimum-expansion tree. Therefore, 
we will further investigate the different ways of 
combining flat and syntactic structure knowledge 
to improve resolution performances. 
 Precision Recall F-score 
Flat 0.406 0.406 0.406 
Min-Exp 0.355 0.596 0.444 
Simple-Exp 0.347 0.512 0.414 
Full-Exp 0.323 0.476 0.385 
Table 2: Contribution from Single Knowledge Source 
The second set of experiments is conducted to 
verify the performances of various tree structures 
combined with flat features. The performances 
are reported in table 3. Each experiment is re-
ported with two performances. The upper one is 
done with default hyper-plane setting. The lower 
one is done using the hyper-plane adjustment as 
we discussed in section 2.3. 
 Precision Recall F-score 
Min-Exp + 
Flat 
0.433 0.512 0.469 
(0.727) (0.446) (0.553) 
Simple-Exp 
+Flat 
0.423 0.534 0.472 
(0.652) (0.492) (0.561) 
Full-Exp + 
Flat 
0.416 0.526 0.465 
(0.638) (0.496) (0.558) 
Table 3: Comparison of Different Tree Structure +Flat 
As table 3 shows, minimum-expansion gives 
highest precision in both experiment settings. 
Minimum-expansion emphasizes syntactic struc-
tures linking the anaphor and antecedent. Al-
though using only the syntactic path may lose the 
contextual information, but it also prune out the 
potential noise within the contextual structures. 
In contrast, the full-expansion gives the highest 
recall. This is probably due to the widest know-
ledge coverage provides by the full-expansion 
syntactic tree. As a trade-off, the precision of 
full-expansion is the lowest in the experiments. 
One reason for this may be due to OntoNotes 
corpus is from broadcasting news domain. Its 
texts are less-formally structured. Another type 
of noise is that a narrator of news may read an 
abnormally long sentence. It should appear as 
several separate sentences in a news article. 
However, in broadcasting news, these sentences 
maybe simply joined by conjunction word ?and?. 
Thus a very nasty and noisy structure is created 
from it. Comparing the three knowledge source, 
simple-expansion achieves moderate precision 
and recall which results in the highest f-score. 
From this, we can draw a conclusion that simple-
expansion achieves a balance between the indica-
tive structural information and introduced noises. 
In the next set of experiments, we will com-
pare different setting for training instances gen-
eration. A typical setting contains no negative 
194
instances generated from non-event anaphoric 
pronoun. This is not an issue for object pronoun 
resolution as majority of pronouns in an article is 
anaphoric. However in our case, the event pro-
noun consists of only 14.7% of the total pro-
nouns in OntoNotes. Thus we incorporate the 
instances from non-event pronouns to improve 
the precision of the classifier. However, if we 
include all the negative instances from non-event 
anaphoric pronouns, the positive instances will 
be overwhelmed by the negative instances. A 
down sampling is applied to the training in-
stances to create a more balanced class distribu-
tion. Table 4 reports various training settings 
using simple-expansion tree structure.  
Simple-Exp Tree Precision Recall F-score 
Without Non-
event Negative 
0.423 0.534 0.472 
Incl. All Negative 0.733 0.410 0.526 
Balanced Negative 0.599 0.506 0.549 
Development Data 0.652 0.492 0.561 
Table 4: Comparison of Training Setup, Simple-Exp 
In table 4, the first line is experiment without 
any negative instances from non-event pronouns. 
The second line is the performance with all nega-
tive instances from non-event pronouns. Third 
line is performance using a balanced training set 
using down sampling. The last line is experiment 
using hyper-plane adjustment. The first line 
gives the highest recall measure because it has no 
discriminative knowledge on non-event anaphor-
ic pronoun. The second line yields the highest 
precision which complies with our claim that 
including negative instances from non-event 
pronouns will improve precision of the classifier 
because more discriminative power is given by 
non-event pronoun instances. The balanced train-
ing set achieves a better f-score comparing to 
models with no/all negative instances. This is 
because balanced training set provides a better 
weighted positive/negative instances which im-
plies a balanced positive/negative knowledge 
representation. As a result of that, we achieve a 
better balanced f-score. In (Ng and Cardie, 
2002b), they concluded that only the negative 
instances in between the anaphor and antecedent 
are useful in the resolution. It is same as our 
strategy without negative instances from non-
event anaphoric pronouns. However, our study 
showed an improvement by adding in negative 
instances from non-event anaphoric pronouns as 
showed in table 4. This is probably due to our 
random sampling strategy over the negative in-
stances near to the event anaphoric instances. It 
empowers the system with more discriminative 
power. The best performance is given by the hy-
per-plane adaptation model. Although the num-
ber of training instances is further reduced for 
development data, we can have an adjustment of 
the hyper-plane which is more fit to dataset.  
In the last set of experiments, we will present 
the performance from the twin-candidates based 
approach in table 5. The first line is the best per-
formance from single candidate system with hy-
per-plane adaptation. The second line is perfor-
mance using the twin-candidates approach. 
Simple-Exp Tree Precision Recall F-score 
Single Candidate 0.652 0.492 0.561 
Twin-Candidates 0.626 0.540 0.579 
Table 5: Single vs. Twin Candidates, Simple-Exp 
Comparing to the single candidate model, the 
recall is significantly improved with a small 
trade-off in precision. The difference in results is 
statistically significant using t-test at 5% level of 
significance. It reinforced our intuition that pre-
ferences between two candidates are contributive 
information sources in co-reference resolution.  
5 Conclusion and Future Work 
The purpose of this paper is to conduct a syste-
matic study of the event pronoun resolution. We 
propose a resolution system utilizing a set of flat 
positional, lexical and syntactic feature and 
structural syntactic feature. The state-of-arts 
convolution tree kernel is used to extract indica-
tive structural syntactic knowledge. A twin-
candidates preference learning based approach is 
incorporated to reinforce the resolution system 
with candidates? preferences knowledge. Last but 
not least, we also proposed a study of the various 
incorporations of negative training instances, 
specially using random sampling to handle the 
imbalanced data. Development data is also used 
to select more accurate hyper-plane in SVM for 
better determination. 
To further our research work, we plan to em-
ploy more semantic information into the system 
such as semantic role labels and verb frames.  
Acknowledgment 
We would like to thank Professor Massimo Poesio 
from University of Trento for the initial discussion of 
this work. 
195
References  
N. Asher. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Publisher. 1993. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer.1995. 
M. Kubat and S. Matwin, 1997. Addressing the curse 
of imbalanced data set: One sided sampling. In 
Proceedings of the Fourteenth International Con-
ference on Machine Learning,1997. pg179?186. 
T. Joachims. 1999. Making large-scale svm learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.1999. 
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun 
phrases. In Computational Linguistics, Vol:27(4), 
pg521? 544. 
D. Byron. 2002. Resolving Pronominal Reference to 
Abstract Entities, in Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02). July 2002. , USA  
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?02). July 
2002. , USA 
V. Ng and C. Cardie. 2002a. Improving machine 
learning approaches to coreference resolution. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?02). 
July 2002. , USA. pg104?111. 
V. Ng, and C. Cardie. 2002b. Identifying anaphoric 
and non-anaphoric noun phrases to improve core-
ference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING02). (2002) 
M. Strube and C. M?ller. 2003. A Machine Learning 
Approach to Pronoun Resolution in Spoken Dialo-
gue. . In Proceedings of the 41st Annual Meeting of 
the Association for Computational Linguistics 
(ACL?03), 2003 
D. Klein and C. Manning. 2003a. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
D. Klein and C.Manning. 2003b. Accurate Unlexica-
lized Parsing. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?03), 2003.  pg423-430. 
X. Yang, G. Zhou, J. Su, and C.Tan. 2003. Corefe-
rence Resolution Using Competition Learning Ap-
proach. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics 
(ACL?03), 2003. pg176?183. 
A. Moschitti. 2004. A study on convolution kernels 
for shallow semantic parsing. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL?04), pg335?342. 
A. Estabrooks, T. Jo, and N. Japkowicz. 2004. A mul-
tiple resampling method for learning from imba-
lanced data sets. In Computational Intelligence  
Vol:20(1). pg18?36. 
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improving 
pronoun resolution by incorporating coreferential 
information of candidates. In Proceedings of 42th 
Annual Meeting of the Association for Computa-
tional Linguistics, 2004. pg127?134. 
X. Yang, J. Su and C.Tan. 2005a. Improving Pronoun 
Resolution Using Statistics-Based Semantic Com-
patibility Information. In Proceedings of Proceed-
ings of the 43rd Annual Meeting of the Association 
for Computational Linguistics (ACL?05). June 
2005.  
X. Yang, J. Su and C.Tan. 2005b. A Twin-Candidates 
Model for Coreference Resolution with Non-
Anaphoric Identification Capability. In Proceed-
ings of IJCNLP-2005. Pp. 719-730, 2005 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. 
Weischedel. 2006. OntoNotes: The 90\% Solution. 
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, 2006 
X. Yang, J. Su and C.Tan. 2006. Kernel-Based Pro-
noun Resolution with Structured Syntactic Know-
ledge. In Proceedings of the 44th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?06). July 2006. Australia. 
A. Moschitti, Making tree kernels practical for natural 
language learning. In Proceedings EACL 2006, 
Trento, Italy, 2006. 
C. M?ller. 2007. Resolving it, this, and that in unre-
stricted multi-party dialog. In Proceedings of the 
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL?07). 2007.  Czech Re-
public. pg816?823. 
X. Yang, J. Su and C.Tan. 2008. A Twin-Candidates 
Model for Learning-Based Coreference Resolution. 
In Computational Linguistics, Vol:34(3). pg327-
356. 
S. Pradhan, L. Ramshaw, R. Weischedel, J. Mac-
Bride, and L. Micciulla. 2007. Unrestricted Corefe-
rence: Identifying Entities and Events in Onto-
Notes. In Proceedings of the IEEE International 
Conference on Semantic Computing (ICSC), Sep. 
2007. 
196
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1047?1055,
Beijing, August 2010
Discriminative Induction of Sub-Tree Alignment                                 
using Limited Labeled Data 
 
        Jun Sun1,2                             Min Zhang1                      Chew Lim Tan2 
1Institute for Infocomm Research      2School of Computing, National University of Singapore     
        sunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sg  
 
Abstract 
We employ Maximum Entropy model to con-
duct sub-tree alignment between bilingual 
phrasal structure trees. Various lexical and 
structural knowledge is explored to measure the 
syntactic similarity across Chinese-English bi-
lingual tree pairs. In the experiment, we evalu-
ate the sub-tree alignment using both gold 
standard tree bank and the automatically parsed 
corpus with manually annotated sub-tree align-
ment. Compared with a heuristic similarity 
based method, the proposed method significant-
ly improves the performance with only limited 
sub-tree aligned data. To examine its effective-
ness for multilingual applications, we further at-
tempt different approaches to apply the sub-tree 
alignment in both phrase and syntax based SMT 
systems. We then compare the performance 
with that of the widely used word alignment. 
Experimental results on benchmark data show 
that sub-tree alignment benefits both systems by 
relaxing the constraint of the word alignment. 
1 Introduction 
Recent research in Statistical Machine Translation 
(SMT) tends to incorporate more linguistically 
grammatical information into the translation mod-
el known as linguistically motivated syntax-based 
models. To develop such models, the phrasal 
structure parse tree is usually adopted as the repre-
sentation of bilingual sentence pairs either on the 
source side (Huang et al, 2006; Liu et al, 2006) 
or on the target side (Galley et al, 2006; Marcu et 
al., 2006), or even on both sides (Graehl and 
Knight, 2004; Zhang et al, 2007). Most of the 
above models either construct a pipeline to trans-
form from/to tree structure, or synchronously gen-
erate two trees in parallel (i.e., synchronous pars-
ing). Both cases require syntactically rich transla-
tional equivalences to handle non-local reordering. 
However, most current works obtain the syntactic 
translational equivalences by initially conducting 
alignment on the word level. To employ word 
alignment as a hard constraint for rule extraction 
has difficulty in capturing such non-local phenom-
ena and will fully propagate the word alignment 
error to the later stage of rule extraction. 
Alternatively, some initial attempts have been 
made to directly conduct syntactic structure 
alignment. As mentioned in Tinsley et al (2007), 
the early work usually constructs the structure 
alignment by hand, which is time-consuming. Re-
cent research tries to automatically align the bilin-
gual syntactic sub-trees. However, most of these 
works suffer from the following problems. Firstly, 
the alignment is conducted based on heuristic 
rules, which may lose extensibility and generality 
in spite of accommodating some common cases 
(Groves et al, 2004). Secondly, various similarity 
computation methods are used based merely on 
lexical translation probabilities (Tinsley et al, 
2007; Imamura, 2001) regardless of structural fea-
tures. We believe the structure information is an 
important issue to capture the non-local structural 
divergence of languages by modeling beyond the 
plain text.  
To address the above issues, we present a statis-
tical framework based on Maximum Entropy 
(MaxEnt) model. Specifically, we consider sub-
tree alignment as a binary classification problem 
and use Maximum Entropy model to classify each 
instance as aligned or unaligned. Then, we per-
form a greedy search within the reduced search 
space to conduct sub-tree alignment links based on 
the alignment probabilities obtained from the clas-
sifier. 
Unlike the previous approaches that can only 
measure the structural divergence via lexical fea-
tures, our approach can incorporate both lexical 
and structural features. Additionally, instead of 
explicitly describing the instances of sub-tree pairs 
as factorized sub-structures, we frame most of our 
features as score based feature functions, which 
helps solve the problem using limited sub-tree 
alignment annotated data. To train the model and 
evaluate the alignment performance, we adopt 
1047
HIT Chinese-English parallel tree bank for gold 
standard evaluation. To explore its effectiveness in 
SMT systems, we also manually annotate sub-tree 
alignment on automatically parsed tree pairs and 
perform the noisy data evaluation. Experimental 
results show that by only using limited sub-tree 
aligned data of both corpora, the proposed ap-
proach significantly outperforms the baseline 
method (Tinsley et al, 2007). The proposed fea-
tures are very effective in modeling the bilingual 
structural similarity. We further apply the sub-tree 
alignment to relax the constraint of word align-
ment for both phrase and syntax based SMT sys-
tems and gain an improvement in BLEU. 
2 Problem definition  
A sub-tree alignment process pairs up the sub-
trees across bilingual parse trees, whose lexical 
leaf nodes covered are translational equivalent, i.e., 
sharing the same semantics.  Grammatically, the 
task conducts links between syntactic constituents 
with the maximum tree structures generated over 
their word sequences in bilingual tree pairs.  
In general, sub-tree alignment can also be inter-
preted as conducting multiple links across internal 
nodes between sentence-aligned tree pairs as 
shown in Fig. 1. The aligned sub-tree pairs usually 
maintain a non-isomorphic relation with each oth-
er especially for higher layers. We adapt the same 
criteria as Tinsley et al (2007) in our study: 
(i) a node can only be linked once; 
(ii) descendants of a source linked node may 
only link to descendants of its target 
linked counterpart; 
(iii) ancestors of a source linked node may on-
ly link to ancestors of its target linked 
counterpart. 
where the term ?node? refers to root of a sub-
tree, which can be used to represent the sub-tree. 
3 Model  
We solve the problem as binary classification and 
employ MaxEnt model with a greedy search.  
Given a bilingual tree pair    and   ,    
{            } is the source tree consisting of   
sub-trees?where   is also the number of nodes in 
the source tree        {            } is the tar-
get tree consisting of   sub-trees, where   is also 
the number of nodes in the target tree   . 
For each sub-tree pair         in the given bilin-
gual parse trees         , the sub-tree alignment 
probability is given by: 
      ( |       )  
   [?     
 
   (        )]
?    [?     
 
   ( 
        )]  
   (1) 
where 
                 {
           (     )            
                               
                     (2) 
 
Feature functions are defined in a quadruple 
(         ).   is an additional variable to incorpo-
rate new dependencies other than the sub-tree 
pairs. For each feature function   (         ), a 
weight    is applied to tailor the distribution. 
After classifying the candidate sub-tree pairs as 
aligned or unaligned, we perform a greedy search 
within the reduced search space to conduct sure 
links based on the conditional probability 
  ( |       )  obtained from the classifier. The 
alignment probability is independently normalized 
for each sub-tree pair and hence suitable as a 
searching metric. 
The greedy search algorithm can be described 
as an automaton. A state in the search space is a 
partial alignment with respect to the given bilin-
gual tree pair. A transition is to add one more link 
of node pairs to the current state. The initial state 
has no link. The terminal state is a state where no 
more links can be added according to the defini-
tion in Section 2. We use greedy search to gener-
ate the best-links at the early stage. There are cas-
es that the correctly-aligned tree pairs have very 
few links, while we have a bunch of candidates 
with lower alignment probabilities. However, the 
sum of the lower probabilities is larger than that of 
the correct links?, since the number of correct 
links is much fewer. This makes the alignment 
results biased to be with more links. The greedy 
search helps avoid this asymmetric problem.  
4 Feature Functions 
In this section, we introduce a variety of feature 
functions to capture the semantically equivalent 
S
VBA
?
(NULL)
?
(me)
?
(give)
??
(pen)
?
(.)
P WJRVGNG
VO
Give topenthe me .
VBP DT NN TO PRP PUNC.
NP PP
VP
S
Ts:
Tt:
 
Figure 1: Sub-tree alignment as referred to  
Node alignment 
1048
counterparts and structural divergence across lan-
guages. For the semantic equivalence, we define 
lexical and word alignment feature functions. 
Since those feature functions are directional, we  
describe most of these functions as conditional 
feature functions based on the conditional lexical 
probabilities. We also introduce the tree structural 
features to deal with the structural divergence of 
bilingual parse trees. Inspired by Burkett and 
Klein (2008), we introduce the feature functions in 
an internal-external manner based on the fact that 
the feature scores for an aligned sub-tree pair tend 
to be high inside both sub-trees, while they tend to 
be low inside one sub-tree and outside the other. 
4.1 Internal Lexical Features  
We use this feature to measure the degree of se-
mantic equivalence of the sub-tree pair. According 
to the definition of sub-tree alignment in Section 2, 
the word sequence covered by the sub-tree pair 
should be translational equivalence. Therefore, the 
lexicons within the two corresponding sub-spans 
should be highly related in semantics. We define 
the internal lexical features as follows: 
 (  |  )  (? ?                       )
 
   (  )   
 (  |  )  (? ?                       )
 
   (  )   
where        refers to the lexical translation 
probability from the source word   to the target 
word   within the sub-tree spans, while        
refers to that from target to source;        refers to 
the word set for the internal span of the source 
sub-tree   , while   (  ) refers to that of the target 
sub-tree   . 
4.2 Internal-External Lexical Features  
Intuitively, lexical translation probabilities tend to 
be high within the translational equivalence, while 
low within the non-equivalent counterparts. Ac-
cording to this, we define the internal-external lex-
ical feature functions as follows: 
 
 (  |  )  
?          (  )
{(             )
 
 }
    (  )
|  (  )|
  
 (  |   )  
?    
      (  )
{(             )
 
 }    (  )
        
  
 
where         refers to the word set for the ex-
ternal span of the source sub-tree   , while         
refers to that of the target sub-tree   . We choose a 
representation different from the internal lexical 
feature scores, since for cases with small inner 
span and large outer span, the sum of internal-
external scores may be overestimated. As a result, 
we change the sum operation into max, which is 
easy to be normalized. 
4.3 Internal Word Alignment Features  
Although the word alignment information within 
bilingual sentence pairs is to some extent not reli-
able, the links of word alignment account much 
for the co-occurrence of the aligned terms. We 
define the internal word alignment features as fol-
lows: 
 (     )  
? ?        (             )
 
 
    (  )    (  )
(         |  (  )|)
 
 
  
where 
                 {
                             
                                 
 
 
The binary function        is introduced to 
trigger the computation only when a word aligned 
link exists for the two words       within the sub-
tree span. 
4.4 Internal-External Word Alignment Fea-
tures  
Similar to lexical features, we also introduce in-
ternal-external word alignment features as follows: 
 (     )  
? ?        (             )
 
 
             (  )
(          |  (  )|)
 
 
 
 
 (     )  
? ?        (             )
 
 
             (  )
(         |   (  )|)
 
 
 
where 
                 {
                             
                                  
 
4.5 Tree Structural Features 
In addition to the lexical correspondence, we also 
capture the structural divergence by introducing 
the tree structural features as follows: 
Span difference: Translational equivalent sub-
tree pairs tend to share similar length of spans. 
Thus the model will penalize the candidate sub-
tree pairs with largely different length of spans. 
 
 (     )  |
        
                  
 
   (  ) 
                   
|  
 
Number of Descendants: Similarly, the num-
ber of the root?s descendants of the aligned sub-
trees should also correspond. 
 
 (     )  |
       
                 
 
  (  ) 
                 
|  
1049
where      refers to the descendant set of the 
root to an individual sub-tree. 
Tree Depth difference: Intuitively, translation-
al equivalent sub-tree pairs tend to have similar 
depth from the root node of the parse tree. We can 
further allow the model to penalize the candidate 
sub-tree pairs with different distance from the root 
node. 
 (     )  |
         
      (  )
 
     (  )
      (  )
|  
4.6 Binary Grammatical Features 
In the previous sections, we design some score 
based feature functions to describe syntactic tree 
structural similarities, rather than directly using 
the substructures. This is because for limited anno-
tated tree alignment data, features like tokens and 
grammar rules are rather sparse. In spite of this, 
we still have a closed set of grammatical tags 
which can be covered by a small amount of data. 
Therefore, we use the combination of root gram-
mar tags of the sub-tree pairs as binary features. 
5 Training 
We train the sub-tree alignment model in two 
steps:  
Firstly, we learn the various feature functions. 
On one hand, GIZA++ is offline trained on a large 
amount of bilingual sentences to compute the lexi-
cal and word alignment features. On the other 
hand, the tree structural features, similar to word 
and phrase penalty features in phrase based SMT 
models, are computed online for both training and 
testing. 
Secondly, we train the MaxEnt model in Eq. 1, 
using the training corpus which consists of the 
bilingual parse tree pairs with manually annotated 
sub-tree alignment. We apply the widely used GIS 
(Generalized Iterative Scaling) algorithm (Darroch 
and Ratcliff, 1972) to optimize   
 . In practice, we 
modify Och?s implementation YASMET. 
Since we consider each sub-tree pair as an indi-
vidual instance, it is easy to see that the negative 
samples heavily overwhelm the positive ones. For 
GIS training, such a skewed distribution easily 
drives the parameters to facilitate the negative in-
stances. We address this problem by giving more 
weight to the positive training instances.  
6 Experiments on Sub-Tree Alignments 
We utilize two different corpora to evaluate the 
proposed sub-tree alignment method and its capa-
bility to plug in the related applications respective-
ly. One is HIT English Chinese parallel tree bank 
with both tree structure and sub-tree alignment 
manually annotated. The other is the automatically 
parsed bilingual tree pairs (allowing minor parsing 
errors) with manually annotated sub-tree align-
ment. The latter benefits MT task, since most lin-
guistically motivated syntax SMT systems require 
a held-out automatic parser to achieve rule induc-
tion. 
6.1 Data preparation 
For the gold standard corpus based experiment, we 
use HIT 1  Chinese-English parallel tree bank, 
which is collected from English learning text 
books in China as well as example sentences in 
dictionaries. It consists of 16131 gold standard 
parse tree pairs with manually annotated sub-tree 
alignments. The annotation strictly preserves the 
semantic equivalence, i.e., it only conducts sure 
links in the internal node level, while ignoring 
possible links adopted in word alignment. In con-
trast, in the POS level, n-to-n links are allowed in 
annotation. In order to be consistent with the defi-
nition in Section 2, we delete those n-to-n links in 
POS level. The word segmentation, tokenization 
and parse-tree in the corpus are manually con-
structed or checked. The Chinese parse tree in HIT 
tree bank adopts a different annotation criterion 
from the Penn TreeBank annotation, which is de-
signed by the HIT research team. The new criteri-
on can better facilitate the description of some rare 
structural phenomena in Chinese. The English 
parse tree still uses Penn TreeBank annotation. 
The statistics of HIT corpus is shown in Table 1. 
 
 Chinese English 
# of Sentence pair 16131 
Avg. Sentence Length 13.06 13.00 
Avg. # of sub-tree 21.60 23.74 
Avg. # of alignment 11.71 
 
Table 1. Statistics for HIT gold standard Tree bank  
 
Since the induction of sub-tree alignment is de-
signed to benefit the machine translation modeling, 
it is preferable to conduct the sub-tree alignment 
experiment on the corpus for MT evaluation. 
However, most syntax based SMT systems use an 
automatic parser to facilitate training and decoding, 
which introduces parsing errors. Additionally, the 
gold standard HIT corpus is not applicable for MT 
                                                 
1  HIT corpus is designed and constructed by HIT mitlab. 
http://mitlab.hit.edu.cn/index.php/resources.html .  We li-
censed the corpus from them for research usage. 
1050
experiment due to problems of domain divergence, 
annotation discrepancy (Chinese parse tree adopts 
a different grammar from Penn Treebank annota-
tions) and degree of tolerance for parsing errors. 
Due to the above issues, we annotate a new data 
set to apply the sub-tree alignment in machine 
translation. We randomly select 300 bilingual sen-
tence pairs from the Chinese-English FBIS corpus 
with the length     in both the source and target 
sides. The selected plain sentence pairs are further 
parsed by Stanford parser (Klein and Manning, 
2003) on both the English and Chinese sides. We 
manually annotate the sub-tree alignment for the 
automatically parsed tree pairs according to the 
definition in Section 2. To be fully consistent with 
the definition, we strictly preserve the semantic 
equivalence for the aligned sub-trees to keep a 
high precision. In other words, we do not conduct 
any doubtful links. The corpus is further divided 
into 200 aligned tree pairs for training and 100 for 
testing. Some initial statistic of the automatically 
parsed corpus is shown in Table 2. 
6.2 Baseline approach 
We implement the work in Tinsley et al (2007) as 
our baseline methodology. 
Given a tree pair        , the baseline ap-
proach first takes all the links between the sub-tree 
pairs as alignment hypotheses, i.e., the Cartesian 
product of the two sub-tree sets: 
{            }  {            } 
 By using the lexical translation probabilities, 
each hypothesis is assigned an alignment score. 
All hypotheses with zero score are pruned out. 
Then the algorithm iteratively selects the link of 
the sub-tree pairs with the maximum score as a 
sure link, and blocks all hypotheses that contradict 
with this link and itself, until no non-blocked hy-
potheses remain. 
The baseline system uses many heuristics in 
searching the optimal solutions with alternative 
score functions. Heuristic skip1 skips the tied hy-
potheses with the same score, until it finds the 
highest-scoring hypothesis with no competitors of 
the same score. Heuristic skip2 deals with the 
same problem. Initially, it skips over the tied hy-
potheses. When a hypothesis sub-tree pair          
without any competitor of the same score is found, 
where neither    nor    has been skipped over, the 
hypothesis is chosen as a sure link. Heuristic 
span1 postpones the selection of the hypotheses 
on the POS level. Since the highest-scoring hy-
potheses tend to appear on the leaf nodes, it may 
introduce ambiguity when conducting the align-
ment for a POS node whose child word appears 
twice in a sentence. 
The baseline method proposes two score func-
tions based on the lexical translation probability. 
They also compute the score function by splitting 
the tree into the internal and external components. 
Tinsley et al (2007) adopt the lexical transla-
tion probabilities dumped by GIZA++ (Och and 
Ney, 2003) to compute the span based scores for 
each pair of sub-trees. Although all of their heuris-
tics combinations are re-implemented in our study, 
we only present the best result among them with 
the highest Recall and F-value as our baseline, 
denoted as skip2_s1_span12. 
6.3 Experimental settings 
? To examine the effectiveness of the proposed 
features, we  
    (1) learn the word alignment using the combina-
tion of the 14k of HIT tree bank and FBIS (240k) 
corpus for both our approach and the baseline 
method, and divide the remaining HIT corpus as 
1k for training and 1k for testing. 
    (2) learn the word alignment on the entire FBIS 
training corpus (240k) for both our approach and 
the baseline method. We then train and test on 
FBIS corpus of 200 and 100 respectively as stated 
in Table 2. 
? In our task, annotating large amount of sub-tree 
alignment corpus is time consuming and more dif-
ficult compared with the tasks like sequence label-
ing. One of the important issues we are concerned 
about is whether we can achieve an acceptable 
performance with limited training data. We  
    (3) adopt the entire FBIS data (240k) to learn 
the word alignment and various amount of HIT 
gold standard corpus to train the MaxEnt model. 
Then we test the alignment performance on the 
same HIT test set (1k) as (1). 
                                                 
2 s1 denotes score function 1 in Tinsley et al (2007) 
  Chinese English 
 # of Sentence pair 200 
Train Avg. Sentence Length 17 20.84 
 Avg. # of sub-tree 28.87 34.54 
 Avg. # of alignment 17.07 
Test # of Sentence pair 100 
 Avg. Sentence Length 16.84 20.75 
 Avg. # of sub-tree 29.18 34.1 
 Avg. # of alignment 17.75 
 
Table 2. FBIS selected Corpus Statistics 
1051
? We further test the robustness of our method 
under different amount of data to learn the lexical 
and word alignment feature functions. We gradu-
ally change the amount of FBIS corpus to train the 
word alignment. Then we  
    (4) use the same training (1k) and testing data 
(1k) with (1);  
    (5) use FBIS corpus 200 to train MaxEnt model 
and 100 for testing similar to (2). 
6.4 Experimental results 
We use Precision, Recall and F-score to measure 
the alignment performance and obtain the results 
as follows: 
? In Table 3 and 4 for Exp (1) and (2) respectively, 
we show that by incrementally adding new fea-
tures in a certain order, the F-value consistently 
increases and both outperform the baseline method. 
From both tables, we find that the Binary fea-
tures, with the combination of root grammar tags 
of the sub-tree pairs, significantly improve the 
alignment performance. We also try the different 
combinations of the parent, child or even siblings 
to the root nodes. However, all these derivative 
configurations decrease the performance. We at-
tribute the ineffectiveness to data sparseness. Fur-
ther exploration suggests that the binary feature in 
HIT gold standard corpus exhibits a substantially 
larger improvement against other features than 
FBIS corpus (Table 3 against Table 4). The reason 
could be that the grammar tags in the gold stand-
ard corpus are accurate, while FBIS corpus suffers 
from parsing errors. Apart from that, the lexi-
cal/word-alignment features in Table 3 do not per-
form well, since the word alignment is trained 
mainly on the cross domain FBIS corpus. This is 
also an important reason why there is a large gap 
in performance between Table 3 and 4, where the 
automatic parsed FBIS corpus performs better 
than HIT gold standard tree bank in all configura-
tions as well as the baseline. 
? In Fig. 2(a) for Exp (3), we examine perfor-
mance under different amount of training data 
from 1k to 15k. The results change very little with 
over the amount of 1k. Even with only 0.25k train-
ing data, we are able to gain a result close to the 
best performance. This suggests that by utilizing 
only a small amount of sub-tree aligned corpus, 
we can still achieve a satisfactory alignment result. 
The benefits come from the usage of the score 
based feature functions by avoiding using sub-
structures as binary features, which suffers from 
the data sparseness problem.  
? In Fig. 2(b-e) for Exp (4&5), we find that in-
creasing the amount of corpus to train GIZA++ 
does not improve much for the proposed method 
on both HIT gold standard corpus (Fig. 2: b, c) 
and the automatic parsed data (Fig. 2: d, e). This is 
due to the various kinds of features utilized by the 
MaxEnt model, which does not bet on the lexical 
and word alignment feature too much. As for the 
baseline method, we can only detect a relatively 
large improvement in the initial increment of cor-
pus, while later additions do not help. This result 
suggests that the baseline method is relatively less 
extensible since it works completely on the lexical 
similarities which can be only learned from the 
word alignment corpus.  
7 Experiments on Machine Translation 
In addition to the alignment evaluation, we con-
duct MT evaluation as well. We explore the effec-
tiveness of sub-tree alignment for both phrase and 
linguistically motivated syntax based systems. 
7.1 Experimental configuration 
In the experiments, we train the translation model 
on FBIS corpus (7.2M (Chinese) + 9.2M (English) 
words in 240,000 sentence pairs) and train a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002). We use these 
Features Precision Recall F-value 
   In Lexical 50.96 48.11 49.49 
+ InOut Lexical 55.26 53.84 54.54 
+ In word align 56.16 60.59 58.29 
+ InOut word align 55.80 62.25 58.85 
+ Tree Structure  57.64 63.11 60.25 
+ Binary Feature 73.14 85.11 78.67 
 Baseline [Tinsley 2007] 64.14 66.99 65.53 
 
Table 3. Sub-tree alignment of different feature  
combination for HIT gold standard test set 
Features Precision Recall F-value 
   In Lexical 63.53 54.87 58.88 
+ InOut Lexical 66.00 63.66 64.81 
+ In word align 70.89 75.88 73.30 
+ InOut word align 72.05 80.16 75.89 
+ Tree Structure  72.03 80.95 76.23 
+ Binary Feature 76.08 85.29 80.42 
  Baseline [Tinsley 2007] 70.48 78.70 74.36 
 
Table 4. Sub-tree alignment of different  
feature combination for FBIS test set 
1052
sentences with less than 50 characters from the 
NIST MT-2002 test set as the development set (to 
speed up tuning for syntax based system) and the 
NIST MT-2005 test set as our test set. We use the 
Stanford parser (Klein and Manning, 2003) to 
parse bilingual sentences on the training set and 
Chinese sentences on the development and test set. 
The evaluation metric is case-sensitive BLEU-4. 
For the phrase based system, we use Moses 
(Koehn et al 2007) with its default settings. For 
the syntax based system, since sub-tree alignment 
can directly benefit Tree-2-Tree based systems, 
we apply the sub-tree alignment in an SMT system 
based on Synchronous Tree Substitution Grammar 
(STSG) (Zhang et al, 2007). The STSG based 
decoder uses a pair of elementary tree as a basic 
translation unit. Recent research on tree based sys-
tems shows that relaxing the restriction from tree 
structure to tree sequence structure (Synchronous 
Tree Sequence Substitution Grammar: STSSG) 
significantly improves the translation performance 
(Zhang et al, 2008). We implement the 
STSG/STSSG based model in Pisces decoder with 
the same features and settings in Sun et al (2009). 
The STSSG based decoder translates each span 
iteratively in a bottom up manner which guaran-
tees that when translating a source span, any of its 
sub-spans has already been translated. The STSG 
based experiment can be easily achieved by re-
stricting the translation rule set in the STSSG de-
coder to be elementary tree pairs only. 
For the alignment setting of the baselines, we 
use the word alignment trained on the entire 
FBIS(240k) corpus by GIZA++ with heuristic 
grow-diag-final for Moses and the syntax systems 
and perform rule extraction constrained on the 
word alignment. As for the experiments adopting 
sub-tree alignment, we use the above word align-
ment to learn lexical/word alignment features, and 
train the sub-tree alignment model with FBIS 
training data (200).  
7.2 Experimental results 
Utilizing the syntactic rules only has been argued 
to be ineffective (Koehn et al, 2003). Therefore, 
instead of using the sub-tree aligned rules only, we 
try to improve the word alignment constrained 
rule set by sub-tree alignment as shown in Table 5.  
Firstly, we try to Directly Concatenate (DirC) 
the sub-tree alignment constraint rule set3 to the 
original syntax/phrase rule set based on word 
alignment. Then we re-train the MT model based 
                                                 
3 For syntax based system, it?s just the sub-tree pairs deducted 
from the sub-tree alignment; for phrase based system, it's the 
phrases with context equivalent to the aligned sub-tree pairs. 
 
                                    a                                                                                        b                                                                                     c 
 
             d         e 
 
Figure 2: a. Precision/Recall/F-score for various amount of training data (k).  
b~e. Various amount of data to train word alignment 
b. Precision/Recall for HIT test set. c. F-score for HIT test set.  
d. Precision/Recall for FBIS test set. e. F-score for FBIS test set. 
0.25 1 3 5 10 15
0.5
0.6
0.7
0.8
0.9
Precision
Recall
F-value
0 50 100 150 200 250
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
Recall
Baseline-Precision
Baseline-Recall
0 50 100 150 200 250
0.5
0.6
0.7
0.8
0.9
1
F-valule
Baseline-F-value
0 50 100 150 200 250
0.5
0.6
0.7
0.8
0.9
1
Precision
Recall
Baseline-Precision
Baseline-Recall
0 50 100 150 200 250
0.5
0.6
0.7
0.8
0.9
1
F-valule
Baseline-F-value
1053
on the obtained rule set. Tinsley et al (2009) at-
tempts different duplication of sub-tree alignment 
constraint rule set to append to the original phrase 
rule set and reports positive results. However, as 
shown in Table 5, we only achieve very minor 
improvement (in STSSG based model the score 
even drops) by direct introducing the new rules.  
Secondly, we propose a new approach to utilize 
sub-tree alignment by modifying the rule extrac-
tion process. We allow the bilingual phrases which 
are consistent with Either Word alignment or Sub-
tree alignment (EWoS) instead of to be consistent 
with word alignment only. The results in Table 5 
show that EWoS achieves consistently better per-
formance than the baseline and DirC method. We 
also find that sub-tree alignment benefits the 
STSSG based model less compared with other 
systems. This is probably due to the fact that the 
STSSG based system relies much on the tree se-
quence rules. 
To benefit intuitive understanding, we provide 
two alignment snippets in the MT training corpus 
in Fig. 3, where the red lines across the non-
terminal nodes are the sub-tree aligned links con-
ducted by our model, while the purple lines across 
the terminal nodes are the word alignment links 
trained by GIZA++. In the first example, the word 
Israel is wrongly aligned to two ?????s by 
GIZA++, where the wrong link is denoted by the 
dash line. This is common, since in a compound 
sentence in English, the entities appeared more 
than once are often replaced by pronouns at its 
later appearances. Therefore, the syntactic rules 
constraint by NR1-NNP1, IP2-VP2 and PP3-VP3 
respectively cannot be extracted for syntax sys-
tems; while for phrase systems, context around the 
first ????? cannot be fully explored. In the 
second example, the empty word ??? is wrongly 
aligned, which usually occurs in Chinese-English 
word alignment. As shown in Fig. 3, both cases 
can be resolved by sub-tree alignment conducted 
by our model, indicating that sub-tree alignment is 
a decent supplement to the word alignment rule set.  
8 Conclusion 
In this paper, we propose a framework for bilin-
gual sub-tree alignment using Maximum Entropy 
model. We explore various lexical and structural 
features to improve the alignment performance. 
We also manually annotated the automatic parsed 
tree pairs for both alignment evaluation and MT 
experiment. Experimental results show that our 
alignment framework significantly outperforms 
the baseline method and the proposed features are 
very effective to capture the bilingual structural 
similarity. Additionally, we find that our approach 
can perform well using only a small amount of 
sub-tree aligned training corpus. Further experi-
ment shows that our approach benefits both phrase 
and syntax based MT systems. 
System Rules BLEU 
Moses BP* 23.86 
 DirC  24.12 
EWoS  24.45 
Syntax 
STSG 
STSG 24.71 
DirC  24.91 
 EWoS  25.21 
Syntax STSSG 25.92 
STSSG DirC  25.88 
 EWoS  26.12 
 
Table 5. MT evaluation on various systems 
BP* denotes bilingual phrases.  
BP, STSG, STSSG are baseline rule sets using word 
alignment to constrain rule extraction. 
 
S
1
:
T
1
:
VP
2
PP
3
TO
VP
3
P IP
2
CP VP
VP DEC AD VV
VV NR
1
?
(to)
??
(oppose)
???
(Israel)
?
(`s)
??
(illegal)
??
(occupation)
NP
NNNP
VB
JJ
NNP
1
POS
To oppose Israel `s illegal occupation
???
(Israel)
...
NP VP
NR NN VV AS
???
(Barak)
??
(government)
??
(choose)
?
(NULL)
NP
NNPDT NNP
the Barak Government
NNP
chose
S
2
:
T
2
:
 
Figure 3: Comparison between Sub-tree alignment results and Word alignment results  
 
1054
References  
David Burkett and Dan Klein. 2008. Two languages 
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP-08. 877-886. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and training of 
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL-06. 961-968. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. In Proceedings of HLT-NAACL-2004. 
105-112. 
Declan Groves, Mary Hearne, and Andy Way. 2004. 
Robust sub-sentential alignment of phrase-structure 
trees. In Proceedings of COLING-04, pages 1072-
1078. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical syntax-directed translation with extended 
domain of Locality. In Proceedings of AMTA-06. 
Kenji Imamura. 2001. Hierarchical Phrase Alignment 
Harmonized with Parsing. In Proceedings of NLPRS. 
377-384. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL-
03. 423-430. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open Source 
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL-07. 177-180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL-2003. 48-54. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignment template for statistical machine 
translation. In Proceedings of ACL-06, 609-616. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: statistical machine 
translation with syntactified target language 
phrases. In Proceedings of EMNLP-06. 44-52. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51, March. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP-
02. 901-904. 
Jun Sun, Min Zhang and Chew Lim Tan. 2009. A non-
contiguous Tree Sequence Alignment-based Model 
for Statistical Machine Translation. In Proceedings 
of ACL-IJCNLP-09. 914-922. 
John Tinsley, Ventsislav Zhechev, Mary Hearne, and 
Andy Way. 2007. Robust language pair-independent 
sub-tree alignment. In Proceedings of Machine 
Translation Summit-XI-07. 
John Tinsley, Mary Hearne, and Andy Way. 2009. 
Parallel treebanks in phrase-based statistical ma-
chine translation. In Proceedings of CICLING-09. 
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A tree-to-tree align-
ment-based model for statistical machine translation. 
In Proceedings of MT Summit-XI -07. 535-542. 
Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based tree-to-tree translation model. In 
Proceedings of ACL-08. 559-567. 
1055
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290?1298,
Beijing, August 2010
Entity Linking Leveraging
Automatically Generated Annotation 
Wei Zhang?    Jian Su? Chew Lim Tan?   Wen Ting Wang?
?School of Computing 
National University of Singapore 
{z-wei, tancl} 
@comp.nus.edu.sg
? Institute for Infocomm Research 
{sujian, wwang}
@i2r.a-star.edu.sg
Abstract
Entity linking refers entity mentions in a 
document to their representations in a 
knowledge base (KB). In this paper, we 
propose to use additional information 
sources from Wikipedia to find more 
name variations for entity linking task. In 
addition, as manually creating a training 
corpus for entity linking is labor-
intensive and costly, we present a novel 
method to automatically generate a large 
scale corpus annotation for ambiguous 
mentions leveraging on their unambi-
guous synonyms in the document collec-
tion. Then, a binary classifier is trained 
to filter out KB entities that are not simi-
lar to current mentions. This classifier 
not only can effectively reduce the am-
biguities to the existing entities in KB, 
but also be very useful to highlight the 
new entities to KB for the further popu-
lation. Furthermore, we also leverage on 
the Wikipedia documents to provide ad-
ditional information which is not availa-
ble in our generated corpus through a 
domain adaption approach which pro-
vides further performance improve-
ments.  The experiment results show that 
our proposed method outperforms the 
state-of-the-art approaches. 
1 Introduction 
The named entity (NE) ambiguation has raised 
serious problems in many areas, including web 
people search, knowledge base population 
(KBP), and information extraction, because an 
entity (such as Abbott Laboratories, a diversified 
pharmaceuticals health care company) can be 
referred to by multiple mentions (e.g. ?ABT? and 
?Abbott?), and a mention (e.g. ?Abbott?) can be 
shared by different entities (e.g. Abbott Texas: a 
city in United States; Bud Abbott, an American 
actor; and Abbott Laboratories, a diversified 
pharmaceutical health care company).  
Both Web People Search (WePS) task (Artiles 
et al 2007) and Global Entity Detection & Rec-
ognition task (GEDR) in Automatic Content Ex-
traction 2008 (ACE08) disambiguate entity men-
tions by clustering documents with these men-
tions. Each cluster then represents a unique enti-
ty. Recently entity linking has been proposed in 
this field. However, it is quite different from the 
previous tasks.
Given a knowledge base, a document collec-
tion, entity linking task as defined by KBP-091
(McNamee and Dang, 2009) is to determine for 
each name string and the document it appears, 
which knowledge base entity is being referred to, 
or if the entity is a new entity which is not 
present in the reference KB.  
Compared with GEDR and WePS, entity link-
ing has a given entity list (i.e. the reference KB) 
to which we disambiguate the entity mentions. 
Moreover, in document collection, there are new 
entities which are not present in KB and can be 
used for further population. In fact, new entities 
with or without the names in KB cover more 
than half of testing instances. 
1 http://apl.jhu.edu/~paulmac/kbp.html 
1290
Entity linking has been explored by several re-
searchers. Without any training data available, 
most of the previous work ranks the similarity 
between ambiguous mention and candidate enti-
ties through Vector Space Model (VSM). Since 
they always choose the entity with the highest 
rank as the answer, the ranking approaches hard-
ly detect a situation where there may be a new 
entity that is not present in KB. It is also difficult 
to combine bag of words (BOW) with other fea-
tures. For example, to capture the ?category? 
information, the method of Cucerzan (2007) in-
volves a complicated optimization issue and the 
approach has to be simplified for feasible com-
putation, which compromises the accuracy.  Be-
sides unsupervised methods, some supervised 
approaches (Agirre et al 2009, Li et al 2009 and 
McNamee et al 2009) also have been proposed 
recently for entity linking. However, the super-
vised approaches for this problem require large 
amount of training instances. But manually 
creating a corpus is labor-intensive and costly.  
In this paper, we explore how to solve the enti-
ty linking problem. We present a novel method 
that can automatically generate a large scale 
corpus for ambiguous mentions leveraging on 
their unambiguous synonyms in the document 
collection.  A binary classifier based on Support 
Vector Machine (SVM) is trained to filter out 
some candidate entities that are not similar to 
ambiguous mentions. This classifier can effec-
tively reduce the ambiguities to the existing enti-
ties in KB, and it is very useful to highlight the 
new entities to KB for the further population. 
We also leverage on the Wikipedia documents to 
provide additional information which is not 
available in our generated corpus through a do-
main adaption approach which provides further 
performance improvements. Besides, more in-
formation sources for finding more variations 
also contribute to the overall 22.9% accuracy 
improvements on KBP-09 test data over baseline. 
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work for entity 
linking. In Section 3 we detail our algorithm in-
cluding name variation and entity disambigua-
tion. Section 4 describes the experimental setup 
and results. Finally, Section 5 concludes the pa-
per.
2 Related Work 
The crucial component of entity linking is the 
disambiguation process. Raphael et al (2007) 
report a disambiguation algorithm for geography. 
The algorithm ranks the candidates based on the 
manually assigned popularity scores in KB. The 
class with higher popularity will be assigned 
higher score. It causes that the rank of entities 
would never change, such as Lancaster (Califor-
nia) would always have a higher rank than Lan-
caster (UK) for any mentions. However, as the 
popularity scores for the classes change over 
time, it is difficult to accurately assign dynamic 
popularity scores. Cucerzan (2007) proposes a 
disambiguation approach based on vector space 
model for linking ambiguous mention in a doc-
ument with one entity in Wikipedia. The ap-
proach ranks the candidates and chooses the ent-
ity with maximum agreement between the con-
textual information extracted from Wikipedia 
and the context of a document, as well as the 
agreement among the category tags associated 
with the candidate entities. Nguyen and Cao 
(2008) refer the mentions in a document to KIM 
(Popov et al 2004) KB. KIM KB is populated 
with over 40,000 named entities. They represent 
a mention and candidates as vectors of their con-
textual noun phrase and co-occurring NEs, and 
then the similarity is determined by the common 
terms of the vectors and their associated weights. 
For linking mentions in news articles with a Wi-
kipedia-derived KB (KBP-09 data set), Varma et 
al. (2009) rank the entity candidates using a 
search engine. Han and Zhao (2009) rank the 
candidates based on BOW and Wikipedia se-
mantic knowledge similarity. 
All the related work above rank the candidates 
based on the similarity between ambiguous men-
tion and candidate entities. However, the ranking 
approach hardly detects the new entity which is 
not present in KB. 
Some supervised approaches also have been 
proposed. Li et al (2009) and McNamee et al 
(2009) train their models on a small manually 
created data set containing only 1,615 examples. 
But entity linking requires large training data. 
Agirre et al (2009) use Wikipedia to construct 
their training data by utilizing Inter-Wikipedia 
links and the surrounding snippets of text. How-
ever, their training data is created from a         
1291
different domain which does not work well in 
the targeted news article domain.  
3 Approach
In this section we describe our two-stage ap-
proach for entity linking: name variation and 
entity disambiguation. The first stage finds vari-
ations for every entity in the KB and generates 
an entity candidate set for a given query. The 
second stage is entity disambiguation, which 
links an entity mention with the real world entity 
it refers to. 
3.1 Name Variation 
The aim for Name Variation is to build a 
Knowledge Repository of entities that contains 
vast amount of world knowledge of entities like 
name variations, acronyms, confusable names, 
spelling variations, nick names etc. We use 
Wikipedia to build our knowledge repository 
since Wikipedia is the largest encyclopedia in 
the world and surpasses other knowledge bases 
in its coverage of concepts and up-to-date 
content. We obtain useful information from 
Wikipedia by the tool named Java Wikipedia 
Library 2  (Zesch et al 2008), which allows to 
access all information contained in Wikipedia. 
Cucerzan (2007) extracts the name variations 
of an entity by leveraging four knowledge 
sources in Wikipedia: ?entity pages?, ?disam-
biguation pages?  ?redirect pages? and ?anchor 
text?.
Entity page in Wikipedia is uniquely identified 
by its title ? a sequence of words, with the first 
word always capitalized. The title of Entity Page 
represents an unambiguous name variation for 
the entity. A redirect page in Wikipedia is an aid 
to navigation. When a page in Wikipedia is redi-
rected, it means that those set of pages are refer-
ring to the same entity. They often indicate syn-
onym terms, but also can be abbreviations, more 
scientific or more common terms, frequent 
misspellings or alternative spellings etc. Disam-
biguation pages are created only for ambiguous 
mentions which denote two or more entities in 
Wikipedia, typically followed by the word ?dis-
ambiguation? and containing a list of references 
to pages for entities that share the same name. 
This is more useful in extracting the abbrevia-
2 http://www.ukp.tu-darmstadt.de/software/JWPL 
tions of entities, other possible names for an ent-
ity etc. Besides, both outlinks and inlinks in Wi-
kipedia are associated with anchor texts that 
represent name variations for the entities.
Using these four sources above, we extracted 
name variations for every entity in KB to form 
the Knowledge Repository as Cucerzan?s (2007) 
method. For example, the variation set for entity 
E0272065 in KB is {Abbott Laboratories, Ab-
bott Nutrition, Abbott ?}. Finally, we can gen-
erate the entity candidate set for a given query 
using the Knowledge Repository. For example, 
for the query containing ?Abbott?, the entity 
candidate set retrieved is {E0272065, E0064214 
?}.
From our observation, for some queries the re-
trieved candidate set is empty. If the entity for 
the query is a new entity, not present in KB, 
empty candidate set is correct. Otherwise, we 
fail to identify the mention in the query as a var-
iation, commonly because the mention is a miss-
pelling or infrequently used name. So we pro-
pose to use two more sources ?Did You Mean? 
and ?Wikipedia Search Engine? when Cucerzan 
(2007) algorithm returns empty candidate set. 
Our experiment results show that both proposed 
knowledge sources are effective for entity link-
ing. This contributes to a performance improve-
ment on the final entity linking accuracy. 
Did You Mean: The ?did you mean? feature 
of Wikipedia can provide one suggestion for 
misspellings of entities. This feature can help to 
correct the misspellings. For example, ?Abbot 
Nutrition? can be corrected to ?Abbott Nutri-
tion?.
Wikipedia Search Engine: This key word 
based search engine can return a list of relevant 
entity pages of Wikipedia. This feature is more 
useful in extracting infrequently used name. 
Algorithm 1 below presents the approach to 
generate the entity candidate set over the created 
Knowledge Repository. Ref
E
(s) is the entity set 
indexed by mention s retrieved from Knowledge 
Repository.  In Step 8, we use the longest com-
mon subsequence algorithm to measure the simi-
larity between strings s and the title of the entity 
page with highest rank. More details about long-
est common subsequence algorithm can be 
found in Cormen et al (2001). 
1292
Algorithm 1 Candidate Set Generation 
Input: mention s;       
1: if RefE(s) is empty
2:        s??Wikipedia?did you 
           mean?Suggestion 
3:        If s? is not NULL  
4:             s ? s?
5:        else
6:            EntityPageList ? WikipediaSear
               chEngine(s) 
7:            EntityPage?FirstPage of EntityPageL 
               ist 
8:            Sim=Similarity(s,EntityPage.title)
9:            if Sim > Threshold 
10:   s? EntityPage.title
11:          end if 
12: end if 
13: end if 
Output: RefE(s);
3.2 Entity Disambiguation 
The disambiguation component is to link the 
mention in query with the entity it refers to in 
candidate set. If the entity to which the mention 
refers is a new entity which is not present in KB, 
nil will be returned. In this Section, we will de-
scribe the method for automatic data creation, 
domain adaptation from Wikipedia data, and our 
supervised learning approach as well. 
3.2.1 Automatic Data Creation  
The basic idea is to take a document with an un-
ambiguous reference to an entity E1 and replac-
ing it with a phrase which may refer to E1, E2 or 
others.
Observation: Some full names for the entities 
in the world are unambiguous. This phenomenon 
also appears in the given document collection of 
entity linking. The mention ?Abbott Laborato-
ries? appearing at multiple locations in the doc-
ument collection refers to the same entity ?a
pharmaceuticals health care company? in KB.
From this observation, our method takes into 
account the mentions in the Knowledge Reposi-
tory associated with only one entity and we treat 
these mentions as unambiguous name. Let us 
take Abbott Laboratories-{E0272065} in the 
Knowledge Repository as an example. We first 
use an index and search tool to find the docu-
ments with unambiguous mentions. Such as, the 
mention ?Abbott Laboratories? occurs in docu-
ment LDC2009T13 and LDC2007T07 in the 
document collection. The chosen text indexing 
and searching tool is the well-known Apache 
Lucene information retrieval open-source li-
brary3.
Next, to validate the consistency of NE type 
between entities in KB and in document,   we 
run the retrieved documents through a Named 
Entity Recognizer, to tag the named entities in 
the documents. Then we link the document to 
the entity in KB if the document contains a 
named entity whose name exactly matches with 
the unambiguous mention and type (i.e. Person, 
Organization and Geo-Political Entity) exactly 
matches with the type of entity in KB. In this 
example, after Named Entity Recognition, ?Ab-
bott Laboratories? in document LDC2009T13 is
tagged as an Organization which is consistent 
with the entity type of E0272065 in KB. We link 
the ?Abbott Laboratories? occurring in 
LDC2009T13 with entity E0272065.  
Finally, we replace the mention in the selected 
documents with the ambiguous synonyms. For 
example, we replace the mention ?Abbott La-
boratories? in document LDC2009T13 with
?Abbott? where Abbott-{E0064214, 
E0272065?} is an entry in Knowledge Reposi-
tory. ?Abbott? is ambiguous, because it is refer-
ring not only to E0272065, but also to E0064214 
in Knowledge Repository. Then, we can get two 
instances for the created data set as Figure 1, 
where one is positive and the other is negative.  
Figure 1: An instance of the data set 
However, from our studies, we realize some 
limitations on our training data. For example, as 
shown in Figure 1, the negative instance for 
E0272065 and the positive instance for 
3 http://lucene.apache.org 
(Abbott, LDC2009T13)  E0272065    +
(Abbott, LDC2009T13)  E0064214    -
          ? 
                         +   refer to  -  not refer to
1293
E0064214 are not in our created data set. 
However, those instances exist in the current 
document collection. We do not retrieve them 
since there is no unambiguous mention for 
E0064214 in the document collection.   
To reduce the effect of this problem, we pro-
pose to use the Wikipedia data as well, since 
Wikipedia data has training examples for all the 
entities in KB. Articles in Wikipedia often con-
tain mentions of entities that already have a cor-
responding article, and at least the first occur-
rence of the mentions of an entity in a Wikipedia 
article must be linked to its corresponding Wiki-
pedia article, if such an article exists. Therefore, 
if the mention is ambiguous, the hyperlink is 
disambiguating it. Next, we will describe how to 
incorporate Wikipedia data. 
Incorporating Wikipedia Data. The docu-
ment collection for entity linking is commonly 
from other domains, but not Wikipedia. To ben-
efit from Wikipedia data, we introduce a domain 
adaption approach (Daum? III, 2007) which is 
suitable for this work since we have enough 
?target? domain data. The approach is to aug-
ment the feature vectors of the instances. Denote 
by X the input space, and by Y the output space, 
in this case, X is the space of the real vectors 
???? for the instances in data set and Y= {+1,-1} 
is the label. Ds is the Wikipedia domain dataset 
and Dt is our automatically created data set. 
Suppose for simplicity that X=RF for some F > 0 
(RF is the space of F-dimensions). The aug-
mented input space will be defined by ??  =R3F.
Then, define mappings ?s, ?t : X ? ?? for map-
ping the Wikipedia and our created data set re-
spectively.  These are defined as follows:
????? ?? ????? ????? ? ?
????? ?? ??????? ???? ?
Where 0=<0,0,?,0> ?RF is the zero vector. We 
use the simple linear kernel in our experiments. 
However, the following kernelized version can 
help us to gain some insight into the method. K
denotes the dot product of two vectors. 
K(x,x?)=< ?  (x), ?  (x?)>. When the domain is 
the same, we get: ????? ??? ?? ????? ????? ?
?? ????? ????? ? ????? ??? . When they are 
from different domains, we get: ????? ??? ??
????? ????? ?? ???? ???. Putting this togeth-
er, we have: 
?? ? ???
??? ?????????????
???? ???????? ??????
This is an intuitively pleasing result. Loosely 
speaking, this means that data points from our 
created data set have twice as much influence as 
Wikipedia points when making predictions 
about test data from document collection. 
3.2.2 The Disambiguation Framework 
To disambiguate a mention in document collec-
tion, the ranking method is to rank the entities in 
candidate set based on the similarity score. In 
our work, we transform the ranking problem into 
a classification problem: deciding whether a 
mention refers to an entity on an SVM classifier.
If there are 2 or more than 2 candidate entities 
that are assigned positive label by the binary 
classifier, we will use the baseline system (ex-
plained in Section 4.2) to rank the candidates 
and the entity with the highest rank will be cho-
sen.
In the learning framework, the training or test-
ing instance is formed by (query, entity) pair.
For Wikipedia data, (query, entity) is positive if 
there is a hyperlink from the article containing 
the mention in query to the entity, otherwise 
(query, entity) is negative. Our automatically 
created data has been assigned labels in Section 
3.2.1.  Based on the training instances, a binary 
classifier is generated by using particular learn-
ing algorithm.  During disambiguation, (query,
entity) is presented to the classifier which then 
returns a class label.  
Each (query, entity) pair is represented by the 
feature vector using different features and simi-
larity metrics. We chose the following three 
classes of features as they represent a wide range 
of information - lexical features, word-category 
pair, NE type - that have been proved to be ef-
fective in previous works and tasks. We now 
discuss the three categories of features used in 
our framework in details. 
Lexical features. For Bag of Words feature in 
Web People Search, Artiles et al (2009) illu-
strated that noun phrase and n-grams longer than 
2 were not effective in comparison with token-
based features and using bi-grams gives the best 
1294
results only reaching recall 0.7. Thus, we use 
token-based features. The similarity metric we 
choose is cosine (using standard tf.idf weight-
ing). Furthermore, we also take into account the 
co-occurring NEs and represent it in the form of 
token-based features. Then, the single cosine 
similarity feature is based on Co-occurring NEs 
and Bag of Words. 
Word Category Pair. Bunescu (2007) dem-
onstrated that word-category pairs extracted 
from the document and Wikipedia article are a 
good signal for disambiguation. Thus we also 
consider word-category pairs as a feature class, 
i.e., all (w,c) where w is a word from Bag of 
Words of document and c is a category to which 
candidate entity belongs.  
NE Type. This feature is a single binary fea-
ture to guarantee that the type of entity in docu-
ment (i.e. Person, Geo-Political Entity and Or-
ganization) is consistent with the type of entity 
in KB. 
4 Experiments and Discussions 
4.1 Experimental Setup 
    In our study, we use KBP-09 knowledge base 
and document collection for entity linking. In the 
current setting of KBP-09 Data, the KB has been 
generated automatically from Wikipedia. The 
KB contains 818,741 different entities. The doc-
ument collection is mainly composed of news-
wire text from different press agencies. The col-
lection contains 1.3 million documents that span 
from 1994 to the end of 2008. The test data has 
3904 queries across three named entity types: 
Person, Geo-Political Entity and Organization. 
Each query contains a document with an ambi-
guous mention.    
Wikipedia data can be obtained easily from 
the website4 for free research use. It is available 
in the form of database dumps that are released 
periodically. In order to leverage various infor-
mation mentioned in Section 3.1 to derive name 
variations, make use of the links in Wikipedia to 
generate our training corpus and get word cate-
gory information for the disambiguation, we fur-
ther get Wikipedia data directly from the website. 
The version we used in our experiments was re-
leased on Sep. 02, 2009. The automatically 
4 http://download.wikipedia.org   
created corpus (around 10K) was used as the 
training data, and 30K training instances asso-
ciated with the entities in our corpus was derived 
from Wikipedia. 
For pre-processing, we perform sentence 
boundary detection and Chunking derived from 
Stanford parser (Klein and Manning, 2003), 
Named Entity Recognition using a SVM based 
system trained and tested on ACE 2005 with 
92.5(P) 84.3(R) 88.2(F), and coreference resolu-
tion using a SVM based coreference resolver 
trained and tested on ACE 2005 with 79.5%(P), 
66.7%(R) and 72.5%(F).  
We select SVM as the classifier used in this 
paper since SVM can represent the stat-of-the-
art machine learning algorithm. In our imple-
mentation, we use the binary SVMLight devel-
oped by Joachims (1999). The classifier is 
trained with default learning parameters. 
We adopt the measure used in KBP-09 to eva-
luate the performance of entity linking. This 
measure is micro-averaged accuracy: the number 
of correct link divided by the total number of 
queries.
4.2 Baseline Systems 
We build the baseline using the ranking ap-
proach which ranks the candidates based on si-
milarity between mention and candidate entities. 
The entity with the highest rank is chosen. Bag 
of words and co-occurring NEs are represented 
in the form of token-based feature vectors. Then 
tf.idf is employed to calculate similarity between 
feature vectors.  
To make the baseline system with token-
based features state-of-the-art, we conduct a se-
ries of experiments.  Table 1 lists the perfor-
mances of our token-based ranking systems. In 
our experiment, local tokens are text segments 
generated by a text window centered on the 
mention. We set the window size to 55, which is 
the value that was observed to give optimum 
performance for the disambiguation problem 
(Gooi and Allan, 2004). Full tokens and NE are 
all the tokens and named entities co-occurring in 
the text respectively. We notice that tokens of 
the full text as well as the co-occurring named 
entity produce the best baseline performance, 
which we use for the further experiment. 
1295
 Micro-averaged 
Accuracy 
local tokens 60.0 
local tokens + NE 60.6 
full tokens + NE 61.9 
Table 1: Results of the ranking methods 
4.3 Experiment and Result 
As discussed in Section 3.1, we exploit two 
more knowledge sources in Wikipedia: ?did you 
mean? (DYM) and ?Wikipedia search engine? 
(SE) for name variation step. We conduct some 
experiments to compare our name variation me-
thod using Algorithm 1 in Section 3.1 with the 
name variation method of Cucerzan (2007). Ta-
ble 2 shows the comparison results of different 
name variation methods for entity linking. The 
experiments results show that, in entity linking 
task, our name variation method outperforms the 
method of Cucerzan (2007) for both entity dis-
ambiguation methods. 
Name Variation 
Approaches 
Ranking
Method 
Our Disambig-
uation Method 
Cucerzan
(2007) 
60.9 82.2 
+DYM+SE 61.9 83.8 
Table 2: Entity Linking Result for two name 
variation approaches. Column 1 used the base-
line method for entity disambiguation step. Col-
umn 2 used our proposed entity disambiguation 
method.
Table 3 compares the performance of different 
methods for entity linking on the KBP-09 test 
data. Row 1 is the result for baseline system. 
Row 2 and Row 3 show the results training on 
Wikipedia data and our automatically data re-
spectively. Row 4 is the result training on both 
Wikipedia and our created data using the domain 
adaptation method mentioned in Section 3.2.1. It 
shows that our method trained on the automati-
cally generated data alone significantly outper-
forms baseline. Compared Row 3 with Row 2, 
our created data set serves better at training the 
classifier than Wikipedia data. This is due to the 
reason that Wikipedia is a different domain from 
newswire domain. By comparing Row 4 with 
Row 3, we find that by using the domain adapta-
tion method in Section 3.2.1, our method for 
entity linking can be further improved by 1.5%. 
Likely, this is because of the limitation of the 
auto-generated corpus as discussed in Section 
3.2.1. In another hand, Wikipedia can comple-
ment the missing information with the auto-
generated corpus. So combining Wikipedia data 
with our generated data can achieve better result. 
Compared with baseline system using Cucerzan 
(2007) name variation method in Table 2, in to-
tal our proposed method achieves a significant 
22.9% improvement.  
 Micro-averaged Accu-
racy
Baseline 61.9 
Wiki 79.9 
Created Data 82.3 
Wiki? Created Data 83.8 
Table 3: Micro-averaged Accuracy for Entity 
Linking   
     To test the effectiveness of our method to 
deal with new entities not present in KB and ex-
isting entities in KB respectively, we conduct 
some experiments to compare with Baseline.  
Table 4 shows the performances of entity linking 
systems for existing entities (non-NIL) in KB 
and new entity (NIL) which is not present in KB. 
We can see that the binary classifier not only 
effectively reduces the ambiguities to the exist-
ing entities in KB, but also is very useful to 
highlight the new entities to KB for the further 
population. Note that, in baseline system, all the 
new entities are found by the empty candidate 
set of name variation process, while the disam-
biguation component has no contribution.  How-
ever, our approach finds the new entities not on-
ly by the empty candidate set, but also leverag-
ing on disambiguation component which also 
contributes to the performance improvement.  
 non-NIL NIL 
Baseline 72.6  52.4  
Wiki? Created 
Data 
79.2 87.8  
Table 4: Entity Linking on Existing and New 
Entities
1296
Finally, we also compare our method with the 
top 5 systems in KBP-09. Among them, 
Siel_093 (Varma et al 2009) and NLPR_KBP1
(Han and Zhao 2009) use similarity ranking ap-
proach; Stanford_UBC2 (Agirre et al 2009),
QUANTA1 (Li et al 2009) and hltcoe1 (McNa-
mee et al 2009) use supervised approach. From 
the results shown in Figure 2, we observe that 
our method outperforms all the top 5 systems 
and the baseline system of KBP-09. Specifically, 
our method achieves better result than both simi-
larity ranking approaches. This is due to the li-
mitations of the ranking approach which have 
been discussed in Section 2. We also observe 
that our method gets a 5% improvement over 
Stanford_UBC2. This is because they collect 
their training data from Wikipedia which is a 
different domain from document collection of 
entity linking, news articles in this case; while 
our automatic data generation method can create 
a data set from the same domain as the docu-
ment collection. Our system also outperforms 
QUANTA1 and hltcoe1 because they train their 
model on a small manually created data set 
(1,615 examples), while our method can auto-
matically generate a much larger data set. 
Figure 2: A comparison with KBP09 systems 
5 Conclusion
 The purpose of this paper is to explore how 
to leverage the automatically generated large 
scale annotation for entity linking. Traditionally, 
without any training data available, the solution 
is to rank the candidates based on similarity. 
However, it is difficult for the ranking approach 
to detect a new entity that is not present in KB, 
and it is also difficult to combine different fea-
tures. In this paper, we create a large corpus for 
entity linking by an automatic method. A binary 
classifier is then trained to filter out KB entities 
that are not similar to current mentions. We fur-
ther leverage on the Wikipedia documents to 
provide other information which is not available 
in our generated corpus through a domain adap-
tion approach. Furthermore, new information 
sources for finding more variations also contri-
bute to the overall 22.9% accuracy improve-
ments on KBP-09 test data over baseline. 
References  
E. Agirre et al Stanford-UBC at TAC-KBP. In Pro-
ceedings of Test Analysis Conference 2009 (TAC 
09).
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The se-
meval-2007 web evaluation: Establishing a 
benchmark for the web people search task. In Pro-
ceeding of the Fourth International Work-shop on 
Semantic Evaluations (SemEval-2007).
J. Artiles, E. Amigo and J. Gonzalo. 2009. The role 
of named entities in Web People Search. In pro-
ceeding of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics. 
R. Bunescu. 2007. Learning for information extrac-
tion from named entity recognition and disambig-
uation to relation extraction. Ph.D thesis, Universi-
ty of Texas at Austin, 2007. 
T. H. Cormen, et al 2001. Introduction To Algo-
rithms (Second Edition). The MIT Press, Page 
350-355. 
S. Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. Empirical 
Methods in Natural Language Processing, June 
28-30, 2007. 
H. Daum? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics . 
C. H. Gooi and J. Allan. 2004. Cross-document core-
ference on a large scale corpus. In proceedings of 
Human Language Technology Conference North 
American Association for Computational Linguis-
tics Annual Meeting, Boston, MA. 
X. Han and J. Zhao. NLPR_KBP in TAC 2009 KBP 
Track: A Two-Stage Method to Entity Linking. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).
0.838
0.8217
0.8033
0.7984
0.7884
0.7672
0.571
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
1297
T. Joachims. 1999. Making large-scale SVM learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press. 
D. Klein and C. D. Manning. 2003. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
F. LI et al THU QUANTA at TAC 2009 KBP and 
RTE Track. In Proceedings of Test Analysis Con-
ference 2009 (TAC 09).  
P. McNamee and H. T. Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).  
P. McNamee et al HLTCOE Approaches to Know-
ledge Base Population at TAC 2009.  In Proceed-
ings of Test Analysis Conference 2009 (TAC 09).  
H. T. Nguyen and T. H. Cao. 2008. Named Entity 
Disambiguation on an Ontology Enriched by Wi-
kipedia. 2008 IEEE International Conference on 
Research, Innovation and Vision for the Future in 
Computing & Communication Technologies. 
B. Popov et al 2004. KIM - a Semantic Platform for 
Information Extraction and Retrieval. In Journal 
of Natural Language Engineering, Vol. 10, Issue 
3-4, Sep 2004, pp. 375-392, Cambridge University 
Press.
V. Raphael, K. Joachim and M. Wolfgang, 2007. 
Towards ontology-based disambiguation of geo-
graphical identifiers. In Proceeding of the 16th
WWW workshop on I3: Identity, Identifiers, Identi-
fications, 2007.  
V. Varma et al 2009. IIIT Hyderabad at TAC 2009. 
In Proceedings of Test Analysis Conference 2009 
(TAC 09).  
T. Zesch, C. Muller and I. Gurevych. 2008. Extrac-
tiong Lexical Semantic Knowledge from Wikipe-
dia and Wiktionary. In Proceedings of the Confe-
rence on Language Resources and Evaluation 
(LREC), 2008.  
1298
Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 872?881,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
	
				
		
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 276?285, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
N-gram-based Tense Models for Statistical Machine Translation
Zhengxian Gong1 Min Zhang2 Chewlim Tan3 Guodong Zhou1?
1 School of Computer Science and Technology, Soochow University, Suzhou, China 215006
2 Human Language Technology, Institute for Infocomm Research, Singapore 138632
3 School of Computing, National University of Singapore, Singapore 117417
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Tense is a small element to a sentence, how-
ever, error tense can raise odd grammars and
result in misunderstanding. Recently, tense
has drawn attention in many natural language
processing applications. However, most of
current Statistical Machine Translation (SMT)
systems mainly depend on translation model
and language model. They never consider and
make full use of tense information. In this pa-
per, we propose n-gram-based tense models
for SMT and successfully integrate them in-
to a state-of-the-art phrase-based SMT system
via two additional features. Experimental re-
sults on the NIST Chinese-English translation
task show that our proposed tense models are
very effective, contributing performance im-
provement by 0.62 BLUE points over a strong
baseline.
1 Introduction
For many NLP applications, such as event extraction
and summarization, tense has been regarded as a key
factor in providing temporal order. However, tense
information has been largely overlooked by current
SMT research. Consider the following example:
SRC:??B$?e?(J , ?U?Ny3??
,???N?I???m??'X"
REF:The embargo is a result of the Cold War and does not
reflect the present situation nor the partnership between China
and the EU.
MOSES: the embargo is the result of the cold war, not reflect
the present situation, it did not reflect the partnership with the
european union.
?*Corresponding author.
Although the translated text produced by Moses1
is understandable, it has very odd tense combination
from the grammatical aspect, i.e. with tense incon-
sistency (is/does in REF vs. is/did in Moses). Ob-
viously, slight modification, such as changing ?is?
into ?was?, can much improve the readability of the
translated text. It is also interesting to note that such
modification can much affect the evaluation. If we
change ?did? to ?does?, the BLEU-4 score increases
from 22.65 to 27.86 (as matching the 4-gram ?does
not reflect the? in REF). However, if we change ?is?
to ?was?, the BLEU score decreases from 22.65 to
21.44.
The above example seems special. To testify its
impact on SMT in wider range, we design a special
experiment based on the 2005 NIST MT data (see
section 6.1). This data has 4 references. We choose
one reference and modify its sentences with error
tense2. After that, we use other 3 references to mea-
sure this reference. The modified reference leads to
a sharp drop in BLEU-4 score, from 52.46 to 50.27
in all. So it is not a random phenomenon that tense
can affect translation results.
The key is how to detect tense errors and choose
correct tenses during the translation procedure. By
carefully comparing the references with Moses out-
put, we obtain the following useful observations,
Observation(1): to most simple sentences, coor-
dinate verbs should be translated with the same tense
while they have different tense in Moses output;
Observation(2): to some compound sentences,
1http://www.statmt.org/moses/
2Such changes are small by mainly modifying one auxiliary
verb for a sentence, such as ?is? was?, ?has? had?.
276
the subordinate clause should have the consisten-
t tense with its main clause while Moses fails;
Observation(3): the diversity of tense usage in a
document is common. However, in most cases, the
neighbored sentences tends to share the same main
tense. In some extreme examples, one tense (past or
present), even dominates the whole document.
One possible solution to model above observa-
tions is using rules. Dorr (2002) refers to six ba-
sic English tense structures and defines the possible
paired combinations of ?present, past, future?. But
the practical cases are very complicated, especial-
ly in news domain. There are a lot of complicat-
ed sentences in news articles. Our preliminary in-
vestigation shows that such six paired combinations
can only cover limited real cases in Chinese-English
SMT.
This paper proposes a simple yet effective method
to model above observations. For each target sen-
tence in the training corpus, we first parse it and ex-
tract its tense sequence. Then, a target-side tense
n-gram model is constructed. Such model can be
used to estimate the rationality of tense combina-
tion in a sentence and thus supervise SMT to reduce
tense inconsistency errors against Observations (1)
and (2) in the sentence-level. In comparison, Ob-
servation (3) actually reflects the tense distributions
among one document. After extracting each main
tense for each sentence, we build another tense n-
gram model in the document-level. For clarity, this
paper denotes document-level tense as ?inter-tense?
and sentence-level tense as ?intra-tense?.
After that, we propose to integrate such tense
models into SMT systems in a dynamic way. It
is well known there are many errors in the current
MT output (David et al2006). Unlike previously
making trouble with reference texts, the BLEU-4 s-
core cannot be influenced obviously by modifying
a small part of abnormal sentences in a static way.
In our system, both inter-tense and intra-tense mod-
el are integrated into a SMT system via additional
features and thus can supervise the decoding proce-
dure. During decoding, once some words with cor-
rect tense can be determined, with the help of lan-
guage model and other related features, the smal-
l component??tense??can affect surrounding words
and improve the performance of the whole sentence.
Our experimental results (see the examples in Sec-
tion 6.4) show the effectiveness of this way.
Rather than the rule-based model, our models are
fully statistical-based. So they can be easily scaled
up and integrated into either phrase-based or syntax-
based SMT systems. In this paper, we employ a
strong phrase-based SMT baseline system, as pro-
posed in Gong et al2011), which uses document as
translation unit, for better incorporating document-
level information.
The rest of this paper is organized as follows: Sec-
tion 2 reviews the related work. Section 3 and 4 are
related to tense models. Section 3 describes the pre-
processing work for building tense models. Section
4 presents how to build target-side tense models and
discuss their characteristics. Section 5 shows our
way of integrating such tense models into a SMT
system. Session 6 gives the experimental results. Fi-
nally, we conclude this paper in Section 7.
2 Related Work
In this section, we focus on related work on integrat-
ing the tense information into SMT. Since both inter-
and intra-tense models need to analyze and extract
tense information, we also give a brief overview on
tense prediction (or tagging).
2.1 Tense Prediction
The tense prediction task often needs to build a mod-
el based on a large corpus annotated with temporal
relations and thus its focus is on how to recognize,
interpret and normalize time expressions. As a rep-
resentative, Lapata and Lascarides (2006) proposed
a simple yet effective data-intensive approach. In
particular, they trained models on main and subor-
dinate clauses connected with some special tempo-
ral marker words, such as ?after? and ?before?, and
employed them in temporal inference.
Another typical task is cross-lingual tense pred-
ication. Some languages, such as English, are in-
flectional, whose verbs can express tense via certain
stems or suffix, while others, such as Chinese of-
ten lack inflectional forms. Take Chinese to English
translation as example, if Chinese text contains par-
ticle word ?
(Le)?, the nearest Chinese verb prefers
to be translated into English verb with the past tense.
Ye and Zhang (2005), Ye et al2007) and Liu et al
(2011) focus on labeling the tenses for keywords in
277
source-side language.
Ye and Zhang (2005) first built a small amoun-
t of manually-labeled data, which provide the tense
mapping from Chinese text to English text. Then,
they trained a CRF-based tense classifier to label
tense on Chinese documents. Ye et al2007) fur-
ther reported that syntactic features contribute most
to the marking of aspectual information. Liu et al
(2011) proposed a parallel mapping method to au-
tomatically generate annotated data. In particular,
they used English verbs to label tense information
for Chinese verbs via a parallel Chinese-English cor-
pus.
It is reasonable to label such source-side verb to
supervise the translation process since the tense of
English sentence is often determined by verbs. The
problem is that due to the diversity of English ver-
b inflection, it is difficult to map such Chinese tense
information into the English text. To our best knowl-
edge, although above works attempt to serve for
SMT, all of them fail to address how to integrate
them into a SMT system.
2.2 Machine Translation with Tense
Dorr (1992) described a two-level knowledge repre-
sentation model based on Lexical Conceptual Struc-
tures (LCS) for machine translation which integrates
the aspectual information and the lexical-semantic
information. Her system is based on an inter-lingual
model and does not belong to a SMT system.
Olsen et al2001) relied on LCS to generate
appropriately-tensed English translations for Chi-
nese. In particular, they addressed tense reconstruc-
tion on a binary taxonomy (present and past) for
Chinese text and reported that incorporating lexical
aspect features of telicity can obtain a 20% to 35%
boost in accuracy on tense realization.
Ye et al2006) showed that incorporating latent
features into tense classifiers can boost the perfor-
mance. They reported the tense resolution results
based on the best-ranked translation text produced
by a SMT system. However, they did not report the
variation of translation performance after introduc-
ing tense information.
3 Preprocessing for Tense Modeling
In this paper, tense modeling is done on the target-
side language. Since our experiments are done
on Chinese to English SMT, our tense models are
learned only from the English text. In the literature,
the taxonomy of English tenses typically includes
three basic tenses (i.e. present, past and future) plus
their combination with the progressive and perfec-
tive aspects. Here, we consider four basic tenses:
present, past, future and UNK (unknown) and ignore
the aspectual information. Furthermore, we assume
that one sentence has only one main tense but maybe
has many subordinate tenses.
This section describes the preprocessing work of
building tense models, which mainly involves how
to extract tense sequence via tense verbs.
3.1 Tense Verbs
Lapata et al006) used syntactic parse trees to find
clauses connected with special aspect markers and
collected them to train some special classifiers for
temporal inference. Inspired by their work, we use
the Stanford parser3 to parse tense sequence for each
sentence.
Take the following three typical sentences as ex-
amples, (a) is a simple sentence which contains two
coordinate verbs, while (b) and (c) are compound
sentences and (b) contains a quoted text.
(a) Japan?s constitution renounces the right to go to war and
prohibits the nation from having military forces except for self-
defense.
(b) ?We also hope Hong Kong will not be affected by diseases
like the severe acute respiratory syndrome again!? , added Ms.
Yang.
(c) Cheng said he felt at home in Hong Kong and he sincerely
wished Hong Kong more peaceful and more prosperous.
Figure 1 shows the parse tree with Penn Treebank
style for each sentence, which has strict level struc-
tures and POS tags for all the terminal words. Here,
the level structures mainly contribute to extract main
tense for each sentence (to be described in Section
3.2) and POS tags are utilized to detect tense verbs,
i.e. verbs with tense information.
Normally, POS tags in the parse tree can distin-
guish five different forms of verbs: the base form
(tagged as VB), and forms with overt endings D for
3http://nlp.stanford.edu/software/lex-parser.shtml
278
Figure 1: The Stanford parse trees with Penn Treebank style
past tense, G for present participle, N for past par-
ticiple, and Z for third person singular present. It is
worth noting that VB, VBG and VBN cannot deter-
mine the specific tenses by themselves. In addition,
the verbs with POS tag ?MD? need to be special-
ly considered to distinguish future tense from other
tenses.
Algorithm 1 illustrates how to determine what
tense a node has. If the return value is not ?UNK?,
the node belongs to a tense verb.
Algorithm 1 Determine the tense of a node.
Input:
The TreeNode of one parse tree, leafnode;
Output:
The tense, tense;
1: tense = ?UNK ??
2: Obtaining the POS tag lpostag from leafnode;
3: Obtaining the word lword from leafnode;
4: if (lpostag in [?V BP ??, ?V BZ ??]) then
5: tense = ?present??
6: else if (lpostag == ?V BD??]) then
7: tense = ?past??
8: else if (lpostag == ?MD??]) then
9: if (lword in [?will??, ?ll??, ?shall??]) then
10: tense = ?future??
11: else if (lword in [?would??, ?could??]) then
12: tense = ?past??
13: else
14: tense = ?present??
15: end if
16: end if
17: return tense;
3.2 Tense Extraction Based on Tense Verbs
As described in Section 1, the inter-tense
(document-level) refers to the main tense of
one sentence and the intra-tense (sentence-level)
corresponds to all tense sequence of one sentence.
This section introduces how to recognize the main
tense and extract all useful tense sequence for each
sentence.
The idea of determining the main tense is to find
the tense verb located in the top level of a parse tree.
According to the Penn Treebank style, the method
to determine the main tense can be described as fol-
lows:
(1) Traverse the parse tree top-down until a tree node
containing more than one child is identified, denot-
ed as Sm .
(2) Consider each child of Sm with tag ?VP?, recursive-
ly traverse such ?VP? node to find a tense verb. If
found, use it as the main tense and return the tense;
if not, go to step (3).
(3) Consider each child of Sm with tag ?S?, which ac-
tually corresponds to subordinate clause of this sen-
tence. Starting from the first subordinate clause, ap-
ply the similar policy of step (2) to find the tense
verb. If not found, search remaining subordinate
clauses.
(4) If no tense verb found, return ?UNK? as the main
tense.
Here, ?VP? nodes dominated by Sm directly are
preferred over those located in subordinate clauses.
This is to ensure that the main tense is decided by
the top-level tense verb.
279
Take Figure 1 as an example, the main tense of
sentence (a) and (b) can be determined only by step
(2). The tense verb of ?(VBZ renounces)? dominat-
ed in the ?VP? tag determines that (a) is in present
tense. Similarly the node ?(VBD added)? indicates
that (b) is in past tense. Sentence (c) needs to be fur-
ther treated by step (3) since there is no ?VP? nodes
dominated by Sm directly. The node ?(VBD said)?
located in the first subordinate clause shows its main
tense is ?past?.
The next task is to extract the tense sequence for
each sentence. They are determined by all tense
verbs in this sentence according to the strict top-
down order. For example, the tense sequence of
sentence (a), (b) and (c) are {present, present},
{present, future, past} and {past, past, past}. In or-
der to explore whether the main tense of intra-tense
model has an impact on SMT or not, we introduce
a special marker ?*? to denote the main tense. So
the tense sequence marked with main tense of (a),
(b) and (c) are {present*, present},{present, future,
past*} and {past*, past, past}. It is worth noting, the
intra-tense model (see Section 4) based on the latter
tense sequence is different to the former.
4 N-gram-based Tense Models
4.1 Tense N-gram Estimation
After applying the previous method to extract tense
for an English text corpus, we can obtain a big tense
corpus.
Given the current tense is indexed as ti, we call
the previous n ? 1 tenses plus the current tense as
tense n-gram.
Based on the tense corpus, tense n-gram statistics
can be done according to the Formula 1.
P (ti|t(i?(n?1)), ..., t(i?1)) =
count(t(i?(n?1)), . . . , t(i?1), ti)
count(t(i?(n?1)), ..., t(i?1))
(1)
Here, the function of ?count? return the tense n-gram
frequency. In order to avoid doing specific smooth-
ing work, we estimate tense n-gram probability us-
ing SRI language modeling (SRILM) tool (Stolcke,
2002).
To compute the probability of intra-tense n-gram,
we first extract all tense sequence for each sentence
and put them into a new file. Based on this new file,
we can get the intra-tense n-gram model via SRILM
tool.
To compute the probability of inter-tense n-gram,
we need to extract the main tense for each sentence
at first. Then, for each document, we re-organized
the main tenses of all sentences into a special line.
After putting all these special lines into a new file,
we can use SRILM to obtain the inter-tense n-gram
model.
4.2 Characteristic of Tense N-gram Models
We construct n-gram-based tense models on English
Gigaword corpus (LDC2003T05). This corpus is
used to build language model for most SMT sys-
tems. It includes 30221 documents (we remove such
files: file size is less than 1K or the number of con-
tinuous ?UNK? tenses is greater than 5).
Figure 2 shows the unigram and bigram probabil-
ities (Log10-style) for intra-tense and inter-tense.
The part (a) and (c) in Figure 2 refer to unigram.
The horizontal axis indicts tense type, and the ver-
tical axis shows its probabilities. The parts (a) and
(c) also indicate ?present? and ?past? are two main
tense types in news domain.
The part (b) and (d) refer to bigram. The horizon-
tal axis indicts history tense. Each different color-
ful bar indicts one current tense. The vertical axis
shows the transfer possibilities from a history tense
to a current tense.
The part (b)4 reflects transfer possibilities of tense
types in one sentence. It also slightly reflects some
linguistic information. For example, in one sen-
tence, the probability of co-occurrence of ?present
? present?, ?past ? past? and ?future ? present?
is more than other combinations, which can be a-
gainst tense inconsistency errors described in Obser-
vation (1) and (2) (see Section 1). However, it seem-
s strange that ?present? past? exceeds ?present?
future?. We checked our corpus and found a lot of
sentences like this??the bill has been . . . , he said. ?.
The part (d) shows tense type can be switched be-
tween two neighbored sentences. However, it shows
the strong tendency to use the same tense type for
4The co-occurrence of the ?UNK? tense and other tense
types in one sentence cannot happen, so the ?UNK? tense is
omitted.
280
Figure 2: statistics of intra-tense and inter-tense N-gram
neighbored sentences. This statistics conform to the
previous observation (3) very much.
5 Integrating N-gram-based Tense Models
into SMT
In this section, we discuss how to integrate the pre-
vious tense models into a SMT system.
5.1 Basic phrase-based SMT system
It is well known that the translation process of SMT
can be modeled as obtaining the best translation e
of the source sentence f by maximizing following
posterior probability(Brown et al1993):
ebest = argmax
e
P (e|f)
= argmax
e
P (f |e)Plm(e)
(2)
where P (e|f) is a translation model and Plm is a
language model.
Our baseline is a modified Moses, which follows
Koehn et al2003) and adopts similar six groups
of features. Besides, the log-linear model ( Och and
Ney, 2000) is employed to linearly interpolate these
features for obtaining the best translation according
to the formula 3:
ebest = argmax
e
M?
m=1
?mhm(e, f) (3)
where hm(e, f) is a feature function, and ?m is
the weight of hm(e, f) optimized by a discrimina-
tive training method on a held-out development da-
ta(Och, 2003).
5.2 The Workflow of Our System
Our system works as follows:
When a hypothesis has covered all source-side
words during the decoding procedure, the decoder
first obtains tense sequence for such hypothesis and
computes intra-tense feature Fs(see Section 5.3). At
the same time, it recognizes the main tense of this
hypothesis and associate the main tense of previous
sentence to compute inter-tense feature Fm (see Sec-
tion 5.3).
Next, the decoder uses such two additional feature
values to re-score this hypothesis automatically and
choose one hypothesis with highest score as the final
translation.
After translating one sentence, the decoder caches
its main tense and pass it to the next sentence.
When one document has been processed, the de-
coder cleans this cache.
In order to successfully implement above work-
flow, we should firstly design some related features,
then resolve another key problem of determining
tense (especially main tense) for SMT output. They
are described in Section 5.3 and 5.4 respectively.
5.3 Two Additional Features
Although the previous tense models show strong
tendency to use the consistent tenses for one sen-
tence or one document, other tense combinations al-
so can be permitted. So we should use such models
in a soft and dynamic way. We design two features:
inter-tense feature and intra-tense feature. And the
weight of each feature is tuned by the MERT script
in Moses packages.
Given main tense sequence of one documen-
t t1, . . . , tm, the inter-tense feature Fm is calculated
according to the following formula:
Fm =
m?
i=2
P (ti|ti?(n?1), . . . , t(i?1)) (4)
The P (?) of formula 4 can be estimated by the for-
mula 1. It is worth noting the first sentence of one
281
document often scares tense information since it cor-
responds to the title at most cases. To the first sen-
tence, the P (?) value is set to 14 (4 tense types).
Given tense sequence of one sentence
s1, . . . , se (e > 1), the intra-tense feature Fs
is calculated as follows:
Fs = e?1
?
?
?
?
e?
i=2
P (si|si?(n?1), . . . , s(i?1)) (5)
Here the square-root operator is used to avoid pun-
ishing translations with long tense sequence. It is
worth noting if the sentence only contains one tense,
the P (?) value of formula 5 is also set to 14 .
Since the average length of intra-tense sequence
is about 2.5, we mainly consider intra-tense bigram
model and thus n equals to 2. 5
5.4 Determining Tense For SMT Output
The current SMT systems often produce odd transla-
tions partly because of abnormal word ordering and
uncompleted text etc. For these abnormal translated
texts, the syntactic parser cannot work well in our
initial experiments, so the previous method to parse
main tense and tense sequence of regular texts can-
not be applied here too.
Fortunately, the solely utilization of Stanford POS
tagger for our SMT output is not bad although it has
the same issues described in Och et al2002). The
reason may be that phrase-based SMT contains short
contexts that POS tagger can utilize while the syntax
parser fails.
Once obtaining a completed hypothesis, the de-
coder will pass it to the Stanford POS tagger and ac-
cording to tense verbs to get alense sequence for
this hypothesis. However, since the POS tagger can
not return the information about level structures, the
decoder cannot recognize the main tense from such
tense sequence.
Liu et al2011) once used target-side verbs to la-
bel tense of source-side verbs. It is natural to consid-
er whether Chinese verbs can provide similar clues
in an opposite direction.
Since Chinese verbs have good correlation with
English verbs (described in section 6.2), we obtain
5In our experiment, the intra-tense bigram model can ob-
tain the comparable performance to the trigram model. And the
inter-tense trigram model can not exceed the bigram one.
Figure 3: trees for parallel sentences
main tense for SMT output according to such tense
verb, which corresponds to the ?VV?(Chinese POS
labels are different to English ones, ?VV? refers to
Chinese verb) node in the top level of the source-side
parse tree. Take Figure 3 as an example, the English
node ?(VBD announced)? is a tense verb which can
tell the main tense for this English sentence. The
Chinese verb ?(VV??)? in the top-level of the
Chinese parse tree is just the corresponding part for
this English verb.
So, before translating one sentence, the decoder
first parses it and records the location of one Chinese
?VV? node which located in the top-level, denotes
this location as Sarea.
Once a completed hypothesis is generated, ac-
cording to the phrase alignment information, the de-
coder can map Sarea into the English location Tarea
and obtain the main tense according to the POS tags
in Tarea .
If Tarea does not contain tense verb, such as the
verb POS tags in the list of {VB, VBN, VBG},
which cannot tell tense type by themselves, our sys-
tem permits to find main tense in the left/right 3
words neighbored to Tarea. And the proportion that
the top-level verb of Chinese has a verb correspon-
dence in English can reach to 83% in this way.
6 Experimentation
6.1 Experimental Setting for SMT
In our experiment, SRI language modeling toolk-
it was used to train a 5-Gram general language
model on the Xinhua portion of the Gigaword cor-
pus. Word alignment was performed on the train-
ing parallel corpus using GIZA++ ( Och and Ney,
2000) in two directions. For evaluation, the NIST
282
BLEU script (version 13) with the default setting is
used to calculate the BLEU score (Papineni et al
2002), which measures case-insensitive matching of
4-grams. To see whether an improvement is statisti-
cally significant, we also conduct significance tests
using the paired bootstrap approach (Koehn, 2004).
In this paper, ?***? and ?**? denote p-values equal
to 0.05, and bigger than 0.05, which mean signifi-
cantly better, moderately better respectively.
Corpus Sentences Documents
Role Name
Train FBIS 228455 10000
Dev NIST2003 919 100
Test NIST2005 1082 100
Table 1: Corpus statistics
We use FBIS as the training data, the 2003 NIST
MT evaluation test data as the development data, and
the 2005 NIST MT test data as the test data. Table 1
shows the statistics of these data sets (with document
boundaries annotated).
6.2 The Correlation of Chinese Verbs and
English Verbs
In this section, an additional experiment is designed
to show English Verbs have close correspondence
with Chinese Verbs.
We use the Stanford POS tagger to tag the Chi-
nese and English sentences in our training corpus
respectively at first. Then we utilize Giza++ to build
alignment for these special Word-POS pairs. Ac-
cording to the alignment results, we find the corre-
sponding relation for some special POS tags in two
languages.
Chinese Verb POS English POS Number
VV Verb VBD 89830
POS VBP 27276
VBZ 32588
MD 40378
VBG 86025
VBN 75019
VB 153596
In sum: 504712
Other Non-Verb 149318
Verb Corresponding Ratio 0.77169
Table 2: The Chinese and English Verb Pos Alignment
The ?Number? column of Table 2 shows the num-
bers of Chinese words with ?VV? tag correspond-
ing to English words with different verb POS tags.
We found Chinese verbs have more than 77% possi-
bilities to align to English verbs in total. However,
our method will fail when some special Chinese sen-
tences only contain noun predicates.
6.3 Experimental Results
All the experiment results are showed on the table 3.
Our Baseline is a modified Moses. The major modi-
fication is input and output module in order to trans-
late using document as unit. The performance of our
baseline exceeds the baseline reported by Gong et al
(2011) about 2 percent based on the similar training
and test corpus.
System BLEU BLEU on Test(%)
Dev(%) BLEU NIST
Moses Md(Baseline) 29.21 28.30 8.4528
Baseline+Fm 30.56 28.87(***) 8.7935
Baseline+Fs 31.28 28.61(**) 8.5645
Baseline+Fs(?) 31.04 28.74(**) 8.6271
Baseline+Fm+Fs 31.75 28.88(***) 8.7987
Baseline+Fm+Fs(*) 31.42 28.92(***) 8.8201
Table 3: The performance of using different feature com-
binations
The system denoted as ?Baseline+Fm? integrates
the inter-tense feature. The performance boosts
0.57(***) in BLEU score.
The system denoted as ?Baseline+Fs? integrates
the intra-tense feature into the baseline. The im-
provement is less than the inter-tense model, on-
ly 0.31(**). It seems the tenses in one sentence
has more flexible formats than the document-level
ones. It is worth noting, this method can gain high-
er performance on the develop data than the one of
?Baseline+Fm? while fail to improve the test data.
Maybe the related weight is tuned over-fit.
The system denoted as ?Baseline+Fs(*)? is s-
lightly different from ?Baseline+Fs?. This experi-
ment is to check whether the main tense has an im-
pact on intra-tense model or not (see Section 3.2).
Here, the intra-tense model based on the tense se-
quence with main tense marker is slightly different
to the model showed in Figure 2. The results are
slightly better than the previous system by 0.13.
Finally, we use the two features together
(Baseline+Fm+Fs). The best way improved the
performance by 0.62(***) in BLEU score over our
baseline.
283
6.4 Discussion
Table 4 shows special examples whose intra-tenses
are changed in our proposed system. The exam-
ple 1 and 2 show such modification can improve
the BLEU score but the example 3 obtains negative
impact. From these examples, we can see not only
tense verbs have changed but also their surrounding
words have subtle variation.
No. BLEU Translation sentence
1 8.64 Baseline: the gulf countries , the bahraini royal fam-
ily members by the military career of part of the
banned to their marriage stories like children , have
become the theme of television films .
19.71 Ours: the gulf country is a member of the bahraini
royal family , a risk by military career risks part of
the banned to their marriage like children , has be-
come a story of the television and film industry .
2 17.16 Baseline:economists said that the sharp appreciation
of the euro , in the recent investigation continues to
have an impact on economic confidence , it is esti-
mated that the economy is expected to rebound to
pick up .
24.25 Ours: economists said that the sharp appreciation of
the euro , in the recent investigation continued to
have an impact on economic confidence and there-
fore no reason to predict the economy expected to
pick up a rebound .
3 73.03 Baseline: the middle east news agency said that , af-
ter the concerns of all parties concerned in the mid-
dle east peace process , israel and palestine , egypt ,
the united states , russia and several european coun-
tries will hold a meeting in washington .
72.95 Ours: the middle east news agency said that after the
concerns of all parties in the middle east peace pro-
cess , israel and palestine , egypt , the united states ,
russia and several european countries held a meeting
in washington .
Table 4: Examples with tense variation using intra-tense
model
From the results showed on Table 3, the
document-level tense model seems more effective
than the sentence-level one. We manually choose
and analyzed 5 documents with significant improve-
ment in the test data. The part (a) of Figure 4 shows
the main tense distributions of one reference. The
main tense distributions for the baseline and our pro-
posed system are showed in the part (b) and (c) re-
spectively. These documents have different numbers
of sentences but all less than 10. The vertical axis in-
dicates different tense: 1 to ?past?, 2 to ?present?, 3
to ?future? and 4 to ?UNK?. It is obvious that our
system has closer distributions to the ones of this
reference.
The examples in Table 5 indicate the joint impact
of inter-tense and intra-tense model on SMT. Sen-
Src:
1)????K???^??? , |????; ??
??"
2)nVd")?|?+ECnd??Oc  ??W
?	?|? ,??Loc5????gONnV
d"??+<??\?! ?;"
Ref:
1)Israeli settlers blockaded a major road to protest a mortar attack
on the settlement area.
2)PLO leader Abbas had also been allowed to go to the West Bank
town of Bethlehem , which is the first time in the past four years
Israeli authorities have allowed a senior Palestinian leader to attend
Christmas celebrations.
Baseline:
1)israel has imposed a main road to protest by mortars attack .
2)the palestinian leader also visited the west bank cities and towns
to bethlehem , which in the past four years , the israeli authorities
allowed the palestinian leading figures attended the ceremony .
Ours:
1)israel has imposed a main road to protest against the mortars at-
tack .
2)leader of the palestinian liberation organization have also been
allowed to go to the west bank towns , bethlehem in the past four
years . this is the first time the israeli authorities allow palestinian
leading figures attended the ceremony .
Table 5: the joint impact of inter- tense and intra-tense
models on SMT
tence 1) and 2) are two neighbored sentences in one
document. Both the reference and our output tend
to use the same main tense type, but the former is in
?past? tense and the latter is in ?present? tense. The
baseline cannot show such tendency. Although our
main tense is different to the reference one, the con-
sistent tenses in document level bring better trans-
lation results than the baseline. And the tenses in
sentence level also show better consistency than the
baseline.
7 Conclusion
This paper explores document-level SMT from the
tense perspective. In particular, we focus on how to
build document-level and sentence-level tense mod-
els and how to integrate such models into a popular
SMT system.
Compared with the inter-tense model which great-
ly improves the performance of SMT, the intra-tense
model needs to be further explored. The reasons are
many folds, e.g. the failure to exclude quoted texts
when modeling intra-tense, since tenses in quoted
texts behave much diversely from normal texts. In
the future work, we will focus on modeling intra-
tense variation according to specific sentence types
and using more features to improve it.
284
Figure 4: the comparison of the inter-tense distributions for reference, baseline and our proposed system
Acknowledgments
This research is supported by part by NUS FRC
Grant R252-000-452-112, the National Natural Sci-
ence Foundation of China under grant No.90920004
and 61003155, the National High Technology Re-
search and Development Program of China (863
Program) under grant No.2012AA011102.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra and R.L.
Mercer. 1992. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263-311.
Vilar David, Jia Xu, DH`aro L. F., and Hermann Ney.
2006. Error Analysis of Machine Translation Output.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 697-702,
Genoa, Italy.
Bonnie J. Dorr. 1992. A parameterized approach to
integrating aspect with lexical-semantics for machine
translation. In Proceedings of of ACL-2002, pages
257-264.
Bonnie J. Dorr and Terry Gaasterland. 2002. Constraints
on the Generation of Tense, Aspect, and Connecting
Words from Temporal Expressions. Technical Reports
from UMIACS.
Zhengxian Gong, Min Zhang and Guodong Zhou.
2011. Cached-based Document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909-919.
Philipp Koehn, Franz Josef Och ,and DanielMarcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of NAACL-2003, pages 48-54.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388-395.
Mirella Lapata, Alex Lascarides. 2006. Learning
Sentence-internal Temporal Relations. Journal of Ar-
tificial Intelligence Research, 27:85-117.
Feifan Liu, Fei Liu and Yang Liu. 2011. Learning from
Chinese-English Parallel Data for Chinese Tense Pre-
diction. In Proceedings of IJCNLP-2011, pages 1116-
1124,Chiang Mai, Thailand.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of of ACL-
2000, pages 440-447.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
et.al. 2002. A smorgasbord of Features for Statistical
Machine Translation. In Proceedings of NAACL-2004,
pages 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL-2003,pages 160-167, Sapporo, Japan,July.
Mari Olsen, David Traum,Carol Van Ess-Dykema and
Amy Weinberg. 2001. Implicit Cues for Explicit Gen-
eration: Using Telicity as a Cue for Tense Structure in
a Chinese to English MT System. In Proceedings of
MT Summit VIII, Santiago de Compostella, Spain.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic E-
valuation of Machine Translation. In Proceedings of
ACL-2002, pages 311-318.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference on Spoken Language Processing,pages
901-904.
Yang Ye and Zhu Zhang. 2005. Tense tagging for verbs
in cross-lingual context: A case study. In Proceedings
of IJCNLP-2005, pages 885-895.
Yang Ye, V.li Fossum, Steven Abney. 2006. Laten-
t features in Temporal Reference Translation. Fifth
SIGHAN Workshop on Chinese Language Processing,
pages 48-55.
Yang Ye, Karl-Michael Schnelder, Steven Abney. 2007.
Aspect Marker Generation in English-to-Chinese Ma-
chine Translation. Proceedings of MT Summit XI,
pages 521-527,Copenhagen, Denmark.
285
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1563?1573,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Lexical Chain Based Cohesion Models for
Document-Level Statistical Machine Translation
Deyi Xiong1, Yang Ding2, Min Zhang1? and Chew Lim Tan2
1School of Computer Science and Technology, Soochow University, Suzhou, China 215006
{dyxiong, minzhang}@suda.edu.cn
2School of Computing, National University of Singapore, Singapore 117417
{a0082379, tancl}@comp.nus.edu.sg
Abstract
Lexical chains provide a representation of the
lexical cohesion structure of a text. In this pa-
per, we propose two lexical chain based co-
hesion models to incorporate lexical cohesion
into document-level statistical machine trans-
lation: 1) a count cohesion model that rewards
a hypothesis whenever a chain word occurs in
the hypothesis, 2) and a probability cohesion
model that further takes chain word transla-
tion probabilities into account. We compute
lexical chains for each source document to be
translated and generate target lexical chains
based on the computed source chains via max-
imum entropy classifiers. We then use the
generated target chains to provide constraints
for word selection in document-level machine
translation through the two proposed lexical
chain based cohesion models. We verify the
effectiveness of the two models using a hier-
archical phrase-based translation system. Ex-
periments on large-scale training data show
that they can substantially improve translation
quality in terms of BLEU and that the prob-
ability cohesion model outperforms previous
models based on lexical cohesion devices.
1 Introduction
Given a source document, traditionally most statisti-
cal machine translation (SMT) systems translate the
document sentence by sentence. In such a transla-
tion scheme, sentences are translated independent
of any other sentences. However, a text is normally
written cohesively, in which sentences are connected
?Corresponding author
to each other via syntactic and lexical devices. This
linguistic phenomenon is called as textual cohesion
(Halliday and Hasan, 1976).
Cohesion is a surface-level property of well-
formed texts. It deals with five categories of rela-
tionships between text units, namely co-reference,
ellipsis, substitution, conjunction and lexical cohe-
sion that is realized via semantically related words.
The former four cohesion relations can be grouped
as grammatical cohesion. Generally speaking,
grammatical cohesion is less common and harder
to identify than lexical cohesion (Barzilay and El-
hadad, 1997).
As most SMT systems translate a text in a
sentence-by-sentence fashion, they tend to build less
lexical cohesion than human translators (Wong and
Kit, 2012). We therefore study lexical cohesion for
document-level translation. We use lexical chains
(Morris and Hirst, 1991) to capture lexical cohe-
sion in a text. Lexical chains are connected graphs
that represent the lexical cohesion structure of a text.
They have been successfully used for information
retrieval (Stairmand, 1996), document summariza-
tion (Barzilay and Elhadad, 1997) and so on. In this
paper, we investigate how lexical chains can be used
to incorporate lexical cohesion into document-level
translation.
Our basic assumption is that the lexical chains of
a target document are direct correspondences of the
lexical chains of its counterpart source document.
This assumption is reasonable as the target docu-
ment translation should be faithful to the source doc-
ument in terms of both text meaning and structure.
Based on this assumption, we propose a framework
1563
to incorporate lexical cohesion into target document
translation via lexical chains, which works as fol-
lows.
? Compute lexical chains for each source docu-
ment that is to be translated;
? Project the computed source lexical chains onto
the corresponding target document by translat-
ing source chain words into target chain words
using maximum entropy classifiers;
? Incorporate lexical cohesion into the target doc-
ument translation via cohesion models built on
the projected target lexical chains .
We build two lexical chain based cohesion mod-
els. The first model is a count model that rewards a
hypothesis whenever a word in the projected target
lexical chains occur in the hypothesis. As a source
chain word may be translated into many different
target words, we further extend the count model to
a second cohesion model: a probability model that
takes chain word translation probabilities into ac-
count.
We test the two lexical chain based cohesion mod-
els on a hierarchical phrase-based SMT system that
is trained with large-scale Chinese-English bilin-
gual data. Experiment results show that our lexi-
cal chain based cohesion models can achieve sub-
stantial improvements over the baseline. Further-
more, the probability cohesion model is better than
the count model and it also outperforms previous
cohesion models based on lexical cohesion devices
(Xiong et al, 2013).
To the best of our knowledge, this is the first at-
tempt to explore lexical chains for statistical ma-
chine translation. The remainder of this paper is or-
ganized as follows. Section 2 discusses related work
and highlights the differences between our method
and previous work. Section 3 briefly introduces
lexical chains and algorithms that compute lexical
chains. Section 4 elaborates the proposed lexical
chain based framework, including details on source
lexical chain computation, target lexical chain gen-
eration and the two lexical chain based cohesion
models. Section 5 presents our large-scale experi-
ments and results. Finally, we conclude with future
directions in Section 6.
2 Related Work
Recent years have witnessed growing research in-
terests in document-level statistical machine trans-
lation. Such research efforts can be roughly di-
vided into two groups: 1) general document-level
machine translation that does not explore or ex-
plores very little linguistic discourse information;
2) linguistically-motivated document-level machine
translation that incorporates discourse information
such as cohesion and coherence into SMT. Recent
studies (Guillou, 2013; Beigman Klebanov and
Flor, 2013) show that this discourse information is
very important for document-level machine transla-
tion.
General Document-Level Machine Translation
Tiedemann (2010) propose cache-based language
and translation models for document-level machine
translation. These models are built on recently trans-
lated sentences. Following this cache-based ap-
proach, Gong et al (2011) further introduce two
additional caches. They use a static cache to store
bilingual phrases extracted from documents in train-
ing data that are similar to the document being trans-
lated. They also adopt a topic cache with target
language topic words. Xiao et al (2011) study
the translation consistency issue in document-level
machine translation. They use a hard constraint to
consistently translate ambiguous source words into
the most frequent translation options. Ture et al
(2012) soften this consistency constraint by integrat-
ing three counting features into decoder.
Using Lexical Cohesion Devices in Document-
Level SMT Lexical cohesion devices are seman-
tically related words, including word repetition,
synonyms/near-synonyms, hyponyms and so on.
They are also the cohesion-building elements in lex-
ical chains.
Wong and Kit (2012) use lexical cohesion device
based metrics to improve machine translation evalu-
ation at the document level. These metrics measure
the proportion of content words that are used as lex-
ical cohesion devices in machine-generated transla-
tions. Hardmeier et al (2012) propose a document-
wide phrase-based decoder and integrate a semantic
language model into the decoder. They argue that
their semantic language model can capture lexical
cohesion by exploring n-grams that cross sentence
1564
boundaries.
Most recently Xiong et al (2013) integrate
three categories of lexical cohesion devices into
document-level machine translation. They define
three cohesion models based on lexical cohesion de-
vices: a direct reward model, a conditional probabil-
ity model and a mutual information trigger model.
The latter two models measure the strength of lexical
cohesion relation between two lexical items. They
are incorporated into SMT to calculate how appro-
priately lexical cohesion devices are used in doc-
ument translation. As lexical chains capture lexi-
cal cohesion relations among sequences of related
words rather than those only between two words, ex-
periments in Section 5 show that our lexical chain
based probability cohesion model is better than the
lexical cohesion device based trigger model, which
is the best among the three cohesion models pro-
posed by Xiong et al (2013).
Modeling Coherence in Document-Level SMT
In discourse analysis, cohesion is often studied to-
gether with coherence which is another dimension
of the linguistic structure of a text (Barzilay and
Elhadad, 1997). Cohesion is related to the sur-
face structure of a text while coherence is concerned
with the underlying meaning connectedness in a text
(Vasconcellos, 1989). Compared with cohesion, co-
herence is not easy to be detected. Even so, various
models have been proposed to explore coherence for
document summarization and generation (Barzilay
and Lapata, 2008; Louis and Nenkova, 2012). Fol-
lowing this line, Xiong and Zhang (2013) integrate
a topic-based coherence model into document-level
machine translation, where coherence is defined as a
continuous sentence topic transition.
Our lexical chain based cohesion models are also
related to previous work on using word and phrase
sense disambiguation for lexical choice in SMT
(Carpuat and Wu, 2007b; Carpuat and Wu, 2007a;
Chan et al, 2007). The difference is that we use
document-wide lexical chains to build our cohesion
models rather than sentence-level context features.
In our framework, lexical choice is performed to
make the selected words consistent with the lexical
cohesion structure of a document.
Carpuat (2009) explores the principle of one sense
per discourse (Gale et al, 1992) in the context of
SMT and imposes the constraint of one translation
per discourse on document translation. We also
use the one sense per discourse principle to perform
word sense disambiguation on the source side in our
lexical chaining algorithm (See Section 4.1).
3 Background: Lexical Chain and Chain
Computation
Lexical chains are sequences of semantically related
words (Morris and Hirst, 1991). They represent the
lexical cohesion structure of a text. Figure 2 displays
six lexical chains computed from the Chinese news
article shown in Figure 1. Words in these lexical
chains have lexical cohesion relations such as rep-
etition, synonym, which may range over the entire
text. For example, in the lexical chain LC1 of Fig-
ure 2, the same word ?dWgu_? (Germany) repeats
9 times. In the lexical chain LC3, the two words
?z`ngcSi? (president) and ?zhdx[? (chairman) are
synonym words. Generally, a text can have many
different lexical chains, each of which represents a
thread of cohesion through the text.
Several lexical chaining algorithms have been
proposed to compute lexical chains from texts. Nor-
mally they need an ontology to obtain semantic re-
lations between words. Word sense disambiguation
(WSD) is also used to determine the sense of each
word in a text. Generally a lexical chain compu-
tation algorithm completes the following three sub-
tasks:
? Building a representation of a text with a set
of candidate words and assigning semantic re-
lations between the candidate words according
to the ontology;
? Choosing the right sense for each candidate
word via WSD;
? Building chains over the semantically related
and disambiguated candidate words.
These three sub-tasks can be done separately or si-
multaneously.
Morris and Hirst (Morris and Hirst, 1991) de-
fine the first lexical chain computation algorithm
that adopts a greedy strategy to immediately disam-
biguate a word at its first occurrence. This algo-
rithm runs in linear time but suffers from inaccu-
rate disambiguation. Barzilay and Elhadad (Barzi-
lay and Elhadad, 1997) significantly improve WSD
1565
dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[
dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi
g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\"
( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tR
shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn ,
c[zh[ sh] tR wWiyZ de xuTnzW"
t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc
gdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZsh[yZ y\shUng"
su`ma zUi dWgu_ diUnx]n g^ngsZ b^ Sng z`ngbe zhUokRi jiRnsh]hu] tYbiW hu]y] zh^ng fRbiTo
yZ xiUng shVngm[ng , tR shu^ : 7 w` y\ yUoqic jiRnsh]hu] jiXchc w` de zh[we"8
y_uyc liTng gY yuY hau jiRng jdx[ng dUxuTn , dUn liSnhWzhYngfd zUi m[n diUo zh^ng shVngwUng
luahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh] xZn dZ sh[ , dWgu_ diUnx]n
g^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch[ tR"
dWgu_ diUnx]n gdjiU haulSi hu[ wXn , y\ sh[yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTng
bTifYnzhZbRdiTnwds]"
dWgu_ cSizhYngbe huRny[ng su`ma c[zh[ de juWd]ng"
Figure 1: An example of a Chinese news article (written in pinyin).
LC1: {dWgu_, dWgu_, b^, dWgu_, dWgu_, dWgu_,
dWgu_, b^, dWgu_, dWgu_, dWgu_}
LC2:{jiRnsh]hu], fTxZnshY, jiRnsh]hu], z`ngbe,
jiRnsh]hu], jiRnsh]hu]}
LC3: {z`ngcSi, zhdx[, z`ngcSi, z`ngl\}
LC4: {c[zh[, c[qe, c[zh[, c[zh[}
LC5: {zhTng, xiUcua, shUngzhTng}
LC6: {xuRnbe, xuRnbe, fRbiTo}
Figure 2: Six lexical chains from the example in Figure
1.
accuracy by processing all possible combinations of
word senses in a text to disambiguate words. Un-
fortunately, their algorithm runs slowly in quadratic
time. Galley and Mckeown (2003) present an algo-
rithm that are better than the former two algorithms
both in terms of running efficiency and WSD accu-
racy. They separate the WSD sub-task from the task
of lexical chain building and impose a ?one sense
per discourse? constraint in the WSD step.
4 Translating Documents Using Lexical
Chains
In this section, we describe how we incorporate lex-
ical cohesion into document-level machine transla-
tion using lexical chains. We divide the lexical chain
based document-level machine translation process
into three steps: (1) computing lexical chains for
source documents with a source language ontology,
(2) generating target lexical chains from the com-
puted source lexical chains, and finally (3) incorpo-
rating lexical cohesion encoded in the generated tar-
get lexical chains into document-level translation via
lexical chain based cohesion models. The remainder
of this section will elaborate these three steps.
4.1 Source Lexical Chains Computation
We follow the chain computation algorithm intro-
duced by Galley and McKeown (2003) to build lex-
ical chains on source (Chinese) documents. In the
algorithm, the chaining process includes three steps:
choosing candidate words to build a disambiguation
graph (Galley and McKeown, 2003) for each doc-
ument, disambiguating the candidate words and fi-
nally building lexical chains over the disambiguated
candidate words.
The disambiguation graph can be considered as
a representation of all possible interpretations of its
corresponding text. In the graph, nodes are candi-
date words with different senses and edges between
word senses are weighted according to their seman-
tic relations, such as synonym, hypernym and so on.
We use an extended version of a Chinese thesaurus
Tongyici Cilin (Cilin for short) to define word senses
and semantic relations between senses. The ex-
1566
level 1
level 2
level 3
level 4
level 5
Figure 3: The architecture of the extended Cilin. For sim-
plicity, we only draw a binary tree to represent the hier-
archical structure of Cilin. This doesn?t mean that each
semantic class at level i has only two sub-classes at level
i+ 1. Actually, they have multiple sub-classes.
tended Cilin contains 77,343 Chinese words, which
are organized in a hierarchical structure containing
5 levels as shown in Figure 3. In the 5th level, each
node represents an atomic concept which consists of
a set of synonyms. These atomic concepts are just
like synsets in WordNet. We use them to represent
senses of words in the disambiguation graph.
We select nouns, verbs, abbreviations and idioms
as candidate words for the disambiguation graph.
These words are identified by a Chinese part-to-
speech tagger LTP (Che et al, 2010) in a preprocess-
ing step. In order to build the disambiguation graph,
we first build an array indexed by the atomic con-
cepts of Cilin, then insert a copy of each candidate
word into its all concept (sense) entries in the array.
After that, we create all semantic links among senses
of different candidate words in the disambiguation
graph following Galley and McKeown (2003).
In the second step, we use the principle of one
sense per discourse to perform WSD for each can-
didate word in the disambiguation graph. We sum
the weights of all semantic links under the different
senses of the candidate word in question. The sense
with the highest sum of weights is considered as the
most probable sense for this word. We then assign
this sense to all occurrences of the word in the doc-
ument by adopting the constraint of one sense per
discourse.
Once all candidate words are disambiguated, we
can build lexical chains over these words by remov-
ing all semantic links that connect those unselected
word senses. The six lexical chains shown in Fig-
ure 2 are computed from the Chinese document in
Figure 1 exactly following the algorithm of Galley
and McKeown (2003). The only difference is that
we use Cilin rather than WordNet as the ontology.
4.2 Target Lexical Chains Generation
Since a faithful target document translation should
follow the same cohesion structure as that in its cor-
responding source document, we generate target lex-
ical chains from the computed source lexical chains.
Given a source lexical chain LCs = {s
j
i} where the
ith chain word sji is from the jth sentence of the
source document Ds, we generate a target lexical
chain LCt = {t
j
i} using maximum entropy (Max-
Ent) classifiers. Particularly, we translate a word sji
in the source lexical chain into a target word tji in
the target lexical chain using a corresponding Max-
Ent classifier as follows1.
P (tji |C(s
j
i )) =
exp(
?
k ?kfk(t
j
i , C(s
j
i )))
?
t exp(
?
k ?kfk(t, C(s
j
i )))
(1)
where fk are binary features, ?k are weights of these
features, and C(sji ) is the surrounding context of
chain word sji .
We train one MaxEnt classifier per unique source
chain word. For each classifier, we define two
groups of binary features: 1) the preceding and
succeeding two words of sji in the jth sentence
({w?2, w?1, s
j
i , w+1, w+2}); 2) the preceding and
succeeding one word of sji in the lexical chain LCs
({spi?1, s
j
i , s
q
i+1}). All features are in the following
binary form.
f(tji , C(s
j
i )) =
{
1, if tji = ? and C(s
j
i ).? = ?
0, else
(2)
where the symbol ? is a placeholder for a possible
target word, the symbol? indicates a contextual ele-
ment for the chain word sji (e.g., the preceding word
in the jth sentence or the succeeding word in the
lexical chain LCs), and the symbol ? represents the
value of ?.
Given a source document Ds and its N lexical
chains {LCks }
N
k=1 computed from the document as
1We collect training instances from word-aligned bilingual
data to train the MaxEnt classifier.
1567
described in Section 4.1, we can generate the N
target lexical chains {LCkt }
N
k=1 using our MaxEnt
classifiers. Each target word tji in the target lexi-
cal chain LCkt is the translation of its corresponding
source word sji in the source lexical chain LC
k
s with
the highest probability P (tji |C(s
j
i )) according to Eq.
(1).
As we know, the MaxEnt classifier can gen-
erate multiple translations for each source word.
In order to incorporate these multiple chain word
translations, we can generate a super target lexi-
cal chain LCt from a source lexical chain LCs,
where  is a pre-defined threshold used to se-
lect multiple translations. For example, given a
source lexical chain LCs = {a, b, c}, we can
have the corresponding super target lexical chain
LCt = {{a1t , a
2
t ...}, {b
1
t , b
2
t ...}, {c
1
t , c
2
t ...}}, where
xit is the translation of x with a translation probabil-
ity P (xit|C(x)) ?  according to Eq. (1). Integrat-
ing multiple translations for each source chain word,
we can reduce the error propagation of the MaxEnt
classifier to some extent. Our experiments also con-
firm that the super target lexical chains with multi-
ple translation options for each chain word are better
than the target lexical chains with only one transla-
tion per chain word. Therefore we build our cohe-
sion models based on the super target lexical chains,
which will be described in the next section.
4.3 Lexical Chain Based Cohesion Models
Once we generate the super target lexical chains
{LCkt }
N
k=1 for the target document Dt, we can use
them to provide constraints for the target document
translation. Our key interest is to make the target
document translation TDt as cohesive as possible.
We therefore propose lexical chain based cohesion
models to measure the cohesion of the target docu-
ment translation. The basic idea is to reward a trans-
lation hypothesis if a word from the super target lexi-
cal chains occurs in the hypothesis. According to the
difference in the reward strategy, we have two cohe-
sion models: a count cohesion model and a proba-
bility cohesion model.
Count Cohesion Model Mc(TDt , {LC
k
t }
N
k=1):
This model rewards a translation hypothesis of the
jth sentence in the document whenever a lexical
chain word tji occurs in the hypothesis. The model
maintains a counter and accumulates the counter
when necessary. It is factorized into the sentence
cohesion metric Mc(Tj , {LCkt }
N
k=1), where Tj is
the translation of the jth sentence in the target docu-
ment. Mc(Tj , {LCkt }
N
k=1) is formulated as follows.
Mc(Tj , {LC
k
t }
N
k=1) =
?
w?Tj
?
tji?C
e?(w,t
j
i ) (3)
where C represents {LCkt }
N
k=1, and the ? function
is defined as follows.
?(w, tji ) =
{
1, if tji = w
0, otherwise
(4)
Probability Cohesion Model
Mp(TDt , {LC
k
t }
N
k=1): This model rewards a
translation hypothesis according to the translation
probability of a chain word that occurs in the
hypothesis. The translation probability is computed
by Eq. (1). The model is also factorized into the
sentence cohesion metric Mp(Tj , {LCkt }
N
k=1)
which is formulated as follows.
Mp(Tj ,{LC
k
t }
M
k=1) =
?
w?Tj
?
tji?C
e?(w,t
j
i ) ? P (tji |C(s
j
i )) (5)
where P (tji |C(s
j
i ) is the translation probability com-
puted according to Eq. (1).
4.4 Decoding
The proposed lexical chain based cohesion models
are integrated into the log-linear translation frame-
work of SMT as a cohesion feature. Before translat-
ing a source document, we compute lexical chains
for the source document as described in Section 4.1.
We then generate the super target lexical chains. In
order to efficiently calculate our lexical chain based
cohesion models, we reorganize words in the super
target lexical chains into vectors. We associate each
source sentence Sj a vector to store target lexical
chain words that are to occur in the corresponding
target sentence Tj .
Although we still translate a source document
sentence by sentence, we capture the global cohe-
sion structure of the document via lexical chains and
use the lexical chain based cohesion models to con-
strain word selection in document translation. Fig-
ure 4 shows the architecture of an SMT system with
the lexical chain based cohesion model.
1568
Figure 4: Architecture of an SMT system with the lexical
chain based cohesion model.
5 Experiments
In this section, we conducted a series of experiments
to validate the effectiveness of the proposed lexical
chain based cohesion models for Chinese-to-English
document-level machine translation. We used a hier-
archical phrased-based SMT system (Chiang, 2007)
trained on large-scale data. In particular, we aim at:
? Measuring the impact of the threshold  on the
probability cohesion model and selecting the
best threshold on a development test set.
? Investigating the effect of the two lexical-chain
based cohesion models.
? Comparing our lexical chain based cohesion
models against the previous lexical cohesion
device based models (Xiong et al, 2013).
5.1 Setup
We collected our bilingual training data from
LDC, which includes the corpus LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004E12,
LDC2004T07, LDC2004T08 (Only Hong Kong
News), LDC2005T06 and LDC2005T10. The
collected bilingual training data contains 3.8M
sentence pairs with 96.9M Chinese words and
109.5M English words. We trained a 4-gram
language model on the Xinhua portion of the
English Gigaword corpus (306 million words) via
the SRILM toolkit (Stolcke, 2002) with Kneser-Ney
smoothing.
Training MT05 MT06 MT08
#Doc 103,236 100 79 109
#Sent 2.80M 1,082 1,664 1,357
#Chain 3.52M 1700 2172 1693
#AvgC 35.72 17 27.49 15.53
#AvgW 14.81 5.89 6.89 5.63
Table 1: Statistics of the training, development and test
sets, which show the number of documents (#Doc) and
sentences (#Sent), the number of lexical chains extracted
from the source documents (#Chain), the average number
of lexical chains per document (#AvgC) and the average
number of words per lexical chain (#AvgW).
In order to build the lexical chain based cohesion
models, we selected corpora with document bound-
aries explicitly provided from the bilingual training
data together with the whole Hong Kong parallel
text corpus as the cohesion model training data2. We
show the statistics of these selected corpora in Table
1. They contain 103,236 documents and 2.80M sen-
tences. Averagely, each document consists of 28.4
sentences. From the source documents of the se-
lected corpora, we extract 3.52M lexical chains. On
average, there are 35.72 lexical chains per document
and 14.81 words per lexical chain.
We used the off-the-shelf MaxEnt toolkit3 to train
one MaxEnt classifier per unique source lexical
chain word (61,121 different source chain words in
total). We performed 100 iterations of the L-BFGS
algorithm implemented in the training toolkit for
each chain word with both Gaussian prior and event
cutoff set to 1 to avoid overfitting. After event cutoff,
we have an average of 17.75 different classes (target
translations) per source chain word.
We used the NIST MT05 as the tuning set for the
minimum error rate training (MERT) [Och, 2003],
the NIST MT06 as the development test set and the
MT08 as the final test set. The numbers of doc-
uments/sentences in the NIST MT05, MT06 and
MT08 are 100/1082, 79/1664 and 109/1357 respec-
tively. They contain 17, 27.49 and 15.53 lexical
chains per document respectively.
We used the case-insensitive BLEU-4 (Papineni
2The training data includes LDC2003E14, LDC2004T07,
LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong
Hansards/Laws/News).
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
1569
 MT06
0.05 30.53
0.1 31.64
0.2 31.45
0.3 30.73
0.4 31.01
Table 2: BLEU scores of the probability cohesion
model Mp(TDt , {LC
k
t }
N
k=1) with different values for
the threshold .
et al, 2002) as our evaluation metric. As MERT is
normally instable, we ran the tuning process three
times for all our experiments and presented the av-
erage BLEU scores on the three MERT runs as sug-
gested by Clark et al(2011).
5.2 Setting the Threshold 
As the two lexical chain based cohesion models are
built on the super target lexical chains that are asso-
ciated with a parameter , we need to tune the thresh-
old parameter  on the development test set NIST
MT06. We conducted a group of experiments using
the probability cohesion model defined in Eq. (5)
to find the best threshold. Experiment results are
shown in Table 2.
If we set the threshold too small (e.g., 0.05), the
super target lexical chains may contain too many
noisy words that are not the translations of source
lexical chain words, which may jeopardise the qual-
ity of the super target lexical chains. The cohesion
model built on these noisy super target lexical chains
may select incorrect words rather than the proper
lexical chain words. On the other hand, if we set the
threshold too large (e.g., 0.3 or 0.4), we may take
the risk of not selecting the appropriate chain word
translations into the super target lexical chains. It
seems that the best threshold is 0.1 as we obtained
the highest BLEU score 31.64 on the NIST MT06
with this threshold. Therefore we set the threshold 
to 0.1 in all experiments thereafter.
5.3 Effect of the Count and Probability
Cohesion Model
After we found the best threshold, we carried out ex-
periments to test the effect of the two lexical chain
based cohesion models: the count and probability
cohesion model. We compared them against the
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexChainCount(top 1) 30.46 23.52 26.99
LexChainCount 30.79 23.34 27.07
LexChainProb 31.64 24.54 28.09
Table 3: Effects of the lexical chain based count and
probability cohesion models. LexChainCount: the count
model defined in Eq. (3). LexChainProb: the probability
model defined in Eq. (5).
baseline system that does not integrate any lexical
chain information. We also compared the count co-
hesion model (LexChainCount(top1)) built on the
target lexical chains where each target chain word is
the best translation of its corresponding source lex-
ical chain word according to Eq. (1). Experiment
results are shown in Table 3.
From Table 3, we can observe that
? Our lexical chain based cohesion models are
able to substantially improve the translation
quality in terms of BLEU score. We achieve
an average improvement of up to 1.21 BLEU
points over the baseline on the two test sets
MT06 and MT08.
? The count cohesion model built on the super
target lexical chains is better than that based
on the target lexical chains only with top one
translations (27.07 vs. 26.99). This shows
the advantage of the super target lexical chains
{LCkt }
N
k=1 over the standard target lexical chi-
ans {LCkt }
N
k=1.
? Finally, the probability cohesion model is much
better than the count cohesion model (28.09
vs. 27.07). This suggests that we should take
into account chain word translation probabili-
ties when we reward hypotheses where target
lexical chain words occur.
5.4 Lexical Chains vs. Lexical Cohesion
Devices
As we have mentioned in Section 2, lexical cohe-
sion devices can be also used to build lexical cohe-
sion models to capture lexical cohesion relations in a
text. We therefore want to compare our lexical chain
based cohesion models with the lexical cohesion de-
vice based cohesion models.
1570
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexDeviceTrigger 31.35 24.11 27.73
LexChainProb 31.64 24.54 28.09
Table 4: The lexical chain based probability cohesion
model (LexChainProb) vs. the lexical cohesion device
based trigger model (LexDeviceTrigger).
We re-implemented the mutual information trig-
ger model that is the best lexical cohesion model
based on lexical cohesion devices among the three
models proposed by Xiong et al (2013). The mu-
tual information trigger model measures the associ-
ation strength of two lexical cohesion items x and y
in a lexical cohesion relation xRy. In the model, it
is required that x occurs in a sentence preceding the
sentence where y occurs and that the two items have
a lexical cohesion relation such as word repetition,
synonym. The model treats x as the trigger and y as
the triggered item. The mutual information between
the trigger x and the triggered item y estimates how
possible y will occur given x is mentioned in a text.
The comparison results are reported in Table 4.
Our lexical chain based probability cohesion model
outperforms the lexical cohesion device based trig-
ger model by 0.36 BLEU points. The reason for this
superiority of our cohesion model over the trigger
model may be that the former model captures lex-
ical cohesion relations among sequences of words
through lexical chains while the latter model cap-
tures lexical cohesion relations only between two re-
lated words.
6 Conclusions
We have presented two lexical chain based cohesion
models that incorporate the lexical cohesion struc-
ture of a text into document-level machine transla-
tion. We project the lexical chains of a source docu-
ment to the corresponding target document by trans-
lating each word in each source lexical chain into
their counterparts via MaxEnt classifiers. The pro-
jected target lexical chains provide a representation
of the lexical cohesion structure of the target doc-
ument that is to be generated. We build two co-
hesion models based on the projected target lexi-
cal chains: a count model that rewards a hypothesis
according to the time of occurrence of target lexi-
cal chain words in the hypothesis and a probability
model that further takes translation probabilities into
account when rewarding hypotheses. These two co-
hesion models are used to constrain word selection
for document translation so that the generated doc-
ument is consistent with the projected lexical cohe-
sion structure.
We have integrated the two proposed cohesion
models into a hierarchical phrase-based SMT sys-
tem. Experiment results on large-scale data validate
that
? The lexical chain based cohesion models are
able to substantially improve translation qual-
ity in terms of BLEU.
? The probability cohesion model is better than
the count cohesion model.
? The lexical chain based probability cohesion
model is better than the previous mutual infor-
mation trigger model that adopts lexical cohe-
sion devices to capture lexical cohesion rela-
tions between two related words.
As we mentioned in Section 2, cohesion is closely
connected to coherence. It provides a surface indi-
cator for coherence identification (Barzilay and El-
hadad, 1997). In the future, we would like to use
lexical chains to identify coherence and incorporate
both cohesion and coherence into document-level
machine translation.
References
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In In Proceedings
of the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Beata Beigman Klebanov and Michael Flor. 2013. As-
sociative texture is lost in translation. In Proceedings
of the Workshop on Discourse in Machine Translation,
pages 27?32, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In Pro-
ceedings of the 11th Conference on Theoretical and
1571
Methodological Issues in Machine Translation, pages
43?52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 61?72, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado, June.
Association for Computational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
a chinese language technology platform. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, COLING ?10,
pages 13?16, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA, June.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, Harriman, NY, February.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
Proceedings of the 18th international joint conference
on Artificial intelligence, IJCAI?03, pages 1486?1488,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 909?919, Edinburgh, Scotland, UK., July.
Liane Guillou. 2013. Analysing lexical consistency in
translation. In Proceedings of the Workshop on Dis-
course in Machine Translation, pages 10?18, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in
English. London: Longman.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Document-wide decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1179?1190, Jeju Island,
Korea, July.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator of
the structure of text. Comput. Linguist., 17(1):21?48,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
M.A. Stairmand. 1996. A computational analysis of
lexical cohesion with applications in information re-
trieval. UMIST.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
Jo?rg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 8?15, Uppsala, Sweden, July.
Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012.
Encouraging consistent translation choices. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 417?
426, Montre?al, Canada, June.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E.Alatis, editor, Geogetown University Round
Table on Languages and Linguistics 1989, pages 89?
105, Washington, D.C. Georgetown University Press.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In Proceedings of the
1572
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1060?1068, Jeju Island,
Korea, July.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Proceedings of the 2011 MT sum-
mit XIII, pages 131?138, Xiamen, China, September.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
Proceedings of the Twenty-Seventh AAAI Conference
on Artificial Intelligence (AAAI-13), Bellevue, Wash-
ington, USA, July.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third International Joint Conference on
Artificial Intelligence (IJCAI-13), Beijing, China, Au-
gust.
1573
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306?315,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
 Exploring Syntactic Structural Features for Sub-Tree Alignment      
using Bilingual Tree Kernels 
 
         Jun Sun1,2                            Min Zhang1                    Chew Lim Tan2 
1 Institute for Infocomm Research    2School of Computing, National University of Singapore 
         sunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sg  
 
 
 
 
Abstract 
 
We propose Bilingual Tree Kernels (BTKs) to 
capture the structural similarities across a pair of 
syntactic translational equivalences and apply 
BTKs to sub-tree alignment along with some 
plain features. Our study reveals that the struc-
tural features embedded in a bilingual parse tree 
pair are very effective for sub-tree alignment 
and the bilingual tree kernels can well capture 
such features. The experimental results show 
that our approach achieves a significant im-
provement on both gold standard tree bank and 
automatically parsed tree pairs against a heuris-
tic similarity based method. We further apply 
the sub-tree alignment in machine translation 
with two methods. It is suggested that the sub-
tree alignment benefits both phrase and syntax 
based systems by relaxing the constraint of the 
word alignment. 
1 Introduction 
Syntax based Statistical Machine Translation 
(SMT) systems allow the translation process to be 
more grammatically performed, which provides 
decent reordering capability. However, most of the 
syntax based systems construct the syntactic trans-
lation rules based on word alignment, which not 
only suffers from the pipeline errors, but also fails 
to effectively utilize the syntactic structural fea-
tures. To address those deficiencies, Tinsley et al 
(2007) attempt to directly capture the syntactic 
translational equivalences by automatically con-
ducting sub-tree alignment, which can be defined 
as follows: 
A sub-tree alignment process pairs up sub-tree 
pairs across bilingual parse trees whose contexts 
are semantically translational equivalent. Accord-
ing to Tinsley et al (2007), a sub-tree aligned 
parse tree pair follows the following criteria: 
(i) a node can only be linked once; 
(ii) descendants of a source linked node may 
only link to descendants of its target 
linked counterpart; 
(iii) ancestors of a source linked node may on-
ly link to ancestors of its target linked 
counterpart. 
By sub-tree alignment, translational equivalent 
sub-tree pairs are coupled as aligned counterparts. 
Each pair consists of both the lexical constituents 
and their maximum tree structures generated over 
the lexical sequences in the original parse trees. 
Due to the 1-to-1 mapping between sub-trees and 
tree nodes, sub-tree alignment can also be consi-
dered as node alignment by conducting multiple 
links across the internal nodes as shown in Fig. 1. 
Previous studies conduct sub-tree alignments by 
either using a rule based method or conducting 
some similarity measurement only based on lexi-
cal features. Groves et al (2004) conduct sub-tree 
alignment by using some heuristic rules, lack of 
extensibility and generality. Tinsley et al (2007) 
 
Figure 1: Sub-tree alignment as referred to  
Node alignment 
306
and Imamura (2001) propose some score functions 
based on the lexical similarity and co-occurrence. 
These works fail to utilize the structural features, 
rendering the syntactic rich task of sub-tree align-
ment less convincing and attractive. This may be 
due to the fact that the syntactic structures in a 
parse tree pair are hard to describe using plain fea-
tures. In addition, explicitly utilizing syntactic tree 
fragments results in exponentially high dimen-
sional feature vectors, which is hard to compute. 
Alternatively, convolution parse tree kernels (Col-
lins and Duffy, 2001), which implicitly explore 
the tree structure information, have been success-
fully applied in many NLP tasks, such as Semantic 
parsing (Moschitti, 2004) and Relation Extraction 
(Zhang et al 2006). However, all those studies are 
carried out in monolingual tasks. In multilingual 
tasks such as machine translation, tree kernels are 
seldom applied. 
In this paper, we propose Bilingual Tree Ker-
nels (BTKs) to model the bilingual translational 
equivalences, in our case, to conduct sub-tree 
alignment. This is motivated by the decent effec-
tiveness of tree kernels in expressing the similarity 
between tree structures. We propose two kinds of 
BTKs named dependent Bilingual Tree Kernel 
(dBTK), which takes the sub-tree pair as a whole 
and independent Bilingual Tree Kernel (iBTK), 
which individually models the source and the tar-
get sub-trees. Both kernels can be utilized within 
different feature spaces using various representa-
tions of the sub-structures.  
Along with BTKs, various lexical and syntactic 
structural features are proposed to capture the cor-
respondence between bilingual sub-trees using a 
polynomial kernel. We then attempt to combine 
the polynomial kernel and BTKs to construct a 
composite kernel. The sub-tree alignment task is 
considered as a binary classification problem. We 
employ a kernel based classifier with the compo-
site kernel to classify each candidate of sub-tree 
pair as aligned or unaligned. Then a greedy search 
algorithm is performed according to the three cri-
teria of sub-tree alignment within the space of 
candidates classified as aligned. 
We evaluate the sub-tree alignment on both the 
gold standard tree bank and an automatically 
parsed corpus. Experimental results show that the 
proposed BTKs benefit sub-tree alignment on both 
corpora, along with the lexical features and the 
plain structural features. Further experiments in 
machine translation also suggest that the obtained 
sub-tree alignment can improve the performance 
of both phrase and syntax based SMT systems.  
2 Bilingual Tree Kernels 
In this section, we propose the two BTKs and 
study their capability and complexity in modeling 
the bilingual structural similarity. Before elaborat-
ing the concepts of BTKs, we first illustrate some 
notations to facilitate further understanding.  
Each sub-tree pair ?? ? ?? can be explicitly de-
composed into multiple sub-structures which be-
long to the given sub-structure spaces. ?? ?
???, ? , ??, ? , ???  refers to the source tree sub-
structure space; while ?? ? ???, ? , ??, ? , ???  refers 
to the target sub-structure space. A sub-structure 
pair ???, ??? refers to an element in the set of the 
Cartesian product of the two sub-structure spaces: 
???, ??? ? ?? ? ??.  
2.1 Independent Bilingual Tree Kernel 
(iBTK) 
Given the sub-structure spaces ?? and ??, we con-
struct two vectors using the integer counts of the 
source and target sub-structures: 
 
???? ? ?#????, ? , #????, ? , #??|??|?? 
???? ? ?#????, ? , #????, ? , #???????? 
 
where #???? and #???? are the numbers of oc-
currences of the sub-structures ?? and ??. In order 
to compute the dot product of the feature vectors 
in the exponentially high dimensional feature 
space, we introduce the tree kernel functions as 
follows: 
 
??????? ? ?, ?? ? ??? ? ???, ??? ? ???, ??? 
 
The iBTK is defined as a composite kernel con-
sisting of a source tree kernel and a target tree 
kernel which measures the source and the target 
structural similarity respectively. Therefore, the 
composite kernel can be computed using the ordi-
nary monolingual tree kernels (Collins and Duffy, 
2001). 
 
           ???, ??? ?? ????, ????? ? 
? ? ?? ??????????? ? ? ?????
? ???????? ?
|??|
???   
           ? ? ? ????, ??? ?????????????   
 
where ??  and ???  refer to the node sets of the 
source sub-tree ?  and ??  respectvely. ??????  is an 
indicator function which equals to 1 iff the sub-
structure ??  is rooted at the node ??  and 0 other-
wise.????, ??? ? ? ? ??????? ? ?????? ??
|??|
???  is the num-
ber of identical sub-structures rooted at ?? and ??? . 
Then we compute the ????, ??? ? function as follows: 
 
307
(1) If the production rule at ?? and ???  are different, 
????, ??? ? ? 0; 
(2)else if both ??and ??? are POS tags, ????, ??? ? ? ?; 
(3)else, ????, ??? ? ? ?? ?1 ? ??????, ??, ????? , ????
??????
??? . 
where ?????? is the child number of ??,  ????, ?? 
is the lth child of ??, ? is the decay factor used to 
make the kernel value less variable with respect to 
the number of sub-structures. 
Similarly, we can decompose the target kernel 
as ???, ??? ? ? ? ????, ???????????????? and run the 
algorithm above as well. 
The disadvantage of the iBTK is that it fails to 
capture the correspondence across the sub-
structure pairs. However, the composite style of 
constructing the iBTK helps keep the computa-
tional complexity comparable to the monolingual 
tree kernel, which is ??| ??| ? | ???|  ? |??| ? |??? |?. 
2.2 Dependent Bilingual Tree Kernel (dBTK) 
The iBTK explores the structural similarity of the 
source and the target sub-trees respectively. As an 
alternative, we further define a kernel to capture 
the relationship across the counterparts without 
increasing the computational complexity. As a 
result, we propose the dependent Bilingual Tree 
kernel (dBTK) to jointly evaluate the similarity 
across sub-tree pairs by enlarging the feature 
space to the Cartesian product of the two sub-
structure sets. 
A dBTK takes the source and the target sub-
structure pair as a whole and recursively calculate 
over the joint sub-structures of the given sub-tree 
pair. We define the dBTK as follows:  
Given the sub-structure space ?? ? ?? , we con-
struct a vector using the integer counts of the sub-
structure pairs to represent a sub-tree pair: 
 
??? ? ?? ? ?
#???, ???, ? , #???, ??????, #???, ???,
? , #??|??|, ???, ? , #??|??|, ??????
? 
 
where #???, ??? is the number of occurrences of 
the sub-structure pair ???, ???.  
 
 ??????? ? ?, ?? ? ??? 
 
?? ??? ? ??, ???? ? ??? ?  
 
? ? ?
? ? ?????, ????????????? ?
? ? ?????? , ??? ???????????????
?
|?????|
???   
? ? ? ? ? ??
???, ???,
???? , ????
?????????????????????????    (1) 
? ? ? ? ? ?
????, ??? ? ?
????, ????
?????????????????????????   (2) 
 
? ? ? ????, ??? ????????????? ? ? ????, ??
??????????????   
 
? ???, ??? ? ???, ??? 
It is infeasible to explicitly compute the kernel 
function by expressing the sub-trees as feature 
vectors. In order to achieve convenient computa-
tion, we deduce the kernel function as the above. 
The deduction from (1) to (2) is derived accord-
ing to the fact that the number of identical sub-
structure pairs rooted in the node pairs ???, ??? and 
???? , ????  equals to the product of the respective 
counts. As a result, the dBTK can be evaluated as 
a product of two monolingual tree kernels. Here 
we verify the correctness of the kernel by directly 
constructing the feature space for the inner prod-
uct. Alternatively, Cristianini and Shawe-Taylor 
(2000) prove the positive semi-definite characte-
ristic of the tensor product of two kernels. The 
decomposition benefits the efficient computation 
to use the algorithm for the monolingual tree ker-
nel in Section 2.1. 
The computational complexity of the dBTK is 
still ??| ??| ? | ???|  ? |??| ? |??? |?. 
3 Sub-structure Spaces for BTKs 
The syntactic translational equivalences under 
BTKs are evaluated with respective to the sub-
structures factorized from the candidate sub-tree 
pairs. In this section, we propose different sub-
structures to facilitate the measurement of syntac-
tic similarity for sub-tree alignment. Since the 
proposed BTKs can be computed by individually 
evaluating the source and target monolingual tree 
kernels, the definition of the sub-structure can be 
simplified to base only on monolingual sub-trees. 
3.1 Subset Tree 
Motivated from Collins and Duffy (2002) in mo-
nolingual tree kernels, the Subset Tree (SST) can 
be employed as sub-structures. An SST is any sub-
graph, which includes more than one non-terminal 
node, with the constraint that the entire rule pro-
ductions are included. Fig. 2 shows an example of 
the SSTs decomposed from the source sub-tree 
rooted at VP*.  
3.2 Root directed Subset Tree 
Monolingual Tree kernels achieve decent perfor-
mance using the SSTs due to the rich exploration 
of syntactic information. However, the sub-tree 
alignment task requires strong capability of dis-
criminating the sub-trees with their roots across 
adjacent generations, because those candidates 
share many identical SSTs. As illustrated in Fig 2, 
the source sub-tree rooted at VP*, which should 
be aligned to the target sub-tree rooted at NP*, 
may be likely aligned to the sub-tree rooted at PP*, 
308
which shares quite a similar context with NP*. It 
is also easy to show that the latter shares all the 
SSTs that the former obtains. In consequence, the 
values of the SST based kernel function are quite 
similar between the candidate sub-tree pair rooted 
at (VP*,NP*) and (VP*,PP*). 
In order to effectively differentiate the candi-
dates like the above, we propose the Root directed 
Subset Tree (RdSST) by encapsulating each SST 
with the root of the given sub-tree. As shown in 
Fig 2, a sub-structure is considered identical to the 
given examples, when the SST is identical and the 
root tag of the given sub-tree is NP. As a result, 
the kernel function in Section 2.1 is re-defined as: 
 
???, ??? ? ? ? ????, ??? ?????, ????????????????   
                      ? ????, ???? ? ? ????, ??? ?????????????   
 
where ??  and ???  are the root nodes of the sub-
tree ?  and ?? respectively. The indicator function 
????, ???? equals to 1 if ?? and ??? are identical, and 0 
otherwise. Although defined for individual SST, 
the indicator function can be evaluated outside the 
summation, without increasing the computational 
complexity of the kernel function. 
3.3 Root generated Subset Tree 
Some grammatical tags (NP/VP) may have iden-
tical tags as their parents or children which may 
make RdSST less effective. Consequently, we step 
further to propose the sub-structure of Root gener-
ated Subset Tree (RgSST). An RgSST requires the 
root node of the given sub-tree to be part of the 
sub-structure. In other words, all sub-structures 
should be generated from the root of the given 
sub-tree as presented in Fig. 2. Therefore the ker-
nel function can be simplified to only capture the 
sub-structure rooted at the root of the sub-tree. 
 
???, ??? ? ????, ???? 
 
where ??  and ???  are the root nodes of the sub-
tree ? and ?? respectively. The time complexity is 
reduced to  ??| ??| ? | ???| ? |??| ? |??? |?.   
3.4 Root only 
More aggressively, we can simplify the kernel to 
only measure the common root node without con-
sidering the complex tree structures. Therefore the 
kernel function is simplified to be a binary func-
tion with time complexity ??1?. 
 
???, ??? ? ????, ???? 
4 Plain features 
Besides BTKs, we introduce various plain lexical 
features and structural features which can be ex-
pressed as feature functions. The lexical features 
with directions are defined as conditional feature 
functions based on the conditional lexical transla-
tion probabilities. The plain syntactic structural 
features can deal with the structural divergence of 
bilingual parse trees in a more general perspective. 
4.1 Lexical and Word Alignment Features  
In this section, we define seven lexical features to 
measure semantic similarity of a given sub-tree 
pair.  
Internal Lexical Features: We define two lex-
ical features with respective to the internal span of 
the sub-tree pair. 
????|?? ? ?? ? ???|???????????????? ?
?
|?????|  
 
 
 
Figure 2: Illustration of SST, RdSST and RgSST  
309
????|?? ? ?? ? ???|???????????????? ?
?
|?????|  
 
where ???|??  refers to the lexical translation 
probability from the source word ?  to the target 
word ?  within the sub-tree spans, while ???|?? 
refers to that from target to source; ????? refers to 
the word set for the internal span of the source 
sub-tree ?, while ????? refers to that of the target 
sub-tree ?. 
Internal-External Lexical Features: These 
features are motivated by the fact that lexical 
translation probabilities within the translational 
equivalence tend to be high, and that of the non-
equivalent counterparts tend to be low. 
????|?? ? ?? ? ???|????????????????? ?
?
|?????|  
????|?? ? ?? ? ???|????????????????? ?
?
|?????|  
 
where ?????? refers to the word set for the ex-
ternal span of the source sub-tree ?, while ?????? 
refers to that of the target sub-tree ?.  
Internal Word Alignment Features: The word 
alignment links account much for the co-
occurrence of the aligned terms. We define the 
internal word alignment features as follows: 
????, ?? ?
? ? ???, ?? ? ????|?? ? ???|???
?
?
??????????????
?|?????| ? |?????|?
?
?
 
where 
          ???, ?? ? ?1        if ??, ?? is aligned 
0                       otherwise
 
 
The binary function ???, ?? is employed to trig-
ger the computation only when a word aligned 
link exists for the two words ??, ?? within the sub-
tree span. 
Internal-External Word Alignment Features: 
Similar to the lexical features, we also introduce 
the internal-external word alignment features as 
follows: 
????, ?? ?
? ? ???, ?? ? ????|?? ? ???|???
?
?
???????????????
?|??????| ? |?????|?
?
?
 
????, ?? ?
? ? ???, ?? ? ????|?? ? ???|???
?
?
???????????????
?|?????| ? |??????|?
?
?
 
where 
          ???, ?? ? ?1        if ??, ?? is aligned 
0                        otherwise
 
4.2 Online Structural Features 
In addition to the lexical correspondence, we also 
capture the structural divergence by introducing 
the following tree structural features. 
Span difference: Translational equivalent sub-
tree pairs tend to share similar length of spans. 
Thus the model will penalize the candidate sub-
tree pairs with largely different length of spans. 
 
????, ?? ? ?
|?????|
|?????|
? |??
???|
|?????| 
?  
 
? and ? refer to the entire source and target parse 
trees respectively. Therefore, |?????| and |?????| are 
the respective span length of the parse tree used for 
normalization. 
Number of Descendants: Similarly, the num-
ber of the root?s descendants of the aligned sub-
trees should also correspond. 
 
????, ?? ? ?
|????|
|????|
? |?
???|
|????|
?  
 
where ??. ? refers to the descendant set of the 
root to a sub-tree. 
Tree Depth difference: Intuitively, translation-
al equivalent sub-tree pairs tend to have similar 
depth from the root of the parse tree. We allow the 
model to penalize the candidate sub-tree pairs with 
quite different distance of path from the root of the 
parse tree to the root of the sub-tree.  
 
????, ?? ? ?
????????
?????????
? ?????
???
?????????
?  
5 Alignment Model 
Given feature spaces defined in the last two sec-
tions, we propose a 2-phase sub-tree alignment 
model as follows:  
In the 1st phase, a kernel based classifier, SVM 
in our study, is employed to classify each candi-
date sub-tree pair as aligned or unaligned. The 
feature vector of the classifier is computed using a 
composite kernel: 
 
    ??? ? ?, ?? ? ??? ? 
 
   ??????? ? ?, ?? ? ??? ? ? ???????
? ?? ? ?, ?? ? ???K???   
 
?????,?? is the normalized form of the polynomi-
al kennel ????,?? , which is a polynomial kernel 
with the degree of 2, utilizing the plain features. 
?????
? ??,??  is the normalized form of the BTK 
????
? ??,?? , exploring the corresponding sub-
structure space. The composite kernel can be con-
structed using the polynomial kernel for plain fea-
tures and various BTKs for tree structure by linear 
combination with coefficient ??, where ? ??K??? ? 1. 
In the 2nd phase, we adopt a greedy search with 
respect to the alignment probabilities. Since SVM 
is a large margin based discriminative classifier 
rather than a probabilistic model, we introduce a 
sigmoid function to convert the distance against 
the hyperplane to a posterior alignment probability 
as follows: 
310
????|?, ?? ?
1
1 ? ????
 
 
????|?, ?? ?
1
1 ? ????
 
 
where ?? is the distance for the instances classi-
fied as aligned and ??  is that for the unaligned. 
We use ????|?, ??  as the confidence to conduct 
the sure links for those classified as aligned. On 
this perspective, the alignment probability is suit-
able as a searching metric. The search space is 
reduced to that of the candidates classified as 
aligned after the 1st phase. 
6 Experiments on Sub-Tree Alignments 
In order to evaluate the effectiveness of the align-
ment model and its capability in the applications 
requiring syntactic translational equivalences, we 
employ two corpora to carry out the sub-tree 
alignment evaluation. The first is HIT gold stan-
dard English Chinese parallel tree bank referred as 
HIT corpus1. The other is the automatically parsed 
bilingual tree pairs selected from FBIS corpus (al-
lowing minor parsing errors) with human anno-
tated sub-tree alignment. 
6.1 Data preparation 
HIT corpus, which is collected from English learn-
ing text books in China as well as example sen-
tences in dictionaries, is used for the gold standard 
corpus evaluation. The word segmentation, toke-
nization and parse-tree in the corpus are manually 
constructed or checked. The corpus is constructed 
with manually annotated sub-tree alignment. The 
annotation strictly reserves the semantic equiva-
lence of the aligned sub-tree pair. Only sure links 
are conducted in the internal node level, without 
considering possible links adopted in word align-
ment. A different annotation criterion of the Chi-
nese parse tree, designed by the annotator, is em-
ployed. Compared with the widely used Penn 
TreeBank annotation, the new criterion utilizes 
some different grammar tags and is able to effec-
tively describe some rare language phenomena in 
Chinese. The annotator still uses Penn TreeBank 
annotation on the English side. The statistics of 
HIT corpus used in our experiment is shown in 
Table 1. We use 5000 sentences for experiment 
and divide them into three parts, with 3k for train-
ing, 1k for testing and 1k for tuning the parameters 
of kernels and thresholds of pruning the negative 
instances. 
                                                 
1HIT corpus is designed and constructed by HIT-MITLAB. 
http://mitlab.hit.edu.cn/index.php/resources.html .  
Most linguistically motivated syntax based 
SMT systems require an automatic parser to per-
form the rule induction. Thus, it is important to 
evaluate the sub-tree alignment on the automati-
cally parsed corpus with parsing errors. In addition, 
HIT corpus is not applicable for MT experiment 
due to the problems of domain divergence, annota-
tion discrepancy (Chinese parse tree employs a 
different grammar from Penn Treebank annota-
tions) and degree of tolerance for parsing errors. 
Due to the above issues, we annotate a new data 
set to apply the sub-tree alignment in machine 
translation. We randomly select 300 bilingual sen-
tence pairs from the Chinese-English FBIS corpus 
with the length ? 30 in both the source and target 
sides. The selected plain sentence pairs are further 
parsed by Stanford parser (Klein and Manning, 
2003) on both the English and Chinese sides. We 
manually annotate the sub-tree alignment for the 
automatically parsed tree pairs according to the 
definition in Section 1. To be fully consistent with 
the definition, we strictly reserve the semantic 
equivalence for the aligned sub-trees to keep a 
high precision. In other words, we do not conduct 
any doubtful links. The corpus is further divided 
into 200 aligned tree pairs for training and 100 for 
testing as shown in Table 2. 
6.2 Baseline approach 
We implement the work in Tinsley et al (2007) as 
our baseline methodology. 
Given a tree pair ? ?, ? ? , the baseline ap-
proach first takes all the links between the sub-tree 
pairs as alignment hypotheses, i.e., the Cartesian 
product of the two sub-tree sets: 
? ??, ? , ??, ? , ??? ? ? ??, ? , ?? , ? , ??? 
 By using the lexical translation probabilities, 
each hypothesis is assigned an alignment score. 
All hypotheses with zero score are pruned out. 
 Chinese English 
# of Sentence pair 300 
Avg. Sentence Length 16.94 20.81 
Avg. # of sub-tree 28.97 34.39 
Avg. # of alignment 17.07 
 
Table 2. Statistics of FBIS selected Corpus 
 Chinese English 
# of Sentence pair 5000 
Avg. Sentence Length 12.93 12.92 
Avg. # of sub-tree 21.40 23.58 
Avg. # of alignment 11.60 
 
Table 1. Corpus Statistics for HIT corpus 
311
Then the algorithm iteratively selects the link of 
the sub-tree pairs with the maximum score as a 
sure link, and blocks all hypotheses that contradict 
with this link and itself, until no non-blocked hy-
potheses remain. 
The baseline system uses many heuristics in 
searching the optimal solutions with alternative 
score functions. Heuristic skip1 skips the tied hy-
potheses with the same score, until it finds the 
highest-scoring hypothesis with no competitors of 
the same score. Heuristic skip2 deals with the 
same problem. Initially, it skips over the tied hy-
potheses. When a hypothesis sub-tree pair ? ??, ???  
without any competitor of the same score is found, 
where neither ?? nor ?? has been skipped over, the 
hypothesis is chosen as a sure link. Heuristic 
span1 postpones the selection of the hypotheses 
on the POS level. Since the highest-scoring hypo-
theses tend to appear on the leaf nodes, it may in-
troduce ambiguity when conducting the alignment 
for a POS node whose child word appears twice in 
a sentence. 
The baseline method proposes two score func-
tions based on the lexical translation probability. 
They also compute the score function by splitting 
the tree into the internal and external components. 
Tinsley et al (2007) adopt the lexical transla-
tion probabilities dumped by GIZA++ (Och and 
Ney, 2003) to compute the span based scores for 
each pair of sub-trees. Although all of their heuris-
tics combinations are re-implemented in our study, 
we only present the best result among them with 
the highest Recall and F-value as our baseline, 
denoted as skip2_s1_span12. 
                                                 
2  s1 denotes score function 1 in Tinsley et al (2007), 
skip2_s1_span1 denotes the utilization of heuristics skip2 and 
span1 while using score function 1 
6.3 Experimental settings 
We use SVM with binary classes as the classifier. 
In case of the implementation, we modify the Tree 
Kernel tool (Moschitti, 2004) and SVMLight 
(Joachims, 1999). The coefficient ?? for the com-
posite kernel are tuned with respect to F-measure 
(F) on the development set of HIT corpus. We 
empirically set C=2.4 for SVM and use ? ? 0.23,  
the default parameter ? ? 0.4 for BTKs. 
Since the negative training instances largely 
overwhelm the positive instances, we prune the 
negative instances using the thresholds according 
to the lexical feature functions (??, ??, ??, ??) and 
online structural feature functions (??, ??, ?? ). 
Those thresholds are also tuned on the develop-
ment set of HIT corpus with respect to F-measure.  
To learn the lexical and word alignment fea-
tures for both the proposed model and the baseline 
method, we train GIZA++ on the entire FBIS bi-
lingual corpus (240k). The evaluation is conducted 
by means of Precision (P), Recall (R) and F-
measure (F). 
6.4 Experimental results 
In Tables 3 and 4, we incrementally enlarge the 
feature spaces in certain order for both corpora 
and examine the feature contribution to the align-
ment results. In detail, the iBTKs and dBTKs are 
firstly combined with the polynomial kernel for 
plain features individually, then the best iBTK and 
dBTK are chosen to construct a more complex 
composite kernel along with the polynomial kernel 
for both corpora. The experimental results show 
that: 
? All the settings with structural features of the 
proposed approach achieve better performance 
than the baseline method. This is because the 
Feature Space P R F 
Lex 73.48 71.66 72.56 
Lex +Online Str 77.02 73.63 75.28 
Plain +dBTK-STT 81.44 74.42 77.77 
Plain +dBTK-RdSTT 81.40 69.29 74.86 
Plain +dBTK-RgSTT 81.90 67.32 73.90 
Plain +dBTK-Root 78.60 80.90 79.73 
Plain +iBTK-STT 82.94 79.44 81.15 
Plain +iBTK-RdSTT 83.14 80 81.54 
Plain +iBTK-RgSTT 83.09 79.72 81.37 
Plain +iBTK-Root 78.61 79.49 79.05 
Plain +dBTK-Root  
         +iBTK-RdSTT 
82.70 82.70 82.70 
   Baseline 70.48 78.70 74.36 
 
Table 4. Structure feature contribution for FBIS test set 
Feature Space P R F 
Lex 61.62 58.33 59.93 
Lex +Online Str 70.08 69.02 69.54 
Plain +dBTK-STT 80.36 78.08 79.20 
Plain +dBTK-RdSTT 87.52 74.13 80.27 
Plain +dBTK-RgSTT 88.54 70.18 78.30 
Plain +dBTK-Root 81.05 84.38 82.68 
Plain +iBTK-STT 81.57 73.51 77.33 
Plain +iBTK-RdSTT 82.27 77.85 80.00 
Plain +iBTK-RgSTT 82.92 78.77 80.80 
Plain +iBTK-Root 76.37 76.81 76.59 
Plain +dBTK-Root  
         +iBTK-RgSTT 
85.53 85.12 85.32 
   Baseline 64.14 66.99 65.53 
 
Table 3. Structure feature contribution for HIT test set 
*Plain= Lex +Online Str 
312
baseline only assesses semantic similarity using 
the lexical features. The improvement suggests 
that the proposed framework with syntactic 
structural features is more effective in modeling 
the bilingual syntactic correspondence. 
? By introducing BTKs to construct a composite 
kernel, the performance in both corpora is sig-
nificantly improved against only using the poly-
nomial kernel for plain features. This suggests 
that the structural features captured by BTKs are 
quite useful for the sub-tree alignment task. We 
also try to use BTKs alone without the poly-
nomial kernel for plain features; however, the 
performance is rather low. This suggests that the 
structure correspondence cannot be used to 
measure the semantically equivalent tree struc-
tures alone, since the same syntactic structure 
tends to be reused in the same parse tree and 
lose the ability of disambiguation to some extent. 
In other words, to capture the semantic similari-
ty, structure features requires lexical features to 
cooperate. 
? After comparing iBTKs with the corresponding 
dBTKs, we find that for FBIS corpus, iBTK 
greatly outperforms dBTK in any feature space 
except the Root space. However, when it comes 
the HIT corpus, the gaps between the corres-
ponding iBTKs and dBTKs are much closer, 
while on the Root space, dBTK outperforms 
iBTK to a large amount. This finding can be ex-
plained by the relationship between the amount 
of training data and the high dimensional feature 
space. Since dBTKs are constructed in a joint 
manner which obtains a much larger high di-
mensional feature space than those of iBTKs, 
dBTKs require more training data to excel its 
capability, otherwise it will suffer from the data 
sparseness problem. The reason that dBTK out-
performs iBTK in the feature space of Root in 
FBIS corpus is that although it is a joint feature 
space, the Root node pairs can be constructed 
from a close set of grammar tags and to form a 
relatively low dimensional space. 
    As a result, when applying to FBIS corpus, 
which only contains limited amount of training 
data, dBTKs will suffer more from the data 
sparseness problem, and therefore, a relatively 
low performance. When enlarging the amount of 
training corpus to the HIT corpus, the ability of 
dBTKs excels and the benefit from data increas-
ing of dBTKs is more significant than iBTKs.  
? We also find that the introduction of BTKs gains 
more improvement in HIT gold standard corpus 
than in FBIS corpus.  Other than the factor of 
the amount of training data, this is also because 
the plain features in Table 3 are not as effective 
as those in Table 4, since they are trained on 
FBIS corpus which facilitates Table 4 more with 
respect to the domains. On the other hand, the 
grammatical tags and syntactic tree structures 
are more accurate in HIT corpus, which facili-
tates the performance of BTKs in Table 3. 
? On the comparison across the different feature 
spaces of BTKs, we find that STT, RdSTT and 
TgSTT are rather selective, since Recalls of 
those feature spaces are relatively low, exp. for 
HIT corpus. However, the Root sub-structure 
obtains a satisfactory Recall for both corpora. 
That?s why we attempt to construct a more 
complex composite kernel in adoption of the 
kernel of dBTK-Root as below. 
? To gain an extra performance boosting, we fur-
ther construct a composite kernel which includes 
the best iBTK and the best dBTK for each cor-
pus along with the polynomial kernel for plain 
features. In the HIT corpus, we use dBTK in the 
Root space and iBTK in the RgSST space; while 
for FBIS corpus, we use dBTK in the Root 
space and iBTK in the RdSST space. The expe-
rimental results suggest that by combining iBTK 
and dBTK together, we can achieve more im-
provement. 
7 Experiments on Machine Translation 
In addition to the intrinsic alignment evaluation, 
we further conduct the extrinsic MT evaluation. 
We explore the effectiveness of sub-tree alignment 
for both phrase based and linguistically motivated 
syntax based SMT systems. 
7.1 Experimental configuration 
In the experiments, we train the translation model 
on FBIS corpus (7.2M (Chinese) + 9.2M (English) 
words in 240,000 sentence pairs) and train a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002). We use these 
sentences with less than 50 characters from the 
NIST MT-2002 test set as the development set (to 
speed up tuning for syntax based system) and the 
NIST MT-2005 test set as our test set. We use the 
Stanford parser (Klein and Manning, 2003) to 
parse bilingual sentences on the training set and 
Chinese sentences on the development and test set. 
The evaluation metric is case-sensitive BLEU-4. 
313
For the phrase based system, we use Moses 
(Koehn et al, 2007) with its default settings. For 
the syntax based system, since sub-tree alignment 
can directly benefit Tree-2-Tree based systems, 
we apply the sub-tree alignment in a syntax sys-
tem based on Synchronous Tree Substitution 
Grammar (STSG) (Zhang et al, 2007). The STSG 
based decoder uses a pair of elementary tree3 as a 
basic translation unit. Recent research on tree 
based systems shows that relaxing the restriction 
from tree structure to tree sequence structure 
(Synchronous Tree Sequence Substitution Gram-
mar: STSSG) significantly improves the transla-
tion performance (Zhang et al, 2008). We imple-
ment the STSG/STSSG based model in the Pisces 
decoder with the identical features and settings in 
Sun et al (2009). In the Pisces decoder, the 
STSSG based decoder translates each span itera-
tively in a bottom up manner which guarantees 
that when translating a source span, any of its sub-
spans is already translated. The STSG based de-
coding can be easily performed with the STSSG 
decoder by restricting the translation rule set to be 
elementary tree pairs only. 
As for the alignment setting, we use the word 
alignment trained on the entire FBIS (240k) cor-
pus by GIZA++ with heuristic grow-diag-final for 
both Moses and the syntax system. For sub-tree-
alignment, we use the above word alignment to 
learn lexical/word alignment feature, and train 
with the FBIS training corpus (200) using the 
composite kernel of Plain+dBTK-Root+iBTK-
RdSTT. 
7.2 Experimental results 
Compared with the adoption of word alignment, 
translational equivalences generated from struc-
tural alignment tend to be more grammatically 
                                                 
3 An elementary tree is a fragment whose leaf nodes can be 
either non-terminal symbols or terminal symbols.  
aware and syntactically meaningful. However, 
utilizing syntactic translational equivalences alone 
for machine translation loses the capability of 
modeling non-syntactic phrases (Koehn et al, 
2003). Consequently, instead of using phrases 
constraint by sub-tree alignment alone, we attempt 
to combine word alignment and sub-tree align-
ment and deploy the capability of both with two 
methods. 
? Directly Concatenate (DirC) is operated by di-
rectly concatenating the rule set genereted from 
sub-tree alignment and the original rule set gen-
erated from word alignment (Tinsley et al, 
2009). As shown in Table 5, we gain minor im-
provement in the Bleu score for all configura-
tions. 
? Alternatively, we proposed a new approach to 
generate the rule set from the scratch. We con-
strain the bilingual phrases to be consistent with 
Either Word alignment or Sub-tree alignment 
(EWoS) instead of being originally consistent 
with the word alignment only. The method helps 
tailoring the rule set decently without redundant 
counts for syntactic rules. The performance is 
further improved compared to DirC in all sys-
tems. 
The findings suggest that with the modeling of 
non-syntactic phrases maintained, more emphasis 
on syntactic phrases can benefit both the phrase 
and syntax based SMT systems. 
8 Conclusion 
In this paper, we explore syntactic structure fea-
tures by means of Bilingual Tree Kernels and ap-
ply them to bilingual sub-tree alignment along 
with various lexical and plain structural features. 
We use both gold standard tree bank and the au-
tomatically parsed corpus for the sub-tree align-
ment evaluation. Experimental results show that 
our model significantly outperforms the baseline 
method and the proposed Bilingual Tree Kernels 
are very effective in capturing the cross-lingual 
structural similarity. Further experiment shows 
that the obtained sub-tree alignment benefits both 
phrase and syntax based MT systems by deliver-
ing more weight on syntactic phrases. 
 
Acknowledgments 
We thank MITLAB4 in Harbin Institute of Tech-
nology for licensing us their sub-tree alignment 
corpus for our research. 
                                                 
4 http://mitlab.hit.edu.cn/  .  
System Model BLEU 
Moses BP* 23.86 
 DirC  23.98 
EWoS  24.48 
Syntax 
STSG 
STSG 24.71 
DirC  25.16 
 EWoS  25.38 
Syntax STSSG 25.92 
STSSG DirC  25.95 
 EWoS  26.45 
 
Table 5. MT evaluation on various systems 
*BP denotes bilingual phrases 
314
References  
David Burkett and Dan Klein. 2008. Two languages 
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP-08. 877-886. 
Nello Cristianini and John Shawe-Taylor. 2000. An 
introduction to support vector machines and other 
kernelbased learning methods. Cambridge: Cam-
bridge University Press. 
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS-01. 
Declan Groves, Mary Hearne and Andy Way. 2004. 
Robust sub-sentential alignment of phrase-structure 
trees. In Proceedings of COLING-04, pages 1072-
1078. 
Kenji Imamura. 2001. Hierarchical Phrase Alignment 
Harmonized with Parsing. In Proceedings of 
NLPRS-01, Tokyo. 377-384. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical. In B. Sch?olkopf, C. Burges, and 
A. Smola, editors, Advances in Kernel Methods - 
Support Vector Learning, MIT press. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL-
03. 423-430. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL-03. 48-54. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of ACL-07. 177-180. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51, March. 
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Proceed-
ings of ACL-04. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP-02. 
901-904. 
Jun Sun, Min Zhang and Chew Lim Tan. 2009. A non-
contiguous Tree Sequence Alignment-based Model 
for Statistical Machine Translation. In Proceedings 
of ACL-IJCNLP-09. 914-922. 
John Tinsley, Ventsislav Zhechev, Mary Hearne and 
Andy Way. 2007. Robust language pair-independent 
sub-tree alignment. In Proceedings of MT Summit 
XI -07. 
John Tinsley, Mary Hearne and Andy Way. 2009. Pa-
rallel treebanks in phrase-based statistical machine 
translation. In Proceedings of CICLING-09. 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-COLING-06. 825-
832. 
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A tree-to-tree align-
ment-based model for statistical machine transla-
tion. In Proceedings of MT Summit XI -07. 535-542. 
Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based tree-to-tree translation model. In 
Proceedings of ACL-08. 559-567. 
315
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710?719,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Kernel Based Discourse Relation Recognition with Temporal  
Ordering Information 
 
 
WenTing Wang1                   Jian Su1                   Chew Lim Tan2 
1Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis 
Singapore 138632 
{wwang,sujian}@i2r.a-star.edu.sg 
2Department of Computer Science 
University of Singapore 
Singapore 117417 
tacl@comp.nus.edu.sg 
 
  
 
Abstract 
Syntactic knowledge is important for dis-
course relation recognition. Yet only heu-
ristically selected flat paths and 2-level 
production rules have been used to incor-
porate such information so far. In this 
paper we propose using tree kernel based 
approach to automatically mine the syn-
tactic information from the parse trees for 
discourse analysis, applying kernel func-
tion to the tree structures directly. These 
structural syntactic features, together 
with other normal flat features are incor-
porated into our composite kernel to cap-
ture diverse knowledge for simultaneous 
discourse identification and classification 
for both explicit and implicit relations. 
The experiment shows tree kernel ap-
proach is able to give statistical signifi-
cant improvements over flat syntactic 
path feature. We also illustrate that tree 
kernel approach covers more structure in-
formation than the production rules, 
which allows tree kernel to further incor-
porate information from a higher dimen-
sion space for possible better discrimina-
tion. Besides, we further propose to leve-
rage on temporal ordering information to 
constrain the interpretation of discourse 
relation, which also demonstrate statistic-
al significant improvements for discourse 
relation recognition on PDTB 2.0 for 
both explicit and implicit as well. 
1 Introduction 
Discourse relations capture the internal structure 
and logical relationship of coherent text, includ-
ing Temporal, Causal and Contrastive relations 
etc. The ability of recognizing such relations be-
tween text units including identifying and classi-
fying provides important information to other 
natural language processing systems, such as 
language generation, document summarization, 
and question answering. For example, Causal 
relation can be used to answer more sophisti-
cated, non-factoid ?Why? questions. 
Lee et al (2006) demonstrates that modeling 
discourse structure requires prior linguistic anal-
ysis on syntax. This shows the importance of 
syntactic knowledge to discourse analysis. How-
ever, most of previous work only deploys lexical 
and semantic features (Marcu and Echihabi, 
2002; Pettibone and PonBarry, 2003; Saito et al, 
2006; Ben and James, 2007; Lin et al, 2009; Pit-
ler et al, 2009) with only two exceptions (Ben 
and James, 2007; Lin et al, 2009). Nevertheless, 
Ben and James (2007) only uses flat syntactic 
path connecting connective and arguments in the 
parse tree. The hierarchical structured informa-
tion in the trees is not well preserved in their flat 
syntactic path features. Besides, such a syntactic 
feature selected and defined according to linguis-
tic intuition has its limitation, as it remains un-
clear what kinds of syntactic heuristics are effec-
tive for discourse analysis. 
The more recent work from Lin et al (2009) 
uses 2-level production rules to represent parse 
tree information. Yet it doesn?t cover all the oth-
er sub-trees structural information which can be 
also useful for the recognition. 
In this paper we propose using tree kernel 
based method to automatically mine the syntactic 
710
information from the parse trees for discourse 
analysis, applying kernel function to the parse 
tree structures directly. These structural syntactic 
features, together with other flat features are then 
incorporated into our composite kernel to capture 
diverse knowledge for simultaneous discourse 
identification and classification. The experiment    
shows that tree kernel is able to effectively in-
corporate syntactic structural information and 
produce statistical significant improvements over 
flat syntactic path feature for the recognition of 
both explicit and implicit relation in Penn Dis-
course Treebank (PDTB; Prasad et al, 2008). 
We also illustrate that tree kernel approach cov-
ers more structure information than the produc-
tion rules, which allows tree kernel to further 
work on a higher dimensional space for possible 
better discrimination. 
Besides, inspired by the linguistic study on 
tense and discourse anaphor (Webber, 1988), we 
further propose to incorporate temporal ordering 
information to constrain the interpretation of dis-
course relation, which also demonstrates statis-
tical significant improvements for discourse rela-
tion recognition on PDTB v2.0 for both explicit 
and implicit relations. 
The organization of the rest of the paper is as 
follows. We briefly introduce PDTB in Section 
2. Section 3 gives the related work on tree kernel 
approach in NLP and its difference with produc-
tion rules, and also linguistic study on tense and 
discourse anaphor. Section 4 introduces the 
frame work for discourse recognition, as well as 
the baseline feature space and the SVM classifi-
er. We present our kernel-based method in Sec-
tion 5, and the usage of temporal ordering feature 
in Section 6. Section 7 shows the experiments 
and discussions.  We conclude our works in Sec-
tion 8. 
2 Penn Discourse Tree Bank 
The Penn Discourse Treebank (PDTB) is the 
largest available annotated corpora of discourse 
relations (Prasad et al, 2008) over 2,312 Wall 
Street Journal articles. The PDTB models dis-
course relation in the predicate-argument view, 
where a discourse connective (e.g., but) is treated 
as a predicate taking two text spans as its argu-
ments. The argument that the discourse connec-
tive syntactically bounds to is called Arg2, and 
the other argument is called Arg1. 
The PDTB provides annotations for both ex-
plicit and implicit discourse relations. An explicit 
relation is triggered by an explicit connective. 
Example (1) shows an explicit Contrast relation 
signaled by the discourse connective ?but?. 
 
     (1). Arg1. Yesterday, the retailing and finan-
cial services giant reported a 16% drop in 
third-quarter earnings to $257.5 million, 
or 75 cents a share, from a restated $305 
million, or 80 cents a share, a year earlier. 
             Arg2. But the news was even worse for 
Sears's core U.S. retailing operation, the 
largest in the nation. 
 
In the PDTB, local implicit relations are also 
annotated. The annotators insert a connective 
expression that best conveys the inferred implicit 
relation between adjacent sentences within the 
same paragraph. In Example (2), the annotators 
select ?because? as the most appropriate connec-
tive to express the inferred Causal relation be-
tween the sentences. There is one special label 
AltLex pre-defined for cases where the insertion 
of an Implicit connective to express an inferred 
relation led to a redundancy in the expression of 
the relation. In Example (3), the Causal relation 
derived between sentences is alternatively lexi-
calized by some non-connective expression 
shown in square brackets, so no implicit connec-
tive is inserted. In our experiments, we treat Alt-
Lex Relations the same way as normal Implicit 
relations. 
 
     (2). Arg1. Some have raised their cash posi-
tions to record levels. 
            Arg2. Implicit = Because High cash po-
sitions help buffer a fund when the market 
falls. 
 
     (3). Arg1. Ms. Bartlett?s previous work, 
which earned her an international reputa-
tion in the non-horticultural art world, of-
ten took gardens as its nominal subject. 
             Arg2. [Mayhap this metaphorical con-
nection made] the BPC Fine Arts Com-
mittee think she had a literal green thumb. 
 
The PDTB also captures two non-implicit cas-
es: (a) Entity relation where the relation between 
adjacent sentences is based on entity coherence 
(Knott et al, 2001) as in Example (4); and (b) No 
relation where no discourse or entity-based cohe-
rence relation can be inferred between adjacent 
sentences. 
 
711
    (4).   But for South Garden, the grid was to be 
a 3-D network of masonry or hedge walls 
with real plants inside them. 
              In a Letter to the BPCA, kelly/varnell 
called this ?arbitrary and amateurish.? 
 
Each Explicit, Implicit and AltLex relation is 
annotated with a sense. The senses in PDTB are 
arranged in a three-level hierarchy. The top level 
has four tags representing four major semantic 
classes: Temporal, Contingency, Comparison 
and Expansion. For each class, a second level of 
types is defined to further refine the semantic of 
the class levels. For example, Contingency has 
two types Cause and Condition. A third level of 
subtype specifies the semantic contribution of 
each argument. In our experiments, we use only 
the top level of the sense annotations. 
3 Related Work 
Tree Kernel based Approach in NLP.  While 
the feature based approach may not be able to 
fully utilize the syntactic information in a parse 
tree, an alternative to the feature-based methods, 
tree kernel methods (Haussler, 1999) have been 
proposed to implicitly explore features in a high 
dimensional space by employing a kernel func-
tion to calculate the similarity between two ob-
jects directly. In particular, the kernel methods 
could be very effective at reducing the burden of 
feature engineering for structured objects in NLP 
research (Culotta and Sorensen, 2004). This is 
because a kernel can measure the similarity be-
tween two discrete structured objects by directly 
using the original representation of the objects 
instead of explicitly enumerating their features. 
Indeed, using kernel methods to mine structur-
al knowledge has shown success in some NLP 
applications like parsing (Collins and Duffy, 
2001; Moschitti, 2004) and relation extraction 
(Zelenko et al, 2003; Zhang et al, 2006). How-
ever, to our knowledge, the application of such a 
technique to discourse relation recognition still 
remains unexplored. 
Lin et al (2009) has explored the 2-level pro-
duction rules for discourse analysis. However, 
Figure 1 shows that only 2-level sub-tree struc-
tures (e.g. ?? - ?? ) are covered in production 
rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) 
are only captured in the tree kernel, which allows 
tree kernel to further leverage on information 
from higher dimension space for possible better 
discrimination. Especially, when there are 
enough training data, this is similar to the study  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
on language modeling that N-gram beyond uni-
gram and bigram further improves the perfor-
mance in large corpus. 
Tense and Temporal Ordering Information.   
Linguistic studies (Webber, 1988) show that a 
tensed clause ??  provides two pieces of semantic 
information: (a) a description of an event (or sit-
uation) ?? ; and (b) a particular configuration of 
the point of event (??), the point of reference 
(??) and the point of speech (??). Both the cha-
racteristics of ??  and the configuration of ??, ?? 
and ?? are critical to interpret the relationship of 
event ??  with other events in the discourse mod-
el. Our observation on temporal ordering infor-
mation is in line with the above, which is also 
incorporated in our discourse analyzer. 
4 The Recognition Framework 
In the learning framework, a training or testing 
instance is formed by a non-overlapping 
clause(s)/sentence(s) pair. Specifically, since im-
plicit relations in PDTB are defined to be local, 
only clauses from adjacent sentences are paired 
for implicit cases. During training, for each dis-
course relation encountered, a positive instance 
is created by pairing the two arguments. Also a 
Figure 1. Different sub-tree sets for ?1 used by 
2-level production rules and convolution tree 
kernel approaches. ?? -??  and ?1  itself are cov-
ered by tree kernel, while only ?? -??  are covered 
by production rules. 
Decomposition 
C 
E 
G 
F 
H 
A 
B 
D 
(?1) A 
B C 
(??) D 
F E 
(??) 
C 
D 
(??) E 
G 
(??) 
F 
H 
(??) 
D 
E 
G 
F 
H 
(??) (??) A 
C 
D 
B 
D 
E 
G 
F 
H 
C 
(?? ) C (??) 
D 
F E 
(??) A 
C 
D 
B 
F E 
712
set of negative instances is formed by paring 
each argument with neighboring non-argument 
clauses or sentences. Based on the training in-
stances, a binary classifier is generated for each 
type using a particular learning algorithm. Dur-
ing resolution, (a) clauses within same sentence 
and sentences within three-sentence spans are 
paired to form an explicit testing instance; and 
(b) neighboring sentences within three-sentence 
spans are paired to form an implicit testing in-
stance. The instance is presented to each explicit 
or implicit relation classifier which then returns a 
class label with a confidence value indicating the 
likelihood that the candidate pair holds a particu-
lar discourse relation. The relation with the high-
est confidence value will be assigned to the pair. 
4.1 Base Features 
In our system, the base features adopted include 
lexical pair, distance and attribution etc. as listed 
in Table 1. All these base features have been 
proved effective for discourse analysis in pre-
vious work. 
 
 
 
4.2 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn the classifier for discourse 
analysis. In our study, we use Support Vector 
Machine (Vapnik, 1995) to allow the use of ker-
nels to incorporate the structure feature. 
Suppose the training set ? consists of labeled 
vectors { ?? ,?? }, where ??  is the feature vector 
of a training instance and ??  is its class label. The 
classifier learned by SVM is: 
? ? = ???   ????? ? ?? + ?
?=1
  
where ??  is the learned parameter for a feature 
vector ?? , and ? is another parameter which can 
be derived from ??  . A testing instance ? is clas-
sified as positive if ? ? > 01. 
One advantage of SVM is that we can use tree 
kernel approach to capture syntactic parse tree 
information in a particular high-dimension space. 
In the next section, we will discuss how to use 
kernel to incorporate the more complex structure 
feature. 
5 Incorporating Structural Syntactic 
Information 
A parse tree that covers both discourse argu-
ments could provide us much syntactic informa-
tion related to the pair. Both the syntactic flat 
path connecting connective and arguments and 
the 2-level production rules in the parse tree used 
in previous study can be directly described by the 
tree structure. Other syntactic knowledge that 
may be helpful for discourse resolution could 
also be implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to which level 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. 
The value returned from the tree kernel re-
flects the similarity between two instances in 
syntax. Such syntactic similarity can be further 
combined with other flat linguistic features to 
compute the overall similarity between two in-
stances through a composite kernel. And thus an 
SVM classifier can be learned and then used for 
recognition. 
5.1 Structural Syntactic Feature 
Parsing is a sentence level processing. However, 
in many cases two discourse arguments do not 
occur in the same sentence. To present their syn-
tactic properties and relations in a single tree 
structure, we construct a syntax tree for each pa-
ragraph by attaching the parsing trees of all its 
sentences to an upper paragraph node. In this 
paper, we only consider discourse relations with-
in 3 sentences, which only occur within each pa-
                                                 
1 In our task, the result of ? ?  is used as the confidence 
value of the candidate argument pair ? to hold a particular 
discourse relation. 
Feature 
Names 
 Description 
(F1)  cue phrase 
(F2) neighboring punctuation 
(F3)  position of connective if 
presents 
(F4) extents of arguments 
(F5)  relative order of  arguments 
(F6)  distance between  arguments 
(F7)  grammatical role of  arguments 
(F8)  lexical pairs 
(F9) attribution  
Table 1. Base Feature Set 
713
ragraph, thus paragraph parse trees are sufficient. 
Our 3-sentence spans cover 95% discourse rela-
tion cases in PDTB v2.0. 
Having obtained the parse tree of a paragraph, 
we shall consider how to select the appropriate 
portion of the tree as the structured feature for a 
given instance. As each instance is related to two 
arguments, the structured feature at least should 
be able to cover both of these two arguments. 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information would likely be introduced. In our 
study, we examine three structured features that 
contain different substructures of the paragraph 
parse tree: 
Min-Expansion This feature records the mi-
nimal structure covering both arguments 
and connective word in the parse tree. It 
only includes the nodes occurring in the 
shortest path connecting Arg1, Arg2 and 
connective, via the nearest commonly 
commanding node. For example, consi-
dering Example (5), Figure 2 illustrates 
the representation of the structured feature 
for this relation instance. Note that the 
two clauses underlined with dashed lines 
are attributions which are not part of the 
relation. 
 
     (5). Arg1. Suppression of the book, Judge 
Oakes observed, would operate as a prior 
restraint and thus involve the First 
Amendment. 
              Arg2. Moreover, and here Judge Oakes 
went to the heart of the question, ?Respon-
sible biographers and historians constantly 
use primary sources, letters, diaries and 
memoranda.? 
 
Simple-Expansion Min-Expansion could, to 
some degree, describe the syntactic rela-
tionships between the connective and ar-
guments. However, the syntactic proper-
ties of the argument pair might not be 
captured, because the tree structure sur-
rounding the argument is not taken into 
consideration. To incorporate such infor-
mation, Simple-Expansion not only con-
tains all the nodes in Min-Expansion, but 
also includes the first-level children of  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       these nodes2. Figure 3 illustrates such a 
feature for Example (5). We can see that 
the nodes ?PRN? in both sentences are in-
cluded in the feature. 
Full-Expansion This feature focuses on the 
tree structure between two arguments. It 
not only includes all the nodes in Simple-
Expansion, but also the nodes (beneath 
the nearest commanding parent) that cov-
er the words between the two arguments. 
Such a feature keeps the most information 
related to the argument pair. Figure 4 
                                                 
2 We will not expand the nodes denoting the sentences other 
than where the arguments occur. 
Figure 2. Min-Expansion tree built from gol-
den standard parse tree for the explicit dis-
course relation in Example (5). Note that to 
distinguish from other words, we explicitly 
mark up in the structured feature the arguments 
and connective, by appending a string tag 
?Arg1?, ?Arg2? and ?Connective? respective-
ly. 
Figure 3. Simple-Expansion tree for the expli-
cit discourse relation in Example (5).  
714
shows the structure for feature Full-
Expansion of Example (5). As illustrated, 
different from in Simple-Expansion, each 
sub-tree of ?PRN? in each sentence is ful-
ly expanded and all its children nodes are 
included in Full-Expansion. 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.2 Convolution Parse Tree Kernel 
Given the parse tree defined above, we use the 
same convolution tree kernel as described in 
(Collins and Duffy, 2002) and (Moschitti, 2004). 
In general, we can represent a parse tree ? by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
? ? = (#?? ???????? ?? ???? 1,? , # ??  
     ???????? ?????? ?,? , # ?? ???????? ??   
     ???? ?). 
This results in a very high dimensionality 
since the number of different sub-trees is expo-
nential in its size. Thus, it is computational in-
feasible to directly use the feature vector ?(?). 
To solve the computational issue, a tree kernel 
function is introduced to calculate the dot prod-
uct between the above high dimensional vectors 
efficiently. 
Given two tree segments ?1  and ?2 , the tree 
kernel function is defined:  
   ? ?1 ,?2 = < ? ?1 ,? ?2 > 
                   =  ? ?1  ? ,? ?2 [?]?  
                   =    ?? ?1 ? ??(?2)??2??2?1??1  
where  ?1and ?2 are the sets of all nodes in trees 
?1and ?2, respectively; and ??(?) is the indicator 
function that is 1 iff a subtree of type ?  occurs 
with root at node ? or zero otherwise. (Collins 
and Duffy, 2002) shows that ?(?1 ,?2) is an in-
stance of convolution kernels over tree struc-
tures, and can be computed in ?( ?1 ,  ?2 ) by 
the following recursive definitions: 
            ? ?1 ,?2 =  ?? ?1 ? ??(?2)?                                                                                                   
(1) ? ?1 ,?2 = 0  if ?1  and ?2  do not have the 
same syntactic tag or their children are different; 
(2) else if both ?1 and  ?2 are pre-terminals (i.e. 
POS tags), ? ?1 ,?2 = 1 ? ?; 
(3)  else, ? ?1 ,?2 = 
              ? (1 + ?(??(
?? (?1)
?=1 ?1 , ?), ??(?2 , ?))),                                 
where ??(?1) is the number of the children of 
?1 , ??(?, ?)  is the ?
??  child of node ?  and ? 
(0 < ? < 1) is the decay factor in order to make 
the kernel value less variable with respect to the 
sub-tree sizes. In addition, the recursive rule (3) 
holds because given two nodes with the same 
children, one can construct common sub-trees 
using these children and common sub-trees of 
further offspring. 
    The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two instances. The time com-
plexity for computing this kernel is ?( ?1 ?
 ?2 ). 
5.3 Composite Tree Kernel 
Besides the above convolution parse tree kernel 
? ????  ?1 , ?2 = ?(?1 ,?2) defined to capture the 
syntactic information between two instances ?1 
and ?2, we also use another kernel ? ????  to cap-
ture other flat features, such as base features (de-
scribed in Table 1) and temporal ordering infor-
mation (described in Section 6). In our study, the 
composite kernel is defined in the following 
way: 
? 1 ?1 , ?2 = ? ? ? ????  ?1 , ?2 + 
                                    1 ? ? ? ? ????  ?1 , ?2 . 
Here, ? (?,?) can be normalized by ?  ?, ? =
? ?, ?  ? ?, ? ? ? ?, ?   and ? is the coeffi-
cient. 
6 Using Temporal Ordering Informa-
tion 
In our discourse analyzer, we also add in tem-
poral information to be used as features to pre-
dict discourse relations. This is because both our 
observations and some linguistic studies (Web-
ber, 1988) show that temporal ordering informa-
tion including tense, aspectual and event orders 
between two arguments may constrain the dis-
course relation type. For example, the connective 
Figure 4. Full-Expansion tree for the explicit 
discourse relation in Example (5).  
715
word is the same in both Example (6) and (7), 
but the tense shift from progressive form in 
clause 6.a to simple past form in clause 6.b, indi-
cating that the twisting occurred during the state 
of running the marathon, usually signals a tem-
poral discourse relation; while in Example (7), 
both clauses are in past tense and it is marked as 
a Causal relation. 
 
     (6). a. Yesterday Holly was running a mara-
thon  
            b. when she twisted her ankle. 
 
      (7). a. Use of dispersants was approved 
            b. when a test on the third day showed  
some positive results. 
 
Inspired by the linguistic model from Webber 
(1988) as described in Section 3, we explore the 
temporal order of events in two adjacent sen-
tences for discourse relation interpretation. Here 
event is represented by the head of verb, and the 
temporal order refers to the logical occurrence 
(i.e. before/at/after) between events. For in-
stance, the event ordering in Example (8) can be 
interpreted as:  
     ????? ?????? ??????? ?????(????) . 
 
     8.  a.  John went to the hospital.  
          b. He had broken his ankle on a patch of 
ice. 
 
We notice that the feasible temporal order of 
events differs for different discourse relations. 
For example, in causal relations, cause event 
usually happens before effect event, i.e.           
     ????? ????? ??????? ?????(??????). 
So it is possible to infer a causal relation in 
Example (8) if and only if 8.b is taken to be the 
cause event and 8.a is taken to be the effect 
event. That is, 8.b is taken as happening prior to 
his going into hospital. 
In our experiments, we use the TARSQI3  sys-
tem to identify event, analyze tense and aspectual 
information, and label the temporal order of 
events. Then the tense and temporal ordering 
information is extracted as features for discourse 
relation recognition. 
 
                                                 
3 http://www.isi.edu/tarsqi/ 
7 Experiments and Results 
In this section we provide the results of a set of 
experiments focused on the task of simultaneous 
discourse identification and classification. 
7.1 Experimental Settings 
We experiment on PDTB v2.0 corpus. Besides 
four top-level discourse relations, we also con-
sider Entity and No relations described in Section 
2. We directly use the golden standard parse 
trees in Penn TreeBank. We employ an SVM 
coreference resolver trained and tested on ACE 
2005 with 79.5% Precision, 66.7% Recall and 
72.5% F1 to label coreference mentions of the 
same named entity in an article. For learning, we 
use the binary SVMLight developed by (Joa-
chims, 1998) and Tree Kernel Toolkits devel-
oped by (Moschitti, 2004). All classifiers are 
trained with default learning parameters. 
The performance is evaluated using Accuracy 
which is calculated as follow: 
???????? =
????????????+ ????????????
???
 
Sections 2-22 are used for training and Sec-
tions 23-24 for testing. In this paper, we only 
consider any non-overlapping clauses/sentences 
pair in 3-sentence spans. For training, there were 
14812, 12843 and 4410 instances for Explicit, 
Implicit and Entity+No relations respectively; 
while for testing, the number was 1489, 1167 and 
380. 
7.2 System with Structural Kernel 
Table 2 lists the performance of simultaneous 
identification and classification on level-1 dis-
course senses. In the first row, only base features 
described in Section 4 are used. In the second 
row, we test Ben and James (2007)?s algorithm 
which uses heuristically defined syntactic paths 
and acts as a good baseline to compare with our 
learned-based approach using the structured in-
formation. The last three rows of Table 2 reports 
the results combining base features with three 
syntactic structured features (i.e. Min-Expansion, 
Simple-Expansion and Full-Expansion) de-
scribed in Section 5. 
We can see that all our tree kernels outperform 
the manually constructed flat path feature in all 
three groups including Explicit only, Implicit 
only and All relations, with the accuracy increas-
ing by 1.8%, 6.7% and 3.1% respectively. Espe-
cially, it shows that structural syntactic informa-
tion is more helpful for Implicit cases which is 
generally much harder than Explicit cases. We  
716
 
 
 
 
conduct chi square statistical significance test on 
All relations between flat path approach and 
Simple-Expansion approach, which shows the 
performance improvements are statistical signifi-
cant (? < 0.05) through incorporating tree ker-
nel. This proves that structural syntactic informa-
tion has good predication power for discourse 
analysis in both explicit and implicit relations. 
We also observe that among the three syntactic 
structured features, Min-Expansion and Simple-
Expansion achieve similar performances which 
are better than the result for Full-Expansion. This 
may be due to that most significant information 
is with the arguments and the shortest path con-
necting connectives and arguments. However, 
Full-Expansion that includes more information 
in other branches may introduce too many details 
which are rather tangential to discourse recogni-
tion. Our subsequent reports will focus on Sim-
ple-Expansion, unless otherwise specified. 
As described in Section 5, to compute the 
structural information, parse trees for different 
sentences are connected to form a large tree for a 
paragraph. It would be interesting to find how 
the structured information works for discourse 
relations whose arguments reside in different 
sentences. For this purpose, we test the accuracy 
for discourse relations with the two arguments 
occurring in the same sentence, one-sentence 
apart, and two-sentence apart. Table 3 compares 
the learning systems with/without the structured 
feature present. From the table, for all three cas-
es, the accuracies drop with the increase of the 
distances between the two arguments. However, 
adding the structured information would bring 
consistent improvement against the baselines 
regardless of the number of sentence distance. 
This observation suggests that the structured syn-
tactic information is more helpful for inter-
sentential discourse analysis.  
We also concern about how the structured in-
formation works for identification and classifica-
tion respectively. Table 4 lists the results for the 
two sub-tasks. As shown, with the structured in-
formation incorporated, the system (Base + Tree 
Kernel) can boost the performance of the two 
baselines (Base Features in the first row andBase 
+ Manually selected paths in the second row), for 
both identification and classification respective-
ly. We also observe that the structural syntactic 
information is more helpful for classification task 
which is generally harder than identification. 
This is in line with the intuition that classifica-
tion is generally a much harder task. We find that 
due to the weak modeling of Entity relations, 
many Entity relations which are non-discourse 
relation instances are mis-identified as implicit 
Expansion relations. Nevertheless, it clearly di-
rects our future work. 
 
 
 
 
 
 
 
 
 
 
 
7.3 System with Temporal Ordering Infor-
mation 
To examine the effectiveness of our temporal 
ordering information, we perform experiments 
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Manually 
selected flat path 
features 
70.3 32 52.6 
Base + Tree kernel 
(Min-Expansion) 
71.9 38.6 55.6 
Base + Tree kernel 
(Simple-Expansion) 
72.1 38.7 55.7 
Base + Tree kernel 
(Full-Expansion) 
71.8 38.4 55.4 
Sentence Dis-
tance 
0 
(959) 
1 
(1746) 
2 
(331) 
Base Features 52 49.2 35.5 
Base + Manually 
selected flat path 
features 
56.7 52 43.8 
Base + Tree 
Kernel 
58.3 55.6 49.7 
Tasks Identifica-
tion 
Classifica-
tion 
Base Features 58.6 50.5 
Base + Manually 
selected flat path 
features 
59.7 52.6 
Base + Tree 
Kernel 
63.3 59.3 
Table 3. Results of the syntactic structured kernel 
for discourse relations recognition with argu-
ments in different sentences apart. 
Table 4. Results of the syntactic structured ker-
nel for simultaneous discourse identification and 
classification subtasks. 
Table 2. Results of the syntactic structured ker-
nels on level-1 discourse relation recognition. 
717
on simultaneous identification and classification 
of level-1 discourse relations to compare with 
using only base feature set as baseline. The re-
sults are shown in Table 5.  We observe that the 
use of temporal ordering information increases 
the accuracy by 3%, 3.6% and 3.2% for Explicit, 
Implicit and All groups respectively. We conduct 
chi square statistical significant test on All rela-
tions, which shows the performance improve-
ment is statistical significant (? < 0.05). It indi-
cates that temporal ordering information can 
constrain the discourse relation types inferred 
within a clause(s)/sentence(s) pair for both expli-
cit and implicit relations. 
 
 
 
 
We observe that although temporal ordering 
information is useful in both explicit and implicit 
relation recognition, the contributions of the spe-
cific information are quite different for the two 
cases. In our experiments, we use tense and as-
pectual information for explicit relations, while 
event ordering information is used for implicit 
relations. The reason is explicit connective itself 
provides a strong hint for explicit relation, so 
tense and aspectual analysis which yields a relia-
ble result can provide additional constraints, thus 
can help explicit relation recognition. However, 
event ordering which would inevitably involve 
more noises will adversely affect the explicit re-
lation recognition performance. On the other 
hand, for implicit relations with no explicit con-
nective words, tense and aspectual information 
alone is not enough for discourse analysis. Event 
ordering can provide more necessary information 
to further constrain the inferred relations. 
7.4 Overall Results 
We also evaluate our model which combines 
base features, tree kernel and tense/temporal or-
dering information together on Explicit, Implicit 
and All Relations respectively. The overall re-
sults are shown in Table 6. 
 
 
 
 
 
 
 
 
 
8 Conclusions and Future Works 
The purpose of this paper is to explore how to 
make use of the structural syntactic knowledge to 
do discourse relation recognition. In previous 
work, syntactic information from parse trees is 
represented as a set of heuristically selected flat 
paths or 2-level production rules. However, the 
features defined this way may not necessarily 
capture all useful syntactic information provided 
by the parse trees for discourse analysis. In the 
paper, we propose a kernel-based method to in-
corporate the structural information embedded in 
parse trees. Specifically, we directly utilize the 
syntactic parse tree as a structure feature, and 
then apply kernels to such a feature, together 
with other normal features. The experimental 
results on PDTB v2.0 show that our kernel-based 
approach is able to give statistical significant 
improvement over flat syntactic path method. In 
addition, we also propose to incorporate tempor-
al ordering information to constrain the interpre-
tation of discourse relations, which also demon-
strate statistical significant improvements for 
discourse relation recognition, both explicit and 
implicit. 
In future, we plan to model Entity relations 
which constitute 24% of Implicit+Entity+No re-
lation cases, thus to improve the accuracy of re-
lation detection. 
Reference 
Ben W. and James P. 2007. Automatically Identifying 
the Arguments of Discourse Connectives. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pages 
92-101.  
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 423-
429.  
Collins M. and Duffy N. 2001. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Dis-
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Tem-
poral Ordering 
Information 
70.1 32.6 51.8 
Relations Accuracy 
Explicit 74.2 
Implicit 40.0 
All 57.3 
Table 5. Results of tense and temporal order 
information on level-1 discourse relations. 
Table 6. Overall results for combined model 
(Base  + Tree Kernel + Tense/Temporal). 
718
crete Structures and the Voted Perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), 
pages 263-270. 
Collins M. and Duffy N. 2002. Convolution Kernels 
for Natural Language. NIPS-2001. 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T.  1999. Making Large-scale SVM Learn-
ing Practical. In Advances in Kernel Methods ? 
Support Vector Learning. MIT Press. 
Knott, A., Oberlander, J., O?Donnel, M., and Mellish, 
C. 2001. Beyond elaboration: the interaction of re-
lations and focus in coherent text. In T. Sanders, J. 
Schilperoord, and W. Spooren, editors, Text Re-
presentation: Linguistic and Psycholinguistics As-
pects, pages 181-196. Benjamins, Amsterdam. 
Lee A., Prasad R., Joshi A., Dinesh N. and Webber  
B. 2006. Complexity of dependencies in discourse: 
are dependencies in discourse more complex than 
in syntax? In Proceedings of the 5th International 
Workshop on Treebanks and Linguistic Theories. 
Prague, Czech Republic, December. 
Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit 
Discourse Relations in the Penn Discourse Tree-
bank. In Proceedings of the 2009 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2009), Singapore, August. 
Marcu D. and Echihabi A. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of the 40th Annual Meeting of ACL, 
pages 368-375. 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. In Proceedings of 
the 42th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 335-
342. 
Pettibone J. and Pon-Barry H. 2003. A Maximum En-
tropy Approach to Recognizing Discourse Rela-
tions in Spoken Language. Working Paper. The 
Stanford Natural Language Processing Group, June 
6. 
Pitler E., Louis A. and Nenkova A. 2009. Automatic 
Sense Predication for Implicit Discourse Relations 
in Text. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics and the 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing (ACL-IJCNLP 2009). 
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo 
L., Joshi A. and Webber B. 2008. The Penn Dis-
course TreeBank 2.0. In Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC 2008). 
Saito M., Yamamoto K. and Sekine S. 2006. Using 
phrasal patterns to identify discourse relations. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics (HLT-
NAACL 2006), pages 133?136, New York, USA. 
Vapnik V.  1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
Webber Bonnie. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14:61?73. 
Zelenko D., Aone C. and Richardella A. 2003.  Ker-
nel Methods for Relation Extraction.  Journal of 
Machine Learning Research, 3(6):1083-1106. 
Zhang M., Zhang J. and Su J. Exploring Syntactic 
Features for Relation Extraction using a Convolu-
tion Tree Kernel. In Proceedings of the Human 
Language Technology conference - North Ameri-
can chapter of the Association for Computational 
Linguistics annual meeting (HLT-NAACL 2006), 
New York, USA. 
719
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534?539,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonparametric Bayesian Machine Transliteration with Synchronous
Adaptor Grammars
Yun Huang1,2 Min Zhang1 Chew Lim Tan2
huangyun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
1Human Language Department 2Department of Computer Science
Institute for Infocomm Research National University of Singapore
1 Fusionopolis Way, Singapore 13 Computing Drive, Singapore
Abstract
Machine transliteration is defined as auto-
matic phonetic translation of names across
languages. In this paper, we propose syn-
chronous adaptor grammar, a novel nonpara-
metric Bayesian learning approach, for ma-
chine transliteration. This model provides
a general framework without heuristic or re-
striction to automatically learn syllable equiv-
alents between languages. The proposed
model outperforms the state-of-the-art EM-
based model in the English to Chinese translit-
eration task.
1 Introduction
Proper names are one source of OOV words in many
NLP tasks, such as machine translation and cross-
lingual information retrieval. They are often trans-
lated through transliteration, i.e. translation by pre-
serving how words sound in both languages. In
general, machine transliteration is often modelled
as monotonic machine translation (Rama and Gali,
2009; Finch and Sumita, 2009; Finch and Sumita,
2010), the joint source-channel models (Li et al,
2004; Yang et al, 2009), or the sequential label-
ing problems (Reddy and Waxmonsky, 2009; Ab-
dul Hamid and Darwish, 2010).
Syllable equivalents acquisition is a critical phase
for all these models. Traditional learning approaches
aim to maximize the likelihood of training data
by the Expectation-Maximization (EM) algorithm.
However, the EM algorithm may over-fit the training
data by memorizing the whole training instances. To
avoid this problem, some approaches restrict that a
single character in one language could be aligned
to many characters of the other, but not vice versa
(Li et al, 2004; Yang et al, 2009). Heuristics are
introduced to obtain many-to-many alignments by
combining two directional one-to-many alignments
(Rama and Gali, 2009). Compared to maximum
likelihood approaches, Bayesian models provide a
systemic way to encode knowledges and infer com-
pact structures. They have been successfully applied
to many machine learning tasks (Liu and Gildea,
2009; Zhang et al, 2008; Blunsom et al, 2009).
Among these models, Adaptor Grammars (AGs)
provide a framework for defining nonparametric
Bayesian models based on PCFGs (Johnson et al,
2007). They introduce additional stochastic pro-
cesses (named adaptors) allowing the expansion of
an adapted symbol to depend on the expansion his-
tory. Since many existing models could be viewed
as special kinds of PCFG, adaptor grammars give
general Bayesian extension to them. AGs have been
used in various NLP tasks such as topic modeling
(Johnson, 2010), perspective modeling (Hardisty et
al., 2010), morphology analysis and word segmenta-
tion (Johnson and Goldwater, 2009; Johnson, 2008).
In this paper, we extend AGs to Synchronous
Adaptor Grammars (SAGs), and describe the in-
ference algorithm based on the Pitman-Yor process
(Pitman and Yor, 1997). We also describe how
transliteration could be modelled under this formal-
ism. It should be emphasized that the proposed
method is language independent and heuristic-free.
Experiments show the proposed approach outper-
forms the strong EM-based baseline in the English
to Chinese transliteration task.
534
2 Synchronous Adaptor Grammars
2.1 Model
A Pitman-Yor Synchronous Adaptor Grammar
(PYSAG) is a tuple G = (Gs,Na,a, b,?), where
Gs = (N ,Ts,Tt,R, S,?) is a Synchronous
Context-Free Grammar (SCFG) (Chiang, 2007),
N is a set of nonterminal symbols, Ts/Tt are
source/target terminal symbols, R is a set of rewrite
rules, S ? N is the start symbol, ? is the distri-
bution of rule probabilities, Na ? N is the set of
adapted nonterminals, a ? [0, 1], b ? 0 are vec-
tors of discount and concentration parameters both
indexed by adapted nonterminals, and ? are Dirich-
let prior parameters.
Algorithm 1 Generative Process
1: draw ?A ? Dir(?A) for all A ? N
2: for each yield pair ?s / t? do
3: SAMPLE(S) . Sample from root
4: return
5: function SAMPLE(A) . For A ? N
6: if A ? Na then
7: return SAMPLESAG(A)
8: else
9: return SAMPLESCFG(A)
10: function SAMPLESCFG(A) . For A /? Na
11: draw rule r = ?? / ?? ? Multi(?A)
12: tree tB ?SAMPLE(B) for nonterminalB ? ???
13: return BUILDTREE(r, tB1 , tB2 , . . .)
14: function SAMPLESAG(A) . For A ? Na
15: draw cache index zn+1 ? P (z|zi<n), where
P (z|zi<n) =
{
ma+b
n+b if zn+1 = m+ 1
nk?a
n+b if zn+1 = k ? {1, ? ? ? ,m}
16: if zn+1 = m+ 1 then . New entry
17: tree t? SAMPLESCFG(A)
18: m? m+ 1; nm = 1 . Update counts
19: INSERTTOCACHE(CA, t).
20: else . Old entry
21: nk ? nk + 1
22: tree t? FINDINCACHE(CA, zn+1)
23: return t
The generative process of a synchronous tree set
T is described in Algorithm 1. First, rule probabil-
ities are sampled for each nonterminal A ? N (line
1) according to the Dirichlet distribution. Then syn-
chronous trees are generated in the top-down fashion
from the start symbol S (line 3) for each yield pair.
For nonterminals that are not adapted, the grammar
expands it just as the original synchronous grammar
(function SAMPLESCFG). For each adapted non-
terminal A ? Na, the grammar maintains a cache
CA to store previously generated subtrees under A.
Let zi be the subtree index in CA, denoting the syn-
chronous subtree generated at the ith expansion of
A. At some particular time, assuming n subtrees
rooted at A have been generated with m different
types in the cache of A, each of which has been gen-
erated for n1, . . . , nm times respectively1 . Then the
grammar either generates the (n+1)th synchronous
subtree as SCFG (line 17) or chooses an existing
subtree (line 22), according to the conditional prob-
ability P (z|zi<n).
The above generative process demonstrates ?rich
get richer? dynamics, i.e. previous sampled subtrees
under adapted nonterminals would more likely be
sampled again in following procedures. This is suit-
able for many learning tasks since they prefer sparse
solutions to avoid the over-fitting problems. If we
integrate out the adaptors, the joint probability of a
particular sequence of indexes z with cached counts
(n1, . . . , nm) under the Pitman-Yor process is
PY (z|a, b) =
?m
k=1(a(k ? 1) + b)
?nk?1
j=1 (j ? a)
?n?1
i=0 (i+ b)
.
(1)
Given synchronous tree set T , the joint probability
under the PYSAG is
P (T |?,a, b) =
?
A?N
B(?A + fA)
B(?A)
PY (z(T )|a, b)
(2)
where fA is the vector containing the number of
times that rules r ? RA are used in the T , and B
is the Beta function.
2.2 Inference for PYSAGs
Directly drawing samples from Equation (2) is
intractable, so we extend the component-wise
Metropolis-Hastings algorithm (Johnson et al,
2007) to the synchronous case. In detail, we
draw sample T ?i from some proposal distribution
Q(Ti|yi,T?i)2, then accept the new sampled syn-
1Obviously, n =
?m
k=1 nk.
2T?i means the set of sampled trees except the ith one.
535
chronous tree T ?i with probability
A(Ti, T ?i ) = min
{
1, P (T
?|?,a, b)Q(Ti|yi,T?i)
P (T |?,a, b)Q(T ?i |yi,T?i)
}
.
(3)
In theory, Q could be any distribution if it never
assigns zero probability. For efficiency reason, we
choose the probabilistic SCFG as the proposal dis-
tribution. We pre-parse the training instances3 be-
fore inference and save the structure of synchronous
parsing forests. During the inference, we only
change rule probabilities in parsing forests without
changing the forest structures. The probability of
rule r ? RA in Q is estimated by relative frequency
?r = [fr]?i?
r??RA
[fr? ]?i
, where RA is the set of rules
rooted at A, and [fr]?i is the number of times that
rule r is used in the tree set T?i. We use the sam-
pling algorithm described in (Blunsom and Osborne,
2008) to draw a synchronous tree from the parsing
forest according to the proposal Q.
Following (Johnson and Goldwater, 2009), we put
an uninformative Beta(1,1) prior on a and a ?vague?
Gamma(10, 0.1) prior on b to model the uncertainty
of hyperparameters.
3 Machine Transliteration
3.1 Grammars
For machine transliteration, we design the following
grammar to learn syllable mappings4:
Name ? ?Syl / Syl?+
Syl ? ?NECs / NECs?
Syl ? ?NECs SECs / NECs SECs?
Syl ? ?NECs TECs / NECs TECs?
NECs ? ?NEC / NEC?+
SECs ? ?SEC / SEC?+
TECs ? ?TEC / TEC?+
NEC ? ?si / tj?
SEC ? ?? / tj?
TEC ? ?si / ??
3We implement the CKY-like bottom up parsing algorithm
described in (Wu, 1997). The complexity is O(|s|3|t|3).
4Similar to (Johnson, 2008), the adapted nonterminal are un-
derlined. Similarly, we also use rules in the regular expression
style X? ?A / A?+ to denote the following three rules:
X ? ?As / As?
As ? ?A / A?
As ? ?A As / A As?
where the adapted nonterminal Syl is designed to
capture the syllable equivalents between two lan-
guages, and the nonterminal NEC, SEC and TEC cap-
ture the character pairs with no empty character,
empty source and empty target respectively. Note
that this grammar restricts the leftmost characters on
both sides must be aligned one-by-one. Since our
goal is to learn the syllable equivalents, we are not
interested in the subtree tree inside the syllables. We
refer this grammar as syllable grammar.
The above grammar could capture inner-syllable
dependencies. However, the selection of the target
characters also depend on the context. For example,
the following three instances are found in the train-
ing set:
?a a b y e / c[ao] '[bi]?
?a a g a a r d / D[ai] ?[ge] [de]?
?a a l t o / C[a] [er] ?[tuo]?
where the same English syllable ?a a? are translit-
erated to ?c[ao]?, ?D[ai]? and ?C[a]? respec-
tively, depending on the following syllables. To
model these contextual dependencies, we propose
the hierarchical SAG. The two-layer word grammar
is obtained by adding following rules:
Name ? ?Word / Word?+
Word ? ?Syl / Syl?+
We might further add a new adapted nonterminal
Col to learn the word collocations. The following
rules appear in the collocation grammar:
Name ? ?Col / Col?+
Col ? ?Word / Word?+
Word ? ?Syl / Syl?+
Figure 1 gives one synchronous parsing trees
under the collocation grammar of the example
?m a x / ?[mai] ?[ke] d[si]?.
3.2 Translation Model
After sampling, we need a translation model to
transliterate new source string to target string.
Following (Li et al, 2004), we use the n-gram
translation model to estimate the joint distribution
P (s, t) = ?Kk=1 P (pk|pk?11 ), where pk is the kth
syllable pair of the string pair ?s / t?.
The first step is to construct joint segmentation
lattice for each training instance. We first generate a
merged grammar G? using collected subtrees under
adapted nonterminals, then use synchronous parsing
536
Name
Cols
Col
Words
Word
Syls
Syl
NECs
NEC
TECs
TEC
Syls
Syl
NECs
NEC
SECs
SEC
m/? a/? x/? ?/d
Figure 1: An example of parse tree.
to obtain probabilities in the segmentation lattice.
Specifically, we ?flatten? the collected subtrees un-
der Syl, i.e. removing internal nodes, to construct
new synchronous rules. For example, we could get
two rules from the tree in Figure 1:
Syl ? ?m a / ??
Syl ? ?x / ?d?
If multiple subtrees are flattened to the same syn-
chronous rule, we sum up the counts of these sub-
trees. For rules with non-adapted nonterminal as
parent, we assign the probability as the same of the
sampled rule probability, i.e. let ??r = ?r. For
the adapted nonterminal Syl, there are two kinds
of rules: (1) the rules in the original probabilistic
SCFG, and (2) the rules flattened from subtrees. We
assign the rule probability as
??r =
{
ma+b
n+b ? ?r if r is original SCFG rule
nr?a
n+b if r is flatten from subtree
(4)
where a and b are the parameters associated with
Syl, m is the number of types of different rules flat-
ten from subtrees, nr is the count of rule r, and n is
the total number of flatten rules. One may verify that
the rule probabilities are well normalized. Based
on this merged grammar G?, we parse the training
string pairs, then encode the parsed forest into the
lattice. Figure 2 show a lattice example for the string
pair ?a a l t o / C[a] [er] ?[tuo]?. The
transition probabilities in the lattice are the ?inside?
probabilities of corresponding Syl node in the pars-
ing forest.
start
a/C
aa/C
aal/C
aalto/C?
? a/C ?
? aa/C ?
? al/ ?
? l/ ?
? lto/? ?
? to/? ?
Figure 2: Lattice example.
After building the segmentation lattice, we train
3-order language model from the lattice using the
SRILM5. In decoding, given a new source string, we
use the Viterbi algorithm with beam search (Li et al,
2004) to find the best transliteration candidate.
4 Experiments
4.1 Data and Settings
We conduct experiments on the English-Chinese
data in the ACL Named Entities Workshop (NEWS
2009) 6. Table 1 gives some statistics of the data. For
evaluation, we report the word accuracy and mean
F-score metrics defined in (Li et al, 2009).
Train Dev Test
# Entry 31,961 2,896 2,896
# En Char 218,073 19,755 19,864
# Ch Char 101,205 9,160 9,246
# Ch Type 370 275 283
Table 1: Transliteration data statistics
In the inference step, we first run sampler through
the whole training corpus for 10 iterations, then col-
lect adapted subtree statistics for every 10 iterations,
and finally stop after 20 collections. After each it-
eration, we resample each of hyperparameters from
the posterior distribution of hyperparameters using a
slice sampler (Neal, 2003).
4.2 Results
We implement the joint source-channel model (Li et
al., 2004) as the baseline system, in which the ortho-
graphic syllable alignment is automatically derived
by the Expectation-Maximization (EM) algorithm.
5http://www.speech.sri.com/projects/srilm/
6http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
537
Since EM tends to memorize the training instance
as a whole, Li et al (2004) restrict the Chinese side
to be single character in syllable equivalents. Our
method can be viewed as the Bayesian extension of
the EM-based baseline. Since PYSAGs could learn
accurate and compact transliteration units, we do not
need the restriction any more.
Grammar Dev (%) Test (%)
Baseline 67.8/86.9 66.6/85.7
Syl 66.6/87.0 66.6/86.6
Word 67.1/87.2 67.0/86.7
Col 67.2/87.1 66.9/86.7
Table 2: Transliteration results, in the format of word ac-
curacy / mean F-score. ?Syl?,?Word? and ?Col? denote
the syllable, word and collocation grammar respectively.
Table 2 presents the results of all experiments.
From this table, we draw following conclusions:
1. The best results of our model are 67.1%/87.2%
on development set and corresponding
67.0%/86.7% on test set, achieved by word
grammars. The results on test set outperform
the EM-based baseline system on both word
accuracy and mean F-score.
2. Comparing grammars of different layers, we
find that the word grammars perform consis-
tently better than the syllable grammars. These
support the assumption that the context infor-
mation are helpful to identify syllable equiva-
lents. However, the collocation grammars do
not further improve performance. We guess
the reason is that the instances in transliter-
ation are very short, so two-layer grammars
are good enough while the collocations become
very sparse, which results in unreliable proba-
bility estimation.
4.3 Discussion
Table 3 shows some examples of learned syllable
mappings in the final sampled tree of the syllable
grammar. We can see that the PYSAGs could find
good syllable mappings from the raw name pairs
without any heuristic or restriction. In this point of
view, the proposed method is language independent.
Specifically, we are interested in the English to-
ken ?x?, which is the only one that has two corre-
s/d[si]/1669 k/?[ke]/408 ri/p[li]/342
t/A[te]/728 ma/?[ma]/390 ra/.[la]/339
man/?[man]/703 co/?[ke]/387 ca/k[ka]/333
d/[de]/579 ll/[er]/383 m/0[mu]/323
ck/?[ke]/564 la/.[la]/382 li/|[li]/314
de/[de]/564 tt/A[te]/380 ber/?[bo]/311
ro/?[luo]/531 l/[er]/367 ley/|[li]/310
son/?[sen]/442 ton/?[dun]/360 na/B[na]/302
x/?d[ke si]/40 x/?[ke]/3 x/d[si]/1
Table 3: Examples of learned syllable mappings. Chinese
Pinyin are given in the square bracket. The counts of syl-
lable mappings in the final sampled tree are also given.
sponding Chinese characters (??d[ke si]?). Ta-
ble 3 demonstrates that nearly all these correct map-
pings are discovered by PYSAGs. Note that these
kinds of mapping can not be learned if we restrict the
Chinese side to be only one character (the heuristic
used in (Li et al, 2004)). We will conduct experi-
ments on other language pairs in the future.
5 Conclusion
This paper proposes synchronous adaptor gram-
mars, a nonparametric Bayesian model, for machine
transliteration. Based on the sampling, the PYSAGs
could automatically discover syllable equivalents
without any heuristic or restriction. In this point
of view, the proposed model is language indepen-
dent. The joint source-channel model is then used
for training and decoding. Experimental results on
the English-Chinese transliteration task show that
the proposed method outperforms the strong EM-
based baseline system. We also compare grammars
in different layers and find that the two-layer gram-
mars are suitable for the transliteration task. We
plan to carry out more transliteration experiments on
other language pairs in the future.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful comments and suggestions. We also
thank Zhixiang Ren, Zhenghua Li, and Jun Sun for
insightful discussions. Special thanks to Professor
Mark Johnson for his open-source codes7.
7Available from http://web.science.mq.edu.
au/~mjohnson/Software.htm
538
References
Ahmed Abdul Hamid and Kareem Darwish. 2010. Sim-
plified feature set for arabic named entity recognition.
In Proceedings of the 2010 Named Entities Workshop,
pages 110?115, Uppsala, Sweden, July.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215?223, Honolulu,
Hawaii, October.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 782?790, Sun-
tec, Singapore, August.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Andrew Finch and Eiichiro Sumita. 2009. Transliter-
ation by bidirectional statistical machine translation.
In Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages
52?56, Suntec, Singapore, August.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proceedings of the 7th International Workshop on
Spoken Language Translation (IWSLT), pages 259?
266, Paris, France, December.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Cambridge, MA, October.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric bayesian models.
In B. Sch?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of ACL-08: HLT,
pages 398?406, Columbus, Ohio, June.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 159?166, Barcelona, Spain, July.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009. Report of news 2009 machine transliter-
ation shared task. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 1?18, Suntec, Singapore, August.
Ding Liu and Daniel Gildea. 2009. Bayesian learning
of phrasal tree-to-string templates. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1308?1317, Singapore,
August.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705?767.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855?900.
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 124?127, Suntec, Singa-
pore, August.
Sravana Reddy and Sonjia Waxmonsky. 2009.
Substring-based transliteration with conditional ran-
dom fields. In Proceedings of the 2009 Named Enti-
ties Workshop: Shared Task on Transliteration (NEWS
2009), pages 92?95, Suntec, Singapore, August.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura, and Sadaoki Furui. 2009.
Combining a two-step conditional random field model
and a joint source channel model for machine translit-
eration. In Proceedings of the 2009 Named Enti-
ties Workshop: Shared Task on Transliteration (NEWS
2009), pages 72?75, Suntec, Singapore, August.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of ACL-08: HLT, pages 97?105, Colum-
bus, Ohio, June.
539
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 983?992,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Probabilistic Sense Sentiment Similarity through Hidden Emotions 
 
 
Mitra Mohtarami1, Man Lan2, and Chew Lim Tan1 
1Department of Computer Science, National University of Singapore; 
2Department of Computer Science, East China Normal University 
{mitra,tancl}@comp.nus.edu.sg;mlan@cs.ecnu.edu.cn 
 
  
 
Abstract 
Sentiment Similarity of word pairs reflects the 
distance between the words regarding their 
underlying sentiments. This paper aims to in-
fer the sentiment similarity between word 
pairs with respect to their senses. To achieve 
this aim, we propose a probabilistic emotion-
based approach that is built on a hidden emo-
tional model. The model aims to predict a vec-
tor of basic human emotions for each sense of 
the words. The resultant emotional vectors are 
then employed to infer the sentiment similarity 
of word pairs. We apply the proposed ap-
proach to address two main NLP tasks, name-
ly, Indirect yes/no Question Answer Pairs in-
ference and Sentiment Orientation prediction. 
Extensive experiments demonstrate the effec-
tiveness of the proposed approach. 
1 Introduction 
Sentiment similarity reflects the distance be-
tween words based on their underlying senti-
ments. Semantic similarity measures such as La-
tent Semantic Analysis (LSA) (Landauer et al, 
1998) can effectively capture the similarity be-
tween semantically related words like "car" and 
"automobile", but they are less effective in relat-
ing words with similar sentiment orientation like 
"excellent" and "superior". For example, the fol-
lowing relations show the semantic similarity 
between some sentiment words computed by 
LSA: 
 :			
		, 	 = 0.40		 < 		
		,  = 0.46	 < 		,   = 0.65 
Clearly, the sentiment similarity between the 
above words should be in the reversed order. In 
fact, the sentiment intensity in "excellent" is 
closer to "superior" than "good". Furthermore, 
sentiment similarity between "good" and "bad" 
should be 0. 
In this paper, we propose a probabilistic ap-
proach to detect the sentiment similarity of 
words regarding their senses and underlying sen-
timents. For this purpose, we propose to model 
the hidden emotions of word senses. We show 
that our approach effectively outperforms the 
semantic similarity measures in two NLP tasks: 
Indirect yes/no Question Answer Pairs (IQAPs) 
Inference and Sentiment Orientation (SO) pre-
diction that are described as follows: 
In IQAPs, answers do not explicitly contain 
the yes or no keywords, but rather provide con-
text information to infer the yes or no answer 
(e.g. Q: Was she the best one on that old show? 
A: She was simply funny). Clearly, the sentiment 
words in IQAPs are the pivots to infer the yes or 
no answers. We show that sentiment similarity 
between such words (e.g., here the adjectives 
best and Funny) can be used effectively to infer 
the answers. 
The second application (SO prediction) aims to 
determine the sentiment orientation of individual 
words. Previous research utilized the semantic 
relations between words obtained from WordNet 
(Hassan and Radev, 2010) and semantic similari-
ty measures (e.g. Turney and Littman, 2003) for 
this purpose. In this paper, we show that senti-
ment similarity between word pairs can be effec-
tively utilized to compute SO of words.  
The contributions of this paper are follows: 
? We propose an effective approach to predict 
the sentiment similarity between word pairs 
through hidden emotions at the sense level,  
? We show the utility of sentiment similarity 
prediction in IQAP inference and SO predic-
tion tasks, and 
? Our hidden emotional model can infer the type 
and number of hidden emotions in a corpus. 
983
2 Sentiment Similarity through Hidden 
Emotions 
As we discussed above, semantic similarity 
measures are less effective to infer sentiment 
similarity between word pairs. In addition, dif-
ferent senses of sentiment words carry different 
human emotions. In fact, a sentiment word can 
be represented as a vector of emotions with in-
tensity values from "very weak" to "very strong". 
For example, Table 1 shows several sentiment 
words and their corresponding emotion vectors 
based the following set of emotions: e = [anger, 
disgust, sadness, fear, guilt, interest, joy, shame, 
surprise]. For example, "deceive" has 0.4 and 0.5 
intensity values with respect to the emotions 
"disgust" and "sadness" with an overall -0.9 (i.e. 
-0.4-0.5) value for sentiment orientation 
(Neviarouskaya et al, 2007; Neviarouskaya et 
al., 2009).  
Word Emotional Vector SO 
e = [anger, disgust, sadness, fear, guilt, interest, joy, shame, surprise] 
Rude ['0.2', '0.4',0,0,0,0,0,0,0] -0.6 
doleful [0, 0, '0.4',0,0,0,0,0,0] -0.4 
smashed [0,0, '0.8', '0.6',0,0,0,0,0] -1.4 
shamefully [0,0,0,0,0,0,0, '0.7',0] -0.7 
deceive [0, '0.4', '0.5',0,0,0,0,0,0] -0.9 
Table  1. Sample of emotional vectors  
 
The difficulty of the sentiment similarity predic-
tion task is evident when terms carry different 
types of emotions. For instance, all the words in 
Table 1 have negative sentiment orientation, but, 
they carry different emotions with different emo-
tion vectors. For example, "rude" reflects the 
emotions "anger" and "disgust", while the word 
"doleful" only reflects the emotion "sadness". As 
such, the word "doleful" is closer to the words 
"smashed" and "deceive" involving the emotion 
"sadness" than others. We show that emotion 
vectors of the words can be effectively utilized to 
predict the sentiment similarity between them. 
Previous research shows little agreement about 
the number and types of the basic emotions 
(Ortony and Turner 1990; Izard 1971). Thus, we 
assume that the number and types of basic emo-
tions are hidden and not pre-defined and propose 
a Probabilistic Sense Sentiment Similarity 
(PSSS) approach to extract the hidden emotions 
of word senses to infer their sentiment similarity.  
3 Hidden Emotional Model  
Online review portals provide rating mechanisms 
(in terms of stars, e.g. 5- or 10-star rating) to al- 
 
Figure 1.The structure of PSSS model 
 
low users to attach ratings to their reviews. A 
rating indicates the summarized opinion of a user 
who ranks a product or service based on his feel-
ings. There are various feelings and emotions 
behind such ratings with respect to the content of 
the reviews.  
Figure 1 shows the intermediate layer of hid-
den emotions behind the ratings (sentiments) 
assigned to the documents (reviews) containing 
the words. This Figure indicates the general 
structure of our PSSS model. It shows that hid-
den emotions (ei) link the rating (rj) and the doc-
uments (dk). In this Section, we aim to employ 
ratings and the relations among ratings, docu-
ments, and words to extract the hidden emotions.  
Figure 2 illustrates a simple graphical model 
using plate representation of Figure 1. As Figures 
2 shows, the rating r from a set of ratings R= 
{r1,?,rp} is assigned to a hidden emotion set 
E={e1,?,ek}. A document d from a set of docu-
ments D= {d1,?,dN} with vocabulary set W= 
{w1,?,wM} is associated with the hidden emotion 
set.  
 
 
 
 
 
 
 
 
 
 
 
 
The model presented in Figure 2(a) has been 
explored in (Mohtarami et al, 2013) and is called 
Series Hidden Emotional Model (SHEM). This 
representation assumes that the word w is de-
pendent to d and independent to e (we refer to 
this Assumption as A1). However, in reality, a 
word w can inherit properties (e.g., emotions) 
(b): Bridged model 
Figure 1. he structure of PSSS odel 
(a): Series model 
Figure 2. Hidden emotional model 
984
from the document d that contains w. Thus, we 
can assume that w is implicitly dependant on e. 
To account for this, we present Bridged Hidden 
Emotional Model (BHEM) shown in Figure 2(b). 
Our assumption, A2, in the BHEM model is as 
follows: w is dependent to both d and e.  
Considering Figure 1, we represent the entire 
text collection as a set of (w,d,r) in which each 
observation (w,d,r) is associated with a set of 
unobserved emotions. If we assume that the ob-
served tuples are independently generated, the 
whole data set is generated based on the joint 
probability of the observation tuples (w,d,r) as 
the follows (Mohtarami et al, 2013): 
" =	###$%, , &',(,)																																						
'()
 
=	###$%, , &',(&(,) 									1
'()
 
where, P(w,d,r) is the joint probability of the tu-
ple (w,d,r), and n(w,d,r) is the frequency of w in 
document d of rating r (note that n(w,d) is the 
term frequency of w in d and n(d,r) is one if r is 
assigned to d, and 0 otherwise). The joint proba-
bility for the BHEM is defined as follows con-
sidering hidden emotion e: 
- regarding class probability of the hidden emotion e 
to be assigned to the observation (w,d,r): 
	$%, ,  = 	+$%, , |	$	
-
= 
	=	+$%, |	$|	$	
-
 
- regarding assumption A2 and Bayes' Rule: 
=	+$%|, 	$, 	$|	
-
 
- using Bayes' Rule: 
=	+$, 	|%$%$|	
-
 
- regarding A2 and conditional independency: 
		=	+$|%$	|%$%$|	
-
 
		= $|%+$%|	$	$|																																							2
-
 
In the bridged model, the joint probability does 
not depend on the probability P(d|e) and the 
probabilities P(w|e), P(e) and P(r|e) are un-
known, while in the SHEM model explained in 
(Mohtarami et al, 2013), the joint probability 
does not depend on P(w|e), and probabilities 
P(d|e), P(e), and P(r|e) are unknown.  
We employ Maximum Likelihood approach to 
learn the probabilities and infer the possible hid-
den emotions. The log-likelihood of the whole 
data set D in Equation (1) can be defined as fol-
lows: 
 
 = 	+++%, , log$%, , 														3
'()
 
Replacing P(w,d,r) by the values computed us-
ing the bridged model in Equation (2) results in: 

= 	+++%, , log[$|%+$%|	$	$|	
-
]
'()
 
										4 
The above optimization problems are hard to 
compute due to the log of sum. Thus, Expecta-
tion-maximization (EM) is usually employed. 
EM consists of two following steps: 
1. E-step: Calculates posterior probabilities for 
hidden emotions given the words, documents 
and ratings, and 
2. M-step: Updates unknown probabilities (such 
as P(w|e) etc) using the posterior probabilities 
in the E-step. 
The steps of EM can be computed for BHEM 
model. EM of the model employs assumptions 
A2 and Bayes Rule and is defined as follows: 
E-step: 
$	|%, ,  = $|	$	$%|	? $|	$	$%|	- 																												5 
M-step: 
$|	 = ? ? %, , $e|%, , '(? ? ? %, ,  $e|%, , '()  
														=	 ? %, $e|%, , '? ? %, $e|%, , ') 																														6 
$%|	 = ? ? %, , $e|%, , ()? ? ? %, , $e|%, , ()' 	 
															=	 ? %, $e|%, , )? ? %, $e|%, , )' 																													7 
$	 = ? ? ? %, , $e|%, , '()? ? ? ? %,, $e|%, , ')(8  
									= 	 ? ? %,  $e|%, , ')? ? ? %,  $e|%, , ')8 																								8 
Note that in Equation (5), the probability 
P(e|w,d,r) does not depend on the document d. 
Also, in Equations (6)-(8) we remove the de-
pendency on document d using the following 
Equation: 
+%, ,  =%, 
(
																					9 
where n(w,r) is the occurrence of w in all the 
documents in the rating r. 
The EM steps computed by the bridged model 
do not depend on the variable document d, and 
discard d from the model. The reason is that w 
bypasses d to directly associate with the hidden 
emotion e in Figure 2(b). 
985
  Similar to BHEM, the EM steps for SHEM can 
be computed by considering assumptions A1 and 
Bayes Rule as follows (Mohtarami et al, 2013): 
E-step: 
$	|%, ,  = $|	$	$|	? $|	$	$|	- 																											10 
M-step: 
$|	 = ? ? %, , $e|%, , '(? ? ? %, ,  $e|%, , '() 										11 
$|	 = ? ? %, , $e|%, , ')? ? ? %, ,  $e|%, , ')( 										12 
$	 = ? ? ? %, ,  $e|%, , '()? ? ? ? %, , $e|%, , ')(8 							13 
 
Finally, we construct the emotional vectors us-
ing the algorithm presented in Table 2. The algo-
rithm employs document-rating, term-document 
and term-rating matrices to infer the unknown 
probabilities. This algorithm can be used with 
both bridged or series models. Our goal is to in-
fer the emotional vector for each word w that can 
be obtained by the probability P(w|e). Note that, 
this probability can be simply computed for the 
SHEM model using P(d|e) as follows: 
$%|	 =+$%|$|	
(
																						14 
3.1 Enriching Hidden Emotional Models 
We enrich our emotional model by employing 
the requirement that the emotional vectors of two 
synonym words w1 and w2 should be similar. For 
this purpose, we utilize the semantic similarity 
between each two words and create an enriched 
matrix. Equation (15) shows how we compute 
this matrix. To compute the semantic similarity 
between word senses, we utilize their synsets as 
follows: 
 
%;%< = $=>%;|>%<? 
	= 1|>%;|	 +
1
|>%<| + $=%;|%<?
|@A&'B|
C
|@A&'D|
E
				15 
where, syn(w) is the synset of w. Let count(wi, 
wj) be the co-occurrence of the wi and wj, and let 
count(wj) be the total word count. The probabil-
ity of wi given wj will then be P(wi|wj) = 
count(wi, wj)/ count(wj). In addition, note that 
employing the synset of the words help to obtain 
different emotional vectors for each sense of a 
word.  
The resultant enriched matrix W?W is multi-
plied to the inputs of our hidden model (matrices 
W?D	or	W?R. Note that this takes into account  
Input: 
Series Model: Document-Rate D?R, Term-Document 
W?D 
Bridged Model: Term-Rate W?R 
Output: Emotional vectors {e1, e2, ?,ek} for w 
Algorithm: 
1. Enriching hidden emotional model: 
Series Model: Update Term-Document W?D 
Bridged Model: Update Term-Rate W?R 
2. Initialize unknown probabilities:  
Series Model: Initialize P(d|e), P(r|e), and P(e), ran-
domly 
Bridged Model: Initialize P(w|e), P(r|e), and P(e) 
3. while L  has not converged to a pre-specified value do 
4. E-step;  
Series Model: estimate the value of P(e|w,d,r) in 
Equation 10  
Bridged Model: estimate the value of P(e|w,d,r) in 
Equation 5 
5. M-step;  
Series Model: estimate the values of P(r|e), P(d|e), 
and P(e) in Equations 11-13, respectively 
Bridged Model: estimate the values of P(r|e), P(w|e), 
and P(e) in Equations 6-8, respectively 
6. end while 
7. If series hidden emotional model is used then 
8.  Infer word emotional vector: estimate P(w|e) in 
Equation 14.  
9. End if 
Table  2. Constructing emotional vectors via P(w|e)  
the senses of the words as well. The learning step 
of EM is done using the updated inputs. In this 
case, the correlated words can inherit the proper-
ties of each other. For example, if wi does not 
occur in a document or rating involving another 
word (i.e., wj), the word wi can still be indirectly 
associated with the document or rating through 
the word wj. However, the distribution of the 
opinion words in documents and ratings is not 
uniform. This may decrease the effectiveness of 
the enriched matrix.  
The nonuniform distribution of opinion words 
has been also reported by Amiri et al (2012) 
who showed that the positive words are frequent-
ly used in negative reviews. We also observed 
the same pattern in the development dataset. Fig-
ure 3 shows the overall occurrence of some posi-
tive and negative seeds in various ratings. As 
shown, in spite of the negative words, the posi-
tive words may frequently occur in both positive 
and negative documents. Such distribution of  
986
 Figure 3. Nonuniform distribution of opinion words 
positive words can mislead the enriched model. 
To address this issue, we measure the confi-
dence of an opinion word in the enriched matrix 
as follows.  
KL		' = M[NO'
P ?"O'P ? NO'R ? "O'R]NO'P ?"O'P + NO'R ?"O'R  16 
where, NO'P (NO'R) is the frequency of w in the 
ratings 1 to 4 (7 to 10), and "O'P ("O'R) is the 
total number of documents with rating 1 to 4 (7 
to 10) that contain w. The confidence value of w 
varies from 0 to 1, and it increases if: 
? There is a large difference between the occur-
rences of w in positive and negative ratings. 
? There is a large number of reviews involving 
w in the relative ratings. 
   To improve the efficiency of enriched matrix, 
the columns corresponding to each word in the 
matrix are multiplied by its confidence value.        
4 Predicting Sentiment Similarity 
We utilize the approach proposed in (Mohtarami 
et al, 2013) to compute the sentiment similarity 
between two words. This approach compares the 
emotional vector of the given words. Let X and Y 
be the emotional vectors of two words. Equation 
(17) computes their correlation: 
V, W = ? V; ? VXW; ? WX&;YZ? 1[\ 																																17 
where,  is number of emotional categories, V,] WX 
and [ , \  are the mean and standard deviation 
values of ^  and _  respectively. V, W = ?1 
indicates that the two vectors are completely dis-
similar, and V, W = 1 indicates that the vec-
tors have perfect similarity.  
The approach makes use of a thresholding 
mechanism to estimate the proper correlation 
value to find sentimentally similar words. For 
this, as in Mohtarami et al (2013) we utilized the 
antonyms of the words. We consider two words,  
Input: 
`: The adjective in the question of given IQAP. : The adjective in the answer of given IQAP. 
Output: answer ? {>	, , 	 } 
Algorithm: 
1. if ` or  are missing from our corpus then 
2.       answer=Uncertain; 
3. else if  `,  < 0 then 
4.             answer=No;  
5.        else if `,  > 0 then 
6.                   answer=yes; 
Figure 4. Sentiment similarity for IQAP inference 
%; and %< as similar in sentiment iff they satisfy 
both of the following conditions: 
1. =%; ,%<? > =%;,~%<?,  2. =%; ,%<? > =~%;,%<? 
where, ~%;  is antonym of %; , and =%; , %<? 
is obtained from Equation (17). Finally, we com-
pute the sentiment similarity (SS) as follows: 
=%; ,%<? = 
=%; ,%<? ?f 
g=%; ,~%<?, =~%;,%<?h			18 
Equation (18) enforces two sentimentally simi-
lar words to have weak correlation to the anto-
nym of each others. A positive value of SS(.,.) 
indicates the words are sentimentally similar and 
a negative value shows that they are dissimilar.  
5 Applications 
We explain our approach in utilizing sentiment 
similarity between words to perform IQAP infer-
ence and SO prediction tasks respectively.  
In IQAPs, we employ the sentiment similarity 
between the adjectives in questions and answers 
to interpret the indirect answers. Figure 4 shows 
the algorithm for this purpose. SS(.,.) indicates 
sentiment similarity computed by Equation (18). 
A positive SS means the words are sentimentally 
similar and thus the answer is yes. However, 
negative SS leads to a no response. 
In SO-prediction task, we aim to compute 
more accurate SO using our sentiment similarity 
method. Turney and Littman (2003) proposed a 
method in which the SO of a word is calculated 
based on its semantic similarity with seven posi-
tive words minus its similarity with seven nega-
tive words as shown in Figure 5. As the similari-
ty function, A(.,.), they employed point-wise mu-
tual information (PMI) to compute the similarity 
between the words. Here, we utilize the same 
approach, but instead of PMI we use our SS(.,.) 
measure as the similarity function. 
987
Input: $%: seven words with positive SO i%: seven words with negative SO . , . : similarity function, and %: a given word with 
unknown SO 
Output: sentiment orientation of w  
Algorithm: 
1. $ = j_% = 
+ %, %?	 + %, %
&'l)(m	n'l)(@o'l)(m	p'l)(@
 
Figure 5. SO based on the similarity function A(.,.) 
6 Evaluation and Results 
6.1 Data and Settings 
We used the review dataset employed by Maas et 
al. (2011) as the development dataset that con-
tains movie reviews with star rating from one 
star (most negative) to 10 stars (most positive). 
We exclude the ratings 5 and 6 that are more 
neutral. We used this dataset to compute all the 
input matrices in Table 2 as well as the enriched 
matrix. The development dataset contains 50k 
movie reviews and 90k vocabulary.  
We also used two datasets for the evaluation 
purpose: the MPQA (Wilson et al, 2005) and 
IQAPs (Marneffe et al, 2010) datasets. The 
MPQA dataset is used for SO prediction experi-
ments, while the IQAP dataset is used for the 
IQAP experiments. We ignored the neutral 
words in MPQA dataset and used the remaining 
4k opinion words. Also, the IQAPs dataset 
(Marneffe et al, 2010) contains 125 IQAPs and 
their corresponding yes or no labels as the 
ground truth. 
6.2 Experimental Results 
To evaluate our PSSS model, we perform exper-
iments on the SO prediction and IQAPs infer-
ence tasks. Here, we consider six emotions for 
both bridged and series models. We study the 
effect of emotion numbers in Section 7.1. Also, 
we set a threshold of 0.3 for the confidence value 
in Equation (16), i.e. we set the confidence val-
ues smaller than the threshold to 0. We explain 
the effect of this parameter in Section 7.3. 
Evaluation of SO Prediction 
We evaluate the performance of our PSSS mod-
els in the SO prediction task using the algorithm 
explained in Figure 5 by setting our PSSS as 
similarity function (A). The results on SO predic-
tion are presented in Table 3. The first and se- 
Method Precision Recall F1 
PMI 56.20 56.36 55.01 
ER 65.68 65.68 63.27 
PSSS-SHEM 68.51 69.19 67.96 
PSSS-BHEM 69.39 70.07 68.68 
Table 3. Performance on SO prediction task 
cond rows present the results of our baselines, 
PMI (Turney and Littman, 2003) and Expected 
Rating (ER) (Potts, 2011) of words respectively.  
PMI extracts the semantic similarity between 
words using their co-occurrences. As Table 3 
shows, it leads to poor performance. This is 
mainly due to the relatively small size of the de-
velopment dataset which affects the quality of 
the co-occurrence information used by the PMI.  
ER computes the expected rating of a word 
based on the distribution of the word across rat-
ing categories. The value of ER indicates the SO 
of the word. As shown in the two last rows of the 
table, the results of PSSS approach are higher 
than PMI and ER. The reason is that PSSS is 
based on the combination between sentiment 
space (through using ratings, and matrices W?R 
in BHEM, D?R in SHEM) and semantic space 
(through the input W?D in SHEM and enriched 
matrix W?W in both hidden models). However, 
the PMI employs only the semantic space (i.e., 
the co-occurrence of the words) and ER uses oc-
currence of the words in rating categories. 
Furthermore, the PSSS model achieves higher 
performance with BHEM rather than SHEM. 
This is because the emotional vectors of the 
words are directly computed from the EM steps 
of BHEM. However, the emotional vectors of 
SHEM are computed after finishing the EM steps 
using Equation (14). This causes the SHEM 
model to estimate the number and type of the 
hidden emotions with a lower performance as 
compared to BHEM, although the performances 
of SHEM and BHEM are comparable as ex-
plained in Section 7.1.  
Evaluation of IQAPs Inference  
To apply our PSSS on IQAPs inference task, we 
use it as the sentiment similarity measure in the 
algorithm explained in Figure 4. The results are 
presented in Table 4. The first and second rows 
are baselines. The first row is the result obtained 
by Marneffe et al (2010) approach. This ap-
proach is based on the similarity between the SO 
of the adjectives in question and answer. The 
second row of Table 4 show the results of using a 
popular semantic similarity measure, PMI, as the 
sentiment similarity (SS) measure in Figure 4.  
988
Method Prec. Rec. F1 
Marneffe et al (2010) 60.00 60.00 60.00 
PMI 60.61 58.70 59.64 
PSSS-SHEM  62.55 61.75 61.71 
PSSS-BHEM (w/o WSD) 65.90 66.11 63.74 
SS-BHEM (with WSD) 66.95 67.15 65.66 
Table 4. Performance on IQAP inference task 
The result shows that PMI is less effective to 
capture the sentiment similarity. 
Our PSSS approach directly infers yes or no 
responses using SS between the adjectives and 
does not require computing SO of the adjectives. 
In Table 4, PSSS-SHEM and PSSS-BHEM indi-
cate the results when we use our PSSS with 
SHEM and BHEM respectively. Table 4 shows 
the effectiveness of our sentiment similarity 
measure. Both models improve the performance 
over the baselines, while the bridged model leads 
to higher performance than the series model. 
Furthermore, we employ Word Sense Disam-
biguation (WSD) to disambiguate the adjectives 
in the question and its corresponding answer. For 
example, Q: ? Is that true? A: This is extraor-
dinary and preposterous. In the answer, the cor-
rect sense of the extraordinary is unusual and as 
such answer no can be correctly inferred. In the 
table, (w/o WSD) is based on the first sense (most 
common sense) of the words, whereas (with 
WSD) utilizes the real sense of the words. As 
Table 4 shows, WSD increases the performance. 
WSD could have higher effect, if more IQAPs 
contain adjectives with senses different from the 
first sense. 
7 Analysis and Discussions 
7.1 Number and Types of Emotions   
In our PSSS approach, there is no limitation on 
the number and types of emotions as we assumed 
emotions are hidden. In this Section, we perform 
experiments to predict the number and type of 
hidden emotions.  
Figure 6 and 7 show the results of the hidden 
models (SHEM and BHEM) on SO prediction 
and IQAPs inference tasks respectively with dif-
ferent number of emotions. As the Figures show, 
in both tasks, SHEM achieved high performanc-
es with 11 emotions. However, BHEM achieved 
high performances with six emotions. Now, the 
question is which emotion number should be 
considered? To answer this question, we further 
study the results as follows.  
First, for SHEM, there is no significant differ-
ence between the performances with six and 11 
emotions in the SO prediction task. This is the  
 
Figure 6. Performance of BHEM and SHEM on SO 
prediction through different #of emotions 
 
 
Figure 7. Performance of BHEM and SHEM on 
IQAPs inference through different #of emotions 
same for BHEM. Also, the performances of 
SHEM on the IQAP inference task with six and 
11 emotions are comparable. However, there is a 
significant difference between the performances 
of BHEM in six and 11 emotions. So, we consid-
er the dimension in which both hidden emotional 
models present a reasonable performance over 
both tasks. This dimension is six here. 
Second, as shown in the Figures 6 and 7, in 
contrast to BHEM, the performance of SHEM 
does not considerably change with different 
number of emotions over both tasks. This is be-
cause, in SHEM, the emotional vectors of the 
words are derived from the emotional vectors of 
the documents after the EM steps, see Equation 
(14). However, in BHEM, the emotional vectors 
are directly obtained from the EM steps. Thus, 
the bridged model is more sensitive than series 
model to the number of emotions. This could 
indicate that the bridged model is more accurate 
than the series model to estimate the number of 
emotions. 
Therefore, based on the above discussion, the 
estimated number of emotions is six in our de-
velopment dataset. This number may vary using 
different development datasets. 
In addition to the number of emotions, their 
types can also be interpreted using our approach. 
To achieve this aim, we sort the words based on 
their probability values, P(w|e), with respect to  
989
 Figure 8. Effect of synonyms & antonyms in SO pre-
diction task with different emotion numbers in BHEM 
Emotion#1 Emotion#2 Emotion#3 
excellent (1) 
magnificently(1) 
blessed (1) 
sublime (1) 
affirmation (1) 
tremendous (2) 
unimpressive (1) 
humorlessly (1) 
paltry (1) 
humiliating (1) 
uncreative (1) 
lackluster (1) 
disreputable (1) 
villian (1) 
onslaught (1) 
ugly (1) 
old (1) 
disrupt (1) 
Table 5. Sample words in three emotions 
each emotion. Then, the type of the emotions can 
be interpreted by observing the top k words in 
each emotion. For example, Table 5 shows the 
top 6 words for three out of six emotions ob-
tained for BHEM. The numbers in parentheses 
show the sense of the words. The corresponding 
emotions for these categories can be interpreted 
as "wonderful", "boring" and "disreputable", re-
spectively.  
We also observed that, in SHEM with eleven 
emotion numbers, some of the emotion catego-
ries have similar top k words such that they can 
be merged to represent the same emotion. Thus, 
it indicates that the BHEM is better than SHEM 
to estimates the number of emotions than SHEM. 
7.2 Effect of Synsets and Antonyms  
We show the important effect of synsets and an-
tonyms in computing the sentiment similarity of 
words. For this purpose, we repeat the experi-
ment for SO prediction by computing sentiment 
similarity of word pairs with and without using 
synonyms and antonyms. Figure 8 shows the 
results of obtained from BHEM. As the Figure 
shown, the highest performance can be achieved 
when synonyms and antonyms are used, while 
the lowest performance is obtained without using 
them. Note that, when the synonyms are not 
used, the entries of enriched matrix are computed 
using P(wi|wj) instead of P(syn(wi)|syn(wj)) in the 
Equation (15). Also, when the antonyms are not 
used, the Max(,) in Equation (18) is 0 and SS is 
computed using only correlation between words.  
The results show that synonyms can improve 
the performance. As Figure 8 shows, the two  
 
Figure 9. Effect of confidence values in SO prediction 
with different emotion numbers in BHEM 
highest performances are obtained when we use 
synonyms and the two lowest performances are 
achieved when we don't use synonyms. This is 
indicates that the synsets of the words can im-
prove the quality of the enriched matrix. The re-
sults also show that the antonyms can improve 
the result (compare WOSynWAnt with 
WOSynWOAnt). However, synonyms lead to 
greater improvement than antonyms (compare 
WSynWOAnt with WOSynWAnt). 
7.3 Effect of Confidence Value 
In Section 3.1, we defined a confidence value for 
each word to improve the quality of the enriched 
matrix. To illustrate the utility of the confidence 
value, we repeat the experiment for SO predic-
tion by BHEM using all the words appears in 
enriched matrix with different confidence 
thresholds. The results are shown in Figure 9, 
"w/o confidence" shows the results when we 
don?t use the confidence values, while "with con-
fidence" shows the results when use the confi-
dence values. Also, "confidence>x" indicates the 
results when we set al the confidence value 
smaller than x to 0. The thresholding helps to 
eliminate the effect of low confident words.  
As Figure 9 shows, "w/o confidence" leads to 
the lowest performance, while "with confidence" 
improves the performance with different number 
of emotions. The thresholding is also effective. 
For example, a threshold like 0.3 or 0.4 improves 
the performance. However, if a large value (e.g., 
0.6) is selected as threshold, the performance 
decreases. This is because a large threshold fil-
ters a large number of words from enriched mod-
el that decreases the effect of the enriched ma-
trix.        
7.4 Convergence Analysis 
The PSSS approach is based on the EM algo-
rithm for the BHEM (or SHEM) presented in 
Table 2. This algorithm performs a predefined 
990
number of iterations or until convergence. To 
study the convergence of the algorithm, we re-
peat our experiments for SO prediction and 
IQAPs inference tasks using BHEM with differ-
ent number of iterations. Figure 10 shows that 
after the first 15 iterations the performance does 
not change dramatically and is nearly constant 
when more than 30 iterations are performed. This 
shows that our algorithm will converge in less 
than 30 iterations for BHEM. We observed the 
same pattern in SHEM. 
7.5 Bridged Vs. Series Model  
The bridged and series models are both based on 
the hidden emotions that were developed to pre-
dict the sense sentiment similarity. Although 
their best results on the SO prediction and IQAPs 
inference tasks are comparable, they have some 
significant differences as follows: 
? BHEM is considerably faster than SHEM. The 
reason is that, the input matrix of BHEM (i.e., 
W?R) is significantly smaller than the input 
matrix of SHEM (i.e., W?D). 
?  In BHEM, the emotional vectors are directly 
computed from the EM steps. However, the 
emotional vector of a word in SHEM is com-
puted using the emotional vectors of the doc-
uments containing the word. This adds noises 
to the emotional vectors of the words.  
? BHEM gives more accurate estimation over 
type and number of emotions versus SHEM. 
The reason is explained in Section 7.1. 
8 Related Works 
Sentiment similarity has not received enough 
attention to date. Most previous works employed 
semantic similarity of word pairs to address SO 
prediction and IQAP inference tasks. Turney and 
Littman (2003) proposed to compute pair-wised 
mutual information (PMI) between a target word 
and a set of seed positive and negative words to 
infer the SO of the target word. They also uti-
lized Latent Semantic Analysis (LSA) (Landauer 
et al, 1998) as another semantic similarity meas-
ure. However, both PMI and LSA are semantic 
similarity measure. Similarly, Hassan and Radev 
(2010) presented a graph-based method for pre-
dicting SO of words. They constructed a lexical 
graph where nodes are words and edges connect 
two words with semantic similarity obtained 
from Wordnet (Fellbaum 1998). They propagat-
ed the SO of a set of seeds through this graph. 
However, such approaches did not take into ac-
count the sentiment similarity between words.  
 
Figure 10. Convergence of BHEM 
In IQAPs, Marneffe et al (2010) inferred the 
yes/no answers using SO of the adjectives. If SO 
of the adjectives have different signs, then the 
answer conveys no, and Otherwise, if the abso-
lute value of SO for the adjective in question is 
smaller than the absolute value of the adjective in 
answer, then the answer conveys yes, and other-
wise no. In Mohtarami et al (2012), we used two 
semantic similarity measures (PMI and LSA) for 
the IQAP inference task. We showed that meas-
uring the sentiment similarities between the ad-
jectives in question and answer leads to higher 
performance as compared to semantic similarity 
measures. 
In Mohtarami et al (2012), we proposed an 
approach to predict the sentiment similarity of 
words using their emotional vectors. We as-
sumed that the type and number of emotions are 
pre-defined and our approach was based on this 
assumption. However, in previous research, there 
is little agreement about the number and types of 
basic emotions. Furthermore, the emotions in 
different dataset can be varied. We relaxed this 
assumption in Mohtarami et al, (2013) by con-
sidering the emotions as hidden and presented a 
hidden emotional model called SHEM. This pa-
per also consider the emotions as hidden and pre-
sents another hidden emotional model called 
BHEM that gives more accurate estimation of 
the numbers and types of the hidden emotions.   
9 Conclusion 
We propose a probabilistic approach to infer the 
sentiment similarity between word senses with 
respect to automatically learned hidden emo-
tions. We propose to utilize the correlations be-
tween reviews, ratings, and words to learn the 
hidden emotions. We show the effectiveness of 
our method in two NLP tasks. Experiments show 
that our sentiment similarity models lead to ef-
fective emotional vector construction and signif-
icantly outperform semantic similarity measures 
for the two NLP task. 
991
References  
Hadi Amiri and Tat S. Chua. 2012. Mining Slang 
and Urban Opinion Words and Phrases from 
cQA Services: An Optimization Approach. 
Proceedings of the fifth ACM international confer-
ence on Web search and data mining (WSDM). Pp. 
193-202. 
Christiane Fellbaum. 1998. WordNet: An Electron-
ic Lexical Database. Cambridge, MA: MIT 
Press. 
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing Text Polarity Using Random Walks. Pro-
ceeding in the Association for Computational Lin-
guistics (ACL). Pp: 395?403. 
Aminul Islam and Diana Inkpen. 2008. Semantic text 
similarity using corpus-based word similarity 
and string similarity. ACM Transactions on 
Knowledge Discovery from Data (TKDD). 
Carroll E. Izard. 1971. The face of emotion. New 
York: Appleton-Century-Crofts. 
Soo M. Kim and Eduard Hovy. 2004. Determining 
the sentiment of opinions. Proceeding of the 
Conference on Computational Linguistics 
(COLING). Pp: 1367?1373. 
Thomas K. Landauer, Peter W. Foltz, and Darrell 
Laham. 1998. Introduction to Latent Semantic 
Analysis. Discourse Processes. Pp: 259-284. 
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, 
Dan Huang, Andrew Y. Ng, and Christopher Potts. 
2011. Learning Word Vectors for Sentiment 
Analysis. Proceeding in the Association for Com-
putational Linguistics (ACL). Pp:142-150. 
Marie-Catherine D. Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2010. "Was it good? 
It was provocative." Learning the meaning of 
scalar adjectives. Proceeding in the Association 
for Computational Linguistics (ACL). Pp: 167?
176. 
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh P. 
Tran, and Chew L. Tan. 2012. Sense Sentiment 
Similarity: An Analysis. Proceeding of the Con-
ference on Artificial Intelligence (AAAI). 
Mitra Mohtarami, Man Lan, and Chew L. Tan. 2013. 
From Semantic to Emotional Space in Proba-
bilistic Sense Sentiment Analysis. Proceeding of 
the Conference on Artificial Intelligence (AAAI). 
Alena Neviarouskaya, Helmut Prendinger, and 
Mitsuru Ishizuka. 2007. Textual Affect Sensing 
for Sociable and Expressive Online Communi-
cation. Proceedings of the conference on Affective 
Computing and Intelligent Interaction (ACII). Pp: 
218-229. 
Alena Neviarouskaya, Helmut Prendinger, and 
Mitsuru Ishizuka. 2009. SentiFul: Generating a 
Reliable Lexicon for Sentiment Analysis. Pro-
ceeding of the conference on Affective Computing 
and Intelligent Interaction (ACII). Pp: 363-368. 
Andrew Ortony and Terence J. Turner. 1990. What's 
Basic About Basic Emotions. American Psycho-
logical Association. 97(3), 315-331. 
Christopher Potts, C. 2011. On the negativity of 
negation. In Nan Li and David Lutz, eds., Pro-
ceedings of Semantics and Linguistic Theory 20, 
636-659. 
Peter D. Turney and Michael L. Littman. 2003. 
Measuring Praise and Criticism: Inference of 
Semantic Orientation from Association. ACM 
Transactions on Information Systems, 21(4), 315?
346. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing contextual polarity in 
phrase-level sentiment analysis. Proceeding in 
HLT-EMNLP. Pp: 347?354. 
 
992
