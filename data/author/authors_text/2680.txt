A Comparative Study of Mixture Models for Automatic Topic Segmentation
of Multiparty Dialogues
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
susan.armstrong@issco.unige.ch
Abstract
In this article we address the task of auto-
matic text structuring into linear and non-
overlapping thematic episodes at a coarse
level of granularity. In particular, we
deal with topic segmentation on multi-party
meeting recording transcripts, which pose
specific challenges for topic segmentation
models. We present a comparative study
of two probabilistic mixture models. Based
on lexical features, we use these models in
parallel in order to generate a low dimen-
sional input representation for topic segmen-
tation. Our experiments demonstrate that in
this manner important information is cap-
tured from the data through less features.
1 Introduction
Some of the earliest research related to the prob-
lem of text segmentation into thematic episodes used
the word distribution as an intrinsic feature of texts
(Morris and Hirst, 1991). The studies of (Reynar,
1994; Hearst, 1997; Choi, 2000) continued in this
vein. While having quite different emphasis at dif-
ferent levels of detail (basically from the point of
view of the employed term weighting and/or the
adopted inter-block similarity measure), these stud-
ies analyzed the word distribution inside the texts
through the instrumentality of merely one feature,
i.e. the one-dimensional inter-block similarity.
More recent work use techniques from graph the-
ory (Malioutov and Barzilay, 2006) and machine
learning (Galley et al, 2003; Georgescul et al,
2006; Purver et al, 2006) in order to find patterns
in vocabulary use.
We investigate new approaches for topic segmen-
tation on corpora containing multi-party dialogues,
which currently represents a relatively less explored
domain. Compared to other types of audio content
(e.g. broadcast news recordings), meeting record-
ings are less structured, often exhibiting a high de-
gree of participants spontaneity and there may be
overlap in finishing one topic while introducing an-
other. Moreover while ending the discussion on a
certain topic, there can be numerous new attempts
to introduce a new topic before it becomes the fo-
cus of the dialogue. Therefore, the task of automatic
topic segmentation of meeting recordings is more
difficult and requires a more refined analysis. (Gal-
ley et al, 2003; Georgescul et al, 2007) dealt with
the problem of topic segmentation of multiparty di-
alogues by combining various features based on cue
phrases, syntactic and prosodic information. In this
article, our investigation is based on using merely
lexical features.
We study mixture models in order to group the
words co-occurring in texts into a small number
of semantic concepts in an automatic unsupervised
way. The intuition behind these models is that a
text document has an underlying structure of ?la-
tent? topics, which is hidden. In order to reveal
these latent topics, the basic assumption made is that
words related to a semantic concept tend to occur in
the proximity of each other. The notion of proxim-
ity between semantically related words can vary for
various tasks. For instance, bigrams can be consid-
ered to capture correlation between words at a very
925
short distance. At the other extreme, in the domain
of document classification, it is often assumed that
the whole document is concerned with one specific
topic and in this sense all words in a document are
considered to be semantically related. We consider
for our application that words occurring in the same
thematic episode are semantically related.
In the following, the major issues we will discuss
include the formulations of two probabilistic mix-
ture approaches, their methodology, aspects of their
implementation and the results obtained when ap-
plied in the topic segmentation context. Section 2
presents our approach on using probabilistic mix-
ture models for topic segmentation and shows com-
parisons between these techniques. In Section 3 we
discuss our empirical evaluation of these models for
topic segmentation. Finally, some conclusions are
drawn in Section 4.
2 Probabilistic Mixture Models
The probabilistic latent models described in the fol-
lowing exploit hierarchical Bayesian frameworks.
Based on prior distributions of word rate variability
acquired from a training corpus, we will compute a
density function to further analyze the text content in
order to perform topic segmentation at a coarse level
of granularity. In this model, we will be working
with ?blocks? of text which consist of a fixed num-
ber of consecutive utterances.
In the following two subsections, we use the fol-
lowing notation:
? We consider a text corpus B = {b1, b2, ..., bM}
containing M blocks of text with words from
a vocabulary W = {w1, w2, ..., wN}. M is
a constant scalar representing the number of
blocks of text. N is a constant scalar represent-
ing the number of terms in vocabulary W .
? We pre-process the data by eliminating con-
tent free words such as articles, prepositions
and auxiliary verbs. Then, we proceed by lem-
matizing the remaining words and by adopt-
ing a bag-of-words representation. Next,
we summarize the data in a matrix F =
(f(bi, wi,j))(i,j)?M?N , where f(bi, wi,j) de-
notes the log.entropy weighted frequency of
word wi,j in block bi.
? Each occurrence of a word in a block of
text is considered as representing an ob-
servation (wm,n, bm), i.e. a realization from
an underlying sequence of random variables
(Wm,n, Bm)
1?m?M
1?n?N . wm,n denotes the term
indicator for the n-th word in the m-th block
of text.
? Each pair (wm,n, bm) is associated with a dis-
crete hidden random variable Zm,n over some
finite set Z ={z1, z2, ..., zK}. K is a constant
scalar representing the number of mixture com-
ponents to generate.
? We denote by P (zm,n = zk) or simply by
P (zk) the probability that the k-th topic has
been sampled for the n-th word in the m-th
block of text.
2.1 Aspect Model for Dyadic Data (AMDD)
In this section we describe how we apply latent mod-
eling for dyadic data (Hofmann, 2001) to text repre-
sentation for topic segmentation.
2.1.1 Model Setting
 
 
 
 
 
n,mw  
n,mz  
mb  
M 
block  plate 
n,mw  
n,mz  
mb  
block  plate 
M 
word  plate 
N 
word  plate 
N 
1) Asymmetric PLSA parameterization 2) Symmetric PLSA parameterization 
Figure 1: Graphical model representation of the as-
pect model.
We express the joint or conditional probability
of words and blocks of text, by assuming that the
choice of a word during the generation of a block
of text is independent of the block itself, given some
(unobserved) hidden variable, also called latent vari-
able or aspect.
The graphical representation of the AMDD data
generation process is illustrated in Figure 1 by using
926
the plate notation. That is, the ovals (i.e. the nodes
of the graph) represent probabilistic variables. The
double ovals around the variables wm,n and bm de-
note observed variables. zm,n is the mixture indi-
cator, the hidden variable, that chooses the topic for
the n-th word in the m-th block of text. Arrows in-
dicate conditional dependencies between variables.
For instance, the wm,n variable in the word space
and the bm variable in the block space have no di-
rect dependencies, i.e. it is assumed that the choice
of words in the generation of a block of text is in-
dependent of the block given a hidden variable. The
boxes represent ?plates?, i.e. replicates of sampling
steps with the variable in the lower left corner re-
ferring to the number of samples. For instance, the
?word plate? in Figure 1 illustrates N independently
and identically distributed repeated trials of the ran-
dom variable wm,n.
According to the topology of the asymmetric
AMDD Bayesian network from Figure 1, we can
specify the joint distribution of a word wm,n, a latent
topic zk and a block of text bm: P (wm,n, zk, bm) =
P (bm) ? P (zk|bm) ? P (wm,n|zk). The joint distribu-
tion of a block of text bm and a word wm,n is thus:
P (bm, wm,n) =
K?
k=1
P (wm,n, zk, bm) = P (bm)
?
?K
k=1 P (zk|bm)? ?? ?
mixing proportions
? P (wm,n|zk)
? ?? ?
mixture components
(1)
Equation 1 describes a special case of a finite mix-
ture model, i.e. it uses a convex combination of a set
of component distributions to model the observed
data. That is, each word in a block of text is seen
as a sample from a mixture model, where mixture
components are multinomials P (wm,n|zk) and the
mixing proportions are P (zk|bm).
2.1.2 Inferring and Employing the AMDD
Model
The Expectation-Maximization (EM) algorithm is
the most popular method to estimate the parameters
for mixture models to fit a training corpus. The
EM algorithm for AMDD is based on iteratively
maximizing the log-likelihood function: LPLSA =?M
m=1
?N
n=1f(bm, wm,n) ? logP (wm,n, bm). How-
ever, the EM algorithm for AMDD is prone to over-
fitting since the number of parameters to be esti-
mated grows linearly with the number of blocks of
text. In order to avoid this problem, we employed
the tempered version of the EM algorithm that has
been proposed by Hofmann (2001).
We use the density estimation method in AMDD
to reduce the dimension of the blocks-by-words
space. Thus, instead of using the words as ba-
sic units for each block of text representation, we
employ a ?topic? basis, assuming that a few top-
ics will capture more information than the entire
huge amount of words in the vocabulary. Thus,
the m-th block of text is represented by the vector
(P (z1|bm), P (z2|bm), ..., P (zk|bm)). Then, we use
these posterior probabilities as a threshold to iden-
tify the boundaries of thematic episodes via sup-
port vector classification (Georgescul et al, 2006).
That is, we consider the topic segmentation task as a
binary-classification problem, where each utterance
should be classified as marking the presence or the
absence of a topic shift in the dialogue.
2.2 Latent Dirichlet Allocation (LDA)
Latent Dirichlet Allocation (Blei et al, 2003) can
be seen as an extension of AMDD by defining a
probabilistic mixture model that includes Dirichlet-
distributed priors over the masses of the multinomi-
als P (w|z) and P (z|b).
2.2.1 Model Setting
In order to describe the formal setting of LDA in
our context, we use the following notation in addi-
tion to those given at the beginning of Section 2:
? ~?m is a parameter notation for P (z|b = bm),
the topic mixture proportion for the m-th block
of text;
? ~? is a hyperparameter (a vector of dimension
K) on the mixing proportions ~?m;
? ? =
{
~?m
}M
m=1
is a matrix (of dimension
M ? K), composed by placing the vectors
~?1, ~?2, ..., ~?M as column components;
? ~?k is a parameter notation for P (w|zk), the
mixture component for topic k;
? ~? is a hyperparameter (a vector of dimension
N ) on the mixture components ~?k ;
927
? ? = {~?k}
K
k=1 is a matrix of dimension
K ? N composed by placing the vectors
~?1, ~?2, ..., ~?K as column components;
? Nm denotes the length of the m-th block of text
and is modeled with a Poisson distribution with
constant parameter ?;
 
 
 
 
 
word plate 
?
?  
??  
topic plate 
K 
n,mw  ?k?  
n,mz  
Nm 
m
??  
M 
block  plate 
Figure 2: Graphical model representation of latent
Dirichlet alocation.
LDA generates a stream of observable words
wm,n partitioned into blocks of text ~bm as shown
by the graphical model in Figure 2. The Bayesian
network can be interpreted as follows: the variables
?, ? and z are the three sets of latent variables that
we would like to infer. The plate surrounding ~?k il-
lustrates the repeated sampling of word distributions
for each topic zk until K topics have been generated.
The plate surrounding ~?m illustrates the sampling of
a distribution over topics for each block b for a to-
tal of M blocks of text. The inner plate over zm,n
and wm,n illustrates the repeated sampling of topics
and words until Nm words have been generated for
a block~bm.
Each block of text is first generated by drawing
a topic proportion ~?m, i.e. by picking a distribution
over topics from a Dirichlet distribution. For each
word wm,n from a block of text~bm, a topic indicator
k is sampled for zm,n according to the block-specific
mixture proportion ~?m. That is, ~?m determines
P (zm,n). The topic probabilities ~?k are also sam-
pled from a Dirichlet distribution. The words in each
block of text are then generated by using the corre-
sponding topic-specific term distribution ~?zm,n .
Given the graphical representation of LDA illus-
trated in Figure 2, we can write the joint distribution
of a word wm,n and a topic zk as:
P (wm,n, zk|~?m,?) = P (zk|~?m) ? P (wm,n|~?k).
Summing over k, we obtain the marginal distribu-
tion:
P (wm,n|~?m,?) =
?K
k=1
?
?
? P (zk|~?m)
? ?? ?
mixture proportion
? P (wm,n|~?k)
? ?? ?
mixture component
?
?
?.
Hence, similarly to AMDD (see Equation 1), the
LDA model assumes that a word wm,n is generated
from a random mixture over topics. Topic proba-
bilities are conditioned on the block of text a word
belongs to. Moreover LDA leaves flexibility to
assign a different topic to every observed word and
a different proportion of topics for every block of
text.
The joint distribution of a block of text ~bm
and the latent variables of the model ~zm, ~?m,
?, given the hyperparameters ~?, ~? is further
specified by: P (~bm, ~zm, ~?m,?|~?, ~?) =
topic plate
? ?? ?
P (?|~?) ?
P (~?m|~?) ?
Nm?
n=1
word plate
? ?? ?
P (zm,n|~?m) ? P (wm,n|~?zm,n)
? ?? ?
block plate
.
Therefore, the likelihood of a block~bm is derived
as the marginal distribution obtained by summing
over the zm,n and integrating out the distributions
~?m and ?.
2.2.2 Inferring and Employing the LDA Model
Since the integral involved in computing the like-
lihood of a block ~bm is computationally intractable,
several methods for approximating this posterior
have been proposed, including variational expecta-
tion maximization (Blei et al, 2003) and Markov
chain Monte Carlo methods (Griffiths and Steyvers,
2004).
We follow an approach based on Gibbs sampling
as proposed in (Griffiths and Steyvers, 2004). As
the convergence criteria for the Markov chain, we
928
check how well the parameters cluster semantically
related blocks of text in a training corpus and then
we use these values as estimates for comparable set-
tings.
The LDA model provides a soft clustering of the
blocks of text, by associating them to topics. We
exploit this clustering information, by using the dis-
tribution of topics over blocks of text to further
measure the inter-blocks similarity. As in Section
2.1.2, the last step of our system consists in em-
ploying binary support vector classification to iden-
tify the boundaries of thematic episodes in the text.
That is, we consider as input features for support
vector learning the component values of the vector
(?m,z1 , ?m,z2 , ..., ?m,zk).
3 Experiments
In order to evaluate the performance of AMDD and
LDA for our task of topic segmentation, in our ex-
periments we used the transcripts of ICSI-MR cor-
pus (Janin et al, 2004), which consists of 75 meet-
ing recordings. A subset of 25 meetings, which are
transcribed by humans and annotated with thematic
boundaries (Galley et al, 2003), has been kept for
testing purposes and support vector machine train-
ing. The transcripts of the remaining 50 meetings
have been used for the unsupervised inference of
our latent models. The fitting phase of the mix-
ture models rely on the same data set that have been
pre-processed by tokenization, elimination of stop-
words and lemmatization.
Once the models? parameters are learned, the in-
put data representation is projected into the lower
dimension latent semantic space. The evaluation
phase consists in checking the performance of each
model for predicting thematic boundaries. That is,
we check the performance of the models for predict-
ing thematic boundaries on the same test set. The
size of a block of text during the testing phase has
been set to one, i.e. each utterance has been consid-
ered as a block of text.
Figure 3 compares the performance obtained for
various k values, i.e. various dimensions of the latent
semantic space, or equivalently different numbers of
latent topics. We have chosen k={50, ...400} using
incremental steps of 50.
The performance of each latent model is mea-
0.000
0
0.100
0
0.200
0
0.300
0
0.400
0
0.500
0
0.600
0
0.700
0
0.800
0
0.900
0
50
100
150
200
250
300
350
400
Laten
t spa
ce di
men
sion
Accuracy
PLSA LDA
Figure 3: Results of applying the mixture models for
topic segmentation.
sured by the accuracy Acc = 1 ? Pk, where Pk
denotes the error measure proposed by (Beeferman
et al, 1999). Note that the Pk error allows for a
slight variation in where the hypothesized thematic
boundaries are placed. That is, wrong hypothesized
thematic boundaries occurring in the proximity of
a reference boundary (i.e. in a fixed-size interval of
text) are tolerated. As proposed by (Beeferman et
al., 1999), we set up the size of this interval to half
of the average number of words per segment in the
gold standard segmentation.
As we observe from Figure 3, LDA and AMDD
achieved rather comparable thematic segmenta-
tion accuracy. While LDA steadily outperformed
AMDD, the results do not show a notable advan-
tage of LDA over AMDD. In contrast, AMDD has
better performances for less dimensionality reduc-
tion. That is, the LDA performance curve goes down
when the number of latent topics exceeds over 300.
LDA LCSeg SVMs
Pk error rate 21% 32 % 22%
Table 1: Comparative performance results.
In Table 1, we provide the best results obtained
on ICSI data via LDA modeling. We also reproduce
the results reported on in the literature by (Galley
et al, 2003) and (Georgescul et al, 2006), when
the evaluation of their systems was also done on
ICSI data. The LCSeg system proposed by (Gal-
ley et al, 2003) is based on exploiting merely lex-
ical features. Improved performance results have
929
been obtained by (Galley et al, 2003) when extra
non-lexical features have been adopted in a decision
tree classifier. The system proposed by (Georges-
cul et al, 2006) is based on support vector machines
(SVMs) and is labeled in the table as SVMs. We
observe from the table that our approach based on
combining LDA modeling with SVM classification
outperforms LCSeg and performs comparably to the
system of Georgescul et al (2006). Thus, our exper-
iments show that the LDA word density estimation
approach does capture important information from
the data through 90% less features than a bag-of-
words representation.
4 Conclusions
With the goal of performing linear topic segmen-
tation by exploiting word distributions in the input
text, the focus of this article was on both comparing
theoretical aspects and experimental results of two
probabilistic mixture models. The algorithms are
applied to a meeting transcription data set and are
found to provide an appropriate method for reduc-
ing the size of the data representation, by perform-
ing comparably to previous state-of-the-art methods
for topic segmentation.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210. Special Issue on Natural
Language Learning.
David M. Blei, Andrew Y. Ng, and Michael Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, pages 993?1022.
Freddy Choi. 2000. Advances in Domain Indepen-
dent Linear Text Segmentation. In Proceedings of the
1st Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL),
Seattle, USA.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multi-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 562?569,
Sapporo, Japan.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. Word Distributions for Thematic Seg-
mentation in a Support Vector Machine Approach. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 101?108,
New York City, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2007. Exploiting Structural Meeting-Specific
Features for Topic Segmentation. In Actes de la
14e`me Confe?rence sur le Traitement Automatique des
Langues Naturelles (TALN), pages 15?24, Toulouse,
France.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the National
Academy of Sciences, volume 101, pages 5228?5235.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177?196.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004.
The ICSI Meeting Project: Resources and Research.
In Proceedings of the International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
Meeting Recognition Workshop, Montreal, Quebec,
Canada.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL), pages 25?32, Sydney, Australia.
Jane Morris and Graeme Hirst. 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indicator
of the Structure of Text. Computational Linguistics,
17(1):21?48.
Matthew Purver, Konrad P. Ko?rding, Thomas L. Grif-
fiths, and Joshua B. Tenenbaum. 2006. Unsupervised
Topic Modelling for Multi-Party Spoken Discourse.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL), pages 17?24, Sydney, Australia.
Jeffrey Reynar. 1994. An Automatic Method of Finding
Topic Boundaries. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 331?333, Las Cruces, New Mexico,
USA.
930
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144?151,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Analysis of Quantitative Aspects in the Evaluation of Thematic
Segmentation Algorithms
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We consider here the task of linear the-
matic segmentation of text documents, by
using features based on word distributions
in the text. For this task, a typical and of-
ten implicit assumption in previous stud-
ies is that a document has just one topic
and therefore many algorithms have been
tested and have shown encouraging results
on artificial data sets, generated by putting
together parts of different documents. We
show that evaluation on synthetic data is
potentially misleading and fails to give an
accurate evaluation of the performance on
real data. Moreover, we provide a criti-
cal review of existing evaluation metrics in
the literature and we propose an improved
evaluation metric.
1 Introduction
The goal of thematic segmentation is to iden-
tify boundaries of topically coherent segments
in text documents. Giving a rigorous definition
of the notion of topic is difficult, but the task
of discourse/dialogue segmentation into thematic
episodes is usually described by invoking an ?in-
tuitive notion of topic? (Brown and Yule, 1998).
Thematic segmentation also relates to several no-
tions such as speaker?s intention, topic flow and
cohesion.
Since it is elusive what mental representations
humans use in order to distinguish a coherent
text, different surface markers (Hirschberg and
Nakatani, 1996; Passonneau and Litman, 1997)
and external knowledge sources (Kozima and Fu-
rugori, 1994) have been exploited for the purpose
of automatic thematic segmentation. Halliday and
Hasan (1976) claim that the text meaning is re-
alised through certain language resources and they
refer to these resources by the term of cohesion.
The major classes of such text-forming resources
identified in (Halliday and Hasan, 1976) are: sub-
stitution, ellipsis, conjunction, reiteration and col-
location. In this paper, we examine one form of
lexical cohesion, namely lexical reiteration.
Following some of the most prominent dis-
course theories in literature (Grosz and Sidner,
1986; Marcu, 2000), a hierarchical representation
of the thematic episodes can be proposed. The
basis for this is the idea that topics can be re-
cursively divided into subtopics. Real texts ex-
hibit a more intricate structure, including ?seman-
tic returns? by which a topic is suspended at one
point and resumed later in the discourse. However,
we focus here on a reduced segmentation prob-
lem, which involves identifying non-overlapping
and non-hierarchical segments at a coarse level of
granularity.
Thematic segmentation is a valuable initial
tool in information retrieval and natural language
processing. For instance, in information ac-
cess systems, smaller and coherent passage re-
trieval is more convenient to the user than whole-
document retrieval and thematic segmentation has
been shown to improve the passage-retrieval per-
formance (Hearst and Plaunt, 1993). In cases such
as collections of transcripts there are no headers
or paragraph markers. Therefore a clear separa-
tion of the text into thematic episodes can be used
together with highlighted keywords as a kind of
?quick read guide? to help users to quickly navi-
gate through and understand the text. Moreover
automatic thematic segmentation has been shown
to play an important role in automatic summariza-
tion (Mani, 2001), anaphora resolution and dis-
144
course/dialogue understanding.
In this paper, we concern ourselves with the task
of linear thematic segmentation and are interested
in finding out whether different segmentation sys-
tems can perform well on artificial and real data
sets without specific parameter tuning. In addi-
tion, we will refer to the implications of the choice
of a particular error metric for evaluation results.
This paper is organized as follows. Section 2
and Section 3 describe various systems and, re-
spectively, different input data selected for our
evaluation. Section 4 presents several existing
evaluation metrics and their weaknesses, as well
as a new evaluation metric that we propose. Sec-
tion 5 presents our experimental set-up and shows
comparisons between the performance of different
systems. Finally, some conclusions are drawn in
Section 6.
2 Comparison of Systems
Combinations of different features (derived for ex-
ample from linguistic, prosodic information) have
been explored in previous studies like (Galley et
al., 2003) and (Kauchak and Chen, 2005). In
this paper, we selected for comparison three sys-
tems based merely on the lexical reiteration fea-
ture: TextTiling (Hearst, 1997), C99 (Choi, 2000)
and TextSeg (Utiyama and Isahara, 2001). In the
following, we briefly review these approaches.
2.1 TextTiling Algorithm
The TextTiling algorithm was initially developed
by Hearst (1997) for segmentation of exposi-
tory texts into multi-paragraph thematic episodes
having a linear, non-overlapping structure (as re-
flected by the name of the algorithm). TextTiling
is widely used as a de-facto standard in the eval-
uation of alternative segmentation systems, e.g.
(Reynar, 1998; Ferret, 2002; Galley et al, 2003).
The algorithm can briefly be described by the fol-
lowing steps.
Step 1 includes stop-word removal, lemmatiza-
tion and division of the text into ?token-sequences?
(i.e. text blocks having a fixed number of words).
Step 2 determines a score for each gap between
two consecutive token-sequences, by computing
the cosine similarity (Manning and Schu?tze, 1999)
between the two vectors representing the frequen-
cies of the words in the two blocks.
Step 3 computes a ?depth score? for each token-
sequence gap, based on the local minima of the
score computed in step 2.
Step 4 consists in smoothing the scores.
Step 5 chooses from any potential boundaries
those that have the scores smaller than a certain
?cutoff function?, based on the average and stan-
dard deviation of score distribution.
2.2 C99 Algorithm
The C99 algorithm (Choi, 2000) makes a linear
segmentation based on a divisive clustering strat-
egy and the cosine similarity measure between any
two minimal units. More exactly, the algorithm
consists of the following steps.
Step 1: after the division of the text into min-
imal units (in our experiments, the minimal unit
is an utterance1), stop words are removed and a
stemmer is applied.
The second step consists of constructing a sim-
ilarity matrix Sm?m, where m is the number of
utterances and an element sij of the matrix corre-
sponds to the cosine similarity between the vectors
representing the frequencies of the words in the i-
th utterance and the j-th utterance.
Step 3: a ?rank matrix? Rm?m is computed, by
determining for each pair of utterances, the num-
ber of neighbors in Sm?m with a lower similarity
value.
In the final step, the location of thematic bound-
aries is determined by a divisive top-down cluster-
ing procedure. The criterion for division of the
current segment B into b1, ...bm subsegments is
based on the maximisation of a ?density? D, com-
puted for each potential repartition of boundaries
as
D =
?m
k=1 sumk?m
k=1 areak
,
where sumk and areak refers to the sum of rank
and area of the k-th segment in B, respectively.
2.3 TextSeg Algorithm
The TextSeg algorithm (Utiyama and Isahara,
2001) implements a probabilistic approach to de-
termine the most likely segmentation, as briefly
described below.
The segmentation task is modeled as a problem
of finding the minimum cost C(S) of a segmenta-
tion S. The segmentation cost is defined as:
C(S) ? ?logPr(W|S)Pr(S),
1Occasionally within this document we employ the term
utterance to denote either a sentence or an utterance in its
proper sense.
145
where W = w1w2...wn represents the text con-
sisting of n words (after applying stop-words re-
moval and stemming) and S = S1S2...Sm is a po-
tential segmentation of W in m segments. The
probability Pr(W|S) is defined using Laplace
law, while the definition of the probability Pr(S)
is chosen in a manner inspired by information the-
ory.
A directed graph G is defined such that a path
in G corresponds to a possible segmentation of
W . Therefore, the thematic segmentation pro-
posed by the system is obtained by applying a dy-
namic programming algorithm for determining the
minimum cost path in G.
3 Input Data
When evaluating a thematic segmentation system
for an application, human annotators should pro-
vide the gold standard. The problem is that the
procedure of building such a reference corpus is
expensive. That is, the typical setting involves an
experiment with several human subjects, who are
asked to mark thematic segment boundaries based
on specific guidelines and their intuition. The
inter-annotator agreement provides the reference
segmentation. This expense can be avoided by
constructing a synthetic reference corpus by con-
catenation of segments from different documents.
Therefore, the use of artificial data for evaluation
is a general trend in many studies, e.g. (Ferret,
2002; Choi, 2000; Utiyama and Isahara, 2001).
In our experiment, we used artificial and real
data, i.e. the algorithms have been tested on the
following data sets containing English texts.
3.1 Artificially Generated Data
Choi (2000) designed an artificial dataset, built by
concatenating short pieces of texts that have been
extracted from the Brown corpus. Any test sample
from this dataset consists of ten segments. Each
segment contains the first n sentences (where 3 ?
n ? 11) of a randomly selected document from
the Brown corpus. From this dataset, we randomly
chose for our evaluation 100 test samples, where
the length of a segment varied between 3 and 11
sentences.
3.2 TDT Data
One of the commonly used data sets for topic seg-
mentation emerged from the Topic Detection and
Tracking (TDT) project, which includes the task
of story segmentation, i.e. the task of segmenting
a stream of news data into topically cohesive sto-
ries. As part of the TDT initiative several datasets
of news stories have been created. In our evalua-
tion, we used a subset of 28 documents randomly
selected from the TDT Phase 2 (TDT2) collection,
where a document contains an average of 24.67
segments.
3.3 Meeting Transcripts
The third dataset used in our evaluation contains
25 meeting transcripts from the ICSI-MR corpus
(Janin et al, 2004). The entire corpus contains
high-quality close talking microphone recordings
of multi-party dialogues. Transcriptions at word
level with utterance-level segmentations are also
available. The gold standard for thematic segmen-
tations has been kindly provided by (Galley et
al., 2003) and has been chosen by considering the
agreement between at least three human annota-
tions. Each meeting is thus divided into contigu-
ous major topic segments and contains an average
of 7.32 segments.
Note that thematic segmentation of meeting
data is a more challenging task as the thematic
transitions are subtler than those in TDT data.
4 Evaluation Metrics
In this section, we will look in detail at the error
metrics that have been proposed in previous stud-
ies and examine their inadequacies. In addition,
we propose a new evaluation metric that we con-
sider more appropriate.
4.1 Pk Metric
(Passonneau and Litman, 1996; Beeferman et al,
1999) underlined that the standard evaluation met-
rics of precision and recall are inadequate for the-
matic segmentation, namely by the fact that these
metrics did not account for how far away is a hy-
pothesized boundary (i.e. a boundary found by
the automatic procedure) from a reference bound-
ary (i.e. a boundary found in the reference data).
On the other hand, it is desirable that an algorithm
that places for instance a boundary just one utter-
ance away from the reference boundary to be pe-
nalized less than an algorithm that places a bound-
ary two (or more) utterances away from the ref-
erence boundary. Hence (Beeferman et al, 1999)
proposed a new metric, called PD, that allows for
a slight vagueness in where boundaries lie. More
146
specifically, (Beeferman et al, 1999) define PD
as follows2:
PD(ref, hyp) =
?
1?i?j?N D(i, j)[?ref (i, j) ?
?hyp(i, j)].
N is the number of words in the reference data.
The function ?ref (i, j) is evaluated to one if the
two reference corpus indices specified by its pa-
rameters i and j belong in the same segment, and
zero otherwise. Similarly, the function ?hyp(i, j)
is evaluated to one, if the two indices are hypothe-
sized by the automatic procedure to belong in the
same segment, and zero otherwise. The ? opera-
tor is the XNOR function ?both or neither?. D(i, j)
is a ?distance probability distribution over the set
of possible distances between sentences chosen
randomly from the corpus?. In practice, a distri-
bution D having ?all its probability mass at a fixed
distance k? (Beeferman et al, 1999) was adopted
and the metric PD was thus renamed Pk.
In the framework of the TDT initiative, (Allan
et al, 1998) give the following formal definition
of Pk and its components:
Pk = PMiss ? Pseg + PFalseAlarm ? (1? Pseg),
where:
PMiss =
PN?k
i=1 [?hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
PFalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
and Pseg is the a priori probability that in
the reference data a boundary occurs within an
interval of k words. Therefore Pk is calculated by
moving a window of a certain width k, where k is
usually set to half of the average number of words
per segment in the gold standard.
Pevzner and Hearst (2002) highlighted several
problems of the Pk metric. We illustrate below
what we consider the main problems of the Pk
metric, based on two examples.
Let r(i, k) be the number of boundaries be-
tween positions i and i + k in the gold standard
segmentation and h(i, k) be the number of bound-
aries between positions i and i+k in the automatic
hypothesized segmentation.
? Example 1: If r(i, k) = 2 and h(i, k) = 1
then obviously a missing boundary should
2Let ref be a correct segmentation and hyp be a segmen-
tation proposed by a text segmentation system. We will keep
this notations in equations introduced below.
be counted in Pk, i.e. PMiss should be in-
creased.
? Example 2: If r(i, k) = 1 and h(i, k) =
2 then obviously PFalseAlarm should be in-
creased.
However, considering the first example, we will
obtain ?ref (i, i + k) = 0, ?hyp(i, i + k) = 0
and consequently PMiss is not increased. By tak-
ing the case from the second example we obtain
?ref (i, i + k) = 0 and ?hyp(i, i + k) = 0, involv-
ing no increase of PFalseAlarm.
In (TDT, 1998), a slightly different defini-
tion is given for the Pk metric: the definition of
miss and false alarm probabilities is replaced with:
P ?Miss =
PN?k
i=1 [1??hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
P ?FalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
where:
?hyp(i, i+ k) =
{
1, if r(i, k) = h(i, k),
0, otherwise.
We will refer to this new definition of Pk by
P ?k. Therefore, by taking the definition of
P ?k and the first example above, we obtain
?ref (i, i+ k) = 0 and ?hyp(i, i+ k) = 0 and thus
P ?Miss is correctly increased. However for the case
of example 2 we will obtain ?ref (i, i + k) = 0
and ?hyp(i, i + k) = 0, involving no increase of
P ?FalseAlarm and erroneous increase of P ?Miss.
4.2 WindowDiff metric
Pevzner and Hearst (2002) propose the alternative
metric called WindowDiff. By keeping our nota-
tions concerning r(i, k) and h(i, k) introduced in
the subsection 4.1, WindowDiff is defined as:
WindowDiff =
PN?k
i=1 [|r(i,k)? h(i,k)|>0]
N?k .
Similar to both Pk and P ?k, WindowDiff is
also computed by moving a window of fixed size
across the test set and penalizing the algorithm
misses or erroneous algorithm boundary detec-
tions. However, unlike Pk and P ?k, WindowDiff
takes into account how many boundaries fall
within the window and is penalizing in ?how
many discrepancies occur between the reference
and the system results? rather than ?determining
how often two units of text are incorrectly labeled
147
as being in different segments? (Pevzner and
Hearst, 2002).
Our critique concerning WindowDiff is that
misses are less penalised than false alarms and
we argue this as follows. WindowDiff can be
rewritten as:
WindowDiff = WDMiss +WDFalseAlarm,
where:
WDMiss =
PN?k
i=1 [r(i,k)>h(i,k)]
N?k ,
WDFalseAlarm =
PN?k
i=1 [r(i,k)<h(i,k)]
N?k .
Hence both misses and false alarms are weighted
by 1N?k .
Note that, on the one hand, there are indeed (N-
k) equiprobable possibilities to have a false alarm
in an interval of k units. On the other hand, how-
ever, the total number of equiprobable possibil-
ities to have a miss in an interval of k units is
smaller than (N-k) since it depends on the num-
ber of reference boundaries (i.e. we can have a
miss in the interval of k units only if in that interval
the reference corpus contains at least one bound-
ary). Therefore misses, being weighted by 1N?k ,
are less penalised than false alarms.
Let Bref be the number of thematic boundaries
in the reference data. Let?s say that the refer-
ence data contains about 20% boundaries and 80%
non-boundaries from the total number of potential
boundaries. Therefore, since there are relatively
few boundaries compared with non-boundaries, a
strategy introducing no false alarms, but introduc-
ing a maximum number of misses (i.e. k ? Bref
misses) can be judged as being around 80% cor-
rect by the WindowDiff measure. On the other
hand, a segmentation with no misses, but with a
maximum number of false alarms (i.e. (N ? k)
false alarms) is judged as being 100% erroneous
by the WindowDiff measure. That is, misses and
false alarms are not equally penalised.
Another issue regarding WindowDiff is that it is
not clear ?how does one interpret the values pro-
duced by the metric? (Pevzner and Hearst, 2002).
4.3 Proposal for a New Metric
In order to address the inadequacies of Pk and
WindowDiff, we propose a new evaluation metric,
defined as follows:
Prerror = Cmiss ? Prmiss + Cfa ? Prfa,
where:
Cmiss (0 ? Cmiss ? 1) is the cost of a miss, Cfa
(0 ? Cfa ? 1) is the cost of a false alarm,
Prmiss =
PN?k
i=1 [?ref hyp(i,k)]
PN?k
i=1 [?ref (i,k)]
,
P rfa =
PN?k
i=1 [?ref hyp(i,k)]
N?k ,
?ref hyp(i, k) =
{
1, if r(i, k) > h(i, k)
0, otherwise
?ref hyp(i, k) =
{
1, if r(i, k) < h(i, k)
0, otherwise.
?ref (i, k) =
{
1, if r(i, k) > 0
0, otherwise.
Prmiss could be interpreted as the probability
that the hypothesized segmentation contains less
boundaries than the reference segmentation in an
interval of k units3, conditioned by the fact that
the reference segmentation contains at least one
boundary in that interval. Analogously Prfa is
the probability that the hypothesized segmentation
contains more boundaries than the reference seg-
mentation in an interval of k units.
For certain applications where misses are more
important than false alarms or vice versa, the
Prerror can be adjusted to tackle this trade-off via
the Cfa and Cmiss parameters. In order to have
Prerror ? [0, 1], we suggest that Cfa and Cmiss
be chosen such that Cfa + Cmiss = 1. By choos-
ing Cfa=Cmiss=12 , the penalization of misses and
false alarms is thus balanced. In consequence, a
strategy that places no boundaries at all is penal-
ized as much as a strategy proposing boundaries
everywhere (i.e. after every unit). In other words,
both such degenerate algorithms will have an error
rate Prerror of about 50%. The worst algorithm,
penalised as having an error rate Prerror of 100%
when k = 2, is the algorithm that places bound-
aries everywhere except the places where refer-
ence boundaries exist.
5 Results
5.1 Test Procedure
For the three datasets we first performed two
common preprocessing steps: common words are
eliminated using the same stop-list and remaining
words are stemmed by using Porter?s algorithm
(1980). Next, we ran the three segmenters de-
scribed in Section 2, by employing the default val-
ues for any system parameters and by letting the
3A unit can be either a word or a sentence / an utterance.
148
systems estimate the number of thematic bound-
aries.
We also considered the fact that C99 and
TextSeg algorithms can take into account a fixed
number of thematic boundaries. Even if the num-
ber of segments per document can vary in TDT
and meeting reference data, we consider that in a
real application it is impossible to provide to the
systems the exact number of boundaries for each
document to be segmented. Therefore, we ran C99
and TextSeg algorithms (for a second time), by
providing them only the average number of seg-
ments per document in the reference data, which
gives an estimation of the expected level of seg-
mentation granularity.
Four additional naive segmentations were also
used for evaluation, namely: no boundaries,
where the whole text is a single segment; all
boundaries, i.e. a thematic boundary is placed af-
ter each utterance; random known, i.e. the same
number of boundaries as in gold standard, distrib-
uted randomly throughout text; and random un-
known: the number of boundaries is randomly
selected and boundaries are randomly distributed
throughout text. Each of the segmentations was
evaluated with Pk, P ?k and WindowDiff, as de-
scribed in Section 4.
5.2 Comparative Performance of
Segmentation Systems
The results of applying each segmentation algo-
rithm to the three distinct datasets are summa-
rized in Figures 1, 2 and 3. Percent error values
are given in the figures and we used the follow-
ing abbreviations: WD to denote WindowDiff er-
ror metric; TextSeg KA to denote the TextSeg algo-
rithm (Utiyama and Isahara, 2001) when the av-
erage number of boundaries in the reference data
was provided to the algorithm; C99 KA to denote
the C99 algorithm (Choi, 2000) when the aver-
age number of boundaries in the reference data
was provided to the algorithm; N0 to denote the al-
gorithm proposing a segmentation with no bound-
aries; All to denote the algorithm proposing the de-
generate segmentation all boundaries; RK to de-
note the algorithm that generates a random known
segmentation; and RU to denote the algorithm that
generates a random unknown segmentation.
5.2.1 Comparison of System Performance
from Artificial to Realistic Data
From the artificial data to the more realistic
data, we expect to have more noise and thus the
algorithms to constantly degrade, but as our ex-
periments show a reversal of the assessment can
appear. More exactly: as can be seen from Figure
1, both C99 and TextSeg algorithms significantly
outperformed TextTiling algorithm on the artifi-
cially created dataset, when the number of seg-
ments was determined by the systems. A com-
parison between the error rates given in Figure
1 and Figure 2 show that C99 and TextSeg have
a similar trend, by significantly decreasing their
performance on TDT data, but still giving bet-
ter results than TextTiling on TDT data. When
comparing the systems by Prerror, C99 has simi-
lar performance with TextTiling on meeting data
(see Figure 3). Moreover, when assessment is
done by using WindowDiff, Pk or P ?k, both C99
and TextSeg came out worse than TextTiling on
meeting data. This demonstrates that rankings ob-
tained when evaluating on artificial data are dif-
ferent from those obtained when evaluating on re-
alistic data. An alternative interpretation can be
given by taking into account that the degenerative
no boundaries segmentation has an error rate of
only 30% by the WindowDiff, Pk and P ?k metrics
on meeting data. That is, we could interpret that
all three systems give completely wrong segmen-
tations on meeting data (due to the fact that topic
shifts are subtler and not as abrupt as in TDT and
artificial data). Nevertheless, we tend to adopt the
first interpretation, given the weaknesses of Pk, P ?k
and WindowDiff (where misses are less penalised
than false alarms), as discussed in Section 4.
5.2.2 The Influence of the Error Metric on
Assessment
By following the quantitative assessment given
by the WindowDiff metric, we observe that the
algorithm labeled N0 is three times better than
the algorithm All on meeting data (see Figure 3),
while the same algorithm N0 is considered only
two times better than All on the artificial data (see
Figure 1). This verifies the limitation of the Win-
dowDiff metric discussed in Section 4.
The four error metrics described in detail in
Section 4 have shown that the effect of knowing
the average number of boundaries on C99 is posi-
tive when testing on meeting data. However if we
want to take into account all the four error met-
149
0
20
40
60
80
100
120
Er
ro
r r
ate
Pk 34.75 11.01 7.89 10 7.15 44.12 55.5 47.71 52.51
P'k 35.1 13.21 8.55 10.94 7.87 44.13 99.58 48.85 80.84
WD 35.73 13.58 9.21 11.34 8.59 43.1 99.59 48.89 80.63
Pr_error 33.33 9.1 7.71 9.34 6.87 49.87 49.79 41.61 45.01
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 1: Error rates of the segmentation systems on artificial data, where k = 42 and Pseg = 0.44.
0
20
40
60
80
100
120
Err
or
 
ra
te
Pk 40.7 21.36 13.97 18.83 11.33 36.02 63.93 37.03 60.04
P'k 44.92 29.5 20.37 27.69 21.4 36.04 100 45.28 89.93
WD 44.76 36.28 30.3 40.26 31.46 46.69 100 53.75 91.92
Pr_error 34.09 25.69 25.62 27.17 21.05 49.96 50 44.89 48.31
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 2: Error rates of the segmentation systems on TDT data, where k = 55 and Pseg = 0.3606.
rics, it is difficult to draw definite conclusions re-
garding the influence of knowing the average num-
ber of boundaries on TextSeg and C99 algorithms.
For example, when tested on TDT data, C99 KA
seems to work better than C99 by Pk and P ?k met-
rics, while the WindowDiff metric gives a contra-
dictory assessment.
6 Conclusions
By comparing the performance of three systems
for thematic segmentation on different kinds of
data, we address two important issues in a quan-
titative evaluation. Strong emphasis was put on
the kind of data used for evaluation and we have
demonstrated experimentally that evaluation on
synthetic data is potentially misleading. The sec-
ond major issue addressed in this paper concerns
the choice of a valuable error metric and its side
effects on the evaluation assessment.
Acknowledgments
This work is supported by the Interactive
Multimodal Information Management project
(http://www.im2.ch/). Many thanks to Andrei
Popescu-Belis and the anonymous reviewers for
their valuable comments. We are grateful to the
International Computer Science Institute (ICSI),
University of California for sharing the data with
us. We also wish to thank Michael Galley who
kindly provided us the thematic annotations of
ICSI data.
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
Detection and Tracking Pilot Study: Final Re-
port. In DARPA Broadcast News Transcription and
Understanding Workshop, pages 194?218, Lands-
downe, VA. Morgan Kaufmann.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation.
Machine Learning, 34(Special Issue on Natural Lan-
guage Learning):177?210.
Gillian Brown and George Yule. 1998. Discourse
Analysis. (Cambridge Textbooks in Linguistics),
Cambridge.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
150
0
20
40
60
80
100
120
Err
or
 
ra
te
P_k 38.22 54.62 40.82 35.65 35.94 30.82 69.09 45.42 68.48
P'_k 39.12 66.78 45.66 39.04 39.6 30.89 100 47.97 95.99
WD 40.82 69.41 49.27 41.98 42.48 29.31 100 49.64 95.48
Pr_error 40.17 40.27 35.45 35.83 36.61 49.8 50 50.7 53.38
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 3: Error rates of the segmentation systems on meeting data, where k = 85 and Pseg = 0.3090.
Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle,
USA.
Olivier Ferret. 2002. Using Collocations for Topic
Segmentation and Link Detection. In The 19th In-
ternational Conference on Computational Linguis-
tics, Taipei, Taiwan.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Annual
Meeting of the Association for Computational Lin-
guistics, pages 562?569.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions and the Structure of Discourse.
Computational Linguistics, 12:175?204.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst and Christian Plaunt. 1993. Subtopic
Structuring for Full-Length Document Access.
In Proceedings of the 16th Annual International
ACM/SIGIR Conference, pages 59?68, Pittsburgh,
Pennsylvania, United States.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Julia Hirschberg and Christine Nakatani. 1996.
A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of
the 34th Annual Meeting on Association for Com-
putational Linguistics, pages 286 ? 293, Santa Cruz,
California.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Macias-Guarasa, Nel-
son Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI Meeting Project: Resources and Re-
search. In ICASSP 2004 Meeting Recognition Work-
shop (NIST RT-04 Spring Recognition Evaluation),
Montreal.
David Kauchak and Francine Chen. 2005. Feature-
based segmentation of narrative documents. In Pro-
ceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing, pages 32?39, Ann Arbor; MI; USA.
Hideki Kozima and Teiji Furugori. 1994. Segmenting
Narrative Text into Coherent Scenes. Literary and
Linguistic Computing, 9:13?19.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Pub Co.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press
Cambridge, MA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1996.
Empirical Analysis of Three Dimensions of Spoken
Discourse: Segmentation, Coherence and Linguistic
Devices.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1).
Lev Pevzner and Marti Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 16(1):19?
36.
Martin Porter. 1980. An Algorithm for Suffix Strip-
ping. Program, 14:130 ? 137.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Penn-
sylvania.
TDT. 1998. The Topic Detection and Tracking - Phase
2 Evaluation Plan. Available from World Wide Web:
http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.
Masao Utiyama and Hitoshi Isahara. 2001. A Statisti-
cal Model for Domain-Independent Text Segmenta-
tion. In ACL/EACL, pages 491?498.
151
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 101?108, New York City, June 2006. c?2006 Association for Computational Linguistics
Word Distributions for Thematic Segmentation in a Support Vector
Machine Approach
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We investigate the appropriateness of us-
ing a technique based on support vector
machines for identifying thematic struc-
ture of text streams. The thematic seg-
mentation task is modeled as a binary-
classification problem, where the different
classes correspond to the presence or the
absence of a thematic boundary. Exper-
iments are conducted with this approach
by using features based on word distri-
butions through text. We provide em-
pirical evidence that our approach is ro-
bust, by showing good performance on
three different data sets. In particu-
lar, substantial improvement is obtained
over previously published results of word-
distribution based systems when evalua-
tion is done on a corpus of recorded and
transcribed multi-party dialogs.
1 Introduction
(Todd, 2005) distinguishes between ?local-level top-
ics (of sentences, utterances and short discourse seg-
ments)? and ?discourse topics (of more extended
stretches of discourse)?.1 (Todd, 2005) points out
that ?discourse-level topics are one of the most elu-
sive and intractable notions in semantics?. Despite
this difficulty in giving a rigorous definition of dis-
course topic, the task of discourse/dialogue segmen-
tation into thematic episodes can be described by
1In this paper, we make use of the term topic or theme as
referring to the discourse/dialogue topic.
invoking an ?intuitive notion of topic? (Brown and
Yule, 1998). Thematic segmentation also relates
to several notions such as speaker?s intention, topic
flow and cohesion.
In order to find out if thematic segment identi-
fication is a feasible task, previous state-of-the-art
works appeal to experiments, in which several hu-
man subjects are asked to mark thematic segment
boundaries based on their intuition and a minimal
set of instructions. In this manner, previous studies,
e.g. (Passonneau and Litman, 1993; Galley et al,
2003), obtained a level of inter-annotator agreement
that is statistically significant.
Automatic thematic segmentation (TS), i.e. the
segmentation of a text stream into topically coher-
ent segments, is an important component in ap-
plications dealing with large document collections
such as information retrieval and document brows-
ing. Other tasks that could benefit from the thematic
textual structure include anaphora resolution, auto-
matic summarisation and discourse understanding.
The work presented here tackles the problem
of TS by adopting a supervised learning approach
for capturing linear document structure of non-
overlapping thematic episodes. A prerequisite for
the input data to our system is that texts are divided
into sentences or utterances.2 Each boundary be-
tween two consecutive utterances is a potential the-
matic segmentation point and therefore, we model
the TS task as a binary-classification problem, where
each utterance should be classified as marking the
2Occasionally within this document we employ the term ut-
terance to denote either a sentence or an utterance in its proper
sense.
101
presence or the absence of a topic shift in the dis-
course/dialogue based only on observations of pat-
terns in vocabulary use.
The remainder of the paper is organised as fol-
lows. The next section summarizes previous tech-
niques, describes how our method relates to them
and presents the motivations for a support vector ap-
proach. Sections 3 and 4 present our approach in
adopting support vector learning for thematic seg-
mentation. Section 5 outlines the empirical method-
ology and describes the data used in this study. Sec-
tion 6 presents and discusses the evaluation results.
The paper closes with Section 7, which briefly sum-
marizes this work and offers some conclusions and
future directions.
2 Related Work
As in many existing approaches to the thematic seg-
mentation task, we make the assumption that the
thematic coherence of a text segment is reflected at
lexical level and therefore we attempt to detect the
correlation between word distribution and thematic
changes throughout the text. In this manner, (Hearst,
1997; Reynar, 1998; Choi, 2000) start by using a
similarity measure between sentences or fixed-size
blocks of text, based on their word frequencies in
order to find changes in vocabulary use and there-
fore the points at which the topic changes. Sen-
tences are then grouped together by using a cluster-
ing algorithm. (Utiyama and Isahara, 2001) models
the problem of TS as a problem of finding the mini-
mum cost path in a graph and therefore adopts a dy-
namic programming algorithm. The main advantage
of such methods is that no training time and corpora
are required.
By modeling TS as binary-classification problem,
we introduce a new technique based on support vec-
tor machines (SVMs). The main advantage offered
by SVMs with respect to methods such as those de-
scribed above is related to the distance (or similarity)
function used. Thus, although (Choi, 2000; Hearst,
1997) employ a distance function (i.e. cosine dis-
tance) to detect thematic shifts, SVMs are capable
of using a larger variety of similarity functions.
Moreover, SVMs can employ distance functions
that operate in extremely high dimensional feature
spaces. This is an important property for our task,
where handling high dimensionality data represen-
tation is necessary (see section 4).
An alternative to dealing with high dimension
data may be to reduce the dimensionality of the
data representation. Therefore, linear algebra di-
mensionality reduction methods like singular value
decomposition have been adopted by (Choi et al,
2001; Popescu-Belis et al, 2004) in Latent Seman-
tic Analysis (LSA) for the task of thematic segmen-
tation. A Probabilistic Latent Semantic Analysis
(PLSA) approach has been adopted by (Brants et
al., 2002; Farahat and Chen, 2006) for the TS task.
(Blei and Moreno, 2001) proposed a TS approach,
by embedding a PLSA model in an extended Hid-
den Markov Model (HMM) approach, while (Yam-
ron et al, 1998) have previously proposed a HMM
approach for TS.
A shortcoming of the methods described above
is due to their typically generative manner of train-
ing, i.e. using the maximum likelihood estimation
for a joint sampling model of observation and la-
bel sequences. This poses the challenge of finding
more appropriate objective functions, i.e. alterna-
tives to the log-likelihood that are more closely re-
lated to application-relevant performance measures.
Secondly, efficient inference and learning for the TS
task often requires making questionable conditional
independence assumptions. In such cases, improved
performance may be obtained by using methods
with a more discriminative character, by allowing
direct dependencies between a label and past/future
observations and by efficient handling higher-order
combinations of input features. Given the discrim-
inative character of SVMs, we expect our model to
attain similar benefits.
3 Support Vector Learning Task and
Thematic Segmentation
The theory of Vapnik and Chervonenkis (Vapnik,
1995) motivated the introduction of support vector
learning. SVMs have originally been used for clas-
sification purposes and their principles have been ex-
tended to the task of regression, clustering and fea-
ture selection. (Kauchak and Chen, 2005) employed
SVMs using features (derived for instance from in-
formation given by the presence of paragraphs, pro-
nouns, numbers) that can be reliably used for topic
102
segmentation of narrative documents. Aside from
the fact that we consider the TS task on different
datasets (not only on narrative documents), our ap-
proach is different from the approach proposed by
(Kauchak and Chen, 2005) mainly by the data repre-
sentation we propose and by the fact that we put the
emphasis on deriving the thematic structure merely
from word distribution, while (Kauchak and Chen,
2005) observed that the ?block similarities provide
little information about the actual segment bound-
aries? on their data and therefore they concentrated
on exploiting other features.
An excellent general introduction to SVMs and
other kernel methods is given for instance in (Cris-
tianini and Shawe-Taylor, 2000). In the section be-
low, we give some highlights representing the main
elements in using SVMs for thematic segmentation.
The support vector learner L is given a training
set of n examples, usually denoted by Strain= ((~u1,
y1),...,(~un, yn))? (U ? Y )n drawn independently
and identically distributed according to a fixed dis-
tribution Pr(u, y) = Pr(y|u)Pr(u). Each train-
ing example consists of a high-dimensional vector ~u
describing an utterance and the class label y. The
utterance representations we chose are further de-
scribed in Section 4. The class label y has only
two possible values: ?thematic boundary? or ?non-
thematic boundary?. For notational convenience, we
replace these values by +1 and -1 respectively, and
thus we have y ? {-1, 1}. Given a hypothesis space
H, of functions h : U ? {?1,+1} having the form
h(~u) = sign(< ~w, ~u > +b), the inductive sup-
port vector learner Lind seeks a decision function
hind from H, using Strain so that the expected num-
ber of erroneous predictions is minimized. Using
the structural risk minimization principle (Vapnik,
1995), the support vector learner gets the optimal de-
cision function h by minimizing the following cost
function:
W ind(~w, b, ?1, ?2, ..., ?n) = 12 < ~w, ~w > +
+ C+
n?
i=0,yi=1
?i + C?
n?
i=0,yi=?1
?i,
subject to:
yi[< ~w ? ~ui > +b] ? 1? ?i for i = 1, 2, ..., n;
?i ? 0 for i = 1, 2, ..., n.
The parameters ~w and b follow from the optimi-
sation problem, which is solved by applying La-
grangian theory. The so-called slack variables ?i,
are introduced in order to be able to handle non-
separable data. The positive parameters C+ and C?
are called regularization parameters and determine
the amount up to which errors are tolerated. More
exactly, training data may contain noisy or outlier
data that are not representative of the underlying dis-
tribution. On the one hand, fitting exactly to the
training data may lead to overfitting. On the other
hand, dismissing true properties of the data as sam-
pling bias in the training data will result in low accu-
racy. Therefore, the regularization parameter is used
to balance the trade-off between these two compet-
ing considerations. Setting the regularization para-
meter too low can result in poor accuracy, while set-
ting it too high can lead to overfitting. In the TS task,
we used an automated procedure to select the regu-
larization parameters, as further described in section
5.3.
In cases where non-linear hypothesis functions
should be optimised, each ~ui can be mapped into
?(~ui) ? F , where F is a higher dimensional space
usually called feature space, in order to make linear
the relation between ~ui and yi. Thus the original lin-
ear learning machine can be adopted in finding the
classification solution in the feature space.
When using a mapping function ? : U ? F ,
if we have a way of computing the inner product
??(~ui), ?(~uj)? directly as a function of the origi-
nal input point, then the so-called kernel function
K(~ui, ~uj) = ??(~ui), ?(~uj)? is proved to simplify
the computational complexity implied by the direct
use of the mapping function ?. The choice of appro-
priate kernels and its specific parameters is an empir-
ical issue. In our experiments, we used the Gaussian
radial basis function (RBF) kernel:
KRBF (~ui, ~uj) = exp(??
2||~ui ? ~uj ||
2).
For the SVM calculations, we used the LIBSVM li-
brary (Chang and Lin, 2001).
4 Representation of the information used
to determine thematic boundaries
As presented in section 3, in the thematic segmen-
tation task, an input ~ui to the support vector classi-
fier is a vectorial representation of the utterance to
103
be classified and its context. Each dimension of the
input vector indicates the value of a certain feature
characterizing the utterance. All input features here
are indicator functions for a word occurring within
a fixed-size window centered on the utterance being
labeled. More exactly, the input features are com-
puted in the following steps:
1. The text has been pre-processed by tokeniza-
tion, elimination of stop-words and lemmatiza-
tion, using TreeTagger (Schmid, 1996).
2. We make use of the so-called bag of words ap-
proach, by mapping each utterance to a bag, i.e.
a set that contains word frequencies. Therefore,
word frequencies have been computed to count
the number of times that each term (i.e. word
lemma) is used in each utterance. Then a trans-
formation of the raw word frequency counts
is applied in order to take into account both
the local (i.e. for each utterance) word fre-
quencies as well as the overall frequencies of
their occurrences in the entire text collection.
More exactly, we made experiments in paral-
lel with three such transformations, which are
very commonly used in information retrieval
domain (Dumais, 1991): tf.idf, tf.normal and
log.entropy.
3. Each i-th utterance is represented by a vector
~ui, where a j-th element of ~ui is computed as:
ui,j =
?
?
i?
t=i?winSize
ft,j
?
?
?
?
i+winSize?
k=i+1
fk,j
?
? ,
where winSize ? 1 and fi,j is the weighted
frequency (determined in the previous step) of
the j-th word from the vocabulary in the i-th ut-
terance. In this manner, we will have ui,j > 0 if
and only if at least two occurrences of the j-th
term occur within (2 ? winSize) utterances on
opposite sides of a boundary candidate. That
is, each ui,j is capturing how many word co-
occurrences appear across the candidate utter-
ance in an interval (of (2?winSize) utterances)
centered in the boundary candidate utterance.
4. Each attribute value from the input data is
scaled to the interval [0, 1].
Note that the vector space representation adopted in
the previous steps will result in a sparse high dimen-
sional input data for our system. More exactly, table
1 shows the average number of non-zero features per
example corresponding to each data set (further de-
scribed in section 5.1).
Data set Non zero features
ICSI 3.67%
TDT 0.40%
Brown 0.12%
Table 1: The percentage of non-zero features per ex-
ample.
5 Experimental Setup
5.1 Data sets used
In order to evaluate how robust our SVM approach
is, we performed experiments on three English data
sets of approximately the same dimension (i.e. con-
taining about 260,000 words).
The first dataset is a subset of the ICSI-MR cor-
pus (Janin et al, 2004), where the gold standard for
thematic segmentations has been provided by tak-
ing into account the agreement of at least three hu-
man annotators (Galley et al, 2003). The corpus
consists of high-quality close talking microphone
recordings of multi-party dialogues. Transcriptions
at word level with utterance-level segmentations are
also available. A test sample from this dataset con-
sists of the transcription of an approximately one-
hour long meeting and contains an average of about
seven thematic episodes.
The second data set contains documents randomly
selected from the Topic Detection and Tracking
(TDT) 2 collection, made available by (LDC, 2006).
The TDT collection includes broadcast news and
newswire text, which are segmented into topically
cohesive stories. We use the story segmentation pro-
vided with the corpus as our gold standard labeling.
A test sample from our subset contains an average
of about 24 segments.
The third dataset we use in this study was origi-
nally proposed in (Choi, 2000) and contains artifi-
cial thematic episodes. More precisely, the dataset
is built by concatenating short pieces of texts that
104
Data set Weighting schema winSize ? C
ICSI log.entropy 57 0.0625 0.01
TDT tf.idf 17 0.0625 0.1
Brown tf.idf 5 0.0625 0.001
Table 2: The optimal settings found for the SVM model, using the RBF kernel.
have been randomly extracted from the Brown cor-
pus. Any test sample from this dataset consists of
ten segments. Each segment contains at least three
sentences and no more than eleven sentences.
While the focus of our paper is not on the method
of evaluation, it is worth pointing out that the per-
formance on the synthetic data set is a very poor
guide to the performance on naturally occurring data
(Georgescul et al, 2006). We include the synthetic
data for comparison purposes.
5.2 Handling unbalanced data
We have a small percentage of positive examples
relative to the total number of training examples.
Therefore, in order to ensure that positive points are
not considered as being noisy labels, we change the
penalty of the minority (positive) class by setting the
parameter C+ of this class to:
C+ = ? ?
(
n
n+ ? 1
? 1
)
? C?,
where n+ is the number of positive training exam-
ples, n is the total number of training examples and
? is the scaling factor. In the experiments reported
here, we set the value for the scale factor ? to ? = 1
and we have: C+ = 7 ? C? for the synthetic data
derived from Brown corpus; C+ = 18 ? C?for the
TDT data and C+ = 62 ? C? for the ICSI meeting
data.
5.3 Model selection
We used 80% of each dataset to determine the best
model settings, while the remaining 20% is used
for testing purposes. Each training set (for each
dataset employed) was divided into disjoint subsets
and five-fold cross-validation was applied for model
selection.
In order to avoid too many combinations of pa-
rameter settings, model selection is done in two
phases, by distinguishing two kinds of parameters.
First, the parameters involved in data representation
(see section 4) are addressed. We start with choosing
an appropriate term weighting scheme and a good
value for the winSize parameter. This choice is
based on a systematic grid search over 20 differ-
ent values for winSize and the three variants tf.idf,
tf.normal and log.entropy for term weighting. We
ran five-fold cross validation, by using the RBF ker-
nel with its parameter ? fixed to ? = 1. We also set
the regularization parameter C equal to C = 1.
In the second phase of model selection, we
take the optimal parameter values selected in the
previous phase as a constant factor and search
the most appropriate values for C and ? para-
meters. The range of values we select from is:
C ?
{
10?3, 10?2, 10?1, 1, 10, 102, 103
}
and ? ?
{
2?6, 2?5, 2?4, ..., 24, 26
}
and for each possible
value we perform five-fold cross validation. There-
fore, we ran the algorithm five times for the 91 =
7 ? 13 parameter settings. The most suitable model
settings found are shown in Table 2. For these set-
tings, we show the algorithm?s results in section 6.
6 Evaluation
6.1 Evaluation Measures
Beeferman et al (1999) underlined that the stan-
dard evaluation metrics of precision and recall are
inadequate for thematic segmentation, namely by
the fact that these metrics did not account for how
far away a hypothesized boundary (i.e. a boundary
found by the automatic procedure) is from the ref-
erence boundary. On the other hand, for instance,
an algorithm that places a boundary just one utter-
ance away from the reference boundary should be
penalized less than an algorithm that places a bound-
ary ten (or more) utterances away from the reference
boundary.
Hence the use of two other evaluation metrics
is favored in thematic segmentation: the Pk met-
ric (Beeferman et al, 1999) and the WindowDiff
error metric (Pevzner and Hearst, 2002). In con-
105
020406080100120 Algorith
ms
Error rates
P_k18.
5411.0
152.51
20.492
1.3660
.04
21.683
1.912
354.6
268.48
WD19.
4713.5
880.63
23.993
6.2891
.92
25.535
.8825.4
769.41
95.48
SVMC
99   Ran
d      
SVMC
99Ran
d  
SVMG
03 G03
* C99
Rand
Brown d
ata
TDT da
ta
ICSI da
ta
Figure 1: Error rates of the segmentation systems.
trast to precision and recall, these metrics allow for a
slight vagueness in where the hypothesized thematic
boundaries are placed and capture ?the notion of
nearness in a principled way, gently penalizing algo-
rithms that hypothesize boundaries that aren?t quite
right, and scaling down with the algorithm?s degra-
dation? (Beeferman et al, 1999). That is, comput-
ing both Pk and WindowDiff metrics involves the
use of a fixed-size (i.e. having a fixed number of
either words or utterances) window that is moved
step by step over the data. At each step, Pk and
WindowDiff are basically increased (each metric in
a slightly different way) if the hypothesized bound-
aries and the reference boundaries are not within the
same window.
During the model selection phase, we used pre-
cision and recall in order to measure the system?s
error rate. This was motivated by the fact that pos-
ing the TS task as a classification problem leads to a
loss of the sequential nature of the data, which is an
inconvenient in computing the Pk and WindowDiff
measures. However, during the final testing phase
of our system, as well as for the evaluation of the
previous systems, we use both the Pk and the Win-
dowDiff error metric.
The relatively small size of our datasets does not
allow for dividing our test set into multiple sub-test
sets for applying statistical significance tests. This
would be desirable in order to indicate whether the
differences in system error rates are statistically sig-
nificant over different data sets. Nevertheless, we
believe that measuring differences in error rates ob-
tained on the test set is indicative of the relative per-
formance. Thus, the experimental results shown in
this paper should be considered as illustrative rather
than exhaustive.
6.2 Results
In order to determine the adequacy of our SVM ap-
proach over different genres, we ran our system over
three datasets, namely the ICSI meeting data, the
TDT broadcast data and the Brown written genre
data.
By measuring the system error rates using the
Pk and the WindowDiff metrics, Figure 1 summa-
rizes the quantitative results obtained in our empir-
ical evaluation. In Figure 1, our SVM approach is
labeled as SVM and we abbreviate WindowDiff as
WD. The results of our SVM system correspond to
the parameter values detected during model selec-
tion (see Table 2). We compare our system against
an existing thematic segmenter in the literature: C99
(Choi, 2000). We also give for comparison the
error rates of a naive algorithm, labeled as Rand
algorithm, which randomly distributes boundaries
throughout the text.
The LCseg system (Galley et al, 2003), labeled
here as G03, is to our knowledge the only word dis-
tribution based system evaluated on ICSI meeting
data. Therefore, we replicate the results reported by
(Galley et al, 2003) when evaluation of LCseg was
done on ICSI data. The so-labeled G03* algorithm
106
indicates the error rates obtained by (Galley et al,
2003) when extra (meeting specific) features have
been adopted in a decision tree classifier. However,
note that the results reported by (Galley et al) are
not directly comparable with our results because of
a slight difference in the evaluation procedure: (Gal-
ley et al) performed 25-fold cross validation and the
average Pk and WD error rates have been computed
on the held-out sets.
Figure 1 illustrates the following interesting re-
sults. For the ICSI meeting data, our SVM approach
provides the best performance relative to the com-
peting word distribution based state-of-the-art meth-
ods. This proves that our SVM-based system is able
to build a parametric model that leads to a segmenta-
tion that highly correlates to a human thematic seg-
mentation. Furthermore, by taking into account the
relatively small size of the data set we used for train-
ing, it can be concluded that the SVM can build
qualitatively good models even with a small train-
ing data. The work of (Galley et al, 2003) shows
that the G03* algorithm is better than G03 by ap-
proximately 10%, which indicates that on meeting
data the performance of our word-distribution based
approach could possibly be increased by using other
meeting-specific features.
By examining the error rates given by Pk metric
for the three systems on the TDT data set, we ob-
serve that our system and C99 performed more or
less equally. With respect to the WindowDiff met-
ric, our system has an error rate approximately 10%
smaller than C99.
On the synthetic data set, the SVM approach
performed slightly worse than C99, avoiding how-
ever catastrophic failure, as observed with the C99
method on ICSI data.
7 Conclusions
We have introduced a new approach based on word
distributions for performing thematic segmentation.
The thematic segmentation task is modeled here as
a binary classification problem and support vector
machine learning is adopted. In our experiments, we
make a comparison of our approach versus existing
linear thematic segmentation systems reported in the
literature, by running them over three different data
sets. When evaluating on real data, our approach ei-
ther outperformed the other existing methods or per-
forms comparably to the best. We view this as a
strong evidence that our approach provides a unified
and robust framework for the thematic segmentation
task. The results also suggest that word distributions
themselves might be a good candidate for capturing
the thematic shifts of text and that SVM learning can
play an important role in building an adaptable cor-
relation.
Our experiments also show the sensitivity of a
segmentation method to the type of a corpus on
which it is tested. For instance, the C99 algorithm
which achieves superior performance on a synthetic
collection performs quite poorly on the real-life data
sets.
While we have shown empirically that our tech-
nique can provide considerable gains by using sin-
gle word distribution features, future work will in-
vestigate whether the system can be improved by ex-
ploiting other features derived for instance from syn-
tactic, lexical and, when available, prosodic infor-
mation. If further annotated meeting data becomes
available, it would be also interesting to replicate our
experiments on a bigger data set in order to verify
whether our system performance improves.
Acknowledgments This work is partially sup-
ported by the Interactive Multimodal Information
Management project (http://www.im2.ch/). Many
thanks to the reviewers for their insightful sugges-
tions. We are grateful to the International Computer
Science Institute (ICSI), University of California for
sharing the data with us. The authors also thank
Michael Galley who kindly provided us the thematic
annotations of the ICSI data.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177?210.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with an Aspect Hidden Markov Model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 343?348. ACM Press.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. In Pro-
ceedings of the Eleventh International Conference on
107
Information and Knowledge Management, pages 211?
218, McLean, Virginia, USA. ACM Press.
Gillian Brown and George Yule. 1998. Discourse Analy-
sis. Cambridge Textbooks in Linguistics, Cambridge.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th Conference on
Empirical Methods in Natural Language Processing,
Seattle, WA.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33,
Seattle, USA.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge, UK.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229?236.
Ayman Farahat and Francine Chen. 2006. Improving
Probabilistic Latent Semantic Analysis with Principal
Component Analysis. In Proceedings of the 11th Con-
ference of the European Chapter of the Asociation for
Computational Linguistics, Trento, Italy.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 562?569.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An Analysis of Quantitative Aspects in
the Evaluation of Thematic Segmentation Algorithms.
To appear.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004. The
ICSI Meeting Project: Resources and Research. In
ICASSP 2004 Meeting Recognition Workshop (NIST
RT-04 Spring Recognition Evaluation), Montreal.
David Kauchak and Francine Chen. 2005. Feature-
Based Segmentation of Narrative Documents. In Pro-
ceedings of the ACL Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, pages 32?39, Ann Arbor; MI; USA.
LDC. 2006. The Linguistic Data Consortium. Available
from World Wide Web: http://www.ldc.upenn.edu.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based Segmentation: Human Reliability and
Correlation with Linguistic Cues. In Proceedings of
the 31st conference on Association for Computational
Linguistics, pages 148 ? 155, Columbus, Ohio.
Lev Pevzner and Marti Hearst. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics, 16(1):19?36.
Andrei Popescu-Belis, Alexander Clark, Maria Georges-
cul, Sandrine Zufferey, and Denis Lalanne. 2004.
Shallow Dialogue Processing Using Machine Learn-
ing Algorithms (or Not). In Bourlard H. and Ben-
gio S., editors, Multimodal Interaction and Related
Machine Learning Algorithms, pages 277?290. LNCS
3361, Springer-Verlag, Berlin.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Pennsyl-
vania.
Helmut Schmid. 1996. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Technical report, Insti-
tute for Computational Linguistics of the University
of Stuttgart.
Richard Watson Todd. 2005. A fuzzy approach to dis-
course topics. Journal of the International Association
for Semiotic Studies, 155:93?123.
Masao Utiyama and Hitoshi Isahara. 2001. A Statis-
tical Model for Domain-Independent Text Segmenta-
tion. In Proceedings of the 39th Annual Meeting of
the ACL joint with the 10th Meeting of the European
Chapter of the ACL, pages 491?498, Toulouse, France.
Vladimir Naumovich Vapnik. 1995. The Nature of Sta-
tistical Learning Theory. Springer-Verlag, New York.
Jonathan P. Yamron, Ira Carp, Lawrence Gillick, Stewe
Lowe, and Paul van Mulbregt. 1998. A Hidden
Markov Model Approach to Text Segmentation and
Event Tracking. In Proceedings of the IEEE Confer-
ence on Acoustics, Speech, and Signal Processing, vol-
ume 17, pages 333?336, Seattle, WA.
108
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 26?31,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
 
 
A Hedgehop over a Max-Margin Framework Using Hedge Cues 
 
 
Maria Georgescul 
ISSCO, ETI, University of Geneva 
40 bd. du Pont-d'Arve 
 CH-1211 Geneva 4  
maria.georgescul@unige.ch 
 
 
  
 
Abstract 
In this paper, we describe the experimental 
settings we adopted in the context of the 2010 
CoNLL shared task for detecting sentences 
containing uncertainty. The classification results 
reported on are obtained using discriminative 
learning with features essentially incorporating 
lexical information. Hyper-parameters are tuned 
for each domain: using BioScope training data 
for the biomedical domain and Wikipedia 
training data for the Wikipedia test set. By 
allowing an efficient handling of combinations of 
large-scale input features, the discriminative 
approach we adopted showed highly competitive 
empirical results for hedge detection on the 
Wikipedia dataset: our system is ranked as the 
first with an F-score of 60.17%. 
1 Introduction and related work  
One of the first attempts in exploiting a Support 
Vector Machine (SVM) classifier to select 
speculative sentences is described in Light et al 
(2004). They adopted a bag-of-words 
representation of text sentences occurring in 
MEDLINE abstracts and reported on preliminary 
results obtained. As a baseline they used an 
algorithm based on finding speculative sentences 
by simply checking whether any cue (from a 
given list of 14 cues) occurs in the sentence to be 
classified. 
Medlock and Briscoe (2007) also used single 
words as input features in order to classify 
sentences from scientific articles in biomedical 
domain as speculative or non-speculative. In a 
first step they employed a weakly supervised 
Bayesian learning model in order to derive the 
probability of each word to represent a hedge 
cue. In the next step, they perform feature 
selection based on these probabilities. In the last 
step a classifier trained on a given number of 
selected features was applied. Medlock and 
Briscoe (2007) use a similar baseline as the one 
adopted by Light et al (2004), i.e. a na?ve 
algorithm based on substring matching, but with 
a different list of terms to match against. Their 
baseline has a recall/precision break-even point 
of 0.60, while their system improves the 
accuracy to a recall/precision break-even point of 
0.76. However Medlock and Briscoe (2007) note 
that their model is unsuccessful in identifying 
assertive statements of knowledge paucity which 
are generally marked rather syntactically than 
lexically. 
Kilicoglu and Bergler (2008) proposed a semi-
automatic approach incorporating syntactic and 
some semantic information in order to enrich or 
refine a list of lexical hedging cues that are used 
as input features for automatic detection of 
uncertain sentences in the biomedical domain. 
They also used lexical cues and syntactic 
patterns that strongly suggest non-speculative 
contexts (?unhedges?). Then they manually 
expanded and refined the set of lexical hedging 
and ?unhedging? cues using conceptual semantic 
and lexical relations extracted from WordNet 
(Fellbaum, 1998) and the UMLS SPECIALIST 
Lexicon (McCray et al 1994). Kilicoglu and 
Bergler (2008) did experiments on the same 
dataset as Medlock and Briscoe (2007) and their 
experimental results proved that the 
classification accuracy can be improved by 
approximately 9% (from an F-score of 76% to an 
F-score of 85%) if syntactic and semantic 
information are incorporated. 
The experiments run by Medlock (2008) on 
the same dataset as Medlock and Briscoe (2007) 
show that adding features based on part-of-
speech tags to a bag-of-words input 
representation can slightly improve the accuracy, 
but the ?improvements are marginal and not 
statistically significant?. Their experimental 
results also show that stemming can slightly 
26
  
 
Dataset 
#sente
nces 
%uncertain 
sentences 
#distinct 
cues 
#ambiguous 
cues 
 
P 
 
R 
 
F 
Wikipedia training 11111 22% 1912 0.32 0.96 0.48 
Wikipedia test 9634 23% - 188 0.45 0.86 0.59 
BioScope training  14541 18% 168 0.46 0.99 0.63 
BioScope test  5003 16% - 96 0.42 0.98 0.59 
 
Table 1: The percentage of ?uncertain? sentences (% uncertain sentences) given the total number of 
available sentences (#sentences) together with the number of distinct cues in the training corpus and 
the performance of the baseline algorithm based on the list of cues extracted from the training corpus. 
 
improve the classification accuracy, while using 
bigrams brings a statistically significant 
improvement over a simple bag-of-words 
representation. However, Medlock (2008) 
illustrates that ?whether a particular term acts as 
a hedge cue is quite often a rather subtle function 
of its sense usage, in which case the distinctions 
may well not be captured by part-of-speech 
tagging?. 
M?ra et al (2009) also used a machine 
learning framework based on lexical input 
features and part-of-speech tags. Other recent 
work on hedge detection (Ganter and Strube, 
2009; Marco and Mercer, 2004; Mercer et al, 
2004; Morante and Daelemans, 2009a; Szarvas, 
2008) relied primarily on word frequencies as 
primary features including various shallow 
syntactic or semantic information. 
The corpora made available in the CoNLL 
shared task (Farkas et al, 2010; Vincze et al, 
2008) contains multi-word expressions that have 
been annotated by linguists as cue words tending 
to express hedging. In this paper, we test whether 
it might suffice to rely on this list of cues alone 
for automatic hedge detection. The classification 
results reported on are obtained using support 
vector machines trained with features essentially 
incorporating lexical information, i.e. features 
extracted from the list of hedge cues provided 
with the training corpus. 
In the following, we will first describe some 
preliminary considerations regarding the results 
that can be achieved using a na?ve baseline 
algorithm (Section 2). Section 3 summarizes the 
experimental settings and the input features 
adopted, as well as the experimental results we 
obtained on the CoNLL test data. We also report 
on the intermediate results we obtained when 
only the CoNLL training dataset was available. 
In Section 4, we conclude with a brief 
description of the theoretical and practical 
advantages of our system. Future research 
directions are mentioned in Section 5. 
2 Preliminary Considerations  
2.1 Benchmarking 
As a baseline for our experiments, we consider a 
naive algorithm that classifies as ?uncertain? any 
sentence that contains a hedge cue, i.e. any of the 
multi-word expressions labeled as hedge cues in 
the training corpus. 
Table 1 shows the results obtained when using 
the baseline na?ve algorithm on the CoNLL 
datasets provided for training and test purposes1. 
The performance of the baseline algorithm is 
denoted by Precision (P), Recall (R) and F-score 
(F) measures. The first three columns of the table 
show the total number of available sentences 
together with the percentage of ?uncertain? 
sentences occurring in the dataset. The fourth 
column of the table shows the total number of 
distinct hedge cues extracted from the training 
corpus. Those hedge cues occurring in ?certain? 
sentences are denoted as ?ambiguous cues?. The 
fifth column of the table shows the number of 
distinct ambiguous cues. 
As we observe from Table 1, the baseline 
algorithm has very high values for the recall 
score on the BioScope corpus (both training and 
test data). The small percentage of false 
negatives on the BioScope test data reflects the 
fact that only a small percentage of ?uncertain? 
sentences in the reference test dataset do not 
contain a hedge cue that occurs in the training 
dataset. 
The precision of the baseline algorithm has 
values under 0.5 on all four datasets (i.e. on both 
BioScope and Wikipedia data). This illustrates 
that ambiguous hedge cues are frequently used in 
?certain? sentences. That is, the baseline 
algorithm has less true positives than false 
                                                 
1
 In Section 3.2, we provide the performance of the baseline 
algorithm obtained when only the CoNLL training dataset 
was available. When we tuned our system, we obviously 
had available only the results provided in Table 2 (Section 
3.2). 
27
  
positives, i.e. more than 50% of the sentences 
containing a hedge cue are labeled as ?certain? in 
the reference datasets. 
2.2 Beyond bag-of-words 
In order to verify whether simply the frequencies 
of all words (except stop-words) occurring in a 
sentence might suffice to discriminate between 
?certain? and ?uncertain? sentences, we 
performed preliminary experiments with a SVM 
bag-of-words model. The accuracy of this system 
is lower than the baseline accuracy on both 
datasets (BioScope and Wikipedia). For instance, 
the classifier based on a bag-of-words 
representation obtains an F-score of 
approximately 42% on Wikipedia data, while the 
baseline has an F-score of 49% on the same 
dataset. Another disadvantage of using a bag-of-
words input representation is obviously the large 
dimension of the system?s input matrix. For 
instance, the input matrix representation of the 
Wikipedia training dataset would have 
approximately 11111 rows and over 150000 
columns which would require over 6GB of RAM 
for a non-sparse matrix representation. 
3 System Description 
3.1 Experimental Settings  
In our work for the CoNLL shared task, we used 
Support Vector Machine classification (Fan et 
al., 2005; Vapnik, 1998) based on the Gaussian 
Radial Basis kernel function (RBF). We tuned 
the width of the RBF kernel (denoted by gamma) 
and the regularization parameter (denoted by C) 
via grid search over the following range of 
values: {2-8, 2-7, 2-6, ?24} for gamma and {1, 
10..200 step 10, 200..500 step 100} for C. 
During parameter tuning, we performed 10-fold 
cross validation for each possible value of these 
parameters. Since the training data are 
unbalanced (e.g. 18% of the total number of 
sentences in the BioScope training data are 
labeled as ?uncertain?), for SVM training we 
used the following class weights: 
? 0.1801 for the ?certain? class and 0.8198 
for the ?uncertain? class on the BioScope 
dataset;  
? 0.2235 for the ?certain? class and 0.7764 
for the ?uncertain? class on the 
Wikipedia dataset. 
The system was trained on the training set 
provided by the CoNLL shared task organizers 
and tested on the test set provided. As input 
features in our max-margin framework, we 
simply used the frequency of each hedge cue 
provided with the training corpus in each 
sentence. We also used as input features during 
the tuning phase of our system 2-grams and 3-
grams extracted from the list of hedge cues 
provided with the training corpus. 
3.2 Classification results 
 
 
Figure 1: Contour plot of the classification error 
landscape resulting from a grid search over a 
range of values of {2-8, 2-7, 2-6, 2-5, 2-4} for the 
gamma parameter and a range of values of {10, 
20, ?, 110} for the C parameter on Wikipedia 
data. 
 
 
 
Figure 2: Contour plot of the classification error 
landscape resulting from a grid search over a 
range of values of {2-8, 2-7, 2-6, ?2-2} for the 
gamma parameter and a range of values of {1, 
10, 20, 30, ?110} for the C parameter on 
BioScope data. 
 
Figure 1 shows the variability of hedge 
detection results on Wikipedia training data 
when changing the RBF-specific kernel 
parameter and the regularization parameter C. 
The contour plot shows that there are three 
regions (represented in the figure by the darkest 
landscape color) for parameter values where the 
cross validation error is lower than 18.2%. One 
of these optimal settings for parameter values 
was used for the results submitted to the CoNLL 
shared task and we obtained an F-score of 
60.17%. When the CoNLL test data containing  
28
  
Table 2: The performance of our system corresponding to the best parameter values. The performance 
is denoted in terms of true positives (TP), false positives (FP), false negatives (FN), precision (P), 
recall (R) and F-score (F)
.
the reference labels were made available, we also 
did tests with our system 
using the other two optimal settings for 
parameter values. 
The optimal classification results on the 
Wikipedia dataset were obtained for a gamma 
value equal to 0.0625 and for a C value equal to 
10, corresponding to a cross validation 
classification error of 17.94%. The model 
performances corresponding to these best 
parameter values are provided in Table 2. The P, 
R, F-score values provided in Table 2 are 
directly comparable to P, R, F-score values given 
in Table 1 since exactly the same datasets were 
used during the evaluation. 
The SVM approach we adopted shows highly 
competitive empirical results for weasel 
detection on the Wikipedia test dataset in the 
sense that our system was ranked as the first in 
the CoNLL shared task. However, the baseline 
algorithm described in Section 2 proves to be 
rather difficult to beat given its F-score 
performance of 59% on the Wikipedia test data. 
This provides motivation to consider other 
refinements of our system. In particular, we 
believe that it might be possible to improve the 
recall of our system by enriching the list of input 
features using a lexical ontology in order to 
extract synonyms for verbs, adjectives and 
adverbs occurring in the current hedge cue list. 
Figure 2 exemplifies the SVM classification 
results obtained during parameter tuning on 
BioScope training data. The optimal 
classification results on the BioScope dataset 
were obtained for gamma equal to 0.0625
 
and C 
equal to 110, corresponding to a cross validation 
classification error of 3.73%. The model 
performance corresponding to the best parameter 
settings is provided in Table 2. Our system 
obtained an F-score of 0.78 on the BioScope test 
dataset while the best ranked system in the 
CoNLL shared task obtained an F-score of 0.86. 
In order to identify the weaknesses of our system 
in this domain, in Subsection 3.2 we will furnish 
the intermediate results we obtained on the 
CoNLL training set. 
The system is platform independent. We ran 
the experiments under Windows on a Pentium 4, 
3.2GHz with 3GB RAM. The run times 
necessary for training/testing on the whole 
training/test dataset are provided in Table 2.  
Table 3 shows the approximate intervals of 
time required for running SVM parameter tuning 
via grid search on the entire CoNLL training 
datasets. 
 
Dataset Range of values Run 
time  
Wikipedia 
training data 
{2-8,2-7, ?2-1} for 
gamma;  
{10, 20, ?110} for 
C 
13 hours  
BioScope 
training data 
{2-8,2-7, ?2-2} for 
gamma;  
{10, 20, ?110} for 
C 
4 hours 
 
Table 3 : Approximate run times for parameter 
tuning via 10-fold cross validation 
 
3.3 Intermediate results 
In the following we discuss the results obtained 
when the system was trained on approximately 
80% of the CoNLL training corpus and the 
remaining 20% was used for testing. The 80% of 
the training corpus was also used to extract the 
list of hedge cues that were considered as input 
features for the SVM machine learning system. 
The BioScope training corpus provided in 
CoNLL shared task framework contains 11871 
sentences from scientific abstracts and 2670 
sentences from scientific full articles. 
In a first experiment, we only used sentences 
from scientific abstracts for training and testing: 
we randomly selected 9871 sentences for training 
and the remaining 2000 sentences were used for 
testing. The results thus obtained are shown in 
Table 4 on the second line of the table. 
Dataset TP FP FN P R F Run Time 
Wikipedia training 1899 1586 585 0.5449 0.7644 0.6362 49.1 seconds 
Wikipedia test 1213 471 1021 0.7203 0.5429 0.6191 21.5 seconds 
BioScope training 2508 515 112 0.8296 0.9572 0.8888 19.5 seconds 
BioScope test 719 322 71 0.6907 0.9101 0.7854 2.6 seconds 
29
  
 
Table 4: Performances when considering separately the dataset containing abstracts only and the 
dataset containing articles from BioScope corpus. The SVM classifier was trained with gamma = 1 
and c=10. Approximately 80% of the CoNLL train corpus was used for training and 20% of the train 
corpus was held out for testing. 
 
Second, we used only the available 2670 
sentences from scientific full articles. We 
randomly split this small dataset into a training 
set of 2170 sentences and test set of 500 
sentences. 
Third, we used the entire set of 14541 
sentences (composing scientific abstracts and full 
articles) for training and testing: we randomly 
selected 11541 sentences for training and the 
remaining 3000 sentences were used for testing. 
The results obtained in this experiment are 
shown in Table 4 on the fourth line. 
We observe from Table 3 a difference of 10% 
between the F-score obtained on the dataset 
containing abstracts and the F-score obtained on 
the dataset containing full articles. This 
difference in accuracy might simply be due to the 
fact that the available abstracts training dataset is 
approximately 5 times larger than the full articles 
training dataset. In order to check whether this 
difference in accuracy is only attributable to the 
small size of the full articles dataset, we further 
analyze the learning curve of SVM on the 
abstracts dataset. 
To measure the learning curve, we randomly 
selected from the abstracts dataset 2000 
sentences for testing. We divided the remaining 
sentences into 10 parts, we used two parts for 
training, then we increased the size of the 
training dataset by one part incrementally. We 
show the results obtained in Figure 3. The x-axis 
shows the number of sentences used for training 
divided by 1000. We observe that the F-score on 
the test dataset changed only slightly when more 
than 4/10 of the training data (i.e. more than 
4800 sentences) were used for training. We also 
observe that using 2 folds for training (i.e. 
approximately 2000 sentences) gives an F-score 
of around 87% on the held-out test data. 
Therefore, using a similar amount of training 
data for BioScope abstracts as used for BioScope 
full articles, we still have a difference of 8% 
between the F-score values obtained. That is, our 
system is more efficient on abstracts than on full 
articles. 
 
 
 
Figure 3: The performance of our system when 
we used for training various percentages of the 
BioScope training dataset composed of abstracts 
only. 
4 Conclusions 
Our empirical results show that our approach 
captures informative patterns for hedge detection 
through the intermedium of a simple low-level 
feature set. 
Our approach has several attractive theoretical 
and practical properties. Given that the system 
formulation is based on the max-margin 
framework underlying SVMs, we can easily 
incorporate other kernels that induce a feature 
space that might better separate the data. 
Furthermore, SVM parameter tuning and the 
process of building the feature vector matrix, 
which are the most time and resource consuming, 
can be easily integrated in a distributed 
environment considering either cluster-based 
computing or a GRID technology (Wegener et 
al., 2007).  
From a practical point of view, the key aspects 
of our proposed system are its simplicity and 
flexibility. Additional syntactic and semantic 
SVM Baseline Dataset content #sentences 
used for 
training 
#sentences 
used for 
test 
P R F P R F 
Abstracts only 9871 2000 0.85 0.94 0.90 0.49 0.97 0.65 
Full articles only 2170 500 0.72 0.87 0.79 0.46 0.91 0.61 
Abstracts and  
full articles 
11541 3000 0.81 0.92 0.86 0.47 0.98 0.64 
30
  
based features can easily be added to the SVM 
input. Also, the simple architecture facilitates the 
system?s integration in an information retrieval 
system. 
5 Future work 
The probabilistic discriminative model we have 
explored appeared to be well suited to tackle the 
problem of weasel detection. This provides 
motivation to consider other refinements of our 
system, by incorporating syntactic or semantic 
information. In particular, we believe that the 
recall score of our system can be improved by 
identifying a list of new potential hedge cues 
using a lexical ontology. 
References 
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin. 
2005. Working set selection using the second order 
information for training SVM. Journal of 
Machine Learning Research, 6: 1889-918. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12.  
Christiane Fellbaum. 1998. WordNet: An 
Electronic Lexical Database. MIT Press, 
Cambridge, MA. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
Wikipedia tags and shallow linguistic features. In 
Proceedings of joint conference of the 47th 
Annual Meeting of the ACL-IJCNLP.  
Halil Kilicoglu, and Sabine Bergler. 2008. 
Recognizing Speculative Language in Biomedical 
Research Articles: A Linguistically Motivated 
Perspective. In Proceedings of Current Trends 
in Biomedical Natural Language Processing 
(BioNLP), Columbus, Ohio, USA. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, 
Speculations, and Statements in between. In 
Proceedings of the HLT BioLINK. 
Chrysanne Di Marco, and Robert E. Mercer. 2004. 
Hedging in Scientific Articles as a Means of 
Classifying Citations. In Proceedings of 
Working Notes of AAAI Spring Symposium on 
Exploring Attitude and Affect in Text: 
Theories and Applications, Stanford University. 
Alexa T. McCray, Suresh Srinivasan, and Allen C. 
Browne. 1994. Lexical methods for managing 
variation in biomedical terminologies. In 
Proceedings of the 18th Annual Symposium on 
Computer Applications in Medical Care. 
Ben Medlock. 2008. Exploring hedge identification in 
biomedical literature. Journal of Biomedical 
Informatics, 41:636-54. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
Supervised Learning for Hedge Classification in 
Scientific Literature. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics.  
Robert E.Mercer, Chrysanne Di Marco, and Frederick 
Kroon. 2004. The frequency of hedging cues in 
citation contexts in scientific writing. In 
Proceedings of the Canadian Society for the 
Computational Studies of Intelligence 
(CSCSI), London, Ontario. 
Gy?rgy M?ra, Rich?rd Farkas, Gy?rgy Szarvas, and 
Zsolt Moln?r. 2009. Exploring ways beyond the 
simple supervised learning approach for biological 
event extraction. In Proceedings of the BioNLP 
2009 Workshop Companion Volume for 
Shared Task.  
Roser Morante, and Walter Daelemans. 2009a. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the BioNLP 2009 
Workshop, Boulder, Colorado. Association for 
Computational Linguistics. 
Roser Morante, and Walter Daelemans. 2009b. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP.  
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proceedings of the 
ACL-08: HLT.  
Vladimir N. Vapnik. 1998. Statistical learning 
theory. Wiley, New York. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The 
BioScope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9. 
Dennis Wegener, Thierry Sengstag, Stelios R. 
Sfakianakis, and Anthony Assi. 2007. GridR: An 
R-based grid-enabled tool for data analysis in 
ACGT clinico-genomic trials. In Proceedings of 
the 3rd International Conference on e-Science 
and Grid Computing (eScience 2007). 
 
31
