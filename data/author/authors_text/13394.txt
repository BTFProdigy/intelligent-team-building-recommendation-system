Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 797?805,
Beijing, August 2010
Using Cross-Lingual Projections to Generate Semantic Role  
Labeled Corpus for Urdu - A Resource Poor Language 
Smruthi Mukund 
CEDAR 
University at Buffalo 
smukund@buffalo.edu 
Debanjan Ghosh 
Thomson Reuters R&D 
debanjan.ghosh@ 
thomsonreuters.com 
Rohini K. Srihari 
CEDAR 
University at Buffalo 
rohini@cedar.buffalo.edu 
 
Abstract 
In this paper we explore the possibility of 
using cross lingual projections that help 
to automatically induce role-semantic 
annotations in the PropBank paradigm 
for Urdu, a resource poor language. This 
technique provides annotation projections 
based on word alignments. It is relatively 
inexpensive and has the potential to re-
duce human effort involved in creating 
semantic role resources. The projection 
model exploits lexical as well as syntac-
tic information on an English-Urdu paral-
lel corpus. We show that our method ge-
nerates reasonably good annotations with 
an accuracy of 92% on short structured 
sentences. Using the automatically gen-
erated annotated corpus, we conduct pre-
liminary experiments to create a semantic 
role labeler for Urdu. The results of the 
labeler though modest, are promising and 
indicate the potential of our technique to 
generate large scale annotations for Urdu.  
1 Introduction 
Semantic Roles (also known as thematic roles) 
help to understand the semantic structure of a 
document (Fillmore, 1968). At a fundamental 
level, they help to capture the similarities and 
differences in the meaning of verbs via the ar-
guments they define by generalizing over surface 
syntactic configurations.  In turn, these roles aid 
in domain independent understanding as the se-
mantic frames and semantic understanding sys-
tems do not depend on the syntactic configura-
tion for each new application domain. Identify-
ing semantic roles benefit several language 
processing tasks - information extraction (Sur-
deanu et al, 2003), text categorization (Moschitti, 
2008) and finding relations in textual entailment 
(Burchardt and Frank 2006). 
Automatically identifying semantic roles is of-
ten referred to as shallow semantic parsing (Gil-
dea and Jurafsky, 2002). For English, this 
process is facilitated by the existence of two 
main SRL annotated corpora ? FrameNet (Baker 
et al, 1998) and PropBank (Palmer et al, 2005). 
Both datasets mark almost all surface realizations 
of semantic roles. FrameNet has 800 semantic 
frames that cover 120,000 example sentences1. 
PropBank has annotations that cover over 
113,000 predicate-argument structures. Clearly 
English is well supported with resources for se-
mantic roles. However, there are other widely 
spoken resource poor languages that are not as 
privileged. The PropBank based resources avail-
able for languages like Chinese (Xue and Palmer, 
2009), Korean (Palmer et al, 2006) and Spanish 
(Taule, 2008) are only about two-thirds the size 
of the English PropBank.  
Several alternative techniques have been ex-
plored in the literature to generate semantic role 
labeled corpora for resource poor languages as 
providing manually annotated data is time con-
suming and involves intense human labor. Am-
bati and Chen (2007) have conducted an exten-
sive survey and outlined the benefits of using 
parallel corpora to transfer annotations. A wide 
range of annotations from part of speech (Hi and 
Hwa, 2005) and chunks (Yarowsky et al, 2001) 
to word senses (Diab and Resnik, 2002), depen-
dencies (Hwa et al, 2002) and semantic roles 
(Pado and Lapata, 2009) have been successfully 
transferred between languages. FrameNet style 
annotations in Chinese is obtained by mapping 
English FrameNet entries directly to concepts 
listed in HowNet2 (online ontology for Chinese) 
with an accuracy of 68% (Fung and Chen, 2004). 
                                                 
1 Wikipedia - http://en.wikipedia.org/wiki/PropBank 
2 http://www.keenage.com/html/e_index.html 
797
Fung et al (2007) analyze an automatically an-
notated English-Chinese parallel corpus and 
show high cross-lingual agreement for PropBank 
roles (range of 75%-95% based on the roles).  
In this paper we explore the possibility of us-
ing English-Urdu parallel corpora to generate 
SRL annotations for Urdu, a less commonly 
taught language (LCTL). Earlier attempts to gen-
erate SRL corpora using annotation projections 
have been for languages such as German, French 
(Pado and Lapata, 2009) and Italian (Moschitti, 
2009) that have high vocabulary overlap with 
English. Also, German belongs to the same lan-
guage family as English (Germanic family). Ur-
du on the other hand is an Indic language that is 
grammatically very different and shares almost 
no vocabulary with English.  
The technique of cross lingual projections war-
rants good BLEU score that ensures correct word 
alignments. According to NIST 2008 Open Ma-
chine Translation challenge 3 , a 0.2280 best 
BLEU score was achieved for Urdu to English 
translation. This is comparable to the BLEU 
scores achieved for German to English ? 0.253 
and French to English ? 0.3 (Koehn, 2005). But, 
for SRL transfer, perfect word alignment is not 
mandatory as SRL requires semantic correspon-
dence only. According to Fillmore (1982) se-
mantic frames are based on conceptual structures. 
They are generalizations over surface structures 
and hence less prone to syntactic variations. 
Since English and Urdu have a reasonable se-
mantic correspondence (Example 3), we believe 
that the projections when capped with a post 
processing step will considerably reduce the 
noise induced by inaccurate alignments and pro-
duce acceptable mappings. 
Hindi is syntactically similar to Urdu. These 
languages are standardized forms of Hindustani. 
They are free word order languages and follow a 
general SOV (Subject-Object-Verb) structure. 
Projection approach has been used by (Mukerjee 
et al, 2006) and (Sinha, 2009) to transfer verb 
predicates from English onto Hindi. Sinha (2009) 
achieves a 90% F-Measure in verb predicate 
transfer from English to Hindi. This shows that 
using cross lingual transfer approach to obtain 
semantic annotations for Urdu from English is an 
idea worth exploring. 
                                                 
3http://www.itl.nist.gov/iaui/894.01/tests/mt/2008/doc/mt08
_official_results_v0.html 
1.1 Approach 
Our approach leverages existing English 
PropBank annotations provided via the SemLink4 
corpus. SemLink provides annotations for 
VerbNet using the pb (PropBank) attribute. By 
using English-Urdu parallel corpus we acquire 
verb predicates and their arguments. When we 
transfer verb predicates (lemmas), we also 
transfer pb attributes. We obtain annotation 
projections from the parallel corpora as follows:  
1. Take a pair of sentences E (in English) and U 
(in Urdu) that are translations of each other.  
2. Annotate E with semantic roles. 
3. Project the annotations from E onto U using 
word alignment information, lexical 
information and linguistic rules that involve 
syntactic information. 
There are several challenges to the annotation 
projection technique. Dorr (1994) presents some 
major lexical-semantic divergence problems 
applicable in this scenario:  
(a) Thematic Divergence - In some cases, al-
though there exists semantic parallelism, the 
theme of the English sentence captured in 
the subject changes into an object in the Ur-
du sentence (Example 1). 
(b) Conflatational Divergence - Sometimes tar-
get translations spans over a group of words 
(Example 1: plays is mapped to kirdar ada). 
Trying to ascertain this word span for se-
mantic roles is difficult as the alignments 
can be incomplete and very noisy. 
(c) Demotional divergence and Structural di-
vergence - Despite semantic relatedness, in 
some sentence pairs, alignments obtained 
from simple projections generate random 
matchings as the usage is syntactically dis-
similar (Example 2). 
Handling all challenges adds complexity to our 
model. The heuristic rules that we implement are 
guided by linguistic knowledge of Urdu. This 
increases the effectiveness of the alignments. 
 
Example 1: 
I 
(subject) 
am Angry at Reheem 
(object) 
 
Raheem 
(subject)  
mujhe 
(object) 
Gussa dilate hai 
(Raheem brings anger in me) 
                                                 
4 http://verbs.colorado.edu/semlink/ 
798
Example 2: (noun phrase to prepositional pharse) 
Ali attended work today 
 
Ali aaj daftar mein haazir tha 
(Ali was present at work today) 
2 Generating Parallel Corpora 
PropBank provides SRL annotated corpora for 
English. It uses predicate independent labels 
(ARG0, ARG1, etc.) which indicate how a verb 
relates to its arguments. The argument types are 
consistent across all uses of a single verb and do 
not consider the sense of the verb. We use the 
PropBank annotations provided for the Wall 
Street Journal (WSJ) part of the Penn Tree bank 
corpus (Marcus et al, 2004). The arguments of a 
verb are labeled sequentially from ARG0 to 
ARG5 where ARG0 is the proto-typical Agent, 
ARG1 is the proto-typical patient, ARG2 is the 
recipient, and so on. There are other adjunct tags 
in the dataset that are indicated by ARGM that 
include tags for location (ARGM-LOC), tempor-
al tags (ARGM-TMP) etc.  
An Urdu corpus of 6000 sentences corres-
ponding to 317 WSJ articles of Penn Tree Bank 
corpus is provided by CRULP5 (used in the NIST 
2008 machine translation task). We consider 
2350 English sentences with PropBank annota-
tions that have corresponding Urdu translations 
(CRULP corpus) for our experiments. 
2.1 Sentence Alignment 
Sentence alignment is a prerequisite for any pa-
rallel corpora processing. As the first step, we 
had to generate a perfect sentence aligned paral-
lel corpus as the translated sentences, despite 
belonging to the same domain (WSJ ? Penn tree 
bank), had several errors in demarcating the sen-
tence boundaries.  
Sentence alignment between English and Urdu 
is achieved over two iterations. In the first itera-
tion, the length of each sentence is calculated 
based on the occurrence of words belonging to 
important part of speech categories such as prop-
er nouns, adjectives and verbs. Considering main 
POS categories for length assessment helps over-
come the conflatational divergence issue. For 
each English sentence, Urdu sentences with the 
same length are considered to be probable candi-
                                                 
5http://www.crulp.org/ 
dates for alignment. In the second iteration, an 
Urdu-English lexicon is used on the Urdu corpus 
and English translations are obtained. An Eng-
lish-Urdu sentence pair with maximum lexical 
match is considered to be sentence aligned.  
Clearly this method is highly dependent on the 
existence of an exhaustive Urdu-English dictio-
nary. The lexicons that we use to perform loo-
kups are collected by mining Wikipedia and oth-
er online resources (Mukund et al, 2010). How-
ever, lexicon lookups will fail for Out-Of-
Vocabulary words. There could also be a colli-
sion if Urdu sentences have English transliterated 
words (Example 3, ?office?). Such errors are 
manually verified for correctness. 
 
Example 3: 
Kya  aaj tum office gaye ? 
 
Did you go to the office today ? 
2.2 Word Alignment 
In the case of generating word alignments it is 
beneficial to calculate alignments in both transla-
tion directions (English ? Urdu and Urdu - Eng-
lish). This nature of symmetry will help to re-
duce alignment errors. We use the Berkeley 
Aligner6 word alignment package which imple-
ments a joint training model with posterior de-
coding (Liang et al, 2006) to consider bidirec-
tional alignments. Predictions are made based on 
the agreements obtained by two bidirectional 
models in the training phase. The intuitive objec-
tive function that incorporates data likelihood 
and a measure of agreement between the models 
is maximized using an EM-like algorithm. This 
alignment model is known to provide 29% re-
duction in AER over IBM model 4 predictions.  
On our data set the word alignment accuracy 
is 71.3% (calculated over 200 sentence pairs). In 
order to augment the alignment accuracy, we 
added 3000 Urdu-English words and phrases ob-
tained from the Urdu-English dictionary to our 
parallel corpus. The alignment accuracy im-
proved by 3% as the lexicon affects the word co-
occurrence count. 
Word alignment in itself does not produce ac-
curate semantic role projections from English to 
Urdu. This is because the verb predicates in Urdu 
can span more than one token. Semantic roles 
                                                 
6 http://nlp.cs.berkeley.edu/Main.html 
799
can cover sentential constituents of arbitrary 
length, and simply using word alignments for 
projection is likely to result in wrong role spans. 
Also, alignments are not obtained for all words. 
This could lead to missing projections. 
One way to correct these alignment errors is to 
devise token based heuristic rules. This is not 
very beneficial as writing generic rules is diffi-
cult and different errors demand specific rules. 
We propose a method that considers POS, tense 
and chunk information along with word align-
ments to project annotations. 
 
 
Figure 1: Projection model 
 
Our proposed approach can be explained in 
two stages as shown in figure 1. In Stage 1 only 
verb predicates are transferred from English to 
Urdu. Stage 2 involves transfer of arguments and 
depends on the output of Stage 1. Predicate 
transfer cannot rely entirely on word alignments 
(?3). Rules devised around the chunk boundaries 
boost the verb predicate recognition rate. 
Any verb group sequence consisting of a main 
verb and its auxiliaries are marked as a verb 
chunk. Urdu data is tagged using the chunk tag 
set proposed exclusively for Indian languages by 
Bharati et al, (2006). Table 1 shows the tags that 
are important for this task. 
 
Verb Chunk Description 
VGF Verb group is finite  (decided by the auxiliaries) 
VGNF Verb group for non-finite adverbial and adjectival chunk 
VGNN Verb group has a gerund 
Table 1: Verb chunk tags in Urdu 
The sentence aligned parallel corpora that we 
feed as input to our model is POS tagged for both 
English and Urdu. Urdu data is also tagged for 
chunk boundaries and morphological features 
like tense, gender and number information.  
Named Entities are also marked on the Urdu data 
set as they help in tagging the ARGM arguments. 
All the NLP taggers (POS, NE, Chunker, and 
Morphological Analyzer) used in this work are 
detailed in Mukund et al, (2010). 
English data is not chunked using a conven-
tional chunk tagger. Each English sentence is 
split into virtual phrases at boundaries deter-
mined by the following parts of speech ? IN, TO, 
MD, POS, CC, DT, SYM,: (Penn Tree Bank tag-
set). These tags represent positions in a sentence 
that typically mark context transitions (they are 
mostly the closed class words). We show later 
how these approximate chunks assist in correct-
ing predicate mappings. 
We use an Urdu-English dictionary (?2.1) that 
assigns English meanings to Urdu words in each 
sentence. Using translation information from a 
dictionary can help transfer verb predicates when 
the translation equivalent preserves the lexical 
meaning of the source language.  
The first rule that gets applied for predicate 
transfer is based on lexicon lookup. If the Eng-
lish verb is found to be a synonym to an Urdu 
word that is part of a verb chunk, then the lemma 
associated with the English word is transferred to 
the entire verb chunk in Urdu. However not all 
translations? equivalents are lexically synonym-
ous. Sometimes the word used in Urdu is differ-
ent in meaning to that in English but relevant in 
the context (lexical divergence).  
The word alignments considered in proximity 
to the approximate English chunks come to res-
cue in such scenarios. Here, for all the words 
occurring in each Urdu verb chunk, correspond-
ing English aligned words are found from the 
word alignments. If the words that are found be-
long to the same approximate English chunk, 
then the verb predicate of that chunk (if present) 
is projected onto the verb chunk in Urdu. This 
heuristic technique increases the verb projection 
accuracy by about 15% as shown in ?4. 
The Penn tree bank tag set for English part of 
speech has different tags for verbs based on the 
tense information. VBD is used to indicate past 
tense, and VBP and VBZ for present tense. Urdu 
800
also has the tense information associated with the 
verbs in some cases. We exploit this similarity to 
project the verb predicates from English onto 
Urdu. 
The adverbial chunk in Urdu includes pure ad-
verbial phrases. These chunks also form part of 
the verb predicates.  
   S 
 
 
RBP          NP                        VGNF 
 
RB         NN   VB     AUXA    
(??????/dobara)     (???/jaan)  (????/dali)        (???/gayi) 
[English meaning ? Revitalized] 
Figure 2: example for demotional divergence 
 
E.g. consider the English word ?revitalized? 
(figure 2). This is tagged VBD. However, the Ur-
du equivalent of this word is ??????? ??? ???? ???? 
(dobara jaan daali gayi ~ to put life in again). 
The POS tags are ?RB, NN, VB, AUXA? (adverb, 
noun, verb, aspectual auxiliary). The word ?do-
bara? is a part of the adverbial chunk RBP and 
the infinite verb chunk VGNF spans across the 
last two words ?daali gayi?. ?jaan? is a noun 
chunk. This kind of demotional divergence is 
commonly observed in languages like Hindi and 
Urdu. In order to consider this entire phrase to be 
the Urdu equivalent representation of the English 
word ?revitalized?, a rule for adverbial chunk is 
included as the last step to account for un-
accommodated English verbs in the projections. 
In the PropBank corpus, predicate argument re-
lations are marked for almost all occurrences of 
non-copula verbs. We however do not have POS 
tags that help to identify non-copula words. 
Words that can be auxiliary verbs occur as non-
copula verbs in Urdu. We maintain a list of such 
auxiliary verbs. When the verb chunk in Urdu 
contains only one word and belongs to the list, 
we simply ignore the verb chunk and proceed to 
the next chunk. This avoids several false posi-
tives in verb projections.  
Stage 2 of the model includes the transfer of 
arguments. In order to see how well our method 
works, we project all argument annotations from 
English onto Urdu. We do not consider word 
alignments for arguments with proper nouns. The 
double metaphone algorithm (Philips 2000) is 
applied on both English NNP (proper noun) 
tagged words as well as English transliterated 
Urdu (NNP) tagged words. Arguments from 
English are mapped onto Urdu for word pairs 
with the same metaphone code. 
For other arguments, we consider word align-
ments in proximity to verb predicates. The argu-
ment boundaries are determined based on chunk 
and POS information. We observe that our me-
thod projects the annotations associated with 
nouns fairly well. However, when the arguments 
contain adjectives, the boundaries are disconti-
nuous. In such cases, we consider the entire 
chunk without the case marker as a probable 
candidate for the projected argument. We also 
have some issues with the ARGM-MOD argu-
ments in that they overlap with the verb predi-
cates. When the verb predicate that it overlaps 
with is a complex predicate, we consider the en-
tire verb chunk to be the Urdu equivalent candi-
date argument. These rules along with word 
alignments yield fairly accurate projections.  
The rules that we propose are dependent on the 
POS, chunk and tense information that are lan-
guage specific. Hence our method is language 
independent only to the extent that the new lan-
guage considered should have similar syntactic 
structure as Urdu. Indic languages fall in this 
category. 
3 Verb Predicates 
Detecting verb predicates can be a challenging 
task especially if very reliable and efficient tools 
such as POS tagger and chunkers are not availa-
ble. We apply the POS tagger (CRULP tagset, 
88% F-Score) and Chunker (Hindi tagset, 90% 
F-Score) provided by Mukund et al, (2010) on 
the Urdu data set and show that syntactic infor-
mation helps to compensate alignment errors. 
Stanford POS tagger7 (Penn Tree bank tagset) is 
applied on the English data set. 
Predicates can be simple predicates that lie 
within the chunk boundary or complex predicates 
when they span across chunk boundaries. When 
verbs in English are expressed in Urdu/Hindi, in 
several cases, more than one word is used to 
achieve perfect translation. In English the tense 
of the verb is mostly captured by the verb mor-
pheme such as ?asked? ?said? ?saying?. In Ur-
du the tense is mostly captured by the auxiliary 
verbs. So a single word English verb such as 
?talking? would be translated into two words 
                                                 
7 http://nlp.stanford.edu/software/tagger.shtml 
801
?batein karna? where ?karna?~ do is the aux-
iliary verb. However this cannot be generalized 
as there are instances when translations are word 
to word. E.g. ?said? is mapped to a single word 
Urdu verb ?kaha?. 
Complex predicates in Urdu can occur in the 
following POS combinations. /oun+Verb, Ad-
jective+Verb, Verb+Verb, Adverb+Verb. Table 2 
lists the main verb tags present in the Urdu POS 
tagset. (refer Penn Tree bank POS tagset for 
English tags). 
 
Urdu Tags Description 
VB Verb 
VBI Infinitive Verb 
VBL Light Verb 
VBLI Infinitive Light Verb 
VBT Verb to be 
AUXA Aspectual Auxiliary 
AUXT Tense Auxiliary 
Table 2: Verb tags 
 
Auxiliary verbs in Urdu occur alongside VB, 
VBI, VBL or VBLI tags. Sinha (2009) defines 
complex predicates as a group of words consist-
ing of a noun (NN/NNP), an adjective (JJ), a verb 
(VB) or an adverb (RB) followed by a light verb 
(VBL/VBLI). Light verbs are those which contri-
bute to the tense and agreement of the verb (Butt 
and Geuder, 2001). However, despite the exis-
tence of a light verb tag, it is noticed that in sev-
eral sentences, verbs followed by auxiliary verbs 
need to be grouped as a single predicate. Hence, 
we consider such combinations as belonging to 
the complex predicate category.  
E1G- According_VBG to_TO some_DT estimates_NNS 
the_DT rule_NN changes_NNS would_MD cut_VB insid-
er_NN filings_NNS by_IN more_JJR than_IN a_DT 
third_JJ 
URD- [Kuch_QN  andaazon_NN  ke_CM  muta-
biq_NNCM]_NP [kanoon_NN mein_CM]_NP [tabdee-
liayan_NN]_NP[ androni_JJ    drjbndywn_NN  
ko_CM]_NP [ayk_CD thayiy_FR se_CM]_NP [zyada_I 
kam_JJ]_JJP [karey_VBL gi_AUXT]_VGF 
Example 4 
Example 4 demonstrates the existence of a light 
verb in a complex predicate. The English verb 
?cut? is mapped to ??? ???? ??? (kam karey gi) 
belonging to the VBF chunk group.  
EG- Rolls_NNP -_: Royce_NNP Motor_NNP 
Cars_NNPS Inc._NNP said_VBD it_PRP expects_VBZ 
its_PRP$ U.S._NNP sales_NNS to_TO remain_VB 
steady_JJ at_IN about_IN 1 200_CD cars_NNS in_IN 
1990_CD 
URD - [Rolls  Royce motor car inc_NNPC ne_CM]_NP 
[kaha_VB]_VBNF [wo_PRP]_NP [apney_PRRFP$]_NP 
[U.S._NNP ki_CM]_NP [ frwKt_NN ko_CM]_NP 
[1990_CD mein_CM]_NP [takreeban_RB]_RBP [1200_CD 
karon_NN par_CM]_NP [mtwazn_JJ]_JJP [rakhne_VBI 
ki_CM]_VGNN [tawaqqo_NN]_NP [karte_VB 
hai_AUXT]_VGF 
Example 5 
 
In example 5, ?said? corresponds to one Urdu 
word ?????(kaha) that also captures the tense in-
formation (past). However, consider the verb 
?expects?. This is a clear case of noun-verb 
complex predicate where ?expects? is mapped to 
????? ???? ???(tawaqqo karte hai). 
E1G- /ot_RB all_PDT those_DT who_WP wrote_VBD 
oppose_VBP the_DT changes_NNS  
URD -wo tamaam  jinhon ne likha tabdeeliyon ke [mukha-
lif_JJ]_JJP [nahi_RB]_RBP [hain_VBT]_VGF 
Example 6 
 
In example 6, verb predicates are ?wrote? and 
?oppose?. Consider the word ?oppose?. There 
are two ways of representing this word in Urdu. 
As a verb chunk the translation would be ?muk-
halifat nahi karte? and as an adjectival chunk 
?mukhalif nahi hai?. The latter form of repre-
sentation is used widely in the available transla-
tion corpus. The Urdu equivalent of ?oppose? is 
?????? ????(mukhalif hai). 
Another interesting observation in example 6 is 
the existence of discontinuous predicates. 
Though ?oppose? is one word in English, the 
Urdu representation has two words that do not 
occur together. The adverb ?nahi? ~?not? oc-
curs between the adjective and the verb. Statisti-
cally dealing with this issue is extremely chal-
lenging and affects the boundaries of other ar-
guments. Generalizing the rules needed to identi-
fy discontinuous predicates requires more de-
tailed analysis of the corpus ? from the linguistic 
aspect ? and has not been attempted in this paper. 
We however map ? ??? ???? ????? ?(mukhalif nahi 
hai) to the predicate ?oppose?. ?nahi? is treated 
as an argument ARG_NEG in PropBank. 
4 Projection Results 
It is impossible for us to report our projection 
results on the entire data set as we do not have it 
manually annotated. For the purpose of evalua-
tion, we manually annotated 100 long sentences 
(L) and 100 short sentences (S) from the full 
2350 sentence set. All the results are reported on 
802
this 200 set of sentences. Set L has sentences that 
each has more than two verb predicates and sev-
eral arguments. The number of words per sen-
tence here is greater than 55.  S; on the other 
hand has sentences with about 40 words each and 
no complex SOV structures. 
The results shown in Table 3 are for all tags 
(verbs+args) that are projected from English onto 
Urdu. In order to understand why the perfor-
mance over L dips, consider the results in Table 
4 that are for verb projections only. Some long 
sentences in English have Urdu translations that 
do not maintain the same structure. For example 
an English phrase ? ?? might prompt individu-
als to get out of stocks altogether? is written in 
Urdu in a way that the English representation 
would be ?what makes individuals to get out of 
stocks is ??. The Urdu equivalent word for 
?prompt? is missing and the associated lemma 
gets assigned to the Urdu equivalent of ?get? 
(the next lemma). This also affects the argument 
projections. Another reason is the effect of word 
alignments itself. Clearly longer sentences have 
greater alignment errors. 
All tags8 100 long sentences 
100 short 
sentences 
Actual Tags 1267 372 
Correct Tags 943 325 
Found Tags 1212 353 
L :  Precision 77.8% Recall 74.4% F-Score 76% 
S:  Precision 92% Recall 87.4% F-Score 89.7% 
Table 3: when all tags are considered 
 
Comparing the results of Table 4 to Table 3, 
we see that argument projections affect the re-
call. This is because the projections of arguments 
depend not only on the word alignments but also 
on the verb predicates. Incorrect verb predicates 
affect the argument projections. 
Only lemma 100 long sentences 
100 short 
sentences 
Actual Tags 670 240 
Correct Tags 490 208 
Overall Tags 720 257 
L: Precision 68% Recall 73.1% F-Score 70.45% 
S : Precision 80.9% Recall 86.6% F-Score 83.65% 
Table 4: for verb projections only 
Table 5 summarizes the results obtained when 
only the word alignments are considered to 
                                                 
8 Tags -  lemma (verb predicates) + arguments, Actual tags 
? number of tags in the English set, Found tags ? number of 
tags transferred to Urdu, Correct Tags ? number of tags 
correctly transferred 
project all tags. But when virtual phrase bounda-
ries in English are also considered, the F-score 
improves by 8% (Table 6). This is because vir-
tual boundaries in a way mark context switch and 
when considered in proximity to the word align-
ments yield better predicate boundaries. 
100 long sentences : only alignments 
Actual Tags 1267 
Correct Tags 617 
Overall Tags 782 
Precision 78.9% Recall 48.7% F-Score 60.2% 
Table 5: with only word alignments  
 
100 long sentences : alignments + virtual boundaries 
Actual Tags 1267 
Correct Tags 792 
Overall Tags 1044 
Precision 75.8% Recall  62.5% F-Score 68.5% 
Table 6: with word alignments and virtual boundaries 
 
100 
Sentences 
ARG
0 
ARG
1 
ARG
2 
ARG
3 
ARG
M 
Long 124 271 67 25 140 
Found 111 203 36 12 114 
P % 89.5 74.9 53.7 48 81.42 
Short 34 47 4 2 19 
Found 30 45 4 2 19 
P % 88.2 95.7 100 100 100 
Table 7: results of argument projections 
Precision (P) on arguments 
 
Table 7 shows the results of argument projec-
tions over the first 4 arguments of PropBank ? 
ARG0, ARG1, ARG2 and ARG3 (out of 24 argu-
ments, majority are sparse in our test set) and the 
adjunct tag set ARGM.  
5 Automatic Detection 
The size of SRL annotated corpus generated for 
Urdu is limited with only 2350 sentences. To 
explore the possibilities of augmenting this data 
set, we train verb predicate and argument detec-
tion models. The results show great promise in 
generating large-scale automatic annotations. 
5.1 Verb Predicate Detection 
Verb predicate detection happens in two stag-
es. In the first stage, the predicate boundaries are 
marked using a CRF (Lafferty et al, 2001) based 
sequence labeling approach. The training data for 
the model is generated by annotating the auto-
matically annotated Urdu SRL corpus using BI 
803
annotations. E.g. kam B-VG, karne par I-VG. The 
non-verb predicates are labeled ?-1?. The model 
uses POS, chunk and lexical information as fea-
tures. We report the results on a set of 77 sen-
tences containing a mix of short and long sen-
tences.  
Number of verb predicates correctly marked 377 
Num of verb predicates found 484 
Actual num of verb predicates 451 
Precision 77.8% Recall 83.5% F-Score 80.54% 
Table 8: CRF results for verb boundaries 
Every verb predicate is associated with a lemma 
mapped from the English VerbNet map file9. E.g. 
the Urdu verb ???  ????  ??? (kam karne par) has 
the lemma ?lower?. The second stage includes 
assigning these lemmas. Lemma assignment is 
based on lookups from a VerbNet like map file. 
We have compiled a large set of Urdu verb pre-
dicates by mapping translations found in the au-
tomatically annotated corpus to the VerbNet map 
file. This Urdu verb predicate list also accommo-
dates complex predicates that occur along with 
verbs such as ?karna ? to do?, ?paana ? to get?, 
etc. (along with different variations of these 
verbs ? karte, kiya, paate etc.). This verb predi-
cate list (manually corrected) consists of 800 en-
tries. Since our gold standard test set is very 
small, the lemma assignment for all verb predi-
cates is 100% (no pb values and hence no 
senses). This list, however, has to be augmented 
further to meet the standards of the English 
VerbNet map file. 
5.2 Argument Detection 
Argument detection (SRL) is done in two steps: 
(1) argument boundary detection (2) argument 
label assignment. We perform tests for step 2 to 
show how well a standard SVM role detection 
model works on the automatically generated Ur-
du data set. For each pair of correct predicate p 
and an argument i we create a feature representa-
tion F p,a  ~ set T of all arguments. To train a mul-
ti-class role-classifier, given the set T of all ar-
guments, T can be rationalized as T arg i+  (positive 
instances) and T arg i?  (negative instances) for each 
argument i. In this way, individual ONE-vs-ALL 
(Gildea and Jurafsky, 2002) classifier for each 
                                                 
9 http://verbs.colorado.edu/semlink/semlink1.1/vn-
pb/README.TXT 
argument i is trained. In the testing phrase, given 
an unseen sentence, for each argument Fp,q is 
generated and classified by each individual clas-
sifier.  
We created a set of standard SRL features as 
shown in table 9. The results (Tables 10 and 11), 
though not impressive, are promising. We be-
lieve that by increasing the number of samples 
(for each argument) in the training set and intel-
ligently controlling the negative samples, the 
results can be improved significantly. 
Training ? 2270 sentences with 7315 argument instances. 
Test ? 77 sentences with 496 argument instances. (22 dif-
ferent role types) 
BaseLine 
Features 
(BL) 
phrase-type (syntactic category; NP, PP etc.), 
predicate (in our case, verb group), path (syn-
tactic path from the argument constituent to 
the predicate), head words (argument and the 
predicate respectively), position (whether the 
phrase is before or after the predicate)  
Detailed 
Features 
BL + POS (of the first word in the predicate), 
chunk tag of the predicate, POS (of the first 
word of the constituent argument), head word 
(of the verb group in a complex predicate), 
named entity (whether the argument contains 
any named entity, such as location, person, 
organization etc.) 
Table 9: Features for SRL 
 
Kernel/features Precision Recall F-Score 
LK ? BL 71.88 48.25 57.74 
LK ? all 73.91 47.55 57.87 
PK ? BL 74.19 48.25 58.47 
PK ?all (best) 73.47 49.65 59.26 
Table 10: Arg0 performance 
 
Kernel/features Precision Recall F-Score 
LK ? BL 69.35 22.87 34.40 
LK ? all 69.84 23.4 35.05 
PK ? BL 73.77 24.14 36.38 
PK ?all (best) 73.8 26.06 38.52 
Table 11: Arg1 Performances 
(PK - polynomial kernel LK ? Linear kernel) 
6 Conclusion 
In this work, we develop an alignment system 
that is tailor made to fit the SRL problem scope 
for Urdu. Furthermore, we have shown that de-
spite English being a totally different language, 
resources for Urdu can be generated if the subtle 
grammatical nuances of Urdu are accounted for 
while projecting the annotations. We plan to 
work on argument boundary detection and ex-
plore other features for argument detection. The 
lemma set generated for Urdu is being refined for 
finer granularity. 
804
References 
 
Ambati, Vamshi and Chen, Wei, 2007. Cross Lingual Syn-
tax Projection for Resource-Poor Languages. CMU. 
Baker, Collin .F., Charles J. Fillmore, John B. Lowe. 1998. 
The Berkeley Frame Net project. COLI/G-ACL. 
Bharati, Akshar, Dipti Misra Sharma, Lakshmi Bai and 
Rajeev Sangal. 2006. AnnCorra: Annotating Corpora 
Guidelines For POS And Chunk Annotation For Indian 
Language. Technical Report, Language Technologies 
Research Centre IIIT, Hyderabad. 
Burchardt, Aljoscha and Anette Frank. 2006. Approaching 
textual entailment with LFG and FrameNet frames. RTE-
2 Workshop. Venice, Italy. 
Butt, Miriam and Wilhelm Geuder. 2001.  On the 
(semi)lexical status of light verbs. /orbert Corver and 
Henk van Riemsdijk, (Eds.), Semi-lexical Categories: On 
the content of function words and the function of content 
words, Mouton de Gruyter, pp. 323?370, Berlin.  
Diab, Mona and Philip Resnik. 2002. An unsupervised me-
thod for word sense tagging using parallel corpora. 40th 
Annual Meeting of ACL, pp. 255-262, Philadelphia, PA. 
Dorr, Bonnie, J. 1994. Machine Translation Divergences: A 
Formal Description and Proposed Solution. ACL, Vol. 
20(4), pp. 597-631. 
Fillmore, Charles J. 1968. The case for case. Bach, & 
Harms( Eds.), Universals in Linguistic Theory, pp. 1-88. 
Holt, Rinehart, and Winston, New York. 
Fillmore, Charles J. 1982. Frame semantics. Linguistics in 
the Morning Calm, pp.111-137. Hanshin, Seoul, S. Ko-
rea. 
Fung, Pascale and Benfeng Chen. 2004. BiFrameNet: Bilin-
gual frame semantics resources construction by cross-
lingual induction. 20th International Conference on 
Computational Linguistics, pp. 931-935, Geneva, Swit-
zerland. 
Fung, Pascale, Zhaojun Wu, Yongsheng Yang and Dekai 
Wu. 2007. Learning bilingual semantic frames: Shallow 
semantic parsing vs. semantic role projection. 11th Con-
ference on Theoretical and Methodological Issues in 
Machine Translation, pp. 75-84, Skovde, Sweden. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labe-
ling of semantic roles. Computational Linguistics, Vol. 
28(3), pp. 245-288. 
Hi, Chenhai and Rebecca Hwa. 2005. A backoff model for 
bootstrapping resources for non-english languages. Joint 
Human Language Technology Conference and Confe-
rence on EM/LP, pp. 851-858, Vancouver, BC. 
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and Okan 
Kolak. 2002. Evaluation translational correspondance us-
ing annotation projection. 40th Annual Meeting of ACL, 
pp. 392-399, Philadelphia, PA. 
Koehn, Phillip. 2005. ?Europarl: A parallel corpus for statis-
tical machine translation,? MT summit, Citeseer. 
Lafferty, John D., Andrew McCallum and C.N. Pereira. 
2001. Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. 18th Inter-
national Conference on Machine Learning, pp. 282-289. 
Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment 
by Agreement, /AACL.  
Marcus, Mitchell P., Beatrice Santorini and Mary Ann Mar-
cinkiewicz. 2004. Building a large annotated corpus of 
English: The Penn Treebank. Computational Linguistics, 
Vol. 19(2), pp. 313-330.  
Moschitti, Alessandro. 2008. Kernel methods, syntax and 
semantics for relational text categorization. 17th ACM 
CIKM, pp. 253-262, Napa Valley, CA. 
Mukerjee, Amitabh , Ankit Soni and Achala M. Raina. 
2006. Detecting Complex Predicates in Hindi using POS 
Projection across Parallel Corpora. Workshop on Multi-
word Expressions: Identifying and Exploiting Underly-
ing Properties, pp. 11?18. Sydney. 
Mukund, S., Srihari, R. K., and Peterson, E. 2010. An In-
formation Extraction System for Urdu ? A Resource Poor 
Language. Special Issue on Information Retrieval for In-
dian Languages. TALIP. 
Pado, Sebastian and Mirella Lapata. 2009. Cross-Lingual 
annotation Projection of Semantic Roles. Journal of Ar-
tificial Intelligence Research, Vol. 36, pp. 307-340. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, Vol. 31(1). 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, and Yeongmi Jeon. 2006. Korean Propbank. Lin-
guistic data consortium, Philadelphia. 
Philips, Lawrence. 2000. The Double Metaphone Search 
Algorithm. C/C++ Users Journal. 
Sinha, R. Mahesh K. 2009. Mining Complex Predicates In 
Hindi Using A Parallel Hindi-English Corpus. ACL In-
ternational Joint Conference in /atural Language 
Processing, pp 40. 
Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul 
Aarseth. 2003. Using predicate-argument structures for 
information extraction. 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 8-15, Sappo-
ro, Japan. 
Taule, Mariona, M. Antonio Marti, and Marta Recasens. 
2008. Ancora: Multi level annotated corpora for Catalan 
and Spanish. 6th International Conference on Language 
Resources and Evaluation, Marrakesh, Morocco. 
Xue, Nianwen and Martha Palmer. 2009. Adding semantic 
roles to the Chinese treebank. /atural Language Engi-
neering, Vol. 15(1), pp. 143-172. 
Yarowsky, David, Grace Ngai and Richard Wicentowski. 
2001. Inducing multi lingual text analysis tools via ro-
bust projection across aligned corpora. 1st Human Lan-
guage Technology Conference, pp. 161-168, San Fran-
cisco, CA. 
805
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 58?67,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Sequence Kernels to identify Opinion Entities in Urdu 
 
 
Smruthi Mukund? and Debanjan Ghosh* Rohini K Srihari 
?SUNY at Buffalo, NY 
smukund@buffalo.edu 
*Thomson Reuters Corporate R&D 
debanjan.ghosh@thomsonreuters.com 
SUNY at Buffalo, NY 
rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
Automatic extraction of opinion holders 
and targets (together referred to as opinion 
entities) is an important subtask of senti-
ment analysis. In this work, we attempt to 
accurately extract opinion entities from 
Urdu newswire. Due to the lack of re-
sources required for training role labelers 
and dependency parsers (as in English) for 
Urdu, a more robust approach based on (i) 
generating candidate word sequences 
corresponding to opinion entities, and (ii) 
subsequently disambiguating these se-
quences as opinion holders or targets is 
presented. Detecting the boundaries of such 
candidate sequences in Urdu is very differ-
ent than in English since in Urdu, 
grammatical categories such as tense, 
gender and case are captured in word 
inflections. In this work, we exploit the 
morphological inflections associated with 
nouns and verbs to correctly identify 
sequence boundaries. Different levels of 
information that capture context are 
encoded to train standard linear and se-
quence kernels. To this end the best per-
formance obtained for opinion entity 
detection for Urdu sentiment analysis is 
58.06% F-Score using sequence kernels 
and 61.55% F-Score using a combination 
of sequence and linear kernels. 
1 Introduction 
Performing sentiment analysis on newswire da-
ta facilitates the development of systems capable 
of answering perspective questions like ?How did 
people react to the latest presidential speech?? and 
?Does General Musharraf support the Indo-Pak 
peace treaty??. The components involved in de-
veloping such systems require accurate identifica-
tion of opinion expressions and opinion entities. 
Several of the approaches proposed in the literature 
to automatically extract the opinion entities rely on 
the use of thematic role labels and dependency 
parsers to provide new lexical features for opinion 
words (Bethard et al, 2004). Semantic roles (SRL) 
also help to mark the semantic constituents (agent, 
theme, proposition) of a sentence. Such features 
are extremely valuable for a task like opinion en-
tity detection. 
English is a privileged language when it comes 
to the availability of resources needed to contribute 
features for opinion entity detection. There are 
other widely spoken, resource poor languages, 
which are still in the infantile stage of automatic 
natural language processing (NLP). Urdu is one 
such language. The main objective of our research 
is to provide a solution for opinion entity detection 
in the Urdu language. Despite Urdu lacking NLP 
resources required to contribute features similar to 
what works for the English language, the perform-
ance of our approach is comparable with English 
for this task (compared with the work of Weigand 
and Klalow, 2010 ~ 62.61% F1). The morphologi-
cal richness of the Urdu language enables us to 
extract features based on noun and verb inflections 
that effectively contribute to the opinion entity ex-
traction task. Most importantly, these features can 
be generalized to other Indic languages (Hindi, 
Bengali etc.) owing to the grammatical similarity 
between the languages. 
58
English has seen extensive use of sequence ker-
nels (string and tree kernels) for tasks such as rela-
tion extraction (Culotta and Sorensen, 2004) and 
semantic role labeling (Moschitti et al, 2008). But, 
the application of these kernels to a task like opin-
ion entity detection is scarcely explored (Weigand 
and Klalow, 2010). Moreover, existing works in 
English perform only opinion holder identification 
using these kernels. What makes our approach 
unique is that we use the power of sequence ker-
nels to simultaneously identify opinion holders and 
targets in the Urdu language. 
Sequence kernels allow efficient use of the 
learning algorithm exploiting massive number of 
features without the traditional explicit feature rep-
resentation (such as, Bag of Words). Often, in case 
of sequence kernels, the challenge lies in choosing 
meaningful subsequences as training samples in-
stead of utilizing the whole sequence. In Urdu 
newswire data, generating candidate sequences 
usable for training is complicated. Not only are the 
opinion entities diverse in that they can be con-
tained within noun phrases or clauses, the clues 
that help to identify these components can be con-
tained within any word group - speech events, 
opinion words, predicates and connectors. 
 
1 Pakistan ke swaat sarhad ke janoobi shahar Banno 
ka havayi adda zarayye ablaagk tavvju ka markaz 
ban gaya hai. 
[ Pakistan?s provincial border?s south city?s airbase 
has become the center of attraction for all reporters.] 
 
Here, the opinion target spans across four noun 
chunks, ? Pakistan?s | provincial border?s | south 
city?s | airbase ?. The case markers (connectors) 
?ke?and?ka ?  indicate the span. 
2  Habib miyan ka ghussa bad gaya aur wo apne aurat 
ko maara. 
[Habib miya?s anger increased and he hit his own 
wife.] 
 
Here, the gender (Masculine) inflection of the verb 
?maara? (hit) indicates that the agent performing 
this action is ? Habib miya?  (Masculine) 
3  Ansari ne kaha ? mere rayee mein Aamir Sohail eek 
badimaak aur Ziddi insaan hai?. 
[Ansari said, ? according to me Aamir Sohail is one 
crazy and stubborn man?] 
 
Here, cues similar to English such as ?mere rayee 
mein ? (according to)  indicate the opinion holder.  
Another interesting behavior here is the presence of 
nested opinion holders. ?kaha? (said)  indicates that 
this statement was made by Ansari only. 
4  Sutlan bahut khush tha, naseer key kaam se.  
[Sultan was very happy with Naseer?s work ] 
 
Here, the target of the expression ?khush? is after the 
verb ?khush tha?(was happy) ? SV O structure  
Table 1: Examples to outline the complexity of the task 
 
Another contributing factor is the free word or-
der of the Urdu language. Although the accepted 
form is SOV, there are several instances where the 
object comes after the verb or the object is before 
the subject. In Urdu newswire data, the average 
number of words in a sentence is 42 (Table 3). 
This generates a large number of candidate se-
quences that are not opinion entities, on account of 
which the data used for training is highly unbal-
anced. The lack of tools such as dependency pars-
ers makes boundary detection for Urdu different 
from English, which in turn makes opinion entity 
extraction a much harder task. Examples shown in 
table 1 illustrate the complexity of the task. 
One safe assumption that can be made for opin-
ion entities is that they are always contained in a 
phrase (or clause) that contains a noun (common 
noun, proper noun or pronoun), which is either the 
subject or the object of the predicate. Based on 
this, we generate candidate sequences by consider-
ing contextual information around noun phrases. In 
example 1 of Table 1, the subsequence that is gen-
erated will consider all four noun phrases ? Paki-
stan?s | provincial border?s | south city?s | 
airbase?  as a single group for opinion entity. 
We demonstrate that investigating postpositions 
to capture semantic relations between nouns and 
predicates is crucial in opinion entity identifica-
tion. Our approach shows encouraging perform-
ance. 
2  Related Work 
Choi et al, (2005) consider opinion entity iden-
tification as an information extraction task and the 
opinion holders are identified using a conditional 
random field (Lafferty et al, 2001) based se-
quence-labeling approach. Patterns are extracted 
using AutoSlog (Riloff et al, 2003). Bloom et al, 
(2006) use hand built lexicons for opinion entity 
identification. Their method is dependent on a 
combination of heuristic shallow parsing and de-
pendency parsing information. Kim and Hovy 
59
(2006) map the semantic frames of FrameNet 
(Baker et al, 1998) into opinion holder and target 
for adjectives and verbs to identify these compo-
nents.  Stoyanov and Cardie (2008) treat the task of 
identifying opinion holders and targets as a co-
reference resolution problem. Kim et al, (2008) 
used a set of communication words, appraisal 
words from Senti-WordNet (Esuli and Sebastiani, 
2006) and NLP tools such as NE taggers and syn-
tactic parsers to identify opinion holders accu-
rately. Kim and Hovy (2006) use structural 
features of the language to identify opinion enti-
ties. Their technique is based on syntactic path and 
dependency features along with heuristic features 
such as topic words and named entities. Weigand 
and Klalow (2010) use convolution kernels that 
use predicate argument structure and parse trees. 
For Urdu specifically, work in the area of clas-
sifying subjective and objective sentences is at-
tempted by Mukund and Srihari, (2010) using a 
vector space model. NLP tools that include POS 
taggers, shallow parser, NE tagger and morpho-
logical analyzer for Urdu is provided by Mukund 
et al, (2010). This is the only extensive work done 
for automating Urdu NLP, although other efforts to 
generate semantic role labels and dependency 
parsers are underway. 
3  Linguistic Analysis for Opinion Entities 
In this section we introduce the different cues 
used to capture the contextual information for cre-
ating candidate sequences in Urdu by exploiting 
the morphological richness of the language. 
Table 2: Case Inflections on Nouns  
Urdu is a head final language with post-
positional case markers. Some post-positions are 
associated with grammatical functions and some 
with specific roles associated with the meaning of 
verbs (Davison, 1999). Case markers play a very 
important role in determining the case inflections 
of nouns. The case inflections that are useful in the 
context of opinion entity detection are ?ergative?, 
?dative?, ?genitive?, ?instrumental?  and ?loca-
tive? . Table 2 outlines the constructs. 
Consider example 1 below. (a) is a case where 
? Ali ? is nominative. However, in (b) ? Ali?  is da-
tive. The case marker ?ko?  helps to identify sub-
jects of certain experiential and psychological 
predicates: sensations, psychological or mental 
states and obligation or compulsion. Such predi-
cates clearly require the subject to be sentient, and 
further, indicate that they are aected in some 
manner, correlating with the semantic properties 
ascribed to the dative?s primary use (Grimm, 
2007). 
 
E xample (1): 
(a)  Ali khush hua  (Ali became happy) 
(b)  Ali ko khushi hui (Ali became happy) 
E xample (2): 
(a) Sadaf kaam karne ki koshish karti hai ( Sadaf 
tries to do work)  
 
Semantic information in Urdu is encoded in a 
way that is very different from English. Aspect, 
tense and gender depend on the noun that a verb 
governs. Example 2 shows the dependency that 
verbs have on nouns without addressing the lin-
guistic details associated with complex predicates. 
In example 2, the verb ?karti?(do)  is feminine 
and the noun it governs ~ Sadaf  is also feminine. 
The doer for the predicate ?karti hai?(does)  is 
? Sadaf? and there exists a gender match. This 
shows that we can obtain strong features if we are 
able to accurately (i) identify the predicates, (ii) 
find the governing noun, and (iii) determine the 
gender. 
In this work, for the purpose of generating can-
didate sequences, we encompass the post-position 
responsible for case inflection in nouns, into the 
noun phrase and group the entire chunk as one sin-
gle candidate. In example 1, the dative inflection 
on ? Ali?  is due to the case marker ? ko?.  Here, ? Ali 
ko?  will always be considered together in all candi-
date sequences that this sentence generates. This 
Case Clitic 
Form 
Examples 
Ergative (ne) Ali  ne ghussa dikhaya ~ 
Ali showed anger  
Accusa-
tive 
(ko) Ali ko mainey maara ~ 
I hit Ali  
Dative (ko,ke) Similar to accusative 
Instru-
mental 
(se) Yeh kaam Ali  se hua ~ 
This work was done by 
Ali  
Genitive (ka, ke, ki) Ali ka ghussa, baap re 
baap! ~ Ali?s anger, oh 
my God!  
Locative (mein, par, 
tak, tale, 
talak) 
Ali mein ghussa zyaada 
hai ~ there is a lot of 
anger in Ali  
60
behavior can also be observed in example 1 of ta-
ble 1.  
We use SemantexTM (Srihari et al, 2008) - an 
end to end NLP framework for Urdu that provides 
POS, NE, shallow parser and morphological ana-
lyzer, to mark tense, mood, aspect, gender and 
number inflections of verbs and case inflections of 
nouns. For ease of parsing, we enclose dative and 
accusative inflected nouns and the respective case 
markers in a tag called POSSE S S . We also enclose 
locative, genitive and ergative inflections and case 
markers in a tag called DOER.  
4  Methodology 
Sequence boundaries are first constructed based 
on the POSSESS, DOER and NP (noun chunk) 
tags prioritized by the position of the tag while 
parsing. We refer to these chunks as ?candidates?  
as they are the possible opinion entity candidates. 
We generate candidate sequences by combining 
these candidates with opinion expressions (Mu-
kund and Srihari, 2010) and the predicates that 
contain or follow the expression words (~khushi in 
(b) of example 1 above). 
We evaluate our approach in two steps: 
(i) Boundary Detection - detecting opinion 
entities that contain both holders and tar-
gets 
(ii) Entity Disambiguation - disambiguating 
opinion holders from opinion targets 
In the following sections, we briefly describe 
our research methodology including sequence 
creation, choice of kernels and the challenges thus 
encountered. 
4.1  Data Set 
The data used for the experiments are newswire 
articles from BBC Urdu1 that are manually anno-
tated to reflect opinion holders, targets, and ex-
pressions (emotion bearing words).  
 
Number of  subjective sentences 824 
Average word length of each sentence 42 
Number of opinion holders  974 
Number of opinion targets 833 
Number of opinion expressions 894 
Table 3: Corpus Statistics 
 
                                                           
1 www.bbc.co.uk/urdu/ 
Table 3 summarizes the corpus statistics. The inter 
annotator agreement established between two an-
notators over 30 documents was found to be 0.85 
using Cohen?s Kappa score (averaged over all 
tags). The agreement is acceptable as tagging emo-
tions is a difficult and a personalized task. 
4.2  Support Vector Machines (SVM) and 
Kernel Methods 
SVMs belong to a class of supervised machine 
learning techniques that merge the nuances of sta-
tistical learning theory, kernel mapping and opti-
mization techniques to discover separating 
hyperplanes. Given a set of positive and negative 
data points, based on structural risk minimization, 
SVMs attempt to find not only a separating hyper-
plane that separates two categories (Vapnik and 
Kotz, 2006) but also maximize the boundary be-
tween them (maximal margin separation tech-
nique). In this work, we propose to use a variation 
of sequence kernels for opinion entity detection. 
4.3  Sequence Kernels 
The lack of parsers that capture dependencies in 
Urdu sentences inhibit the use of ?tree kernels? 
(Weigand and Klalow, 2010). In this work, we ex-
ploit the power of a set of sequence kernels known 
as ?gap sequence string kernels? (Lodhi et al, 
2002). These kernels provide numerical compari-
son of phrases as entire sequences rather than a 
probability at the chunk level. Gap sequence ker-
nels measure the similarity between two sequences 
(in this case a sequence of Urdu words) by count-
ing the number of common subsequences. Gaps 
between words are penalized with suitable use of 
decay factor to compensate for 
matches between lengthy word sequences. 
Formally, let  be the feature space over 
words. Consequently, we declare other disjoint 
feature spaces  (stem words, POS, 
chunks, gender inflections, etc.) 
and
.
For any two-feature 
vectors  let  compute the number 
of common features between s and t. Table 5 lists 
the features used to compute . 
Given two sequences, s and t and the kernel 
function  that calculates the number of 
61
weighted sparse subsequences of length n (say, 
n =2: bigram) common to both s and t, then 
is as shown in eq 1 (Bunescu and 
Mooney, 2005). 
 
(i,j,k are dimensions)                                ------ Eq 1. 
Generating correct sequences is a prior require-
ment for sequence kernels. For example, in the task 
of relation extraction, features included in the 
shortest path between the mentions of the two se-
quences (which hold the relation) play a decisive 
role (Bunescu and Mooney, 2005). Similarly, in 
the task of role labeling (SRL - Moschitti et al, 
2008), syntactic sub-trees containing the arguments 
are crucial in finding the correct associations. Our 
approach to create candidate sequences for opinion 
entity detection in Urdu is explained in the next 
section. 
4.4  Candidate Sequence Generation 
Each subjective sentence in Urdu contains sev-
eral noun phrases with one or more opinion ex-
pressions. The words that express opinions 
(expression words) can be contained within a verb 
predicate (if the predicate is complex) or precede 
the verb predicate. These subjective sentences are 
first pre-processed to mark the morphological in-
flections as mentioned in ?3. 
Table 4: Candidate Sequence Generation 
 
We define training candidate sequences as the 
shortest substring t which is a tuple that contains 
the candidate noun phrase (POSSESS, DOER or 
NP), an emotion expression and the closest predi-
cate. Table 4 outlines the steps taken to create the 
candidate sequences and figure 1 illustrates the 
different tuples for a sample sentence.  
Experiments conducted by Weigand and 
Klakow (2010) consider <candidate, predicate> 
and <candidate, expression> tuples. However, in 
Urdu the sense of expression and predicate are so 
tightly coupled (in many examples they subsume 
each other and hence inseparable), that specifically 
trying to gauge the influence of predicate and 
expression separately on candidates is impossible. 
There are three advantages in our approach to 
creating candidate sequences: (i) by pairing ex-
pressions with their nearest predicates, several un-
necessary candidate sequences are eliminated, (ii) 
phrases that do not contain nouns are automatically 
not considered (see RBP chunk in figure 1), and 
(iii) by considering only one candidate chunk at a 
time in generating the candidate sequence, we en-
sure that the sequence that is generated is short for 
better sequence kernel performance. 
 
4 . 4 .1  Linear Kernel features 
 
For linear kernels we define features explicitly 
based on the lexical relationship between the can-
didate and its context. Table 5 outlines the features 
used.  
 
Feature Sets and Description  
Set 1 
Baseline 
1. head word of candidate 
2. case marker contained within candidate? 
3. expression words  
4. head word of predicate 
5. POS sequence of predicate words 
6. # of NPs between candidate and emotion 
Set 2 7. the DOER 
8. expression right after candidate? 
Set 3  9. gender match between candidate and 
predicate 
10. predicate contains emotion words? 
Set 4  11. POS sequence of candidate 
Set 5   12. ?kah?  feature in the predicate 
13. locative feature? 
14. genitive feature on noun? 
Table 5: Linear Kernel Features 
 
 
 
 
1 A sentence is parsed to extract all likely candi-
date chunks ? POSSESS, DOER, NP in that 
order. 
2 <expression, predicate> t uples are first selected 
based on nearest neighbor rule : 
1. Predicates that are paired with the expres-
sion words either contain the expressions or 
follow the expressions.  
2. Stand alone predicates are simply ignored as 
they do not contribute to the holder identifi-
cation task (they contribute to either the sen-
tence topic or the reason for the emotion). 
3 For each candidate, 
<candidate, expression, predicate> tuples are 
generated without changing the word order.  
(Fig. 1 ? example candidates maintain the same 
word order) 
62
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Illustration of candidate sequences 
 
4 . 4 .1  Sequence Kernel features 
 
Features commonly used for sequence kernels 
are based on words (such as character-based or 
word-based sequence kernels). In this work, we 
consider to be a feature space over Urdu words 
along with other disjoint features such as POS, 
gender, case inflections. In the kernel, however, for 
each combination (see table 6) the similarity 
matching function that computes the num-
ber of similar features remains the same. 
Table 6: Disjoint feature set for sequence kernels 
 
Sequence kernels are robust and can deal with 
complex structures. There are several overlapping 
features between the feature sets used for linear 
kernel and sequence kernel.  Consider the POS 
path information feature. This is an important fea-
ture for the linear kernel. However this feature  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
need not be explicitly mentioned for the sequence 
kernel as the model internally learns the path in-
formation. In addition, several Boolean features 
explicitly described for the linear kernel (2 and 13 
in table 5) are also learned automatically in the 
sequence kernel by matching subsequences. 
5  Experiments 
The data used for our experiments is explained 
in ?4.1. Figure 2 gives a flow diagram of the entire 
process. LIBSVM?s (Cha ng and Lin, 2001) linear 
kernel is trained using the manually coded features 
mentioned in table 5. We integrated our proposed 
sequence kernel with the same toolkit. This se-
quence kernel uses the features mentioned in table 
6 and the decay factor is set to 0.5. 
 
 
 
 
 
 
 
 
 
Figure 2: Overall Process 
KID Kernel Type 
1 word based kernel (baseline) 
2 word + POS (parts of speech) 
3 word + POS + chunk  
4 word + POS + chunk + gender inflection  
63
The candidate sequence generation algorithm gen-
erated 8,329 candidate sequences (contains all opi-
nion holders and targets ? table 3) that are used for 
training both the kernels. The data is parsed using 
SemantexTM to apply POS, chunk and morphology 
information. Our evaluation is based on the exact 
candidate boundary (whether the candidate is en-
closed in a POSSESS, DOER or NP chunk).All 
scores are averaged over a 5-fold cross validation 
set. 
5.1  Comparison of Kernels 
We apply both linear kernels (LK) and se-
quence kernels (SK) to identify the entities as well 
as disambiguate between the opinion holders and 
targets. Table 7 illustrates the baselines and the 
best results for boundary detection of opinion enti-
ties. ID 1 of table 7 represents the result of using 
LK with feature set 1 (table 5). We interpret this as 
our baseline result. The best F1 score for this clas-
sifier is 50.17%. 
Table 7: Boundary detection of Opinion Entities 
 
Table 8 compares various kernels and combina-
tions. Set 1 of table 8 shows the relative effect of 
feature sets for LK and how each set contributes to 
detecting opinion entity boundaries. Although sev-
eral features are inspired by similar classification 
techniques (features used for SRL and opinion 
mining by Choi et al, (2005) ~ set 1, table 5), the 
free word nature of Urdu language renders these 
features futile. Moreover, due to larger average 
length of each sentence and high occurrences of 
NPs (candidates) in each sentence, the number of 
candidate instances (our algorithm creates 10 se-
quences per sentence on average) is also very high 
as compared to any English corpus.  This makes 
the training corpus highly imbalanced. Interest-
ingly, when features like ? occurrence of postposi-
tions, ?kah? predicate, gender inflections etc. are 
used, classification improves (set 1, Feature set 
1,2,3,4,5, table 8). 
Table 8: Kernel Performance 
 
ID 3 of table 7 displays the baseline result for SK. 
Interestingly enough, the baseline F1 for SK is 
very close to the best LK performance. This shows 
the robustness of SK and its capability to learn 
complex substructures with only words. A se-
quence kernel considers all possible subsequence 
matching and therefore implements a concept of 
partial (fuzzy) matching. Because of its tendency 
to learn all fuzzy matches while penalizing the 
gaps between words intelligently, the performance 
of SK in general has better recall (Wang, 2008). To 
explain the recall situation, consider set 2 of table 
8. This illustrates the effect of disjoint feature 
scopes of each feature (POS, chunk, gender). Each 
feature adds up and expands the feature space of 
sequence kernel and allows fuzzy matching there-
by improving the recall. Hence KID 4 has almost 
20% recall gain over the baseline (SK baseline).  
However, in many cases, this fuzzy matching 
accumulates in wrong classification and lowers 
precision. A fairly straightforward approach to 
overcome this problem is to employ a high preci-
sion kernel in addition to sequence kernel. Another 
limitation of SK is its inability to capture complex 
I
D Kernel 
Features  
(table 
5/6 ) 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
1 LK Baseline (Set 1) 39.58 51.49 44.75  
2 LK(best) Set 1, 2, 3, 4, 5 44.20 57.99 50.17  
3  SK Baseline (KID 1) 58.22 42.75 49.30  
4 SK (best) KID 4 54.00 62.79 58.06  
5 
Best LK 
+ best 
SK 
KID 4, 
Set 1, 2, 
3, 4, 5 
5 8 . 4 3  65 . 0 4  61.55  
Set Kernel KID Prec.  
(% )  
Rec.  
(% )  
F1 
(% )  
Baseline 
(Set 1) 
39.58  51.49  44.75  
Set 1,2 39.91  52.57  45.38  
Set 1, 2, 3 43.55  57.72  49.65  
Set 1,2,3,4 44.10  56.90  49.68  
 
 
 
 
1 
 
 
 
 
LK 
Feature set 
1,2,3,4,5 
4 4 . 2 0  57 . 9 9  50 .17  
Baseline - 
KID 1 
58.22 42.75  49.30  
KID 2 5 8 . 9 8  47.55  52.65 
KID 3 58.18  49.62  53.59  
 
 
2 
 
 
SK 
KID 4 54.00 6 2 . 7 9  58 . 0 6  
KID 1 + 
best LK 
51.44 6 8 . 8 9  58.90  
KID 2 + 
best LK 
5 9 .18  62.98  61.02 
KID 3 + 
best LK 
55.18 68.38  61.07  
 
 
 
3 
 
 
 
SK  +  
LK 
KID 4 + 
best LK 
58.43  65.04 61.55  
64
grammatical structure and dependencies making it 
highly dependent on only the order of the string 
sequence that is supplied. 
We also combine the similarity scores of SK 
and LK to obtain the benefits of both kernels. This 
permits SK to expand the feature space by natu-
rally adding structural features (POS, chunk) re-
sulting in high recall. At the same time, LK with 
strict features (such as the use of ?kah?  verb) or 
rigid word orders (several Boolean features) will 
help maintain acceptable precision. By summing 
the contribution of both kernels, we achieve an F1 
of 61.55% (Set 3, table 8), which is 17.8%, more 
(relative gain ? around 40%) than the LK baseline 
results (ID 1, table 7). 
 
Table 9: Opinion entity disa mbiguation for best features 
Our next sets of experiments are conducted to dis-
ambiguate opinion holders and targets. A large 
number of candidate sequences that are created are 
not candidates for opinion entities. This results in a 
huge imbalance in the data set. Jointly classify 
opinion holders, opinion targets and false candi-
dates with one model can be attempted if this im-
balance in the data set due to false candidates can 
be reduced. However, this has not been attempted 
in this work. In order to showcase the feasibility of 
our method, we train our model only on the gold 
standard candidate sequences that contain opinion 
entities for entity disambiguation. 
The two kernels are applied on just the two 
classes (opinion holder vs. opinion target). Com-
bined kernels identify holders with a 65.26% F1 
(table 9). However, LK performs best for target 
identification (61.23%). We believe that this is due 
to opinion holders and targets sharing similar syn-
tactic structures. Hence, the sequence information 
that SK learns affects accuracy but improves recall. 
6  Challenges 
Based on the error analysis, we observe some 
common mistakes and provide some examples. 
1. Mistakes resulting due to POS tagger and shal-
low chunker errors. 
2. Errors due to heuristic rules for morphological 
analysis. 
3.  Mistakes due to inaccurate identification of ex-
pression words by the subjectivity classifier. 
4. Errors due to complex and unusual sentence 
structures which the kernels failed to capture. 
 
Example (3):  
Is na-insaafi ka badla hamein zaroor layna chahiye. 
[ we  have to certainly take revenge for this injustice. ] 
E xample (4): 
Kya hum dayshadgardi ka shikar banna chahatein 
hai? 
[Do we  want to become victims of terrorism ? ] 
E xample (5): 
Jab secretary kisi aur say baat karke husthi hai, tho 
Pinto ko ghussa aata hai. 
[When the secretary talks to someone and laughs, 
Pinto  gets angry.] 
 
Example 3 is a false positive. The emotion is ?an-
ger?, indicated by ?na-insaafi ka badla? (revenge 
for injustice) and ?zaroor? (certainly) .  But only 
the second expression word is identified accu-
rately. The sequence kernel model determines na-
insaafi (injustice) to be the opinion holder when it 
is actually the reason for the emotion. However, it 
also identifies the correct opinion holder - hamein 
(we) .  Emotions associated with interrogative sen-
tences are not marked (example 4) as there exists 
no one word that captures the overall emotion. 
However, the subjectivity classifier identifies such 
sentences as subjective candidates. This results in 
false negatives for opinion entity detection. The 
target (secretary) in example 5, fails to be detected 
as no candidate sequence that we generate indi-
cates the noun ?secretary?  to be the target. We 
propose to address these issues in our future work. 
7  Conclusion 
We describe an approach to identify opinion en-
tities in Urdu using a combination of kernels. To 
the best of our knowledge this is the first attempt 
where such an approach is used to identify opinion 
entities in a language lacking the availability of 
resources for automatic text processing. The per-
formance for this task for Urdu is equivalent to the 
state of the art performance for English (Weigand 
and Klakow, 2010) on the same task.  
Kernel Opinion 
Entity 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
Holder 58.71 66.67 62.44 LK 
(best) Target 6 5 . 5 3 57.48 61.23 
Holder 60.26 69.46 64.54 SK 
 Target 59.75 49.73 54.28 
Holder 62.90 6 9 . 81 65. 2 6 Both 
kernels Target 60.71 55.44 57.96 
65
References  
Collin F. Baker, Charles J. Fillmore, John B. Lowe. 
1998. The Berkeley FrameN et Project, Proceedings 
of the 17th international conference on Computa-
tional linguistics, August 10-14. Montreal, Quebec, 
Canada 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic 
Extraction of Opinion Propositions and their Holders, 
AAAI Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications. 
Kenneth Bloom, Sterling Stein, and Shlomo Argamon. 
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Work-
shop Meeting, Tokyo, Japan. 
R. C. Bunescu and R. J. M ooney. 2005. A shortest path 
dependency kernel for relation extraction. In Pro-
ceedings of HLT/EMNLP. 
R. C. Bunescu and R. J.  Mooney. 2005. Subsequence 
Kernels for Relation Extraction. NIPS. Vancouver. 
December. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.cs ie.ntu.edu.tw/~cjlin/libsvm 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan. 2005. Identifying Sources of Opinions 
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human 
Language Technology and Empirical Methods in 
Natural Language Processing (HLT/EMNLP), Van-
couver, Canada.  
Aaron Culotta and Jeffery Sorensen. 2004. Dependency 
tree kernels for relation extraction. In Proceedings of 
the 42rd Annual Meeting of the Association for 
Computational Linguistics. pp. 423-429.   
Alice Davison. 1999. Syntax  and Morphology in Hindi 
and Urdu: A Lexical Resource.  University of Iowa.  
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiWordNet: A publicly available lexical resource for 
opinion mining. In Proc of LREC. Vol 6, pp 417-422.  
Scott Gimm. 2007. Subject Ma rking in Hindi/Urdu: A 
Study in Case and Agency. ESSLLI Student Session. 
Malaga, Spain. 
Youngho Kim, Seaongchan Kim and Sun-Hyon 
Myaeng. 2008. Extracting Topic-related Opinions 
and their Targets in NTCIR-7. In Proceedings of the 
7th NTCIR Workshop Meeting. Tokyo. Japan. 
John Lafferty, Andrew McCa llum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In: Proc. 
18th International Conf. on Machine Learning, Mor-
gan Kaufmann, San Francisco, CA . pp. 282?289 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, 
Nello Cristianini, Chris Watkins. 2002. Text 
classification using string kernels. J. Mach. Learn. 
Res. 2 (March 2002), 419-44. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In ACL Workshop on Sen-
timent and Subjectivity in Text. 
Alessandro Moschitti, Daniele Pighin, Roberto Basili. 
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics. Vol 34, num 2, pp 193-224. 
Smruthi Mukund and Rohini K. Srihari. 2010. A Vector 
Space Model for Subjectivity Classification in Urdu 
aided by Co-Training, In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics, 
Beijing, China.  
Smruthi Mukund, Rohini K. Srihari and Erik Peterson. 
2010. An Information Extraction System for Urdu ? 
A Resource Poor Language. Special Issue on Infor-
mation Retrieval for Indian Languages. 
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Seventh Confer-
ence on Natural Language Learning (CoNLL-03). 
Rohini K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. 
InfoXtract: A Customizable Intermediate Level In-
formation Extraction Engine, Journal of Natural Lan-
guage Engineering, Cambridge U. Press, 14(1), pp. 
33-69. 
Veselin Stoyanov and Claire Cardie. 2008.  Annotating 
Topic Opinions. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2008), Marrakech, Morocco. 
John Shawe-Taylor and Nello  Cristianni. 2004. Kernel 
methods for pattern analysis. Cambridge University 
Press. 
Mengqiu Wang. 2008. A Re-examination of Depend-
ency Path Kernels for Relation Extraction, In 
Proceedings of IJCNLP 2008. 
Michael Wiegand and Dietrich Klalow. 2010. Convolu-
tion kernels for opinion holder extraction. In Proc. of 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. pp 795-
803, ACL 
66
Vladimir Vapnik, S.Kotz . 2006. Estimation of De-
pendences Based on Empirical Data. Springer,  510 
pages. 
67
Proceedings of the First Workshop on Argumentation Mining, pages 39?48,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Analyzing Argumentative Discourse Units in Online Interactions
Debanjan Ghosh* Smaranda Muresan? Nina Wacholder* Mark Aakhus* Matthew Mitsui**
*School of Communication and Information, Rutgers University
?Center of Computational Learning Systems, Columbia University
**Department of Computer Science, Rutgers University
debanjan.ghosh|ninwac|aakhus|mmitsui@rutgers.edu, smara@ccls.columbia.edu
Abstract
Argument mining of online interactions is
in its infancy. One reason is the lack of
annotated corpora in this genre. To make
progress, we need to develop a principled
and scalable way of determining which
portions of texts are argumentative and
what is the nature of argumentation. We
propose a two-tiered approach to achieve
this goal and report on several initial stud-
ies to assess its potential.
1 Introduction
An increasing portion of information and opin-
ion exchange occurs in online interactions such
as discussion forums, blogs, and webpage com-
ments. This type of user-generated conversation-
al data provides a wealth of naturally occurring
arguments. Argument mining of online interac-
tions, however, is still in its infancy (Abbott et al.,
2011; Biran and Rambow, 2011; Yin et al., 2012;
Andreas et al., 2012; Misra and Walker, 2013).
One reason is the lack of annotated corpora in this
genre. To make progress, we need to develop a
principled and scalable way of determining which
portions of texts are argumentative and what is the
nature of argumentation.
We propose a multi-step coding approach
grounded in findings from argumentation re-
search on managing the difficulties of coding ar-
guments (Meyers and Brashers, 2010). In the first
step, trained expert annotators identify basic ar-
gumentative features (coarse-grained analysis) in
full-length threads. In the second step, we explore
the feasibility of using crowdsourcing and novice
annotators to identify finer details and nuances of
the basic argumentative units focusing on limited
thread context. Our coarse-grained scheme for ar-
gumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby,
Figure 1: Argumentative annotation of an Online
Thread
2013; Maynard, 1985). PAT states that an argu-
ment can arise at any point when two or more
actors engage in calling out and making prob-
lematic some aspect of another actor?s prior con-
tribution for what it (could have) said or meant
(Van Eemeren et al., 1993). The argumentative
relationships among contributions to a discussion
are indicated through links between what is tar-
geted and how it is called-out. Figure 1 shows
an example of two Callouts that refer back to the
same Target.
The annotation task performed by the trained
annotators includes three subtasks that Peldszus
and Stede (2013a) identify as part of the argu-
ment mining problem: 1) Segmentation, 2) Seg-
ment classification, and 3) Relationship identifi-
cation. In the language of Peldszus and Stede
(2013a), Callouts and Targets are the basic Argu-
ment Discourse Units (ADUs) that are segmented,
classified, and linked. There are two key advan-
tages of our coarse-grained annotation scheme:
1) It does not initially prescribe what constitutes
an argumentative text; 2) It makes it possible for
Expert Annotators (EAs) to find ADUs in long
39
threads. Assigning finer grained (more complex)
labels would have unduly increased the already
heavy cognitive load for the EAs. In Section
2 we present the corpus, describe the annotation
scheme and task, calculate Inter Annotator Agree-
ment (IAA), and propose a hierarchical clustering
approach to identify text segments that the EAs
found easier or harder to annotate.
In Section 3, we report on two Amazon
Mechanical Turk (MTurk) experiments, which
demonstrate that crowdsourcing is a feasible way
to obtain finer grained annotations of basic ADUs,
especially on the text segments that were easier
for the EAs to code. In the first crowd sourc-
ing study, the Turkers (the workers at MTurk,
who we consider novice annotators) assigned la-
bels (Agree/Disagree/Other) to the relations be-
tween Callout and Target identified by the EAs.
In the second study, Turkers labeled segments of
Callouts as Stance or Rationale. Turkers saw only
a limited context of the threaded discussion, i.e.
a particular Callout-Target pair identified by the
EA(s) who had analyzed the entire thread. In addi-
tion we report on initial classification experiments
to detect agreement/disagreement, with the best
F1 of 66.9% for the Agree class and 62.6% for the
Disagree class.
2 Expert Annotation for Coarse-Grained
Argumentation
Within Pragmatic Argumentation Theory, argu-
mentation refers to the ways in which people (seek
to) make some prior action or antecedent event
disputable by performing challenges, contradic-
tions, negations, accusations, resistance, and other
behaviors that call out a ?Target?, a prior action
or event. In this section, we present the corpus,
the annotation scheme based on PAT and the an-
notation task, the inter-annotator agreement, and a
method to identify which pieces of text are easier
or harder to annotate using a hierarchical cluster-
ing approach.
2.1 Corpus
Our corpus consists of blog comments posted as
responses to four blog postings selected from a
dataset crawled from Technorati between 2008-
2010
1
. We selected blog postings in the general
topic of technology and considered only postings
1
http://technorati.com/blogs/directory/
that had more than 200 comments. For the an-
notation we selected the first one hundred com-
ments on each blog together with the original post-
ing. Each blog together with its comments con-
stitutes a thread. The topics of each thread are:
Android (comparison of features of iPhone and
Android phones), iPad (the usefulness of iPads),
Twitter (the usefulness of Twitter as a microblog-
ging platform), and Layoffs (downsizing and out-
sourcing efforts of technology companies). We re-
fer to these threads as the argumentative corpus.
We plan to make the corpus available to the re-
search community.
2.2 Annotation Scheme and Expert
Annotation Task
The coarse-grained annotation scheme for argu-
mentation is based on the concept of Callout and
Target of Pragmatic Argumentation Theory. The
experts? annotation task was to identify expres-
sions of Callout and their Targets while also indi-
cating the links between them. We prepared a set
of guidelines with careful definitions of all techni-
cal terms. The following is an abbreviated excerpt
from the guidelines:
? Callout: A Callout is a subsequent action
that selects (i.e., refers back to) all or some
part of a prior action (i.e., Target) and com-
ments on it in some way. In addition to re-
ferring back to the Target, a Callout explic-
itly includes either one or both of the fol-
lowing: Stance (indication of attitude or posi-
tion relative to the Target) and Rationale (ar-
gument/justification/explanation of the Stance
taken).
? Target: A Target is a part of a prior action that
has been called out by a subsequent action.
Fig. 1 shows two examples of Callouts from
two comments referring back to the same Target.
Annotators were instructed to mark any text seg-
ment (from words to entire comments) that sat-
isfied the definitions above. A single text seg-
ment could be a Target and a Callout. To per-form
the expert annotation, we hired five graduate stu-
dents who had a strong background in humanities
and who received extensive training for the task.
The EAs performed three annotation subtasks
mentioned by Peldszus and Stede (2013a): Seg-
mentation (identify the Argumentative Dis-course
40
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 72 92.5 81.5 112.3 116
Table 1: Number of Callouts by threads and EA
Thread F1 EM F1 OM ?
Android 54.4 87.8 0.64
iPad 51.2 86.0 0.73
Layoffs 51.9 87.5 0.87
Twitter 53.8 88.5 0.82
Table 2: IAA for 5 EA: F1 and alpha values per
thread
Units (ADUs) including their boundaries), Seg-
ment classification (label the roles of the ADUs,
in this case Callout and Target) and relation iden-
tification (indicate the link between a Callout and
the most recent Target to which is a response).
The segmentation task, which Artstein and Poe-
sio (2008) refer to as the unitization problem, is
particularly challenging. Table 1 shows extensive
variation in the number of ADUs (Callout in this
case) identified by the EAs for each of the four
threads. Annotator A1 identified the fewest Call-
outs (72) while A4 and A5 identified the most
(112.3 and 116, respectively). Although these dif-
ferences could be due to the issues with training,
we interpret the consistent variation among coders
as an indication that judges can be characterized
as ?lumpers? or ?splitters?. What lumpers con-
sidered a single long unit was treated as two (or
more) shorter units by splitters. This is an example
of the problem of annotator variability discussed
in (Peldszus and Stede, 2013b). Similar behavior
was noticed for Targets.
2
2.3 Inter Annotator Agreement
Since the annotation task includes the segmen-
tation step, to measure the IAA we have to ac-
count for fuzzy boundaries. Thus, we con-sider
two IAA metrics usually used in literature for
such cases: the information retrieval (IR) in-spired
precision-recall (P/R/F1) measure (Wiebe et al.,
2005) and Krippendorff?s ? (Krippendorff, 2004).
We present here the main results; a detailed dis-
cussion of the IAA is left for a different paper. Fol-
lowing Wiebe et al. (2005), to calculate P/R/F1 for
two annotators, one annotator?s ADUs are selected
2
Due to space limitations, here and in the rest of this paper
we report only on Callouts.
as the gold standard. If more than two annotators
are employed, the IAA is the average of the pair-
wise P/R/F1. To determine if two annotators have
selected the same text span to represent an ADU,
we use the two methods of Somasundaran et al.
(2008): exact match (EM) - text spans that vary
at the start or end point by five characters or less,
and overlap match (OM) - text spans that have at
least 10% of same overlapping characters. Table 2
shows the F1 measure for EM and OM for the five
EAs on each of the four threads. As expected, the
F1 measures are much lower for EM than for OM.
For the second IAA metric, we implement
Krippendorff?s ? (Krippendorff, 2004), where the
character overlap between any two annotations
and the gap between them are utilized to mea-
sure the expected disagreement and the observed
disagreement. Table 2 shows ? values for each
thread, which means significant agreement.
While the above metrics show reasonable agree-
ment across annotators, they do not tell us what
pieces of text are easier or harder to annotate. In
the next section we report on a hierarchical cluster-
ing technique that makes it possible to assess how
difficult it is to identify individual text segments as
Callouts.
2.4 Clustering of Callout ADUs
We use a hierarchical clustering technique (Hastie
et al., 2009) to cluster ADUs that are variants of
the same Callout. Each ADU starts in its own clus-
ter. The start and end points of each ADU are uti-
lized to identify overlapping characters in pairs of
ADUs. Then, using a ?bottom up? clustering ap-
proach, two ADUs (in this case, pairs of Callouts)
that share overlapping characters are merged into
a cluster. This process continues until no more
text segments can be merged. Clusters with five
overlapping ADUs include a text segment that all
five annotators have labeled as a Callout, while
clusters with one ADU indicates that only one an-
notator classified the text segment as a Callout
(see Table 3). These numbers provide information
about what segments of text are easier or harder to
code. For instance, when a cluster contains only
two ADUs, it means that three of the five anno-
tators did not label the text segment as a Callout.
Our MTurk study of Stance/Rationale (Sec. 3.2)
could highlight one reason for the variation ? some
coders consider a segment of text as Callout when
an implicit Stance is present, while others do not.
41
# Of EAs Callout Target
5 I disagree too. some things they get right, some
things they do not.
the iPhone is a truly great design.
I disagree too . . . they do not. That happened because the iPhone is a truly
great design.
I disagree too. But when we first tried the iPhone it felt natural
immediately . . . iPhone is a truly great design.
Hi there, I disagree too . . . they do not. Same as
OSX.
?Same as above-
I disagree too. . . Same as OSX . . . no problem. ?Same as above-
2 Like the reviewer said . . . (Apple) the industry
leader.. . . Good luck with that (iPhone clones).
Many of these iPhone . . . griping about issues
that will only affect them once in a blue moon
Like the reviewer said. . . (Apple) the industry
leader.
Many of these iPhone. . .
1 Do you know why the Pre . . . various hand-
set/builds/resolution issues?
Except for games?? iPhone is clearly dominant
there.
Table 3: Examples of Callouts lusters and their corresponding Targets
Thread # of Clusters # of EA ADUs per cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 4: Number of clusters for each cluster type
Table 4 shows the number of Callout clusters in
each thread. The number of clusters with five and
four annotators shows that in each thread there are
Callouts that are plausibly easier to identify. On
the other hand, the clusters selected by only one
or two annotators are harder to identify.
3 Crowdsourcing for Fine-grained
Argumentation
To understand better the nature of the ADUs, we
conducted two studies asking Turkers to perform
finer grained analysis of Callouts and Targets. Our
first study asked five Turkers to label the relation
between a Callout and its corresponding Target
as Agree, Disagree, or Other. The Other relation
may be selected in a situation where the Callout
has no relationship with the Target (e.g., a pos-
sible digression) or is in a type of argumentative
relationship that is difficult to classify as either
Agreement or Disagreement. The second study
asked five Turkers to identify Stance and Ratio-
nale in Callouts identified by EAs. As discussed
in Section 2, by definition, a Callout contains an
explicit instance of Stance, Rationale or both. In
both of these crowdsourcing studies the Turkers
were shown only a limited portion of the threaded
discussion, i.e. the Callout-Target pairs that the
EAs had linked.
Crowdsourcing is becoming a popular mecha-
nism to collect annotations and other type of data
for natural language processing research (Wang
and Callison-Burch, 2010; Snow et al., 2008;
Chen and Dolan, 2011; Post et al., 2012). Crowd-
sourcing platforms such as Amazon Mechanical
Turk (MTurk) provide a flexible framework to sub-
mit various types of NLP tasks where novice anno-
tators (Turkers) can generate content (e.g., transla-
tions, paraphrases) or annotations (labeling) in an
inexpensive way and with limited training. MTurk
also provides researchers with the ability to con-
trol the quality of the Turkers, based on their past
performances. Section 3.1 and 3.2 describe our
two crowdsourcing studies for fine grain argumen-
tation annotation.
3.1 Crowdsourcing Study 1: Labeling the
Relation between Callout and Target
In this study, the Turkers? task was to assign a rela-
tion type between a Callout and its associated Tar-
get. The choices were Agree, Disagree, or Other.
Turkers were provided with detailed instructions,
including multiple examples of Callout and Target
pairs and their relation type. Each HIT (Human
Intelligence Task, in the language of MTurk) con-
tained one Callout-Target pair and Turkers were
paid 2 cents per HIT. To assure a level of qual-
ity control, only qualified Turkers were allowed
to perform the task (i.e., Master level with more
than 95% approval rate and at least 500 approved
HITs).
For this experiment, we randomly selected a
Callout from each cluster, along with its corre-
sponding Target. Our assumption is that all Call-
out ADUs in a given cluster have the same relation
type to their Targets (see Table 3). While this as-
sumption is logical, we plan to fully investigate it
42
in future work by running an MTurk experiment
on all the Callout ADUs and their corresponding
Targets.
We utilized Fleiss? kappa (Fleiss, 1971) to
compute IAA between the Turkers (every HIT
was completed by five Turkers). Kappa is be-
tween 0.45-0.55 for each thread showing moder-
ate agreement between the Turkers (Landis et al.,
1977). These agreement results are in line with the
agreement noticed in previous studies on agree-
ment/disagreement annotations in online interac-
tions (Bender et al., 2011; Abbott et al., 2011).
To select a gold standard for the relation type, we
used majority voting. That is, if three or more
Turkers agreed on a label, we selected that label
as the gold standard. In cases where there was
no majority, we assigned the label Other. The to-
tal number of Callouts that are in agreement and
in disagreement with Targets are 143 and 153, re-
spectively.
Table 5 shows the percentage of each
type of relation identified by Turkers
(Agree/Disagree/Other) for clusters annotated by
different number of EAs. The results suggest
that there is a correlation between text segments
that are easier or harder to annotate by EAs with
the ability of novice annotators to identify an
Agree/Disagree relation type between Callout and
Target. For example, Turkers generally discovered
Agree/Disagree relations between Callouts and
their Targets when the Callouts are part of those
clusters that are annotated by a higher number
of EAs. Turkers identified 57% as showing
a disagreement relation between Callout and
Target, and 39% as showing an agreement relation
(clusters with 5 EAs). For those clusters, only
4% of the Callouts are labeled as having an Other
relation with the Target. For clusters selected
by fewer EAs, however, the number of Callouts
having a relation with the Target labeled as Other
is much higher (39% for clusters with two EAs
and 32% for clusters with one EA). These results
show that those Callouts that are easier to discover
(i.e., identified by all five EAs) mostly have a
relation with the Target (Agree or Disagree) that
is clearly expressed and thus recognizable to the
Turkers. Table 5 also shows that in some cases
even if some EAs agreed on a piece of text to be
considered as a Callout, the novice annotators
assigned the Other relation to the Callout and Tar-
get ADUs. There are two possible explanations:
Relation label # of EA ADUs per cluster
5 4 3 2 1
Agree 39.36 43.33 42.50 35.48 48.39
Disagree 56.91 31.67 32.50 25.81 19.35
Other 3.72 25.00 25.00 38.71 32.26
Table 5: Percentage of Relation labels per EA
cluster type
either the novice annotators could not detect an
implicit agreement or disagreement and thus they
selected Other, or there are other types of relations
besides Agreement and Disagreement between
Callouts and their corresponding Targets. We
plan to extend this study to other fine grained
relation types in future work. In the next section
we discuss the results of building a supervised
classifier to predict the Agree or Disagree relation
type between Callout/Target pairs.
3.1.1 Predicting the Agree/Disagree Relation
Label
We propose a supervised learning setup to clas-
sify the relation types of Callout-Target pairs. The
classification categories are the labels collected
from the MTurk experiment. We only consider
the Agree and Disagree categories since the Other
category has a very small number of instances
(53). Based on the annotations from the Turkers,
we have 143 Agree and 153 Disagree training in-
stances.
We first conducted a simple baseline exper-
iment to check whether participants use words
or phrases to express explicit agreement or dis-
agreement such as ?I agree?, ?I disagree?. We
collected two small lists (twenty words each)
of words from Merriam-Webster dictionary that
explicitly represent agreement and disagreement
Stances. The agreement list contains the word
?agree? and its synonyms such as ?accept?, ?con-
cur?, and ?accede?. The disagreement list con-
tains the word ?disagree? and synonyms such as
?differ? and ?dissent?. We then checked whether
the text of the Callouts contains these explicit
agreement/disagreement markers. Note, that these
markers are utilized as rules and no statistical
learning is involved in this stage of experiment.
The first row of the Table 6 represents the base-
line results. Though the precision is high for
agreement category, the recall is quite low and that
results in a poor overall F1 measure. This shows
that even though markers like ?agree? or ?disagree?
43
Features Category P R F1
Baseline Agree 83.3 6.9 12.9
Disagree 50.0 5.2 9.5
Unigrams Agree 57.9 61.5 59.7
Disagree 61.8 58.2 59.9
MI-based unigram Agree 60.1 66.4 63.1
Disagree 65.2 58.8 61.9
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.63
Table 6: Classification of Agree/Disagree
are very precise, they occur in less than 15% of
all the Callouts expressing agreement or disagree-
ment.
For the next set of experiments we used a super-
vised machine learning approach for the two-way
classification (Agree/Disagree). We use Support
Vector Machines (SVM) as our machine-learning
algorithm for classification as implemented in
Weka (Hall et al., 2009) and ran 10-fold cross val-
idation. As a SVM baseline, we first use all un-
igrams in Callout and Target as features (Table
6, Row 2). We notice that the recall improves
significantly when compared with the rule-based
method. To further improve the classification ac-
curacy, we use Mutual Information (MI) to se-
lect the words in the Callouts and Targets that are
likely to be associated with the categories Agree
and Disagree, respectively. Specifically, we sort
each word based on its MI value and then se-
lect the first 180 words in each of the two cate-
gories to represent our new vocabulary set of 360
words. The feature vector includes only words
present in the MI list. Compared to the all uni-
grams baseline, the MI-based unigrams improve
the F1 by 4% (Agree) and 2% (Disagree) (Table
6). The MI approach discovers the words that
are highly associated with Agree/Disagree cate-
gories and these words turn to be useful features
for classification. In addition, we consider several
types of lexical features (LexF) inspired by previ-
ous work on agreement and disagreement (Galley
et al., 2004; Misra and Walker, 2013).
? Sentiment Lexicon (SL): Two features are de-
signed using a sentiment lexicon (Hu and Liu,
2004) where the first feature represents the num-
ber of times the Callout and the Target contain a
positive emotional word and the second feature
represents the number of the negative emotional
words.
? Initial unigrams in Callout (IU): Instead of
using all unigrams in the Callout and Target,
Features Category P R F1
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.6
LexF-SL Agree 60.6 74.1 66.7
Disagree 69.4 54.9 61.3
LexF-IU Agree 58.1 69.9 63.5
Disagree 65.3 52.9 58.5
LexF-LO Agree 57.2 74.8 64.8
Disagree 67.0 47.7 55.7
Table 7: Importance of Lexical Features
we only select the first words from the Call-
out (maximum ten). The assumption is that the
stance is generally expressed at the beginning
of a Callout. We used the same MI-based tech-
nique to filter any sparse words.
? Lexical Overlap and Length (LO): This set of
features represents the lexical overlap between
the Callout and the Target and the length of each
ADU.
Table 6 shows that using all these types of
lexical features improves the F1 score for both
categories as compared to the MI-based unigram
features. Table 7 shows the impact of remov-
ing each type of lexical features. From these re-
sults it seems that initial unigrams of Callout (IU)
and lexical overlap (LO) are useful features: re-
moving each of them lowers the results for both
Agree/Disagree categories. In future work, we
plan to explore context-based features such as the
thread structure, and semantic features such as
WordNet-based semantic similarity. We also hy-
pothesize that with additional training instances
the ML approaches will achieve better results.
3.2 Crowdsourcing Study 2: Analysis of
Stance and Rationale
In the second study aimed at identifying the ar-
gumentative nature of the Callouts identified by
the expert annotators, we focus on identifying the
Stance and Rationale segments of a Callout. Since
the presence of at least an explicit Stance or Ra-
tionale was part of the definition of a Callout, we
selected these two argumentation categories as our
finer-grained scheme for this experiment.
Given a pair of Callout and Target ADUs, five
Turkers were asked to identify the Stance and Ra-
tionale segments in the Callout, including the ex-
act boundaries of the text segments. Identifying
Stance and Rationale is a difficult task and thus,
we also asked Turkers to mark the level of diffi-
culty in the identification task. We provided the
44
Diff Number of EAs per cluster
5 4 3 2 1
VE 22.11 22.38 20.25 16.67 10.71
E 28.55 24.00 24.02 28.23 20.00
M 19.69 17.87 20.72 19.39 23.57
D 11.50 10.34 11.46 9.52 12.86
VD 7.02 5.61 4.55 4.42 6.43
TD 11.13 19.79 19.00 21.77 26.33
Table 8: Difficulty judgments by Turkers com-
pared to number of EAs who selected a cluster
Turkers with a scale of difficulty (similar to a Lik-
ert scale), where the Turkers have to choose one
of the following: very easy (VE), easy (E), moder-
ate (M), difficult (D), very difficult (VD), too diffi-
cult to code (TD). Turkers were instructed to select
the too difficult to code choice only in cases where
they felt it was impossible to detect a Stance or
Rationale in the Callout.
The Turkers were provided with detailed in-
structions including examples of Stance and Ra-
tionale annotations for multiple Callouts and only
highly qualified Turkers were allowed to perform
the task. Unlike the previous study, we also ran a
pre-screening testing phase and only Turkers that
passed the screening were allowed to complete the
tasks. Because of the difficult nature of the anno-
tation task, we paid ten cents per HIT.
For the Stance/Rationale study, we considered
all the Callouts in each cluster along with the asso-
ciated Targets. We selected all the Callouts from
each cluster because of variability in the bound-
aries of ADUs, i.e., in the segmentation process.
One benefit of this crowdsourcing experiment is
that it helps us understand better what the variabil-
ity means in terms of argumentative structure. For
example, one EA might mark a text segment as a
Callout only when it expresses a Stance, while an-
other EA might mark as Callout a larger piece of
text expressing both the Stance and Rationale (See
examples of Clusters in Table 3). We leave this
deeper analysis as future work.
Table 8 shows there is a correlation between
the number of EAs who selected a cluster and the
difficulty level Turkers assigned to identifying the
Stance and Rationale elements of a Callout. This
table shows that for more than 50% of the Callouts
that are identified by 5 EAs, the Stance and Ra-
tionale can be easily identified (refer to the ?VE?
and ?E? rows), where as in the case of Callouts
that are identified by only 1 EA, the number is
just 31%. Similarly, more than 26% of the Call-
Diff Number of EAs per cluster
5 4 3 2 1
E 81.04 70.76 60.98 63.64 25.00
M 7.65 7.02 17.07 6.06 25.00
D 5.91 5.85 7.32 9.09 12.50
TD 5.39 16.37 14.63 21.21 37.50
Table 9: Difficulty judgment (majority voting)
outs in that same category (1 EA) were labeled as
?Too difficult to code?, indicating that the Turk-
ers could not identify either a Stance or Rationale
in the Callout. These numbers are comparable to
what our first crowdsourcing study showed for the
Agree/Disagree/Other relation identification (Ta-
ble 5). Table 9 shows results where we selected
overall difficulty level by majority voting. We
combined the easy and very easy categories to the
category easy (E) and the difficult and very diffi-
cult categories to the category difficult (D) for a
simpler presentation.
Table 9 also shows that more than 80% of the
time, Turkers could easily identify Stance and/or
Rationale in Callouts identified by 5 EAs, while
they could perform the finer grained analysis eas-
ily only 25% of time for Callouts identified by a
single EA. Only 5% of Callouts identified by all
5 EAs were considered too difficult to code by the
Turkers (i.e., the novice annotators could not iden-
tify a Stance or a Rationale). In contrast, more
than 37% of Callouts annotated only by 1 EA were
considered too difficult to code by the novice an-
notators. Table 10 presents some of the examples
of Stance and Rationale pairs as selected by the
Turkers along with the difficulty labels.
4 Related Work
Primary tasks for argument analysis are to seg-
ment the text to identify ADUs, detect the roles
of each ADUs, and to establish the relationship
between the ADUs (Peldszus and Stede, 2013a).
Similarly, Cohen (1987) presented a computa-
tional model of argument analysis where the struc-
ture of each argument is restricted to the claim and
evidence relation. Teufel et al. (2009) introduce
the argumentative zoning (AZ) idea that identifies
important sections of scientific articles and later
Hachey and Grover (2005) applied similar idea of
AZ to summarize legal documents. Wyner et al.
(2012) propose a rule-based tool that can high-
light potential argumentative sections of text ac-
cording to discourse cues like ?suppose? or ?there-
fore?. They tested their system on product reviews
45
Target Callout Stance Rationale Difficulty
the iPhone is a truly
great design.
I disagree too. some
things they get right,
some things they do
not.
I. . . too Some things . . . do not Easy
the dedicated ?Back?
button
that back button is key.
navigation is actually
much easier on the an-
droid.
That back button is key Navigation
is. . . android
Moderate
It?s more about the fea-
tures and apps and An-
droid seriously lacks on
latter.
Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
? Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
Difficult
I feel like your com-
ments about Nexus One
is too positive . . .
I feel like your poor
grammar are to obvious
to be self thought...
? ? Too difficult to
code
Table 10: Examples of Callout/Target pairs with difficulty level (majority voting)
(Canon Camera) from Amazon e-commerce site.
Relatively little attention has so far been de-
voted to the issue of building argumentative cor-
pora from naturally occurring texts (Peldszus and
Stede, 2013a; Feng and Hirst, 2011). However,
(Reed et al., 2008; Reed and Rowe, 2004) have
developed the Araucaria project that maintains
an online repository of arguments (AraucariaDB),
which recently has been used as research cor-
pus for several automatic argumentation analyses
(Palau and Moens, 2009; Wyner et al., 2010; Feng
and Hirst, 2011). Our work contributes a new prin-
cipled method for building annotated corpora for
online interactions. The corpus and guidelines will
also be shared with the research community.
Another line of research that is correlated with
ours is recognition of agreement/disagreement
(Misra and Walker, 2013; Yin et al., 2012; Ab-
bott et al., 2011; Andreas et al., 2012; Galley et
al., 2004; Hillard et al., 2003) and classification of
stances (Walker et al., 2012; Somasundaran and
Wiebe, 2010) in online forums. For future work,
we can utilize textual features (contextual, depen-
dency, discourse markers), relevant multiword ex-
pressions and topic modeling (Mukherjee and Liu,
2013), and thread structure (Murakami and Ray-
mond, 2010; Agrawal et al., 2003) to improve the
Agree/Disagree classification accuracy.
Recently, Cabrio and Villata (2013) proposed
a new direction of argumentative analysis where
the authors show how arguments are associated
with Recognizing Textual Entailment (RTE) re-
search. They utilized RTE approach to detect the
relation of support/attack among arguments (en-
tailment expresses a ?support? and contradiction
expresses an ?attack?) on a dataset of arguments
collected from online debates (e.g., Debatepedia).
5 Conclusion and Future Work
To make progress in argument mining for online
interactions, we need to develop a principled and
scalable way to determine which portions of texts
are argumentative and what is the nature of argu-
mentation. We have proposed a two-tiered ap-
proach to achieve this goal. As a first step we
adopted a coarse-grained annotation scheme based
on Pragmatic Argumentation Theory and asked
expert annotators to label entire threads using this
scheme. Using a clustering technique we iden-
tified which pieces of text were easier or harder
for the Expert Annotators to annotate. Then we
showed that crowdsourcing is a feasible approach
to obtain annotations based on a finer grained ar-
gumentation scheme, especially on text segments
that were easier for the Expert Annotators to la-
bel as being argumentative. While more qualita-
tive analysis of these results is still needed, these
results are an example of the potential benefits of
our multi-step coding approach.
Avenues for future research include but are not
limited to: 1) analyzing the differences between
the stance and rationale annotations among the
novice annotators; 2) improving the classification
accuracies of the Agree/Disagree classifier using
more training data; 3) using syntax and seman-
tics inspired textual features and thread structure;
and 4) developing computational models to detect
Stance and Rationale.
46
Acknowledgements
Part of this paper is based on work supported by
the DARPA-DEFT program for the first two au-
thors. The views expressed are those of the au-
thors and do not reflect the official policy or po-
sition of the Department of Defense or the U.S.
Government.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E
Fox Tree, Robeson Bowmani, and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing disagreement in informal political argument. In
Proceedings of the Workshop on Languages in So-
cial Media, pages 2?11. Association for Computa-
tional Linguistics.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th international conference on
World Wide Web, pages 529?535. ACM.
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagree-
ment in threaded discussion. In LREC, pages 818?
822.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Emily M Bender, Jonathan T Morgan, Meghan Oxley,
Mark Zachry, Brian Hutchinson, Alex Marin, Bin
Zhang, and Mari Ostendorf. 2011. Annotating so-
cial acts: Authority claims and alignment moves in
wikipedia talk pages. In Proceedings of the Work-
shop on Languages in Social Media, pages 48?57.
Association for Computational Linguistics.
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 5(04):363?381.
Elena Cabrio and Serena Villata. 2013. A natural
language bipolar argumentation approach to support
users in online debate interactions. Argument &
Computation, 4(3):209?230.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190?200.
Association for Computational Linguistics.
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational linguistics,
13(1-2):11?24.
Vanessa Wei Feng and Graeme Hirst. 2011. Clas-
sifying arguments by scheme. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 987?996. Association
for Computational Linguistics.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 669. Association for Computational Lin-
guistics.
Ben Hachey and Claire Grover. 2005. Automatic le-
gal text summarisation: experiments with summary
structuring. In Proceedings of the 10th international
conference on Artificial intelligence and law, pages
75?84. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Dustin Hillard, Mari Ostendorf, and Elizabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: companion volume of the Proceedings
of HLT-NAACL 2003?short papers-Volume 2, pages
34?36. Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Ian Hutchby. 2013. Confrontation talk: Arguments,
asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 2004. Measuring the reliability
of qualitative text analysis data. Quality & quantity,
38:787?800.
J Richard Landis, Gary G Koch, et al. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, 33(1):159?174.
Douglas W Maynard. 1985. How children start argu-
ments. Language in society, 14(01):1?29.
47
Renee A Meyers and Dale Brashers. 2010. Extend-
ing the conversational argument coding scheme: Ar-
gument categories, units, and coding procedures.
Communication Methods and Measures, 4(1-2):27?
45.
Amita Misra and Marilyn A Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Proceedings of
the SIGDIAL 2013 Conference, pages 41?50. Asso-
ciation for Computational Linguistics.
Arjun Mukherjee and Bing Liu. 2013. Discovering
user interactions in ideological discussions. In Pro-
ceedings of the 51st Annual Meeting on Association
for Computational Linguistics, pages 671?681. Cite-
seer.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th international conference on ar-
tificial intelligence and law, pages 98?107. ACM.
Andreas Peldszus and Manfred Stede. 2013a. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking
the annotators: An agreement study on argumenta-
tion structure. In Proceedings of the 7th linguistic
annotation workshop and interoperability with dis-
course, pages 196?204.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 401?409. Association for Computational
Linguistics.
Chris Reed and Glenn Rowe. 2004. Araucaria: Soft-
ware for argument analysis, diagramming and repre-
sentation. International Journal on Artificial Intelli-
gence Tools, 13(04):961?979.
Chris Reed, Raquel Mochales Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
conference on language resources and evaluation-
LREC 2008, pages 91?100.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124. Association
for Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 129?
137. Association for Computational Linguistics.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 3-Volume 3, pages
1493?1502. Association for Computational Linguis-
tics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jack-
son, and Scott Jacobs. 1993. Reconstructing argu-
mentative discourse. University of Alabama Press.
Marilyn A Walker, Pranav Anand, Rob Abbott, Jean
E Fox Tree, Craig Martell, and Joseph King. 2012.
That is your evidence?: Classifying stance in on-
line political debate. Decision Support Systems,
53(4):719?729.
Rui Wang and Chris Callison-Burch. 2010. Cheap
facts and counter-facts. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 163?167. Association for Computa-
tional Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165?210.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Semantic
processing of legal texts, pages 60?79. Springer.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor JM Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
COMMA, pages 43?50.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
pages 61?69. Association for Computational Lin-
guistics.
48
LAW VIII - The 8th Linguistic Annotation Workshop, pages 120?128,
Dublin, Ireland, August 23-24 2014.
Annotating Multiparty Discourse: Challenges for Agreement Metrics
Nina Wacholder* Smaranda Muresan? Debanjan Ghosh* Mark Aakhus*
*School of Communication and Information, Rutgers University
?Center for Computational Learning Systems, Columbia University
ninwac|debanjan.ghosh|aakhus@rutgers.edu, smara@ccls.columbia.edu
Abstract
To computationally model discourse phenomena such as argumentation we need corpora with
reliable annotation of the phenomena under study. Annotating complex discourse phenomena
poses two challenges: fuzziness of unit boundaries and the need for multiple annotators. We show
that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorff?s ?
provide inconsistent results for the same text. In addition, IAA metrics do not tell us what parts of
a text are easier or harder for human judges to annotate and so do not provide sufficiently specific
information for evaluating systems that automatically identify discourse units. We propose a
hierarchical clustering approach that aggregates overlapping text segments of text identified by
multiple annotators; the more annotators who identify a text segment, the easier we assume that
the text segment is to annotate. The clusters make it possible to quantify the extent of agreement
judges show about text segments; this information can be used to assess the output of systems
that automatically identify discourse units.
1 Introduction
Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units,
including their boundaries), segment classification (labeling the role of discourse units) and relation iden-
tification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty
of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agree-
ment, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the
boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in
annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although
annotators tended to identify essentially the same units, the boundaries differed slightly. The need for
annotators to identify the boundaries of text segments makes measurement of IAA more difficult because
standard coefficients such as ? assume that the units to be coded have been identified before the coding
begins (Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is
associated with larger numbers of annotators. Because of the many ways that ideas are expressed in hu-
man language, using multiple annotators to study discourse phenomena is important. Such an approach
capitalizes on the aggregated intuitions of multiple coders to overcome the potential biases of any one
coder and helps identify limitations in the coding scheme, thus adding to the reliability and validity of
the annotation study. The more annotators, however, the harder it is to achieve an IAA of .80 (Bayerl and
Paul, 2011). What to annotate also depends, among other characteristics, on the phenomenon of interest,
the text being annotated, the quality of the annotation scheme and the effectiveness of training. But even
if these are excellent, there is natural variability in human judgment for a task that involves subtle dis-
tinctions about which competent coders disagree. An accurate computational model should reflect this
variability (Aakhus et al., 2013).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
120
# Type Statement
Target I?m going to quit the iphone and switch to an android phone because I
can no long (sic) put up with the AT&T service contract
Callout I am going to switch too
Callout There is no point quitting the iphone because of the service package,
just jail break it and use the provider you want
Table 1: Examples of Callouts and Targets
Figure 1: Cluster where 3 judges identify a core
We propose an approach for overcoming these challenges based on evidence from an annotation study
of arguments in online interactions. Our scheme for argumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby, 2013; Maynard, 1985). PAT states that argument can
arise at any point when two or more actors engage in calling out and making problematic some aspect
of another actor?s prior contribution for what it (could have) said or meant (Van Eemeren et al., 1993).
The argumentative relationships among contributions to a discussion are indicated through links between
what is targeted and how it is called out. Table 1 shows two Callouts that refer back to the same Target.
Callouts and Targets are Argumentative Discourse Units (ADUs) in the sense of Peldszus and Stede
(2013a), ?minimal units of analysis . . . inspired . . . by a . . . relation-based discourse theory? (p.20). In our
case the theory is PAT. Callouts are related to Targets by a relationship that we may refer to as Response,
though we do not discuss the Response relationship in this paper.
The hierarchical clustering technique that we propose systematically identifies clusters of ADUs; each
cluster contains a core of overlapping text that two or more judges have identified. Figure 1 shows
a schematic example of a cluster with a core identified by three judges. The variation in boundaries
represents the individual judges? differing intuitions; these differences reflect natural variation of human
judgments about discourse units. We interpret differences in the number (or percentage) of judges that
identify a core as evidence of how hard or easy a discourse unit is to recognize.
The contributions of this paper are two-fold. First, we show that methods for assessing IAA, such as
the information retrieval inspired (P/R/F1) approach (Wiebe et al., 2005) and Krippendorff?s ? (Krip-
pendorff, 1995; Krippendorff, 2004b), which was developed for content analysis in the social sciences,
provide inconsistent results when applied to segmentations involving fuzzy boundaries and multiple
coders.
In addition, these metrics do not tell us which parts of a text are easier or harder to annotate, or help
choose a reliable gold standard. Our second contribution is a new method for assessing IAA using hier-
archical clustering to find parts of text that are easier or harder to annotate. These clusters could serve as
the basis for assessing the performance of systems that automatically identify ADUs - the system would
be rewarded for identifying ADUs that are easier for people to recognize and penalized for identifying
ADUs that are relatively hard for people to recognize.
2 Annotation Study of Argumentative Discourse Units: Callouts and Targets
In this section, we describe the annotation study we conducted to determine whether trained human
judges can reliably identify Callouts and Targets. The main annotation task was to find Callouts and the
Targets to which they are linked and unitize them, i.e., assign boundaries to each ADU. As mentioned
above, these are the steps for argument mining delineated in Peldszus and Stede (2013a). The design of
121
the study was consistent with the conditions for generating reliable annotations set forth in Krippendorff
(2004a, p. 217).
We selected five blog postings from a corpus crawled from Technorati (technorati.com) between 2008-
2010; the comments contain many disputes. We used the first 100 comments on each blog as our corpus,
along with the original posting. We refer to each blog and the associated comments as a thread.
The complexity of the phenomenon required the perspective of multiple independent annotators, de-
spite the known difficulty in achieving reliable IAA with more than two annotators. For our initial
study, in which our goal was to obtain naturally occurring examples of Callouts and Targets and assess
the challenges of reliably identifying them, we engaged five graduate students with a strong humanities
background. The coding was performed with the open-source Knowtator software (Ogren, 2006). All
five judges annotated all 100 comments in all five threads. While the annotation process was under way,
annotators were instructed not to communicate with each other about the study.
The annotators? task was to find each instance of a Callout, determine the boundaries, link the Callout
to the most recent Target and determine the boundaries of the Target. We prepared and tested a set of
guidelines with definitions and examples of key concepts. The following is an adapted excerpt from the
guidelines:
? Callout: A Callout is (a part of) a subsequent action that selects (a part of) a prior action and marks
and comments on it in some way. In Table 1, Statements 2 and 3 are both Callouts, i.e., they perform
the action of calling out on Statement 1. Statement 2 calls out the first part of Statement 1 dealing
with switching phones. Statement 3 calls out all of Statement 1 ? both what?s proposed and the
rationale for the disagreement.
? Target: A Target is a part of a prior action that has been called out by a subsequent action. Statement
1 is a Target of Statements 2 and 3. But Statements 2 and 3 link to different parts of Statement 1, as
described above.
? Response: A link between Callout and Target that occurs when a subsequent action refers back to
(is a response to) a prior action.
Annotators were instructed to mark any text segment (from words to entire comments) that satisfied
the definitions above. A single text segment could be a Target and a Callout. To save effort on a difficult
task, judges were asked only to annotate the most recent plausible Target. We plan to study chains of
responses in future work.
Prior to the formal study, each annotator spent approximately eight hours in training, spread over about
two weeks, under the supervision of a PhD student who had helped to develop the guidelines. Training
materials included the guidelines and postings and comments from Technorati that were not used in the
formal study. Judges were reminded that our research goal was to find naturally occurring examples of
Callouts and Targets and that the research team did not know in advance what were the right answers
? the subjects? job was to identify Callouts and Targets that satisfied the definitions in the guidelines.
In response to the judges? questions, the guidelines were iteratively updated: definitions were reviewed,
additional examples were added, and a list of FAQs was developed
1
.
Table 2 shows the wide range of results among the annotators for Callouts that illustrates a problem to
be addressed when assessing reliability for multiple annotators.
Averaged over all five threads, A1 identified the fewest Callouts (66.8) while A4 and A5 identified
the most (107 and 109, respectively). Furthermore, the number of annotations assigned by A4 and A5
to each corpus is consistently higher than those of the other annotators, while the number of annotations
A1 assigned to each thread is consistently lower than that of all of the other annotators. Although these
differences could be due to issues with training, we interpret the consistent variation among coders as
potential evidence of two distinct types of behavior: some judges are ?lumpers? who consider a text
string as a single unit; others are ?splitters? who treat the same text string as two (or more) distinct units.
The high degree of variability among coders is consistent with the observations of Peldszus and Stede
1
The corpus, annotations and guidelines are available at <http://wp.comminfo.rutgers.edu/salts/projects/opposition/>.
122
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
Ban 46 73 66 86 83
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 66.8 88.6 78.4 107 109.4
Table 2: Callouts per annotator per thread
(2013b). These differences could be due to issues with training and individual differences among coders,
but even so, the variability highlights an important challenge for calculating IAA with multiple coders
and fuzzy unit boundaries.
3 Some Problems of Unitization Reliability with Existing IAA Metrics
In this section we discuss two state-of-the-art metrics frequently used for measuring IAA for discourse
annotation and we show that these methods offer limited informativeness when text boundaries are fuzzy
and there are multiple judges. These methods are the information retrieval inspired precision-recall
(P/R/F1) metrics used in Wiebe and her collaborators? important work on sentiment analysis (Wiebe
et al., 2005; Somasundaran et al., 2008) and Krippendorff?s ?, a variant of the ? family of IAA coef-
ficients specifically designed to handle fuzzy boundaries and multiple annotators (Krippendorff, 1995;
Krippendorff, 2004b). Krippendorff?s ? determines IAA based on observed disagreement relative to
expected agreement and calculates differences in annotators? judgments. Although it is possible to use
number of words or even clauses to measure IAA, we use length in characters both for consistency with
Wiebe?s approach and because Krippendorff (2004b, pp.790-791) recommends using ?. . . the smallest
distinguishable length, for example the characters in text. . .? to measure IAA. We next show the results
of using P/R/F and Krippendorff?s ? to measure IAA for our annotation study and provide examples of
some challenges that need to be addressed.
3.1 Precision, Recall and F measures
Implementing P/R/F1 requires a gold standard annotation against which the other annotations can be
compared. P/R/F1 is calculated here, following (Wiebe et al., 2005), as follows: the units selected by
one annotator are taken as the gold standard and the remaining annotators are calculated against the
selected gold standard. To determine whether annotators selected the same text span, two different types
of matches were considered, as in Somasundaran et al. (2008): exact matches and overlap matches
(variation of their lenient match):
? Exact Matches (EM): Text spans that vary at the start or end point by five characters or less are
considered an exact match. This minor relaxation of exact matching (Somasundaran et al., 2008)
compensates for minor inconsistencies such as whether a judge included a sentence ending punctu-
ation mark in the unit.
? Overlap Matches (OM): Any overlap between text spans of more than 10% of the total number of
characters is considered a match. OM is weaker than EM but still an indicator of shared judgments
by annotators.
Tables 3 and 5 and Tables 4 and 6 show the P/R/F1-based IAA using EM and OM respectively. The
results are averaged across all five threads. Besides average P/R/F1 we also show Max F1 and Min F1,
which represent the maximum and minimum F1 relative to a particular annotator used as gold standard.
These tables show that the results vary greatly. Among the reasons for the variation are the following:
? Results are sensitive to which annotator is selected as the gold standard. In Table 4, pairing A4 with
the judge who agrees maximally produces an F measure of 90.2 while pairing A4 with the annotator
who agrees minimally produces an F measure of 73.3. In Tables 3 and 4, if we select A4 as the gold
standard we get the most variation; selecting A3 produces the least.
123
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 40.7 57.7 47.8 60 36.7
A2 51.7 51.2 51.4 58.3 43
A3 54.2 57.8 55.9 61.4 47.9
A4 59.7 49.1 53.9 61.4 47.3
A5 55 45.6 49.9 58.3 36.7
Table 3: Callouts: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 67.4 95.7 79.1 86.8 73.3
A2 85 83.7 84.3 88.7 76.1
A3 82.7 88 85.2 88.7 80.9
A4 92.7 76.8 84 90.2 73.3
A5 91.4 75.1 82.4 89.6 74
Table 4: Callouts: OM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 24.1 34.6 28.4 34.5 18.7
A2 26.9 24.7 25.7 37.6 18.7
A3 35.2 35.1 35.1 48.4 19.4
A4 37.3 34.5 35.8 50.4 22.1
A5 36.9 31.4 33.9 50.4 19.9
Table 5: Targets: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 60.1 86.5 70.9 76.1 64.2
A2 74.5 69.4 71.9 79.6 62.9
A3 75.9 74.5 75.1 80.1 67.7
A4 78.1 71.5 74.6 84.2 64
A5 83.8 70.3 76.4 83.8 67.2
Table 6: Targets: OM P/R/F1 over 5 threads
? The type of matching matters. As expected, OM, which is less strict than EM, produces substantially
higher F1 scores both for Callouts (Tables 3 and 4 ) and Targets (Tables 5 and 6).
? Different phenomena are associated with different levels of difficulty of annotation. The F1 scores
for Targets are considerably lower than the F1 scores for Callouts. We suspect that Callouts are
easier to recognize since they are often introduced with standard expressions that signal agreement
or disagreement such as ?yes?, ?no?, ?I agree?, or ?I disagree?. Targets, on the other hand, generally
lack such distinguishing lexical features.
We also observe differences across threads. For example, the Ban thread seems harder to annotate
than the other threads. Figure 2 and 3 show IAA results for OM for Callout and Target annotations for
annotators A1 and A5 respectively, across the five threads. We chose A1 and A5 because in general
A1 annotated the fewest Callouts and A5 annotated the most Callouts in the corpus. These figures show
different annotator behavior. For instance, for both Callout and Target annotations, A1 has higher average
R than P, while A5 has higher P but lower R. Figures 2 and 3 hint that the Ban thread is harder to annotate
than the others.
The examples in this section show two downsides to the P/R/F1 metric. First, the scores do not reflect
the extent to which two annotations match. This is crucial information for fuzzy boundary matching, be-
cause the agreement between two annotations can be over only a few characters or over the full length of
the selected text. Second, the variation across multiple judges demonstrates the disadvantage of arbitrary
selection of a gold standard set of annotations against which to measure IAA.
3.2 Krippendorff?s ?
Krippendorff?s ? calculates IAA based on the observed and expected disagreement between annotators.
We use the version of Kripendorff?s ? discussed in Krippendorff (2004b) which takes into account mul-
tiple annotators and fuzzy boundaries. Detailed proof and an explanation of the calculation can be found
in (Krippendorff, 2004b; Krippendorff, 1995).
Thread F1 Krippendorff?s ?
Android 87.8 0.64
Ban 85.3 0.75
iPad 86.0 0.73
Layoffs 87.5 0.87
Twitter 88.5 0.82
Table 7: F1 and ? for all 5 threads
Thread Rank by IAA (Descending)
F1 K?s ?
Twitter Layoffs
Android Twitter
Layoffs Ban
iPad iPad
Ban Android
Table 8: Threads ranked by IAA in descending order
Comparison of ? and P/R/F1 metrics shows that they generate inconsistent results that are difficult to
interpret. For example, in Table 7, the F1 measure for Callouts indicates lower agreement on the Ban
thread in comparison to Android while ? suggests higher agreement on the Ban subcorpus relative to the
124
Figure 2: IAA metrics per thread when A1 is gold standard (Left: Callout. Right: Target.)
Figure 3: IAA metrics per thread when A5 is gold standard ( Left: Callout. Right: Target.)
Android subcorpus. The inconsistencies are also apparent in Table 8, which ranks threads in descending
order of IAA. For example, the Android corpus receives the highest IAA using F1 but the lowest using
?.
We do not show the results for Krippendorff?s ? for Targets for the following reason. Relevant units
from a continuous text string are assigned to categories by individual annotators. But identification of
Targets is dependent on (temporally secondary to) identification of Callouts. In multiple instances we
observe that an annotator links multiple Callouts to two or more overlapping Targets. Depending on
the Callout, the same unit (i.e., text segment) can represent an annotation (a Target) or a gap between
two Targets. Computation of ? is based on the overlapping characters of the annotations and the gaps
between the annotations. Naturally, if a single text string is assigned different labels (i.e. annotation
or a gap between annotations) in different annotations, ? does not produce meaningful results. The
inapplicability of Krippendorff?s ? to Targets is a significant limitation for its use in discourse annotation
(To save space we only show results for Callouts in subsequent tables.)
The examples in Section 3 show a fundamental limitation of both P/R/F1 and Krippendorff?s ?: They
do not pinpoint the location in a document where the extent of variation can be observed. This limits the
usefulness of these measures for studying the discourse phenomenon of interest and for analyzing the
impact of factors such as text difficulty, corpus and judges on IAA. The impact of these factors on IAA
also makes it hard to pick gold standard examples on a principled basis.
4 Hierarchical Clustering of Discourse Units
In this section we introduce a clustering approach that aggregates overlapping annotations, thereby mak-
ing it possible to quantify agreement among annotators within a cluster. Then we show examples of
clusters from our annotation study in which the extent of annotator support for a core reflects how hard
or easy an ADU is for human judges to identify. The hierarchical clustering technique (Hastie et al.,
2009) assumes that overlapping annotations by two or more judges constitutes evidence of the approxi-
mate location of an instance of the phenomenon of interest. In our case, this is the annotation of ADUs
that contain overlapping text. Each ADU starts in its own cluster. The start and end points of each ADU
are utilized to identify overlapping characters in pairs of ADUs. Then, using a bottom-up clustering
125
# Annots Text selected
A1, A2, A3,
A4, A5
I remember Apple telling people give the UI and the keyboard a month
and you?ll get used to it. Plus all the commercials showing the interface.
So, no, you didn?t just pick up the iPhone and know how to use it. It
was pounded into to you.
Table 9: A cluster in which all five judges agreement on the boundaries of the ADU
# Annots Text selected
A1 I?m going to agree that my experience required a bit of getting used to
. . .
A2, A3, A4 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing
A5 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing. Great
browser on the iPhone but . . . Opera Mini can work wonders
Table 10: A cluster in which all 5 annotators agree on the core but disagree on the closing boundary of
the ADU
technique, pairs of clusters (e.g. pairs of Callout ADUs) with overlapping text strings are merged as they
move up in the hierarchy. An ADU that does not overlap with ADUs identified by any other judge will
remain in its own cluster.
Aggregating overlapping annotations makes it possible to quantify agreement among the annotators
within a cluster. Table 9 shows an example of a cluster that contains five annotations; all five annotators
assign identical unit boundaries, which means that there is a single core, with no variation in the extent of
the ADU. Table 9 thus shows an optimal case ? there is complete agreement among the five annotators.
We take this as strong evidence that the text string in Table 9 is an instance of a Callout that is relatively
easy to identify.
But of course, natural language does not make optimal annotation easy (even if coders were perfect).
Table 10 shows a cluster in which all five annotators agree on the core (shown in italics) but do not
agree about the boundaries of the ADU. A1 picked the shortest text segment. A2, A3 and A4 picked the
same text segment as A1 but they also included the rest of the sentence, up to the word ?browsing?. In
A5?s judgment, the ADU is still longer - it also includes the sentence ?Great browser . . . work wonders.?
Although not as clear-cut as the examples in Table 9, the fact that in Table 10 all annotators chose
overlapping text is evidence that the core has special status in the context of in an annotation task where it
is known that even expert annotators disagree about borders. Examples like those in Table 10 can be used
to study the reasons for variation in the judges? assignment of boundaries. Besides ease of recognition
of an ADU and differing human intuitions, the instructions in the guidelines or characteristics of the
Callouts may be also having an effect.
Table 11 shows a more complex annotation pattern in a cluster. Annotators A1 and A2 agree on the
boundaries of the ADU, but their annotation does not overlap with A4 at all. A3?s boundaries subsume
all other annotations. But because A4?s boundaries do not overlap with those of A1 and A2, technically
this cluster has no core (a text segment included in all ADUs in a cluster). 5% or less of the clusters
have this problem. To handle the absence of a core in this type of cluster, we split the clusters that fit this
pattern into multiple ?overlapping? clusters, that is, we put A1, A2, and A3 into one cluster and we put
A3 and A4 into another cluster. Using this splitting technique, we get two cores, each selected by two
judges: i) ?actually the only . . . app?s developer? from the cluster containing A1, A2, and A3 (shown in
italics) and ii) ?I think it hilarious . . . another device? from the cluster containing A3 and A4 (shown in
bold). The disagreement of the judges in identifying the Callout suggests that judges have quite different
judgments about boundaries of the Callouts.
Table 12 and 13 respectively show the number of clusters with overlapping annotations for Callouts
for each thread before and after splitting. The splitting process has only a small impact on results. The
number of clusters with five and four annotators shows that in each corpus there are Callouts that are
evidently easier to identify. On the other hand, clusters selected by only two or three judges are harder to
126
# Annots Text selected
A1, A2 Actually the only one responsible for the YouTube and Twitter multitask-
ing is the app?s developer
A3 Actually the . . . app?s developer. The Facebook app allows you to watch
videos posted by . . . I think it hilarious that people complain about
features that arent even available on another device
A4 I think it hilarious that people complain about features that arent
even available on another device
Table 11: A cluster with 2 cores, each selected by 2 judges
identify. The clusters containing a text string picked by only one annotator are hardest to identify. This
may be an indication that this text string is not a good example of a Callout, though it also could be an
indication that the judge is particularly good at recognizing subtly expressed Callouts. The clustering
technique thus scaffolds deeper examination of annotation behavior and annotation/concept refinement.
Table 13 also shows that overall, the number of clusters with five or four annotators is well over 50% for
each thread except Ban, even when we exclude the clusters with an ADU identified by only one judge.
This is another hint that the IAA in this thread should be much lower than in the other threads. (See also
Figures 2 and 3).
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ban 89 25 18 12 20 14
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 12: Callouts: Clusters before splitting process
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 93 51 15 14 8 5
Ban 91 25 19 12 21 14
iPad 89 41 16 9 13 10
Layoffs 89 40 17 14 8 10
Twitter 87 43 15 20 4 5
Table 13: Callouts: Clusters after splitting process
The clusters with cores supported by four or five annotators show strong annotator agreement and are
very strong candidates for a gold standard, regardless of the IAA for the entire thread. Clusters with
an ADU selected by only one annotator are presumably harder to annotate and are more likely than
other clusters not to be actual instances of the ADU. This information can be used to assess the output
of systems that automatically identify discourse units. For example a system could be penalized more
for missing to identifying ADUs on which all five annotators agree on the boundaries, as in Table 9;
the penalty would be decreased for not identifying ADUs on which fewer annotators agree. Qualitative
analysis may help discover the reason for the variation in strength of clusters, thereby supporting our
ability to interpret IAA and to create accurate computational models of human judgments about dis-
course units. As a related research, PAT and the clustering technique discussed in this paper allow the
development of a finer-grained annotation scheme to analyze the type of links between Target-Callout
(e.g., Agree/Disagree/Other), and the nature of Callouts (e.g., Stance/Rationale) (Ghosh et al., 2014).
5 Conclusion and Future Work
Reliability of annotation studies is important both as part of the demonstration of the validity of the
phenomena being studied and also to support accurate computational modeling of discourse phenomena.
The nature of ADUs, with their fuzzy boundaries, makes it hard to achieve IAA of .80 or higher. Fur-
thermore, the use of a single figure for IAA is a little like relying on an average to convey the range of
variation of a set of numbers. The contributions of this paper are i) to provide concrete examples of the
difficulties of using state of the art metrics like P/R/F1 and Krippendorff?s ? to assess IAA for ADUs
and ii) to open up a new approach to studying IAA that can help us understand how factors like coder
variability and text difficulty affect IAA. Our approach supports reliable identification of discourse units
independent of the overall IAA of the document.
127
References
Mark Aakhus, Smaranda Muresan, and Nina Wacholder. 2013. Integrating natural language processing and
pragmatic argumentation theories for argumentation support. pages 1?12.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011. What determines inter-coder agreement in manual annota-
tions? a meta-analytic investigation. Computational Linguistics, 37(4):699?725.
Debanjan Ghosh, Smaranda Muresan, Nina Wacholder, Mark Aakhus, and Matthew Mitsui. 2014. Analyzing
argumentative discourse units in online interactions. In Proceedings of the First Workshop on Argumentation
Mining, pages 39?48, Baltimore, Maryland, June. Association for Computational Linguistics.
Barbara J Grosz and Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational
linguistics, 12(3):175?204.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Ian Hutchby. 2013. Confrontation talk: Arguments, asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 1995. On the reliability of unitizing continuous data. Sociological Methodology, pages
47?76.
Klaus Krippendorff. 2004a. Content analysis: An introduction to its methodology. Sage.
Klaus Krippendorff. 2004b. Measuring the reliability of qualitative text analysis data. Quality & quantity, 38:787?
800.
Douglas W Maynard. 1985. How children start arguments. Language in society, 14(01):1?29.
Philip V Ogren. 2006. Knowtator: a prot?eg?e plug-in for annotated corpus construction. In Proceedings of the
2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: companion volume: demonstrations, pages 273?275. Association for Computational
Linguistics.
Andreas Peldszus and Manfred Stede. 2013a. From argument diagrams to argumentation mining in texts: A
survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking the annotators: An agreement study on argumentation
structure. In Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages
196?204.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137.
Association for Computational Linguistics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jackson, and Scott Jacobs. 1993. Reconstructing argumentative
discourse. University of Alabama Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language resources and evaluation, 39(2-3):165?210.
128
