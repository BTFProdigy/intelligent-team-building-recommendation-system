Evaluation and Improvement
of Cross-Lingual Question Answering Strategies
Anne-Laure Ligozat and Brigitte Grau and Isabelle Robba and Anne Vilnat
LIMSI-CNRS
91403 Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
This article presents a bilingual question
answering system, which is able to process
questions and documents both in French
and in English. Two cross-lingual strate-
gies are described and evaluated. First, we
study the contribution of biterms trans-
lation, and the influence of the comple-
tion of the translation dictionaries. Then,
we propose a strategy for transferring the
question analysis from one language to the
other, and we study its influence on the
performance of our system.
1 Introduction
When a question is asked in a certain language
on the Web, it can be interesting to look for the
answer to the question in documents written in
other languages in order to increase the number of
documents returned. The CLEF evaluation cam-
paign for cross-language question answering sys-
tems addresses this issue by encouraging the deve-
lopment of such systems.
The objective of question answering systems
is to return precise answers to natural-language
questions, instead of the list of documents usually
returned by a search engine. The opening to mul-
tilingualism of question answering systems raises
issues both for the Information Retrieval and the
Information Extraction points of view.
This article presents a cross-language question
answering system able to treat questions and docu-
ments either in French or in English. Two different
strategies for shifting language are evaluated, and
several possibilities of evolution are presented.
2 Presentation of our question answering
system
Our bilingual question answering system has
participated in the CLEF 2005 evaluation cam-
paign 1. The CLEF QA task aims at evaluating dif-
ferent question answering systems on a given set
of questions, and a given corpus of documents, the
questions and the documents being either in the
same language (except English) or in two diffe-
rents languages. Last year, our system participated
in the French to English task, for which the ques-
tions are in French and the documents to search in
English.
This system is composed of several modules
that are presented Figure 1. The first module ana-
lyses the questions, and tries to detect a few of
their characteristics, that will enable us to find the
answers in the documents. Then the collection is
processed thanks to MG search engine 2. The do-
cuments returned are reindexed according to the
presence of the question terms, and more preci-
sely to the number and type of these terms ; next,
a module recognizes the named entities, and the
sentences from the documents are weighted accor-
ding to the information on the question. Finally,
different processes are applied depending on the
expected answer type, in order to extract answers
from the sentences.
3 Cross-language strategies for question
answering systems
Two main approaches are possible to deal with
multilingualism in question answering systems :
1Multilingual Question Answering task at the Cross Lan-
guage Evaluation Forum, http ://clef-qa.itc.it/
2MG for Managing Gigabytes
http ://www.cs.mu.oz.au/mg/
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
23
English
terms
Fusion
English
answers
English
questions
   
Collection
French
questions
   Selection
   Named entity tagging
Answer extraction
   Reindexing and ranking
   Sentence weighting
Document processing
   Answer extraction
English
   Focus 
   Answer type
   Semantically linked words
   Main verb
   Terms
   Syntactic relations
Question analysis
translation
answers
2 lists of ranked
                  (a)
                 (b)
Search
engine
FIG. 1 ? Architecture of our cross-language question answering system
question translation and term-by-term translation.
These approaches have been implemented and
evaluated by many systems in the CLEF evalua-
tions, which gives a wide state-of-the-art of this
domain and of the possible cross-language strate-
gies.
The first approach consists in translating the
whole question into the target language, and then
processing the question analysis in this target lan-
guage. This approach is the most widely used, and
has for example been chosen by the following sys-
tems : (Perret, 2004), (Jijkoun et al, 2004), (Neu-
mann and Sacaleanu, 2005), (de Pablo-Sa?nchez et
al., 2005), (Tanev et al, 2005). Among these sys-
tems, several have measured the performance loss
between their monolingual and their bilingual sys-
tems. Thus, the English-French version of (Perret,
2004) has a 11 % performance loss (in terms of ab-
solute loss), dropping from 24.5% to 13.5% of cor-
rect answers. The English-Dutch version of (Jij-
koun et al, 2004)?s system has an approximative
10% performance loss of correct answers : the per-
centage of correct answers drops from 45.5% to
35%. As for (de Pablo-Sa?nchez et al, 2005), they
lose 6% of correct answers between their Spanish
monolingual system and their English-Spanish bi-
lingual system. (Hartrumpf, 2005) also conducted
an experiment by translating the questions from
English to German, and reports a drop from about
50% of performance.
For their cross-language system, (Neumann and
Sacaleanu, 2004) chose to use several machine
translation tools, and to gather the different trans-
lations into a ?bag of words? that is used to ex-
pand queries. Synonyms are also added to the
?bag of words? and EuroWordNet 3 is used to
3Multilingual database with wordnets for several Euro-
disambiguate. They lose quite few correct ans-
wers between their German monolingual system
and their German-English bilingual system, with
which they obtain respectively 25 and 23.5% of
correct answers.
Translating the question raises two main pro-
blems : syntactically incorrect questions may be
produced, and the resolution of translation am-
biguities may be wrong. Moreover, the unknown
words such as some proper names are not or in-
correctly translated. We will describe later several
possibilities to deal with these problems, as well
as our own solution.
Other systems such as (Sutcliffe et al, 2005) or
(Tanev et al, 2004) use a term-by-term translation.
In this approach, the question is analyzed in the
source language and then the information retur-
ned by the question analysis is translated into the
target language. (Tanev et al, 2004), who partici-
pated in the Bulgarian-English and Italian-English
tasks in 2004, translate the question keywords by
using bilingual dictionaries and MultiWordNet 4.
In order to limit the noise stemming from the dif-
ferent translations and to have a better cohesion,
they validate the translations in two large cor-
pora, AQUAINT and TIPSTER. This system got a
score of 22.5% of correct answers in the bilingual
task, and 28% in the monolingual task in 2004.
(Sutcliffe et al, 2005) combine two translation
tools and a dictionary to translate phrases. Even-
tually, (Laurent et al, 2005) also translate words
or idioms, by using English as a pivot language.
The performance of this system is of 64% of cor-
rect answers for the French monolingual task, and
pean languages, http ://www.illc.uva.nl/EuroWordNet/
4Multilingual lexical database in which the Italian Word-
Net is strictly aligned with Princeton WordNet, http ://multi-
wordnet.itc.it
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
24
39.5% for the English-French bilingual task.
4 Adopted approach
In order to deal with the conversion from French
to English in our system, two strategies are ap-
plied in parallel. They differ on what is translated
to treat the question asked in French. The first sub-
system called MUSQAT proceeds to the question
analysis in French, and then translates the ques-
tion terms extracted by this question analysis mo-
dule, following the - - - arrows in Figure 1. The
second sub-system makes use of a machine trans-
lation tool (Reverso 5) to obtain translations of the
questions and then our English monolingual sys-
tem called QALC is applied, following the ..-.. ar-
rows in Figure 1 . These strategies will be detailed
later in the article.
If they represent the most common strategies for
this kind of task, an original feature of our system
is the implementation of both strategies, which en-
ables us to merge the results obtained by following
these strategies, in order to improve the global per-
formance of our system.
In Table 1, we present an analysis of the results
we obtained for the CLEF evaluation campaign.
We evaluate the results obtained at two different
points of the question-answering process, i.e. af-
ter the sentence selection (point (a) in Figure 1),
and after the answer extraction (point (b) in Fi-
gure 1). At point (a), we count how many ques-
tions (among the global evaluation set of 200 ques-
tions) have an appropriate answer in the first five
sentences. At point (b), we distinguish the answers
the analysis process labels as named entities (NE),
from the others, since the corresponding answe-
ring processes are different. We also detail how
many answers are ranked first, or in the first five
ranks, as we take into account the first five ans-
wers.
As illustrated in Table 1, the two strategies for
dealing with multilingualism give quite different
results, which can be explained by each strategy
characteristics.
MUSQAT proceeds to the question analysis
with French questions correctly expressed, and
which analysis is therefore more reliable. Yet, the
terms translations are then obtained from every
possible translation of each term, and thus without
taking account any context ; moreover, they de-
pend on the quality of the dictionaries used, and
5http ://www.reverso.net/
MUSQAT Reverso
+QALC
% %
(a) : Sentences first 5 41 46
with an answer ranks
(b) : Correct rank 1 18 14
NE answers
first 5 26 17
ranks
(b) : Correct rank 1 16 13
other answers
first 5 23 20
ranks
(b) : Total rank 1 17 13
(NE + non NE)
first 5 24 19
ranks
Final result 19
(fusion of both strategies)
TAB. 1 ? Performance of our system in CLEF
2005
introduce noise because of the erroneous transla-
tions.
In MUSQAT, we do not only translate mono-
terms (i.e. terms composed of single word) : the
biterms (composed of two words) of the French
questions are also extracted by the question analy-
sis. Every sequence of two terms which are tagged
as adjective/common noun or proper noun/proper
noun... constitutes a biterm. Each word of the bi-
term is translated, and then the existence of the
corresponding biterm built in English is checked
in the corpus. The biterms thus obtained are then
used by the further modules of the system. Taking
biterms into account is useful since they provide
a minimal context to the words forming them, as
well for the translation as for the re-indexing and
re-ranking of the documents (see Figure 1), as ex-
plained in (Ferret et al, 2002). Moreover, the pre-
sence of the biterm translations in the corpus is a
kind of validation of the monoterms translations.
As for translating the question, which is imple-
mented by Reverso+QALC, it presents the advan-
tage of giving a unique translation of the question
terms, which is quite reliable. But the grammati-
cality or realism of the question are not assured,
and thus the question analysis, based on regular
expression patterns, can be disturbed.
In this work, we tried to evaluate each strategy
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
25
and to bypass their drawbacks : on the one hand
(Section 5), by examining how the biterm transla-
tion in MUSQAT could be more reliable, and on
the other hand (Section 6) by improving the ques-
tion analysis, by relying on the French questions,
for QALC.
5 Biterm translation
The translation of terms and biterms present in
the question is achieved using two dictionaries.
The first of them, which was used last year for
our participation to CLEF is Magic-Dic 6. It is a
dictionary under GPL licence, which was retained
for its capacity to evolve. Indeed users can sub-
mit new translations which are controlled before
being integrated. Yet, it is quite incomplete. This
year we used FreeDict as well (FreeDict is also un-
der GPL licence), to fill in the gaps of Magic-Dic.
FreeDict added 424 translations to the 690 terms
already obtained. By mixing both sets of transla-
tions we obtained 463 additional biterms, making
a total of 777 biterms.
Nevertheless, whatever the quality and the size
of the dictionaries are, the problem of biterm trans-
lation remains the same : since biterms are not in
the dictionaries, the only way for us to get their
translation is to combine all the different term
translations. The main drawback of this approach
is the generated noise, for none of the terms consti-
tuting the biterm is disambiguated. For example,
three different translations are found for the bi-
term Conseil de de?fense : defense council, defense
advice and defense counsel ; but only the first of
those should be finally retained by our system.
To reduce this noise, an interesting possibility is
to validate the obtained biterms by searching them
or their variants in the complete collection of do-
cuments. (Grefenstette, 1999) reports a quite simi-
lar experiment in the context of a machine trans-
lation task : he uses the Web in order to order the
possible translations of noun phrases, and in par-
ticular noun biterms. Fastr (Jacquemin, 1996) is
a parser which takes as input a corpus and a list
of terms (multi or monoterms) and outputs the in-
dexed corpus in which terms and their variants are
recognized. Hence, Fastr is quite adequate for bi-
terms validation : it tags all the biterms present in
the collection, whether in their original form or in
a variant that can be semantic or syntactic.
In order to validate the biterms, the complete
6http ://magic-dic.homeunix.net
collection of the CLEF campaign (500 Mbyte) was
first tagged using the TreeTagger, then Fastr was
applied. The results are presented Table 2 : 39.5%
of the 777 biterms were found in the collection, in
a total of 63,404 occurrences. Thus there is an ave-
rage of 206 occurrences for each biterm. If we do
not take into account the biterm which is the most
represented (last year with 30,981 occurrences),
this average falls to 105. The 52 biterms which are
found in their original form only are most of the
time names of persons. Lastly, biterms that are ne-
ver found in their original form, are often consti-
tuted of one term badly translated, for example the
biterm oil importation is not present in the collec-
tion but its variant import of oil is found 28 times.
Then, it may be interesting to replace these biterms
by the most represented of their variants.
Whenever a biterm is thus validated (found in
the collection beyond a chosen threshold), the
translation of its terms is itself validated, other
translations being discarded. Thus, biterm valida-
tion enables us to validate monoterm translations.
Then, the following step will be to evaluate how
this new set of terms and biterms improves the re-
sults of MUSQAT.
After CLEF 2005 evaluation, we had at our dis-
posal the set of questions in their English original
version (this set was provided by the organizers).
We had also the English translation (far less cor-
rect) provided by the automatic translator Reverso.
As we can see it Table 3, for each set of ques-
tions the number of terms and biterms is nearly
the same. In the set of translations given by Re-
verso, we manually examined how many biterms
were false and found that here again the figures
were close to those of the original version. There
are two main reasons for which a biterm may be
false :
? in two thirds of cases, the association itself is
false : the two terms should not have been as-
sociated ; it is the case for example of many
country from the question How many coun-
tries joined the international coalition to res-
tore the democratic government in Haiti ? 7
? in one third of cases, one of the terms is
not translated or translated with an erroneous
term, like movement zapatiste coming from
the question What carry the courtiers of the
movement zapatiste in Mexico ? 8
7This sentence is an example of very good translation gi-
ven by Reverso
8This sentence is an example of bad translation given by
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
26
Total Number of biterms 777
Number of biterms found in the collection 307 - 39.5%
Number of biterms found in their original form only 52 - 17%
Number of biterms found with semantic variations only 150 - 54%
TAB. 2 ? Magic-Dic and FreeDict biterms validated by Fastr
Questions Questions Questions
in French translated in English in English
by Reverso (original version)
Terms 1180 1122 1163
Biterms 272 204 261
False Biterms 33 38 27
Common Biterms - 106
TAB. 3 ? Biterms in the different sets of questions
However, we calculated that among the 204 bi-
terms given by Reverso, 106 are also present in the
original set of questions in English. Among the 98
remaining biterms, 38 are false (for the reasons gi-
ven above). Then, there are 60 biterms which are
neither erroneous nor present in the original ver-
sion. Some of them contain a term which has been
translated using a different word, but that is never-
theless correct ; yet, most of these 60 biterms have
a different syntax from those constructed from the
original version, which is due to the syntax of the
questions translated by Reverso.
This leads us to conclude that even if Reverso
produces syntactically erroneous questions, the
vocabulary it chooses is most of the time adequate.
Yet, it is still interesting to use also the biterms
constructed from the dictionaries since they are
much more numerous and provide variants of the
biterms returned by Reverso.
6 Multilingual question analysis
We have developed for the evaluations a ques-
tion analysis in both languages. It is based on the
morpho-syntactic tagging and the syntactic analy-
sis of the questions. Then different elements are
detected from both analyses : recognition of the
expected answer type, of the question category, of
the temporal context...
There are of course lexicons and patterns which
are specific to each language, but the core of the
module is independent from the language. This
Reverso, which should have produced What do supporters of
the Zapatistas in Mexico wear ?
module was evaluated on corpora of similar ques-
tions in French and in English, and its results on
both languages are quite close (around 90% of re-
call and precision for the expected answer type
for example ; for more details, see (Ligozat et al,
2006)).
As presented above, our system relies on two
distinct strategies to answer to a cross-language
question :
? Either the question is analyzed in the ori-
ginal language, and next translated term-by-
term. The question analysis is then more re-
liable since it processes a grammatically cor-
rect question ; yet, the translation of terms has
no context to rely on.
? Or the question is first translated into the
target language before being analyzed. Al-
though this strategy improves the translation,
its main inconvenient is that each translation
error has strong consequences on the ques-
tion analysis. We will now try to evaluate to
which extent the translation errors actually
influence our question analysis and to find so-
lutions to avoid minimize this influence in the
Reverso+QALC system.
An error in the question translation can lead to
wrong terms or an incorrect English construction.
Thus, the translation of the question ?Combien y
a-t-il d?habitants en France ?? (?How many inhabi-
tants are there in France ??) is ?How much is there
of inhabitants in France ??.
In order to evaluate our second strategy, Re-
verso+QALC, using question translation and then
a monolingual system, it is interesting to estimate
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
27
the influence of a such a coarse translation on the
results of our system.
In order to avoid these translating problems, it
is possible to adapt either the input or the out-
put of the translating module. (Ahn et al, 2004)
present an example of a system processing pre-
and post-corrections thanks to surface reformu-
lation rules. However, this type of correction is
highly dependent on the kind of questions to pro-
cess, as well as on the errors of the translation tool
that is used.
We suggest to use another kind of processing,
which makes the most of the cross-lingual charac-
ter of the task, in order to improve the analysis of
the translated questions and to take into account
the possibilities of errors in these questions.
Our present system already takes into account
some of the most frequent translation errors, by
allowing the question analysis module to loosen
some of its rules in case the question be transla-
ted. Thus, a definition question such as ?Qu?est-
ce que l?UNITA ??, translated ?What UNITA ??
by our translating tool, instead of ?What is the
UNITA ??, will nevertheless be correctly analyzed
by our rules : indeed, the pattern WhatGN will be
considered as corresponding to a definition ques-
tion, while on a non-translated question, only the
pattern WhatBeGN will be allowed.
In order to try and improve our processing of
approximations in the translated questions, the so-
lution we suggest here consists in making the
question analysis in both the source and the target
languages, and in reporting the information (or at
least part of it) returned by the source analysis into
the target analysis. This is possible first because
our system treats both the languages in a parallel
way, and second, some of the information retur-
ned by the question analysis module use the same
terms in English and in French, like for example
the question category or the expected Named En-
tity type.
More precisely, we propose, in the task with
French questions and English documents, to ana-
lyse the French questions, and their English trans-
lations, and then to report the question category
and the expected answer type of the French ques-
tions into the English question analysis. The in-
formation found in the source language should be
more reliable since obtained on a real question.
For example, for the question ?Combien de
communaute?s Di Mambro a-t-il cre?e ?? (?How
many communities has Di Mambro created ??),
Reverso?s translation is ?How many Di Mambro
communities has he create ?? which prevents the
question analysis module to analyze it correctly.
The French analysis is thus used, which provides
the question category combien (how many) and the
expected named entity type NUMBER. This infor-
mation is reported in the English analysis file.
These characteristics of the question are used at
two different steps of the question answering pro-
cess : when selecting the candidate sentences and
when extracting the answers. Improving their re-
liability should then enable us to increase the num-
ber of correct answers after these two steps.
In order to test this strategy, we conducted an
experiment based on the CLEF 2005 FR-EN task,
and the 200 corresponding French questions. We
launched the question answering system on three
question files :
? The first question file (here called English
file) contained the original English questions
(provided by the CLEF organizers). This file
will be considered as a test file, since the re-
sults of our system on this file represent those
that would be reached without translation er-
rors.
? The second file (called Translated file) contai-
ned the translated questions analysis.
? The last file (called Improved file) contained
the same analysis, but for which the question
category and the expected answer type were
replaced by those of the French analysis.
Then we searched for the number of correct ans-
wers for each input question file after the sentence
selection and after the answer extraction. The re-
sults obtained by our system on each file are pre-
sented on Figure 2, Figure 3 and Figure 4. These
figures present the number of questions expecting
a named entity answer, expecting another kind of
answer, and the total number of questions, as well
as the results of our system on each type of ques-
tion : the number of correct questions are given at
the first five ranks, and at the first rank, first for the
sentences (?long answers?) and then for the short
answers.
These results show that the information trans-
fer from the source language to the target lan-
guage significantly improves the system?s results ;
the number of correct answers increases in every
case. It increases from 34 on the translated ques-
tions file to 36 on the improved file, and from 52
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
28
FIG. 2 ? QALC?s results (i.e. number of correct
answers) on the 200 questions
FIG. 3 ? Results on the named entities questions
FIG. 4 ? Results on the non named entities ques-
tions
to 55 for the first 5 ranks. These results are closer
to those of the monolingual system, which returns
41 correct answers at the first rank, and 59 on the
first 5 ranks.
It is interesting to see that the difference bet-
ween the monolingual and the bilingual systems
is less noticeable after the sentence selection step
than after the answer extraction step, which tends
to prove that the last step of our process is more
sensitive to translation errors. Moreover, this expe-
riment shows that this step can be improved thanks
to an information transfer between the source and
the target languages. In order to extend this stra-
tegy, we could also match each French question
term to its English equivalent, in order to trans-
late all the information given by the French analy-
sis into English. Thus, the question analysis errors
would be minimized.
7 Conclusion
The originality of our cross-language question
answering system is to use in parallel the two
most widely used strategies for shifting language,
which enables us to benefit from the advantages
of each strategy. Yet, each method presents draw-
backs, that we tried to evaluate in this article, and
to bypass.
For the term-by-term translation, we make the
most of the question biterms in order to restrict the
possible translation ambiguities. By validating the
biterms in the document collection, we have im-
proved the quality of both the biterms and the mo-
noterms translations. We hope this improvement
will lead to a better selection of the candidate sen-
tences from the documents.
For the question translation, we use the infor-
mation deduced from the source language to avoid
the problems coming from a bad or approximative
translation. This strategy enables us to solve some
of the problems coming from non-grammatical
translations ; matching each term of the French
question with its English equivalent would enable
us to transfer all the information of the French ana-
lysis. But the disambiguation errors of the transla-
tion remain.
References
Kisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Del-
mas, Jochen L. Leidner, and Matthew B. Smillie.
2004. Cross-lingual question answering with QED.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
29
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 335?342, Bath, UK.
Ce?sar de Pablo-Sa?nchez, Ana Gonza?lez-Ledesma,
Jose? Luis Mart??nez-Ferna?ndez, Jose? Maria Guirao,
Paloma Martinez, and Antonio Moreno. 2005.
MIRACLE?s 2005 approach to cross-lingual ques-
tion answering. In Working Notes, CLEF Cross-
Language Evaluation Forum, Vienna, Austria.
Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet,
Gabriel Illouz, Christian Jacquemin, Laura Mon-
ceaux, Isabelle Robba, and Anne Vilnat. 2002. How
NLP can improve question answering. Knowledge
Organization, 29(3-4).
Gregory Grefenstette. 1999. The world wide web as
a resource for example-based machine translation
tasks. In ASLIB Conference on Translating and the
Computer, volume 21, London, UK.
Sven Hartrumpf. 2005. University of Hagen at
QA@CLEF 2005 : Extending knowledge and dee-
pening linguistic processing for question answering.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Christian Jacquemin. 1996. A symbolic and surgical
acquisition of terms through variation. Connectio-
nist, Statistical and Symbolic Approaches to Lear-
ning for Natural Language Processing, pages 425?
438.
Valentin Jijkoun, Gilad Mishne, Maarten de Rijke, Ste-
fan Schlobach, David Ahn, and Karin Muller. 2004.
The University of Amsterdam at QA@CLEF2004.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 321?325, Bath, UK.
Dominique Laurent, Patrick Se?gue?la, and Sophie
Ne`gre. 2005. Cross lingual question answering
using QRISTAL for CLEF 2005. In Working Notes,
CLEF Cross-Language Evaluation Forum, Vienna,
Austria.
Anne-Laure Ligozat, Brigitte Grau, Isabelle Robba,
and Anne Vilnat. 2006. L?extraction des re?ponses
dans un syste`me de question-re?ponse. In Traitement
Automatique des Langues Naturelles (TALN 2006),
Leuven, Belgium.
Gu?nter Neumann and Bogdan Sacaleanu. 2004.
Experiments on robust NL question interpretation
and multi-layered doument annotation for a cross-
language question / answering system. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 311?320, Bath, UK.
Gu?nter Neumann and Bogdan Sacaleanu. 2005. DF-
KI?s LT-lab at the CLEF 2005 multiple language
question answering track. In Working Notes, CLEF
Cross-Language Evaluation Forum, Vienna, Aus-
tria.
Laura Perret. 2004. Question answering system for the
French language. In Working Notes, CLEF Cross-
Language Evaluation Forum, pages 295?305, Bath,
UK.
Richard F.E. Sutcliffe, Michael Mulcahy, Igal Gabbay,
Aoife O?Gorman, Kieran White, and Darina Slat-
tery. 2005. Cross-language French-English ques-
tion answering using the DLT system at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Hristo Tanev, Matteo Negri, Bernardo Magnini, and
Milen Kouylekov. 2004. The DIOGENE ques-
tion answering system at CLEF-2004. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 325?333, Bath UK.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2005. Exploiting
linguistic indices and syntactic structures for multi-
lingual question answering : ITC-irst at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
30
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 429?433,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Question Classification Transfer
Anne-Laure Ligozat
LIMSI-CNRS / BP133, 91403 Orsay cedex, France
ENSIIE / 1, square de la re?sistance, Evry, France
firstname.lastname@limsi.fr
Abstract
Question answering systems have been de-
veloped for many languages, but most re-
sources were created for English, which
can be a problem when developing a sys-
tem in another language such as French.
In particular, for question classification,
no labeled question corpus is available for
French, so this paper studies the possi-
bility to use existing English corpora and
transfer a classification by translating the
question and their labels. By translating
the training corpus, we obtain results close
to a monolingual setting.
1 Introduction
In question answering (QA), as in most Natural
Language Processing domains, English is the best
resourced language, in terms of corpora, lexicons,
or systems. Many methods are based on super-
vised machine learning which is made possible by
the great amount of resources for this language.
While developing a question answering system
for French, we were thus limited by the lack of
resources for this language. Some were created,
for example for answer validation (Grappy et al,
2011). Yet, for question classification, although
question corpora in French exist, only a small part
of them is annotated with question classes, and
such an annotation is costly. We thus wondered
if it was possible to use existing English corpora,
in this case the data used in (Li and Roth, 2002),
to create a classification module for French.
Transfering knowledge from one language to
another is usually done by exploiting parallel cor-
pora; yet in this case, few such corpora exists
(CLEF QA datasets could be used, but question
classes are not very precise). We thus investigated
the possibility of using machine translation to cre-
ate a parallel corpus, as has been done for spoken
language understanding (Jabaian et al, 2011) for
example. The idea is that using machine transla-
tion would enable us to have a large training cor-
pus, either by using the English one and translat-
ing the test corpus, or by translating the training
corpus. One of the questions posed was whether
the quality of present machine translation systems
would enable to learn the classification properly.
This paper presents a question classification
transfer method, which results are close to those
of a monolingual system. The contributions of the
paper are the following:
? comparison of train-on-target and test-on-
source strategies for question classification;
? creation of an effective question classification
system for French, with minimal annotation
effort.
This paper is organized as follows: The problem
of Question Classification is defined in section 2.
The proposed methods are presented in section 3,
and the experiments in section 4. Section 5 details
the related works in Question Answering. Finally,
Section 6 concludes with a summary and a few
directions for future work.
2 Problem definition
A Question Answering (QA) system aims at re-
turning a precise answer to a natural language
question: if asked ?How large is the Lincoln
Memorial??, a QA system should return the an-
swer ?164 acres? as well as a justifying snippet.
Most systems include a question classification step
which determines the expected answer type, for
example area in the previous case. This type can
then be used to extract the correct answer in docu-
ments.
Detecting the answer type is usually consid-
ered as a multiclass classification problem, with
each answer type representing a class. (Zhang and
429
Englishtraining?corpus Englishtraining?corpus
Englishtest?corpus(translation)
Frenchtest?corpus Frenchtest?corpus
Frenchtraining?corpus(translation)
Questionclassificationfor?English
Questionclassificationfor?French
learn
predict
translation
learn
predict
translation
Figure 1: Methods for transfering question classi-
fication
Lee, 2003) showed that a training corpus of sev-
eral thousands of questions was required to obtain
around 90% correct classification, which makes it
a costly process to adapt a system to another lan-
guage than English. In this paper, we wish to learn
such a system for French, without having to man-
ually annotate thousands of questions.
3 Transfering question classification
The two methods tested for transfering the classi-
fication, following (Jabaian et al, 2011), are pre-
sented in Figure 1:
? The first one (on the left), called test-on-
source, consists in learning a classification
model in English, and to translate the test cor-
pus from French to English, in order to apply
the English model on the translated test cor-
pus.
? The second one (on the right), called train-
on-target, consists in translating the training
corpus from English to French. We obtain an
labeled French corpus, on which it is possible
to learn a classification model.
In the first case, classification is learned on well
written questions; yet, as the test corpus is trans-
lated, translation errors may disturb the classifier.
In the second case, the classification model will
be learned on less well written questions, but the
corpus may be large enough to compensate for the
loss in quality.
Figure 2: Some of the question categories pro-
posed by (Li and Roth, 2002)
4 Experiments
4.1 Question classes
We used the question taxonomy proposed by (Li
and Roth, 2002), which enabled us to compare
our results to those obtained by (Zhang and Lee,
2003) on English. This taxonomy contains two
levels: the first one contains 50 fine grained cat-
egories, the second one contains 6 coarse grained
categories. Figure 2 presents a few of these cate-
gories.
4.2 Corpora
For English, we used the data from (Li and Roth,
2002), which was assembled from USC, UIUC
and TREC collections, and has been manually la-
beled according to their taxonomy. The training
set contains 5,500 labeled questions, and the test-
ing set contains 500 questions.
For French, we gathered questions from several
evaluation campaigns: QA@CLEF 2005, 2006,
2007, EQueR and Qu?ro 2008, 2009 and 2010.
After elimination of duplicated questions, we ob-
tained a corpus of 1,421 questions, which were di-
vided into a training set of 728 questions, and a test
set of 693 questions 1. Some of these questions
were already labeled, and we manually annotated
the rest of them.
Translation was performed by Google Trans-
late online interface, which had satisfactory per-
formance on interrogative forms, which are not
well handled by all machine translation systems 2.
1This distribution is due to further constraints on the sys-
tem.
2We tested other translation systems, but Google Trans-
late gave the best results.
430
Train en en fr fr
(trans.)
Test en en fr fr
(trans.)
Method test-
on-
source
train-
on-
target
50 .798 .677 .794 .769
classes
6 .90 .735 .828 .84
classes
Table 1: Question classification precision for both
levels of the hierarchy (features = word n-grams,
classifier = libsvm)
4.3 Classification parameters
The classifier used was LibSVM (Chang and Lin,
2011) with default parameters, which offers one-
vs-one multiclass classification, and which (Zhang
and Lee, 2003) showed to be most effective for this
task.
We only considered surface features, and ex-
tracted bag-of-ngrams (with n = 1..2).
4.4 Results and discussion
Table 1 shows the results obtained with the basic
configuration, for both transfer methods.
Results are given in precision, i.e. the propor-
tion of correctly classified questions among the
test questions 3.
Using word n-grams, monolingual English clas-
sification obtains .798 correct classification for the
fine grained classes, and .90 for the coarse grained
classes, results which are very close to those ob-
tained by (Zhang and Lee, 2003).
On French, we obtain lower results: .769 for
fine grained classes, and .84 for coarse grained
classes, probably mostly due to the smallest size
of the training corpus: (Zhang and Lee, 2003) had
a precision of .65 for the fine grained classification
with a 1,000 questions training corpus.
When translating test questions from French
to English, classification precision decreases, as
was expected from (Cumbreras et al, 2006). Yet,
when translating the training corpus from English
to French and learning the classification model
3We measured the significance of precision differences
(Student t test, p=.05), for each level of the hierarchy between
each test, and, unless indicated otherwise, comparable results
are significantly different in each condition.
Train en fr fr
(trans.)
Test en fr fr
Method train-
on-
target
50 .822 .798 .807
classes
6
classes .92 .841 .872
Table 2: Question classification precision for both
levels of the hierarchy (features = word n-grams
with abbreviations, classifier = libsvm)
on this translated corpus, precision is close to
the French monolingual one for coarse grained
classes and a little higher than monolingual for
fine grained classification (and close to the English
monolingual one): this method gives precisions of
.794 for fine grained classes and .828 for coarse
grained classes.
One possible explanation is that the condition
when test questions are translated is very sensitive
to translation errors: if one of the test questions
is not correcly translated, the classifier will have
a hard time categorizing it. If the training cor-
pus is translated, translation errors can be counter-
balanced by correct translations. In the following
results, we do not consider the ?en to en (trans)?
method since it systematically gives lower results.
As results were lower than our existing rule-
based method, we added parts-of-speech as fea-
tures in order to try to improve them, as well as
semantic classes: the classes are lists of words re-
lated to a particular category; for example ?pres-
ident? usually means that a person is expected as
an answer. Table 2 shows the classification perfor-
mance with this additional information.
Classification is slightly improved, but only for
coarse grained classes (the difference is not signif-
icant for fine grained classes).
When analyzing the results, we noted that most
confusion errors were due to the type of features
given as inputs: for example, to correctly clas-
sify the question ?What is BPH?? as a question
expecting an expression corresponding to an ab-
breviation (ABBR:exp class in the hierarchy), it
is necessary to know that ?BPH? is an abbrevia-
tion. We thus added a specific feature to detect if
a question word is an abbreviation, simply by test-
431
Train en fr fr
(trans.)
Test en fr fr
50 .804 .837 .828
classes
6
classes .904 .869 .900
Table 3: Question classification precision for both
levels of the hierarchy (features = word n-grams
with abbreviations, classifier = libsvm)
ing if it contains only upper case letters, and nor-
malizing them. Table 3 gives the results with this
additional feature (we only kept the method with
translation of the training corpus since results were
much higher).
Precision is improved for both levels of the hi-
erarchy: for fine grained classes, results increase
from .794 to .837, and for coarse grained classes,
from .828 to .869. Remaining classification errors
are much more disparate.
5 Related work
Most question answering systems include ques-
tion classification, which is generally based on su-
pervised learning. (Li and Roth, 2002) trained
the SNoW hierarchical classifier for question clas-
sification, with a 50 classes fine grained hierar-
chy, and a coarse grained one of 6 classes. The
features used are words, parts-of-speech, chunks,
named entities, chunk heads and words related to
a class. They obtain 98.8% correct classification
of the coarse grained classes, and 95% on the fine
grained one. This hierarchy was widely used by
other QA systems.
(Zhang and Lee, 2003) studied the classifica-
tion performance according to the classifier and
training dataser size, as well as the contribution of
question parse trees. Their results are 87% correct
classification on coarse grained classes and 80%
on fine grained classes with vectorial attributes,
and 90% correct classification on coarse grained
classes and 80% on fine grained classes with struc-
tured input and tree kerneks.
These question classifications were used for
English only. Adapting the methods to other
languages requires to annotated large corpora of
questions.
In order to classify questions in different lan-
guages, (Solorio et al, 2004) proposed an in-
ternet based approach to determine the expected
type. By combining this information with ques-
tion words, they obtain 84% correct classification
for English, 84% for Spanish and 89% for Ital-
ian, with a cross validation on a 450 question cor-
pus for 7 question classes. One of the limitations
raised by the authors is the lack of large labeled
corpora for all languages.
A possibility to overcome this lack of resources
is to use existing English resources. (Cumbreras
et al, 2006) developed a QA system for Spanish,
based on an English QA system, by translating the
questions from Spanish to English. They obtain a
65% precision for Spanish question classification,
while English classification are correctly classified
with an 80% precision. This method thus leads to
an important drop in performance.
Crosslingual QA systems, in which the question
is in a different language than the documents, also
usually rely on English systems, and translate an-
swers for example (Bos and Nissim, 2006; Bow-
den et al, 2008).
6 Conclusion
This paper presents a comparison between two
transfer modes to adapt question classification
from English to French. Results show that trans-
lating the training corpus gives better results than
translating the test corpus.
Part-of-speech information only was used, but
since (Zhang and Lee, 2003) showed that best re-
sults are obtained with parse trees and tree kernels,
it could be interesting to test this additional in-
formation; yet, parsing translated questions may
prove unreliable.
Finally, as interrogative forms occur rarely is
corpora, their translation is usually of a slightly
lower quality. A possible future direction for this
work could be to use a specific model of transla-
tion for questions in order to learn question classi-
fication on higher quality translations.
References
J. Bos and M. Nissim. 2006. Cross-lingual question
answering by answer translation. In Working Notes
of the Cross Language Evaluation Forum.
M. Bowden, M. Olteanu, P. Suriyentrakorn, T. d?Silva,
and D. Moldovan. 2008. Multilingual question
answering through intermediate translation: Lcc?s
poweranswer at qa@clef 2007. Advances in Mul-
432
tilingual and Multimodal Information Retrieval,
5152:273?283.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
M.A?.G. Cumbreras, L. Lo?pez, and F.M. Santiago.
2006. Bruja: Question classification for spanish. us-
ing machine translation and an english classifier. In
Proceedings of the Workshop on Multilingual Ques-
tion Answering, pages 39?44. Association for Com-
putational Linguistics.
Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,
Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-
nat. 2011. Selecting answers to questions from
web documents by a robust validation process. In
IEEE/WIC/ACM International Conference on Web
Intelligence.
Bassam Jabaian, Laurent Besacier, and Fabrice
Lefe`vre. 2011. Combination of stochastic under-
standing and machine translation systems for lan-
guage portability of dialogue systems. In Acous-
tics, Speech and Signal Processing (ICASSP), 2011
IEEE International Conference on, pages 5612?
5615. IEEE.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of the 19th international conference
on Computational linguistics-Volume 1, pages 1?7.
Association for Computational Linguistics.
T. Solorio, M. Pe?rez-Coutino, et al 2004. A language
independent method for question classification. In
Proceedings of the 20th international conference on
Computational Linguistics, pages 1374?1380. Asso-
ciation for Computational Linguistics.
D. Zhang and W.S. Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings
of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, pages 26?32. ACM.
433
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 487?492,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ANNLOR: A Na??ve Notation-system for Lexical Outputs Ranking
Anne-Laure Ligozat
LIMSI-CNRS/ENSIIE
rue John von Neumann
91400 Orsay, France
annlor@limsi.fr
Cyril Grouin
LIMSI-CNRS
rue John von Neumann
91400 Orsay, France
cyril.grouin@limsi.fr
Anne Garcia-Fernandez
CEA-LIST
NANO INNOV, Bt. 861
91191 Gif-sur-Yvette cedex, France
anne.garcia-fernandez@cea.fr
Delphine Bernhard
LiLPa, Universite? de Strasbourg
22 rue Rene? Descartes, BP 80010
67084 Strasbourg cedex, France
dbernhard@unistra.fr
Abstract
This paper presents the systems we developed
while participating in the first task (English
Lexical Simplification) of SemEval 2012. Our
first system relies on n-grams frequencies
computed from the Simple English Wikipedia
version, ranking each substitution term by de-
creasing frequency of use. We experimented
with several other systems, based on term fre-
quencies, or taking into account the context in
which each substitution term occurs. On the
evaluation corpus, we achieved a 0.465 score
with the first system.
1 Introduction
In this paper, we present the methods we used while
participating to the Lexical Simplification task at Se-
mEval 2012 (Specia et al, 2012). We experimented
with several methods:
? using word frequencies or other statistical fig-
ures from the BNC corpus, Google Books
NGrams, the Simple English Wikipedia, and
results from the Bing search engine (with/with-
out lemmatization);
? using association measures for a word and its
context based on language models (with/with-
out inflection);
? making a combination of previous methods
with SVMRank.
Depending on the results obtained on the training
corpus, we chose the methods that seemed to best fit
the data.
2 Task description
2.1 Presentation
The Lexical Simplification task aimed at determin-
ing the degree of simplicity of words. The inputs
given were a short text, in which a target word was
chosen, and several substitutes for the target word
that fit the context.
An example of a short text follows; the target
word is ?outdoor?, and other words of this text will
be considered as the context of this target word.
< i n s t a n c e i d =? 270 ?>
<c o n t e x t>With t h e growing demand f o r
t h e s e f i n e g a r de n f u r n i s h i n g s ,
t h e y found i t n e c e s s a r y t o d e d i c a t e
a p o r t i o n o f t h e i r b u s i n e s s t o
<head>o u t d o o r< / head> l i v i n g and
p a t i o f u r n i s h i n g s .< / c o n t e x t>
< / i n s t a n c e>
The substitutes given for this target word were
the following: ?alfresco;outside;open-air;outdoor;?.
The objective was to order these words by descend-
ing simplicity.
2.2 Corpora
Two corpora were provided: the trial corpus with
development examples, and the test corpus for eval-
uation.
In the trial corpus, a gold standard was also given.
For the previous example, it stated that the substi-
tutes had to be in the following order: ?outdoor
open-air outside, alfresco?, ?outdoor? being consid-
ered as the simplest substitute, and ?outside? and
?alfresco? being considered as the less simple ones.
487
Three baselines have been given by the organiz-
ers: the first one is a simple randomization of the
substitute list, the second one keeps the substitute
list as it is, and the third one (called ?simple fre-
quency?) relies on the use of the Google Web 1T
corpus.
3 Preprocessing
3.1 Corpus constitution
In order to use machine-learning based approaches,
we produced two sub-corpora respectively for the
training and evaluation stages from the trial corpus.
The training sub-corpus is used to develop and tune
the systems we produced while the evaluation sub-
corpus is used to evaluate the results of these sys-
tems.
For each set from the SemEval trial corpus, if the
set is composed of at least eight lexical elements be-
longing to the same morpho-syntactic category (e.g.,
a set with at least eight instances of ?bright? as an
adjective), we extracted three instances from this
set for the evaluation sub-corpus, the remaining in-
stances being part of the training sub-corpus. If the
set is composed of less than eight instances, all in-
stances are used in the training sub-corpus. We also
kept two complete sets of lexical elements for the
evaluation sub-corpus in order to test the robustness
of our methods on new lexical elements that have not
been studied yet. This distribution allows us to bene-
fit from a repartition between training and evaluation
sub-corpora where the instances ratio is of 66/33%.
3.2 Corpus cleaning
While studying the trial corpus, we noticed that the
texts were not always in plain text, and in particular
contained HTML entities. As some of our methods
used the context of target words, we decided to cre-
ate a cleaner version of the corpora. For the dash and
quote HTML entities (&#8211; &#8220; etc.), we
replaced each entity by its refering symbol. When
replacing the apostrophe HTML entity (&apos;), we
decided to link the abbreviated token with the previ-
ous one because all n-grams methods worked better
with abbreviated terms of one token-length (don?t)
than two token-length (do n?t) (see section 5).
3.3 Inflection
In some sentences, the target words are inflected, but
the substitutes are given in their lemmatized forms.
For example, one of the texts was the following :
<c o n t e x t>In f a c t , d u r i n g a t l e a s t s i x
d i s t i n c t p e r i o d s i n Army h i s t o r y
s i n c e World War I , l a c k o f t r u s t and
c o n f i d e n c e i n s e n i o r l e a d e r s c au se d
t h e so?c a l l e d b e s t and
<head>b r i g h t e s t< / head> t o l e a v e t h e
Army i n d r o v e s .< / c o n t e x t>
For this text and target word, the proposed sub-
stitutes were ?capable; most able; motivated; in-
telligent; bright; clever; sharp; promising?, and if
we want to test the simplicity of the words in con-
text, for example with a 2-words left context, we
will obtain unlikely phrases such as ?best and capa-
ble? (which should be ?best and most capable?). We
thus used several resources to get inflected forms of
words: we used the Lingua::EN::Conjugate and Lin-
gua::EN::Inflect Perl modules, which give inflected
forms of verbs and plural forms of nouns, as well as
the English dictionary of inflected forms DELA,1 to
validate the Perl modules outputs if necessary, and
get comparatives and superlatives of adjectives, and
a list of irregular English verbs, also to validate the
Perl modules outputs.
4 Simple English Wikipedia based system
Our best system, called ANNLOR-simple, is based
on Simple English Wikipedia frequencies. As the
challenge focused on substitutions performed by
non-native English speakers, we tried to use linguis-
tic resources that best fit this kind of data. In this
way, we made the hypothesis that training our sys-
tem on documents written by or written for non-
native English speakers would be useful.
The use of the Simple English version from
Wikipedia seems to be a good solution as it is tar-
geted at people who do not have English as their
mother tongue. Our hypothesis seems to be correct
due to the results we obtained. Morevover, the Sim-
ple English Wikipedia has been used previously in
work on automatic text simplification, e.g. (Zhu et
al., 2010).
1http://infolingu.univ-mlv.fr/
DonneesLinguistiques/Dictionnaires/
telechargement.html
488
First, we produced a plain text version of the Sim-
ple English Wikipedia. We downloaded the dump
dated February 27, 2012 and extracted the textual
contents using the wikipedia2text tool.2 The
final plaintext file contains approximately 10 million
words.
We extracted word n-grams (n ranging from 1 to
3) and their frequencies from this corpus thanks to
the Text-NSP Perl module 3 and its count.pl pro-
gram, which produces the list of n-grams of a docu-
ment, with their frequencies. Table 1 gives the num-
ber of n-grams produced.
Table 1: Number of distinct n-grams extracted from the
Simple English Wikipedia
n #n-grams
1 301,718
2 2,517,394
3 6,680,906
1 to 3 9,500,018
Some of these n-grams are invalid, and result
from problems when extracting plain text from
Wikipedia, such as ?27|ufc 1?, which corresponds
to wiki syntax. As we would not find these n-grams
in our substitution lists, we did not try to clean the
n-gram data.
Then, we ranked the possible substitutes of a lex-
ical item according to these frequencies, in descend-
ing order. For example, for the substitution list (in-
telligent, bright, clever, smart), the respective fre-
quencies in the Simple English Wikipedia are (206,
475, 141, 201), and the substitutes will be ranked in
descending frequencies: (bright, intelligent, smart,
clever).
Several tests were conducted, with varying pa-
rameters. We used the plain text version of the Sim-
ple English Wikipedia, but also tried to lemmatize it,
since substitutes are lemmatized. We used the Tree-
Tagger 4 (Schmid, 1994) and applied it on the whole
2See http://www.polishmywriting.com/
download/wikipedia2text\_rsm\_mods.tgz
and http://blog.afterthedeadline.com/
2009/12/04/generating-a-plain-text-corpus
-from-wikipedia
3http://search.cpan.org/?tpederse/
Text-NSP-1.25/lib/Text/NSP.pm
4http://www.ims.uni-stuttgart.de/
corpus, before counting n-grams. Moreover, since
bigrams and trigrams increase a lot, the size of n-
gram data, we evaluated their influence on results.
These tests are summed up in table 2.
Table 2: Results obtained with the Simple English
Wikipedia based system, on the trial and test corpora
reference lemmas score on score on
n-grams trial corpus test corpus
1-grams only no 0.333 ?
1 and 2-grams no 0.371 ?
1 to 3-grams no 0.381 0.465
1 to 3-grams yes 0.380 0.462
Simple Frequency
0.398 0.471
baseline
WLV-SHEF-SimpLex
(best system ? 0.496
@SemEval2012)
With unigrams only, 158 substitutes of the trial
corpus are absent of the reference dataset, 105 when
adding bigrams, and 91 when adding trigrams. Most
of the missing n-grams (when using 1 to 3-grams)
indeed seem to be very uncommon, such as ?undo-
mesticated? or ?telling untruths?.
The small difference between the lemmatized and
inflected versions of Wikipedia is due to two rea-
sons: some substitutes are found in the lemmatized
version because substitutes are given in the lemma-
tized form (for example ?abnormal growth? is only
present in its plural form ?abnormal growths? in
the inflected Wikipedia); and some other substitutes
are missing in the lemmatized version, mostly be-
cause of errors from the TreeTagger (for example
?be scared of? becomes ?be scare of?).
We kept the system that obtained the best scores
on the trial corpus, that is with 1 to 3-grams and non-
lemmatized n-grams, with a score of 0.381. This
system obtained a score of 0.465 on the evalua-
tion corpus, thus ranking second ex-aequo at the Se-
mEval evaluation.
projekte/corplex/TreeTagger/
489
5 Other frequency-based methods
We tried several other reference corpora, always
with the idea that the more frequent a word is, the
simpler it is. We used the BNC corpus,5 as well
as the Google Books NGrams.6 These NGrams
were calculated on the books digitized by Google,
and contain for each encountered n-gram, its num-
ber of occurrences for a given year. As the Google
Books NGrams are quite voluminous, we selected a
random year (2008), and kept only alphabetical n-
grams with potential hyphens, and used n-grams for
n ranging from 1 to 4. The dataset used contains
477,543,736 n-grams.
We also used the Microsoft Web N-gram Service
(more details on this service are given in the fol-
lowing section) to rank substitutes in descending or-
der. The results of these methods on the trial corpus
are given in table 3. The result of the simple fre-
quency baseline is also given: this baseline is also
frequency-based, but words are ranked according to
the number of hits found when querying the Google
Web 1T corpus with each substitute.
Table 3: Results obtained with frequency-based methods,
on the trial corpus
reference corpus score
BNC 0.347
Google Books NGrams 0.367
Microsoft NGrams 0.383
Simple Frequency baseline 0.398
This table shows that all frequency-based meth-
ods have lower scores than the Simple Frequency
baseline, although the score obtained with the Mi-
crosoft NGrams is quite close to the baseline. The
results from Microsoft Ngrams and the Simple En-
glish are very close. We decided to submit the Sim-
ple English Wikipedia-based system because it was
more different from the simple frequency baseline.
6 Contextual methods
We also wanted to use contextual information, since,
according to the contexts of the target word, dif-
ferent substitutes can be used, or ranked differ-
5http://www.natcorp.ox.ac.uk/
6http://books.google.com/ngrams/datasets
ently. In the following two examples, the same word
?film? is targetted, and the same substitutes are pro-
posed ?film;picture;movie;?; yet, in the gold stan-
dard, ?film? is placed before ?movie? in instance 19,
and after it in instance 15.
< i n s t a n c e i d =? 15 ?>
<c o n t e x t>Film Music L i t e r a t u r e
C y b e r p l a c e ? I n c l u d e s
<head>f i l m< / head> r e v i e w s , message
b o a r d s , c h a t room , and images
from v a r i o u s f i l m s .< / c o n t e x t>
< / i n s t a n c e>
( . . . )
< i n s t a n c e i d =? 19 ?>
<c o n t e x t>A f i n e s c o r e by George Fen ton
( THE CRUCIBLE ) and b e a u t i f u l
p h o t o g r a h y by Roger P r a t t add
g r e a t l y t o t h e e f f e c t i v e n e s s o f t h e
<head>f i l m< / head> .< / c o n t e x t>
< / i n s t a n c e>
Ranking substitutes thus depends on the context
of the target word. We implemented two systems
taking the context of target words into account.
6.1 Language model probabilities
The other system submitted (called ANNLOR-
lmbing) relies on language models, which was the
method used by the organizers in their Simple Fre-
quency baseline. While the organizers used Google
n-grams to rank terms to be substituted by decreas-
ing frequency of use, we used Microsoft Web n-
grams in the same way. Nevertheless, we also added
the contexts of each term to be substituted.
We used the Microsoft Web N-gram Service7 to
obtain joint probability for text units, and more
precisely its Python library.8 We used the bing-
body/apr10/ ) N-Gram model.
We considered a text unit composed of the lexi-
cal item and a contextual window of 4 words to the
left and 4 words to the right (words being separated
by spaces). For example, in the following sentence,
we tested ?He brings an incredibly rich and diverse
background that?, and the same unit with the tar-
get word replaced by substitutes, for example ?He
brings an incredibly lush and diverse background
that?.
7http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
8http://web-ngram.research.microsoft.
com/info/MicrosoftNgram-1.02.zip
490
< i n s t a n c e i d =? 118 ?>
<c o n t e x t>He b r i n g s an i n c r e d i b l y
<head> r i c h< / head> and d i v e r s e
background t h a t i n c l u d e s e v e r y t h i n g
from e x e c u t i v e c o a c h i n g , l e a r n i n g
&amp ; deve lopmen t and management
c o n s u l t i n g , t o s e n i o r o p e r a t i o n s
r o l e s , mixed wi th a m a s t e r s i n
o r g a n i z a t i o n a l
deve lopmen t .< / c o n t e x t>
< / i n s t a n c e>
We performed several tests, with different N-
Gram models, and different context sizes. Some of
these results for the trial corpus are given in table 4.
Table 4: Results obtained with Microsoft Web N-gram
Service, on the trial corpus
Size of left context Size of right context Score
0 3 0.362
3 0 0.358
2 2 0.365
3 3 0.358
4 4 0.370
For the evaluation, this system was our second
run, with the parameters that obtained the best scores
on the training corpus (contexts of 4 words to the
left and to the right). This method obtained a 0.370
score on the trial corpus and a 0.396 score on the test
corpus.9
7 Combination of methods
As each method seemed to have its own benefits, we
tried to combine them using SVMRank 10(Joachims,
2006). The output of each system is converted into
a feature file. For example, the output of the Simple
English Wikipedia based system begins with:
1 bright 475 1
1 intelligent 206 2
1 smart 201 3
1 clever 141 4
2 light 3241 1
2 clear 707 2
9This result is different from the official one, because an
incorrect file was submitted at the time.
10http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html
2 bright 475 3
2 luminous 14 4
2 well-lit 0 5
The first column represent the instance id, the sec-
ond one the considered substitute, the third one the
feature (in this case, the frequency of the substitute
in the Simple English Wikipedia), and the last one,
the substitute rank according to this method. Then,
we combined these files to include all features (after
basic query-wise feature scaling). For example, the
training file begins with:
1 qid:1 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
2 qid:1 2:-0.00485010755325339
3:-0.0213467053270483 #clever
3 qid:1 2:-0.00462903653787422
3:0.092640777900771 #smart
4 qid:1 2:-0.00361947890097599
3:0.0489145618699556 #bright
1 qid:4 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
The first column gives the gold standard rank for
the substitute (in training phase), the second one the
instance id, and then feature ids and values for each
substitute. Default parameters were used.
We used the division of the trial corpus into a
training corpus and a development corpus. Table 5
gives some examples of scores obtained by combin-
ing two methods. The scores are not exactly those
presented earlier, since they correspond to a part of
the trial corpus only. Even though some improve-
ment can be obtained by this combination, it was
quite small, and so we did not use it for the evalua-
tion.
Table 5: Results obtained with combination of methods
with SVMRank, on the trial corpus
Simple English Microsoft
SVM
Wikipedia NGrams
0.352 0.352 0.354
491
8 Conclusion
In this paper, we present several systems developed
for the English Lexical Simplification task of Se-
mEval 2012. The best results are obtained using fre-
quencies from the Simple English Wikipedia. We
found the task quite hard to solve, since none of
our experiments significantly outperforms the Sim-
ple Frequency baseline. On the trial corpus, our
system based upon the Simple English Wikipedia
achieved a score of 0.381 (below the 0.399 base-
line score); on the test corpus, we achieved a score
of 0.465 with the Simple English Wikipedia system
while the baseline achieved a score of 0.471 score.
All our systems using contextual information did not
achieve high scores.
References
Thorsten Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proc. of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplification.
In Proc. of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montre?al, Canada.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A Monolingual Tree-based Translation Model
for Sentence Simplification. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1353?1361.
492
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47?56,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Syntactic Sentence Simplification for French
Laetitia Brouwers
Aspirante FNRS
CENTAL, IL&C
UCLouvain
Belgium
Delphine Bernhard
LiLPa
Universit?e de Strasbourg
France
Anne-Laure Ligozat
LIMSI-CNRS
ENSIIE
France
Thomas Franc?ois
CENTAL, IL&C
UCLouvain
Belgium
Abstract
This paper presents a method for the syn-
tactic simplification of French texts. Syn-
tactic simplification aims at making texts
easier to understand by simplifying com-
plex syntactic structures that hinder read-
ing. Our approach is based on the study
of two parallel corpora (encyclopaedia ar-
ticles and tales). It aims to identify the lin-
guistic phenomena involved in the manual
simplification of French texts and organ-
ise them within a typology. We then pro-
pose a syntactic simplification system that
relies on this typology to generate simpli-
fied sentences. The module starts by gen-
erating all possible variants before select-
ing the best subset. The evaluation shows
that about 80% of the simplified sentences
produced by our system are accurate.
1 Introduction
In most of our daily activities, the ability to read
quickly and effectively is an undeniable asset,
even often a prerequisite (Willms, 2003). How-
ever, a sizeable part of the population is not able
to deal adequately with the texts they face. For
instance, Richard et al. (1993) reported that, in 92
applications for an unemployment allowance filled
by people with a low level of education, about half
of the required information was missing (some of
which was crucial for the processing of the appli-
cation), mainly because of comprehension issues.
These comprehension issues are often related
to the complexity of texts, particularly at the lex-
ical and syntactic levels. These two factors are
known to be important causes of reading difficul-
ties (Chall and Dale, 1995), especially for young
children, learners of a foreign language or people
with language impairments or intellectual disabil-
ities.
In this context, automatic text simplification
(ATS) appears as a means to help various peo-
ple access more easily the contents of the written
documents. ATS is an application domain of Nat-
ural Language Processing (NLP) aiming at mak-
ing texts more accessible for readers, while en-
suring the integrity of their contents and structure.
Among the investigations in this regard are those
of Caroll et al. (1999), Inui et al. (2003) and, more
recently, of Rello et al. (2013), who developed
tools to produce more accessible texts for people
with language disabilities such as aphasia, deaf-
ness or dyslexia. In the FIRST project, Barbu et
al. (2013) and Evans and Or?asan (2013) imple-
mented a simplification system for patients with
autism, who may also struggle to understand diffi-
cult texts.
However, reading assistance is not only in-
tended for readers with disabilities, but also for
those who learn a new language (as first or sec-
ond language). De Belder and Moens (2010) fo-
cused on ATS for native English schoolchildren,
while Siddharthan (2006), Petersen and Ostendorf
(2007) and Medero and Ostendorf (2011) focused
on learners of a second language. Williams and
Reiter (2008), Aluisio et al. (2008) and Gasperin
et al. (2009) addressed ATS for illiterate adults.
Most of these studies are dealing with the En-
glish language, with the exception of some work
in Japanese (Inui et al., 2003), Spanish (Saggion
et al., 2011; Bott et al., 2012), Portuguese (Alu??sio
et al., 2008) and French (Seretan, 2012).
ATS was also used as a preprocessing step to
increase the effectiveness of subsequent NLP op-
erations on texts. Chandrasekar et al. (1996)
first considered that long and complex sentences
were an obstacle for automatic parsing or machine
translation and they showed that a prior simplifi-
cation may result in a better automatic analysis
of sentences. More recently, Heilman and Smith
(2010) showed that adding ATS in the context
47
of automatic question generation yields better re-
sults. Similarly, Lin and Wilbur (2007) and Jon-
nalagadda et al. (2009) optimized information ex-
traction from biomedical texts using ATS as a pre-
processing step.
In these studies, the simplifications carried out
are generally based on a set of manually defined
transformation rules. However, ATS may also
be solved with methods from machine transla-
tion and machine learning. This lead some re-
searchers (Zhu et al., 2010; Specia, 2010; Wood-
send and Lapata, 2011) to train statistical models
from comparable corpora of original and simpli-
fied texts. The data used in these studies are of-
ten based on the English Wikipedia (for original
texts) and the Simple English Wikipedia, a simpli-
fied version for children and non-native speakers
that currently comprises more than 100,000 arti-
cles. Similar resources exist for French, such as
Vikidia and Wikimini, but texts are far less nu-
merous in these as in their English counterpart.
Moreover, the original and simplified versions of
an article are not strictly parallel, which further
complicates machine learning. This is why, so far,
there was no attempt to adapt this machine learn-
ing methodology to French. The only previous
work on French, to our knowledge, is that of Sere-
tan (2012), which analysed a corpus of newspa-
pers to semi-automatically detect complex struc-
tures that has to be simplified. However, her sys-
tem of rules has not been implemented and evalu-
ated.
In this paper, we aim to further investigate the
issue of syntactic simplification for French. We
assume a midway point between the two main ten-
dencies in the field. We use parallel corpora sim-
ilar to those used in machine learning approaches
and analyse it to manually define a set of simpli-
fication rules. We have also implemented the syn-
tactic part of our typology through a simplification
system. It is based on the technique of overgen-
eration, which consists in generating all possible
simplified variants of a sentence, and then on the
selection of the best subset of variants for a given
text with the optimization technique known as in-
teger linear programming (ILP). ILP allows us to
specify a set of constraints that regulate the selec-
tion of the output by the syntactic simplification
system. This method has already been applied to
ATS in English by Belder and Moens (2010) and
Woodsend and Lapata (2011).
To conclude, the contributions of this paper are:
(1) a first corpus-based study of simplification pro-
cesses in French that relies on a corpus of parallel
sentences, (2) the organization of this study?s re-
sults in what might be the first typology of sim-
plification for French based on a corpus analysis
of original and simplified texts; (3) two new crite-
ria to select the best subset of simplified sentences
among the set of variants, namely the spelling list
of Catach (1985) and the use of keywords, and
finally (4) a syntactic simplification system for
French, a language with little resources as regards
text simplification.
In the next sections, we first present the cor-
pora building process (Section 2.1) and describe
a general typology of simplification derived from
our corpora (Section 2.2). Then, we present the
system based on the syntactic part of the typol-
ogy, which operates in two steps: overgeneration
of all possible simplified sentences (Section 2.3.1)
and selection of the best subset of candidates us-
ing readability criteria (Section 2.3.2) and ILP.
Finally, we evaluate the quality of the syntacti-
cally simplified sentences as regards grammatical-
ity, before performing some error analysis (Sec-
tion 3).
2 Methodology
2.1 Corpus Description
We based our typology of simplification rules on
the analysis of two corpora. More specifically,
since our aim is to identify and classify the var-
ious strategies used to transform a complex sen-
tence into a more simple one, the corpora had to
include parallel sentences. The reason why we
analysed two corpora is to determine whether dif-
ferent genres of texts lead to different simplifica-
tion strategies. In this study, we focused on the
analysis of informative and narrative texts. The in-
formative corpus comprises encyclopaedia articles
from Wikipedia
1
and Vikidia
2
. For the narrative
texts, we used three classic tales by Perrault, Mau-
passant and Daudet and their simplified versions
for learners of French as a foreign language.
To collect the first of our parallel corpora, we
used the MediaWiki API to retrieve Wikipedia
and Vikidia articles with the same title. The
1
http://fr.wikipedia.org
2
This site is intended for young people from eight to thirteen years and gathers more
accessible articles than Wikipedia, both in terms of language and content. It is available at
the address http://fr.vikidia.org
48
WikiExtractor
3
was then applied to the articles
to discard the wiki syntax and only keep the raw
texts. This corpus comprises 13,638 texts (7,460
from Vikidia and only 6,178 from Wikipedia,
since some Vikidia articles had no counterpart in
Wikipedia).
These articles were subsequently processed to
identify parallel sentences (Wikipedia sentence
with a simplified equivalent in Vikidia). The align-
ment has been made partly manually and partly
automatically with the monolingual alignment al-
gorithm described in Nelken and Shieber (2006),
which relies on a cosine similarity between sen-
tence vectors weighted with the tf-idf. This pro-
gram outputs alignments between sentences, along
with a confidence score. Among these files,
twenty articles or excerpts from Wikipedia were
selected along with their equivalent in Vikidia.
This amounts to 72 sentences for the former and
80 sentences for the latter.
The second corpus is composed of 16 narra-
tive texts, and more specifically tales, by Perrault,
Maupassant, and Daudet. We used tales since
their simplified version was closer to the origi-
nal than those of longer novels, which made the
sentence alignment simpler. The simplified ver-
sions of these tales were found in two collections
intended to learners of French as a foreign lan-
guage (FFL): ?Hachette - Lire en franc?ais facile?
and ?De Boeck - Lire et s?entrainer?. Their level
of difficulty ranges from A1 (Daudet) to B1 (Mau-
passant) on the CEFR scale (Council of Europe,
2001), with Perrault being A2. The texts were dig-
itized by OCR processing and manually aligned,
by two annotators, with an adjudication phase for
the disagreement cases. In this corpus, we anal-
ysed 83 original sentences and their correspond-
ing 98 simplified versions, which gives us a size
roughly similar to the Wikipedia-Vikidia corpus.
The two corpora created are relevant for a man-
ual analysis, as done in the next section, but they
are too small for automatic processing. We plan
to implement a method to align automatically the
narrative texts in the near future and thus be able
to collect a larger corpus.
2.2 Simplification Typology
The observations carried out on these two cor-
pora have made it possible to establish a typol-
ogy organised according to three main linguistic
3
http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor
levels of transformation: lexical, discursive and
syntactic, which can be further divided into sub-
categories. It is worth mentioning that in previ-
ous work, simplification is commonly regarded as
pertaining to two categories of phenomena: lexi-
cal and syntactic (Carroll et al., 1999; Inui et al.,
2003; De Belder and Moens, 2010). Little atten-
tion has been paid to discourse in the area of auto-
matic simplification (Siddharthan, 2006).
The typology is summarized in Table 1. As
regards the lexicon, the phenomena we observed
involve four types of substitution. First, dif-
ficult terms can be replaced by a synonym or
an hypernym perceived as simpler. Second,
some anaphoric expressions, considered simpler
or more explicit, are preferred to their counter-
parts in the original texts. For example, in our
three tales, simplified nominal anaphora are regu-
larly used instead of pronominal anaphora. Third,
rather than using synonymy, the authors of the
simplified texts sometimes replace difficult words
with a definition or an explanatory paraphrase. Fi-
nally, in the particular case where the original texts
contain concepts in a foreign language, these non-
French terms are translated.
At the discourse level, the authors of simple
texts pay particular attention to the organization
of the information which has to be clear and con-
cise. To this end, clauses may be interchanged
to ensure a better presentation of the information.
In addition, information of secondary importance
can be removed while explanations or examples
are added for clarity. These two phenomena can
appear to be contradictory (deletion and addition),
but they actually operate in a common goal: make
the main information more comprehensible. Par-
ticular attention is also placed on the coherence
and cohesion of the text: Authors tend to explain
the pronouns and explicit the relations between
sentences. The last observed strategy is that im-
personal structures are often personalized.
Finally, at the syntactic level, five types of
changes are observed: tense modification, dele-
tion, modification, splitting and grouping. The last
two types can be considered together since they
are two opposite phenomena.
? First, the tenses used in the simplified ver-
sions are more common and less literary than
those used in the original texts. Thus, the
present and present perfect are preferred to
the simple past, imperfect and past perfect.
49
Lexicon Discourse Syntax
Translation Reorganisation Tense
Anaphoric synonyms Addition Modification
Definition and paraphrase Deletion Grouping
Synonym or hypernym Coherence and cohesion Deletion
Personalisation Splitting
Table 1: Typology of simplifications
? Secondary or redundant information, that is
generally considered removable at the syn-
tactic level, is not included in the simplified
texts. Adverbial clauses, some adverbs and
adjectives and subordinate clauses, among
others, are omitted.
? When some complex structures are not
deleted, then they are often moved or modi-
fied for better clarity. Such structures include
negative sentences, impersonal structures, in-
direct speech and subordinate clauses.
? The authors sometimes choose to divide long
sentences or conversely merge several sen-
tences into one. The grouping of elements is
much less frequent than the division of sen-
tences. To split a sentence, the authors gen-
erally transform a secondary clause?be it rel-
ative, coordinate, subordinate, participial or
adjectival?into an independent clause.
This classification can be compared with that of
Medero et al. (2011) who propose three categories
? division, deletion and extension ? or that of Zhu
et al. (2010), which includes division, deletion, re-
organization, and substitution.
Among those transformations, some are hardly
implementable. This is the case when a change
requires the use of semantics. For example, noun
modifiers may sometimes be removed, but in other
cases, they are necessary. However, there are of-
ten neither typographical nor grammatical marked
differences between the two cases.
Another issue is that other syntactic changes
should be accompanied by lexical transforma-
tions, which are difficult to generalize. For exam-
ple, transforming a negative sentence into its af-
firmative equivalent requires to find a verb whose
affirmative form includes the meaning of the neg-
ative construction to replace.
There are also changes that are very particular
and require a manual rather than an automatic pro-
cessing of the text, in the sense that each case is
different (even if part of a more global rule). In
addition, they usually involve discourse or lexical
information and not just syntactic one.
Finally, the syntactic changes impacting other
parts of the text or concerning elements that de-
pend on another structure require more compre-
hensive changes to the text. Therefore, they are
also difficult to handle automatically. Thus, to
change the tense of a verb in a sentence, we must
ensure that the sequence of tenses agree in the en-
tire text.
2.3 The Sentence Simplification System
We used this typology to implement a system of
syntactic simplification for French sentences. The
simplification is performed as a two-step process.
First, for each sentence of the text, we generate the
set of all possible simplifications (overgeneration
step), and then, we select the best subset of sim-
plified sentences using several criteria.
2.3.1 Generation of the Simplified Sentences
The sentence overgeneration module is based on a
set of rules (19 rules), which rely both on morpho-
syntactic features of words and on syntactic rela-
tionships within sentences. To obtain this infor-
mation, the texts from our corpus are analyzed
by MELT
4
(Denis and Sagot, 2009) and Bon-
sai
5
(Candito et al., 2010) during a preprocessing
phase. As a result, texts are represented as syn-
tax trees that include the information necessary to
apply our simplification rules. After preprocess-
ing, the set of simplification rules is applied recur-
sively, one sentence at a time, until there is no fur-
ther structure to simplify. All simplified sentences
produced by a given rule are saved and gathered in
a set of variants.
The rules for syntactic simplification included
in our program are of three kinds: deletion rules
(12 rules), modification rules (3 rules) and split-
ting rules (4 rules). With regards to our typology, it
can be noted that two types of rules have not been
implemented: aggregation rules and tense simpli-
fication rules. The merging strategies (in which
several sentences are aggregated into one) were
4
https://gforge.inria.fr/projects/lingwb
5
http://alpage.inria.fr/statgram/frdep/fr_stat_dep_
parsing.html
50
not observed consistently in the corpus. Moreover,
aggregation rules could have come into conflict
with the deletion rules, since they have opposite
goals. Concerning tense aspects, some of them
are indeed more likely to be used than others in
Vikidia. However, this strategy has not been im-
plemented, since it implies global changes to the
text. For instance, when a simple past is replaced
by a present form, we must also adapt the verbs in
the surrounding context in accordance with tense
agreement. This requires to consider the whole
text, or at least the paragraph that contains the
modified verbal form, and be able to automatically
model tense agreement. Otherwise, we may alter
the coherence of the text and decrease its readabil-
ity.
This leaves us with 19 simplification rules.
6
To
apply them, the candidate structures for simplifi-
cation first need to be detected using regular ex-
pressions, via Tregex
7
(Levy and Andrew, 2006)
that allows the retrieval of elements and relation-
ships in a parse tree. In a second step, syntactic
trees in which a structure requires simplification
are modified according a set of operations imple-
mented through Tsurgeon.
The operations to perform depend on the type
of rules:
1. For the deletion cases, simply deleting all
the elements involved is sufficient (via the
delete operation in Tsurgeon). The ele-
ments affected by the deletion rules are ad-
verbial clauses, clauses between brackets,
some of the subordinate clauses, clauses be-
tween commas or introduced by words such
as ?comme? (as), ?voire? (even), ?soit? (ei-
ther), or similar terms, some adverbs and
agent prepositional phrases.
2. For the modification rules, several opera-
tions need to be combined: some terms are
dropped (via Tsurgeon delete), others
are moved (operation Tsurgeon move)
and specific labels are added to the text to sig-
nal a possible later processing. These labels
are useful for rules implying a modification
of tense or mode aspects for a verb. In such
cases, tags are added around the verb to indi-
cate that it needs to be modified. The mod-
ification is performed later, using the conju-
6
These 19 rules are available at http://cental.fltr.ucl.ac.be/team/
lbrouwers/rules.pdf
7
http://nlp.stanford.edu/software/tregex.shtml
gation system Verbiste.
8
For instance, to
change a passive into an active structure, not
only the voice must be changed, but some-
times also the person, so that the verb agrees
well with the agent that has become the new
subject. As regards modification rules, three
changes were implemented: moving adver-
bial clauses at the beginning of the sentence,
transforming passive structures into active
forms, and transforming a cleft to a non-cleft.
3. For the splitting rules, we followed a two-
step process. The subordinate clause is first
deleted, while the main clause is saved as a
new sentence. Resuming from the original
sentence, the main clause is, in turn, removed
to keep only the subordinate clause, which
must then be transformed into an independent
clause. In general, the verbal form of the sub-
ordinate clause needs to be altered in order to
operate as a main verb. Moreover, the pro-
noun governing the subordinated clause must
be substituted with its antecedent and the sub-
ject must be added when missing. In the case
of a relative clause, the relative pronoun thus
needs to be substituted by its antecedent, but
it is also important to consider the function
of the pronoun to find out where to insert this
antecedent. Our splitting rules apply when a
sentence includes either relative or participle
clauses, or clauses introduced by a colon or a
coordinating conjunction.
All these simplification rules are applied recur-
sively to a sentence until all possible alternatives
have been generated. Therefore, it is common to
have more than one simplified variant for a given
sentence. In this case, the next step consists in se-
lecting the most suitable variant to substitute the
original one. The selection process is described in
the next section.
2.3.2 Selection of the Best Simplifications
Given a set of candidate simplified sentences for a
text, our goal is to select the best subset of simpli-
fied sentences, that is to say the subset that max-
imizes some measure of readability. More pre-
cisely, text readability is measured through differ-
ent criteria, which are optimized with an Integer
Linear Programming (ILP) approach (Gillick and
Favre, 2009). These criteria are rather simple in
8
This software is available at the address http://sarrazip.com/dev/
verbiste.html under GNU general public license and was developed by Pierre Sar-
razin.
51
this approach. They are used to ensure that not
only the syntactic difficulty, but also the lexical
complexity decrease, since syntactic transforma-
tions may cause lexical or discursive alterations in
the text.
We considered four criteria to select the most
suitable sentences among the simplified set: sen-
tence length (in words) (h
w
), mean word length
(in characters) in the sentence (h
s
), familiarity of
the vocabulary (h
a
), and presence of some key-
words (h
c
). While the first two criteria are pretty
obvious as regards implementation, we measured
word familiarity based on Catach?s list (1985).
9
It
contains about 3,000 of the most frequent words
in French, whose spelling should be taught in pri-
ority to schoolchildren. The keywords were in this
study simply defined as any term occurring more
than once in the text.
These four criteria were combined using integer
linear programming as follows:
10
Maximize : h
w
+ h
s
+ h
a
+ h
c
Where : h
w
= wps?
?
i
s
i
?
?
i
l
w
i
s
i
h
s
= cpw?
?
i
l
w
i
s
i
?
?
i
l
c
i
s
i
h
a
= aps?
?
i
s
i
?
?
i
l
a
i
s
i
h
c
=
?
j
w
j
c
j
Subject to:
?
i?g
k
s
i
= 1 ?g
k
s
i
occ
ij
? c
j
?i, j
?
i
s
i
occ
ij
? c
j
?j
(1)
The above variables are defined as follows:
? wps: desired (mean) number of words per sentence
? cpw: desired (mean) number of characters per word
? aps: desired (mean) number of words absent from
Catach?s list for a sentence
? s
i
: binary variable indicating whether the sentence i
should be kept or not, with i varying from 1 to the total
number of simplified sentences
? c
j
: binary variable indicating whether keyword j is in
the simplification or not, with j varying from 1 to the
total number of keywords
? l
w
i
: length of sentence i in words
? l
c
i
: number of characters in sentence i
? l
a
i
: number of words absent from Catach?s list in sen-
tence i
? w
j
: number of occurrences of keyword j
? g
k
: set of simplified sentences obtained from the same
original sentence k
? occ
ij
: binary variable indicating the presence of term j
in sentence i
9
This list is available at the site http://www.ia93.ac-creteil.fr/spip/
spip.php?article2900.
10
We used an ILP module based on glpk that is available at the address http://
www.gnu.org/software/glpk/
wps, cpw and aps are constant parameters
whose values have been set respectively to 10, 5
and 2 for this study. 5 for cpw corresponds to the
value computed on the Vikidia corpus, while for
wps and aps, lower values than observed were
used to force simplification (respectively 10 in-
stead of 17 and 2 instead of 31).
However, these parameters may vary depending
on the context of use and the target population, as
they determine the level of difficulty of the simpli-
fied sentences obtained.
The constraints specify that (i) for each original
sentence, at most one simplification set should be
chosen, (ii) selecting a sentence means selecting
all the terms it contains and (iii) selecting a key-
word is only possible if it is present in at least one
selected sentence.
We illustrate this process with the Wikipedia ar-
ticle entitled Abel. This article contains 25 sen-
tences, from which 67 simplified sentences have
been generated. For the original sentence (1a) for
example, 5 variants were generated and simplifi-
cation (2) was selected by ILP.
(1a) Original sentence
11
: Ca??n, l?a??n?e, cultive la
terre et Abel ( ?etymologie : de l?h?ebreu

souffle

,

vapeur

,

existence pr?ecaire

) garde le trou-
peau.
(1b) Possible simplifications :
Simplification 1 : Ca??n, l?a??n?e, cultive la terre et
Abel garde le troupeau.
Simplification 2 : Ca??n, l?a??n?e, cultive la terre.
Abel garde le troupeau.
Simplification 3 : Ca??n, l?a??n?e, cultive la terre.
Simplification 4 : Abel garde le troupeau.
(...)
(1c) Selected simplification (2) : Ca??n, l?a??n?e,
cultive la terre. Abel garde le troupeau.
3 Evaluation
Syntactic simplification involves substantial
changes within the sentence both in terms of
contents and form. It is therefore important to
check that the application of a rule does not cause
errors that would make the sentences produced
unintelligible or ungrammatical. A manual
evaluation of our system?s efficiency to generate
correct simplified sentences was carried out on
our two corpora. In each of them, we selected a
set of texts that had not been previously used for
11
Ca??n, the eldest brother, farms the land and Abel (etymology : from Hebrew

breath

,

steam

,

fragile existence

) looks after the flock.
52
Sentence length Word length Word familiarity Keywords
Expected values 10 5 2 /
Original 19 6.1 11 5
Simplification 1 11 4.3 5 5
Simplification 2 5 4.6 2 5
Simplification 3 6 4.5 3 3
Simplification 4 4 4.7 2 2
Simplification 5 9 6.3 5 5
Simplification 6 12 7.3 8 2
Table 2: Values of the criteria in IPL for example (1).
the typological analysis, that is to say 9 articles
from Wikipedia (202 sentences) and two tales
from Perrault (176 sentences). In this evaluation,
all simplified sentences are considered, not only
those selected by ILP. The results are displayed in
Table 3 and discussed in Section 3.1. Two types
of errors can be detected: those resulting from
morpho-syntactic preprocessing, and particularly
the syntactic parser, and the simplification errors
per se, that we discuss in larger details in Section
3.2.
3.1 Quantitative Evaluation
Out of the 202 sentences selected in the informa-
tive corpus for evaluation, 113 (56%) have under-
gone one or more simplifications, which gives us
333 simplified variants. Our manual error analy-
sis revealed that 71 sentences (21%) contain some
errors, among which we can distinguish those due
to the preprocessing from those actually due to the
simplification system itself. It is worth mention-
ing that the first category amounts to 89% of the
errors, while the simplification rule are only re-
sponsible for 11% of those. We further refined the
analysis of the system?s errors distinguishing syn-
tactic from semantic errors.
The scores obtained on the narrative corpus are
slightly less good: out of the 369 simplified vari-
ants produced from the 154 original sentences, 77
(20.9%) contain errors. This value is very simi-
lar to the percentage for the informative corpus.
However, only 50.7% of these errors are due to the
preprocessing, while the remaining 49.3% come
from our rules. It means that our rules yield about
10.3% incorrect simplified variants compared to
2.7% for the informative corpus. Nevertheless,
these errors are caused mostly by 2 or 3 rules:
the deletion of subordinate clauses, of infinitives
or of clauses coordinated with a colon. This loss
in efficiency can be partly explained by the greater
presence of indirect speech in the tales that include
more non-removable subordinate clauses, difficult
to distinguish from removable clauses.
Globally, our results appear to be in line with
those of similar systems developed for English.
12
Yet, few studies have a methodology and evalu-
ation close enough to ours to allow comparison
of the results. Siddharthan (2006) assessed his
system output using three judges who found that
about 80% of the simplified sentences were gram-
matical, while 87% preserved the original mean-
ing. These results are very similar to our find-
ings that mixed the syntactic and discourse dimen-
sions. Drndarevi?c et al. (2013) also presented the
output of their system to human judges who esti-
mated that 60% of the sentences were grammatical
and that 70% preserved the initial meaning. These
scores appear lower than ours, but Drndarevi?c et
al. also used lexical rules, which means that their
error rate includes both grammatical and lexical
errors.
3.2 Error Analysis
As regards syntax, the structure of a sentence can
be modified so that it becomes grammatically in-
correct. Three simplification rules are concerned.
Deletion rules may cause this kind of problem, be-
cause they involve removing a part of the sentence,
considered as secondary. However, sometimes the
deleted element is essential, as in the case of the
removal of the referent of a pronoun. This type of
problem arises both with the deletion of a subor-
dinate clause or that of an infinitive clause. Dele-
tion rules are also subject to a different kind of
errors. During the reconstruction of the sentence
resulting from the subordinate clause, some con-
stituents, such as the subject, may not be properly
identified and will be misplaced in the new sen-
tence.
At the semantic level, the information conveyed
by the original sentence may be modified or even
removed. When an agent or an infinitive clause
12
We do not discuss French here, since no simplification system were found for French,
as explained previously.
53
Wikipedia-Vikidia corpus
nb. sent. % correct % preproc. errors % simplification errors
333 262 (78.7 %) 63 (18.9%)
8 (2.4 %)
syntax: semantics:
6 (1.8%) 2 (0.6%)
Narrative corpus
nb. sent. % correct % preproc. errors % simplification errors
369 292 (79.1 %) 39 (10.6%)
38 (10.3 %)
syntax: semantics:
20 (5.4%) 18 (4.9%)
Table 3: Performance of the simplification system on both corpora
are suppressed, the meaning of the sentence may
be disrupted or some of the content lost. For in-
stance, in the following sentence ? extracted from
the Wikipedia article abb?e (abbot) ? the infinitive
clause explaining the term is dropped:
(2a) C?est aussi depuis le XVIIIe si`ecle
le terme en usage pour d?esigner un clerc
s?eculier ayant au moins rec?u la ton-
sure.
13
(2b) C?est aussi depuis le XVIIIe si`ecle
le terme en usage.
To fix the errors identified above, our rules should
be refined and developed, with the addition of
better tools for sentence regeneration as well as
some exclusion criteria for the incorrect sentences
within the ILP module, as discussed in the next
section.
4 Perspectives and Conclusions
This article describes an automatic syntactic sim-
plification system for French intended for children
and language learners. It is based on a set of rules
defined after a corpus study, which also led to the
development of a typology of simplifications in
French. It would be easy to extend our typology to
other target users based on other appropriate cor-
pora, such as people with language disorders.
Our approach also uses the technique of over-
generation, which makes it possible to retain the
best set of simplifications based on readability cri-
teria. Note that among those employed, some had
not been considered previously and produce inter-
esting results. Finally, we showed that the perfor-
mance of our system is good (about 80 % of the
generated sentences are correct) and in line with
previous studies.
13
It is also the term in use since the 18th to refer to a secu-
lar cleric who, at least, received the tonsure.
The evaluation showed that the rules imple-
mented are more suitable for expository texts,
probably because they are more explicit, as style
there is of a minor importance. In addition, the
system set up was first tested on and therefore
adapted to Wikipedia. It was only subsequently
applied to narratives, that revealed new challenges,
especially concerning the deletion rules. The in-
formation provided in secondary clauses or com-
plements indeed seems most essential to under-
standing the story, especially when it comes to di-
rect or indirect speech. In order to comprehend
the differences in terms of efficiency and rules to
be applied between genres, it would be necessary
to extend our study to other texts collected in the
corpora.
We envision multiple perspectives to improve
our system. First, syntactic simplification could be
supplemented by lexical simplification, as is done
in some studies for English (Woodsend and Lap-
ata, 2011). Moreover, our error analysis has high-
lighted the need to add or repeat words when a
sentence is split. It would therefore be useful to
use a tool that manages references in order to im-
prove the quality of simplified text. In addition,
the sentence selection module could include addi-
tional selection criteria, based on the work done in
readability of French (Franc?ois and Fairon, 2012).
A final perspective of improvement would be to
make the rule system adapt to the target audience
and the genre of the texts. This would require as-
sessing the relevance of various transformations
and selection criteria of the best simplifications.
This perspective would also require assessing the
effectiveness of the rules by means of comprehen-
sion tests both on the original and simplified sen-
tences, which we plan to do.
54
References
S. Alu??sio, L. Specia, T. Pardo, E. Maziero, and
R. Fortes. 2008. Towards brazilian portuguese auto-
matic text simplification systems. In Proceedings of
the eighth ACM symposium on Document engineer-
ing, pages 240?248.
E. Barbu, P. de Las Lagunillas, M. Mart?n-Valdivia, and
L. Urena-L?opez. 2013. Open book: a tool for help-
ing asd users? semantic comprehension. NLP4ITA
2013, pages 11?19.
S. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012.
Can Spanish Be Simpler? LexSiS: Lexical Simpli-
fication for Spanish. In Proceedings of COLING
2012, pages 357?374.
M. Candito, B. Crabb?e, and P. Denis. 2010. Statisti-
cal french dependency parsing: treebank conversion
and first results. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 1840?1847.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying Text for
Language-Impaired Readers. In Proceedings of
EACL, pages 269?270.
N. Catach. 1985. Les listes orthographiques de base
du franc?ais. Nathan, Paris.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Cambridge.
R. Chandrasekar, C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of the 16th conference on Computa-
tional linguistics, pages 1041?1044.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University
of Cambridge.
J. De Belder and M.-F. Moens. 2010. Text Simplifica-
tion for Children. In Proceedings of the Workshop
on Accessible Search Systems.
P. Denis and B. Sagot. 2009. Coupling an annotated
corpus and a morphosyntactic lexicon for state-of-
the-art pos tagging with less human effort. In Pro-
ceedings of PACLIC.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic text simplification in
spanish: a comparative evaluation of complement-
ing modules. In Computational Linguistics and In-
telligent Text Processing, pages 488?500.
R. Evans and C. Or?asan. 2013. Annotating signs of
syntactic complexity to support sentence simplifica-
tion. In Text, Speech, and Dialogue, pages 92?104.
T. Franc?ois and C. Fairon. 2012. An ?AI readability?
formula for French as a foreign language. In Pro-
ceedings of EMNLP 2012, pages 466?477.
C. Gasperin, E. Maziero, L. Specia, T. Pardo, and
S. Aluisio. 2009. Natural language processing
for social inclusion: a text simplification architec-
ture for different literacy levels. Proceedings of
SEMISH-XXXVI Semin?ario Integrado de Software e
Hardware, pages 387?401.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
M. Heilman and N. A. Smith. 2010. Extracting Simpli-
fied Statements for Factual Question Generation. In
Proceedings of the 3rd Workshop on Question Gen-
eration.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second interna-
tional workshop on Paraphrasing, pages 9?16.
S. Jonnalagadda, L. Tari, J. Hakenberg, C. Baral, and
G. Gonzalez. 2009. Towards Effective Sentence
Simplification for Automatic Processing of Biomed-
ical Text. In Proceedings of NAACL-HLT 2009.
R. Levy and G. Andrew. 2006. Tregex and tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proceedings of LREC, pages 2231?2234.
L. Lin and W. J. Wilbur. 2007. Syntactic sentence
compression in the biomedical domain: facilitat-
ing access to related articles. Information Retrieval,
10(4):393?414, October.
J. Medero and M. Ostendorf. 2011. Identifying Tar-
gets for Syntactic Simplification. In Proceedings of
the SLaTE 2011 workshop.
R. Nelken and S.M. Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, pages 161?
168.
S. E. Petersen and M. Ostendorf. 2007. Text Simpli-
fication for Language Learners: A Corpus Analysis.
In Proceedings of SLaTE2007, pages 69?72.
L. Rello, C. Bayarri, A. G`orriz, R. Baeza-Yates,
S. Gupta, G. Kanvinde, H. Saggion, S. Bott, R. Car-
lini, and V. Topac. 2013. Dyswebxia 2.0!: more
accessible text for people with dyslexia. In Proceed-
ings of the 10th International Cross-Disciplinary
Conference on Web Accessibility, page 25.
J.F. Richard, J. Barcenilla, B. Brie, E. Charmet,
E. Clement, and P. Reynard. 1993. Le traite-
ment de documents administratifs par des popula-
tions de bas niveau de formation. Le Travail Hu-
main, 56(4):345?367.
H. Saggion, E. Mart??nez, E. Etayo, A. Anula, and
L. Bourg. 2011. Text simplification in simplext.
making text more accessible. Procesamiento del
lenguaje natural, 47:341?342.
V. Seretan. 2012. Acquisition of syntactic simplifica-
tion rules for french. In LREC, pages 4019?4026.
A. Siddharthan. 2006. Syntactic Simplification and
Text Cohesion. Research on Language & Computa-
tion, 4(1):77?109, jun.
L. Specia. 2010. Translating from Complex to Sim-
plified Sentences. In Proceedings of the 9th Inter-
national Conference on Computational Processing
of the Portuguese Language (Propor-2010)., pages
30?39.
S. Williams and E. Reiter. 2008. Generating basic
skills reports for low-skilled readers. Natural Lan-
guage Engineering, 14(4):495?525.
J.D. Willms. 2003. Literacy proficiency of youth:
Evidence of converging socioeconomic gradients.
International Journal of Educational Research,
55
39(3):247?252.
K. Woodsend and M. Lapata. 2011. Learning to
Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of
EMNLP, pages 409?420.
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of COLING 2010,
pages 1353?1361.
56
