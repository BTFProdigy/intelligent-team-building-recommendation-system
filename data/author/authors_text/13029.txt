Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Semantic Role Labeling
Anders Bjo?rkelund Love Hafdell Pierre Nugues
Department of Computer Science, Lund University
S-221 00 Lund, Sweden
fte04abj@student.lth.se
love hafdell@hotmail.com
Pierre.Nugues@cs.lth.se
Abstract
This paper describes our contribution to the
semantic role labeling task (SRL-only) of the
CoNLL-2009 shared task in the closed chal-
lenge (Hajic? et al, 2009). Our system con-
sists of a pipeline of independent, local clas-
sifiers that identify the predicate sense, the ar-
guments of the predicates, and the argument
labels. Using these local models, we carried
out a beam search to generate a pool of candi-
dates. We then reranked the candidates using
a joint learning approach that combines the lo-
cal models and proposition features.
To address the multilingual nature of the data,
we implemented a feature selection procedure
that systematically explored the feature space,
yielding significant gains over a standard set
of features. Our system achieved the second
best semantic score overall with an average la-
beled semantic F1 of 80.31. It obtained the
best F1 score on the Chinese and German data
and the second best one on English.
1 Introduction
In this paper, we describe a three-stage analysis ap-
proach that uses the output of a dependency parser
and identifies the arguments of the predicates in a
sentence. The first stage consists of a pipeline of
independent classifiers. We carried out the pred-
icate disambiguation with a set of greedy classi-
fiers, where we applied one classifier per predicate
lemma. We then used a beam search to identify
the arguments of each predicate and to label them,
yielding a pool of candidate propositions. The sec-
ond stage consists of a reranker that we applied to
the candidates using the local models and proposi-
tion features. We combined the score of the greedy
classifiers and the reranker in a third stage to select
the best candidate proposition. Figure 1 shows the
system architecture.
We evaluated our semantic parser on a set of seven
languages provided by the organizers of the CoNLL-
2009 shared task: Catalan and Spanish (Taule? et
al., 2008), Chinese (Palmer and Xue, 2009), Czech
(Hajic? et al, 2006), English (Surdeanu et al, 2008),
German (Burchardt et al, 2006), and Japanese
(Kawahara et al, 2002). Our system achieved an
average labeled semantic F1 of 80.31, which cor-
responded to the second best semantic score over-
all. After the official evaluation was completed, we
discovered a fault in the training procedure of the
reranker for Spanish. The revised average labeled
semantic F1 after correction was 80.80.
2 SRL Pipeline
The pipeline of classifiers consists of a predicate
disambiguation (PD) module, an argument identi-
fication module (AI), and an argument classifica-
tion (AC) module. Aside from the lack of a pred-
icate identification module, which was not needed,
as predicates were given, this architecture is identi-
cal to the one adopted by recent systems (Surdeanu
et al, 2008), as well as the general approach within
the field (Gildea and Jurafsky, 2002; Toutanova et
al., 2005).
We build all the classifiers using the L2-
regularized linear logistic regression from the LIB-
LINEAR package (Fan et al, 2008). The package
implementation makes models very fast to train and
43
N candidates
N candidates
Reranker
Local features + proposition features
Global model
Linear combination of models
Local classifier pipeline
Sense disambiguation
greedy search
Argument identification
beam search
Argument labeling
beam search
Reranked 
candidates
Figure 1: System architecture.
use for classification. Since models are logistic, they
produce an output in the form of probabilities that
we use later in the reranker (see Sect. 3).
2.1 Predicate Disambiguation
We carried out a disambiguation for all the lem-
mas that had multiple senses in the corpora and we
trained one classifier per lemma. We did not use the
predicate lexicons and we considered lemmas with a
unique observed sense as unambiguous.
English required a special processing as the sense
nomenclature overlapped between certain nominal
and verbal predicates. For instance, the nominal
predicate plan.01 and the verbal predicate plan.01
do not correspond to the same semantic frame.
Hence, we trained two classifiers for each lemma
plan that could be both a nominal and verbal predi-
cate.
Table 1: Feature sets for predicate disambiguation.
ca ch cz en ge sp
PredWord ? ? ?
PredPOS ? ?
PredDeprel ? ? ?
PredFeats ? ? ?
PredParentWord ? ? ? ? ?
PredParentPOS ? ? ?
PredParentFeats ? ?
DepSubCat ? ? ? ? ?
ChildDepSet ? ? ? ? ? ?
ChildWordSet ? ? ? ? ? ?
ChildPOSSet ? ? ? ? ?
2.2 Argument Identification and Classification
We implemented the argument identification and
classification as two separate stages, because it en-
abled us to apply and optimize different feature sets
in each step. Arguments were identified by means
of a binary classifier. No pruning was done, each
word in the sentence was considered as a potential
argument to all predicates of the same sentence.
Arguments were then labeled using a multiclass
classifier; each class corresponding to a certain la-
bel. We did not apply any special processing with
multiple dependencies in Czech and Japanese. In-
stead, we concatenated the composite labels (i.e.
double edge) to form unique labels (i.e. single edge)
having their own class.
2.3 Identification and Classification Features
For the English corpus, we used two sets of features
for the nominal and the verbal predicates both in the
AI and AC steps. This allowed us to create different
classifiers for different kinds of predicates. We ex-
tended this approach with a default classifier catch-
ing predicates that were wrongly tagged by the POS
tagger. For both steps, we used the union of the two
feature sets for this catch-all class.
We wanted to employ this procedure with the two
other languages, Czech and Japanese, where predi-
cates had more than one POS type. As feature selec-
tion (See Sect. 2.4) took longer than expected, par-
ticularly in Czech due to the size of the corpus and
the annotation, we had to abandon this idea and we
trained a single classifier for all POS tags in the AI
and AC steps.
For each data set, we extracted sets of features
similar to the ones described by Johansson and
Nugues (2008). We used a total of 32 features that
we denote with the prefixes: Pred-, PredParent-,
Arg-, Left-, Right-, LeftSibling-, and RightSibling-
for, respectively, the predicate, the parent of the
predicate, the argument, the leftmost and rightmost
dependents of the argument, and the left and right
44
Table 2: Feature sets for argument identification and classification.
Argument identification Argument classification
ca ch cz en ge ja sp ca ch cz en ge ja sp
PredWord ? N ?
PredPOS N ? ? V ?
PredLemma N ? ? ? ? N,V ? ?
PredDeprel
Sense ? ? V ? ? ? ? N,V ? ? ?
PredFeats ? ? ?
PredParentWord V ? V ?
PredParentPOS V V ?
PredParentFeats ?
DepSubCat ? ?
ChildDepSet ? ? ? ? V ? ? ?
ChildWordSet N ? ?
ChildPOSSet ? ? N ?
ArgWord ? ? N,V ? ? ? ? ? ? N,V ? ? ?
ArgPOS ? ? N,V ? ? ? ? ? ? N,V ?
ArgFeats ? ? ? ? ?
ArgDeprel ? ? ? V ? ? ? ? ? V ? ?
DeprelPath ? ? ? N,V ? ? ? ? ? V ?
POSPath ? ? ? N,V ? ? ? ? V ? ?
Position ? N,V ? ? ? ? N,V ? ?
LeftWord ? ? ? ? N ? ?
LeftPOS ? ? V
LeftFeats ? ? ?
RightWord ? N ? ? N,V ?
RightPOS N ? ? N,V ?
RightFeats ? ?
LeftSiblingWord ? ? ? ? N ?
LeftSiblingPOS ? ? ? ? N,V ?
LeftSiblingFeats ? ? ?
RightSiblingWord ? ? V ? ? ? ? ? ?
RightSiblingPOS ? ?
RightSiblingFeats ?
sibling of the argument. The suffix of these names
corresponds to the column name of the CoNLL for-
mat, except Word which corresponds to the Form
column. Additional features are:
? Sense: the value of the Pred column, e.g.
plan.01.
? Position: the position of the argument with re-
spect to the predicate, i.e. before, on, or after.
? DepSubCat: the subcategorization frame of the
predicate, e.g. OBJ+OPRD+SUB.
? DeprelPath: the path from predicate to argu-
ment concatenating dependency labels with the
direction of the edge, e.g. OBJ?OPRD?SUB?.
? POSPath: same as DeprelPath, but depen-
dency labels are exchanged for POS tags, e.g.
NN?NNS?NNP?.
? ChildDepSet: the set of dependency labels of
the children of the predicate, e.g. {OBJ, SUB}.
? ChildPOSSet: the set of POS tags of the chil-
dren of the predicate, e.g. {NN, NNS}.
? ChildWordSet: the set of words (Form) of the
children of the predicate, e.g. {fish, me}.
45
2.4 Feature Selection
We selected the feature sets using a greedy forward
procedure. We first built a set of single features and,
to improve the separability of our linear classifiers,
we paired features to build bigrams. We searched
the space of feature bigrams using the same proce-
dure. See Johansson (2008, page 83), for a com-
plete description. We intended to carry out a cross-
validation search. Due to the lack of time, we re-
sorted to using 80% of the training set for training
and 20% for evaluating the features. Table 2 con-
tains the complete list of single features we used.
We omitted the feature bigrams.
Feature selection turned out to be a massive task.
It took us three to four weeks searching the feature
spaces, yet in most cases we were forced to interrupt
the selection process after a few bigram features in
order to have our system ready in time. This means
that our feature sets can probably be further opti-
mized.
When the training data was initially released,
we used the exact feature set from Johansson and
Nugues (2008) to compute baseline results on the
development set for all the languages. After feature
selection, we observed an increase in labeled seman-
tic F1 close to 10% in most languages.
2.5 Applying Beam Search
The AI module proceeds left to right considering
each word as an argument of the current predicate.
The current partial propositions are scored by com-
puting the product of the probabilities of all the
words considered so far. After each word, the cur-
rent pool of partial candidates is reduced to the beam
size, k, and at the end of the sentence, the top k scor-
ing propositions are passed on to the AC module.
Given k unlabeled propositions, the AC module
applies a beam search on each of these propositions
independently. This is done in a similar manner,
proceeding from left to right among the identified
arguments, keeping the l best labelings in its beam,
and returning the top l propositions, when all iden-
tified arguments have been processed. This yields
n = k ? l complete propositions, unless one of the
unlabeled propositions has zero arguments, in which
case we have n = (k ? 1) ? l + 1.
The probability of a labeled proposition according
to the local pipeline is given by PLocal = PAI ?
PAC , where PAI and PAC is the output probability
from the AI and AC modules, respectively. In the
case of empty propositions, PAC was set to 1.
3 Global Reranker
We implemented a global reranker following
Toutanova et al (2005). To generate training ex-
amples for the reranker, we trained m AI and AC
classifiers by partitioning the training set in m parts
and using m ? 1 of these parts for each AI and AC
classifier, respectively.
We applied these AI and AC classifiers on the part
of the corpus they were not trained on and we then
generated the top n propositions for each predicate.
We ran the CoNLL evaluation script on the proposi-
tions and we marked the top scoring one(s) as pos-
itive. We marked the others negative. If the correct
proposition was not in the pool of candidates, we
added it as an extra positive example. We used these
positive and negative examples as training data for
the global reranker.
3.1 Reranker Features
We used all the features from the local pipeline for
all the languages. We built a vector where the AI
features were prefixed with AI- and the AC features
prefixed with lab?, where lab was any of the argu-
ment labels.
We added one proposition feature to the concate-
nation of local features, namely the sequence of core
argument labels, e.g. A0+plan.01+A1. In Catalan
and Spanish, we considered all the labels prefixed by
arg0, arg1, arg2, or arg3 as core labels. In Chinese
and English, we considered only the labels A0, A1,
A2, A3, and A4. In Czech, German, and Japanese,
we considered all the labels as core labels.
Hence, the total size of the reranker vector space
is |AI| + |L| ? |AC| + |G|, where |AI| and |AC|
denotes the size of the AI and AC vector spaces, re-
spectively, |L| corresponds to the number of labels,
and |G| is the size of additional global features.
We ran experiments with the grammatical
voice that we included in the string represent-
ing the sequence of core argument labels, e.g.
A1+plan.01/Passive+A0. The voice was derived by
hand-crafted rules in Catalan, English, German, and
46
Spanish, and given in the Feat column in Czech.
However, we did not notice any significant gain in
performance. The hand-crafted rules use lexical
forms and dependencies, which we believe classi-
fiers are able to derive themselves using the local
model features. This also applies to Czech, as Pred-
Feats was a feature used in the local pipeline, both
in the AI and AC steps.
3.2 Weighting the Models
In Sect. 2.5, we described how the pipeline was used
to generate the top n propositions, each with its own
local probability PLocal. Similar to softmax, we nor-
malized these local probabilities by dividing each of
them by their total sum. We denote this normalized
probability by P ?Local. The reranker gives a proba-
bility on the complete proposition, PReranker. We
weighted these probabilities and chose the proposi-
tion maximizing PF inal = (P ?Local)? ? PReranker.
This is equivalent to a linear combination of the log
probabilities.
3.3 Parameters Used
For the submission to the CoNLL 2009 Shared Task,
we set the beam widths to k = l = 4, yielding can-
didate pools of size n = 13 or n = 16 (See Sec-
tion 2.5). We used m = 5 for training the reranker
and ? = 1 for combining the local model with the
reranker.
4 Results
Our system achieved the second best semantic score,
all tasks, with an average labeled semantic F1 of
80.31. It obtained the best F1 score on the Chinese
and German data and the second best on English.
Our system also reached the third rank in the out-of-
domain data, all tasks, with a labeled semantic F1 of
74.38. Post-evaluation, we discovered a bug in the
Spanish reranker model causing the poor results in
this language. After correcting this, we could reach
a labeled semantic F1 of 79.91 in Spanish. Table 3
shows our official results in the shared task as well
as the post-evaluation update.
We also compared the performance of a greedy
strategy with that of a global model. Table 4 shows
these figures with post-evaluation figures in Spanish.
Table 5 shows the training time, parsing time, and
the parsing speed in predicates per second. These
figures correspond to complete execution time of
parsing, including loading models into memory, i.e.
a constant overhead, that explains the low parsing
speed in German. We implemented our system to
be flexible for easy debugging and testing various
ideas. Optimizing the implementation would reduce
execution times significantly.
Table 3: Summary of submitted results: closed challenge,
semantic F1. * denotes the post-evaluation results ob-
tained for Spanish after a bug fix.
Unlabeled Labeled
Catalan 93.60 80.01
Chinese 84.76 78.60
Czech 92.63 85.41
English 91.17 85.63
German 92.13 79.71
Japanese 83.45 76.30
Spanish 92.69 76.52
Spanish* 93.76 79.91
Average 90.06 80.31
Average* 90.21 80.80
Table 4: Improvement of reranker. * denotes the post-
evaluation results obtained for Spanish after a bug fix.
Greedy Reranker Gain
Catalan 79.54 80.01 0.47
Chinese 77.84 78.60 0.76
Czech 84.99 85.41 0.42
English 84.44 85.63 1.19
German 79.01 79.71 0.70
Japanese 75.61 76.30 0.69
Spanish 79.28 76.52 -2.76
Spanish* 79.28 79.91 0.63
Average 80.10 80.31 0.21
Average* 80.10 80.80 0.70
5 Conclusion
We have built and described a streamlined and ef-
fective semantic role labeler that did not use any
lexicons or complex linguistic features. We used a
generic feature selection procedure that keeps lan-
guage adaptation minimal and delivers a relatively
even performance across the data sets. The system is
47
Table 5: Summary of training and parsing times on an Apple Mac Pro, 3.2 GHz.
Training Parsing (Greedy) Speed (Greedy) Parsing (Reranker) Speed (Reranker)
(min) (min:sec) (pred/sec) (min:sec) (pred/sec)
Catalan 46 1:10 71 1:21 62
Chinese 139 2:35 79 3:45 55
Czech 299 18:47 40 33:49 22
English 421 6:25 27 8:51 20
German 15 0:21 26 0:22 25
Japanese 48 0:37 84 1:02 50
Spanish 51 1:15 69 1:47 48
robust and can handle incorrect syntactic parse trees
with a good level of immunity. While input parse
trees in Chinese and German had a labeled syntac-
tic accuracy of 78.46 (Hajic? et al, 2009), we could
reach a labeled semantic F1 of 78.60 and 79.71 in
these languages. We also implemented an efficient
global reranker in all languages yielding a 0.7 av-
erage increase in labeled semantic F1. The reranker
step, however, comes at the expense of parsing times
increased by factors ranging from 1.04 to 1.82.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University, December 5.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
48
Coling 2010: Demonstration Volume, pages 33?36,
Beijing, August 2010
A High-Performance Syntactic and Semantic Dependency Parser
Anders Bjo?rkelund? Bernd Bohnet? Love Hafdell? Pierre Nugues?
?Department of Computer science ?Institute for Natural Language Processing
Lund University University of Stuttgart
anders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.de
love.hafdell@cs.lth.se
pierre.nugues@cs.lth.se
Abstract
This demonstration presents a high-
performance syntactic and semantic de-
pendency parser. The system consists of a
pipeline of modules that carry out the to-
kenization, lemmatization, part-of-speech
tagging, dependency parsing, and seman-
tic role labeling of a sentence. The sys-
tem?s two main components draw on im-
proved versions of a state-of-the-art de-
pendency parser (Bohnet, 2009) and se-
mantic role labeler (Bjo?rkelund et al,
2009) developed independently by the au-
thors.
The system takes a sentence as input and
produces a syntactic and semantic anno-
tation using the CoNLL 2009 format. The
processing time needed for a sentence typ-
ically ranges from 10 to 1000 millisec-
onds. The predicate?argument structures
in the final output are visualized in the
form of segments, which are more intu-
itive for a user.
1 Motivation and Overview
Semantic analyzers consist of processing
pipelines to tokenize, lemmatize, tag, and parse
sentences, where all the steps are crucial to their
overall performance. In practice, however, while
code of dependency parsers and semantic role
labelers is available, few systems can be run as
standalone applications and even fewer with a
processing time per sentence that would allow a
?Authors are listed in alphabetical order.
user interaction, i.e. a system response ranging
from 100 to 1000 milliseconds.
This demonstration is a practical semantic
parser that takes an English sentence as input
and produces syntactic and semantic dependency
graphs using the CoNLL 2009 format. It builds
on lemmatization and POS tagging preprocessing
steps, as well as on two systems, one dealing with
syntax and the other with semantic dependencies
that reported respectively state-of-the-art results
in the CoNLL 2009 shared task (Bohnet, 2009;
Bjo?rkelund et al, 2009). The complete system ar-
chitecture is shown in Fig. 1.
The dependency parser is based on Carreras?s
algorithm (Carreras, 2007) and second order span-
ning trees. The parser is trained with the margin
infused relaxed algorithm (MIRA) (McDonald et
al., 2005) and combined with a hash kernel (Shi et
al., 2009). In combination with the system?s lem-
matizer and POS tagger, this parser achieves an
average labeled attachment score (LAS) of 89.88
when trained and tested on the English corpus
of the CoNLL 2009 shared task (Surdeanu et al,
2008).
The semantic role labeler (SRL) consists of a
pipeline of independent, local classifiers that iden-
tify the predicates, their senses, the arguments of
the predicates, and the argument labels. The SRL
module achieves an average labeled semantic F1
of 80.90 when trained and tested on the English
corpus of CoNLL 2009 and combined with the
system?s preprocessing steps and parser.
2 The Demonstration
The demonstration runs as a web application and
is available from a server located at http://
33
	

	


	
	
	


	
	

Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 928?939, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generating Non-Projective Word Order in Statistical Linearization
Bernd Bohnet Anders Bjo?rkelund Jonas Kuhn Wolfgang Seeker Sina Zarrie?
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.de
Abstract
We propose a technique to generate non-
projective word orders in an efficient statisti-
cal linearization system. Our approach pre-
dicts liftings of edges in an unordered syntac-
tic tree by means of a classifier, and uses a
projective algorithm for tree linearization. We
obtain statistically significant improvements
on six typologically different languages: En-
glish, German, Dutch, Danish, Hungarian, and
Czech.
1 Introduction
There is a growing interest in language-independent
data-driven approaches to natural language genera-
tion (NLG). An important subtask of NLG is sur-
face realization, which was recently addressed in the
2011 Shared Task on Surface Realisation (Belz et
al., 2011). Here, the input is a linguistic representa-
tion, such as a syntactic dependency tree lacking all
precedence information, and the task is to determine
a natural, coherent linearization of the words.
The standard data-driven approach is to traverse
the dependency tree deciding locally at each node on
the relative order of the head and its children. The
shared task results have proven this approach to be
both effective and efficient when applied to English.
It is what federal support should try to achieve
SBJ
ROOT OBJ
NMOD SBJ
PRD
VC OPRD IM
Figure 1: A non-projective example from the CoNLL
2009 Shared Task data set for parsing (Hajic? et al 2009).
However, the approach can only generate pro-
jective word orders (which can be drawn with-
out any crossing edges). Figure 1 shows a non-
projective word order: the edge connecting the ex-
tracted wh-pronoun with its head crosses another
edge. Once what has been ordered relative to
achieve, there are no ways of inserting intervening
material. In this case, only ungrammatical lineariza-
tions can be produced from the unordered input tree:
(1) a. *It is federal support should try to what achieve
b. *It is federal support should try to achieve what
c. *It is try to achieve what federal support should
Although rather infrequent in English, non-
projective word orders are quite common in lan-
guages with a less restrictive word order. In these
languages, it is often possible to find a grammati-
cally correct projective linearization for a given in-
put tree, but discourse coherence, information struc-
ture, and stylistic factors will often make speak-
ers prefer some non-projective word order.1 Figure
2 shows an object fronting example from German
where the edge between the subject and the finite
verb crosses the edge between the object and the full
verb. Various other constructions, such as extraposi-
tion of (relative) clauses or scrambling, can lead to
non-projectivity. In languages where word order is
driven to an even larger degree by information struc-
ture, such as Czech and Hungarian, non-projectivity
can likewise result from various ordering decisions.
These phenomena have been studied extensively in
1A categorization of non-projective edges in the Prague
Dependency Treebank (Bo?hmova? et al 2000) is presented in
Hajic?ova? et al(2004).
928
the linguistic literature, and for certain languages,
work on rule-based generation has addressed certain
aspects of the problem.
Das Mandat will er zuru?ckgeben .
the.ACC mandate.ACC want.3SG he.NOM return.INF .
NK
OA#?
SB OC
?
?He wants to return the mandate.?
Figure 2: German object fronting with complex verb in-
troducing a non-projective edge.
In this paper, we aim for a general data-driven ap-
proach that can deal with various causes for non-
projectivity and will work for typologically dif-
ferent languages. Our technique is inspired by
work in data-driven multilingual parsing, where
non-projectivity has received considerable attention.
In pseudo-projective parsing (Kahane et al 1998;
Nivre and Nilsson, 2005), the parsing algorithm is
restricted to projective structures, but the issue is
side-stepped by converting non-projective structures
to projective ones prior to training and application,
and then restoring the original structure afterwards.
Similarly, we split the linearization task in two
stages: initially, the input tree is modified by lifting
certain edges in such a way that new orderings be-
come possible even under a projectivity constraint;
the second stage is the original, projective lineariza-
tion step. In parsing, projectivization is a determin-
istic process that lifts edges based on the linear or-
der of a sentence. Since the linear order is exactly
what we aim to produce, this deterministic conver-
sion cannot be applied before linearization. There-
fore, we use a statistical classifier as our initial lift-
ing component. This classifier has to be trained on
suitable data, and it is an empirical question whether
the projective linearizer can take advantage of this
preceding lifting step.
We present experiments on six languages with
varying degrees of non-projective structures: En-
glish, German, Dutch, Danish, Czech and Hungar-
ian, which exhibit substantially different word order
properties. Our approach achieves significant im-
provements on all six languages. On German, we
also report results of a pilot human evaluation.
2 Related Work
An important concept for tree linearization are word
order domains (Reape, 1989). The domains are bags
of words (constituents) that are not allowed to be dis-
continuous. A straightforward method to obtain the
word order domains from dependency trees and to
order the words in the tree is to use each word and
its children as domain and then to order the domains
and contained words recursively. As outlined in the
introduction, the direct mapping of syntactic trees to
domains does not provide the possibility to obtain
all possible correct word orders.
Linearization systems can be roughly distin-
guished as either rule-based or statistical systems. In
the 2011 Shared Task on Surface Realisation (Belz
et al 2011), the top performing systems were all
statistical dependency realizers (Bohnet et al 2011;
Guo et al 2011; Stent, 2011).
Grammar-based approaches map dependency
structures or phrase structures to a tree that repre-
sents the linear precedence. These approaches are
mostly able to generate non-projective word orders.
Early work was nearly exclusively applied to phrase
structure grammars (e.g. (Kathol and Pollard, 1995;
Rambow and Joshi, 1994; Langkilde and Knight,
1998)). Concerning dependency-based frameworks,
Bro?ker (1998) used the concept of word order do-
mains to separate surface realization from linear
precedence trees. Similarly, Duchier and Debus-
mann (2001) differentiate Immediate Dominance
trees (ID-trees) from Linear Precedence trees (LP-
trees). Gerdes and Kahane (2001) apply a hierarchi-
cal topological model for generating German word
order. Bohnet (2004) employs graph grammars to
map between dependency trees and linear prece-
dence trees represented as hierarchical graphs. In the
frameworks of HPSG, LFG, and CCG, a grammar-
based generator produces word order candidates that
might be non-projective, and a ranker is used to se-
lect the best surface realization (Cahill et al 2007;
White and Rajkumar, 2009).
Statistical methods for linearization have recently
become more popular (Langkilde and Knight, 1998;
Ringger et al 2004; Filippova and Strube, 2009;
Wan et al 2009; He et al 2009; Bohnet et al 2010;
Guo et al 2011). They typically work by travers-
ing the syntactic structure either bottom-up (Filip-
929
pova and Strube, 2007; Bohnet et al 2010) or top-
down (Guo et al 2011; Bohnet et al 2011). These
linearizers are mostly applied to English and do not
deal with non-projective word orders. An excep-
tion is Filippova and Strube (2007), who contribute
a study on the treatment of preverbal and postver-
bal constituents for German focusing on constituent
order at the sentence level. The work most similar
to ours is that of Gamon et al(2002). They use
machine-learning techniques to lift edges in a pre-
processing step to a surface realizer. Their objec-
tive is the same as ours: by lifting, they avoid cross-
ing edges. However, contrary to our work, they use
phrase-structure syntax and focus on a limited num-
ber of cases of crossing branches in German only.
3 Lifting Dependency Edges
In this section, we describe the first of the two stages
in our approach, namely the classifier that lifts edges
in dependency trees. The classifier we aim to train
is meant to predict liftings on a given unordered de-
pendency tree, yielding a tree that, with a perfect lin-
earization, would not have any non-projective edges.
3.1 Preliminaries
The dependency trees we consider are of the form
displayed in Figure 1. More precisely, all words (or
nodes) form a rooted tree, where every node has ex-
actly one parent (or head). Edges point from head
to dependent, denoted in the text by h? d, where h
is the head and d the dependent. All nodes directly
or transitively depend on an artificial root node (de-
picted in Figure 1 as the incoming edge to is).
We say that a node a dominates a node d if a is
an ancestor of d. An edge h ? d is projective iff
h dominates all nodes in the linear span between h
and d. Otherwise it is non-projective. Moreover,
a dependency tree is projective iff all its edges are
projective. Otherwise it is non-projective.
A lifting of an edge h? d (or simply of the node
d) is an operation that replaces h ? d with g ? d,
given that there exists an edge g ? h in the tree, and
undefined otherwise (i.e. the dependent d is reat-
tached to the head of its head).2 When the lifting
2The undefined case occurs only when d depends on the
root, and hence cannot be lifted further; but these edges are by
definition projective, since the root dominates the entire tree.
operation is applied n successive times to the same
node, we say the node was lifted n steps.
3.2 Training
During training we make use of the projectivization
algorithm described by Nivre and Nilsson (2005).
It works by iteratively lifting the shortest non-
projective edges until the tree is projective. Here,
shortest edge refers to the edge spanning over the
fewest number of words. Since finding the shortest
edge relies on the linear order, instead of lifting the
shortest edge, we lift non-projective edges ordered
by depth in the tree, starting with the deepest nested
edge. A lifted version of the tree from Figure 1 is
shown in Figure 3. The edge of what has been lifted
three steps (the original edge is dotted), and the tree
is no longer non-projective.
It is what federal support should try to achieve
SBJ
ROOT OBJ
OBJNMOD SBJ
PRD
VC OPRD IM
Figure 3: The sentence from Figure 1, where what has
been assigned a new head (solid line). The original edge
is dotted.
We model the edge lifting problem as a multi-
class classification problem and consider nodes one
at a time and ask the question ?How far should this
edge be lifted??, where classes correspond to lifting
0, 1, 2, ..., n steps. To create training instances we
use the projectivization algorithm mentioned above.
We traverse the nodes of the tree sorted by depth.
For multiple nodes at the same depth, ties are broken
by linear order, i.e. for multiple nodes at the same
depth, the leftmost is visited first. When a node is
visited, we create a training instance out of it. Its
class is determined by the number of steps it would
be lifted by the projectivization algorithm given the
linear order (in most cases the class corresponds to
no lifting, since most edges are projective). As we
traverse the nodes, we also execute the liftings (if
any) and update the tree on the fly.
The training instances derived are used to train a
logistic regression classifier using the LIBLINEAR
package (Fan et al 2008). The features used for
the lifting classifier are described in Table 1. Since
we use linear classifiers, our feature set al con-
tains conjunctions of atomic features. The features
930
Atomic features
?x ? {w,wp, wgp, wch, ws, wun} morph(x), label(x), lemma(x), PoS(x)
?x ? {wgc, wne, wco} label(x), lemma(x), PoS(x)
Complex features
?x ? {w,wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)
?x ? {wch, ws, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)
?x ? {w,wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)
?x ? {w,wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
?x ? {wch, ws, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
Non-binary features
?x ? {w,wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)
Table 1: Features used for lifting. w refers to the word (dependent) in question. And with respect to w, wp is the
parent; wgp is the grandparent; wch are children; ws are siblings; wun are uncles (i.e. children of the grandparent,
excluding the parent); wgc are grandchildren; wne are nephews (i.e. grandchildren of the parent that are not children
of w); wco are cousins (i.e. grandchildren of the grandparent that are not w or siblings of w). The non-binary feature
functions refer to: SUBTREESIZE ? the absolute number of nodes below x, RELSUBTREESIZE ? the relative size of
the subtree rooted at x with respect to the whole tree.
involve the lemma, dependency edge label, part-of-
speech tag, and morphological features of the node
in question, and of several neighboring nodes in the
dependency tree. We also have a few non-binary fea-
tures that encode the size of the subtree headed by
the node and its ancestors.
We ran preliminary experiments to determine the
optimal architecture. First, other ways of modeling
the liftings are conceivable. To find new reattach-
ment points, Gamon et al(2002) propose two other
ways, both using a binary classifier: applying the
classifier to each node x along the path to the root
asking ?Should d be reattached to x??; or lifting one
step at a time and applying the classifier iteratively
until it says stop. They found that the latter outper-
formed the former. We tried this method, but found
that it was inferior to the multi-class model and more
frequently over- or underlifted.
Second, to avoid data sparseness for infrequent
lifting distances, we introduce a maximum number
of liftings. We found that a maximum of 3 gave the
best performance. In the pseudocode below, we re-
fer to this number as maxsteps.3 This means that we
are able to predict the correct lifting for most (but
not all) of the non-projective edges in our data sets
(cf. Table 3).
Third, as Nivre and Nilsson (2005) do for pars-
3During training, nodes that are lifted further than maxsteps
are assigned to the class corresponding to maxsteps. This ap-
proach worked better than ignoring the training instance or
treating it as a non-lifting (i.e. a lifting of 0 steps).
ing, we experimented with marking edges that were
lifted by indicating this on the edge labels. In the
case of parsing, this step is necessary in order to re-
verse the liftings in the parser output. In our case,
it could potentially be beneficial for both the lifting
classifier, and for the linearizer. However, we found
that marking liftings at best gave similar results as
not marking, so we kept the original labels without
marking.
3.3 Decoding
In the decoding stage, an unordered tree is given and
the goal is to lift edges that would be non-projective
with respect to the gold linear order. Similarly to
how training instances are derived, the decoding al-
gorithm traverses the tree bottom-up and visits every
node once. Ties between nodes at the same depth are
broken in an arbitrary but deterministic way. When
a node is visited, the classifier is applied and the cor-
responding lifting is executed. Pseudocode is given
in Algorithm 1.4
Different orderings of nodes at the same depth
can lead to different lifts. The reason is that lift-
ings are applied immediately and this influences the
features when subsequent nodes are considered. For
instance, consider two sibling nodes ni and nj . If
ni is visited before nj , and ni is lifted, this means
4The MIN function is used to guarantee that the edge is not
lifted beyond the root node of the tree. This does not happen
in practice though, since the feature set of the classifier include
features that implicitly encode the proximity to the root node.
931
that at the time we visit nj , ni is no longer a sibling
of nj , but rather an uncle. An obvious extension of
the decoding algorithm presented above is to apply
beam search. This allows us to consider nj both in
the context where ni has been lifted and when it has
not been lifted.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 foreach node ? N do
4 feats? EXTRACT-FEATURES(node, T )
5 steps? CLASSIFY(feats)
6 steps? MIN(steps,ROOT-DIST(node))
7 LIFT(node, T, steps)
8 return T
Algorithm 1: Greedy decoding for lifting.
Pseudocode for the beam search decoder is given
in Algorithm 2. The algorithm keeps an agenda of
trees to explore as each node is visited. For every
node, it clones the current tree and applies every pos-
sible lifting. Every tree also has an associated score,
which is the sum of the scores of each lifting so far.
The score of a lifting is defined to be the log proba-
bility returned from the logistic classifier. After ex-
ploring all trees in the agenda, the k-best new trees
from the beam are extracted and put back into the
agenda. When all nodes have been visited, the best
tree in the agenda is returned. For the experiments
the beam size (k in Algorithm 2) was set to 20.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 Tscore ? 0
4 Agenda? {T}
5 foreach node ? N do
6 Beam? ?
7 foreach tree ? Agenda do
8 feats? EXTRACT-FEATURES(node, tree)
9 m? MIN(maxsteps,ROOT-DIST(node))
10 foreach s ? 0 .. maxsteps do
11 t? CLONE(tree)
12 score? GET-LIFT-SCORE(feats, s)
13 tscore = tscore + score
14 LIFT(node, t, s)
15 Beam? Beam ? {t}
16 Agenda? EXTRACTKBEST(Beam, k)
17 return EXTRACTKBEST(Agenda, 1)
Algorithm 2: Beam decoding for lifting.
While beam search allows us to explore the search
space somewhat more thoroughly, a large number of
possibilities remain unaccounted for. Again, con-
sider the sibling nodes ni and nj when ni is visited
before nj . The beam allows us to consider nj both
when ni is lifted and when it is not. However, the
situation where nj is visited before ni is still never
considered. Ideally, all permutations of nodes at the
same depth should be explored before moving on.
Unfortunately this leads to a combinatorial explo-
sion of permutations, and exhaustive search is not
tractable. As an approximation, we create two or-
derings and run the beam search twice. The dif-
ference between the orderings is that in the second
one all ties are reversed. As this bibeam consistently
improved over the beam in Algorithm 2, we only
present these results in Section 5 (there denoted sim-
ply Beam).
4 Linearization
A linearizer searches for the optimal word order
given an unordered dependency tree, where the op-
timal word order is defined as the single reference
order of the dependency tree in the gold standard.
We employ a statistical linearizer that is trained on a
corpus of pairs consisting of unordered dependency
trees and their corresponding sentences. The lin-
earization method consists of the following steps:
Creating word order domains. In the first step,
we build the word order domains dh for all nodes
h ? y of a dependency tree y. A domain is defined
as a node and all of its direct dependents. For ex-
ample, the tree shown in Figure 3 has the following
domains: {it, be, should}, {what, support, should, try},
{federal, support}, {try, to}, {to, achieve}
If an edge was lifted before the linearization, the
lifted node will end up in the word order domain of
its new head rather than in the domain of its original
head. This way, the linearizer can deduce word or-
ders that would result in non-projective structures in
the non-lifted tree.
Ordering the words of the domains. In the sec-
ond step, the linearizer orders the words of each do-
main. The position of a subtree is determined by the
position of the head of the subtree in the enclosing
domain. Algorithm 3 shows the tree linearization
algorithm. In our implementation, the linearizer tra-
verses the tree either top-down or bottom-up.
932
1 // T is the dependency tree with lifted nodes
2 beam-size? 1000
3 for h ? T do
4 domainh? GET-DOMAIN(T ,h)
5 // initialize the beam with a empty word list
6 Agendah? ()
7 foreach w ? domainh do
8 // beam for extending word order lists
9 Beam? ()
10 foreach l ? Agendah do
11 // clone list l and append the word w
12 if w 6? l then
13 l? ? APPEND(l,m)
14 Beam? Beam ? l?
15 score[l?]? COMPUTE-SCORE(l?)
16 if | Beam | > beam-size then
17 SORT-LISTS-DESCENDING-TO-
SCORE(Beam,score)
18 Agendah? SUBLIST(0,beam-size,Beam)
19 else
20 Agendah? Beam
21 foreach l ? Beam do
22 SCOREg[l]? SCORE[l] +
GLOBAL-SCORE(l)
23 Agendah? Beam
24 return Beam
Algorithm 3: Dependency Tree Linearization.
The linearization algorithm initializes the word
order beam (agendah) with an empty order () (line
6). It then iterates over the words of a domain (lines
7-20). In the first iteration, the algorithm clones and
extends the empty word order list () by each word
of the sentence (line 12-15). If the beam (beam)
exceeds a certain size (beam-size), it is sorted by
score and pruned to maximum beam size (beam-
size) (lines 16-20). The following example illus-
trates the extensions of the beam for the top domain
shown in Figure 3.
Iter. agendabe
0: ()
1: ((it) (be) (should))
2: ((it be) (it should) (be it) (be should) ...)
The beam enables us to apply features that encode
information about the first tokens and the last token,
which are important for generating, e.g. the word
order of questions, i. e. if the last token is a question
mark then the sentence should probably be a ques-
tion (cf. feature set shown in Table 2). Furthermore,
the beam enables us to generate alternative lineariza-
tions. For this, the algorithm iterates over the alter-
native word orders of the domains in order to as-
semble different word orders on the sentence level.5
Finally, when traversing the tree bottom-up, the al-
gorithm has to use the different orders of the already
ordered subtrees as context, which also requires a
search over alternative word orders of the domains.
Training of the Linearizer. We use MIRA
(Crammer et al 2006) for the training of the lin-
earizer. The classifier provides a score that we use to
rank the alternative word orders. Algorithm 3 calls
two functions to compute the score: compute-score
(line 15) for features based on pairs of words and tri-
grams and compute-global-score for features based
on word patterns of a domain. Table 2 shows the
feature set for the two functions. In the case that the
linearization of a word order domain is incorrect the
algorithm updates its weight vector w. The follow-
ing equation shows the update function of the weight
vector:
w = w + ?h(?(dh, T, xg)? ?(dh, T, xp))
We update the weight vector w by adding the dif-
ference of the feature vector representation of the
correct linearization xg and the wrongly predicted
linearization xp, multiplied by ? . ? is the passive-
aggressive update factor as defined below. The suf-
fered lossh is ?(dh, T, xp)? ?(dh, T, xg).
? = lossh||?(dh,T,xg)??(dh,T,xp)||2
Creating the word order of a sentence. The lin-
earizer traverses the tree either top-down or bottom-
up and assembles the results in the surface order.
The bottom-up linearization algorithm can take into
account features drawn from the already ordered
subtrees while the top-down algorithm can employ
as context only the unordered nodes. However, the
bottom-up algorithm additionally has to carry out a
search over the alternative linearization of the sub-
domains, as different orders of the subdomain pro-
vide different context features. This leads to a higher
linearization time. We implemented both, but could
only find a rather small accuracy difference. In the
following, we therefore present results only for the
top-down method.
5The beam also makes it possible to employ a generative
language model to rerank alternative linearizations.
933
Atomic features
For nodes w ?
domainh
lemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),
PoS-children(w)
For domain
domainh
head(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn?1),
contains-?(domainh)
Complex features
For bigrams
(w1, w2) ?
domainh
feat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2
feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)
feat4: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)
feat5: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)
For trigrams
(w1, w2, w3) ?
domainh
feat3: lemma(w1)+lemma(w2)+lemma(w3)
feat4: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)
feat5: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)
For sentence s feat6: label(w1)+label(wn?1)+lemma(head)+lemma(w1)+lemma(wn?1)
feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn?1)+PoS(wn?2)+PoS(wn?3)+contains-?(s)
Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3). Atomic
features which represent properties of a node or a domain are conjoined into feature vectors of different lengths.
Linearizations are scored based on bigrams, trigrams, and global sentence-level features.
5 Experiments
We conduct experiments on six European languages
with varying degrees of word order restrictions:
While English word order is very restrictive, Czech
and Hungarian exhibit few word order constraints.
Danish, Dutch, and German (so-called V2, i. e.
verb-second, languages) show a relatively free word
order that is however more restrictive than in Hun-
garian or Czech. The English and the Czech data
are from the CoNLL 2009 Shared Task data sets
(Hajic? et al 2009). The Danish and the Dutch data
are from the CoNLL 2006 Shared Task data sets
(Buchholz and Marsi, 2006). For Hungarian, we use
the Hungarian Dependency Treebank (Vincze et al
2010), and for German, we use a dependency con-
version by Seeker and Kuhn (2012).
# sent?s np sent?s np edges np ? 3 lifts
English 39,279 7.63 % 0.39% 98.39%
German 36,000 28.71% 2.34% 94.98%
Dutch 13,349 36.44% 5.42% 99.80%
Danish 5,190 15.62 % 1.00% 96.72%
Hungarian 61,034 15.81% 1.45% 99.82%
Czech 38,727 22.42% 1.86% 99.84%
Table 3: Size of training sets, percentage of non-
projective (np) sentences and edges, percentage of np
edges covered by 3 lifting steps.
Table 3 shows the sizes of the training corpora
and the percentage of non-projective sentences and
edges in the data. Note that the data sets for Dan-
ish and Dutch are quite small. English has the least
percentage of non-projective edges. Czech, Ger-
man, and Dutch show the highest percentage of non-
projective edges. The last column shows the per-
centage of non-projective edges that can be made
projective by at most 3 lifting steps.
5.1 Setup
In our two-stage approach, we first train the lifting
classifier. The results for this classifier are reported
in Section 5.2.
Second, we train the linearizer on the output of
the lifting classifier. To assess the impact of the
lifting technique on linearization, we built four sys-
tems on each language: (a) a linearizer trained on
the original, non-lifted dependency structures (No-
lift), two trained on the automatically lifted edges
(comparing (b) the beam and (c) greedy decoding),
(d) one trained on the oracle, i. e. gold-lifted struc-
tures, which gives us an upper bound for the lifting
technique. The linearization results are reported in
Section 5.3.
In this two-stage setup, we have the problem that,
if we re-apply the lifting classifier on the data it was
trained on, the input for the linearizer will be better
during training than during testing. To provide real-
istic training data for the linearizer, we make a 10-
fold cross-validation of the lifting classifier on the
training set, and use this as training data for the lin-
earizer. The lifting classifier that is applied to the
test set is trained on the entire training set.
934
5.2 Lifting results
To evaluate the performance of the lifting classifier,
we present precision, recall, and F-measure results
for each language. We also compute the percentage
of sentences that were handled perfectly by the lift-
ing classifier. Precision and recall are defined the
usual way in terms of true positives, false positives,
and false negatives, where true positives are edges
that should be lifted and were lifted correctly; false
positives are edges that should not be lifted but were
and edges that should be lifted and were lifted, but
were reattached in the wrong place; false negatives
are edges that should be lifted but were not.
The performance of both the greedy decoder and
the bibeam decoder are shown in Table 4. The scores
are taken on the cross-validation on the training set,
as this provides more reliable figures. The scores
are micro-averaged, i.e. all folds are concatenated
and compared to the entire training set.
Although the major evaluation of the lifting is
given by the performance of the linearizer, Table 4
gives us some clues about the lifting. We see that
precision is generally much higher than recall. We
believe this is related to the fact that some phenom-
ena encoded by non-projective edges are more sys-
tematic and thus easier to learn than others (e. g. wh-
extraction vs. relative clause extraposition). We also
find that beam search consistently yields modest in-
creases in performance.
Greedy Beam
P R F1 Perfect P R F1 Perfect
Eng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83
Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97
Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34
Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74
Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73
Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33
Table 4: Precision, recall, F-measure and perfect projec-
tivization results for the lifting classifier.
5.3 Linearization Results and Discussion
We evaluate the linearizer with standard metrics: n-
gram overlap measures (BLEU, NIST), edit distance
(Edit), and the proportion of exactly linearized sen-
tences (Exact). As a means to assess the impact of
lifting more precisely, we propose the word-based
measure Exactlift which only looks at the words
with an incoming lifted edge. The Exactlift score
then corresponds to the percentage of these words
that has been realized in the exact same position as
in the original sentence.
LangLift BLEU NIST Edit Exact Exactlift Nlift
EngNolift 0.911 15.09 0.922 56.40 0.00 0
EngGreedy 0.914 15.10 0.923 57.27 59.87 152
EngBeam 0.916 15.11 0.925 58.48 62.82 156
EngOracle 0.923 15.14 0.928 60.73 70.42 240
GerNolift 0.792 13.76 0.844 40.4 0.00 0
GerGreedy 0.811 13.86 0.864 42.9 55.21 480
GerBeam 0.813 13.86 0.866 43.3 56.47 487
GerOracle 0.843 13.97 0.889 49.95 72.87 634
DutNolift 0.743 11.31 0.796 30.05 0.00 0
DutGreedy 0.784 11.47 0.797 37.56 41.02 256
DutBeam 0.778 11.46 0.8 37.05 47.45 255
DutOracle 0.825 11.63 0.848 44.82 70.55 292
DanNolift 0.836 11.80 0.886 44.41 0.00 0
DanGreedy 0.852 11.88 0.90 45.96 67.65 34
DanBeam 0.858 11.90 0.90 48.76 67.65 34
DanOracle 0.865 11.92 0.90 50.93 74.42 43
HunNolift 0.755 15.70 0.839 30.71 0.00 0
HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538
HunBeam 0.764 15.71 0.844 31.98 41.37 1,581
HunOracle 0.777 15.79 0.849 34.30 57.53 1,933
CzeNolift 0.693 14.32 0.789 25.14 0.00 0
CzeGreedy 0.711 14.45 0.797 26.85 42.04 923
CzeBeam 0.712 14.45 0.795 26.37 41.34 941
CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282
Table 5: Performance of linearizers using different lift-
ings, Exactlift is the exact match for words with an in-
coming lifted edge, Nlift is the total number of lifted
edges.
The results are presented in Table 5. On each
language, the predicted liftings significantly im-
prove on the non-lifted baseline (except the greedy
decoding in English).6 The differences between
the beam and the greedy decoding are not signif-
icant. The scores on the oracle liftings suggest
that the impact of lifting on linearization is heav-
ily language-dependent: It is highest on the V2-
languages, and somewhat smaller on English, Hun-
garian, and Czech. This is not surprising since the
V2-languages (especially German and Dutch) have
the highest proportion of non-projective edges and
sentences (see Table 3). On the other hand, En-
glish has a very small number of non-projective
edges, such that the BLEU score (which captures
the n-gram level) reflects the improvement by only
6We used a t-test, with ? = 0.01.
935
a small increase. However, note that, on the sen-
tence level, the percentage of exactly regenerated
sentences increases by 2 points which suggests that
a non-negligible amount of non-projective sentences
can now be generated more fluently.
50556065707580
Eng
Ger
Dut
Dan
Hun
Cze
langua
ge
accuracy
periph
ery left right
Figure 4: Accuracy for the linearization of the sentences?
left and right periphery, the bars are upper and lower
bounds of the non-lifted and the gold-lifted baseline.
The Exactlift measure refines this picture: The
linearization of the non-projective edges is relatively
exact in English, and much less precise in Hungarian
and Czech where Exactlift is even low on the gold-
lifted edges. The linearization quality is also quite
moderate on Dutch where the lifting leads to con-
siderable improvements. These tendencies point to
some important underlying distinctions in the non-
projective word order phenomena over which we
are generalizing: In certain cases, the linearization
seems to systematically follow from the fact that the
edge has to be lifted, such as wh-extraction in En-
glish (Figure 1). In other cases, the non-projective
linearization is just an alternative to other grammati-
cal, but maybe less appropriate, realizations, such as
the prefield-occupation in German (Figure 2).
Since a lot of non-projective word orders affect
the clause-initial or clause-final position, we evalu-
ate the exact match of the left periphery (first three
words) and the right periphery (last three words) of
the sentence. The accuracies obtained are plotted
in Figure 4, where the lower and upper bars corre-
spond to the lower and upper bound from the non-
lifted and the gold-lifted baseline. It clearly emerges
from this figure that the range of improvements ob-
tainable from lifting is closely tied to the general
linearization quality, and also to word order prop-
erties of the languages. Thus, the range of sentences
affected by the lifting is clearly largest for the V2-
languages. The accuracies are high, but the ranges
are small for English, whereas the accuracies are low
and the ranges quite small for Czech and Hungarian.
System BLEU NIST
(Bohnet et al 2011) (ranked 1st) 0.896 13.93
(Guo et al 2011) (ranked 2nd) 0.862 13.68
Baseline-Non-Lifted + LM 0.896 13.94
Beam-Lifted + LM 0.901 13.96
Table 6: Results on the development set of the 2011
Shared Task on Surface Realisation data, (the test set was
not officially released).
We also evaluated our linearizer on the data of
2011 Shared Task on Surface Realisation, which is
based on the English CoNLL 2009 data (like our
previous evaluations) but excludes information on
morphological realization. For training and evalu-
ation, we used the exact set up of the Shared Task.
For the morphological realization, we used the mor-
phological realizer of Bohnet et al(2010) that pre-
dicts the word form using shortest edit scripts. For
the language model (LM), we use a 5-gram model
with Kneser-Ney (Kneser and Ney, 1995) smoothing
derived from 11 million sentences of the Wikipedia.
In Table 6, we compare our two linearizers (with
and without lifting) to the two top systems of the
2011 Shared Task on Surface Realisation, (Bohnet et
al., 2011) and (Guo et al 2011). Without the lifting,
our system reaches a score comparable to the top-
ranked system in the Shared Task. With the lifting,
we get a small7 but statistically significant improve-
ment in BLEU such that our system reaches a higher
score than the top ranked systems. This shows that
the improvements we obtain from the lifting carry
over to more complex generation tasks which in-
clude morphological realization.
5.4 Human Evaluation
We have carried out a pilot human evaluation on the
German data in order to see whether human judges
prefer word orders obtained from the lifting-based
7Remember that English has the least percentage of non-
projective edges in our data sets, which are however important
to linearize correctly (see Figure 1).
936
linearizer. In particular, we wanted to check whether
the lifting-based linearizer produces more natural
word orders for sentences that had a non-projective
tree in the corpus, and maybe less natural word or-
ders on originally projective sentences. Therefore,
we divided the evaluated items into originally pro-
jective and non-projective sentences.
We asked four annotators to judge 60 sentence
pairs comparing the lifting-based against the non-
lifted linearizer using the toolkit by Kow and Belz
(2012). All annotators are students, two of them
have a background in linguistics. The items were
randomly sampled from the subset of the develop-
ment set containing those sentences where the lin-
earizers produced different surface realizations. The
items are subdivided into 30 originally projective
and 30 originally non-projective sentences.
For each item, we presented the original context
sentence from the corpus and the pair of automat-
ically produced linearizations for the current sen-
tence. The annotators had to decide on two crite-
ria: (i) which sentence do they prefer? (ii) how flu-
ent is that sentence? In both cases, we used con-
tinuous sliders as rating tools, since humans seem
to prefer them (Belz and Kow, 2011). For the first
criterion, the slider positions were mapped to values
from -50 (preference for left sentence) to 50 (pref-
erence for right sentence). If the slider position is
zero, both sentences are equally preferred. For the
second criterion, the slider positions were mapped
to values from 0 (absolutely broken sentence) to 100
(perfectly fluent sentence).
Sentences Scores Equal Lifted Non-lifted
All
% selected 44.58% 35.0% 20.42%
Fluency 56.14 75.77 72.78
Preference 0 34.75 31.06
Non-
Proj.
% selected 29.63% 58.33% 12.04%
Fluency 43.06 76.27 68.85
Preference 0 37.52 24.46
Proj.
% selected 56.82% 15.91% 27.27%
Fluency 61.72 74.29 74.19
Preference 0 26.43 33.44
Table 7: Results from human evaluation.
Table 7 presents the results averaged over all sen-
tences, as well as for the subsets of non-projective
and projective sentences. We report the percentage
of items where the judges selected both, the lifted, or
non-lifted sentence, alongside with the average flu-
ency score (0-100) and preference strength (0-50).
On the entire set of items, the judges selected both
sentences in almost half of the cases. However, on
the subset of non-projective sentences, the lifted ver-
sion is clearly preferred and has a higher average
fluency and preference strength. The percentage of
zero preference items is much higher on the sub-
set of projective sentences. Moreover, the average
fluency of the zero preference items is remarkably
higher on the projective sentences than on the non-
projective subset. We conclude that humans have
a strong preference for lifting-based linearizations
on non-projective sentences. We attribute the low
fluency score on the non-projective zero preference
items to cases where the linearizer did not get a cor-
rect lifting or could not linearize the lifting correctly
such that the lifted and the non-lifted version were
not appropriate. On the other hand, incorrect lift-
ings on projective sentences do not necessarily seem
to result in deprecated linearizations, which leads to
the high percentage of zero preferences with a good
average fluency on this subset.
6 Conclusion
We have presented a novel technique to linearize
sentences for a range of languages that exhibit non-
projective word order. Our approach deals with non-
projectivity by lifting edges in an unordered input
tree which can then be linearized by a standard pro-
jective linearization algorithm.
We obtain significant improvements for the
lifting-based linearization on English, German,
Dutch, Danish, Czech and Hungarian, and show that
lifting has the largest impact on the V2-languages.
In a human evaluation carried out on German we
also show that human judges clearly prefer lifting-
based linearizations on originally non-projective
sentences, and, on the other hand, that incorrect lift-
ings do not necessarily result in bad realizations of
the sentence.
Acknowledgments
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG) via the SFB 732 ?Incremental
Specification in Context?. We would also like to
thank Anna Ha?tty and our four annotators for their
contribution to the human evaluation.
937
References
A. Belz and E. Kow. 2011. Discrete vs. Continuous Rat-
ing Scales for Language Evaluation in NLP. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June. Association for Computational Linguistics.
A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and
A. Stent. 2011. The First Surface Realisation Shared
Task: Overview and Evaluation Results. In ENLG?11.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2000.
The Prague Dependency Treebank: A Three-level an-
notation scenario. In A. Abeille?, editor, Treebanks:
Building and using syntactically annotated corpora.,
chapter 1, pages 103?127. Kluwer Academic Publish-
ers, Amsterdam.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Coling 2010,
pages 98?106.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011.
<stumaba>: From deep representation to surface. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on NLG, pages 232?235,
Nancy, France.
B. Bohnet. 2004. A Graph Grammar Approach to Map
Between Dependency Trees and Topological Models.
In IJCNLP, pages 636?645.
N. Bro?ker. 1998. Separating Surface Order and Syntactic
Relations in a Dependency Grammar. In COLING-
ACL 98.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 149?164, Morristown,
NJ, USA. Association for Computational Linguistics.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic real-
isation ranking for a free word order language. ENLG
?07, pages 17?24.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551?585.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In Proceedings of the ACL.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?1874.
K. Filippova and M. Strube. 2007. Generating con-
stituent order in german clauses. In ACL, pages 320?
327.
K. Filippova and M. Strube. 2009. Tree linearization in
English: improving language model based approaches.
In NAACL, pages 225?228, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier,
and Z. Zhang. 2002. Extraposition: A case study in
German sentence realization. In Proceedings of Col-
ing 2002. Association for Computational Linguistics.
K. Gerdes and S. Kahane. 2001. Word order in german:
A formal dependency grammar using a topological hi-
erarchy. In Proceedings of the ACL.
Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at
generation challenges 2011 surface realisation track.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on NLG, pages 227?
229.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.-
A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. Stepa?nek, P. Strana?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and Semantic dependencies in multiple lan-
guages. In Proceedings of the 13th CoNLL Shared
Task, pages 1?18, Boulder, Colorado.
E. Hajic?ova?, J. Havelka, P. Sgall, K. Vesela?, and D. Ze-
man. 2004. Issues of projectivity in the prague de-
pendency treebank. Prague Bulletin of Mathematical
Linguistics, 81.
W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency
Based Chinese Sentence Realization. In Proceedings
of the ACL and of the IJCNLP, pages 809?816.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In COLING-ACL, pages 646?
652.
A. Kathol and C. Pollard. 1995. Extraposition via com-
plex domain formation. In Meeting of the Association
for Computational Linguistics, pages 174?180.
R. Kneser and H. Ney. 1995. In In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 181?184.
E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for
Creating Online Language Evaluation Experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 99?106, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
O. Rambow and A. K. Joshi. 1994. A formal look at
dependency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
938
In Leo Wanner, editor, Current Issues in Meaning-Text
Theory. Pinter, London, UK.
M. Reape. 1989. A logical treatment of semi-free word
order and bounded discontinuous constituency. In
Proceedings of the EACL, EACL ?89, pages 103?110.
E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In COLING ?04, pages 673?
679.
W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit
in Dependency Conversion for a German Treebank. In
Proceedings of LREC 2012, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
A. Stent. 2011. Att-0: Submission to generation chal-
lenges 2011 surface realization shared task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 230?231, Nancy, France, September. As-
sociation for Computational Linguistics.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian Dependency Tree-
bank. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC 2010), pages 1855?1862, Valletta, Malta.
S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving
grammaticality in statistical sentence generation: In-
troducing a dependency spanning tree algorithm with
an argument satisfaction model. In EACL, pages 852?
860.
M. White and R. Rajkumar. 2009. Perceptron reranking
for CCG realization. In EMNLP?09, pages 410?419,
Singapore, August.
939
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ICARUS ? An Extensible Graphical Search Tool
for Dependency Treebanks
Markus Ga?rtner Gregor Thiele Wolfgang Seeker Anders Bjo?rkelund Jonas Kuhn
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
firstname.lastname@ims.uni-stuttgart.de
Abstract
We present ICARUS, a versatile graphi-
cal search tool to query dependency tree-
banks. Search results can be inspected
both quantitatively and qualitatively by
means of frequency lists, tables, or depen-
dency graphs. ICARUS also ships with
plugins that enable it to interface with tool
chains running either locally or remotely.
1 Introduction
In this paper we present ICARUS1 a search and
visualization tool that primarily targets depen-
dency syntax. The tool has been designed such
that it requires minimal effort to get started with
searching a treebank or system output of an auto-
matic dependency parser, while still allowing for
flexible queries. It enables the user to search de-
pendency treebanks given a variety of constraints,
including searching for particular subtrees. Em-
phasis has been placed on a functionality that
makes it possible for the user to switch back and
forth between a high-level, aggregated view of the
search results and browsing of particular corpus
instances, with an intuitive visualization of the
way in which it matches the query. We believe this
to be an important prerequisite for accessing anno-
tated corpora, especially for non-expert users.
Search queries in ICARUS can be constructed
either in a graphical or a text-based manner. Build-
ing queries graphically removes the overhead of
learning a specialized query language and thus
makes the tool more accessible for a wider audi-
ence. ICARUS provides a very intuitive way of
breaking down the search results in terms of fre-
quency statistics (such as the distribution of part-
of-speech on one child of a particular verb against
the lemma of another child). The dimensions for
1Interactive platform for Corpus Analysis and Research
tools, University of Stuttgart
the frequency break-down are simply specified by
using grouping operators in the query. The fre-
quency tables are filled and updated in real time
as the search proceeds through the corpus ? allow-
ing for a quick detection of misassumptions in the
query.
ICARUS uses a plugin-based architecture that
permits the user to write his own plugins and in-
tegrate them into the system. For example, it
comes with a plugin that interfaces with an exter-
nal parser that can be used to parse a sentence from
within the user interface. The constraints for the
query can then be copy-pasted from the resulting
parse visualization. This facilitates example-based
querying, which is particularly helpful for inexpe-
rienced users ? they do not have to recall details
of the annotation conventions outside of their fo-
cus of interests but can go by what the parser pro-
vides.2
ICARUS is written entirely in Java and runs out
of the box without requiring any installation of
the tool itself or additional libraries. This makes
it platform independent and the only requirement
is that a Java Runtime Environment (JRE) is in-
stalled on the host system. It is open-source and
freely available for download.3
As parsers and other Natural Language Pro-
cessing (NLP) tools are starting to find their way
into other sciences such as (digital) humanities or
social sciences, it gets increasingly important to
provide intuitive visualization tools that integrate
seamlessly with existing NLP tools and are easy
to use also for non-linguists. ICARUS interfaces
readily with NLP tools provided as web services
by CLARIN-D,4 the German incarnation of the
European Infrastructure initiative CLARIN.
2This is of course only practical with rather reliable auto-
matic parsers, but in our experience, the state-of-the-art qual-
ity is sufficient.
3www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/icarus.en.html
4http://de.clarin.eu
55
The remainder of this paper is structured as fol-
lows: In Section 2 we elaborate on the motivation
for the tool and discuss related work. Section 3
presents a running example of how to build queries
and how results are visualized. In Section 4 we
outline the details of the architecture. Section 5
discusses ongoing work, and Section 6 concludes.
2 Background
Linguistically annotated corpora are among the
most important sources of knowledge for empir-
ical linguistics as well as computational modeling
of natural language. Moreover, for most users the
only way to develop a systematic understanding
of the phenomena in the annotations is through a
process of continuous exploration, which requires
suitable and intuitive tools.
As automatic analysis tools such as syntactic
parsers have reached a high quality standard, ex-
ploration of large collections of auto-parsed cor-
pus material becomes more and more common. Of
course, the querying problem is the same no matter
whether some target annotation was added manu-
ally, as in a treebank, or automatically. Yet, the
strategy changes, as the user will try to make sure
he catches systematic parsing errors and develops
an understanding of how the results he is deal-
ing with come about. While there is no guaran-
teed method for avoiding erroneous matches, we
believe that an easy-to-use transparent querying
mechanism that allows the user to look at the same
or similar results from various angles is the best
possible basis for an informed usage: frequency
tables breaking down the corpus distributions in
different dimensions are a good high-level hint,
and the actual corpus instances should be only one
or two mouse clicks away, presented with a con-
cise visualization of the respective instantiation of
the query constraints.
Syntactic annotations are quite difficult to query
if one is interested in specific constructions that
are not directly encoded in the annotation labels
(which is the case for most interesting phenom-
ena). Several tools have been developed to enable
researchers to do this. However, many of these
tools are designed for constituent trees only.
Dependency syntax has become popular as a
framework for treebanking because it lends itself
naturally to the representation of free word order
phenomena and was thus adopted in the creation of
treebanks for many languages that have less strict
word order, such as the Prague Dependency Tree-
bank for Czech (Hajic? et al, 2000) or SynTagRus
for Russian (Boguslavsky et al, 2000).
A simple tool for visualization of dependency
trees is What?s wrong with my NLP? (Riedel,
2008). Its querying functionality is however lim-
ited to simple string-searching on surface forms. A
somewhat more advanced tool is MaltEval (Nils-
son and Nivre, 2008), which offers a number of
predefined search patterns ranging from part-of-
speech tag to branching degree.
On the other hand, powerful tools such as PML-
TQ (Pajas and S?te?pa?nek, 2009) or INESS (Meurer,
2012) offer expressive query languages and can
facilitate cross-layer queries (e.g., involving both
syntactic and semantic structures). They also
accommodate both constituent and dependency
structures.
In terms of complexity in usage and expressiv-
ity, we believe ICARUS constitutes a middle way
between highly expressive and very simple visu-
alization tools. It is easy to use, requires no in-
stallation, while still having rich query and visual-
ization capabilities. ICARUS is similar to PML-
TQ in that it also allows the user to create queries
graphically. It is also similar to the search tool
GrETEL (Augustinus et al, 2012) as it interfaces
with a parser, allowing the user to create queries
starting from an automatic parse. Thus, queries
can be created without any prior knowledge of the
treebank annotation scheme.
As for searching constituent treebanks, there
is a plethora of existing search tools, such
as TGrep2 (Rohde, 2001), TigerSearch (Lezius,
2002), MonaSearch (Maryns, 2009), and Fangorn
(Ghodke and Bird, 2012), among others. They im-
plement different query languages with varying ef-
ficiency and expressiveness.
3 Introductory Example
Before going into the technical details, we show
an example of what you can do with ICARUS.
Assume that a user is interested in passive con-
structions in English, but does not know exactly
how this is annotated in a treebank. As a first step,
he can use a provided plugin that interfaces with
a tool chain5 to parse a sentence that contains a
passive construction (thus adopting the example-
based querying approach laid out in the introduc-
5using mate-tools by Bohnet (2010); available at
http://code.google.com/p/mate-tools
56
tion). Figure 1 shows the parser interface. In the
lower field, the user entered the sentence. The
other two fields show the output of the parser, once
as a graph and once as a feature value description.
Figure 1: Parsing the sentence ?Mary was kissed
by a boy.? with a predefined tool chain.
In the second step, the user can then mark parts
of the output graph by selecting some nodes and
edges, and have ICARUS construct a query struc-
ture from it, following the drag-and-drop scheme
users are familiar with from typical office soft-
ware. The automatically built query can be man-
ually adjusted by the user (relaxing constraints)
and then be used to search for similar structures
in a treebank. The parsing step can of course be
skipped altogether, and a query can be constructed
by hand right away. Figure 2 shows the query
builder, where the user can define or edit search
graphs graphically in the main window, or enter
them as a query string in the lower window.
Figure 2: Query builder for constructing queries.
For the example, Figure 3 shows the query as it
is automatically constructed by ICARUS from the
partial parse tree (3a), and what it might look like
after the user has changed it (3b). The modified
query matches passive constructions in English, as
annotated in the CoNLL 2008 Shared Task data set
(Surdeanu et al, 2008), which we use here.
(a) automatically extracted (b) manually edited
Figure 3: Search graphs for finding passive con-
structions. (a) was constructed automatically from
the parsed sentence, (b) is a more general version.
The search returns 6,386 matches. Note that
the query (Figure 3b) contains a <*>-expression.
This grouping operator groups the results accord-
ing to the specified dimension, in this case by the
lemma of the passivized verb. Figure 4 shows
the result view. On the left, a list of lemmas is
presented, sorted by frequency. Clicking on the
lemma displays the list of matches containing that
particular lemma on the right side. The match-
ing sentences can then be browsed, with the active
sentence also being shown as a tree. Note that the
instantiation of the query constraints is highlighted
in the tree display.
Figure 4: Passive constructions in the treebank
grouped by lemma and sorted by frequency.
The query could be further refined to restrict it
to passives with an overt logical subject, using a
more complex search graph for the by-phrase and
a second instance of the grouping operator. The
results will then also be grouped by the lemma of
the logical subject, and are therefore presented as
a two-dimensional table. Figure 5 shows the new
query and the resulting view. The user is presented
with a frequency table, where each cell contains
the number of hits for this particular combination
of verb lemma and logical subject. Clicking on
the cell opens up a view similar to the right part of
Figure 4 where the user can then again browse the
actual trees.
57
Figure 5: Search graph and result view for passive
constructions with overt logical subjects, grouped
by lemma of the verb and the lemma of the logical
subject.
Finally, we can add a third grouping operator.
Figure 6 shows a further refined query for passives
with an overt logical subject and an object. In the
results, the user is presented with a list of values
for the first grouping operator to the left. Clicking
on one item in that list opens up a table on the right
presenting the other two dimensions of the query.
Figure 6: Search graph and result view for passive
constructions with an overt logical subject and an
object, grouped by lemma of the verb, the logical
subject, and the object.
This example demonstrates a typical use case
for a user that is interested in certain linguistic
constructions in his corpus. Creating the search
graph and interpreting the results does not re-
quire any specialized knowledge other than fa-
miliarity with the annotation of the corpus being
searched. It especially does not require any pro-
gramming skills, and the possibility to graphically
build a query obviates the need to learn a special-
ized query language.
4 Architecture
This section goes into more details about the in-
ner workings of ICARUS. A main component
is the search engine, which enables the user to
quickly search treebanks for whatever he is inter-
ested in. A second important feature of ICARUS
is the plugin-based architecture, which allows for
the definition of custom extensions. Currently,
ICARUS can read the commonly used CoNLL de-
pendency formats, and it is easy to write exten-
sions in order to add additional formats.
4.1 Search Engine and Query Builder
ICARUS has a tree-based search engine for tree-
banks, and includes a graphical query builder.
Structure and appearance of search graphs are sim-
ilar to the design used for displaying dependency
trees (cf. Figure 1), which is realized with the
open-source library JGraph.6 Queries and/or their
results can be saved to disk and later reloaded for
further processing.
Defining a query graphically basically amounts
to drawing a partial graph structure that defines
the type of structure that the user is interested in.
In practice, this is done by creating nodes in the
query builder and connecting them by edges. The
nodes correspond to words in the dependency trees
of the treebank. Several features like word iden-
tity, lemma, part of speech, etc. can be specified
for each node in the search graph in order to re-
strict the query. Dominance and precedence con-
straints over a set of nodes can be defined by sim-
ply linking nodes with the appropriate edge type.
Edges can be further specified for relation type,
distance, direction, projectivity, and transitivity. A
simple example is shown in Figures 2 and 3. The
search engine supports regular expressions for all
string-properties (form, lemma, part of speech, re-
lation). It also supports negation of (existence of)
nodes and edges, and their properties.
As an alternative to the search graph, the user
can also specify the query in a text-based format
by constructing a comma separated collection of
constraints in the form of key=value pairs for a
single node contained within square brackets. Hi-
erarchical structures are expressed by nesting their
textual representation. Figure 7 shows the text-
based form of the three queries used in the exam-
ples in Section 3.
6http://www.jgraph.com/
58
Query 1: [lemma=be[pos=VBN,lemma=<*>,rel=VC]]
Query 2: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]]]
Query 3: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]
[lemma=<*>,rel=OBJ]]]
Figure 7: Text representation of the three queries used in the example in Section 3.
A central feature of the query language is the
grouping operator (<*>), which will match any
value and cause the search engine to group result
entries by the actual instance of the property de-
clared to be grouped. The results of the search
will then be visualized as a list of instances to-
gether with their respective frequencies. Results
can be sorted alphabetically or by frequency (ab-
solute or relative counts) . Depending on the num-
ber of grouping operators used (up to a maximum
of three) the result is structured as a list of fre-
quencies (cf. Figure 4), a table of frequencies for
pairs of instances (cf. Figure 5), or a list where
each item then opens up a table of frequency re-
sults (cf. Figure 6). In the search graph and the
result view, different colors are used to distinguish
between different grouping operators.
The ICARUS search engine offers three differ-
ent search modes:
Sentence-based. Sentence based search stops at
the first successful hit in a sentence and returns
every sentence on a list of results at most once.
Exhaustive sentence-based. The exhaustive
sentence-based search mode extends the sentence
based search by the possibility of processing mul-
tiple hits within a single sentence. Every sentence
with at least one hit is returned exactly once. In the
result view, the user can then browse the different
hits found in one sentence.
Hit-based. Every successful hit is returned sepa-
rately on the corresponding list of results.
When a query is issued, the search results are
displayed on the fly as the search engine is pro-
cessing the treebank. The sentences can be ren-
dered in one of two ways: either as a tree, where
nodes are arranged vertically by depth in the tree,
or horizontally with all the nodes arranged side-
by-side. If a tree does not fit on the screen, part of
it is automatically collapsed but can be expanded
again by the user.
4.2 Extensibility
ICARUS relies on the Java Plugin Framework,7
which provides a powerful XML-based frame-
7http://jpf.sourceforge.net/
work for defining plugins similarly to the engine
used by the popular Eclipse IDE project. The
plugin-based architecture makes it possible for
anybody to write extensions to ICARUS that are
specialized for a particular task. The parser inte-
gration of mate-tools demonstrated in Section 3 is
an example for such an extension.
The plugin system facilitates custom extensions
that make it possible to intercept certain stages
of an ongoing search process and interact with it.
This makes it possible for external tools to pre-
process search data and apply additional annota-
tions and/or filtering, or even make use of exist-
ing indices by using search constraints to limit the
amount of data passed to the search engine. With
this general setup, it is for example possible to eas-
ily extend ICARUS to work with constituent trees.
ICARUS comes with a dedicated plugin that
enables access to web services provided by
CLARIN-D. The project aims to provide tools and
services for language-centered research in the hu-
manities and social sciences. In contrast to the in-
tegration of, e.g., mate-tools, where the tool chain
is executed locally, the user can define a tool chain
by chaining several web services (e.g., lemmatiz-
ers, part-of-speech taggers etc.) together and ap-
ply them to his own data. To do this, ICARUS
is able to read and write the TCF exchange for-
mat (Heid et al, 2010) that is used by CLARIN-D
web services. The output can then be inspected
and searched using ICARUS. As new NLP tools
are added as CLARIN-D web services they can be
immediately employed by ICARUS.
5 Upcoming Extensions
An upcoming release includes the following ex-
tensions:
? Currently, treebanks are assumed to fit into
the executing computer?s main memory.
The new implementation will support asyn-
chronous loading of data, with notifications
passed to the query engine or a plugin when
required data is available. Treebanks with
millions of entries can then be loaded in less
59
memory consuming chunks, thus keeping the
system responsive when access is requested.
? The search engine is being extended with an
operator that allows disjunctions of queries.
This will enable the user to aggregate fre-
quency output over multiple queries.
6 Conclusion
We have presented ICARUS, a versatile and user-
friendly search and visualization tool for depen-
dency trees. It is aimed not only at (computa-
tional) linguists, but also at people from other dis-
ciplines, e.g., the humanities or social sciences,
who work with language data. It lets the user
create queries graphically and returns results (1)
quantitatively by means of frequency lists and ta-
bles as well as (2) qualitatively by connecting the
statistics to the matching sentences and allowing
the user to browse them graphically. Its plugin-
based architecture enables it to interface for exam-
ple with external processing pipelines, which lets
the user apply processing tools directly from the
user interface.
In the future, specialized plugins are planned
to work with different linguistic annotations, e.g.
cross-sentence annotations as used to annotate
coreference chains. Additionally, a plugin is in-
tended that interfaces the search engine with a
database.
Acknowledgments
This work was funded by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB
732 ?Incremental Specification in Context?,
project D8, and by the Bundesministerium fu?r
Bildung und Forschung (BMBF) via project No.
01UG1120F, CLARIN-D center Stuttgart. The
authors are also indebted to Andre? Blessing and
Heike Zinsmeister for reading an earlier draft of
this paper.
References
Liesbeth Augustinus, Vincent Vandeghinste, and
Frank Van Eynde. 2012. Example-based Treebank
Querying. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. ELRA.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency Treebank for Russian: Concept, Tools,
Types of Information. In COLING 2000, pages
987?991, Saarbru?cken, Germany.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In COLING
2010, pages 89?97, Beijing, China.
Sumukh Ghodke and Steven Bird. 2012. Fangorn: A
System for Querying very large Treebanks. In COL-
ING 2012: Demonstration Papers, pages 175?182,
Mumbai, India.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora,
pages 103?127. Amsterdam:Kluwer.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A Corpus Representation For-
mat for Linguistic Web Services: The D-SPIN Text
Corpus Format and its Relationship with ISO Stan-
dards. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. ELRA.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart.
Hendrik Maryns. 2009. MonaSearch ? A Tool for
Querying Linguistic Treebanks. In Proceedings of
TLT 2009, Groningen.
Paul Meurer. 2012. INESS-Search: A Search System
for LFG (and Other) Treebanks. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG2012 Conference. CSLI Publications.
Jens Nilsson and Joakim Nivre. 2008. MaltEval: an
Evaluation and Visualization Tool for Dependency
Parsing. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. ELRA.
Petr Pajas and Jan S?te?pa?nek. 2009. System for Query-
ing Syntactically Annotated Corpora. In Proceed-
ings of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 33?36, Suntec, Singapore. Association
for Computational Linguistics.
Sebastian Riedel. 2008. What?s Wrong With My
NLP?
http://code.google.com/p/
whatswrong/.
Douglas L.T. Rohde. 2001. TGrep2 the next-
generation search engine for parse trees.
http://tedlab.mit.edu/?dr/Tgrep2/.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL 2008,
pages 159?177, Manchester, England.
60
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 45?50,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Exploring Lexicalized Features for Coreference Resolution
Anders Bjo?rkelund
Lund University / LTH
Lund / Sweden
Anders.Bjorkelund@cs.lth.se
Pierre Nugues
Lund University / LTH
Lund / Sweden
Pierre.Nugues@cs.lth.se
Abstract
In this paper, we describe a coreference solver
based on the extensive use of lexical fea-
tures and features extracted from dependency
graphs of the sentences. The solver uses Soon
et al (2001)?s classical resolution algorithm
based on a pairwise classification of the men-
tions.
We applied this solver to the closed track of
the CoNLL 2011 shared task (Pradhan et al,
2011). We carried out a systematic optimiza-
tion of the feature set using cross-validation
that led us to retain 24 features. Using this set,
we reached a MUC score of 58.61 on the test
set of the shared task. We analyzed the impact
of the features on the development set and we
show the importance of lexicalization as well
as of properties related to dependency links in
coreference resolution.
1 Introduction
In this paper, we present our contribution to the
closed track of the 2011 CoNLL shared task (Prad-
han et al, 2011). We started from a baseline system
that uses Soon et al (2001)?s architecture and fea-
tures. Mentions are identified by selecting all noun
phrases and possessive pronouns. Then, the reso-
lution algorithm relies on a pairwise classifier that
determines whether two mentions corefer or not.
Lexicalization has proved effective in numerous
tasks of natural language processing such as part-
of-speech tagging or parsing. However, lexicalized
models require a good deal of annotated data to
avoid overfit. The data set used in the CoNLL 2011
shared task has a considerable size compared to cor-
pora traditionally used in coreference resolution ?
the training set comprises 2,374 documents. See
Pradhan et al (2007) for a previous work using an
earlier version of this dataset. Leveraging this size,
we investigated the potential of lexicalized features.
Besides lexical features, we created features that
use part-of-speech tags and semantic roles. We also
constructed features using dependency tree paths
and labels by converting the constituent trees pro-
vided in the shared task into dependency graphs.
The final feature set was selected through an au-
tomated feature selection procedure using cross-
validation.
2 System Architecture
During both training and decoding, we employed
the same mention detection and preprocessing steps.
We considered all the noun phrases (NP) and posses-
sive pronouns (PRP$) as mentions. In order to ex-
tract head words from the NP constituents, we con-
verted the constituent trees provided in the data sets
to dependency graphs using the Penn treebank con-
verter of Johansson and Nugues (2007). Using the
dependency tree, we extracted the head word of all
the NPs by taking the word that dominates the sub-
tree constructed from the NP.
The dependency tree is also used later to ex-
tract features of mentions based on dependency tree
paths, which is further described in Sec. 3.
In the preprocessing step, we assigned a number
and a gender to each mention. For the pronominal
mentions, we used a manually compiled lists of pro-
nouns, where we marked the number and gender.
45
For nonpronominal mentions, we used the number
and gender data (Bergsma and Lin, 2006) provided
by the task organizers and queried it for the head
word of the mention. In cases of ambiguity (e.g. the
pronoun you), or missing entries in the data for non-
pronominals, we assigned an unknown value.
2.1 Generation of training examples
To create a set of training examples, we used pairs
of mentions following the method outlined by Soon
et al (2001). For each anaphoric mention mj and
its closest preceding antecedent mi, we built a pos-
itive example: P = {(mi,mj)}. We constructed
the negative examples with noncoreferring pairs of
mentions, where the first term is a mention occur-
ring between mi and mj and the second one is mj :
N = {(mk,mj)|i < k < j)}.
The training examples collected from the CoNLL
2011 training set consist of about 5.5% of positive
examples and 94.5% of negative ones.
2.2 Learning method
We evaluated two types of classifiers: decision trees
and logistic regression. We used the decision trees
and the C4.5 algorithm from the Weka distribution
(Hall et al, 2009) for our baseline system. We then
opted for linear logistic regression as it scaled better
with the number of features and feature values.
Logistic regression is faster to train and allowed
us to carry out an automated feature selection, which
is further described in Sec. 3.4. In addition, the lo-
gistic classifiers enabled us to interpret their results
in terms of probabilities, which we used for the de-
coding step. We trained the logistic regression clas-
sifiers using the LIBLINEAR package (Fan et al,
2008).
2.3 Decoding
The decoding algorithm devised by Soon et al
(2001) selects the closest preceding mention deemed
to be coreferent by the classifier. This clustering
algorithm is commonly referred to as closest-first
clustering. Ng and Cardie (2002) suggested a dif-
ferent clustering procedure, commonly referred to
as best-first clustering. This algorithm selects the
most likely antecedent classified as coreferent with
the anaphoric mention. During early experiments,
we found that while the best-first method increases
the performance on nonpronominal anaphoric ex-
pressions, it has the opposite effect on pronominal
anaphoric expressions. Consequently, we settled on
using the closest-first clustering method for pronom-
inal mentions, and the best-first clustering method
otherwise. For the best-first clustering, we used the
probability output from our logistic classifiers and a
threshold of 0.5.
After clustering mentions in a document, we dis-
card all remaining singleton mentions, as they were
excluded from the annotation in the CoNLL 2011
shared task.
2.4 Postprocessing
The initial detection of mentions is a direct mapping
from two categories of constituents: NP and PRP$.
In the postprocessing step, we reclaim some of the
mentions that we missed in the initial step.
The automatically generated constituent trees pro-
vided in the data set contain errors and this causes
the loss of many mentions. Another source of loss
is the bracketing of complex NPs, where the in-
ternal structure uses the tag NML. In a few cases,
these nested nodes participate in coreference chains.
However, when we tried to include this tag in the
mention detection, we got worse results overall.
This is possibly due to an even more skewed dis-
tribution of positive and negative training examples.
In the postprocessing step, we therefore search
each document for sequences of one or more proper
noun tokens, i.e. tokens with the part-of-speech
tags NNP or NNPS. If their common ancestor, i.e.
the parse tree node that encloses all the tokens, is
not already in a mention, we try to match this se-
quence to any existing chain using the binary fea-
tures: STRINGMATCH and ALIAS (cf. Sec. 3). If
either of them evaluates to true, we add this span of
proper nouns to the matched chain.
3 Features
For our baseline system, we started with the feature
set described in Soon et al (2001). Due to space
limitations, we omit the description of these features
and refer the reader to their paper.
We also defined a large number of feature tem-
plates based on the syntactic dependency tree, as
well as features based on semantic roles. In the fol-
46
lowing sections, we describe these features as well
as the naming conventions we use. The final feature
set we used is given in Sec. 4.
3.1 Mention-based features
On the mention level, we considered the head word
(HD) of the mention, and following the edges in the
dependency tree, we considered the left-most and
right-most children of the head word (HDLMC and
HDRMC), the left and right siblings of the head word
(HDLS and HDRS), as well as the governor1 of the
head word (HDGOV).
For each of the above mentioned tokens, we ex-
tracted the surface form (FORM), the part-of-speech
tag (POS), and the grammatical function of the token
(FUN), i.e. the label of the dependency edge of the
token to its parent. For head words that do not have
any leftmost or rightmost children, or left or right
siblings, we used a null-value placeholder.
In each training pair, we extracted these values
from both mentions in the pair, i.e. both the anaphor
and the tentative antecedent. Table 3 shows the fea-
tures we used in our system. We used a naming
nomenclature consisting of the role in the anaphora,
where I stands for antecedent and J for anaphor; the
token we selected from the dependency graph, e.g.
HD or HDLMC; and the value extracted from the
token, e.g. POS or FUN. For instance, the part-of-
speech tag of the governor of the head word of the
anaphor is denoted: J-HDGOVPOS.
The baseline features taken from Soon et al
(2001) include features such as I-PRONOUN and J-
DEMONSTRATIVE that are computed using a word
list and by looking at the first word in the mention,
respectively. Our assumption is that these traits can
be captured by our new features by considering the
part-of-speech tag of the head word and the surface
form of the left-most child of the head word, respec-
tively.
3.2 Path-based features
Between pairs of potentially coreferring mentions,
we also considered the path from the head word of
the anaphor to the head word of the antecedent in
the syntactic dependency tree. If the mentions are
not in the same sentence, this is the path from the
1We use the term governor in order not to confuse it with
head word of an NP.
anaphor to the root of its sentence, followed by the
path from the root to the antecedent in its sentence.
We differentiate between the features depending on
whether they are in the same sentence or in different
sentences. The names of these features are prefixed
with SS and DS, respectively.
Following the path in the dependency tree, we
concatenated either the surface form, the part-of-
speech tag, or the grammatical function label with
the direction of the edge to the next token, i.e. up or
down. This way, we built six feature templates. For
instance, DSPATHFORM is the concatenation of the
surface forms of the tokens along the path between
mentions in different sentences.
Bergsma and Lin (2006) built a statistical model
from paths that include the lemma of the intermedi-
ate tokens, but replace the end nodes with noun, pro-
noun, or pronoun-self for nouns, pronouns, and re-
flexive pronouns, respectively. They used this model
to define a measure of coreference likelihood to re-
solve pronouns within the same sentence. Rather
than building an explicit model, we simply included
these paths as features in our set. We refer to this
feature template as BERGSMALINPATH in Table 3.
3.3 Semantic role features
We tried to exploit the semantic roles that were in-
cluded in the CoNLL 2011 data set. Ponzetto and
Strube (2006) suggested using the concatenation of
the predicate and the role label for a mention that
has a semantic role in a predicate. They introduced
two new features, I SEMROLE and J SEMROLE, that
correspond to the semantic roles filled by each of the
mentions in a pair. We included these features in our
pool of feature templates, but we could not see any
contribution from them during the feature selection.
We also introduced a number of feature templates
that only applied to pairs of mentions that occur in
the same semantic role proposition. These templates
included the concatenation of the two labels of the
arguments and the predicate sense label, and vari-
ations of these that also included the head words
of either the antecedent or anaphor, or both. The
only feature that was selected during our feature se-
lection procedure corresponds to the concatenation
of the argument labels, the predicate sense, and the
head word of the anaphor: SEMROLEPROPJHD in
Table 3. In the sentence A lone protestor parked
47
herself outside the UN, the predicate park has the
arguments A lone protestor, labeled ARG0, and her-
self, labeled ARG1. The corresponding value of this
feature would be ARG0-park.01-ARG1-herself.
3.4 Feature selection
Starting from Soon et al (2001)?s feature set, we
performed a greedy forward selection. The fea-
ture selection used a 5-fold cross-validation over the
training set, where we evaluated the features using
the arithmetic mean of MUC, BCUB, and CEAFE.
After reaching a maximal score using forward se-
lection, we reversed the process using a backward
elimination, leaving out each feature and removing
the one that had the worst impact on performance.
This backwards procedure was carried out until the
score no longer increased. We repeated this forward-
backward procedure until there was no increase in
performance. Table 3 shows the final feature set.
Feature bigrams are often used to increase the
separability of linear classifiers. Ideally, we would
have generated a complete bigram set from our fea-
tures. However, as this set is quadratic in nature
and due to time constraints, we included only a sub-
set of it in the selection procedure. Some of them,
most notably the bigram of mention head words (I-
HDFORM+J-HDFORM) were selected in the proce-
dure and appear in Table 3.
4 Evaluation
Table 1 shows some baseline figures using the binary
features STRINGMATCH and ALIAS as sole corefer-
ence properties, as well as our baseline system using
Soon et al (2001)?s features.
MD MUC BCUB
STRINGMATCH 59.91 44.43 63.65
ALIAS 19.25 16.77 48.07
Soon baseline/LR 60.79 47.50 63.97
Soon baseline/C4.5 58.96 47.02 65.36
Table 1: Baseline figures using string match and alias
properties, and our Soon baseline using decision trees
with the C4.5 induction program and logistic regression
(LR). MD stands for mention detection.
4.1 Contribution of postprocessing
The postprocessing step described in Sec. 2.4 proved
effective, contributing from 0.21 to up to 1 point to
the final score across the metrics. Table 2 shows the
detailed impacts on the development set.
MD MUC BCUB CEAFE
No postproc. 66.56 54.61 65.93 40.46
With postproc. 67.21 55.62 66.29 40.67
Increase 0.65 1.01 0.36 0.21
Table 2: Impact of the postprocessing step on the devel-
opment set.
4.2 Contribution of features
The lack of time prevented us from running a com-
plete selection from scratch and describing the con-
tribution of each feature on a clean slate. Nonethe-
less, we computed the scores when one feature is
removed from the final feature set. Table 3 shows
the performance degradation observed on the devel-
opment set, which gives an indication of the impor-
tance of each feature. In these runs, no postprocess-
ing was not used.
Toward the end of the table, some features show
a negative contribution to the score on the devel-
opment set. This is explained by the fact that our
feature selection was carried out in a cross-validated
manner over the training set.
4.3 Results on the test set
Table 4 shows the results we obtained on the test set.
The figures are consistent with the performance on
the development set across the three official metrics,
with an increase of the MUC score and a decrease
of both BCUB and CEAFE. The official score in the
shared task is computed as the mean of these three
metrics.
The shared task organizers also provided a test set
with given mention boundaries. The given bound-
aries included nonanaphoric and singleton mentions
as well. Using this test set, we replaced our mention
extraction step and used the given mention bound-
aries instead. Table 4 shows the results with this
setup. As mention boundaries were given, we turned
off our postprocessing module for this run.
48
Metric\Corpus Development set Test set Test set with gold mentions
R P F1 R P F1 R P F1
Mention detection 65.68 68.82 67.21 69.87 68.08 68.96 74.18 70.74 72.42
MUC 55.26 55.98 55.62 60.20 57.10 58.61 64.33 60.05 62.12
BCUB 65.07 67.56 66.29 66.74 64.23 65.46 68.26 65.17 66.68
CEAFM 52.51 52.51 52.51 51.45 51.45 51.45 53.84 53.84 53.84
CEAFE 41.02 40.33 40.67 38.09 41.06 39.52 39.86 44.23 41.93
BLANC 69.6 70.41 70 71.99 70.31 71.11 72.53 71.04 71.75
Official CoNLL score 53.78 54.62 54.19 55.01 54.13 54.53 57.38 56.48 56.91
Table 4: Scores on development set, on the test set, and on the test set with given mention boundaries: recall (R),
precision (P), and harmonic mean (F1). The official CoNLL score is computed as the mean of MUC, BCUB, and
CEAFE.
MD MUC BCUB
All features 66.56 54.61 65.93
I-HDFORM+J-HDFORM -1.35 -2.66 -1.82
STRINGMATCH? -1.12 -1.32 -1.55
DISTANCE? -0.16 -0.62 -0.59
J-HDGOVPOS -0.51 -0.49 -0.13
I-HDRMCFUN -0.27 -0.39 -0.2
ALIAS? -0.47 -0.36 -0.06
I-HDFORM -0.42 -0.18 0.04
I-GENDER+J-GENDER -0.3 -0.15 0.05
NUMBERAGREEMENT? 0.01 -0.14 -0.41
I-HDPOS -0.32 -0.14 0.05
J-PRONOUN? -0.25 -0.08 -0.09
I-HDLMCFORM+
J-HDLMCFORM -0.41 -0.04 0.08
I-HDLSFORM -0.01 0.01 0
SSBERGSMALINPATH -0.04 0.02 -0.13
I-HDGOVFUN -0.09 0.09 0.01
J-HDFUN -0.01 0.13 -0.04
I-HDLMCPOS -0.08 0.13 -0.09
DSPATHFORM -0.03 0.16 -0.02
J-HDGOVFUN -0.04 0.16 -0.05
J-DEMONSTRATIVE? -0.03 0.18 0.03
GENDERAGREEMENT? 0 0.18 -0.01
SEMROLEPROPJHD 0.01 0.2 0.01
I-PRONOUN? 0.01 0.22 0.04
I-HDFUN 0.05 0.22 -0.06
Table 3: The final feature set and, for each feature, the
degradation in performance when leaving out this feature
from the set. All evaluations were carried out on the de-
velopment set. The features marked with a dagger ? orig-
inate from the Soon et al (2001) baseline feature set.
5 Conclusions
The main conclusions and contributions of our work
to the CoNLL 2011 shared task concern the detec-
tion of mention boundaries, feature lexicalization,
and dependency features.
The mention boundaries are relatively difficult to
identify. Although far from perfect, we applied a di-
rect mapping from constituents to extract the men-
tions used in the resolution procedure. We then re-
claimed some mentions involving proper nouns in a
postprocessing step. Using the gold-standard men-
tion boundaries in the test set, we saw an increase in
all metrics with up to 3.51 for the MUC score.
The lexicalization of the feature set brings a sig-
nificant improvement to the scores. By order of per-
formance loss in Table 3, the first feature of our
model is a lexical one. This property does not seem
to have been systematically explored before, possi-
bly because of a tradition of using corpora of modest
sizes in coreference resolution.
Grammatical dependencies seem to play an im-
portant role in the anaphoric expressions. Results in
Table 3 also show this, although in a less pronounced
manner than lexicalization. Features extracted from
dependencies are implicit in many systems, but are
not explicitly mentioned as such. We hope our work
helped clarified this point through a more systematic
exploration of this class of features.
Acknowledgements
This research was supported by Vetenskapsra?det, the
Swedish research council, under grant 621-2010-
4800.
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
49
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages
33?40, July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10?18, July.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, NODALIDA 2007 Conference
Proceedings, pages 105?112, Tartu, May 25-26.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Simone Paolo Ponzetto and Michael Strube. 2006. Se-
mantic role labeling for coreference resolution. In
Proceedings of the 11th Conference of EACL: Posters
and Demonstrations, pages 143?146, April.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), Irvine,
CA, September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon,
June.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
50
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 49?55,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Data-driven Multilingual Coreference Resolution using Resolver Stacking
Anders Bjo?rkelund and Richa?rd Farkas
Institute for Natural Language Processing
University of Stuttgart
{anders,farkas}@ims.uni-stuttgart.de
Abstract
This paper describes our contribution to the
CoNLL 2012 Shared Task.1 We present a
novel decoding algorithm for coreference res-
olution which is combined with a standard
pair-wise coreference resolver in a stacking
approach. The stacked decoders are evaluated
on the three languages of the Shared Task. We
obtain an official overall score of 58.25 which
is the second highest in the Shared Task.
1 Introduction
In this paper we present our contribution to the
CoNLL 2012 Shared Task (Pradhan et al, 2012).
We follow the standard architecture where mentions
are extracted in the first step, then they are clustered
using a pair-wise classifier (see e.g., (Ng, 2010)).
For English, the set of extracted mentions is filtered
by removing non-referential occurrences of certain
pronouns. Our coreference resolver at its core re-
lies on a pair-wise classifier. To overcome the prob-
lems associated with the isolated pair-wise deci-
sions, we devised a novel decoding algorithm which
compares a mention to partially built clusters. For
our Shared Task contribution we combined this al-
gorithm with conventional pair-wise decoding algo-
rithms in a stacking approach.
In the Shared Task evaluation, our system re-
ceived an overall official score of 58.25, which is
the second highest among the sixteen participants.2
1The system is available for download on http://www.
ims.uni-stuttgart.de/?anders/
2The overall score is the average of MUC, B3, and CEAFE,
averaged over all three languages
2 Mention Extraction
Since all mentions are not annotated in Shared Task
data, but only mentions that take part in coreference
chains, training a general-purpose anaphoricity clas-
sifier is non-trivial. We thus implemented a high-
recall, low-precision mention extraction module that
allows the coreference resolver to see most of the
possible mentions, but has to learn to sort out the
non-referential mentions.
The mention extraction module relies mainly on
the syntactic parse tree, but also on named entities
(which were only provided for English in the pre-
dicted versions of the Shared Task data).
Since the part-of-speech tags vary a bit across the
languages, so do our extraction rules: For Arabic,
we extract all NP?s, and all terminals with part-of-
speech tags PRP and PRP$; for Chinese, we extract
all NP?s, and all terminals with part-of-speech tags
PN and NR; for English, we extract all NP?s, all ter-
minals with part-of-speech tags PRP and PRP$, and
all named entities.
Early experiments indicated that the English
coreference resolver frequently makes mistakes re-
lated to non-referential instances of the pronouns it
(often referred to as expletive or pleonastic in the lit-
erature), we, and you (generic mentions, which are
not annotated according to the OntoNotes annota-
tion guidelines). To address this issue, we developed
a referential/non-referential mention classifier in
order to identify these mentions. The classifier acts
as a filter after the mention extraction module and
removes clear cases of non-referential mentions.
Our basic assumption was that when these pro-
49
th = 0.5 th = 0.95
Precision Recall F1 Precision Recall F1 # occurrences
it 75.41 61.92 68 86.78 38.65 53.48 10,307
we 65.93 41.61 51.02 75.41 24.20 36.64 5,323
you 79.10 74.26 76.60 88.36 51.59 65.15 11,297
Average 75.73 63.05 68.81 86.17 41.04 55.60 26,927
Table 1: Performance of the non-referential classifier used for English. Precision, recall, and F-measure are broken
down by pronoun (top three rows), and the micro-average over all three (bottom row). The left side uses a probability
threshold of 0.5, and the right one a threshold of 0.95. The last column denotes the number of occurrences of the
corresponding token. All numbers are computed on the development set.
nouns do not participate in any coreference chain,
they are examples of non-referential mentions.
Based on this assumption, we extracted referential
and non-referential examples from the training set
and trained binary MaxEnt classifiers using the Mal-
let toolkit (McCallum, 2002).
Since the mentions filtered by these classifiers
are permanently removed, they are never presented
as potential mentions to the coreference resolver.
Hence, we aim for a classifier that yields few false
positives (i.e., mentions classified as non-referential
although they were not). False negatives, on the
other hand, may be passed on to the resolver, which,
ideally, does not assign them to a cluster. The pre-
cision/recall tradeoff can easily be controlled by ad-
justing the threshold of the posterior probability of
these classifiers, requiring a very high probability
that a mention is non-referential. Preliminary ex-
periments indicated that a threshold of 0.95 worked
best when the coreference resolver was trained and
evaluated on these filtered mentions.
We also found that the target pronouns should be
handled separately, i.e., instead of training one sin-
gle classifier we trained independent classifiers for
each of the target pronouns. The individual per-
formance of the classifiers, as well as the micro-
average over all three pronouns are shown in Ta-
ble 1, both using the default probability threshold
of 0.5, and the higher 0.95. In the final, fine-tuned
English coreference system, we found that the use
of the classifiers with the higher threshold improved
in all coreference metrics, and gave an increase of
about 0.5 in the official CoNLL score.
The feature set used by the classifiers describes
the (in-sentence) context of the pronoun. It consists
of the uni-, bi-, and trigrams of word forms and POS
tags in a window of ?5; the position inside the sen-
tence; the preceding and following verb and adjec-
tive; the distance to the following named entity; the
genre of the document; and whether the mention is
between quotes. For English, we additionally ex-
tended this general feature set by re-implementing
the features of Boyd et al (2005).
We investigated similar classifiers for Arabic and
Chinese as well. We selected targets based on the
frequency statistics of tokens being referential and
non-referential on the training set and used the gen-
eral feature set described above. However, these
classifiers did not contribute to the more complex
coreference system, hence the non-referential clas-
sifiers are included only in the English system.
3 Training Instance generation
To generate training instances for the pair-wise clas-
sifier, we employed the approach described by Soon
et al (2001). In this approach, for every extracted
anaphoric mention mj , we create a positive train-
ing instance with its closest preceding antecedent
mi: P = {(mi,mj)}. Negative examples are con-
structed by considering all the pairs of mj and the
(non-coreferent) mentions mk between mi and mj :
N = {(mk,mj)|i < k < j}. We extract the train-
ing examples on the version of the training set that
uses predicted information, and restrict the mentions
considered to the ones extracted by our mention ex-
traction module. Using these training examples, we
train a linear logistic regression classifier using the
LIBLINEAR package (Fan et al, 2008).
To create training examples for the English clas-
sifier, which uses the non-referential classifier for
pronouns, we made a 10-fold cross-annotation on
the training set with this classifier. I.e., the docu-
ments were partitioned into 10 sets D1, D2, ..., D10,
and when extracting training examples for docu-
50
ments in Dp, the non-referential classifier trained on
Dtp =
?
i 6=p
Di was applied.
4 Decoding
We implemented several decoding algorithms for
our resolver. The two most common decoding al-
gorithms often found in literature are the so-called
BestFirst (henceforth BF) and ClosestFirst (CF) al-
gorithms (Ng, 2010). Both work in a similar man-
ner and consider mentions linearly ordered as they
occur in the document. They proceed left-to-right
and for every mention mj , they consider all pairs
(mi,mj), where mi precedes mj , and queries the
classifier whether they are coreferent or not. The
main difference between the two algorithms is that
the CF algorithm selects the closest preceding men-
tion deemed coreferent with mj by the classifier,
while the BF algorithm selects the most probable
preceding mention. Most probable is determined
by some sort of confidence measure of how likely
two mentions are to corefer according to the classi-
fier. For both algorithms, the threshold can also be
tuned separately, e.g., requiring a probability larger
than a certain threshold thcoref in order to establish
a link between two mentions. Since the logistic clas-
sifiers we use directly model a probability distribu-
tion, we simply use the posterior probability of the
coref class as our confidence score.
Following Bjo?rkelund and Nugues (2011) we also
implemented a decoder that works differently de-
pending on whether mj is a pronoun or not. Specifi-
cally, for pronouns, the CF algorithm is used, other-
wise the BF algorithm is used. In the remainder, we
shall refer to this decoder as PronounsClosestFirst,
or simply PCF.
4.1 Disallowing transitive nesting
A specific kind of mistake we frequently saw in our
output is that two clearly disreferent nested mentions
are put in the same cluster. Although nestedness
can be used as a feature for the classifier, and this
appeared to improve performance, two nested men-
tions can still be put into the same cluster because
they are both classified as coreferent with a different,
preceding mention. The end result is that the two
nested mentions are inadvertently clustered through
transitivity.
For example, consider the two occurrences of the
phrase her mother in (1) below. The spans in the ex-
ample are labeled alphabetically according to their
linear order in the document.3 Before the resolver
considers the last mention d, it has already success-
fully placed (a, c) in the same cluster. The first pair
involving d is (c, d), which is correctly classified as
disreferent (here, the feature set informs the classi-
fier that (c, d) are nested). However, the pair (a, d)
is easily classified as coreferent since the head noun
of a agrees in gender and number with d (and they
are not nested).
A different problem is related to named entities
in possessive constructions. Consider (2), where our
mention extractor extracted e, because it was an NP,
and f , because it was tagged as a GPE by the named
entity recognizer. Again, the pair (e, f) is correctly
classified as disreferent, but both e and f are likely
to be classified as coreferent with preceding men-
tions of Taiwan, since our string matching feature
ignores possessive markers.
(1) ... she seemed to have such a good relation-
ship with [[her]b mother]a. Like [[her]d mother]c
treated her like a human being ...
(2) [[Taiwan]f ?s]e
To circumvent this problem, we let the decoders
build the clusters incrementally as they work their
way through a document and disallow this type of
transitive nesting. For instance, when the decoder is
trying to find an antecedent for d in (1), a and c have
already been clustered together, and when the pair
(c, d) is classified as disreferent, the decoder is con-
strained to skip over other members of c?s cluster as
it moves backwards in the document. This modifi-
cation gave an increase of about 0.6 in the CoNLL
score for English, and about 0.4 for Arabic and Chi-
nese, and we used this constraint whenever we use
the above-mentioned decoders.
4.2 A Cluster-Mention Decoding Algorithm
The pair-wise classifier architecture has, justifiably,
received much criticism as it makes decisions based
on single pairs of mentions only. We therefore de-
3We impose a total order on the mentions by sorting them
by starting point. For multiple mentions with the same starting
point, the longer is considered to precede the shorter.
51
vised a decoding algorithm that has a better perspec-
tive on entire clusters.
The algorithm works by incrementally merging
clusters as mentions are processed. Initially, every
mention forms its own cluster. When the next men-
tion mj is processed, it is compared to all the pre-
ceding mentions, M = {mi|i < j}. The score of
linking mj with mi is defined according to:
score(mi,mj) = (
?
mc?C
P (coref |(mc,mj)))
1/|C|
where P (coref |(mi,mj)) is the posterior probabil-
ity that mi and mj are coreferent according to the
pair-wise classifier, and C denotes the cluster that
mi belongs to.
After considering all preceding mentions, the
cluster of mj is merged with the cluster of the men-
tion with which it had the highest score, assuming
this score is higher than a given threshold thcoref .
Otherwise it remains in its own cluster.
The task of the score function is to capture
cluster-level information. When mj is compared to
a mention mi, the score is computed as the geo-
metric mean of the product of the probabilities of
linking mj to all mentions in the cluster that mi
belongs to. Also note that for two preceding men-
tions mi1 and mi2 that already belong to the same
cluster, score(mi1 ,mj) = score(mi2 ,mj). I.e., the
score is the same when mj is compared to all men-
tions belonging to the same cluster. Since this algo-
rithm works by maximizing the average probability
for linking a mention, we dub this algorithm Aver-
ageMaxProb, or AMP for short.
It should also be noted that other definitions
of the cluster score function score are conceiv-
able.4 However, initial experiments with other clus-
ter score functions performed worse than the defi-
nition above, and time prevented us from exploring
this conclusively.
Contrary to the pair-wise decoding algorithms
where pair-wise decisions are made in isolation, the
order in which mentions are processed make a dif-
ference to the AMP decoder. It is generally ac-
cepted that named entities are more informative and
4In the extreme case, one could take the maximum of the
link probabilities over the mentions that belong to the cluster
C, in which case the algorithm collapses into the BF algorithm.
easier to resolve than common noun phrases and
pronouns. To leverage this, we follow Sapena et
al. (2010) who reorder mentions based on mention
type. Specifically, we first process proper noun
phrases, then common noun phrases, and finally pro-
nouns. This implies that common noun phrases
have to have a reasonable agreement not only with
preceding proper noun phrases of a cluster, but all
proper noun phrases in a document (where reason-
able means that the geometric average of all poste-
rior probabilities stay reasonably high). Similarly,
pronouns are forced agree reasonably with all proper
and common nouns phrases in a given cluster, and
not only the preceding ones. Early experiments
showed an increase in performance using reorder-
ing, and we consequently used reordering for all lan-
guages in the experiments.
5 Features
An advantage of the pair-wise model and of the lin-
ear classifiers we use is that they can easily accom-
modate very large feature spaces, while still remain-
ing reasonably fast. We exploited this by building a
large number of parametrized feature templates, that
allowed us to experiment easily and quickly with
different feature sets. Additionally, since our clas-
sifiers are linear, we also evaluated a large number
of feature conjunctions, which proved to be crucial
to gain reasonable performance.
Due to space restrictions we can not list the com-
plete set of features used in this paper but mention
briefly what type of features we used. Most of them
are taken from previous work on coreference reso-
lution (Soon et al, 2001; Luo and Zitouni, 2005;
Sapena et al, 2010; Bjo?rkelund and Nugues, 2011).
For a complete list of features the reader can refer
to the download of the resolver, which includes the
feature sets and parameters used for every language.
One set of feature templates we use is based on
surface forms and part-of-speech tags of the first and
last, previous and following, and head tokens of the
spans that make up mentions. Another set of tem-
plates are based on the syntax trees, including both
subcategorization frames as well as paths in the syn-
tax tree. To extract head words of mentions, we
used the head percolation rules of Choi and Palmer
(2010) for Arabic and English, and those of Zhang
52
and Clark (2011) for Chinese.
While Chinese and English display no or rela-
tively small variety in morphological inflection, Ara-
bic has a very complex morphology. This means
that Arabic suffers from greater data sparseness with
respect to lexical features. This is exaggerated by
the fact that the Arabic training set is considerably
smaller than the Chinese and English ones. Hence,
we used the lemmas and unvocalised Buckwalter
forms that were provided in the Arabic dataset.
We also tried to extract number and gender in-
formation based on affixes of Arabic surface forms.
These features did, however, not help much. We
did however see a considerable increase in perfor-
mance when we added features that correspond to
the Shortest Edit Script (Myers, 1986) between sur-
face forms and unvocalised Buckwalter forms, re-
spectively. We believe that edit scripts are better at
capturing the differences in gender and number sig-
naled by certain morphemes than our hand-crafted
rules.
6 Resolver Stacking
In Table 2 we present a comparison of the BF, PCF,
and AMP resolvers. We omit the results of the CF
decoder, since it always did worse and the corre-
sponding numbers would not add more to the pic-
ture. The table shows F-measures of mention de-
tection (MD), the MUC metric, the B3 metric, and
the entity-based CEAF metric. The CoNLL score,
which is computed as the arithmetic mean of MUC,
B3, and CEAFE, is shown in the last row.
Comparing the AMP decoder to the pair-wise de-
coders, we find that it generally ? i.e., with respect
to the CoNLL average ? performs worse though it
always obtains higher scores with the CEAFE met-
ric. When we looked at the precision and recall for
mention detection, we also found that the AMP de-
coder suffers from lower recall, but higher precision.
This led us to conclude that this decoder is more con-
servative in terms of clustering mentions, and builds
smaller, but more consistent clusters. We could also
verify this when we computed average cluster sizes
on the output of the different decoders.
In order to combine the strengths of the AMP
decoder and the pair-wise decoders we employed
stacking, i.e., we feed the output of one resolver
Arabic BF PCF AMP Stacked
MD 58.63 58.49 58.21 60.51
MUC 45.8 45.4 43.2 46.66
B3 66.65 66.56 66.39 66.3
CEAFE 41.52 41.58 43.1 42.57
CoNLL 51.32 51.18 50.9 51.84
Chinese BF PCF AMP Stacked
MD 67.22 67.19 66.79 67.61
MUC 59.58 59.43 57.23 59.84
B3 72.9 72.82 72.7 73.35
CEAFE 46.99 46.98 48.25 47.7
CoNLL 59.82 59.74 59.39 60.30
English BF PCF AMP Stacked
MD 74.33 74.42 73.75 74.96
MUC 66.76 66.93 62.74 67.12
B3 70.96 71.11 68.05 71.18
CEAFE 45.46 45.83 46.49 46.84
CoNLL 61.06 61.29 59.09 61.71
Table 2: Performance of different decoders on the devel-
opment set for each language. The configuration of the
Stacked systems is described in detail in Section 7.
as input to a second. The second resolver is in-
formed about the decision of the first one by intro-
ducing an additional feature that encodes the deci-
sion of the first resolver. This feature can take five
values, depending on how the first resolver treated
the two mentions in question: NEITHER, when none
of the mentions were placed in a cluster; IONLY,
when only the first (antecedent) mention was placed
in a cluster; JONLY, when only the second (anaphor)
mention was placed in a cluster; COREF, when both
mentions were placed in the same cluster; and DIS-
REF, when both mentions were clustered, but in dif-
ferent clusters.
In addition to the stacking feature, the second re-
solver uses the exact same feature set as the first re-
solver. To generate the information for the stack fea-
ture for training, we made a 10-fold cross-annotation
on the training set, in the same way that we cross-
annotated the non-referential classifier for English.
In early stacking experiments, we experimented
with several combinations of the different decoders.
We found that stacking different pair-wise decoders
did not give any improvement. We believe the rea-
son for this is that these decoders are too similar and
hence can not really benefit from each other. How-
ever, when we used the AMP decoder as the first
53
step, and a pair-wise decoder as the second, we saw
an increase in performance, particularly with respect
to the CEAFE metric.
7 Feature and Parameter Tuning
For every language we tuned decoder parameters
and feature sets individually. The feature sets were
tuned semi-automatically by evaluating the addition
of a new feature template (or template conjunction)
to a baseline set. Ideally, we would add feature
templates to the baseline set incrementally one at a
time, following a cross-validation on the training set.
However, to reduce computational effort and time
consumption, we resorted to doing only one or two
folds out of a 4-fold cross-validation, and adding the
two to three most contributing templates in every it-
eration to the baseline set. The feature sets were op-
timized to maximize the official CoNLL score using
the standard BF decoder.
For the final submission we tuned the thresholds
for each decoder, and the choice of pair-wise de-
coder to use as the second decoder for each lan-
guage. Modifying the threshold of the AMP decoder
gave very small differences in overall score and we
kept the threshold for this decoder at 0.5. How-
ever, when we increased the probability threshold
for the second resolver, we found that performance
increased across all languages.
The choice of decoder for the second resolver, and
the probability threshold for this, was determined by
a 4-fold cross-validation on the training set. For our
final submission, as well as in the column Stacked
in Table 2, we used the following combinations: For
Arabic, the threshold was set to 0.60, and the PCF
decoder was used; for Chinese, the threshold was set
to 0.65, and the BF decoder was used; for English,
the threshold was set to 0.65, and the PCF decoder
was used.
8 Official Results
The final scores of our system are presented in Ta-
ble 3. The table also includes the results on the sup-
plementary tracks: gold mention boundaries (GB),
when the perfect boundaries of mentions were given;
and gold mentions (GM), when only the mentions in
the gold standard were given (with gold boundaries).
For all three settings we used the same model, which
Arabic PM GB GM
MD 60.55 60.61 76.43
MUC 47.82 47.90 60.81
B3 68.54 68.61 67.29
CEAFE 44.3 44 49.32
CoNLL 53.55 53.50 59.14
Chinese PM GB GM
MD 66.37 71.02 83.47
MUC 58.61 63.56 76.85
B3 73.10 74.52 76.30
CEAFE 48.19 50.20 56.61
CoNLL 59.97 62.76 69.92
English PM GB GM
MD 75.38 75.3 86.16
MUC 67.58 67.29 78.70
B3 70.26 69.70 72.67
CEAFE 45.87 45.27 53.23
CoNLL 61.24 60.75 68.20
Table 3: Performance on the shared task test set. Us-
ing predicted mentions (PM; i.e., the official evalua-
tion), gold mentions boundaries (GB), and gold mentions
(GM).
was trained on the concatenation of the training and
the development sets.
Compared to the results on the development set
(cf. Table 2), we see a slight drop for Chinese and
English, but a fairly big increase for Arabic. Given
that Chinese and English have the biggest training
sets, we speculate that the increase in Arabic might
stem from the increased lexical coverage provided
by training on both the training and the development
sets.
9 Conclusion
We have presented a novel cluster-based coreference
resolution algorithm. This algorithm was combined
with conventional pair-wise resolution algorithms in
a stacking approach. We applied our system to all
three languages in the Shared Task, and obtained an
official overall final score of 58.25 which was the
second highest in the Shared Task.
Acknowledgments
This work was supported by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB 732
?Incremental Specification in Context?, projects D4
(PI Helmut Schmid) and D8 (PI Jonas Kuhn).
54
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50, June.
Adriane Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: A machine
learning approach incorporating linguistically moti-
vated patterns. In Proceedings of the ACL Workshop
on Feature Engineering for Machine Learning in Nat-
ural Language Processing, pages 40?47, June.
Jinho D. Choi and Martha Palmer. 2010. Robust
Constituent-to-Dependency Conversion for English.
In Proceedings of 9th Treebanks and Linguistic The-
ories Workshop (TLT), pages 55?66.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660?667, October.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Eugene W. Myers. 1986. An O(ND) difference algo-
rithm and its variations. Algorithmica, 1:251?266.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1396?1411, July.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012).
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010. A
global relaxation labeling approach to coreference res-
olution. In Coling 2010: Posters, pages 1086?1094,
August.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
55
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Linguistic Analysis Using OntoNotes
Sameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5
Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong5
1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA
2 University of Trento, University of Trento, 38123 Povo (TN), Italy
3 QCRI, Qatar Foundation, 5825 Doha, Qatar
4 Brandeis University, Brandeis University, Waltham, MA 02453, USA
5 National University of Singapore, Singapore, 117417
6 University of Stuttgart, 70174 Stuttgart, Germany
Abstract
Large-scale linguistically annotated cor-
pora have played a crucial role in advanc-
ing the state of the art of key natural lan-
guage technologies such as syntactic, se-
mantic and discourse analyzers, and they
serve as training data as well as evaluation
benchmarks. Up till now, however, most
of the evaluation has been done on mono-
lithic corpora such as the Penn Treebank,
the Proposition Bank. As a result, it is still
unclear how the state-of-the-art analyzers
perform in general on data from a vari-
ety of genres or domains. The completion
of the OntoNotes corpus, a large-scale,
multi-genre, multilingual corpus manually
annotated with syntactic, semantic and
discourse information, makes it possible
to perform such an evaluation. This paper
presents an analysis of the performance of
publicly available, state-of-the-art tools on
all layers and languages in the OntoNotes
v5.0 corpus. This should set the bench-
mark for future development of various
NLP components in syntax and semantics,
and possibly encourage research towards
an integrated system that makes use of the
various layers jointly to improve overall
performance.
1 Introduction
Roughly a million words of text from the Wall
Street Journal newswire (WSJ), circa 1989, has
had a significant impact on research in the lan-
guage processing community ? especially those
in the area of syntax and (shallow) semantics, the
reason for this being the seminal impact of the
Penn Treebank project which first selected this text
for annotation. Taking advantage of a solid syn-
tactic foundation, later researchers who wanted to
annotate semantic phenomena on a relatively large
scale, also used it as the basis of their annota-
tion. For example the Proposition Bank (Palmer et
al., 2005), BBN Name Entity and Pronoun coref-
erence corpus (Weischedel and Brunstein, 2005),
the Penn Discourse Treebank (Prasad et al, 2008),
and many other annotation projects, all annotate
the same underlying body of text. It was also con-
verted to dependency structures and other syntac-
tic formalisms such as CCG (Hockenmaier and
Steedman, 2002) and LTAG (Shen et al, 2008),
thereby creating an even bigger impact through
these additional syntactic resources. The most re-
cent one of these efforts is the OntoNotes corpus
(Weischedel et al, 2011). However, unlike the
previous extensions of the Treebank, in addition
to using roughly a third of the same WSJ subcor-
pus, OntoNotes also added several other genres,
and covers two other languages ? Chinese and
Arabic: portions of the Chinese Treebank (Xue et
al., 2005) and the Arabic Treebank (Maamouri and
Bies, 2004) have been used to sample the genre of
text that they represent.
One of the current hurdles in language process-
ing is the problem of domain, or genre adaptation.
Although genre or domain are popular terms, their
definitions are still vague. In OntoNotes, ?genre?
means a type of source ? newswire (NW), broad-
cast news (BN), broadcast conversation (BC), mag-
azine (MZ), telephone conversation (TC), web data
(WB) or pivot text (PT). Changes in the entity and
event profiles across source types, and even in the
same source over a time duration, as explicitly ex-
pressed by surface lexical forms, usually account
for a lot of the decrease in performance of mod-
els trained on one source and tested on another,
usually because these are the salient cues that are
relied upon by statistical models.
Large-scale corpora annotated with multiple
layers of linguistic information exist in various
languages, but they typically consist of a single
source or collection. The Brown corpus, which
consists of multiple genres, have been usually used
to investigate issues of genres of sensitivity, but it
is relatively small and does not include any infor-
1A portion of the English data in the OntoNotes corpus
is a selected set of sentences that were annotated for parse
and word sense information. These sentences are present in a
document of their own, and so the documents for parse layers
for English are inflated by about 3655 documents and for the
word sense are inflated by about 8797 documents.
143
Language Parse Proposition Sense Name Coreference
Documents Words Documents Verb Prop. Noun Prop. Documents Verb Sense Noun Sense Documents Words Documents Words
English 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7M
Chinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950K
Arabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300K
Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and
some other attributes. The numbers in parenthesis are the total number of parts in the documents.
mal genres such as web data. Very seldom has it
been the case that the exact same phenomena have
been annotated on a broad cross-section of the
same language before OntoNotes. The OntoNotes
corpus thus provides an opportunity for studying
the genre effect on different syntactic, semantic
and discourse analyzers.
Parts of the OntoNotes Corpus have been used
for various shared tasks organized by the language
processing community. The word sense layer was
the subject of prediction in two SemEval-2007
tasks, and the coreference layer was the subject
of prediction in the SemEval-20102 (Recasens et
al., 2010), CoNLL-2011 and 2012 shared tasks
(Pradhan et al, 2011; Pradhan et al, 2012). The
CoNLL-2012 shared task provided predicted in-
formation to the participants, however, that did not
include a few layers such as the named entities
for Chinese and Arabic, propositions for Arabic,
and for better comparison of the English data with
the CoNLL-2011 task, a smaller OntoNotes v4.0
portion of the English parse and propositions was
used for training.
This paper is a first attempt at presenting a co-
herent high-level picture of the performance of
various publicly available state-of-the-art tools on
all the layers of OntoNotes in all three languages,
so as to pave the way for further explorations in
the area of syntax and semantics processing.
The possible avenues for exploratory studies
on various fronts are enormous. However, given
space considerations, in this paper, we will re-
strict our presentation of the performance on all
layers of annotation in the data by using a strat-
ified cross-section of the corpus for training, de-
velopment, and testing. The paper is organized
as follows: Section 2 gives an overview of the
OntoNotes corpus. Section 3 explains the param-
eters of the evaluation and the various underlying
assumptions. Section 4 presents the experimental
results and discussion, and Section 5 concludes the
paper.
2 OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of
2A small portion 125K words in English was used for this
evaluation.
multiple layers of syntactic, semantic and dis-
course information in text. The English lan-
guage portion comprises roughly 1.7M words and
Chinese language portion comprises roughly 1M
words of newswire, magazine articles, broadcast
news, broadcast conversations, web data and con-
versational speech data3. The Arabic portion is
smaller, comprising 300K words of newswire ar-
ticles. This rich, integrated annotation covering
many layers aims at facilitating the development
of richer, cross-layer models and enabling bet-
ter automatic semantic analysis. The corpus is
tagged with syntactic trees, propositions for most
verb and some noun instances, partial verb and
noun word senses, coreference, and named enti-
ties. Table 1 gives an overview of the number of
documents that have been annotated in the entire
OntoNotes corpus.
2.1 Layers of Annotation
This section provides a very concise overview of
the various layers of annotations in OntoNotes.
For a more detailed description, the reader is re-
ferred to (Weischedel et al, 2011) and the docu-
mentation accompanying the v5.04 release.
2.1.1 Syntax
This represents the layer of syntactic annotation
based on revised guidelines for the Penn Tree-
bank (Marcus et al, 1993; Babko-Malaya et al,
2006), the Chinese Treebank (Xue et al, 2005)
and the Arabic Treebank (Maamouri and Bies,
2004). There were two updates made to the parse
trees as part of the OntoNotes project: i) the in-
troduction of NML phrases, in the English portion,
to mark nominal sub-constituents of flat NPs that
do not follow the default right-branching structure,
and ii) re-tokenization of hyphenated tokens into
multiple tokens in English and Chinese. The Ara-
bic Treebank on the other hand was also signifi-
cantly revised in an effort to increase consistency.
2.1.2 Word Sense
Coarse-grained word senses are tagged for the
most frequent polysemous verbs and nouns, in or-
3These numbers are for the portion that has all layers of
annotations. The word count for each layer is mentioned in
Table 1
4For all the layers of data used in this study, the
OntoNotes v4.99 pre-release that was used for the CoNLL-
2012 shared task is identical to the v5.0 release.
144
der to maximize token coverage. The word sense
granularity is tailored to achieve very high inter-
annotator agreement as demonstrated by Palmer et
al. (2007). These senses are defined in the sense
inventory files. In the case of English and Arabic
languages, the sense-inventories (and frame files)
are defined separately for each part of speech that
is realized by the lemma in the text. For Chinese,
however the sense inventories (and frame files) are
defined per lemma ? independent of the part of
speech realized in the text.
2.1.3 Proposition
The propositions in OntoNotes are PropBank-style
semantic roles for English, Chinese and Arabic.
Most English verbs and few nouns were anno-
tated using the revised guidelines for the English
PropBank (Babko-Malaya et al, 2006) as part of
the OntoNotes effort. Some enhancements were
made to the English PropBank and Treebank to
make them synchronize better with each other:
one of the outcomes of this effort was that two
types of LINKs that represent pragmatic coref-
erence (LINK-PCR) and selectional preferences
(LINK-SLC) were added to the original PropBank
(Palmer et al, 2005). More details can be found in
the addendum to the PropBank guidelines5 in the
OntoNotes v5.0 release. A part of speech agnostic
Chinese PropBank (Xue and Palmer, 2009) guide-
lines were used to annotate most frequent lem-
mas in Chinese. Many verbs and some nouns and
adjectives were annotated using the revised Ara-
bic PropBank guidelines (Palmer et al, 2008; Za-
ghouani et al, 2010).
2.1.4 Named Entities
The corpus was tagged with a set of 18 well-
defined proper named entity types that have been
tested extensively for inter-annotator agreement
by Weischedel and Burnstein (2005).
2.1.5 Coreference
This layer captures general anaphoric corefer-
ence that covers entities and events not limited
to noun phrases or a limited set of entity types
(Pradhan et al, 2007). It considers all pronouns
(PRP, PRP$), noun phrases (NP) and heads of verb
phrases (VP) as potential mentions. Unlike En-
glish, Chinese and Arabic have dropped subjects
and objects which were also considered during
coreference annotation6. The mentions formed by
these dropped pronouns total roughly about 11%
for both Chinese and Arabic. Coreference is the
only document-level phenomenon in OntoNotes.
Some of the documents in the corpus ? especially
the ones in the broadcast conversation, web data,
5doc/propbank/english-propbank.pdf
6As we will see later these are not used during the task.
and telephone conversation genre ? are very long
which prohibited efficient annotation in their en-
tirety. These are split into smaller parts, and each
part is considered a separate document for the sake
of coreference evaluation.
3 Evaluation Setting
Given the scope of the corpus and the multitude of
settings one can run evaluations, we had to restrict
this study to a relatively focused subset. There has
already been evidence of models trained on WSJ
doing poorly on non-WSJ data on parses (Gildea,
2001; McClosky et al, 2006), semantic role label-
ing (Carreras and Ma`rquez, 2005; Pradhan et al,
2008), word sense (Escudero et al, 2000; ?), and
named entities. The phenomenon of coreference is
somewhat of an outlier. The winning system in the
CoNLL-2011 shared task was one that was com-
pletely rule-based and not directly trained on the
OntoNotes corpus. Given this overwhelming evi-
dence, we decided not to focus on potentially com-
plex cross-genre evaluations. Instead, we decided
on evaluating the performance on each layer of an-
notation using an appropriately selected, stratified
training, development and test set, so as to facili-
tate future studies.
3.1 Training, Development and Test
Partitions
In this section we will have a brief discussion
on the logic behind the partitioning of the data
into training, development and test sets. Before
we do that, it would help to know that given the
range and peculiarities of the layers of annota-
tion and presence of various resource and techni-
cal constraints, not all the documents in the cor-
pus are annotated with all the layers of informa-
tion, and token-centric phenomena (such as word
sense and propositions of predicates) were not an-
notated with 100% coverage. Most of the propo-
sition annotation in English and Arabic is for the
verb predicates, with a few nouns annotated in
English and some adjectives in Arabic. In Chi-
nese, the selection is part of speech agnostic, and is
based on the lemmas that can be considered predi-
cates. Some documents in the corpora are actually
snippets from larger documents, and have been an-
notated for a combination of parse, propositions,
word sense and names, but not coreference. If one
considers each layer independently, then an ideal
partitioning scheme would create a separate parti-
tion for each layer such that it maximizes the num-
ber of examples that can be extracted for that layer
from the corpus. The upside is that one would
get as much data there is to train and estimate the
performance of each layer across the entire cor-
pus. The downside is that this might cover vari-
145
ous cross sections of the documents in the corpus,
and would not provide a clean picture when look-
ing at the collective performance for all the lay-
ers. The documents that are annotated with coref-
erence correspond to the intersection of all anno-
tations. These are the documents that have also
been annotated with all the other layers of infor-
mation. The amount of data we can get together
in such a test set is big enough to be represen-
tative. Therefore, we decided that it would be
ideal to choose a portion of these documents as
the test collection for all layers. An additional ad-
vantage is that it is the exact same test set used
in the CoNLL-2012 shared task, and so in a way
is already a standard. On the training and devel-
opment side however, one can still imagine using
all possible information for training models for a
particular layer, and that is what we decided to
do. The training and development data is gener-
ated by providing all documents with all available
layers of annotation for input, however, the test
set is generated by providing as input to the algo-
rithm the set of documents in the corpus that have
been annotated for coreference. This algorithm
tries to reuse previously established partitions for
English, i.e., the WSJ portion. Unfortunately, in
the case of Chinese and Arabic, either the histor-
ical partitions were not in the selection used for
OntoNotes, or were partially overlapping with the
ones created using this scheme, and/or had a very
small portion of OntoNotes covered in the test set.
Therefore, we decided to create a fresh partition
for the Chinese and Arabic data. Note, however,
that the these test sets also match the ones used
in the CoNLL-2012 evaluation. The algorithm for
selecting the training, development and test parti-
tions is described on the CoNLL-2012 shared task
webpage, along with the list of training, develop-
ment, and test document IDs7.
3.2 Assumptions
Next we had to decide on a set of assumptions
to use while designing the experiments to mea-
sure the automatic prediction accuracy for each of
the layers. Since some of these decisions affect
more than one layer of annotation, we will de-
scribe these in this section instead of in the section
where we discuss the experiment with a particular
layer of annotation.
7http://conll.cemantix.org/2012/download/ids/
For each language there are two sub-directories ? ?all?
contains more general lists which include documents
that had at least one of the layers of annotation, and
?coref? contains the lists that include documents that
have coreference annotation. The former were used to
generate training, development, test sets for layers other
than coreference, and the latter was used to generate
training/development/test sets for the coreference layer
used in the CoNLL-2012 shared task.
Word Segmentation The three languages that
we are evaluating are from quite different lan-
guage families. Arabic has a complex morphol-
ogy, English has limited morphology, whereas
Chinese has very little morphology. English word
segmentation amounts to rule-based tokenization,
and is close to perfect. In the case of Chinese and
Arabic, although the tokenization/segmentation is
not as good as English, the accuracies are in the
high 90s. Given this we decided to use gold,
Treebank segmentation for all languages. In the
case of Chinese, the words themselves are lem-
mas, whereas in English they can be predicted
with very high accuracy. For Arabic, by default
written text is unvocalised, and lemmatization is a
complex process which we considered out of the
scope of this study, so we decided to use correct,
gold standard lemmas, along with the correct vo-
calized version of the tokens.
Traces and Function Tags Treebank traces
have hardly played a role in the mainstream parser
and semantic role labeling evaluation. Function
tags also have received similar treatment in the
parsing community, and though they are impor-
tant, there is also a significant information overlap
between them and the proposition structure pro-
vided by the PropBank layer. Whereas in English,
most traces represent syntactic phenomena such
as movement and raising, in Chinese and Arabic,
they can also represent dropped subjects/objects.
These subset of traces directly affect the corefer-
ence layer, since, unlike English, traces in Chinese
and Arabic (*pro* and * respectively) are legit-
imate targets of mentions and are considered for
coreference annotation in OntoNotes. Recovering
traces in text is a hard problem, and the most re-
cently reported numbers in literature for Chinese
are around a F-score of 50 (Yang and Xue, 2010;
Cai et al, 2011). For Arabic there have not been
much studies on recovering these. A study by
Gabbard (2010) shows that these can be recovered
with an F-score of 55 with automatic parses and
roughly 65 using gold parses. Considering the low
level of prediction accuracy of these tokens, and
their relative low frequency, we decided to con-
sider predicting traces in trees out of the scope of
this study. In other words, we removed the man-
ually identified traces and function tags from the
Treebanks across all three languages, in all the
three ? training, development and test partitions.
This meant removing any and all dependent an-
notation in layers such as PropBank and Coref-
erence. In the case of PropBank these are the
argument bearing traces, whereas in coreference
these are the mentions formed by these elided sub-
jects/objects.
146
Disfluencies One thing that needs to be dealt
with in conversational data is the presence of dis-
fluencies (restarts, etc.). In the English parses of
the OntoNotes, disfluencies are marked using a
special EDITED8 phrase tag ? as was the case for
the Switchboard Treebank. Computing the accu-
racy of identifying disfluencies is also out of the
scope of this study. Given the frequency of dis-
fluencies and the performance with which one can
identify them automatically,9 a probable process-
ing pipeline would filter them out before parsing.
We decided to remove them using oracle infor-
mation available in the English Treebank, and the
coreference chains were remapped to trees with-
out disfluencies. Owing to various technical con-
straints, we decided to retain the disfluencies in the
Chinese data.
Spoken Genre Given the scope of this study, we
make another significant assumption. For the spo-
ken genres ? BC, BN and TC ? we use the manual
transcriptions rather than the output of a speech
recognizer, as would be the case in real world. The
performance on various layers for these genres
would therefore be artificially inflated, and should
be taken into account while analyzing results. Not
many studies have previously reported on syntac-
tic and semantic analysis for spoken genre. Favre
et al (2010) report the performance on the English
subset of an earlier version of OntoNotes.
Discourse The corpus contains information on
the speaker for broadcast communication, conver-
sation, telephone conversation and writer for the
web data. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be
automatically deduced, but is also not within the
scope of our study. Therefore, we decided to pro-
vide gold, instead of predicted, data both during
training and testing. Table 2 lists the status of the
layers.
4 Experiments
In this section, we will report on the experiments
carried out using all available data in the train-
ing set for training models for a particular layer,
and using the CoNLL-2012 test set as the test set.
8There is another phrase type ? EMBED in the telephone
conversation genre which is similar to the EDITED phrase
type, and sometimes identifies insertions, but sometimes con-
tains logical continuation of phrases by different speakers, so
we decided not to remove that from the data.
9A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 precision
and 67 recall.
10The predicted part of speech for Arabic are a mapped
down version of the richer gold version present in the Tree-
bank
Layer English Chinese Arabic
Segmentation ? ? ?
Lemma ? ? ?
Parse ? ? ?10
Proposition ? ? ?
Predicate Frame ? ? ?
Word Sense ? ? ?
Name Entities ? ? ?
Coreference ? ? ?
Speaker ? ? ?
Number ? ? ?
Gender ? ? ?
Table 2: Status of layers used during prediction
of other layers. A ??? indicates gold annotation,
a ??? indicates predicted, a ??? indicates an ab-
sence of the predicted layer, and a ??? indicates
that the layer is not applicable to the language.
The predicted annotation layers input to down-
stream models were automatically annotated by
using NLP processors learned with n-cross fold
validation on the training data. This way, the n
chunks of training data are annotated avoiding de-
pendencies with the data used for training the NLP
processors.
4.1 Syntax
Predicted parse trees for English were produced
using the Charniak parser11 (Charniak and John-
son, 2005). Some additional tag types used in
the OntoNotes trees were added to the parser?s
tagset, including the nominal (NML) tag, and the
rules used to determine head words were extended
correspondingly. Chinese and Arabic parses were
generated using the Berkeley parser (Petrov and
Klein, 2007). In the case of Arabic, the pars-
ing community uses a mapping from rich Arabic
part of speech tags to Penn-style part of speech
tags. We used the mapping that is included with
the Arabic Treebank. The predicted parses for
the training portion of the data were generated us-
ing 10-fold (5-folds for Arabic) cross-validation.
For testing, we used a model trained on the entire
training portion. Table 3 shows the precision, re-
call and F1-scores of the re-trained parsers on the
CoNLL-2012 test along with the part of speech ac-
curacies (POS) using the standard evalb scorer.
The performance on the PT genre for English is
the highest among other English genres. This is
possibly because of the professional, clean trans-
lations of the underlying text, and are mostly
shorter sentences. The MZ genre and the NW both
of which contain well edited text, share similar
scores. There is a few points gap between these
and the other genres. As for Chinese, the per-
formance on MZ is the highest followed by BN.
Surprisingly, the WB genre has a similar score and
the others are close behind except for TC. As ex-
pected, the Arabic parser performance is the low-
11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz
147
All Sentences
N POS P R F
English BC 2,211 97.33 86.36 86.11 86.23
BN 1,357 97.32 87.61 87.03 87.32
MZ 780 96.58 89.90 89.49 89.70
NW 2,327 97.15 87.68 87.25 87.47
TC 1,366 96.11 85.09 84.13 84.60
WB 1,787 96.03 85.46 85.26 85.36
PT 1,869 98.77 95.29 94.66 94.98
Overall 11,697 97.09 88.08 87.65 87.87
Chinese BC 885 94.79 80.17 79.35 79.76
BN 929 93.85 83.49 80.13 81.78
MZ 451 97.06 88.48 83.85 86.10
NW 481 94.07 82.26 77.28 79.69
TC 968 92.22 71.90 69.19 70.52
WB 758 92.37 82.57 78.92 80.70
Overall 4,472 94.12 82.23 78.93 80.55
Arabic NW 1,003 94.12 74.71 75.67 75.19
Table 3: Parser performance on the CoNLL-2012
test set.
est among the three languages.
4.2 Word Sense
We used the IMS12 (It Makes Sense) (Zhong and
Ng, 2010) word sense tagger. IMS was trained on
all the word sense data that is present in the train-
ing portion of the OntoNotes corpus using cross-
validated predictions on the input layers similar
to the proposition tagger. During testing, for En-
glish and Arabic, IMS must first use the auto-
matic POS information to identify the nouns and
verbs in the test data, and then assign senses to
the automatically identified nouns and verbs. In
the case of Arabic, IMS uses gold lemmas. Since
automatic POS tagging is not perfect, IMS does
not always output a sense to all word tokens that
need to be sense tagged due to wrongly predicted
POS tags. As such, recall is not the same as pre-
cision on the English and Arabic test data. For
Chinese the measure of performance is just the
accuracy since the senses are defined per lemma
rather than per part of speech. Since we provide
gold word segmentation, IMS attempts to sense
tag all correctly segmented Chinese words, so re-
call and precision are the same and so is the F1-
score. Table 4 shows the performance of this clas-
sifier aggregated over both the verbs and nouns
in the CoNLL-2012 test set and an overall score
split by nouns and verbs for English and Ara-
bic. For both nouns and verbs in English, the
F1-score is over 80%. The performance on En-
glish nouns is slightly higher than English verbs.
Comparing to the other two languages, the perfor-
mance on Arabic is relatively lower, especially the
performance on Arabic verbs, whose F1-score is
less than 70%. For English, genres PT and TC,
and for Chinese genres TC and WB, no gold stan-
dard senses were available, and so their accuracies
could not be computed. Previously, Zhong et al
(2008) reported the word sense performance on
the Wall Street Journal portion of an earlier ver-
12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gz
Performance
P R F A
English BC 81.2 81.3 81.2 -
BN 82.0 81.5 81.7 -
MZ 79.1 78.8 79.0 -
NW 85.7 85.7 85.7 -
WB 77.5 77.6 77.5 -
Overall 82.5 82.5 82.5 -
Nouns 83.4 83.1 83.2 -
Verbs 81.8 81.9 81.8 -
Chinese BC - - - 80.5
BN - - - 85.4
MZ - - - 82.4
NW - - - 89.1
Overall - - - 84.3
Arabic NW 75.9 75.2 75.6 -
Nouns 79.2 77.7 78.4 -
Verbs 68.8 69.5 69.1 -
Table 4: Word sense performance on the CoNLL-
2012 test set.
sion of OntoNotes, but the results are not directly
comparable.
4.3 Proposition
The revised PropBank has introduced two new
links ? LINK-SLC and LINK-PCR. Since the com-
munity is not used to the new PropBank represen-
tation which (i) relies heavily on the trace struc-
ture in the Treebank and (ii) we decided to ex-
clude, we unfold the LINKs back to their original
representation as in the PropBank 1.0 release. We
used ASSERT15 (Pradhan et al, 2005) to predict
the propositional structure for English. We made
a small modification to ASSERT, and replaced
the TinySVM classifier with a CRF16 to speed
up training the model on all the data. The Chi-
nese propositional structure was predicted with the
Chinese semantic role labeler described in (Xue,
2008), retrained on the OntoNotes v5.0 data. The
Arabic propositional structure was predicted us-
ing the system described in Diab et al (2008).
(Diab et al, 2008) Table 5 shows the detailed per-
14The Frame ID column indicates the F-score for English
and Arabic, and accuracy for Chinese for the same reasons as
word sense.
15http://cemantix.org/assert.html
16http://leon.bottou.org/projects/sgd
Frame Total Total % Perfect Argument ID + Class
ID Sent. Prop. Prop. P R F
English BC 93.2 1994 5806 52.89 80.76 69.69 74.82
BN 92.7 1218 4166 54.78 80.22 69.36 74.40
MZ 90.8 740 2655 50.77 79.13 67.78 73.02
NW 92.8 2122 6930 46.45 79.80 66.80 72.72
TC 91.8 837 1718 49.94 79.85 72.35 75.91
WB 90.7 1139 2751 42.86 80.51 69.06 74.35
PT 96.6 1208 2849 67.53 89.35 84.43 86.82
Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53
Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38
BN 93.3 929 4,419 35.44 64.34 66.05 65.18
MZ 92.3 451 2,620 31.68 65.04 65.40 65.22
NW 96.6 481 2,210 27.33 69.28 55.74 61.78
TC 82.2 968 1,622 32.74 48.70 59.12 53.41
WB 87.8 758 1,761 35.21 62.35 68.87 65.45
Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83
Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68
Table 5: Proposition and frameset disambiguation
performance14 in the CoNLL-2012 test set.
148
formance numbers17. The CoNLL-2005 scorer18
was used to compute the scores. At first glance,
the performance on the English newswire genre is
much lower than what has been reported for WSJ
Section 23. This could be attributed to several fac-
tors: i) the newswire in OntoNotes not only con-
tains WSJ data, but also Xinhua news, and some
other newswire evaluation data, ii) The WSJ train-
ing and test portions in OntoNotes are a subset of
the standard ones that have been used to report
performance earlier; iii) the PropBank guidelines
were significantly revised during the OntoNotes
project in order to synchronize well with the Tree-
bank, and finally iv) it includes propositions for
be verbs missing from the original PropBank. It
looks like the newly added Pivot Text data (com-
prised of the New Testament) shows very good
performance. The Chinese and Arabic19 accuracy
is much worse. In addition to automatically pre-
dicting the arguments, we also trained the IMS
system to tag PropBank frameset IDs.
Language Genre Entity Performance
Count P R F
English BC 1671 80.17 77.20 78.66
BN 2180 88.95 85.69 87.29
MZ 1161 82.74 82.17 82.45
NW 4679 86.79 84.25 85.50
TC 362 74.09 61.60 67.27
WB 1133 77.72 68.05 72.56
Overall 11186 84.04 80.86 82.42
Chinese BC 667 72.49 58.47 64.73
BN 3158 82.17 71.50 76.46
NW 1453 86.11 76.39 80.96
MZ 1043 65.16 56.66 60.62
TC 200 48.00 60.00 53.33
WB 886 80.60 51.13 62.57
Overall 7407 78.20 66.45 71.85
Arabic NW 2550 74.53 62.55 68.02
Table 6: Performance of the named entity recog-
nizer on the CoNLL-2012 test set.
4.4 Named Entities
We retrained the Stanford named entity recog-
nizer20 (Finkel et al, 2005) on the OntoNotes data.
Table 6 shows the performance details for all the
languages across all 18 name types broken down
by genre. In English, BN has the highest perfor-
mance followed by the NW genre. There is a sig-
nificant drop from those and the TC and WB genre.
Somewhat similar trend is observed in the Chi-
nese data, with Arabic having the lowest scores.
Since the Pivot Text portion (PT) of OntoNotes
was not tagged with names, we could not com-
pute the accuracy for that cross-section of the data.
Previously Finkel and Manning (2009) performed
17The number of sentences in this table are a subset of the
ones in the table showing parser performance, since these are
the sentences for which at least one predicate has been tagged
with its arguments
18http://www.lsi.upc.es/?srlconll/srl-eval.pl
19The system could not not use the morphology features in
Diab et al (2008).
20http://nlp.stanford.edu/software/CRF-NER.shtml
a joint estimation of named entity and parsing.
However, it was on an earlier version of the En-
glish portion of OntoNotes using a different cross-
section for training and testing and therefore is not
directly comparable.
4.5 Coreference
The task is to automatically identify mentions of
entities and events in text and to link the corefer-
ring mentions together to form entity/event chains.
The coreference decisions are made using auto-
matically predicted information on other structural
and semantic layers including the parses, seman-
tic roles, word senses, and named entities that
were produced in the earlier sections. Each docu-
ment part from the documents that were split into
multiple parts during coreference annotation were
treated as separate document.
We used the number and gender predictions
generated by Bergsma and Lin (2006). Unfortu-
nately neither Arabic, nor Chinese have compara-
ble data available. Chinese, in particular, does not
have number or gender inflections for nouns, but
(Baran and Xue, 2011) look at a way to infer such
information.
We trained the Bjo?rkelund and Farkas (2012)
coreference system21 which uses a combination of
two pair-wise resolvers, the first is an incremen-
tal chain-based resolution algorithm (Bjo?rkelund
and Farkas, 2012), and the second is a best-first
resolver (Ng and Cardie, 2002). The two resolvers
are combined by stacking, i.e., the output of the
first resolver is used as features in the second one.
The system uses a large feature set tailored for
each language which, in addition to classic coref-
erence features, includes both lexical and syntactic
information.
Recently, it was discovered that there is pos-
sibly a bug in the official scorer used for the
CoNLL 2011/2012 and the SemEval 2010 corefer-
ence tasks. This relates to the mis-implementation
of the method proposed by (Cai and Strube, 2010)
for scoring predicted mentions. This issue has also
been recently reported in Recasens et al, (2013).
As of this writing, the BCUBED metric has been
fixed, and the correctness of the CEAFm, CEAFe
and BLANC metrics is being verified. We will
be updating the CoNLL shared task webpages22
with more detailed information and also release
the patched scripts as soon as they are available.
We will also re-generate the scores for previous
shared tasks, and the coreference layer in this pa-
per and make them available along with the mod-
els and system outputs for other layers. Table
7 shows the performance of the system on the
21http://www.ims.uni-stuttgart.de/?anders/coref.html
22http://conll.cemantix.org
149
CoNLL-2012 test set, broken down by genre. The
same metrics that were used for the CoNLL-2012
shared task are computed, with the CONLL col-
umn being the official CONLL measure.
Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLL
PREDICTED MENTIONS
English BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19
BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30
MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33
NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80
PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08
TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52
WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78
Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74
Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27
BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09
MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25
NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08
TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59
WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26
Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79
Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09
GOLD MENTIONS
English BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89
BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79
MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21
NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19
PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59
TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01
WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20
Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2
Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17
BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11
MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36
NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28
TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90
WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77
Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92
Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14
Table 7: Performance of the coreference system
on the CoNLL-2012 test set.
The varying results across genres mostly meet
our expectations. In English, the system does best
on TC and the PT genres. The text in the TC set
often involve long chains where the speakers re-
fer to themselves which, given speaker informa-
tion, is fairly easy to resolve. The PT section
includes many references to god (e.g. god and
the lord) which the lexicalized resolver is quite
good at picking up during training. The more dif-
ficult genres consist of texts where references to
many entities are interleaved in the discourse and
is as such harder to resolve correctly. For Chi-
nese the numbers on the TC genre are also quite
good, and the explanation above also holds here
? many mentions refer to either of the speak-
ers. For Chinese the NW section displays by far
the highest scores, however, and the reason for
this is not clear to us. Not surprisingly, restricting
the set of mentions only to gold mentions gives
a large boost across all genres and all languages.
This shows that mention detection (MD) and sin-
gleton detection (which is not part of the annota-
tion) remain a big source of errors for the coref-
erence resolver. For these experiments we used
a combination of training and development data
for training ? following the CoNLL-2012 shared
task specification. Leaving out the development
set has a very negligible effect on the CoNLL-
score for all the languages (English: 0.14; Chi-
nese 0.06; Arabic: 0.40 F-score respectively). The
effect on Arabic is the most (0.40 F-score) most
likely because of its much smaller size. To gauge
the performance improvement between 2011 and
2012 shared tasks, we performed a clean com-
parison of over the best performing system and
an earlier version of this system (Bjo?rkelund and
Nugues, 2011) on the CoNLL 2011 test set us-
ing the CoNLL 2011 train and development set
for training. The current system has a CoNLL
score of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund and
Nugues, 2011), and the 57.79 reported for the best
performing system of CoNLL-2011. One caveat
is that these score comparison are done using the
earlier version (v4) of the CoNLL scorer. Nev-
ertheless, it is encouraging to see that within a
short span of a year, there has been significant
improvement in system performance ? partially
owing to cross-pollination of research generated
through the shared tasks.
5 Conclusion
In this paper we reported work on finding a rea-
sonable training, development and test split for
the various layers of annotation in the OntoNotes
v5.0 corpus, which consists of multiple genres in
three typologically very different languages. We
also presented the performance of publicly avail-
able, state-of-the-art algorithms on all the different
layers of the corpus for the different languages.
The trained models as well as their output will
be made publicly available24 to serve as bench-
marks for language processing community. Train-
ing so many different NLP components is very
time-consuming, thus, we hope the work reported
here has lifted the burden of having to create rea-
sonable baselines for researchers who wish to use
this corpus to evaluate their systems. We created
just one data split in training, development and test
set, covering a collection of genres for each layer
of annotation in each language in order to keep the
workload manageable However, the results do not
discriminate the performance on individual gen-
res: we believe such a setup is still a more realistic
gauge for the performance of the state-of-the-art
NLP components than a monolithic corpus such
as the Wall Street Journal section of the Penn Tree-
bank. It can be used as a starting point for devel-
oping the next generation of NLP components that
are more robust and perform well on a multitude
of genres for a variety of different languages.
23(MUC + BCUBED + CEAFe)/3
24http://cemantix.org
150
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022for sponsoring the creation of the OntoNotes
corpus. This work was partially supported
by grants R01LM10090 and U54LM008748
from the National Library Of Medicine, and
R01GM090187 from the National Institutes ofGeneral Medical Sciences. We are indebted toSlav Petrov for helping us to retrain his syntactic
parser for Arabic. Alessandro Moschitti and
Olga Uryupina have been partially funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under the grant
number 288024 (LIMOSINE). The content
is solely the responsibility of the authors and
does not necessarily represent the official views
of the National Institutes of Health. NianwenXue and Yuchen Zhang are supported in part
by the DAPRA via contract HR0011-11-C-0145
entitled ?Linguistic Resources for Multilingual
Processing.?
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Libin
Shen. 2006. Issues in synchronizing the English treebank
and propbank. In Workshop on Frontiers in Linguistically
Annotated Corpora 2006, July.
Elizabeth Baran and Nianwen Xue. 2011. Singular or plural?
exploiting parallel corpora for Chinese number prediction.
In Proceedings of Machine Translation Summit XIII, Xia-
men, China.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?40, Sydney, Australia, July.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-driven
multilingual coreference resolution using resolver stack-
ing. In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 49?55, Jeju Island, Korea, July. Association
for Computational Linguistics.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring lex-
icalized features for coreference resolution. In Proceed-
ings of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 45?50, Port-
land, Oregon, USA, June. Association for Computational
Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGDIAL ?10, pages
28?36.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 212?216, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL), Ann Arbor, MI,
June.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
Second Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Ann Arbor, MI,
June.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. 2000.
An empirical study of the domain dependence of super-
vised word disambiguation systems. In 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 172?
180, Hong Kong, China, October. Association for Com-
putational Linguistics.
Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur. 2010.
Evaluation of semantic role labeling and dependency
parsing of automatic speech recognition output. In
Proceedings of 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), page
5342?5345.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint
parsing and named entity recognition. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 326?334, Boulder,
Colorado, June. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, page 363?370.
Ryan Gabbard. 2010. Null Element Restoration. Ph.D. the-
sis, University of Pennsylvania.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In 2001 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference, page
1974?1981.
Mohamed Maamouri and Ann Bies. 2004. Developing an
Arabic treebank: Methods, guidelines, procedures, and
tools. In Ali Farghaly and Karine Megerdoomian, edi-
tors, COLING 2004 Computational Approaches to Arabic
Script-based Languages, pages 2?9, Geneva, Switzerland,
August 28th. COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceedings
of the Human Language Technology Conference/North
American Chapter of the Association for Computational
Linguistics (HLT/NAACL), New York City, NY, June.
151
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the Association for Computational Linguistics
(ACL-02), pages 104?111.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
2007. Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Journal of
Natural Language Engineering, 13(2).
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,
Mohammed Maamouri, Aous Mansouri, and Wajdi Za-
ghouani. 2008. A pilot Arabic propbank. In Proceedings
of the International Conference on Language Resources
and Evaluation (LREC), Marrakech, Morocco, May 28-
30.
Slav Petrov and Dan Klein. 2007. Improved inferencing for
unlexicalized parsing. In Proc of HLT-NAACL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1):11?39.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-
sica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Indentifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), September
17-19.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics Special Issue on Semantic Role Labeling,
34(2).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 shared task: Modeling unrestricted corefer-
ence in OntoNotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1?27, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared
task: Modeling multilingual unrestricted coreference in
OntoNotes. In Joint Conference on EMNLP and CoNLL -
Shared Task, pages 1?40, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,
Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.
The Penn discourse treebank 2.0. In Proceedings of the
Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, May.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena, M. Anto`nia
Mart??, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,
and Yannick Versley. 2010. Semeval-2010 task 1: Coref-
erence resolution in multiple languages. In Proceedings of
the 5th International Workshop on Semantic Evaluation,
pages 1?8, Uppsala, Sweden, July.
Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627?633, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank. Language Resources and
Evaluation, 42(1):1?19, March.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog no.:
LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,
and Nianwen Xue. 2011. OntoNotes: A large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language Ex-
ploitation. Springer.
Nianwen Xue and Martha Palmer. 2009. Adding semantic
roles to the Chinese Treebank. Natural Language Engi-
neering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11(2):207?238.
Nianwen Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational Linguistics, 34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese treebank.
In Proceedings of the 23rd International Conference on
Computational Linguistics (COLING), Beijing, China.
Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-
han, and Martha Palmer. 2010. The revised Arabic prop-
bank. In Proceedings of the Fourth Linguistic Annotation
Workshop, pages 222?226, Uppsala, Sweden, July.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free text.
In Proceedings of the ACL 2010 System Demonstrations,
pages 78?83, Uppsala, Sweden.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An empiri-
cal study. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1002?
1010.
152
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135?145,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
(Re)ranking Meets Morphosyntax: State-of-the-art Results
from the SPMRL 2013 Shared Task?
Anders Bjo?rkelund?, O?zlem C?etinog?lu?, Richa?rd Farkas?, Thomas Mu?ller??, and Wolfgang Seeker?
?Institute for Natural Language Processing , University of Stuttgart, Germany
?Department of Informatics, University of Szeged, Hungary
?Center for Information and Language Processing, University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
rfarkas@inf.u-szeged.hu
Abstract
This paper describes the IMS-SZEGED-CIS
contribution to the SPMRL 2013 Shared Task.
We participate in both the constituency and
dependency tracks, and achieve state-of-the-
art for all languages. For both tracks we make
significant improvements through high quality
preprocessing and (re)ranking on top of strong
baselines. Our system came out first for both
tracks.
1 Introduction
In this paper, we present our contribution to the 2013
Shared Task on Parsing Morphologically Rich Lan-
guages (MRLs). MRLs pose a number of interesting
challenges to today?s standard parsing algorithms,
for example a free word order and, due to their rich
morphology, greater lexical variation that aggravates
out-of-vocabulary problems considerably (Tsarfaty
et al, 2010).
Given the wide range of languages encompassed
by the term MRL, there is, as of yet, no clear con-
sensus on what approaches and features are gener-
ally important for parsing MRLs. However, devel-
oping tailored solutions for each language is time-
consuming and requires a good understanding of
the language in question. In our contribution to the
SPMRL 2013 Shared Task (Seddah et al, 2013), we
therefore chose an approach that we could apply to
all languages in the Shared Task, but that would also
allow us to fine-tune it for individual languages by
varying certain components.
?Authors in alphabetical order.
For the dependency track, we combined the n-
best output of multiple parsers and subsequently
ranked them to obtain the best parse. While this
approach has been studied for constituency parsing
(Zhang et al, 2009; Johnson and Ural, 2010; Wang
and Zong, 2011), it is, to our knowledge, the first
time this has been applied successfully within de-
pendency parsing. We experimented with different
kinds of features in the ranker and developed fea-
ture models for each language. Our system ranked
first out of seven systems for all languages except
French.
For the constituency track, we experimented
with an alternative way of handling unknown words
and applied a products of Context Free Grammars
with Latent Annotations (PCFG-LA) (Petrov et al,
2006), whose output was reranked to select the best
analysis. The additional reranking step improved
results for all languages. Our system beats vari-
ous baselines provided by the organizers for all lan-
guages. Unfortunately, no one else participated in
this track.
For both settings, we made an effort to automat-
ically annotate our data with the best possible pre-
processing (POS, morphological information). We
used a multi-layered CRF (Mu?ller et al, 2013) to
annotate each data set, stacking with the information
provided by the organizers when this was beneficial.
The high quality of our preprocessing considerably
improved the performance of our systems.
The Shared Task involved a variety of settings as
to whether gold or predicted part-of-speech tags and
morphological information were available, as well
as whether the full training set or a smaller (5k sen-
135
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
MarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13
Stacked 98.23/89.05 98.56/92.63 97.83/97.62
Table 1: POS/morphological feature accuracies on the development sets.
tences) training set was used for training. Through-
out this paper we focus on the settings with pre-
dicted preprocessing information with gold segmen-
tation and the full1 training sets. Unless stated other-
wise, all given numbers are drawn from experiments
in this setting. For all other settings, we refer the
reader to the Shared Task overview paper (Seddah et
al., 2013).
The remainder of the paper is structured as fol-
lows: We present our preprocessing in Section 2 and
afterwards describe both our systems for the con-
stituency (Section 3) and for the dependency tracks
(Section 4). Section 5 discusses the results on the
Shared Task test sets. We conclude with Section 6.
2 Preprocessing
We first spent some time on preparing the data sets,
in particular we automatically annotated the data
with high-quality POS and morphological informa-
tion. We consider this kind of preprocessing to be an
essential part of a parsing system, since the quality
of the automatic preprocessing strongly affects the
performance of the parsers.
Because our tools work on CoNLL09 format, we
first converted the training data from the CoNLL06
format to CoNLL09. We thus had to decide whether
to use coarse or fine part-of-speech (POS) tags. In
a preliminary experiment we found that fine tags are
the better option for all languages but Basque and
Korean. For Korean the reason seems to be that the
fine tag set is huge (> 900) and that the same infor-
mation is also provided in the feature column.
We predict POS tags and morphological features
jointly using the Conditional Random Field (CRF)
tagger MarMoT2 (Mu?ller et al, 2013).
MarMoT incrementally creates forward-
backward lattices of increasing order to prune
the sizable space of possible morphological analy-
ses. We use MarMoT with the default parameters.
1Although, for Hebrew and Swedish only 5k sentences were
available for training, and the two settings thus coincide.
2https://code.google.com/p/cistern/
Since morphological dictionaries can improve au-
tomatic POS tagging considerably, we also created
such dictionaries for each language. For this, we an-
alyzed the word forms provided in the data sets with
language-specific morphological analyzers except
for Hebrew and German where we just extracted the
morphological information from the lattice files pro-
vided by the organizers. For the other languages
we used the following tools: Arabic: AraMorph
a reimplementation of Buckwalter (2002), Basque:
Apertium (Forcada et al, 2011), French: an IMS
internal tool,3 Hungarian: Magyarlanc (Zsibrita et
al., 2013), Korean: HanNanum (Park et al, 2010),
Polish: Morfeusz (Wolin?ski, 2006), and Swedish:
Granska (Domeij et al, 2000).
The created dictionaries were shared with the
other Shared Task participants. We used these dic-
tionaries as additional features for MarMoT.
For some languages we also integrated the pre-
dicted tags provided by the organizers into the fea-
ture model. These stacked models gave improve-
ments for Swedish, Polish and Basque (cf. Table 1
for accuracies).
For the full setting the training data was annotated
using 5-fold jackknifing. In the 5k setting, we addi-
tionally added all sentences not present in the parser
training data to the training data sets of the tagger.
This is similar to the predicted 5k files provided by
the organizers, where more training data than the 5k
was also used for prediction.
Table 3 presents a comparison between our graph-
based baseline parser using the preprocessing ex-
plained in this section (denoted mate) and the
preprocessing provided by the organizers (denoted
mate?). Our preprocessing yields improvements
for all languages but Swedish. The worse perfor-
mance for Swedish is due to the fact that the pre-
dictions provided by the organizers were produced
by models that were trained on a much larger data
3The French morphology was written by Zhenxia Zhou,
Max Kisselew and Helmut Schmid. It is an extension of Zhou
(2007) and implemented in SFST (Schmid, 2005).
136
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50
Replaced 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52
Product 80.30 86.21 81.42 84.56 90.49 89.80 84.15 88.32 79.25
Reranked 81.24 87.35 82.49 85.01 90.49 91.07 84.63 88.40 79.53
Table 2: PARSEVAL scores on the development sets.
set. The comparison with other parsers demonstrates
that for some languages (e.g., Hebrew or Korean)
the improvements due to better preprocessing can
be greater than the improvements due to a better
parser. For instance, for Hebrew the parser trained
on the provided preprocessing is more than three
points (LAS) behind the three parsers trained on
our own preprocessing. However, the difference be-
tween these three parsers is less than a point.
3 Constituency Parsing
The phrase structure parsing pipeline is based on
products of Context Free Grammars with Latent An-
notations (PCFG-LA) (Petrov et al, 2006) and dis-
criminative reranking. We further replace rare words
by their predicted morphological analysis.
We preprocess the treebank trees by removing the
morphological annotation of the POS tags and the
function labels of all non-terminals. We also reduce
the 177 compositional Korean POS tags to their first
atomic tag, which results in a POS tag set of 9 tags.
PCFG-LAs are incrementally built by split-
ting non-terminals, refining parameters using EM-
training and reversing splits that only cause small
increases in likelihood.
Running the Berkeley Parser4 ? the reference im-
plementation of PCFG-LAs ? on the data sets results
in the PARSEVAL scores given in Table 2 (Berke-
ley). The Berkeley parser only implements a simple
signature-based unknown word model that seems to
be ineffective for some of the languages, especially
Basque and Korean.
We thus replace rare words (frequency < 20) by
the predicted morphological tags of Section 2 (or the
true morphological tag for the gold setup). The intu-
ition is that our discriminative tagger has a more so-
phisticated unknown word treatment than the Berke-
ley parser, taking for example prefixes, suffixes and
4http://code.google.com/p/
berkeleyparser/
the immediate lexical context into account. Further-
more, the morphological tag contains most of the
necessary syntactic information. An exception, for
instance, might be the semantic information needed
to disambiguate prepositional attachment. We think
that replacing rare words by tags has an advan-
tage over constraining the pre-terminal layer of the
parser, because the parser can still decide to assign
a different tag, for example in cases were the tag-
ger produces errors due to long-distance dependen-
cies. The used frequency threshold of 20 results
in token replacement rates of 18% (French) to 57%
(Korean and Polish), which correspond to 209 (for
Polish) to 3221 (for Arabic) word types that are not
replaced. The PARSEVAL scores for the described
method are again given in Table 2 (Replaced). The
method yields improvements for all languages ex-
cept for French where we observe a drop of 0.06.
The improvements range from 0.46 for Arabic to
1.02 for Swedish, 3.1 for Polish and more than 10
for Basque and Korean.
To further improve results, we employ the
product-of-grammars procedure (Petrov, 2010),
where different grammars are trained on the same
data set but with different initialization setups. We
trained 8 grammars and used tree-level inference.
In Table 2 (Product) we can see that this leads to
improvements from 0.72 for Hungarian to 3.73 for
Swedish.
On the 50-best output of the product parser,
we also carry out discriminative reranking. The
reranker is trained for the maximum entropy objec-
tive function of Charniak and Johnson (2005) and
use the standard feature set ? without language-
specific feature engineering ? from Charniak and
Johnson (2005) and Collins (2000). We use a
slightly modified version of the Mallet toolkit (Mc-
Callum, 2002) for reranking.
Improvements range from negligible differences
(< .1) for Hebrew and Polish to substantial differ-
ences (> 1.) for Basque, French, and Hungarian.
137
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system.
For our final submission, we used the reranker
output for all languages except French, Hebrew, Pol-
ish, and Swedish. This decision was based on an
earlier version of the evaluation setting provided by
the organizers. In this setup, reranking did not help
or was even harmful for these four languages. The
figures in Table 2 use the latest evaluation script and
are thus consistent with the test set results presented
in Section 5.
After the submission deadline the Shared Task
organizers made us aware that we had surprisingly
low exact match scores for Polish (e.g., 1.22 for
the reranked setup). The reason seems to be that
the Berkeley parser cannot produce unary chains of
length > 2. The gold development set contains 1783
such chains while the prediction of the reranked sys-
tem contains none. A particularly frequent unary
chain with 908 occurences in the gold data is ff ?
fwe ? formaczas. As this chain cannot be pro-
duced the parser leaves out the fwe phrase. Inserting
new fwe nodes between ff and formacszas nodes
raises the PARSEVAL scores of the reranked model
from 88.40 to 90.64 and the exact match scores to
11.34. This suggests that the Polish results could be
improved substantially if unary chains were properly
dealt with, for example by collapsing unary chains.5
4 Dependency Parsing
The core idea of our dependency parsing system
is the combination of the n-best output of several
5Thanks to Slav Petrov for pointing us to the unary chain
length limit.
parsers followed by a ranking step on the com-
bined list. Specifically, we first run two parsers that
each output their 50-best analyses for each sentence.
These 50-best analyses are merged together into one
single n-best list of between 50 and 100 analyses
(depending on the overlap between the n-best lists
of the two parsers). We then use the two parsers
plus an additional one to score each tree in the n-
best lists according to their parsing model, thus pro-
viding us with three different scores for each tree in
the n-best lists. The n-best lists are then given to
a ranker, which ranks the list using the three scores
and a small set of additional features in order to find
the best overall analysis. Figure 1 shows a schematic
of the process.
As a preprocessing step, we reduced the depen-
dency label set for the Hungarian training data.
The Hungarian dependency data set encodes ellipses
through composite edge labels which leads to a pro-
liferation of edge labels (more than 400). Since
many of these labels are extremely rare and thus hard
to learn for the parsers, we reduced the set of edge la-
bels during the conversion. Specifically, we retained
the 50 most frequent labels, while reducing the com-
posite labels to their base label.
For producing the initial n-best lists, we use
the mate parser6 (Bohnet, 2010) and a variant of
the EasyFirst parser (Goldberg and Elhadad, 2010),
which we here call best-first parser.
The mate parser is a state-of-the-art graph-based
dependency parser that uses second-order features.
6https://code.google.com/p/mate-tools
138
The parser works in two steps. First, it uses dy-
namic programming to find the optimal projective
tree using the Carreras (2007) decoder. It then
applies the non-projective approximation algorithm
proposed by McDonald and Pereira (2006) in or-
der to produce non-projective parse trees. The non-
projective approximation algorithm is a greedy hill
climbing algorithm that starts from the optimal pro-
jective parse and iteratively tries to reattach all to-
kens, one at a time, everywhere in the sentence as
long as the tree property holds. It halts when the in-
crease in the score of the tree according to the pars-
ing model is below a certain threshold.
n-best lists are obtained by applying the non-
projective approximation algorithm in a non-greedy
manner, exploring multiple possibilities. All trees
are collected in a list, and when no new trees are
found, or newer trees have a significantly lower
score than the currently best one, search halts. The
n best trees are then retrieved from the list. It
should be noted that, in the standard case, the non-
projective approximation algorithm may find a local
optimum, and that there may be other trees that have
a higher score which were not explored. Thus the
best parse in the greedy case may not necessarily
be the one with the highest score in the n-best list.
Since the parser is trained with the greedy version
of the non-projective approximation algorithm, the
greedily chosen output parse tree is of special in-
terest. We thus flag this tree as the baseline mate
parse, in order to use that for features in the ranker.
The baseline mate parse is also our overall baseline
in the dependency track.
The best-first parser deviates from the EasyFirst
parser in several small respects: The EasyFirst de-
coder creates dependency links between the roots of
adjacent substructures, which gives an O(n log n)
complexity, but restricts the output to projective
trees. The best-first parser is allowed to choose as
head any node of an adjacent substructure instead of
only the root, which increases complexity to O(n2),
but accounts for a big part of possible non-projective
structures. We additionally implemented a swap-
operation (Nivre, 2009; Tratz and Hovy, 2011) to
account for the more complex structures. The best-
first parser relies on a beam-search strategy7 to pur-
7Due to the nature of the decoder, the parser can produce
sue multiple derivations, which we also use to pro-
duce the n-best output.
In the scoring step, we additionally apply the tur-
boparser8 (Martins et al, 2010), which is based on
linear programming relaxations.9 We changed all
three parsers such that they would return a score for
a given tree. We use this to extract scores from each
parser for all trees in the n-best lists. It is impor-
tant to have a score from every parser for every tree,
as previously observed by Zhang et al (2009) in the
context of constituency reranking.
4.1 Ranking
Table 3 shows the performance of the individual
parsers measured on the development sets. It also
displays the oracle scores over the different n-best
lists, i.e., the maximal possible score over an n-best
list if the best tree is always selected.
The mate parser generally performs best followed
by turboparser, while the best-first parser comes last.
But we can see from the oracle scores that the best-
first parser often shows comparable or even higher
oracle scores than mate, and that the combination
of the n-best lists always adds substantial improve-
ments to the oracle scores. These findings show that
the mate and best-first parsers are providing differ-
ent sets of n-best lists. Moreover, all three parsers
rely on different parsing algorithms and feature sets.
For these reasons, we hypothesized that the parsers
contribute different views on the parse trees and that
their combination would result in better overall per-
formance.
In order to leverage the diversity between the
parsers we experimented with ranking10 on the
n-best lists. We used the same ranking model in-
troduced in Section 3 here as well. The model is
trained to select the best parse according to the la-
beled attachment score (LAS). The training data for
the ranker was created by 5-fold jackknifing on the
training sets. The feature sets for the ranker for
spurious ambiguities in the beam. If this occurs, only the one
with the higher score is kept.
8http://www.ark.cs.cmu.edu/TurboParser/
9Ideally we would also extract n-best lists from the tur-
boparser, however time prevented us from making the necessary
modifications.
10We refrain from calling it reranking in this setting, since
we are using merged n-best lists and the initial ranking is not
entirely clear to begin with.
139
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline results for individual parsers
mate? 88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16
mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36
turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15
Oracle scores for n-best lists
mate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66
bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46
merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate? uses the prepro-
cessing provided by the organizers, the other parsers use the preprocessing described in Section 2.
each language were optimized manually via cross-
validation on the training sets. The features used for
each language, as well as a default (baseline) fea-
ture set, are shown in Table 4. We now outline the
features we used in the ranker:
Score from the base parsers ? denoted B, M,
T, for the best-first, mate, and turbo parsers, re-
spectively. We also have indicator features whether
a certain parse was the best according to a given
parser, denoted GB, GM, GT, respectively. Since
the mate parser does not necessarily assign the high-
est score to the baseline mate parse, the GM fea-
ture is a ternary feature which indicates whether a
parse is the same as the baseline mate parse, or bet-
ter, or worse. We also experimented with transfor-
mations and combinations of the scores from the
parsers. Specifically, BMProd denotes the product
of B and M; BMeProd denotes the sum of B and M
in e-space, i.e., eB+M ; reBMT, reBT, reMT denote
the normalized product of the corresponding scores,
where scores are normalized in a softmax fashion
such that all features take on values in the interval
(0, 1).
Projectivity features (Hall et al, 2007) ? the
number of non-projective edges in a tree, denoted
np. Whether a tree is ill-nested, denoted I. Since ill-
nested trees are extremely rare in the treebanks, this
helps the ranker filter out unlikely candidates from
the n-best lists. For a definition and further discus-
sion of ill-nestedness, we refer to (Havelka, 2007).
Constituent features ? from the constituent track
we also have constituent trees of all sentences which
can be used for feature extraction. Specifically, for
every head-dependent pair, we extract the path in the
constituent tree between the nodes, denoted ptbp.
Case agreement ? on head-dependent pairs that
both have a case value assigned among their mor-
phological features, we mark whether it is the same
case or not, denoted case.
Function label uniqueness ? on each training set
we extracted a list of function labels that generally
occur at most once as the dependent of a node, e.g.,
subjects or objects. Features are then extracted from
all nodes that have one or more dependents of each
label aimed at capturing mistakes such as double
subjects on a verb. This template is denoted FL.
In addition to the features mentioned above, we
experimented with a variety of feature templates, in-
cluding features drawn from previous work on de-
pendency reranking (Hall, 2007), e.g., lexical and
POS-based features over edges, ?subcategorization?
frames (i.e., the concatenation of POS-tags that are
headed by a certain node in the tree), etc, although
these features did not seem to help. For German we
created feature templates based on the constraints
used in the constraint-based parser by Seeker and
Kuhn (2013). This includes, e.g., violations in case
or number agreement between heads and depen-
dents, as well as more complex features that con-
sider labels on entire verb complexes. None of these
features yielded any clear improvements though. We
also experimented with features that target some
specific constructions (and specifics of annotation
schemes) which the parsers typically cannot fully
see, such as coordination, however, also here we saw
no clear improvements.
4.2 Effects of Ranking
In Table 5, we show the improvements from using
the ranker, both with the baseline and optimized fea-
tures sets for the ranker. For the sake of comparison,
140
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83
Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57
Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 5: Performance (UAS/LAS) of the reranker on the development sets. Baseline denotes our baseline. Ranked-dflt
and Ranked denote the default and optimized ranker feature sets, respectively. Oracle denotes the oracle scores.
default B, M, T, GB, GM, GT, I
Arabic B, M, T, GB, GM, I, ptbp, reBMT
Basque B, M, T, GB, GM, GT, I, ptbp, I, reMT, case
French B, M, T, GB, GM, GT, I, ptbp
German B, M, T, GM, I, BMProd, FL
Hebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProd
Hungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Korean B, M, T, GB, GM, GT, I, ptbp, reMT, FL
Polish B, M, T, GB, GM, GT, I, ptbp, np
Swedish B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Table 4: Feature sets for the dependency ranker for each
language. default denotes the default ranker feature set.
the baseline mate parses as well as the oracle parses
on the merged n-best lists are repeated from Table 3.
We see that ranking clearly helps, both with a tai-
lored feature set, as well as the default feature set.
The improvement in LAS between the baseline and
the tailored ranking feature sets ranges from 1.1%
(French) to 1.6% (Hebrew) absolute, with the excep-
tion of Hungarian, where improvements on the dev
set are more modest (contrary to the test set results,
cf. Section 5). Even with the default feature set, the
improvements range from 0.5% (French) to 1.1%
(Hebrew) absolute, again setting Hungarian aside.
We believe that this is an interesting result consid-
ering the simplicity of the default feature set.
5 Test Set Results
In this section we outline our final results on the test
sets. As previously, we focus on the setting with
predicted tags in gold segmentation and the largest
training set. We also present results on Arabic and
Hebrew for the predicted segmentation setting. For
the gold preprocessing and all 5k settings, we refer
the reader to the Shared Task overview paper (Sed-
dah et al, 2013).11
In Table 7, we present our results in the con-
11Or the results page online: http://www.spmrl.org/
spmrl2013-sharedtask-results.html
stituency track. Since we were the only participat-
ing team in the constituency track, we compare our-
selves with the best baseline12 provided by the or-
ganizers. Our system outperforms the baseline for
all languages in terms of PARSEVAL F1. Follow-
ing the trend on the development sets, reranking is
consistently helping across languages.13 Despite the
lack of other submissions in the shared task, we be-
lieve our numbers are generally strong and hope that
they can serve as a reference for future work on con-
stituency parsing on these data sets.
Table 8 displays our results in the dependency
track. We submitted two runs: a baseline, which
is the baseline mate parse, and the reranked trees.
The table also compares our results to the best per-
forming other participant in the shared task (denoted
Other) as well as the MaltParser (Nivre et al, 2007)
baseline provided by the shared task organizers (de-
noted ST Baseline). We obtain the highest scores
for all languages, with the exception of French. It is
also clear that we make considerable gains over our
baseline, confirming our results on the development
sets reported in Section 4. It is also noteworthy that
our baseline (i.e., the mate parser with our own pre-
processing) outperforms the best other system for 5
languages.
Arabic Hebrew
Other 90.75/8.48 88.33/12.20
Dep. Baseline 91.13/9.10 89.27/15.01
Dep. Ranked 91.74/9.83 89.47/16.97
Constituency 92.06/9.49 89.30/13.60
Table 6: Unlabeled TedEval scores (accuracy/exact
match) for the test sets in the predicted segmentation set-
ting. Only sentences of length ? 70 are evaluated.
12It should be noted that the Shared Task organizers com-
puted 2 different baselines on the test sets. The best baseline
results for each language thus come from different parsers.
13We remind the reader that our submission decisions are not
based on figures in Table 2, cf. Section 3.
141
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 79.19 74.74 80.38 78.30 86.96 85.22 78.56 86.75 80.64
Product 80.81 87.18 81.83 80.70 89.46 90.58 83.49 87.55 83.99
Reranked 81.32 87.86 82.86 81.27 89.49 91.85 84.27 87.76 84.88
Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the
best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21
Other 85.78/83.20 89.19/84.25 89.19/85.86 90.80/88.66 81.05/73.63 88.93/84.97 85.84/82.65 88.12/82.56 87.28/80.88
Baseline 86.96/84.81 89.32/84.25 87.87/84.37 90.54/88.37 85.88/79.67 89.09/85.31 87.41/85.51 90.30/85.51 86.85/80.67
Ranked 88.32/86.21 89.88/85.14 88.68/85.24 91.64/89.65 86.70/80.89 89.81/86.13 88.47/86.62 91.75/87.07 88.06/82.13
Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest
scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task
organizers.
Table 6 shows the unlabeled TedEval (Tsarfaty et
al., 2012) scores (accuracy/exact match) on the test
sets for the predicted segmentation setting for Ara-
bic and Hebrew. Note that these figures only include
sentences of length less than or equal to 70. Since
TedEval enables cross-framework comparison, we
compare our submissions from the dependency track
to our submission from the constituency track. In
these runs we used the same systems that were used
for the gold segmentation with predicted tags track.
The predicted segmentation was provided by the
Shared Task organizers. We also compare our re-
sults to the best other system from the Shared Task
(denoted Other).
Also here we obtain the highest results for both
languages. However, it is unclear what syntactic
paradigm (dependencies or constituents) is better
suited for the task. All in all it is difficult to assess
whether the differences between the best and second
best systems for each language are meaningful.
6 Conclusion
We have presented our contribution to the 2013
SPMRL Shared Task. We participated in both the
constituency and dependency tracks. In both tracks
we make use of a state-of-the-art tagger for POS and
morphological features. In the constituency track,
we use the tagger to handle unknown words and em-
ploy a product-of-grammars-based PCFG-LA parser
and parse tree reranking. In the dependency track,
we combine multiple parsers output as input for a
ranker.
Since there were no other participants in the con-
stituency track, it is difficult to draw any conclusions
from our results. We do however show that the ap-
plication of product grammars, our handling of rare
words, and a subsequent reranking step outperforms
a baseline PCFG-LA parser.
In the dependency track we obtain the best re-
sults for all languages except French among 7 partic-
ipants. Our reranking approach clearly outperforms
a baseline graph-based parser. This is the first time
multiple parsers have been used in a dependency
reranking setup.
Aside from minor decisions made on the basis
of each language, our approach is language agnos-
tic and does not target morphology in any particu-
lar way as part of the parsing process. We show
that with a strong baseline and with no language
specific treatment it is possible to achieve state-of-
the-art results across all languages. Our architec-
ture for the dependency parsing track enables the use
of language-specific features in the ranker, although
we only had minor success with features that target
morphology. However, it may be the case that ap-
proaches from previous work on parsing MRLs, or
the approaches taken by other teams in the Shared
Task, can be successfully combined with ours and
improve parsing accuracy even more.
Acknowledgments
Richa?rd Farkas is funded by the European Union and
the European Social Fund through the project Fu-
turICT.hu (grant no.: TA?MOP-4.2.2.C-11/1/KONV-
142
2012-0013). Thomas Mu?ller is supported by a
Google Europe Fellowship in Natural Language
Processing. The remaining authors are funded by
the Deutsche Forschungsgemeinschaft (DFG) via
the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank
providers for each language: Arabic (Maamouri et
al., 2004; Habash and Roth, 2009; Habash et al,
2009; Green and Manning, 2010), Basque (Aduriz
et al, 2003), French (Abeille? et al, 2003), He-
brew (Sima?an et al, 2001; Tsarfaty, 2010; Gold-
berg, 2011; Tsarfaty, 2013), German (Brants et al,
2002; Seeker and Kuhn, 2012), Hungarian (Csendes
et al, 2005; Vincze et al, 2010), Korean (Choi
et al, 1994; Choi, 2013), Polish (S?widzin?ski and
Wolin?ski, 2010), and Swedish (Nivre et al, 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In TLT-03, pages 201?204.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961, Prague, Czech Republic, June.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W
Kwon. 1994. Kaist tree bank project for korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for
the shared task on parsing morphologically rich lan-
guages. ArXiv e-prints.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of the Sev-
enteenth International Conference on Machine Learn-
ing, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Rickard Domeij, Ola Knutsson, Johan Carlberger, and
Viggo Kann. 2000. Granska-an efficient hybrid sys-
tem for Swedish grammar checking. In In Proceed-
ings of the 12th Nordic Conference in Computational
Linguistics.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio Pe?rez-Ortiz, Felipe Sa?nchez-Mart??nez, Gema
Ram??rez-Sa?nchez, and Francis M Tyers. 2011. Aper-
tium: A free/open-source platform for rule-based ma-
chine translation. Machine Translation.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
Spence Green and Christopher D. Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
143
Keith Hall, Jiri Havelka, and David A. Smith. 2007.
Log-Linear Models of Non-Projective Trees, k-best
MST Parsing and Tree-Ranking. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 962?966, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Keith Hall. 2007. K-best Spanning Tree Parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Jiri Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 608?615, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44, Cambridge, MA, October. Association for Compu-
tational Linguistics.
Andrew Kachites McCallum. 2002. ?mal-
let: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 81?88, Trento, Italy. Asso-
ciation for Computational Linguistics.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze.
2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13:95?135, 6.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association for
Computational Linguistics.
S Park, D Choi, E-k Kim, and KS Choi. 2010. A plug-in
component-based Korean morphological analyzer. In
Proceedings of HCLT2010.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 433?440. Associa-
tion for Computational Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
Helmut Schmid. 2005. A programming language for
finite state transducers. In FSMNLP.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-
dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, and Alina Wro?blewska.
2013. Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morphologi-
cally Rich Languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological
and Syntactic Case in Statistical Dependency Parsing.
Computational Linguistics, 39(1):23?55.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
144
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference
(TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Ac-
curate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL) What, How and Whither. In Proc. of the
SPMRL Workshop of NAACL-HLT, pages 1?12, Los
Angeles, CA, USA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint Evaluation of Morphological Segmen-
tation and Syntactic Parsing. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
6?10, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic
Scheme of Stanford Dependencies. Proceedings of
ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
Zhiguo Wang and Chengqing Zong. 2011. Parse Rerank-
ing Based on Higher-Order Lexical Dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Marcin Wolin?ski. 2006. Morfeusz - A practical tool for
the morphological analysis of Polish. In Intelligent in-
formation processing and web mining, pages 511?520.
Springer.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-Best Combination of Syntactic Parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore, August. Association for Com-
putational Linguistics.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diplomarbeit, Institute for
Natural Language Processing, University of Stuttgart.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
145
