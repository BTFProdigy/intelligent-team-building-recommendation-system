Automatic Corpus-Based Thai Word Extraction 
with the C4.5 Learning Algorithm 
VIRACH SORNLERTLAMVANICH, TANAPONG POTIPITI AND THATSANEE 
CHAROENPORN 
National Electronics and Computer Technology Centel, 
National Science and Technology Development Agency, 
Ministry of Science and Technology Environntent, 
22 '~1 Floor Gypsum Metiw)olitan Tower 539/2 Sriayudhya Rd. Rajthevi Bangkok 10400 ThailatM 
Email: virach@nectec.or.th, tanapong@nectec.or.th, thatsanee@nectec.or.th 
Abstract 
"Word" is difficult to define in the languages that 
do not exhibit explicit word boundary, such as 
Thai. Traditional methods on defining words for 
this kind of languages have to depend on human 
judgement which bases on unclear criteria o1" 
procedures, and have several limitations. This 
paper proposes an algorithm for word extraction 
from Thai texts without borrowing a hand from 
word segmentation. We employ the c4.5 learning 
algorithm for this task. Several attributes uch as 
string length, frequency, nmtual information and 
entropy are chosen for word/non-word 
determination. Our experiment yields high 
precision results about 85% in both training and 
test corpus. 
1 In t roduct ion  
in the Thai language, there is no explicit word 
boundary; this causes a lot of problems in Thai 
language processing including word 
segmentation, information retrieval, machine 
translation, and so on. Unless there is regularity in 
defining word entries, Thai language processing 
will never be effectively done. The existing Thai 
language processing tasks mostly rely on the 
hand-coded dictionaries to acquire the information 
about words. These manually created ictionaries 
have a lot of drawbacks. First, it cannot deal with 
words that are not registered in the dictionaries. 
Second, because these dictionaries are manually 
created, they will never cover all words that occur 
in real corpora. This paper, therefore, proposes an 
automatic word-extraction algorithm, which 
hopefully can overcome this Thai language- 
processing barrier. 
An essential and non-trivial task for the 
languages that exhibit inexplicit word boundary 
such as Thai, Japanese, and many other Asian 
languages undoubtedly is the task in identifying 
word boundary. "Word", generally, means a unit 
of expression which has universal intuitive 
recognition by native speakers. Linguistically, 
word can be considered as the most stable unit 
which has little potential to rearrangement and is 
uninterrupted as well. "Uninterrupted" here 
attracts our lexical knowledge bases so much. 
There are a lot of uninterrupted sequences of 
words functioning as a single constituent of a 
sentence. These uninterrupted strings, of course 
are not the lexical entries in a dictionary, but each 
occurs in a very high frequency. The way to point 
out whether they are words or not is not 
distinguishable even by native speakers. Actually, 
it depends on individual judgement. For example, 
a Thai may consider 'oonfila~mu' (exercise) a whole 
word, but another may consider 'n~n~m~' as a 
compound: 'oon' (take)+ 'filg~' (power)+ 'too' (body). 
Computationally, it is also difficult to decide 
where to separate a string into words. Even 
though it is reported that the accuracy of recent 
word segmentation using a dictionary and some 
heuristic methods is in a high level. Currently, 
lexicographers can make use of large corpora and 
show the convincing results from the experiments 
over corpora. We, therefore, introduce here a new 
efficient method for consistently extracting and 
identifying a list of acceptable Thai words. 
2 Previous Works  
Reviewing the previous works on Thai word 
extraction, we found only the work of 
Sornlertlamvanich and Tanaka (1996). They 
employed the fiequency of the sorted character n- 
grams to extract Thai open compounds; the strings 
that experienced a significant change of 
occurrences when their lengths are extended. This 
algorithm reports about 90% accuracy of Thai 
802 
open compound extraction. However, the 
algorithm emphasizes on open compotmd 
extraction and has to limit tile range of n-gram to 
4-20 grams for the computational reason. This 
causes limitation in the size of corpora and 
efficiency in the extraction. 
The other works can be found in the 
research on the Japanese language. Nagao et al 
(1994) has provided an effective method to 
construct a sorted file that facilitates the 
calculation of n-gram data. But their algorithm did 
not yield satisfactory accuracy; there were many 
iuwflid substrings extracted. The following work 
(lkehara et al, 1995) improved the sorted file to 
avoid repeating in counting strings. The extraction 
cesult was better, but the determination of the 
longest strings is always made consecutively from 
left to right. If an erroneous tring is extracted, its 
errors will propagate through the rest of the input 
:~trings. 
:3 Our Approach 
3.1 The C4.5 Learning Algorithm 
Decision tree induction algorithms have been 
successfully applied for NLP problems such as 
sentence boundary dismnbiguation (Pahner et al 
1997), parsing (Magerman 1995) and word 
segmentation (Mekuavin et al 1997). We employ 
the c4.5 (Quinhln 1993) decision tree induction 
program as the learning algorithm for word 
extraction. 
The induction algorithm proceeds by 
evaluating content of a series of attributes and 
iteratively building a tree fiom the attribute values 
with the leaves of the decision tree being the value 
of the goal attribute. At each step of learning 
procedure, the evolving tree is branched on the 
attribute that pal-titions tile data items with the 
highest information gain. Branches will be added 
until all items in the training set arc classified. To 
reduce the effect of overfitting, c4.5 prunes the 
entire decision tree constructed. It recursively 
examines each subtree to determine whether 
replacing it with a leaf or brauch woukt reduce 
expected error rate. This pruning makes the 
decision tree better in dealing with tile data 
different froul tile training data. 
3.2 Attributes 
We treat the word extraction problem as the 
problem of word/nou-word string disambiguation. 
The next step is to identify the attributes that are 
able to disambiguate word strings flom non-word 
strings. The attributes used for the learning 
algorithm are as follows. 
3.2.1 Left Mutual hfomlation and Right Mutual 
h{fbrmation 
Mutual information (Church et al 1991) of 
random variable a and b is the ratio of probability 
that a and b co-occur, to tile indepeudent 
probability that a and b co-occur. High mutual 
information indicates that a and b co-occur lnore 
than expected by chance. Our algorithm employs 
left and right mutual information as attributes in 
word extraction procedure. Tile left mutual 
information (Lm), and right mutual information 
(Rm) of striug ayz are defined as: 
Lm(xyz)  - 
Rm(xyr.)  - 
p(xyz.) 
p(x)p(yz) 
p(xy~.) 
p ( ,y )p (z )  
where 
x is the leftmost character ofayz 
y is the lniddle substring ol'ayz 
is the rightmost character of :tlVz 
p( ) is tile probability function. 
If xyz is a word, both Lm(xyz) and Rm(~yz) should 
be high. On the contra W, if .rye is a non-word 
string but consists of words and characters, either 
of its left or right mutual information or both lnust 
be low. For example, 'ml~qn~" ( n'(a Thai alphabet) 
'fl~anq'(The word means appear in Thai.) ) must 
have low left mutual information. 
3.2.2 Left Entropy and Right Entropy 
Eutropy (Shannon 1948) is the information 
measuring disorder of wu'iables. The left and right 
entropy is exploited as another two attributes in 
our word extraction. Left entropy (Le), and right 
entropy (Re) of stringy are defined as: 
803 
Le(y) = - Z p(xy I Y)' Iog2p(xYlY) 
V.r~ A 
Re(y) = - Z p(yz l y ) " log 2 p(yz l y ) 
Vz~A 
where 
y is the considered string, 
A is the set of all alphabets 
x, z is any alphabets in A. 
I fy  is a word, the alphabets that come before and 
aflery should have varieties or high entropy. If y 
is not a complete word, either of its left or right 
entropy, or both must be low. For example, 'ahan' 
is not a word but a substring of word 'O~3n~l' 
(appear). Thus the choices of the right adjacent 
alphabets to '~qn' must be few and the right 
entropy of 'ahw, when the right adjacent alphabet 
is '~', must be low. 
3.2.3 Frequency 
It is obvious that the iterative occurrences of 
words must be higher than those of non-word 
strings. String frequency is also useful 
information for our task. Because the string 
frequency depends on the size of corpus, we 
normalize the count of occurrences by dividing by 
the size of corpus and multiplying by the average 
value of Thai word length: 
F(s) = N(s).Avl 
Sc 
where 
s is the considered string 
N(s) is the number of the occurrences 
of s in corpus 
Sc is the size of corpus 
Avl is the average Thai word length. 
We employed the frequency value as another 
attribute for the c4.5 learning algorithm. 
3.2.4 Length 
Short strings are more likely to happen by chance 
than long strings. Then, short and long strings 
should be treated ifferently in the disambiguation 
process. Therefore, string length is also used as an 
attribute for this task. 
3.2.5 Functional Words 
Functional words such as '~' (will) and '~' (then) 
are frequently used in Thai texts. These functional 
words are used often enough to mislead the 
occurrences of string patterns. To filter out these 
noisy patterns from word extraction process, 
discrete attribute Func(s): 
Func(s) : 1 if string s contains 
fnnctional words, 
= 0 if otherwise, 
is applied. 
3.2.6 First Two and Last Two Characters 
A very useful process for our disambiguation is to 
check whether the considered string complies with 
Thai spelling rules or not. We employ the words 
in the Thai Royal Institute dictionary as spelling 
examples for the first and last two characters. 
Then we define attributes Fc(s)and Lc(s) for 
this task as follows. 
N(s, s2*) 
Fc(s )  - 
ND 
N(*s,,_l  s,, ) Lc( s ) - 
ND 
where s is the considered string and 
S .= S IS2 . . .Sn_ IS  n 
N(sls2* ) is the number of words in 
the dictionary that begin with s~s 2 
N(*s,_ls,,) is the nmnber of 
words in the dictionary that 
end with s,,_~s,, 
ND is the number of words in 
the dictionary. 
3.3 Applying C4.5 to Thai Word Extraction 
The process of applying c4.5 to our word 
extraction problem is shown in Figure 1. Firstly, 
we construct a training set for the c4.5 learning 
algorithm. We apply Yamamoto et al(1998)'s 
algorithm to extract all strings from a plain and 
unlabelled I-MB corpus which consists of 75 
articles from various fields. For practical and 
reasonable purpose, we select only the 2-to-30- 
character strings that occur more than 2 times, 
804 
Extracting Strings 
from 
the Training 
Corpus 
Computing the\] 
Attributes I 
Value J 
iTagging the 
Strings 1 
'qV 
i Extracting Strings 
from 
the Test Corpus 
~ t ~  the 
Attributes 
Value 
J -  --We r ~  
1 Extraction 
Figure. 1 : Overview o1' the Process 
Re > 1.78 / ,  
-2~Lm 14233--:. / is notaword ' 
\ 
/ \ \  
Y//" \~  N 
.2" Func= 0 "> s nota wor 
i s  a word  
Figure 2: Exanlple of the Decision tree 
have positive right and left entropy, and conform 
to simple Thai spelling rules. To this step, we get 
about 30,000 strings. These strings are lnalmally 
tagged as words or non-word strings. The strings' 
statistics explained above are calculated for each 
string. Then the strings' attributes and tags are 
used as the training example for the learning 
algorithln. The decision tree is then constructed 
from the training data. 
In order to test the decision tree, another 
plain I-MB corpus (the test corpus), which 
consists of 72 articles fi'om various fields, is 
employed. All strings in the test corpus are 
extracted and filtered out by the same process as 
used in the training set. After the filtering process, 
we get about 30,000 strings to be tested. These 
30,000 strings are manually tagged in order that 
the precision and recall of the decision tree can be 
evaluated. The experimental results will be 
discussed in the next section. 
4 Exper imental  Results 
4.1 The Results 
To measure the accuracy of the algorithln, we 
consider two statistical values: precision and 
recall. The precision of our algorithm is 87.3% for 
the training set and 84.1% for the test set. The 
recall of extraction is 56% in both training and 
test sets. We compare the recall of our word 
extraction with the recall from using the Thai 
Royal Institute dictionary (RID). The recall froln 
our approach and from using RID are comparable 
and our approach should outperform the existing 
dictionary for larger corpora. Both precision and 
recall fiom training and test sets are quite close. 
This indicates that the created decision tree is 
robust for unseen data. Table 3 also shows that 
more than 30% of the extracted words are not 
found in RID. These would be the new entries for 
the dictionary. 
Table 1 : The precision of word extraction 
No. of strings 
extracted by the 
decision tree 
Training 1882 
Set (100%) 
'lest Set 1815 
(100%) 
No. of No. of non- 
words word strings 
extracted extracted 
1643 239 
(87.3%) (12.7%) 
1526 289 
(84.1%) (15.9%) 
Table 2: Tile recall of word extraction 
Training 
Set 
Test Set 
No. of words 
that ill 30,000 
strings 
extracted 
No. of words 
extracted by 
the decision 
t ree  
No. of words 
in corpus that 
are found 
RID 
2933 1643 1833 
(100%) (56.0%) (62.5%) 
2720 1526 1580 
(100%) (56.1%) (58.1%) 
805 
Table 3: Words extracted 
No. of words 
extracted by 
the decision 
tree 
by the decision 
No. of words 
extracted by 
the decision 
tree which is 
inRID 
tree and RID 
No. of words 
extracted by 
the decision 
tree which is 
not in RID 
Training 1643 1082 561 
Set (100.0%) (65.9%) (34.1%) 
Test Set 1526 1046 480 
(100.1%) (68.5%) (31.5%) 
4.2 The Relationship of Accuracy, Occurrence 
and Length 
In this section, we consider the relationship of the 
extraction accuracy to the string lengths and 
occurrences. Figure 2 and 3 depict that both 
precision and recall have tendency to increase as 
string occurrences are getting higher. This implies 
that the accuracy should be higher for larger 
corpora. Similarly, in Figure 4 and 5, the accuracy 
tends to be higher in longer strings. The new 
created words or loan words have tendency to be 
long. Our extraction, then, give a high accuracy 
and very useful for extracting these new created 
words. 
T ra in  in  g 
. . . . . . .  T cs t  
, r 1 r I I I I 
2 6 10  14  18  22  26  3O 34  3 \ [{  
0 ccur rcncc  (x  I O0  ) 
Figurc 3: Prccision-Occurrence R lationship 
lOO 
Z .  
~4o - -T ra in ing  
2o . . . . . .  Test 
o r r r T T 1 T T ? ? 
2 6 10  14  18  22  26  30  34  38  
Occurrence (xl00) 
Figure 4: Recall-Occurrence Relationship 
lOO 
"~ 40  r, 
2O 
0 
120 
I T raining 
. . . . . .  Tcst  
T r E r i r ~ i 
1 3 5 7 9 11  13  15  17  
Length (No. of characters) 
Figure 5: Precision-Length Relationship 
I 90  
I 8o 
1 70 
i 60 
50 i!40? 
~" 30  
20  
. . . . . .  Test  
lO 
0 ? i i 
1 3 5 7 ? 11 13  15  17  
\ [ , cng lh  (No .  of  characters )  
Figure 6: Prccision-Length P,elationship 
5 Conclusion 
In this paper, we have applied the c4.5 learning 
algorithm for the task of Thai word extraction. 
C4.5 can construct a good decision tree for 
word/non-word disambiguation. The learned 
attributes, which are mutual information, entropy, 
word frequency, word length, functional words, 
first two and last two characters, can capture 
useful information for word extraction. Our 
approach yields about 85% and 56% in precision 
and recall measures respectively, which is 
comparable to employing an existing dictionary. 
The accuracy should be higher in larger corpora. 
Our future work is to apply this algorithm with 
larger corpora to build a corpus-based Thai 
dictionary. And hopefully, out" approach should be 
successful for other non-word-boundary 
languages. 
Acknowledgement 
Special thanks to Assistant Professor Mikio 
Yamamoto for providing the useful program to 
extract all substrings from the corpora in linear 
time. 
806 
References 
Church, K.W., Robert L. and Mark L.Y. 
(1991) A Status Report on ACL/DCL. 
Proceedings of 7 a' Annual Co#(ference of 
the UW Centre New OED attd Text 
Reseatrh: Using Corpora, pp. 84-91 
Ikehara, S., Shirai, S. and Kawaoka, T. (1995) 
Automatic Extraction of Uninterrupted 
Collocations by n-gran~ Statistics. Piwceeding q\[ 
The fitwt Annual Meeting of the Association for 
Natural Language Processing, pp. 313-316 (in 
Japancse) 
Magerman, D.M. (1995) Statistical decision-tree 
models for parsing., hwceeding of 33rd 
Amtual Meeting of Association for Computational 
Linguistics 
Meknavin, S., Charoenpornsawat, P. and Kijsirikul, B. 
(1997) Feature-based Thai Word Segmentation. 
Proceeding of the Natural Language Processing 
Pacific Rim Symposium 1997, pp. 35-46 
Nagao, M. and Mort, S. (1994) A New Method of N- 
gram Statistics for Large Number of n and 
Automatic Extraction of Words and Phrases fl'om 
Large Text l)ata of Japanese. Proceeding of 
COLING 94, Vol. 1, pp. 611-15 
Pahner, D.D. and Hearst M.A. (1997) Adaptive 
Multilingual Sentence Boundary Disambiguation. 
ComputationalLinguistics Vol.27, pp. 241-267 
Quinhm, J.R. (1993) C4.5 Programs for Machine 
Learning.Morgan Publishers San Mated, 
California, 302 p. 
Shannon, C.E. (1948) A Mathematical Theory of 
CommunicatiomJ. Bell System Technical Jolu'nal 
27, pp. 379-423 
Sornlertlamvanich, V. and Tanaka, H. (1996) The 
Automatic Extraction of Open Compounds from 
Text. Proceeding o\[ COLING 96 Vol. 2, pp. 1143- 
1146 
Yamamoto, M. and Church, K.W. (1998) Using Suffix 
Arrays to Compare Term Frequency and 
Document Frequency for All Substrings in Corpus. 
Proceeding of Sixth Workshop on Veo' Large 
Corpora pp. 27-37 
807 
Towards an Intelligent Multilingual Keyboard System
Tanapong Potipiti, Virach Sornlertlamvanich, Kanokwut Thanadkran
National Electronics and Computer Technology Center,
National Science and Technology Development Agency,
 Ministry of Science and Technology Environment,
22nd Floor Gypsum Metropolitan Tower 539/2 Sriayudhya Rd. Rajthevi Bangkok 10400 Thailand
Email: tanapong@nectec.or.th, virach@nectec.or.th, kanokwutt@notes.nectec.or.th
ABSTRACT
This paper proposes a practical approach employing n-gram
models and error-correction rules for Thai key prediction and
Thai-English language identification. The paper also proposes
rule-reduction algorithm applying mutual information to reduce
the error-correction rules. Our algorithm reported more than
99% accuracy in both language identification and key
prediction.
1 INTRODUCTION
For Thai users, there are always two annoyances while typing
Thai-English bilingual documents, which are usual for Thais.
The first is when the users want to switch from typing Thai to
English, they have to input a special key to tell the operating
system to change the language mode. Further, if the language-
switching key is ignored, they have to delete the token just typed
and re-type that token after language switching. The second is
that Thai has more than 100 alphabets, to input about half of all
Thai characters, the user has to use combinations of two keys
(shift key + another key) to input them. Some of the other Asian
users also have the same problem.
It will be wonderful, if there is a intelligent keyboard
system that is able to perform these two tasks ?switching
language and shifting key? automatically. This paper proposes a
practical solution for these disturbances by applying trigram
character probabilistic model and error-correction rules. To
optimize number of the generated error-correction rules, we
propose a rule reduction approach using mutual information.
More than 99 percent of key prediction accuracy results are
reported.
2 RELATED WORKS
There is only one related work on inputting Chinese words
through 0-9 numpad keys. [8] applied lexical trees and Chinese
word n-grams to word prediction for inputting Chinese
sentences by using digit keys. They reported 94.4% prediction
accuracy. However, they did not deal with automatic language
identification process. The lexical trees they employed required
a large amount of space. Their algorithm is required some
improvement for a practical use.
3 THE APPROACH
3.1 Overview
In the traditional Thai keyboard input system, a key button with
the help of language-switching key and the shift key can output
4 different characters. For example, in the Thai keyboard the ?a?-
key button can represent 4 different characters in different
modes as shown in Table 1.
Table 1: A key button can represent different characters in
different modes.
English Mode
without Shift
English Mode
with Shift
Thai Mode
without Shift
Thai Mode
with Shift
?a? ?A? ??? ???
However, using NLP technique, the Thai-English
keyboard system which can predict the key users intend to type
without the language-selection key and the shift key, should be
efficiently implemented. We propose an intelligent keyboard
system to solve this problem and have implemented with
successful result.
To solve this problem, there are basically two steps:
language identification and Thai key prediction. Figure 1 shows
how the system works.
                     Figure 1: How the System Works
3.2 Language Identification
The following example illustrates the disturbance of language
switching. In the Thai input mode, typing a word ?language?
will result ???g????. It is certain that the user has to delete
sequence ???g???? and then switches to the English mode before
retyping the key sequence to get the correct result of ?language?.
Language
Identification
Key Input
Thai Key
Prediction
O utput
Eng
Yes
Thai
Therefore an intelligent system to perform language switching
automatically is helpful in eliminating the annoyance.
In general, different languages are not typed
connectedly without spaces between them. The language-
identification process starts when a non-space character is typed
after a space. Many works in language identification, [3] and [5],
have claimed that the n-gram model gives a high accuracy on
language identification. After trying both trigrams and bigrams,
we found that bigrams were superior.  We then compare the
following bigram probability of each language.
Tprob = 





?
=
+
P
L
LL7
..S
Eprob = 





?
=
+
P
L
LL(
..S
where

7
S is the probability of the bi-gram key buttons
 considered in Thai texts.
 K is the key button considered.

(
S is the probability of the bi-gram key buttons
considered in English texts.
Tprob is the probability of the considered key-button
 sequence to be Thai.
Eprob is the probability of the considered key-button
sequence to be English.
m is the number of the leftmost characters of the token
considered. (See more details in the experiment.)
The language being inputted is identified by
comparing the key sequence probability. The language will be
identified as Thai if Tprob > Eprob and vice versa.
3.3 Key Prediction without Using Shift Key
for Thai Input
3.3.1 Trigram Key Prediction
The trigram model is selected to apply for the Thai key
prediction. The problem of the Thai key prediction can be
defined as:
__PD[DUJ



LLLLL
Q
L
FFF
F.SFFFS
Q
=
?=?
where
? is the sequence of characters that maximizes the
    character string  sequence  probability,
c is the possible input character for the key button
   K,
 K is the key button,
n is the length of the token considered.
3.3.2 Error Correction for Thai Key Prediction
In some cases of Thai character sequence, the trigram model
fails to predict the correct key. To correct these errors, the error-
correction rules proposed by [1] and [2] is employed.
3.3.2.1 Error-correction Rule Extraction
After applying trigram prediction to the training corpus are
considered to prepare the error correction rule. The left and right
three keys input around each error character and the correct
pattern corresponding with the error will be collected as an
error-correction pattern. For example, if the input key sequence
?glik[lkl9in? is predicted as ???????????r?, where the correct
prediction is ???????????r?. The string ?ik[lkl9? is then collected as
an error sequence and ???????? is collected as the correct pattern
to amend the error.
3.3.2.2 Rule Reduction
In the process of collecting the patterns, there are a lot of
redundant patterns collected. For example, patterns no.1-3 in
Table 2 should be reduced to pattern 4. To reduce the number of
rules, left mutual information and right mutual information ([7])
are employed. When all patterns are shortened, the duplicate
patterns are then eliminated in the final.
Table 2: Error-Correction Rule Reduction
Pattern
No.
Error Key Sequences Correct Patterns
1. k[lkl9
??????
2. mpklkl9
???????
3. kkklkl9
???????
4. lkl9
????
Left mutual information (Lm) and right mutual
information (Rm) are the statistics used to shorten the patterns.
Lm and right Rm are defined as follows.
)()(
)()(
zpxp
zxp
zxLm
y
yy = ,
)()(
)()(
zpxp
zxp
zxRm
y
yy = ,
where
xyz is the pattern being considered,
x is the leftmost character of xyz,
y is the middle substring of xyz,
z is the rightmost character of xyz,
p( ) is the probability function.
Training Corpus
Trigram Prediction Model
Errors from Trigram
Prediction
Error-Correction Rules
Figure 2: Error-Correction Rule Extraction
The pattern-shortening rules are as follows.
1) If the Rm(xyz) is  less than 1.20 then pattern xyz is reduced
to xy.
2) Similarly, If the Lm(xyz) is  less than 1.20 then pattern xyz
is reduced to yz.
3) Rules 1 and 2 are applied recursively until the considered
pattern cannot be shortened anymore.
After all patterns are shortened, the following rules are applied
to eliminate the redundant patterns.
1) All duplicate rules are unified.
2) The rules that contribute less 0.2 per cent of error corrections
are eliminated.
3.3.3 Applying Error-correction Rules
There are three steps in applying the error-correction rules:
1) Search the error patterns in the text being typed.
2) Replace the error patterns with the correct patterns.
3) If there are more than one pattern matched, the longest
pattern will be selected.
In order to optimize the speed of error-correction processing and
correct the error in the real time, the finite-automata pattern
matching ([4] and [6]) is applied to search error sequences.  We
constructed an automaton for each pattern, then merge these
automata into one as illustrated in Figure 3.
4. EXPERIMENTS
4.1 Language Identification
To create an artificial corpus to test the automatic
language switching, 10,000 random words from an English
dictionary and 10,000 random words from a Thai dictionary are
selected to build a corpus for language identification experiment.
All characters in the test corpus are converted to their mapping
characters of the same key button in normal mode (no shift key
applied) without applying the language-switching key. For
example, character ???, ??? and ?a? will be converted to ?a?. For
the language identification, we employ the key-button bi-grams
extracted As a conclusion the first 6 characters of the token are
enough to yield a high accuracy on English-Thai language
identification.
Table 3: The Accuracy of Thai-English Language
Identification
m (the number of the first
characters to be considered)
Identification Accuracy
(%)
3
4
5
6
7
94.27
97.06
98.16
99.10
99.11
4.2 Thai Key Prediction
4.2.1 Corpus Information
The sizes of training and test sets applied to our key prediction
algorithm are 25 MB and 5 MB respectively. The table below
shows the percentage of shift and unshift alphabets used in the
corpora.
Table 4: Information on Alphabets Used in Corpus
Training Corpus
(%)
Test Corpus
(%)
Unshift Alphabets 88.63 88.95
Shift Alphabets 11.37 11.05
Figure 3: The Example of Constructing and
Merging Automata
0 1 2 3
G
N L
(i) The automaton for patterntGNLu
0 1 2 3
L
N G
(ii) The automaton for patterntLNGu
(iii) Merging automata (I) and (ii)
G
1 2 3
L
N
0
G
4 5 6
N L
N
N
Figure 4: The Error-Correction Process
Key Input
F in ite Autom ata
term inal s tate?
Correction
N
Y
4.2.2 Thai Key Prediction with Trigram
Because the Thai language has no word boundary, we trained
the trigram model from a 25-MB Thai corpus instead of a word
list from a dictionary as in the language identification. The
trigram model was tested on another 5-MB corpus (the test set).
Similarly, a typing situation without applying shift key was
simulated for the test. The result is shown in Table 4.
    Table 5: Thai Key Prediction Using Trigram Model
Training Corpus Test Corpus
93.11 92.21
4.2.3 Error-correction Rules
From the errors of trigram key prediction when applied to the
training corpus, about 12,000 error-correction rules are extracted
and then reduced to 1,500. These error-correction rules are
applied to the result of key prediction. The results are shown in
the table below.
Table 6: The Accuracy of Key Prediction Using Trigram
Model and Applying Error-correction Rules
Prediction
Accuracy from
Training Corpus
(%)
Prediction Accuracy
from Test Corpus
(%)
Trigram Prediction 93.11 92.21
Trigram Prediction
+ Error Correction
99.53 99.42
5 CONCLUSION
In this paper, we have applied trigram model and error-
correction rules for intelligent Thai key prediction and English-
Thai language identification. The result of the experiment shows
the high accuracy of more than 99 percent accuracy, which is
very impressive.  Through this system typing is much more
easier and enjoyable for Thais. This technique is expected to be
able to apply to other Asian languages. Our future work is to
apply the algorithm to mobile phones, handheld devices and
multilingual input systems.
REFERENCES
[1] Brill, E. (1997) Automatic Rule Acquisition for Spelling
 Correction. ICML.
[2] Brill, E. (1993) A Corpus-Based Approach to Language
Learning. Ph.D. Dissertation, University of Pennsylvania.
[3] Cavnar, W. (1994) N-gram Based Text Categorization.
 Proceedings of the Third Annual
 Symposium on Document Analysis and Information
 Retrieval, pp.161-169.
[4] Cormen, T., Leiserson, C. and Rivest, R. (1990)
 Introduction to Algorithms, MIT Press
[5] Kikui, G. (1998) Identifying the Coding System and
 Language of On-line Documents on the Internet.
 Proceedings of the 16th International Conference on
 Computational Linguistics, pp. 652-657.
[6] Knuth, D., Morris J., and Pratt V. (1977) Fast pattern
matching in strings. SIAM Journal on Computing.  6(2),
pp.323-350.
[7] Sornlertlamvanich, V., Potipiti, T., and Charoenporn, T.
 (2000) Automatic Corpus-Based Thai Word Extraction with
 the C4.5 Machine Learning Algorithm. The Proceedings of
 the 18th International Conference on Computational
 Linguistics,  pp. 802-807.
[8] Zheng, F., Wu, J. and Wu, W. (2000) Input Chinese
 Sentences Using Digits. The Proceedings of the
 6th International Conference on Spoken Language
 Processing, vol. 3, pp. 127-130.
The State of the Art in Thai Language Processing
Virach Sornlertlamvanich, Tanapong Potipiti, Chai Wutiwiwatchai and Pradit Mittrapiyanuruk
National Electronics and Computer Technology Center (NECTEC),
National Science and Technology Development Agency,  Ministry of Science and Technology Environment.
22nd Floor Gypsum Metropolitan Tower 539/2 Sriayudhya Rd. Rajthevi Bangkok 10400 Thailand.
Email: {virach, tanapong, chai}@nectec.or.th, pmittrap@notes.nectec.or.th
Abstract
This paper reviews the current state of tech-
nology and research progress in the Thai
language processing. It resumes the charac-
teristics of the Thai language and the ap-
proaches to overcome the difficulties in each
processing task.
1 Some Problematic Issues in the Thai
Processing
It is obvious that the most fundamental semantic
unit in a language is the word. Words are ex-
plicitly identified in those languages with word
boundaries. In Thai, there is no word boundary.
Thai words are implicitly recognized and in
many cases, they depend on the individual
judgement. This causes a lot of difficulties in the
Thai language processing. To illustrate the
problem, we employed a classic English exam-
ple.
The segmentation of  ? GODISNOWHERE?.
No. Segmentation Meaning
(1) God is now here. God is here.
(2) God is no where. God doesn?t exist.
(3) God is nowhere. God doesn?t exist.
With the different segmentations, (1) and (2)
have absolutely opposite meanings. (2) and 3
are ambiguous that nowhere is one word or two
words. And the difficulty becomes greatly ag-
gravated when unknown words exist.
As a tonal language, a phoneme with differ-
ent tone has different meaning. Many unique
approaches are introduced for both the tone gen-
eration in speech synthesis research and tone
recognition in speech recognition research.
These difficulties propagate to many levels in
the language processing area such as lexical ac-
quisition, information retrieval, machine trans-
lation, speech processing, etc. Furthermore the
similar problem also occurs in the levels of sen-
tence and paragraph.
2 Word and Sentence Segmentation
The first and most obvious problem to attack is
the problem of word identification and segmen-
tation. For the most part, the Thai language
processing relies on manually created dictionar-
ies, which have inconsistencies in defining word
units and limitation in the quantity.  [1] proposed
a word extraction algorithm employing C4.5
wit  some string features such as entropy and
mutual information. They reported a result of
85% in precision and 50% in recall measures.
For word segmentation, the longest matching,
maximal matching and probabilistic segmenta-
tion had been applied in the early research [2],
[3]. However, these approaches have some
limitations in dealing with unknown words.
More advanced techniques of word segmenta-
tion captured many language features such as
context words, parts of speech, collocations and
semantics [4], [5]. These reported about 95-99 %
of accuracy. For sentence segmentation, the tr-
gram model was adopted and yielded 85% of
accuracy [6].
3 Machine Translation
Currently, there is only one machine
translation system available to
the public, called ParSit (http://www.
links.nectec.or.th/services/parsit),   it is a service
of English-to-Thai webpage translation.  ParSiT
is a collaborative work of NECTEC, Thailand
and NEC, Japan. This system is based on an in-
terlingual approach MT and the translation accu-
racy is about 80%.  Other approaches such as
generate-and-repair [7] and sentence pattern
mapping have been also studied [8].
4 Language Resources
The only Thai text corpus available for research
use is the ORCHID corpus. ORCHID is a 9-MB
Thai part-of-speech tagged corpus initiated by
NECTEC, Thailand and Communications Re-
search Laboratory, Japan. ORCHID is available
at http://www.links.nectec.or.th /orchid.
5 Research in Thai OCR
Frequently used Thai characters are about 80
characters, including alphabets, vowels, tone
marks, special marks, and numerals. Thai writ-
ing are in 4 levels, without spaces between
words, and the problem of similarity among
many patterns has made research challenging.
Moreover, the use of English and Thai in general
Thai text creates many more patterns which
must be recognized by OCR.
For more than 10 years, there has been a con-
siderable growth in Thai OCR research,
especially for ?printed character? task. The early
proposed approaches focused on structural
matching and tended towards neural-network-
based algorithms with input for some special
characteristics of Thai characters e.g., curves,
heads of characters, and placements. At least 3
commercial products have been launched in-
cluding ?ArnThai? by NECTEC, which claims
to achieve 95% recognition performance on
clean input. Recent technical improvement of
ArnThai has been reported in [9]. Recently, fo-
cus has been changed to develop system that are
more robust with any unclean scanning input.
The approach of using more efficient features,
fuzzy algorithms, and document analysis is re-
quired in this step.
At the same time, ?Offline Thai handwritten
character recognition? task has been investigated
but is only in the research phase of isolated
characters. Almost all proposed engines were
neural network-based with several styles of in-
put features [10], [11]. There has been a small
amount of research on ?Online handwritten
character recognition?. One attempt was pro-
posed by [12], which was also neural network-
based with chain code input.
6 Thai Speech Technology
Regarding speech, Thai, like Chinese, is a tonal
language. The tonal perception is important to
the meaning of the speech. The research cur-
rently being done in speech technology can be
divided into 3 major fields: (1) speech analysis,
(2) speech recognition and (3) speech synthesis.
Most of the research in (1) done by the linguists
are on the basic study of Thai phonetics e.g.
[13].
In speech recognition, most of the current
research [14] focus on the recognition of isolated
words. To develop continuous speech recogni-
tion, a large-scale speech corpus is needed. Th
status of practical research on continuous speech
recognition is in its initial step with at least one
published paper [15]. In contrast to western
speech recognition, topics specifying tonal lan-
guages or tone recognition have been deeply
researched as seen in many papers e.g., [16].
For text-to-speech synthesis, processing the
idiosyncrasy of Thai text and h ling the tones
i terplaying with intonation are the topics that
make the TTS algorithm for the Thai language
different from others. In the research, the first
su cessful system was accomplished by [14] and
later by NECTEC [15]. Both systems employ
the same synthesis technique based on the con-
aten tion of demisyllable inventory units.
R ferences
[1] V. Sornlertlamvanich, T. Potipiti and T. Charoenporn. Auto-
matic Corpus-Based Thai Word Extraction with the C4.5
Learning Algorithm. In forthcoming Proceedings of COLING
2000.
[2] V. Sornlertlamvanich. Word Segmentation for Thai in Machine
Translation System Machine Translation. N ti al Electronics
and Computer Technology Center, Bangkok. pp. 50-56, 1993.
(in Thai).
[3] A. Kawtrakul, S. Kumtanode, T. Jamjunya nd A. Jewriyavech.
Lexibase Model for Writing Production Assistant System. In
Proceedings of the Symposium on Natural Language Processing
in Thailand, 1995.
[4] S. Meknavin, P. Charoenpornsawat and B. Kijsirikul. Featured
Based Thai Word Segmentation. In Proceedings of Natural
Language Processing Pacific Rim Symposium, pp. 41-46, 1997.
[5] A. Kawtrakul, C. Thumkanon, P. Varasarai and M. Sukta-
rachan. Autmatic Thai Unknown Word Recognition. I  Proceed-
ings of Natural Language Processing Pacific Rim Symposium,
pp. 341-347, 1997.
[6] P. Mitrapiyanurak and V. Sornlertlamvanich. The Automatic
Thai Sentence Extraction. In Proceedings of the Fourth Sympo-
sium on Natural Language Processing, pp. 23-28, May 2000.
[7] K. Naruedomkul and N. Cercone. Generate and Repair
Machine Translation. I  Proceedings of the Fourth Symposium
on Natural Language Processing, pp. 63-79, May 2000.
[8] K. Chancharoen and B. Sirinaowakul. English Thai Machine
Translation Using Sentence Pattern Mapping. In Proceedings of
the Fourth Symposium on Natural Language Processing, pp. 29-
36, May 2000.
[9] C. Tanprasert and T. Koanantakool. Thai OCR: A Neural Net-
work Application. I  Proceedings of IEEE Region Ten Confer-
ence, vol.1, pp.90-95, November 1996.
[10] I. Methasate, S. Jitapankul, K. Kiratiratanaphung and W.
Unsiam. Fuzzy Feature Extraction for Thai Handwritten Char-
acter Recognition. I  Proceedings of the Forth Symposium on
Natural Language Processing, pp.136-141, May 2000.
[11] P. Phokharatkul and C. Kimpan. Handwritten Thai Character
Recognition using Fourior Descriptors and Genetic Neural Net-
works. In Proceedings of the Fourth Symposium on Natural
Language Processing, pp.108-123, May 2000.
[12] S. Madarasmi and P. Lekhachaiworakul. Customizable Online
Thai-English Handwriting Recognition. In Proceedings of the
Forth Symposium on Natural Language Processing, pp.142-153,
May 2000.
[13] J. T. Gandour, S. Potisuk and S. Dechongkit. Tonal Coarticu-
lation in Thai, Journal of Phonetics, vol 22, pp.477-492, 1994.
[14] S. Luksaneeyanawin, et al A Thai Text-to-Speech System. In
Proceedings of Fourth NECTEC Conference, pp.65-78, 1992. (in
Thai).
[15] P. Mittrapiyanuruk, C. Hansakunbuntheung, V. Tesprasit and
V. Sornlertlamvanich. Improving Naturalness of Thai Text-to-
Speech Synthesis by Prosodic Rule. In forthcoming Proceedings
of ICSLP2000.
[16] S. Jitapunkul, S. Luksaneeyanawin, V. Ahkuputra, C. Wuti-
wiwatchai. Recent Advances of Thai Speech Recognition in
Thailand. In Proceedings of IEEE Asia-Pacific conference on
Circuits and  Systems,  pp.173-176, 1998.
