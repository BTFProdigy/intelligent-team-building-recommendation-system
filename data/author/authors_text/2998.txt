  
Computer-Aided Generation of Multiple-Choice Tests 
 
Ruslan Mitkov, Le An Ha 
School of Humanities, Languages and Social Sciences 
University of Wolverhampton, WV1 1SB 
Email {r.mitkov, l.a.ha}@wlv.ac.uk 
 
Abstract  
This paper describes a novel computer-aided 
procedure for generating multiple-choice tests from 
electronic instructional documents. In addition to 
employing various NLP techniques including term 
extraction and shallow parsing, the program makes 
use of language resources such as a corpus and 
WordNet. The system generates test questions and 
distractors, offering the user the option to post-edit 
the test items. 
1. Introduction 
Multiple-choice tests have proved to be an efficient tool 
for measuring students' achievement.1 The manual 
construction of such tests, however, is a time-
consuming and labour-intensive task. 
In this paper we seek to provide an alternative to the 
lengthy and demanding activity of developing multiple-
choice tests and propose a new, NLP-based approach 
for generating tests from narrative texts (textbooks, 
encyclopaedias).  The approach uses a simple set of 
transformational rules, a shallow parser, automatic term 
extraction, word sense disambiguation, a corpus and 
WordNet. While in the current experiment we have used 
an electronic textbook in linguistics to automatically 
generate test items in this area, we should note that the 
methodology is general and can be extended to 
practically any other area. 
To the best of our knowledge, no related work has 
been reported addressing such a type of application.2 
                                                           
1 This work is not concerned with (and does not discuss) 
the issue of whether multiple-choice tests are better 
assessment methodology that other types of tests. What 
it focuses on is a new NLP methodology to generate 
multiple-choice tests about facts explicitly stated in a 
text. 
2 Fairon (1999) reports that their exercises ?can take the 
appearance of a multiple choice test? (if distractors are 
added), but does not explain exactly as to how this can 
be done. 
2. NLP-based methodology for generation 
of multiple-choice test items 
The proposed methodology for generating multiple-
choice test items is based on the premise that questions 
should focus on key concepts rather than addressing less 
central and even irrelevant concepts or ideas. Therefore 
the first stage of the procedure is to identify domain-
specific terms which serve as ?anchors? of each 
question. By way of example, syntax is a prime 
candidate for a domain-specific term in the sentence 
"Syntax is the branch of linguistics which studies the 
way words are put together into sentences". This 
sentence can be then transformed into questions asking 
about this term such as "Which branch of linguistics 
studies the way words are put together into sentences?" 
or "Which discipline studies the way words are put 
together into sentences?" both of which can act as stems 
in multiple-choice test items. 
Another important premise is that distractors3 
should be as semantically close to the correct answer as 
possible so that no additional clues are provided for the 
students. Semantically close distractors are more 
plausible and therefore better at distinguishing good, 
confident students from poor and uncertain ones. In the 
above example, the distractors for the correct answer 
syntax should preferably be semantics or pragmatics 
and not chemistry or football, for instance.  
In order to keep the test item comprehensible and 
avoid additional complexity, the test questions are 
generated from declarative sentences using simple 
transformational rules which, in turn, results in only 
minimal change of the original wording. 
Underpinned by the above principles, a system for 
computer-aided generation of multiple-choice test items 
from instructional documents in electronic form has 
been implemented. The system is built on separate 
components, which perform the following tasks: (i) term 
extraction, (ii) selection of distractors and (iii) question 
generation.  
                                                           
3 Known also as ?distracters? in the literature of classical 
test theory. 
  
2.1 Term extraction 
To retrieve terms, nouns and noun phrases are first 
identified, using the FDG shallow parser (Tapanainen 
and J?rvinen 1997).  Next, their frequency is counted 
and sorted, and nouns with a frequency over a certain 
threshold4 are considered as key terms. In addition, noun 
phrases having these key terms as heads, and satisfying 
the regular expression [AN]+N or [AN]*NP[AN]*N 
(Justeson and Katz 1996), are considered as terms. 
Although this method is very simple,5 the results show 
that, for this particular application, the performance is 
more than acceptable (only 3 questions did not address a 
domain-specific term). One of the main reasons not to 
employ more complicated methods for term extraction 
derives from the small size of the corpus used in the 
current experiment (10 000 words). 
It should be noted that, from a keyword, as in the 
case of the keyword "phrase", a list of semantically 
close terms including noun phrase, verb phrase, 
adjective phrase and adverb phrase can be obtained. In 
addition, a word sense disambiguation program is used 
to identify the correct sense of the alternatives given 
that WordNet frequently returns an unnecessarily high 
number of senses. The word sense disambiguation 
algorithm compares the definition of sense (as extracted 
from WordNet) and the context of the keyword (words 
around the keyword in the corpus). 
As an illustration, in the following extract (Kies 
2003) 
 
(1) A prepositional phrase at the beginning of a 
sentence constitutes an introductory modifier. 
 
one of the terms identified is introductory modifier 
which can serve as an ?anchor? for generating  the test 
question. 
2.2 Selection of distractors 
WordNet is consulted to compute concepts semantically 
close to the correct answer/concept which can then be 
selected as distractors. WordNet retrieves hypernyms, 
hyponyms, and coordinates of the term, if applicable. If 
WordNet returns too many concepts, those appearing in 
the corpus are given preference. If, as in (1), the term is 
                                                           
4 For this particular project the threshold has been 
determined through experiments. The value of the 
threshold of course depends on a number of parameters 
such as the size of the corpus, number of nouns etc.  
5 We experimented with the tf.idf method for key term 
extraction and noted that while precision is slightly 
higher, recall is much lower. As the time needed to 
validate a question is much less than the time needed to 
produce it, we believe that the recall rate is more 
important. 
a noun phrase and WordNet fails to return any 
semantically close concept, the corpus is searched for 
noun phrases with the same head which are then used as 
distractors.6 As an illustration, the electronic textbook 
contains the following noun phrases with modifier as the 
head, each one of which can act as a distractor: modifier 
that accompanies a noun, associated modifier, 
misplaced modifier. As a result, the program generates 
the following multiple-choice test item: 
 
(2) What does a prepositional phrase at the 
beginning of a sentence constitute? 
 
i. a modifier that accompanies a 
noun 
ii. an associated modifier 
iii. an introductory modifier 
iv. a misplaced modifier 
 
2.3 Generation of test questions 
Sentences eligible for question generation are those 
containing domain-specific terms. Another condition for 
a sentence to be eligible is that its structure is of SVO or 
SV type.7 Currently, a number of simple question 
generation rules have been implemented. Example rules 
include the transformation of an SVO sentence in which 
the subject is a term, into the question  "Which HVO" 
where H is a hypernym of the term. Such a rule would 
generate the question "Which part of speech is the most 
central element in a clause" from the sentence "The verb 
is the most central element in a clause". This rule 
operates in several variants, one being that if the 
hypernym is a key term, then a ?Which kind of? 
question may be generated  (e.g. ?Transitive verbs 
require objects? would trigger the question "Which kind 
of verbs require objects?"). Another rule often used 
transforms an SVO sentence with object representing a 
term into the question "What do/does/did the S V". By 
way of example, this rule would convert the sentence in 
example (1) into the question "What does a 
prepositional phrase at the beginning of a sentence 
constitute?" 
The system makes use of agreement rules which 
ensure the grammaticality of the question generated. 
These rules also check for agreement between concepts 
mentioned in the question and the distractors. As an 
illustration, in addition to the local agreement in the 
question "What kind of phrases can act as adjectives, 
                                                           
6 In the rare case of the program not being able to 
extract suitable distractors from WordNet or/and from 
the corpus, no test item is generated. 
7 Sentences of such types are identified by the FDG 
parser which returns syntax functions. 
  
?? 
29 of 36 
Which kind of pronoun will agree with the
subject in number, person, and gender? 
  relative pronoun    
  second person pronoun     
  indefinite pronoun    
  reflexive pronoun   
?? 
 
 
 
adverbs and nouns", the alternatives selected will be 
plural (e.g. infinitive phrases, prepositional phrases, 
adverbial phrases, noun phrases). On the other hand, 
the alternatives belonging to the test item featuring the 
question "What grammatical category does a 
prepositional phrase at the beginning of a sentence 
constitute?" will be singular. 
The generation strategy of multiple-choice items 
included additional genre-specific heuristics such as 
discounting examples for further processing, excluding 
sentences that refer to tables or previously mentioned 
entities, not splitting compound verbs, etc. 
3. In-class experiments and system interface 
We introduced a controlled set8 of the generated test 
items into a classroom environment in order to obtain 
sufficient evaluation data related to their 
acceptability/revision and quality. The controlled set 
currently consists of 24 test items generated with the 
help of the program and 12 items produced manually. 
A total of 45 undergraduate students in 
language/linguistics took the class test. The majority of 
students were from our university, but several students 
were studying in other UK or European Universities. 
Students were asked not to spend more than 2 minutes 
on a test question. 
Figure 1: A snapshot of the interface 
 
The system works through the Questionmark 
Perception web-based testing software which in addition 
to providing a user-friendly interface, computes diverse 
statistics related to the test questions answered.  Figure 
1 shows the interface of the system in a class test 
environment. The test item displayed is one of the 24 
                                                           
8 Only items approved by a linguistics lecturer were 
used in the experiment (e.g. it was made sure that the 
items addressed material covered by undergraduate 
students). 
items generated with the help of the system that are used 
in the experiment.9  
The current experimental setting does not look at the 
problem of delivering a balanced test of preset overall 
difficulty based on random (or constraint-driven) 
selection of test items. Instead, it focuses on exploring 
the feasibility of the computer-aided procedure and on 
the quality of the test items produced. 
4. Evaluation 
In order to validate the efficiency of the method, we 
evaluated the performance of the system in two different 
ways. Firstly, we investigated the efficiency of the 
procedure by measuring the average time needed to 
produce a test item with the help of the program as 
opposed to the average time needed to produce a test 
item manually.10 Secondly, we examined the quality of 
the items generated with the help of the program, and 
compared it with the quality of the items produced 
manually. The quality was assessed via standard test 
theory measures such as discriminating power and 
difficulty of each test item, and the usefulness of each 
alternative was applied. 
4.1 The procedure of generating test items with the 
help of the program and its efficiency 
The first step of the procedure consists of the automatic 
generation of test items. The items so generated were 
then either (i) declared as ?worthy? and accepted for 
direct use without any revision, or further post-edited 
before being put into use, or (ii) declared as ?unworthy? 
and discarded. ?Unworthy? items were those that did not 
focus on a central concept or required too much 
revision, and so they were rejected. 
The items selected for further post-editing required 
minor, fair or major revisions. ?Minor? revision 
describes minor syntactical post-editing of the test 
question, including minor operations such insertions of 
articles, correction of spelling and punctuation. ?Fair? 
revision refers to some grammatical post-editing of the 
test question, including re-ordering or deletion of words 
and replacement of one distractor at most. ?Major? 
revision applied to the generated test items involved 
more substantial grammatical revision of the test 
question and replacement of two or more of the 
                                                           
9 The position of the correct answer (in this case 
?reflexive pronoun?) is generated randomly. 
10 Two graduate students in linguistics acted as post-
editors. The same students were involved in the 
production of test items manually. The texts used were 
selected with care so that possible influence of 
potentially similar or familiar texts was minimised. See 
also the discussion in section 5 on the effect of 
familiarity. 
  
distractors. As an illustration, the automatically 
generated test item  
 
(3) Which kind of language unit seem to be the 
most obvious component of language, and any 
theory that fails to account for the contribution 
of words to the functioning of language is 
unworthy of our attention?  
 
(a) word  
(b) name  
(c) syllable  
(d) morpheme 
 
was not acceptable in this form and required the 
deletion of the text ?and any theory that fails to account 
for the contribution of words to the functioning of 
language is unworthy of our attention? which was 
classed as ?fair? revision. 
From a total of about 575 items automatically 
generated by the program, 57% were deemed to be 
?worthy? i.e. considered for further use. From the 
worthy items, 6% were approved for direct class test use 
without any post-editing and 94% were subjected to 
post-editing.  From the items selected for revision, 17% 
needed minor revision, 36% needed fair revision and 
47% needed major revision. 
The time needed to produce 300 test items with the 
help of the program, including the time necessary to 
reject items, accept items for further editing or approve 
for direct use, amounted to 9 hours. The time needed to 
manually produce 65 questions was 7 hours and 30 
minutes. This results in an average of 1 minute and 48 
seconds to produce a test item with the help of the 
program and an average of 6 minutes and 55 seconds to 
develop a test item manually (Table 1). 
 
  items produced Time 
average 
time  
per item 
computer-aided 300 540' 1' 48'' 
Manual 65 450' 6' 55'' 
Table 1: Effectiveness of the method. 
 
4.2 Analysis of the items generated with the help of 
the program 
Item analysis is an important procedure in classical test 
theory which provides information as to how well each 
item has functioned. The item analysis for multiple-
choice tests usually consists of the following 
information (Gronlund 1982): (i) the difficulty of the 
item, (ii) the discriminating power and (iii) the 
usefulness11 of each alternative. This information can 
tell us if a specific test item was too easy or too hard, 
how well it discriminated between high and low scorers 
on the test and whether all of the alternatives functioned 
as intended. Such types of analysis help improve test 
items or discard defective items. 
In order to conduct this type of analysis, we used a 
simplified procedure, described in  (Gronlund 1982). 
We arranged the test papers in order from the highest 
score to the lowest score. We selected one third of the 
papers and called this the upper group (15 papers). We 
also selected the same number of papers with the lowest 
scores and called this the lower group (15 papers). For 
each item, we counted the number of students in the 
upper group who selected each alternative; we made the 
same count for the lower group. 
 
(i) Item Difficulty 
 
We estimated the Item Difficulty (ID) by establishing 
the percentage of students from the two groups who 
answered the item correctly (ID = C/T x 100, where C is 
the number who answered the item correctly and T is 
the total number of students who attempted the item). 
From the 24 items subjected to analysis, there were 0 
too difficult and 3 too easy items.12 The average item 
difficulty was 0.75. 
 
(ii) Discriminating Power 
 
We estimated the item's Discriminating Power (DP) 
by comparing the number students in the upper and 
lower groups who answered the item correctly. It is 
desirable that the discrimination is positive which means 
that the item differentiates between students in the same 
way that the total test score does.13 The formula for 
computing the Discriminating Power is as follows: DP 
= (CU ? CL): T/2 where CU is the number of students in 
the upper group who answered the item correctly and  
CL - the number of the students in the lower group that 
                                                           
11 Originally called ?effectiveness?. We chose to term 
this type of analysis ?usefulness? to distinguish it from 
the (cost/time) ?effectiveness? of the (semi-) automatic 
procedure as opposed to the manual construction of 
tests. 
12 For experimental purposes, we consider an item to be 
?too difficult? if ID  0.15 and an item ?too easy? if ID  
0.85. 
13 Zero DP is obtained when an equal number of 
students in each group respond to the item correctly. On 
the other hand, negative DP is obtained when more 
students in the lower group than the upper group answer 
correctly. Items with zero or negative DP should be 
either discarded or improved. 
  
did so. Here again T is the total number of students 
included in the item analysis.14 The average DP for the 
set of items used in the class test was 0.40. From the 
analysed test items, there were was only one item that 
had a negative discrimination. 
 
(iii) Usefulness of the distractors 
  
The usefulness of the distractors is estimated by 
comparing the number of students in the upper and 
lower groups who selected each incorrect alternative. A 
good distractor should attract more students from the 
lower group than the upper group.  
The evaluation of the distractors estimated the 
average difference between students in the lower and 
upper groups to be 1.92. Distractors classed as poor are 
those that attract more students from the upper group 
than from the lower group, and there were 6 such 
distractors. On the other hand, we term distractors not 
useful if they are selected by no student. The evaluation 
showed that there were 3 distractors deemed not useful. 
4.3 Analysis of the items constructed manually 
An experiment worthwhile pursing was to conduct item 
analysis of the manually produced test items and 
compare the results obtained regarding the items 
produced with the help of the program. A set of 12 
manually produced items were subjected to the above 
three types of item analysis. There were 0 too difficult 
and 1 too easy items. The average item difficulty of the 
items was 0.59. The average discriminating power was 
assessed to be 0.25 and there were 2 items with negative 
discrimination. The evaluation of the usefulness of the 
distractors resulted in an average difference between 
students in the upper and lower groups of 1.18. There 
were 10 distractors that attracted more students from the 
                                                           
14 Maximum positive DP is obtained only when all 
students in the upper group answer correctly and no one 
in the lower group does. An item that has a maximum 
DP (1.0) would have an ID 0.5; therefore, test authors 
are advised to construct items at the 0.5 level of 
difficulty. 
upper group and were therefore, declared as poor and 2 
distractors not selected at all, and therefore deemed to 
be not useful. 
Table 2 summarises the item analysis results for 
both test items produced with the help of the program 
and those produced by hand. 
5. Discussion and plans for future work 
The evaluation results clearly show that the construction 
of multiple-choice test items with the help of the 
program is much more effective than purely manual 
construction. We believe that this is the main advantage 
of the proposed methodology. As an illustration, the 
development of a test databank of considerable size 
consisting of 1000 items would require 30 hours of 
human input when using the program, and 115 hours if 
done manually. This has direct financial implications as 
the time and cost in developing test items would be 
dramatically cut. 
At the same time, the test item analysis shows that 
the quality of test items produced with the help program 
is not compromised in exchange for time and labour 
savings. The test items produced with of the program 
were evaluated as being of very satisfactory quality. As 
a matter of fact, in many cases they scored even better 
than those manually produced. Whereas the item 
difficulty factor assessed for manual items emerges as 
better15, of those produced with the help of the program, 
there were only 3 too easy items and 0 too difficult ones. 
In addition, whilst the values obtained for the 
discriminating power are not as high as we would have 
desired, the items produced with the help of the program 
scored much better on that measure and what is also 
very important, is that there was only one item among 
them with negative discrimination (as opposed to 2 
from those manually constructed). Finally, the analysis 
of the distractors confirms that it is not possible to class 
the manually produced test items as better quality than 
the ones produced with the help of the program. The test 
items generated with the help of the program scored 
                                                           
15 Ideally, item difficulty should be around the mark of 
0.5 
item difficulty item discriminating power usefulness of distractors 
 
avg 
item 
difficulty 
too 
easy 
Too 
difficult 
average 
discriminating 
power 
negative 
discriminating 
power 
poor 
not 
useful 
Total 
avg 
difference 
computer-
aided 
0.75 3 0 0.4 1 6 3 65 1.92 
manual 0.59 1 0 0.25 2 10 2 33 1.18 
Table 2: Item analysis 
 
  
better on the number of distractors deemed as not useful, 
were assessed to contain fewer poor distractors and had 
a higher average difference between students in the 
lower and upper groups. 
In order to ensure a more objective assessment of the 
efficiency of the procedure, we plan to run the following 
experiment. At least 6 months after a specific set of 
items has been produced with the help of the program, 
the post-editors involved will be asked to produce 
another, based on the same material, manually. 
Similarly, after such a period items originally produced 
manually will be produced by the same post-editors 
with the help of the program. Such an experiment is 
expected to extinguish any effect of familiarity and to 
provide a more objective measure as to how computer-
aided construction of tests is more effective than manual 
production.  
It should be noted that the post-editors were not 
professional test developers. It would be interesting to 
investigate the impact of the program on professional 
test developers. This is an experiment envisaged as part 
of our future work. 
In addition to extending the set of test items to be 
evaluated and the samples of students taking the test, 
further work includes experimenting with more 
sophisticated term extraction techniques and with other 
more elaborate models for measuring semantic 
similarity of concepts. We would like to test the 
feasibility of using collocations from an appropriate 
domain corpus with a view to extending the choice of 
plausible distractors. We also envisage the development 
of a more comprehensive grammar for generating 
questions, which in turn will involve studying and 
experimenting with existing question generation 
theories. As our main objective has been to investigate 
the feasibility of the methodology, we have so far 
refrained from more advanced NLP processing of the 
original documents such as performing anaphora 
resolution and temporal or spatial reasoning which will 
certainly allow for more questions to be generated. 
Future work also envisages evaluation as to what extent 
the questions cover the course material. Finally, even 
though the agreement between post-editors appears to 
be a complex issue, we would like to investigate it in 
more depth. This agreement should be measured on 
semantic rather than syntactic principles, as the post-
editors may produce syntactically different test 
questions which are semantically equivalent. Similarly, 
different distractors may be equally good if they are 
equal in terms of semantic distance to the correct 
answer. 
 
6. Conclusion 
This paper describes a novel NLP-based and computer-
aided procedure for the construction of multiple-choice 
tests from instructional documents in electronic form. 
The results from the evaluation conducted suggest that 
the new procedure is very effective in terms of time and 
labour, and that the test items produced with the help of 
the program are not of inferior quality to those produced 
manually. 
 
References 
 
Fairon, C. (1999). ?A Web-based System for Automatic 
Language Skill Assessment: EVALING?. 
Proceedings of Computer Mediated Language 
Assessment and Evaluation in Natural Language 
Processing Workshop. 
Gronlund, N. (1982) Constructing achievement tests. 
New York: Prentice-Hall Inc. 
Justeson, J. S. and S. L. Katz (1996) ?Technical 
terminology: some linguistic properties and an 
algorithm for identification in text?. Natural 
Language Engineering, 3, (2), 259-289. 
Kies, D. (2003) Modern English Grammar. Online 
textbook. 
http://www.papyr.com/hypertextbooks/engl_126/b
ook126.htm 
Tapanainen, P. and J?rvinen, T. (1997) ?A non-
projective dependency parser?. Proceedings of the 
5th Conference of Applied Natural Language 
Processing (ANLP-5), 64-71. 
Proceedings of the Fourth International Natural Language Generation Conference, pages 111?113,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating Multiple-Choice Test Items from Medical Text:
A Pilot Study
Nikiforos Karamanis
Computer Laboratory
University of Cambridge
CB3 0FD, UK
nk304@cam.ac.uk
Le An Ha and Ruslan Mitkov
Computational Linguistics Research Group
University of Wolverhampton
WV1 1SB, UK
{L.A.Ha, R.Mitkov}@wlv.ac.uk
Abstract
We report the results of a pilot study on generating
Multiple-Choice Test Items from medical text and
discuss the main tasks involved in this process and
how our system was evaluated by domain experts.
1 Introduction
AlthoughMultiple-Choice Test Items (MCTIs) are
used daily for assessment, authoring them is a
laborious task. This gave rise to a relatively new
research area within the emerging field of Text-
to-Text Generation (TTG) called Multiple-Choice
Test Item Generation (MCTIG).1
Mitkov et al (2006) developed a system
which detects the important concepts in a
text automatically and produces MCTIs testing
explicitly conveyed factual knowledge.2 This
differs from most related work in MCTIG such as
Brown et al (2005) and the papers in BEAUNLP-
II (2005) which deploy various NLP techniques to
produce MCTIs for vocabulary assessment, often
using preselected words as the input (see Mitkov
et al for more extensive comparisons).
The approach of Mitkov et al is semi-automatic
since the MCTIs have to be reviewed by domain
experts to assess their usability. They report that
semi-automatic MCTIG can be more than 3 times
quicker than authoring of MCTIs without the aid
of their system.
1TTG, in which surface text is used as the input to
algorithms for text production, contrasts with Concept-
to-Text Generation (better known as Natural Language
Generation) which is concerned with the automatic
production of text from some underlying non-linguistic
representation of information (Reiter and Dale, 2000).
2Mitkov et al used an online textbook on Linguistics as
their source text. Clearly, their approach is not concerned
with concepts or facts derived through inferencing. Neither
does it address the problem of compiling a balanced test from
the generated MCTIs.
Moreover, analysis of MCTIs produced semi-
automatically and used in the classroom reveals
that their educational value is not compromised in
exchange for time and labour savings. In fact, the
semi-automatically produced MCTIs turn out to
fare better than MCTIs produced without the aid
of the system in certain aspects of item quality.
This paper reports the results of a pilot study on
generating MCTIs from medical text which builds
on the work of Mitkov et al
2 Multiple-Choice Test Item Generation
A MCTI such as the one in example (1) typically
consists of a question or stem, the correct answer
or anchor (in our example, ?chronic hepatitis?)
and a list of distractors (options b to d):
(1) Which disease or syndrome may progress to cirrhosis
if it is left untreated?
a) chronic hepatitis
b) hepatic failure
c) hepatic encephalopathy
d) hypersplenism
The MCTI in (1) is based on the following clause
from the source text (called the source clause; see
section 2.3 below):
(2) Chronic hepatitis may progress to cirrhosis if it is left
untreated.
We aim to automatically generate (1) from (2)
using our simple Rapid Item Generation (RIG)
system that combines several components
available off-the-shelf. Based on Mitkov et al, we
saw MCTIG as consisting of at least the following
tasks: a) Parsing b) Key-Term Identification c)
Source Clause Selection d) Transformation to
Stem e) Distractor Selection. These are discussed
in the following sections.
111
2.1 Sentence Parsing
Sentence Parsing is crucial for MCTIG since the
other tasks rely greatly on this information. RIG
employs Charniak?s (1997) parser which appeared
to be quite robust in the medical domain.
2.2 Key-Term Identification
One of our main premises is that an appropriate
MCTI should have a key-term as its anchor
rather than irrelevant concepts. For instance, the
concepts ?chronic hepatitis? and ?cirrhosis? are
quite prominent in the source text that example (2)
comes from, which in turn means that MCTIs
containing these terms should be generated using
appropriate sentences from that text.
RIG uses the UMLS thesaurus3 as a domain
specific resource to compute an initial set of
potential key terms such as ?hepatitis? from the
source text. Similarly to Mitkov et al, the initial
set is enlarged with NPs featuring potential key
terms as their heads and satisfying certain regular
expressions. This step adds terms such as ?acute
hepatitis? (which was not included in the version
of UMLS utilised by our system) to the set.
The tf.idf method (that Mitkov et al did
not find particularly effective) is used to promote
the 30 most prominent potential key terms within
the source text for subsequent processing, ruling
out generic terms such as ?patient? or ?therapy?
which are very frequent within a larger collection
of medical texts (our reference corpus).
2.3 Source Clause Selection
Mitkov et al treat a clause in the source text
as eligible for MCTIG if it contains at least one
key term and is finite as well as of the SV(O)
structure. They acknowledge, however, that this
strategy gives rise to a lot of inappropriate source
clauses, which was the case in our domain too.
To address this problem, we implemented a
module which filters out inappropriate structures
for MCTIG (see Table 1 for examples). This
explains why the number of key terms and MCTIs
varies among texts (Table 2).
A finite main clause which contains an NP
headed by a key term and functioning as a
subject or object with all the subordinate clauses
which depend on it is a source clause eligible
for MCTIG provided that it satisfies our filters.
Example (2) is such an eligible source clause.
3http://www.nlm.nih.gov/research/umls/
Structure Example (key term in italics)
Subordinate clause Although asthma is a lung disease, ...
Negated clause Autoimmune hepatitis should not
be treated with interferon.
Coordinated NP Excessive salt intake causes
hypertension and hypokalemia.
Initial pronoun It associates with hypertension instead.
Table 1: Inappropriate structures for MCTIG.
Experimentation during development showed that
our module improves source clause selection by
around 30% compared to the baseline approach of
Mitkov et al
2.4 Transformation to Stem
Once an appropriate source clause is identified,
it has to be turned to the stem of a MCTI. This
involves getting rid of discourse cues such as
?however? and substituting the NP headed by the
key term such as ?chronic hepatitis? in (1) with a
wh-phrase such as ?which disease or syndrome?.
The wh-phrase is headed by the semantic type of
the key-term derived from UMLS.
RIG utilises a simple transformational
component which produces a stem via minimal
changes in the ordering of the source clause. The
filtering module discussed in the previous section
disregards the clauses in which the key term
functions as a modifier or adjunct. Additionally,
most of the key terms in the eligible source clauses
appear in subject position which in turn means
that wh-fronting and inversion is performed in just
a handful of cases. The following example, again
based on the source clause in (2), is one such case:
(3) To which disease or syndrome may chronic hepatitis
progress if it is left untreated?
2.5 Selection of Appropriate Distractors
MCTIs aim to test the ability of the student
to identify the correct answer among several
distractors. An appropriate distractor is a concept
semantically close to the anchor which, however,
cannot serve as the right answer itself.
RIG computes a set of potential distractors
for a key term using the terms with the same
semantic type in UMLS (rather than WordNet
coordinates employed by Mitkov et al). Then, we
apply a simple measure of distributional similarity
derived from our reference corpus to select the
best scoring distractors. This strategy means that
MCTIs with the same answer feature very similar
distractors.
112
# of # of Usable Usable Items w/out Replaced distractors Total Average Time
Chapter Words Key-terms Items Items post-edited stems per term Time per Item
Asthma 8,843 9 66 42 (64%) 18 (27%) 2.0 140 mins 3 mins 20 secs
Hepatitis 10,259 17 92 49 (53%) 19 (21%) 0.9 150 mins 3 mins 04 secs
Hypertension 12,941 22 121 59 (49%) 15 (12%) 0.8 200 mins 3 mins 23 secs
Total 32,043 40 279 150 (54%) 52 (19%) ? 490 mins 3 mins 16 secs
Table 2: Usability and efficiency of Multiple-Choice Test Item Generation from medical text.
3 Evaluation
RIG is a simple system which often avoids
tough problems such as dealing with key-terms in
syntactic positions that might puzzle the parser or
might be too difficult to question upon. So how
does it actually perform?
Three experts in producing MCTIs for medical
assessment jointly reviewed 279 MCTIs (each
featuring four distractors) generated by the
system. Three chapters from a medical textbook
served as the source texts while a much larger
collection of MEDLINE texts was used as the
reference corpus.
The domain experts regarded a MCTI as
unusable if it could not be used in a test or required
too much revision to do so. The remaining items
were considered to be usable and could be post-
edited by the experts to improve their content and
readability or replace inappropriate distractors.
As Table 2 shows, more than half of the items in
total were judged to be usable. Additionally, about
one fifth of the usable items did not require any
editing. The Table also shows the total number of
key-terms identified in each chapter as well as the
average number of distractors replaced per term.
The last column of Table 2 reports on the
efficiency of MCTIG in our domain. This variable
is calculated by dividing the total time it took
the experts to review all MCTIs by the amount
of usable items which represent the actual end-
product. This is a bit longer than 3 minutes
per usable item across all chapters. Anecdotal
evidence and the experts? own estimations suggest
that it normally takes them at least 10 minutes to
produce an MCTI manually.
Given the distinct domains in which our system
and the one of Mitkov et al were deployed (as
well as the differences between them), a direct
comparison between them could be misleading.
We note, however, that our usability scores are
always higher than their worst score (30%) and
quite close to their best score (57%). The amount
of directly usable items in Mitkov et al was
between just 3.5% and 5%, much lower than
what we achieved. They also report an almost
3-fold improvement in efficiency for computer-
aided MCTIG, which is very similar to our
estimate. These results indicate what our work has
contributed to the state of the art in MCTIG.
In our future work, we aim to address the
following issues: (a) As in Mitkov et al, the
anchor of a MCTI produced by RIG always
corresponds to a key-term. However, the domain
experts pointed out several cases in which it is
better for the key-term to stay in the stem and
for another less prominent concept to serve as the
answer. (b) Students who simply memorise the
input chapter might be able to answer the MCTI if
its surface form is too close to the source clause so
another interesting suggestion was to paraphrase
the stem during MCTIG. (c) We also intend to
introduce greater variability in our process for
distractor selection by investigating several other
measures of semantic similarity.
Acknowledgments
We are grateful to Tony LaDuca, Manfred Straehle and
Robert Galbraith from the National Board of Medical
Examiners (NBME) for their expert-based feedback and to
three anonymous reviewers for their comments.
References
BEAUNLP-II. 2005. Papers on MCTIG by Hoshino and
Nakagawa, Liu et al, and Sumita et al In Proceedings of
the 2nd Workshop on Building Educational Applications
Using NLP.
Jonathan Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. In Proceedings of HLT-EMNLP 2005, pages
249?254.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of AAAI
1997, pages 598?603.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006.
A computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2):177?194.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge University
Press.
113
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 49?56,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Semantic similarity of distractors in multiple-choice tests:  extrinsic evaluation 
Ruslan Mitkov, Le An Ha, Andrea Varga and Luz Rello University of Wolverhampton Wolverhampton, UK {R.Miktov, L.A.Ha, Andrea.Varga, L.RelloSanchez}@wlv.ac.uk     Abstract Mitkov and Ha (2003) and Mitkov et al (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. One of the interesting research questions which emerged during these projects was how better quality distractors could automatically be chosen. This paper reports the results of a study seeking to establish which similarity measures generate better quality distractors of multiple-choice tests. Similarity measures employed in the procedure of selection of distractors are collocation patterns, four different methods of WordNet-based semantic similarity (extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s as well as Lin?s measures), distributional similarity, phonetic similarity as well as a mixed strategy combining the aforementioned measures. The evaluation results show that the methods based on Lin?s measure and on the mixed strategy outperform the rest, albeit not in a statistically significant fashion. 1 Introduction  Multiple-choice tests are sets of test items, the latter consisting of a question or stem (e.g. Who was voted the best international footballer for 2008?), the correct answer (e.g. 
Ronaldo) and distractors (e.g. Messi, Ronaldino, Torres). This type of test has proved to be an efficient tool for measuring students? achievement and is used on a daily basis both for assessment and diagnostics worldwide.1 According to Question Mark Computing Ltd (p.c.), who have licensed their Perception software to approximately three million users so far, 95% of their users employ this software to administrate multiple-choice tests.2  Despite their popularity, the manual construction of such tests remains a time-consuming and labour-intensive task. One of the main challenges in constructing a multiple-choice test item is the selection of plausible alternatives to the correct answer which will better distinguish confident students from unconfident ones. Mitkov and Ha (2003) and Mitkov et al (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. This methodology makes use of NLP techniques including shallow parsing, term extraction, sentence transformation and semantic distance computing and employs resources such as corpora and ontologies like WordNet. More specifically, the system identifies important terms in a textbook text,                                                            1 This paper is not concerned with the issue of whether multiple-choice tests are better assessment methodology that other types of tests. What it focuses is on improving our new NLP methodology to generate multiple-choice tests about facts explicitly stated in single declarative sentences by establishing which semantic similarity measures give rise to better distractors. 2 More information on the Perception software can be found at: www.questionmark.com/perception 
49
transforms declarative sentences into questions and mines for terms which are semantically close to the correct answer, to serve as distractors.  The system for generation of multiple-choice tests described in Mitkov and Ha (2003) and in Mitkov et al (2006) was evaluated in practical environment where the user was offered the option to post-edit and in general to accept, or reject the test items generated by the system3. The formal evaluation showed that even though a significant part of the generated test items had to be discarded, and that the majority of the items classed as ?usable? had to be revised and improved by humans, the quality of the items generated and proposed by the system was not inferior to the tests authored by humans, were more diverse in terms of topics and very importantly ? their production needed 4 times less time than the manually written items. The evaluation was conducted both in terms of measuring the time needed to develop test items and in terms of classical test analysis to assess the quality of test items.  The paper is structured as follows. Section 2 will outline the importance of distractors in multiple-choice testing as the different strategies for automatic selection of the distractors are the subject of this study. Section 3 will describe how test items are produced and will detail the different strategies (semantic similarity measures and phonetic similarity) used for the selection of distractors. Section 4 outlines the in-class experiments, presents the evaluation methodology, reports on the results and discusses these results. 2 The importance of quality distractors One of the interesting research questions which emerged during the above research was how better quality distractors could automatically be chosen. In fact user evaluation showed that from the three main tasks performed in the generation of multiple-choice tests (term identification, sentence transformation and distractor selection), it was distractor selection which needed further improvement with a view to putting it in practical use.                                                            3 A post-editor?s interface was developed to this end. 
Distractors play a vital role for the process of multiple-choice testing in that good quality distractors ensure that the outcome of the tests provides more credible and objective picture of the knowledge of the testees involved. On the other hand, poor distractors would not contribute much to the accuracy of the assessment as obvious or too easy distractors will pose no challenge to the students and as a result, will not be able to distinguish high performing from low performing learners. The principle according to which the distractors were chosen, was semantic similarity (Mitkov and Ha, 2003). The semantically closer were the distractors to the correct answer, the most ?plausible? they were deemed to be. The rationale behind this consists in the fact that distractors semantically distant from the correct answer could make guessing a ?straightforward task?. By way an example, if processing the sentence ?Syntax is the branch of linguistics which studies the way words are put together into sentences?, the multiple-choice generation system would identify syntax as an important term, would transform the sentence into the question ?Which branch of linguistics studies the way words are put together into sentences?? and would choose ?Pragmatics?, ?Morphology? and ?Semantics? as distractors to the correct answer ?Syntax?, being closer to it than ?Chemistry?, ?Football? or ?Beer? for instance (which if offered as distractors, would be easily dismissed by people who do not have even any knowledge of linguistics). While the semantic similarity premise appears as a logical way forward to automatically select distractors, there are different methods or measures which compute semantic similarity. Each of these methods could be evaluated individually but here we evaluate their suitability for the task of selection of distractors in multiple-choice tests. This type of evaluation could be regarded as extrinsic evaluation of each of the methods, where the benchmark for their performance would not be an annotated corpus or human judgement on accuracy, but to what extent a specific NLP application can benefit from employing a method. Another premise that this study seeks to verify is whether orthographically close distractors, in addition to being semantically related, could yield even better results.  
50
3 Production of test items and selection of distractors Test items were constructed by a program based on the methodology described in the previous section. We ran the program on an on-line course materials in linguistics (Vajda, 2001). A total of 144 items were initially generated. 31 out of these 144 items were kept for further considerations as they either did not need any or, only minor revision. The remaining 113 items were deemed to require major post-editing revision. The 31 items kept for consideration were further revised by a second linguist and finally, we narrowed down the selection to 20 questions for the experiments4. These 20 questions gave a rise to a total of eight different assessments. Each assessment had the same 20 questions but they differed in the sets of distractors as these were chosen using different similarity measures5 (sections 3.1-3.5). To generate a list of distractors for single-word terms the function coordinate terms in WordNet is employed. For multi-word terms, noun phrases with the same head as the correct answers appearing in the source text as well as entry terms from Wikipedia having the same head with the correct answers, are used to compile the list of distractors. This list of distractors is offered to the user from which he or she could choose his/her preferred distractors.  In this study we explore which is the best way to narrow down the distractors to the 4 most suitable ones. To this end, the following strategies for computing semantic (and in one case, phonetic) similarity were employed: (i) collocation patterns, (ii-v) four different methods of WordNet-based semantic similarity (Extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s and Lin?s measures), (vi) Distributional Similarity, and (vii) Phonetic similarity.  
                                                           4 The following is an example of an item generated of the program and then post-edited.  "Which type of clause might contain verb and dependent words? i) verb clause ii) adverb clause iii) adverbial clause    iv) multiple subordinate clause v) subordinate clause". 5 It should be noted that there were cases where the different selection/similarity strategies picked the same distractors. 
3.1 Collocation patterns The collocation extraction strategy used in this experiment is based on the method reported in (Mitkov and Ha, 2003). Distractors that appear in the source text are given preference. If there are not enough distractors, distractors are selected randomly from the list. For the other methods described below (sections 3.2-3.5), instead of giving preference to noun phrases appearing in the same text, and randomly pick the rest from the list, we ranked the distractors in the list based on the similarity scores between each distractor and the correct answer and chose the top 4 distractors. We compute similarity for words rather than multi-word terms. When the correct answers and distractors are multi-word terms, we calculate the similarities between their modifier words. By way of example, in the case of "verb clause" and "adverbial clause", the similarity score between "verb" and "adverbial" is computed. When the correct answer or distractor contains more than one modifiers we compute the similarity for each modifier pairs and we choose the maximum score. (e.g. for "verb clause" and "multiple subordinate clause", similarity scores of "verb" and "multiple" and of "verb" and "subordinate" are calculated, the higher one is considered to represent the similarity score). 3.2 Four different methods for WordNet-based similarity For computing WordNet-based semantic similarity we employed the package made available by Ted Pedersen6. Pedersen?s tool computes (i) extended gloss overlap measure (Banerjee and Pedersen, 2003), (ii) Leacock and Chodorow?s (1998) measure, (iii) Jiang and Conrath?s (1997) measure and (iv) Lin?s (1997) measure.  The extended gloss overlap measure calculates the overlaps between not only the definitions of the two concepts measured but also among those concepts to which they are related. The relatedness score is the sum of the squares of the overlap lengths.  Leacock and Chodorow?s measure uses the normalised path length between the two concepts c1 and c2 and is computed as follows: 
                                                           6 http://search.cpan.org/~tpederse/WordNet-Similarity 
51
 (1) 
where len is the number of edges on the shortest path in the taxonomy between the two concepts and MAX is the depth of the taxonomy. Jiang and Conrath?s measure compares the sum of the information content of the individual concepts with that of their lowest common subsumer: 
 (2) 
where IC(c) is the information content (Patwardhan et al, 2003) of the concept c, and lcs denotes the lowest common subsumer, which represents the most specific concept that the two concepts have in common. The Lin measure scales the information content of lowest common subsumer with the sum of information content of two concepts.  
 (3) 
 3.3 Distributional similarity For computing distributional similarity we made use of Viktor Pekar's implementation7 based on Information Radius, which according to a comparative study by Dagan et al (1997) performs consistently better than the other similar measures. Information Radius (or Jensen-Shannon divergence) is a variant of Kullback-Leiber divergence measuring similarity between two words as the amount of information contained in the difference between the two corresponding co-occurrence vectors. Every word wj is presented by the set of words wi1...n with which it co-occurs. The semantics of wj are modelled as a vector in an n-dimensional space where n is the number of words co-occurring with wj, and the features of the vector are the probabilities of the co-occurrences established from their observed frequencies, as in (4). In Pekar?s implementation, if one word is identified as dependent on another word by a dependency 
                                                           7 http://clg.wlv.ac.uk/demos/similarity/index.html 
parser, these two words are said to be ?co-occuring?8. The corpus used to collect the co-occurance vector was the BNC and the dependency parsed used the FDG parser (Tapanainen and J?rvinen, 1997). The Information Radius (JS) is calculated using (5).  (4) 
 (5) 
where  
3.4 Phonetic similarity For measuring phonetic similarity we use Soundex, phonetic algorithm for indexing words by sound. It operates on the principle of term based evaluation where each term is given a Soundex code. Each Soundex code itself consists of a letter and three numbers between 0 and 6. By way of example the Soundex code of verb is V610 (the first character in the code is always the first letter of the word encoded). Vowels are not used and digits are based on the consonants as illustrate by the following table:  1. B, P, F, V 2. C, S, K, G, J, Q, X, Z 3. D, T 4. L 5. M, N 6. R Table 1 Digits based on consonants First the Soundex code for each word is generated9. Then similarity is computed using the Difference method, returning an integer result ranging in value from 1 (least similar) to 4 (most similar). 3.5 Mixed Strategy After items have been generated by the above seven methods, we pick three items from each method, except from Soundex, where only two items have been picked, to compose an                                                            8 There are many other ways to construct the co-occurrence vectors. This paper does not intend to exploit these different ways. 9 We adopt the phonetic representation used in MS SQL Server. As illustrated above, each soundex code consists of a letter and three numbers, such as A252. 
52
assessment of 20 items. This assessment is called ?mixed?, and used to assess whether or not an assessment with distractors generated by combining different methods would produce a different result from an assessment featuring distractors generated by a single method. 4 In-class experiments, evaluation, results and discussion The tests (papers) generated with the help of our program with the distractors chosen according the different methods described above, were taken by a total of 243 students from different European universities: University of Wolverhampton (United Kingdom), University College Ghent (Belgium), University of Saarbr?cken (Germany), University of Cordoba (Spain), University of Sofia (Bulgaria). A prerequisite for the students taking the test was that they studied language and linguistics and that they had a good command of English. Each test paper consisted of 20 questions and the students had 30 minutes to reply to the questions. The tests were offered through the Questionmark Perception web-based testing software which in addition to providing a user-friendly interface, computes diverse statistics related to the test questions answered.  In order to evaluate the quality of the multiple-choice test items generated by the program (and subsequently post-edited by humans), we employed standard item analysis. Item analysis is an important procedure in classical test theory which provides information as to how well each item has functioned. The item analysis for multiple-choice tests usually consists of the following information (Gronlund, 1982): (i) the difficulty of the item, (ii) the discriminating power and (iii) the usefulness10 of each distractor. This information can tell us if a specific test item was too easy or too hard, how well it discriminated between high and low scorers on the test and whether all of the alternatives functioned as intended. Such types of analysis help improve test items or discard defective items.                                                            10 Originally called ?effectiveness?. We chose to term this type of analysis ?usefulness? to distinguish it from the (cost/time) ?effectiveness? of the semi-automatic procedure as opposed to the manual construction of tests. 
Whilst this study focuses on the quality of the distractors generated, we believe that the distractors are essential for the quality of the overall test and hence the difficulty of an item and its discriminating power are deemed appropriate to assess the quality of distractors, even though the quality of the test stem also pays in important part. On the other hand usefulness is a completely independent measure as it looks at distractors only and not only the combination of stems and distractors. In order to conduct this type of analysis, we used a simplified procedure, described in (Gronlund, 1982). We arranged the test papers in order from the highest score to the lowest score. We selected one third of the papers and called this the upper group. We also selected the same number of papers with the lowest scores and called this the lower group. For each item, we counted the number of students in the upper group who selected each alternative; we made the same count for the lower group. (i) Item Difficulty 
We estimated the Item Difficulty (ID) by establishing the ratio of students from the two groups who answered the item correctly (ID = C/T, where C is the number who answered the item correctly and T is the total number of students who attempted the item). As Table 2 shows, from the items featuring distractors generated using the collocation method11, there were 4 too easy and 0 too difficult items.12 The average Item Difficulty was 0.61. From the items with distractors generated using WordNet-based similarity13, the results were the following. When employing the extended gloss overlap measure there were 2 too easy and 0 too difficult items, showing an average ID of 0.58. Leacock and Chodorow?s measure produced 1 too easy and 3 too difficult items with item average difficulty of 0.54. The use of Jiang and Conrath?s measure resulted in 3 too easy and 1 too difficult items; the average item difficulty observed was 0.57. Lin?s measure delivered the best results from the                                                            11 Henceforth referred to as ?collocation items?; the distractors generated are referred to as ?collocation distractors?. 12 For experimental purposes, we consider an item to be ?too difficult? if ID ? 0.15 and an item ?too easy? if ID ? 0.85. 13 Henceforth referred to as ?WordNet items?; the distractors are referred to as  ?WordNet distractors?. 
53
point of item difficulty with an almost ideal average item difficulty of 0.51 (the recommended item difficult is 0.5; see also footnote 16); there were 2 too easy and 1 too difficult items. The items constructed on the basis of distractors selected via the distributional similarity metric14, scored an average ID of 0.64 with 6 items being too easy and 1 ? too difficult.  From the items with distractors produced using the phonetic similarity algorithm15, there were 4 too easy and 0 too difficult questions with overall average difficult of 0.60. Finally, a mixed strategy produced test items with average difficulty of 0.53, 1 of them being too easy and 0 ? too difficult. The results showed that almost all items produced after selecting distractors using the strategies described above, featured very reasonable ID values. In many cases the average values were close to the recommended ID value of 0.5 with Lin?s measure delivering the best ID of 0.51. Runners-up are the mixed strategy delivering items with average ID 0.53 Leacock and Chodorow?s measure contributing to the generation of items with average ID of 0.54. (ii) Discriminating Power 
We estimated the item's Discriminating Power (DP) by comparing the number students in the upper and lower groups who answered the item correctly. It is desirable that the discrimination is positive which means that the item differentiates between students in the same way that the total test score does.16 The formula for computing the Discriminating Power is as follows: DP = (CU ? CL) : T/2, where CU is the number of students in the upper group who answered the item correctly and  CL the number of the students in the lower group that did so. Here again T is the 
                                                           14 Henceforth referred to as ?distributional items?; the distractors are referred to as ?distributional distractors?. 15 Henceforth referred to as ?phonetic items?; the distractors are referred to as ?phonetic distractors?. 16 Zero DP is obtained when an equal number of students in each group respond to the item correctly. On the other hand, negative DP is obtained when more students in the lower group than the upper group answer correctly. Items with zero or negative DP should be either discarded or improved. 
total number of students included in the item analysis.17  The average Discriminating Power for the collocation items was 0.33 and there were no negative discriminating collocation test items.18 The figures associated to the WordNet items were as follows. The average DP for items produced with the extended gloss overlap measure was 0.32, and there were 2 items with negative discrimination. Leacock and Chodorow?s measure did not produce any items with negative discrimination and the average DP of these was 0.38. Jiang and Conrath?s measure gave rise to 2 negatively discriminating items and the average DP of the items based on this measure was 0.29. The selection of distractors with Lin?s measure resulted in items with average DP of 0.37; none of them had a negative discrimination. The average discrimination power for the distributional items was 0.29 (1 item with negative discrimination) and for phonetic items ? 0.34 (0 item with negative discrimination). The employment of mixed strategy when selecting distractors which resulted in items with average DP of 0.39 (0 items with negative discrimination). The figures related to the Discriminating Power of the items generated showed that whereas the DP was not of the desired high level, as a whole the proportion of items with negative discrimination was fairly low (Table 2). The items did not differ substantially in terms of the values of DP, the top performer being the items where the distractors were selected on the basis of the mixed strategy, followed by those selected by Leacock and Chodorow?s measure and phonetic similarity. (iii) Usefulness of the distractors  The usefulness of the distractors is estimated by comparing the number of students in the upper and lower groups who selected each incorrect alternative. A good distractor should attract more students from the lower group than the upper group.  The evaluation of the distractors estimated the average difference between students in the                                                            17 Maximum positive DP is obtained only when all students in the upper group answer correctly and no one in the lower group does. An item that has a maximum DP (1.0) would have an ID 0.5; therefore, test authors are advised to construct items at the 0.5 level of difficulty. 18 Obviously a negative discriminating test item is not regarded as a good one. 
54
lower and upper groups to be 0.74 for the sets of distractors generated using collocations.  For the WordNet distractors the results were as follows. The average distance between the students in the lower and upper groups was found to be 0.71 for the extended gloss overlap distractors, 0.76 for the Leacock and Chodorow distractors, 0.71 for the Jiang and Conrath distractors and 0.83 for the Lin distractors. For the distractors selected by way of distributional similarity the average difference between students in the lower and upper groups was 0.79, for the phonetic distractors ? 0.66 and for those selected by a mixed strategy ? 0.89. In our evaluation we also used the notions of poor distractors as well as not-useful distractors. Distractors are classed as poor if they attract more students from the upper group than from the lower group. There were 2 (2.5%) poor distractors from the collocation distractors. The WordNet distractors fared as follows with regard to the number of poor distractors. There were altogether 9 (11%) poor distractors from the extended gloss overlap distractors, 9 (11%) from the Leacock and Chodorow distractors, 10 (12%) from the Jiang and Conrath distractors and 10 (12%) from the Lin ones. There were 6 (7.5%) from the distributional similarity which were classed as poor, 5 (6%) from the phonetic similarity ones were classed as poor and 5 (6%) from the distractors selected through a mixed strategy were classed as such (Table 2).  
On the other hand, distractors are termed not useful if they are not selected by any students at all. The evaluation showed (see Table 2) that there were 24 (30%) distractors deemed not useful from the collocation distractors. The figures for not useful distractors for those selected by way of WordNet similarity were as follows: 17 (21%) for extended gloss overlap distractors, 20 (25%) for the Leacock and Chodorow distractors, 19 (24%) for the Jiang and Conrath distractors and 16 (20%) for the Lin ones. From the distributional distractors, 27 (34%) emerged as not useful, whereas 31 (39%) phonetic similarity and 14 (18%) mixed strategy distractors were found not useful.  The overall figures suggest that the ?most useful? distractors are those chosen with mixed strategy (highest average difference 0.89; lowest number of not useful distractors, second lowest number of poor distractors), followed by those chosen with Lin?s WordNet measure (second highest average distance of 0.83; second lowest number of not useful distractors).  Summarising the results of the item analysis, it is clear that there is not a method that outperforms the rest in terms of producing best quality items or distractors. At the same time it is also clear that in general the mixed strategy and Lin?s measure consistently perform better than the rest of methods/measures. Phonetic similarity did not deliver as expected. 
 Item Difficulty Item Discriminating Power Usefulness of distractors 
 average item difficulty too easy too difficult average discriminating power negative discriminating power poor not useful average difference Collocation  items 0.61 4 0 0.33 0 2 24 0.74 WordNet items -  Extended gloss overlap -  Leacock and Chodorow -  Jiang and Conrath -  Lin 
 0.58 0.54 0.57 0.51 
 2 1 3 2 
 0 3 1 1 
 0.32 0.38 0.29 0.37 
 2 0 2 0 
 9 9 10 10 
 17 20 19 16 
 0.71 0.76 0.71 0.83 Distributional items 0.64 6 1 0.29 1 6 27 0.79 Phonetic items 0.60 4 0 0.34 0 5 31 0.66 Mixed strategy items 0.53 1 0 0.39 0 5 14 0.89 Table 2: Item analysis 
55
Although the results indicate that the Lin items have the best average item difficulty, none of the difference (between item difficulty of Lin and other methods, or between any pair of methods) is statistically significant. From the DP point of view, only the difference between mixed strategy (0.39) and distributional items (0.29) is statistically significant (p<0.05). For the distractor usefulness measure, none of the difference is statistically significant (p<0.05). 5 Conclusion In this study we conducted extrinsic evaluation of several similarity methods (collocation patterns; four different methods of WordNet-based semantic similarity: extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s as well as Lin?s measures; distributional similarity; phonetic similarity; mixed strategy) by seeking to establish which one would be most suitable for the task of selection of distractors in multiple-choice tests. The evaluation results based on item analysis suggests that whereas there is not a method that clearly outperforms in terms of delivering better quality distractors, mixed strategy and Lin?s measure consistently perform better than the rest of methods/measures. However, these two methods do not offer any statistically significant improvement over their closest competitors. Acknowledgments We would like to express our gratitude to Kathelijne Denturck, Johann Haller, Veronique Hoste, Constantin Orasan, Miriam Seghiri, Andrea Stockero and Irina Temnikova for helping us in the organisation of the in-class experiments.  References Banerjee, S. and Perderson, T. 2003. Extended gloss overlaps as a measure of semantic relatedness. Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, 805-810. Dagan I., Lee L., and Pereira F. 1997. Similarity-based methods for word sense disambiguation. Proceedings of the 35th Annual Meeting of 
the Association for Computational Linguistics. Madrid, Spain, 56-63. Gronlund, N. 1982. Constructing achievement tests. New York: Prentice-Hall Inc. Jiang, J. and Conrath, D. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of the International Conference on Research in Computational Linguistics. Taiwan, 19-33. Lin, D. 1997. Using syntactic dependency as a local context to resolve word sense ambiguity. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics. Madrid, Spain, 4-71. Leacock, C., Chodorow, M. 1998. Combining local context and WordNet similarity for word sense identification. In: Fellbaum, C., 1998, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, 265?283. Mitkov R. and Ha L.A. 2003. Computer-aided generation of multiple-choice tests. Proceedings of the HLT/NAACL 2003 Workshop on Building educational applications using Natural Language Processing. Edmonton, Canada, 17-22. Mitkov, R., An, L.A. and Karamanis, N. 2006. "A computer-aided environment for generating multiple-choice test items". Journal of Natural Language Engineering, 12 (2): 177-194. Patwardhan, S, Banerjee, S. and Pedersen, T. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico City, 241-257. Resnik, P. 1995. Using information content to evaluate semantic similarity in a taxonomy. Proceedings of the 14th International Joint Conference on Artificial Intelligence. Montreal, 448?453. Tapanainen, P. and J?rvinen, T. 1997. A non-projective dependency parser. Proceedings of the 5th Conference of Applied Natural Language Processing, Washington, 64-71.  Vajda, E.J. 2001 Course Materials from the module of Introduction to Linguistics. Professor Edward J. Vajda Homepage, Washington, Western Washington University, Modern and Classical Languages.http://pandora.cii.wwu.edu/vajda/ling201/ling201home.htm 
56
