Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 49?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Natural Language Processing at the School of Information Studies for Africa
Bjo?rn Gamba?ck
Userware Laboratory
Swedish Institute of Computer Science
Box 1263, SE?164 29 Kista, Sweden
gamback@sics.se
Gunnar Eriksson
Department of Linguistics
Stockholm University
SE?106 91 Stockholm, Sweden
gunnar@ling.su.se
Athanassia Fourla
Swedish Program for ICT in Developing Regions
Royal Institute of Technology/KTH
Forum 100, SE?164 40 Kista, Sweden
afourla@dsv.su.se
Abstract
The lack of persons trained in computa-
tional linguistic methods is a severe obsta-
cle to making the Internet and computers
accessible to people all over the world in
their own languages. The paper discusses
the experiences of designing and teach-
ing an introductory course in Natural Lan-
guage Processing to graduate computer
science students at Addis Ababa Univer-
sity, Ethiopia, in order to initiate the ed-
ucation of computational linguists in the
Horn of Africa region.
1 Introduction
The development of tools and methods for language
processing has so far concentrated on a fairly small
number of languages and mainly on the ones used
in the industrial part of the world. However, there
is a potentially even larger need for investigating the
application of computational linguistic methods to
the languages of the developing countries: the num-
ber of computer and Internet users of these coun-
tries is growing, while most people do not speak the
European and East-Asian languages that the com-
putational linguistic community has so far mainly
concentrated on. Thus there is an obvious need to
develop a wide range of applications in vernacular
languages, such as translation systems, spelling and
grammar checkers, speech synthesis and recogni-
tion, information retrieval and filtering, and so forth.
But who will develop those systems? A prerequisite
to the creation of NLP applications is the education
and training of computer professionals skilled in lo-
calisation and development of language processing
resources. To this end, the authors were invited to
conduct a course in Natural Language Processing at
the School of Information Studies for Africa, Addis
Ababa University, Ethiopia. As far as we know, this
was the first computational linguistics course given
in Ethiopia and in the entire Horn of Africa region.
There are several obstacles to progress in lan-
guage processing for new languages. Firstly, the par-
ticulars of a language itself might force new strate-
gies to be developed. Secondly, the lack of already
available language processing resources and tools
creates a vicious circle: having resources makes pro-
ducing resources easier, but not having resources
makes the creation and testing of new ones more dif-
ficult and time-consuming.
Thirdly, there is often a disturbing lack of interest
(and understanding) of the needs of people to be able
to use their own language in computer applications
? a lack of interest in the surrounding world, but
also sometimes even in the countries where a lan-
guage is used (?Aren?t those languages going to be
extinct in 50?100 years anyhow?? and ?Our com-
pany language is English? are common comments).
And finally, we have the problem that the course
described in this paper mainly tries to address, the
lack of skilled professionals and researchers with
knowledge both of language processing techniques
and of the domestic language(s) in question.
49
The rest of the paper is laid out as follows: The next
section discusses the language situation in Ethiopia
and some of the challenges facing those trying to in-
troduce NLP methods in the country. Section 3 gives
the background of the students and the university,
before Section 4 goes into the effects these factors
had on the way the course was designed.
The sections thereafter describe the actual course
content, with Section 5 being devoted to the lectures
of the first half of the course, on general linguistics
and word level processing; Section 6 is on the sec-
ond set of lectures, on higher level processing and
applications; while Section 7 is on the hands-on ex-
ercises we developed. The evaluation of the course
and of the students? performance is the topic of Sec-
tion 8, and Section 9 sums up the experiences and
novelties of the course and the effects it has so far
had on introducing NLP in Ethiopia.
2 Languages and NLP in Ethiopia
Ethiopia was the only African country that managed
to avoid being colonised during the big European
power struggles over the continent during the 19th
century. While the languages of the former colonial
powers dominate the higher educational system and
government in many other countries, it would thus
be reasonable to assume that Ethiopia would have
been using a vernacular language for these purposes.
However, this is not the case. After the removal
of the Dergue junta, the Constitution of 1994 di-
vided Ethiopia into nine fairly independent regions,
each with its own ?nationality language?, but with
Amharic being the language for countrywide com-
munication. Until 1994, Amharic was also the prin-
cipal language of literature and medium of instruc-
tion in primary and secondary schools, but higher
education in Ethiopia has all the time been carried
out in English (Bloor and Tamrat, 1996).
The reason for adopting English as the Lingua
Franca of higher education is primarily the linguis-
tic diversity of the country (and partially also an ef-
fect of the fact that British troops liberated Ethiopia
from a brief Italian occupation during the Second
World War). With some 70 million inhabitants,
Ethiopia is the third most populous African country
and harbours more than 80 different languages ?
exactly how many languages there are in a country
is as much a political as a linguistic issue; the count
of languages of Ethiopia and Eritrea together thus
differs from 70 up to 420, depending on the source;
with, for example, the Ethnologue (Gordon, 2005)
listing 89 different ones.
Half-a-dozen languages have more than 1 million
speakers in Ethiopia; three of these are dominant:
the language with most speakers today is probably
Oromo, a Cushitic language spoken in the south and
central parts of the country and written using the
Latin alphabet. However, Oromo has not reached
the same political status as the two large Semitic
languages Tigrinya and Amharic. Tigrinya is spo-
ken in Northern Ethiopia and is the official lan-
guage of neighbouring Eritrea; Amharic is spoken
in most parts of the country, but predominantly in
the Eastern, Western, and Central regions. Oromo
and Amharic are probably two of the five largest lan-
guages on the continent; however, with the dramatic
population size changes in many African coun-
tries in recent years, this is difficult to determine:
Amharic is estimated to be the mother tongue of
more than 17 million people, with at least an addi-
tional 5 million second language speakers.
As Semitic languages, Amharic and Tigrinya are
distantly related to Arabic and Hebrew; the two lan-
guages themselves are probably about as close as
are Spanish and Portuguese (Bloor, 1995). Speak-
ers of Amharic and Tigrinya are mainly Orthodox
Christians and the languages draw common roots
to the ecclesiastic Ge?ez still used by the Coptic
Church. Both languages use the Ge?ez (Ethiopic)
script, written horizontally and left-to-right (in con-
trast to many other Semitic languages). Written
Ge?ez can be traced back to at least the 4th century
A.D. The first versions of the script included con-
sonants only, while the characters in later versions
represent consonant-vowel pairs. Modern Amharic
words have consonantal roots with vowel variation
expressing difference in interpretation.
Several computer fonts have been developed for
the Ethiopic script, but for many years the languages
had no standardised computer representation. An
international standard for the script was agreed on
only in year 1998 and later incorporated into Uni-
code, but nationally there are still about 30 differ-
ent ?standards? for the script, making localisation of
language processing systems and digital resources
50
difficult; and even though much digital information
is now being produced in Ethiopia, no deep-rooted
culture of information exchange and dissemination
has been established. In addition to the digital di-
vide, several other factors have contributed to this
situation, including lack of library facilities and cen-
tral resource sites, inadequate resources for digital
production of journals and books, and poor docu-
mentation and archive collections. The difficulties
of accessing information have led to low expecta-
tions and consequently under-utilisation of existing
information resources (Furzey, 1996).
UNESCO (2001) classifies Ethiopia among the
countries with ?moribund or seriously endangered
tongues?. However, the dominating languages of
the country are not under immediate threat, and seri-
ous efforts have been made in the last years to build
and maintain linguistic resources in Amharic: a lot
of work has been carried out mainly by Ethiopian
Telecom, Ethiopian Science and Technology Com-
mission and Addis Ababa University, as well as by
Ethiopian students abroad, in particular in Germany,
Sweden and the United States. Except for some ini-
tial efforts for the related Tigrinya, work on other
Ethiopian languages has so far been scarce or non-
existent ? see Alemu et al (2003) or Eyassu and
Gamba?ck (2005) for short overviews of the efforts
that have been made to date to develop language pro-
cessing tools for Amharic.
One of the reasons for fostering research in lan-
guage processing in Ethiopia was that the exper-
tise of a pool of researchers in the country would
contribute to maintaining those Ethiopian languages
that are in danger of extinction today. Starting
with Amharic and developing a robust linguistic re-
source base in the country, together with including
the Amharic language in modern language process-
ing tools could create the critical mass of experience,
which is necessary in order to expand to other ver-
nacular languages, too.
Moreover, the development of those conditions
that lay the foundations for language and speech
processing research and development in the country
would prevent potential brain drain from Ethiopia;
instead of most language processing work being
done by Ethiopian students abroad (at present), in
the future it could be done by students, researchers
and professionals inside the country itself.
3 Infrastructure and Student Body
Addis Ababa University (AAU) is Ethiopia?s old-
est, largest and most prestigious university. The De-
partment of Information Science (formerly School
of Information Studies for Africa) at the Faculty of
Informatics conducts a two-year Master?s Program.
The students admitted to the program come from
all over the country and have fairly diverse back-
grounds. All have a four-year undergraduate degree,
but not necessarily in any computer science-related
subject. However, most of the students have been
working with computers for some time after their
under-graduate studies. Those admitted to the pro-
gram are mostly among the top students of Ethiopia,
but some places are reserved for public employees.
The initiative of organising a language process-
ing course as part of the Master?s Program came
from the students themselves: several students ex-
pressed interest in writing theses on speech and lan-
guage subjects, but the faculty acknowledged that
there was a severe lack of staff qulified to teach the
course. In fact, all of the university is under-staffed,
while admittance to the different graduate programs
has been growing at an enormous speed; by 400%
only in the last two years. There was already an
ICT support program in effect between AAU and
SAREC, the Department for Research Cooperation
at the Swedish International Development Coopera-
tion Agency. This cooperation was used to establish
contacts with Stockholm University and the Swedish
Institute of Computer Science, that both had experi-
ence in developing computational linguistic courses.
Information Science is a modern department with
contemporary technology. It has two computer labs
with PCs having Internet access and lecture rooms
with all necessary aids. A library supports the teach-
ing work and is accessible both to students and staff.
The only technical problems encountered arose from
the frequent power failures in the country that cre-
ated difficulties in teaching and/or loss of data. In-
ternet access in the region is also often slow and un-
reliable. However, as a result of the SAREC ICT
support program, AAU is equipped with both an in-
ternal network and with broadband connection to the
outside world. The central computer facilities are
protected from power failures by generators, but the
individual departments have no such back-up.
51
4 Course Design
The main aim of the course plan was to introduce
the students successfully to the main subjects of lan-
guage and speech processing and trigger their inter-
est in further investigation. Several factors were im-
portant when choosing the course materials and de-
ciding on the content and order of the lectures and
exercises, in particular the fact that the students did
not have a solid background in either Computer Sci-
ence or Linguistics, and the time limitations as the
course could only last for ten weeks. As a result, a
curriculum with a holistic view of NLP was built in
the form of a ?crash course? (with many lectures and
labs per week, often having to use Saturdays too)
aiming at giving as much knowledge as possible in
a very short time.
The course was designed before the team travelled
to Ethiopia, but was fine-tuned in the field based on
the day-by-day experience and interaction with the
students: even though the lecturers had some knowl-
edge of the background and competence of the stu-
dents, they obviously would have to be flexible and
able to adjust the course set-up, paying attension
both to the specific background knowledge of the
students and to the students? particular interests and
expectations on the course.
From the outset, it was clear that, for example,
very high programming skills could not be taken for
granted, as given that this is not in itself a require-
ment for being admitted to the Master?s Program.
On the other hand, it was also clear that some such
knowledge could be expected, this course would be
the last of the program, just before the students were
to start working on their theses; and several labora-
tory exercises were developed to give the students
hands-on NLP experience.
Coming to a department as external lecturers is
also in general tricky and makes it more difficult to
know what actual student skill level to expect. The
lecturer team had quite extensive previous experi-
ences of giving external courses this way (in Sweden
and Finland) and thus knew that ?the home depart-
ment? often tends to over-estimate the knowledge of
their students; another good reason for trying to be
as flexible as possible in the course design. and for
listening carefully to the feedback from the students
during the course.
The need for flexibility was, however, somewhat
counter-acted by the long geographical distance and
time constraints. It was necessary to give the course
in about two months time only, and with one of the
lecturers present during the first half of the course
and the other two during the second half, with some
overlap in the middle. Thus the course was split into
two main parts, the first concentrating on general lin-
guistic issues, morphology and lexicology, and the
second on syntax, semantics and application areas.
The choice of reading was influenced by the need
not to assume very elaborated student programming
skills. This ruled out books based mainly on pro-
gramming exercises, such as Pereira and Shieber
(1987) and Gazdar and Mellish (1989), and it was
decided to use Jurafsky and Martin (2000) as the
main text of the course. The extensive web page
provided by those authors was also a factor, since it
could not be assumed that the students would have
full-time access to the actual course book itself. The
costs of buying a regular computer science book is
normally too high for the average Ethiopian student.
To partially ease the financial burden on the stu-
dents, we brought some copies of the book with us
and made those available at the department library.
We also tried to make sure that as much as possible
of the course material was available on the web. In
addition to the course book we used articles on spe-
cific lecture topics particularly material on Amharic,
for which we also created a web page devoted to on-
line Amharic resources and publications.
The following sections briefly describe the differ-
ent parts of the course and the laboratory exercises.
The course web page contains the complete course
materials including the slides from the lectures and
the resources and programs used for the exercises:
www.sics.se/humle/ile/kurser/Addis
5 Linguistics and word level processing
The aim of the first part of the course was to give the
students a brief introduction to Linguistics and hu-
man languages, and to introduce common methods
to access, manipulate, and analyse language data at
the word and phrase levels. In total, this part con-
sisted of seven lectures that were accompanied by
three hands-on exercises in the computer laboratory.
52
5.1 Languages: particularities and structure
The first two lectures presented the concept of a
human language. The lectures focused around five
questions: What is language? What is the ecolog-
ical situation of the world?s languages and of the
main languages of Ethiopia? What differences are
there between languages? What makes spoken and
written modalities of language different? How are
human languages built up?
The second lecture concluded with a discussion of
what information you would need to build a certain
NLP application for a language such as Amharic.
5.2 Phonology and writing systems
Phonology and writing systems were addressed in
a lecture focusing on the differences between writ-
ing systems. The SERA standard for transliterating
Ethiopic script into Latin characters was presented.
These problems were also discussed in a lab class.
5.3 Morphology
After a presentation of general morphological con-
cepts, the students were given an introduction to
the morphology of Amharic. As a means of hand-
ling morphology, regular languages/expressions and
finite-state methods were presented and their limi-
tations when processing non-agglutinative morphol-
ogy were discussed. The corresponding lab exercise
aimed at describing Amharic noun morphology us-
ing regular expressions.
In all, the areas of phonology and morphology
were allotted two lectures and about five lab classes.
5.4 Words, phrases and POS-tagging
Under this heading the students were acquainted
with word level phenomena during two lectures. To-
kenisation problems were discussed and the concept
of dependency relations introduced. This led on
to the introduction of the phrase-level and N-gram
models of syntax. As examples of applications us-
ing this kind of knowledge, different types of part-
of-speech taggers using local syntactic information
were discussed. The corresponding lab exercise,
spanning four lab classes, aimed at building N-gram
models for use in such a system.
The last lecture of the first part of the course
addressed lexical semantics with a quick glance at
word sense ambiguation and information retrieval.
6 Applications and higher level processing
The second part of the course started with an
overview lecture on natural language processing
systems and finished off by a final feedback lecture,
in which the course and the exam were summarised
and students could give overall feedback on the total
course contents and requirements.
The overview lecture addressed the topic of what
makes up present-day language processing systems,
using the metaphor of Douglas Adams? Babel fish
(Adams, 1979): ?What components do we need to
build a language processing system performing the
tasks of the Babel fish?? ? to translate unrestricted
speech in one language to another language ? with
Gamba?ck (1999) as additional reading material.
In all, the second course part consisted of nine
regular lectures, two laboratory exercises, and the
final evaluation lecture.
6.1 Machine Translation
The first main application area introduced was Ma-
chine Translation (MT). The instruction consisted
of two 3-hour lectures during which the following
subjects were presented: definitions and history of
machine translation; different types of MT systems;
paradigms of functional MT systems and translation
memories today; problems, terminology, dictionar-
ies for MT; other kinds of translation aids; a brief
overview of the MT market; MT users, evaluation,
and application of MT systems in real life. Parts of
Arnold et al (1994) complemented the course book.
There was no obligatory assignment in this part
of the course, but the students were able to try out
and experiment with online machine translation sys-
tems. Since there is no MT system for Amharic, they
used their knowledge of other languages (German,
French, English, Italian, etc.) to experience the use
of automatic translation tools.
6.2 Syntax and parsing
Three lectures and one laboratory exercise were de-
voted to parsing and the representation of syntax,
and to some present-day syntactic theories. After in-
troducing basic context-free grammars, Dependency
Grammar was taken as an example of a theory un-
derlying many current shallow processing systems.
Definite Clause Grammar, feature structures, the
53
concept of unification, and subcategorisation were
discussed when moving on to more deeper-level,
unification-based grammars.
In order to give the students an understanding of
the parsing problem, both processing of artificial and
natural languages was discussed, as well as human
language processing, in the view of Kimball (1973).
Several types of parsers were introduced, with in-
creasing complexity: top-down and bottom-up pars-
ing; parsing with well-formed substring tables and
charts; head-first parsing and LR parsing.
6.3 Semantics and discourse
Computational semantics and pragmatics were cov-
ered in two lectures. The first lecture introduced
the basic tools used in current approaches to se-
mantic processing, such as lexicalisation, compo-
sitionality and syntax-driven semantic analysis, to-
gether with different ways of representing meaning:
first-order logic, model-based and lambda-based se-
mantics. Important sources of semantic ambiguity
(quantifiers, for example) were discussed together
with the solutions allowed by using underspecified
semantic representations.
The second lecture continued the semantic repre-
sentation thread by moving on to how a complete
discourse may be displayed in a DRS, a Discourse
Representation Structure, and how this may be used
to solve problems like reference resolution. Dia-
logue and user modelling were introduced, covering
several current conversational systems, with Zue and
Glass (2000) and Wilks and Catizone (2000) as extra
reading material.
6.4 Speech technology
The final lecture before the exam was the only one
devoted to speech technology and spoken language
translation systems. Some problems in current spo-
ken dialogue systems were discussed, while text-to-
speech synthesis and multimodal synthesis were just
briefly touched upon. The bulk of the lecture con-
cerned automatic speech recognition: the parts and
architectures of state-of-the-art speech recognition
systems, Bayes? rule, acoustic modeling, language
modeling, and search strategies, such as Viterbi and
A-star were introduced, as well as attempts to build
recognition systems based on hybrids between Hid-
den Markov Models and Artificial Neural Networks.
7 Laboratory Exercises
Even though we knew before the course that the stu-
dents? actual programming skills were not extensive,
we firmly believe that the best way to learn Compu-
tational Linguistics is by hands-on experience. Thus
a substantial part of the course was devoted to a set
of laboratory exercises, which made up almost half
of the overall grade on the course.
Each exercise was designed so that there was an
(almost obligatory) short introductory lecture on the
topic and the requirements of the exercise, followed
by several opportunities for the students to work on
the exercise in the computer lab under supervision
from the lecturer. To pass, the students both had
to show a working system solving the set problem
and hand in a written solution/explanation. Students
were allowed to work together on solving the prob-
lem, while the textual part had to be handed in by
each student individually, for grading purposes.
7.1 Labs 1?3: Word level processing
The laboratory exercises during the first half of the
course were intended to give the students hands-
on experience of simple language processing using
standard UNIX tools and simple Perl scripts. The
platform was cygwin,1 a freeware UNIX-like envi-
ronment for Windows. The first two labs focused
on regular expressions and the exercises included
searching using ?grep?, simple text preprocessing us-
ing ?sed?, and building a (rather simplistic) model
of Amharic noun morphology using regular expres-
sions in (template) Perl scripts. The third lab exer-
cise was devoted to the construction of probabilis-
tic N-gram data from text corpora. Again standard
UNIX tools were used.
Due to the students? lack of experience with this
type of computer processing, more time than ex-
pected was spent on acquainting them with the
UNIX environment during the first lab excercises.
7.2 Labs 4?5: Higher level processing
The practical exercises during the second half of
the course consisted of a demo and trial of on-line
machine translation systems, and two obligatory as-
signments, on grammars and parsing and on seman-
tics and discourse, respectively. Both these exercises
1www.cygwin.com
54
consisted of two parts and were carried out in the
(freeware) SWI-Prolog framework.2
In the first part of the fourth lab exercise, the
students were to familiarise themselves with basic
grammars by trying out and testing parsing with a
small context-free grammar. The assignments then
consisted in extending this grammar both to add cov-
erage and to restrict it (to stop ?leakage?). The
second part of the lab was related to parsing. The
students received parsers encoding several different
strategies: top-down, bottom-up, well-formed sub-
string tables, head parsing, and link parsing (a link
parser improves a bottom-up parser in a similar way
as a WFST parser improves a top-down parser, by
saving partial parses). The assignments included
creating a test corpus for the parsers, running the
parsers on the corpus, and trying to determine which
of the parsers gave the best performance (and why).
The assignments of the fifth lab were on lambda-
based semantics and the problems arising in a gram-
mar when considering left-recursion and ambiguity.
The lab also had a pure demo part where the students
tried out Johan Bos? ?Discourse Oriented Represen-
tation and Inference System?, DORIS.3
8 Course Evaluation and Grading
The students were encouraged from the beginning
to interact with the lecturers and to give feedback
on teaching and evaluation issues. With the aim of
coming up with the best possible assessment strat-
egy ? in line with suggestions in work reviewed by
Elwood and Klenowski (2002), three meetings with
the students took place at the beginning, the middle,
and end of the course. In these meetings, students
and lecturers together discussed the assessment cri-
teria, the form of the exam, the percentage of the
grade that each part of the exam would bear, and
some examples of possible questions.
This effort to better reflect the objectives of the
course resulted in the following form of evaluation:
the five exercises of the previous section were given,
with the first one carrying 5% of the total course
grade, the other four 10% each, and an additional
written exam (consisting of thirteen questions from
the whole curriculum taught) 55%.
2www.swi-prolog.org
3www.cogsci.ed.ac.uk/?jbos/doris
While correcting the exams, the lecturers tried to
bear in mind that this was the first acquaintance of
the students with NLP. Given the restrictions on the
course, the results were quite positive, as none of the
students taking the exam failed the course. After the
marking of the exams an assessment meeting with
all the students and the lecturers was held, during
which each question of the exam was explained to-
gether with the right answer. The evaluation of the
group did not present particular problems. For grad-
ing, the American system was used according to the
standards of Addis Ababa University (i.e., with the
grades ?A+?, ?A?, ..., ?F?).
9 Results
Except for the contents of the course, the main inno-
vation for the Information Science students was that
the bulk of the course reading list and relevant ma-
terials were available online. The students were able
to access the materials according to their own needs
? in terms of time schedule ? and download and
print it without having to go to the library to copy
books and papers.
Another feature of the on-line availability was that
after the end of the course and as the teaching team
left the country, the supervision of the students? the-
ses was carried out exclusively through the Internet
by e-mail and chat. The final papers with the signa-
tures of the supervisors were even sent electronically
to the department. The main difficulty that had to be
overcome concerned the actual writing of the theses;
the students were not very experienced in producing
academic text and required some distance training,
through comments and suggestions, on the subject.
The main results of the course were that, based
strictly on the course aims, students were success-
fully familiarised with the notion of NLP. This also
led to eight students choosing to write their Mas-
ter theses on speech and language issues: two on
speech technology, on text-to-speech synthesis for
Tigrinya and on speech recognition for Amharic;
three on Amharic information access, on informa-
tion filtering, on information retrieval and text cat-
egorisation, and on automatic text summarisation;
one on customisation of a prototype English-to-
Amharic transfer-based machine translation system;
one on predictive SMS (Short Message Service) text
55
input for Amharic; and one on Amharic part-of-
speech tagging. Most of these were supervised from
Stockholm by the NLP course teaching team, with
support from the teaching staff in Addis Ababa.
As a short-term effect, several scientific papers
were generated by the Master theses efforts. As
a more lasting effect, a previously fairly unknown
field was not only tapped, but also triggered the stu-
dents? interest for further research. Another impor-
tant result was the strengthening of the connections
between Ethiopian and Swedish academia, with on-
going collaboration and supervision, also of students
from later batches. Still, the most important long-
term effect may have been indirect: triggered by the
success of the course, the Addis Ababa Faculty of
Informatics in the spring of 2005 decided to estab-
lish a professorship in Natural Language Processing.
10 Acknowledgments
Thanks to the staff and students at the Department
of Information Science, Addis Ababa University,
in particular Gashaw Kebede, Kinfe Tadesse, Saba
Amsalu, and Mesfin Getachew; and to Lars Asker
and Atelach Alemu at Stockholm University.
This NLP course was funded by the Faculty of
Informatics at Addis Ababa University and the ICT
support program of SAREC, the Department for
Research Cooperation at Sida, the Swedish Inter-
national Development Cooperation Agency.
References
Douglas Adams. 1979. The Hitch-Hiker?s Guide to the
Galaxy. Pan Books, London, England.
Atelach Alemu, Lars Asker, and Mesfin Getachew. 2003.
Natural language processing for Amharic: Overview
and suggestions for a way forward. In Proceedings
of the 10th Conference on Traitement Automatique des
Langues Naturelles, volume 2, pages 173?182, Batz-
sur-Mer, France, June.
Douglas Arnold, Lorna Balkan, Siety Meijer, R. Lee
Humphreys, and Louisa Sadler. 1994. Machine Trans-
lation: An Introductory Guide. Blackwells-NCC,
London, England.
Thomas Bloor and Wondwosen Tamrat. 1996. Issues
in Ethiopian language policy and education. Jour-
nal of Multilingual and Multicultural Development,
17(5):321?337.
Thomas Bloor. 1995. The Ethiopic writing system: a
profile. Journal of the Simplified Spelling Society,
19:30?36.
Jannette Elwood and Val Klenowski. 2002. Creating
communities of shared practice: the challenges of as-
sessment use in learning and teaching. Assessment &
Evaluation in Higher Education, 27(3):243?256.
Samuel Eyassu and Bjo?rn Gamba?ck. 2005. Classifying
Amharic news text using Self-Organizing Maps. In
ACL 2005 Workshop on Computational Approaches to
Semitic Languages, Ann Arbor, Michigan, June. ACL.
Jane Furzey. 1996. Enpowering socio-economic devel-
opment in Africa utilizing information technology. A
country study for the United Nations Economic Com-
mission for Africa (UNECA), African Studies Center,
University of Pennsylvania.
Bjo?rn Gamba?ck. 1999. Human language technology:
The Babel fish. Technical Report T99-09, SICS,
Stockholm, Sweden, November.
Gerald Gazdar and Chris Mellish. 1989. Natural Lan-
guage Processing in Prolog. Addison-Wesley, Wok-
ingham, England.
Raymond G. Gordon, Jr, editor. 2005. Ethnologue: Lan-
guages of the World. SIL International, Dallas, Texas,
15 edition.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall, Upper Saddle
River, New Jersey.
John Kimball. 1973. Seven principles of surface
structure parsing in natural languages. Cognition,
2(1):15?47.
Fernando C. N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. Number 10
in Lecture Notes. CSLI, Stanford, California.
Yorick Wilks and Roberta Catizone. 2000. Human-
computer conversation. In Encyclopedia of Microcom-
puters. Dekker, New York, New York.
Stephen Wurm, editor. 2001. Atlas of the World?s Lan-
guages in Danger of Disappearing. The United Na-
tions Educational, Scientific and Cultural Organization
(UNESCO), Paris, France, 2 edition.
Victor Zue and James Glass. 2000. Conversational inter-
faces: Advances and challenges. Proceedings of the
IEEE, 88(8):1166?1180.
56
   	
	 	  	  	 	
  	
 

 	
 	  


 
 	 	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 296?299,
Prague, June 2007. c?2007 Association for Computational Linguistics
SICS: Valence annotation based on seeds in word space
Magnus Sahlgren
SICS
Box 1263
SE-164 29 Kista
Sweden
mange@sics.se
Jussi Karlgren
SICS
Box 1263
SE-164 29 Kista
Sweden
jussi@sics.se
Gunnar Eriksson
SICS
Box 1263
SE-164 29 Kista
Sweden
guer@sics.se
Abstract
This paper reports on a experiment to iden-
tify the emotional loading (the ?valence?)
of news headlines. The experiment re-
ported is based on a resource-thrifty ap-
proach for valence annotation based on a
word-space model and a set of seed words.
The model was trained on newsprint, and va-
lence was computed using proximity to one
of two manually defined points in a high-
dimensional word space ? one represent-
ing positive valence, the other representing
negative valence. By projecting each head-
line into this space, choosing as valence the
similarity score to the point that was closer
to the headline, the experiment provided re-
sults with high recall of negative or positive
headlines. These results show that working
without a high-coverage lexicon is a viable
approach to content analysis of textual data.
1 The Semeval task
This a report of an experiment proposed as the ?Af-
fective Text? task of the 4th international Work-
shop on Semantic Evaluation (SemEval) to deter-
mine whether news headlines are loaded with pre-
eminently positive or negative emotion or valence.
An example of a test headline can be:
DISCOVERED BOYS BRING SHOCK, JOY
2 Working without a lexicon
Our approach takes as its starting point the obser-
vation that lexical resources always are noisy, out
of date, and most often suffer simultaneously from
being both too specific and too general. For our ex-
periments, our only lexical resource consists of a list
of eight positive words and eight negative words, as
shown below in Table 1. We use a medium-sized
corpus of general newsprint to build a general word
space, and use our minimal lexical resource to orient
ourselves in it.
3 Word space
A word space is a high-dimensional vector space
built from distributional statistics (Schu?tze, 1993;
Sahlgren, 2006), in which each word in the vocab-
ulary is represented as a context vector
 
 of occur-
rence frequencies:
 
	
 where  is the
frequency of word  in some context  .
The point of this representation is that seman-
tic similarity between words can be computed us-
ing vector similarity measures. Thus, the similar-
ity in meaning between the words  and ffProceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84?91,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Uncertainty Detection as Approximate Max-Margin Sequence Labelling
Oscar Ta?ckstro?m
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Gunnar Eriksson
SICS
Kista, Sweden
guer@sics.se
Sumithra Velupillai
DSV, Stockholm University
Kista, Sweden
sumithra@dsv.su.se
Hercules Dalianis
DSV, Stockholm University
Kista, Sweden
hercules@dsv.su.se
Martin Hassel
DSV, Stockholm University
Kista, Sweden
xmartin@dsv.su.se
Jussi Karlgren
SICS
Kista, Sweden
jussi@sics.se
Abstract
This paper reports experiments for the
CoNLL-2010 shared task on learning to
detect hedges and their scope in natu-
ral language text. We have addressed
the experimental tasks as supervised lin-
ear maximum margin prediction prob-
lems. For sentence level hedge detection
in the biological domain we use an L1-
regularised binary support vector machine,
while for sentence level weasel detection
in the Wikipedia domain, we use an L2-
regularised approach. We model the in-
sentence uncertainty cue and scope de-
tection task as an L2-regularised approxi-
mate maximum margin sequence labelling
problem, using the BIO-encoding. In ad-
dition to surface level features, we use a
variety of linguistic features based on a
functional dependency analysis. A greedy
forward selection strategy is used in ex-
ploring the large set of potential features.
Our official results for Task 1 for the bio-
logical domain are 85.2 F1-score, for the
Wikipedia set 55.4 F1-score. For Task 2,
our official results are 2.1 for the entire
task with a score of 62.5 for cue detec-
tion. After resolving errors and final bugs,
our final results are for Task 1, biologi-
cal: 86.0, Wikipedia: 58.2; Task 2, scopes:
39.6 and cues: 78.5.
1 Introduction
This paper reports experiments to detect uncer-
tainty in text. The experiments are part of the two
shared tasks given by CoNLL-2010 (Farkas et al,
2010). The first task is to identify uncertain sen-
tences; the second task is to detect the cue phrase
which makes the sentence uncertain and to mark
its scope or span in the sentence.
Uncertainty as a target category needs to be ad-
dressed with some care. Sentences, utterances,
statements are not uncertain ? their producer, the
speaker or author, is. Statements may explicitly
indicate this uncertainty, employing several differ-
ent linguistic and textual mechanisms to encode
the speaker?s attitude with respect to the verac-
ity of an utterance. The absence of such markers
does not necessarily indicate certainty ? the oppo-
sition between certain and uncertain is not clearly
demarkable, but more of a dimensional measure.
Uncertainty on the part of the speaker may be dif-
ficult to differentiate from a certain assessment of
an uncertain situation, It is unclear whether this
specimen is an X or a Y vs. The difference between
X and Y is unclear.
In this task, the basis for identifying uncertainty
in utterances is almost entirely lexical. Hedges,
the main target of this experiment, are an estab-
lished category in lexical grammar analyses - see
e.g. Quirk et al (1985), for examples of English
language constructions. Most languages use vari-
ous verbal markers or modifiers for indicating the
speaker?s beliefs in what is being said, most proto-
typically using conditional or optative verb forms,
Six Parisiens seraient morts, or auxiliaries, This
mushroom may be edible, but aspectual markers
may also be recruited for this purpose, more indi-
rectly, I?m hoping you will help vs. I hope you will
help; Do you want to see me now vs. Did you want
to see me now. Besides verbs, there are classes
of terms that through their presence, typically in
an adverbial role, in an utterance make explicit
its tentativeness: possibly, perhaps... and more
complex constructions with some reservation, es-
pecially such that explicitly mention the speaker
and the speaker?s beliefs or doubts, I suspect that
X.
Weasels, the other target of this experiment,
on the other hand, do not indicate uncertainty.
84
Weasels are employed when speakers attempt to
convince the listener of something they most likely
are certain of themselves, by anchoring the truth-
fulness of the utterance to some outside fact or au-
thority (Most linguists believe in the existence of
an autonomous linguistic processing component),
but where the authority in question is so unspecific
as not to be verifiable when scrutinised.
We address both CoNLL-2010 shared tasks
(Farkas et al, 2010). The first, detecting uncer-
tain information on a sentence level, we solve by
using an L1-regularised support vector machine
with hinge loss for the biological domain, and
an L2-regularised maximum margin model for the
Wikipedia domain. The second task, resolution of
in-sentence scopes of hedge cues, we approach as
an approximate L2-regularized maximum margin
structured prediction problem. Our official results
for Task 1 for the biological domain are 85.2 F1-
score, for the Wikipedia set 55.4 F1-score. For
Task 2, our official results were 2.1 for the entire
task with a score of 62.5 for cue detection. After
resolving errors and unfortunate bugs, our final re-
sults are for Task 1, biological: 86.0, Wikipedia:
58.2; Task 2: 39.6 and 78.5 for cues.
2 Detecting Sentence Level Uncertainty
On the sentence level, word- and lemma-based
features have been shown to be useful for uncer-
tainty detection (see e.g. Light et al (2004), Med-
lock and Briscoe (2007), Medlock (2008), and
Szarvas (2008)). Medlock (2008) and Szarvas
(2008) employ probabilistic, weakly supervised
methods, where in the former, a stemmed single
term and bigram representation achieved best re-
sults (0.82 BEP), and in the latter, a more complex
n-gram feature selection procedure was applied
using a Maximum Entropy classifier, achieving
best results when adding reliable keywords from
an external hedge keyword dictionary (0.85 BEP,
85.08 F1-score on biomedical articles). More lin-
guistically motivated features are used by Kil-
icoglu and Bergler (2008), such as negated ?un-
hedging? verbs and nouns and that preceded by
epistemic verbs and nouns. On the fruit-fly dataset
(Medlock and Briscoe, 2007) they achieve 0.85
BEP, and on the BMC dataset (Szarvas, 2008) they
achieve 0.82 BEP. Light et al (2004) also found
that most of the uncertain sentences appeared to-
wards the end of the abstract, indicating that the
position of an uncertain sentence might be a use-
ful feature.
Ganter and Strube (2009) consider weasel tags
in Wikipedia articles as hedge cues, and achieve
results of 0.70 BEP using word- and distance
based features on a test set automatically derived
from Wikipedia, and 0.69 BEP on a manually an-
notated test set using syntactic patterns as fea-
tures. These results suggest that syntactic features
are useful for identifying weasels that ought to be
tagged. However, evaluation is performed on bal-
anced test sets, which gives a higher baseline.
2.1 Learning and Optimization Framework
A guiding principle in our approach to this shared
task has been to focus on highly computationally
efficient models, both in terms of training and pre-
diction times. Although kernel based non-linear
separators may sometimes obtain better predic-
tion performance, compared to linear models, the
speed penalty at prediction time is often substan-
tial, since the number of support patterns often
grows linearly with the size of the training set. We
therefore restrict ourselves to linear models, but
allow for a restricted family of explicit non-linear
mappings by feature combinations.
For sentence level hedge detection in the bio-
logical domain, we employ an L1-regularised sup-
port vector machine with hinge loss, as provided
by the library implemented by Fan et al (2008),
while for weasel detection in the Wikipedia do-
main, we instead use the L2-regularised maximum
margin model described in more detail in section
3.1. In both cases, we approximately optimise the
F1-measure by weighting each class by the inverse
of its proportion in the training data.
The reason for using L1-regularisation in the bi-
ological domain is that the annotation is heavily
biased towards a rather small number of lexical
cues, making most of the potential surface features
irrelevant. The Wikipedia weasel annotation, on
the other hand, is much more noisy and less de-
termined by specific lexical markers. Regularising
with respect to the L1-norm is known to give pref-
erence to sparse models and for the special case
of logistic regression, Ng (2004) proved that the
sample complexity grows only logarithmically in
the number of irrelevant features, instead of lin-
early as when regularising with respect to the L2-
norm. Our preliminary experiments indicated that
L1-regularisation is superior to L2-regularisation
in the biological domain, while slightly inferior in
85
the Wikipedia domain.
2.2 Feature Definitions
The asymmetric relationship between certain and
uncertain sentences becomes evident when one
tries to learn this distinction based on surface level
cues. While the UNCERTAIN category is to a large
extent explicitly anchored in lexical markers, the
CERTAIN category is more or less defined implic-
itly as the complement of the UNCERTAIN cate-
gory. To handle this situation, we use a bias fea-
ture to model the weight of the CERTAIN category,
while explicit features are used to model the UN-
CERTAIN category.
The following list describes the feature tem-
plates explored for sentence level uncertainty de-
tection. Some features are based on a linguistic
analysis by the Connexor Functional Dependency
(FDG) parser (Tapanainen and Ja?rvinen, 1997).
SENLEN Preliminary experiments indicated that taking sen-
tence length into account is beneficial. We incorporate
this by using three different bias terms, according to the
length (in tokens) of the sentences. This feature takes
the following values: S < 18 ? M ? 32 < L.
DOCPT Document part, e.g., TITLE, ABSTRACT and BODY
TEXT, allowing for different models for different docu-
ment parts.
TOKEN, LEMMA Tokens in most cases equals words, but
may in some special cases also be multiword units, e.g.
of course, as defined by the FDG tokenisation. Lemmas
are base forms of words, with some special features
introduced for numeric tokens, e.g., year, short number,
and long number.
QUANT Syntactic function of a noun phrase with a quanti-
fier head (at least some of the isoforms are conserved
between mouse and humans), or a modifying quantifier
(Recently, many investigators have been interested in
the study on eosinophil biology).
HEAD, DEPREL Functional dependency head of the token,
and the type of dependency relation between the head
and the token, respectively.
SYN Phrase-level and clause-level syntactic functions of a
word.
MORPH Part-of-speech and morphological traits of a word.
Each feature template defines a set of features
when applied to data. The TOKEN, LEMMA,
QUANT, HEAD, DEPREL templates yield single-
ton sets of features for each token, while the SYN
and MORPH templates extends to sets consisting
of several features for each token. A sentence is
represented as the union of all active token level
features and the SENLEN and DOCPT, if active.
In addition to the linear combination of concrete
features, we allow combined features by the Carte-
sian product of the feature set extensions of two or
more feature templates.
2.3 Feature Template Selection
Although regularised maximum margin models
often cope well even in the presence of irrelevant
features, it is a good idea to search the large set of
potential features for an optimal subset.
In order to make this search feasible we make
two simplifications. First, we do not explore the
full set of individual features, but instead the set of
feature templates, as defined above. Second, we
perform a greedy search in which we iteratively
add the feature template that gives the largest per-
formance improvement, when added to the cur-
rent optimal set of templates. The performance of
a feature set for sentence level detection is mea-
sured as the mean F1-score, with respect to the
UNCERTAIN class, minus one standard deviation
? the mean and standard deviation are computed
by three fold cross-validation on the training set.
We subtract one standard deviation from the mean
in order to promote stable solutions over unstable
ones.
Of course, these simplifications do not come for
free. The solution of the optimisation problem
might be quite unstable with respect to the optimal
hyper-parameters of the learning algorithm, which
in turn may depend on the feature set used. This
risk could be reduced by conducting a more thor-
ough parameter search for each candidate feature
set, however, this was simply too time consuming
for the present work. A further risk of using for-
ward selection is that feature interactions are ig-
nored. This issue is handled better with backward
elimination, but that is also more time consuming.
The full set of explored feature templates is too
large to be listed here; instead we list the features
selected in each iteration of the search, together
with their corresponding scores, in Table 1.
3 Detecting In-sentence Uncertainty
When it comes to the automatic identification of
hedge cues and their linguistic scopes, Morante
and Daelemans (2009) and O?zgu?r and Radev
(2009) report experiments on the BioScope cor-
pus (Vincze et al, 2008), achieving best results
(10-fold cross evaluation) on the identification of
hedge cues of 71.59 F-score (using IGTree with
current, preceding and subsequent word and cur-
86
Task Template set Dev F1 Test F1
Bio
SENLEN - -
? LEMMA 88.9 (.25) 78.79
? LEMMABI 90.3 (.19) 85.86
? LEMMA?QUANT 90.3 (.07) 85.97
Wiki
SENLEN - -
? TOKEN?DOCPT 59.0 (.76) 60.12
? TOKENBI?SENLEN 59.9 (.09) 58.26
Table 1: Top feature templates for sentence level
hedge and weasel detection.
rent lemma as features) and 82.82 F-score (using a
Support Vector Machine classifier and a complex
feature set including keyword and dependency re-
lation information), respectively. On the task of
automatic scope resolution, best results are re-
ported as 59.66 (F-score) and 61.13 (accuracy),
respectively, on the full paper subset. O?zgu?r and
Radev (2009) use a rule-based method for this sub-
task, while Morante and Daelemans (2009) use
three different classifiers as input to a CRF-based
meta-learner, with a complex set of features, in-
cluding hedge cue information, current and sur-
rounding token information, distance information
and location information.
3.1 Learning and Optimisation Framework
In recent years, a wide range of different ap-
proaches to general structured prediction prob-
lems, of which sequence labelling is a special
case, have been suggested. Among others, Con-
ditional Random Fields (Lafferty et al, 2001),
Max-Margin Markov Networks (Taskar et al,
2003), and Structured Support Vector Machines
(Tsochantaridis et al, 2005). A drawback of
these approaches is that they are all quite com-
putationally demanding. As an alternative, we
propose a much more computationally lenient ap-
proach based on the regularised margin-rescaling
formulation of Taskar et al (2003), which we in-
stead optimise by stochastic subgradient descent
as suggested by Ratliff et al (2007). In addi-
tion we only perform approximate decoding, us-
ing beam search, which allows arbitrary complex
joint feature maps to be employed, without sacri-
ficing speed.
3.1.1 Technical Details
Let X denote the pattern set and let Y denote the
set of structured labels. Let A denote the set of
atomic labels and let each label y ? Y consist of
an indexed sequence of atomic labels yi ? A. De-
note by Yx ? Y the set of possible label assign-
ments to pattern x ? X and by yx ? Yx its cor-
rect label. In the specific case of BIO-sequence
labelling, A = {BEGIN, INSIDE, OUTSIDE} and
Yx = A|x|, where |x| is the length of the sequence
x ? X .
A structured classification problem amounts
to learning a mapping from patterns to labels,
f : X 7? Y , such that the expected loss
EX?Y [?(yx, f(x))] is minimised. The prediction
loss, ? : Y ? Y 7? <+, measures the loss of
predicting label y = f(x) when the correct la-
bel is yx, with ?(yx, yx) = 0. Here we assume
the Hamming loss, ?H(y, y?) = ?|y|i=1 ?(yi, y?i),
where ?(yi, y?i) = 1 if yi 6= y?i and 0 otherwise.
The idea of the margin-rescaling approach is to
let the structured margin between the correct label
yx and a hypothesis y ? Yx scale linearly with the
prediction loss ?(yx, y) (Taskar et al, 2003). The
structured margin is defined in terms of a score
function S : X ? Y 7? <, in our case the linear
score function S(x, y) = wT?(x, y), where w ?
<m is a vector of parameters and? : X?Y 7? <m
is a joint feature function. The learning problem
then amounts to finding parameters w such that
S(x, yx) ? S(x, y) + ?(yx, y) for all y ? Yx \
{yx} over the training data D. In other words, we
want the score of the correct label to be higher than
the score plus the loss, of all other labels, for each
instance. In order to balance margin maximisation
and margin violation, we add theL2-regularisation
term ?w?2.
By making use of the loss augmented decoding
function
f?(x, yx) = argmax
y?Yx
[S(x, y) + ?(yx, y)] , (1)
we get the following regularised risk functional:
Q?,D(w) =
|D|?
i=1
S?(x(i), yx(i)) + ?2 ?w?
2, (2)
where
S?(x, yx) = maxy?Yx [S(x, y) + ?(yx, y)]?S(x, yx)
(3)
We optimise (2) by stochastic approximate subgra-
dient descent with step size sequence [?0/?t]?t=1
(Ratliff et al, 2007). The initial step size ?0
and the regularisation factor ? are data depen-
dent hyper-parameters, which we tune by cross-
validation.
87
This framework is highly efficient both at learn-
ing and prediction time. Training cues and scopes
on the biological data, takes about a minute, while
prediction times are in the order of seconds, using
a Java based implementation on a standard laptop;
the absolute majority of that time is spent on read-
ing and extracting features from an inefficient in-
ternal JSON-based format.
3.1.2 Hashed Feature Functions
Joint feature functions enable encoding of depen-
dencies between labels and relations between pat-
tern and label. Most feature templates are de-
fined based on input only, while some are de-
fined with respect to output features as well. Let
?(x, y1:i?1, i) ? <m denote the joint feature func-
tion corresponding to the application of all active
feature templates to pattern x ? X and partially
decoded label y1:i?1 ? Ai?1 when decoding at
position i. The feature mapping used in scoring
candidate label yi ? A is then computed as the
Cartesian product ?(x, y, i) = ?(x, y1:i?1, i) ?
?(yi), where ?(yi) ? <m is a unique unitary fea-
ture vector representation of label yi. The feature
representation for a complete sequence x and its
associated label y is then computed as
?(x, y) =
|x|?
i=1
?(x, y, i)
When employing joint feature functions and com-
bined features, the number of unique features may
grow very large. This is a problem when the
amount of internal memory is limited. Feature
hashing, as described by Weinberger et al (2009),
is a simple trick to circumvent this problem. As-
sume that we have an original feature function
? : X ? Y 7? <m, where m might be arbitrar-
ily large. Let h : N+ 7? [1, n] be a hash function
and let h?1(i) ? [1,m] be the set of integers such
that j ? h?1(i) iff h(j) = i. We now use this
hash function to map the index of each feature in
?(x, y) to its corresponding index in ?(x, y), as
?i(x, y) =?j?h?1(i) ?j(x, y). The features in ?
are thus unions of multisets of features in ?. Given
a hash function with good collision properties, we
can expect that the subset of features mapped to
any index in?(x, y) is small and composed of ele-
ments drawn at random from ?(x, y). Weinberger
et al (2009) contains proofs of bounds on these
distributions. Furthermore, by using a k-valued
hash function h : Nk 7? [1, n], the Cartesian prod-
uct of k feature sets can be computed much more
efficiently, compared to using a dictionary.
3.2 Position Based Feature Definitions
For in-sentence cue and scope prediction we make
use of the same token level feature templates as
for sentence level detection. An additional level
of expressivity is added in that each token level
template is associated with a token position. A
template is addressed either relative to the token
currently being decoded, or by the dependency arc
of a token, which in turn is addressed by a relative
position. The addressing can be either to a single
position, or a range of positions. Feature templates
may further be defined with respect to features of
the input pattern, the token level labels predicted
so far, or with respect to combinations of input
and label features. Joint features, just as complex
feature combinations, are created by forming the
Cartesian product of an input feature set and a la-
bel feature set.
The feature templates are instantiated by pre-
fixing the template name to each member of the
feature set. To exemplify, the single position tem-
plate TOKENi, given that the token currently be-
ing decoded at position i is suggests, is instanti-
ated as the singleton set {TOKENi = suggests}.
The range template TOKENi,i+1, given that the
current token is suggests and the next token is
that, is instantiated as the set {TOKENi,i+1 =
suggests, TOKENi,i+1 = that}; i.e. each member
of the set is prefixed by the range template name.
In addition to the token level templates used for
sentence level prediction, the following templates
were explored:
LABEL Label predicted so far at the addressed position(s).
HEAD.X An arbitrary feature, X, addressed by follow-
ing the dependency arc(s) from the addressed posi-
tion(s). For example, HEAD.LEMMAi corresponds to
the lemma found by looking at the dependency head of
the current token.
CUE, CUESCOPE Whether the token(s) addressed is re-
spectively, a cue marker, or within the syntactic scope
of the current cue, following the definition of scope
provided by Vincze et al (2008).
3.3 Feature Template Selection
Just as with sentence level detection, we used a
greedy forward selection strategy when searching
for the optimal subset of feature templates. The
cue and scope detection subtasks were optimised
separately.
88
The scoring measures used in the search for
cue and scope detection features differ. In order
to match the official scoring measure for cue de-
tection, we optimise the F1-score of labels cor-
responding to cue tags, i.e. we treat the BEGIN
and INSIDE cue tags as an equivalence class. The
official scoring measure for scope prediction, on
the other hand, corresponds to the exact match
of scope boundaries. Unfortunately using exact
match performance turned out to be not very well
suited for use in greedy forward selection. This
is because before a sufficient per token accuracy
has been reached, and even when it has, the ex-
act match score may fluctuate wildly. Therefore,
as a substitute, we instead guide the search by to-
ken level accuracy. This discrepancy between the
search criterion and the official scoring metric is
unfortunate.
Again, when taking into account position ad-
dressing, joint features and combined features, the
complete set of explored templates is too large to
fit in the current experiment. The selected features
together with their corresponding scores are found
in Table 2.
Task Template set Dev F1 Test F1
Cue
TOKENi 74.0 (1.5) -
? TOKENi?1 81.0 (.30) 68.78
? MORPHi 83.6 (.10) 74.06
? LEMMAi ? LEMMAi+1 85.6 (.20) 78.41
? SYNi 86.5 (.41) 78.28
? LEMMAi?1 ? LEMMAi 86.7 (.42) 78.52
Scope
CueScopei 66.9 (.92) -
? LABELi?2,i?1 79.5 (.67) 34.80
? LEMMAi 82.4 (1.1) 33.18
? MORPHi 83.1 (.35) 35.70
? CUEi?2,i?1 83.4 (.13) 40.14
? CUEi,i+1,i+2 83.6 (.11) 41.15
? LEMMAi?1 84.1 (.16) 40.04
? MORPHi 84.4 (.33) 40.04
? TOKENi+1 84.5 (.09) 39.64
Table 2: Top feature templates for in-sentence de-
tection of hedge cues and scopes.
4 Discussion
Our final F1-score results for the corrected system
are, in Task 1 for the biological domain 85.97, for
the Wikipedia domain 58.25; for Task 2, our re-
sults are 39.64 for the entire task with a score of
78.52 for cue detection.
Any gold standard-based shared experiment un-
avoidably invites discussion on the reliability of
the gold standard. It is easy to find borderline ex-
amples in the evaluation corpus, e.g. sentences
that may just as well be labeled ?certain? rather
than ?uncertain?. This gives an indication of the
true complexity of assessing the hidden variable of
uncertainty and coercing it to a binary judgment
rather than a dimensional one. It is unlikely that
everyone will agree on a binary judgment every
time.
To improve experimental results and the gen-
eralisability of the results for the task of detect-
ing uncertain information on a sentence level, we
would need to break reliance on the purely lexical
cues. For instance, we now have identified possi-
ble and putative as markers for uncertainty, but in
many instances they are not (Finally, we wish to
ensure that others can use and evaluate the GREC
as simply as possible). This would be avoidable
through either a deeper analysis of the sentence
to note that possible in this case does not modify
anything of substance in the sentence, or alterna-
tively through a multi-word term preprocessor to
identify as simply as possible as an analysis unit.
In the Wikipedia experiment, where the objec-
tive is to identify weasel phrases, the judicious en-
coding of quantifiers such as ?some of the most
well-known researchers say that X? would be
likely to identify the sought-for sentences when
the quantified NP is in subject position. In our
experiment we find that our dependency analysis
did not distinguish between the various syntactic
roles of quantified NPs. As a result, we marked
several sentences with a quantifier as a ?weasel?
sentence, even where the quantified NP was in a
non-subject role ? leading to overly many weasel
sentences. An example is given in Table 3.
If certainty can be identified separately, not as
absence of overt uncertainty, identifying uncer-
tainty can potentially be aided through the iden-
tification of explicit certainty together with nega-
tion, as found by Kilicoglu and Bergler (2008). In
keeping with their results, we found negations in a
sizeable proportion of the annotated training mate-
rial. Currently we capture negation as a lexical cue
in immediate bigrams, but with longer range nega-
tions, we will miss some clear cases: Table 3 gives
two examples. To avoid these misses, we will both
need to identify overt expressions of certainty and
to identify and track the scope of negation ? the
first challenge is unexplored but would not seem
to be overly complex; the second is a well-known
89
and established challenge for NLP systems in gen-
eral.
In the task of detecting in-sentence uncertainty
? identification of hedge cues and their scopes ?
we find that an evaluation method based on ex-
act match of a token sequence is overly unforgiv-
ing. There are many cases where the marginal to-
kens of a sequence are less than central or irrele-
vant for the understanding of the hedge cue and its
scope: moving the boundary by one position over
an uninteresting token may completely invalidate
an otherwise arguably correct analysis. A token-
by-token scoring would be a more functional eval-
uation criterion, or perhaps a fuzzy match, allow-
ing for a certain amount of erroneous characters.
For our experiments, this has posed some chal-
lenges. While we model the in-sentence un-
certainty detection as a sequence labelling prob-
lem in the BIO-representation (BEGIN, INSIDE,
OUTSIDE), the provided corpus uses an XML-
representation. Moreover, the official scoring tool
requires that the predictions are well formed XML,
necessitating a conversion from XML to BIO prior
to training and from BIO to XML after prediction.
Consistent tokenisation is important, but the syn-
tactic analysis components used by us distorted the
original tokenisation and restoring the exact same
token sequence proved problematic.
Conversion from BIO to XML is straightforward
for cues, while some care must be taken when an-
notating scopes, since erroneous scope predictions
may result in malformed XML. When adding the
scope annotation, we use a stack based algorithm.
For each sentence, we simultaneously traverse the
scope-sequence corresponding to each cue, left to
right, token by token. The stack is used to en-
sure that scopes are either separated or nested and
an additional restriction ensures that scopes may
never start or end inside a cue. In case the al-
gorithm fails to place a scope according to these
restrictions, we fall back and let the scope cover
the whole sentence. Several of the more frequent
errors in our analyses are scoping errors, many
likely to do with the fallback solution. Our analy-
sis quite frequently fails also to assign the subject
of a sentence to the scope of a hedging verb. Ta-
ble 3 shows one example each of these errors ?
overextended scope and missing subject.
Unfortunately, the tokenisation output by our
analysis components is not always consistent with
the tokenisation assumed by the BioScope annota-
tion. A post-processing step was therefore added
in which each, possibly complex, token in the pre-
dicted BIO-sequence is heuristically mapped to its
corresponding position in the XML structure. This
post-processing is not perfect and scopes and cues
at non-word token boundaries, such as parenthe-
ses, are quite often misplaced with respect to the
BioScope annotation. Table 3 gives one example
which is scored ?erroneous? since the token ?(63)?
is in scope, where the ?correct? solution has it out-
side the scope. These errors are not important to
address, but are quite frequent in our results ? ap-
proximately 80 errors are of this type.
To achieve more general and effective methods
to detect uncertainty in an argument, we should
note that uncertainty is signalled in a text through
many mechanisms, and that the purely lexical and
explicit signal found through the present experi-
ments in hedge identification is effective and use-
ful, but will not catch everything we might want to
find. Lexical approaches are also domain depen-
dent. For instance, Szarvas (2008) and Morante
and Daelemans (2009) report loss in performance,
when applying the same methods developed on bi-
ological data, on clinical text. Using the systems
developed for scientific text elsewhere poses a mi-
gration challenge. It would be desirable both to
automatically learn a hedging lexicon from a gen-
eral seed set and to have features on a higher level
of abstraction.
Our main result is that casting this task as a se-
quence labelling problem affords us the possibility
to combine linguistic analyses with a highly effi-
cient implementation of a max-margin prediction
algorithm. Our framework processes the data sets
in minutes for training and seconds for prediction
on a standard personal computer.
5 Acknowledgements
The authors would like to thank Joakim Nivre
for feedback in earlier stages of this work. This
work was funded by The Swedish National Grad-
uate School of Language Technology and by the
Swedish Research Council.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learn-
ing Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
90
Neg + certain However, how IFN-? and IL-4 inhibit IL-17 production is not yet known.
Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.
?some?: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj .
?some?: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.
Prediction dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR? because
dRas85DV12 can activate endogenous PI3K signaling [16]</xcope>.
Gold standard dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?</xcope> because
dRas85DV12 can activate endogenous PI3K signaling [16].
Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR?t
<xcope .1>are still <cue .1>unclear</cue></xcope>.
Gold standard However, <xcope .1>the precise molecular mechanisms of Stat3-mediated expression of ROR?t
are still <cue .1>unclear</cue></xcope>.
Prediction Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit ROR?t
activity on its target genes, at least in par,t through direct interaction with ROR?t (63)</xcope>.
Gold standard Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit RORt
activity on its target genes, at least in par,t through direct interaction with RORt</xcope> (63).
Table 3: Examples of erroneous analyses.
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope
in Natural Language Text. In Proceedings of the 14th
Conference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 1?12, Uppsala,
Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding hedges
by chasing weasels: hedge detection using Wikipedia tags
and shallow linguistic features. In ACL-IJCNLP ?09: Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a linguis-
tically motivated perspective. BMC Bioinformatics, 9.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. In
Proc. 18th Int. Conf. on Machine Learning. Morgan Kauf-
mann Publishers.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In Lynette Hirschman and James
Pustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontologies
and Databases, Boston, USA. ACL.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Informatics,
41(4):636?654.
Roser Morante and Walter Daelemans. 2009. Learning the
scope of hedge cues in biomedical texts. In BioNLP ?09:
Proceedings of Workshop on BioNLP, Morristown, NJ,
USA. ACL.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regulariza-
tion, and rotational invariance. In ICML ?04: Proceedings
of the 21st International Conference on Machine learning,
page 78, New York, NY, USA. ACM.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detecting
speculations and their scopes in scientific text. In Pro-
ceedings of 2009 Conference on Empirical Methods in
Natural Language Processing, Singapore. ACL.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A comprehensive grammar of the
English language. Longman.
Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-
vich. 2007. (Online) subgradient methods for structured
prediction. In Eleventh International Conference on Arti-
ficial Intelligence and Statistics (AIStats).
Gyo?rgy Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Jour-
nal of Machine Learning Research, 6:1453?1484.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas, Gyo?rgy
Mo?ra, and Ja?nos Csirik. 2008. The BioScope corpus:
biomedical texts annotated for uncertainty, negation and
their scopes. BMC Bioinformatics, 9(S-11).
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML ?09: Proceedings
of the 26th Annual International Conference on Machine
Learning, New York, NY, USA. ACM.
91
