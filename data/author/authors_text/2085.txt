Named Entity Recognition in Bengali: A Conditional Random Field
Approach
Asif Ekbal
Department of CSE
Jadavpur University
Kolkata-700032, India
asif.ekbal@gmail.com
Rejwanul Haque
Department of CSE
Jadavpur University
Kolkata-700032, India
rejwanul@gmail.com
Sivaji Bandyopadhyay
Department of CSE
Jadavpur University
Kolkata-700032, India
sivaji cse ju@yahoo.com
Abstract
This paper reports about the development of
a Named Entity Recognition (NER) system
for Bengali using the statistical Conditional
Random Fields (CRFs). The system makes
use of the different contextual information
of the words along with the variety of fea-
tures that are helpful in predicting the var-
ious named entity (NE) classes. A portion
of the partially NE tagged Bengali news cor-
pus, developed from the archive of a lead-
ing Bengali newspaper available in the web,
has been used to develop the system. The
training set consists of 150K words and has
been manually annotated with a NE tagset
of seventeen tags. Experimental results of
the 10-fold cross validation test show the ef-
fectiveness of the proposed CRF based NER
system with an overall average Recall, Pre-
cision and F-Score values of 93.8%, 87.8%
and 90.7%, respectively.
1 Introduction
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Process-
ing (NLP) application areas. Proper identifica-
tion and classification of named entities (NEs) are
very crucial and pose a very big challenge to the
NLP researchers. The level of ambiguity in NER
makes it difficult to attain human performance.
NER has applications in several domains includ-
ing information extraction, information retrieval,
question-answering, automatic summarization, ma-
chine translation etc.
The current trend in NER is to use the machine-
learning approach, which is more attractive in that
it is trainable and adoptable and the maintenance
of a machine-learning system is much cheaper than
that of a rule-based one. The representative ma-
chine-learning approaches used in NER are Hid-
den Markov Model (HMM) (BBN?s IdentiFinder
in (Bikel et al, 1999)), Maximum Entropy (New
York University?s MENE in (Borthwick, 1999)) and
Conditional Random Fields (CRFs) (Lafferty et al,
2001; McCallum and Li, 2003).
There is no concept of capitalization in Indian
languages (ILs) like English and this fact makes
the NER task more difficult and challenging in ILs.
There has been very little work in the area of NER
in ILs. In Indian languages particularly in Ben-
gali, the work in NER can be found in (Ekbal and
Bandyopadhyay, 2007a; Ekbal and Bandyopadhyay,
2007b) with pattern directed shallow parsing ap-
proach and in (Ekbal et al, 2007c) with HMM.
Other than Bengali, a CRF based NER system can
be found in (Li and McCallum, 2004) for Hindi.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are used to calculate the conditional proba-
bility of values on designated output nodes given
values on other designated input nodes. The con-
ditional probability of a state sequence S =<
s1, s2, . . . , sT > given an observation sequence
O =< o1, o2, . . . , oT > is calculated as:
P?(s|o) =
1
Z0
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
589
where, fk(st?1, st, o, t) is a feature function whose
weight ?k, is to be learned via training. The val-
ues of the feature functions may range between
??, . . .+?, but typically they are binary. To make
all conditional probabilities sum up to 1, we must
calculate the normalization factor,
Z0 =
?
s
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
which as in HMMs, can be obtained efficiently by
dynamic programming.
To train a CRF, the objective function to be maxi-
mized is the penalized log-likelihood of the state se-
quences given the observation sequences:
L? =
N
?
i=1
log(P?(s(i)|o(i))) ?
?
k
?2k
2?2 ,
where {< o(i), s(i) >} is the labeled training data.
The second sum corresponds to a zero-mean, ?2
-variance Gaussian prior over parameters, which
facilitates optimization by making the likelihood
surface strictly convex. Here, we set parameters
? to maximize the penalized log-likelihood using
Limited-memory BFGS (Sha and Pereira, 2003), a
quasi-Newton method that is significantly more ef-
ficient, and which results in only minor changes in
accuracy due to changes in ?.
When applying CRFs to the NER problem, an ob-
servation sequence is a token of a sentence or docu-
ment of text and the state sequence is its correspond-
ing label sequence. While CRFs generally can use
real-valued functions, in our experiments maximum
of the features are binary valued. A feature function
fk(st?1, st, o, t) has a value of 0 for most cases and
is only set to be 1, when st?1, st are certain states
and the observation has certain properties. We have
used the C++ based OpenNLP CRF++ package 1.
3 Named Entity Recognition in Bengali
Bengali is one of the widely used languages all over
the world. It is the seventh popular language in the
world, second in India and the national language of
Bangladesh. A partially NE tagged Bengali news
corpus (Ekbal and Bandyopadhyay, 2007d), devel-
oped from the archive of a widely read Bengali news
1http://crfpp.sourceforge.net
paper available in the web, has been used in this
work to identify and classify NEs. The corpus con-
tains around 34 million word forms in ISCII (Indian
Script Code for Information Interchange) and UTF-
8 format. The location, reporter, agency and differ-
ent date tags (date, ed, bd, day) in the partially NE
tagged corpus help to identify some of the location,
person, organization and miscellaneous names, re-
spectively, that appear in some fixed places of the
newspaper. These tags cannot detect the NEs within
the actual news body. The date information obtained
from the news corpus provides example of miscella-
neous names. A portion of this partially NE tagged
corpus has been manually annotated with the seven-
teen tags as described in Table 1.
NE tag Meaning Example
PER Single-word sachin/ PER
person name
LOC Single-word jadavpur/LOC
location name
ORG Single-word infosys/ ORG
organization name
MISC Single-word 100%/ MISC
miscellaneous name
B-PER Beginning, Internal sachin/B-PER
I-PER or End of a multiword ramesh/I-PER
E-PER person name tendulkar/E-PER
B-LOC Beginning, Internal or mahatma/B-LOC
I-LOC End of a multiword gandhi/I-LOC
E-LOC location name road/E-LOC
B-ORG Beginning, Internal or bhaba/B-ORG
I-ORG End of a multiword atomic/I-ORG
E-ORG organization name research/I-ORG
center/E-ORG
B-MISC Beginning, Internal or 10e/B-MISC
I-MISC End of a multiword magh/ I-MISC
E-MISC miscellaneous name 1402/E-MISC
NNE Words that are not NEs neta/NNE
Table 1: Named Entity Tagset
3.1 Named Entity Tagset
A CRF based NER system has been developed
in this work to identify NEs in Bengali and clas-
sify them into the predefined four major categories,
namely, ?Person name?, ?Location name?, ?Organi-
zation name? and ?Miscellaneous name?. In order to
590
properly denote the boundaries of NEs and to apply
CRF in NER task, sixteen NE and one non-NE tags
have been defined as shown in Table 1. In the out-
put, sixteen NE tags are replaced appropriately with
the four major NE tags by some simple heuristics.
3.2 Named Entity Features
Feature selection plays a crucial role in CRF frame-
work. Experiments were carried out to find out the
most suitable features for NER in Bengali. The
main features for the NER task have been iden-
tified based on the different possible combination
of available word and tag context. The features
also include prefix and suffix for all words. The
term prefix/suffix is a sequence of first/last few
characters of a word, which may not be a lin-
guistically meaningful prefix/suffix. The use of
prefix/suffix information works well for highly in-
flected languages like the Indian languages. In
addition, various gazetteer lists have been devel-
oped for use in the NER task. We have consid-
ered different combination from the following set
for inspecting the best feature set for NER task:
F={wi?m, . . . , wi?1, wi, wi+1, . . . wi+n, |prefix| ?
n, |suffix| ? n, previous NE tag, POS tags, First
word, Digit information, Gazetteer lists}.
Following are the details of the set of features that
were applied to the NER task:
? Context word feature: Previous and next words of
a particular word might be used as a feature.
? Word suffix: Word suffix information is helpful
to identify NEs. This feature can be used in two
different ways. The first and the nai?ve one is, a
fixed length word suffix of the current and/or the sur-
rounding word(s) might be treated as feature. The
second and the more helpful approach is to modify
the feature as binary valued. Variable length suf-
fixes of a word can be matched with predefined lists
of useful suffixes for different classes of NEs. The
different suffixes that may be particularly helpful in
detecting person (e.g., -babu, -da, -di etc.) and lo-
cation names (e.g., -land, -pur, -lia etc.) have been
considered also. Here, both types of suffixes have
been used.
? Word prefix: Prefix information of a word is also
helpful. A fixed length prefix of the current and/or
the surrounding word(s) might be treated as features.
? Part of Speech (POS) Information: The POS of
the current and/or the surrounding word(s) can be
used as features. Multiple POS information of the
words can be a feature but it has not been used in the
present work. The alternative and the better way is
to use a coarse-grained POS tagger.
Here, we have used a CRF-based POS tagger,
which was originally developed with the help of 26
different POS tags2, defined for Indian languages.
For NER, we have considered a coarse-grained POS
tagger that has only the following POS tags:
NNC (Compound common noun), NN (Com-
mon noun), NNPC (Compound proper noun), NNP
(Proper noun), PREP (Postpositions), QFNUM
(Number quantifier) and Other (Other than the
above).
The POS tagger is further modified with two
POS tags (Nominal and Other) for incorporating
the nominal POS information. Now, a binary val-
ued feature ?nominalPOS? is defined as: If the cur-
rent/previous/next word is ?Nominal? then the ?nom-
inalPOS? feature of the corresponding word is set to
1; otherwise, it is set to 0. This ?nominalPOS? fea-
ture has been used additionally with the 7-tag POS
feature. Sometimes, postpositions play an important
role in NER as postpositions occur very frequently
after a NE. A binary valued feature ?nominalPREP?
is defined as: If the current word is nominal and the
next word is PREP then the feature ?nomianlPREP?
of the current word is set to 1, otherwise set to 0.
? Named Entity Information: The NE tag of the pre-
vious word is also considered as the feature. This is
the only dynamic feature in the experiment.
? First word: If the current token is the first word of
a sentence, then the feature ?FirstWord? is set to 1.
Otherwise, it is set to 0.
? Digit features: Several binary digit features
have been considered depending upon the presence
and/or the number of digits in a token (e.g., Con-
tainsDigit [token contains digits], FourDigit [token
consists of four digits], TwoDigit [token consists
of two digits]), combination of digits and punctu-
ation symbols (e.g., ContainsDigitAndComma [to-
ken consists of digits and comma], ConatainsDigi-
tAndPeriod [token consists of digits and periods]),
combination of digits and symbols (e.g., Contains-
DigitAndSlash [token consists of digit and slash],
2http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
591
ContainsDigitAndHyphen [token consists of digits
and hyphen], ContainsDigitAndPercentage [token
consists of digits and percentages]). These binary
valued features are helpful in recognizing miscella-
neous NEs such as time expressions, monetary ex-
pressions, date expressions, percentages, numerical
numbers etc.
? Gazetteer Lists: Various gazetteer lists have been
developed from the partially NE tagged Bengali
news corpus (Ekbal and Bandyopadhyay, 2007d).
These lists have been used as the binary valued fea-
tures of the CRF. If the current token is in a particu-
lar list then the corresponding feature is set to 1 for
the current and/or the surrounding word(s); other-
wise, set to 0. The following is the list of gazetteers:
(i) Organization suffix word (94 entries): This list
contains the words that are helpful in identifying or-
ganization names (e.g., kong, limited etc). The fea-
ture ?OrganizationSuffix? is set to 1 for the current
and the previous words.
(ii) Person prefix word (245 entries): This is useful
for detecting person names (e.g., sriman, sree, sri-
mati etc.). The feature ?PersonPrefix? is set to 1 for
the current and the next two words.
(iii) Middle name (1,491 entries): These words gen-
erally appear inside the person names (e.g., chandra,
nath etc.). The feature ?MiddleName? is set to 1 for
the current, previous and the next words.
(iv) Surname (5,288 entries): These words usually
appear at the end of person names as their parts. The
feature ?SurName? is set to 1 for the current word.
(v) Common location word (547 entries): This list
contains the words that are part of location names
and appear at the end (e.g., sarani, road, lane etc.).
The feature ?CommonLocation? is set to 1 for the
current word.
(vi) Action verb (221 entries): A set of action verbs
like balen, ballen, ballo, shunllo, haslo etc. often
determines the presence of person names. The fea-
ture ?ActionVerb? is set to 1 for the previous word.
(vii) Frequent word (31,000 entries): A list of most
frequently occurring words in the Bengali news cor-
pus has been prepared using a part of the corpus.
The feature ?RareWord? is set to 1 for those words
that are not in this list.
(viii) Function words (743 entries): A list of func-
tion words has been prepared manually. The feature
?NonFunctionWord? is set to 1 for those words that
are not in this list.
(ix) Designation words (947 entries): A list of com-
mon designation words has been prepared. This
helps to identify the position of the NEs, partic-
ularly person names (e.g., neta, sangsad, kheloar
etc.). The feature ?DesignationWord? is set to 1 for
the next word.
(x) Person name (72, 206 entries): This list contains
the first name of person names. The feature ?Person-
Name? is set to 1 for the current word.
(xi) Location name (7,870 entries): This list contains
the location names and the feature ?LocationName?
is set to 1 for the current word.
(xii) Organization name (2,225 entries): This list
contains the organization names and the feature ?Or-
ganizationName? is set to 1 for the current word.
(xiii) Month name (24 entries): This contains the
name of all the twelve different months of both En-
glish and Bengali calendars. The feature ?Month-
Name? is set to 1 for the current word.
(xiv) Weekdays (14 entries): It contains the name of
seven weekdays in Bengali and English both. The
feature ?WeekDay? is set to 1 for the current word.
4 Experimental Results
A partially NE tagged Bengali news corpus (Ekbal
and Bandyopadhyay, 2007d) has been used to cre-
ate the training set for the NER experiment. Out of
34 million wordforms, a set of 150K wordforms has
been manually annotated with the 17 tags as shown
in Table 1 with the help of Sanchay Editor 3, a text
editor for Indian languages. Around 20K NE tagged
corpus has been selected as the development set and
the rest 130K wordforms has been used as the train-
ing set of the CRF based NER system.
We define the baseline model as the one where
the NE tag probabilities depend only on the cur-
rent word: P (t1, t2, . . . , tn|w1, w2, . . . , wn) =
?
i=1,...,n P (ti, wi).
In this model, each word in the test data will be
assigned the NE tag which occurred most frequently
for that word in the training data. The unknown
word is assigned the NE tag with the help of vari-
ous gazetteers and NE suffix lists.
Ninety-five different experiments were conducted
taking the different combinations from the set ?F? to
3Sourceforge.net/project/nlp-sanchay
592
Feature (word, tag) FS
(in %)
pw, cw, nw, FirstWord 71.31
pw2, pw, cw, nw, nw2, FirstWord 72.23
pw3, pw2, pw, cw, nw, nw2, nw3, 71.12
FirstWord
pw2, pw, cw, nw, nw2, FirstWord, pt 74.91
pw2, pw, cw, nw, nw2, FirstWord, pt, 77.61
|pre| ? 4, |suf| ? 4
pw2, pw, cw, nw, nw2, FirstWord, pt, 79.70
|suf| ? 3, |pre| ? 3
pw2, pw, cw, nw, nw2, FirstWord, pt, 81.50
|suf| ? 3, |pre| ? 3, Digit features
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.60
|suf| ? 3, |pre| ? 3, Digit features, pp,
cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 82.20
|suf| ? 3, |pre| ? 3, Digit features,
pp2, pp, cp, np, np2
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.10
|suf| ? 3, |pre| ? 3, Digit features, pp, cp
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.70
|suf| ? 3, |pre| ? 3, Digit features, cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 89.30
|suf| ? 3,|pre| ? 3, Digit features, pp,
cp, np, nominalPOS, nominalPREP,
Gazetteer lists
Table 2: Results on Development Set
identify the best suited set of features for the NER
task. From our empirical analysis, we found that the
following combination gives the best result with 744
iterations:
F=[wi?2, wi?1, wi, wi+1, wi+2, |prefix| ? 3,
|sufix| ? 3, NE information of the previous word,
POS information of the window three, nominalPOS
of the current word, nominalPREP, FirstWord, Digit
features, Gazetteer lists].
The meanings of the notations, used in experi-
mental results, are defined as below:
cw, pw, nw: Current, previous and next word; pwi,
nwi: Previous and the next ith word from the current
word; pre, suf: Prefix and suffix of the current word;
pt: NE tag of the previous word; cp, pp, np: POS tag
of the current, previous and the next word; ppi, npi:
POS tag of the previous and the next ith word.
Evaluation results of the system for the develop-
ment set in terms of overall F-Score (FS) are pre-
sented in Table 2. It is observed from Table 2 that
word window [?2,+2] gives the best result with
?FirstWord? feature only and the further increase of
the window size reduces the overall F-Score value.
Results of Table 2 (3rd and 5th rows) show that
the inclusion of NE information of the previous
word increases the overall F-Score by 2.68%. It is
also indicative from the evaluation results that the
performance of the system can be improved by in-
cluding the prefix and suffix features. Results (6th
and 7th rows) also show the fact that prefix and suf-
fix of length upto three of the current word is more
effective. In another experiment, it has been also ob-
served that the surrounding word suffixes and/or pre-
fixes do not increase the F-Score value. The overall
F-Score value is further improved by 1.8% (7th and
8th rows) with the inclusion of various digit features.
Results (8th and 9th rows) show that POS in-
formation of the words improves the overall F-score
by 2.1%. In the above experiment, the POS tag-
ger was developed with 26 POS tags. Experimen-
tal results (9th, 10th, 11th and 12th rows) suggest
that the POS tags of the previous, current and the
next words, i.e., POS information of the window
[?1,+1] is more effective than POS information of
the window [?2,+2], [?1, 0] or [0,+1]. In another
experiment, we also observed that the POS informa-
tion of the current word alone is less effective than
the window [?1,+1]. The modified POS tagger that
is developed with 7 POS tags increases the overall F-
Score to 85.2%, while other set of features are kept
unchanged. So, it can be decided that smaller POS
tagset is more effective than the larger POS tagset
in NER. We have observed from two separate ex-
periments that the overall F-Score values can further
be improved by 0.4% and 0.2%, respectively, with
the ?nominalPOS? and ?nominalPREP? features. Fi-
nally, an overall F-Score value of 89.3% is obtained
by including the gazetteer lists.
The best set of features is identified by training
the system with 130K wordforms and testing with
the development set of 20K wordforms. Now, the
development set is included as part of the train-
ing set and resultant training set is thus consists of
150K wordforms. The training set has 20,455 per-
son names, 11,668 location names, 963 organization
593
names and 11,554 miscellaneous names. We have
performed 10-fold cross validation test on this train-
ing set. The Recall, Precision and F-Score values
for the 10 different experiments in the 10-fold cross
validation test are presented in Table 3. The over-
all average Recall, Precision and F-Score values are
93.8%, 87.8% and 90.7%, respectively.
The other existing Bengali NER systems along
with the baseline model are also trained and tested
under the same experimental setup. The baseline
model has demonstrated the overall F-Score value of
56.3%. The overall F-Score value of the CRF based
NER system is 90.7%, which is an improvement of
more than 6% over the HMM based system, best re-
ported Bengali NER system (Ekbal et al, 2007c).
The reason behind the rise in overall F-Score value
might be its better capability than HMM to capture
the morphologically rich and overlapping features of
Bengali language. The system has been evaluated
also for the four individual NE classes and it has
shown the average F-Score values of 91.2%, 89.7%,
87.1% and 99.2%, respectively, for person, location,
organization and miscellaneous names.
5 Conclusion
In this paper, we have developed a NER system us-
ing CRF with the help of a partially NE tagged Ben-
gali news corpus, developed from the archive of a
leading Bengali newspaper available in the web. Ex-
perimental results with the 10-fold cross validation
test have shown reasonably good Recall, Precision
and F-Score values. It has been shown that the con-
textual window [-2, +2], prefix and suffix of length
upto three, first word of the sentence, POS informa-
tion of the window [-1, +1], current word, NE infor-
mation of the previous word, different digit features
and the various gazetteer lists are the best-suited fea-
tures for the Bengali NER.
Analyzing the performance using other methods
like MaxEnt and Support Vector Machines (SVMs)
will be other interesting experiments.
References
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, 34(1-3):211?231.
A. Borthwick. 1999. Maximum Entropy Approach to
Test set no. Recall Precision FS (%)
1 92.4 87.3 89.78
2 92.3 87.4 89.78
3 91.4 86.6 88.94
4 95.2 87.7 91.29
5 91.6 86.7 89.08
6 92.2 87.1 89.58
7 94.5 87.9 91.08
8 93.8 89.3 91.49
9 96.9 88.4 92.45
10 97.7 89.6 93.47
Average 93.8 87.8 90.7
Table 3: Results for the 10-fold Cross Validation
Test
Named Entity Recognition. Ph.D. thesis, New York
University.
A. Ekbal and S. Bandyopadhyay. 2007a. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of ICON, pages 123?128, India.
A. Ekbal and S. Bandyopadhyay. 2007b. Pattern Based
Bootstrapping Method for Named Entity Recognition.
In Proceedings of ICAPR, pages 349?355, India.
A. Ekbal and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Recog-
nition. Language Resources and Evaluation Journal
(accepted).
A. Ekbal, S.K. Naskar, and S. Bandyopadhyay. 2007c.
Named Entity Recognition and Transliteration in Ben-
gali. Named Entities: Recognition, Classification
and Use, Special Issue of Lingvisticae Investigationes
Journal, 30(1):95?114.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In ICML, pages 282?289.
Wei Li and Andrew McCallum. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. ACM
TALIP, 2(3):290?294.
A. McCallum and W. Li. 2003. Early results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Web-enhanced Lexicons. In
Proceedings of CoNLL, pages 188?191, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
NAACL ?03, pages 134?141, Canada.
594
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33?40,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
 
Language Independent Named Entity Recognition in  Indian Languages  
Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka 
and Sivaji Bandyopadhyay 
 Department of Computer Science and Engineering 
Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com, rejwanul@gmail.com, 
amit_santu_kuntal@yahoo.com, venkat.ju@gmail.com and 
sivaji_cse_ju@yahoo.com 
            
Abstract 
This paper reports about the development 
of a Named Entity Recognition (NER) sys-
tem for South and South East Asian lan-
guages, particularly for Bengali, Hindi, Te-
lugu, Oriya and Urdu as part of the 
IJCNLP-08 NER Shared Task
1
. We have 
used the statistical Conditional Random 
Fields (CRFs). The system makes use of 
the different contextual information of the 
words along with the variety of features 
that are helpful in predicting the various 
named entity (NE) classes. The system uses 
both the language independent as well as 
language dependent features. The language 
independent features are applicable for all 
the languages. The language dependent 
features have been used for Bengali and 
Hindi only. One of the difficult tasks of 
IJCNLP-08 NER Shared task was to iden-
tify the nested named entities (NEs) though 
only the type of the maximal NEs were 
given. To identify nested NEs, we have 
used rules that are applicable for all the 
five languages. In addition to these rules, 
gazetteer lists have been used for Bengali 
and Hindi. The system has been trained 
with Bengali (122,467 tokens), Hindi 
(502,974 tokens), Telugu (64,026 tokens), 
Oriya (93,173 tokens) and Urdu (35,447 
tokens) data. The system has been tested 
with the 30,505 tokens of Bengali, 38,708 
tokens of Hindi, 6,356 tokens of Telugu, 
                                                
1
http://ltrc.iiit.ac.in/ner-ssea-08  
24,640 tokens of Oriya and 3,782 tokens of 
Urdu. Evaluation results have demonstrated 
the highest maximal F-measure of 53.36%, 
nested F-measure of 53.46% and lexical F-
measure of 59.39% for Bengali.   
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Proc-
essing (NLP) application areas. Proper identifi-
cation and classification of named entities are 
very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
named entity recognition (NER) makes it diffi-
cult to attain human performance.     
NER has drawn more and more attention 
from the named entity (NE) tasks (Chinchor 
95; Chinchor 98) in Message Understanding 
Conferences (MUCs) [MUC6; MUC7]. The 
problem of correct identification of named enti-
ties is specifically addressed and benchmarked 
by the developers of Information Extraction 
System, such as the GATE system (Cunning-
ham, 2001). NER also finds application in 
question-answering systems (Maldovan et al, 
2002) and machine translation (Babych and 
Hartley, 2003).  
The current trend in NER is to use the ma-
chine-learning approach, which is more attrac-
tive in that it is trainable and adoptable and the 
maintenance of a machine-learning system is 
much cheaper than that of a rule-based one. 
The representative machine-learning ap-
proaches used in NER are HMM (BBN?s Iden-
tiFinder in (Bikel, 1999)), Maximum Entropy 
33
(New York University?s MENE in (Borthwick, 
1999)), Decision Tree (New York University?s 
system in (Sekine 1998), SRA?s system in 
(Bennet, 1997) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001; McCallum and 
Li, 2003).       
There is no concept of capitalization in Indian 
languages (ILs) like English and this fact makes 
the NER task more difficult and challenging in 
ILs. There has been very little work in the area of 
NER in Indian languages. In Indian languages par-
ticularly in Bengali, the work in NER can be found 
in (Ekbal and Bandyopadhyay, 2007a) and  (Ekbal 
and Bandyopadhyay, 2007b). These two systems 
are based on the pattern directed shallow parsing 
approach. An HMM-based NER in Bengali can be 
found in (Ekbal et al, 2007c). Other than Bengali, 
the work on NER can be found in (Li and 
McCallum, 2004) for Hindi. This system is based 
on CRF.  
In this paper, we have reported a named entity 
recognition system for the south and south east 
Asian languages, particularly for Bengali, Hindi, 
Telugu, Oriya and Urdu. Bengali is the seventh 
popular language in the world, second in India and 
the national language of Bangladesh. Hindi is the 
third popular language in the world and the na-
tional language of India. Telugu is one of the popu-
lar languages and predominantly spoken in the 
southern part of India. Oriya and Urdu are the 
other two popular languages of India and widely 
used in the eastern and the northern part, respec-
tively. The statistical Conditional Random Field 
(CRF) model has been used to develop the system, 
as it is more efficient than HMM to deal with the 
non-independent and diverse overlapping features 
of the highly inflective Indian languages. We have 
used a fine-grained named entity tagset
2
, defined as 
part of the IJCNLP-08 NER Shared Task for 
SSEA. The system makes use of the different con-
textual information of the words along with the 
variety of orthographic word level features that are 
helpful in predicting the various named entity 
classes. In this work, we have considered language 
independent features as well as the language de-
pendent features. Language independent features 
include the contextual words, prefix and suffix in-
formation of all the words in the training corpus, 
several digit features depending upon the presence 
                                                
 
2
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
and/or the number of digits in a token and the fre-
quency features of the words. The system consid-
ers linguistic features particularly for Bengali and 
Hindi. Linguistic features of Bengali include the 
set of known suffixes that may appear with named 
entities, clue words that help in predicating the lo-
cation and organization names, words that help to 
recognize measurement expressions, designation 
words that help in identifying person names, the 
various gazetteer lists like the first names, middle 
names, last names, location names and organiza-
tion names. As part of linguistic features for Hindi, 
the system uses only the lists of first names, middle 
names and last names along with the list of words 
that helps to recognize measurements. No linguis-
tic features have been considered for Telugu, Oriya 
and Urdu. It has been observed from the evaluation 
results that the use of linguistic features improves 
the performance of the system. A number of ex-
periments have been carried out to find out the 
best-suited set of features for named entity recog-
nition in Bengali, Hindi, Telugu, Oriya and Urdu.  
2 Conditional Random Fields 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) are undirected graphical models, a special 
case of which corresponds to conditionally trained 
probabilistic finite state automata. Being 
conditionally trained, these CRFs can easily 
incorporate a large number of arbitrary, non-
independent features while still having efficient 
procedures for non-greedy finite-state inference 
and training. CRFs have shown success in various 
sequence modeling tasks including noun phrase 
segmentation (Sha and Pereira, 2003) and table 
extraction (Pinto et al, 2003).     
CRFs are used to calculate the conditional 
probability of values on designated output nodes 
given values on other designated input nodes. The 
conditional probability of a state sequence 
1, 2, , TS s s s given an observation 
sequence 1 2,, ....., )TO o o o  is calculated as: 
1 ,
1
1
( | ) exp( ( , , )),
T
k k t t
o
t k
P s o f s s o t
Z
where
1 ,( , , )k t tf s s o t is a feature function whose weight 
k is to be learned via training. The values of the 
feature functions may range between ..... , 
but typically they are binary. To make all 
34
conditional probabilities sum up to 1, we must 
calculate the normalization 
factor, 0 1 ,
1
exp( ( , , ))
T
s k k t t
t k
Z f s s o t , 
which, as in HMMs, can be obtained efficiently by 
dynamic programming. 
To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given observation sequences: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i
k
i k
L P s o , 
where, {
( ) ( )
,
i i
o s } is the labeled training 
data. The second sum corresponds to a zero-mean,  
2
-variance Gaussaian prior over parameters, 
which facilitates optimization by making the like-
lihood surface strictly convex. Here, we set pa-
rameters 
 
to maximize the penalized log-
likelihood using Limited-memory BFGS (Sha and 
Pereira, 2003), a quasi-Newton method that is sig-
nificantly more efficient, and which results in only 
minor changes in accuracy due to changes in .  
When applying CRFs to the named entity 
recognition problem, an obsevation sequence is a 
token of a sentence or document of text and the 
state sequence is its corresponding label sequence. 
While CRFs generally can use real-valued 
functions, in our experiments maximum of the 
features are binary. A feature function 
1 ,( , , )k t tf s s o t has a value of 0 for most cases and 
is only set to be 1, when 1,t ts s are certain states 
and the observation has certain properties. We 
have used the C
++ 
based OpenNLP CRF++ pack-
age
3
, a simple, customizable, and open source im-
plementation of Conditional Random Fields 
(CRFs) for segmenting /labeling sequential data. 
3 Named Entity Recognition in Indian 
Languages 
Named Entity Recognition in Indian languages 
(ILs) is difficult and challenging as capitalization 
is not a clue in ILs. The training data were pro-
vided for five different Indian languages, namely 
Bengali, Hindi, Telugu, Oriya and Urdu in Shakti 
Standard Format
4
. The training data in all the lan-
                                                
3
http://crfpp.sourceforge.net  
4
http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
guages were annotated with the twelve NE tags, as 
defined for the IJCNLP-08 NER shared task taget
5
. 
Only the maximal named entities and not the inter-
nal structures of the entities were annotated in the 
training data. For example, mahatma gandhi road 
was annotated as location and assigned the tag 
?NEL? even if mahatma and gandhi  are named 
entity title person (NETP) and person name (NEP) 
respectively, according to the IJCNLP-08 shared 
task tagset. These internal structures of the entities 
were to be identified during testing. So, mahatma 
gandhi road will be tagged as mahatma /NETP 
gandhi/NEP road/NEL. The structure of the tagged 
element using the SSF form will be as follows:  
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
3.1 Training Data Preparation for CRF  
Training data for all the languages required some 
preprocessing in order to use in the Conditional 
Random Field framework. The training data is 
searched for the multiword NEs. Each component 
of the multiword NE is searched in the training set 
to find whether it occurs as a single-word NE. The 
constituent components are then replaced by their 
NE tags (NE type of the single-word NE). For ex-
ample, mahatma gandhi road/NEL will be tagged 
as mahatma/NETP gandhi/NEP road/NEL if the 
internal components are found to appear with these 
NE tags in the training set. Each component of a 
multiword NE is also checked whether the compo-
nent is made up of digits only. If a component is 
made up digits only, then it is assigned the tag 
?NEN?. Various gazetteers for Bengali and Hindi 
have been also used in order to identify the internal 
structure of the NEs properly.  The list of gazet-
teers, which have been used in preparing the train-
ing data, is shown in Table 1. 
The individual components (not occurring as a 
single-word NE in the training data) of a multi-
word NE are searched in the gazetteer lists and 
                                                
5
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
35
assigned the appropriate NE tags. Other than NEs 
are marked with the NNE tags. The procedure is 
given below:  
Gazetteer list Number of entries 
First person name in Ben-
gali 
27,842 
Last person name in Ben-
gali 
5,288 
Middle name in Bengali 1,491 
Person name designation 
in Bengali 
947 
Location name in Bengali 7,870 
First person name in Hindi 1,62,881 
Last person name in Hindi 3,573 
Middle name in Hindi 450 
Cardinals in Bengali, 
Hindi and Telugu 
100 
Ordinals in Bengali, Hindi 
and Telugu 
65 
Month names in Bengali, 
Hindi and Telugu 
24 
Weekdays in Bengali, 
Hindi and Telugu 
14 
Words that denote meas-
urement in Bengali, Hindi 
and Telugu 
52 
Table 1. Gazetteer lists used during training data 
preparation  
 Step 1: Search the multiword NE in the training 
data 
Step 2: Extract each component from the mult-
word NE. 
Step 3: Check whether the constituent individual 
component (except the last one) appears in the 
training data as a single-word NE. 
Step 4: If the constituent NE appears in the training 
data as a single-word NE then 
Step 4.1: Assign the NE tag, extracted from the 
single-word NE, to the component of the multi-
word NE. 
else 
Step 4.2: Search the component in the gazetteer 
lists and assign the appropriate NE tag. 
Step 4.2.1: If the component is not found to appear 
in the gazetteer list then assign the NE tag of the 
maximal NE to the individual component.  
For example, if mahatma gandhi road is tagged 
as NEL, i.e., mahatma gandhi road/NEL then each 
component except the last one (road ) of this mult-
word NE is searched in the training set to look for 
it?s appearance (Step 3). Gazetteer lists are 
searched in case the component is not found in the 
training set (Step 4.2). If the components are found 
either in the training set or in the gazetteer list, 
then mahatma gandhi road/NEL will be tagged as: 
mahatma/NETP gandhi/NEP road/NEL. 
3.2 Named Entity Features 
Feature selection plays a crucial role in CRF 
framework. Experiments were carried out to find 
out most suitable features for NE tagging task. The 
main features for the NER task have been identi-
fied based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed to use in 
the NER task particularly for Bengali and Hindi. 
We have considered different combination from 
the following set for inspecting the best feature set 
for the NER task: 
 F={
1 1
,..., , , ,...,
i m i i i i n
w w w w w , |prefix| n, 
|suffix| n, previous NE tag, POS tags, First word, 
Digit information, Gazetteer lists} 
     Following is the details of the set of features 
that were applied to the NER task: 
 
Context word feature: Previous and next words 
of a particular word might be used as a feature. We 
have considered the word window of size five, i.e., 
previous and next two words from the current word 
for all the languages.  
Word suffix: Word suffix information is helpful 
to identify NEs. A fixed length word suffix of the 
current and surrounding words might be treated as 
feature. In this work, suffixes of length up to three 
the current word have been considered for all the 
languages. More helpful approach is to modify the 
feature as binary feature. Variable length suffixes 
of a word can be matched with predefined lists of 
useful suffixes for different classes of NEs. For 
Bengali, we have considered the different suffixes 
that may be particularly helpful in detecting person 
(e.g., -babu, -da, -di etc.). 
36
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and the 
surrounding words might be treated as features. 
Here, the prefixes of length up to three have been 
considered for all the language. 
Rare word: The lists of most frequently occurring 
words in the training sets have been calculated for 
all the five languages. The words that occur more 
than 10 times are considered as the frequently oc-
curring words in Bengali and Hindi. For Telugu, 
Oriya and Urdu, the cutoff frequency was chosen 
to be 5. Now, a binary feature ?RareWord? is de-
fined as: If current word is found to appear in the 
frequent word list then it is set to 1; otherwise, set 
to 0.   
First word: If the current token is the first word of 
a sentence, then this feature is set to 1. Otherwise, 
it is set to 0. 
Contains digit: For a token, if it contains digit(s) 
then the feature ?ContainsDigit? is set to 1. This 
feature is helpful for identifying the numbers.  
Made up of four digits: For a token if all the char-
acters are digits and having 4 digits then the fea-
ture ?FourDigit? is set to 1. This is helpful in iden-
tifying the time (e.g., 2007sal) and numerical (e.g., 
2007) expressions. 
Made up of two digits: For a token if all the char-
acters are digits and having 2 digits then the fea-
ture ?TwoDigit? is set to 1. This is helpful for iden-
tifying the time expressions (e.g., 12 ta, 8 am, 9 pm) 
in general. 
Contains digits and comma: For a token, if it con-
tains digits and commas then the feature ?Con-
tainsDigitsAndComma? is set to 1. This feature is 
helpful in identifying named entity measurement 
expressions (e.g., 120,45,330 taka) and numerical 
numbers (e.g., 120,45,330) 
Contains digits and slash: If the token contains 
digits and slash then the feature ?ContainsDigi-
tAndslash? is set to 1. This helps in identifying 
time expressions (e.g., 15/8/2007). 
Contains digits and hyphen: If the token contains 
digits and hyphen then the feature ?ContainsDigit-
sAndHyphen? is set to 1. This is helpful for the 
identification of time expressions (e.g., 15-8-2007). 
Contains digits and period: If the token contains 
digits and periods then the feature ?ContainsDigit-
sAndPeriod? is set to 1. This helps to recognize 
numerical quantities (e.g., 120453.35) and meas-
urements (e.g., 120453.35 taka). 
Contains digits and percentage: If the token con-
tains digits and percentage symbol then the feature 
?ContainsDigitsAndPercentage? is set to 1. This 
helps to recognize measurements (e.g., 120%). 
Named Entity Information: The NE tag of the 
previous word is also considered as the feature, i.e., 
the combination of the current and the previous 
output token has been considered. This is the only 
dynamic feature in the experiment. 
Gazetteer Lists: Various gazetteer lists have been 
created from a tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) for Bengali. The first, 
last and middle names of person for Hindi have 
been created from the election commission data
6
. 
The person name collections had to be processed in 
order to use it in the CRF framework. The simplest 
approach of using these gazetteers is to compare 
the current word with the lists and make decisions. 
But this approach is not good, as it can?t resolve 
ambiguity. So, it is better to use these lists as the 
features of the CRF. If the current token is in a par-
ticular list, then the corresponding feature is set to 
1 for the current/previous/next token; otherwise, 
set to 0. The list of gazetteers is shown in Table 2. 
3.3 Nested Named Entity Identification 
One of the important tasks of the IJCNLP-NER 
shared task was to identify the internal named enti-
ties within the maximal NEs. In the training data, 
only the type of the maximal NEs were given. In 
order to identify the internal NEs during testing, 
we have defined some rules. After testing the un-
annotated test data with the CRF based NER sys-
tem, it is searched to find the sequence of NE tags. 
The last NE tag in the sequence is assigned as the 
NE tag of the maximal NE. The NE tags of the 
constituent NEs may either be changed or may not 
be changed. The NE tags are changed with the help 
of rules and various gazetteer lists. We identified 
NEM (Named entity measurement), NETI (Named 
entity time expressions), NEO (Named entity or-
ganization names), NEP (Named entity person 
names) and NEL (Named entity locations) to be 
the potential NE tags, where nesting could occur. 
A NEM expression may contain NEN, an NETI 
may contain NEN, an NEO may contain NEP/ 
NEL, an NEL may contain NEP/NETP/NED and 
an NEP may contain NEL expressions. The nested 
                                                
 
6 
http://www.eci.gov.in/DevForum/Fullname.asp 
37
NEN tags could be identified by simply checking 
whether it contains digits only and checking the 
lists of cardinal and ordinal numbers.  
  Gazetteer  Number 
of entries
 
 Feature Descrip-
tions 
Designation 
words in Bengali 
947 ?Designation? set to 
1, otherwise 0  
Organization 
names in Bengali 
2, 225 ?Organization? set 
to 1, otherwise 0. 
Organization 
suffixes in Ben-
gali 
94 ?OrgSuffix? set to 
1, otherwise 0 
Person prefix for 
Bengali 
245 ?PersonPrefix? set 
to 1, otherwise set 
to 0 
First person 
names in Bengali
27,842 ?FirstName? set to 
1, otherwise 0 
Middle names in 
Bengali 
1,491 ?MiddleName? set 
to 1, otherwise 0 
Surnames in 
Bengali 
5,288 ?SurName? set to 1, 
otherwise 0 
Common loca-
tion word in 
Bengali 
75 ?CommonLocation? 
set 1, otherwise 0 
Action verb in 
Bengali 
215 ?ActionVerb? set to 
1, otherwise 0 
First person 
names in Hindi 
1,62,881 ?FirstName? set to 
1, otherwise 0 
Middle person 
names in Hindi 
450 ?MiddleName? set 
to 1, otherwise 0 
Last person 
names in Hindi 
3,573 ?SurName? set to 1, 
otherwise 0 
Location names 
in Bengali 
7,870 ?LocationName? 
set to 1, otherwise 
0 
Week days in 
Bengali, Hindi 
and Telugu 
14 ?WeekDay? set to 
1, otherwise 0 
Month names in 
Bengali, Hindi 
and Telugu 
24 ?MonthName? set 
to 1, otherwise 0 
Measurements in 
Bengali, Hindi 
and Telugu 
52 ?Measurement? set 
to 1, otherwise 0. 
 Table 2. Named entity gazetteer list   
The procedure for identifying the nested NEs are 
shown below: 
Step1: Test the unannotated test set. 
Step 2: Look for the sequence of NE tags. 
Step 3: All the words in the sequence will belong     
to a maximal NE. 
Step 4: Assign the last NE tag in the sequence to 
the maximal NE. 
Step 5: The test set is searched to look whether 
each component word appears with a NE tag.  
Step 6: Assign the particular NE tag to the compo-
nent if it appears in the test set with that NE tag. 
Otherwise, search the gazetteer lists as shown in 
Tables 1-2 to assign the tag. 
4 Evaluation 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
(i). Maximal matches: The largest possibles 
named entities are matched with the reference data. 
(ii). Nested matches: The largest possible as 
well as nested named entities are matched. 
(iii). Maximal lexical item matches: The lexical 
items inside the largest possible named entities are 
matched. 
(iv). Nested lexical item matches: The lexical 
items inside the largest possible as well as nested 
named entities are matched.  
5 Experimental Results 
The CRF based NER system has been trained and 
tested with five different Indian languages namely, 
Bengali, Hindi, Telugu, Oriya and Urdu data.  The 
training and test sets statistics are presented in Ta-
ble 3. Results of evaluation as explained in the 
previous section are shown in Table 4. The F-
measures for the nested lexical match are also 
shown individually for each named entity tag sepa-
rately in Table 5. 
Experimental results of Table 4 show that the 
CRF based NER system performs best for Bengali 
with maximal F-measure of 55.36%, nested F-
measure of 61.46% and lexical F-measure 59.39%. 
The system has demonstrated the F-measures of 
35.37%, 36.75% and 33.12%, respectively for 
maximal, nested and lexical matches. The system 
has shown promising precision values for Hindi. 
But due to the low recall values, the F-measures 
get reduced. The large difference between the re-
call and precision values in the evaluation results 
of Hindi indicates that the system is not able to 
retrieve a significant number of NEs from the test 
38
data. In comparison to Hindi, the precision values 
are low and the recall values are high for Bengali. 
It can be decided from the evaluation results that 
system retrieves more NEs in Bengali than Hindi 
but involves more errors. The lack of features in 
Oriya, Telugu and Urdu might be the reason be-
hind their poor performance.   
Language
 
Number of 
tokens in the 
training set 
Number of to-
kens in the test 
set 
Bengali 122,467 30,505 
Hindi 502,974 38,708 
Telugu 64,026 6,356 
Oriya 93,173 24,640 
Urdu 35,447 3,782 
Table 3: Training and Test Sets Statistics  
Tag Bengali Hindi Oriya Telugu Urdu 
NEP 85.68 21.43 43.76 1.9 7.69 
NED 35.9 38.70 NF NF NF 
NEO 52.53 NF 5.60 NF 22.02 
NEA 26.92 30.77 NF NF NF 
NEB NF NF NF NF NF 
NETP 61.44 NF 12.55 NF NF 
NETO 45.98 NF NF NF NF 
NEL 80.00 22.70 31.49 0.73 50.14 
NETI 53.43 49.60 27.08 7.64 49.28 
NEN 30.12 85.40 9.19 9.16 NF 
NEM 79.08 36.64 7.56 NF 79.27 
NETE 18.06 1.64 NF 5.74 NF 
Table 4. Evaluation for Specific NE Tags (F-
Measures for nested lexical match) [NF: Nothing 
found]  
Experimental results of Table 5 show the F-
measures for the nested lexical item matches for 
individual NE tags. For Bengali, the system has 
shown reasonably high F-measures for NEP, NEL 
and NEM tags and medium F-measures for NETP, 
NETI, NEO and NETO tags. The overall F-
measures in Bengali might have reduced due to 
relatively poor F-measures for NETE, NEN, NEA 
and NED tags. For Hindi, the highest F-measure 
obtained is 85.4% for NEN tag followed by NETI, 
NED, NEM, NEA, NEL and NEP tags. In some 
cases, the system has shown better F-measures for 
Hindi than Bengali also. The system has performed 
better for NEN, NED and NEA tags in Hindi than 
all other languages. 
6 Conclusion  
We have developed a named entity recognition 
system using Conditional Random Fields for the 
five different Indian languages, namely Bengali, 
Hindi, Telugu, Oriya and Urdu. We have consid-
ered the contextual window of size five, prefix and 
suffix of length upto three of the current word, NE 
information of the previous word, different digit 
features and the frequently occurring word lists. 
The system also uses linguistic features extracted 
from the various gazetteer lists for Bengali and 
Hindi. Evaluation results show that the system per-
forms best for Bengali. The performance of the 
system for Bengali can further be improved by in-
cluding the part of speech (POS) information of the 
current and/or the surrounding word(s). The per-
formance of the system for other languages can be 
improved with the use of different linguistic fea-
tures as like Bengali.  
The system did not perform as expected due to 
the problems faced during evaluation regarding the 
tokenization. We have tested the system for Ben-
gali with 10-fold cross validation and obtained im-
pressive results.  
References 
Babych, Bogdan, A. Hartley. Improving machine trans-
lation quality with automatic named entity recogni-
tion. In Proceedings of EAMT/EACL 2003 Workshop 
on MT and other language technology tools, 1-8, 
Hungary. 
Bennet, Scott W.; C. Aone; C. Lovell. 1997. Learning to 
Tag Multilingual Texts Through Observation. In 
Proceedings of EMNLP, 109-116,  Rhode Island.  
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Columbia, Mary-
land.   
39
     
Table 5. Evaluation of the Five Languages  
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7. Fairfax, Vir-
ginia. 
Cunningham, H. 2001. GATE: A general architecture 
for text engineering. Comput.  Humanit. (36), 223-
254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity 
Recognition. In Proceedings of 6
th 
International 
Conference on Advances in Pattern Recognition, 
Kolkata,    India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named En-
tity Recognition. In Proceedings of the 5
th 
Interna-
tional Conference on Natural Language Process-
ing, Hyderabad, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal (Accepted) 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proc. of 18
th
 International Conference on Machine 
learning. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions, ACM TALIP, 2(3), (2003), 290-294. 
McCallum, A.; W. Li. 2003. Early Results for Named 
Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced 
Lexicons. In Proceedings CoNLL-03, Edmanton, 
Canada. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana 
Girju, P. Morarescu, V. F. Lacatusu, A. Novischi, 
A. Badulescu, O. Bolohan. 2002. LCC Tools for 
Question Answering. In Proceedings of the TREC, 
Maryland, 1-10. 
Pinto, D., McCallum, A., Wei, X., and Croft, W. B. 
2003. Table extraction using conditional random 
fields. In Proceedings of SIGIR 03 Conference, 
Toronto, Canada. 
Sekine, Satoshi. 1998. Description of the Japanese 
NE System Used for MET-2, MUC-7, Fairfax, 
Virginia. 
Sha, F. and Pereira, F. 2003. Shallow parsing with 
conditional random fields. In Proceedings of Hu-
man Language Technology, NAACL. 
Measure
Precision Recall F-measure 
Language P
m 
P
n 
P
l 
R
m 
R
n 
R
l 
F
m 
F
n 
F
l 
Bengali 51.63 47.74 52.90 59.60 61.46 67.71 55.36 61.46 59.39
Hindi 71.05 76.08 80.59 23.54 24.23 20.84 35.37 36.75 33.12
Oriya 27.12 27.18 50.40 12.88 10.53 20.07 17.47 15.18 28.71
Telugu 1.70 2.70 8.10 0.538 0.539 3.34 0.827 0.902 4.749
Urdu 49.16 48.93 54.45 21.95 20.15 26.36 30.35 28.55 35.52
M: Maximal,  n: Nested, l: Lexical 
40
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51?58,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Bengali Named Entity Recognition using Support Vector Machine 
Asif Ekbal  
Department of Computer Science and 
Engineering, Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and  
Engineering, Jadavpur University 
Kolkata-700032, India 
svaji_cse_ju@yahoo.com 
Abstract 
Named Entity Recognition (NER) aims to 
classify each word of a document into prede-
fined target named entity classes and is nowa-
days considered to be fundamental for many 
Natural Language Processing (NLP) tasks 
such as information retrieval, machine transla-
tion, information extraction, question answer-
ing systems and others. This paper reports 
about the development of a NER system for 
Bengali using Support Vector Machine 
(SVM). Though this state of the art machine 
learning method has been widely applied to 
NER in several well-studied languages, this is 
our first attempt to use this method to Indian 
languages (ILs) and particularly for Bengali. 
The system makes use of the different contex-
tual information of the words along with the 
variety of features that are helpful in predicting 
the various named entity (NE) classes. A por-
tion of a partially NE tagged Bengali news 
corpus, developed from the archive of a lead-
ing Bengali newspaper available in the web, 
has been used to develop the SVM-based NER 
system. The training set consists of approxi-
mately 150K words and has been manually 
annotated with the sixteen NE tags. Experi-
mental results of the 10-fold cross validation 
test show the effectiveness of the proposed 
SVM based NER system with the overall av-
erage Recall, Precision and F-Score of 94.3%, 
89.4% and 91.8%, respectively. It has been 
shown that this system outperforms other ex-
isting Bengali NER systems. 
1 Introduction 
Named Entity Recognition (NER) is an important 
tool in almost all NLP application areas such as 
information retrieval, machine translation, ques 
tion-answering system, automatic summarization 
etc. Proper identification and classification of NEs 
are very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
NER makes it difficult to attain human perform-
ance 
NER has drawn more and more attention from 
the NE tasks (Chinchor 95; Chinchor 98) in Mes-
sage Understanding Conferences (MUCs) [MUC6; 
MUC7]. The problem of correct identification of 
NEs is specifically addressed and benchmarked by 
the developers of Information Extraction System, 
such as the GATE system (Cunningham, 2001). 
NER also finds application in question-answering 
systems (Maldovan et al, 2002) and machine 
translation (Babych and Hartley, 2003).  
The current trend in NER is to use the machine-
learning approach, which is more attractive in that 
it is trainable and adoptable and the maintenance of 
a machine-learning system is much cheaper than 
that of a rule-based one. The representative ma-
chine-learning approaches used in NER are Hidden 
Markov Model (HMM) (BBN?s IdentiFinder in 
(Bikel, 1999)), Maximum Entropy (New York 
University?s MEME in (Borthwick, 1999)), Deci-
sion Tree (New York University?s system in (Se-
kine, 1998) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001). Support Vector Ma-
chines (SVMs) based NER system was proposed 
by Yamada et al (2002) for Japanese. His system 
is an extension of Kudo?s chunking system (Kudo 
and Matsumoto, 2001) that gave the best perform-
ance at CoNLL-2000 shared tasks. The other 
SVM-based NER systems can be found in (Takeu-
chi and Collier, 2002) and (Asahara and Matsu-
moto, 2003).  
 Named entity identification in Indian languages 
in general and particularly in Bengali is difficult 
and challenging. In English, the NE always ap-
pears with capitalized letter but there is no concept 
of capitalization in Bengali. There has been a very
51
little work in the area of NER in Indian languages. 
In Indian languages, particularly in Bengali, the 
works in NER can be found in (Ekbal and 
Bandyopadhyay, 2007a; Ekbal and Bandyop-
adhyay, 2007b) with the pattern directed shallow 
parsing approach and in (Ekbal et al, 2007c) with 
the HMM. Other than Bengali, a CRF-based Hindi 
NER system can be found in (Li and McCallum, 
2004).  
The rest of the paper is organized as follows.  
Support Vector Machine framework is described 
briefly in Section 2. Section 3 deals with the 
named entity recognition in Bengali that describes 
the named entity tagset and the detailed descrip-
tions of the features for NER. Experimental results 
are presented in Section 4. Finally, Section 5 con-
cludes the paper.  
2 Support Vector Machines 
Support Vector Machines (SVMs) are relatively 
new machine learning approaches for solving two-
class pattern recognition problems. SVMs are well 
known for their good generalization performance, 
and have been applied to many pattern recognition 
problems. In the field of NLP, SVMs are applied to 
text categorization, and are reported to have 
achieved high accuracy without falling into over-
fitting even though with a large number of words 
taken as the features. 
Suppose we have a set of training data for a two-
class problem: 1 1{( , ),.....( , )}N Nx y x y , where 
D
ix R  is a feature vector of the i-th sample in the 
training   data and { 1, 1}iy     is the class to which 
ix belongs. The goal is to find a decision function 
that accurately predicts class y for an input vector 
x. A non-linear SVM classifier gives a decision 
function f(x) sign(g(x) for an input vector 
where, 
1
( ) ( , )i
m
i
i
g x wK x z b

 

 
Here, f(x) +1 means x is a member of a cer-
tain class and f(x)  -1 means x is not a member. 
zi s are called support vectors and are representa-
tives of training examples, m is the number of sup-
port vectors. Therefore, the computational com-
plexity of ( )g x  is proportional to m. Support vec-
tors and other constants are determined by solving 
a certain quadratic programming problem. 
( , )iK x z is a kernel that implicitly maps vectors 
into a higher dimensional space. Typical kernels 
use dot products: ( , ) ( . )iK x z k x z . A polynomial 
kernel of degree d is given by 
( , )iK x z =(1 )
d
x . We can use various kernels, 
and the design of an appropriate kernel for a par-
ticular application is an important research issue.  
We have developed our system using SVM 
(Jochims, 1999) and (Valdimir, 1995), which per-
forms classification by constructing an N-
dimensional hyperplane that optimally separates 
data into two categories. Our general NER system 
includes two main phases: training and classifica-
tion. Both the training and classification processes 
were carried out by YamCha1 toolkit, an SVM 
based tool for detecting classes in documents and 
formulating the NER task as a sequential labeling 
problem. Here, the pair wise multi-class decision 
method and second degree polynomial kernel func-
tion were used. We have used TinySVM-0.072 
classifier that seems to be the best optimized 
among publicly available SVM toolkits. 
3 Named Entity Recognition in Bengali 
Bengali is one of the widely used languages all 
over the world. It is the seventh popular language 
in the world, second in India and the national lan-
guage of Bangladesh. A partially NE tagged Ben-
gali news corpus (Ekbal and Bandyopadhyay, 
2007d), developed from the archive of a widely 
read Bengali newspaper. The corpus contains 
around 34 million word forms in ISCII (Indian 
Script Code for Information Interchange) and 
UTF-8 format. The location, reporter, agency and 
different date tags (date, ed, bd, day) in the par-
tially NE tagged corpus help to identify some of 
the location, person, organization and miscellane-
ous names, respectively that appear in some fixed 
places of the newspaper. These tags cannot detect 
the NEs within the actual news body. The date in-
formation obtained from the news corpus provides 
example of miscellaneous names. A portion of this 
partially NE tagged corpus has been manually an-
notated with the sixteen NE tags as described in 
Table 1. 
3.1 Named Entity Tagset 
A SVM based NER system has been developed in 
this work to identify NEs in Bengali and classify 
                                                 
1http://chasen-org/~taku/software/yamcha/  
2http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM  
52
them into the predefined four major categories, 
namely, ?Person name?, ?Location name?, ?Organi-
zation name? and ?Miscellaneous name?. In order 
to properly denote the boundaries of the NEs and 
to apply SVM in NER task, sixteen NE and one 
non-NE tags have been defined as shown in Table 
1. In the output, sixteen NE tags are replaced ap-
propriately with the four major NE tags by some 
simple heuristics. 
 
NE tag Meaning Example 
PER Single word per-
son name 
sachin / PER 
LOC Single word loca-
tion name 
jdavpur/LOC 
ORG Single word or-
ganization name 
infosys / ORG 
MISC Single word mis-
cellaneous name 
100%/ MISC 
B-PER 
I-PER 
E-PER 
Beginning, Inter-
nal or the End of 
a multiword per-
son name 
sachin/B-PER 
ramesh/I-PER  
tendulkar/E-PER 
B-LOC 
I-LOC 
E-LOC 
Beginning, Inter-
nal or the End of 
a multiword loca-
tion name 
mahatma/B-LOC 
gandhi/I-LOC   
road/E-LOC 
B-ORG 
I-ORG 
E-ORG 
Beginning, Inter-
nal or the End of 
a multiword or-
ganization name 
bhaba/B-ORG 
atomic/I-ORG  
research/I-ORG 
center/E-ORG 
B-MISC 
I-MISC 
E-MISC 
Beginning, Inter-
nal or the End of 
a multiword mis-
cellaneous name 
10e/B-MISC 
magh/I-MISC 
1402/E-MISC 
NNE Words that are 
not named enti-
ties  
neta/NNE,  
bidhansabha/NNE
Table 1. Named Entity Tagset 
3.2 Named Entity Feature Descriptions 
Feature selection plays a crucial role in the Support 
Vector Machine (SVM) framework. Experiments 
have been carried out in order to find out the most 
suitable features for NER in Bengali. The main 
features for the NER task have been identified 
based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed for use in 
the NER task. We have considered different com-
bination from the following set for inspecting the 
best feature set for NER task: 
F={ 1 1,..., , , ,...,i m i i i i nw w w w w    , |prefix|n, |suffix|n, 
previous NE tags, POS tags, First word, Digit in-
formation, Gazetteer lists} 
Following are the details of the set of features 
that have been applied to the NER task: 
Context word feature: Previous and next words of 
a particular word might be used as a feature.  
Word suffix: Word suffix information is helpful 
to identify NEs. This feature can be used in two 
different ways. The first and the na?ve one is, a 
fixed length word suffix of the current and/or the 
surrounding word(s) might be treated as feature. 
The second and the more helpful approach is to 
modify the feature as binary valued. Variable 
length suffixes of a word can be matched with pre-
defined lists of useful suffixes for different classes 
of NEs. The different suffixes that may be particu-
larly helpful in detecting person (e.g., -babu, -da, -
di etc.) and location names (e.g., -land, -pur, -lia 
etc.) are also included in the lists of variable length 
suffixes. Here, both types of suffixes have been 
used. 
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and/or 
the surrounding word(s) might be treated as fea-
tures.  
Part of Speech (POS) Information: The POS of 
the current and/or the surrounding word(s) can be 
used as features. Multiple POS information of the 
words can be a feature but it has not been used in 
the present work. The alternative and the better 
way is to use a coarse-grained POS tagger.  
Here, we have used a CRF-based POS tagger, 
which was originally developed with the help of 26 
different POS tags3, defined for Indian languages.  
For NER, we have considered a coarse-grained 
POS tagger that has only the following POS tags: 
NNC (Compound common noun), NN (Com-
mon noun), NNPC (Compound proper noun), NNP 
(Proper noun), PREP (Postpositions), QFNUM 
(Number quantifier) and Other (Other than the 
above). 
                                                 
3http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf 
53
 The POS tagger is further modified with two 
POS tags (Nominal and Other) for incorporating 
the nominal POS information. Now, a binary val-
ued feature ?nominalPOS? is defined as: If the cur-
rent/surrounding word is ?Nominal? then the  
?nominalPOS? feature of the corresponding word is 
set to ?+1?; otherwise, it is set to ?-1?. This binary 
valued ?nominalPOS? feature has been used in ad-
dition to the 7-tag POS feature.  Sometimes, post-
positions play an important role in NER as postpo-
sitions occur very frequently after a NE. A binary 
valued feature ?nominalPREP? is defined as: If the 
current word is nominal and the next word is PREP 
then the feature ?nomianlPREP? of the current 
word is set to ?+1?, otherwise, it is set to ?-1?. 
Named Entity Information: The NE tag(s) of the 
previous word(s) can also be considered as the fea-
ture. This is the only dynamic feature in the ex-
periment. 
First word: If the current token is the first word of 
a sentence, then the feature ?FirstWord? is set to 
?+1?; Otherwise, it is set to ?-1?. 
Digit features: Several digit features have been 
considered depending upon the presence and/or the 
number of digit(s) in a token (e.g., ContainsDigit 
[token contains digits], FourDigit [token consists 
of four digits], TwoDigit [token consists of two 
digits]), combination of digits and punctuation 
symbols (e.g., ContainsDigitAndComma [token 
consists of digits and comma], ConatainsDigi-
tAndPeriod [token consists of digits and periods]), 
combination of digits and symbols (e.g., Con-
tainsDigitAndSlash [token consists of digit and 
slash], ContainsDigitAndHyphen [token consists 
of digits and hyphen], ContainsDigitAndPercent-
age [token consists of digits and percentages]). 
These binary valued features are helpful in recog-
nizing miscellaneous NEs such as time expres-
sions, monetary expressions, date expressions, per-
centages, numerical numbers etc.     
Gazetteer Lists: Various gazetteer lists have been 
developed from the partially NE tagged Bengali 
news corpus (Ekbal and Bandyopadhyay, 2007d). 
These lists have been used as the binary valued 
features of the SVM framework. If the current to-
ken is in a particular list, then the corresponding 
feature is set to ?+1? for the current and/or sur-
rounding word(s); otherwise, it is set to ?-1?. The 
following is the list of gazetteers: 
 (i). Organization suffix word (94 entries): This list 
contains the words that are helpful in identifying 
organization names (e.g., kong, limited etc.). The 
feature ?OrganizationSuffix? is set to ?+1? for the 
current and the previous words.  
 (ii). Person prefix word (245 entries): This is use-
ful for detecting person names (e.g., sriman, sree, 
srimati etc.). The feature ?PersonPrefix? is set to 
?+1? for the current and the next two words.  
 (iii). Middle name (1,491 entries): These words 
generally appear inside the person names (e.g., 
chandra, nath etc.). The feature ?MiddleName? is 
set to ?+1? for the current, previous and the next 
words.  
 (iv). Surname (5,288 entries): These words usually 
appear at the end of person names as their parts. 
The feature ?SurName? is set to ?+1? for the current 
word. 
(v). Common location word (547 entries): This list 
contains the words that are part of location names 
and appear at the end (e.g., sarani, road, lane etc.). 
The feature ?CommonLocation? is set to ?+1? for 
the current word. 
(vi). Action verb (221 entries): A set of action 
verbs like balen, ballen, ballo, shunllo, haslo etc. 
often determines the presence of person names. 
The feature ?ActionVerb? is set to ?+1? for the 
previous word. 
(vii). Frequent word (31,000 entries): A list of 
most frequently occurring words in the Bengali 
news corpus has been prepared using a part of the 
corpus. The feature ?RareWord? is set to ?+1? for 
those words that are not in this list. 
(viii). Function words (743 entries): A list of func-
tion words has been prepared manually. The fea-
ture ?NonFunctionWord? is set to ?+1? for those 
words that are not in this list. 
(ix). Designation words (947 entries): A list of 
common designation words has been prepared. 
This helps to identify the position of the NEs, par-
ticularly person names (e.g., neta, sangsad, 
kheloar etc.). The feature ?DesignationWord? is set 
to ?+1? for the next word. 
(x). Person name (72, 206 entries): This list con-
tains the first name of person names. The feature 
?PersonName? is set to ?+1? for the current word. 
(xi). Location name (7,870 entries): This list con-
tains the location names and the feature ?Loca-
tionName? is set to ?+1? for the current word. 
(xii). Organization name (2,225 entries): This list 
contains the organization names and the feature 
?OrganizationName? is set to ?+1? for the current 
word.  
(xiii). Month name (24 entries): This contains the 
name of all the twelve different months of both 
54
English and Bengali calendars. The feature 
?MonthName? is set to ?+1? for the current word.  
(xiv). Weekdays (14 entries): It contains the name 
of seven weekdays in Bengali and English both. 
The feature ?WeekDay? is set to ?+1? for the cur-
rent word. 
4 Experimental Results 
A partially NE tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) has been used to cre-
ate the training set for the NER experiment. Out of 
34 million wordforms, a set of 150K wordforms 
has been manually annotated with the 17 tags as 
shown in Table 1 with the help of Sanchay Editor4, 
a text editor for Indian languages. Around 20K NE 
tagged corpus is selected as the development set 
and the rest 130K wordforms are used as the train-
ing set of the SVM based NER system.  
We define the baseline model as the one where 
the NE tag probabilities depend only on the current 
word: 
1 2 3 1 2 3
1...
( , , ..., | , , ..., ) ( , )n n i i
i n
P t t t t w w w w P t w


	
 
In this model, each word in the test data is as-
signed the NE tag that occurs most frequently for 
that word in the training data. The unknown word 
is assigned the NE tag with the help of various 
gazetteers and NE suffix lists. 
Seventy four different experiments have been 
conducted taking the different combinations from 
the set ?F? to identify the best-suited set of features 
for NER in Bengali. From our empirical analysis, 
we found that the following combination gives the 
best result for the development set.  
F={ 3 2 1 1 2i i i i i iw w w w w w     , |prefix|<=3, 
|suffix|<=3, NE information of the window [-2, 0], 
POS information of the window [-1, +1], nominal-
POS of the current word, nominalPREP, 
FirstWord, Digit features, Gazetteer lists} 
The meanings of the notations, used in experi-
mental results, are defined below: 
pw, cw, nw: Previous, current and the next 
word; pwi, nwi: Previous and the next ith word 
from the current word; pt: NE tag of the previous 
word; pti: NE tag of the previous ith word; pre, 
suf: Prefix and suffix of the current word; ppre, 
psuf: Prefix and suffix of the previous word; npre, 
nsuf: Prefix and suffix of the next word; pp, cp, np: 
POS tag of the previous, current and the next word; 
                                                 
4Sourceforge.net/project/nlp-sanchay 
ppi, npi: POS tag of the previous and the next ith 
word; cwnl: Current word is nominal. 
Evaluation results of the development set are 
presented in Tables 2-4. 
 
Feature (word, tag)  FS (%) 
pw, cw, nw, FirstWord 71.23 
pw2, pw, cw, nw, nw2, FirstWord 73.23 
pw3, pw2, pw, cw, nw, nw2, 
FirstWord 
74.87 
pw3, pw2, pw, cw, nw, nw2, nw3, 
FirstWord 
74.12 
pw4, pw3, pw2, pw, cw, nw, nw2, 
FirstWord 
74.01 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt 
75.30 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2 
76.23 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, pt3 
75.48 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, | |suf|<=4, pre|<=4 
78.72 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3 
81.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3 
|psuf|<=3 
80.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|psuf|<=3, |nsuf|<=3, |ppre|<=3, 
|npre|<=3 
78.14 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|nsuf|<=3, |npre|<=3 
79.90 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|psuf|<=3, |ppre|<=3, 
80.10 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit  
82.8 
Table 2. Results on the Development Set  
 
It is observed from Table 2 that the word win-
dow [-3, +2] gives the best result (4th row) with the 
?FirstWord? feature and further increase or de-
crease in the window size reduces the overall F-
Score value. Results (7th-9th rows) show that the 
inclusion of NE information increases the F-Score 
value and the NE information of the previous two 
words gives the best results (F-Score=81.2%). It is 
indicative from the evaluation results (10th and 11th 
55
rows) that prefixes and suffixes of length up to 
three of the current word are very effective. It is 
also evident (12th-15th rows) that the surrounding 
word prefixes and/or suffixes do not increase the 
F-Score value. The F-Score value is improved by 
1.6% with the inclusion of various digit features 
(15th and 16th rows). 
 
Feature (word, tag)  FS ( %) 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, pp, cp, np 
    87.3 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, pp2, pp, cp, np, np2 
85.1 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit,  pp, cp 
86.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, cp, np 
85.8 
pp2, pp, cp, np, np2, pt, pt2, 
|pre|<=3, |suf|<=3, FirstWord, Digit 
41.9 
pp, cp, np, pt, pt2, |pre|<=3, |suf|<=3, 
FirstWord, Digit 
36.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, cp 
86.1 
Table 3. Results on the Development Set 
 
Experimental results (2nd-5th rows) of Table 3 
suggest that the POS tags of the previous, current 
and the next words, i.e., POS information of the 
window [-1, +1] is more effective than the window 
[-2, +2], [-1, 0], [0, +1] or the current word alone. 
In the above experiment, the POS tagger was de-
veloped with 7 POS tags. Results (6th and 7th rows) 
also show that POS information with the word is 
helpful but only the POS information without the 
word decreases the F-Score value significantly. 
Results (4th and 5th rows) also show that the POS 
information of the window [-1, 0] is more effective 
than the POS information of the window [0, +1]. 
So, it can be argued that the POS information of 
the previous word is more helpful than the POS 
information of the next word. 
In another experiment, the POS tagger was de-
veloped with 26 POS tags and the use of this tag-
ger has shown the F-Score value of 85.6% with the 
feature (word, tag)=[pw3, pw2, pw, cw, nw, nw2, 
FirstWord, pt, pt2, |suf|<=3, |pre|<=3, Digit, pp, cp, 
np]. So, it can be decided that the smaller POS 
tagset is more effective than the larger POS tagset 
in NER. We have observed from two different ex-
periments that the overall F-Score values can fur-
ther be improved by 0.5% and 0.3%, respectively, 
with the ?nominalPOS? and ?nominalPREP? fea-
tures. It has been also observed that the ?nominal-
POS? feature of the current word is only helpful 
and not of the surrounding words. The F-Score 
value of the NER system increases to 88.1% with 
the feature: feature (word, tag)=[pw3, pw2, pw, 
cw, nw, nw2, FirstWord, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominalPREP]. 
Experimental results with the various gazetteer 
lists are presented in Table 4 for the development 
set. Results demonstrate that the performance of 
the NER system can be improved significantly 
with the inclusion of various gazetteer lists. The 
overall F-Score value increases to 90.7%, which is 
an improvement of 2.6%, with the use of gazetteer 
lists. 
The best set of features is identified by training 
the system with 130K wordforms and tested with 
the help of development set of 20K wordforms. 
Now, the development set is included as part of the 
training set and resultant training set is thus con-
sisting of 150K wordforms. The training set has 
20,455 person names, 11,668 location names, 963 
organization names and 11,554 miscellaneous 
names. We have performed 10-fold cross valida-
tion test on this resultant training set. The Recall, 
Precision and F-Score values of the 10 different 
experiments for the 10-fold cross validation test 
are presented in Table 5. The overall average Re-
call, Precision and F-Score values are 94.3%, 
89.4% and 91.8%, respectively. 
The other existing Bengali NER systems along 
with the baseline model have been also trained and 
tested with the same data set. Comparative evalua-
tion results of the 10-fold cross validation tests are 
presented in Table 6 for the four different models. 
It presents the average F-Score values for the four 
major NE classes: ?Person name?, ?Location 
name?, ?Organization name? and ?Miscellaneous 
name?. Two different NER models, A and B, are 
defined in (Ekbal and Bandyopadhyay, 2007b). 
The model A denotes the NER system that does 
not use linguistic knowledge and B denotes the 
system that uses linguistic knowledge. Evaluation 
results of Table 6 show that the SVM based NER 
model has reasonably high F-Score value. The av-
erage F-Score value of this model is 91.8%, which 
is an improvement of 7.3% over the best-reported 
56
HMM based Bengali NER system (Ekbal et al, 
2007c). The reason behind the rise in F-Score 
value might be its better capability to capture the 
morphologically rich and overlapping features of 
Bengali language.  
 
Feature (word, tag) FS (%) 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord
 
89.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord
 
89.5 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord OrganizationSuf-
fix, PersonPrefix
 
90.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord OrganizationSuf-
fix, PersonPrefix MiddleName, 
CommonLocation 
90.5 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, No-
FunctionWord OrganizationSuf-
fix, PersonPrefix MiddleName, 
CommonLocation,  Other gazet-
teers 
90.7 
 Table 4. Results on the Development Set  
 
The F-Score value of the system increases with 
the increment of training data. This fact is repre-
sented in Figure 1. Also, it is evident from Figure 1 
that the value of ?Miscellaneous name? is nearly 
close to 100% followed by ?Person name?, ?Loca-
tion name? and ?Organization name? NE classes 
with the training data of 150K words. 
 
Test set no. Recall Precision FS (%) 
1 92.5 87.5 89.93 
2 92.3 87.6 89.89 
3 94.3 88.7 91.41 
4 95.4 87.8 91.40 
5 92.8 87.4 90.02 
6 92.4 88.3 90.30 
7 94.8 91.9 93.33 
8 93.8 90.6 92.17 
9 96.9 91.8 94.28 
10 97.8 92.4 95.02 
Average 94.3 89.4 91.8 
 Table 5. Results of the 10-fold cross validation 
test  
      
Model F_P F_L F_O F_M F_T 
Baseline 
 
61.3 58.7 58.2 52.2 56.3 
A 75.3 74.7 73.9 76.1 74.5 
B 79.3 78.6 78.6 76.1 77.9 
HMM 85.5 82.8 82.2 92.7 84.5 
SVM 91.4 89.3 87.4 99.2 91.8 
 Table 6. Results of the 10-fold cross validation 
test (F_P: Avg. f-score of ?Person?, F_L: Avg. f-
score of ?Location?, F_O: Avg. f-score of ?Organi-
zation?, F_M: Avg. f-score of ?Miscellaneous? and 
F_T: Overall avg. f-score of all classes)   
5 Conclusion 
We have developed a NER system using the SVM 
framework with the help of a partially NE tagged 
Bengali news corpus, developed from the archive 
of a leading Bengali newspaper available in the 
web. It has been shown that the contextual window 
of size six, prefix and suffix of length up to three 
of the current word, POS information of the win-
dow of size three, first word, NE information of 
the previous two words, different digit features and 
the various gazetteer lists are the best-suited fea-
tures for NER in Bengali. Experimental results 
with the 10-fold cross validation test have shown 
reasonably good Recall, Precision and F-Score 
values. The performance of this system has been 
compared with the existing three Bengali NER sys-
tems and it has been shown that the SVM-based 
system outperforms other systems. One possible 
reason behind the high Recall, Precision and F-
Score values of the SVM based system might be its 
effectiveness to handle the diverse and overlapping 
features of the highly inflective Indian languages.    
57
The proposed SVM based system is to be 
trained and tested with the other Indian languages, 
particularly Hindi, Telugu, Oriya and Urdu. Ana-
lyzing the performance of the system using other 
methods like MaxEnt and CRFs will be other in-
teresting experiments. 
 
F-Score(%) vs Training file size(K)
0
20
40
60
80
100
120
0 100 200
Number of Words (K) 
F-
Sc
or
e 
(%
)
Person
Location
Organisation
Miscellaneous
 Fig. 1. F-Score VS Training file size 
References 
Anderson, T. W. and Scolve, S. 1978. Introduction to 
the Statistical Analysis of Data. Houghton Mifflin. 
Asahara, Masayuki and Matsumoto, Yuji. 2003. Japa-
nese Named Entity Extraction with Redundant Mor-
phological Analysis.  In Proc. of HLT-NAACL. 
Babych, Bogdan, A. Hartley. 2003. Improving Machine 
Translation Quality with Automatic Named Entity 
Recognition. In Proceedings of EAMT/EACL 2003 
Workshop on MT and other language technology 
tools, 1-8, Hungary. 
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Maryland. 
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7, Fairfax, Virginia. 
Cunningham, H. 2001. GATE: A General Architecture 
for Text Engineering. Comput.  Humanit. (36), 223-254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity Rec-
ognition. In Proceedings of ICAPR, India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named Entity 
Recognition. In Proc.  of ICON, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Transliteration 
in Bengali. Named Entities: Recognition, Classifica-
tion and Use, Special Issue of Lingvisticae Investiga-
tiones Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Rec-
ognition. Language Resources and Evaluation Jour-
nal (To appear December). 
Joachims , T. 1999. Making Large Scale SVM Learning 
Practical. In B. Scholkopf, C. Burges and A. Smola 
editions, Advances in Kernel Methods-Support Vec-
tor Learning, MIT Press. 
Kudo, Taku and Matsumoto, Yuji. 2001. Chunking with 
Support Vector Machines. In Proceedings of NAACL, 
192-199. 
Kudo, Taku and Matsumoto, Yuji. 2000. Use of Support 
Vector Learning for Chunk Identification. In Pro-
ceedings of CoNLL-2000. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of 
18th International Conference on Machine learning, 
282-289. 
Li, Wei and Andrew McCallum. 2003. Rapid Develop-
ment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Inductions. 
ACM TALIP, 2(3), (2003), 290-294. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana Girju, 
P. Morarescu, V. F. Lacatusu, A. Novischi, A. 
Badulescu, O. Bolohan. 2002. LCC Tools for Ques-
tion Answering. In Proceedings of the TREC, 1-10. 
Sekine, Satoshi. 1998. Description of the Japanese NE 
System Used for MET-2. MUC-7, Fairfax, Virginia. 
Takeuchi, Koichi and Collier, Nigel. 2002. Use of Sup-
port Vector Machines in Extended Named Entity 
Recognition. In Proceedings of  6th CoNLL, 119-125. 
Vapnik, Valdimir N. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Yamada, Hiroyasu, Taku Kudo and Yuji Matsumoto. 
2002. Japanese Named Entity Extraction using Sup-
port Vector Machine. In Transactions of IPSJ, Vol. 
43, No. 1, 44-53.  
 
58
Bengali, Hindi and Telugu to English Ad-hoc Bilingual task  
 
Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, 
Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy 
 
Abstract 
 
This paper presents the experiments carried out at Jadavpur University as 
part of participation in the CLEF 2007 ad-hoc bilingual task. This is our first 
participation in the CLEF evaluation task and we have considered Bengali, 
Hindi and Telugu as query languages for the retrieval from English 
document collection. We have discussed our Bengali, Hindi and Telugu to 
English CLIR system as part of the ad-hoc bilingual task, English IR system 
for the ad-hoc monolingual task and the associated experiments at CLEF. 
Query construction was manual for Telugu-English ad-hoc bilingual task, 
while it was automatic for all other tasks. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Development of Bengali Named Entity Tagged Corpus and its Use in 
NER Systems 
Asif Ekbal 
Department of Computer Science and 
Engineering, Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and  
Engineering, Jadavpur University 
Kolkata-700032, India 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The rapid development of language tools 
using machine learning techniques for less 
computerized languages requires appropri-
ately tagged corpus. A Bengali news cor-
pus has been developed from the web ar-
chive of a widely read Bengali newspaper. 
A web crawler retrieves the web pages in 
Hyper Text Markup Language (HTML) 
format from the news archive. At present, 
the corpus contains approximately 34 mil-
lion wordforms. The date, location, re-
porter and agency tags present in the web 
pages have been automatically named en-
tity (NE) tagged. A portion of this partially 
NE tagged corpus has been manually anno-
tated with the sixteen NE tags with the help 
of Sanchay Editor1, a text editor for Indian 
languages. This NE tagged corpus contains 
150K wordforms. Additionally, 30K word-
forms have been manually annotated with 
the twelve NE tags as part of the IJCNLP-
08 NER Shared Task for South and South 
East Asian Languages 2 . A table driven 
semi-automatic NE tag conversion routine 
has been developed in order to convert the 
sixteen-NE tagged corpus to the twelve-NE 
tagged corpus. The 150K NE tagged cor-
pus has been used to develop Named Entity 
Recognition (NER) system in Bengali us-
ing pattern directed shallow parsing ap-
proach, Hidden Markov Model (HMM), 
Maximum Entropy (ME) Model, Condi-
                                                 
1 sourceforge.net/project/nlp-sanchay 
2 http://ltrc.iiit.ac.in/ner-ssea-08 
tional Random Field (CRF) and Support 
Vector Machine (SVM). Experimental re-
sults of the 10-fold cross validation test 
have demonstrated that the SVM based 
NER system performs the best with an 
overall F-Score of 91.8%. 
1 Introduction 
The mode of language technology work has been 
changed dramatically since the last few years with 
the web being used as a data source in a wide 
range of research activities. The web is anarchic, 
and its use is not in the familiar territory of compu-
tational linguistics. The web walked into the ACL 
meetings started in 1999. The use of the web as a 
corpus for teaching and research on language tech-
nology has been proposed a number of times 
(Rundel, 2000; Fletcher, 2001; Robb, 2003; 
Fletcher, 2003). There is a long history of creating 
a standard for western language resources. The 
human language technology (HLT) society in 
Europe has been particularly zealous for the stan-
dardization, making a series of attempts such as 
EAGLES3, PROLE/SIMPLE (Lenci et al, 2000), 
ISLE/MILE (Calzolari et al, 2003; Bertagna et al, 
2004) and more recently multilingual lexical data-
base generation from parallel texts in 20 European 
languages (Giguet and Luquet, 2006). On the other 
hand, in spite of having great linguistic and cul-
tural diversities, Asian language resources have 
received much less attention than their western 
counterparts. A new project (Takenobou et al, 
2006) has been started to create a common stan-
dard for Asian language resources. They have ex-
tended an existing description framework, the 
                                                 
3 http://www.ilc.cnr.it/Eagles96/home.html 
The 6th Workshop on Asian Languae Resources, 2008
1
MILE (Bertagna et al, 2004), to describe several 
lexical entries of Japanese, Chinese and Thai. India 
is a multilingual country with the enormous cul-
tural diversities. (Bharati et al, 2001) reports on 
efforts to create lexical resources such as transfer 
lexicon and grammar from English to several In-
dian languages and dependency tree bank of anno-
tated corpora for several Indian languages. Corpus 
development work from web can be found in (Ek-
bal and Bandyopadhyay, 2007d) for Bengali. 
Named Entity Recognition (NER) is one of the 
core components in most Information Extraction 
(IE) and Text Mining systems. During the last few 
years, the probabilistic machine learning methods 
have become state of the art for NER (Bikel et al, 
1999; Chieu and Ng, 2002) and for field extraction 
(McCallum et al, 2000). Most prominently, Hid-
den Markov Models (HMMs) have been used for 
the information extraction task (Bikel et al, 1999). 
Beside HMM, there are other systems based on 
Support Vector Machine (Sun et al, 2003) and 
Na?ve Bayes (De Sitter and Daelemans, 2003). 
Maximum Entropy (ME) conditional models like 
ME Markov models (McCallum et al, 2000) and 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) were reported to outperform the generative 
HMM models on several IE tasks.  
The existing works in the area of NER are 
mostly in non-Indian languages. There has been a 
very little work in the area of NER in Indian lan-
guages (ILs). In ILs, particularly in Bengali, the 
work in NER can be found in (Ekbal and 
Bandyopadhyay, 2007a; Ekbal and Bandyop-
adhyay, 2007b; Ekbal et al, 2007c). Other than 
Bengali, the work on NER can be found in (Li and 
McCallum, 2003) for Hindi. 
Newspaper is a huge source of readily available 
documents. In the present work, the corpus has 
been developed from the web archive of a very 
well known and widely read Bengali newspaper. 
Bengali is the seventh popular language in the 
world, second in India and the national language of 
Bangladesh. A code conversion routine has been 
written that converts the proprietary codes used in 
the newspaper into the standard Indian Script Code 
for Information Interchange (ISCII) form, which 
can be processed for various tasks. A separate code 
conversion routine has been developed for convert-
ing ISCII codes to UTF-8 codes. A portion of this 
corpus has been manually annotated with the six-
teen NE tags as described in Table 3. Another por-
tion of the corpus has been manually annotated 
with the twelve NE tags as part of the IJCNLP-08 
NER Shared Task for South and South East Asian 
Languages. A table driven semi-automatic NE tag 
conversion routine has been developed in order to 
convert this corpus to a form tagged with the 
twelve NE tags. The NE tagged corpus has been 
used to develop Named Entity Recognition (NER) 
system in Bengali using pattern directed shallow 
parsing approach, HMM, ME, CRF and SVM 
frameworks.  
A number of detailed experiments have been 
conducted to identify the best set of features for 
NER in Bengali. The ME, CRF and SVM based 
NER models make use of the language independ-
ent as well as language dependent features. The 
language independent features could be applicable 
for NER in other Indian languages also. The sys-
tem has demonstrated the highest F-Score value of 
91.8% with the SVM framework. One possible 
reason behind its best performance may be the 
flexibility of the SVM framework to handle the 
morphologically rich Indian languages.    
2 Development of the Named Entity 
Tagged Bengali News Corpus  
2.1 Language Resource Acquisition 
A web crawler has been developed that retrieves 
the web pages in Hyper Text Markup Language 
(HTML) format from the news archive of a leading 
Bengali newspaper within a range of dates 
provided as input. The crawler generates the 
Universal Resource Locator (URL) address for the 
index (first) page of any particular date. The index 
page contains actual news page links and links to 
some other pages (e.g., Advertisement, TV 
schedule, Tender, Comics and Weather etc.) that 
do not contribute to the corpus generation. The 
HTML files that contain news documents are 
identified and the rest of the HTML files are not 
considered further. 
2.2 Language Resource Creation 
The HTML files that contain news documents are 
identified by the web crawler and require cleaning 
to extract the Bengali text to be stored in the 
corpus along with relevant details. The HTML file 
is scanned from the beginning to look for tags like 
<fontFACE=BENGALI_FONT_NAME>...<font>, 
where the BENGALI_FONT_NAME is the name 
The 6th Workshop on Asian Languae Resources, 2008
2
of one of the Bengali font faces as defined in the 
news archive. The Bengali text enclosed within 
font tags are retrieved and stored in the database 
after appropriate tagging. Pictures, captions and 
tables may exist anywhere within the actual news. 
Tables are integral part of the news item. The 
pictures, its captions and other HTML tags that are 
not relevant to our text processing tasks are 
discarded during the file cleaning. The Bengali 
news corpus has been developed in both ISCII and 
UTF-8 codes.  The tagged news corpus contains 
108,305 number of news documents with about 
five (5) years (2001-2005) of news data collection. 
Some statistics about the tagged news corpus are 
presented in Table 1. 
 
Total number of news documents 
in the corpus 
108, 305 
Total number of sentences in the 
corpus 
2, 822, 737 
Avgerage number of sentences in 
a document  
27 
Total number of wordforms in 
the corpus 
33, 836, 736 
Avgerage number of wordforms 
in a document 
313 
Total number of distinct 
wordforms in the corpus 
467, 858 
Table 1.  Corpus Statistics 
2.3 Language Resource Annotation 
The Bengali news corpus collected from the web is 
annotated using a tagset that includes the type and 
subtype of the news, title, date, reporter or agency 
name, news location and the body of the news. A 
part of this corpus is then tagged with a tagset, 
consisting of sixteen NE tags and one non-NE tag.  
Also, a part of the corpus has been tagged with a 
tagest of twelve NE tags4, defined for the IJCNLP-
08 NER Shared Task for South and South East 
Asian Languages. 
A news corpus, whether in Bengali or in any 
other language, has different parts like title, date, 
reporter, location, body etc. To identify these parts 
in a news corpus the tagset, described in Table 2, 
has been defined. The reporter, agency, location, 
date, bd, day and ed tags help to recognize the 
person name, organization name, location name 
                                                 
4http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3 
and the various date expressions that appear in the 
fixed places of the newspaper. These tags are not 
able to recognize the various NEs that appear 
within the actual news body. 
In order to identify NEs within the actual news 
body, we have defined a tagset consisting of 
seventeen tags. We have considered the major four 
NE classes, namely ?Person name?, ?Location 
name?, ?Organization name? and ?Miscellaneous 
name?. Miscellaneous names include the date, 
time, number, percentage and monetary 
expressions. The four major NE classes are further 
divided in order to properly denote each 
component of the multiword NEs. The NE tagset is 
shown in Table 3 with the appropriate examples. 
We have also tagged a portion of the corpus as 
part of the IJCNLP-08 NER Shared Task for South 
and South East Asian Languages. This tagset has 
twelve different tags. The underlying reason for 
adopting these tags was the necessity of a slightly 
finer tagset for various natural language processing 
(NLP) applications and particularly for machine 
translation. The IJCNLP-08 NER shared task 
tagset is shown in Table 4.      
One important aspect of IJCNLP-08 NER 
shared task was to annotate only the maximal NEs 
and not the structures of the entities. For example, 
mahatma gandhi road is annotated as location and 
assigned the tag ?NEL? even if  mahatma and gan-
dhi are NE title person and person name, respec-
tively, according to the IJCNLP-08 shared task 
tagset. These internal structures of the entities need 
to be identified during testing. So, mahatma gan-
dhi road will be tagged as mahatma/NETP gan-
dhi/NEP road/NEL. The structure of the tagged 
element using the Shakti Standard Format (SSF)5 
will be as follows: 
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
                                                 
5http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
The 6th Workshop on Asian Languae Resources, 2008
3
2.4 Partially Tagged News Corpus Develop-
ment 
A news document is stored in the corpus in XML 
format using the tagset, mentioned in Table 2. In 
the HTML news file, the date is stored at first and 
is divided into three parts. The first one is the date 
according to Bengali calendar, second one is the 
day in Bengali and the last one is the date accord-
ing to English calendar. Both Bengali and English 
dates are stored in the form ?day month year?.  
A sequence of four Bengali digits separates the 
Bengali date from the Bengali day. The English 
date starts with one/two digits in Bengali font. 
Bengali date, day and English date can be distin-
guished by checking the appearance of the numer-
als and these are tagged as <bd>, <day> and <ed>, 
respectively. For e.g., 25 sraban 1412  budhbar  10 
august 2005 is tagged as shown in Table 5. 
 
Table 2. News Corpus Tagset 
 
 
Tag Meaning Example 
PER Single word person name sachin / PER, manmohan/PER 
LOC Single word location name jadavpur / LOC, delhi/LOC 
ORG Single word organization name infosys / ORG, tifr/ORG 
MISC Single word miscellaneous name 100% / MISC, 100/MISC 
B-PER 
I-PER 
E-PER 
Beginning, Internal or the end of 
a multiword person name 
sachin/ B-PER ramesh / I-PER  
tendulkar / E- PER 
B-LOC 
I-LOC 
E-LOC 
Beginning, Internal or the end of 
a multiword location name 
mahatma/ B-LOC gandhi / I-LOC  road / E-LOC 
B-ORG 
I-ORG 
E-ORG 
Beginning, Internal or the end of 
a multiword organization name 
bhaba / B-ORG atomic / I-ORG   
research / I-ORG centre / E-ORG 
B-MISC 
I-MISC 
E-MISC 
Beginning, Internal or the end of 
a multiword miscellaneous name 
10 e / B-MISC   magh / I-MISC  
1402 / E-MISC 
NNE Words that are not named entities neta/NNE, bidhansabha/NNE 
 
 
Table 3. Named Entity Tagset 
 
 
Tag Definition Tag Definition Tag Definition 
header Header of the news 
documents 
day Day body Body of the news 
document 
title Headline of the news 
document 
ed English date p Paragraph 
t1 1st headline of the 
title 
reporter Reporter name table Information in tabular 
form 
t2 
2nd headline of the 
title 
agency Agency 
providing news 
tc 
Table column 
date 
Date of the news 
document 
location News location tr Table row 
bd Bengali date  
The 6th Workshop on Asian Languae Resources, 2008
4
NE tag Meaning Example 
NEP Person name sachin ramesh  tendul-
kar / NEP 
NEL Location 
name 
mahatma gandhi road / 
NEL 
NEO Organization 
name 
bhaba atomic   
research   centre / NEO 
NED Designation chairman/NED, 
sangsad/NED  
NEA Abbreviation b a/NEA, c m d a/NEA, 
b j p/NEA 
NEB Brand fanta/NEB, 
windows/NEB 
NETP Title-person sriman/NED, sree/NED 
NETO Title-object american beauty/NETO
NEN Number 10/NEN, dash/NEN 
NEM Measure tin din/NEM, panch 
keji/NEM 
NETE Terms hidden markov 
model/NETE 
NETI Time 10 e   magh   1402/ 
NETI 
Table 4. IJCNLP-08 NER Shared Task Tagset 
 
Original date pattern Tagged date pattern 
 <date> 
   25 sraban 1412 <bd>25 sraban 1412 
</bd> 
    budhbar <day>budhbar 
</day> 
10 august 2005 <ed>10 august 2005 
</ed> 
 </date> 
Table 5. Example of a Tagged Date Pattern 
2.5 Named Entity Tagged Corpus 
Development 
The partially NE tagged corpus contains 34 
million wordforms and are in both ISCII and UTF-
8 forms. A portion of this corpus, containing 150K 
wordforms, has been manually annotated with the 
sixteen NE tags that are listed in Table 3. The 
corpus has been annotated with the help of 
Sanchay Editor, a text editor for Indian languages. 
The detailed statistics of this NE-tagged corpus are 
given in Table 6. The corpus is in SSF form, 
which has the following structure: 
 
 
 
<Story id=""> 
<Sentence id=""> 
1 biganni NNE  
2 newton PER 
3 .  
</Sentence id=""> 
 .  
</Story id=""> 
 
Another portion of the partially NE tagged 
Bengali news corpus has been manually annotated 
as part of the IJCNLP-08 NER shared task with 
the twelve NE tags, as listed in Table 4. The 
annotation process has been very difficult due to 
the presence of a number of ambiguous NE tags. 
The potential ambiguous NE tags are: NED vs 
NETP, NEO vs NEB, NETE vs NETO, NETO vs 
NETP and NEN vs NEM. For example, it is 
difficult to decide whether ?Agriculture? is 
?NETE?, and if no then whether ?Horticulture? is 
?NETE? or not. In fact, this the most difficult class 
to identify. This NE tagged corpus contains 
approximately 30K wordforms. Details statistics 
of this tagged corpus are shown in Table 7. This 
NE tagged corpus is in the following SSF form.  
   
   <Story id=""> 
   <Sentence id=""> 
1 (( NP <ne=NEP> 
1.1 (( NP <ne=NED> 
1.1.1 biganni 
)) 
   1.1.2 newton NEP 
 )) 
2 . 
   </Sentence id=""> 
   </Story id=""> 
   
NE Class Number of 
wordforms 
Number of dis-
tinct wordforms 
Person name 20, 455 15, 663 
Location 
name 
11, 668    5, 579 
Organization 
name 
963 867 
Miscellane-
ous name 
11,554 3, 227 
Table 6. Statistics of the 150K-tagged Corpus 
The 6th Workshop on Asian Languae Resources, 2008
5
2.6 Tag Conversion 
A tag conversion routine has been developed in 
order to convert the sixteen-NE tagged corpus of 
150K wordforms to the corpus, tagged with the 
IJCNLP-08 twelve-NE tags. This conversion is a 
semi-automatic process. Some of our sixteen NE 
tags can be automatically mapped to some of the 
IJCNLP-08 shared task tags. The tags that repre-
sent person, location and organization names can 
be directly mapped to the NEP, NEL and NEO 
tags, respectively. Other IJCNLP-08 shared task 
tags can be obtained with the help of gazetteer lists 
and simple heuristics. In our earlier NER experi-
ments, we have already developed a number of 
gazetteer lists such as: lists of person, location and 
organization names; list of prefix words (e.g., sree, 
sriman etc.) that predict the left boundary of a per-
son name; list of designation words (e.g., mantri, 
sangsad etc.) that helps to identify person names. 
The lists of prefix and designation words are help-
ful to find the NETP and NED tags. The sixteen-
NE tagged corpus is searched for the person name 
tags and the previous word is matched against the 
lists of prefix and designation words. The previous 
word is tagged as NED or NETP if there is a 
match with the lists of designation words and pre-
fix words, respectively. The NEN and NETI tags 
can be obtained by looking at our miscellaneous 
tags and using some simple heuristics. The NEN 
tags can be simply obtained by checking whether 
the MISC tagged element consists of digits only. 
The lists of cardinal and ordinal numbers have 
been also kept to recognize NETI tags. A list of 
words that denote the measurements (e.g., kilo-
gram, taka, dollar etc.) has been kept in order to 
get the NEM tag. The lists of words, denoting the 
brand names, title-objects and terms, have been 
prepared to get the NEB, NETO and NETE tags. 
The NEA tags can be obtained with the help of a 
gazetteer list and using some simple heuristics 
(whether the word contains the dot and there is no 
space between the characters). The mapping from 
our NE tagset to the IJCNLP-08 NER shared task 
tagset is shown in Table 8. 
3 Use of Language Resources 
The NE tagged news corpus, developed in this 
work, has been used to develop the Named Entity 
Recognition (NER) systems in Bengali using pat-
tern directed shallow parsing, HMM, ME, CRF 
and SVM frameworks. 
 
NE Class Number of 
wordforms 
Number of dis-
tinct wordforms 
Person name 5, 123 3, 201 
Location 
name 
1, 675 1, 119 
Organization 
name 
168 131 
Designation 231 102 
Abbreviation 32 21 
Brand 15 12 
Title-person 79 51 
Title-object 63 42 
Number 324 126 
Measure 54 31 
Time 337 212 
Terms 46 29 
Table 7. Statistics of the 30K-tagged Corpus 
 
Sixteen-NE tagset IJCNLP-08 
tagset 
PER, LOC, ORG NEP, NEL, 
NEO 
B-PER, I-PER, E-PER NEP 
B-LOC, I-LOC, E-LOC NEL 
B-ORG, I-ORG, E-ORG NEO 
MISC  NEN  
B-MISC, I-MISC, E-MISC NETI, NEM 
Brand name gazetteer  NEB 
Title-object gazetteer  NETO 
Term gazetteer  NETE 
Person prefix word + PER/B-
PER, I-PER, E-PER 
NETP 
Designation word +PER/B-
PER, I-PER, E-PER 
NED 
Abbreviation gazetteer + 
Heuristics 
NEA 
Table 8. Tagset Mapping Table  
 
 
We have considered the sixteen NE tags to de-
velop these systems. Named entity recognition in 
Indian Languages (ILs) in general and particularly 
in Bengali is difficult and challenging as there is 
no concept of capitalization in ILs. 
The Bengali NER systems that use pattern di-
rected shallow parsing approach can be found in 
The 6th Workshop on Asian Languae Resources, 2008
6
(Ekbal and Bandyopadhyay, 2007a) and (Ekbal 
and Bandyopadhyay, 2007b). An HMM-based 
Bengali NER system can be found in (Ekbal and 
Bandyopadhyay, 2007c). These systems have been 
trained and tested with the corpus tagged with the 
sixteen NE tags.  
A number of experiments have been conducted 
in order to find out the best feature set for NER in 
Bengali using the ME, CRF and SVM frameworks. 
In all these experiments, we have used a number of 
gazetteer lists such as: first names (72, 206 en-
tries), middle names (1,491 entries) and sur names 
(5,288 entries) of persons; prefix (245 entries) and 
designation words (947 entries) that help to detect 
person names; suffixes (45 and 23 entries) that 
help to identify person and location names; clue 
words (94 entries) that help to detect organization 
names; location name (7, 870 entries) and organi-
zation name (2,225 entries). Apart from these gaz-
etteer lists, we have used the prefix and suffix 
(may not be linguistically meaningful suf-
fix/prefix) features, digit features, first word fea-
ture and part of speech information of the words 
etc. We have used the C++ based Maximum En-
tropy package6, C++ based OpenNLP CRF++ pack-
age7 and open source YamCha8 tool for ME based 
NER, CRF based NER and SVM based NER, re-
spectively. For SVM based NER system, we have 
used TinySVM 9  classifier, pair wise multi-class 
decision method and the second-degree polynomial 
kernel function. The brief descriptions of all the 
models are given below: 
?A: Pattern directed shallow parsing approach 
without linguistic knowledge. 
?B: Pattern directed shallow parsing approach with 
linguistic knowledge. 
?HMM based NER: Trigram model with additional 
context dependency, NE suffix lists for handling 
unknown words. 
?ME based NER: Contextual window of size three 
(current, previous and the next word), prefix and 
suffix of length upto three of the current word, 
POS information of the current word, NE informa-
tion of the previous word (dynamic feature), dif-
ferent digit features and the various gazetteer liststs. 
                                                 
6http://homepages.inf.ed.ac.uk/s0450736/software/maxe
nt/maxent-20061005.tar.bz2 
7 http://crfpp.sourceforge.net 
8 http://chasen.org/~taku/software/yamcha/ 
9http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM 
?CRF based NER: Contextual window of size five 
(current, previous two words and the next two 
words), prefix and suffix of length upto three of the 
current word, POS information of window three 
(current word, previous word and the next word), 
NE information of the previous word (dynamic 
feature), different digit features and the various 
gazetteer lists. 
?SVM based NER: Contextual window of size six 
(current, previous three words and the next two 
words), prefix and suffix of length upto three of the 
current word, POS information of window three 
(current word, previous word and the next word), 
NE information of the previous two words (dy-
namic feature), different digit features and the vari-
ous gazetteer lists. 
Evaluation results of the 10-fold cross validation 
test for all the models are presented in Table 9. 
Evaluation results clearly show that the SVM 
based NER model outperforms other models due to 
it?s efficiency to handle the non-independent, di-
verse and overlapping features of Bengali lan-
guage.    
 
Model F-Score (in %) 
A 74.5 
B 77.9 
HMM 84.5 
ME 87.4 
CRF 90.7 
SVM 91.8 
Table 9.Results of 10-fold Cross Validation Test 
4 Conclusion 
In this work we have developed a Bengali news 
corpus of approximately 34 million wordforms 
from the web archive of a leading Bengali newspa-
per. The date, location, reporter and agency tags 
present in the web pages have been automatically 
NE tagged. Around 150K wordforms of this par-
tially NE tagged corpus has been manually anno-
tated with the sixteen NE tags. We have also 
tagged around 30K wordforms with the twelve NE 
tags, defined for the IJCNLP-08 NER shared task. 
A tag conversion routine has also been developed 
in order to convert any sixteen-NE tagged corpus 
to the twelve-NE tagged corpus.  The sixteen-NE 
tagged corpus of 150K wordforms has been used to
The 6th Workshop on Asian Languae Resources, 2008
7
develop the NER systems using various machine-
learning approaches. 
This NE tagged corpus can be used for other 
NLP research activities such as machine 
translation, information retrieval, cross-lingual 
event tracking, automatic summarization etc. 
References 
Bertagna, M. and A. Lenci, M. Monachini and N. Cal-
zolari. 2004. CotentInteroperability of Lexical Re-
sources, Open Issues and ?MILE? Perspectives, In 
Proceedings of the LREC, 131-134. 
Bharthi, A., D.M. Sharma, V. Chaitnya, A. P. Kulkarni 
and R. Sanghal. 2001. LERIL: Collaborative Effort 
for Creating Lexical Resources. In Proceedings of 
the 6th NLP Pacific Rim Symposium Post-Conference 
Workshop, Japan. 
Bikel, D. M., Schwartz, R., Weischedel, R. M. 1999. An 
Algorithm that Learns What?s in a Name. Machine 
Learning, 34, 211-231. 
Calzolari, N., F. Bertagna, A. Lenci and M. Monachni. 
2003. Standards and best Practice for Miltilingual 
Computational Lexicons, MILE (the multilingual 
ISLE lexical entry). ISLE Deliverable D2.2 &3.2.  
Chieu, H. L., Tou Ng, H. 2002. Named Entity Recogni-
tion: A Maximum Entropy Approach Using Global 
Information, In Proc. of the 6th  Workshop on Very 
Large Corpora.  
De Sitter, A., Daelemans W. 2003. Information Extrac-
tion via Double Classification. In Proeedings of In-
ternational Workshop on Adaptive Text Extraction 
and Mining, Dubronik. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity Rec-
ognition. In Proceedings of the 6th International Con-
ference on Advances in Pattern Recognition, 2007, 
India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named Entity 
Recognition. In Proceedings of the 5th International 
Conference on Natural Language Processing 
(ICON), India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Transliteration 
in Bengali, Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisticae 
Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Rec-
ognition. Language Resources and Evaluation Jour-
nal (Accepted and to appear by December 2007). 
Fletcher, W. H. 2001. Making the Web More Useful as 
Source for Linguistics Corpora. In Ulla Conor and 
Thomas A. Upton (eds.), Applied corpus Linguistics: 
A Multidimensional Perspective, 191-205. 
Fletcher, W. H. 2003. Concording the Web with 
KwiCFinder. In Proceedings of the Third North 
American Symposium on Corpus Linguistics and 
Language Teaching, Boston. 
Giguet, E., and P. Luquet. 2006. Multilingual Lexical 
Database Generation from Parallel Texts in 20 Euro-
pean Languages with Endogeneous Resources. In 
Proceedings of the COLING/ACL, Sydney, 271-278. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, In Proceedings 
of the 18th International Conference on Machine 
Learning, 282-289. 
Lenci, A., N. Bel, F. Busu, N. Calzolari, E. Gola, M. 
Monachini, A. Monachini, A. Ogonowski, I. Peters, 
W. Peters, N. Ruimy, M. Villegas and A. Zampolli. 
2000. SIMPLE: A general Framework for the Devel-
opment of Multilingual Lexicons. International 
Journal of Lexicography, Special Issue, Dictionaries, 
Thesauri and Lexical-Semantic Relations, XIII(4): 
249-263. 
Li, Wei and Andrew McCallum. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Inductions. 
ACM TALIP, Vol. 2(3), 290-294. 
McCallum, A., Freitag, D., Pereira, F. 2000. Maximum 
Entropy Markov Models for Information Extraction 
and Segmentation.  In Proceedings of the 17th Inter-
national Conference Machine Learning. 
Robb, T. 2003. Google as a Corpus Tool? ETJ Journal, 
4(1), Spring 2003. 
Rundell, M. 2000. The Biggest Corpus of All. Humanis-
ing Language Teaching, 2(3). 
Sun, A., et al 2003. Using Support Vector Machine for 
Terrorism Information Extraction. In Proceedings of 
1st NSF/NIJ Symposium on Intelligence and Security.   
Takenobou, T., V. Sornlertlamvanich, T. Charoenporn, 
N. Calzolari, M. Monachini, C. Soria, C. Huang, X. 
YingJu, Y. Hao, L. Prevot and S. Kiyoaki. 2006. In-
frastructure for Standardization of Asian Languages 
Resources. In Proceedings of the COLING/ACL 
2006, Sydney, 827-834. 
The 6th Workshop on Asian Languae Resources, 2008
8
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 191?198,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Modified Joint Source-Channel Model for Transliteration 
 
 
Asif Ekbal 
Comp.  Sc. & Engg. Deptt. 
 Jadavpur University 
India 
ekbal_asif12@ 
yahoo.co.in 
Sudip Kumar Naskar 
Comp.  Sc. & Engg. Deptt. 
Jadavpur University 
India 
sudip_naskar@ 
hotmail.com 
Sivaji Bandyopadhyay 
Comp.  Sc. & Engg. Deptt.  
Jadavpur University 
India 
sivaji_cse_ju@ 
yahoo.com 
 
 
Abstract 
Most machine transliteration systems 
transliterate out of vocabulary (OOV) 
words through intermediate phonemic 
mapping. A framework has been 
presented that allows direct 
orthographical mapping between two 
languages that are of different origins 
employing different alphabet sets. A 
modified joint source?channel model 
along with a number of alternatives have 
been proposed. Aligned transliteration 
units along with their context are 
automatically derived from a bilingual 
training corpus to generate the 
collocational statistics. The transliteration 
units in Bengali words take the pattern 
C+M where C represents a vowel or a 
consonant or a conjunct and M represents 
the vowel modifier or matra. The English 
transliteration units are of the form C*V* 
where C represents a consonant and V 
represents a vowel. A Bengali-English 
machine transliteration system has been 
developed based on the proposed models. 
The system has been trained to 
transliterate person names from Bengali 
to English. It uses the linguistic 
knowledge of possible conjuncts and 
diphthongs in Bengali and their 
equivalents in English. The system has 
been evaluated and it has been observed 
that the modified joint source-channel 
model performs best with a Word 
Agreement Ratio of 69.3% and a 
Transliteration Unit Agreement Ratio of 
89.8%.    
1 Introduction 
In Natural Language Processing (NLP) 
application areas such as information retrieval, 
question answering systems and machine 
translation, there is an increasing need to 
translate OOV words from one language to 
another. They are translated through 
transliteration, the method of translating into 
another language by expressing the original 
foreign words using characters of the target 
language preserving the pronunciation in their 
original languages. Thus, the central problem in 
transliteration is predicting the pronunciation of 
the original word. Transliteration between two 
languages, that use the same set of alphabets, is 
trivial: the word is left as it is. However, for 
languages that use different alphabet sets, the 
names must be transliterated or rendered in the 
target language alphabets.  
Technical terms and named entities make up 
the bulk of these OOV words. Named entities 
hold a very important place in NLP applications. 
Proper identification, classification and 
translation of named entities are very crucial in 
many NLP applications and pose a very big 
challenge to NLP researchers. Named entities are 
usually not found in bilingual dictionaries and 
they are very productive in nature. Translation of 
named entities is a tricky task: it involves both 
translation and transliteration. Transliteration is 
commonly used for named entities, even when 
the words could be translated. Different types of 
named entities are translated differently. 
Numerical and temporal expressions typically 
use a limited set of vocabulary words (e.g., 
names of months, days of the week etc.) and can 
be translated fairly easily using simple 
translation patterns. The named entity machine 
transliteration algorithms presented in this work 
191
focus on person names, locations and 
organizations. A machine transliteration system 
that is trained on person names is very important 
in a multilingual country like India where large 
name collections like census data, electoral roll 
and railway reservation information must be 
available to multilingual citizens of the country 
in their vernacular. In the present work, the 
various proposed models have been evaluated on 
a training corpus of person names. 
A hybrid neural network and knowledge-based 
system to generate multiple English spellings for 
Arabic personal names is described in (Arbabi et 
al., 1994). (Knight and Graehl, 1998) developed 
a phoneme-based statistical model using finite 
state transducer that implements transformation 
rules to do back-transliteration. (Stalls and 
Knight, 1998) adapted this approach for back 
transliteration from Arabic to English for English 
names. A spelling-based model is described in 
(Al-Onaizan and Knight, 2002a; Al-Onaizan and 
Knight, 2002c) that directly maps English letter 
sequences into Arabic letter sequences with 
associated probability that are trained on a small 
English/Arabic name list without the need for 
English pronunciations. The phonetics-based and 
spelling-based models have been linearly 
combined into a single transliteration model in 
(Al-Onaizan and Knight, 2002b) for 
transliteration of Arabic named entities into 
English.  
Several phoneme-based techniques have been 
proposed in the recent past for machine 
transliteration using transformation-based 
learning algorithm (Meng et al, 2001; Jung et 
al., 2000; Vigra and Khudanpur, 2003). 
(Abduljaleel and Larkey, 2003) have presented a 
simple statistical technique to train an English-
Arabic transliteration model from pairs of names. 
The two-stage training procedure first learns 
which n-gram segments should be added to 
unigram inventory for the source language, and 
then a second stage learns the translation model 
over this inventory. This technique requires no 
heuristic or linguistic knowledge of either 
language. 
 (Goto et al, 2003) described an English-
Japanese transliteration method in which an 
English word is divided into conversion units 
that are partial English character strings in an 
English word and each English conversion unit is 
converted into a partial Japanese Katakana 
character string. It calculates the likelihood of a 
particular choice of letters of chunking into 
English conversion units for an English word by 
linking them to Katakana characters using 
syllables. Thus the English conversion units 
consider phonetic aspects. It considers the 
English and Japanese contextual information 
simultaneously to calculate the plausibility of 
conversion from each English conversion unit to 
various Japanese conversion units using a single 
probability model based on the maximum 
entropy method. 
 (Haizhou et al, 2004) presented a framework 
that allows direct orthographical mapping 
between English and Chinese through a joint 
source-channel model, called n-gram 
transliteration model. The orthographic 
alignment process is automated using the 
maximum likelihood approach, through the 
Expectation Maximization algorithm to derive 
aligned transliteration units from a bilingual 
dictionary. The joint source-channel model tries 
to capture how source and target names can be 
generated simultaneously, i.e., the context 
information in both the source and the target 
sides are taken into account. 
A tuple n-gram transliteration model (Marino 
et al, 2005; Crego et al, 2005) has been log-
linearly combined with feature functions to 
develop a statistical machine translation system 
for Spanish-to-English and English-to-Spanish 
translation tasks. The model approximates the 
joint probability between source and target 
languages by using trigrams. 
The present work differs from (Goto et al, 
2003; Haizhou et al, 2004) in the sense that 
identification of the transliteration units in the 
source language is done using regular 
expressions and no probabilistic model is used. 
The proposed modified joint source-channel 
model is similar to the model proposed by (Goto 
et. al., 2003) but it differs in the way the 
transliteration units and the contextual 
information are defined in the present work. No 
linguistic knowledge is used in (Goto et al, 
2003; Haizhou et al, 2004) whereas the present 
work uses linguistic knowledge in the form of 
possible conjuncts and diphthongs in Bengali. 
The paper is organized as follows. The 
machine transliteration problem has been 
formulated under both noisy-channel model and 
joint source-channel model in Section 2. A 
number of transliteration models based on 
collocation statistics including the modified joint 
source-channel model and their evaluation 
scheme have been proposed in Section 3. The 
Bengali-English machine transliteration scenario 
has been presented in Section 4. The proposed 
192
models have been evaluated and the result of 
evaluation is reported in Section 5. The 
conclusion is drawn in Section 6. 
2 Machine Transliteration and Joint 
Source-Channel Model 
A transliteration system takes as input a character 
string in the source language and generates a 
character string in the target language as output. 
The process can be conceptualized as two levels 
of decoding: segmentation of the source string 
into transliteration units; and relating the source 
language transliteration units with units in the 
target language, by resolving different 
combinations of alignments and unit mappings. 
The problem of machine transliteration has been 
studied extensively in the paradigm of the noisy 
channel model.  
For a given Bengali name B as the observed 
channel output, we have to find out the most 
likely English transliteration E that maximizes 
P(E?B). Applying Bayes? rule, it means to find 
E to maximize 
  P(B,E) = P(B?E) * P(E)                             (1) 
with equivalent effect. This is equivalent to 
modelling two probability distributions: P(B|E), 
the probability of transliterating E to B through a 
noisy channel, which is also called 
transformation rules, and P(E), the probability 
distribution of source, which reflects what is 
considered good English transliteration in 
general. Likewiswe, in English to Bengali (E2B) 
transliteration, we could find B that maximizes 
P(B,E) = P(E?B) * P(B)                               (2) 
for a given English name. In equations (1) and 
(2), P(B) and P(E) are usually estimated using n-
gram language models. Inspired by research 
results of grapheme-to-phoneme research in 
speech synthesis literature, many have suggested 
phoneme-based approaches to resolving P(B?E) 
and P(E?B), which approximates the probability 
distribution by introducing a phonemic 
representation. In this way, names in the source 
language, say B, are converted into an 
intermediate phonemic representation P, and then 
the phonemic representation is further converted 
into the target language, say English E. In 
Bengali to English (B2E) transliteration, the 
phoneme-based approach can be formulated as 
P(E?B) = P(E?P) * P(P?B) and conversely we 
have P(B?E) = P(B?P) * P(P?E) for E2B back-
transliteration. 
However, phoneme-based approaches are 
limited by a major constraint that could 
compromise transliteration precision. The 
phoneme-based approach requires derivation of 
proper phonemic representation for names of 
different origins. One may need to prepare 
multiple language-dependent grapheme-to-
phoneme(G2P) and phoneme-to-grapheme(P2G) 
conversion systems accordingly, and that is not 
easy to achieve. 
In view of close coupling of the source and 
target transliteration units, a joint source-channel 
model, or n-gram transliteration model (TM) has 
been proposed in (Haizhou et al, 2004). For K 
alligned transliteration units, we have 
P(B,E) = P(
  
b1, b2.....bk, e1, e2......ek ) 
           = P (<b,e>1, <b,e>2, .....<b,e>k) 
              K   
           = ? P ( <b,e>k? <b,e>1k-1)               (3) 
              k=1 
which provides an alternative to the phoneme-
based approach for resolving equations (1) and 
(2) by eliminating the intermediate phonemic 
representation. 
Unlike the noisy-channel model, the joint 
source-channel model does not try to capture 
how source names can be mapped to target 
names, but rather how source  and target names 
can be generated simultaneously. In other words, 
a joint probability model is estimated  that can be 
easily marginalized in order to yield conditional 
probability models for both transliteration  and 
back-transliteration. 
Suppose that we have a Bengali name ? = 
x1x2............xm  and an English transliteration ? = 
y1y2........yn where xi, i = 1: m are Bengali 
transliteration units and yj, j = 1: n are English 
transliteration units. An English transliteration 
unit may correspond to zero, one or more than 
one transliteration unit in Bengali. Often the 
values of m and n are different. 
 
x1 x2x3..... xi-1xixi+1....xm 
      
 
         y1      y2 ..yi .... yn 
 
where there exists an alignment ? with <b,e>1 
= <x1,y1>; <b,e>2 = <x2x3, y2>; ?. and <b,e>k = 
<xm,yn>. A transliteration unit correspondence 
<b, e> is called a transliteration pair. Thus B2E 
transliteration can be formulated as    
 
         ?  = argmax P (?, ?, ? )          (4) 
                   ?, ?  
 
and similarly the E2B back-transliteration as  
193
  ?   = argmax P (?, ?, ? )         (5) 
                   ?, ?  
An n-gram transliteration model is defined as 
the conditional probability or transliteration 
probability of a transliteration pair <b, e>k 
depending on its immediate n predecessor pairs: 
 
  P (B, E) = P (?, ?, ?) 
                         
               K   
           = ? P ( <b, e>k? <b, e>k-n+1k-1)     (6) 
             k=1   
3 Proposed Models and Evaluation 
Scheme 
  Machine transliteration has been viewed as a 
sense disambiguation problem. A number of 
transliteration models have been proposed that 
can generate the English transliteration from a 
Bengali word that is not registered in any 
bilingual or pronunciation dictionary. The 
Bengali word is divided into Transliteration 
Units (TU) that have the pattern C+M, where C 
represents a vowel or a consonant or conjunct 
and M represents the vowel modifier or matra. 
An English word is divided into TUs that have 
the pattern C*V*, where C represents a 
consonant and V represents a vowel. The TUs 
are considered as the lexical units for machine 
transliteration. The system considers the Bengali 
and English contextual information in the form 
of collocated TUs simultaneously to calculate the 
plausibility of transliteration from each Bengali 
TU to various English candidate TUs and 
chooses the one with maximum probability. This 
is equivalent to choosing the most appropriate 
sense of a word in the source language to identify 
its representation in the target language. The 
system learns the mappings automatically from 
the bilingual training corpus guided by linguistic 
features. The output of this mapping process is a 
decision-list classifier with collocated TUs in the 
source language and their equivalent TUs in 
collocation in the target language along with the 
probability of each decision obtained from a 
training corpus. The machine transliteration of 
the input Bengali word is obtained using direct 
orthographic mapping by identifying the 
equivalent English TU for each Bengali TU in 
the input and then placing the English TUs in 
order. The various proposed models differ in the 
nature of collocational stastistics used during 
machine transliteration process: monogram 
model with no context, bigram model with 
previous (with respect to the current TU to be 
transliterated) source TU as the context, bigram 
model with next source TU as the context, 
bigram model with previous source and target 
TUs as the context (this is the joint source 
channel model), trigram model with previous and 
next source TUs as the context and the modified 
joint source-channel model with previous and 
next source TUs and the previous target TU as 
the context.  
 
? Model A 
 
In this model, no context is considered in 
either the source or the target side. This is 
essentially the monogram model. 
                K 
P(B,E) = ? P(<b,e>k) 
                k=1 
 
? Model B 
 
This is essentially a bigram model with 
previous source TU, i.e., the source TU occurring 
to the left of the current TU to be transliterated, 
as the context. 
                K 
P(B,E) = ? P(<b,e>k | bk-1) 
              k=1  
 
?Model C 
 
 This is  essentially a bigram model with next 
source TU, i.e., the source TU occurring to the 
right of the current TU to be transliterated, as the 
context. 
                K 
P(B,E) =  ?  P(<b,e>k? bk+1 )           
               k=1   
 
? Model D 
 
This is essentially the joint source-channel 
model where the previous TUs in both the source 
and the target sides are considered as the context. 
The previous TU on the target side refers to the 
transliterated TU to the immediate left of the 
current target TU to be transliterated. 
                 K 
P(B,E) =  ? P( <b,e>k ?? | <b,e>k-1) 
                k=1 
 
 
 
194
? Model E 
 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context  
                K 
P(B,E) =  ? P(<b,e>k | bk-1, bk+1) 
                k=1 
  
? Model F 
 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the modified 
joint source-channel model . 
                K 
P(B,E) = ? P (<b,e>k | <b,e>k-1, bk+1) 
              k=1  
 
The performance of the system is evaluated in 
terms of Transliteration Unit Agreement Ratio 
(TUAR) and Word Agreement Ratio (WAR) 
following the evaluation scheme in (Goto et al, 
2003). The evaluation parameter Character 
Agreement Ratio in (Goto et al, 2003) has been 
modified to Transliteration Unit Agreement 
Ratio as vowel modifier matra symbols in 
Bengali words are not independent and must 
always follow a consonant or a conjunct in a 
Transliteration Unit. Let, B be the input Bengali 
word, E be the English transliteration given by 
the user in open test and E/ be the system 
generates the transliteration..TUAR is defined as, 
TUAR = (L-Err)/ L, where L is the number of 
TUs in E, and Err is the number of wrongly 
transliterated TUs in E/ generated by the system. 
WAR is defined as, WAR= (S-Err/) / S, where S 
is the test sample size and Err/ is is the number of 
erroneous names generated by the system (when 
E/ does not match with E). Each of these models 
has been evaluated with linguistic knowledge of 
the set of possible conjuncts and diphthongs in 
Bengali and their equivalents in English. It has 
been observed that the Modified Joint Source 
Channel Model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
and Transliteration Unit Agreement Ratio. 
4 Bengali-English Machine 
Transliteration 
Translation of named entities is a tricky task: it 
involves both translation and transliteration. 
Transliteration is commonly used for named 
entities, even when the words could be translated 
[LXT?? V_ (janata dal) is translated to Janata Dal 
(literal translation) although LXT?? (Janata) and 
V_ (Dal) are vocabulary words]. On the other 
hand ^?V[?Y??[? ?[? ?`?[?V??_?^  (jadavpur 
viswavidyalaya) is translated to Jadavpur 
University in which ^?V[?Y??[? (Jadavpur) is 
transliterated to Jadavpur and ?[? ?`?[?V??_?^  
(viswavidyalaya) is translated to University.  
A bilingual training corpus has been kept that 
contains entries mapping Bengali names to their 
respective English transliterations. To 
automatically analyze the bilingual training 
corpus to acquire knowledge in order to map new 
Bengali names to English, TUs are extracted 
from the Bengali names and the corresponding 
English names, and Bengali TUs are associated 
with their English counterparts. 
Some examples are given below: 
%?\?X?VX (abhinandan) ? [% | ?\? | X | ?V | X] 
abhinandan  ? [a | bhi | na | nda | n ]  
E??b?]??T?? (krishnamoorti) ?  [E?? | b? | ]? | ?T??]  
krishnamurthy ? [ kri | shna | mu | rthy ]  
?`?E????? (srikant) ? [ ?`? | E?? | ??? ] 
srikant ? [ sri | ka | nt ]  
 
After retrieving the transliteration units from a 
Bengali-English name pair, it associates the     
Bengali TUs to the English TUs along with the 
TUs in context. 
For example, it derives the following 
transliteration pairs or rules from the name-pair: 
?[??[???V?X?U (rabindranath)  ?   rabindranath 
  
Source Language                 Target Language 
                      
previous TU  TU  next TU       previous TU    TU        
          -            ?[?      [??   ?       -                ra 
     ?[          [??     ?V?  ?           ra               bi  
     [??      ?V?     X?   ?        bi             ndra  
          ?V?      X?     U    ?       ndra            na 
        X?      U       -    ?        na              th 
                                              
195
But, in some cases, the number of 
transliteration units retrieved from the Bengali 
and English words may differ. The [ [??L?]?c?X 
(brijmohan) ? brijmohan ] name pair yields  5 
TUs  in Bengali side and  4 TUs in English side   
[ [?? | L | ?]? | c? | X ?  bri | jmo | ha | n]. In such 
cases, the system cannot align the TUs 
automatically and linguistic   knowledge is used 
to resolve the confusion. A knowledge base that 
contains a list of Bengali conjuncts and 
diphthongs and their possible English 
representations has been kept. The hypothesis 
followed in the present work is that the problem 
TU in the English side has always the maximum 
length.  If more than one English TU has the 
same length, then system starts its analysis from 
the first one.  In the above example, the TUs bri 
and jmo have the same length. The system 
interacts with the knowledge base and ascertains 
that bri is valid and jmo cannot be a valid TU in 
English since there is no corresponding conjunct 
representation in Bengali. So jmo is split up into 
2 TUs j and mo, and the system aligns the 5 TUs 
as [[?? | L | ?]? | c? | X ?  bri | j | mo | ha | n]. 
Similarly, [?_?E?X?U (loknath) ? loknath] is 
initially split as [ ?_? | E? | X? | U ]   ?   lo | kna | 
th], and then as [ lo | k | na | th ] since kna has the 
maximum length and it does not have any valid 
conjunct representation in Bengali. 
In some cases, the knowledge of Bengali 
diphthong resolves the problem. In the following           
example, [ ?[?? | + | ]? (raima) ? rai | ma], the 
number of TUs on both sides do not                  
match. The English TU rai is chosen for analysis 
as its length is greater than the other TU ma. The 
vowel sequence ai corresponds to a diphthong in 
Bengali that has two valid representations < %?+, 
B >. The first representation signifies that a 
matra is associated to the previous character 
followed by the character +. This matches the 
present Bengali input. Thus, the English vowel 
sequence ai is separated from the TU rai (rai ? r 
| ai) and the intermediate form of the name pair 
appears to be [?[?? | + | ]? (raima) ? r | ai | ma].  
Here, a matra is associated with the Bengali TU 
that corresponds to English TU r and so there 
must be a vowel attached with the TU r. TU ai is 
further splitted as a and i (ai ? a | i) and the first 
one (i.e. a) is assimilated with the previous TU 
(i.e. r) and finally the name pair appears as: [ ?[?? | 
+ | ]? (raima) ? ra | i | ma]. 
In the following two examples, the number of 
TUs on both sides does not match. 
[ ?V | [? | ?[?? | L (devraj)    ?   de | vra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | mna | th] 
 
It is observed that both vr and mn represent 
valid conjuncts in Bengali but these examples 
contain the constituent Bengali consonants in 
order and not the conjunct representation. During 
the training phase, if, for some conjuncts, 
examples with conjunct representation are 
outnumbered by examples with constituent 
consonants representation, the conjunct is 
removed from the linguistic knowledge base and 
training examples with such conjunct 
representation are moved to a Direct example 
base which contains the English words and their 
Bengali transliteration. The above two name 
pairs can then be realigned as  
[ ?V | [? | ?[?? | L (devraj)    ?   de | v | ra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | m | na | th] 
 
Otherwise, if such conjuncts are included in 
the linguistic knowledge base, training examples 
with constituent consonants representation are to 
be moved to the Direct example base. 
The Bengali names and their English 
transliterations are split into TUs in such a way 
that, it   results in a one-to-one correspondence 
after using the linguistic information. But in 
some       cases there exits zero-to-one or many-
to-one relationship. An example of Zero-to-One 
relationship [? ? h] is the name-pair [%? | {? 
(alla) ?  a | lla | h] while the name-pair [%? | + | 
?\? (aivy)   ? i | vy] is an example of Many-to-
One relationship [%?, + ? i]. These bilingual 
examples should also be included in the Direct 
example base. 
In some cases, the linguistic knowledge 
apparently solves the mapping problem, but not        
always. From the name-pair [[??[?F? (barkha) ? 
barkha], the system initially generates the       
mapping [[? | ?[? | F? ? ba | rkha] which is not 
one-to-one. Then it consults the linguistic          
knowledge base and breaks up the transliteration 
unit as (rkha ? rk | ha ) and generates the final 
196
aligned transliteration pair [[? | ?[? | F? ? ba | rk | 
ha ] (since it finds out that rk has a valid conjunct 
representation in Bengali but not rkh), which is 
an incorrect transliteration pair to train   the 
system. It should have been [[? | ?[? | F? ?  ba | r | 
kha]. Such type of errors can be detected by 
following the alignment process from the target 
side during the training phase. Such training 
examples may be either manually aligned or 
maintained in the Direct Example base. 
5 Results of the Proposed Models 
Approximately 6000 Indian person names have 
been collected and their English transliterations 
have been stored manually. This set acts as the 
training corpus on which the system is trained to 
generate the collocational statistics. These 
statistics serve as the decision list classifier to 
identify the target language TU given the source 
language TU and its context. The system also 
includes the linguistic knowledge in the form of 
valid conjuncts and diphthongs in Bengali and 
their English representation.  
All the models have been tested with an open 
test corpus of about 1200 Bengali names that 
contains their English transliterations. The total 
number of transliteration units (TU) in these 
1200 (Sample Size, i.e., S) Bengali names is 
4755 (this is the value of L), i.e., on an average a 
Bengali name contains 4 TUs. The test set was 
collected from users and it was checked that it 
does not contain names that are present in the 
training set. The total number of transliteration 
unit errors (Err) in the system-generated 
transliterations and the total number of words 
erroneously generated (Err/) by the system have 
been shown in Table 1 for each individual model. 
The models are evaluated on the basis of the two 
evaluation metrics, Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). The results of the tests in terms 
of the evaluation metrics are shown in Table 2. 
The modified joint source-channel model (Model 
F) that incorporates linguistic knowledge 
performs best among all the models with a Word 
Agreement Ratio (WAR) of 69.3% and a 
Transliteration Unit Agreement Ratio (TUAR) of 
89.8%. The joint source-channel model with 
linguistic knowledge (Model D) has not 
performed well in the Bengali-English machine 
transliteration whereas the trigram model (Model 
E) needs further attention as its result are 
comparable to the modified joint source-channel 
model (Model F). All the models were also tested 
for back-transliteration, i.e., English to Bengali 
transliteration, with an open test corpus of 1000 
English names that contain their Bengali 
transliterations. The results of these tests in terms 
of the evaluation metrics WAR and TUAR are 
shown in Table 3. It is observed that the 
modified joint source-channel model performs 
best in back-transliteration with a WAR of 
67.9% and a TUAR of 89%.  
 
Model Error in TUs 
(Err) 
Error words 
(Err/) 
A 990 615 
B 795 512 
C 880 532 
D 814 471 
E 604 413 
F 486 369 
 
Table 1: Value of Err and Err/ for each model 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 48.8 79.2 
B 57.4 83.3 
C 55.7 81.5 
D 60.8 82.9 
E 65.6 87.3 
F 69.3 89.8 
 
Table 2: Results with Evaluation Metrics 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 49.6 79.8 
B 56.2 83.8 
C 53.9 82.2 
D 58.2 83.2 
E 64.7 87.5 
F 67.9 89.0 
 
Table 3: Results with Evaluation Metrics 
(E2B transliteration) 
6.    Conclusion 
It has been observed that the modified joint 
source-channel model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). Detailed examination of the 
197
evaluation results reveals that Bengali has 
separate short and long vowels and the 
corresponding matra representation while these 
may be represented in English by the same 
vowel. It has been observed that most of the 
errors are at the matra level i.e., a short matra 
might have been replaced by a long matra or vice 
versa. More linguistic knowledge is necessary to 
disambiguate the short and the long vowels and 
the matra representation in Bengali. The system 
includes conjuncts and diphthongs as part of the 
linguistic knowledge base. Triphthongs or 
tetraphthongs usually do not appear in Indian 
names. But, inclusion of them will enable the 
system to transliterate those few names that may 
include them. The models are to be trained 
further on sets of additional person names from 
other geographic areas. Besides person names, 
location and organization names are also to be 
used for training the proposed models. 
Acknowledgement 
Our thanks go to Council of Scientific and 
Industrial Research, Human Resource 
Development Group, New Delhi, India for 
supporting Sudip Kumar Naskar under Senior 
Research Fellowship Award (9/96(402) 2003-
EMR-I). 
References 
Abdul Jaleel Nasreen and Leah S. Larkey. 2003. 
Statistical Transliteration for English-Arabic Cross 
Language Information Retrieval. Proceedings of 
the Twelfth International Conference on 
Information and Knowledge Management (CIKM 
2003), New Orleans, USA, 139-146. 
Al-Onaizan Y. and Knight K. 2002a. Named Entity 
Translation: Extended Abstract. Proceedings of the 
Human Language Technology Conference (HLT 
2002), 122-124. 
Al-Onaizan Y. and Knight K.2002b. Translating 
Named Entities Using Monolingual and Bilingual 
Resources.  Proceedings of the 40th Annual 
Meeting of the ACL (ACL 2002), 400-408. 
Al-Onaizan Y. and Knight K. 2002c. Machine 
Transliteration of Names in Arabic Text. 
Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages. 
Arbabi Mansur, Scott M. Fischthal, Vincent C. 
Cheng, and Elizabeth Bar. 1994. Algorithms for 
Arabic name transliteration. IBM Journal of 
Research and Development, 38(2): 183-193. 
Crego J.M., Marino J.B. and A. de Gispert. 2005. 
Reordered Search and Tuple Unfolding for Ngram-
based SMT. Proceedings of the MT-Summit X, 
Phuket, Thailand, 283-289. 
Marino J. B., Banchs R., Crego J. M., A. de Gispert, 
P.  Lambert, J. A. Fonollosa and M. Ruiz, Bilingual 
N-gram Statistical Machine Translation.  
Proceedings of the MT-Summit X, Phuket, 
Thailand, 275-282. 
Goto I., N. Kato, N. Uratani, and T. Ehara. 2003. 
Transliteration considering Context Information 
based on the Maximum Entropy Method. 
Proceeding of the MT-Summit IX, New Orleans, 
USA, 125?132. 
Haizhou Li, Zhang Min, Su Jian. 2004. A Joint 
Source-Channel Model for Machine 
Transliteration. Proceedings of the 42nd Annual 
Meeting of the ACL (ACL 2004), Barcelona, 
Spain, 159-166. 
Jung Sung Young, Sung Lim Hong, and Eunok Paek. 
2000. An English to Korean Transliteration Model 
of Extended Markov Window. Proceedings of 
COLING 2000, 1, 383-389. 
Knight K. and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics, 24(4): 
599-612. 
Meng Helen M., Wai-Kit Lo, Berlin Chen and Karen 
Tang. 2001. Generating Phonetic Cognates to 
handle Name Entities in English-Chinese Cross-
language Spoken Document Retrieval. Proceedings 
of the Automatic Speech Recognition and 
Understanding (ASRU) Workshop, Trento, Italy. 
Stalls, Bonnie Glover and Knight K. 1998. 
Translating names and technical terms in Arabic 
text. Proceedings of the COLING/ACL Workshop 
on Computational Approaches to Semitic 
Languages, Montral, Canada, 34-41. 
Virga Paola and Sanjeev Khudanpur. 2003. 
Transliteration of Proper Names in Crosslingual 
Information Retrieval. Proceedings of the ACL 
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, Sapporo, 
Japan, 57-60.  
 
198
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 80?83,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English to Hindi Machine Transliteration System at NEWS 2009 
 
Amitava Das, Asif Ekbal, Tapabrata Mandal and Sivaji Bandyopadhyay 
Computer Science and Engineering Department 
Jadavpur University, Kolkata-700032, India 
amitava.research@gmail.com, asif.ekbal@gmail.com, ta-
pabratamondal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2009 Machine Transliteration Shared 
Task held as part of ACL-IJCNLP 2009. We 
submitted one standard run and two non-
standard runs for English to Hindi translitera-
tion. The modified joint source-channel model 
has been used along with a number of alterna-
tives. The system has been trained on the 
NEWS 2009 Machine Transliteration Shared 
Task datasets. For standard run, the system 
demonstrated an accuracy of 0.471 and the 
mean F-Score of 0.861. The non-standard runs 
yielded the accuracy and mean F-scores of 
0.389 and 0.831 respectively in the first one 
and 0.384 and 0.828 respectively in the second 
one. The non-standard runs resulted in sub-
stantially worse performance than the standard 
run. The reasons for this are the ranking algo-
rithm used for the output and the types of to-
kens present in the test set. 
1 Introduction 
Technical terms and named entities (NEs) consti-
tute the bulk of the Out Of Vocabulary (OOV) 
words. Named entities are usually not found in 
bilingual dictionaries and are very generative in 
nature. Proper identification, classification and 
translation of Named entities (NEs) are very im-
portant in many Natural Language Processing 
(NLP) applications. Translation of NEs involves 
both translation and transliteration. Translitera-
tion is the method of translating into another lan-
guage by expressing the original foreign word 
using characters of the target language preserv-
ing the pronunciation in their source language. 
Thus, the central problem in transliteration is 
predicting the pronunciation of the original word. 
Transliteration between two languages that use 
the same set of alphabets is trivial: the word is 
left as it is. However, for languages those use 
different alphabet sets the names must be transli-
terated or rendered in the target language alpha-
bets. Transliteration of NEs is necessary in many 
applications, such as machine translation, corpus 
alignment, cross-language Information Retrieval, 
information extraction and automatic lexicon 
acquisition. In the literature, a number of transli-
teration algorithms are available involving Eng-
lish (Li et al, 2004; Vigra and Khudanpur, 2003; 
Goto et al, 2003), European languages (Marino 
et al, 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). 
 
2 Machine Transliteration Systems  
Three transliteration models have been used that 
can generate the Hindi transliteration from an 
English named entity (NE). An English NE is 
divided into Transliteration Units (TUs) with 
patterns C*V*, where C represents a consonant 
and V represents a vowel. The Hindi NE is di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the lexical units for machine transli-
teration. The system considers the English and 
Hindi contextual information in the form of col-
located TUs simultaneously to calculate the plau-
sibility of transliteration from each English TU 
to various Hindi candidate TUs and chooses the 
one with maximum probability. This is equiva-
lent to choosing the most appropriate sense of a 
word in the source language to identify its repre-
sentation in the target language. The system 
learns the mappings automatically from the bi-
lingual NEWS training set being guided by lin-
80
guistic features/knowledge. The system consid-
ers the linguistic knowledge in the form of con-
juncts and/or diphthongs in English and their 
possible transliteration in Hindi. The output of 
the mapping process is a decision-list classifier 
with collocated TUs in the source language and 
their equivalent TUs in collocation in the target 
language along with the probability of each deci-
sion obtained from the training set. Linguistic 
knowledge is used in order to make the number 
of TUs in both the source and target sides equal. 
A Direct example base has been maintained that 
contains the bilingual training examples that do 
not result in the equal number of TUs in both the 
source and target sides during alignment. The 
Direct example base is checked first during ma-
chine transliteration of the input English word. If 
no match is obtained, the system uses direct or-
thographic mapping by identifying the equivalent 
Hindi TU for each English TU in the input and 
then placing the Hindi TUs in order. The transli-
teration models are described below in which S 
and T denotes the source and the target words 
respectively: 
 
? Model A 
This is essentially the joint source-channel model 
(Hazhiou et al, 2004) where the previous TUs 
with reference to the current TUs in both the 
source (s) and the target sides (t) are considered 
as the context.  
1
1
( | ) ( , | , )k k
k
K
P S T P s t s t
?
=
= < > < >?  
( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model B 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context.  
 1, 1
1
( | ) ( , | )k k k
k
K
P S T P s t s s
? +
=
= < >?  
  ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model C 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the  improved 
modified joint source-channel model. 
1, 1
1
( | ) ( , | , )k k k
k
K
P S T P s t s t s
? +
=
= < > < >?   
 ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?               
For NE transliteration, P(T), i.e., the 
probability of transliteration in the target 
language, is calculated from a English-Hindi 
bilingual database of approximately 961,890 
English person names, collected from the web1.  
If, T is not found in the dictionary, then a very 
small value is assigned to P(T). These models 
have been desribed in details in Ekbal et al 
(2007). 
 
? Post-Processing 
Depending upon the nature of errors involved in 
the results, we have devised a set of translitera-
tion rules. A few rules have been devised to pro-
duce more spelling variations. Some examples 
are given below. 
Spelling variation rules 
Badlapur ??????? | ??????? 
Shree | Shri ? 
 
3 Experimental Results   
We have trained our transliteration models using 
the English-Hindi datasets obtained from the 
NEWS 2009 Machine Transliteration Shared 
Task (Li et al, 2009). A brief statistics of the 
datasets are presented in Table 1. Out of 9975 
English-Hindi parallel examples in the training 
set, 4009 are multi-words. During training, we 
have split these multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in 22 multi-words and these 
cases were not considered further. Following are 
some examples:  
Paris Charles de Gaulle ????  
???? ??? ? ?????  
South Arlington Church of 
Christ ???? ???? 
In the training set, some multi-words were partly 
translated and not transliterated. Such examples 
were dropped from the training set. Finally, the 
training set consists of 15905 single word Eng-
lish-Hindi parallel examples.  
                                                 
1http://www.eci.gov.in/DevForum/Fullname.asp  
81
      
Set Number of examples 
Training 9975 
Development 974 
Test 1000 
Table 1. Statistics of Dataset 
 
The output of the modified joint source-
channel model is given more priority during out-
put ranking followed by the trigram and the joint 
source-channel model. During testing, the Direct 
example base is searched first to find the transli-
teration. Experimental results on the develop-
ment set yielded the accuracy of 0.442 and mean 
F-score of 0.829. Depending upon the nature of 
errors involved in the results, we have devised a 
set of transliteration rules. The use of these trans-
literation rules increased the accuracy and mean 
F-score values up to 0.489 and 0.881 respective-
ly.  
The system has been evaluated for the test set 
and the detailed reports are available in Li et al 
(2009). There are 88.88% unknown examples in 
the test set. We submitted one standard run in 
which the outputs are provided for the modified 
joint source-channel model (Model C), trigram 
model (Model B) and joint source-channel model 
(Model A). The same ranking procedure (i.e., 
Model C, Model B and Model A) has been fol-
lowed as that of the development set. The output 
of each transliteration model has been post-
processed with the set of transliteration rules. For 
each word, three different outputs are provided in 
a ranked order. If the outputs of any two models 
are same for any word then only two outputs are 
provided for that particular word. Post-
processing rules generate more number of possi-
ble transliteration output. Evaluation results of 
the standard run are shown in Table 2.  
 
Parameters Accuracy 
Accuracy in top-1 0.471 
Mean F-score 0.861 
Mean Reciprocal Rank 
(MRR) 
0.519 
Mean Average Preci-
sion (MAP)ref 
0.463 
MAP10 0.162 
MAPsys 0.383 
Table 2. Results of the standard run  
 
The results of the two non-standard runs are 
presented in Table 3 and Table 4 respectively.  
Parameters Accuracy 
Accuracy in top-1 0.389 
Mean F-score 0.831 
Mean Reciprocal Rank 
(MRR) 
0.487 
Mean Average Preci-
sion (MAP)ref 
0.385 
MAP10 0.16 
MAPsys 0.328 
  
Table 3. Results of the non-standard run 1 
 
Parameters Accuracy 
Accuracy in top-1 0.384 
Mean F-score 0.823 
Mean Reciprocal Rank 
(MRR) 
0.485 
Mean Average Precision 
(MAP)ref 
0.380 
MAP10 0.16 
MAPsys 0.325 
 
Table 4. Results of the non-standard run2 
 
In both the non-standard runs, we have used 
an English-Hindi bilingual database of approx-
imately 961, 890 examples that have been col-
lected from the web2. This database contains the 
(frequency) of the corresponding English-Hindi 
name pair. Along with the outputs of three mod-
els, the output obtained from this bilingual data-
base has been also provided for each English 
word. In the first non-standard run, only the most 
frequent transliteration has been considered. But, 
in the second non-standard run all the possible 
transliteration have been considered. It is to be 
noted that in these two non-standard runs, the 
transliterations obtained from the bilingual data-
base have been kept first in the ranking. Results 
of the tables show quite similar performance in 
both the runs. But the non-standard runs resulted 
in substantially worse performance than the stan-
dard run. The reasons for this are the ranking 
algorithm used for the output and the types of 
tokens present in the test set. The additional da-
                                                 
2http://www.eci.gov.in/DevForum/Fullname.asp  
82
taset used for the non-standard runs is mainly 
census data consisting of only Indian person 
names. The NEWS 2009 Machine Transliteration 
Shared Task training set is well distributed with 
foreign names (Ex. Sweden, Warren), common 
nouns (Mahfuz, Darshanaa) and a few non 
named entities. Hence the training set for the 
non-standard runs was biased towards the Indian 
person name transliteration pattern. Additional 
training set was quite larger (961, 890) than the 
shared task training set (9,975). Actually outputs 
of non-standard runs have more alternative trans-
literation outputs than the standard set. That 
means non-standard sets are superset of standard 
set. Our observation is that the ranking algorithm 
used for the output and biased training are the 
main reasons for the worse performance of the 
non-standard runs. 
4 Conclusion  
This paper reports about our works as part of the 
NEWS 2009 Machine Transliteration Shared 
Task. We have used the modified joint source-
channel model along with two other alternatives 
to generate the Hindi transliteration from an Eng-
lish word (to generate more spelling variations of 
Hindi names). We have also devised some post-
processing rules to remove the errors. During 
standard run, we have obtained the word accura-
cy of 0.471 and mean F-score of 0.831. In non-
standard rune, we have used a bilingual database 
obtained from the web. The non-standard runs 
yielded the word accuracy and mean F-score 
values of 0.389 and 0.831 respectively in the first 
run and 0.384 and 0.823 respectively in the 
second run. 
 
References  
Al-Onaizan, Y. and Knight, K. 2002a. Named 
Entity Translation: Extended Abstract. In 
Proceedings of the Human Language Tech-
nology Conference, 122? 124. 
Al-Onaizan, Y. and Knight, K. 2002b. Translat-
ing Named Entities using Monolingual and 
Bilingual Resources. In Proceedings of the 
40th Annual Meeting of the ACL, 400?408, 
USA. 
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 
2007. Named Entity Transliteration. Interna-
tional Journal of Computer Processing of 
Oriental Languages (IJCPOL), Volume 
(20:4), 289-310, World Scientific Publishing 
Company, Singapore. 
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 
2006. A Modified Joint Source Channel 
Model for Transliteration. In Proceedings of 
the COLING-ACL 2006, 191-198, Australia. 
Goto, I., Kato, N., Uratani, N. and Ehara, T. 
2003. Transliteration Considering Context 
Information based on the Maximum Entropy 
Method. In Proceeding of the MT-Summit 
IX, 125?132, New Orleans, USA.  
Jung, Sung Young , Sung Lim Hong and Eunok 
Paek. 2000. An English to Korean Translite-
ration Model of Extended Markov Window. 
In Proceedings of International Conference 
on Computational Linguistics (COLING 
2000), 383-389. 
Knight, K. and Graehl, J. 1998. Machine Transli-
teration, Computational Linguistics, Volume 
(24:4), 599?612. 
Kumaran, A. and Tobias Kellner. 2007. A gener-
ic framework for machine transliteration. In 
Proc. of the 30th SIGIR. 
Li, Haizhou, A Kumaran, Min Zhang and Vla-
dimir Pervouchine. 2009. Whitepaper of 
NEWS 2009 Machine Transliteration Shared 
Task. In Proceedings of ACL-IJCNLP 2009 
Named Entities Workshop (NEWS 2009), Sin-
gapore. 
Li, Haizhou, A Kumaran, Vladimir Pervouchine 
and Min Zhang. 2009.  Report on NEWS 2009 
Machine Transliteration Shared Task. In Pro-
ceedings of ACL-IJCNLP 2009  amed Entities 
Workshop (NEWS 2009), Singapore. 
Li, Haizhou, Min Zhang and Su Jian. 2004. A 
Joint Source-Channel Model for Machine 
Transliteration. In Proceedings of the 42nd 
Annual Meeting of the ACL, 159-166. Spain. 
Marino, J. B., R. Banchs, J. M. Crego, A. de 
Gispert, P. Lambert, J. A. Fonollosa and M. 
Ruiz. 2005.  Bilingual n-gram Statistical 
Machine Translation. In Proceedings of the 
MT-Summit X, 275?282. 
Surana, Harshit, and Singh, Anil Kumar. 2008. A 
More Discerning and Adaptable Multilingual 
Transliteration Mechanism for Indian Lan-
guages. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 64-71, In-
dia. 
Vigra, Paola and Khudanpur, S. 2003. Translite-
ration of Proper Names in Cross-Lingual In-
formation Retrieval. In Proceedings of the 
ACL 2003 Workshop on Multilingual and 
Mixed-Language Named Entity Recognition, 
57?60. 
83
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 202?210,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Voted NER System using Appropriate Unlabeled Data 
 
Asif Ekbal 
Dept. of Computer Science &Engg., 
Jadavpur University, Kolkata-700032, 
India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Dept. of Computer Science &Engg., 
Jadavpur University, Kolkata-700032, 
India 
sivaji_cse_ju@yahoo.com 
 
Abstract 
 
This paper reports a voted Named Entity Rec-
ognition (NER) system with the use of appro-
priate unlabeled data. The proposed method is 
based on the classifiers such as Maximum En-
tropy (ME), Conditional Random Field (CRF) 
and Support Vector Machine (SVM) and has 
been tested for Bengali. The system makes use 
of the language independent features in the 
form of different contextual and orthographic 
word level features along with the language 
dependent features extracted from the Part of 
Speech (POS) tagger and gazetteers. Context 
patterns generated from the unlabeled data us-
ing an active learning method have been used 
as the features in each of the classifiers. A 
semi-supervised method has been used to de-
scribe the measures to automatically select ef-
fective documents and sentences from unla-
beled data. Finally, the models have been 
combined together into a final system by 
weighted voting technique. Experimental re-
sults show the effectiveness of the proposed 
approach with the overall Recall, Precision, 
and F-Score values of 93.81%, 92.18% and 
92.98%, respectively. We have shown how the 
language dependent features can improve the 
system performance. 
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Process-
ing (NLP) application areas. Machine learning 
(ML) approaches are more popularly used in 
NER because these are easily trainable, adopt-
able to different domains and languages as well 
as their maintenance are also less expensive. 
Some of the very effective ML approaches used 
in NER are ME (Borthwick, 1999), CRF 
(Lafferty et al, 2001) and SVM (Yamada et al, 
2002). In the earlier work (Florian et al, 2003), it 
has been shown that combination of several ML 
models yields better performance than any single 
ML model. One drawback of the ML techniques 
to NLP tasks is the requirement of a large 
amount of annotated data to achieve a reasonable 
performance. 
Indian languages are resource-constrained and 
the manual preparation of NE annotated data is 
both time consuming and cost intensive. It is im-
portant to decide how the system should effec-
tively select unlabeled data and how the size and 
relevance of data impact the performance. India 
is a multilingual country with great cultural di-
versities. Named Entity (NE) identification in 
Indian languages in general and Bengali in par-
ticular is difficult and challenging as: 
1. Unlike English and most of the European 
languages, Bengali lacks capitalization infor-
mation, which plays a very important role in 
identifying NEs. 
2. Indian person names are generally found in 
the dictionary as common nouns with some 
specific meanings. For example, kabitA 
[Kabita] is a person name and can also be 
found in the dictionary as a common noun with 
the meaning ?poem?. 
3.  Bengali is an inflectional language provid-
ing one of the richest and most challenging sets 
of linguistic and statistical features resulting in 
long and complex wordforms. For example, the 
person name sachin [root] can appear as sa-
chiner [inflection:-er], sachInke [inflection:-
ke], sachInbAbu [inflection: -bAbu], sachIndA 
[ inflection:-dA] etc. The location name kol-
kAtA [root] can appear in different wordforms 
like kolkAtAr  [inflection:-r], kolkAtAte [inflec-
tion:-te], kolkAtAi  [inflection:-i] etc. 
4. Bengali is a relatively free phrase order lan-
guage. Thus, NEs can appear in any position of 
the sentence making the NER task more diffi-
cult.   
5. Bengali, like other Indian languages, is a re-
source-constrained language. The annotated 
corpus, name dictionaries, good morphological 
202
analyzers, POS taggers etc. are not yet avail-
able in the required measure. 
6. Although Indian languages have a very old 
and rich literary history, technological devel-
opments are of recent origin. 
7. Web sources for name lists are available in 
English, but such lists are not available in Ben-
gali. This necessitates the use of transliteration 
for creating such lists. 
A HMM based NER system for Bengali has 
been reported in Ekbal et al (2007b), where ad-
ditional contextual information has been consid-
ered during emission probabilities and NE suf-
fixes are used for handling the unknown words. 
More recently, the works in the area of Bengali 
NER can be found in Ekbal et al (2008a), and 
Ekbal and Bandyopadhyay (2008b) with the CRF, 
and SVM approach, respectively. Other than 
Bengali, the works on Hindi can be found in Li 
and McCallum (2004) with CRF and Saha et al 
(2008) with a hybrid feature set based ME ap-
proach. Various works of NER involving Indian 
languages are reported in IJCNLP-08 NER 
Shared Task on South and South East Asian 
Languages (NERSSEAL) 1  using various tech-
niques. 
2 Named Entity Recognition in Bengali  
We have used a Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008c), developed from the 
web-archive of a widely read Bengali newspaper 
for NER. A portion of this corpus containing 
200K wordforms has been manually annotated 
with the four NE tags namely, Person, Location, 
Organization and Miscellaneous. We have also 
used the NE annotated data of 122K wordforms, 
collected from the NERSSEAL shared task. The 
shared task data was originally annotated with a 
fine-grained NE tagset of twelve tags. We con-
sider only those tags that represent person, loca-
tion, organization, and miscellaneous names 
(NEN [number], NEM [Measurement] and NETI 
[Time]). Other tags have been mapped to the 
NNE tags that represent the ?other-than-NE? 
category. In order to properly denote the bounda-
ries of NEs, four NE tags are further divided into 
the following forms:  
 B-XXX: Beginning of a multiword NE, I-
XXX: Internal of a multiword NE consisting of 
more than two words, E-XXX: End of a multi-
word NE, XXX?PER/LOC/ORG/MISC. For 
example, the name sachin ramesh tendulkar is 
                                                 
1 http://ltrc.iiit.ac.in/ner-ssea-08/proc/index.html 
tagged as sachin/B-PER ramesh/I-PER tendul-
kar/E-PER. The single word NE is tagged as, 
PER: Person name, LOC: Location name, ORG: 
Organization name and MISC: Miscellaneous 
name. In the output, sixteen NE tags are replaced 
with the four NE tags. 
2.1 Our Approaches 
Initially, we started with the development of a 
NER system using an active learning method. 
This is used as the baseline model. Four super-
vised NER systems based on ME, CRF and SVM 
have been developed. Two different systems with 
the SVM model, one using forward parsing 
(SVM-F) that parses from left to right and other 
using backward parsing (SVM-B) that parses 
from right to left, have been developed. The 
SVM system has been developed based on 
(Valdimir, 1995), which perform classification 
by constructing an N-dimensional hyperplane 
that optimally separates data into two categories. 
We have used YamCha toolkit (http://chasen-
org/~taku/software/yamcha), an SVM based tool 
for detecting classes in documents and formulat-
ing the NER task as a sequential labeling prob-
lem. Here, the pairwise multi-class decision 
method and polynomial kernel function have 
been used. The TinySVM-0.02 classifier has been 
used for classification. The C++ based CRF++ 
package (http://crfpp.sourceforge.net) and the 
C++ based ME package 3 have been used for NER.  
Performance of the supervised NER models is 
limited in part by the amount of labeled training 
data available. A part of the available unlabeled 
corpus (Ekbal and Bandyopadhyay, 2008c) has 
been used to address this problem. Based on the 
original training on the labeled corpus, there will 
be some tags in the unlabeled corpus that the 
taggers will be very sure about. We have pro-
posed a semi-supervised learning technique that 
selects appropriate data from the available large 
unlabeled corpora and adds to the initial training 
set in order to improve the performance of the 
taggers. The models are retrained with this new 
training set and this process is repeated in a boot-
strapped manner. 
2.2 Named Entity Features 
The main features for the NER task have been 
identified based on the different possible combi-
nations of available word and tag contexts. In 
                                                 
2http://cl.aist-nara.ac.jp/~taku ku/software/TinySVM  
3http://homepages.inf.ed.ac.uk/s0450736/software/ma
xent/maxent-20061005.tar.bz2 
203
addition to these, various gazetteer lists have 
been developed for use in the NER tasks.  
The set of features ?F? contains language inde-
pendent as well as language dependent features. 
The set of language independent features in-
cludes the context words, fixed length prefixes 
and suffixes of all the words, dynamic NE infor-
mation of the previous word(s), first word, length 
of the word, digit and infrequent word informa-
tion. Language dependent features include the set 
of known suffixes that may appear with the vari-
ous NEs, clue words that help in predicting the 
location and organization names, words that help 
to recognize measurement expressions, designa-
tion words that help to identify person names, 
various gazetteer lists that include the first 
names, middle names, last names, location 
names, organization names, function words, 
weekdays and month names. We have also used 
the part of speech (POS) information of the cur-
rent and/or the surrounding word(s) as the fea-
tures. 
Language independent NE features can be ap-
plied for NER in any language without any prior 
knowledge of that language. The lists or gazet-
teers are basically language dependent at the 
lexical level and not at the morphology or syntax 
level. Also, we include the POS information in 
the set of language dependent features as the 
POS information depends on some language spe-
cific phenomenon such as person, number, tense, 
gender etc. Also, the particular POS tagger, used 
in this work, makes use of the several language 
specific resources such as lexicon, inflection lists 
and a NER system to improve its performance. 
Evaluation results have demonstrated that the use 
of language specific features is helpful to im-
prove the performance of the NER system. In the 
resource-constrained Indian language environ-
ment, the non-availability of language specific 
resources acts as a stimulant for the development 
of such resources for use in NER systems. This 
leads to the necessity of apriori knowledge of the 
language. The features are described below very 
briefly. 
  ?Context words: Such words include the pre-
ceding and succeeding words of the current 
word. This is based on the observation that the 
surrounding words carry effective information 
for the identification of NEs. 
?Word suffix and prefix: Fixed length word 
suffixes and prefixes are helpful to identify NEs. 
In addition, variable length word suffixes are 
also used. Word suffixes and prefixes are the ef-
fective features and work well for the inflective 
Indian languages like Bengali. 
?Named Entity Information: This is the only 
dynamic feature in the experiment.  The previous 
word NE tag is very informative in deciding the 
current word NE tag. 
?First word (binary valued): This feature 
checks whether the current token is the first word 
of the sentence or not. Though Bengali is a rela-
tively free phrase order language, the first word 
of the sentence is most likely a NE as it appears 
most of the time in the subject position. 
?Length of the word (binary valued): This fea-
ture checks whether the length of the token is 
less than three or not. We have observed that 
very short words are most probably not the NEs.  
?Infrequent word (binary valued): A cut off 
frequency has been chosen in order to consider 
the infrequent words in the training corpus. This 
is based on the observation that the infrequent 
words are rarely NEs. 
?Digit features: Several digit features have 
been considered depending upon the presence 
and/or the number of digit(s) in a token. These 
binary valued features are helpful in recognizing 
miscellaneous NEs such as time, monetary and 
date expressions, percentages, numerical num-
bers etc.     
?Position of the word (binary valued):  Posi-
tion of the word (whether last word or not) in a 
sentence is a good indicator of NEs.  
?Part of Speech (POS) Information: We have 
used an SVM-based POS tagger (Ekbal and 
Bandyopadhyay, 2008d) that was originally de-
veloped with 26 POS tags, defined for the Indian 
languages. For SVM models, we have used this 
POS tagger. However, for the ME and CRF 
models, we have considered a coarse-grained 
POS tagger that has the following tags: Nominal, 
PREP (Postpositions) and Other.  
?Gazetteer Lists: Gazetteer lists, developed 
manually as well as semi-automatically from the 
news corpus (Ekbal and Bandyopadhyay, 2008c), 
have been used as the features in each of the 
classifiers. The set of gazetteers along with the 
number of entries are as follows: 
 (1). Organization clue word (e.g., ko.m [Co.], 
limited [Limited] etc): 94, Person prefix words 
(e.g., shrimAn [Mr.], shrImati [Mrs.] etc.): 145, 
Middle names: 2,491, Surnames: 5,288, NE suf-
fixes (e.g., -bAbu [-babu], -dA [-da], -di [-di] for 
person and  -lyAnd [-land] -pur[-pur],  -liyA [-lia] 
etc for location):115, Common location (e.g., 
sarani [Sarani], roDa [Road] etc.): 147, Action 
204
verb (e.g., balen [says], ballen [told] etc.):141, 
Function words:743, Designation words (e.g., 
netA[leader], sA.msad [MP] etc.): 139, First 
names:72,206, Location names:7,870, Organiza-
tion names:2,225, Month name (English and 
Bengali calendars):24, Weekdays (English and 
Bengali calendars):14 
 (2). Common word (521 entries): Most of the 
Indian language NEs appears in the dictionary 
with some meanings. For example, the word ka-
mol may be the name of a person but also ap-
pears in the dictionary with another meaning lo-
tus, the name of a flower; the word dhar may be 
a verb and also can be the part of a person name. 
We have manually created a list, containing the 
words that can be NEs as well as valid dictionary 
words.  
3  Active Learning Method for Baseline 
NER System  
We have used a portion, containing 35,143 news 
documents and approximately 10 million word-
forms, of the Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008c) for developing the base-
line NER system. 
The frequently occurring words have been col-
lected from the reporter, location and agency 
tags of the Bengali news corpus. The unlabeled 
corpus is tagged with the elements from the seed 
lists. In addition, various gazetteers have been 
used that include surname, middle name, person 
prefix words, NE suffixes, common location and 
designations for further tagging of the NEs in the 
training corpus. The following linguistic rules 
have been used to tag the training corpus: 
  (i). If there are two or more words in a se-
quence that represent the characters of Bengali or 
English alphabet, then such words are part of 
NEs. For example, bi e (B A), ci em di e (C M D 
A), bi je pi (B J P) are all NEs. 
  (ii). If at the end of a word, there are strings like 
- era(-er),  -eraa (-eraa),  -ra (-ra), -rA (-raa), -ke 
(-ke), -dera (-der) then the word is likely to be a 
person name. 
  (iii). If a clue word like saranI (sarani), ro.Da 
(road), lena (lane) etc. is found after an unknown 
word then the unknown word along with the clue 
word may be a location name. 
  (iv). A few names or words in Bengali consist 
of the characters chandrabindu or khanda ta. So, 
if a particular word W is not identified as NE by 
any of the above rules but includes any of these 
two characters, then W may be a NE. For 
example o.NrI (onry) is a person name.  
  (v). The set of action verbs like balen (says), 
ballen (told), ballo (told), shunla (heared), 
ha.Nslo (haslo) etc. often determines the 
presence of person names. If an unknown word 
W appears in the sentence followed by the action 
verbs, then W is most likely a person name. 
Otherwise, W is not likely to be a NE. 
  (vi). If there is reduplication of a word W in a 
sentence then W is not likely to be a NE. This is 
so because rarely name words are reduplicated. 
In fact, reduplicated name words may signify 
something else. For example, rAm rAm (ram 
ram)  is used to greet a person. 
  (vii). If at the end of any word W there are 
suffixes like -gulo (-gulo), -guli (guli), -khAnA (-
khana) etc., then W is not a NE. 
For each tag T inserted in the training corpus, 
the algorithm generates a lexical pattern p using 
a context window of maximum width 6 (exclud-
ing the tagged NE) around the left and the right 
tags, e.g.,  
    p = [l-3l-2 l-1  <T> ...</T> l+1 l+2 l+3],  
 where, l?i   are the context of p. All these pat-
terns, derived from the different tags of the la-
beled and unlabeled training corpora, are stored 
in a Pattern Table (or, set P), which has four dif-
ferent fields namely, pattern id (identifies any 
particular pattern), pattern example (pattern), pat-
tern type (Person/Location/Organization) and 
relative frequency (indicates the number of times 
any pattern of a particular type appears in the 
entire training corpus relative to the total number 
of patterns generated of that type). This table has 
20,967 distinct entries.  
Every pattern p in the set P is matched against 
the same unlabeled corpus. In a place, where the 
context of p matches, p predicts the occurrence 
of the left or right boundary of name. POS in-
formation of the words as well as some linguistic 
rules and/or length of the entity have been used 
in detecting the other boundary. The extracted 
entity may fall in one of the following categories: 
? positive example: The extracted entity is 
of the same NE type as that of the pattern. 
? negative example: The extracted entity is 
of the different NE type as that of the pattern. 
? error example: The extracted entity is 
not at all a NE. 
The type of the extracted entity is determined 
by checking whether it appears in any of the seed 
lists; otherwise, its type is determined manually. 
The positive and negative examples are then 
added to the appropriate seed lists. The accuracy 
of the pattern is calculated as follows:  
205
     accuracy(p)= |positive (p)|/[| positive (p)| + 
|negative (p)| + |error(p)|] 
A threshold value of accuracy has been cho-
sen in order to discard the patterns below this 
threshold. A pattern is also discarded if its total 
positive count is less than a predetermined 
threshold value. The remaining patterns are 
ranked by their relative frequency values. The n 
top high frequent patterns are retained in the pat-
tern set P and this set is denoted as Accept Pat-
tern.  
All the positive and negative examples ex-
tracted by a pattern p can be used to generate 
further patterns from the same training corpus. 
Each new positive or negative instance (not ap-
pearing in the seed lists) is used to further tag the 
training corpus. We repeat the previous steps for 
each new NE until no new patterns can be gener-
ated. A newly generated pattern may be identical 
to a pattern that is already in the set P. In such a 
case, the type and relative frequency fields in the 
set P are updated accordingly. Otherwise, the 
newly generated pattern is added to the set with 
the type and relative frequency fields set prop-
erly. The algorithm terminates after 13 iterations 
and there are 20,176 distinct entries in the set P.   
 
4 Semi-supervised Approach for Unla-
beled Document and Sentence Selec-
tion 
A method for automatically selecting the appro-
priate unlabeled data from a large collection of 
unlabeled documents for NER has been de-
scribed in Ekbal and Bandyopadhyay (2008e). 
This work reported the selection of unlabeled 
documents based on the overall F-Score value of 
the individual system. In this work, the unlabeled 
documents have been selected based on the Re-
call, Precision as well as the F-Score values of 
the participating systems. Also, we have consid-
ered only the SVM-F model trained with the lan-
guage independent, language dependent and con-
text features for selecting the appropriate sen-
tences to be included into the initial training data. 
The use of single model makes the training faster 
compared to Ekbal and Bandyopadhyay (2008e). 
The SVM-F model has been considered as it 
produced the best results for the development set 
as well as during the 10-fold cross validation test. 
The unlabeled 35,143 news documents have been 
divided based on news sources/types in order to 
create segments of manageable size, separately 
evaluate the contribution of each segment using a 
gold standard development test set and reject 
those that are not helpful and to apply the latest 
updated best model to each subsequent segment. 
It has been observed that incorporation of unla-
beled data can only be effective if it is related to 
the target problem, i.e., the test set. Once the ap-
propriate documents are selected, it is necessary 
to select the tagged sentences that are useful to 
improve both the Recall and Precision values of 
the system. Appropriate sentences are selected 
using the SVM-F model depending upon the 
structure and/or contents of the sentences. 
4.1 Unlabeled Document Selection 
The unlabeled data supports the acquisition of 
new names and contexts to provide new evi-
dences to be incorporated in the models. Unla-
beled data can degrade rather than improve the 
classifier?s performance on the test set if it is ir-
relevant to the test document. So, it is necessary 
to measure the relevance of the unlabeled data to 
our target test set. We construct a set of key 
words from the test set T to check whether an 
unlabeled document d is useful or not.     
 
? We do not use all words in the test set T as 
the key words since we are only concerned 
about the distribution of name candidates. 
So, each document is tested with the CRF 
model using the language independent fea-
tures, language dependent features and the 
context features.  
? We take all the name candidates in the top N 
best hypotheses (N=10) for each sentence of 
the test set T to construct a query set Q. Us-
ing this query set, we find all the relevant 
documents that include three (heuristically 
set) names belonging to the set Q. In addi-
tion, the documents are not considered if 
they contain fewer than seven (heuristic) 
names.   
4.2 Sentence Selection 
All the tagged sentences of a relevant document 
are not added to training corpus as incorrectly 
tagged or irrelevant sentences can lead to the 
degradation in model performance. Our main 
concern is on how much new information is ex-
tracted from each sentence of the unlabeled data 
compared to the training corpus that already we 
have in our hand.  
The SVM-F model has been used to select the 
relevant sentences. All the relevant documents 
are tagged with the SVM-F model developed 
with the language independent, language de-
206
pendent and context features along with the class 
decomposition technique. If both Recall and Pre-
cision values of the SVM-F model increase then 
that sentence is selected to be added to the initial 
training corpus. A close investigation reveals the 
fact that this criterion often selects a number of 
sentences which are too short or do not include 
any name. These words may make the model 
worse if added to the training data. For example, 
the distribution of non-names may increase sig-
nificantly that may lead to degradation of model 
performance. In this experiment, we have not 
included the sentences that include fewer than 
five words or do not include any names. The 
bootstrapping procedure is given as follows: 
1. Select a relevant document RelatedD 
from a large corpus of unlabeled data 
with respect to the test set T using the 
document selection method described in 
Section 4.1. 
2. Split RelatedD into n subsets and mark 
them C1, C2, ?., Cn.    
3. Call the development set DevT. 
4. For I=1 to n 
4.1. Run SVM-F model, developed with the 
language independent features, language 
dependent feature and context features 
along with the class decomposition tech-
nique, on Ci. 
4.2. If the length of each tagged sentence S is 
less than five or it does not contain any 
name then discard S. 
4.3. Add Ci to the training data and retrain 
SVM-F model. This produces the up-
dated model. 
4.4. Run the updated model on DevT; if the 
Recall and Precision values reduce then 
don?t use Ci and use the old model. 
5. Repeat steps 1-4 until Recall and Precision 
values of the SVM-F model either become equal 
or differ by some threshold values (set to 0.01) in 
consecutive two iterations.  
5 Evaluation Results and Discussions 
Out of 200K wordforms, 150K wordforms along 
with the IJCNLP-08 shared task data has been 
used for training the models. Out of 200K word-
forms, 50K wordforms have been used as the 
development data. The system has been tested 
with a gold standard test set of 35K wordforms. 
Each of the models has been evaluated in two 
different ways, being guided by language inde-
pendent features (language independent system 
denoted as LI) and being guided by language 
independent as well as language dependent fea-
tures (language dependent system denoted as 
LD).  
5.1 Language Independent Evaluation 
A number of experiments have been carried out 
in order to identify the best-suited set of lan-
guage independent features for NER in each of 
models. Evaluation results of the development 
set for the NER models are presented in Table 1 
in terms of percentages of Recall (R), Precision 
(P) and F-Score (FS). The ME based system has 
demonstrated the F-Score value of 74.67% for 
the context word window of size three, i.e., pre-
vious one word, current word and the next word, 
prefixes and suffixes of length up to three char-
acters of only the current word, dynamic NE tag 
of the previous word, first word, infrequent word, 
length and the various digit features. The CRF 
based system yielded the highest F-Score value 
of 76.97% for context window of size five, i.e., 
two preceding, current and two succeeding words 
along with the other set of features as in the ME 
model. Both the SVM based systems have dem-
onstrated the best performance for the context 
window of size seven, i.e., three preceding, cur-
rent and two succeeding words, dynamic NE in-
formation of the previous two words along with 
the other set of features as in the ME and CRF 
based systems. In SVM models, we have con-
ducted experiments with the different polynomial 
kernel functions and observed the highest F-
Score value with degree 2. It has been also ob-
served that pairwise multiclass decision method 
performs better than the one vs rest method. For 
all the models, context words and prefixes and/or 
suffixes have been found to be the most effective 
features. 
 
Model R  P  FS  
ME 76.82 72.64 74.67 
CRF 78.17 75.81 76.97 
SVM-F 79.14 77.26 78.19 
SVM-B 79.09 77.15 78.11 
Table 1. Results on the development set for 
the language independent supervised models 
5.2 Language Dependent Evaluation 
Evaluation results of the systems that include the 
POS information and other language dependent 
features are presented in the Table 2. During the 
experiments, it has been observed that all the 
language dependent features are not equally im-
portant. POS information is the most effective 
207
followed by NE suffixes, person prefix words, 
designations, organization clue words and loca-
tion clue words. Table 1 and Table 2 show that 
the language dependent features can improve the 
overall performance of the systems significantly. 
 
Model R  P  FS  
ME 87.02 80.77 83.78 
CRF 87.63 84.03 85.79  
SVM-F 87.74 85.89 86.81  
SVM-B 87.69 85.17 86.72  
Table 2. Results on the development set for the 
language dependent supervised models 
 
5.3 Use of Context Features as Features 
Now, the high ranked patterns of the Accept Pat-
tern set (Section 3) can be used as the features of 
the individual classifier. A feature ?ContextInf? is 
defined by observing the three preceding and 
succeeding words of the current word. Evalua-
tion results are presented in Table 3. Clearly, it is 
evident from the results of Table 2 and Table 3 
that context features are very effective to im-
prove the Precision values in each of the models.  
 
Model R  P  FS  
ME 88.22 83.71 85.91 
CRF 89.51 85.94 87.69 
SVM-F 89.67 86.49 88.05 
SVM-B 89.61 86.47 88.01 
Table 3. Results on the development set by in-
cluding context features 
5.4 Results on the Test Set 
A gold standard test set of 35K wordforms has 
been used to report the evaluation results. The 
models have been trained with the language in-
dependent, language dependent and the context 
features. Results have been presented in Table 4 
for the test set. In the baseline model, each pat-
tern of the Accept Pattern set is matched against 
the test set. Results show that SVM-F model per-
forms best for the test set. 
Error analyses have been conducted with the 
help of confusion matrix. In order to improve the 
performance of the classifiers, we have used 
some post-processing techniques.  
Output of the ME based system has been post-
processed with a set of heuristics (Ekbal and 
Bandyopadhyay, 2009) to improve the perform-
ance further. The post-processing as described in 
Ekbal and Bandyopadhyay (2008e) tries to as-
sign the correct tag according to the n-best re-
sults for every sentence of the test set in the CRF 
framework. In order to remove the unbalanced 
class distribution between names and non-names 
in the training set, we have considered the class 
decomposition technique (Ekbal and Bandyop-
adhyay, 2008e) for SVM. Evaluation results of 
the post-processed systems are presented in Ta-
ble 5.  
 
 Model R  P  FS  
Baseline 68.11 71.37 69.32 
ME 86.04 84.98 85.51 
CRF 87.94 87.12 87.53 
SVM-F 89.91 85.97 87.89 
SVM-B 89.82 85.93 87.83 
      Table 4. Results on the test set 
 
Model R  P  FS  
ME 87.29 86.81 87.05 
CRF 89.19 88.85 89.02 
SVM-F 90.23 88.62 89.41 
SVM-B 90.05 88.61 89.09 
Table 5. Results of the post-processed models 
on the test set 
Each of the models has been also evaluated for 
the 10-fold cross validation tests. Initially all the 
models have been developed with the language 
independent features along with the context fea-
tures. Then, language dependent features have 
been included into the models. In each run of the 
10 tests, the outputs have been post-processed 
with the several post-processing techniques as 
described earlier. Results are shown in Table 6.  
  
 Model R  P  FS  
ME  81.34 79.01 80.16 
CRF 82.66 80.75 81.69 
SVM-F 83.87 81.83 82.83 
LI 
SVM-B 83.87 81.77 82.62 
ME  87.54 87.97 87.11 
CRF 89.5 88.73 89.19 
SVM-F 89.97 88.61 89.29 
LD
SVM-B 89.76 88.51 89.13 
Table 6. Results of the 10-fold cross validation 
tests   
Statistical ANOVA tests (Anderson and 
Scolve, 1978) demonstrated that the performance 
improvement in each of the language dependent 
model is statistically significant over the lan-
guage independent model. We have also carried 
out the statistical tests to show that performance 
improvement in CRF over ME and SVM-F over 
CRF are statistically significant.    
208
5.5 Impact of Unlabeled Data Selection 
In order to investigate the contribution of 
document selection in bootstrapping, the post-
processed models are run on 35,143 news 
documents. This yields the gradually improving 
performance for the SVM-F model as shown in 
Table 7. After selection of the appropriate 
unlabeled data, all the models have been 
retrained by including the unlabeled documents. 
Results have been presented in Table 8. 
 
Itera-
tion 
Sentences 
added 
R  P FS 
0 0 89.97 88.61 89.29 
1 129 90.19 88.97 89.58 
2 223 90.62 89.14 89.87 
3 332 90.89 89.73 90.31 
4 416 91.24 90.11 90.67 
5 482 91.69 90.65 91.16 
6 543 91.88 90.97 91.42 
7 633 92.07 91.05 91.56 
8 682 92.33 91.31 91.82 
9 712 92.52 91.39 91.95 
10 723 92.55 91.44 91.99 
11 729 92.57 91.45 92.01 
12 734 92.58 91.45 92.01 
Table 7. Incremental improvement of perform-
ance 
 
Model R  P  FS  
ME 90.7 89.78 90.24 
CRF 92.02 91.66 91.84 
SVM-B 92.34 91.42 91.88 
SVM-F 92.58 91.45 92.01 
Table 8. Results after unlabeled data selection 
5.6 Voting Techniques 
In order to obtain higher performance, we have 
applied weighted voting to the four models. We 
have used the following weighting methods: 
 (1). Uniform weights (Majority voting): All 
the models are assigned the same voting weight. 
The combined system selects the classifications, 
which are proposed by the majority of the mod-
els. In case of a tie, the output of the SVM-F 
model is selected. The output of the SVM-F 
model has been selected due to its highest per-
formance among all the models.  
  (2). Cross validation Precision values: Two 
different types of weights have been defined de-
pending on the 10-fold cross validation Precision 
on the training data as follows:  
   (a). Total Precision: In this method, the 
overall average Precision of any classifier is as-
signed as the weight for it.  
  (b). Tag Precision: In this method, the aver-
age Precision value of the individual tag is as-
signed as the weight for the corresponding model. 
 
Experimental results of the voted system are 
presented in Table 9. Evaluation results show 
that the system achieves the highest performance 
for the voting scheme ?Tag Precision?. Voting 
shows (Tables 8-9) an overall improvement of 
2.74% over the least performing ME based sys-
tem and 0.97% over the best performing SVM-F 
system. This also shows an improvement of 
23.66% F-Score over the baseline model. 
 
Voting  R  P  FS  
Majority 92.59 91.47 92.03 
Total Precision 93.08 91.79 92.43 
Tag Precision 93.81 92.18 92.98 
Table 9. Results of the voted system 
 
6 Conclusion 
In this paper, we have reported a voted system 
with the use of appropriate unlabeled data. We 
have also demonstrated how language dependent 
features can improve the system performance. It 
has been experimentally verified that effective 
measures to select relevant documents and useful 
labeled sentences are important. The system has 
demonstrated the overall Recall, Precision, and 
F-Score values of 93.81%, 92.18%, and 92.98%, 
respectively.   
Future works include the development of NER 
system using other machine learning techniques 
such as decision tree, AdaBoost etc. We would 
like to apply the proposed voted technique for 
the development of NER systems in other Indian 
languages. Future direction of the work will be to 
investigate an appropriate clustering technique 
that can be very effective for the development of 
NER systems in the resource-constrained Indian 
language environment. Instead of the words, the 
cluster of words can be used as the features of 
the classifiers. It may reduce the cost of training 
as well as may be helpful to improve the per-
formance. We would like to explore other voting 
techniques.  
 
 
209
References 
Anderson, T. W. and Scolve, S. Introduction to the 
Statistical Analysis of Data. Houghton Mifflin, 
1978. 
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name.  
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
NYU.  
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay. 
2007b. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, Haque, R and S. Bandyopadhyay. 2008a. 
Named Entity Recognition in Bengali: A Condi-
tional Random Field Approach. In Proceedings of 
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 589-594. 
Ekbal, Asif, and S. Bandyopadhyay. 2008b. Bengali 
Named Entity Recognition using Support Vector 
Machine. In Proceedings of the Workshop on 
Named Entity Recognition on South and South East 
Asian Languages (NERSSEAL), IJCNLP-08, 51-58. 
Ekbal, Asif, and S. Bandyopadhyay. 2008c. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal, Volume (40), 173-182. 
Ekbal, Asif and S. Bandyopadhyay. 2008d. Web-
based Bengali News Corpus for Lexicon Develop-
ment and POS Tagging. In POLIBITS, an Interna-
tional Journal, Volume (37), 20-29, ISSN: 1870-
9044.  
Ekbal, Asif and S. Bandyopadhyay. 2008e. Appropri-
ate Unlabeled Data, Post-processing and Voting 
Can Improve the Performance of NER System. In 
Proceedings of the 6th International Conference on 
Natural Language Processing (ICON-08), 234-
239, India. 
Ekbal, Asif and S. Bandyopadhyay. 2009. Improving 
the Performance of a NER System by Post-
processing, Context Patterns and Voting. In W. Li 
and D. Molla-Aliod (Eds): ICCPOL 2009, Lecture 
Notes in Artificial Intelligence (LNAI), Springer 
Berlin/Heidelberg, Volume (5459), 45-56. 
Florian, Radu, Ittycheriah, A., Jing, H. and Zhang, T. 
2003. Named Entity Recognition through Classifier 
Combination. In Proceedings of CoNLL-2003. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
Proceedings  of 18th International Conference on 
Machine Learning (ICML), 282-289. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions. ACM TALIP, 2(3), (2003), 290-294. 
Saha, Sujan, Sarkar, S and Mitra, P. 2008. A Hybrid 
Feature Set based Maximum Entropy Hindi Named 
Entity Recognition. In Proceedings of the 3rd Inter-
national Joint Conference on Natural Language 
Processing (IJCNLP-08), 343-349. 
Valdimir N., Vapnik 1995. The Nature of Statistical 
Learning Theory. Springer. 
Yamada, Hiroyasu, Taku Kudo and Yuji Matsumoto. 
2002. Japanese Named Entity Extraction using 
Support Vector Machine. In Transactions of IPSJ, 
Vol. 43 No. 1, 44-53. 
 
210
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 345?350,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU_CSE_TEMP: A First Step towards Evaluating Events, Time Ex-
pressions and Temporal Relations 
Anup Kumar Kolya1, Asif Ekbal2 and Sivaji Bandyopadhyay3 
 
1,3Department of Computer Science and Engineering, Jadavpur University,  
Kolkata-700032, India 
2Department of Computational Linguistics, Heidelberg University,  
Heidelberg-69120, Germany 
Email: anup.kolya@gmail.com1, asif.ekbal@gmail.com2  
and sivaji_cse_ju@yahoo.com3  
 
Abstract 
Temporal information extraction is a 
popular and interesting research field in 
the area of Natural Language Processing 
(NLP). In this paper, we report our works 
on TempEval-2 shared task. This is our 
first participation and we participated in 
all the tasks, i.e., A, B, C, D, E and F. We 
develop rule-based systems for Tasks A 
and B, whereas the remaining tasks are 
based on a machine learning approach, 
namely Conditional Random Field 
(CRF). All our systems are still in their 
development stages, and we report the 
very initial results. Evaluation results on 
the shared task English datasets yield the 
precision, recall and F-measure values of 
55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respec-
tively for Task B (event recognition).  
The rest of tasks, namely C, D, E and F 
were evaluated with a relatively simpler 
metric: the number of correct answers di-
vided by the number of answers. Experi-
ments on the English datasets yield the 
accuracies of 63%, 80%, 56% and 56% 
for tasks C, D, E and F, respectively.        
1 Introduction 
Temporal information extraction is, nowadays, a 
popular and interesting research area of Natural 
Language Processing (NLP). Generally, events 
are described in different newspaper texts, sto-
ries and other important documents where 
events happen in time and the temporal location 
and ordering of these events are specified. One 
of the important tasks of text analysis clearly re-
quires identifying events described in a text and 
locating these in time. This is also important in a 
wide range of NLP applications that include 
temporal question answering, machine transla-
tion and document summarization.  
   In the literature, temporal relation identifica-
tion based on machine learning approaches can 
be found in Boguraev et el. (2005), Mani et al 
(2006), Chambers et al (2007) and some of the 
TempEval 2007 participants (Verhagen et al, 
2007). Most of these works tried to improve 
classification accuracies through feature engi-
neering. The performance of any machine learn-
ing based system is often limited by the amount 
of available training data. Mani et al (2006) in-
troduced a temporal reasoning component that 
greatly expands the available training data. The 
training set was increased by a factor of 10 by 
computing the closure of the various temporal 
relations that exist in the training data. They re-
ported significant improvement of the classifica-
tion accuracies on event-event and event-time 
relations. Their experimental result showed the 
accuracies of 62.5%-94.95% and 73.68%-
90.16% for event-event and event-time relations, 
respectively. However, this has two shortcom-
ings, namely feature vector duplication caused 
by the data normalization process and the unreal-
istic evaluation scheme.  The solutions to these 
issues are briefly described in Mani et al (2007).  
In TempEval 2007 task, a common standard da-
taset was introduced that involves three temporal 
relations. The participants reported F-measure 
scores for event-event relations ranging from 
42% to 55% and for event-time relations from 
73% to 80%. Unlike (Mani et al, 2007; 2006), 
event-event temporal relations were not dis-
course-wide (i.e., any pair of events can be tem-
porally linked) in TempEval 2007. Here, the 
event-event relations were restricted to events 
within two consecutive sentences. Thus, these 
two frameworks produced highly dissimilar re-
345
sults for solving the problem of temporal relation 
classification.  
   In order to apply various machine learning al-
gorithms, most of the authors formulated tempo-
ral relation as an event paired with a time or an-
other event and translated these into a set of fea-
ture values. Some of the popularly used machine 
learning techniques were Naive-Bayes, Decision 
Tree (C5.0), Maximum Entropy (ME) and Sup-
port Vector Machine (SVM). Machine learning 
techniques alone cannot always yield good accu-
racies. To achieve reasonable accuracy, some 
researchers (Mao et al, 2006) used hybrid ap-
proach. The basic principle of hybrid approach is 
to combine the rule-based component with ma-
chine learning.  It has been shown in (Mao et al, 
2006) that classifiers make most mistakes near 
the decision plane in feature space. The authors 
carried out a series of experiments for each of the 
three tasks on four models, namely naive-Bayes, 
decision tree (C5.0), maximum entropy and sup-
port vector machine. The system was designed in 
such a way that they can take the advantage of 
rule-based as well as machine learning during 
final decision making. But, they did not explain 
exactly in what situations machine learning or 
rule based system should be used given a particu-
lar instance. They had the option to call either 
component on the fly in different situations so 
that they can take advantage of the two empirical 
approaches in an integrated way. 
The rest of the paper is structured as follows. 
We present very brief descriptions of the differ-
ent tasks in Section 2. Section 3 describes our 
approach in details with rule-based techniques 
for tasks A and B in Subsection 3.1, CRF based 
techniques in Subsection 3.2 for tasks C, D, E 
and F, and features in Subsection 3.3. Detailed 
evaluation results are reported in Section 4. Fi-
nally, Section 5 concludes the paper with a direc-
tion to future works.  
2 Task Description 
The main research in this area involves identifi-
cation of all temporal referring expressions, 
events and temporal relations within a text. The 
main challenges involved in this task were first 
addressed during TempEval-1 in 2007 (Verhagen 
et al, 2007). This was an initial evaluation exer-
cise based on three limited tasks that were con-
sidered realistic both from the perspective of as-
sembling resources for development and testing 
and from the perspective of developing systems 
capable of addressing the tasks. In TempEval 
2007, following types of event-time temporal 
relations were considered: Task A (relation be-
tween the events and times within the same sen-
tence), Task B (relation between events and 
document creation time) and Task C (relation 
between verb events in adjacent sentences). The 
data sets were based on TimeBank, a hand-built 
gold standard of annotated texts using the Ti-
meML markup scheme1. The data sets included 
sentence boundaries, timex3 tags (including the 
special document creation time tag), and event 
tags. For tasks A and B, a restricted set of events 
was used, namely those events that occur more 
than 5 times in TimeBank. For all three tasks, the 
relation labels used were before, after, overlap, 
before-or-overlap, overlap-or-after and vague. 
Six teams participated in the TempEval tasks. 
Three of the teams used statistics exclusively, 
one used a rule-based system and the other two 
employed a hybrid approach. For task A, the 
range of F-measure scores were from 0.34 to 
0.62 for the strict scheme and from 0.41 to 0.63 
for the relaxed scheme. For task B, the scores 
were from 0.66 to 0.80 (strict) and 0.71 to 0.81 
(relaxed). Finally, task C scores range from 0.42 
to 0.55 (strict) and from 0.56 to 0.66 (relaxed). 
   In TempEval-2, the following six tasks were 
proposed:  
 A:  The main task was to determine the extent of 
the time expressions in a text as defined by the 
TimeML timex3 tag. In addition, values of the 
features type and val had to be determined. The 
possible values of type are time, date, duration, 
and set; the value of val is a normalized value as 
defined by the timex2 and timex3 standards. 
B. Task was to determine the extent of the events 
in a text as defined by the TimeML event tag. In 
addition, the values of the features tense, aspect, 
polarity, and modality had to be determined. 
C. Task was to determine the temporal relation 
between an event and a time expression in the 
same sentence. 
D. Temporal relation between an event and the 
document creation time had to be determined. 
E. Temporal relation between two main events in 
consecutive sentences had to be determined.  
F. Temporal relation between two events, where 
one event syntactically dominates the other 
event.  
     In our present work, use handcrafted rules for 
Task A and Task B. All the other tasks, i.e., C, 
D, E and F are developed based on the well 
known statistical algorithm, Conditional Random 
                                                 
1www.timeml.org for details on TimeML  
346
Field (CRF). For CRF, we use only those fea-
tures that are available in the training data. All 
the systems are evaluated on the TempEval-
2 shared task English datasets. Evaluation results 
yield the precision, recall and F-measure values 
of 55%, 17% and 26%, respectively for Task A 
and 48%, 56% and 52%, respectively for Task B. 
Experiments on the other tasks demonstrate the 
accuracies of 63%, 80%, 56% and 56% for C, D, 
E and F, respectively.   
3 Our Approach  
In this section, we present our systematic ap-
proach for evaluating events, time expressions 
and temporal relations as part of our first partici-
pation in the TempEval shared task. We partici-
pated in all the six tasks of TempEval-2. Rule-
based systems are developed using a preliminary 
handcrafted set of rules for tasks A and B. We 
use machine learning approach, namely CRF for 
solving the remaining tasks, i.e., C, D, E and F.  
 
3.1 Rules for Task A and Task B 
We manually identify a set of rules studying the 
various features available in the training data. 
There were some exceptions to these rules. How-
ever, a rule is used if it is found to be correct 
most of the time throughout the training data. It 
is to be noted that these are the very preliminary 
rules, and we are still working on finding out 
more robust rules. Below, we present the rules 
for tasks A and B.  
 
Task A. The time expression is identified by de-
fining appropriate regular expression. The regu-
lar expressions are based on several entities that 
denote month names, year, weekdays and the 
various digit expressions. We also use a list of 
keywords (e.g., day, time, AM, PM etc.) that de-
note the various time expressions. The values of 
various attributes (e.g., type and value) of time 
expressions are computed by some simple tem-
plate matching algorithms.  
 
Task B. In case of Task B, the training data is 
initially passed through the Stanford PoS tagger2. 
We consider the tokens as the events that are 
tagged with POS tags such as VB, VBG, VBN, 
VBP, VBZ and VBD, denoting the various verb 
expressions. Values of different attributes are 
computed as follows.  
                                                 
2 http://nlp.stanford.edu/software/tagger.shtml 
 
a. Tense: A manually augmented suffix list such 
as: "ed","d","t" etc. is used to capture the proper 
tense of any event verb from surface level ortho-
graphic variations. 
b. Aspect: The Tense-Aspect-Modality (TAM) 
for English verbs is generally associated with 
auxiliaries. A list is manually prepared. Any oc-
currence of main verb with continuous aspect 
leads to search for the adjacent previous auxil-
iary and rules are formulated to extract TAM 
relation using the manually generated checklist. 
A separate list of auxiliaries is prepared and suc-
cessfully used for detection of progressive verbs.  
c. Polarity: Verb-wise polarity is assigned by the 
occurrence of previous negation words. If any 
negation word appears before any event verb 
then the resultant polarity is negative; otherwise, 
the verb considered as positive by default. 
d. Modality: We prepare a manual list that con-
tains the words such as: may, could, would etc. 
The presence of these modal auxiliaries gives 
modal tag to the targeted verb in a sentence oth-
erwise it is considered a non-modal. 
e. Class: We select ?occurrence? to be class val-
ue by default.  
 
3.2 Machine Learning Approach for Tasks 
C, D, E and F 
 
For tasks C-F, we use a supervised machine 
learning approach that is based on CRF. We con-
sider the temporal relation identification task as a 
pair-wise classification problem in which the 
target pairs?a TIMEX3 tag and an EVENT?are 
modelled using CRF, which can include arbitrary 
set of features, and still can avoid overfitting in a 
principled manner.  
 
Introduction to CRF.  CRF (Lafferty et al, 
2001), is used to calculate the conditional prob-
ability of values on designated output nodes 
given values on other designated input nodes. 
The conditional probability of a state sequence 
1, 2, ..., TS s s s=<
1 2,O o
>  given an observation se-
quence , ....., )To o=<  is calculated as: 
1 ,
1 1
1
( | ) exp( ( , , ))
T K
k k t t
o t k
P s o f s s o t
Z
??
= =
= ?? ?
)
                                 
where, 1 ,( , ,k t tf s s o t?
k
is a feature function 
whose weight ? is to be learned via training. 
The values of the feature functions may range 
between .....? ? + ? , but typically they are 
347
binary. To make all conditional probabilities sum 
up to 1, we must calculate the normalization 
factor, 
0
1 1
exp( ( , , ))
T K
s k k t
t k
1 ,tZ f s s o t? ?
= =
= ? ? ? ,                                             
which, as in HMMs, can be obtained efficiently 
by dynamic programming. 
   To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given the observation sequence: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i k
i
L P s o
1
K
k
?
?? ?==? =??
>
,                                         
where, { } is the labeled training da-
ta. The second sum corresponds to a zero-mean, 
( ) ( ),i io s<
2? -variance Gaussian prior over parameters, 
which facilitates optimization by making the li-
kelihood surface strictly convex.  
  CRFs generally can use real-valued functions 
but it is often required to incorporate the binary 
valued features. A feature function 
1 ,( , ,k t t )f s s o t? has a value of 0 for most cases 
and is only set to  1, when 1,t ts s?  are certain 
states and the observation has certain properties. 
Here, we set parameters ?  to maximize the pe-
nalized log-likelihood using Limited-memory 
BFGS (Sha and Pereira, 2003) a quasi-Newton 
method that is significantly more efficient, and 
which results in only minor changes in accuracy 
due to changes in ? . 
   We use the OpenNLP C++ based CRF++ pack-
age 3 , a simple, customizable, and open source 
implementation of CRF for segmenting /labeling 
sequential data.  
 
3.3 Features of Tasks C, D, E and F 
 
We extract the gold-standard TimeBank features 
for events and times in order to train/test the 
CRF. In the present work, we mainly use the 
various combinations of the following features:  
 
(i). Part of Speech (POS) of event terms: It de-
notes the POS information of the event. The fea-
tures values may be either of ADJECTIVE, 
NOUN, VERB, and PREP. 
 (ii). Event Tense: This feature is useful to cap-
ture the standard distinctions among the gram-
matical categories of verbal phrases. The tense 
attribute can have values, PRESENT, PAST, 
                                                 
3http://crfpp.sourceforge.net  
FUTURE, INFINITIVE, PRESPART, PAST-
PART, or NONE. 
 (iii). Event Aspect: It denotes the aspect of the 
events. The aspect attribute may take values, 
PROGRESSIVE, PERFECTIVE and PERFEC-
TIVE PROGRESSIVE or NONE. 
(iv). Event Polarity: The polarity of an event 
instance is a required attribute represented by the 
boolean attribute, polarity. If it is set to ?NEG?, 
the event instance is negated.  If it is set to ?POS? 
or not present in the annotation, the event in-
stance is not negated. 
(v). Event Modality: The modality attribute is 
only present if there is a modal word that modi-
fies the instance. 
(vi). Event Class: This is denoted by the 
?EVENT? tag and used to annotate those ele-
ments in a text that mark the semantic events 
described by it. Typically, events are verbs but 
can be nominal also. It may belong to one of the 
following classes:  
 REPORTING: Describes the action of a person 
or an organization declaring something, narrating 
an event, informing about an event, etc.  For ex-
ample, say, report, tell, explain, state etc. 
 PERCEPTION: Includes events involving the 
physical perception of another event. Such 
events are typically expressed by verbs like: see, 
watch, glimpse, behold, view, hear, listen, over-
hear etc. 
ASPECTUAL: Focuses on different facets of 
event history. For example, initiation, reinitia-
tion, termination, culmination, continuation etc. 
 I_ACTION: An intentional action. It introduces 
an event argument which must be in the text ex-
plicitly describing an action or situation from 
which we can infer something given its relation 
with the I_ ACTION. 
I_STATE: Similar to the I_ACTION class. This 
class includes states that refer to alternative or 
possible words, which can be introduced by sub-
ordinated clauses, nominalizations, or untensed 
verb phrases (VPs). 
 STATE: Describes circumstances in which 
something obtains or holds true. 
 Occurrence: Includes all of the many other 
kinds of events that describe something that hap-
pens or occurs in the world. 
(vii). Type of temporal expression: It repre-
sents the temporal relationship holding between 
events, times, or between an event and a time of 
the event.  
(viii). Event Stem:  It denotes the stem of the 
head event.  
348
(ix). Document Creation Time: The document 
creation time of the event.  
4 Evaluation Results 
Each of the tasks is evaluated with the Tem-
pEval-2 shared task datasets. 
  
4.1 Evaluation Scheme 
 
For the extents of events and time expressions 
(tasks A and B), precision, recall and the F-
measure are used as evaluation metrics, using the 
following formulas: 
Precision (P) = tp/ (tp + fp) 
Recall (R) = tp/ (tp + fn) 
F-measure = 2 *(P * R)/ (P + R) 
   Where, tp is the number of tokens that are part 
of an extent in both keys and response,  
fp is the number of tokens that are part of an ex-
tent in the response but not in the key, and  
fn is the number of tokens that are part of an ex-
tent in the key but not in the response. 
  An even simpler evaluation metric similar to 
the definition of ?accuracy? is used to evaluate 
the attributes of events and time expressions (the 
second part of tasks, A and B) and for relation 
types (tasks C through F). The metric, henceforth 
referred to as ?accuracy?, is defined as below:  
    Number of correct answers/ Number of an-
swers present in the test data  
 
4.2 Results 
 
For tasks A and B, we identify a set of rules from 
the training set and apply them on the respective 
test sets.  
   The tasks C, D, E and F are based on CRF. We 
develop a number of models based on CRF using 
the different features included into it. A feature 
vector consisting of the subset of the available 
features as described in Section 2.3 is extracted 
for each of <event, timex>, <event, DCT>, 
<event, event> and <event, event> pairs in tasks 
C, D, E and F, respectively. Now, we have a 
training data in the form ( , , where,  is 
the ith pair along with its feature vector and  is 
it?s corresponding TempEval relation class. 
Models are built based on the training data and 
the feature template. The procedure of training is 
summarized below: 
)i iW T iW
iT
1. Define the training corpus, C. 
2. Extract the corresponding relation from 
the training corpus. 
3. Create a file of candidate features, in-
cluding lexical features derived from the 
training corpus. 
4. Define a feature template.  
5. Compute the CRF weights ?k for every fK 
using the CRF toolkit with the training 
file and feature template as input. 
  During evaluation, we consider the following 
feature templates for the respective tasks:  
 
(i) Task C: Feature vector consisting of current 
token, polarity, POS, tense, class and value; 
combination of token and type, combination of 
tense and value of the current token, combination 
of aspect and type of current token, combination 
of aspect, value and type of the current token.      
(ii) Task D: Feature vector consisting of current 
token and POS; combination of POS and tense of 
the current token, combination of polarity and 
POS of the current token, combination of POS 
and aspect of current token, combination of po-
larity and POS of current token, combination of 
POS, tense and aspect of the current token.      
(iii). Task E: Current token, combination of 
event-class and event-id of the current token, 
combination of POS tags of the pair of events, 
combination of (tense, aspect) values of the event 
pairs. 
(iv). Task F: Current token, combination of POS 
tags of the pair of events, combination of tense 
values of the event pairs, combination of the as-
pect values of the event pairs, combination of the 
event classes of the event pairs. 
  Experimental results of tasks A and B are re-
ported in Table 1 for English datasets. The re-
sults for task A, i.e., recognition and normaliza-
tion of time expressions, yield the precision, re-
call and F-measure values of 55%, 17% and 
26%, respectively. For task B, i.e., event recogni-
tion, the system yields precision, recall and F-
measure values of 48%, 56% and 52%, respec-
tively. Event attribute identification shows the 
accuracies of 98%, 98%, 30%, 95% and 53% for 
polarity, mood, modality, tense, aspect and class, 
respectively. These systems are the baseline 
models, and the performance can further be im-
proved with a more carefully handcrafted set of 
robust rules. In further experiments, we would 
also like to apply machine learning methods to 
these problems.  
 
 
 
349
Task  precision 
(in %)  
recall   
(in %) 
F-measure  
(in %) 
A 55% 17% 26% 
B 48% 56% 52% 
 
Table 1. Experimental results on tasks A and B 
 
  Evaluation results on the English datasets for 
tasks C, D, E and F are presented in Table 2. Ex-
periments show the accuracies of 63%, 80%, 
56% and 56% for tasks C, D, E and F, respec-
tively. Results show that our system performs 
best for task D, i.e., relationships between event 
and document creation time. The system 
achieves an accuracy of 63% for task C that finds 
the temporal relation between an event and a time 
expression in the same sentence. The system per-
forms quite similarly for tasks E and F. It is to be 
noted that there is still the room for performance 
improvement. In the present work, we did not 
carry out sufficient experiments to identify the 
most suitable feature templates for each of the 
tasks. In future, we would experiment after se-
lecting a development set for each task; and find 
out appropriate feature template depending upon 
the performance on the development set.  
 
 
Task  Accuracy (in %) 
C 63%  
D 80% 
E 56% 
F 56% 
 
Table 2. Experimental results on tasks C, D, E 
and F 
   
5 Conclusion and Future Works 
In this paper, we report very preliminary results 
of our first participation in the TempEval shared 
task. We participated in all the tasks of Tem-
pEval-2, i.e., A, B, C, D, E and F for English. 
We develop the rule-based systems for tasks A 
and B, whereas the remaining tasks are based on 
a machine learning approach, namely CRF. All 
our systems are still in their development stages. 
Evaluation results on the shared task English 
datasets yield the precision, recall and F-measure 
values of 55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respectively for 
Task B (event recognition).  Experiments on the 
English datasets yield the accuracies of 63%, 
80%, 56% and 56% for tasks C, D, E and F, re-
spectively. 
  Future works include identification of more 
precise rules for tasks A and B. We would also 
like to experiment with CRF for these two tasks.  
We would experiment with the various feature 
templates for tasks C, D, E and F. Future works 
also include experimentations with other ma-
chine learning techniques like maximum entropy 
and support vector machine.          
References  
Boguraev, B. and R. K. Ando. 2005. TimeML 
Compliant Text Analysis for Temporal Rea-
soning. In Proceedings of Nineteenth Interna-
tional Joint Conference on Artificial Intelli-
gence (IJCAI-05), Edinburgh, Scotland, Au-
gust, pages 997?1003. 
Chambers, N., S., Wang, and D., Jurafsky. , 
2007. Classifying Temporal Relations between 
Events. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, Prague, Czech Republic, 
June, pages 173?176. 
 Lafferty, J., McCallum, A., and Pereira, F. 
Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Se-
quence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning, 
2001. 
Mani, I., B., Wellner, M., Verhagen, and J. 
Pustejovsky. 2007. Three Approaches to 
Learning TLINKs in TimeML. Technical Re-
port CS-07-268, Computer Science Depart-
ment, Brandeis University, Waltham, USA. 
Mani, I., Wellner, B., Verhagen, M., Lee C.M.,   
Pustejovsky, J. 2006. Machine Learning of 
Temporal Relation. In Proceedings of the 
COLING/ACL, Sydney, Australia, ACL. 
Mao, T., Li., T., Huang, D., Yang, Y. 2006. Hy-
brid Models for Chinese Named Entity Rec-
ognition. In Proceedings of the Fifth SIGHAN 
Workshop on Chinese Language Processing. 
Sha, F., Pereira, F. 2003. Shallow  Parsing  with  
Conditional Random Fields. In Proceedings of  
HLT-NAACL, 2003. 
Verhagen, M., Gaizauskas, R., Schilder, F., Hep-
ple, M., Katz, G., Pustejovsky, and J.: SemE-
val-2007 Task 15: TempEval Temporal Rela-
tion Identification. 2007. In Proceedings of the 
SemEval-2007, Prague, June 2007, pages 75-
80. 
350
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
JU_CSE: A CRF Based Approach to Annotation of Temporal Expres-
sion, Event and Temporal Relations 
 
 
Anup Kumar Kolya1, Amitava Kundu1, 
 Rajdeep Gupta1  
Asif Ekbal2, Sivaji Bandyopadhyay1 
1Dept. of Computer Science & Engineering 2Dept. of Computer Science & Engineering 
Jadavpur Univeristy IIT Patna 
Kolkata-700 032, India Patna-800 013, India 
{anup.kolya,amitava.jucse, 
rajdeepgupta20}@gmail.com 
asif@iitp.ac.in, 
sivaji_ju_cse@yahoo.com 
 
 
 
 
 
Abstract 
In this paper, we present the JUCSE system, 
designed for the TempEval-3 shared task. The 
system extracts events and temporal infor-
mation from natural text in English. We have 
participated in all the tasks of TempEval-3, 
namely Task A, Task B & Task C. We have 
primarily utilized the Conditional Random 
Field (CRF) based machine learning tech-
nique, for all the above tasks. Our system 
seems to perform quite competitively in Task 
A and Task B. In Task C, the system?s per-
formance is comparatively modest at the ini-
tial stages of system development. We have 
incorporated various features based on differ-
ent lexical, syntactic and semantic infor-
mation, using Stanford CoreNLP and Wordnet 
based tools. 
1 Introduction 
Temporal information extraction has been a popu-
lar and interesting research area of Natural Lan-
guage Processing (NLP) for quite some time. 
Generally, a lot of events are described in a variety 
of newspaper texts, stories and other important 
documents where the different events described 
happen at different time instants. The temporal 
location and ordering of these events are either 
specified or implied. Automatic identification of 
time expressions and events and annotation of 
temporal relations constitute an important task in 
text analysis. These are also important in a wide 
range of NLP applications that include temporal 
question answering, machine translation and doc-
ument summarization.  
A lot of research in the area of temporal infor-
mation extraction has been conducted on multiple 
languages, including English and several European 
languages. The TimeML was first developed in 
2002 in an extended workshop called TERQAS 
(Time and Event Recognition for Question An-
swering Systems) and, in 2003, it was further de-
veloped in the context of the TANGO workshop 
(TimeML Annotation Graphical Organizer). Since 
then most of the works in this research arena have 
been conducted in English. The variety of works 
include TimeML (Pustejovsky et al, 2003), the 
development of a temporally annotated corpus 
Time-Bank (Pustejovsky et al, 2003), the temporal 
evaluation challenges TempEval-1 (Verhagen et 
al., 2007), TempEval-2 (Pustejovsky and Verha-
gen, 2010). In the series of Message Understanding 
Conferences (MUCs) that started from 1987 and 
the Sheffield Temporal Annotation scheme 
(STAG) (Setzer &Gaizauskas, 2000) the  aim  was 
to identify events in news text and determine their 
relationship with points on a temporal line. 
In the series of TempEval evaluation exercises, 
TempEval-1 was the first one where the focus was 
on identification of three types of temporal rela-
tion: relation between an event and a time expres-
sion in the same sentence, relation between an 
64
event and the document creation time, and relation 
between two main events in consecutive sentences. 
 TempEval-2 was a follow up to TempEval-1 
and consisted of six subtasks rather than three. It 
added (i) identification of time expressions and 
determination of values of the attributes TYPE and 
VAL (ii) identification of event expressions and 
determination of its attribute values. It included the 
previous three relation tasks from TempEval-1 and 
an additional task of annotating temporal relation 
between a pair of events where one subordinates 
the other.  
We have participated in all three tasks of 
TempEval-3- Task A, Task B and Task C. A com-
bination of CRF based machine learning and rule 
based techniques has been adopted for temporal 
expression extraction and determination of attrib-
ute values of the same   (Task A). We have used a 
CRF based technique for event extraction (Task 
B), with the aid of lexical, semantic and syntactic 
features. For determination of event attribute val-
ues we have used simple rule based techniques. 
Automatic annotation of temporal relation between 
event-time in the same sentence, event-DCT rela-
tions, mainevent-mainevent relations in consecu-
tive sentences and subevent-subevent relations in 
the same sentences has been introduced as a new 
task (Task-C) in the TempEval-3 exercise. We 
have adopted a CRF based technique for the same 
as well. 
2 The JU_CSE System Approach  
The JU_CSE system for the TempEval-3 shared 
task uses mainly a Conditional Random Field 
(CRF) machine learning approach to achieve Task 
A, Task B & Task C. The workflow of our system 
is illustrated in Figure 1. 
2.1 Task A: Temporal Expression Identifica-
tion and Normalization 
Temporal Expression Identification: 
 We have used CRF++ 0.571, an open source im-
plementation of the Conditional Random Field 
(CRF) machine learning classifier for our experi-
ments. CRF++ templates have been used to capture 
the relation between the different features in a se-
quence to identify temporal expressions. Temporal 
                                                        
1 http://crfpp.googlecode.com/svn/trunk/doc/index.html 
expressions mostly appear as multi-word entities 
such as ?the next three days?. Therefore the use of 
CRF classifier that uses context information of a 
token seemed most appropriate.  
 Initially, all the sentences have been changed to 
a vertical token-by-token level sequential structure 
for temporal expressions representation by a B-I-O 
encoding, using a set of mostly lexical features. In 
this encoding of temporal expression, ?B? indi-
cates the ?beginning of sequence?, ?I? indicates a 
token inside a sequence and ?O? indicates an out-
side word. We have carefully chosen the features 
list based on the several entities that denote month 
names, year, weekdays, various digit expressions 
(day, time, AM, PM etc.) In certain temporal ex-
pression patterns (several months, last evening) 
some words (several, last) act as modifiers to the 
following words that represent the time expression. 
Temporal expressions include time expression 
modifiers, relative days, periodic temporal set, 
year-eve day, month name with their short pattern 
forms, season of year, time of day, decade list and 
so on. We have used the POS information of each 
token as a feature. We have carefully accounted for 
a simple intuition revelation that most temporal 
expressions contain some tokens conveying the 
?time? information while others possibly convey-
ing the ?quantity? of time. For example, in the ex-
pression ?next three days?, ?three? quantifies 
?days?. Following are the different temporal ex-
pressions lists that have been utilized: 
 
? A list of time expression modifiers: this, 
mid, recent, earlier, beginning, late etc. 
? A list of relative days: yesterday, tomor-
row etc. 
? A list of periodic temporal set: hourly, 
nightly etc. 
? A list of year eve day: Christmas Day, 
Valentine Day etc. 
? A list of month names with their short pat-
tern forms: April, Apr. etc. 
? A list of season of year: spring, winter etc. 
? A list of time of day: morning, afternoon, 
evening etc. 
? A list of decades list: twenties, thirties etc. 
 
 
65
  
  
Raw Text: 
For his part, Fidel Castro is the ultimate political 
survivor. People have predicted his demise so 
many times, and the US has tried to hasten it on 
several occasions. Time and again, he endures.  
? Tokenize with Stanford CoreNLP 
? Obtain POS tags of tokens 
? Extract features from tokens 
? Identify the features for event annotation and 
temporal annotation separately 
 
CRF  
 
Event & 
Time 
 Features 
T
ag E
V
E
N
T
 
tokens 
Tag 
TIMEX3 
tokens 
. 
       For???  OTHERS 
  nearly ???.. TIMEX3 
       forty?. ?  TIMEX3 
years??.. TIMEX3 
. 
. 
 
. 
People???  OTHERS 
have ???..   OTHERS 
      predicted ?. ?  EVENT 
his ????.. OTHERS 
. 
. 
Annotated Text 
 
For his part, Fidel Castro is the ultimate political survivor. 
People have <EVENT class="I_ACTION" 
eid="e1">predicted</EVENT> his <EVENT 
class="OCCURRENCE" eid="e2">demise</EVENT> so 
many times, and the US has <EVENT class="I_ACTION" 
eid="e3">tried</EVENT> to <EVENT 
class="OCCURRENCE" eid="e4">hasten</EVENT> it on 
several occasions. 
D
eterm
ine 
E
vent 
C
lass 
CoreNLP 
for ?type? 
& ?velue? 
<MAKEINSTANCE eiid="ei1? eventID="e1" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei2? eventID="e2" pos="NOUN" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei3? eventID="e3" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
R
ule based approach to obtain tense, as-
pect, polarity, m
odality etc. for events 
 
Enlist entity pairs with features 
<mainevent-mainevent> 
<event-event> 
<event-dct>  
<event-time> 
 
 
CRF  
 
Temporal Relations: 
 
<TLINK lid="l1" relType="BEFORE" 
eventInstanceID="ei1" relatedTo-
Time="t0" /> 
 
<TLINK lid="l2" relType="BEFORE" 
eventInstanceID="ei2" relatedToEven-
tInstance="ei1" /> 
Figure 1.The JU_CSE System Architecture 
66
Determination of Normalized value and type 
of Temporal Expressions: 
 Temporal expressions in documents are generally 
defined with the type and value attributes. All the 
temporal expressions can be differentiated into 
three types (i) explicit (ii) relative and (iii) implicit 
temporal expressions. For example, the expression 
?October 1998? refers to a specific month of the 
year which can be normalized without any addi-
tional information. On the other hand, the relative 
expression ?yesterday? can?t be normalized with-
out the knowledge of a corresponding reference 
time. The reference time can either be a temporal 
expression or the Document Creation Time marked 
in the document. Consider the following piece of 
text: ?Yesterday was the 50th independence of In-
dia?. The First Independence Day of India is 15th 
august 1947.? Here ?Yesterday? can be normal-
ized as ?15-08-1997?. It may be noted that infor-
mation such as ?First Independence Day of India? 
can be directly accessed from the timestamp calen-
dar, through the metadata of a document. The third 
type of temporal expressions includes implicit ex-
pressions such as names of festival days, birthdays 
and holidays or events. These expressions are 
mapped to available calendar timeline to find out 
their normalized values. 
 
Temporal 
Expression 
Type Value 
A couple of 
years 
 
DURATION P2Y 
October DATE ?1997-10? 
Every day SET P1D 
2 P.M. TIME 2013-02-01T14:00 
Now DATE PRESENT_REF" 
Table 1: TimeML normalized type and value attributes 
for temporal expressions 
 
We have implemented a combined technique us-
ing our handcrafted rules and annotations given by 
the Stanford CoreNLP tool to determine the ?type?-
s and ?value?-s. Four types TIME, DATE, 
DURATION and SET of temporal expressions are 
defined in the TimeML framework. Next, we have 
evaluated the normalized value of temporal expres-
sions using Document Creation Time (DCT) from 
the documents.  In this way, values of different 
dates have been inferred e.g. last year, Monday, 
and today. 
2.2 Task B: Extraction of Event Words and 
Determination of Event Attribute Values  
Event Extraction 
In our evaluation framework, we have used the 
Stanford CoreNLP tool extensively to tokenize, 
lemmatize, named-entity annotate and part-of-
speech tag the text portions of the input files. For 
event extraction, the features have been considered 
at word level, where each word has its own set of 
features. The general features used to train our 
CRF model are: 
Morphological Features: Event words are rep-
resented mostly as verbs and nouns. The major 
problem is detecting the events having non-verbal 
PoS labels. Linguistically, non-verbal wordforms 
are derived from verbal wordforms. Various inflec-
tional and derivational morphological rules are 
involved in the process of evolving from verbal to 
non-verbal wordforms. We have used a set of 
handcrafted rules to identify the suffixes such as (?-
ci?n?, ?-tion? or ?-ion?), i.e., the morphological 
markers of word token, where Person, Location 
and Organization words are not considered. The 
POS and lemma, in a 5-window (-2, +2), has been 
used for event extraction. 
Syntactic Feature: Different event words no-
tions are contained in the sentences such as: verb-
noun combinations structure, the complements of 
aspectual prepositional phrases (PPs) headed by 
prepositions and a particular type of complex 
prepositions. These notions are captured to be used 
as syntactic features for event extraction. 
WordNet Feature: The RiTa Wordnet2 package 
has been effectively used to extract different prop-
erties of words, such as Synonyms, Antonyms, 
Hypernyms, & Hyponyms, Holonyms, Meronyms, 
Coordinates, & Similars, Nominalizations, Verb-
Groups, & Derived-terms. We have used these 
Wordnet properties in the training file for the CRF 
in the form of binary features for verbs and nouns 
indicating if  the words like ?act?, ?activity?, ?phe-
nomenon? etc. occur  in different relations of the 
Wordnet ontology. 
                                                        
2 http://www.rednoise.org/rita/wordnet/documentation/ 
67
Features using Semantic Roles: We use Se-
mantic Role Label (SRL) (Gildea et el, 2002; Pra-
dhan et al 2004; Gurevich et al 2006) to identify 
different useful features for event extraction. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents; determine 
their arguments (agent, patient, etc.) and adjuncts 
(locative, temporal, etc.). Some of the other fea-
tures like predicate, voice and verb sub-
categorization are shared by all the nodes in the 
tree. In the present work, we use predicate as an 
event.  Semantic roles can be used to detect the 
events that are nominalizations of verbs such as 
agreement for agree or construction for construct.  
Event nominalizations often share the same seman-
tic roles as verbs, and often replace them in written 
language. Noun words, morphologically derived 
from verbs, are commonly defined as deverbal 
nouns. Event and result nominalizations constitute 
the bulk of deverbal nouns. The first class refers to 
an event/activity/process, with the nominal ex-
pressing this action (e.g., killing, destruction etc.). 
Nouns in the second class describe the result or 
goal of an action (e.g., agreement, consensus etc.). 
Many nominals denote both the event and result 
(e.g., selection). A smaller class is agent/patient 
nominalizations that are usually identified by suf-
fixes such as -er, -or etc., while patient nominaliza-
tions end with -ee, -ed (e.g. employee).   
Object information of Dependency Relations 
(DR): We have developed handcrafted rules to 
identify features for CRF training, based on the 
object information present in the dependency rela-
tions of parsed sentences. Stanford Parser (de 
Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different Part-of-Speech 
(PoS) tags of Penn Treebank is used to get the 
parsed sentences with dependency relations. The 
dependency relations are found out for the predi-
cates ?dobj? so that the direct object related com-
ponents in the ?dobj? predicate is considered as the 
feature for the event expression. Initially the input 
sentences are passed to the dependency parser3.  
From the parsed output verb noun combination 
direct object (dobj) dependency relations are ex-
tracted. These dobj relations basically inform us 
that direct object of a VP is the noun phrase which 
is the (accusative) object of the verb; the direct 
object of a clause is the direct object of the VP 
                                                        
3 http://nlp.stanford.edu:8080/parser/ 
which is the predicate of that clause. Within the 
dobj relation governing verb word and dependent 
noun words are acting as important features for 
event identification when dependent word is not 
playing any role in other dependency relation 
(nsubj, prep_of, nn ,etc.) of the sentence. 
 
In this way, we have set list of word tokens and 
its features to train the recognition model. Then the 
model will give to each word one of the valid la-
bels.  
Determination of various Event Attribute 
Values: 
Values of different event attributes have been 
computed as follows: 
Class: Identification of the class of an event has 
been done using a simple, intuitive, rule based ap-
proach. Here too, the hypernym list of an event 
token from RitaWordnet has been deployed to de-
termine the class of the respective event. In this 
case, OCCURRENCE has been considered the de-
fault class. 
Tense, Aspect, POS: These three attributes are 
the obligatory attributes of MAKEINSTANCE 
tags. To determine the tense, aspect and polarity of 
an event, we have used the ?parse? annotator in 
CoreNLP. We annotated each sentence with the 
Stanford dependency relations using the above an-
notator. Thereafter various specific relations were 
used to determine the tense, aspect and POS of an 
event token, with another rule based approach. For 
example, in the phrase ?has been abducted?, the 
token ?been? appears as the dependent in an ?aux? 
relation with the event token ?abducted?; and 
hence the aspect ?PERFECTIVE? is inferred. The 
value ?NONE? has been used as the default value 
for both tense and aspect. 
Polarity and Modality: Polarity of event tokens 
are determined using Stanford dependency rela-
tions too; here the ?neg? relation. To determine the 
modality we search for modal words in ?aux? rela-
tions with the event token. 
2.3 Task C: Temporal Relation Annotation 
We have used the gold-standard TimeBank fea-
tures for events and times for training the CRF. In 
the present work, we mainly use the various com-
binations of the following features:  
68
 
(i)  Part of Speech (POS) 
(ii)  Event Tense 
(iii)  Event Aspect 
(iv)  Event Polarity 
(v)  Event Modality 
(vi)  Event Class 
(vii)       Type of temporal expression 
(vii)  Event Stem 
(viii)  Document Creation Time (DCT). 
 
The following subsections describe how various 
temporal relations are computed. 
Event-DCT 
We take the combined features of every event pre-
sent in the text and the DCT for this purpose. 
 
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, following fea-
tures help us to identify the event-DCT relations, 
specially ?AFTER? temporal relations: 
(i)Modal Context: Whether or not the event word 
has one of the modal context words like- will, 
shall, can, may, or any of their variants (might, 
could, would, etc.).In the sentence: ?The entire 
world will [EVENT see] images of the Pope in Cu-
ba?. Here ?will? context word helps us to deter-
mine event-DCT relation ?AFTER?. 
(ii)Preposition Context: Any prepositions preced-
ing an event or time expression. We consider an 
example:?Children and invalids would be permit-
ted to [EVENT leave] Iraq?. Here the preposition 
to helps us to determine event-DCT relation 
?AFTER?. The same principle goes for time too: in 
the expressions on Friday and for nearly forty 
years, the prepositions on and for governs the time.  
(iii)Context word before or after temporal expres-
sion: context words like before, after, less than, 
greater than etc. help us to determine event-time 
temporal relation identification. Consider an ex-
ample: ?After ten years of [EVENT boom] ?.? 
Event-Time 
Derived Features: We extract all events from eve-
ry sentence. For every temporal expression in a 
sentence, we pair an event in the sentence with the 
former so that the temporal relation can be deter-
mined. 
Similar to annotation of event-DCT relations, 
here too, we have identified different types of con-
text based temporal expression features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, the following 
features help us to distinguish between event and 
time relations, specially ?AFTER? and ?BEFORE? 
temporal relations. The following features are de-
rived from text. 
(i)Type of temporal expression: Represents the 
temporal relationship holding between events, 
times, or between an event and a time of the event.   
(ii)Temporal signal: Represents temporal preposi-
tions ?on? (on this coming Sunday) and slightly 
contribute to the overall score of classifiers 
(iii)Temporal Expression in the target sentence: 
Takes the values greater than, less than, equal or 
none. These values contribute to the overall score 
of classifiers. 
Mainevent-Mainevent and Subevent-
Subevent 
The task demands that the main event of every sen-
tence be determined. As a heuristic decision, we 
have assumed that the first event that appears in a 
sentence is its main event. We pair up main events 
(if present) from consecutive sentences and use 
their combined features to determine their temporal 
relation. For the events belonging to a single sen-
tence, we take into account the combined features 
of all possible pairs of sentential events. 
   
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. 
(i)Relational context: If a relation holding be-
tween the previous event and the current event is 
?AFTER?, the current one is in the past. This in-
formation helps us to identify the temporal relation 
between the current event and successive event. 
(ii)Modal Context: Whether or not the event word 
has one of the context words like, will, shall, can, 
may, or any of their variants (might, could, would, 
etc.).  The verb and auxiliaries governing the next 
event play as an important feature in event-event 
temporal relation identification.   
69
(iii)Ordered based context: In event-event rela-
tion identification, when EVENT-1, EVENT-2, 
and EVENT-3 are linearly ordered, then we have 
assigned true/false as feature value from tense and 
aspect shifts in this ordered pair.  
(iv) Co-reference  based feature: We have used 
co-referential features as derived feature using our 
in-house system based on Standford CoreNLP tool, 
where two event words within or outside one sen-
tence are referring to the same event, i.e. two event 
words co-refer in a discourse.  
(v)Event-DCT relation based feature: We have 
included event-document creation times (DCT) 
temporal relation types as feature of event-event 
relation identification. 
(ii) Preposition Context: Any prepositions before 
the event or time, we consider an exam-
ple:?Children and invalids would be permitted to 
[EVENT leave] Iraq?. Here the preposition to 
helps us determine the event-DCT relation 
?AFTER?.  
(vi) Context word before or after temporal ex-
pression: Context words like before, after, less 
than, greater than help us determine event- event 
temporal relations .We consider an example:?After 
ten years of [EVENT boom] ?.? 
(vii)Stanford parser based clause boundaries 
features: The two consecutive sentences are first 
parsed using Stanford dependency parser and then 
clause boundaries are identified. Then, considering 
the prepositional context and tense verb of the 
clause, temporal relations are identified where all 
temporal expressions are situated in the same 
clause.  
 
 
3 Results and Evaluation 
For the extraction of time expressions and events 
(tasks A and B), precision, recall and F1-score 
have been used as evaluation metrics, using the 
following formulae: 
 
precision (P) = tp/(tp + fp) 
recall (R) = tp/(tp + fn) 
F-measure = 2 *(P * R) / (P + R). 
 
Where, tp is the number of tokens that are part of 
an extent in keys and response, fp is the number of 
tokens that are part of an extent in the response but 
not in the key, and fn is the number of tokens that 
are part of an extent in the key but not in the re-
sponse. Additionally attribute accuracies computed 
according to the following formulae have also been 
reported. 
 
Attr. Accuracy = Attr. F1 / Entity Extraction F1  
Attr. R = Attr. Accuracy * Entity R 
Attr. P = Attr. Accuracy * Entity P 
 
Performance in task C is judged with the aid of the 
Temporal Awareness score proposed by UzZaman 
and Allen (2011) 
The JU_CSE system was evaluated on the TE-3 
platinum data. Table 2 reports JU_CSE?s perfor-
mance in timex extraction Task A. Under the re-
laxed match scheme, the F1-score stands at 
86.38% while the strict match scheme yields a F1-
score of 75.41%. As far as TIMEX attributes are 
concerned, the F1-scores are 63.81% and 73.15% 
for value and type respectively.  
 
Timex Extraction Timex Attribute 
F1 P R Strict F1 Strict P Strict R 
Value 
F1 
Type 
F1 
Value 
Accuracy 
Type 
Accuracy 
86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68 
Table 2:JU_CSE system?s TE-3 Results on Timex Task A 
 
 
 
 
Event Extraction Event Attribute 
F1 P R 
Class 
F1 
Tense 
F1 
Aspect 
F1 
Class 
Accuracy 
Tense 
Accuracy 
Aspect 
Accuracy 
78.57 80.85 76.41 52.65 58.58 72.09 67.01 74.56 91.75 
Table 3:JU_CSE system?s TE-3 Results on Event Task B 
  
70
  
Table 3 reports the system?s performance in 
event extraction (Task B) on TE-3 platinum da-
ta. F1-score for event extraction is 78.57%. At-
tribute F1-scores are 52.65%, 58.58% and 
72.09% for class, tense and aspect respectively.  
In both entities extraction tasks recall is nota-
bly lower than precision. The F1-scores for 
event attributes are modest given that the attrib-
utes were computed using handcrafted rules. 
However, the handcrafted approach can be treat-
ed as a good baseline to start with. Normaliza-
tion is proved to be a challenging task. 
 
Task F1 P R 
Task-ABC 24.61 19.17 34.36 
Task-C 26.41 21.04 35.47 
Task-C-relation-only 34.77 35.07 34.48 
 
Table 4: JU_CSE system?s TE-3 Temporal Aware-
ness results on Task ABC, TaskC-only & TaskC-
relation-only 
 
 
Table 4 presents the Temporal Awareness F1-
score for TaskABC, TaskC and TaskC-relation-
only. For TaskC-only evaluation, the event and 
timex annotated data was provided and one had 
to identify the TLINKs and classify the temporal 
relations. In the TaskC-relation-only version the 
timex and event annotations including their at-
tributes as well as TLINKs were provided save 
the relation classes. Only the relation classes had 
to be determined. The system yielded a temporal 
awareness F1-score of 24.6% for TaskABC, 
26.41% for TaskC-only and 34.77% for TaskC-
relation-only version. 
 
4 Conclusions and Future Directions 
  
In this paper, we have presented the JU_CSE 
system for the TempEval-3 shared task. Our sys-
tem in TempEval-3 may be seen upon as an im-
provement over our earlier endeavor in 
TempEval-2. We have participated in all tasks of 
the TempEval-3 exercise. We have incorporated 
a CRF based approach in our system for all 
tasks. The JU_CSE system for temporal infor-
mation extraction is currently undergoing a lot 
of extensive experimentation. The one reported 
in this article seemingly has a significant scope 
of improvement. Preliminarily, the results yield-
ed are quite competitive and encouraging. Event 
extraction and Timex extraction F1-scores at 
78.58% and 86.38% encourage us to further de-
velop our CRF based scheme. We expect better 
results with additional features and like to con-
tinue our experimentations with other semantic 
features for the CRF classifier. Our rule-based 
approach for event attribute determination how-
ever yields modest F1-scores- 52.65% & 
58.58% for class and tense. We intend to explore 
other machine learning techniques for event at-
tribute classification. We also intend to use parse 
tree based approaches for temporal relation an-
notation. 
Acknowledgments 
This work has been partially supported by a 
grant from the English to Indian language Ma-
chine Translation (EILMT) project funded by 
the Department of Information and Technology 
(DIT), Government of India. We would also like 
to thank to Mr. Jiabul Sk. for his technical con-
tribution.  
 
References  
A. Setzer, and R. Gaizauskas. 2000. Annotating 
Events and Temporal Information in Newswire 
Texts. In LREC 2000, pages 1287?1294, Athens. 
D. Gildea, and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
James Pustejovsky, Jos? Castano, Robert Ingria, 
Roser Sauri, Robert Gaizauskas, Andrea Setzer, 
Graham Katz, and Dragomir Radev. 2003. 
TimeML: Robust specification of event and tem-
poral expressions in text. New directions in ques-
tion answering, 3: 28-34. 
Marc Verhagen, Robert Gaizauskas, Frank Schilder, 
Mark Hepple, Graham Katz, and James 
Pustejovsky. 2007. Semeval-2007 task 15: 
Tempeval temporal relation identification. In Pro-
ceedings of the 4th International Workshop on 
Semantic Evaluations, pages 75-80, ACL. 
71
Marc Verhagen, Roser Sauri, Tommaso Caselli, and 
James Pustejovsky. 2010. Semeval-2010 task 13: 
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 
57- 62. ACL. 
Olga Gurevich, Richard Crouch, Tracy H. King, and 
V. de Paiva. 2006. Deverbal Nouns in Knowledge 
Representation. Proceedings of FLAIRS, pages 
670?675, Melbourne Beach, FL. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chine. Proceedings of HLT/NAACL-2004, 
Boston, MA. 
UzZaman, N. and J.F. Allen (2011), ?Temporal 
Evaluation.? In Proceedings of The 49th Annual 
Meeting of the Association for Computational 
Linguistics: Human Language Technologies 
(Short Paper), Portland, Oregon, USA.
   
 
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 314?318,
Dublin, Ireland, August 23-24, 2014.
IITP: A Supervised Approach for Disorder Mention Detection and
Disambiguation
Utpal Kumar Sikdar, Asif Ekbal and Sriparna Saha
Department of Computer Science and Engineering
Indian Institute of Technology Patna, India
{utpal.sikdar,asif,sriparna}@iitp.ac.in
Abstract
In this paper we briefly describe our super-
vised machine learning approach for dis-
order mention detection system that we
submitted as part of our participation in
the SemEval-2014 Shared task. The main
goal of this task is to build a system that
automatically identifies mentions of clini-
cal conditions from the clinical texts. The
main challenge lies due in the fact that the
same mention of concept may be repre-
sented in many surface forms. We develop
the system based on the supervised ma-
chine learning algorithms, namely Condi-
tional Random Field and Support Vector
Machine. One appealing characteristics of
our system is that most of the features for
learning are extracted automatically from
the given training or test datasets with-
out using deep domain specific resources
and/or tools. We submitted three runs, and
best performing system is based on Condi-
tional Random Field. For task A, it shows
the precision, recall and F-measure values
of 50.00%, 47.90% and 48.90%, respec-
tively under the strict matching criterion.
When the matching criterion is relaxed, it
shows the precision, recall and F-measure
of 81.50%, 79.70% and 80.60%, respec-
tively. For task B, we obtain the accuracies
of 33.30% and 69.60% for the relaxed and
strict matches, respectively.
1 Introduction
The SemEval-2014 Shared Task 7 is concerned
with the analysis of clinical texts, particularly for
disorder mention detection and disambiguation.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The purpose of this task is to enhance current
research in Natural Language Processing (NLP)
methods used in the clinical domain. The task is
a continuation of the CLEF/eHealth ShARe 2013
Shared Task. In particular there were two specific
tasks, viz. (i). Task A: To identify disorder men-
tions from biomedicine domain and (ii) Task B:
To classify each mention with respect to the Uni-
fied Medical Language System (UMLS) Concept
Unique Identifier (CUI). The task is challenging
in the sense that the same mention of concept may
be represented in many surface forms and men-
tion may appear in the different parts of texts.
Some systems (Cogley et al., 2013; Zuccon et al.,
2013; Tang et al., 2013; Cogley et al., 2013) are
available for disorder mention detection. Look-
ing at the challenges and resources available at
our hand we planned to adapt our existing system
(Sikdar et al., 2013) for disorder mention detec-
tion. The original architecture was conceptualized
as part of our participation in the BioCreative-IV
Track-2 Shared Task on Chemical Compound and
Drug Name Recognition. Although our submit-
ted system for SemEval-14 shared task is in line
with BioCreative-IV
1
, it has many different fea-
tures and characteristics.
We develop three systems (e.g. Model-1:
sikdar.run-0, Model-2: sikdar.run-1 and Model-
3: sikdar.run-2) based on the popular supervised
machine learning algorithms, namely Conditional
Random Field (CRF) (Lafferty et al., 2001) and
Support Vector Machine (SVM) (Cortes and Vap-
nik, 1995; Joachims, 1999). The models were de-
veloped by varying the features and feature tem-
plates. A baseline model is constructed by us-
ing the UMLS MetaMap
2
tool. During testing
we merge the development set with the train-
ing set. Evaluation results on test data with the
benchmark set up show the F-measure values of
1
www.biocreative.org/tasks/biocreative-iv/chemdner/
2
http://mmtx.nlm.nih.gov/
314
48.90%, 46.50% and 46.50%, respectively under
the strict criterion. Under relaxed matching cri-
terion the models show the F-measure values of
80.60%, 78.20% and 79.60%, respectively. Our
submission for Task-B is simple in nature where
we consider only those mentions that are also pre-
dicted in the baseline model, i.e. only the com-
mon CUIs are considered. It shows the accuracies
of 33.30%, 31.90% and 33.20%, respectively un-
der strict matching criterion; and 69.60%, 69.60%
and 69.10%, respectively under the relaxed match-
ing criterion.
2 Method
Our method for disorder mention detection from
clinical text is based on the supervised machine
learning algorithms, namely CRF and SVM. The
key focus was to develop a system that could be
easily adapted to other domains and applications.
We submitted three runs defined as below:
Model-1:sikdar.run-0: This is based on CRF,
and makes use of the features as mentioned below.
Model-2:sikdar.run-1: This model is built by
training a SVM classifier with the same set of
features as CRF.
Model-3:sikdar.run-2: This model is constructed
by defining a heuristics that looks at the outputs
of both the models. For given instance, if one of
the models predicts it to belong to the category
of candidate disorder mention then this is given
more priority in assigning the class. We observed
performance improvement on the development set
with this heuristic.
We identify and implement different features,
mostly without using any deep domain knowledge
or domain-specific external resources and/or tools.
The features that are used to train the classifiers are
briefly described below:
? Context words: Surrounding words carry ef-
fective information to identify disorder men-
tion. In our case we consider the previous
three and next three words as the features.
? MetaMapmatch: MetaMap is a widely used
tool that maps biomedical mention to the
UMLS CUI
3
. In UMLS, there are 11 seman-
tic types denoting disorders. These are Con-
genital Abnormality, Acquired Abnormality,
Injury or Poisoning, Pathologic Function,
3
http://www.nlm.nih.gov/research/umls/
Disease or Syndrome, Mental or Behavioral
Dysfunction, Cell or Molecular Dysfunction,
Experimental Model of Disease, Anatomical
Abnormality, Neoplastic Process and Signs
and Symptoms. The training set is passed
through the MetaMap, and then we prepare a
list of mentions that belong to the UMLS se-
mantic types. A feature is thereafter defined
that takes a value of 1 if the current token ap-
pears in the list; otherwise the value becomes
0.
? Part-of-Speech (PoS) Information: In this
work, we use PoS information of the current
token as the feature. PoS information was
extracted from the GENIA tagger
4
V2.0.2,
which is a freely available resource.
? Root words: Stems or root words, which
are extracted form GENIA tagger V2.0.2, are
used as the feature.
? Chunk information: We use GENIA tagger
V2.0.2 to extract the chunk information. It
helps to identify the boundaries of disorder
mentions.
? Initial capital: The feature is set to true if the
first character of the current token is a capital
letter.
? All capital: The feature is set to true if all the
letters of the current token are capitalized.
? Stop words: A feature is defined that is set
to one if the current token appears in the list
of stop words.
? Word normalization: Word shapes refer to
the mapping of each word to their equiva-
lence classes. Each capitalized character of
the word is replaced by ?A?, small characters
are replaced by ?a? and digits are replaced by
?0?.
? Word suffix and prefix: These features in-
dicate the fixed-length character sequences
(here 4) stripped either from the end (suffix)
or beginning positions of words. This is use-
ful in the sense that disorder mentions share
some common sub-strings.
4
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger
315
? Unknown word: This feature is imple-
mented depending upon whether the current
token was found during training or not. For
the training set this has been set randomly.
? Word length: If the length of a token is more
than a predefined threshold (here 5) then it is
most likely a disorder mention. This feature
is defined with the observation that very short
words are most probably not disorder men-
tions.
? Alpha digit: If the current token contains
digit character(s), then the feature is set to
true otherwise false.
? Informative words: This feature is devel-
oped from the training dataset. The words or
the sequence of words that precede and fol-
low the disorder mentions could be useful for
mention detection. The most frequently oc-
curring words that appear within the context
of w
i+2
i?2
= w
i?2
. . . w
i+2
of w
i
are extracted
from the training data. Two different lists are
prepared, one for the informative words that
precede the mentions and the other contains
the informative words that follow the men-
tions. Thereafter we define two features that
fire for the words of these lists.
? Disorder mention prefix and suffix: We ex-
tract most frequently occurring prefixes and
suffixes of length 2 from the disorder men-
tions present in the training data. We pre-
pare two lists containing the prefix and suffix
sub-sequences (of length two) that appear at
least 10 times in the training set. We define
two features that go on/off depending upon
whether the current word contains any sub-
sequence present in the lists.
? Dynamic information: The feature is ex-
tracted from the output label(s) of the previ-
ous token(s). The feature value is determined
at run time.
3 Experimental Results
3.1 Datasets
In SemEval-2014 Shared task 7, three types of
data were provided- training, development and
test. Training data contains four different types
of notes- discharge, ecg, echo and radiology. De-
velopment data consists of notes of three different
domains, viz. discharge, echo and radiology. But
the test set contains only the discharge notes. For
a given document, the start and end indices are
mentioned for the disorder mentions. There are
199, 99 and 133 documents in the training, devel-
opment and test set, respectively.
3.2 Results and Analysis
We use a regular expression based simple pattern
(e.g. dot and space) matching techniques for the
sentence splitting and tokenization. We use C
++
based CRF
++
package
5
for CRF experiments. We
set the default values of the following parame-
ters (a). the hyper-parameter of CRF. With larger
value, CRF tends to overfit to the given training
data; (b). parameter which sets the cut-off thresh-
old for the features (default value is 1). CRF uses
only those features, having more than the cut-off
threshold in the given training data.
In case of SVM we used YamCha
6
toolkit
along with TinySVM
7
. We use the polynomial
kernel function of degree two. In order to denote
the boundaries of a multi-word disorder mention
properly we use the standard BIO encoding
scheme, where B, I and O represent the beginning,
intermediate and outside, respectively, for a
multi-word token. Please note that the mentions
are not continuous, i.e. they could appear at the
various positions of the text. For example, in the
sentence The left atrium is moderately dilated,
there is a single mention left atrium dilated. Its
BIO format is represented in Table 1.
Token Tag
The O
left B-Men
atrium I-Men
is O
moderately O
dilated I-Men
. O
Table 1: An example of BIO representation.
Experiments are conducted on the benchmark
setup as provided by the competition organizer. At
first we train our system using the training set and
evaluate using the development set in order to de-
5
http://crfpp.sourceforge.net
6
http://chasen-org/ taku/software/yamcha/
7
http://chasen.org/ taku/software/TinySVM/
316
System Strict Relaxed
P R F P R F
Baseline 19.9 29.0 23.6 44.9 63.0 52.4
Model-1 52.5 43.0 47.3 86.2 72.6 78.8
Model-2 49.3 41.0 44.8 82.8 70.6 76.2
Model-3 46.7 44.0 45.3 81.2 77.5 79.3
Table 2: Results on development set for Task A.
System Strict Relaxed
Accuracy Accuracy
Baseline 24.6 85.1
Model-1 31.2 72.5
Model-2 29.9 73.0
Model-3 31.8 72.4
Table 3: Results on development set for Task B.
termine the best configuration. We define a base-
line model by passing the development set to the
UMLS MetaMap tool. Its results along with the
baseline model are reported in Table 2 for Task A.
Evaluation shows that our proposed system per-
forms reasonably better compared to the baseline
model. It is also to be noted that Model-1 performs
better compared to the other two submitted mod-
els for the strict matching, but for relaxed evalu-
ation, Model-3 performs better than Model-1 and
Model-2. Under strict matching criterion, Model-
1 achieves 2.7% and 5.0% increments in precision
over the second and third models, respectively.
For relaxed matching, Model-3 achieves 4.9% and
6.9% increments in recall over the first and sec-
ond models, respectively. Results on the develop-
ment set for Task-B are reported in Table 3. Please
note that although our system performs better than
the baseline in terms of strict matching, it does not
show better accuracy under relaxed matching cri-
terion. This is because our system for Task-B is
developed by considering only those mentions that
lie in the intersection of baseline and CRF models.
As a result many mentions are missed. During fi-
nal submissions we merged development sets with
the respective training sets, and perform evalua-
tion on the test sets. We report our results on the
test sets in Table 4 and Table 5 for Task-A and
Task-B, respectively.
We carefully analyze the results and find that
most of the errors encountered because of the dis-
contiguous mentions. Different components of a
mention may be mapped to the different concepts.
In our system we treat two mentions as a single
System Strict Relaxed
P R F P R F
Model-1 50.0 47.9 48.9 81.5 79.7 80.6
Model-2 47.3 45.8 46.5 78.9 77.6 78.2
Model-3 45.0 48.1 46.5 76.9 82.6 79.6
Table 4: Evaluation results on test set for Task A.
System Strict Relaxed
Accuracy Accuracy
Model-1 33.3 69.6
Model-2 31.9 69.6
Model-3 33.2 69.1
Table 5: Results of Task B for the test set.
unit if they have some shared tokens. For exam-
ple, the sentence ?She also notes new sharp pain in
left shoulder blade/back area? contains two differ-
ent mentions, viz.?pain shoulder blade? and ?pain
back?. Here shared word of these two mentions
is ?pain?, but we consider these two mentions as
a single unit such as ?pain shoulder blade back?.
This contributes largely to the errors that our sys-
tem faces for the first task. For the second task,
we miss a number of mentions, and this can be
captured if we directly match the system identified
mentions to the entire UMLS database.
4 Conclusion
In this paper we report on our works as part of our
participation in the SemEval-2014 shared task re-
lated to clinical text mining. We submitted three
runs for both the tasks, viz. disorder mention de-
tection and disambiguation. Our submitted runs
for the first task are based on CRF and SVM. We
make use of a set of features that are not very
domain-specific. The system developed for the
second task is very simple and is based on UMLS
Meta Map tool.
There are many avenues for future research:
identification of more features for the first task;
use of some domain-specific resources and/or
tools for the first task; use of entire UMLS the-
saurus for mapping the disorder mentions; use
of some machine learning techniques for disam-
biguation. We also plan to investigate how sys-
tematic feature selection, ensemble learning and
machine learning optimization have impact on dis-
order mention detection and disambiguation.
317
References
James Cogley, Nicola Stokes, and Joe Carthy. 2013.
Medical Disorder Recognition with Structural Sup-
port Vector Machines. In Proceedings of CLEF.
Corinna Cortes and Vladimir Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273?297.
Thorsten Joachims, 1999. Making Large Scale SVM
Learning Practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
John Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML, pages 282?289.
Utpal Kumar Sikdar, Asif Ekbal, and Sriparna Saha.
2013. Domain-independent Model for Chemical
Compound and Drug Name Recognition. Proceed-
ings of the Fourth BioCreative Challenge Evaluation
Workshop, vol. 2:158?161.
Buzhou Tang, Yonghui Wu, M. Jiang, J. C. Denny, and
Hua Xu. 2013. Recognizing and Encoding Disorder
Concepts in Clinical Text using Machine Learning
and Vector Space Model. In Proceedings of CLEF.
Guido Zuccon, A. Holloway, B. Koopman, and
A. Nguyen. 2013. Identify Disorders in Health
Records using Conditional Random Fields and
Metamap. In Proceedings of CLEF.
318
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 319?323,
Dublin, Ireland, August 23-24, 2014.
IITP:Supervised Machine Learning for Aspect based Sentiment Analysis
Deepak Kumar Gupta
Indian Institute of Technology Patna
Patna, India
deepak.mtmc13@iitp.ac.in
Asif Ekbal
Indian Institute of Technology Patna
Patna, India
asif@iitp.ac.in
Abstract
The shared task on Aspect based Senti-
ment Analysis primarily focuses on mining
relevant information from the thousands
of online reviews available for a popular
product or service. In this paper we re-
port our works on aspect term extraction
and sentiment classification with respect
to our participation in the SemEval-2014
shared task. The aspect term extraction
method is based on supervised learning
algorithm, where we use different classi-
fiers, and finally combine their outputs us-
ing a majority voting technique. For senti-
ment classification we use Random Forest
classifier. Our system for aspect term ex-
traction shows the F-scores of 72.13% and
62.84% for the restaurants and laptops re-
views, respectively. Due to some techni-
cal problems our submission on sentiment
classification was not evaluated. However
we evaluate the submitted system with the
same evaluation metrics, and it shows the
accuracies of 67.37% and 67.07% for the
restaurants and laptops reviews, respec-
tively.
1 Introduction
Nowadays user review is one of the means to drive
the sales of products or services. There is a grow-
ing trend among the customers who look at the on-
line reviews of products or services before taking
a final decision. In sentiment analysis and opinion
mining, aspect extraction aims to extract entity as-
pects or features on which opinions have been ex-
pressed (Hu and Liu, 2004; Liu, 2012). An aspect
is an attribute or component of the product that
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
has been commented on in a review. For exam-
ple:?Dell Laptop has very good battery life and
click pads?. Here aspect terms are battery life and
click pads. Sentiment analysis is the task of iden-
tifying the polarity (positive, negative or neutral)
of review. Aspect terms can influence sentiment
polarity within a single domain. As an example,
for the restaurant domain cheap is usually posi-
tive with respect to food, but it denotes a negative
polarity when discussing the decor or ambiance
(Brody and Elhadad, 2010).
A key task of aspect based sentiment analysis
is to extract aspects of entities and determine the
sentiment corresponding to aspect terms that have
been commented in review document. In recent
times there has been huge interest to identify as-
pects and sentiments simultaneously. The method
proposed in (Hu and Liu, 2004) is based on infor-
mation extraction (IE) approach that identifies fre-
quently occurring noun phrases using association
mining. Some other works include the methods,
viz those that define aspect terms using a manually
specified subset of the Wikipedia category (Fahrni
and Klenner, 2008) hierarchy, unsupervised clus-
tering technique (Popescu and Etzionir, 2005) and
semantically motivated technique (Turney, 2002)
etc. Our proposed approach for aspect term ex-
traction is based on supervised machine learning,
where we build many models based on different
classifiers, and finally combine their outputs us-
ing majority voting. Before combining, the out-
put of each classifier is post-processed with a set
of heuristics. Each of these classifiers is trained
with a moderate set of features, which are gen-
erated without using any domain-specific knowl-
edge and/or resources. Our submitted system
for the second task is based on Random Forest
(Breiman, 2001).
319
2 Tasks
The SemEval-2014 shared task on Aspect based
Sentiment Analysis
1
focuses on identifying the
aspects of a given target entities and the senti-
ment expressed towards each aspect. A bench-
mark setup was provided with the datasets con-
sisting of customer reviews with human-annotated
annotations of the aspects and their polarity infor-
mation. There were four subtasks, and we partic-
ipated in the first two of them. These are defined
as follows:
Subtask-1: The first task is related to aspect
term extraction. Given a set of sentences with
pre-identified entities, identify the aspect terms
present in the sentence and return a list containing
all the distinct aspect terms.
Substask-2: The second task addresses the as-
pect term polarity. For a given set of aspect terms
within a sentence, determine whether the polarity
of each aspect term is positive, negative, neutral or
conflict (i.e. both positive and negative).
3 Methods
3.1 Pre-processing
Each review is in the XML form. At first we ex-
tract the reviews along with their identifiers. Each
review is tokenized using the Stanford parser
2
and
Part-of-Speech tagged using the Stanford PoS tag-
ger
3
. At the various levels we need the chunk-
level information. We extract these information
using the OpenNLP chunker available at
4
.
3.2 Aspect Term Extraction
The approach we adopted for aspect term extrac-
tion is based on the supervised machine learn-
ing algorithm. An aspect can be expressed by
a noun, adjective, verb or adverb. But the re-
cent research in (Liu, 2007) shows that 60-70%
of the aspect terms are explicit nouns. The aspect
terms could also consist of multiword entities such
as ?battery life? and ?spicy tuna rolls? etc. As
the classification algorithms we make use of Se-
quential minimal optimization (SMO), Multiclass
classifier, Random forest and Random tree. For
faster computation of Support Vector Machine,
SMO (Platt, 1998) was proposed. Random tree
(Breiman, 2001) is basically a decision tree, and
1
http://alt.qcri.org/semeval2014/task4/
2
http://nlp.stanford.edu/software/lexparser.shtml
3
http://nlp.stanford.edu/software/tagger.shtml
4
http://opennlp.sourceforge.net/models-1.5/
in general used as a weak learner to be included in
some ensemble learning method. Multiclass clas-
sifier is a meta learner based on binary SMO. This
has been converted to multiclass classifier using
the pairwise method. In order to reduce the errors
caused by the incorrect boundary identification we
define a set of heuristics, and apply on each output.
At the end these models are combined together us-
ing a simple majority voting.
We implement the following set of features for
aspect terms extraction.
? Local context: Local contexts that span the
preceding and following few tokens of the
current word are used as the features. Here
we use the previous two and next two tokens
as the features.
? Part-of-Speech information: Part-of-
Speech(PoS)information plays an important
role in identifying the aspect terms. We use
the PoS information of the current token as
the feature.
? Chunk Information: Chunk information
helps in identifying the boundaries of aspect
terms. This is particularly more helpful to
recognize multiword aspect terms.
? Root word: Roots of the surface forms are
used as the features. We use the Porter Stem-
mer algorithm
5
to extract the root forms.
? Stop word: We use the list of stop words
available at
6
. A feature is defined that takes
the value equal to 1 or 0 depending upon
whether it appears in the training/test set or
not.
? Length: Length of token plays an important
role in identifying the aspect terms. We as-
sume an entity as the candidate aspect term
if its length exceeds a predefined threshold
value equal to five.
? Prefix and Suffix: Prefix and suffix of fixed
length character sequences are stripped from
each token and used as the features of classi-
fier. Here we use the prefixes and suffixes of
length upto three characters as the features.
5
http://tartarus.org/martin/PorterStemmer/java.txt
6
http://ir.dcs.gla.ac.uk/resources/linguistic utils/stop words
320
? Frequent aspect term: We extract the the as-
pect terms from the training data, and prepare
a list by considering the most frequently oc-
curring terms. We consider an aspect term to
be frequent if it appears at least five times in
the training data. A feature is then defined
that fires if and only if the current token ap-
pears in this list.
The output of each classifiers is post-processed
with a set of hand-crafted rules, defined as below:
Rule 1: If the PoS tag of the target token is noun,
chunk tag is I-NP (denoting the intermediate to-
ken of a noun phrase) and the observed class of the
previous token is O (other than aspect terms) then
the current token should be assigned the class B-
Aspect (denotes the beginning of an aspect term).
Rule 2: If the current token has PoS tag noun,
chunk tag I-NP and the observed class of the im-
mediately preceding token is B-Aspect then the
current token should be assigned the class I-Aspect
(denoting the intermediate token).
3.3 Polarity Identification
Polarity classification of aspect terms is the classi-
cal problem in sentiment analysis. The task is to
classify the sentiments or opinions into semantic
classes such as positive, negative, and neutral. We
develop a Random Forest classifier for this task.
In this particular task one more class conflict is in-
troduced. It is assigned if the sentiment can either
be positive or negative. For classification we make
use of some of the features such as local context,
PoS, Chunk, prefix and suffix etc., as defined in the
previous Subsection. Some other problem-specific
features that we implement for sentiment classifi-
cation are defined as below:
? MPQA feature: We make use of MPQA
subjectivity lexicon (Wiebe and Mihalcea,
2006) that contains sentiment bearing words
as feature in our classifier. This list was pre-
pared semi-automatically from the corpora of
MPQA
7
and Movie Review dataset
8
. A fea-
ture is defined that takes the values as fol-
lows: 1 for positive; -1 for negative; 0 for
neutral and 2 for those words that do not ap-
pear in the list.
? Function words: A list of function words is
7
http://cs.pitt.edu/mpqa/
8
http://cs.cornell.edu/People/pabo/movie-review-data/
compiled from the web
9
. A binary-valued
feature is defined that fires for those words
that appear in this list.
4 Experiments and Analysis
We use the datasets and the evaluation scripts as
provided by the SemEval-2014 shared task orga-
nizer.
4.1 Datasets
The datasets comprise of the domains of restau-
rants and laptop reviews. The training sets con-
sist of 3,044 and 3,045 reviews. There are 3,699
and 2,358 aspect terms, respectively. The test set
contains 800 reviews for each domain. There are
1,134 and 654 test instances in the respective do-
mains.
4.2 Results and Analysis
At first we develop several machine learning mod-
els based on the different classification algorithms.
All these classifiers were trained using the same
set of features as mentioned in Section 3. We
use the default implementations of these classi-
fiers in Weka
10
. We post-process the outputs of
all the models using some heuristics. Finally, all
these classifiers are combined together using ma-
jority voting. It is to be noted that we determine
the best configuration by carrying out different ex-
periments on the development set, which is con-
structed by taking a part of the training set, and fi-
nally blind evaluation is performed on the respec-
tive test set. We use the evaluation script provided
with the SemEval-2014 shared task. The training
sets contain multiword aspect terms, and so we use
the standard BIO notation
11
for proper boundary
marking.
Experiments show the precision, recall and F-
score values 77.97%, 72.13% and 74.94%, respec-
tively for the restaurant dataset. This is approxi-
mately 10 points below compared to the best sys-
tem. But it shows the increments of 4.16 and
27.79 points over the average and baseline mod-
els, respectively. For the laptop dataset we ob-
tain the precision, recall and F-score values of
70.74%, 62.84% and 66.55%, respectively. This
is 8 points below the best one and 10.35 points
9
http://www2.fs.u-bunkyo.ac.jp/ gilner/wordlists.html
10
www.cs.waikato.ac.nz/ml/weka/
11
B, I and O denote the beginning, intermediate and out-
side tokens
321
Model precision recall F-score
Random Tree 65.21 59.63 62.29
Random Forest 70.93 62.69 66.55
SMO 71.18 64.22 67.52
Multiclass 73.44 68.50 70.88
Ensemble 77.97 72.13 74.94
Best system 85.35 82.71 84.01
Average 76.74 67.26 70.78
Baseline - - 47.15
Table 1: Result of Task-A for restaurants dataset
with different classifiers (in %).
Model precision recall F-score
Random Tree 56.52 56.17 56.34
Random Forest 58.38 58.02 58.19
SMO 63.62 63.22 63.39
Multiclass 65.30 64.90 65.09
Ensemble 70.74 62.84 66.55
Best system 84.80 66.51 74.55
Average 68.97 50.45 56.20
Baseline - - 35.64
Table 2: Results of aspect term extraction for lap-
tops dataset with different classifiers (in %).
above the average system. Compared to the base-
line it achieves more than 20 point increment. De-
tailed evaluation results for all the classifiers are
reported in Table 1 and Table 2 for restaurant and
laptop datasets, respectively. Results show that
multiclass classifier achieves the highest perfor-
mance with precision, recall and F-score values
of 73.44%, 68.50% and 70.88%, respectively for
the restaurant dataset. The same model shows the
highest performance with precision, recall and F-
score values of 65.30%, 64.90% and 65.09%, re-
spectively for the laptop dataset. Because of ma-
jority ensemble we observe increments of 4.06%
and 1.46% F-score points over the best individual
model, respectively.
We also perform error analysis to understand
the possible sources of errors. We show only the
confusion matrix for Task-A in Table 3. It shows
that in most cases I-ASP is misclassified as B-ASP.
System also suffers because of the misclassifica-
tion of aspect terms to others.
Experiments for classification are reported in
Table 4. Evaluation shows that the system
achieves the accuracies of 67.37% and 67.07% for
B-ASP I-ASP Other
B-ASP 853 15 269
I-ASP 114 213 142
Other 123 35 11431
Table 3: Confusion matrix for Task-A on restau-
rants dataset.
Datasets
#Aspect
Terms
#Correct
Identification
Accuracy
(in %)
Restaurants 1134 764 67.37
Laptops 654 438 67.07
Table 4: Results of aspect terms polarity (in %).
the restaurants and laptops datasets, respectively.
Please note that our system for the second task
was not officially evaluated because of the techni-
cal problems of the submitted zipped folder. How-
ever we evaluated the same system with the of-
ficial evaluation script, and it shows the accura-
cies as reported in Table 4. We observe that the
classifier performs reasonably well for the posi-
tive and negative classes, and suffers most for the
conflict classes. This may be due to the number
of instances present in the respective training set.
Results show that our system achieves much lower
classification accuracy (13.58 points below) com-
pared to the best system for the restaurant datasets.
However, for the laptop datasets the classification
accuracy is quite encouraging (just 3.42 points be-
low the best system). It is also to be noted that our
classifier achieves quite comparable performance
for both the datasets. Therefore it is more general
and not biased to any particular domain.
5 Conclusion
In this paper we report our works on aspect term
extraction and sentiment classification as part of
our participation in the SemEval-2014 shared task.
For aspect term extraction we develop an ensem-
ble system. Our aspect term classification model is
based on Random Forest classifier. Runs for both
of our systems were constrained in nature, i.e. we
did not make use of any external resources. Evalu-
ation on the shared task dataset shows encouraging
results that need further investigation.
Our analysis suggests that there are many ways
to improve the performance of the system. In fu-
ture we will identify more features to improve the
performance of each of the tasks.
322
References
Leo Breiman. 2001. Random forests. 45(1):5?32.
S. Brody and N. Elhadad. 2010. An unsupervised
aspect-sentiment model for online reviews. In Pro-
ceedings of NAACL, pages 804?812, Los Angeles,
CA.
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specic sentiment analysis of
adjectives. In Symsposium on Affective Language in
Human and Machine, pages 60?63. The Society for
the Study of Artificial Intelligence and Simulation of
Behavior (AISB).
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the 10th KDD,
pages 168?177, Seattle, WAs.
B. Liu. 2007. Exploring Hyperlinks, Contents, and
Usage Data. Springer.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
John C. Platt. 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vectorma-
chines. Technical report, ADVANCES IN KERNEL
METHODS - SUPPORT VECTOR LEARNING.
Ana-Maria Popescu and Oren Etzionir. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the Conference on HLT/EMNLP,
pages 339?346.
P. D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th ACL,
pages 417?424.
Janyce Wiebe and Rada Mihalcea. 2006. Word
sense and subjectivity. In Proceedings of the COL-
ING/ACL, pages 065?1072, Australia.
323
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 324?328,
Dublin, Ireland, August 23-24, 2014.
IIT Patna: Supervised Approach for Sentiment Analysis in Twitter
Raja Selvarajan and Asif Ekbal
Department of Computer Science and Engineering
Indian Institute of Technology Patna, India
{raja.cs10,asif}@iitp.ac.in
Abstract
In this paper we report our works for
SemEval-2014 Sentiment Analysis in
Twitter evaluation challenge. This is the
first time we attempt for this task, and
our submissions are based on supervised
machine learning algorithm. We use Sup-
port Vector Machine for both the tasks,
viz. contextual polarity disambiguation
and message polarity classification. We
identify and implement a small set of
features for each the tasks, and did not
make use of any external resources and/or
tools. The systems are tuned on the devel-
opment sets and finally blind evaluation is
performed on the respective test set, which
consists of the datasets of five different
domains. Our submission for the first
task shows the F-score values of 76.3%,
77.04%, 70.91%, 72.25% and 66.32% for
LiveJournal2014, SMS2013, Twitter2013,
Twitter2014 and Twitter2014Sarcasm
datasets, respectively. The system devel-
oped for the second task yields the F-score
values of 54.68%, 40.56%, 50.32%,
48.22% and 36.73%, respectively for the
five different test datasets.
1 Introduction
During the past few years, the communications in
the forms of microblogging and text messaging
have emerged and become ubiquitous. Opinions
and sentiments about the surrounding worlds are
widely expressed through the mediums of Twit-
ter messages (Tweets) and Cell phone messages
(SMS). The availability of social content gener-
ated on sites such as Twitter creates new opportu-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
nities to automatically study public opinion. Deal-
ing with these informal text genres presents new
challenges for data mining and language process-
ing techniques beyond those encountered when
working with more traditional text genres such as
newswire. Tweets and SMS messages are short
in length, usually a sentence or a headline rather
than a document. These texts are very informal in
nature and contains creative spellings and punctu-
ation symbols (Nakov et al., 2013). Text also con-
tains lots of misspellings, slang, out-of-vocabulary
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for reTweet and #hash-
tags. The kind of these specific features pose great
challenges for building various lexical and syntac-
tic resources and/or tools, which are required for
efficient processing of texts. These aspects also
introduce complexities to build the state-of-the-
art data mining systems. In recent times, there
has been a huge interest to mine and understand
the opinions and sentiments that people are com-
municating in social media (Barbosa and Feng,
2010; Bifet et al., 2011; Pak and Paroubek, 2010;
Kouloumpis et al., 2011). Recent studies show
the interests in sentiment analysis of Tweets across
a variety of domains such as commerce (Jansen
et al., 2009), health (Chew and Eysenbach, 2010;
Salathe and Khandelwal, 2011) and disaster man-
agement (Mandel et al., 2012).
Another aspect of social media data, such as
twitter messages, is that they include rich informa-
tion about the individuals involved in the commu-
nication. For e.g., twitter maintains information
about who follows whom. ReTweets (reshares of a
Tweet) and tags inside of Tweets provide discourse
information (Nakov et al., 2013). Efficient mod-
elling of such information is crucial in the sense
that it provides a mean to empirically study the
social interactions where opinion is conveyed.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
324
e.g., the MPQA corpus (Barbosa and Feng, 2005)
of newswire text; i-sieve (Kouloumpis et al., 2011)
and TASS corpus2 (Villena-Roman et al., 2013)
for Twitter sentiment. These resources were either
in non-social media or they were small and propri-
etary. They further focused on message-level sen-
timent. The SemEval-2013 shared task (Nakov et
al., 2013) on sentiment analysis in Twitter releases
SemEval Tweet corpus, which contains Tweets
and SMSmessages with sentiment expressions an-
notated with contextual phrase-level polarity as
well as an overall message-level polarity. Among
the 44 submissions, the highest-performing sys-
tem (Mohammad et al., 2013) made use of Sup-
port Vector Machine (SVM) classifier. It obtained
the F-scores of 69.02% in the message-level task
and 88.93% in the term-level task. Variety of fea-
tures were implemented based on surface-forms,
semantics, and sentiment features. They generated
two large wordsentiment association lexicons, one
from Tweets with sentiment-word hashtags, and
one from Tweets with emoticons. They showed
that in message-level task, the lexicon-based fea-
tures gained 5 F-score points over all the others.
SemEval-14 shared task
1
on sentiment analy-
sis in Twitter is a continuing effort to promote the
research in this direction. Similar to the previ-
ous year?s evaluation campaigns two primary tasks
were addressed in this year challenge. The first
task (i.e. Subtask A) deals with contextual polar-
ity disambiguation and the second task (i.e. Sub-
task B) was about message polarity classification.
For Subtask A, for a given message containing a
marked instance of a word or phrase, the goal is to
determine whether that instance is positive, nega-
tive or neutral in that context. In Subtask B, for a
given message, the task is to classify whether the
message is of positive, negative, or neutral sen-
timent. For messages that convey both positive
and negative sentiments, the stronger one should
be chosen.
In this paper we report on our submissions as
part of our first-time participation in this kind of
task (i.e. sentiment classification). We develop the
systems based on supervised machine learning al-
gorithm, namely Support Vector Machine (SVM)
(Joachims, 1999; Vapnik, 1995). We identify and
implement a very small set of features that do not
make use of any external resources and/or tools.
For each task the system is tuned on the devel-
1
http://alt.qcri.org/semeval2014/task9/
opment data, and finally blind evaluation is per-
formed on the test data.
2 Methods
We develop two systems, one for contextual polar-
ity disambiguation and the other for message po-
larity classification. Each of the systems is based
on supervised machine learning algorithm, namely
SVM. Support vector machines (Joachims, 1999;
Vapnik, 1995) have been shown to be highly ef-
fective at traditional text categorization, generally
outperforming many other classifiers such as naive
Bayes (Joachims, 1999; Vapnik, 1995). They are
large-margin, rather than probabilistic, classifiers.
For solving the two-class problem, the basic idea
behind the training procedure is to find a hyper-
plane, represented by vector ~w, that not only sepa-
rates the document vectors in one class from those
in the other, but for which the separation, or mar-
gin, is as large as possible. This search corre-
sponds to a constrained optimization problem; let-
ting c
j
in 1,-1 (corresponding to positive and neg-
ative classes, respectively) be the correct class of
the document d
j
, the solution could be written as:
~w :=
?
j
a
j
cj
~
d
j
, a
j
>= 0
where, the a
j
?s are obtained by solving a dual opti-
mization problem. Those
~
d
j
such that a
j
is greater
than zero are called support vectors, since they are
the only document vectors contributing to ~w. Clas-
sification of test instances consists simply of deter-
mining which side of ~w?s hyperplane they fall on.
2.1 Preprocessing
We pre-process Tweet to normalize it by replac-
ing all ?URLs? to ?http://url? and all user-ids
to ?@usr?, and this is performed by the regular
expression based simple pattern matching tech-
niques. We remove punctuation markers from the
start and end positions of Tweets. For e.g., ?the
day is beautiful!? is converted to ?the day is beauti-
ful?. Multiple whitespaces are replaced with single
whitespace. Stop-words are removed from each
review.
2.2 Features
In this work we use same set of features for both
the tasks. Each Tweet is represented as a vector
consisting of the following features:
1. Local contexts: We extract the unigrams and
bigrams from the training and test datasets.
325
A feature is defined that checks the occur-
rences of these n-grams in a particular Tweet
or phrase.
2. Upper case: This feature is binary valued
with a value set to 1 if all the characters of
a phrase or Tweet are capitalized, and 0 oth-
erwise. This indicates that the target message
or context contains either positive or negative
sentiment.
3. Elongated words: The feature checks
whether a word contains a character that re-
peats more than twice. This indicates the
presence of a positive sentiment word in the
surrounding. This was defined in lines with
the one reported in (Mohammad et al., 2013).
4. Hash tags: This feature checks the number
of hash tags in the Tweet. The value of this
feature is set equal to the absolute number of
features.
5. Repeated characters: This feature checks
whether the word(s) have at least three
consecutive repeated characters (e.g.,
happppppppy, hurrrrrey etc.). In such cases,
the words are normalized to contain only
upto two repeated characters. This helps to
capture the words having similar structures.
6. Negated contexts: A negated word can af-
fect the polarity of the target word. A negated
segment is defined as a sequence of tokens
that starts with a negation word (e..g, no,
couldn?t etc.) and ends with a punctuation
marks (e.g.,,,., :, ;, !, ?). All the words follow-
ing the negation word are suffixed with NEG-
ATIVE, and the polarity features are also
converted with NEGATIVE in line with (Mo-
hammad et al., 2013).
3 Experimental Results and Analysis
The SemEval-2014 shared task datasets are based
on SemEval-2013 competition datasets. It covers
a range of topics, including a mixture of entities,
products and events. Keywords and Twitter hash-
tags were used to identify messages relevant to the
selected topic. The selected test sets were taken
from the five different domains. We perform ex-
periment with the python based NLTK toolki
2
. We
2
http://www.nltk.org/
Class precision recall F-score
Positive 72.02 90.45 80.19
Negative 76.86 53.70 63.23
Neutral 7.69 22.22 3.45
Average 52.19 55.46 53.77
Table 1: Results on development set for Task-A
(%).
Class precision recall F-score
Positive 49.92 63.75 55.99
Negative 42.59 31.94 36.51
Neutral 59.54 53.49 56.35
Average 50.68 49.73 66.39
Table 2: Results on development set for Task-B (in
%).
carried out experiments with the different classi-
fiers. However we report the results of SVM as
it produced the highest accuracy with respect to
this particular feature set. We use the default pa-
rameters of SVM as implemented in this toolkit.
We submitted two runs, one for each task. Both
of our submissions were constrained in nature, i.e.
we did not make use of any additional resources
and/or tools to build our systems.
We perform several experiments using the de-
velopment set. Best results are reported in Table 1
and Table 2 for Task-A and Task-B, respectively.
Evaluation shows that for message polarity dis-
ambiguation we obtain the average precision, re-
call and F-score values of 52.19%, 55.46% and
53.77%, respectively. For message polarity clas-
sification we obtain the precision, recall and F-
Score values of 50.68%, 49.73% and 66.39%, re-
spectively. It is evident from the evaluation that
the first task suffers most due to the problems in
classifying the tweets having neutral sentiments,
whereas the second task faces difficulties in clas-
sifying the negative sentiments. We report the con-
fusion matrices in Table 3 and Table 4 for the first
gs\pred positive negative neutral
positive 502 50 3
negative 160 196 9
neutral 35 9 1
Table 3: Confusion matrix for A. Here, gs: Gold
standard; pred: Predicted class.
326
gs\pred positive negative neutral
positive 313 43 135
negative 102 92 94
neutral 212 81 337
Table 4: Confusion matrix for B. Here, gs: Gold
standard; pred: Predicted class.
and second development sets, respectively. Error
analysis suggests that most miss-classifications are
because of the less number of neutral instances
compared to the positive and negative instances in
Task-A. For the Task-B training set, the number
of instances of positive and neutral sentiments are
very low compared to the negative sentiment.
After tuning the systems on the development
sets, we perform blind evaluation on the test
datasets. Evaluation results on the test sets are
reported in Table 5 for both the tasks. The
evaluation is carried out based on the evaluation
scripts as provided by the organizers. For mes-
sage polarity disambiguation we obtain the high-
est F-score of 77.04% for the SMS data type
in Task-A. The system shows the F-scores of
76.03%, 70.91%, 72.25% and 66.35% for Live-
Journal2014, Twitter2013, Twitter2014 and Twit-
ter2014sarcasm, respectively. For the second task
the system attains the highest F-score value of
54.68% for the LiveJournal2014 dataset. For the
other datasets, the system shows the F-scores of
40.56%, 50.32%, 48.22% and 36.73% for the
SMS2013, Twitter2013 and Twitter2014Sarcasm,
respectively. We followed a simple approach that
needs fine-tuning in many places. Currently our
systems lack behind the best reported systems by
margins of approximately 11-18% F-scores for
Task-A, and 19-30% F-scores for Task-B.
4 Conclusion
In this paper we report our works as part of our
participation to the SemEval-14 shared task on
sentiment analysis for Twitter data. Our systems
were developed based on SVM. We use a small
set of features, and did not make use of any ex-
ternal resources and/or tools in any of the tasks.
Each of the systems is tuned on the development
set, and blind evaluation is performed on the test
set. Evaluation shows that our system achieves the
F-score values in the ranges of 66-76% for Task-A
and 36-55% for Task-B.
It is to be noted that this is our first participa-
Task Test-set Average
F-score
A LiveJournal2014 76.03
SMS2013 77.04
Twitter2013 70.91
Twitter2014 72.25
Twitter2014Sarcasm 66.35
B LiveJournal2014 54.68
SMS2013 40.56
Twitter2013 50.32
Twitter2014 48.22
Twitter2014Sarcasm 36.73
Table 5: Results on the test set.
tion, and there are many ways to improve the per-
formance of the models. Firstly we would like to
identify more features in order to improve the ac-
curacies. We also plan to come up with proper sets
of features for the two task. Efficient feature se-
lection techniques will be implemented to identify
the most effective feature set for each of the tasks.
We would like to apply evolutionary optimization
techniques to optimize the different issues of ma-
chine learning algorithm.
References
Luciano Barbosa and Junlan Feng. 2005. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. 39:2-3.
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald?a. 2011. Detecting Sentiment
Change in Twitter Streaming Data. Journal of Ma-
chine Learning Research - Proceedings Track, 17:5?
11.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis
of Tweets during the 2009 H1N1 Outbreak. PLoS
ONE, 5(11):e14118+.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169?2188.
Thorsten Joachims, 1999. Making Large Scale SVM
Learning Practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
327
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The
Good the Bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM, pages 538?541, Barcelona,
Spain.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A Demographic Analysis of Online Senti-
ment during Hurricane Irene. In Proceedings of
the Second Workshop on Language in Social Media,
LSM 12, Stroudsburg.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In Pro-
ceedings of Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June.
Alexander Pak and Patrick Paroubek. 2010. Twitter
Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives. In Proceedings
of the 5th InternationalWorkshop on Semantic Eval-
uation, SemEval 10, Los Angeles,USA.
Marcel Salathe and Shashank Khandelwal. 2011. As-
sessing Vaccination Sentiments with Online Social
Media: Implications for Infectious Disease Dynam-
ics and Control. PLoS Computational Biology,
7(10):e14118+.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Julio Villena-Roman, Sara Lana-Serrano, Eugenio
Martnez-Camara, Jose Carlos Gonzalez, and Cristo-
bal. 2013. Tass - Workshop on Sentiment Analy-
sis at SEPLN. In Proceedings of Procesamiento del
Lenguaje Natural, pages 50:37?44.
328
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 341?345,
Dublin, Ireland, August 23-24, 2014.
Indian Institute of Technology-Patna: Sentiment Analysis in Twitter
Vikram Singh, Arif Md. Khan and Asif Ekbal
Indian Institute of Technology Patna
Patna, India
(vikram.mtcs13,arif.mtmc13,asif)@iitp.ac.in
Abstract
This paper is an overview of the system
submitted to the SemEval-2014 shared
task on sentiment analysis in twitter. For
the very first time we participated in both
the tasks, viz contextual polarity disam-
biguation and message polarity classifi-
cation. Our approach is supervised in
nature and we use sequential minimal
optimization classifier. We implement
the features for sentiment analysis with-
out using deep domain-specific resources
and/or tools. Experiments within the
benchmark setup of SemEval-14 shows
the F-scores of 77.99%, 75.99%, 76.54
%, 76.43% and 71.43% for LiveJour-
nal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respec-
tively for Subtask A. For Subtask B we
obtain the F-scores of 60.39%, 51.96%,
52.58%, 57.25%, 41.33% for five different
test sets, respectively.
1 Introduction
In current era microblogging is an efficient way
of communication where people can communicate
without physical presence of receiver(s). Twitter
is the medium where people post real time mes-
sages to discuss on the different topics, and ex-
press their sentiments. The texts used in twit-
ter are generally informal and unstructured in na-
ture. Tweets and SMS messages are very short
in length, usually a sentence or a headline rather
than a document. These texts are very informal
in nature and contains creative spellings and punc-
tuation symbols. Text also contains lots of mis-
spellings, slang, out-of-vocabulary words, URLs,
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
and genre-specific terminology and abbreviations,
e.g., RT for re-Tweet and #hashtags. Such kinds of
structures introduce difficulties in building various
lexical and syntactic resources and/or tools, which
are required for efficient processing of texts. Find-
ing relevant information from these posts poses
big challenges to the researchers compared to the
traditional text genres such as newswire.
In recent times, there has been a huge inter-
est to mine and understand the opinions and sen-
timents that people are communicating in social
media (Barbosa and Feng, 2010; Bifet et al.,
; Pak and Paroubek, 2010; Kouloumpis et al.,
2011). There is a tremendous interest in sentiment
analysis of Tweets across a variety of domains
such as commerce (Jansen et al., 2009), health
(Chew and Eysenbach, 2010; Salathe and Khan-
delwal, 2011) and disaster management (Verma
et al., 2011; Mandel et al., 2012). Agarwal et
al.(Agarwal et al., 2011) used tree kernel decision
tree that made use of the features such as Part-
of-Speech (PoS) information, lexicon-based fea-
tures and several other features. They acquired
11,875 manually annotated Twitter data (Tweets)
from a commercial source, and reported an accu-
racy of 75.39%. Semantics has also been used as
the feature to improve the performance of senti-
ment analysis (Saif et al., 2012). For each ex-
tracted entity (e.g. iPhone) from Tweets, they
added its semantic concept (e.g. Apple product)
as an additional feature. Thereafter they devised
a method to measure the correlation of the rep-
resentative concept with negative/positive senti-
ment, and applied this approach to predict sen-
timent for three different Twitter datasets. They
showed that semantic features produce better re-
call and F-score when classifying negative senti-
ment, and better precision with lower recall and
F-score in positive sentiment classification. The
benchmark corpus were made available with the
SemEval-2013 shared task (Nakov et al., 2013) on
341
sentiment analysis in twitter. The datasets used are
from the domains of Tweets and SMS messages.
The datasets were labelled with contextual phrase-
level polarity and overall message-level polarity.
Among the 44 submissions, the support vector ma-
chine based system proposed in (Mohammad et
al., 2013) achieved the highest F-scores of 69.02%
for Task A, i.e. the message-level polarity and and
88.93% for Task B, i.e. term-level polarity.
The issues addressed in SemEval-13 are further
extended in SemEval-14 shared task
1
. The same
two tasks, viz. Subtask A and Subtask B denot-
ing contextual polarity disambiguation and mes-
sage polarity classification. The goal of Subtask
A is to determine, for a given message containing
a marked instance of a word or phrase, whether
that instance is positive, negative or neutral in that
context. Given a message, the task is to classify
it with its entirety whether it is positive, negative,
or neutral sentiment. For messages that convey
both positive and negative sentiments, the stronger
one should be chosen. In this paper we report
on our submitted systems for both the tasks. Our
evaluation for the first task shows the F-scores of
77.99%, 75.99%, 76.54%, 76.43% and 71.43% for
LiveJournal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respectively for
Subtask A. For Subtask B we obtain the F-scores
of 60.39%, 51.96%, 52.58%, 57.25%, 41.33% for
five different test sets, respectively.
2 Methods
In this section we describe preprocessing steps,
features and our methods for sentiment classifica-
tion
2.1 Preprocessing of Data
The data has to be pre-processed before being used
for actual machine learning training. Each Tweet
is processed to extract only those relevant parts
that are useful for sentiment classification. For
example, stop words are removed; symbols and
punctuation markers are filtered out; URLs are
replaced by the word URL etc. Each Tweet is
then passed through the ARK tagger developed by
CMU
2
for tokenization and Part-of-Speech (PoS)
tagging.
1
http://alt.qcri.org/semeval2014/task9/
2
http://www.ark.cs.cmu.edu/TweetNLP/
2.2 Approach
Our approach is based on supervised machine
learning. We explored different models such as
naive Bayes, decision tree and support vector ma-
chine. Based on the results obtained on the de-
velopment sets we finally select SVM for both
the tasks. We also carried out a number of ex-
periments with the various feature combinations.
Once the model is fixed with certain feature com-
binations, these are finally used for blind evalua-
tion on the test sets for both the tasks. We sub-
mit two runs, one for each task. Both of our sub-
missions were constrained in nature, i.e. we did
not make use of any additional resources and/or
tools to build our systems. We adapt a supervised
machine learning algorithm, namely Support Vec-
tor Machine (Joachims, 1999; Vapnik, 1995). We
use its sequential minimal optimization version for
faster training
3
. We use the same set of features
for both the tasks. Development sets are used to
identify the best feature combinations for both the
tasks. Default parameters as implemented inWeka
are used for the SVM experiments.
2.3 Features
Like any other classification algorithm, features
play an important role for sentiment classifica-
tion. For the very first time we participated in
this kind of task, and therefore had to spend quite
long time in conceptualization and implementa-
tion of the features. We focused on implementing
the features without using any domain-dependent
resources and/or tools. Brief descriptions of the
features that we use are presented below:
? Bag-of-words: Bag-of-words in the expres-
sion or in the entire Tweet is used as the fea-
ture(s).
? SentiWordNet feature: This feature is de-
fined based on the scores assigned to each
word of a Tweet using the SentiWordNet
4
. A
feature vector of length three is defined. The
scores of all words of the phrase or Tweet is
summed over and normalized in the scale of
3. We define the following three thresholds:
if the score is less than 0.5 then it is treated to
be a negative polarity; for the score above 0.8,
it is assumed to contain positive sentiment;
3
http://research.microsoft.com/en-
us/um/people/jplatt/smo-book.pdf
4
sentiwordnet.isti.cnr.it/
342
and the polarity is considered to be neutral
for all the other words. Depending upon the
score the corresponding bit of the feature vec-
tor is set.
? Stop word: If a Tweet/phrase is having more
number of stop words then it most likely con-
tains neutral sentiment. We obtain the stop
words from the Wikipedia
5
. We assume that
a particular Tweet or phrase most likely bears
a neutral sentiment if 20% of its words be-
long to the category of stop words.
? All Cap Words: This feature is defined to
count the number of capitalized words in an
entire Tweet/phrase. More the number of
capitalized words, more the chances of being
positive or negative sentiment bearing units.
While counting, the words preceded by # are
not considered. We include this with the as-
sumption that the texts written in capitalized
letters express the sentiment strongly.
? Init Cap: The words starting with capital-
ized letter contribute more towards classify-
ing it.
? Percent Cap: This feature is based on the
percentage of capitalized characters in a
Tweet/phrase. If this is more than 75%, then
most likely it is not of neutral type.
? Psmiley (+ve Smiley): Generally people use
smileys to represent their emotions. A smiley
present in a Tweet/phrase directly represents
its sentiment. A feature is defined that takes
the value equal to the number of positive smi-
leys. We make use of the list available at this
page
6
.
? Nsmiley (-ve Smiley): The value of this fea-
ture is set to the number of negative smileys
present in the Tweet. This list was also ob-
tained from the web
7
.
? NumberPostive words: This feature takes
the value equal to the number of positive
words present in the Tweet/phrase. We search
the adjective words present in the Tweet in
the SentiWordNet to determine whether it
bears positive sentiment.
5
http://en.wikipedia.org/wiki/Stop words
6
http://en.wikipedia.org/wiki/List of emoticons
7
http://en.wikipedia.org/wiki/List of emoticons
? NumberNegative words: This feature takes
the value equal to the number of negative
words present in the Tweet/phrase. The
words are again looked at the SentiWordNet
to determine its polarity.
? NumberNeutral words: This feature deter-
mines the number of neutral words present in
the Tweet or phrase. This information is ob-
tained by looking the adjective words in the
SentiWordNet.
? Repeating char: It has been seen that peo-
ple express strong emotion by typing a char-
acter many times in a Tweet. For exam-
ple, happppppppy, hurrrrrey etc. This feature
checks whether the word(s) have at least three
consecutive repeated characters.
? LenTweet: Length of the Tweet is used as
the feature. The value of this feature is set
equal to the number of words present in the
Tweet/phrase.
? Numhash: The value of this feature is set
equal to the number of hashtags present in the
Tweet.
3 Experiments and Analysis
SemEval-2014 shared task is a continuation of the
SemEval-2013 shared task. In 2014 shared task,
datasets from different domains were incorporated
with a wide range of topics, including a mixture of
entities, products and events. Messages relevant to
the topics are selected based on the keywords and
twitter hashtags.
The training set of Task-A has 4,914 positive,
2,592 negative and 384 neutral class instances.
The Task-B training set contains 3,057 positive,
1,200 negative and 3,941 neutral sentiments. De-
velopments sets contain 555, 45 and 365 positive,
negative and neutral sentiments, respectively for
the first task; and 493, 288 and 632 positive,
negative and neutral sentiments, respectively for
the second task. The selected test sets were taken
mainly from the following domains:
LiveJournal2014: 2000 sentences from Live-
Journal blogs;
SMS2013: SMS test from last year-used as a
progress test for comparison;
Twitter2013: Twitter test data from last year-used
as a progress test for comparison;
Twitter2014: A new Twitter test data of 2000
343
Model Avg. F-score
Model-1 75.75
Model-2 72.69
Model-3 75.45
Model-4 75.77
Table 1: Results for Task-A on development set(in
%).
Tweets;
Twitter2014Sarcasm: 100 Tweets that are known
to contain sarcasm.
We build different models by varying the fea-
tures as follows:
1. Model-1: This model is constructed by
considering the features, ?Repeating char?,
?Numhash?, ?LenTweet?, ?Percent Cap?,
?Init Cap?, ?All Cap?, ?Bag-of-words?,
?Nsmiley?, ?Psmiley?, ?SentiWordNet? and
?Stop Words?.
2. Model-2: This model is constructed by the
features ?Repeating char?, ?Percent Cap?,
?Numhash?, ?LenTweet?, ?Init Cap?,
?All Cap?, ?Bag-of-words?, ?SentiWord-
Net? and ?Stop Words?.
3. Model-3: This model is built by consid-
ering the features ?Repeating char?, ?Bag-
of-words?, ?SentiWordNet?, ?Nsmiley? and
?Psmiley?.
4. Model-4: The model incorporates the fea-
tures ?Repeating char?, ?Bag-of-words?,
?SentiWordNet?, ?Nsmiley?, ?Psmiley?,
?Stop Words?, ?Numhash?, ?LenTweet?,
?Init Cap? and ?All Cap?.
Results on the development set for Task-A are
reported in Table 1 that shows the highest perfor-
mance in Model-4 with the average F-score value
of 75.77%. Thereafter we use this particular fea-
ture combination for training SVM, and to report
the results. Detailed results are reported in Table
2 for both the tasks. It shows 77.99%, 75.99%,
76.54 %, 76.43% and 71.43% F-scores for the
LiveJournal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respectively for
Subtask A. For Subtask B we obtain the F-scores
of 60.39%, 51.96%, 52.58%, 57.25% and 41.33%
for the five different test sets, respectively. A
closer investigation to the evaluation results re-
veals that most of the errors are due to the con-
fusions between positive vs. neutral and negative
vs. neutral classes.
Comparisons with the best system(s) submitted
in this shared task show that we are behind ap-
proximately in the range of 6-14% F-score mea-
sures for all the domains for Task-A. Results that
we obtain in Task-B need more attention as these
fall much shorter compared to the best one (in the
the range of 14-18%).
Features used Classifier Result(Task A) Result(Task B)
SWN +ve LiveJournal2014 LiveJournal2014
SWN -ve 77.99 60.39
SWN neutral SMS2013 SMS2013
#Stop Words 75.99 51.96
#All Cap Words Twitter2013 Twitter2013
#Numhash 76.54 52.58
Len Tweet Twitter2014 Twitter2014
#Init Cap Words 76.43 57.25
% Init Cap SVM T2014S T2014S
#+ve Smiley 71.43 41.33
#-ve Smiley
#+ve Words
#-ve Words
#Neutral Words
#Bag of words
Rep character
Table 2: Result on test sets for Task-A and Task-B.
4 Conclusion
In this paper we report our works as part of our
participation to the SemEval-14 shared task on
sentiment analysis for twitter data. Our systems
were based on supervised classification, where we
fixed SVM to report the test results after conduct-
ing several experiments with different classifiers
on the development data. We implement a set of
features that are applied for both the tasks. Our
runs are constrained in nature, i.e. we did not make
use of any external resources and/or tools. Our re-
sults are quite promising that need further inves-
tigation. A closer analysis to the results suggest
that most of the errors are due to the confusions
between positive vs. neutral and negative vs. neu-
tral classes.
This is our first participation, and within the
short period of time we developed the systems
with reasonable accuracies. There are still many
ways to improve the performance. Possible im-
mediate future extension will be to investigate and
implement more features, specific to the task.
344
References
Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and
Rebecca Passonneau. 2011. Sentiment Analysis of
Twitter Data. ACL Workshop on Languages in So-
cial Media LSM-2011, pages 30?38.
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald?a. Detecting Sentiment Change
in Twitter Streaming Data. Journal of Machine
Learning Research - Proceedings Track, 17.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis
of Tweets during the 2009 H1N1 Outbreak. PLoS
ONE, 5(11):e14118+.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169?2188.
Thorsten Joachims, 1999. Making Large Scale SVM
Learning Practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The
Good the Bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM, pages 538?541, Barcelona,
Spain.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A Demographic Analysis of Online Senti-
ment during Hurricane Irene. In Proceedings of
the Second Workshop on Language in Social Media,
LSM 12, Stroudsburg.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-art in Sentiment Analysis of Tweets. In Pro-
ceedings of Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based System: Using Twitter for Disambiguat-
ing Sentiment Ambiguous Adjectives. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, SemEval 10, pages 436?439, Los Ange-
les,USA.
Hassan Saif, Yulan He, and Harith Alani. 2012. Se-
mantic Sentiment Analysis of Twitter. In ISWC?12
Proceedings of the 11th International Conference on
the Semantic Web - Volume Part I, pages 508?524.
Marcel Salathe and Shashank Khandelwal. 2011. As-
sessing Vaccination Sentiments with Online Social
Media: Implications for Infectious Disease Dynam-
ics and Control. PLoS Computational Biology,
7(10):e14118+.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Sudha Verma, Sarah Vieweg, William Corvey, Leysia
Palen, JamesMartin, Martha Palmer, Aaron Schram,
and Kenneth Anderson. 2011. Natural Language
Processing to the Rescue? Extracting Situational
Awareness Tweets during Mass Emergency. In Pro-
ceedings of the AAAI Conference on Weblogs and
Social Media, Velingrad.
345
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93?101,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Challenge of Fine-Grained
Named Entity Recognition and Classification
Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University, Germany
{ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de
Abstract
Named Entity Recognition and Classi-
fication (NERC) is a well-studied NLP
task typically focused on coarse-grained
named entity (NE) classes. NERC for
more fine-grained semantic NE classes has
not been systematically studied. This pa-
per quantifies the difficulty of fine-grained
NERC (FG-NERC) when performed at
large scale on the people domain. We
apply unsupervised acquisition methods
to construct a gold standard dataset for
FG-NERC. This dataset is used to bench-
mark methods for classifying NEs at var-
ious levels of fine-grainedness using clas-
sical NERC techniques and global contex-
tual information inspired fromWord Sense
Disambiguation approaches. Our results
indicate high difficulty of the task and pro-
vide a ?strong? baseline for future research.
1 Introduction
Named Entity Recognition and Classification (cf.
Nadeau and Sekine (2007)) is a well-established
NLP task relevant for nearly all semantic process-
ing and information access applications. NERC
has been investigated using supervised (McCallum
and Li, 2003), unsupervised (Etzioni et al, 2005)
and semi-supervised (Pas?ca et al, 2006b) learning
methods. It has been investigated in multilingual
settings (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) and special domains, e.g.
biomedicine (Ananiadou et al, 2004).
The classical NERC task is confined to coarse-
grained named entity (NE) classes established
in the MUC (MUC-7, 1998) or CoNLL (Tjong
Kim Sang, 2002) competitions, typically PERS,
LOC, ORG, MISC. While most recent work con-
centrates on feature engineering and robust statis-
tical models for various domains, few researchers
addressed the problem of recognizing and catego-
rizing fine-grained NE classes (such as biologist,
composer, or athlete) in an open-domain setting.
Fine-grained NERC is expected to be benefi-
cial for a wide spectrum of applications, includ-
ing Information Retrieval (Mandl and Womser-
Hacker, 2005), Information Extraction (Pas?ca et
al., 2006a) or Question-Answering (Pizzato et
al., 2006). However, manually compiling wide-
coverage gazetteers for fine-grained NE classes is
time-consuming and error-prone. Also, without an
extrinsic evaluation, it is difficult to define a priori
which classes are relevant for a particular domain
or task. Finally, prior research in FG-NERC is dif-
ficult to evaluate, due to the diversity of NE classes
and datasets used.
Accordingly, in the interest of a general ap-
proach, we address the challenge of capturing a
broad range of NE classes at various levels of con-
ceptual granularity. By turning FG-NERC into
a widely applicable task, applications are free to
choose relevant NE categories for specific needs.
Also, establishing a gold standard dataset for this
task enables comparative benchmarking of meth-
ods. However, the envisaged task is far from triv-
ial, given that the set of possible semantic classes
for a given NE comprises the full space of NE
classes, whereas descriptive nouns may be am-
biguous between a fixed set of meanings only.
The paper aims to establish a general frame-
work for FG-NERC by addressing two goals: (i)
we automatically build a gold standard dataset of
NE instances classified in context with fine-grain-
ed semantic class labels; (ii) we develop strong
baseline methods, to assess the aptness of standard
NLP approaches for this task. The two efforts are
strongly interleaved: a standardized dataset is not
only essential for (comparative) evaluation, but
also a prerequisite for classification approaches
based on supervised learning, the most successful
techniques for sequential labeling problems.
93
2 Related work
An early approach to FG-NERC is Alfonseca and
Manandhar (2002), who identify it as a problem
related to Word Sense Disambiguation (WSD).
They jointly address concept hierarchy learning
and instance classification using topic signatures,
yet the experiments are restricted to a small on-
tology of 9 classes. Similarly, Fleischman and
Hovy (2002) extend previous work from Fleis-
chman (2001) on locations and address the ac-
quisition of instances for 8 fine-grained person
classes. For supervised training they compile a
web corpus which is filtered using high-confident
classifications from an initial classifier trained on
seeds. Due to the limitations of their method to
create a good sample of training data, the perfor-
mance could not be generalized to held-out data.
Recent work takes the task of FG-NERC one
step further by (i) extending the number of classes,
(ii) relating them to reference concept hierar-
chies and (iii) exploring methods for building
training and evaluation data, or applying weakly
and unsupervised learning based on high-volume
data. Tanev and Magnini (2006) selected 10 NE-
subclasses of person and location using Word-
Net as a reference. Datasets were automatically
acquired and manually filtered. They compare
word and pattern-based supervised and a semi-
supervised approach based on syntactic features.
Giuliano & Gliozzo (2007, 2008) perform NE
classification against the People Ontology, an ex-
cerpt of the WordNet hierarchy, comprising 21
people classes populated with at least 40 instances.
Using minimally supervised lexical substitution
methods, they cast NE classification as an ontol-
ogy population task ? as opposed to recognition
and classification in context. In a similar setting,
Giuliano (2009) explores semi-supervised classifi-
cation of the People Ontology classes using latent
semantic kernels, comparing models built from
Wikipedia and from a news corpus. In a differ-
ent line of research Pas?ca (2007) and Pas?ca and
van Durme (2008) make use of query logs to ac-
quire NEs on a large scale. While Pas?ca (2007)
extracts NEs for 10 target classes, Pas?ca and van
Durme (2008) combine web query logs and web
documents to acquire both NE-concept pairs and
concept attributes using seeds.
But while these more recent approaches all of-
fer substantially novel contributions for many NE
acquisition subtasks, none of them addresses the
full task of FG-NERC, i.e., recognition and clas-
sification of NE tokens in context. Compared to
ontology population, focusing on types, classifica-
tion in raw texts needs to consider any token and
cannot rely on special contexts offering indicative
clues for class membership.
Bunescu and Pas?ca (2006) also perform dis-
ambiguation and classification of NEs in context,
yet in a different setup. Disambiguation is per-
formed into one of the known possible classes
for a NE, as determined from Wikipedia disam-
biguation pages. Contexts for training and testing
are acquired from Wikipedia pages, as opposed
to general text. Disambiguation is performed us-
ing vectors of co-occurring terms and a taxonomy-
based kernel that integrates word-category corre-
lations. Evaluation is performed on the task of
predicting, for a given NE in a Wikipedia page
context, the correct class from among its known
classes, including one experiment that included
10% of out-of-Wikipedia entities. The category
space was confined to People by occupation, with
8,202 subclasses. Classification considered 110
broad classes, 540 highly populated classes (w/o
out-of-Wikipedia entities), and 2,847 classes in-
cluding less populated ones. This setup is diffi-
cult to compare given the sense granularities em-
ployed and the special Wikipedia text genre. Even
though classification is performed in context, the
task does not evaluate recognition.
To summarize, the field has developed robust
methods for acquisition and fine-grained classifi-
cation of NEs on a large scale. But, the full task
of NE recognition and classification in context still
remains to be addressed for a wide-coverage, fine-
grained semantic class inventory that can serve as
a common benchmark for future research.
3 Fine-grained NERC on a large-scale
We present experiments that assess the difficulty
of open-domain FG-NERC pursued at a large
scale. We concentrate on instances and classes re-
ferring to people, since it is a well-studied domain
(see Section 2) and structured fine-grained infor-
mation can be readily applied to a well-defined
end-user task such as IR, cf. the Web People
Search task (Artiles et al, 2008). Our method
is general in that it requires only a (PoS tagged
and chunked) corpus and a reference taxonomy
to provide a concept hierarchy. Given a map-
ping between automatically extracted class labels
94
and concepts in a taxonomic resource, it can be
further extended to other domains, e.g. locations
or the biomedical domain by leveraging open-
domain taxonomies such as Yago (Suchanek et
al., 2008) or WikiTaxonomy (Ponzetto and Strube,
2007). The contribution of this work is two-fold:
(i) We develop an unsupervised method for ac-
quiring a comprehensive dataset for FG-NERC by
applying linguistically motivated patterns to a cor-
pus harvested from the Web (Section 4). Large
amounts of NEs are acquired together with their
contexts of occurrence and with their fine-grained
class labels which are mapped to synsets in Word-
Net. The controlled sense inventory and the tax-
onomic structure offered by WordNet enables an
evaluation of FG-NERC performance at different
levels of concept granularity, as given by the depth
at which the concepts are found. As our extraction
patterns reflect a wide-spread grammatical con-
struct, the method can be applied to many lan-
guages and extended to other domains.
(ii) Given this automatically acquired dataset,
we assess the problem of FG-NERC in a sys-
tematic series of experiments, exploring the per-
formance of NERC methods on different levels
of granularities. For recognition and classifica-
tion we apply standard sequential labeling tech-
niques ? i.e. a Maximum Entropy (MaxEnt) tag-
ger (Section 5.1) ? which we adapt to this hier-
archical classification problem (Section 5.2). To
test the hypothesis of whether a sequential la-
beler represents a valid choice to perform FG-
NERC, we compare the latter to a MaxEnt system
trained on a more semantically informed feature
set, and a gloss-overlap method inspired by WSD
approaches (Section 5.3).
4 Acquisition of a FG-NERC dataset
We present an unsupervised method that simul-
taneously acquires NEs, their semantic class and
contexts of occurrence from large textual re-
sources. In order to develop a clean resource of
properly disambiguated NEs, we develop acqui-
sition patterns for a grammatical construction that
unambiguously associates proper names with their
corresponding semantic class.
Pattern-based extraction of NE-concept pairs.
NEs are often introduced by so-called apposi-
tional structures as in (1), which overtly ex-
press which semantic class (here, painter) the NE
(Kandinsky) belongs to. Appositions involving
proper names can be captured by extraction pat-
terns as given in (2).
(1) . . . writings of the abstract painter Kandinsky
frequently explored similarities between . . .
(2) a. [the|The]? [JJ|NN]* [NN] [NP]
the abstract painter Kandinsky
b. [NP] [,]? [a|an|the]* [JJ|NN]* [NN]
W. Kandinsky, a Russian-born painter, ..
Contexts like (2.a) provide a less noisy se-
quence for extraction, due to the class and instance
labels being adjacent ? in contrast to (2.b) where
any number of modifiers can intervene between
the two. Accordingly, we apply in our experiments
only a restricted version of (2.a) ? with a deter-
miner ? to UKWAC, an English web-based cor-
pus (Baroni et al, 2009) that comes in a cleaned,
PoS-tagged and lemmatized form. Due to its size
(>2 billion tokens) and mixed genres, the corpus
is ideally suited for acquiring large quantities of
NEs pertaining to a broad variety of open-domain
semantic classes.
Filtering heuristics. The apposition patterns are
subject to noise, due to PoS-tagging errors, as
well as special constructions, e.g. reduced rela-
tive clauses. The former can be controlled by fre-
quency filters, the latter can be circumvented by
using chunk boundary information1. A more chal-
lenging problem is to recognize whether an ex-
tracted nominal is in fact a valid semantic class for
NEs. Besides, class labels can be ambiguous, so
there is uncertainty as to which class an extracted
entity should be assigned to. We apply two fil-
tering strategies: we set a frequency threshold ft
on the number of extracted NE tokens per class,
to remove infrequent class label extractions; we
then filter invalid semantic classes using informa-
tion from WordNet: given the WordNet PERSON
supersense, i.e. the lexicographer file for nouns de-
noting people, we check whether the first sense of
the class label candidate is found in PERSON.
Mapping to the WordNet person domain. In
order to perform a hierarchical classification of
people, we need a taxonomy for the domain at
hand. We achieve this by mapping the extracted
class labels to WordNet synsets. In our setting, we
map against all synsets found under person#n#1,
1We use YamCha (Kudo and Matsumoto, 2000) to per-
form phrase chunking.
95
which are direct hypernyms of at least one in-
stance in WordNet (CWN pers+Inst).2 Since our
goal is to map class labels to synsets (i.e. our fu-
ture NE classes), we check each class label candi-
date against all synonyms contained in the synset.
At this point we have to deal with two cases: two
extracted class label candidates (synonyms such
as doctor, physician) will map to a single synset,
while ambiguous class labels (e.g. director) can be
mapped to more than one synset. In the latter case,
we heuristically choose the synset which domi-
nates the highest number of instances in WordNet.
Mapping evaluation. We evaluated the cover-
age of our mapping for two sets of class labels
extracted for two different frequency thresholds:
ft = 40 and ft = 1. With ft = 40, we cover
31.1% of the synsets found under person#n#1 in
WordNet, i.e. the set of classes CWN pers+Inst;
conversely, 45.8% of the extracted class labels can
be successfully mapped to CWN pers+Inst. For
threshold ft = 1, we are able to map to 87.9%
of CWN pers+Inst, with only 20.1% of extracted
classes mapped to CWN pers+Inst. For the re-
maining 79.9% of class labels (e.g. goalkeeper,
chancellor, superstar) that have no instances in
WordNet, we manually inspected 20 classes, in 20
contexts each, and established that 76% of them
are appropriate NE person classes.
For threshold ft = 40, we obtain 153 class labels
which are mapped to 146 synsets. Ten class labels
are mapped to more than one synset. Using our
mapping heuristic based on the majority instance
class, we successfully disambiguate all of them.
However, since we only map to CWN pers+Inst,
we introduce errors for 5 classes. E.g. ?manager?
incorrectly gets mapped to manager#n#2, since
the latter is the only synset containing instances.
For these cases we manually corrected the auto-
matic mapping.
A taxonomy for FG-NERC. We create our gold
standard taxonomy of semantic classes by start-
ing with the 146 synsets obtained from the map-
ping, including the 5 classes that were manually
corrected. Since we concentrate on the people
domain, we additionally remove 5 classes that
can refer to other domains as well (e.g. carrier,
guide). Given the remaining 141 synsets, we se-
lect the portion of WordNet rooted at person#n#1
2We use WordNet version 3.0. With w#p#i we denote the
i-th sense of a wordw with part of speech p. E.g., person#n#1
is defined as { person, individual . . .}).
Level #C #C w/inst #inst #inst/C % of inst
1 1 0 0 - -
2 29 8 2,662 332 5.49
3 57 37 18,229 493 37.58
4 63 46 18,422 401 37.94
5 37 30 6,231 208 12.84
6 18 13 2,366 182 4.88
7 6 5 423 85 0.87
8 2 2 179 90 0.36
all 213 141 48,512 344 100
Table 1: Level-wise statistics of classes and in-
stances across the FG-NERC person taxonomy.
which contains them, together with any interven-
ing synset found along the WordNet hierarchy.
Given this WordNet excerpt, the extracted NE to-
kens are then appended to the respective synsets in
the hierarchy. Statistics of the resulting WordNet
fragment augmented with instances are given in
Table 1. The taxonomy has a maximum depth of 8,
and contains 213 synsets, i.e. NE classes (see col-
umn 2). 83.5% of the 31,819 extracted instances
(type-level) sit in leaf nodes. The classes automat-
ically refer back to the acquired appositional con-
texts. Table 1 gives statistics about the number of
instances (token-level) acquired for classes at dif-
ferent embedding levels. In total we have at our
disposal 48,512 instances (token-level) in apposi-
tional contexts. The type-token ratio is 1.52.
Gold standard validation. To create a gold
standard dataset of entities in context labeled with
fine-grained classes, we first randomly select 20
classes, as well as an additional 18 which are
also found in the People Ontology (Giuliano and
Gliozzo, 2008). For each class, we randomly se-
lect 40 occurrences of instances in context, i.e.
the words co-occurring in a window of 60 tokens
before and after the instance. We asked four an-
notators to label these extractions for correctness,
and to provide the correct label for the incorrect
cases, if one was available. Only 52 contexts out
of 1520 were labeled as incorrect, thus giving us
96.58% accuracy on our automatically extracted
data. The manually validated dataset is used to
provide a ground-truth for FG-NERC. However,
the noun (e.g. hunter) denoting the NE class is re-
moved from these contexts for training and testing
in all experiments. This is because, due to the ex-
traction method based on POS-patterns denoting
appositions, class labels are known a priori to oc-
cur in the context of an instance and thus identify
them with high precision.
96
5 Methodology for FG-NERC
We develop methods to perform FG-NERC using
standard techniques developed for coarse-grained
NERC and WSD. These are applied to our dataset
from Section 4, in order to measure performance
at different levels of semantic class granularity, i.e.
corresponding to the depth of the semantic classes
found in our WordNet fragment. We start in Sec-
tion 5.1 to present a Maximum Entropy model to
perform coarse-grained NERC and we extend it
to perform multiclass classification in a hierarchi-
cal taxonomy (Section 5.2). We then present in
Section 5.3 an alternative proposal to perform FG-
NERC using global context information, as found
in state-of-the-art approaches to supervised and
unsupervised WSD.
5.1 NERC using a MaxEnt tagger
Our baseline system is modeled following a Maxi-
mum Entropy approach (Bender et al, 2003, inter
alia). The MaxEnt model produces a probability
for each class label t (the NE tag) of a classifica-
tion instance, conditioned on its context of occur-
rence h. This probability is calculated by:
P (t|h) =
1
Z(h)
exp
?
?
n?
j=1
?jfj(h, t)
?
? (1)
where fj(h, t) is the j-th feature with associated
weight ?j and Z(h) is a normalization constant to
ensure a proper probability distribution.3 Given a
word wi to be classified as Beginning, Inside or
Outside (IOB) of a NE, we extract as features:
1. Context words. The words occurring within
the context window wi+2i?2 = wi?2 . . . wi+2.
2. Word prefix and suffix. Word prefix and suffix
character sequences of length up to n.
3. Infrequent word. A feature that fires if wi oc-
curs in the training set less frequently than a
given threshold (i.e. below 10 occurrences).
4. Part-of-Speech (PoS) and chunk informa-
tion. The PoS and chunk labels of wi.
5. Capitalization. A binary feature that checks
whether wi starts with a capital letter or not.
6. Word length. A binary feature that fires if
the length of wi is smaller than a pre-defined
threshold (i.e. less than 5 characters).
3In our implementation, we use the OpenNLP MaxEnt li-
brary (http://maxent.sourceforge.net).
7. Digit and symbol features. Three features
check whether wi contains digit strings, non-
characters (e.g. slashes) or number expressions.
8. Dynamic feature. The tag ti?1 of the word
wi?1 preceding wi in the sequence wn1 .
5.2 MaxEnt extension for FG-NERC
Extension to hierarchical classification. We
apply our baseline NERC system to FG-NERC.
Given a word in context, the task consists of recog-
nizing it as a NE, and classifying it into the appro-
priate semantic class from our person taxonomy.
We approach this as a hierarchical classification
task by generating a binary classifier4 with sepa-
rate training and test sets for each node in the tree.
To perform level-wise classification from coarse
to fine-grained classes, we need to adjust the class
labels and their corresponding training and test in-
stances for each experiment. For classification at
the deepest level, each concept contains the in-
stances of the original dataset. For classification
at higher levels we leverage the semantics of the
WordNet hyponym relations and expand the set
of target classes (i.e. synsets) of a given level to
contain all instances of hyponym synsets. Given
a set I of classification instances for a given tar-
get class c, we add all instances labeled with the
hyponyms of c to I . All other instances (not in
that subtree) are labeled as being Outside (O-) a
NE. This approach ensures that, for each node, the
dataset contains two classes (NE and O) only, and
implicitly ?propagates? the instances up the tree.
As a result, non-leaf nodes that did not have any
instance in the original dataset become populated.
Also, the classification of classes at higher levels
is based on larger datasets.
Extension to multiclass classification. Since
we train a binary classifier for each node of the
tree, we apply two methods to infer multiclass de-
cisions from these binary classifiers, namely level-
wise and global multiclass classification. In both
paradigms, we combine the single decisions of
the individual classifiers with the winner-takes-all
strategy, using weighted voting. The weights are
calculated based on the confidence value for the
corresponding class, i.e., its conditional probabil-
ity according to Equation (1). The output label is
selected randomly in case of ties.
4The IOB tagging scheme normally assigns three different
labels, i.e. Inside (I-), Outside (O-) and Beginning (B-) of
a chunk. However, our dataset does not have any instance
labeled as B-, since it does not contain any adjacent NEs.
97
For level-wise classification, we combine only
classifiers at the same level of embedding. Given
n concepts at level l, we have n possible out-
put labels for each word. The output label for a
classification instance is determined by the highest
weighted vote among all binary classifiers at level
l. For global classification we combine all binary
classifiers of the entire tree using weighted voting
to determine the winning class label. The weights
are calculated based on the product of confidence
value and depth of the corresponding class in the
tree.
5.3 FG-NERC using global contexts
FG-NERC is a more demanding task than ?classi-
cal? NERC, due to the larger amount of classes,
the paucity of examples for each class, and the
increasingly subtle semantic differences between
these classes. For such a task contextual informa-
tion is expected to be very informative ? e.g. if an
entity co-occurs in context with ?Nobel prize?, this
provides evidence that it is likely to be a scien-
tist or scholar. However, the context window used
by our baseline MaxEnt tagger is very local, in-
cluding at most the two preceding and succeeding
words. Hence, the classifier is not able to capture
informative contextual clues in a larger context.
Previous work has related FG-NERC to WSD
approaches (Alfonseca and Manandhar, 2002).
Accordingly, we investigate two context-sensitive
approaches inspired from WSD proposals, which
consider a more global context for classification.
We first define a new feature set to induce a new
MaxEnt model (MaxEnt-B) which only uses lexi-
cal features from a larger context window, as used
in standard supervised WSD (Lee and Ng, 2002):
1. PoS context. The part-of-speech occur-
ring within the context window posi+3i?3 =
posi?3 . . . posi+3.
2. Local collocation. Local collocations Cnm sur-
rounding wi. We use C?2,?1 and C1,2.
3. Content words in surrounding context. We
consider all unigrams in contexts wi+3i?3 =
wi?3 . . . wi+3 of wi (crossing sentence bound-
aries) for the entire training data. We convert to-
kens to lower case, remove stopwords, numbers
and punctuation symbols. We define a feature
vector of length 10 using the 10 most frequent
content words. Given a classification instance,
the feature corresponding to token t is set to 1
iff the context wi+3i?3 of wi contains t.
In addition, we use a Lesk-like method (Lesk,
1986) which labels instances in context with the
WordNet synset whose gloss has the maximum
overlap with the glosses of the senses of its words
in context. Given the small context provided by
theWordNet glosses, we follow Banerjee and Ped-
ersen (2003) and expand these to also include the
words from the glosses of the hypernym and hy-
ponym synsets.
6 Experiments
6.1 Benchmarking on coarse-grained NERC
We benchmark the performance of our baseline
MaxEnt classifier using the feature set from Sec-
tion 5.1 (MaxEnt-A henceforth) on the CoNLL-
2003 shared task dataset (Tjong Kim Sang and
De Meulder, 2003), the de-facto standard for eval-
uating coarse-grained NERC systems.
In MaxEnt modeling, feature selection is a cru-
cial problem and key to improving classification
performance. MaxEnt, however, does not provide
methods for automatic feature selection. We there-
fore experimented with various combinations of
features standardly used for NERC (1-8 of Section
5.1). Model parameters are computed with 200
iterations without feature frequency cutoff. The
best configuration is found by optimizing the F1
measure on the development data with various fea-
ture representations. The chosen features are: 1, 2
(with n = 3), 4, 5, 6, 7 and 8. Evaluation on the
test set is performed blindly, using this feature set.
The results are presented in Table 2.
TheMaxEnt labeler achieves performance com-
parable with the CoNLL-2003 task participants,
ranking 12th among the 16 systems participating
in the task, with a 2 point margin off the F1 of the
most similar system of Bender et al (2003) and
7 points below the best-performing system (Flo-
rian et al, 2003). The former used a relatively
complex set of features and different gazetteers
extracted from unannotated data. The latter com-
bined four diverse classifiers, namely a robust lin-
ear classifier, maximum entropy, transformation-
based learning and a hidden Markov model. They
used different feature sets, unannotated data and
an additional NE tagger. In comparison, our
NERC system is simpler and based on a small set
of features that can be easily obtained for many
languages. Besides, it does not make use of any
external resources and still shows state-of-the-art
performance on the overall data.
98
Recall Precision F?=1
PER 83.02% 81.40% 82.21%
LOC 88.47% 88.19% 88.23%
ORG 77.20% 68.03% 72.23%
MISC 81.20% 83.92% 82.54%
Overall 83.11% 80.47% 81.77%
Table 2: Results on the CoNLL-2003 test data.
Set # tokens # NEs
Training 2,431,041 38,810
Development 478,871 9,702
Test 181,490 1,520
Table 3: Statistics for training, dev and test sets.
6.2 Evaluating FG-NERC
Experimental setting. For the task of FG-
NERC, we compare the performance of MaxEnt-
A with the MaxEnt-B model from Section 5.3 and
the Lesk method. The data is partitioned into train-
ing and development sets by randomly selecting
80%-20% of the contexts in which the NEs occur.
We use the held-out, manually validated gold stan-
dard from Section 4 for blind evaluation. Statistics
for the dataset are reported in Table 3.
We build a MaxEnt model for each FG-NE
class, using the features that performed best on
the CoNLL task, except the digit and dynamic
NE features (MaxEnt-A), and context features 1-
3 of Section 5.3 (MaxEnt-B). Model parameters
are computed in the same way as for coarse-
grained NERC. Table 3 shows that our training set
is highly unbalanced. The ratio between positive
(NEs) and negative examples (i.e. O classification
instances) at the topmost level is 63:1. Also, with
increasing levels of fine-grainedness, the number
of negative (-O) NE classes is increasing for each
binary classifier. We observed on the develop-
ment set that this skewed distribution heavily bi-
ases the classifiers towards the negative category,
and accordingly investigated sampling techniques
to make the ratio of positive and negative examples
more balanced. We experiment with a sampling
strategy that over-samples the positive examples
and under-samples the negative ones. We define
various ratios of over-sampling depending upon
the number of positive examples in the original
training set. Table 4 lists the factors (f ) of over-
sampling applied to the original positive samples
(P ), with minimum and maximum sizes of the ob-
factor f size of P min P ? max P ?
20 ? P 1 ? 2K 20 40K
15 ? P 2K ? 5K 30K 75K
10 ? P 5K ? 10K 50K 100K
5 ? P 10K ? 20K 50K 100K
2 ? P 20K ? 50K 40K 100K
P 50K ? . . . 50K >50K
Table 4: Oversampling of positive samples.
MaxEnt-A MaxEnt-BLevel
R P F1 R P F1
1 98.7 85.0 91.4 95.1 83.0 88.6
2 96.0 65.5 77.9 48.1 46.3 47.2
3 95.3 54.3 69.3 43.3 41.1 42.2
4 86.8 52.8 65.6 41.1 37.2 39.1
5 90.4 45.9 60.9 49.2 21.5 29.9
6 91.6 36.9 52.6 51.7 13.2 21.1
7 89.5 31.8 46.9 42.2 10.2 16.4
8 100.0 19.9 66.7 87.1 8.1 14.7
global 85.1 43.2 57.3 61.9 26.6 37.2
hierarchical 87.7 44.8 59.4 64.5 29.5 40.5
Table 6: Level-wise NE recognition & classifica-
tion evaluation (in %).
tained oversampled sets P ? for different ranges of
original sizes of P .5 Oversampling is done with-
out replacement. The number of negative instan-
ces is always downsampled on the basis of P ? to
yield a 1:5 ratio of positive and negative samples,
a ratio we estimated from the CoNLL-2003 data.
Level-wise evaluation results on the FG-NE
classification-only (NEC) task for the MaxEnt
classifiers and Lesk are given in Table 5. Table
6 reports results for the evaluation of the MaxEnt
model performing both classification and recog-
nition. As for coarse-grained NERC, we evaluate
using the standard metrics of recall (R), precision
(P) and balanced F-measure (F1). As baseline, we
use a majority class assignment ? i.e. at each level,
we label all instances with the most frequent class
label. For global FG-NE classification, reported in
Table 5, the original fine-grained classes are con-
sidered, across the entire class hierarchy. Global
evaluation is performed by counting exact label
predictions on the entire hierarchy (global) and us-
ing the evaluation metric of Melamed and Resnik
(2000, hierarchical). As baseline we assume the
most frequent class label in the training set.
Discussion. All methods perform reasonably
well, indicating the feasibility of the task. For the
MaxEnt models, Table 5 shows a general high re-
call and decreasing precision as we move down the
hierarchy. Degradation in the overall F1 score is
5Sampling ratios are determined on the development set.
99
Baseline MaxEnt-A MaxEnt-B LeskLevel
R P F1 R P F1 R P F1 R P F1
1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
2 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.9
3 27.9 23.1 25.2 83.9 88.1 85.9 75.5 79.8 77.5 16.2 16.2 16.2
4 18.8 20.4 19.5 74.6 85.0 79.5 65.4 71.3 68.2 11.3 11.3 11.3
5 25.8 19.0 21.9 78.8 83.4 80.9 78.6 74.1 76.3 13.5 14 13.8
6 24.7 7.8 11.9 88.5 73.6 80.4 78.7 74.1 75.7 33.2 37.5 35.2
7 19.1 5.34 8.3 79.2 76.5 77.8 78.1 72.7 75.3 49.4 49.4 49.4
8 34.2 2.9 5.5 82.8 73.8 78.1 81.1 71.1 75.8 0.1 0.1 0.1
global 34.6 18.5 24.1 81.1 84.2 82.6 78.0 74.2 76.6 36.5 38.6 37.5
hierarchical 33.0 21.2 25.8 83.5 86.2 84.8 78.2 77.8 78.1 36.6 38.7 37.6
Table 5: Level-wise evaluation of fine-grained NE classification techniques (in %).
given by the increasingly limited amount of class
instances found towards the low regions of the tree
(down to an average of 85 and 90 instances per
class for levels 7 and 8, respectively) (cf. Table 1).
The ?classical? feature set (MaxEnt-A) yields bet-
ter performance compared to the semantic feature
set (MaxEnt-B). However MaxEnt-B still achieves
a respectable performance, given that it contains a
few semantic features only.
The MaxEnt classifiers achieve a far better per-
formance than Lesk. This is in-line with previ-
ous findings in WSD, namely unsupervised fine-
grained disambiguation methods rarely perform-
ing above the baseline, and suggests that Lesk can
be merely used as a ?strong? baseline. Error anal-
ysis showed that it performs poorly due to the lim-
ited context provided by the WordNet glosses, and
the limited impact of gloss expansions deriving
from the low connectivity between synsets.
Comparison of Tables 5 and 6 shows that per-
formance decreases considerably for a classifier
that not only assigns fine-grained classes, but also
detects which tokens actually are NEs. As for
the classification-only task, the performance de-
creases as one moves to lower levels. This in-
dicates that the complexity of the task is propor-
tional to the fine-grainedness of the class inven-
tory. MaxEnt-B, lacking ?classical? NER features,
shows dramatic losses, compared to MaxEnt-A.
Comparison to other work. We compared the
performance of our system based on global classi-
fication (one vs. rest) against the figures reported
for individual categories in Giuliano (2009). The
MaxEnt-A system compares favorably, although it
considers (i) more classes at each level ? i.e. 213
vs. 21 ? and (ii) classifies NEs at finer-grained lev-
els ? i.e. 8 vs. 4 maximum depth in the respec-
tive WordNet fragments. We achieve overall mi-
cro average R, P and F1 values of 87.5%, 85.7%
and 86.6%, respectively, compared to Giuliano?s
79.6%, 80.9% and 80.2%. Due to the different se-
tups and data used, these figures do not offer a ba-
sis for true comparison. However, the figures sug-
gest that our system achieves respectable perfor-
mance on a more complex classification problem.
7 Conclusions
We presented a method to perform FG-NERC on
a large scale. Our contribution lies in the def-
inition of a benchmarking setup for this task in
terms of gold standard datasets and strong base-
line methods provided by a MaxEnt classifier. We
proposed a pattern-based approach for the acqui-
sition of fined-grained NE semantic classes and
instances. This corpus-based method relies only
on the availability of large text corpora, such as
the WaCky corpora, in contrast to resources diffi-
cult to obtain, such as query-logs (Pas?ca and van
Durme, 2008). It makes use of a very large Web
corpus to extract instances from open-domain con-
texts ? in contrast to standard NERC approaches,
which are tailored for newswire data and do not
generalize well across domains. Our gold stan-
dard training and test datasets are currently based
only on appositional patterns6. Therefore, it does
not include the full spectrum of constructions in
which instances can be found in context. Future
work will investigate semi-supervised and heuris-
tics (e.g. ?one sense per discourse?) methods to ex-
pand the data with examples from follow-up men-
tions, e.g. co-occurring in the same document.
Our MaxEnt models still perform very local
classification decisions, relying on separate mod-
els for each semantic class. We accordingly plan to
explore both global models operating on the over-
all hierarchy, and more informative feature sets.
6The data are available for research purposes at http:
//www.cl.uni-heidelberg.de/fgnerc.
100
References
Enrique Alfonseca and Suresh Manandhar. 2002.
An unsupervised method for general named entity
recognition and automated concept discovery. In
Proc. of GWC-02.
S. Ananiadou, C. Friedman, and J.I. Tsujii. 2004.
Special issue on named entity recognition in
biomedicine. Journal of Biomedical Informatics,
37(6).
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search. In Proc. of LREC ?08.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805?810.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proc. of CoNLL-03, pages 148?151.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING-02, pages 1?7.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001
Student Workshop.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proc. of CoNLL-
03, pages 168?171.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proc. of ACL-07, pages 248?256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proc. of COLING-ACL-08, pages
265?272.
Claudio Giuliano. 2009. Fine-grained classification of
named entities exploiting latent semantic kernels. In
Proc. of CoNLL-09, pages 201?209.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proc.
of CoNLL-00, pages 142?144.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Proc.
of EMNLP-02, pages 41?48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the ACL-SIGDOC Conference, pages 24?26.
Thomas Mandl and Christa Womser-Hacker. 2005.
The effect of named entities on effectiveness in
cross-language information retrieval evaluation. In
Proc. of ACM SAC 2005, pages 1059?1064.
Andrew McCallum and Andrew Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, features induction and web-enhanced
lexicons. In Proc. of CoNLL-03, pages 188?191.
Dan Melamed and Philip Resnik. 2000. Tagger evalu-
ation given hierarchical tag sets. Computers and the
Humanities, pages 79?84.
MUC-7. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, Cal.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Marius Pas?ca and Benjamin van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. of ACL-08, pages 19?27.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006a. Names and similarities on the web: Fact ex-
traction in the fast lane. In Proc. of COLING-ACL-
06, pages 809?816.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006b. Organizing and
searching the world wide web of facts ? Step one:
The one-million fact extraction challenge. In Proc.
of AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM-2007, pages 683?690.
Luiz Augusto Pizzato, Diego Molla, and Ce?cile Paris.
2006. Pseudo relevance feedback using named enti-
ties for question answering. In Proc. of ALTW-2006,
pages 83?90.
Simone Paolo Ponzetto andMichael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Elsevier Journal of Web
Semantics, 6(3):203?217.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proc. of EACL-06, pages 17?24.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Proc. of CoNLL-03, pages 127?132.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-independent
Named Entity Recognition. In Proc. of CoNLL-02,
pages 155?158.
101
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 19?27,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Identifying Event ? Sentiment Association using Lexical Equivalence and 
Co-reference Approaches 
 
 
Anup Kumar Kolya1       Dipankar Das1      Asif Ekbal2      Sivaji Bandyopadhyay1 
1 Computer Science and Engineering Department, Jadavpur University, India  
2 Indian Institute of Technology, Patna (IITP), India 
anup.kolya@gmail.com, dipankar.dipnil2005@gmail.com 
asif.ekbal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
In this paper, we have identified event and sen-
timent expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. A hybrid approach that con-
sists of Conditional Random Field (CRF) based 
machine learning framework in conjunction 
with several rule based strategies has been 
adopted for event identification within the 
TimeML framework. The strategies are based 
on semantic role labeling, WordNet relations 
and some handcrafted rules. The sentiment ex-
pressions are identified simply based on the 
cues that are available in the sentiment lexicons 
such as Subjectivity Wordlist, SentiWordNet 
and WordNet Affect. The identification of lexi-
cal equivalence between event and sentiment 
expressions based on the part-of-speech (POS) 
categories is straightforward. The emotional 
verbs from VerbNet have also been employed 
to improve the coverage of lexical equivalence. 
On the other hand, the association of sentiment 
and event has been analyzed using the notion of 
co-reference. The parsed dependency relations 
along with basic rhetoric knowledge help to 
identify the co-reference between event and 
sentiment expressions. Manual evaluation on 
the 171 sentences of TempEval-2010 dataset 
yields the precision, recall and F-Score values 
of 61.25%, 70.29% and 65.23% respectively.  
1 Introduction 
Event and Sentiment are two abstract entities 
closely coupled with each other from social, psy-
chological and commercial perspectives. Some 
kind of action that is going on or something that is 
being happened are addressed as events in general 
by the Natural Language (NL) researchers. The 
events are described in texts where the time, tem-
poral location and ordering of the events are speci-
fied. Event entities are represented by finite 
clauses, nonfinite clauses, nominalizations, event-
referring nouns, adjectives and even some kinds of 
adverbial clauses.  
On the other hand, text not only contains the in-
formative contents, but also some attitudinal pri-
vate information that includes sentiments. 
Nowadays, in the NLP communities, research ac-
tivities on sentiment analysis are in full swing. But, 
the identification of sentiment from texts is not an 
easy task as it is not open to any objective observa-
tion or verification (Quirk et al, 1985).  
Sometimes, similar or different types of senti-
ments are expressed on a single or multiple events. 
Sentiment of people over different events is impor-
tant as it has great influence on our society. Track-
ing users? sentiments about products or events or 
about political candidates as expressed in online 
forums, customer relationship management, stock 
market prediction, social networking etc., temporal 
question answering, document summarization, in-
formation retrieval systems are some of the impor-
tant applications of sentiment analysis.  
The identification of the association between 
event and sentiment is becoming more popular and 
interesting research challenge in the area of Natu-
ral Language Processing (NLP). Our present task is 
to identify the event and sentiment expressions 
from the text, analyze their associative relationship 
19
and investigate the insides of event-sentiment rela-
tions.  
For example, in the following sentence, the an-
notated events are, talked, sent and hijacked .But, 
it also shows the presence of underlying sentiments 
(as shown in underlined script) inscribed in the 
sentence. Here, sentiment helps to evoke the event 
property at lexical entity level (e.g. negative (-ve) 
sentiment for only the event word hijacked) as well 
as at context level (e.g. positive (+ve) sentiment 
associated with the event hijacked as the event 
word appears with the evaluative expression, re-
cover that gives the +ve polarity).  
 
?The prime minister of India told Friday that he 
has talked with top commander of Indian military 
force and sent a team to recover the host of Taj 
Hotel hijacked.?  
 
 Hence, we have organized the entire task into 
three different steps i) event identification, ii) sen-
timent expression identification and iii) identifica-
tion of event sentiment relationships at context 
level using lexical equivalence and co-reference 
approaches.  
In the first step, we propose a hybrid approach 
for event extraction from the text under the Tem-
pEval-2010 framework. Initially, we have used a 
Conditional Random Field (CRF) (Lafferty et al, 
2001) machine learning framework but we observe 
that it often makes the errors in extracting the 
events denoted by deverbial entities. This observa-
tion prompts us to employ several strategies in 
conjunction with machine learning. These strate-
gies are implemented based on semantic role labe-
ling, WordNet (Miller, 1990) and some 
handcrafted rules. We have experimented with the 
TempEval-2010 evaluation challenge setup (Kolya 
et al, 2010).  Evaluation results yield the preci-
sion, recall and F-measure values of approximate-
ly 93.00%, 96.00% and 94.47% respectively. This 
is approximately 12% higher F-measure in com-
parison to the best system (Llorens et al, 2010) of 
TempEval-2010. 
    On the other hand, the identification of the sen-
timent expressions is carried out based on the sen-
timent word. The words are searched in three 
different sentiment lexicons, the Subjectivity Word 
lists (Banea et al, 2008), SentiWordNet (Baccia-
nella et al, 2010) and WordNet Affect (Strapparava 
and Valitutti, 2004). The coarse-grained (positive 
and negative) as well as Ekman?s (1993) six fine- 
grained sentiment or emotion expressions (happy, 
sadness, anger, disgust, fear and surprise) are 
tagged in the corpus. As there is no annotation in 
the TemEval-2010 corpus for sentiment expres-
sions, the evaluation has been carried out by the 
authors and it achieves the precision, recall and F-
measure values of approximately 73.54%, 86.04% 
and 79.30% respectively 
Determining the lexical equivalence of event 
and sentiment expressions based on the POS prop-
erty at the lexical entity level is straightforward. If 
an event word also expresses the sentiment word, 
we have associated the corresponding sentiment 
type with the event word directly. In addition to the 
sentiment lexicons, the emotional verbs extracted 
from the VerbNet (Kipper-Schuler, 2005) are used 
in this phase. It improves the coverage of lexical 
equivalence by 12.76%. 
But, if the event and sentiment expressions oc-
cupy separate text spans in a sentence, we have 
adopted a co-reference approach for identifying 
their association. The parsed dependency relations 
along with some basic rhetoric components, such 
as nucleus, satellite and locus help in identifying 
the co-reference between the event and sentiment 
expressions. The text span containing sentiment 
word is hypothesized as the locus, the main effec-
tive part of the nucleus or satellite. The text span 
that reflects the primary goal of the writer is 
termed as nucleus (marked as ?{ }?) whereas the 
span that provides supplementary material is 
termed as satellite (marked as ?[ ]?). The distin-
guished identification of nucleus and satellite as 
well as their separation from each other is carried 
out based on the direct and transitive dependency 
relations, causal verbs, relaters or discourse mark-
ers. If both the locus and event are identified to-
gether in either nucleus or satellite, we term their 
association as co-referenced. If they occur sepa-
rately in nucleus and satellite and share at least one 
direct dependency relation, we consider their asso-
ciation as co-referenced.  
The evaluation of the lexical equivalence as 
well as co-reference systems has been performed 
by the authors. Primarily, the evaluation of both 
systems has been conducted on the random sam-
ples of 200 sentences of the TempEval-2010 train-
ing dataset.  Finally, the co-reference system 
achieves the precision, recall and F-Scores of 
20
61.25%, 70.29% and 65.23% respectively on 171 
sentences of the TempEval-2010 test corpus.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. The event 
identification is discussed in Section 3. The identi-
fication of sentiment expressions is described in 
Section 4. Determination of lexical equivalence 
between event and sentiment expressions is speci-
fied in Section 5. The co-reference approach for 
identifying the association between event and sen-
timent is described in Section 6. Finally Section 7 
concludes the paper. 
2 Related Work 
The existing works on event extraction are based 
either on pattern-matching rules (Mani and Wilson 
2000), or on the machine learning approach (Bo-
guraev and Ando, 2005). But, still the problems 
persist with the high complexities involved in the 
proper extractions of events. The events expres-
sions were annotated in the TempEval 2007 
source in accordance with the TimeML standard 
(Pustejovsky et al, 2003). On the other hand, the 
Task B of TempEval-2010 evaluation challenge 
setup (Verhagen et al, 2010) was aimed at identi-
fying events from text. The best achieved result 
was obtained by (Llorens et al, 2010). 
The majority of subjective analysis methods 
that are related to emotion is based on textual key-
words spotting that use specific lexical resources. 
A lexicon that provides appraisal attributes for 
terms was constructed and the features were used 
for emotion classification (Whitelaw et al, 2005). 
The features along with the bag-of-words model 
give 90.2% accuracy. UPAR7 (Chaumartin, 2007), 
a rule-based system uses a combination of Word-
Net Affect and SentiWordNet. The system was 
semi-automatically enriched with the original trial 
data provided during the SemEval task (Strappara-
va and Mihalcea, 2007). SWAT (Katz et al, 2007) 
is another supervised system that uses a unigram 
model trained to annotate emotional content. 
Our motivation is that though events and senti-
ments are closely coupled with each other from 
social, psychological and commercial perspectives, 
very little attention has been given about their de-
tection and analysis. To the best of our knowledge, 
only a few tasks have been attempted (Fukuhara et 
al., 2007) (Das et al, 2010).  
Sometimes, the opinion topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics, as 
well as opinions that are on the same topic sepa-
rated by opinions that do not share that topic 
(Stoyanov and Cardie 2008). The authors have es-
tablished their hypothesis by applying the co-
reference technique. Similarly, we have adopted 
the co-reference technique based on basic rhetoric 
components for identifying the association be-
tween event and sentiment expressions.  In addi-
tion to that, we have also employed the lexical 
equivalence approach for identifying their associa-
tion.  
3 Event Identification 
In this work, we propose a hybrid approach for 
event identification from the text under the Tem-
pEval-2010 framework. We use Conditional Ran-
dom Field (CRF) as the underlying machine 
learning algorithm. We observe that this machine 
learning based system often makes the errors in 
identifying the events denoted by deverbial enti-
ties. This observation prompts us to employ several 
strategies in conjunction with machine learning 
techniques. These strategies have been imple-
mented based on semantic role labeling, WordNet 
senses and some handcrafted rules.  
We have experiment with the TempEval-2010 
evaluation challenge setup (Kolya et al, 2010).  
Evaluation results yield the precision, recall and F-
measure values of approximately 93.00%, 96.00% 
and 94.47% respectively. This is approximately 
12% higher F-measure in comparison to the best 
system (Llorens et al, 2010) of TempEval-2010. 
3.1 CRF based Approach for Event Identifi-
cation 
We extract the gold-standard TimeBank features 
for events in order to train/test the CRF model. In 
the present work, we mainly use the various com-
binations of the following features:  
Part of Speech (POS) of event terms (e.g. Ad-
jective, Noun and Verb), Tense (Present, Past, Fu-
ture, Infinitive, Present part, Past part, or NONE), 
Aspect (Progressive, Perfective and Perfective 
Progressive or NONE), Class (Reporting, Percep-
tion, Aspectual, I_action, I_state, State, Occur-
rence), Stem (e.g., discount /s/).  
21
3.2 Use of Semantic Roles for Event Identifi-
cation 
We use an open source Semantic Role Labeler 
1(SRL) (Gildea et al, 2002) (Pradhan et al, 2004) 
to identify different features of the sentences. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents, determining 
their arguments (agent, patient etc.) and adjuncts 
(locative, temporal etc.). Semantic roles can be 
used to detect the events that are the nominaliza-
tions of verbs such as agreement for agree or con-
struction for construct. Nominalizations (or, 
deverbal nouns) are commonly defined as nouns 
that are morphologically derived from verbs, 
usually by suffixation (Quirk et al, 1985). Event 
nominalizations often afford the same semantic 
roles as verbs and often replace them in written 
language (Gurevich et al, 2006).  Event nominali-
zations constitute the bulk of deverbal nouns.  The 
following example sentence shows how semantic 
roles can be used for event identification.  
 
[ARG1 All sites] were [TARGET inspected] to the satis-
faction of the inspection team and with full coope-
ration of Iraqi authorities, [ARG0 Dacey] [TARGET 
said]. 
 
   The extracted target words are treated as the 
event words. It has been observed that many of 
these target words are identified as the event ex-
pressions by the CRF model. But, there exists ma-
ny nominalised event expressions (i.e., deverbal 
nouns) that are not identified as events by the su-
pervised CRF. These nominalised expressions are 
correctly identified as events by SRL.  
3.3 Use of WordNet for Event Identification 
WordNet is mainly used to identify non-deverbal 
event nouns. We observed that the event entities 
like ?war?, ?attempt?, ?tour? are not properly identi-
fied. These words have noun (NN) POS informa-
tion as the previous approaches, i.e., CRF and SRL 
can only identify those event words that have verb 
(VB) POS information. We know from the lexical 
information of WordNet that the words like ?war? 
and ?tour? are generally used as both noun and 
verb forms in the sentence. Therefore, we have 
                                                        
1 http://cemantix.org/assert.html 
designed the following two rules based on the 
WordNet: 
 
Rule 1: The word tokens having Noun (NN) POS 
categories are looked into the WordNet. If it ap-
pears in the WordNet with noun and verb senses, 
then that word token is considered as an event.  For 
example, war has both noun and verb senses in the 
WordNet, and hence war is considered as an event.  
 
Rule 2: The stems of the noun word tokens are 
looked into the WordNet. If one of the WordNet 
senses is verb then the token is considered as verb. 
For example, the stem of proposal, i.e., propose 
has two different senses, noun and verb in the 
WordNet, and thus it is considered as an event.  
3.4    Use of Rules for Event Identification 
Here, we mainly concentrate on the identification 
of specific lexical classes like ?inspection? and 
?resignation?. These can be identified by the suf-
fixes such as (?-ci?n?), (?-tion?) or (?-ion?), i.e., the 
morphological markers of deverbal derivations. 
  Initially, we have employed the CRF based Stan-
ford Named Entity (NE) tagger2 on the TempEval-
2 test dataset. The output of the system is tagged 
with Person, Location, Organization and Other 
classes. The words starting with the capital letters 
are also considered as NEs. Thereafter, we came 
up with the following rules for event identification: 
  
Cue-1: The deverbal nouns are usually identified 
by the suffixes like ?-tion?, ?-ion?, ?-ing? and ?-ed? 
etc. The nouns that are not NEs, but end with these 
suffixes are considered as the event words. 
  
Cue 2: The verb-noun combinations are searched 
in the sentences of the test set. The non-NE noun 
word tokens are considered as the events.  
 
Cue 3: Nominals and non-deverbal event nouns 
can be identified by the complements of aspectual 
PPs headed by prepositions like during, after and 
before, and complex prepositions such as at the 
end of and at the beginning of etc.  The next word 
token(s) appearing after these clue word(s) or 
phrase(s) are considered as events.  
                                                        
2 http://nlp.stanford.edu/software/CRF-NER.shtml 
22
Cue 4: The non-NE nouns occurring after the ex-
pressions such as frequency of, occurrence of and 
period of are most probably the event nouns. 
 
Cue 5: Event nouns can also appear as objects of 
aspectual and time-related verbs, such as have be-
gun a campaign or have carried out a campaign 
etc. The non-NEs that appear after the expressions 
like ?have begun a?, ?have carried out a? etc.  are 
also denoted as the events.   
4 Sentiment Expression Identification 
Sentiment is an important cue that effectively de-
scribes the events associated with it. The binary 
classification of the sentiments (positive and nega-
tive) as well as the fine-grained categorization into 
Ekman?s (1993) six emotions is therefore em-
ployed for identifying the sentiment expressions. 
200 sentences are randomly selected from the 
training dataset of the TempEval-2010 corpus. 
These sentences have been considered as our de-
velopment set. On the other hand, 171 sentences 
were already provided as the test sentences in the 
TempEval-2010 evaluation challenge.   
The events are already annotated in the Tem-
pEval-2010 corpus. But, no sentiment or emotion 
related annotation is available in the corpus. 
Hence, we have annotated the sentiment expres-
sions at word level in a semi-supervised way. The 
word level entities are tagged by their coarse and 
fine grained sentiment tags using the available sen-
timent related lexical resources. Then the automat-
ic annotation has been evaluated manually by the 
authors. The semi-supervised sentiment annotation 
agreements were 90.23% for the development set 
and 92.45% for the test sets respectively.  
4.1 Lexicon based Approach 
The tagging of the evaluative expressions or more 
specifically the sentiment expressions on the Tem-
pEval-2010 corpus has been carried out using the 
available sentiment lexicons. We passed the sen-
tences through three sentiment lexicons, Subjectivi-
ty Wordlists (Banea et al, 2008), SentiWordNet 
(Baccianella et al, 2010) and WordNet Affect 
(Strapparava and Valitutti, 2004). Subjectivity 
Wordlist assigns words with the strong or weak 
subjectivity and prior polarities of types positive, 
negative and neutral. SentiWordNet, used in opi-
nion mining and sentiment analysis, assigns three 
sentiment scores such as positive, negative and 
objective to each synset of WordNet. WordNet Af-
fect, a small well-used lexical resource but valua-
ble for its affective annotation contains the words 
that convey emotion.  
The algorithm is that, if a word in a sentence is 
present in any of these resources; the word is 
tagged as the sentiment expression. But, if any 
word is not found in any of them, each word of the 
sentence is passed through the WordNet Morpho-
logical analyzer (Miller, 1990) to identify its root 
form and the root form is searched through the re-
sources again. If the root form is found, the corres-
ponding word is tagged as sentiment expression 
accordingly.  
The identified sentiment expressions have been 
evaluated by the authors and it achieves the preci-
sion, recall and F-Score of 73.54%, 86.04% and 
79.30%, respectively on a total of 171 test sen-
tences of the TempEval-2010 corpus.   
The identification of event words that also ex-
press sentiment is straightforward. But, the prob-
lem arises when the event and sentiment 
expressions are present separately in a sentence 
and the sentiment is either closely associated with 
the event or affects it. In case of the former, we 
have adopted the approach of lexical equivalence 
between the event and sentiment entities whereas 
the co-reference technique has been introduced for 
resolving the latter case.  
5 Lexical Equivalence between Event and 
Sentiment Expressions  
It is observed that in general the verbs, nouns and 
adjectives represent events. The sentences are 
passed through an open source Stanford Maximum 
Entropy based POS tagger (Manning and Toutano-
va, 2000). The best reported accuracy for the POS 
tagger on the Penn Treebank is 96.86% overall and 
86.91% on previously unseen words. Our objective 
was to identify the event words that also express 
sentiments. Hence, we have identified the event 
words that have also been tagged as the sentiment 
expressions. The coverage of these lexical re-
sources in identifying the event sentiment associa-
tion is shown in Table 1. 
On the other hand, not only the adjectives or 
nouns, the sentiment or emotional verbs play an 
important role in identifying the sentiment expres-
23
sions. Hence, in addition to the above mentioned 
sentiment resources, we have also incorporated 
English VerbNet (Kipper-Schuler, 2005) for the 
automatic annotation process. VerbNet associates 
the semantics of a verb with its syntactic frames 
and combines traditional lexical semantic informa-
tion such as thematic roles and semantic predi-
cates, with syntactic frames and selectional 
restrictions. Verb entries in the same VerbNet class 
share common syntactic frames and thus they are 
believed to have the same syntactic behavior. For 
example, the emotional verbs ?love? and ?enjoy? 
are members of the admire-31.2-1 class and ?en-
joy? also belongs to the class want-32.1-1.  
The XML files of VerbNet are preprocessed to 
build up a general list that contains all member 
verbs and their available syntax information re-
trieved from VerbNet. The main criterion for se-
lecting the member verbs as sentiment expressions 
is the presence of ?emotional_state? type predicate 
in their frame semantics. The frequencies of the 
event words matched against the above said four 
resources are shown in Table 1.  It has been ob-
served that the adjective events are not identified 
by the lexical resources as their frequency in the 
test corpus was very low. But, the lexical coverage 
has been improved by 12.76% by incorporating 
VerbNet. 
 
Resources Noun   Adjective  Verb 
#114    #4              #380 
Subjectivity Wordlists 
SentiWordNet 
WordNet Affect List 
VerbNet (emotional 
verbs) 
24            --             35 
32            --             59  
12            --             25 
 --            --             79 
Accuracy (in %) 59.64                    52.57 
 
Table 1: Results of Lexical Equivalence between 
Event and Sentiment based on different resources  
6 Co-reference between Event and Senti-
ment Expressions  
The opinion and/or sentiment topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics. 
Sometimes, the opinions that are on the same topic 
are separated by opinions that do not share that 
topic (Stoyanov and Cardie, 2008). We observe the 
similar situation in case of associating sentiments 
with events. Hence, the hypothesis for opinion top-
ic is established for sentiment events by applying 
the co-reference technique along with the rhetori-
cal structure. We have proposed two different sys-
tems for identifying the association of sentiments 
with the events at context level. 
6.1 Baseline Co-reference System 
The baseline system has been developed based on 
the object information present in the dependency 
relations of the parsed sentences. Stanford Parser 
(Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different part of speech (POS) 
tags of Pen Treebank tagset  has been used to get 
the parsed sentences and dependency relations. 
The dependency relations are checked for the pre-
dicates ?dobj? so that the related components 
present in the predicate are considered as the prob-
able candidates for the events.  
If a dependency relation contains both the event 
and sentiment words, we have considered the pres-
ence of co-reference between them. But, it has 
been observed that the event and sentiment expres-
sions are also present in two different relations that 
share a common word element. Hence, if the event 
and sentiment words appear in two different rela-
tions but both of the relations contain at least one 
common element, the event and sentiment words 
are termed as co-referenced.    
Overall, the baseline co-reference system 
achieves the precision, recall and F-Scores of 
40.03%, 46.10% and 42.33% for event-sentiment 
co-reference identification. For example in the fol-
lowing sentence, the writer?s direct as well as indi-
rect emotional intentions are reflected by 
mentioning one or more topics or events (spent, 
thought) and their associated sentiments (great).  
 
?When Wong Kwan spent seventy million dol-
lars for this house, he thought it was a great deal.? 
 
The baseline co-reference system fails to asso-
ciate the sentiment expressions with their corres-
ponding event expressions. Hence, we aimed for 
the rhetoric structure based co-reference system to 
identify their association. 
6.2  Rhetoric Co-reference System 
The distribution of events and sentiment expres-
sions in different text spans of a sentence needs the 
24
analysis of sentential structure. We have incorpo-
rated the knowledge of Rhetorical Structure 
Theory (RST) (Mann and Thompson 1987) for 
identifying the events that are co-referred by their 
corresponding sentiment expressions.  
The theory maintains that consecutive discourse 
elements, termed text spans, are related by a rela-
tively small set (20?25) of rhetorical relations. 
But, instead of identifying the rhetorical relations, 
the present task acquires the basic and coarse rhe-
torical components such as locus, nucleus and sa-
tellite from a sentence.  These rhetoric clues help 
in identifying the individual event span associated 
with the span denoting the corresponding senti-
ment expression in a sentence. The text span that 
reflects the primary goal of the writer is termed as 
nucleus (marked as ?{ }?) whereas the span that 
provides supplementary material is termed as satel-
lite (marked as ?[ ]?). For example, the nucleus and 
satellite textual spans are shown in the following 
sentence as, 
 
{Traders said the market remains extremely 
nervous} because [the wild swings seen on the 
New York Stock Exchange last week]. 
 
The event or topic of an opinion or sentiment 
depends on the context in which the associated 
opinion or sentiment expression occurs (Stoyanov 
and Cardie 2008). Considering the similar hypo-
thesis in case of events instead of topics, the co-
reference between an event and a sentiment ex-
pression is identified from the nucleus and/or satel-
lite by positioning the sentiment expression as 
locus. We have also incorporated the WordNet?s 
(Miller 1990) morphological analyzer to identify 
the stemmed forms of the sentiment words.  
The preliminary separation of nucleus from sa-
tellite was carried out based on the list of frequent-
ly used causal keywords (e.g., as, because, that, 
while, whether etc) and punctuation markers (,) (!) 
(?).The discourse markers and causal verbs are 
also the useful clues if they are explicitly specified 
in the text. The identification of discourse markers 
from written text itself is a research area (Azar 
1999). Hence, our task was restricted to identify 
only the explicit discourse markers that are tagged 
by conjunctive_() or mark_() type dependency re-
lations of the parsed constituents. The dependency 
relations containing conjunctive markers (e.g., 
conj_and(), conj_or(), conj_but()) were considered 
for separating nucleus from satellite if the markers 
are present in between two successive clauses. 
Otherwise, the word token contained in the 
mark_() type dependency relation was considered 
as a discourse marker. 
The list of causal verbs is prepared by 
processing the XML files of VerbNet. If any Verb-
Net class file contains any frame with semantic 
type as Cause, we collect the member verbs of that 
XML class file and term the member verbs as 
causal verbs. We used a list that contains a total 
number of 253 causal verbs.  
If any clause tagged as S or SBAR in the parse 
tree contains any causal verb, that clause is consi-
dered as the nucleus and the rest of the clauses de-
note the satellites. Considering the basic theory of 
rhetorical structure (Mann and Thompson 1987), 
the clauses were separated into nucleus and satel-
lite to identify the event and sentiment expressions. 
The direct dependency is identified based on the 
simultaneous presence of locus and the event word 
in the same dependency relation whereas the tran-
sitive dependency is verified if the word is con-
nected to locus and event via one or more 
intermediate dependency relations.  
If the event and sentiment words are together 
present in either nucleus or satellite, the associa-
tion between the two expressions is considered as 
co-referenced. If they occur in nucleus and satellite 
separately, but the event and sentiment words are 
present in at least one direct dependency relation, 
the expressions are termed as co-referenced.  
In the previous example, the event expressions, 
?said? and ?remains? are associated with the sen-
timent expression ?nervous? as both the event ex-
pressions share the direct dependency relations 
?cop(nervous-7, remains-5)? and ?ccomp(said-2, 
nervous-7)? in the nucleus segment. Similarly, the 
event word, ?seen? and sentiment word ?wild? are 
present in the satellite part and they share a direct 
dependency relation ?partmod(swings-12, seen-
13)?. But, no direct dependency relation is present 
between the ?nervous? and ?seen? or ?said? and 
?wild? or ?remains? and ?wild?.  
6.3 Results 
Though the event annotation is specified in the 
TempEval-2010 corpus, the association between 
the event and sentiment expressions was not speci-
fied in the corpus. Hence, we have carried out the 
25
evaluation manually. The 200 random samples of 
the training set that were used in sentiment expres-
sion identification task have been considered as 
our development set. The Evaluation Vectors 
(EvalV) are prepared manually from each sentence 
of the development and test sets. The vectors 
<EvExp, SentiExp> are filled with the annotated 
events and sentiment expressions by considering 
their association. The annotation of sentiment ex-
pressions using the semi-supervised process has 
been described in Section 4. 
    The rule based baseline and rhetoric based co-
reference systems identify the event and sentiment 
expressions from each sentence and stores them in 
a Co-reference Vector (CorefV). The evaluation is 
carried out by comparing the system generated Co-
reference Vectors (CorefV) with their correspond-
ing Evaluation Vectors (EvalV). The evaluation 
results on 171 test sentences are shown in Table 2. 
 
Co-reference  
Approaches 
Prec.     Rec.    F-Score 
(in %) 
Baseline System 40.03    46.10       42.33 
Rhetoric System 61.25    70.29       65.23 
 
Table 2: Precision (Prec.), Recall (Rec.) and F-
Scores (in %) of the event-sentiment co-reference 
systems  
 
Overall, the precision, recall and F-Scores are 
61.25%, 70.29% and 65.23% for event-sentiment 
co-reference identification using rhetoric clues. 
Though the co-reference technique performs satis-
factorily for identifying the event-sentiment co-
reference, the problem arises in distinguishing the 
corresponding spans of events from an overlapped 
text span of multi-word tokens.  
7 Conclusion  
In this present work, we have identified event and 
sentiment expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. It has been observed that the lex-
ical equivalence based on lexicons performs satis-
factorily but overall, the co-reference entails that 
the presence of indirect affective clues can also be 
traced with the help of rhetoric knowledge and de-
pendency relations. The association of the senti-
ments with their corresponding events can be used 
in future concerning the time based sentiment 
change over events.  
Acknowledgments 
The work is supported by a grant from the India-
Japan Cooperative Programme (DST-JST) 2009 
Research project entitled ?Sentiment Analysis 
where AI meets Psychology? funded by Depart-
ment of Science and Technology (DST), Govern-
ment of India. 
References  
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation. 
Boguraev, B., Ando, R. K. 2005. TimeBank-
DrivenTimeML Analysis. Annotating, Extracting and 
Reasoning about Time and Events 2005. 
Chaumartin, F. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. SemEval-200,  
Czech Republic. 
Ekman Paul. 1993. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Fukuhara T., Nakagawa, H. and Nishida, T. 2007. Un-
derstanding Sentiment of People from News Articles: 
Temporal Sentiment Analysis of Social Events. 
ICWSM?2007, Boulder, Colorado. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
Gurevich, O., R. Crouch, T. King, and V. de Paiva. 
2006. Deverbal Nouns in Knowledge Representation. 
Proceedings of FLAIRS, pages 670?675, Melbourne 
Beach, FL. 
Katz, P., Singleton, M. and Wicentowski, R. 2007. 
Swat-mp: the semeval-2007 systems for task 5 and 
task SemEval-2007.  
Kipper-Schuler, K. 2005.  VerbNet: A broad-coverage, 
comprehensive verb lexicon. Ph.D. thesis, Computer 
and Information Science Dept., University of Penn-
sylvania, Philadelphia, PA. 
26
Kolya, A., Ekbal, A. and Bandyopadhyay, S. 2010. 
JU_CSE_TEMP: A First Step towards Evaluating 
Events, Time Expressions and Temporal Relations. 
In Proceedings of the 5th International Workshop on 
Semantic Evaluation, ACL 2010, July 15-16, Swe-
den, pp. 345?350. 
Lafferty, J., McCallum, A.K., Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. International 
Conference on Machine Learning. 
Llorens Hector, Estela Saquete, Borja Navarro. 2010. 
TIPSem (English and Spanish): Evaluating CRFs and 
Semantic Roles. Proceedings of the 5th International 
Workshop on Semantic Evaluation, ACL 2010, pages 
284?291, Uppsala, Sweden, 15-16 July 2010. 
Mani, I., and Wilson G. 2000. Processing of News. In 
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 69-76. 
Mann, W. and S. Thompson. 1987. Rhetorical Structure 
Theory: Description and Construction of Text Struc-
ture. In G. Kempen (ed.), Natural Language Genera-
tion, Martinus Nijhoff, The Hague, pp. 85?96. 
Manning Christopher and Toutanova, Kristina. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC)  
Marneffe, Marie-Catherine de, Bill MacCartney, and 
Christopher D.Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 5th 
International Conference on Language Resources 
and Evaluation.  
Miller George A. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312 
Pradhan S., Wayne W., Hacioglu, K., Martin, J.H. and 
Jurafsky, D. 2004. Shallow Semantic Parsing using 
Support Vector Machines. Proceedings of the Human 
Language Technology Conference/North American 
chapter of the Association for Computational Lin-
guistics annual meeting Boston, MA, May 2-7. 
Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gai-
zauskas, R., Setzer, A., Katz, G. and Radev, D. 
TimeML: Robust specification of event and temporal 
expressions in text. In AAAI Spring Symposium on 
New Directions in Question-Answering, pp. 28-34, 
CA, 2003. 
Quirk, R., Greenbaum, S. Leech, G. and Svartvik, J. 
1985. A Comprehensive Grammar of the English 
Language. Longman.  
Strapparava C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In 4th Internation-
al Conference on Language Resources and Evalua-
tion, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Stoyanov, V., and Cardie, C. 2008. Annotating topics of 
opinions. In Proceedings of LREC.  
 
27
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61?65,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Multi-metric optimization for coreference: The UniTN / IITP / Essex
submission to the 2011 CONLL Shared Task
Olga Uryupina? Sriparna Saha? Asif Ekbal? Massimo Poesio??
?University of Trento
?Indian Institute of Technology Patna
? University of Essex
uryupina@gmail.com, sriparna@iitp.ac.in,
asif@iitp.ac.in, massimo.poesio@unitn.it
Abstract
Because there is no generally accepted met-
ric for measuring the performance of anaphora
resolution systems, a combination of met-
rics was proposed to evaluate submissions to
the 2011 CONLL Shared Task (Pradhan et
al., 2011). We investigate therefore Multi-
objective function Optimization (MOO) tech-
niques based on Genetic Algorithms to opti-
mize models according to multiple metrics si-
multaneously.
1 Introduction
Many evaluation metrics have been proposed for
anaphora resolution (Vilain et al, 1995; Bagga and
Baldwin, 1998; Doddington et al, 2000; Luo, 2005;
Recasens and Hovy, 2011). Each of these metrics
seems to capture some genuine intuition about the
the task, so that, unlike in other areas of HLT, none
has really taken over. This makes it difficult to com-
pare systems, as dramatically demonstrated by the
results of the Coreference Task at SEMEVAL 2010
(Recasens et al, 2010). It was therefore wise of the
CONLL organizers to use a basket of metrics to as-
sess performance instead of a single one.
This situation suggests using methods to opti-
mize systems according to more than one metric
at once. And as it happens, techniques for doing
just that have been developed in the area of Ge-
netic Algorithms?so-called multi-objective opti-
mization techniques (MOO) (Deb, 2001). The key
idea of our submission is to use MOO techniques
to optimize our anaphora resolution system accord-
ing to three metrics simultaneously: the MUC scorer
(a member of what one might call the ?link-based?
cluster of metrics) and the two CEAF metrics (rep-
resentative of the ?entity-based? cluster). In a pre-
vious study (Saha et al, 2011), we show that our
MOO-based approach yields more robust results than
single-objective optimization.
We test two types of optimization: feature se-
lection and architecture?whether to learn a single
model for all types of anaphors, or to learn sepa-
rate models for pronouns and for other nominals.
We also discuss how the default mention extraction
techniques of the system we used for this submis-
sion, BART (Versley et al, 2008), were modified to
handle the all-mention annotation in the OntoNotes
corpus.
In this paper, we first briefly provide some back-
ground on optimization for anaphora resolution, on
genetic algorithms, and on the method for multi-
objective optimization we used, Non-Dominated
Sorting Genetic Algorithm II (Deb et al, 2002). Af-
ter that we discuss our experiments, and present our
results.
2 Background
2.1 Optimization for Anaphora Resolution
There have only been few attempts at optimization
for anaphora resolution, and with a few exceptions,
this was done by hand.
The first systematic attempt at automatic opti-
mization of anaphora resolution we are aware of was
carried out by Hoste (2005), who used genetic algo-
rithms for automatic optimization of both feature se-
lection and of learning parameters, also considering
61
two different machine learners, TimBL and Ripper.
Her results suggest that such techniques yield im-
provements on the MUC-6/7 datasets. Recasens and
Hovy (2009) carried out an investigation of feature
selection for Spanish using the ANCORA corpus.
A form of multi-objective optimization was ap-
plied to coreference by Munson et al (2005). Mun-
son et al (2005) did not propose to train models so
as to simultaneously optimize according to multi-
ple metrics; instead, they used ensemble selection to
learn to choose among previously trained models the
best model for each example. Their general conclu-
sion was negative, stating that ?ensemble selection
seems too unreliable for use in NLP?, but they did
see some improvements for coreference.
2.2 Genetic Algorithms
Genetic algorithms (GAs) (Goldberg, 1989) are ran-
domized search and optimization techniques guided
by the principles of evolution and natural genetics.
In GAs the parameters of the search space are en-
coded in the form of strings called chromosomes. A
collection of such strings is called a population. An
objective or fitness function is associated with each
chromosome that represents the degree of goodness
of that chromosome. A few of the chromosomes are
selected on the basis of the principle of survival of
the fittest, and assigned a number of copies that go
into the mating pool. Biologically inspired opera-
tors like crossover and mutation are applied on these
chromosomes to yield a new generation of strings.
The processes of selection, crossover and mutation
continues for a fixed number of generations or till a
termination condition is satisfied.
2.3 Multi-objective Optimization
Multi-objective optimization (MOO) can be formally
stated as follows (Deb, 2001). Find the vectors
x? = [x?1, x?2, . . . , x?n]T of decision variables that si-
multaneously optimize the M objective values
{f1(x), f2(x), . . . , fM (x)}
while satisfying the constraints, if any.
An important concept in MOO is that of dom-
ination. In the context of a maximization prob-
lem, a solution xi is said to dominate xj if
?k ? 1, 2, . . . ,M, fk(xi) ? fk(xj) and ?k ?
1, 2, . . . ,M, such that fk(xi) > fk(xj).
Genetic algorithms are known to be more effec-
tive for solving MOO than classical methods such as
weighted metrics, goal programming (Deb, 2001),
because of their population-based nature. A particu-
larly popular genetic algorithm of this type is NSGA-
II (Deb et al, 2002), which we used for our runs.
3 Using MOO for Optimization in
Anaphora Resolution
We used multi-objective optimization techniques for
feature selection and for identifying the optimal ar-
chitecture for the CONLL data. In this section we
briefly discuss each aspect of the methodology.
3.1 The BART System
For our experiments, we use BART (Versley et al,
2008), a modular toolkit for anaphora resolution that
supports state-of-the-art statistical approaches to the
task and enables efficient feature engineering. BART
comes with a set of already implemented features,
along with the possibility to design new ones. It
also implements different models of anaphora reso-
lution, allowing the choice between single and split
classifiers that we explore in our runs, as well as
between mention-pair and entity-mention, and be-
tween best-first and ranking. It also has interfaces
to different machine learners (MaxEnt, SVM, de-
cision trees). It is thus ideally suited for experi-
menting with feature selection and other aspects of
optimization. However, considering all the param-
eters, it was unfeasible to run an optimization on
the amount of data available on CONLL; we fo-
cused therefore on feature selection and the choice
between single and split classifiers. We considered
42 features, including 7 classifying mention type, 8
for string matching of different subparts and differ-
ent levels of exactness, 2 for aliasing, 4 for agree-
ment, 12 for syntactic information including also
binding constraints, 3 encoding salience, 1 encod-
ing patterns extracted from the Web, 3 for proximity,
and 2 for 1st and 2nd person pronouns. Again be-
cause of time considerations, we used decision trees
as implemented in Weka as our classification model
instead of maximum-entropy or SVMs. Finally, we
used a simple mention-pair model without ranking
as in (Soon et al, 2001).
62
3.2 Mention detection
BART supports several solutions to the mention
detection (MD) task. The users can input pre-
computed mentions, thus, experimenting with gold
boundaries or system boundaries computed by ex-
ternal modules (e.g., CARAFE). BART also has
a built-in mention extraction module, computing
boundaries heuristically from the output of a parser.
For the CoNLL shared task, we use the BART
internal MD module, as it corresponds better to
the mention detection guidelines of the OntoNotes
dataset. We have further adjusted this module to im-
prove the MD accuracy. The process of mention de-
tection involves two steps.
First, we create a list of candidate mentions by
merging basic NP chunks with named entities. NP
chunks are computed from the parse trees provided
in the CoNLL distribution, Named entities are ex-
tracted with the Stanford NER tool (Finkel et al,
2005). For each candidate mention, we store it mini-
mal and maximal span. The former is used for com-
puting feature values (e.g., for string matching); it
corresponds to either the basic NP chunk or the NE,
depending on the mention type. The latter is used
for alignment with CoNLL mentions; it is computed
by climbing up the parse tree.
This procedure, combined with the perfect (gold)
coreference resolution, gives us an F-score of
91.56% for the mention detection task on the
CoNLL development set1.
At the second step, we aim at discarding men-
tions that are unlikely to participate in corefer-
ence chains. We have identified several groups of
such mentions: erroneous (?[uh]?), (parts of) multi-
word expressions (?for [example]?), web addresses,
emails (?[http://conll.bbn.com]?), time/date expres-
sions (?two times [a year]?), non-referring pronouns
(?[there]?,?[nobody]?), pronouns that are unlikely
to participate in a chain (?[somebody]?, ?[that]?),
time/date expressions that are unlikely to participate
in a chain (?[this time]?), and expletive ?it?.
Our experiments on the development data show
that the first five groups can be reliably identified
and safely discarded from the processing: even with
1Note that, due to the fact that OntoNotes guidelines exclude
singleton mentions, it is impossible to evaluate the MD compo-
nent independently from coreference resolution.
the perfect resolution, we observe virtually no per-
formance loss (the F-score for our MD module with
the gold coreference resolution remains at 91.45%
once we discard mentions from groups 1-5).
The remaining groups are more problematic:
when we eliminate such mentions, we see perfor-
mance drops with the gold resolution. The exact im-
pact of discarding those mentions can only be as-
sessed once we have trained the classifier.
In practice, we have performed our optimization
experiments, selected the best classifier and then
have done additional runs to fine-tune the mention
detection module.
3.3 Using NSGA-II
Chromosome Representation of Feature and Ar-
chitecture Parameters We used chromosomes of
length 43, each binary gene encoding whether or not
to use a particular feature in constructing the classi-
fier, plus one gene set to 1 to use a split classifier, 0
to use a single classifier for all types of anaphors.
Fitness Computation and Mutations For fitness
computation, the following procedure is executed.
1. Suppose there are N number of features
present in a particular chromosome (i.e., there
are total N number of 1?s in that chromosome).
2. Construct the coreference resolution system
(i.e., BART) with only these N features.
3. This coreference system is evaluated on the de-
velopment data. The recall, precision and F-
measure values of three metrics are calculated.
For MOO, the objective functions corresponding to
a particular chromosome are F1 = F-measureMUC
(for the MUC metric), F2 = F-measure?3 (for CEAF
using the ?3 entity alignment function (Luo, 2005))
and F3 = F-measure?4 (for CEAF using the ?3
entity alignment function). The objective is to:
max[F1, F2, F3]: i.e., these three objective func-
tions are simultaneously optimized using the search
capability of NSGA-II.
We use crowded binary tournament selection as
in NSGA-II, followed by conventional crossover and
mutation for the MOO based optimization. The
most characteristic part of NSGA-II is its elitism op-
eration, where the non-dominated solutions (Deb,
63
2001) among the parent and child populations are
propagated to the next generation. The near-Pareto-
optimal strings of the last generation provide the dif-
ferent solutions to the feature selection problem.
Genetic Algorithms Parameters Using the
CONLL development set, we set the following pa-
rameter values for MOO (i.e., NSGA-II): population
size=20, number of generations=20, probability of
mutation=0.1 and probability of crossover=0.9.
3.4 Running the Optimization
Considering the size of the OntoNotes corpus, it
would be very time-consuming to run an optimiza-
tion experiment on the whole dataset. We have
therefore split the data into 3 sub-samples and per-
formed separate MOO experiments on each one.
The MOO approach provides a set of non-
dominated solutions on the final Pareto optimal
front. All the solutions are equally important from
the algorithmic point of view. We have collected sets
of chromosomes for each sub-sample and evaluated
them on the whole train/development set, picking
the solution with the highest FINAL2 score for our
CoNLL submission.
4 Results
4.1 Development set
Table 1 compares the performance level obtained
using all the features with that of loose re-
implementations of the systems proposed by Soon
et al (2001) and Ng and Cardie (2002), commonly
used as baselines. Our reimplementation of the Ng
& Cardie model uses only a subset of features.
The results in Table 1 show that our system with
a rich feature set does not outperform simpler base-
lines (and, in fact, yields poorer results). A similar
trend has been observed by Ng and Cardie (2002),
where the improvement was only possible after man-
ual feature selection.
The last line of Table 1 shows the performance
level of the best chromosome found through the
MOO technique. As it can be seen, it outperforms
all the baselines according to all the measures, lead-
ing to an improvement of 2-5 percentage points in
the FINAL score.
2The FINAL score is an average of FMUC , FB3 and
FCEAF E .
This suggests that automatic feature selection is
essential to improve performance ? i.e., that an effi-
cient coreference resolution system should combine
rich linguistic feature sets with automatic feature se-
lection mechanisms.
4.2 Test set
We have re-trained our best solution on the com-
bined train and development set, running it on the
test data. This system has showed the following per-
formance in the official evaluation (open track): the
FINAL score of 54.32, FMUC = 57.53%, FB3 =
65.18%, FCEAFE = 40.16%.
5 Conclusion
Our results on the development set suggest that a
linguistically-rich system for coreference resolution
might benefit a lot from feature selection. In partic-
ular, we have investigated Non-Dominated Sorting
Genetic Algorithm II (Deb et al, 2002) for multi-
objective optimization.
In subsequent work, we plan to expand the opti-
mization technique to consider also learning param-
eters optimization, classifier selection, and learning
model selection.
Acknowledgments
This work was in part supported by the Provincia
di Trento Grande Progetto LiveMemories, in part by
an Erasmus Mundus scholarship for Asif Ekbal and
Sriparna Saha.
64
Features FMUC FCEAFE FB3 FINAL
following Soon et al (2001) 54.12 41.08 66.67 53.42
-*-, with splitting 53.81 41.03 66.70 53.31
following Ng & Cardie (2002) 52.97 42.40 66.18 53.31
-*-, with splitting 53.28 40.46 66.03 52.72
All features 50.18 38.54 63.79 50.33
-*-, with splitting 50.19 39.47 65.38 51.16
Optimized feature set (splitting) 57.05 42.61 67.46 55.15
Table 1: Performance on the development set
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proc. of the LREC workshop on
Linguistic Coreference, pages 563?566, Granada.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2):181?197.
Kalyanmoy Deb. 2001. Multi-objective Optimization
Using Evolutionary Algorithms. John Wiley and Sons,
Ltd, England.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassell, and R. Weischedel. 2000. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proc. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370.
D. E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley, New York.
Veronique Hoste. 2005. Optimization Issues in Ma-
chine Learning of Coreference Resolution. Ph.D. the-
sis, Antwerp University.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. NAACL / EMNLP, Vancouver.
Art Munson, Claire Cardie, and Rich Caruana. 2005.
Optimizing to arbitrary NLP metrics using ensem-
ble selection. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 539?546.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting on Association
for C omputational Linguistics, pages 104?111.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
M. Recasens and E. Hovy. 2009. A deeper look into fea-
tures for coreference resolution. In S. Lalitha Devi,
A. Branco, and R. Mitkov, editors, Anaphora Pro-
cessing and Applications (DAARC 2009, number 5847
in LNAI, pages 29?42, Berlin / Heidelberg. Springer-
Verlag.
M. Recasens and E. Hovy. 2011. Blanc: Implement-
ing the rand index for coreference evaluation. Natural
Language Engineering.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proc. SEMEVAL 2010, Uppsala.
Sriparna Saha, Massimo Poesio, Asif Ekbal, and Olga
Uryupina. 2011. Single and multi-objective optimiza-
tion for feature selection in anaphora resolution. Sub-
mitted.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistic, 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies, pages 9?12.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of the Sixth Message Under-
standing Conference, pages 45?52.
65
