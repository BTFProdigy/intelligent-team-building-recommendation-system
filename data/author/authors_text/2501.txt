Exploiting Lexical Conceptual Structure
for Paraphrase Generation
Atsushi Fujita1, Kentaro Inui2, and Yuji Matsumoto2
1 Graduate School of Informatics, Kyoto University
fujita@pine.kuee.kyoto-u.ac.jp
2 Graduate School of Information Science, Nara Institute of Science and Technology
{inui, matsu}@is.naist.jp
Abstract. Lexical Conceptual Structure (LCS) represents verbs as semantic
structures with a limited number of semantic predicates. This paper attempts to
exploit how LCS can be used to explain the regularities underlying lexical and
syntactic paraphrases, such as verb alternation, compound word decomposition,
and lexical derivation. We propose a paraphrase generation model which trans-
forms LCSs of verbs, and then conduct an empirical experiment taking the para-
phrasing of Japanese light-verb constructions as an example. Experimental results
justify that syntactic and semantic properties of verbs encoded in LCS are useful
to semantically constrain the syntactic transformation in paraphrase generation.
1 Introduction
Automatic paraphrasing has recently been attracting increasing attention due to its po-
tential in a broad range of natural language processing tasks. For example, a system that
is capable of simplifying a given text, or showing the user several alternative expres-
sions conveying the same content, would be useful for assisting a reader.
There are several classes of paraphrase that exhibit a degree of regularity. For exam-
ple, paraphrasing associated with verb alternation, lexical derivation, compound word
decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such
classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in
which a compound noun is transformed into a verb phrase. Likewise, paraphrases in-
volving an LVC as in (3) and (4) (from [4]) have considerable similarities.
(1) s. His machine operation is very good.
t. He operates the machine very well.
(2) s. My son?s bat control is unskillful yet.
t. My son controls his bat poorly yet.
(3) s. Steven made an attempt to stop playing.
t. Steven attempted to stop playing.
(4) s. It had a noticeable effect on the trade.
t. It noticeably affected the trade.
1 For each example, ?s? and ?t? denote an original sentence and its paraphrase, respectively. Note
that our target language is Japanese. English examples are used for an explanatory purpose.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 908?919, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
Exploiting Lexical Conceptual Structure for Paraphrase Generation 909
However, the regularity we find in these examples is not so simple that it cannot be
captured only in syntactic terms. For example, the transformation pattern as in (1) and
(2) does not apply to another compound noun ?machine translation.? We can also find
a range of varieties in paraphrasing of LVCs as we describe in Section 3.
In spite of this complexity, the regularity each paraphrase class exhibits were ex-
plained by recent advances in lexical semantics, such as the Lexical Conceptual Struc-
ture (LCS) [8] and the Generative Lexicon [17]. According to the LCS, for instance,
a wide variety of paraphrases including word association within compounds, transitiv-
ity alternation, and lexical derivation, were explained by means of the syntactic and
semantic properties of the verb involved. The systematicity underlying such linguistic
accounts is intriguing also from the engineering viewpoint as it could enable us to take
a more theoretically motivated but still practical approach to paraphrase generation.
The issue we address in this paper is to empirically clarify (i) what types of regular-
ities underlying paraphrases can be explained by means of lexical semantics and how,
and (ii) how lexical semantics theories can be enhanced with feedback from practical
use, namely, paraphrase generation. We make an attempt to exploit the LCS among sev-
eral lexical semantics frameworks, and propose a paraphrase generation model which
utilizes LCS combining with syntactic transformation.
2 Lexical Conceptual Structure
2.1 Basic Framework
Among several frameworks of lexical semantics, we focus on the Lexical Conceptual
Structure (LCS) [8] due to the following reasons. First, several studies [9,3,19] have
shown that the theory of the LCS provides a systematic explanation of semantic de-
composition as well as syntax determines. In particular, Kageyama [9] has shown that
even a simple typology of LCS can explain a wide variety of linguistic phenomena in-
cluding word association within compounds, transitivity alternation, and lexical deriva-
tion. Second, large-scale LCS dictionaries have been developed through practical use
on machine translation and compound noun analysis [3,19]. The LCS dictionary for
English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classifica-
tion [12] with an expansion for the semantic role delivered to arguments. For Japanese,
Takeuchi et al [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) called
the T-LCS dictionary, following Kageyama?s analysis [9]. In this paper, we make use of
the current version of the T-LCS dictionary, because it provides a set of concrete rules
for LCS assignment, which ensures the reliability of the dictionary.
Examples of LCS in the T-LCS dictionary are shown in Table 1. An LCS consists
of a combination of semantic predicates (?CONTROL,? ?BE AT,? etc.) and their argu-
ment slots (x, y, and z). Each argument slot corresponds to a semantic role, such as
?Agent,? ?Theme,? and ?Goal,? depending on its surrounding semantic predicates. Let
us take ?yakusu (to translate)? as an example. The inner structure ?[y BE AT z]? de-
notes the state of affairs where z (?Goal?) indicates the state or physical location of y
(?Theme?). The predicate ?BECOME? expresses a change of y. In the case of example
phrase in Table 1, the change of the language of the book is represented. The leftmost
910 A. Fujita, K. Inui, and Y. Matsumoto
Table 1. Examples of LCS
LCS for verb (example verb)
example Japanese phrase
[y BE AT z] (ichi-suru (to locate), sonzai-suru (to exist))
gakkou-ga kawa-no chikaku-ni ichi-suru.
school-NOM river-GEN near-DAT to locate-PRES
The school (Theme) locates near the river (Goal).
[BECOME [y BE AT z]] (houwa-suru (to become saturate), bunpu-suru (to be distributed))
kono-hana-ga sekaiju-ni bunpu-suru.
this flower-NOM all over the world-DAT to distribute-PRES
This flower (Theme) is distributed all over the world (Goal).
[x CONTROL [BECOME [y BE AT z]]] (yakusu (to translate), shoukai-suru (to introduce))
kare-ga hon-o nihongo-ni yakusu.
he-NOM book-ACC Japanese-DAT to translate-PRES
He (Agent) translates the book (Theme) into Japanese (Goal).
[x ACT ON y] (unten-suru (to drive), sousa-suru (to operate))
kare-ga kikai-o sousa-suru.
he-NOM machine-ACC to operate-PRES
He (Agent) operates the machine (Theme).
[y MOVE TO z] (ido-suru (to move), sen?i-suru (to propagate))
ane-ga tonarimachi-ni ido-suru.
my sister-NOM neighboring town-DAT to move-PRES
My sister (Theme) moves to a neighboring town (Goal).
part ?[x CONTROL . . .]? denotes that the ?Agent? causes the state change. The differ-
ence between ?BECOME BE AT? and ?MOVE TO? is underlying their telicity: the for-
mer indicates telic, and thus the verb can be perfective, while the latter atelic. Likewise,
?CONTROL? implicates a state change, while ?ACT ON? merely denotes an action. The
following are examples of syntactic and semantic properties represented in LCS:
? Semantic role of argument (e.g. ?[x CONTROL . . .]? indicates x=?Agent?)
? Syntactic case particle pattern (e.g. ?[y MOVE TO z]? indicates y=NOM, z=DAT)
? Aspectual property (e.g. ?MOVE TO? is atelic (??ket-tearu (to kick-PERF)?), while
?BECOME BE AT? is telic (?oi-tearu (to place-PERF).?))
? Focus of statement
(e.g. x is focused in ?[x CONTROL . . .]?, while z in ?[z BE WITH . . .]?)
? Semantic relations in lexical derivation
? transitivity alternation (?kowasu (to break (vt))? ? ?kowareru (to break (vi))?)
? lexical active-passive alternation (?oshieru (to teach)? ? ?osowaru (to be
taught)?)
2.2 Disambiguation in LCS Analysis
In principle, a verb is associated with more than one LCS if it has multiple senses.
The mapping from syntactic case assignments to argument slots in LCS is also many-
Exploiting Lexical Conceptual Structure for Paraphrase Generation 911
to-many in general. In the case of Japanese, the case particle ?ni? tends to be highly
ambiguous as demonstrated in (5).
(5) a. shuushin-jikan-o yoru-11ji-ni henkou-shita.
bedtime-ACC 11 p.m.-DAT (complement) to change-PAST
I changed my bedtime to 11 p.m.
b. yoru-11ji-ni yuujin-ni mail-o okut-ta.
11 p.m.-DAT (adjunct) friends-DAT (complement) mail-ACC to send-PAST
I sent a mail to my friends at 11 p.m.
Resolution of these sorts of ambiguity is called semantic parsing and has been ac-
tively studied by many researchers recently [6,2] as semantically annotated corpora and
lexical resources such as the FrameNet [1] and the Proposition Bank [16] have become
available. Relying on the promising results of this trend of research, we do not address
the issue of semantic parsing in this paper to focus our attention on the generation side
of the whole problem.
3 Paraphrasing of Light-Verb Constructions
In this paper, we focus our discussion on one class of paraphrases, i.e., paraphrasing of
light-verb constructions (LVCs). Sentence (6s) shows an example of an LVC. An LVC
is a verb phrase (?kandou-o atae-ta (made an impression),? c.f., Figure 1) that consists
of a light-verb (?atae-ta (to give-PAST)?) that syntactically governs a deverbal noun
(?kandou (an impression)?). A paraphrase of (6s) is shown in sentence (6t), where the
deverbal noun functions as the main verb with its verbalized form (?kandou-s-ase-ta (to
be impressed-CAUSATIVE-PAST)?).
(6) s. eiga-ga kare-ni saikou-no kandou-o atae-ta.
film-NOM him-DAT supreme-GEN impression-ACC to give-PAST
The film made an supreme impression on him.
t. eiga-ga kare-o saikou-ni kandou-s-ase-ta.
film-NOM him-ACC supreme-DAT to be impressed-CAUSATIVE-PAST
The film supremely impressed him.
Example (6) indicates that we need an information to determine how the voice of target
sentence must be changed and how the case particles of the nominal elements must be
reassigned. These decisions depend not only on the syntactic and semantic attributes of
the light-verb, but also on those of the deverbal noun [14]. LVC paraphrasing is thus a
novel challenging material for exploiting LCS.
Figure 1 demonstrates tree representations of source and target expressions involved
in LVC paraphrasing, taking (6) as an example. To generate this type of paraphrase, we
need a computational model that is capable of the following operations:
Change of the dependence: Change the dependences of the elements (a) and (b) due
to the elimination of the original modifiee, the light-verb. This operation can be
done by just making them dependent on the resultant verb.
912 A. Fujita, K. Inui, and Y. Matsumoto
saikou-ni
i - i
kare-o
r -
eiga-ga
i -
*
kandou-o
-
kare-ni
r - i
(c) <noun>+GEN
saikou-no
i -
*
(d) <adjective> or <embedded clause>
(a) <adverb>
(b) <noun>
+ <case particle>
LVC
kandou-
s-ase-ta
-
- -t
<deverbal noun>
+ <verbal suffixes>
(a)
(b)
(d)
eiga-ga
i -
atae-ta
t -t
*
*
(c)<deverbal noun>
+ <case particle>
<light-verb>
+ <verbal suffixes>
Fig. 1. Dependency structure showing the range which the LVC paraphrasing affects. The oval
objects denote Japanese base-chunks so-called bunsetsu.
Re-conjugation: Change the conjugation form of the elements (d) and occasionally
(c), according to the syntactic category change of their modifiee: the given deverbal
noun is verbalized. This operation can be carried out independently of the LVC
paraphrasing.
Selection of the voice: Choose the voice of the target sentence among active, passive,
causative, etc. In example (6), the causative (the auxiliary verb ?ase?) is chosen.
The decision depends on the syntactic and semantic attributes of both the given
light-verb and the deverbal noun [14].
Reassignment of the cases: Assign the case particles of the elements (b) and (c), the
arguments of the main verb. In (6), the syntactic case of ?kare (him),? which was
originally assigned dative case ?ni? is changed to accusative ?o.?
Among these operations, this paper focuses on the last two, namely handling the ele-
ment (b), the sibling cases of the deverbal noun. Triangles in both trees in Figure 1 indi-
cate the range which we handle. Henceforth, elements outside of the triangles, namely,
(a), (c), and (d), are used only for explanatory purposes.
4 LCS-Based Paraphrase Generation Model
Figure 2 illustrates how our model paraphrases the LVC, taking (7) as an example.
(7) s. Ken-ga eiga-ni shigeki-o uke-ta.
Ken-NOM film-DAT inspiration-ACC to receive-PAST
Ken received an inspiration from the film.
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT to inspire-PASSIVE-PAST
Ken was inspired by the film.
The generation process consists of the following three steps:
Step 1. Semantic analysis: The model first analyzes a given input sentence including
an LVC to obtain its LCS representation. In Figure 2, this step generates LCSV 1 by
filling arguments of LCSV 0 with nominal elements.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 913
LCSdic.
i .
Step 2
LCS transformation
t  
 tr f r ti
+
Step 3
Surface generation
t  
rf  r ti
?ukeru?
(to receive)
z-NOM y-ACC x-DAT
?shigeki-suru?
(to inspire)
x?-NOM y?-ACC
Step 1
Semantic analysis
t  
ti  l i
LCS
N0
LCS
V0
LCS
V1
LCS
N1
Input sentence
Paraphrased sentence
Ken-NOM film-DAT to inspire-PASSIVE
fil t  i i I
Ken-NOM film-DAT inspiration-ACC to receive-ACT
fil i i ti t  i
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
[BECOME [[Ken]z BE WITH ?] 
[  [[ ]  I  ] 
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
LCS
S
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
Fig. 2. LCS-based paraphrase generation model
Step 2. Semantic transformation (LCS transformation): The model then transfers
the obtained semantic structure to another semantic structure so that the target struc-
ture consists of the LCS of the verbalized form of the deverbal noun. In our exam-
ple, this step generates LCSN1 together with the supplement ?[BECOME [. . .]]?. We
refer to such a supplement as LCSS .
Step 3. Surface generation: Having obtained the target LCS representation, the model
finally generates the output sentence from it. LCSS triggers another syntactic alter-
nation such as passivization and causativization.
The idea is to use the LCS representation as a semantic representation and to re-
trieve semantic constraints to relieve the syntactic underspecificity underlying the LVC
paraphrasing. Each step consists of a handful of linguistically explainable rules, and
thus is scalable when the typology and resource of LCS is given. The rest of this sec-
tion elaborates on each step, differentiating symbols to denote arguments; x, y, and z
for LCSV , and x?, y?, and z? for LCSN .
4.1 Semantic Analysis
Given an input sentence (a simple clause with an LVC), the model first looks up the
LCS template LCSV 0 for the given light-verb in the T-LCS dictionary, and then applies
the case assignment rule below to obtain its LCS representation LCSV 1:
? In the case of the LCSV 0 having argument x, fill the leftmost argument of the
LCSV 0 with the nominative case of the input, the second leftmost with the ac-
cusative, and the rest with the dative case.
? Otherwise, fill arguments y and z of the LCSV 0 with the nominative and the dative
cases, respectively.
This rule is proposed in [19] instead of semantic parsing in order to tentatively
automate LCS-based processing. In the example shown in Figure 2, LCSV 0 for the
914 A. Fujita, K. Inui, and Y. Matsumoto
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
[BECOME [[Ken]z BE WITH ?] 
[  [[ ]  I  ] 
+
LCS
V1
LCS
N1
Predicate and
argument matching
Treatment of
non-transferred predicates 
LCS
S
Fig. 3. An example of LCS transformation
given light-verb ?ukeru (to receive)? has argument x, thus the nominative case, ?Ken,?
fills the leftmost argument z. Accordingly, the accusative (?shigeki (inspiration)?) and
the dative (?eiga (film)?) fill y and x, respectively.
4.2 LCS Transformation
The second step matches LCSV 1 with the another LCS for the verbalized form of the
deverbal noun LCSN0 to generate the target LCS representation LCSN1. Figure 3 shows
a more detailed view of this process for the example shown in Figure 2.
Muraki [14] described that the direction of action and the focus of statement are
important clues to determine the voice in LVC paraphrasing. We therefore incorporate
the below assumptions into matching process. The model first matches predicates in
LCSV 1 and LCSN0, assuming that the agentive argument x is relevant to the direc-
tion of action. We classify the semantic predicates into the following three groups: (i)
agentive predicates (involve argument x): ?CONTROL,? ?ACT ON,? ?ACT TO,? ?ACT,?
and ?MOVE FROM TO,? (ii) state of affair predicates (involve only argument y or z):
?MOVE TO,? ?BE AT,? and ?BE WITH,? and (iii) aspectual predicates (with no argu-
ment): ?BECOME,? and allowed any pair of predicates in the same group to match. In
our example, ?MOVE FROM TO? matches ?ACT ON? as shown in Figure 3.
Having matched the predicates, the model then fills each argument slot in LCSN0
with its corresponding argument in LCSV 1. In Figure 3, argument z is matched with
y?, and x with x?. As a result, ?Ken? and ?eiga? come to y? and x? slots, respectively.
When an argument is filled with another LCS, arguments within the inner LCS are also
taken into account. Likewise, we introduced some exceptional rules assuming that the
input sentences are periphrastic. For instance, arguments filled with the implicit filler
(e.g. ?name? for ?to sign? is usually not expressed in Japanese) and the deverbal noun,
which is already represented by LCSN0 are never matched. Argument z in LCSV 1 is
allowed to match with y? in LCSN0.
LCS representations have right-embedding structures, and inner-embedded pred-
icates denote the state of affairs. We thus prioritize the rightmost predicates in this
matching process. In other words, the proceeds from the rightmost inner predicates
to the outer ones, and the matching process is repeated until the leftmost predicate in
LCSN0 or that in LCSV 1 matched.
If LCSV 1 has any non-transferred part LCSS when the predicate and argument
matching has been completed, it represents the semantic content that is not expressed
by LCSN1 and needs to be expressed by auxiliary linguistic devices such as voice
auxiliaries. As described in Section 2.1, the leftmost part specifies the focus of state-
Exploiting Lexical Conceptual Structure for Paraphrase Generation 915
ment. The model thus attaches LCSS to LCSN0 as a supplement, and then use it to
determine auxiliaries in the next step, the surface generation. In the case of Figure 3,
?[BECOME [[Ken]z BE WITH]]? in LCSV 1 remains non-transferred and be attached.
4.3 Surface Generation
The model again applies the aforementioned case assignment rule to generate a sentence
from the resultant LCS. From the LCSN1 in Figure 2, sentence (8) is generated.
(8) eiga-ga Ken-o shigeki-shi-ta.
film-NOM Ken-ACC to inspire-PAST
The film inspired Ken.
The model then makes the final decision on the selection of the voice and the reas-
signment of the cases. As we described above, the attached structure LCSS is a clue to
determine what the focus is. We therefore use the following decision list:
1. If the leftmost argument of LCSS has the same value as the leftmost argument in
LCSN1, the viewpoints of LCSS and LCSN1 are same. Thus, the active voice is
selected and the case structure is left as is.
2. If the leftmost argument of LCSS has the same value as either z? or y? in LCSN1,
the model makes the argument a subject (nominative). That is, the passive voice is
selected and case alternation (passivization) is applied.
3. If LCSS has ?BE WITH? and its argument has the same value as x? in LCSN1, the
causative voice is selected and case alternation (causativization) is applied.
4. If LCSS has an agentive predicate, and its argument is filled with a value different
from those of the other arguments, then the causative voice is selected and case
alternation (causativization) is applied.
5. Otherwise, active voice is selected and thus no modification is applied.
The example in Figure 2 satisfies the second condition, thus the model chooses ?s-
are-ru (PASSIVE)? and passivizes the sentence (8). As a result, ?Ken? becomes to be
the nominative ?ga? as in (7t).
5 Experiment
5.1 Paraphrase Generation and Evaluation
To conduct an empirical experiment, we collected the following data sets. Note that
more than one LCS was assigned to a verb if it was polysemous.
Deverbal nouns: We regard ?sahen-nouns? and adverbial forms of verbs as deverbal
nouns. We retrieved 1,210 deverbal nouns from the T-LCS dictionary. The set con-
sists of (i) activity nouns (e.g., ?sasoi (invitation)? and ?odoroki (surprise)?), (ii) Sino-
Japanese verbal nouns (e.g., ?kandou (impression)? and ?shigeki (inspiration)?), and
(iii) English borrowings (e.g., ?drive? and ?support?).
Tuples of light-verb and case particle: A verb takes different meanings when it con-
stitutes LVCs with different case particles, and not every tuple of a light-verb v and a
916 A. Fujita, K. Inui, and Y. Matsumoto
case particle c functions as an LVC. We therefore tailored an objective collection of
tuples ?v, c? from corpus in the following manner:
Step 1. From a corpus consisting of 25 million parsed sentences of newspaper articles,
we collected 876,101 types of triplet ?v, c, n?, where v, c, and n denote a base form
of verb, a case particle, and an deverbal noun.
Step 2. For each of the 50 most frequent ?v, c? tuples, we extracted the 10 most fre-
quent triplets ?v, c, n?.
Step 3. Each ?v, c, n? was manually evaluated to determine whether it functioned as an
LVC. If any of 10 triplets functioned as an LVC, the tuple ?v, c? was merged into
the list of light-verbs, assigning an LCS according to the linguistic tests examined
in [19]. As a result, we collected 40 types of ?v, c? for light-verbs.
Paraphrase examples: A collection of paraphrase examples, pairs of an LVC and its
correct paraphrase, were constructed in the following way:
Step 1. From the 876,101 types of triplet ?v, c, n? collected above, 23,608 types of
?v, c, n? were extracted, whose components, n and ?v, c?, were in the dictionaries.
Step 2. For each of the 245 most frequent ?v, c, n?, the 3 most frequent simple clauses
including the ?v, c, n? were extracted from the same corpus.
Step 3. Two native speakers of Japanese, adults graduated from university, were em-
ployed to build a gold-standard collection. 711 out of 735 sentences were manually
paraphrased in the manner of LVC, while the remaining 24 sentences were not
because ?v, c, n? within them did not function as LVCs.
The real coverage of these 245 ?v, c, n? with regard to all LVCs among the corpus falls
in the range between the below two:
Lower bound: If every ?v, c, n? is an LVC, the coverage of the collection is estimated
at 6.47% (492,737 / 7,621,089) of tokens.
Upper bound: If the dictionaries cover all light-verbs and deverbal nouns, the collec-
tion covers 24.1% (492,737 / 2,044,387) of tokens.
In the experiment, our model generated all the possible paraphrases when a given
verb was polysemous with multiple entries in the T-LCS dictionary. As a result, the
model generated 822 paraphrases from the 735 input sentences, at least one for each
input. We then classified the resultant paraphrases as correct and incorrect by compar-
ing them with the gold-standard, where we ignored ordering of syntactic cases, and
obtained 624 correct and 198 incorrect paraphrases Recall, precision, and F-measure
(? = 0.5) were 0.878 (624 / 711), 0.759 (624 / 822), and 0.814, respectively.
As the baseline, we employed a statistical language model developed in [5]. Among
all the combinations of the voice and syntactic cases, the baseline model selects the
one that has the highest probability. Although the model is trained on a large amount
of data, the generated expression often falls out of the vocabulary. In such a case, the
probability cannot be calculated, and the model outputs nothing for the given sentence.
As a result of an application of this baseline model to the same set of input sentences,
we obtained 320 correct and 215 incorrect paraphrases (Recall: 0.450 (320 / 711), Pre-
cision: 0.598 (320 / 535), and F-measure: 0.514). The significant improvement indicates
that our lexical-semantics-based account benefited on the decisions we considered.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 917
The language model can also be complementary used to our LCS-based paraphrase
generation. By filtering implausible paraphrases out, 66 incorrect and 15 correct para-
phrases were filtered, and the performance was further improved (Recall: 0.857, Preci-
sion: 0.822, and F-measure: 0.839).
5.2 Discussion
Although the performance has room for further improvement, we think the perfor-
mance is reasonably high under the current stage of the T-LCS dictionary. In other
words, the tendency of errors does not so differ from our expectation. As we expected
in Section 2.2, the ambiguity of dative case ?ni? (c.f. (5)) occupied the largest portion
of errors (78 / 198). This was because the case assignment was performed by a rule in-
stead of semantic parsing. Each rule in our model has been created relying on a set of
linguistic tests used in the theory of LCS and our linguistic intuition on handling LCS.
However, the rule set was not sufficiently sophisticated, so that led to 59 errors. Equally,
30 errors occurred due to the immature typology of the T-LCS dictionary.
We consider the improvement of the LCS typology as the primal issue, because
our transformation rules depend on it. For the moment, we have the following two
suggestions. First, more variety of semantic roles should be handled step by step. For
example, we need to handle the object of ?eikyou-suru (to affect),? which is marked by
not accusative but dative. Second, the necessity of ?Source? is inconsistent. Verbs such
as ?hairu (to enter)? do not require this argument (?BECOME BE AT?) , while some
other verbs, such as ?ukeru (to receive),? explicitly require it (?MOVE FROM TO?). The
telicity of ?MOVE FROM TO? should also be discussed. With such a feedback from
the application and an extensive investigation into lexicology, we have to enhance the
typology, and enlarge the dictionary preserving its consistency.
6 Related Work
The paraphrases associated with LVCs are not idiosyncratic to Japanese but also appear
commonly in other languages such as English, French, and Spanish [13,7,4] as shown
in (3) and (4). Our approach raises an interesting issue of whether the paraphrasing of
LVCs can be modeled in an analogous way across languages.
Iordanskaja et al [7] proposed a set of paraphrasing rules including one for LVC
paraphrasing based on the Meaning-Text Theory introduced by [13]. The model seemed
to properly handle LVC paraphrasing, because their rules were described according to
the deep semantic analysis and heavily relied on what were called lexical functions,
such as lexical derivation (e.g., S0(affect) = effect ) and light-verb generation (e.g.,
Oper1(attempt) = make). To take this approach, however, a vast amount of lexical
knowledge to form each lexical function is required, because they only virtually specify
all the choices relevant to LVC paraphrasing for every combination of deverbal noun
and light-verb individually. In contrast, our approach is to employ lexical semantics
to provide a general account of those classes of choices, and thus contributes to the
knowledge development in terms of reducing human-labor and preserving consistency.
Kaji et al [10] proposed a paraphrase generation model which utilized an monolin-
gual dictionary for human. Given an input LVC, their model paraphrases it referring to
918 A. Fujita, K. Inui, and Y. Matsumoto
the glosses of both the deverbal noun and light-verb, and a manually assigned semantic
feature of the light-verb. Their model looks robust due to the availability of resource.
However, their model fails to explain the difference between examples (7) and (9) in
the voice selection, because it selects the voice based only on the light-verb irrespec-
tive of the deverbal noun: the light-verb ?ukeru (to receive)? is always mapped to the
passive voice.
(9) s. musuko-ga kare-no hanashi-ni kandou-o uke-ta.
son-NOM his-GEN talk-DAT impression-ACC to receive-PAST
My son was given a good impression by his talk.
t. musuko-ga kare-no hanashi-ni kandou-shi-ta.
son-NOM his-GEN talk-DAT to be impressed-PAST
My son was impressed by his talk.
In their model, the target expression is restricted only to the LVC itself (c.f., Figure 1).
Hence, their model is unable to reassign the case particles as we saw in example (6).
There is another trend in the research of paraphrase generation: i.e., the automatic
paraphrase acquisition from existing lexical resources such as ordinary dictionaries,
parallel/comparable corpora, and non-parallel corpora. This type of approach may be
able to reduce the cost of resource development. However, there are drawbacks that
must be overcome before they can work practically. First, automatic methods require
large amounts of training data. The issue is how to collect enough large size of data at
low cost. Second, automatically extracted knowledge tends to be rather noisy, requiring
manual correction and maintenance. In contrast, our approach, which focuses on the
regularity underlying paraphrases, is a complementary avenue to develop and maintain
knowledge resources that cover a sufficiently wide range of paraphrases.
Previous case studies [14,18,11] have employed some syntactic properties of verbs
to constrain syntactic transformations in paraphrase generation: e.g. subject agentiv-
ity, aspectual property, passivizability, and causativizability. Several classifications of
verbs have also been proposed [12,15] based on various types of verb alternation and
syntactic case patterns. In contrast, the theory of lexical semantics integrates syntactic
and semantic properties including those above, and gives a perspective to formalize and
maintain the syntactic and semantic properties of words.
7 Conclusion
In this paper, we explored what sorts of lexical properties encoded in LCS can explain
the regularity underlying paraphrases. Based on an existing LCS dictionary, we built
an LCS-based paraphrase generation model, and conducted an empirical experiment on
paraphrasing of LVC. The experiment confirmed that the proposed model was capable
of generating paraphrases accurately in terms of selecting the voice and reassigning the
syntactic cases, and revealed potential difficulties that we have to overcome toward a
practical use of our lexical-semantics-based account. To make our model more accu-
rate, we need further discussion on (i) the enhancement of the T-LCS dictionary with
feedback from experiments, (ii) the LCS transformation algorithm, and (iii) the seman-
tic parsing. Another goal is to practically clarify what extent can be done by LCS for
other classes of paraphrase, such as those exemplified in Section 1.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 919
References
1. C. F. Baker, C. J. Fillmore, and J. B. Lowe. The Berkeley FrameNet project. In Proceedings
of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (COLING-ACL), pages 86?90, 1998.
2. X. Carreras and L. Ma`rques. Introduction to the CoNLL-2004 shared task: semantic role
labeling. In Proceedings of 8th Conference on Natural Language Learning (CoNLL), pages
89?97, 2004.
3. B. J. Dorr. Large-scale dictionary construction for foreign language tutoring and interlingual
machine translation. Machine Translation, 12(4):271?322, 1997.
4. M. Dras. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis,
Division of Information and Communication Science, Macquarie University, 1999.
5. A. Fujita, K. Inui, and Y. Matsumoto. Detection of incorrect case assignments in automat-
ically generated paraphrases of Japanese sentences. In Proceedings of the 1st International
Joint Conference on Natural Language Processing (IJCNLP), pages 14?21, 2004.
6. D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288, 2002.
7. L. Iordanskaja, R. Kittredge, and A. Polgue`re. Lexical selection and paraphrase in a meaning-
text generation model. In C. L. Paris, W. R. Swartout, and W. C. Mann, editors, Natural
Language Generation in Artificial Intelligence and Computational Linguistics, pages 293?
312. Kluwer Academic Publishers, 1991.
8. R. Jackendoff. Semantic structures. The MIT Press, 1990.
9. T. Kageyama. Verb semantics. Kurosio Publishers, 1996. (in Japanese).
10. N. Kaji and S. Kurohashi. Recognition and paraphrasing of periphrastic and overlapping
verb phrases. In Proceedings of the 4th International Conference on Language Resources
and Evaluation (LREC) Workshop on Methodologies and Evaluation of Multiword Units in
Real-world Application, 2004.
11. K. Kondo, S. Sato, and M. Okumura. Paraphrasing by case alternation. IPSJ Journal,
42(3):465?477, 2001. (in Japanese).
12. B. Levin. English verb classes and alternations: a preliminary investigation. Chicago Press,
1993.
13. I. Mel?c?uk and A. Polgue`re. A formal lexicon in meaning-text theory (or how to do lexica
with words). Computational Linguistics, 13(3-4):261?275, 1987.
14. S. Muraki. Various aspects of Japanese verbs. Hitsuji Syobo, 1991. (in Japanese).
15. A. Oishi and Y. Matsumoto. Detecting the organization of semantic subclasses of Japanese
verbs. International Journal of Corpus Linguistics, 2(1):65?89, 1997.
16. M. Palmer, D. Gildea, and P. Kingsbury. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?106, 2005.
17. J. Pustejovsky. The generative lexicon. The MIT Press, 1995.
18. S. Sato. Automatic paraphrase of technical papers? titles. IPSJ Journal, 40(7):2937?2945,
1999. (in Japanese).
19. K. Takeuchi, K. Kageura, and T. Koyama. An LCS-based approach for analyzing Japanese
compound nouns with deverbal heads. In Proceedings of the 2nd International Workshop on
Computational Terminology (CompuTerm), pages 64?70, 2002.
A Class-oriented Approach to Building a Paraphrase Corpus
Atsushi Fujita
Graduate School of Informatics,
Kyoto University
fujita@pine.kuee.kyoto-u.ac.jp
Kentaro Inui
Graduate School of Information Science,
Nara Institute of Science and Technology
inui@is.naist.jp
Abstract
Towards deep analysis of composi-
tional classes of paraphrases, we have
examined a class-oriented framework
for collecting paraphrase examples, in
which sentential paraphrases are col-
lected for each paraphrase class sep-
arately by means of automatic can-
didate generation and manual judge-
ment. Our preliminary experiments on
building a paraphrase corpus have so
far been producing promising results,
which we have evaluated according to
cost-efficiency, exhaustiveness, and re-
liability.
1 Introduction
Paraphrases are alternative ways of conveying the
same content. The technology for paraphrase
generation and recognition has drawn the atten-
tion of an increasing number of researchers be-
cause of its potential contribution to a broad range
of natural language applications.
Paraphrases can be viewed as monolingual
translations. From this viewpoint, research on
paraphrasing has adapted techniques fostered in
the literature of machine translation (MT), such
as transformation algorithms (Lavoie et al, 2000;
Takahashi et al, 2001), corpus-based techniques
for paraphrase pattern acquisition (Barzilay and
McKeown, 2001; Shinyama and Sekine, 2003;
Quirk et al, 2004), and fluency measurements
(Lapata, 2001; Fujita et al, 2004).
One thing the paraphrasing community is still
lacking is shared collections of paraphrase ex-
amples that could be used to analyze problems
underlying the tasks and to evaluate the perfor-
mance of systems under development. To our best
knowledge, the paraphrase corpus developed by
Dolan et al (2004) is one of the very few collec-
tions available for free1. Development of para-
phrase corpora raises several issues: what sorts
of paraphrases should be collected, where para-
phrase examples can be obtained from, how the
coverage and quality of the corpus can be ensured,
how manual annotation cost can be effectively re-
duced, and how collected examples should be or-
ganized and annotated.
Obviously these issues should be discussed
with the purpose of each individual corpus taken
into account. In this paper, we address the is-
sues of building a gold-standard corpus that is to
be used to evaluate paraphrase generation models
and report on our preliminary experiences taking
Japanese as a target language. Our approach is
characterized by the following:
? We define a set of paraphrase classes based
on the syntactic features of transformation
patterns.
? We separately collect paraphrase examples
for each paraphrase class that are considered
to be linguistically explainable.
? We use a paraphrase generation system to
exhaustively collect candidate paraphrases
from a given text collection, which are then
manually labeled.
15801 sentence pairs from their comparable corpus have
been judged manually and available from
http://research.microsoft.com/research/nlp/msr paraphrase.htm
25
2 Goal
Paraphrases exhibit a wide variety of patterns
ranging from lexical paraphrases to syntactic
transformations and their combinations. Some
of them are highly inferential or idiomatic and
do not seem easy to generate only with syntactic
and semantic knowledge. Such groups of para-
phrases require us to pursue corpus-based acquisi-
tion methods such as those described in Section 3.
More importantly, however, we can also find
quite a few patterns of paraphrases that exhibit a
degree of regularity. Those groups of paraphrases
have a potential to be compositionally explained
by combining syntactic and semantic properties
of their constituent words. For instance, the fol-
lowing paraphrases2 in Japanese are considered to
be of these groups.
(1) s. eiga-ni shigeki-o uke-ta.
film-DAT inspiration-ACC to receive-PAST
I received an inspiration from the film.
t. eiga-ni shigeki-s-are-ta.
film-DAT to inspire-PASS-PAST
I was inspired by the film.
(2) s. sentakumono-ga soyokaze-ni yureru.
laundry-NOM breeze-DAT to sway-PRES
The laundry sways in the breeze.
t. soyokaze-ga sentakumono-o yurasu.
breeze-NOM laundry-ACC to sway-PRES
The breeze makes the laundry sways.
(3) s. glass-ni mizu-o mitashi-ta.
glass-DAT water-ACC to fill-PAST
I filled water into the glass.
t. glass-o mizu-de mitashi-ta.
glass-ACC water-IMP to fill-PAST
I filled the glass with water.
(4) s. kare-wa kikai-sousa-ga jouzu-da.
he-TOP machine operation-NOM be good-PRES
He is good at machine operation.
t. kare-wa kikai-o jouzu-ni sousa-suru.
he-TOP machine-ACC well-ADV to operate-PRES
He operates machines well.
(5) s. heya-wa mou atatamat-teiru.
room-TOP already to be warmed-PERF
The room has already been warmed up.
t. heya-wa mou atatakai.
room-TOP already be warm-PRES
The room is warm.
2For each example, ?s? and ?t? denote an original sen-
tence and its paraphrase, respectively.
In example (1), a verb phrase, ?shigeki-o uke-
ta (to receive an inspiration),? is paraphrased into
a verbalized form of the noun, ?shigeki-s-are-ta
(to be inspired).? We can find a number of para-
phrases that exhibit a similar pattern of syntactic
transformation in the same language and group
such paraphrases into a single class, which is
possibly labeled ?paraphrasing of light-verb con-
struction.? Likewise, paraphrases exemplified by
(2) constitute another class, so-called transitiv-
ity alternation. Example (3) is of the locative
alternation class and example (4) the compound
noun decomposition class. In example (5), a verb
?atatamaru (to be warmed)? is paraphrased into
its adjective form, ?atatakai (be warm).? Para-
phrases involving such a lexical derivation are
also in our concern.
One can learn the existence of such groups
of paraphrases and the regularity each group ex-
hibits from the linguistic literature (Mel?c?uk and
Polgue`re, 1987; Jackendoff, 1990; Kageyama,
2001). According to Jackendoff and Kageyama,
for instance, both transitivity alternation and loca-
tive alternation can be explained in terms of the
syntactic and semantic properties of the verb in-
volved, which are represented by what they call
Lexical Conceptual Structure. The systematicity
underlying such linguistic accounts is intriguing
also from the engineering point of view as it could
enable us to take a more theoretically motivated
but still practical approach to paraphrase genera-
tion.
Aiming at this goal leads us to consider build-
ing a paraphrase corpus which enables us to eval-
uate paraphrase generation systems and conduct
error analysis for each paraphrase class sepa-
rately. Our paraphrase corpus should therefore be
organized according to paraphrase classes. More
specifically, we consider a paraphrase corpus such
that:
? The corpus consists of a set of subcorpora.
? Each subcorpus is a collection of paraphrase
sentence pairs of a paraphrase class.
? Paraphrases collected in a subcorpus suffi-
ciently reflect the distribution of the occur-
rences in the real world.
Given a paraphrase class and a text collection,
the goal of building a paraphrase corpus is to col-
lect paraphrase examples belonging to the class
26
as exhaustively as possible from the text collec-
tion at a minimal human labor cost. The resultant
corpus should also be reliable.
3 Related work
Previous work on building paraphrase corpus
(collecting paraphrase examples) can be classified
into two directions: manual production of para-
phrases and automatic paraphrase acquisition.
3.1 Manual production of paraphrases
Manual production of paraphrase examples has
been carried out in MT studies.
For example, Shirai et al (2001) and
Kinjo et al (2003) use collections of Japanese-
English translation sentence pairs. Given
translation pairs, annotators are asked to produce
new translations for each side of the languages.
Sentences that have an identical translation
are collected as equivalents, i.e., paraphrases.
Shimohata (2004), on the other hand, takes a
simpler approach in which he asks annotators to
produce paraphrases of a given set of English
sentences.
Obviously, if we simply asked human annota-
tors to produce paraphrases of a given set of sen-
tences, the labor cost would be expensive while
the coverage not guaranteed. Previous work,
however, has averted their eyes from evaluating
the cost-efficiency of the method and the cover-
age of the collected paraphrases supposedly be-
cause their primary concern was to enhance MT
systems.
3.2 Automatic paraphrase acquisition
Recently, paraphrase examples have been auto-
matically collected as a source of acquiring para-
phrase knowledge, such as pairs of synonymous
phrases and syntactic transformation templates.
Some studies exploit topically related articles
derived from multiple news sources (Barzilay and
Lee, 2003; Shinyama and Sekine, 2003; Quirk et
al., 2004; Dolan et al, 2004). Sentence pairs that
are likely to be paraphrases are automatically col-
lected from the parallel or comparable corpora,
using such clues as overlaps of content words and
named entities, syntactic similarity, and reference
description, such as date of the article and posi-
tions of sentences in the articles.
Automatic acquisition from parallel or compa-
rable corpora, possibly in combination with man-
ual correction, could be more cost-efficient than
manual production. However, it would not ensure
coverage and quality, because sentence pairing al-
gorithms virtually limit the range of obtainable
paraphrases and products tend to be noisy.
Nevertheless, automatic methods are useful to
discover a variety of paraphrases that need further
exploration. We hope that our approach to corpus
construction, which we present below, will work
complementary to those directions of research.
4 Proposed method
Recall that we require a corpus that reflects the
distribution of the occurrences of potential para-
phrases of each class because we aim to use it for
linguistic analysis and quantitative evaluation of
paraphrase generation models.
Since the issues we address here are highly em-
pirical, we need to empirically examine a range
of possible methods to gain useful methodologi-
cal insights. As an initial attempt, we have so far
examined a simple method which falls in the mid-
dle of the aforementioned two approaches. The
method makes use of an existing paraphrase gen-
eration system to reduce human labor cost as well
as to ensure coverage and quality:
Step 1. For a given paraphrase class, develop a
set of morpho-syntactic paraphrasing pat-
terns and lexical resources.
Step 2. Apply the patterns to a given text collec-
tion using the paraphrasing system to gener-
ate a set of candidate paraphrases.
Step 3. Annotate each candidate paraphrase with
information of the appropriateness accord-
ing to a set of judgement criteria.
We use morpho-syntactic paraphrasing patterns
derived from paraphrase samples in an analogous
way to previous methods such as (Dras, 1999).
For instance, from example (1), we derive a para-
phrasing pattern for paraphrasing of light-verb
constructions:
(6) s. N -o(?V ) V
N -ACC V
t. V (N)
V (N)
whereN is a variable which matches with a noun,
V a verb, V (N) denotes the verbalized form of
27
(e) confirmed (revised)
paraphrase
( ) fir  (r i )
r r
(c) annotator?s judge
(correct / incorrect)
( ) t t r?  j
( rr t / i rr t)
(d) error tags
( ) rr r t
(a) source sentence
( ) r  t
(b) automatically
generated
paraphrase
( ) t ti ll
r t
r r
(c) second opinion
(correct / incorrect)
( )  i i
( rr t / i rr t)
Given
Obligatory
Obligatory
Optional
(f) free comments
(f) fr  t
Optional
Figure 1: Annotation schema.
N , and the subscripted arrow in (6s) indicates that
N -o depends on V .
To exhaustively collect paraphrase examples
from a given text collection, we should not exces-
sively constrain paraphrasing patterns. To avoid
overly generating anomalies, on the other hand,
we make use of several lexical resources. For in-
stance, pairs of a deverbal noun and its transitive
form are used to constrainN and V (N) in pattern
(6). This way, we combine syntactic transforma-
tion patterns with lexical constraints to specify a
paraphrase class. This approach is practical given
the recent advances of shallow parsers.
For the judgement on appropriateness in Step 3,
we create a set of criteria separately for each para-
phrase class. When the paraphrase class in focus
is specified, the range of potential errors in candi-
date generation tends to be predictable. We there-
fore specify judgement criteria in terms of a ty-
pology of potential errors (Fujita and Inui, 2003);
namely, we provide annotators with a set of con-
ditions for ruling out inappropriate paraphrases.
Annotators judge each candidate paraphrase
with a view of an RDB-based annotation tool
(Figure 1). Given (a) a source sentence and
(b) an automatically generated candidate para-
phrase, human annotators are asked to (c) judge
the appropriateness of it and, if it is inappropri-
ate, they are also asked to (d) classify the un-
derlying errors into a predefined taxonomy, and
make (e) appropriate revisions (if possible) and
(f) format-free comments.
5 Preliminary trials
To examine how the proposed method actually
work regarding the issues, we conducted prelim-
inary trials, taking two classes of Japanese para-
phrases: paraphrasing of light-verb constructions
and transitivity alternation. This section de-
scribes the settings for each paraphrase class.
We sampled a collection of source sentences
from one year worth of newspaper articles: Ni-
hon Keizai Shinbun3, 2000, where the average
sentence length was 25.3 words. The reason
why we selected newspaper articles as a sample
source was that most of the publicly available
shallow parsers for Japanese were trained on a
tree-bank sampled from newspaper articles, and a
newspaper corpus was available in a considerably
large scale. We used for candidate generation the
morphological analyzer ChaSen4, the dependency
structure analyzer CaboCha5, and the paraphrase
generation system KURA6.
Two native speakers of Japanese, adults grad-
uated from university, were employed as annota-
tors. The process of judging each candidate para-
phrase is illustrated in Figure 2. The first annota-
tor was asked to make judgements on each candi-
date paraphrase. The second annotator inspected
all the candidates judged correct by the first an-
3http://sub.nikkeish.co.jp/gengo/zenbun.htm
4http://chasen.naist.jp/
5http://chasen.org/?taku/software/cabocha/
6http://cl.naist.jp/kura/doc/
28
Candidate
paraphrase
i t
r r
Correct
Incorrect
1st annotator 2nd annotator
Correct
Incorrect
Correct
Incorrect
Correct
Deferred
Incorrect
Discussion
Unseen
Deferred
f
Correct
t
Incorrect
I t
Label
Figure 2: Judgement procedure.
notator. To reduce the labor cost, only a small
subset of candidates that the first annotator judged
incorrect were checked by the second annotator,
leaving the rest labeled incorrect. Once in sev-
eral days, the annotators discussed cases on which
they disagreed, and if possible revised the anno-
tation criteria. When the discussion did not reach
a consensus, the judgement was deferred.
5.1 Paraphrasing of light-verb constructions
(LVC)
An example of this class is given in (1). A light-
verb construction consists of a deverbal noun
(?shigeki (inspiration)? in example (1)) governed
by a light-verb (?ukeru (to receive)?). A para-
phrase of this class is a pair of a light-verb con-
struction and its unmarked form, which consists
of the verbalized form of the deverbal noun where
the light-verb is removed.
Let N , V be a deverbal noun and a verb, and
V (N) be the verbalized form of N . Paraphrases
of this class can be represented by the following
paraphrasing pattern:
(7) s. N -{ga, o, ni}(?V ) V
N -{NOM, ACC, DAT} V
t. V (N)
V (N)
In the experiment, we used three more patterns to
gain the coverage.
We then extracted 20,155 pairs of deverbal
noun and its verbalized form (e.g. ?shigeki (in-
spiration)? and ?shigeki-suru (to inspire)?) from
the Japanese word dictionary, IPADIC (version
2.6.3)3. This set was used as a restriction on
nouns that can match with N in a paraphrasing
pattern. On the other hand, we made no restric-
tion on V , because we had no exhaustive list
of light-verbs. The patterns were automatically
compiled into pairs of dependency trees with
uninstantiated components, and were applied to
source sentences with the paraphrase generation
system, which carried out dependency structure-
based pattern matching. 2,566 candidate para-
phrases were generated from 10,000 source sen-
tences.
In the judgement phase, the annotators were
also asked to revise erroneous candidates if pos-
sible. The following revision operations were al-
lowed for LVC:
? Change of conjugations
? Change of case markers
? Insert adverbs
? Append verbal suffixes, such as voice, as-
pect, or mood devices
When pattern (7) is applied to sentence (1s), for
instance, we need to add a voice device, ?are (pas-
sive),? to correctly produce (1t). In example (8),
on the other hand, an aspectual device, ?dasu (in-
choative),? is appended, and a case marker, ?no
(GEN),? is replaced with ?o (ACC).?
(8) s. concert-no ticket-no hanbai-o hajime-ta.
concert-GEN ticket-GEN sale-ACC to start-PAST
We started to sale tickets for concerts.
t. concert-no ticket-o hanbai-shi-dashi-ta.
concert-GEN ticket-ACC to sell-INCHOATIVE-PAST
We started selling tickets for concerts.
So far, 1,114 candidates have been judged7 with
agreements on 1,067 candidates, and 591 para-
phrase examples have been collected.
5.2 Transitivity alternation (TransAlt)
This class of paraphrases requires a collection of
pairs of intransitive and transitive verbs, such as
?yureru (to sway)? and ?yurasu (to sway)? in ex-
ample (2). Since there was no available resource
of such knowledge, we newly created a mini-
mal set of intransitive-transitive pairs that were
required to cover all the verbs appearing in the
source sentence set (25,000 sentences). We first
retrieved all the verbs from the source sentences
using a set of extraction patterns implemented in
the same manner as paraphrasing patterns. Ex-
ample (9) is one of the patterns used, where Nx
matches with a noun, and V a verb.
7983 candidates for the first 4,500 sentences were fully
judged, and 131 candidates were randomly sampled from
the remaining portion.
29
(9) s. N1-ga(?V ) N2-ni(?V ) V
N1-NOM N2-DAT V
t. no change.
We then manually examined the transitivity of
each of 800 verbs that matched with V , and col-
lected 212 pairs of intransitive verb vi and its tran-
sitive form vt. Using them as constraints, we im-
plemented eight paraphrasing patterns as in (10).
(10) s. N1-ga(?Vi) N2-ni(?Vi) Vi
N1-NOM N2-DAT Vi
t. N2-ga(?Vt(Vi)) N1-o(?Vt(Vi)) Vt(Vi)
N2-NOM N1-ACC Vt(Vi)
where Vi and Vt(Vi) are variables that match with
vi and vt, respectively. By applying the patterns
to the same set of source sentences, we obtained
985 candidate paraphrases.
We created a set of criteria for judging ap-
propriateness (an example will be given in
Section 6.4) and revision examples for the follow-
ing operations allowed for this trial:
? Change of conjugations
? Change of case markers
? Change of voices
964 candidates have gained an agreement, and
484 paraphrase examples have been collected.
6 Results and discussion
Table 1 gives some statistics of the resultant para-
phrase corpora. Figures 3 and 4 show the number
of candidate paraphrases, where the horizontal
axes denote the total working hours of two anno-
tators, and the vertical axes the number of candi-
date paraphrases. The numbers of judged, correct,
incorrect, and deferred candidates are shown.
6.1 Efficiency
2,031 candidate paraphrases have so far been
judged in total and 1,075 paraphrase examples
have been collected in 287.5 hours. The judge-
ment was performed at a constant pace: 7.1 can-
didates (3.7 examples) in one hour. It is hard to
compare these results with other work because
no previous study quantitatively evaluate the effi-
ciency in terms of manual annotation cost. How-
ever, we feel that the results have so far been sat-
isfiable.
For each candidate paraphrase judged incor-
rect, the annotators were asked to classify the un-
derlying errors into the fixed error types ((d) in
Table 1: Statistics of the resultant corpora.
Paraphrase class LVC TransAlt
# of source sentences 10,000 25,000
# of patterns 4 8
Type of lexical resources ?n, vn? ?vi, vt?
Size of lexical resource 20,155 212
# of candidates 2,566 985
# of judged candidates 1,067 964
# of incorrect candidates 520 503
# of correct candidates 547 461
# of paraphrase examples 591 484
Working hours 118 169.5
Figure 1). This error classification consumed ex-
tra time because it required linguistic expertise
which the annotators were not familiar with.
TransAlt was 1.75 times more time-consuming
than LVC because the definition of TransAlt in-
volved several delicate issues, which made the
judgement process complicated. We return to this
issue in Section 6.4.
6.2 Exhaustiveness
To estimate how exhaustively the proposed
method collected paraphrase examples, we ran-
domly sampled 750 sentences from the 4,500
sentences that were used in the trial for LVC,
and manually checked whether the LVC para-
phrasing could apply to each of them. As a re-
sult, 206 examples were obtained, 158 of which
were those already collected by the proposed
method. Thus, the estimated exhaustiveness was
77% (158 / 206). Our manual investigation into
the missed examples has revealed that 47 misses
could have been automatically generated by en-
hancing paraphrasing patterns and dictionaries,
while only one example was missed due to an er-
ror in shallow parsing. 34 cases of the 48 misses
could have been collected by adding a couple of
paraphrasing patterns. For example, pattern (11)
verbalizes a noun followed by a nominalizing suf-
fix, ?ka (-ize),? as in (12).
(11) s. N -ka-{ga, o, ni}(?V ) V
N -ize-{NOM, ACC, DAT} V
t. V (N -ka)
V (N -ize)
(12) s. kore-wa kin?yu-shijo-no kassei-ka-ni
this-TOP financial market-GEN activation-DAT
muke-ta kisei-kanwa-saku-da.
to address-PAST deregulation plan-COP
This is a deregulation plan aiming at the
activation of financial market.
30
 0
 200
 400
 600
 800
 1000
 1200
 0  20  40  60  80  100  120
# 
of
 ju
dg
ed
 ca
nd
ida
tes
working hours
Judged
Correct
Incorrect
Deferred
Figure 3: # of judged candidates (LVC).
 0
 200
 400
 600
 800
 1000
 0  20  40  60  80  100  120  140  160  180
# 
of
 ju
dg
ed
 ca
nd
ida
tes
working hours
Judged
Correct
Incorrect
Deferred
Figure 4: # of judged candidates (TransAlt).
t. kore-wa kin?yu-shijo-o
this-TOP financial market-ACC
kassei-ka-suru kisei-kanwa-saku-da.
to activate-PRES deregulation plan-COP
This is a deregulation plan which activates
financial market.
We cannot know if we have adequate para-
phrasing patterns and resources before trials.
Therefore, manual examination is necessary to re-
fine them to bridge gap between the range of para-
phrases that can be automatically generated and
those of the specific class we consider.
6.3 Reliability
Ideally, more annotators should be employed to
ensure the reliability of the products, which, how-
ever, leads to a matter of balancing the trade-off.
Instead, we specified the detailed judgement cri-
teria for each paraphrase class, and asked the an-
notators to reconsider marginal cases several days
later and to make a discussion when judgements
disagreed. The agreement ratio for correct candi-
dates between two annotators increased as they
became used to the task. In the trial for LVC,
for example, the agreement ratio for each day
changed from 74% (day 3) to 77% (day 6), 88%
(day 9), and 93% (day 11). This indicates that the
judgement criteria were effectively refined based
on the feedback from inter-annotator discussions
on marginal and disagreed cases. To evaluate the
reliability of our judgement procedure more pre-
cisely, we are planing to employ the third annota-
tor who will be asked to judge all the cases inde-
pendently of the others.
6.4 How we define paraphrase classes
One of the motivations behind our class-based ap-
proach is an expectation that specifying the target
classes of paraphrases would simplify the awk-
ward problem of defining the boundary between
paraphrases an non-paraphrases. Our trials for the
two paraphrase classes, however, have revealed
that it can still be difficult to create a clear cri-
terion for judgement even when the paraphrase
class in focus is specified.
As one of the criteria for TransAlt, we tested
the agentivity of the nominative case of intransi-
tive verbs. The test used an adverb, ?muzukara
(by itself),? and classified a candidate paraphrase
as incorrect if the adverb could be inserted im-
mediately before the intransitive verb. For ex-
ample, we considered example (13) as a correct
paraphrase of the TransAlt class whereas (14) in-
correct because the agentivity exhibited by (14s)
did not remain in (14t).
(13) s. kare-ga soup-o atatame-ta.
he-NOM soup-ACC to warm up-PAST
He warmed the soup up.
t. soup-ga atatamat-ta. (correct)
soup-NOM to be warmed up-PAST
The soup was warmed up (by somebody).
(14) s. kare-ga koori-o tokashi-ta.
he-NOM ice-ACC to melt (vt)-PAST
He melted the ice.
t. koori-ga toke-ta. (incorrect)
ice-NOM to melt (vi)-PAST
The ice melted (by itself).
However, one might regard both paraphrases
incorrect because the information given by the
nominative argument of the source sentence is
31
dropped in the target in both cases. Thus, the
problem still remains. Nevertheless, our approach
will provide us with a considerable amounts of
concrete data, which we hope will lead us to bet-
ter understanding of the issue.
7 Conclusion
Towards deep analysis of compositional classes of
paraphrases, we have examined a class-oriented
framework for collecting paraphrase examples,
in which sentential paraphrases are collected for
each paraphrase class separately by means of au-
tomatic candidate generation and manual judge-
ment. Our preliminary experiments on building
a paraphrase corpus have so far been producing
promising results, which we have evaluated ac-
cording to cost-efficiency, exhaustiveness, and re-
liability. The resultant corpus and resources will
be available for free shortly. Our next step is di-
rected to targeting a wider range of paraphrase
classes.
References
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of the 2003 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 16?23.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources.
In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING),
pages 350?356.
Mark Dras. 1999. Tree adjoining grammar and the re-
luctant paraphrasing of text. Ph.D. thesis, Division
of Information and Communication Science, Mac-
quarie University.
Atsushi Fujita and Kentaro Inui. 2003. Explor-
ing transfer errors in lexical and structural para-
phrasing. IPSJ Journal, 44(11):2826?2838. (in
Japanese).
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in
automatically generated paraphrases of Japanese
sentences. In Proceedings of the 1st International
Joint Conference on Natural Language Processing
(IJCNLP), pages 14?21.
Ray Jackendoff. 1990. Semantic structures. The MIT
Press.
Taro Kageyama, editor. 2001. Semantics and syntax
of verb: comparable study between Japanese and
English. Taishukan Shoten. (in Japanese).
Yumiko Kinjo, Kunio Aono, Keishi Yasuda, Toshiyuki
Takezawa, and Genichiro Kikui. 2003. Collec-
tion of Japanese paraphrases of basic expressions
on travel conversation. In Proceedings of the 9th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 101?104. (in Japanese).
Maria Lapata. 2001. A corpus-based account of reg-
ular polysemy: the case of context-sensitive ad-
jectives. In Proceedings of the 2nd Meeting of
the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 63?70.
Benoit Lavoie, Richard Kittredge, Tanya Korelsky,
and Owen Rambow. 2000. A framework for MT
and multilingual NLG systems based on uniform
lexico-structural processing. In Proceedings of the
6th Applied Natural Language Processing Confer-
ence and the 1st Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (ANLP-NAACL), pages 60?67.
Igor Mel?c?uk and Alain Polgue`re. 1987. A formal
lexicon in meaning-text theory (or how to do lex-
ica with words). Computational Linguistics, 13(3-
4):261?275.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 142?149.
Mitsuo Shimohata. 2004. Acquiring paraphrases
from corpora and its application to machine trans-
lation. Ph.D. thesis, Graduate School of Informa-
tion Science, Nara Institute of Science and Tech-
nology.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of the 2nd International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP), pages 65?71.
Satoshi Shirai, Kazuhide Yamamoto, and Francis
Bond. 2001. Japanese-English paraphrase corpus.
In Proceedings of the 6th Natural Language Pro-
cessing Pacific Rim Symposium (NLPRS) Workshop
on Language Resources in Asia, pages 23?30.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, At-
sushi Fujita, and Kentaro Inui. 2001. KURA:
a transfer-based lexico-structural paraphrasing en-
gine. In Proceedings of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Work-
shop on Automatic Paraphrasing: Theories and Ap-
plications, pages 37?46.
32
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 225?232
Manchester, August 2008
A Probabilistic Model for Measuring Grammaticality and Similarity
of Automatically Generated Paraphrases of Predicate Phrases
Atsushi Fujita Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
Abstract
The most critical issue in generating and
recognizing paraphrases is development
of wide-coverage paraphrase knowledge.
Previous work on paraphrase acquisition
has collected lexicalized pairs of expres-
sions; however, the results do not ensure
full coverage of the various paraphrase
phenomena. This paper focuses on pro-
ductive paraphrases realized by general
transformation patterns, and addresses the
issues in generating instances of phrasal
paraphrases with those patterns. Our prob-
abilistic model computes how two phrases
are likely to be correct paraphrases. The
model consists of two components: (i) a
structured N -gram language model that
ensures grammaticality and (ii) a distribu-
tional similarity measure for estimating se-
mantic equivalence and substitutability.
1 Introduction
In many languages, a concept can be expressed
with several different linguistic expressions. Han-
dling such synonymous expressions in a given lan-
guage, i.e., paraphrases, is one of the key issues in
a broad range of natural language processing tasks.
For example, the technology for identifying para-
phrases would play an important role in aggregat-
ing the wealth of uninhibited opinions about prod-
ucts and services that are available on the Web,
from both the consumers and producers viewpoint.
On the other hand, whenever we draw up a docu-
ment, we always seek the most appropriate expres-
sion for conveying our ideas. In such a situation, a
system that generates and proposes alternative ex-
pressions would be extremely beneficial.
c
? Atsushi Fujita and Satoshi Sato, 2008. Licensed
under the Creative Commons Attribution-Noncommercial-
Share Alike 3.0 Unported license. Some rights reserved.
http://creativecommons.org/licenses/by-nc-sa/3.0/
Most of previous work on generating and recog-
nizing paraphrases has been dedicated to develop-
ing context-free paraphrase knowledge. It is typi-
cally represented with pairs of fragmentary expres-
sions that satisfy the following conditions:
Condition 1. Semantically equivalent
Condition 2. Substitutable in some context
The most critical issue in developing such
knowledge is ensuring the coverage of the para-
phrase phenomena. To attain this coverage, we
have proposed a strategy for dividing paraphrase
phenomena into the following two classes (Fujita
et al, 2007):
(1) Non-productive (idiosyncratic) paraphrases
a. burst into tears ? cried
b. comfort ? console
(Barzilay and McKeown, 2001)
(2) Productive paraphrases
a. be in our favor ? be favorable to us
b. show a sharp decrease ? decrease sharply
(Fujita et al, 2007)
Typical examples of non-productive paraphrases
are lexical paraphrases such as those shown in (1)
and idiomatic paraphrases of literal phrases (e.g.,
?kick the bucket? ? ?die?). Knowledge of this
class of paraphrases should be stored statically,
because they cannot be represented with abstract
patterns. On the other hand, a productive para-
phrase is one having a degree of regularity, as
exhibited by the examples in (2). It is therefore
reasonable to represent them with a set of general
patterns such as those shown in (3). This attains
a higher coverage, while keeping the knowledge
manageable.
(3) a. N
1
V N
2
? N
1
?s V -ing of N
2
b. N
1
V N
2
? N
2
be V -en by N
1
(Harris, 1957)
Various methods have been proposed to ac-
quire paraphrase knowledge (these are reviewed
in Section 2.1) where pairs of existing expres-
225
sions are collected from the given corpus, tak-
ing the above two conditions into account. On the
other hand, another issue arises when paraphrase
knowledge is generated from the patterns for pro-
ductive paraphrases such as shown in (3) by instan-
tiating variables with specific words, namely,
Condition 3. Both expressions are grammatical
This paper proposes a probabilistic model for
computing how likely a given pair of expressions
satisfy the aforementioned three conditions. In
particular, we focus on the post-generation assess-
ment of automatically generated productive para-
phrases of predicate phrases in Japanese.
In the next section, we review previous ap-
proaches and models. The proposed probabilis-
tic model is then presented in Section 3, where the
grammaticality factor and similarity factor are de-
rived from a conditional probability. In Section 4,
the settings for and results of an empirical exper-
iment are detailed. Finally, Section 5 summarizes
this paper.
2 Previous work
2.1 Acquiring paraphrase knowledge
The task of automatically acquiring paraphrase
knowledge is drawing the attention of an increas-
ing number of researchers. They are tackling the
problem of how precisely paraphrase knowledge
can be acquired, although they have tended to no-
tice that it is hard to acquire paraphrase knowl-
edge that ensures full coverage of the various
paraphrase phenomena from existing text corpora
alone. To date, two streams of research have
evolved: one acquires paraphrase knowledge from
parallel/comparable corpora, while the other uses
the regular corpus.
Several alignment techniques have been pro-
posed to acquire paraphrase knowledge from par-
allel/comparable corpora, imitating the techniques
devised for machine translation. Multiple trans-
lations of the same text (Barzilay and McKeown,
2001), corresponding articles from multiple news
sources (Barzilay and Lee, 2003; Quirk et al,
2004; Dolan et al, 2004), and bilingual corpus
(Bannard and Callison-Burch, 2005) have been
utilized. Unfortunately, this approach produces
only a low coverage because the size of the par-
allel/comparable corpora is limited.
In the second stream, i.e., paraphrase acquisition
from the regular corpus, the distributional hypothe-
sis (Harris, 1968) has been adopted. The similarity
of two expressions, computed from this hypothe-
sis, is called distributional similarity. The essence
of this measure is summarized as follows:
Feature representation: to compute the similar-
ity, given expressions are first mapped to
certain feature representations. Expressions
that co-occur with the given expression, such
as adjacent words (Barzilay and McKeown,
2001; Lin and Pantel, 2001), and modi-
fiers/modifiees (Yamamoto, 2002; Weeds et
al., 2005), have so far been examined.
Feature weighting: to precisely compute the sim-
ilarity, the weight for each feature is adjusted.
Point-wise mutual information (Lin, 1998)
and Relative Feature Focus (Geffet and Da-
gan, 2004) are well-known examples.
Feature comparison measures: to convert two
feature sets into a scalar value, several mea-
sures have been proposed, such as cosine,
Lin?s measure (Lin, 1998), Kullback-Leibler
(KL) divergence and its variants.
While most researchers extract fully-lexicalized
pairs of words or word sequences only, two algo-
rithms collect template-like knowledge using de-
pendency parsers. DIRT (Lin and Pantel, 2001)
collects pairs of paths in dependency parses that
connect two nominal entities. TEASE (Szpektor et
al., 2004) discovers dependency sub-parses from
theWeb, based on sets of representative entities for
a given lexical item. The output of these systems
contains the variable slots as shown in (4).
(4) a. X wrote Y ? X is the author of Y
b. X solves Y ? X deals with Y
(Lin and Pantel, 2001)
The knowledge in (4) falls between that in (1),
which is fully lexicalized, and that in (3), which
is almost fully abstracted. As a way of enrich-
ing such a template-like knowledge, Pantel et al
(2007) proposed the notion of inferential selec-
tional preference and collected expressions that
would fill those slots.
As mentioned in Section 1, the aim of the stud-
ies reviewed here is to collect paraphrase knowl-
edge. Thus, they need not to take the grammatical-
ity of expressions into account.
2.2 Generating paraphrase instances
Representing productive paraphrases with a set of
general patterns makes them maintainable and at-
tains a higher coverage of the paraphrase phe-
nomena. From the transformation grammar (Har-
226
ris, 1957), this approach has been adopted by
many researchers (Mel?c?uk and Polgue`re, 1987;
Jacquemin, 1999; Fujita et al, 2007). An impor-
tant issue arises when such a pattern is used to gen-
erate instances of paraphrases by replacing its vari-
ables with specific words. This involves assessing
the grammaticality of two expressions in addition
to their semantic equivalence and substitutability.
As a post-generation assessment of automati-
cally generated productive paraphrases, we have
applied distributional similarity measures (Fujita
and Sato, 2008). Our findings from a series of em-
pirical experiments are summarized as follows:
? Search engines are useful for retrieving the
contextual features of predicate phrases de-
spite some limitations (Kilgarriff, 2007).
? Distributional similarity measures produce a
tolerable level of performance.
The grammaticality of a phrase, however, is merely
assessed by issuing the phrase as a query to a com-
mercial search engine. Although a more frequent
expression is more grammatical, the length bias
should also be considered in the assessment.
Quirk et al (2004) built a paraphrase genera-
tion model from a monolingual comparable cor-
pus based on a statistical machine translation
framework, where the language model assesses
the grammaticality of the translations, i.e., gen-
erated expressions. The translation model, how-
ever, is not suitable for generating productive para-
phrases, because it learns word alignments at the
surface level. To cover all of the productive para-
phrases, we require an non-real comparable corpus
in which all instances of productive paraphrases
have a chance of being aligned. Furthermore, as
the translation model optimizes the word align-
ment at the sentence level, the substitutability of
the aligned word sequences cannot be explicitly
guaranteed.
2.3 Existing measures for paraphrases
To date, no model has been established that takes
into account all of the three aforementioned condi-
tions. With the ultimate aim of building an ideal
model, this section overviews the characteristics
and drawbacks of the four existing measures.
Lin?s measure
Lin (1998) proposed a symmetrical measure:
Par
Lin
(s ? t) =
?
f?F
s
?F
t
(w(s, f) + w(t, f))
?
f?F
s
w(s, f) +
?
f?F
t
w(t, f)
,
where F
s
and F
t
denote sets of features with posi-
tive weights for words s and t, respectively.
Although this measure has been widely cited
and has so far exhibited good performance, its
symmetry seems unnatural. Moreover, it may
not work well for dealing with general predicate
phrases because it is hard to enumerate all phrases
to determine the weights of features w(?, f). We
thus simply adopted the co-occurrence frequency
of the phrase and the feature as in (Fujita and Sato,
2008).
Skew divergence
The skew divergence, a variant of KL diver-
gence, was proposed in (Lee, 1999) based on an
insight: the substitutability of one word for another
need not be symmetrical. The divergence is given
by the following formula:
d
skew
(t, s) = D (P
s
??P
t
+ (1? ?)P
s
) ,
where P
s
and P
t
are the probability distributions
of features for the given original and substituted
words s and t, respectively. 0 ? ? ? 1 is a pa-
rameter for approximating KL divergence D. The
score can be recast into a similarity score via, for
example, the following function (Fujita and Sato,
2008):
Par
skew
(s?t) = exp (?d
skew
(t, s)) .
This measure offers an advantage: the weight
for each feature is determined theoretically. How-
ever, the optimization of ? is difficult because it
varies according to the task and even the data size
(confidence of probability distributions).
Translation-based conditional probability
Bannard and Callison-Burch (2005) proposed
a probabilistic model for acquiring phrasal para-
phrases1. The likelihood of t as a paraphrase of
the given phrase s is defined as follows:
P (t|s) =
?
f?tr(s)?tr(t)
P (t|f)P (f |s),
where tr (e) stands for a set of foreign language
phrases that are aligned with e in the given paral-
lel corpus. Parameters P (t|f) and P (f |s) are also
estimated using the given parallel corpus. A large-
scale parallel corpus may enable us to precisely ac-
quire a large amount of paraphrase knowledge. It
1In their definition, the term ?phrase? is a sequence of
words, while in this paper it designates the subtrees governed
by predicates (Fujita et al, 2007).
227
is not feasible, however, to build (or obtain) a par-
allel corpus in which all the instances of productive
paraphrases are translated to the same expression
in the other side of language.
3 Proposed probabilistic model
3.1 Formulation with conditional probability
Recall that our aim is to establish a measure that
computes the likelihood of a given pair of automat-
ically generated predicate phrases satisfying the
following three conditions:
Condition 1. Semantically equivalent
Condition 2. Substitutable in some context
Condition 3. Both expressions are grammatical
Based on the characteristics of the existing mea-
sures reviewed in Section 2.3, we propose a proba-
bilistic model. Let s and t be the source and target
predicate phrase, respectively. Assuming that s is
grammatical, the degree to which the above con-
ditions are satisfied is formalized as a conditional
probability P (t|s), as in (Bannard and Callison-
Burch, 2005). Then, assuming that s and t are
paradigmatic (i.e., paraphrases) and thus do not co-
occur, the proposed model is derived as follows:
P (t|s) =
?
f?F
P (t|f)P (f |s)
=
?
f?F
P (f |t)P (t)
P (f)
P (f |s)
= P (t)
?
f?F
P (f |t)P (f |s)
P (f)
,
where F denotes a set of features. The first
factor P (t) is called the grammaticality factor
because it quantifies the degree to which condi-
tion 3 is satisfied, except that we assume that
the given s is grammatical. The second factor
?
f?F
P (f |t)P (f |s)
P (f)
(Sim(s, t), hereafter), on the
other hand, is called the similarity factor because
it approximates the degree to which conditions 1
and 2 are satisfied by summing up the overlap of
the features of two expressions s and t.
The characteristics and advantages of the pro-
posed model are summarized as follows:
1) Asymmetric.
2) Grammaticality is assessed by P (t).
3) No heuristic is introduced. As the skew diver-
gence, the weight of the features can be simply
estimated as conditional probabilities P (f |t)
and P (f |s) and marginal probability P (f).
4) There is no need to enumerate all the phrases.
s and t are merely the given conditions.
The following subsections describe each factor.
3.2 Grammaticality factor
The factor P (t) quantifies how the phrase t is
grammatical using statistical language model.
Unlike English, in Japanese, predicates such as
verbs and adjectives do not necessarily determine
the order of their arguments, although they have
some preference. For example, both of the two
sentences in (5) are grammatical.
(5) a. kare-wa pasuta-o hashi-de taberu.
he-TOP pasta-ACC chopsticks-IMP to eat
He eats pasta with chopsticks.
b. kare-wa hashi-de pasuta-o taberu.
he-TOP chopsticks-IMP pasta-ACC to eat
He eats pasta with chopsticks.
This motivates us to use structured N -gram lan-
guage models (Habash, 2004). Given a phrase t,
its grammaticality P (t) is formulated as follows,
assuming a (N? 1)-th order Markov process for
generating its dependency structure T (t):
P (t) =
[
?
i=1...|T (t)|
P
d
(
c
i
|d
1
i
, d
2
i
, . . . , d
N?1
i
)
]
1/|T (t)|
,
where |T (t)| stands for the number of nodes in
T (t). To ignore the length bias of the target phrase,
a normalization factor 1/|T (t)| is introduced. dj
i
denotes the direct ancestor node of the i-th node
c
i
, where j is the distance from c
i
; for example, d1
i
and d2
i
are the parent and grandparent nodes of c
i
,
respectively.
Then, a concrete definition of the nodes in
the dependency structure is given. Widely-used
Japanese dependency parsers such as CaboCha2
and KNP3 consider a sequence of words as a node
called a ?bunsetsu? that consists of at least one
content word followed by a sequence of function
words if any. The hyphenated word sequences in
(6) exemplify those nodes.
(6) kitto kare-ha kyou-no
surely he-TOP today-GEN
kaigi-ni-ha ko-nai-daro-u.
meeting-DAT-TOP to come-NEG-must
He will surely not come to today?s meeting.
As bunsetsu can be quite long, involving more
than ten words, regarding it as a node makes
the model complex. Therefore, we compare the
2http://chasen.org/?taku/software/cabocha/
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
228
<EOS>
. = punc
.  
u = aux
 
da = aux
 
nai = aux
i  
N: noun
V: verb
Adv: adverb
AdvN: adverbial noun
Pro: pronoun
cp: case particle
tp: topic-marking particle
ap: adnominal particle
aux: auxiliary verb
punc: punctuation
kuru = V
wa = tp
t
ni = cp
i
kaigi = N
i i
no = ap
kyou = AdvN
kitto = Adv
itt
kare = Pro
r
Japanese base-chunk
(bunsetsu)
wa = tp
t
Figure 1: MDS of sentence (6).
following two versions of dependency structures
whose nodes are smaller than bunsetsu.
MDS: Morpheme-based dependency structure
(Takahashi et al, 2001) regards a morpheme
as a node. MDS of sentence (6) is shown in
Figure 1.
CFDS: The node of a content-function-based de-
pendency structure is either a sequence of
content words or of function words. CFDS
of sentence (6) is shown Figure 2.
Structured N -gram language models were cre-
ated from 15 years of Mainichi newspaper articles4
using a dependency parser Cabocha, with N being
varied from 1 to 3. Then, the 3-gram conditional
probability P
d
(c
i
|d
1
i
, d
2
i
) is given by the linear in-
terpolation of those three models as follows:
P
d
(c
i
|d
1
i
, d
2
i
) = ?
3
P
ML
(c
i
|d
1
i
, d
2
i
)
+?
2
P
ML
(c
i
|d
1
i
)
+?
1
P
ML
(c
i
),
s.t.
?
j
?
j
= 1,
where mixture weights ?
j
are selected via an EM
algorithm using development data5 that has not
been used for estimating P
ML
.
3.3 Similarity factor
The similarity factor Sim(s, t) quantifies how two
phrases s and t are similar by comparing two sets
of contextual features f ? F for s and t.
4Mainichi 1991-2005 (1.5GB, 21M sentences).
5Yomiuri 2005 (350MB, 4.7M sentences) and Asahi 2005
(180MB, 2.7M sentences).
<EOS>
nai-daro-u-. = F
i .  
C: Content part
F: Function part
kuru = C
wa = F ni-wa = F
i
kaigi = C
i i
no = F
kyou = C
kitto = C
itt
kare = C
Japanese base-chunk
(bunsetsu)
Figure 2: CFDS of sentence (6).
We employ the following two types of feature
sets, which we have examined in our previous
work (Fujita and Sato, 2008), where a feature f
consists of an expression e and a relation r:
BOW: A pair of phrases is likely to be seman-
tically similar, if the distributions of the
words surrounding the phrases is similar.
The relation set R
BOW
contains only ?co-
occur in the same sentence?.
MOD: A pair of phrases is likely to be substi-
tutable with each other, provided they share
a number of instances of modifiers and mod-
ifiees: the set of the relation R
MOD
consists
of two relations ?modifier? and ?modifiee?.
Conditional probability distributions P (f |s)
and P (f |t) are estimated using a Web search en-
gine as in (Fujita and Sato, 2008). Given a phrase
p, snippets of Web pages are firstly obtained via
Yahoo API6 by issuing p as a query. The max-
imum number of snippets is set to 1,000. Then,
the features of the phrase are retrieved from those
snippets using a morphological analyzer ChaSen7
and CaboCha. Finally, the conditional probability
distribution P (f |p) is estimated as follows:
P (f |p) = P (?r, e?|p)
=
freq
sni
(p, r, e)
?
r
?
?R
?
e
?
freq
sni
(p, r
?
, e
?
)
,
where freq
sni
(p, r, e) stands for the frequency of
the expression e appealing with the phrase p in re-
lation r within the snippets for p.
The weight for features P (f) is estimated using
a static corpus based on the following equation:
P (f) = P (?r, e?)
=
freq
cp
(r, e)
?
r
?
?R
?
e
?
freq
cp
(r
?
, e
?
)
,
6http://developer.yahoo.co.jp/search/
7http://chasen.naist.jp/hiki/ChaSen/
229
where freq
cp
(r, e) indicates the frequency of the
expression e appearing with something in relation
r within the given corpus. Two different sorts of
corpora are separately used to build two variations
of P (f). The one is Mainichi, which is used for
building structured N -gram language models in
Section 3.2, while the other is a huge corpus con-
sisting of 470M sentences collected from the Web
(Kawahara and Kurohashi, 2006).
4 Experiments
4.1 Data
We conducted an empirical experiment to evalu-
ate the proposed model using the test suite devel-
oped in (Fujita and Sato, 2008). The test suite con-
sists of 176,541 pairs of paraphrase candidates that
are automatically generated using a pattern-based
paraphrase generation system (Fujita et al, 2007)
for 4,002 relatively high-frequency phrases sam-
pled from a newspaper corpus8.
To evaluate the system from a generation view-
point, i.e., how well a system can rank a correct
candidate first, we extracted paraphrase candidates
for 200 randomly sampled source phrases from
the test suite. Table 1 shows the statistics of the
test data. The ?All-Yield? column shows that the
number of candidates for a source phrase varies
considerably, which implies that the data contains
cases that have various difficulties. While the av-
erage number of candidates for each source phrase
was 48.3 (the maximum was 186), it was dramati-
cally reduced through extracting features for each
source and candidate paraphrase from Web snip-
pets: to 5.2 with BOW and to 4.8 with MOD. This
suggests that a large number of spurious phrases
were generated but discarded by going to the Web,
and the task was significantly simplified.
4.2 Questions
Through this experiment, we evaluated several ver-
sions of the proposed model to answer the follow-
ing questions:
Q1. Is the proposed model superior to existing
measures in practice? Par
Lin
and Par
skew
are regarded as being the baseline.
Q2. Which language model performs better at es-
timating P (t)? MDS and CFDS are com-
pared.
Q3. Which corpus performs better at estimating
P (f)? The advantage of Kawahara?s huge
8The grammaticality of the source phrases are guaranteed.
Table 1: Statistics of test data (?Ph.?: # of phrases).
Source All BOW MOD
Phrase type Ph. Ph. Yield Ph. Yield Ph. Yield
N :C:V 18 57 3.2 54 3.0 54 3.0
N
1
:N
2
:C:V 57 4,596 80.6 594 10.4 551 9.7
N :C:V
1
:V
2
54 4,767 88.3 255 4.7 232 4.3
N :C:Adv:V 16 51 3.2 39 2.4 38 2.4
Adj:N :C:V 2 8 4.0 5 2.5 5 2.5
N :C:Adj 53 173 3.3 86 1.6 83 1.6
Total 200 9,652 48.3 1,033 5.2 963 4.8
corpus (WebCP) over Mainichi is evaluated.
Q4. Which set of features performs better? In ad-
dition to BOW and MOD, the harmonic mean
of the scores derived from BOW and MOD is
examined (referred to as HAR).
Q5. Can the quality of P (f |s) and P (f |t) be im-
proved by using a larger number of snippets?
As the maximum number of snippets (N
S
),
we compared 500 and 1,000.
4.3 Results
Two assessors were asked to judge paraphrase can-
didates that are ranked first by either of the above
models if each candidate satisfies each of the three
conditions. The results for all the above options
are summarized in Table 2, where the strict preci-
sion is calculated based on those cases that gain
two positive judgements, while the lenient preci-
sion is for at least one positive judgement.
A1: Our greatest concern is the actual perfor-
mance of our probabilistic model. However, no
variation of the proposed model could outperform
the existing models (Par
Lin
and Par
skew
) that
only assess similarity. Furthermore, McNemer?s
test with p < 0.05 revealed that the precisions of
all the models, except the combination of CFDS
for P (t) and Mainichi for P (f), were significantly
worse than those of the best models.
To clarify the cause of these disappointing re-
sults, we investigated the performance of each fac-
tor. Table 3 shows how well the grammaticality
factors select a grammatical phrase, while Table 4
illustrates how well the similarity factors rank a
correct paraphrase first. As shown in these tables,
neither factor performed the task well, although
combinations produced a slight improvement in
performance. A detailed discussion is given below
in A2 for the grammaticality factors, and in A3-A5
for the similarity factors.
A2: Comparisons between MDS and CFDS
revealed that CFDS always produced better re-
sults than MDS not only when used for measuring
grammaticality (Table 3), but also when used as a
230
Table 2: Precision for 200 test cases.
N
S
= 500 Strict Lenient
Model BOW MOD HAR BOW MOD HAR
Par
Lin
78 (39%) 88 (44%) 87 (44%) 116 (58%) 128 (64%) 127 (64%)
Par
skew
81 (41%) 88 (44%) 88 (44%) 120 (60%) 127 (64%) 128 (64%)
MDS, Mainichi 72 (36%) 73 (37%) 76 (38%) 109 (55%) 112 (56%) 114 (57%)
MDS, WebCP 71 (36%) 73 (37%) 72 (36%) 108 (54%) 110 (55%) 113 (57%)
CFDS, Mainichi 79 (40%) 78 (39%) 83 (42%) 120 (60%) 119 (60%) 123 (62%)
CFDS, WebCP 79 (40%) 77 (39%) 80 (40%) 118 (59%) 116 (58%) 118 (59%)
N
S
= 1,000 Strict Lenient
Model BOW MOD HAR BOW MOD HAR
Par
Lin
79 (40%) 88 (44%) 88 (44%) 116 (58%) 128 (64%) 129 (65%)
Par
skew
84 (42%) 89 (45%) 89 (45%) 121 (61%) 128 (64%) 128 (64%)
MDS, Mainichi 72 (36%) 75 (38%) 76 (38%) 109 (55%) 114 (57%) 114 (57%)
MDS, WebCP 71 (36%) 74 (37%) 72 (36%) 109 (55%) 111 (56%) 113 (57%)
CFDS, Mainichi 79 (40%) 82 (41%) 83 (42%) 121 (61%) 121 (61%) 122 (61%)
CFDS, WebCP 79 (40%) 78 (39%) 79 (40%) 119 (60%) 116 (58%) 119 (60%)
Table 3: Precision of measuring grammaticality.
Model Strict Lenient
MDS 104 (52%) 141 (71%)
CFDS 108 (54%) 142 (71%)
Table 4: Precision of similarity factors.
Strict Lenient
N
S
Corpus BOW MOD HAR BOW MOD HAR
500 Mainichi 60 (30%) 68 (34%) 74 (37%) 98 (49%) 109 (55%) 114 (57%)
500 WebCP 57 (28%) 61 (31%) 74 (37%) 94 (47%) 99 (50%) 120 (60%)
1,000 Mainichi 57 (28%) 70 (35%) 74 (37%) 92 (46%) 113 (57%) 116 (58%)
1,000 WebCP 57 (28%) 60 (30%) 72 (36%) 93 (47%) 96 (48%) 116 (58%)
component of the entire model (Table 2). This re-
sult is quite natural because MDS cannot verify the
collocation between content words in those cases
where a number of function words appear between
them. On the other hand, CFDS with N = 3 could
verify this as a result of treating the sequence of
function words as a single node.
As mentioned in A1, however, a more sophisti-
cated language model must enhance the proposed
model. One way of obtaining a suitable granularity
of nodes is to introduce latent classes, such as the
Semi-Markov class model (Okanohara and Tsujii,
2007). The existence of many orthographic vari-
ants of both the content and function words may
prevent us from accurately estimating the gram-
maticality. We plan to normalize these variations
by using several existing resources such as the
Japanese functional expression dictionary (Mat-
suyoshi, 2008).
A3: Contrary to our expectations, the huge Web
corpus did not offer any advantage over the news-
paper corpus: Mainichi always produced better re-
sults than WebCP when it was combined with the
grammaticality factor or when MOD was used.
We can speculate that morphological and depen-
dency parsers produce errors when features are ex-
tracted, because they are tuned to newspaper arti-
cles. Likewise, P (f |s) and P (f |t) may involve
noise even though they are estimated using rela-
tively clean parts of Web text that are retrieved by
querying phrase candidates.
A4: For Par
Lin
and Par
skew
, different sets of
features led to consistent results with our previous
experiments in (Fujita and Sato, 2008), i.e., BOW
< MOD ' HAR. On the other hand, for the pro-
posed models, MOD and HAR led to only small
or sometimes negative effects. When the similar-
ity factor was used alone, however, these features
beat BOW. Furthermore, the impact of combining
BOW and MOD into HAR was significant.
Given this tendency, it is expected that the gram-
maticality factor might be excessively emphasized.
Our probability model was derived straightfor-
wardly from the conditional probability P (t|s);
however, the combination of the two factors should
be tuned according to their implementation.
A5: Finally, the influence of the number of Web
snippets was analyzed; no significant difference
was observed.
This is because we could retrieve more than 500
snippets for only 172 pairs of expressions among
our test samples. As it is time-consuming to ob-
tain a large number of Web snippets, the trade-off
between the number of Web snippets and the per-
formance should be investigated further, although
the quality of the Web snippets and what appears
at the top of the search results will vary according
to several factors other than linguistic ones.
231
5 Conclusion
A pair of expressions qualifies as paraphrases iff
they are semantically equivalent, substitutable in
some context, and grammatical. In cases where
paraphrase knowledge is represented with abstract
patterns to attain a high coverage of the paraphrase
phenomena, we should assess not only the first and
second conditions, but also the third condition.
In this paper, we proposed a probabilistic model
for computing how two phrases are likely to be
paraphrases. The proposed model consists of two
components: (i) a structured N -gram language
model that ensures grammaticality and (ii) a distri-
butional similarity measure for estimating seman-
tic equivalence and substitutability between two
phrases. Through an experiment, we empirically
evaluated the performance of the proposed model
and analyzed the characteristics.
Future work includes building a more sophis-
ticated structured language model to improve the
performance of the proposed model and conduct-
ing an experiment on template-like paraphrase
knowledge for other than productive paraphrases.
References
Bannard, Colin and Chris Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of the
43rd Annual Meeting of the Association for Computational
Linguistics (ACL), pages 597?604.
Barzilay, Regina and Kathleen R. McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings of
the 39th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 50?57.
Barzilay, Regina and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Human
Language Technology Conference and the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 16?23.
Dolan, Bill, Chris Quirk, and Chris Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora: exploit-
ing massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics (COLING), pages 350?356.
Fujita, Atsushi, Shuhei Kato, Naoki Kato, and Satoshi Sato.
2007. A compositional approach toward dynamic phrasal
thesaurus. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing (WTEP), pages
151?158.
Fujita, Atsushi and Satoshi Sato. 2008. Computing para-
phrasability of syntactic variants using Web snippets. In
Proceedings of the 3rd International Joint Conference on
Natural Language Processing (IJCNLP), pages 537?544.
Geffet, Maayan and Ido Dagan. 2004. Feature vector qual-
ity and distributional similarity. In Proceedings of the
20th International Conference on Computational Linguis-
tics (COLING), pages 247?253.
Habash, Nizar. 2004. The use of a structural N-gram lan-
guage model in generation-heavy hybrid machine transla-
tion. In Proceedings of the 3rd International Natural Lan-
guage Generation Conference (INLG), pages 61?69.
Harris, Zellig. 1957. Co-occurrence and transformation in
linguistic structure. Language, 33(3):283?340.
Harris, Zellig. 1968. Mathematical structures of language.
John Wiley & Sons.
Jacquemin, Christian. 1999. Syntagmatic and paradigmatic
representations of term variation. In Proceedings of the
37th Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?348.
Kawahara, Daisuke and Sadao Kurohashi. 2006. Case frame
compilation from the Web using high-performance com-
puting. In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC).
Kilgarriff, Adam. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147?151.
Lee, Lillian. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 25?32.
Lin, Dekang. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and the
17th International Conference on Computational Linguis-
tics (COLING-ACL), pages 768?774.
Lin, Dekang and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Matsuyoshi, Suguru. 2008. Hierarchically organized dictio-
nary of Japanese functional expressions: design, compi-
lation and application. Ph.D. thesis, Graduate School of
Informatics, Kyoto University.
Mel?c?uk, Igor and Alain Polgue`re. 1987. A formal lexicon
in meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
Okanohara, Daisuke and Jun?ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative samples. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 73?80.
Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In Proceedings of Hu-
man Language Technologies 2007: The Conference of the
North American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), pages 564?571.
Quirk, Chris, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase genera-
tion. In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 142?149.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling Web-based acquisition of entail-
ment relations. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 41?48.
Takahashi, Tetsuro, Tomoya Iwakura, Ryu Iida, Atsushi Fu-
jita, and Kentaro Inui. 2001. KURA: a transfer-based
lexico-structural paraphrasing engine. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS) Workshop on Automatic Paraphrasing:
Theories and Applications, pages 37?46.
Weeds, Julie, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of the
ACL Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 7?12.
Yamamoto, Kazuhide. 2002. Acquisition of lexical para-
phrases from texts. In Proceedings of the 2nd Interna-
tional Workshop on Computational Terminology (Com-
puTerm), pages 22?28.
232
Computing Paraphrasability of Syntactic Variants Using Web Snippets
Atsushi Fujita Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
Abstract
In a broad range of natural language pro-
cessing tasks, large-scale knowledge-base of
paraphrases is anticipated to improve their
performance. The key issue in creating such
a resource is to establish a practical method
of computing semantic equivalence and syn-
tactic substitutability, i.e., paraphrasability,
between given pair of expressions. This
paper addresses the issues of computing
paraphrasability, focusing on syntactic vari-
ants of predicate phrases. Our model esti-
mates paraphrasability based on traditional
distributional similarity measures, where the
Web snippets are used to overcome the data
sparseness problem in handling predicate
phrases. Several feature sets are evaluated
through empirical experiments.
1 Introduction
One of the common characteristics of human lan-
guages is that the same concept can be expressed by
various linguistic expressions. Such linguistic vari-
ations are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of natu-
ral language processing (NLP) tasks. In informa-
tion retrieval, information extraction, and question
answering, technology of recognizing if or not the
given pair of expressions are paraphrases is desired
to gain a higher coverage. On the other hand, a sys-
tem which generates paraphrases for given expres-
sions is useful for text-transcoding tasks, such as
machine translation and summarization, as well as
beneficial to human, for instance, in text-to-speech,
text simplification, and writing assistance.
Paraphrase phenomena can roughly be divided
into two groups according to their compositionality.
Examples in (1) exhibit a degree of compositional-
ity, while each example in (2) is composed of totally
different lexical items.
(1) a. be in our favor ? be favorable for us
b. show a sharp decrease ? decrease sharply
(Fujita et al, 2007)
(2) a. burst into tears ? cried
b. comfort ? console
(Barzilay and McKeown, 2001)
A number of studies have been carried out on
both compositional (morpho-syntactic) and non-
compositional (lexical and idiomatic) paraphrases
(see Section 2). In most research, paraphrases have
been represented with the similar templates, such as
shown in (3) and (4).
(3) a. N
1
V N
2
? N
1
?s V -ing of N
2
b. N
1
V N
2
? N
2
be V -en by N
1
(Harris, 1957)
(4) a. X wrote Y ? X is the author of Y
b. X solves Y ? X deals with Y
(Lin and Pantel, 2001)
The weakness of these templates is that they
should be applied only in some contexts. In other
words, the lack of applicability conditions for slot
fillers may lead incorrect paraphrases. One way
to specify the applicability condition is to enumer-
ate correct slot fillers. For example, Pantel et al
(2007) have harvested instances for the given para-
phrase templates based on the co-occurrence statis-
tics of slot fillers and lexicalized part of templates
(e.g. ?deal with? in (4b)). Yet, there is no method
which assesses semantic equivalence and syntactic
substitutability of resultant pairs of expressions.
537
In this paper, we propose a method of directly
computing semantic equivalence and syntactic sub-
stitutability, i.e., paraphrasability, particularly focus-
ing on automatically generated compositional para-
phrases (henceforth, syntactic variants) of predicate
phrases. While previous studies have mainly tar-
geted at words or canned phrases, we treat predicate
phrases having a bit more complex structures.
This paper addresses two issues in handling
phrases. The first is feature engineering. Gener-
ally speaking, phrases appear less frequently than
single words. This implies that we can obtain only a
small amount of information about phrases. To over-
come the data sparseness problem, we investigate if
the Web snippet can be used as a dense corpus for
given phrases. The second is the measurement of
paraphrasability. We assess how well the traditional
distributional similarity measures approximate the
paraphrasability of predicate phrases.
2 Related work
2.1 Representation of paraphrases
Several types of compositional paraphrases, such
as passivization and nominalization, have been rep-
resented with some grammar formalisms, such as
transformational generative grammar (Harris, 1957)
and synchronous tree adjoining grammar (Dras,
1999). These grammars, however, lack the informa-
tion of applicability conditions.
Word association within phrases has been an at-
tractive topic. Meaning-Text Theory (MTT) is a
framework which takes into account several types
of lexical dependencies in handling paraphrases
(Mel?c?uk and Polgue`re, 1987). A bottleneck of
MTT is that a huge amount of lexical knowledge is
required to represent various relationships between
lexical items. Jacquemin (1999) has represented the
syntagmatic and paradigmatic correspondences be-
tween paraphrases with context-free transformation
rules and morphological and/or semantic relations
between lexical items, targeting at syntactic variants
of technical terms that are typically noun phrases
consisting of more than one word. We have pro-
posed a framework of generating syntactic variants
of predicate phrases (Fujita et al, 2007). Following
the previous work, we have been developing three
sorts of resources for Japanese.
2.2 Acquiring paraphrase rules
Since the late 1990?s, the task of automatic acqui-
sition of paraphrase rules has drawn the attention of
an increasing number of researchers. Although most
of the proposed methods do not explicitly eliminate
compositional paraphrases, their output tends to be
non-compositional paraphrase.
Previous approaches to this task are two-fold. The
first group espouses the distributional hypothesis
(Harris, 1968). Among a number of models based
on this hypothesis, two algorithms are referred to
as the state-of-the-art. DIRT (Lin and Pantel, 2001)
collects paraphrase rules consisting of a pair of paths
between two nominal slots based on point-wise mu-
tual information. TEASE (Szpektor et al, 2004) dis-
covers binary relation templates from the Web based
on sets of representative entities for given binary re-
lation templates. These systems often output direc-
tional rules such as exemplified in (5).
(5) a. X is charged by Y
? Y announced the arrest of X
b. X prevent Y ? X lower the risk of Y
They are actually called inference/entailment rules,
and paraphrase is defined as bidirectional infer-
ence/entailment relation1. While the similarity score
in DIRT is symmetric for given pair of paths, the al-
gorithm of TEASE considers the direction.
The other utilizes a sort of parallel texts, such as
multiple translation of the same text (Barzilay and
McKeown, 2001; Pang et al, 2003), corresponding
articles from multiple news sources (Barzilay and
Lee, 2003; Dolan et al, 2004), and bilingual corpus
(Wu and Zhou, 2003; Bannard and Callison-Burch,
2005). This approach is, however, limited by the dif-
ficulty of obtaining parallel/comparable corpora.
2.3 Acquiring paraphrase instances
As reviewed in Section 1, paraphrase rules gener-
ate incorrect paraphrases, because their applicability
conditions are not specified. To avoid the drawback,
several linguistic clues, such as fine-grained classifi-
cation of named entities and coordinated sentences,
have been utilized (Sekine, 2005; Torisawa, 2006).
Although these clues restrict phenomena to those
appearing in particular domain or those describing
coordinated events, they have enabled us to collect
1See http://nlp.cs.nyu.edu/WTEP/
538
paraphrases accurately. The notion of Inferential Se-
lectional Preference (ISP) has been introduced by
Pantel et al (2007). ISP can capture more general
phenomena than above two; however, it lacks abili-
ties to distinguish antonym relations.
2.4 Computing semantic equivalence
Semantic equivalence between given pair of expres-
sions has so far been estimated under the distribu-
tional hypothesis (Harris, 1968). Geffet and Dagan
(2005) have extended it to the distributional inclu-
sion hypothesis for recognizing the direction of lex-
ical entailment. Weeds et al (2005), on the other
hand, have pointed out the limitations of lexical sim-
ilarity and syntactic transformation, and have pro-
posed to directly compute the distributional similar-
ity of pair of sub-parses based on the distributions
of their modifiers and parents. We think it is worth
examining if the Web can be used as the source for
extracting features of phrases.
3 Computing paraphrasability between
predicate phrases using Web snippets
We define the concept of paraphrasability as follows:
A grammatical phrase s is paraphrasable
with another phrase t, iff t satisfies the fol-
lowing three:
? t is grammatical
? t holds if s holds
? t is substitutable for s in some context
Most previous studies on acquiring paraphrase rules
have evaluated resultant pairs from only the second
viewpoint, i.e., semantic equivalence. Additionally,
we assume that one of a pair (t) of syntactic vari-
ants is automatically generated from the other (s).
Thus, grammaticality of t should also be assessed.
We also take into account the syntactic substitutabil-
ity, because head-words of syntactic variants some-
times have different syntactic categories.
Given a pair of predicate phrases, we compute
their paraphrasability in the following procedure:
Step 1. Retrieve Web snippets for each phrase.
Step 2. Extract features for each phrase.
Step 3. Compute their paraphrasability as distribu-
tional similarity between their features.
The rest of this section elaborates on each step in
turn, taking Japanese as the target language.
3.1 Retrieving Web snippets
In general, phrases appear less frequently than sin-
gle words. This raises a crucial problem in com-
puting paraphrasability of phrases, i.e., the sparse-
ness of features for given phrases. One possible way
to overcome the problem is to take back-off statis-
tics assuming the independence between constituent
words (Torisawa, 2006; Pantel et al, 2007). This ap-
proach, however, has a risk of involving noises due
to ambiguity of words.
We take another approach, which utilizes the Web
as a source of examples instead of a limited size of
corpus. For each of the source and target phrases, we
retrieve snippets via the Yahoo API2. The number of
snippets is set to 500.
3.2 Extracting features
The second step extracts the features for each phrase
from Web snippets. We have some options for fea-
ture set, feature weighting, and snippet collection.
Feature sets
To assess a given pair of phrases against the defi-
nition of paraphrasability, the following three sets of
features are examined.
HITS: A phrase must appear in the Web if it is
grammatical. The more frequently a phrase ap-
pears, the more likely it is grammatical.
BOW: A pair of phrases are likely to be semanti-
cally similar, if the distributions of words sur-
rounding the phrases are similar.
MOD: A pair of phrases are likely to be substi-
tutable with each other, if they share a number
of instances of modifiers and modifiees.
To extract BOW features from sentences includ-
ing the given phrase within Web snippets, a morpho-
logical analyzer MeCab3 was firstly used; however,
it resulted wrong POS tags for unknown words, and
hurt statistics. Thus, finally ChaSen4 is used.
To collect MOD features, a dependency parser
CaboCha5 is used. Figure 1 depicts an example
of extracting MOD features from a sentence within
Web snippet. A feature is generated from a bun-
setsu, the Japanese base-chunk, which is either mod-
2http://developer.yahoo.co.jp/search/
3http://mecab.sourceforge.net/
4http://chasen.naist.jp/hiki/ChaSen/
5http://chasen.org/?taku/software/cabocha/
539
kuwashiku
i
jikken-kekka-no
ji - -
saigen-sei-o
i - i-
kenshou-suru
- r
yotei-da
t i-
kare-no
r -
Features
Sentence within snippet
(dependency tree)
Modifiee/D: yotei
ifi / : t i
Modifier/D: kuwashii
ifi r/ : ii
Modifier/D: kare_no
ifi r/ : r
(plan)
(in detail)
(his)
Given phrase
(I am) planning to verify the reproducibility of his experimental result in detail.
Figure 1: An example of MOD feature extraction.
An oval in the dependency tree denotes a bunsetsu.
ifier or modifiee of the given phrase. Each feature
is composed of three or more elements: (i) modi-
fier or modifiee, (ii) dependency relation types (di-
rect dependency, appositive, or parallel, c.f., RASP
and MINIPAR), (iii) base form of the head-word,
and (iv) case marker following noun, auxiliary verb
and verbal suffixes if they appear. The last feature
is employed to distinguish the subtle difference of
meaning of predicate phrases, such as voice, tense,
aspect, and modality. While Lin and Pantel (2001)
have calculated similarities of paths based on slot
fillers of subject and object slots, MOD targets at
sub-trees and utilizes any modifiers and modifiees.
Feature weighting
Geffet and Dagan (2004) have reported on that the
better quality of feature vector (weighting function)
leads better results. So far, several weighting func-
tions have been proposed, such as point-wise mu-
tual information (Lin and Pantel, 2001) and Rela-
tive Feature Focus (Geffet and Dagan, 2004). While
these functions compute weights using a small cor-
pus for merely re-ranking samples, we are devel-
oping a measure that assesses the paraphrasability
of arbitrary pair of phrases, where a more robust
weighting function is necessary. Therefore we di-
rectly use frequencies of features within Web snip-
pets as weight. Normalization will be done when the
paraphrasability is computed (Section 3.3).
Source-focused feature extraction
Independent collection of Web snippets for each
phrase of a given pair might yield no intersection of
feature sets even if they have the same meaning. To
obtain more reliable feature sets, we retrieve Web
snippets by querying the phrase AND the anchor of
the source phrase. The ?anchored version? of Web
snippets is retrieved in the following steps:
Step 2-1. Determine the anchor using Web snip-
pets for the given source phrase. We regarded
a noun which most frequently modifies the
source phrase as its anchor. Examples of source
phrases and their anchors are shown in (6).
Step 2-2. Retrieve Web snippets by querying the
anchor for the source phrase AND each of
source and target phrases, respectively.
Step 2-3. Extract features for HITS, BOW, MOD.
Those sets are referred to as Anc.?, while the
normal versions are referred to as Nor.?.
(6) a. ?emi:o:ukaberu? ? ? ? ?manmen?
(be smiling ? ? ? from ear to ear)
b. ?doriburu:de:kake:agaru? ? ? ? ?saido?
(overlap by dribbling ? ? ? side)
c. ?yoi:sutaato:o:kiru? ? ? ? ?saisaki?
(make a good start ? ? ? good sign)
3.3 Computing paraphrasability
Paraphrasability is finally computed by two conven-
tional distributional similarity measures. The first is
the measure proposed in (Lin and Pantel, 2001):
Par
Lin
(s?t) =
?
f?F
s
?F
t
(w(s, f) + w(t, f))
?
f?F
s
w(s, f) +
?
f?F
t
w(t, f)
,
where Fs and Ft denote feature sets for s and t, re-
spectively. w(x, f) stands for the weight (frequency
in our experiment) of f in Fx.
While Par
Lin
is symmetric, it has been argued
that it is important to determine the direction of para-
phrase. As an asymmetric measure, we examine ?-
skew divergence defined by the following equation
(Lee, 1999):
dskew(t, s) = D (Ps??Pt + (1 ? ?)Ps) ,
where Px denotes a probability distribution esti-
mated6 from a feature set Fx. How well Pt approx-
imates Ps is calculated based on the KL divergence,
D. The parameter ? is set to 0.99, following tradi-
tion, because the optimization of ? is difficult. To
take consistent measurements, we define the para-
phrasability score Par
skew
as follows:
Par
skew
(s?t) = exp (?dskew(t, s)) .
6We estimate them simply using maximum likelihood esti-
mation, i.e., P
x
(f) = w(x, f)/
P
f
?
?Fx
w(x, f ?).
540
Table 1: # of sampled source phrases and automatically generated syntactic variants.
Phrase type # of tokens # of types th types Cov.(%) Output Ave.
N : C : V 20,200,041 4,323,756 1,000 1,014 10.7 1,536 (489) 3.1
N
1
: N
2
: C : V 3,796,351 2,013,682 107 1,005 6.3 88,040 (966) 91.1
N : C : V
1
: V
2
325,964 213,923 15 1,022 12.9 75,344 (982) 76.7
N : C : Adv : V 1,209,265 923,475 21 1,097 3.9 8,281 (523) 15.7
Adj : N : C : V 378,617 233,952 20 1,049 14.1 128 (50) 2.6
N : C : Adj 788,038 203,845 86 1,003 31.4 3,212 (992) 3.2
Total 26,698,276 7,912,633 6,190 176,541 (4,002) 44.1
Table 2: # of syntactic variants whose paraphrasability scores are computed.
Nor.HITS ? Nor.BOW.? ? Nor.MOD.?. Anc.HITS ? Anc.BOW.? ? Anc.MOD.?.
Nor.HITS ? Anc.HITS. Nor.BOW.? ? Anc.BOW.?. Nor.MOD.? ? Anc.MOD.?. X denotes the set of syntactic variants whose scores are computed based on X.
Nor.HITS Nor.BOW.? Nor.MOD.? Anc.HITS Anc.BOW.? Anc.MOD.? Mainichi
Phrase type Output Ave. Output Ave. Output Ave. Output Ave. Output Ave. Output Ave. Output Ave.
N : C : V 1,405 (489) 2.9 1,402 (488) 2.9 1,396 (488) 2.9 1,368 (488) 2.8 1,366 (487) 2.8 1,360 (487) 2.8 1,103 (457) 2.4
N
1
: N
2
: C : V 9,544 (964) 9.9 9,249 (922)10.0 8,652 (921) 9.4 7,437 (897) 8.3 7,424 (894) 8.3 6,795 (891) 7.6 3,041 (948) 3.2
N : C : V
1
: V
2
3,769 (876) 4.3 3,406 (774) 4.4 3,109 (762) 4.1 2,517 (697) 3.6 2,497 (690) 3.6 2,258 (679) 3.3 1,156 (548) 2.1
N : C : Adv : V 690 (359) 1.9 506 (247) 2.0 475 (233) 2.0 342 (174) 2.0 339 (173) 2.0 322 (168) 1.9 215 (167) 1.3
Adj : N : C : V 45 (20) 2.3 45 (20) 2.3 42 (17) 2.5 41 (18) 2.3 41 (18) 2.3 39 (16) 2.4 14 (7) 2.0
N : C : Adj 1,459 (885) 1.6 1,459 (885) 1.6 1,399 (864) 1.6 1,235 (809) 1.5 1,235 (809) 1.5 1,161 (779) 1.5 559 (459) 1.2
Total 16,912 (3,593) 4.7 16,067 (3,336) 4.8 15,073 (3,285) 4.6 12,940 (3,083) 4.2 12,902 (3,071) 4.2 11,935 (3,020) 4.0 6,088 (2,586) 2.4
Now Par
x
falls within [0, 1], and a larger Par
x
indi-
cates a more paraphrasable pair of phrases.
4 Experimental setting
We conduct empirical experiments to evaluate the
proposed methods. Settings are described below.
4.1 Test collection
First, source phrases were sampled from a 15 years
of newspaper articles (Mainichi 1991-2005, approx-
imately 1.5GB). Referring to the dependency struc-
ture given by CaboCha, we extracted most fre-
quent 1,000+ phrases for each of 6 phrase types.
These phrases were then fed to a system proposed
in (Fujita et al, 2007) to generate syntactic vari-
ants. The numbers of the source phrases and
their syntactic variants are summarized in Table 1,
where the numbers in the parentheses indicate that
of source phrases paraphrased. At least one can-
didate was generated for 4,002 (64.7%) phrases.
Although the system generates numerous syntactic
variants from a given phrase, most of them are er-
roneous. For example, among 159 syntactic vari-
ants that are automatically generated for the phrase
?songai:baishou:o:motomeru? (demand compensa-
tion for damages), only 8 phrases are grammatical,
and only 5 out of 8 are correct paraphrases.
Paraphrasability of each pair of source phrase and
candidate is then computed by the methods pro-
posed in Section 3. Table 2 summarizes the num-
bers of pairs whose features can be extracted from
the Web snippets. While more than 90% of candi-
dates were discarded due to ?No hits? in the Web,
at least one candidate survived for 3,020 (48.8%)
phrases. Mainichi is a baseline which counts HITS
in the corpus used for sampling source phrases.
4.2 Samples for evaluation
We sampled three sets of pairs for evaluation, where
Mainichi, ?.HITS, ?.BOW, ?.MOD, the harmonic
mean of the scores derived from ?.BOW and ?.MOD
(referred to as ?.HAR), and two distributional simi-
larity measures for ?.BOW, ?.MOD, and ?.HAR, in
total 15 models, are compared.
Ev.Gen: This investigates how well a correct can-
didate is ranked first among candidates for a
given phrase using the top-ranked pairs for ran-
domly sampled 200 source phrases for each of
15 models.
Ev.Rec: This assesses how well a method gives
higher scores to correct candidates using the
200-best pairs for each of 15 models.
Ev.Ling: This compares paraphrasability of each
phrase type using the 20-best pairs for each of
6 phrase type and 14 Web-based models.
4.3 Criteria of paraphrasability
To assess by human the paraphrasability discussed
in Section 3, we designed the following four ques-
tions based on (Szpektor et al, 2007):
Qsc: Is s a correct phrase in Japanese?
Qtc: Is t a correct phrase in Japanese?
Qs2t: Does t hold if s holds and can t substituted for
s in some context?
Qt2s: Does s hold if t holds and can s substituted
for t in some context?
541
5 Experimental results
5.1 Agreement of human judge
Two human assessors separately judged all of the
1,152 syntactic variant pairs (for 962 source phrases)
within the union of the three sample sets. They
agreed on all four questions for 795 (68.4%) pairs.
For the 963 (83.6%) pairs that passed Qsc and Qtc
in both two judges, we obtained reasonable agree-
ment ratios 86.9% and 85.0% and substantial Kappa
values 0.697 and 0.655 for assessing Qs2t and Qt2s.
5.2 Ev.Gen
Table 3 shows the results for Ev.Gen, where the
strict precision is calculated based on the number
of two positive judges for Qs2t, while the lenient
precision is for at least one positive judge for the
same question. ?.MOD and ?.HAR outperformed
the other models, although there was no statistically
significant difference7. Significant differences be-
tween Mainichi and the other models in lenient pre-
cisions indicate that the Web enables us to compute
paraphrasability more accurately than a limited size
of corpus.
From a closer look at the distributions of para-
phrasability scores of ?.BOW and ?.MOD shown in
Table 4, we find that if a top-ranked candidate for
a given phrase is assigned enough high score, it is
very likely to be correct. The scores of Anc.? are
distributed in a wider range than those of Nor.?, pre-
serving precision. This allows us to easily skim the
most reliable portion by setting a threshold.
5.3 Ev.Rec
The results for Ev.Rec, as summarized in Table 5,
show the significant differences of performances be-
tween Mainichi or ?.HITS and the other models.
The results of ?.HITS supported the importance of
comparing features of phrases. On the other hand,
?.BOW performed as well as ?.MOD and ?.HAR.
This sounds nice because BOW features can be ex-
tracted extremely quickly and accurately.
Unfortunately, Anc.? led only a small impact on
strict precisions. We speculate that the selection of
the anchor is inadequate. Another possible interpre-
tation is that source phrases are rarely ambiguous,
because they contain at least two content words. In
7p < 0.05 in 2-sample test for equality of proportions.
Table 3: Precision for 200 candidates (Ev.Gen).
Strict Lenient
Model Nor.? Anc.? Nor.? Anc.?
Mainichi 77 (39%) - - 101 (51%) - -
HITS 84 (42%) 83 (42%) 120 (60%) 119 (60%)
BOW.Lin 82 (41%) 85 (43%) 123 (62%) 124 (62%)
BOW.skew 86 (43%) 87 (44%) 125 (63%) 124 (62%)
MOD.Lin 91 (46%) 91 (46%) 130 (65%) 131 (66%)
MOD.skew 92 (46%) 90 (45%) 132 (66%) 130 (65%)
HAR.Lin 90 (45%) 90 (45%) 129 (65%) 130 (65%)
HAR.skew 93 (47%) 90 (45%) 134 (67%) 131 (66%)
Table 4: Distribution of paraphrasability scores and
lenient precision (Ev.Gen).
Nor.BOW Anc.BOW
Par(s?t) Lin skew Lin skew
0.9-1.0 11/ 12 (92%) 0/ 0 - 17/ 18 (94%) 2/ 2 (100%)
0.8-1.0 45/ 49 (92%) 1/ 1 (100%) 45/ 50 (90%) 6/ 6 (100%)
0.7-1.0 72/ 88 (82%) 7/ 7 (100%) 73/ 92 (79%) 10/ 11 (91%)
0.6-1.0 94/127 (74%) 11/ 11 (100%) 83/113 (74%) 12/ 13 (92%)
0.5-1.0 102/145 (70%) 13/ 13 (100%) 96/128 (75%) 14/ 15 (93%)
0.4-1.0 107/158 (68%) 13/ 14 (93%) 103/145 (71%) 21/ 22 (96%)
0.3-1.0 113/173 (65%) 25/ 26 (96%) 114/166 (69%) 31/ 32 (97%)
0.2-1.0 119/184 (65%) 40/ 41 (98%) 121/186 (65%) 49/ 50 (98%)
0.1-1.0 123/198 (62%) 74/ 86 (86%) 124/200 (62%) 82/ 99 (83%)
0.0-1.0 123/200 (62%) 125/200 (63%) 124/200 (62%) 124/200 (62%)
Variance 0.052 0.031 0.061 0.044
Nor.MOD Anc.MOD
Par(s?t) Lin skew Lin skew
0.9-1.0 2/ 2 (100%) 0/ 0 - 7/ 7 (100%) 1/ 1 (100%)
0.8-1.0 10/ 10 (100%) 0/ 0 - 12/ 13 (92%) 2/ 2 (100%)
0.7-1.0 13/ 14 (93%) 0/ 0 - 17/ 18 (94%) 6/ 6 (100%)
0.6-1.0 20/ 21 (95%) 1/ 1 (100%) 27/ 28 (96%) 9/ 9 (100%)
0.5-1.0 31/ 32 (97%) 6/ 6 (100%) 36/ 37 (97%) 10/ 10 (100%)
0.4-1.0 42/ 44 (96%) 11/ 11 (100%) 51/ 53 (96%) 12/ 12 (100%)
0.3-1.0 61/ 68 (90%) 12/ 12 (100%) 61/ 68 (90%) 13/ 14 (93%)
0.2-1.0 81/ 92 (88%) 13/ 13 (100%) 82/ 94 (87%) 18/ 19 (95%)
0.1-1.0 105/133 (79%) 17/ 18 (94%) 104/126 (83%) 24/ 25 (96%)
0.0-1.0 130/200 (65%) 132/200 (66%) 131/200 (66%) 130/200 (65%)
Variance 0.057 0.014 0.072 0.030
paraphrase generation, capturing the correct bound-
ary of phrases is rather vital, because the source
phrase is usually assumed to be grammatical. Qsc
for 55 syntactic variants (for 44 source phrases) were
actually judged incorrect.
The lenient precisions, which were reaching a
ceiling, implied the limitation of the proposed meth-
ods. Most common errors among the proposed
methods were generated by a transformation pattern
N
1
: N
2
: C : V ? N
2
: C : V . Typically,
dropping a nominal element N
1
of the given nominal
compound N
1
: N
2
generalizes the meaning that the
compound conveys, and thus results correct para-
phrases. However, it caused errors in some cases;
for example, since N
1
was the semantic head in (7),
dropping it was incorrect.
(7) s. ?shukketsu:taryou:de:shibou-suru?
(die due to heavy blood loss)
t.??taryou:de:shibou-suru? (die due to plenty)
542
Table 5: Precision for 200 candidates (Ev.Rec).
Strict Lenient
Model Nor.? Anc.? Nor.? Anc.?
Mainichi 78 (39%) - - 111 (56%) - -
HITS 71 (36%) 93 (47%) 113 (57%) 128 (64%)
BOW.Lin 159 (80%) 162 (81%) 193 (97%) 191 (96%)
BOW.skew 154 (77%) 158 (79%) 192 (96%) 191 (96%)
MOD.Lin 158 (79%) 164 (82%) 192 (96%) 193 (97%)
MOD.skew 156 (78%) 161 (81%) 191 (96%) 191 (96%)
HAR.Lin 157 (79%) 164 (82%) 192 (96%) 194 (97%)
HAR.skew 155 (78%) 160 (80%) 191 (96%) 191 (96%)
5.4 Ev.Ling
Finally the results for Ev.Ling is shown in Table 6.
Paraphrasability of syntactic variants for phrases
containing an adjective was poorly computed. The
primal source of errors for Adj : N : C : V type
phrases was the subtle change of nuance by switch-
ing syntactic heads as illustrated in (8), where un-
derlines indicate heads.
(8) s. ?yoi:shigoto:o:suru? (do a good job)
t
1
.=?yoku:shigoto-suru? (work hard)
t
2
.=?shigoto:o:yoku:suru? (improve the work)
Most errors in paraphrasing N : C : Adj type
phrases, on the other hand, were caused due to the
difference of aspectual property and agentivity be-
tween adjectives and verbs. For example, (9s) can
describe not only things those qualities have been
improved as inferred by (9t), but also those origi-
nally having a high quality. Qs2t for (9) was thus
judged incorrect.
(9) s. ?shitsu:ga:takai? (having high quality)
t.=?shitsu:ga:takamaru? (quality rises)
Precisions of syntactic variants for the other types
of phrases were higher, but they tended to include
trivial paraphrases such as shown in (10) and (11).
Yet, collecting paraphrase instances statically will
contribute to paraphrase recognition tasks.
(10) s. ?shounin:o:eru? (clear)
t. ?shounin-sa-re-ru? (be approved)
(11) s. ?eiga:o:mi:owaru? (finish seeing the movie)
t. ?eiga:ga:owaru? (the movie ends)
6 Discussion
As described in the previous sections, our quite
naive methods have shown fairly good performances
in this first trial. This section describes some re-
maining issues to be discussed further.
The aim of this study is to create a thesaurus
of phrases to recognize and generate phrases that
Table 6: Precision for each phrase type (Ev.Ling).
Phrase type Strict Lenient
N : C : V 52/ 98 (53%) 69/ 98 (70%)
N
1
: N
2
: C : V 51/ 72 (71%) 64/ 72 (89%)
N : C : V
1
: V
2
42/ 86 (49%) 60/ 86 (70%)
N : C : Adv : V 33/ 61 (54%) 44/ 61 (72%)
Adj : N : C : V 0/ 25 (0%) 4/ 25 (16%)
N : C : Adj 18/ 73 (25%) 38/ 73 (52%)
Total 196/415 (47%) 279/415 (67%)
Table 7: # of features.
Nor.BOW Nor.MOD Anc.BOW Anc.MOD
# of features (type) 73,848 471,720 72,109 409,379
average features (type) 1,322 211 1,277 202
average features (token) 4,883 391 4,728 383
are semantically equivalent and syntactically substi-
tutable, following the spirit described in (Fujita et
al., 2007). Through the comparisons of Nor.? and
Anc.?, we have shown a little evidence that the am-
biguity of phrases was not problematic at least for
handling syntactic variants, arguing the necessity of
detecting the appropriate phrase boundaries.
To overcome the data sparseness problem, Web
snippets are harnessed. Features extracted from the
snippets outperformed newspaper corpus; however,
the small numbers of features for phrases shown in
Table 7 and the lack of sophisticated weighting func-
tion suggest that the problem might persist. To ex-
amine the proposed features and measures further,
we plan to use TSUBAKI8, an indexed Web corpus
developed for NLP research, because it allow us to
obtain snippets as much as it archives.
The use of larger number of snippets increases
the computation time for assessing paraphrasability.
For reducing it as well as gaining a higher cover-
age, the enhancement of the paraphrase generation
system is necessary. A look at the syntactic variants
automatically generated by a system, which we pro-
posed, showed that the system could generate syn-
tactic variants for only a half portion of the input,
producing many erroneous ones (Section 4.1). To
prune a multitude of incorrect candidates, statisti-
cal language models such as proposed in (Habash,
2004) will be incorporated. In parallel, we plan to
develop a paraphrase generation system which lets
us to quit from the labor of maintaining patterns such
as shown in (4). We think a more unrestricted gener-
ation algorithm will gain a higher coverage, preserv-
ing the meaning as far as handling syntactic variants
of predicate phrases.
8http://tsubaki.ixnlp.nii.ac.jp/se/index.cgi
543
7 Conclusion
In this paper, we proposed a method of assessing
paraphrasability between automatically generated
syntactic variants of predicate phrases. Web snip-
pets were utilized to overcome the data sparseness
problem, and the conventional distributional similar-
ity measures were employed to quantify the similar-
ity of feature sets for the given pair of phrases. Em-
pirical experiments revealed that features extracted
from the Web snippets contribute to the task, show-
ing promising results, while no significant difference
was observed between two measures.
In future, we plan to address several issues such as
those described in Section 6. Particularly, at present,
the coverage and portability are of our interests.
Acknowledgments
We are deeply grateful to all anonymous reviewers
for their valuable comments. This work was sup-
ported in part by MEXT Grant-in-Aid for Young
Scientists (B) 18700143, and for Scientific Research
(A) 16200009, Japan.
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 597?604.
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proceedings of the
39th Annual Meeting of the Association for Computational
Linguistics (ACL), pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16?23.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora: exploit-
ing massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguistics
(COLING), pages 350?356.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi Sato.
2007. A compositional approach toward dynamic phrasal
thesaurus. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing (WTEP), pages 151?
158.
Maayan Geffet and Ido Dagan. 2004. Feature vector quality
and distributional similarity. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics (COL-
ING), pages 247?253.
Maayan Geffet and Ido Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Proceedings
of the 43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 107?116.
Nizar Habash. 2004. The use of a structural N-gram language
model in generation-heavy hybrid machine translation. In
Proceedings of the 3rd International Natural Language Gen-
eration Conference (INLG), pages 61?69.
Zellig Harris. 1957. Co-occurrence and transformation in lin-
guistic structure. Language, 33(3):283?340.
Zellig Harris. 1968. Mathematical structures of language.
John Wiley & Sons.
Christian Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentations of term variation. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 341?348.
Lillian Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 25?32.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Igor Mel?c?uk and Alain Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words). Com-
putational Linguistics, 13(3-4):261?275.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-
based alignment of multiple translations: extracting para-
phrases and generating new sentences. In Proceedings of
the 2003 Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 102?109.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. ISP: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564?571.
Satoshi Sekine. 2005. Automatic paraphrase discovery based
on context and keywords between NE pairs. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 80?87.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Cop-
pola. 2004. Scaling Web-based acquisition of entailment
relations. In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP),
pages 41?48.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-
based evaluation of entailment rule acquisition. In Proceed-
ings of the 45th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 456?463.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57?64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7?12.
Hua Wu and Ming Zhou. 2003. Synonymous collocation ex-
traction using translation information. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics (ACL), pages 120?127.
544
Proceedings of the Fourth International Natural Language Generation Conference, pages 41?43,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Adjective-to-Verb Paraphrasing in Japanese
Based on Lexical Constraints of Verbs
Atsushi Fujita? Naruaki Masuno?? Satoshi Sato? Takehito Utsuro?
?Graduate School of Engineering, Nagoya University
??IBM Engineering & Technology Services, IBM Japan, Ltd.
?Graduate School of Systems and Information Engineering, University of Tsukuba
Abstract
This paper describes adjective-to-verb
paraphrasing in Japanese. In this para-
phrasing, generated verbs require addi-
tional suffixes according to their difference
in meaning. To determine proper suffixes
for a given adjective-verb pair, we have ex-
amined the verbal features involved in the
theory of Lexical Conceptual Structure.
1 Introduction
Textual expressions that (roughly) convey the
same meaning are called paraphrases. Since gen-
erating and recognizing paraphrases has a poten-
tial to contribute to a broad range of natural lan-
guage applications, such as MT, IE, and QA, many
researchers have done a lot of practices on auto-
matic paraphrasing in the last decade.
Most previous studies have addressed para-
phrase phenomena where the syntactic category
is not changed: e.g., noun-to-noun (?document?
? ?article?), verb-to-verb (?raise?? ?bring up?).
In these inner-categorial paraphrasing, only lim-
ited types of problems arise when replacing words
or phrases with their synonymous expressions.
On the other hand, this paper focuses on inter-
categorial paraphrasing, such as adjective-to-verb
(?attractive? ? ?attract?) that leads to novel type
of problems due to the prominent differences in
meaning and usage. In other words, calculating
those differences is more crucial to determine how
they can or cannot be paraphrased.
The aim of this study is to clarify what lexical
knowledge is required for capturing those differ-
ences, and to explore where such a knowledge can
be obtained from. Recent work in lexical seman-
tics has shown that syntactic behaviors and seman-
tic properties of words provide useful informa-
tion to explain the mechanisms of several classes
of paraphrases. More specifically, lexical proper-
ties involved in the theory of Lexical Conceptual
Structure (LCS) (Jackendoff, 1990) have seemed
to be beneficial because each verb does not func-
tion idiosyncratically. However, in the literature,
there have been less studies for other syntactic
categories than verbs. To the best of our knowl-
edge, the Meaning-Text Theory (MTT) (Mel?c?uk
and Polgue`re, 1987) is one of the very few frame-
works. In MTT, lexical properties and inter-
categorial paraphrasing are realized with a unique
semantic representation irrespective of syntactic
categories and what are called lexical functions,
e.g., S
0
(receive) = reception.
To make out how the recent advances in lexi-
cal semantics for verbs can be extended to other
syntactic categories, we assess LCS for inter-
categorial paraphrasing. We choose adjectives as
a counterpart of paraphrasing because they behave
relatively similar to verbs compared with other
categories: both adjectives and verbs have inflec-
tion and function as predicates, adnominal ele-
ments, etc. Yet, we speculate that their difference
in meaning and usage reveal intriguing generation
problems. To put it briefly, adjective-to-verb para-
phrasing in Japanese requires verbal suffixes such
as ?ta (past / attributive)? in example (1)1:
(1) s. furui otera-no jushoku-o tazune-ta.
be old temple-GEN priest-ACC to visit-PAST
I visited a priest in the old temple.
t. furubi-ta otera-no jushoku-o tazune-ta.
to olden-ATTR temple-GEN priest-ACC to visit-PAST
I visited a priest in the olden(ed) temple.
2 Preliminary investigation
To make an investigation into the variation and
distribution of required verbal suffixes, we col-
lected a set of paraphrase examples through the
following semi-automatic procedure:
Step 1. We handcrafted adjective-verb pairs
based on JCore (Sato, 2004), which classifies
Japanese words into five-levels of readability.
Our 128 pairs (for 85 adjectives) contain only
those sharing first few phonemes (reading)
1For each example, ?s? and ?t? denote an original sen-
tence and its paraphrase, respectively.
41
Table 1: Distribution of verbal suffixes used.
Verbal suffix Cadc Cpr
1
Cpr
2
ru 9 16 0
tei-ru 5 42 0
re-ru 14 8 0
re-tei-ru 2 5 0
ta 57 0 7
tei-ta 2 0 2
re-ta 6 0 1
re-tei-ta 0 0 1
both ta and tei-ru 4 0 0
both ta and ru 1 0 0
tea-ru 0 2 0
Total 100 73 11
where
ru: base form
tei: progressive / perfective
re: passive / potential
ta: past / attributive
tea: perfective
and characters (kanji), and either of adjective
or verb falls into the easiest three levels.
Step 2. Candidate paraphrases for a given sen-
tence collection are automatically generated
by replacing adjectives with their corre-
sponding verbs. Multiple candidates are gen-
erated for adjectives that correspond to mul-
tiple verbs.
Step 3. The correctness of each candidate para-
phrase is judged by two human annotators.
The basic criterion for judgement is that two
sentences are regarded as paraphrases if and
only if they share at least one interpretation.
In this step, the annotators are allowed to re-
vise candidates: (i) append verbal suffixes,
(ii) change of case markers, and (iii) insert
adverbs. Finally, candidates that both anno-
tators judge correct qualify as paraphrases.
Assuming that the variation and distribution of
verbal suffixes vary according to the usage of ad-
jectives, we separately collected paraphrase exam-
ples for adnominal and predicative usages.
Adnominal usages: For 960 sentences randomly
extracted from a one-year newspaper corpus,
Mainichi 1995, we obtained 165 examples for 142
source sentences. We then divided them into two
portions: 12 adjectives that appeared only once
and at least one examples for the other adjectives
were kept unseen (Cad
o
), while the remaining ex-
amples (Cad
c
) were used for our investigation.
Predicative usages: For 157 example sentences
within IPAL adjective dictionary (IPA, 1990), we
generated candidate paraphrases. 84 candidates
for 70 sentences qualified as paraphrases. They
are then divided into two portions according to
the tense of adjectives: Cpr
1
consists of examples
where adjectives appear in base form and Cpr
2
is
for ?ta? form (past tense).
Table 1 shows the distribution of verbal suffixes
used for given adjective-verb pairs in each portion
of example collections. We confirmed that their
distribution was fairly different. In the remaining
sections, we focus on adnominal usages because
examples of predicative usages have displayed a
degree of compositionality. Which of ?ru? or ?ta?
must be used is given by the input: if a given ad-
jective accompanies past tense, the resultant ver-
bal suffix is necessarily that for present tense fol-
lowed by ?ta.?
3 Determining verbal suffixes
The task we address here is to determine verbal
suffixes for a given input, a pair of an adnominal
usage of adjective in a certain context and a candi-
date verb given by our adjective-verb list.
From the viewpoint of language generation,
this task can be thought of as generating verbal
expressions where options are already given in
Table 1. A straightforward way for determining
verbal suffixes is to make use of lexical properties
of verbs as constraints on generation. To manifest
them, in particular aspectual properties involved
in LCS, we first designed seven types of linguis-
tic tests shown in Table 2. They are derived from
a classical analysis of verb semantics in Japanese
(Kageyama, 1996) and some ongoing projects on
constructing LCS dictionaries (Kato et al, 2005;
Takeuchi et al, 2006). We then manually ex-
amined 128 verbs in Section 2 under those tests.
To determine the word sense in which the deriva-
tive relationship hold good, example sentences in
IPAL verb dictionary (IPA, 1987) for each verb
were used. For a verb which was out of the dic-
tionary, we manually gave a sample sentence.
Since our aim is to explain why a certain ver-
bal suffix is used for a given input, we have not
feverishly applied a machine learning algorithm to
the task. Instead, we have manually created a rule-
based model shown in Table 3 using Cad
c
, where
each if-then rule assigns either of verbal suffixes in
Table 1 to a given input based on verbal features in
Table 2 and some other features below:
? D: affix pair of the adjective and the candi-
date verb: e.g., ?A shii-V mu? for ?kuyashii
(be regretful)? ? ?kuyamu (to regret)?
? N : disjunction of semantic classes in a the-
saurus (The Natural Institute for Japanese
Language, 2004) for the modified noun
? C: whether the adjective is head of clause
4 Experiment and discussion
By conducting an empirical experiment with Cad
c
and Cad
o
, we evaluate how our model (RULE)
properly determines verbal suffixes. A compar-
ison with a simple baseline model (BL) is also
done. BL selects the most frequently used suffix
(in this experiment ?ta?) for any given input.
42
Table 2: Linguistic tests for verbs derived from Lexical Conceptual Structure (Kageyama, 1996).
Label Description
Va whether the verb allows accusative case
Vb whether the verb can co-occur with a temporal adverb ?ichi-jikan (for one hour)? or its variant
Vc whether the verb can co-occur with a temporal adverb ?ichi-jikan-de (in one hour)? or its variant
Vd whether the verb can be followed by ?tearu (perfective)? when its accusative case is moved to nominative
Ve interpretation of the verb followed by ?tei-ru (progressive / perfective)?
Vf when followed by ?ta,? whether the verb can have the perfective interpretation or just past tense
Vg whether the verb can co-occur with a sort of adverb which indicates intention of the action: e.g. ?wazato (purposely)? and ?iyaiya (reluctantly)?
Table 3: The rule-set for determining verbal suf-
fixes, where ?(non)? indicates non-paraphrasable.
Order Condition (conjunction of ?feature label =? value?) Verbal suffix
1 Va=?yes?? Vb=?yes? ? Vf=?no? ? re-ru
N=?except Human (1.10)? ?
D=?A ui?V bumu? ? ?A i?V mu? ? ?A asii?V u?
2 Va=?yes?? Vb=?yes? ? Vf=?no? ? ta
Vd=?no? ?N=?Mind: mind, attitude (1.303)?
3 Va=?no? ? Vg=?yes? ta
4 Va=?no? ? Vf=?yes??D=?A i?V migakaru? ta / tei-ru
5 C=?clause? ?D=?A i?V maru? ru
6 Va=?no? ? Vf=?yes? ta
7 Va=?no? ? Vb=?yes?? Vf=?no? ta
8 Vb=?yes? ? Vf=?no? ? Vc=?yes?? tei-ru
Vd=?yes?? Ve=?progressive??N=?Subject (1.2)?
9 ? (non)
Table 4 shows the experimental results, where
recall and precision are calculated with regard
to input adjective-verb pairs. Among rules in
Table 3, rules 1 (for ?re-ru?), 3, 6, and 7 (for ?ta?
where Va=?no?) performed much better than the
other rules. This indicates that these rules and fea-
tures in their conditions properly reflect our lin-
guistic intuition. For instance, rule 6 reflects that
a change-of-state intransitive verb expresses re-
sultative meaning as adjectives when it modifies
Theme of the event via ?ta? (Kageyama, 1996)
as shown in (1), and rule 2 does that a psycho-
logical verb modifies a nouns with ?re-ru? when
the noun arouses the specific emotion, such as re-
gretting mistakes (e.g., ?kuyashii (be regretful)?
? ?kuyama-re-ru (be regretted)?). The aspectual
property captured by the tests in Table 2 is used to
classify verbs into these semantic classes.
On the other hand, the rules for the other types
are immature due to lack of examples: we cannot
find out even necessary conditions to be ?ru,? ?tei-
ru,? etc. What is required to induce proper con-
ditions for these suffixes is a larger example col-
lection and discovering another semantic property
and a set of linguistic tests for capturing it.
5 Conclusion and future work
In this paper, we focused on inter-categorial para-
phrasing and reported on our study on an issue
in adjective-to-verb paraphrasing. Two general-
purpose resources and a task-specific rule-set have
been handcrafted to generate proper verbal suf-
fixes. Although the rule-based model has achieved
better performance than a simple baseline model,
there is a plenty of room for improvement.
Table 4: Recall and precision of determining ver-
bal suffix for given adjective-verb pairs.
Cadc Cado
Verbal suffix Recall Precision Recall Precision
ta (Va=?yes?) 3/13 3/3 1/6 1/1
ta (Va=?no?) 42/44 42/63 18/18 18/29
re-ru 12/14 12/19 7/13 7/11
ru 3/9 3/6 0/2 0/5
tei-ru 1/5 1/7 2/8 2/6
ta / tei-ru 2/4 2/2 1/2 1/1
No rule for 11 inputs for 7 inputs
Total (RULE) 63/100 63/100 29/56 29/53
(63%) (63%) (52%) (55%)
BL 57/100 57/148 24/56 24/83
(57%) (39%) (43%) (29%)
Future work includes (i) to enlarge our two
resources as in (Dorr, 1997; Habash and Dorr,
2003) evolving an effective construction method,
(ii) intrinsic evaluation of those resources, and, of
course, (iii) to enhance the paraphrasing models
through further experiments with a larger test-set.
References
B. J. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):271?322.
N. Habash and B. J. Dorr. 2003. A categorial variation
database for English. In Proceedings of the 2003 Human
Language Technology Conference and the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 17?23.
IPA. 1987. IPA Lexicon of the Japanese language for com-
puters (Basic Verbs). Information-technology Promotion
Agency. (in Japanese).
IPA. 1990. IPA Lexicon of the Japanese language for com-
puters (Basic Adjectives). Information-technology Pro-
motion Agency. (in Japanese).
R. Jackendoff. 1990. Semantic structures. The MIT Press.
T. Kageyama. 1996. Verb semantics. Kurosio Publishers.
(in Japanese).
T. Kato, S. Hatakeyama, H. Sakamoto, and T. Ito. 2005.
Constructing Lexical Conceptual Structure dictionary for
verbs of Japanese origin. In Proceedings of the 11th An-
nual Meeting of the Association for Natural Language
Processing, pages 871?874. (in Japanese).
I. Mel?c?uk and A. Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
S. Sato. 2004. Identifying spelling variations of Japanese
words. In Information Processing Society of Japan SIG
Notes, NL-161-14, pages 97?104. (in Japanese).
K. Takeuchi, K. Inui, and A. Fujita. 2006. Construction of
compositional lexical database based on Lexical Concep-
tual Structure for Japanese verbs. In T. Kageyama, editor,
Lexicon Forum No.2. Hitsuji Shobo. (in Japanese).
The Natural Institute for Japanese Language. 2004. Word
list by semantic principles, revised and enlarged edition.
Dainippon Tosho. (in Japanese).
43
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 151?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Compositional Approach toward Dynamic Phrasal Thesaurus
Atsushi Fujita Shuhei Kato Naoki Kato Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
{shuhei,naoki}@sslab.nuee.nagoya-u.ac.jp
Abstract
To enhance the technology for computing
semantic equivalence, we introduce the no-
tion of phrasal thesaurus which is a natural
extension of conventional word-based the-
saurus. Among a variety of phrases that
conveys the same meaning, i.e., paraphrases,
we focus on syntactic variants that are com-
positionally explainable using a small num-
ber of atomic knowledge, and develop a sys-
tem which dynamically generates such vari-
ants. This paper describes the proposed sys-
tem and three sorts of knowledge developed
for dynamic phrasal thesaurus in Japanese:
(i) transformation pattern, (ii) generation
function, and (iii) lexical function.
1 Introduction
Linguistic expressions that convey the same mean-
ing are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of nat-
ural language processing tasks, including machine
translation, information retrieval, information ex-
traction, question answering, summarization, text
mining, and natural language generation.
Conventional approaches to computing semantic
equivalence between two expressions are five-fold.
The first approximates it based on the similarities
between their constituent words. If two words be-
long to closer nodes in a thesaurus or semantic net-
work, they are considered more likely to be similar.
The second uses the family of tree kernels (Collins
and Duffy, 2001; Takahashi, 2005). The degree of
equivalence of two trees (sentences) is defined as
the number of common subtrees included in both
trees. The third estimates the equivalence based on
word alignment composed using templates or trans-
lation probabilities derived from a set of parallel
text (Barzilay and Lee, 2003; Brockett and Dolan,
2005). The fourth espouses the distributional hy-
pothesis (Harris, 1968): given two words are likely
to be equivalent if distributions of their surrounding
words are similar (Lin and Pantel, 2001; Weeds et
al., 2005). The final regards two expressions equiva-
lent if they can be associated by using a set of lexico-
syntactic paraphrase patterns (Mel?c?uk, 1996; Dras,
1999; Yoshikane et al, 1999; Takahashi, 2005).
Despite the results previous work has achieved,
no system that robustly recognizes and generates
paraphrases is established. We are not convinced of
a hypothesis underlying the word-based approaches
because the structure of words also conveys some
meaning. Even tree kernels, which take structures
into account, do not have a mechanism for iden-
tifying typical equivalents: e.g., dative alternation
and passivization, and abilities to generate para-
phrases. Contrary to the theoretical basis, the two
lines of corpus-based approaches have problems in
practice, i.e., data sparseness and computation cost.
The pattern-based approaches seem steadiest. Yet
no complete resource or methodology for handling
a wide variety of paraphrases has been developed.
On the basis of this recognition, we introduce the
notion of phrasal thesaurus to directly compute se-
mantic equivalence of phrases such as follows.
(1) a. be in our favor / be favorable for us
b. its reproducibility / if it is reproducible
c. decrease sharply / show a sharp decrease
d. investigate the cause of a fire /
investigate why there was a fire /
investigate what started a fire /
make an investigation into the cause of a fire
151
Phrasal thesaurus is a natural extension of conven-
tional word-based thesaurus. It is thus promised that
it will bring us the following benefits:
Enhancement of NLP applications: As conven-
tional thesauri, phrasal thesaurus must be
useful to handle paraphrases having different
structures in a wide range of NLP applications.
Reading and writing aids: Showing more appro-
priate alternative phrases must be a power-
ful aid at certain situations such as writing
text. Controlling readability of text by altering
phrases must also be beneficial to readers.
Our aim is to develop resources and mechanisms
for computing semantic equivalence on the working
hypothesis that phrase is the appropriate unit for that
purpose. This paper describes the first version of our
paraphrase generation system and reports on our on-
going work on constructing resources for realizing
phrasal thesaurus.
The following sections describe the range of phe-
nomena we treat (Section 2), the overall architec-
ture of our paraphrase generation system which
functions as phrasal thesaurus (Section 3), the im-
plementation of knowledge bases (Section 4) fol-
lowed by discussion (Section 5), and conclusion
(Section 6).
2 Dynamic phrasal thesaurus
2.1 Issue
Toward realizing phrasal thesaurus, the following
two issues should be discussed.
? What sorts of phrases should be treated
? How to cope with a variety of expressions
Although technologies of shallow parsing have
been dramatically improved in the last decade, it
is still difficult to represent arbitrary expression in
logical form. We therefore think it is reasonable to
define the range relying on lexico-syntactic struc-
ture instead of using particular semantic representa-
tion. According to the work of (Chklovski and Pan-
tel, 2004; Torisawa, 2006), predicate phrase (sim-
ple sentence) is a reasonable unit because it approx-
imately corresponds to the meaning of single event.
Combination of words and a variety of construc-
tion coerce us into handling an enormous number
of expressions than word-based approaches. One
may think taking phrase is like treading a thorny
path because one of the arguments in Section 1 is
about coverage. On this issue, we speculate that
one of the feasible approach to realize a robust sys-
tem is to divide phenomena into compositional and
non-compositional (idiosyncratic) ones1, and sepa-
rately develop resources to handle them as described
in (Fujita and Inui, 2005).
To compute semantic equivalence of idiosyncratic
paraphrases, pairs or groups of paraphrases have to
be statically compiled into a dictionary as word-
based thesaurus. The corpus-based approach is valu-
able for that purpose, although they are not guaran-
teed to collect all idiosyncratic paraphrases. On the
other hand, compositional paraphrases can be cap-
tured by a relatively small number of rules. Thus it
seems tolerable approach to generate them dynam-
ically by applying such rules. Our work is targeted
at compositional paraphrases and the system can be
called dynamic phrasal thesaurus. Hereafter, we
refer to paraphrases that are likely to be explained
compositionally as syntactic variants.
2.2 Target language: Japanese
While the discussion above does not depend on par-
ticular language, our implementation of dynamic
phrasal thesaurus is targeted at Japanese. Sev-
eral methods for paraphrasing Japanese predicate
phrases have been proposed (Kondo et al, 1999;
Kondo et al, 2001; Kaji et al, 2002; Fujita et al,
2005). The range they treat is, however, relatively
narrow because they tend to focus on particular para-
phrase phenomena or to rely on existing resources.
On the other hand, we define the range of phenom-
ena from a top-down viewpoint. As a concrete defi-
nition of predicate phrase in Japanese,
noun phrase + case marker + predicate
is employed which is hereafter referred to ?phrase.?
Noun phrase and predicate in Japanese them-
selves subcategorize various syntactic variants as
shown in Figure 1 and paraphrase phenomena for
above phrase also involve those focused on their in-
teraction. Thus the range of phenomena is not so
narrow, and intriguing ones, such as shown in exam-
ples2 (2) and (3), are included.
1We regard lexical paraphrases (e.g., ?scope? ? ?range?)
and idiomatic paraphrases (e.g., ?get the sack?? ?be dismissed
from employment?) as idiosyncratic.
2In each example, ?s? and ?t? denote an original sentence
and its paraphrase, respectively. SMALLCAPS strings indicate
the syntactic role of their corresponding Japanese expressions.
[N] indicates a nominalizer.
152
(2) Head switching
s. kakunin-o isogu.
checking-ACC to hurry-PRES
We hurry checking it.
t. isoide kakunin-suru.
in a hurry to check-PRES
We check it in a hurry.
(3) Noun phrase ? sub-clause
s. kekka-no saigensei-o kenshou-suru.
result-GEN reproducibility-ACC to validate-PRES
We validate its reproducibility.
t. [ kekka-o saigen-dekiru ]
result-ACC to reproduce-to be able
ka-douka-o kenshou-suru.
[N]-whether-ACC to validate-PRES
We validate whether it is reproducible.
We focus on syntactic variants at least one side of
which is subcategorized into the definition of phrase
above. For the sake of simplicity, we hereafter rep-
resent those expressions using part-of-speech (POS)
patterns. For instance, (2s) is called N : C : V type,
and (3s) is N
1
: no : N
2
: C : V type.
3 Paraphrase generation system
Given a phrase, the proposed system generates its
syntactic variants in the following four steps:
1. Morphological analysis
2. Syntactic transformation
3. Surface generation with lexical choice
4. SLM-based filtering
where no particular domain, occasion, and media is
assumed3. Candidates of syntactic variants are first
over-generated in step 2 and then anomalies among
them are filtered out in steps 3 and 4 using rule-based
lexical choice and statistical language model.
The rest of this section elaborates on each compo-
nent in turn.
3.1 Morphological analysis
Technologies of morphological analysis in Japanese
have matured by introducing machine learning tech-
niques and large-scale annotated corpus, and there
are freely available tools. Since the structure of input
phrase is assumed to be quite simple, employment of
dependency analyzer was put off. We simply use a
morphological analyzer MeCab4.
3This corresponds to the linguistic transformation layer of
KURA (Takahashi et al, 2001).
4http://mecab.sourceforge.net/
noun phrase
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
formal noun
8
<
:
?koto?
?mono?
?no?
content
8
>
>
>
>
<
>
>
>
>
:
single word
compound
j
N
1
N
2
N + suffixes
modified
8
>
<
>
:
N
1
+ ?no? +N
2
Adj+N
Adjectival verb+N
clause+N
predicate
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
verb phrase
8
>
>
>
>
>
<
>
>
>
>
>
:
single word
8
>
>
<
>
>
:
original verb
Sino-Japanese verb
lexical compound
light verb
Adv+ ?suru?
compound
8
>
<
>
:
original + original
Sino + original
Sino + Sino
N + Sino
Adj
j
single word
compound
Adjectival verb+ ?da?
Adv+ ?da?
Copula
Figure 1: Classification of syntactic variants of noun
phrase and predicate in Japanese.
Our system has a post-analysis processing. If ei-
ther of Sino-Japanese verbal nouns (e.g., ?kenshou
(validation)? and ?kandou (impression)?) or translit-
eration of verbs in foreign language (e.g., ?doraibu
(to drive)? and ?shifuto (to shift)?) is immediately
followed by ?suru (to do)? or ?dekiru (to be able),?
these adjacent two morphemes are joined into a sin-
gle morpheme to avoid incorrect transformation.
3.2 Syntactic transformation
The second step over-generates syntactic variants
using the following three sorts of knowledge:
(i) Transformation pattern: It gives skeletons of
syntactic variants. Each variant is represented
by POS symbols designating the input con-
stituents and triggers of the generation function
and lexical function below.
(ii) Generation function: It enumerates different
expressions that are constituted with the same
set of words and subcategorized into the re-
quired syntactic category. Some of generation
functions handle base phrases, while the rest
generates functional words. Base phrases the
former generates are smaller than that transfor-
mation patterns treat. Since some functional
words are disjunctive, the latter generates all
candidates with a separator ?/? and leaves the
selection to the following step.
153
Table 1: Grammar in Backus-Naur form, example, and instantiation for each knowledge.
Knowledge type Grammar / Example / Instantiation
(i) Transformation <transformation pattern> ::= <left pattern> ? <right pattern>
pattern <left pattern> ::= (<POS symbol>|<word form>)+
<POS symbol> ::= (N |C|V |Adj|Adv)
<word form> ::= (<hiragana>|<katakana>|<kanji>)+
<right pattern> ::=
(<POS symbol>|<word form>|<function definition>|<lexical function>)+
(a) N : C : V ? adv(V ) : vp(N)
(b) N
1
: no : N
2
: C : V ?N
1
: genCase() : vp(N
2
) : ka-douka : C : V
(a) kakunin : o : isogu ? adv (isogu) : vp(kakunin)
checking ACC to hurry adv(to hurry) vp(checking)
(b) kekka : no : saigensei : o : kenshou-suru
result GEN reproducibility ACC to validate-PRES
? kekka : genCase() : vp(saigensei) : ka-douka : o : kenshou-suru
result case marker vp(reproducibility) [N]-whether ACC to validate-PRES
(ii) Generation <generation function> ::= <function definition> ? ?{?<right pattern>+?}?
function <function definition> ::= <syntactic category>?(?<POS symbol>*?)?
<syntactic category> ::= (np | vp | lvc)
(a) vp(N) ? {v(N) : genVoice() : genTense()}
(b) np(N
1
, N
2
) ? {N
1
, N
2
, N
1
: N
2
, N
1
: no : N
2
, vp(N
1
) : N
2
,wh(N
2
) : vp(N
1
) : ka, . . .}
(a) vp(kakunin) ? { v(kakunin) : genVoice() : genTense() }
vp(verification) v(verification) verbal suffix for voice verbal suffix for tense
(b) np(shukka, gen-in)
np(starting fire, reason)
? { shukka , gen-in , shukka : gen-in , shukka : no : gen-in ,
starting fire reason starting fire reason starting fire GEN reason
vp(shukka) : gen-in , wh(gen-in) : vp(shukka) : ka , . . . }
vp(starting fire) reason wh(reason) vp(starting fire) [N]
(iii) Lexical <lexical function> ::= <relation>?(?<POS symbol>?)?
function <relation> ::= (n | v | adj | adjv | adv | wh)
(a) adv(V )
(b) wh(N)
(a) adv (isogu)
adv(to hurry)
?
? isoide
in a hurry
(given by a verb?adverb dictionary)
?
(b) wh(gen-in)
wh(reason)
?
? { naze , doushite }
why why
(given by a noun?interrogative dictionary)
?
(iii) Lexical function: It generates different lexi-
cal items in certain semantic relations, such
as derivative form, from a given lexical item.
The back-end of this knowledge is a set
of pre-compiled dictionaries as described in
Section 4.2.
Table 1 gives a summary of grammar in Backus-
Naur form, examples, and instantiations of each
knowledge. Figure 2 illustrates an example of
knowledge application flow for transforming (4s)
into (4t), where ?:? denotes delimiter of con-
stituents.
(4) s. ?kakunin:o:isogu?
t. ?isoide:{kakunin-suru:
{?, reru/rareru, seru/saseru}:{?, ta/da}}?
First, transformation patterns that match to the given
input are applied. Then, the skeletons of syntactic
variants given by the pattern are lexicalized by con-
secutively invoking generation functions and lexical
functions. Plural number of expressions that gen-
eration function and lexical function generate are
enumerated within curly brackets. Transformation
is ended when the skeletons are fully lexicalized.
In fact, knowledge design for realizing the trans-
formation is not really new, because we have been
inspired by the previous pattern-based approaches.
Transformation pattern is thus alike that in the
Meaning-Text Theory (MTT) (Mel?c?uk, 1996), Syn-
chronous Tree Adjoining Grammar (STAG) (Dras,
1999), meta-rule for Fastr (Yoshikane et al, 1999),
154
{v(kakunin) : genVoice() : genTense()}
okakunin
N
:
C
: isogu
V
Trans. Pat.
N:C:V? adv(V):vp(N)
adv(isogu) : vp(kakunin)
Gen. Func.
vp(N)
kakunin-suru
Lex. Func.
v(N)
Gen. Func.
genVoice()
Gen. Func.
genTense()
isoide
Lex. Func.
adv(V)
{?, reru/rareru, seru/saseru} {?, ta/da}
isoide : {kakunin-suru : {?, reru/rareru, seru/saseru} : {?, ta/da}}
Figure 2: Syntactic transformation (for (2)).
and transfer pattern for KURA (Takahashi et al,
2001). Lexical function is also alike that in MTT.
However, our aim in this research is beyond the
design. In other words, as described in Section 1,
we are aiming at the following two: to develop re-
sources for handling syntactic variants in Japanese,
and to confirm if phrasal thesaurus really contribute
to computing semantic equivalence.
3.3 Surface generation with lexical choice
The input of the third component is a bunch of candi-
date phrases such as shown in (4t). This component
does the following three processes in turn:
Step 1. Unfolding: All word sequences are gener-
ated by removing curly brackets one by one.
Step 2. Lexical choice: Disjunctive words are con-
catenated with ?/? (e.g., ?reru/rareru? in (4t)).
One of them is selected based on POS and con-
jugation types of the preceding word.
Step 3. Conjugation: In the transformation step,
conjugative words are moved to different po-
sitions and some of them are newly generated.
Inappropriate conjugation forms are corrected.
3.4 SLM-based filtering
In the final step, we assess the correctness of each
candidate of syntactic variants using a statistical lan-
guage model. Our model simply rejects candidate
phrases that never appear in a large size of raw text
corpus consisting of 15 years of newspaper articles
(Mainichi 1991?2005, approximately 1.8GB). Al-
though it is said that Japanese language has a degree
N:C:V
N1:N2:C:V+N
N:C:V1:V2
+V
N:C:Adv:V+Adv
Adj:N:C:V
+Adj
N:C:Adj
switch V with Adj
Figure 3: Derivations of phrase types.
of freedom in word ordering, current implementa-
tion does not yet employ structured language models
because phrases we handle are simple.
4 Knowledge implementation
4.1 Transformation patterns and generation
functions
An issue of developing resources is how to ensure
their coverage. Our approach to this issue is to de-
scribe transformation patterns by extending those for
simpler phrases. We first described following three
patterns for N : C : V type phrases which we con-
sider the simplest according to Figure 1.
(5) a. N : C : V ? vp(N)
b. N : C : V ? N : genCase() : lvc(V )
c. N : C : V ? adv(V ) : vp(N)
While the pattern (5c) is induced from example (2),
the patterns (5a-b) are derived from examples (6)
and (7), respectively.
(6) s. shigeki-o ukeru
inspiration-ACC to receive
to receive an inspiration
t. shigeki-sareru
to inspire-PASS
to be inspired
(7) s. hada-o shigeki-suru
skin-ACC to stimulate
to stimulate skin
t. hada-ni shigeki-o ataeru
skin-DAT stimulus-ACC to give
to give skin a stimulus
Regarding the patterns in (8) as the entire set of
compositional paraphrases for N : C : V type
phrases, we then extended them to a bit more com-
plex phrases as in Figure 3. For instance, 10 patterns
155
Table 2: Transformation patterns.
Target phrase # of patterns
N : C : V 3
N
1
: N
2
: C : V 10
N : C : V
1
: V
2
10
N : C : Adv : V 7
Adj : N : C : V 4
N : C : Adj 3
Total 37
Table 3: Generation functions.
Definition Syntactic category # of returned value
np(N
1
, N
2
) noun phrase 9
vp(N) verb phrase 1
vp(N
1
, N
2
) verb phrase 2
vp(V
1
, V
2
) verb phrase 3
lvc(V ) light verb construction 1
genCase() case marker 4
genVoice() verbal suffix for voice 3
genTense() verbal suffix for tense 2
genAspect () verbal suffix for aspect 2
for N
1
: N
2
: C : V type phrases shown in (8) have
been described based on patterns in (5), mainly fo-
cusing on interactions between newly introduced N
1
and other constituents.
(8) a. N
1
: N
2
: C : V ? vp(N
1
, N
2
) (5a)
b. N
1
: N
2
: C : V ?
N
1
: genCase() : vp(N
2
) (5a)
c. N
1
: N
2
: C : V ?
N
2
: genCase() : vp(N
1
) (5a)
d. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : genCase() : lvc(V ) (5b)
e. N
1
: N
2
: C : V ? N
1
: genCase() :
N
2
: genCase() : lvc(V ) (5b)
f. N
1
: N
2
: C : V ? N
2
: genCase() :
N
1
: genCase() : lvc(V ) (5b)
g. N
1
: N
2
: C : V ?
adv (V ) : vp(N
1
, N
2
) (5c)
h. N
1
: N
2
: C : V ?
adv (V ) : N
1
: genCase() : vp(N
2
) (5c)
i. N
1
: N
2
: C : V ?
adv (V ) : N
2
: genCase() : vp(N
1
) (5c)
j. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : C : V (new)
The number of transformation patterns we have so
far developed is shown in Table 2.
Generation functions shown in Table 3 are devel-
oped along with creating transformation patterns.
Although this is the heart of the proposed model,
two problems are remained: (i) the granularity of
each generation function is determined according to
Table 4: Dictionaries for lexical functions.
ID POS-pair |D| |C| |D ? C| |J |
(a) noun?verb 3,431 - 3,431 3,431
(b) noun?adjective 308 667 906 475 ?
(c) noun?adjectival verb 1,579 - 1,579 1,579
(d) noun?adverb 271 - 271 271
(e) verb?adjective 252 - 252 192 ?
(f) verb?adjectival verb 74 - 74 68 ?
(g) verb?adverb 74 - 74 64 ?
(h) adjective?adjectival verb 66 95 159 146 ?
(i) adjective?adverb 33 - 33 26 ?
(j) adjectival verb?adverb 70 - 70 70
Total 6,158 762 6,849 6,322
our linguistic intuition, and (ii) they do not ensure of
generating all possible phrases. Therefore, we have
to establish the methodology to create this knowl-
edge more precisely.
4.2 Lexical functions
Except wh(N), which generates interrogatives as
shown in the bottom line of Table 1, the relations
we have so far implemented are lexical derivations.
These roughly correspond to S, V, A, and Adv in
MTT. The back-end of these lexical functions is a
set of dictionaries built by the following two steps:
Step 1. Automatic candidate collection: Most
derivatives in Japanese share the beginning
of words and are characterized by the corre-
spondences of their suffixes. For example,
?amai (be sweet)? and ?amami (sweetness)?
has a typical suffix correspondence ??-i:?-mi?
of adjective?noun derivation. Using this clue,
candidates are collected by two methods.
? From dictionary: Retrieve all word pairs from
the given set of words those satisfying the
following four conditions: (i) beginning with
kanji character, (ii) having different POSs,
(iii) sharing at least the first character and the
first sound, and (iv) having a suffix pattern
which corresponds to at least two pairs.
? Using dictionary and corpus: Generate candi-
dates from a set of words by applying a set of
typical suffix patterns, and then check if each
candidate is an actual word using corpus. This
is based on (Langkilde and Knight, 1998).
Step 2. Manual selection: The set of word pairs
collected in the previous step includes those do
not have particular semantic relationship. This
step involves human to discard noises.
156
Table 4 shows the size of 10 dictionaries, where
each column denotes the number of word pairs re-
trieved from IPADIC5 (|D|), those using IPADIC,
seven patterns and the same corpus as in Section 3.4
(|C|), their union (|D ? C|), and those manu-
ally judged correct (|J |), respectively. The sets of
word pairs J are used as bi-directional lexical func-
tions, although manual screening for four dictionar-
ies without dagger (?) are still in process.
5 Discussion
5.1 Unit of processing
The working hypothesis underlying our work is that
phrase is the appropriate unit for computing seman-
tic equivalence. In addition to the arguments in
Section 1, the hypothesis is supported by what is
done in practice. Let us see two related fields.
The first is the task of word sense disambigua-
tion (WSD). State-of-the-art WSD techniques refer
to context as a clue. However, the range of context
is usually not so wide: words and their POSs within
small window centered the target word and content
words within the same sentence of the target word.
The task therefore can be viewed as determining the
meaning of phrase based on its constituent words
and surrounding content words.
Statistical language model (SLM) is another field.
SLMs usually deal with various things within word
sequence (or structure) at the same time. How-
ever, relations within a phrase should be differen-
tiated from that between phrases, because checking
the former is for grammaticality, while the latter for
cohesion. We think SLMs should take the phrase to
determine boundaries for assessing the correctness
of generated expressions more accurately.
5.2 Compositionality
We examined how large part of manually created
paraphrases could be generated in our compositional
approach. First, a set of paraphrase examples were
created in the following procedure:
Step 1. Most frequent 400 phrases typed N
1
: N
2
:
C : V were sampled from one year of newspa-
per articles (Mainichi 1991).
Step 2. An annotator produced paraphrases for each
phrase. We allowed to record more than one
5http://mecab.sourceforge.jp/
paraphrase for a given phrase and to give up
producing paraphrases. As a result, we ob-
tained 211 paraphrases for 170 input phrases.
Manual classification revealed that 42% (88 / 211)
of paraphrases could be compositionally explain-
able, and the (theoretical) coverage increases to 86%
(182 / 211) if we have a synonym dictionary. This
ratio is enough high to give these phenomena pref-
erence as the research target, although we cannot re-
ject a possibility that data has been biased.
5.3 Sufficient condition of equivalence
In our system, transformation patterns and genera-
tion functions offer necessary conditions for gener-
ating syntactic variants for given input. However,
we have no sufficient condition to control the appli-
cation of such a knowledge.
It has not been thoroughly clarified what clue can
be sufficient condition to ensure semantic equiva-
lence, even in a number of previous work. Though,
at least, roles of participants in the event have to be
preserved by some means, such as the way presented
in (Pantel et al, 2007). Kaji et al (2002) introduced
a method of case frame alignment in paraphrase gen-
eration. In the model, arguments of main verb in the
source are taken over by that of the target according
to the similarities between arguments of the source
and target. Fujita et al (2005) employed a semantic
representation of verb to realize the alignment of the
role of participants governed by the source and tar-
get verbs. According to an empirical experiment in
(Fujita et al, 2005), statistical language models do
not contribute to calculating semantic equivalence,
but to filtering out anomalies. We therefore plan to
incorporate above alignment-based models into our
system, for example, within or after the syntactic
transformation step (Figure 2).
5.4 Ideas for improvement
The knowledge and system presented in Section 3
are quite simple. Thus the following features should
be incorporated to improve the system in addition to
the one described in Section 5.3.
? Dependency structure: To enable flexible
matching, such as Adv : N : C : V type input
and transformation pattern for N : C : Adv :
V type phrases.
? Sophisticated SLM: The generation phase
should also take the structure into account to
157
evaluate generated expressions flexibly.
? Knowledge development: Although we have
not done intrinsic evaluation of knowledge, we
are aware of its incompleteness. We are con-
tinuing manual screening for the dictionaries
and planning to enhance the methodology of
knowledge development.
6 Conclusion
To enhance the technology for computing seman-
tic equivalence, we have introduced the notion of
phrasal thesaurus, which is a natural extension of
conventional word-based thesaurus. Plausibility of
taking phrase as the unit of processing has been dis-
cussed from several viewpoints. On the basis of
that, we have been developing a system to dynam-
ically generate syntactic variants in Japanese predi-
cate phrases which utilizes three sorts of knowledge
that are inspired by MTT, STAG, Fastr, and KURA.
Future work includes implementing more precise
features and larger resources to compute semantic
equivalence. We also plan to conduct an empirical
evaluation of the resources and the overall system.
Acknowledgments
This work was supported in part by MEXT Grants-
in-Aid for Young Scientists (B) 18700143, and for
Scientific Research (A) 16200009, Japan.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16?23.
Chris Brockett and William B. Dolan. 2005. Support Vector
Machines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop on
Paraphrasing (IWP), pages 1?8.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: min-
ing the Web for fine-grained semantic verb relations. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 33?40.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Advances in Neural Information
Processing Systems 14: Proceedings of the 2001 Confer-
ence, pages 625?632.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005. Ex-
ploiting Lexical Conceptual Structure for paraphrase gener-
ation. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP), pages
908?919.
Atsushi Fujita and Kentaro Inui. 2005. A class-oriented ap-
proach to building a paraphrase corpus. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 25?32.
Zellig Harris. 1968. Mathematical structures of language.
New York: Interscience Publishers.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, and
Satoshi Sato. 2002. Verb paraphrase based on case frame
alignment. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
215?222.
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 1999.
Paraphrasing of ?sahen-noun + suru?. IPSJ Journal,
40(11):4064?4074. (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. IPSJ Journal, 42(3):465?
477. (in Japanese).
Irene Langkilde and Kevin Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Conference on
Computational Linguistics (COLING-ACL), pages 704?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Igor Mel?c?uk. 1996. Lexical functions: a tool for the descrip-
tion of lexical relations in a lexicon. In Leo Wanner, editor,
Lexical Functions in Lexicography and Natural Language
Processing, pages 37?102. John Benjamin Publishing Com-
pany.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. Isp: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564?571.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita,
and Kentaro Inui. 2001. KURA: a transfer-based lexico-
structural paraphrasing engine. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium (NL-
PRS) Workshop on Automatic Paraphrasing: Theories and
Applications, pages 37?46.
Tetsuro Takahashi. 2005. Computation of semantic equiva-
lence for question answering. Ph.D. thesis, Graduate School
of Information Science, Nara Institute of Science and Tech-
nology.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57?64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7?12.
Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and Christian
Jacquemin. 1999. Detecting Japanese term variation in tex-
tual corpus. In Proceedings of the 4th International Work-
shop on Information Retrieval with Asian Languages, pages
97?108.
158
Text Simplification for Reading Assistance: A Project Note
Kentaro Inui Atsushi Fujita Tetsuro Takahashi Ryu Iida
Nara Advanced Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
finui,atsush-f,tetsu-ta,ryu-ig@is.aist-nara.ac.jp
Tomoya Iwakura
Fujitsu Laboratories Ltd.
Kamikodanaka, Nakahara, Kawasaki, Kanagawa, 211-8588, Japan
iwakura.tomoya@jp.fujitsu.com
Abstract
This paper describes our ongoing research
project on text simplification for congenitally
deaf people. Text simplification we are aiming
at is the task of offering a deaf reader a syn-
tactic and lexical paraphrase of a given text for
assisting her/him to understand what it means.
In this paper, we discuss the issues we should
address to realize text simplification and re-
port on the present results in three different
aspects of this task: readability assessment,
paraphrase representation and post-transfer er-
ror detection.
1 Introduction
This paper reports on our ongoing research into
text simplification for reading assistance. Potential
users targeted in this research are congenitally deaf
people (more specifically, students at (junior-)high
schools for the deaf), who tend to have difficulties
in reading and writing text. We are aiming at the
development of the technology of text simplification
with which a reading assistance system lexically and
structurally paraphrases a given text into a simpler
and plainer one that is thus more comprehensible.
The idea of using paraphrases for reading as-
sistance is not necessarily novel. For example,
Carroll et al (1998) and Canning and Taito (1999)
report on their project in which they address syn-
tactic transforms aiming at making newspaper text
accessible to aphasics. Following this trend of re-
search, in this project, we address four unexplored
issues as below besides the user- and task-oriented
evaluation of the overall system.
Before going to the detail, we first clarify the four
issues we have addressed in the next section. We
then reported on the present results on three of the
four, readability assessment, paraphrase representa-
tion and post-transfer error detection, in the subse-
quent sections.
2 Research issues and our approach
2.1 Readability assessment
The process of text simplification for reading as-
sistance can be decomposed into the following three
subprocesses:
a. Problem identification: identify which portions of
a given text will be difficult for a given user to
read,
b. Paraphrase generation: generate possible candi-
date paraphrases from the identified portions, and
c. Evaluation: re-assess the resultant texts to choose
the one in which the problems have been resolved.
Given this decomposition, it is clear that one of the
key issues in reading assistance is the problem of as-
sessing the readability or comprehensibility1 of text
because it is involved in subprocesses (a) and (c).
Readability assessment is doubtlessly a tough is-
sue (Williams et al, 2003). In this project, however,
we argue that, if one targets only a particular popu-
lation segment and if an adequate collection of data
is available, then corpus-based empirical approaches
may well be feasible. We have already proven that
one can collect such readability assessment data by
conducting survey questionnaires targeting teachers
at schools for the deaf.
1In this paper, we use the terms readability and comprehen-
sibility interchangeably, while strictly distinguishing them from
legibility of each fragment (typically, a sentence or paragraph)
of a given text.
2.2 Paraphrase acquisition
One of the good findings that we obtained through
the aforementioned surveys is that there are a broad
range of paraphrases that can improve the readabil-
ity of text. A reading assistance system is, therefore,
hoped to be able to generate sufficient varieties of
paraphrases of a given input. To create such a sys-
tem, one needs to feed it with a large collection of
paraphrase patterns. Very timely, the acquisition of
paraphrase patterns has been actively studied in re-
cent years:
 Manual collection of paraphrases in the context of
language generation, e.g. (Robin and McKeown,
1996),
 Derivation of paraphrases through existing lexical
resources, e.g. (Kurohashi et al, 1999),
 Corpus-based statistical methods inspired by the
work on information extraction, e.g. (Jacquemin,
1999; Lin and Pantel, 2001), and
 Alignment-based acquisition of paraphrases from
comparable corpora, e.g. (Barzilay and McKe-
own, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003).
One remaining issue is how effectively these meth-
ods contribute to the generation of paraphrases in our
application-oriented context.
2.3 Paraphrase representation
One of the findings obtained in the previous stud-
ies for paraphrase acquisition is that the automatic
acquisition of candidates of paraphrases is quite re-
alizable for various types of source data but acquired
collections tend to be rather noisy and need manual
cleaning as reported in, for example, (Lin and Pan-
tel, 2001). Given that, it turns out to be important to
devise an effective way of facilitating manual correc-
tion and a standardized scheme for representing and
storing paraphrase patterns as shared resources.
Our approach is (a) to define first a fully express-
ible formalism for representing paraphrases at the
level of tree-to-tree transformation and (b) devise an
additional layer of representation on its top that is de-
signed to facilitate handcoding transformation rules.
2.4 Post-transfer text revision
In paraphrasing, the morpho-syntactic informa-
tion of a source sentence should be accessible
throughout the transfer process since a morpho-
syntactic transformation in itself can often be a mo-
tivation or goal of paraphrasing. Therefore, such
an approach as semantic transfer, where morpho-
syntactic information is highly abstracted away as
in (Dorna et al, 1998; Richardson et al, 2001),
does not suit this task. Provided that the morpho-
syntactic stratum be an optimal level of abstraction
for representing paraphrasing/transfer patterns, one
must recall that semantic-transfer approaches such as
those cited above were motivated mainly by the need
for reducing the complexity of transfer knowledge,
which could be unmanageable in morpho-syntactic
transfer.
Our approach to this problem is to (a) leave the de-
scription of each transfer pattern underspecified and
(b) implement the knowledge about linguistic con-
straints that are independent of a particular trans-
fer pattern separately from the transfer knowledge.
There are a wide range of such transfer-independent
linguistic constraints. Constraints on morpheme
connectivity, verb conjugation, word collocation,
and tense and aspect forms in relative clauses are typ-
ical examples of such constraints.
These four issues can be considered as different
aspects of the overall question how one can make
the development and maintenance of a gigantic re-
source for paraphrasing tractable. (1) The introduc-
tion of readability assessment would free us from
cares about the purposiveness of each paraphrasing
rule in paraphrase acquisition. (2) Paraphrase ac-
quisition is obviously indispensable for scaling up
the resource. (3) A good formalism for representing
paraphrasing rules would facilitate the manual re-
finement and maintenance of them. (4) Post-transfer
error detection and revision would make the system
tolerant to flows in paraphrasing rules.
While many researchers have addressed the issue
of paraphrase acquisition reporting promising results
as cited above, the other three issues have been left
relatively unexplored in spite of their significance in
the above sense. Motivated by this context, in the
rest of this paper, we address these remaining three.
3 Readability assessment
To the best of our knowledge, there have never
been no reports on research to build a computational
model of the language proficiency of deaf people, ex-
cept for the remarkable reports by Michaud and Mc-
Coy (2001). As a subpart of their research aimed at
developing the ICICLE system (McCoy and Master-
man, 1997), a language-tutoring application for deaf
learners of written English, Michaud and McCoy de-
veloped an architecture for modeling the writing pro-
ficiency of a user called SLALOM. SLALOM is de-
signed to capture the stereotypic linear order of ac-
quisition within certain categories of morphological
and/or syntactic features of language. Unfortunately,
the modeling method used in SLALOM cannot be
directly applied to our domain for three reasons.
 Unlike writing tutoring, in reading assistance, tar-
get sentences are in principle unlimited. We
therefore need to take a wider range of morpho-
syntactic features into account.
 SLALOM is not designed to capture the difficulty
of any combination of morpho-syntactic features,
which it is essential to take into account in reading
assistance.
 Given the need to consider feature combinations,
a simple linear order model that is assumed in
SLALOM is unsuitable.
3.1 Our approach: We ask teachers
To overcome these deficiencies, we took yet an-
other approach where we designed a survey ques-
tionnaire targeting teachers at schools for the deaf,
and have been collecting readability assessment data.
In this questionnaire, we ask the teachers to compare
the readability of a given sentence with paraphrases
of it. The use of paraphrases is of critical importance
in our questionnaire since it makes manual readabil-
ity assessment significantly easier and more reliable.
3.1.1 Targets
We targeted teachers of Japanese or English liter-
acy at schools for the deaf for the following reasons.
Ideally, this sort of survey would be carried out
by targeting the population segment in question, i.e.,
deaf students in our study. In fact, pedagogists and
psycholinguists have made tremendous efforts to ex-
amine the language proficiency of deaf students by
giving them proficiency tests. Such efforts are very
important, but they have had difficulty in capturing
enough of the picture to develop a comprehensive
and implementable reading proficiency model of the
population due to the expense of extensive language
proficiency testing.
In contrast, our approach is an attempt to model
the knowledge of experts in this field (i.e., teaching
deaf students). The targeted teachers have not only
rich experiential knowledge about the language pro-
ficiency of their students but are also highly skilled in
paraphrasing to help their students? comprehension.
Since such knowledge gleaned from individual ex-
periences already has some generality, extracting it
through a survey should be less costly and thus more
comprehensive than investigation based on language
proficiency testing.
3.1.2 Questionnaire
In the questionnaire, each question consists of sev-
eral paraphrases, as shown in Figure 1 (a), where
(A) is a source sentence, and (B) and (C) are para-
phrases of (A). Each respondent was asked to as-
sess the relative readability of the paraphrases given
for each source sentence, as shown in Figure 1 (b).
The respondent judged sentence (A) to be the most
difficult and judged (B) and (C) to be comparable.
A judgment that sentence s
i
is easier than sentence
s
j
means that s
i
is judged likely to be understood
by a larger subset of students than s
j
. We asked
the respondents to annotate the paraphrases with
format-free comments, giving the reasons for their
judgments, alternative paraphrases, etc., as shown in
Figure 1 (b).
To make our questionnaire efficient for model ac-
quisition, we had to carefully control the variation in
paraphrases. To do that, we first selected around 50
morpho-syntactic features that are considered influ-
ential in sentence readability for deaf people. For
each of those features, we collected several sim-
ple example sentences from various sources (literacy
textbooks, grammar references, etc.). We then man-
ually produced several paraphrases from each of the
collected sentences so as to remove the feature that
characterized the source sentence from each para-
phrase. For example, in Figure 1, the feature char-
acterizing sentence (A) is a non-restrictive relative
clause (i.e., sentence (A) was selected as an example
of this feature). Neither (B) nor (C) has this feature.
We also controlled the lexical variety to minimize
the effect of lexical factors on readability; we also
restricted the vocabulary to a top-2000 basic word
set (NIJL, 1991).
3.1.3 Administration
We administrated a preliminary survey targeting
three teachers. Through the survey, we observed that
(a) the teachers largely agreed in their assessments of
relative readability, (b) their format-free comments
indicated that the observed differences in readabil-
ity were largely explainable in terms of the morpho-
syntactic features we had prepared, and (c) a larger-
scaled survey was needed to obtain a statistically re-
liable model. Based on these observations, we con-
ducted a more comprehensive survey, in which we
prepared 770 questions and sent questionnaires with
a random set of 240 of them to teachers of Japanese
or English literacy at 50 schools for the deaf. We
Figure 1: Sample question and response
asked them to evaluate as many as possible anony-
mously. We obtained 4080 responses in total (8.0
responses per question).
3.2 Readability ranking model
The task of ranking a set of paraphrases can be de-
composed into comparisons between two elements
combinatorially selected from the set. We consider
the problem of judging which of a given pair of para-
phrase sentences is more readable/comprehensible
for deaf students. More specifically, given para-
phrase pair (s
i
; s
j
), our problem is to classify it into
either left (s
i
is easier), right (s
j
is easier), or com-
parable (s
i
and s
j
are comparable).
Once the problem is formulated this way, we can
use various existing techniques for classifier learn-
ing. So far, we have examined a method of using the
support vector machine (SVM) classification tech-
nique.
A training/testing example is paraphrase pair
(s
i
; s
j
) coupled with its quantified class label
D(s
i
; s
j
) 2 [ 1; 1]. Each sentence s
i
is character-
ized by a binary feature vector F
s
i
, and each pair
(s
i
; s
j
) is characterized by a triple of feature vectors
hF
C
s
i
s
j
; F
L
s
i
s
j
; F
R
s
i
s
j
i, where
 F
C
s
i
s
j
= F
s
i
^ F
s
j
(features shared by s
i
and s
j
),
 F
L
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
i
),
 F
R
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
j
).
D(s
i
; s
j
) represents the difference in readability be-
tween s
i
and s
j
; it is computed in the following way.
1. Let T
s
i
s
j
be the set of respondents who assessed
(s
i
; s
j
).
2. Given the degree of readability respondent t as-
signed to s
i
(s
j
), map it to real value dor(t; s) 2
[0; 1] so that the lowest degree maps to 0 and the
highest degree maps to 1. For example, the de-
gree of readability assigned to (A) in Figure 1 (b)
maps to around 0.1, whereas that assigned to (B)
maps to around 0.9.
3. D(s
i
; s
j
) =
1
jT
s
i
s
j
j
P
t2T
s
i
s
j
dor(t; s
i
)  dor(t; s
j
):
Output score Sc
M
(s
i
; s
j
) 2 [ 1; 1] for input
(s
i
; s
j
) was given by the normalized distance be-
tween (s
i
; s
j
) and the hyperplane.
3.3 Evaluation and discussion
To evaluate the two modeling methods, we con-
ducted a ten-fold cross validation on the set of 4055
paraphrase pairs derived from the 770 questions used
in the survey. To create a feature vector space, we
used 355 morpho-syntactic features. Feature annota-
tion was done semi-automatically with the help of a
morphological analyzer and dependency parser.
The task was to classify a given paraphrase pair
into either left, right, or comparable. Model M ?s
output class for (s
i
; s
j
) was given by
Cls
M
(s
i
; s
j
) =
(
left (Sc
M
(s
i
; s
j
)   
m
)
right (Sc
M
(s
i
; s
j
)  
m
)
comparable (otherwise)
;
where 
m
2 [ 1; 1] is a variable threshold used to
balance precision with recall.
We used the 473 paraphrase pairs that satisfied the
following conditions:
 jD(s
i
; s
j
)j was not less than threshold 
a
(
a
=
0:5). The answer of (s
i
; s
j
) is given by
Cls
Ans
(s
i
; s
j
) =
n
left (D(s
i
; s
j
)   
a
)
right (D(s
i
; s
j
)  
a
) :
 (s
i
; s
j
) must have been assessed by more then one
respondent, i.e., jT
s
i
s
j
j > 1:
 Agreement ratio Agr(s
i
; s
j
) must be suffi-
ciently high, i.e., Agr(s
i
; s
j
)  0:9, where
Agr(s
i
; s
j
) = (for (s
i
; s
j
)   agst(s
i
; s
j
))=
jT
s
i
s
j
j, and for (s
i
; s
j
) and agst(s
i
; s
j
) are the
number of respondents who agreed and disagreed
with Cls
Ans
(s
i
; s
j
), respectively.
We judged output class Cls
M
(s
i
; s
j
) correct if and
only if Cls
M
(s
i
; s
j
) = Cls
Ans
(s
i
; s
j
). The overall
performance was evaluated based on recall Rc and
precision Pr:
Rc =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
Ans
(s
i
;s
j
)2fleft;rightggj
Pr =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
M
(s
i
;s
j
)2fleft;rightgj
.
The model achieved 95% precision with 89% re-
call. This result confirmed that the data we collected
through the questionnaires were reasonably noiseless
and thus generalizable. Furthermore, both models
exhibited a clear trade-off between recall and preci-
sion, indicating that their output scores can be used
as a confidence measure.
4 Paraphrase representation
We represent paraphrases as transfer patterns be-
tween dependency trees. In this section, we propose
a three-layered formalism for representing transfer
patterns.
4.1 Types of paraphrases of concern
There are various levels of paraphrases as the fol-
lowing examples demonstrate:
(1) a. She burst into tears, and he tried to comfort
her.
b. She cried, and he tried to console her.
(2) a. It was a Honda that John sold to Tom.
b. John sold a Honda to Tom.
c. Tom bought a Honda from John.
(3) a. They got married three years ago.
b. They got married in 2000.
Lexical vs. structural paraphrases Example (1)
includes paraphrases of the single word ?comfort?
and the canned phrase ?burst into tears?. The sen-
tences in (2), on the other hand, exhibit structural
and thus more general patterns of paraphrasing. Both
types of paraphrases, lexical and structural para-
phrases, are considered useful for many applications
including reading assistance and thus should be in
the scope our discussion.
Atomic vs. compositional paraphrases The pro-
cess of paraphrasing (2a) into (2c) is compositional
because it can be decomposed into two subpro-
cesses, (2a) to (2b) and (2b) to (2c). In develop-
ing a resource for paraphrasing, we have only to
cover non-compositional (i.e., atomic) paraphrases.
Compositional paraphrases can be handled if an ad-
ditional computational mechanism for combining
atomic paraphrases is devised.
Meaning-preserving vs. reference-preserving
paraphrases It is also useful to distinguish
reference-preserving paraphrases from meaning-
preserving ones. The above example in (3) is of the
reference-preserving type. This types of paraphras-
ing requires the computation of reference to objects
outside discourse and thus should be excluded from
our scope for the present purpose.
4.2 Dependency trees (MDSs)
Previous work on transfer-based machine transla-
tion (MT) suggests that the dependency-based repre-
sentation has the advantage of facilitating syntactic
transforming operations (Meyers et al, 1996; Lavoie
et al, 2000). Following this, we adopt dependency
trees as the internal representations of target texts.
We suppose that a dependency tree consists of a set
of nodes each of which corresponds to a lexeme or
compound and a set of edges each of which repre-
sents the dependency relation between its ends. We
call such a dependency tree a morpheme-based de-
pendency structure (MDS). Each node in an MDS is
supposed to be annotated with an open set of typed
features that indicate morpho-syntactic and semantic
information. We also assume a type hierarchy in de-
pendency relations that consists of an open set of de-
pendency classes including dependency, compound,
parallel, appositive and insertion.
4.3 Three-layered representation
Previous work on transfer-based MT sys-
tems (Lavoie et al, 2000; Dorna et al, 1998)
and alignment-based transfer knowledge acqui-
sition (Meyers et al, 1996; Richardson et al,
2001) have proven that transfer knowledge can be
best represented by declarative structure mapping
(transforming) rules each of which typically consists
of a pair of source and target partial structures as in
the middle of Figure 2.
Adopting such a tree-to-tree style of representa-
tion, however, one has to address the issue of the
trade-off between expressibility and comprehensi-
bility. One may want a formalism of structural
rule editing
translation
compilation
simplified MDS transfer rule
N shika V- nai  ->  V no wa N dake da.
(someone does not V to nothing but N)   (it is only to N that someone does V)
MDS transfer rule
sp_rule(108, negation, RefNode) :-
  match(RefNode, X4=[pos:postp,lex: shika]),
  depend(X3=[pos:verb], empty, X4),
  depend(X1=[pos:aux_verb,lex: nai],
         X2=[pos:aux_verb*], X3),
  depend(X4, empty, X5=[pos:noun]),
  replace(X1, X6=[pos:aux_verb,lex: da]),
  substitute(X5, X12=[pos:noun]),
  move_dtrs(X5, X12),
  substitute(X3, X10=[pos:verb]),
                            :
pos: postp
lex: shika (except)
pos: aux_verb
lex:  da (copula)
pos: postp
lex: wa (TOP)
X6
X11
X12pos: nounlex:  no (thing)
pos: postp
lex: dake (only)
pos: noun
pos: noun
aux_verb*
pos: aux_verb
lex: nai (not)
pos: verbX3
X4
X1
X5
X2 X7
X8
X10 pos: verb
X9 vws
MDS processing operators
(=X5)
(=X2)
(=X3)
Figure 2: Three-layered rule representation
transformation patterns that is powerful enough to
represent a sufficiently broad range of paraphrase
patterns. However, highly expressible formalisms
would make it difficult to create and maintain rules
manually.
To mediate this trade-off, we devised a new layer
of representation to add on the top of the layer of
tree-to-tree pattern representation as illustrated in
Figure 2. At this new layer, we use an extended natu-
ral language to specify transformation patterns. The
language is designed to facilitate the task of hand-
coding transformation rules. For example, to define
the tree-to-tree transformation pattern given in the
middle of Figure 2, a rule editor needs only to spec-
ify its simplified form:
(4) N shika V- nai ! V no ha N dake da.
(Someone does V to nothing but N ! It is only to
N that someone does V)
A rule of this form is then automatically translated
into a fully-specified tree-to-tree transformation rule.
We call a rule of the latter form an MDS rewriting
rule (SR rule), and a rule of the former form a sim-
plified SR rule (SSR rule).
The idea is that most of the specifications of an SR
rule can usually be abbreviated if a means to auto-
matically complement it is provided. We use a parser
and macros to do so; namely, the rule translator com-
plements an SSR rule by macro expansion and pars-
ing to produce the corresponding SR rule specifica-
tions. The advantages of introducing the SSR rule
layer are the following:
 The SSR rule formalism allows a rule writer to
edit rules with an ordinary text editor, which
makes the task of rule editing much more efficient
than providing her/him with a GUI-based com-
plex tool for editing SR rules directly.
 The use of the extended natural language also
has the advantage in improving the readability of
rules for rule writers, which is particularly impor-
tant in group work.
 To parse SSR rules, one can use the same parser
as that used to parse input texts. This also im-
proves the efficiency of rule development because
it significantly reduces the burden of maintaining
the consistency between the POS-tag set used for
parsing input and that used for rule specifications.
The SSR rule layer shares underlying motiva-
tions with the formalism reported by Hermjakob et
al. (2002). Our formalism is, however, considerably
extended so as to be licensed by the expressibility of
the SR rule representation and to be annotated with
various types of rule applicability conditions includ-
ing constraints on arbitrary features of nodes, struc-
tural constraints, logical specifications such as dis-
junction and negation, closures of dependency rela-
tions, optional constituents, etc.
The two layers for paraphrase representation
are fully implemented on our paraphrasing engine
KURA (Takahashi et al, 2001) coupled with another
layer for processing MDSs (the bottom layer illus-
trated in Figure 2). The whole system of KURA
and part of the transer rules implemented on it
(see Section 5 below) are available at http://cl.aist-
nara.ac.jp/lab/kura/doc/.
5 Post-transfer error detection
What kinds of transfer errors tend to occur in lex-
ical and structural paraphrasing? To find it out, we
conducted a preliminary investigation. This section
reports a summary of the results. See (Fujita and
Inui, 2002) for further details.
We implemented over 28,000 transfer rules for
Japanese paraphrases on the KURA paraphrasing en-
gine based on the rules previously reported in (Sato,
1999; Kondo et al, 1999; Kondo et al, 2001; Iida et
al., 2001) and existing lexical resources such as the-
sauri and case frame dictionaries. The implemented
rules ranged from such lexical paraphrases as those
that replace a word with its synonym to such syn-
tactic/structural paraphrases as those that remove a
cleft construction from a sentence, devide a sentence,
etc. We then fed KURA with a set of 1,220 sentences
randomly sampled from newspaper articles and ob-
tained 630 transferred output sentences.
The following are the tendencies we observed:
 The transfer errors observed in the experiment ex-
hibited a wide range of variety from morphologi-
cal errors to semantic and discourse-related ones.
 Most types of errors tended to occur regardless
of the types of transfer. This suggests that if one
creates an error detection module specialized for
a particular error type, it works across different
types of transfer.
 The most frequent error type involved inappropri-
ate conjugation forms of verbs. It is, however,
a matter of morphological generation and can be
easily resolved.
 Errors in regard to verb valency and selectional
restriction also tended to be frequent and fatal,
and thus should have preference as a research
topic.
 The next frequent error type was related to the
difference of meaning between near synonyms.
However, this type of errors could often be de-
tected by a model that could detect errors of verb
valency and selectional restriction.
Based on these observations, we concluded that
the detection of incorrect verb valences and verb-
complement cooccurrence was one of the most se-
rious problems that should have preference as a re-
search topic. We are now conducting experiments
on empirical methods for detecting this type of er-
rors (Fujita et al, 2003).
6 Conclusion
This paper reported on the present results of our
ongoing research on text simplification for reading
assistance targeting congenitally deaf people. We
raised four interrelated issues that we needed address
to realize this application and presented our previ-
ous activities focuing on three of them: readabil-
ity assessment, paraphrase representation and post-
transfer error detection.
Regarding readability assessment, we proposed a
novel approach in which we conducted questionnaire
surveys to collect readability assessment data and
took a corpus-based empirical method to obtain a
readability ranking model. The results of the sur-
veys show the potential impact of text simplification
on reading assistance. We conducted experiments on
the task of comparing the readability of a given para-
phrase pair and obtained promising results by SVM-
based classifier induction (95% precision with 89%
recall). Our approach should be equally applicable
to other population segments such as aphasic read-
ers and second-language learners. Our next steps
includes the investigation of the drawbacks of the
present bag-of-features modeling approach. We also
need to consider a method to introduce the notion
of user classes (e.g. beginner, intermediate and ad-
vanced). Textual aspects of readability will also need
to be considered, as discussed in (Inui and Nogami,
2001; Siddahrthan, 2003).
Regarding paraphrase representation, we pre-
sented our revision-based lexico-structural para-
phrasing engine. It provides a fully expressible
scheme for representating paraphrases, while pre-
serving the easiness of handcraft paraphrasing rules
by providing an extended natural language as a
means of pattern editting. We have handcrafted over
a thousand transfer rules that implement a broad
range of lexical and structural paraphrasing.
The problem of error detection is also critical.
When we find a effective solution to it, we will be
ready to integrate the technologies into an applica-
tion system of text simplification and conduct user-
and task-oriented evaluations.
Acknowledgments
The research presented in this paper was partly
funded by PREST, Japan Science and Technology
Corporation. We thank all the teachers at the schools
for the deaf who cooperated in our questionnaire sur-
vey and Toshihiro Agatsuma (Joetsu University of
Education) for his generous and valuable coopera-
tion in the survey. We also thank Yuji Matsumoto
and his colleagues (Nara Advanced Institute of Sci-
ence and Technology) for allowing us to use their
NLP tools ChaSen and CaboCha, Taku Kudo (Nara
Advanced Institute of Science and Technology) for
allowing us to use his SVM tool, and Takaki Makino
and his colleagues (Tokyo University) for allow-
ing us to use LiLFeS, with which we implemented
KURA. We also thank the anonymous reviewers for
their suggestive and encouraging comments.
References
Barzilay, R. and McKeown, K. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 50?57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrases: an
unsupervised approach using multiple-sequence align-
ment. In Proc. of HLT-NAACL.
Canning, Y. and Taito, J. 1999. Syntactic simplification of
newspaper text for aphasic readers. In Proc. of the 22nd
Annual International ACM SIGIR Conference (SIGIR).
Carroll, J., Minnen, G., Canning, Y., Devlin, S. and Tait, J.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In Proc. of AAAI-98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology.
Dorna, M., Frank, A., Genabith, J. and Emele, M. 1998.
Syntactic and semantic transfer with F-structures. In
Proc. of COLING-ACL, pages 341?347.
Fujita, A. and Inui, K. 2002. Decomposing linguistic
knowledge for lexical paraphrasing. In Information
Processing Society of Japan SIG Technical Reports,
NL-149, pages 31?38. (in Japanese)
Fujita, A., Inui, K. and Matsumoto, Y. 2003. Automatic
detection of verb valency errors in paraphrasing. In In-
formation Processing Society of Japan SIG Technical
Reports, NL-156. (in Japanese)
Hermjakob, U., Echihabi, A. and Marcu, D. 2002. Nat-
ural language based reformulation resource and Web
exploitation for question answering. In Proc. of the
TREC-2002 Conference.
Iida, R., Tokunaga, Y., Inui, K. and Eto, J. 2001. Explo-
ration of clause-structural and function-expressional
paraphrasing using KURA. In Proc. of the 63th Annual
Meeting of Information Processing Society of Japan,
pages 5?6. (in Japanese).
Inui, K. and Nogami, M. 2001. A paraphrase-based explo-
ration of cohesiveness criteria. In Proc. of the Eighth
European Workshop on Natulan Language Generation,
pages 101?110.
Jacquemin, C. 1999. Syntagmatic and paradigmatic rep-
resentations of term variations. In Proc. of the 37th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?349.
Kondo, K., Sato, S. and Okumura, M. 1999. Paraphras-
ing of ?sahen-noun + suru?. Journal of Information
Processing Society of Japan, 40(11):4064?4074. (in
Japanese).
Kondo, K., Sato, S. and Okumura, M. 2001. Para-
phrasing by case alternation. Journal of Informa-
tion Processing Society of Japan, 42(3):465?477. (in
Japanese).
Kurohashi, S. and Sakai, Y. 1999. Semantic analysis of
Japanese noun phrases: a new approach to dictionary-
based understanding. In Proc. of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 481?488.
Lavoie, B. Kittredge, R. Korelsky, T. Rambow, O. 2000.
A framework for MT and multilingual NLG ystems
based on uniform lexico-structural processing. In Proc.
of ANLP-NAACL.
Lin, D. and Pantel, P. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
McCoy ,K. F. and Masterman (Michaud), L. N. 1997. A
Tutor for Teaching English as a Second Language for
Deaf Users of American Sign Language, In Proc. of
ACL/EACL ?97 Workshop on Natural Language Pro-
cessing for Communication Aids.
Meyers, A., Yangarber, R. and Grishman, R. 1996. Align-
ment of shared forests for bilingual corpora. In Proc.
of the 16th International Conference on Computational
Linguistics (COLING), pages 460?465.
Michaud, L. N. and McCoy, K. F. 2001. Error profiling:
toward a model of English acquisition for deaf learn-
ers. In Proc. of the 39th Annual Meeting and the 10th
Conference of the European Chapter of Association for
Computational Linguistics (EACL), pages 386?393.
NIJL, the National Institute for Japanese Language. 1991.
Nihongo Kyo?iku-no tame-no Kihon-Goi Cho?sa (The
basic lexicon for the education of Japanese). Shuei
Shuppan, Japan. (In Japanese)
Richardson, S., Dolan, W., Menezes, A. and Corston-
Oliver, M. 2001. Overcoming the customization bottle-
neck using example-based MT. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 9?16.
Robin, J. and McKeown, K. 1996. Empirically designing
and evaluating a new revision-based model for sum-
mary generation. Artificial Intelligence, 85(1?2):135?
179.
Sato, S. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Shinyama, Y., Sekine, S. Kiyoshi, Sudo. and Grishman,
R. 2002. Automatic paraphrase acquisition from news
articles. In Proc. of HLT, pages 40?46.
Siddahrthan, A. 2003. Preserving discourse structure
when simplifying text. In Proc. of European Workshop
on Natural Language Generation, pages 103?110.
Takahashi, T., Iwakura, T., Iida, R., Fujita, A. and Inui, K.
2001. KURA: a transfer-based lexico-structural para-
phrasing engine. In Proc. of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Workshop
on Automatic Paraphrasing: Theories and Applica-
tions, pages 37?46.
Williams, S., Reiter, E. and Osman, L. 2003. Experiments
with discourse-level choices and readability. In Proc. of
European Workshop on Natural Language Generation,
pages 127?134.
Paraphrasing of Japanese Light-verb Constructions
Based on Lexical Conceptual Structure
Atsushi Fujita? Kentaro Furihata? Kentaro Inui? Yuji Matsumoto? Koichi Takeuchi?
?Graduate School of Information Science,
Nara Institute of Science and Technology
{atsush-f,kenta-f,inui,matsu}@is.naist.jp
?Department of Information Technology,
Okayama University
koichi@it.okayama-u.ac.jp
Abstract
Some particular classes of lexical paraphrases such
as verb alteration and compound noun decomposi-
tion can be handled by a handful of general rules
and lexical semantic knowledge. In this paper, we
attempt to capture the regularity underlying these
classes of paraphrases, focusing on the paraphras-
ing of Japanese light-verb constructions (LVCs).
We propose a paraphrasing model for LVCs that
is based on transforming the Lexical Conceptual
Structures (LCSs) of verbal elements. We also pro-
pose a refinement of an existing LCS dictionary. Ex-
perimental results show that our LCS-based para-
phrasing model characterizes some of the semantic
features of those verbs required for generating para-
phrases, such as the direction of an action and the
relationship between arguments and surface cases.
1 Introduction
Automatic paraphrase generation technology offers
the potential to bridge gaps between the authors and
readers of documents. For example, a system that
is capable of simplifying a given text, or showing
the user several alternative expressions conveying
the same content, would be useful for assisting a
reader (Carroll et al, 1999; Inui et al, 2003).
In Japanese, like other languages, there are sev-
eral classes of paraphrasing that exhibit a degree
of regularity that allows them to be explained by
a handful of sophisticated general rules and lexical
semantic knowledge. For example, paraphrases as-
sociated with voice alteration, verb/case alteration,
compounds, and lexical derivations all fall into such
classes. In this paper, we focus our discussion on
another useful class of paraphrases, namely, the
paraphrasing of light-verb constructions (LVCs),
and propose a computational model for generating
paraphrases of this class.
Sentence (1s) is an example of an LVC1. An LVC
is a verb phrase (?kandou-o ataeta (made an impres-
sion)? in (1s)) that consists of a light-verb (?ataeta
(give-PAST)?) that grammatically governs a nomi-
1For each example, s denotes an input and t denotes its para-
phrase.
nalized verb (?kandou (an impression)?) (also see
Figure 1 in Section 2.2). A paraphrase of (1s) is sen-
tence (1t), in which the nominalized verb functions
as the main verb with its verbal form (?kandou-s-as
e-ta (be impressed-CAU, PAST)?).
(1) s. Eiga-ga kare-ni kandou-o ataeta.
film-NOM him-DAT impression-ACC give-PAST
The film made an impression on him.
t. Eiga-ga kare-o kandou-s-ase-ta.
film-NOM him-ACC be impressed-CAUSATIVE, PAST
The film impressed him.
To generate this type of paraphrase, we need a com-
putational model that is capable of the following
two classes of choice (also see Section 2.2):
Selection of the voice: The model needs to be able
to choose the voice of the target sentence from
active, passive, causative, etc. In example (1),
the causative voice is chosen, which is indi-
cated by the auxiliary verb ?ase (causative)?.
Reassignment of the cases: The model needs to
be able to reassign a case marker to each ar-
gument of the main verb. In (1), the gram-
matical case of ?kare (him),? which was orig-
inally assigned the dative case, is changed to
accusative.
The task is not as simple as it may seem, because
both decisions depend not only on the syntactic and
semantic attributes of the light-verb, but also on
those of the nominalized verb (Muraki, 1991).
In this paper, we propose a novel lexical
semantics-based account of the LVC paraphrasing,
which uses the theory of Lexical Conceptual Struc-
ture (LCS) of Japanese verbs (Kageyama, 1996;
Takeuchi et al, 2001). The theory of LCS offers
an advantage as the basis of lexical resources for
paraphrasing, because it has been developed to ex-
plain varieties of linguistic phenomena including
lexical derivations, the construction of compounds,
and verb alteration (Levin, 1993; Dorr et al, 1995;
Kageyama, 1996; Takeuchi et al, 2001), all of
which are associated with the systematic paraphras-
ing we mentioned above.
The paraphrasing associated with LVCs is not id-
iosyncratic to Japanese but also appears commonly
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 9-16
in other languages such as English (Mel?c?uk and
Polgue`re, 1987; Iordanskaja et al, 1991; Dras,
1999, etc.), as indicated by the following examples.
(2) s. Steven made an attempt to stop playing.
t. Steven attempted to stop playing.
(3) s. It had a noticeable effect on the trade.
t. It noticeably affected the trade.
Our approach raises the interesting issue of whether
the paraphrasing of LVCs can be modeled in an
analogous way across languages.
Our aim in this paper are: (i) exploring the reg-
ularity of the LVC paraphrasing based a lexical
semantics-based account, and (ii) assessing the im-
mature Japanese semantic typology through a prac-
tical task.
The following sections describe our motiva-
tion, target, and related work on LVC paraphras-
ing (Section 2), the basics of LCS and the refine-
ments we made (Section 3), our paraphrasing model
(Section 4), and our experiments (Section 5). Fi-
nally, we conclude this paper with a brief of descrip-
tion of work to be done in the future (Section 6).
2 Motivation, target, and related work
2.1 Motivation
One of the critical issues that we face in para-
phrase generation is how to develop and maintain
knowledge resources that covers a sufficiently wide
range of paraphrasing patterns such as those in-
dicating that ?to make an attempt? can be para-
phrased into ?to attempt,? and that ?potential? can
be paraphrased into ?possibility.? Several attempts
have been made to develop such resources manually
(Sato, 1999; Dras, 1999; Inui and Nogami, 2001);
those work have, however, tended to restrict their
scope to specific classes of paraphrases, and cannot
be used to construct a sufficiently comprehensive re-
source for practical applications.
There is another trend in the research in this field,
namely, the automatic acquisition of paraphrase pat-
terns from parallel or comparable corpora (Barzilay
and McKeown, 2001; Lin and Pantel, 2001; Pang et
al., 2003; Shinyama and Sekine, 2003, etc.). This
type of approach may be able to reduce the cost
of resource development. There are problems that
must be overcome, however, before they can work
practically. First, automatically acquired patterns
tend to be complex. For example, from the para-
phrase of (4s) into (4t), we can naively obtain the
pattern: ?X is purchased by Y ? Y buys X.?
(4) s. This car was purchased by him.
t. He bought this car.
This could also, however, be regarded as a combi-
nation of a simpler pattern of lexical paraphrasing
(?purchase ? buy?) and a voice activization (?X
Adjective
Noun + Case particle"no"(GEN)
Nominalized verb + Case particle
Noun + Case Particle
Adverb
Light-verb (+suffixes)
Embedded clause
LVC
Target of this paper
(a)
(b)
(c)
(d)
(e)
Figure 1: Dependency structure showing the range
which the LVC paraphrasing affects.
be VERB-PP by Y ? Y VERB X?). If we were to
use an acquisition scheme that is not capable of de-
composing such complex paraphrases correctly, we
would have to collect a combinatorial number of
paraphrases to gain the required coverage. Second,
the results of automatic acquisition would likely in-
clude many inappropriate patterns, which would re-
quire manual correction. Manual correction, how-
ever, would be impractical if we were collecting a
combinatorial number of patterns.
Our approach to this dilemma is as follows: first,
we manually develop the resources needed to cover
those paraphrases that appear regularly, and then de-
compose and automatically refine the acquired para-
phrasing patterns using those resources. The work
reported in this paper is aimed at this resource de-
velopment.
2.2 Target structure and required operations
Figure 1 shows the range which the LVC para-
phrasing affects, where the solid boxes denote
Japanese base-chunk so-called ?bunsetsu.?2 Being
involved in the paraphrasing, the modifiers of the
LVC need the following operations:
Change of the dependence: The dependences of
the elements (a) and (b) need to be changed
because the original modifiee, the light-verb,
is eliminated by the paraphrasing.
Re-conjugation: The conjugation form of the ele-
ments (d), (e), and occasionally (c) need to be
changed according to the category change of
their modifiee, the nominalized verb.
Reassignment of the cases: As described in the
previous section, the case markers of the ele-
ments (b) and often (c) need to be reassigned.
Selection of the voice: The voice of the nominal-
ized verb needs to be chosen according to the
combination of the nominalized verb, the light-
verb, and the original voice.
The first two operations are trivial in the field of
text generation. Moreover, they can be done inde-
pendently of the LVC paraphrasing. The most deli-
cate operation is for the element (c) because it acts
either as an adverb or as a case, relying on the con-
2The modifiee of the LVC is not affected because the part-
of-speech of the light-verb and main verb are the same.
Table 1: Examples of LCS
Verb LCS for verb Verb phrase
move [y MOVE TO z] My sister (Theme) moves to a neighboring town (Goal).
transmit [x CONTROL [y MOVE TO z]] The enzyme (Agent) transmits messages (Theme) to the muscles (Goal).
locate [y BE AT z] The school (Theme) locates near the river (Goal).
maintain [x CONTROL [y BE AT z]] He (Agent) maintains a machine (Theme) in good condition (Goal).
text. In the former case, it needs the second opera-
tion. In the latter case, it needs the third operation
as well as the element (b).
In this paper, we take into account only the ele-
ment (b), namely, the sibling cases of the nominal-
ized verb.
2.3 Related work
Based on the Meaning-Text Theory (Mel?c?uk and
Polgue`re, 1987), Iordanskaja et al (1991) pro-
poses a set of paraphrasing rules including one for
LVC paraphrasing. Their rule heavily relies on what
are called lexical functions, by which they virtually
specify all the choices relevant to LVC paraphras-
ing for every combination of nominalized verb and
light-verb individually. Our approach is to employ
lexical semantics to provide a general account of
those classes of choices.
On the other hand, Kaji and Kurohashi (2004)
proposes a paraphrasing model which bases on an
ordinary dictionary. Given an input LVC, their
model paraphrases it using the gloss of both the
nominalized verb and the light-verb with the seman-
tic feature of the light-verb. Their model looks ro-
bust because of the availability of an ordinary dic-
tionary. However, their model fails to explain the
difference in the voice selection between examples
(5) and (6) since it selects the voice based only
on the light-verb ? in their approach, the light-
verb ?ukeru (to receive)? always maps to the passive
voice irrespective of the nominalized verb.
(5) s. Enkai-eno shoutai-o uketa.
party-GEN invitation-ACC receive-PAST
I received an invitation to the party.
t. Enkai-ni shoutai-s-are-ta.
party-DAT invite-PAS, PAST
I was invited to the party.
(6) s. Kare-no hanashi-ni
his-GEN talk-DAT
kandou-o uketa.
impression-ACC receive-PAST
I was given a good impression by his talk.
t. Kare-no hanashi-ni kandou-shi-ta.
his-GEN talk-DAT be impressed-ACT, PAST
I was impressed by his talk.
In (Kaji and Kurohashi, 2004), the target expres-
sion is restricted only to the LVC itself (also see
Figure 1). Hence, their model is unable to reassign
the cases as we saw in example (1).
3 Lexical Conceptual Structure
3.1 Basic framework of LCS
The theory of Lexical Conceptual Structure
(LCS) associates a verb with a semantic struc-
ture as exemplified by Table 1. An LCS consists
of semantic predicates (?CONTROL,? ?BE AT,?
etc.) and their argument slots (x, y, z). Argument
slots x, y, and z correspond to the semantic roles
?Agent,? ?Theme,? and ?Goal,? respectively. Tak-
ing the LCS of the verb ?transmit? as an example,
[y MOVE TO z] denotes the state of affairs that the
state of the ?Theme? changes to the ?Goal,? and
[x CONTROL . . .] denotes that the ?Agent? causes the
state change.
3.2 Refinements
We make use of the TLCS dictionary, a Japanese
verb LCS dictionary developed by Takeuchi et al
(2001), because it offers the following advantages:
? It is based on solid linguistic work, as in
(Kageyama, 1996).
? Its scale is considerably larger than any other
existing collections of verb LCS entries.
? It provides a set of concrete rules for LCS as-
signment, which ensures the reliability of the
dictionary.
In spite of these advantages, our preliminary ex-
amination of the dictionary revealed that further re-
finements were needed. To refine the typology of
TLCS, we collected the following sets of words:
Nominalized verbs: We regard ?sahen-nouns?4
and nominal forms of verbs as nominalized
verbs. We retrieved 1,210 nominalized verbs
from the TLCS dictionary.
Light-verbs: Since a verb takes different meanings
when it is a part of LVCs with different case
particles, we collected pairs ?c, v? of case par-
ticle c and verb v in the following way:
Step 1. We collected 876,101 types of triplets
?n, c, v? of nominalized verb n, case par-
ticle c, and base form of verb v from the
parsed5 sentences of newspaper articles6.
4A sahen-noun is a verbal noun in Japanese, which acts as
a verb in the form of ?sahen-noun + suru?.
5We used the statistical Japanese dependency parser
CaboCha (Kudo and Matsumoto, 2002) for parsing.
http://chasen.naist.jp/?taku/software/cabocha/
6Excerpts from 9 years of the Mainichi Shinbun and 10
years of the Nihon Keizai Shinbun, giving a total of 25,061,504
sentences, were used.
Table 2: Extensions of LCS
Verb Verb phrase and its LCS representation
Ext.1 hankou-suru [[Ken]y BE AGAINST [parents]z]
(resist) Ken-ga oya-ni hankou-suru.
Ken-NOM parents-DAT resist-PRES (Ken resists his parents.)
Ext.2 ukeru [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]]
(receive) Ten?in-ga kyaku-kara kujo-o ukeru.
salesclerk-NOM customer-ABL complaint-ACC receive-PRES
(The salesclerk receives a complaint from a customer.)
Ext.3 motomeru [[Ken]x CONTROL [[apology]y MOVE FROM [George]z TO [FILLED]]]3
(ask) Ken-ga George-ni shazai-o motomeru.
Ken-NOM George-DAT apology-ACC ask-PRES (Ken asks George for an apology.)
Ext.4 kandou-suru [BECOME [[Ken]z BE WITH [[FILLED]y MOVE FROM [music]x TO [Ken]z]]]
(be impressed) Ken-ga ongaku-ni kandou-suru.
Ken-NOM music-DAT be impressed-PRES (Ken is impressed by the music.)
Step 2. For each of the 50 most frequent ?c, v?
tuples, we extracted the 10 most frequent
?n, c, v?.
Step 3. Each ?n, c, v? was manually evaluated
to determine whether it was an LVC. If
any of 10 triplets was determined to be
an LVC, ?c, v? was merged into the list of
light-verbs. As a result, we collected 40
types of ?c, v? for light-verbs.
Through investigating the above 1,210 nominal-
ized verbs and 40 light-verbs, we extended the ty-
pology of TLCS as shown below (also see Table 2).
Ext. 1. Treatment of ?Partner?: The dative case
of ?hankou-suru (resist)? and ?eikyo-suru (af-
fect)? does not indicate the ?Goal? of the ac-
tion but the ?Partner.?
Ext. 2. Verbs of obtaining (Levin, 1993): In con-
trast with ?ataeru (give),? the nominative case
of ?ukeru (receive)? and ?eru (acquire)? is the
?Goal? of the ?Theme,? while the ablative case
indicates ?Source.?
Ext. 3. Require verb: ?motomeru (ask)? and
?yokyu-suru (require)? denote the existence of
the external ?Agent? who controls the action of
the other ?Agent? or ?Theme.?
Ext. 4. Verbs of psychological state (Levin,
1993): ?kandou-suru (be impressed)? and ?os-
oreru (fear)? indicate the change of psycholog-
ical state of the ?Agent.? The ascriptive part of
the change has to be described.
Consequently, we defined a new LCS typology
consisting of 16 types. Note that more than one LCS
can be assigned to a verb if it has a polysemy. For
convenience, we refer to the extended dictionary as
the LCSdic7.
6The predicate ?FILLED? represents an implicit argument
of the verb and the verb assigned this LCS cannot take this
argument. Taking the LCS of the verb ?sign? as an example,
?FILLED? in [x CONTROL [BECOME [[FILLED]y BE AT
z]]] denotes the name of ?Agent.?
7The latest version of the LCSdic is available from
http://cl.it.okayama-u.ac.jp/rsc/lcs/
4 Paraphrasing model
In this section, we describe how we generate para-
phrases of LVCs. Figure 2 illustrates how our model
paraphrases the LVC of example (7).
(7) s. Ken-ga eiga-ni shigeki-o uketa.
Ken-NOM film-DAT inspiration-ACC receive-PAST
Ken received inspiration from the film.
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT inspire-PAS, PAST
Ken was inspired by the film.
The idea is to exploit the LCS representation as a
semantic representation and to model the LVC para-
phrasing by the transformation of the LCS represen-
tation. The process consists of the following three
steps:
Step 1. Semantic analysis: The model first ana-
lyzes a given input sentence including an LVC
to obtain its semantic structure in terms of the
LCS representation. In Figure 2, this step pro-
duces LCS
V 1
.
Step 2. Semantic transformation (LCS transfor-
mation): The model then transfers the ob-
tained semantic structure to another semantic
structure so that the target structure consists of
the LCS of the nominalized verb of the input.
In our example, this step generates LCS
N1
to-
gether with the supplement [BECOME [. . .]].
Step 3. Surface generation: Having obtained the
target LCS representation, the model finally
lexicalizes it to generate the output sentence.
So, the key issue is how to control the second step,
namely, the transformation of the LCS representa-
tion.
The rest of this section elaborates on each step,
using different symbols to denote arguments; x, y,
and z for LCS
V
, and x?, y?, and z? for LCS
N
.
4.1 Semantic analysis
Given an input sentence, which we assume to be a
simple clause with an LVC, we first look up the LCS
template LCS
V 0
for the given light-verb, and then
apply the case assignment rule, below (Takeuchi et
(2) LCS transformation
(3) Surface generation
LCS dictionary
[x? ACT ON y?]
[BECOME [z BE WITH [y MOVE FROM x TO z]]]
Paraphrased sentence
Input sentence
ukeru (receive)
shigeki-suru (inspire)
Ken-ga (Ken-NOM) eiga-ni (film-DAT)
shigeki-o (inspiration-ACC) uketa (receive-PAST).
[[film]x? ACT ON [Ken]y?]
[BECOME [[Ken]z BE WITH
    [[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[BECOME [[Ken]z BE WITH]] +
LCSV0
LCSV1
LCSN0
LCSN1
Ken-ga (Ken-NOM) eiga-ni (film-DAT)
               shigeki-s-are-ta (inspire-PAS, PAST).
(1) Semantic analysis
Figure 2: The LCS-based paraphrasing model.
al., 2001), to obtain its LCS representation LCS
V 1
:
Case assignment rule:
? In the case of the LCS
V 0
having argument x,
fill the leftmost argument of the LCS
V 0
with
the nominative case of the input, the second
leftmost with the accusative, and the rest with
the dative.
? Otherwise, fill arguments y and z of the LCS
V 0
with the nominative and the dative, respec-
tively.
In the example shown in Figure 2, the nominative
?Ken? fills the leftmost argument z. Accordingly,
the accusative ?shigeki (inspiration)? and the dative
?eiga (film)? fill y and x, respectively.
(8) s. Ken-ga eiga-ni shigeki-o uketa.
Ken-NOM film-DAT inspiration-ACC receive-PAST
Ken received inspiration from the film.
LCS
V 0
[BECOME [z BE WITH [y MOVE FROM x
TO z]]]
LCS
V 1
[BECOME [[Ken]z BE WITH [[inspiration]y
MOVE FROM [film]x TO [Ken]z]]]
4.2 LCS transformation
The second step of our paraphrasing model
matches the resultant LCS representation (LCS
V 1
in Figure 2) with the LCS of the nominalized verb
(LCS
N0
) to generate the target LCS representation
(LCS
N1
). Figure 3 shows a more detailed view of
this process for the example shown in Figure 2.
4.2.1 Predicate matching
The first step is to determine the predicate in
LCS
V 1
that should be matched with the predicate
in LCS
N0
. Assuming that only the agentivity is rel-
evant to the selection of the voice in the paraphras-
ing of LVC, which is our primary concern, we clas-
sify the semantic predicates into the following two
classes:
Agentive predicates: ?CONTROL,? ?ACT ON,?
?ACT,? ?BE AGAINST,? and ?MOVE FROM
TO.?
[[film]x? ACT ON [Ken]y?][BECOME [[Ken]z BE WITH]] +
[BECOME [[Ken]z BE WITH
    [[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[x? ACT ON y?]
(b) Argument matching(a) Predicate matching
(c) Attaching the remaining structure
LCSN0
LCSN1
LCSV1
Figure 3: LCS transformation.
State of affair predicates: ?MOVE TO,? ?BE
AT,? and ?BE WITH.?
Aspectual predicates: ?BECOME.?
We also assume that any pair of predicates of
the same class is allowed to match, and that the
aspectual predicates are ignored. In our example,
?MOVE FROM TO? matches ?ACT ON,? as shown
in Figure 3.
LCS representations have right-branching (or
right-embedding) structures. Since inner-embedded
predicates denote the state of affairs, they take pri-
ority in the matching. In other words, the matching
proceeds from the rightmost inner predicates to the
outer predicates.
Having matched the predicates, we then fill each
argument slot in LCS
N0
with its corresponding ar-
gument in LCS
V 1
. In Figure 3, argument z is
matched with y?, and x with x?. As a result, ?Ken?
comes to the y? slot and ?eiga (film)? comes to the
x? slot8.
This process is repeated until the leftmost predi-
cate in LCS
N0
or that in LCS
V 1
is matched.
4.2.2 Treatment of non-transfered predicates
If LCS
V 1
has any non-transfered predicates when
the predicate matching has been completed, they
represent the semantic content that is not covered by
LCS
N1
and which needs to be lexicalized by aux-
iliary linguistic devices such as voice auxiliaries.
In the case of Figure 3, [BECOME [[Ken]z BE WITH]]
in LCS
V 1
remains non-transfered. In such a case,
we attach the non-transfered predicates to LCS
N0
,
which are then lexicalized by auxiliaries in the next
step, the surface generation.
4.3 Surface generation
We again apply the aforementioned case assignment
rule to generate a sentence from the resultant LCS
representation. In this process, the model makes the
final decisions on the selection of the voice and the
reassignment of the cases, according to the follow-
ing decision list:
8When an argument is filled with another LCS, arguments
within the inner LCS are also matched. Likewise, with regard
to an assumption that the input sentences are periphrastic, we
introduced some exceptional rules. That is, arguments filled
with the implicit filler represented by ?FILLED? or the target
nominalized verb N are never matched, and ?Goal? in LCS
V 1
can be matched to ?Theme? in LCS
N0
.
1. If the attached predicate is filled with the same
argument as the leftmost argument in LCS
N1
,
the ?active? voice is selected and the case
structure is left as is.
2. If the argument of the attached predicate has
the same value as either z? or y? in LCS
N1
,
lexicalization is performed to make the argu-
ment a subject. Therefore, the ?passive? voice
is selected and case alternation (passivization)
is applied.
3. If the attached predicate is ?BE WITH? and its
argument has the same value as x? in LCS
N1
,
the ?causative? voice is selected and case alter-
nation (causativizaton) is applied.
4. If the attached predicate is an agentive predi-
cate, and its argument is filled with a value dif-
ferent from those of the other arguments, then
the ?causative? voice is selected and case alter-
nation (causativization) is applied.
5. Otherwise, no modification is applied.
Since the example in Figure 2 satisfies the second
condition, the model chooses ?s-are-ru (passive)?
and passivizes the sentence so that ?Ken? fills the
nominative case.
(9) LCS
N1
[BECOME [[Ken]z BE WITH]]
+ [[film]x? ACT ON [Ken]y?]
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT inspire-PAS, PAST
Ken was inspired by the film.
5 Experiment
5.1 Paraphrase generation and evaluation
To empirically evaluate our paraphrasing model and
the LCSdic, and to clarify the remaining problems,
we analyzed a set of automatically generated para-
phrase candidates. The sentences used in the exper-
iment were collected in the following way:
Step 1. From the 876,101 types of triplet ?n, c, v?
collected in Section 3.2, 23,608 types of
?n, c, v? were extracted, whose components, n
and ?c, v?, are listed in the LCSdic.
Step 2. For each of the 245 most frequent ?n, c, v?,
the 3 most frequent simple clauses includ-
ing the ?n, c, v? were extracted from the cor-
pus from which ?n, c, v? s were extracted in
Section 3.2. As a result, we collected 735 sen-
tences.
Step 3. We input these 735 sentences into our para-
phrasing model, and then automatically gener-
ated paraphrase candidates. When more than
one LCS is assigned to a verb in the LCSdic
due to its polysemy or ergative verb such as
?kaifuku-suru (recover),? our model generates
all the possible paraphrase candidates. As a re-
sult, 825 paraphrase candidates, that is, at least
one for each input, were generated.
Table 3: Error sources
Correct candidates 621 (75.8%)
Erroneous candidates 198 (24.2%)
Definition of LCS 30
LCS for light-verb 24
LCS for nominalized verb 6
Paraphrasing model 61
LCS transformation algorithm 59
Treatment of ?suru (to do)? 2
Ambiguity 107
Ambiguous thematic role of dative 78
Recognition of LVC 24
Selection of transitive/intransitive 5
We manually classified the resultant 825 para-
phrase candidates into 621 correct and 198 erro-
neous candidates. The remaining 6 candidates were
not classified. The precision of the paraphrase gen-
eration was 75.8% (621 / 819).
5.2 Error analysis
To clarify the cause of the erroneous paraphrases,
we manually classified 198 erroneous paraphrase
candidates. Table 3 lists the error sources.
5.2.1 LCS transformation algorithm
The experiment came close to confirming that the
right-first matching algorithm in our paraphrasing
model operates correctly. Unfortunately, the match-
ing rules produced some erroneous paraphrases in
LCS transformation.
Errors in predicate matching: To paraphrase
(10s) below, ?CONTROL? in LCS
V 1
must be
matched with ?CONTROL? in LCS
N0
, and x to x?.
However, our model first matched ?CONTROL? in
LCS
V 1
with ?MOVE FROM TO? in LCS
N0
. Thus,
x was incorrectly matched with z? and x? remained
empty. The desired form of LCS
N1
is shown in
(11).
(10) s. kacho-ga buka-ni
section-chief-NOM subordinate-DAT
shiji-o dasu.
order-ACC issue-PRES
The section chief issues orders to his subordinates.
(N=?order?, V =?issue?)
LCS
V 1
[[chief ]x CONTROL [BECOME [[order]y
BE AT [subordinate]z]]]
LCS
N0
[x? CONTROL [y? MOVE FROM z? TO
[FILLED]]]
LCS
N1
?[x? CONTROL [[subordinate]y? MOVE
FROM [chief ]z? TO [FILLED]]]
(11) LCS
N1
[[chief ]x? CONTROL [y? MOVE FROM
[subordinate] TO [FILLED]]]
This error was caused by the mis-matching of
?CONTROL? with ?MOVE FROM TO.? Although
we regard some predicates as being in the same
classes as those described in Section 4.2.1, these
need to be considered carefully. In particular
?MOVE FROM TO? needs further investigation be-
cause it causes many errors whenever it has the
?FILLED? argument.
Errors in argument matching: Even if all the
predicates are matched properly, there would still
be a chance of errors being caused by incorrect ar-
gument matching. With the present algorithm, z
can be matched with y? if and only if z? contains
?FILLED.? In the case of (12), however, z has to
be matched with y?, even though z? is empty. The
desired form of LCS
N1
is shown in (13).
(12) s. Jikan-ni seigen-ga aru.
time-DAT limitation-NOM exist-PRES
There is a time limitation.
(N=?limitation?, V =?exist?)
LCS
V 1
[BECOME [[limitation]y BE AT [time]z]]
LCS
N0
[x? CONTROL [BECOME [y? BE AT z?]]]
LCS
N1
?[x? CONTROL [BECOME [y? BE AT
[time]z?]]]
(13) LCS
N1
[x? CONTROL [BECOME [[timey]y? BE AT
z?]]]
5.2.2 Ambiguous thematic role of dative
In contrast to dative cases in English, in Japanese,
the dative case has ambiguity. That is, it can be a
complement to the verb or an adjunct9. However,
since LCS is not capable of determining whether the
case is a complement or an adjunct, z is occasion-
ally incorrectly filled with an adjunct. For exam-
ple, ?medo-ni? in (14s) should not fill z, because it
acts as an adverb, even though it consists of a noun,
?medo (prospect)? and a case particle for the dative.
We found that 78 erroneous candidates constitute
this most dominant type of errors.
(14) s. Kin?you-o medo-ni sagyo-o susumeru.
Friday-NOM by-DAT work-ACC carry on-PRES
I plan to finish the work by Friday.
(N=?work?, V =?carry?)
LCS
V 0
[x CONTROL [BECOME [y BE AT z]]]
LCS
V 1
?[x CONTROL [BECOME [[work]y BE AT
[by]z]]]
The ambiguity of dative cases in Japanese has
been discussed in the literature of linguistics and
some natural language processing tasks (Muraki,
1991). To date, however, a practical compli-
ment/adjunct classifier has not been established. We
plan to address this topic in our future research.
Preliminary investigation revealed that only cer-
tain groups of nouns can constitute both compli-
ments and adjuncts according to the governing verb.
Therefore, generally whether a word acts as a com-
plement is determined without combining it with the
verb.
9(Muraki, 1991) classifies dative cases into 11 thematic
roles that can be regarded as complements. In contrast, there
is no typology of dative cases that act as adjuncts.
5.2.3 Recognition of LVC
In our model, we assume that a triplet ?n, c, v? con-
sisting of a nominalized verb n and a light-verb tu-
ple ?c, v? from our vocabulary lists (see Section 3.2)
always act as an LVC. However, not only the triplet
itself but also its context sometimes affects whether
the given triplet can be paraphrased. For exam-
ple, we regard ?imi-ga aru? as an LVC, because the
nominalized verb ?imi? and the tuple ??ga?, ?aru??
appear in the vocabulary lists. However, the ?n, c, v?
in (15s) does not act as an LVC, while the same
triplet in (16s) does.
(15) s. Sanka-suru-koto-ni imi-ga aru.
to participate-DAT meaning-NOM exist-PRES
There is meaning in participating.
t.?Sanka-suru-koto-o imi-suru.
to participate-ACC mean-ACT, PRES
?It means to participate in it.
(16) s. ?kennel?-niwa inugoya-toiu
?kennel?-TOP doghouse-OF
imi-ga aru.
meaning-NOM exist-PRES
?kennel? has the meaning of doghouse.
t. ?kennel?-wa inugoya-o imi-suru.
?kennel?-TOP doghouse-ACC mean-ACT, PRES
?kennel? means doghouse.
The above difference is caused by the polysemy
of the nominalized verb ?imi? that denotes ?worth?
in the context of (15s), but ?meaning? in (16s).
Although incorporating word sense disambiguation
using contextual clues complicates our model, in
fact only a limited number of nominalized verbs are
polysemous. We therefore expect that we can list
them up and use this as a trigger for making a deci-
sion as to whether we need to take the context into
account. Namely, given a ?n, c, v?, we would be
able to classify it into (a) a main verb phrase, (b) a
delicate case in terms of the dependence of its con-
text, and (c) an LVC.
We can adopt a different approach to avoiding
incorrect paraphrase generation. As described in
Section 5.1, our model generates all the possible
paraphrase candidates when more than one LCS is
assigned to a verb. Similarly, our approach can be
extended to (i) over-generate paraphrase candidates
by considering the polysemy of not only assigned
LCS types, but also that of nominalized verbs (see
(15s) and (16s)) and whether the given ?n, c, v? is
an LVC, and (ii) revise or reject the incorrect candi-
dates by using handcrafted solid rules or statistical
language models.
6 Conclusion and future work
In this paper, we presented an LCS-based para-
phrasing model for LVCs and an extension of an ex-
isting LCS dictionary. Our model achieved an accu-
racy of 75.8% in selecting the voice and reassigning
the cases.
To make our paraphrasing model more accurate,
further analysis is needed, especially for the LCS
transformation stage described in Section 4.2. Sim-
ilarly, several levels of disambiguation should also
be solved. The Japanese LCS typology has to be
refined from the theoretical point of view. For ex-
ample, since extensions are no more than human in-
tuition, we must discuss how we can assign LCSs
for given verbs based on explicit language tests, as
described in (Takeuchi et al, 2001).
In future research, we will also extend our LCS-
based approach to other classes of paraphrases that
exhibit some regularity, such as verb alteration and
compound noun decomposition as shown in (17)
and (18), below. LCS has been discussed as a
means of explaining the difference between transi-
tive/intransitive verbs, and the construction of com-
pounds. Therefore, our next goal is to show the ap-
plicability of LCS through practical tasks, namely,
paraphrasing.
(17) s. Jishin-ga building-o kowashita.
earthquake-NOM building-DAT destroy-PAST
The earthquake destroyed the building.
t. Jishin-de building-ga kowareta.
earthquake-LOC building-NOM be destroyed-PAST
The building was destroyed in the earthquake.
(18) s. Kare-wa kikai
he-TOP machine-
sousa-ga jouzu-da.
operation-NOM good-COPULA
He is good at operating the machine.
t. Kare-wa kikai-o jouzu-ni sousa-suru.
he-TOP machine-DAT well-ADV operate-PRES
He operates machines well.
References
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of the
39th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 50?57.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for language-
impaired readers. In Proceedings of the 9th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 269?270.
B. J. Dorr, J. Garman, and A. Weinberg. 1995. From
syntactic encodings to thematic roles: building lexi-
cal entries for interlingual MT. Machine Translation,
9(3):71?100.
M. Dras. 1999. Tree adjoining grammar and the reluc-
tant paraphrasing of text. Ph.D. thesis, Department of
Computing, Macquarie University.
K. Inui and M. Nogami. 2001. A paraphrase-based
exploration of cohesiveness criteria. In Proceedings
of the 8th European Workshop on Natulal Language
Generation (EWNLG), pages 101?110.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the 2nd International
Workshop on Paraphrasing: Paraphrase Acquisition
and Applications (IWP), pages 9?16.
L. Iordanskaja, R. Kittredge, and A. Polgue`re. 1991.
Lexical selection and paraphrase in a meaning-text
generation model. In Paris et al (Eds.) Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293?312. Kluwer Aca-
demic Publishers.
T. Kageyama, editor. 1996. Verb semantics. Kuroshio
Publishers. (in Japanese).
N. Kaji and S. Kurohashi. 2004. Recognition and para-
phrasing of periphrastic and overlapping verb phrases.
In Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC) Work-
shop on Methodologies and Evaluation of Multiword
Units in Real-world Application.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of 6th Conference on Natural Language Learning
(CoNLL), pages 63?69.
B. Levin. 1993. English verb classes and alternations:
a preliminary investigation. Chicago Press.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question answering. Natural Language Engineer-
ing, 7(4):343?360.
I. Mel?c?uk and A. Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
S. Muraki. 1991. Various aspects of Japanese verbs.
Hitsuji Syobo. (in Japanese).
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: extracting para-
phrases and generating new sentences. In Proceed-
ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 102?109.
S. Sato. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Y. Shinyama and S. Sekine. 2003. Paraphrase acquisi-
tion for information extraction. In Proceedings of the
2nd International Workshop on Paraphrasing: Para-
phrase Acquisition and Applications (IWP), pages 65?
71.
K. Takeuchi, K. Uchiyama, S. Yoshioka, K. Kageura,
and T. Koyama. 2001. Categorising deverbal nouns
based on lexical conceptual structure for analysing
Japanese compounds. In Proceedings of IEEE Sys-
tem, Man, and Cybernetics Conference, pages 904?
909.
Effects of Related Term Extraction in Transliteration into Chinese
HaiXiang Huang Atsushi Fujii
Graduate School of Library, Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
{lectas21,fujii}@slis.tsukuba.ac.jp
Abstract
To transliterate foreign words, in Japanese
and Korean, phonograms, such as Katakana
and Hangul, are used. In Chinese, the
pronunciation of a source word is spelled
out using Kanji characters. Because Kanji
is ideogrammatic representation, different
Kanji characters are associated with the
same pronunciation, but can potentially con-
vey different meanings and impressions. To
select appropriate Kanji characters, an ex-
isting method requests the user to provide
one or more related terms for a source word,
which is time-consuming and expensive. In
this paper, to reduce this human effort, we
use the World Wide Web to extract related
terms for source words. We show the effec-
tiveness of our method experimentally.
1 Introduction
Reflecting the rapid growth of science, technology,
and economies, new technical terms and product
names have progressively been created. These new
words have also been imported into different lan-
guages. There are two fundamental methods for im-
porting foreign words into a language.
In the first method?translation?the meaning of
the source word in question is represented by an ex-
isting or new word in the target language.
In the second method?transliteration?the pronun-
ciation of the source word is represented by using
the phonetic alphabet of the target language, such as
Katakana in Japanese and Hangul in Korean. Tech-
nical terms and proper nouns are often transliterated.
In Chinese, Kanji is used to spell out both conven-
tional Chinese words and foreign words. Because
Kanji is ideogrammatic, an individual pronunciation
can be represented by more than one character. If
several Kanji strings are related to the same pronun-
ciation of the source word, their meanings will be
different and convey different impressions.
For example, ?Coca-Cola? can be represented by
different Kanji strings in Chinese with similar pro-
nunciations, such as ?????? and ??????.
The official transliteration is ??????, which
comprises ??? (tasty)? and ??? (pleasant)?, and
is therefore associated with a positive connotation.
However, ?????? is associated with a nega-
tive connotation because this word includes ??? ?,
which is associated with ?choking?.
For another example, the official transliteration of
the musician Chopin?s name in Chinese is ????,
where ??? is commonly used for Chinese family
names. Other Kanji characters with the same pro-
nunciation as ??? include ???. However, ???,
which means ?to disappear?, is not ideal for a per-
son?s name.
Thus, Kanji characters must be selected carefully
during transliteration into Chinese. This is espe-
cially important when foreign companies intend to
introduce their names and products into China.
In a broad sense, the term ?transliteration? has
been used to refer to two tasks. The first task is
transliteration in the strict sense, which creates new
words in a target language (Haizhou et al, 2004;
Wan and Verspoor, 1998; Xu et al, 2006). The sec-
ond task is back-transliteration (Knight and Graehl,
1998), which identifies the source word correspond-
643
ing to an existing transliterated word. Both tasks
require methods that model pronunciation in the
source and target languages.
However, by definition, in back-transliteration,
the word in question has already been transliter-
ated and the meaning or impression of the source
word does not have to be considered. Thus, back-
transliteration is outside the scope of this paper. In
the following, we use the term ?transliteration? to
refer to transliteration in the strict sense.
Existing transliteration methods for Chi-
nese (Haizhou et al, 2004; Wan and Verspoor,
1998), which aim to spell out foreign names of
people and places, do not model the impression the
transliterated word might have on the reader.
Xu et al (2006) proposed a method to model both
the impression and the pronunciation for transliter-
ation into Chinese. In this method, impression key-
words that are related to the source word are used.
However, a user must provide impression keywords,
which is time-consuming and expensive.
In this paper, to reduce the amount of human ef-
fort, we propose a method that uses the World Wide
Web to extract related terms for source words.
2 Overview
Figure 1 shows our transliteration method, which
models pronunciation, impression, and target lan-
guage when transliterating foreign words into Chi-
nese. Figure 1 is an extension of the method pro-
posed by Xu et al (2006) and the part surrounded by
a dotted line is the scheme we propose in this paper.
We will explain the entire process using Figure 1.
There are two parts to the input for our method.
First, a source word to be transliterated into Chi-
nese is requested. Second, the category of the source
word, such as ?company? or ?person?, is requested.
The output is one or more Kanji strings.
Using the pronunciation model, the source word
is converted into a set of Kanji strings whose pro-
nunciation is similar to that of the source word. Each
of these Kanji strings is a transliteration candidate.
Currently, we use Japanese Katakana words as
source words, because Katakana words can be easily
converted into pronunciations using the Latin alpha-
bet. In Figure 1, the Katakana word ?epuson (EP-
SON)? is used as an example source word. How-
Source word
Related term(s)
Transliteration candidates Kanji characters
Category of source word
Ranked list of transliteration candidates
pronunciation model impression model
ranking candidates
language model
????(epuson)
??(like)??(popularize)??(general)?(good)
???(company)
????????? ...
?????? ??????????
World Wide Web
Kanji characters
Figure 1: Overview of our transliteration method.
ever, in principle, any language that uses a phonetic
script can be a source language for our method.
Using the impression model, one or more related
terms are converted into a set of Kanji characters.
In Xu et al (2006), one or more words that de-
scribe the impression of the source word are used as
related terms (i.e., impression keywords). Because
impression keywords are given manually, users must
have a good command of Chinese. In addition, the
task of providing impression keywords is expensive.
We solve these problems by automatically extracting
terms related to the source word from the Web.
Unlike Xu at al. (2006), the language model for
the category of the source word is used. For ex-
ample, if the category is ?person?, Kanji characters
that are often used for personal names in Chinese are
preferably used for the transliteration.
Because of the potentially large number of se-
lected candidates, we need to rank the candidates.
We model pronunciation, impression, and target lan-
guage in a probabilistic framework, so that candi-
dates are sorted according to their probability score.
In practice, the Kanji characters derived via the im-
pression and language models are used to re-rank the
candidates derived via the pronunciation model.
3 Probabilistic Transliteration Model
Given a romanized source word R, a set of related
terms W , and the category of the source word C,
our purpose is to select the Kanji string K that max-
imizes P (K|R,W,C), which is evaluated as shown
in Equation (1), using Bayes?s theorem.
644
P (K|R,W,C)
= P (R,W,C|K)?P (K)
P (R,W,C)
? P (R|K)?P (W |K)?P (C|K)?P (K)
P (R,W,C)
? P (R|K)?P (W |K)?P (C|K)?P (K)
= P (R|K)?P (W |K)?P (C,K)
(1)
Xu et al (2006) did not consider the category of the
source word and computed P (K|R,W ).
In the third line of Equation (1), we assume the
conditional independence of R, W , and C given K.
In the fourth line, we omit P (R,W,C), which is
independent of K. This does not affect the rela-
tive rank of Kanji strings, when ranked in terms of
P (K|R,W,C). If a user intends to select more than
one Kanji string, those Ks associated with higher
probabilities should be selected. In Figure 1, R, W ,
and C are ?epuson?, ?????????? and ??
???, respectively, and a K is ?????.
In Equation (1), P (K|R,W,C) can be approx-
imated by the product of P (R|K), P (W |K), and
P (C,K). We call these three factors the pronuncia-
tion, impression, and language models, respectively.
The implementation of P (R|K) and P (W |K) is
the same as in Xu et al (2006). While P (R|K) has
commonly been used in the literature, the basis of
P (W |K) should perhaps be explained. P (W |K) is
computed using co-occurrence frequencies of each
word in W and each character in K, for which we
extracted co-occurrences of a word and a Kanji char-
acter from a dictionary of Kanji in Chinese. Please
see Xu et al (2006) for details. However, unlike Xu
et al (2006), in which W was provided manually,
we automatically extract W from the Web.
While Xu et al (2006) did not use the language
model, we compute P (C,K) by Equation (2).
P (C,K) = P (C)?P (K|C) ? P (K|C) (2)
We omit P (C), which is independent of K. Thus,
we compute P (K|C), which is the probability that
a Kanji string K is selected given category C.
To compute P (K|C), we decompose K into sin-
gle Kanji characters. We used a character unigram
model and produced the following three language
models.
? general model: one month of newspaper arti-
cles in the PFR corpus1 were used. In this
model, 4 540 character types (12 229 563 to-
kens) are modeled.
? company model: a list of 22 569 company
names in CNLP (Chinese Natural Language
Processing)2 was used. In this model, 2 167
character types (78 432 tokens) are modeled.
? person model: a list of 38 406 personal names
in CNLP was used. In this model, 2 318 char-
acter types (104 443 tokens) are modeled.
To extract Kanji characters from the above corpus
and lists, we performed morphological analysis by
SuperMorpho3 and removed functional words and
symbols. While the general model is not adapted to
any specific category, the other models are adapted
to the company and person categories, respectively.
Although the effect of adapting language models has
been explored in spoken language processing, no at-
tempt has been made for transliteration.
4 Extracting Related Terms
To extract related terms for a source word, we used
Wikipedia4, which is a free encyclopedia on the Web
and includes general words, persons, places, compa-
nies, and products, as headwords. We extracted re-
lated term candidates for a source word as follows.
1. We consulted the Japanese Wikipedia for the
source word and obtained the result page.
2. We deleted HTML tags from the result page
and performed morphological analysis by
ChaSen5.
3. We extracted nouns and adjectives as related
term candidates.
We used mutual information (Turney, 2001) to
measure the degree of relation between the source
word and a related term candidate by Equation (3).
I(X,Y ) = log P (X,Y )
P (X) ? P (Y )
(3)
1http://icl.pky.edu.cn/
2http://www.nlp.org.cn/
3http://www.omronsoft.com/
4http://ja.wikipedia.org/wiki/
5http://chasen.naist.jp/hiki/ChaSen/
645
X and Y denote the source word and a related term
candidate, respectively. P (X) and P (Y ) denote
probabilities of X and Y , respectively. P (X,Y ) de-
notes the joint probability of X and Y .
To estimate the above three probabilities, we fol-
lowed the method proposed by Turney (2001). We
used the Yahoo!JAPAN6 search engine and replaced
P (A) in Equation (3) with the number of pages re-
trieved by the query A. Here, ?A? can be ?X?, ?Y ?,
or ?X and Y ?. Then, we selected up to 10 Y s with
the greatest I(X,Y ) and translated them into Chi-
nese using the Yahoo!JAPAN machine translation
system.
Table 1 shows examples of related terms for the
source word ??? (mass)?, such as ??? (cere-
mony)? and ? ?? (dedication)?. Irrelevant candi-
dates, such as ? ? (meeting)? and ??? (thing)?,
were discarded successfully.
Table 1: Example of related terms for ??? (mass)?.
Extracted related terms Discarded candidates
Japanese English Japanese English
?? ceremony ? meeting
?? dedication ?? thing
?? bishop ?? meeting
?? church ?? join
5 Experiments
5.1 Method
To evaluate the effectiveness of the related term ex-
traction in the transliteration, we compared the ac-
curacy of the following three methods.
? A combination of the pronunciation and lan-
guage models that does not use the impression
model, P (W |K), in Equation (1),
? Our method, which uses Equation (1) and uses
automatically extracted related terms as W ,
? Equation (1), in which manually provided im-
pression keywords are used as W .
To make the difference between the second and
third methods clear, we use the terms ?related term
(RT)? and ?impression keyword (IK)? to refer to
6http://www.yahoo.co.jp/
words provided automatically and manually, respec-
tively. Then, we call the above three methods
?PL?, ?PL+RT?, and ?PL+IK?, respectively. PL and
PL+IK are the lower bound and the upper bound of
the expected accuracy, respectively. PL+IK is the
same as in Xu et al (2006), but the language model
is adapted to the category of source words.
To produce test words for the transliteration, we
first collected 210 Katakana words from a Japanese?
Chinese dictionary. These 210 words were also used
by Xu et al (2006) for experiments. We then con-
sulted Wikipedia for each of the 210 words and se-
lected 128 words that were headwords in Wikipedia,
as test words. Details of the 128 test words are
shown in Table 2.
Table 2: Categories of test words.
Category #Words
Example word
Japanese Chinese English
General 24 ????? ??? angel
Company 35 ???? ??? Intel
Product 27 ???? ?? Audi
Person 13 ???? ?? Chopin
Place 29 ???? ??? Ohio
We selectively used the three language models ex-
plained in Section 3. We used the general model
for general words. We used the company model for
company and product names, and used the person
model for person and place names. A preliminary
study showed that the language model adaptation
was generally effective for transliteration. However,
because the focus of this paper is the related term
extraction, we do not describe the evaluation of the
language model adaptation.
Two Chinese graduate students who had a good
command of Japanese served as assessors and pro-
duced reference data, which consisted of impression
keywords used for PL+IK and correct answers for
the transliteration. Neither of the assessors was an
author of this paper. The assessors performed the
same task for the 128 test words independently, to
enhance the objectivity of the evaluation.
We produced the reference data via the following
procedure that is the same as that of Xu et al (2006).
First, for each test word, each assessor pro-
vided one or more impression keywords in Chinese.
We did not restrict the number of impression key-
646
words per test word; the number was determined
by each assessor. We provided the assessors with
the descriptions for the test words from the source
Japanese?Chinese dictionary, so that the assessors
could understand the meaning of each test word.
Second, for each test word, we applied the three
methods (PL, PL+RT, and PL+IK) independently,
which produced three lists of ranked candidates.
Third, for each test word, each assessor identi-
fied one or more correct transliterations, according
to their impression of the test word. It was impor-
tant not to reveal to the assessors which method pro-
duced which candidates. By these means, we se-
lected the top 100 transliteration candidates from the
three ranked lists. We merged these candidates, re-
moved duplications, and sorted the remaining can-
didates by character code. The assessors judged
the correctness of up to 300 candidates for each
test word. The average number of candidates was
36 976.
The resultant reference data were used to evaluate
the accuracy of each method in ranking translitera-
tion candidates. We used the average rank of correct
answers in the list as the evaluation measure. If more
than one correct answer was found for a single test
word, we first averaged the ranks of these answers
and then averaged the ranks over the test words.
For each test word, there was more than one type
of ?correct answer?, as follows:
(a) transliteration candidates judged as correct by
either of the assessors independently,
(b) transliteration candidates judged as correct by
both assessors,
(c) transliteration defined in the source Japanese?
Chinese dictionary.
In (a), the coverage of correct answers is the largest,
whereas the objectivity of the judgment is the low-
est. In (c), the objectivity of the judgment is the
largest, whereas the coverage of correct answers is
the lowest. In (b), where the assessors did not dis-
agree about the correctness, the coverage of the cor-
rectness and the objectivity are in between.
The number of test words was 128 for both (a) and
(c), but 76 for (b). The average numbers of correct
answers were 1.65, 1.04, and 1 for (a), (b), and (c),
respectively.
5.2 Results and Analyses
Table 3 shows the average rank of correct answers
for different cases. Looking at Table 3, for certain
categories, such as ?Place?, when the impression
model was used, the average rank was low. How-
ever, on average, the average rank for PL+RT was
lower than that for PL+IK, but was higher than that
for PL, irrespective of the answer type.
Figures 2 and 3 show the distribution of correct
answers for different ranges of ranks, using answer
types (a) and (c) in Table 3, respectively. Because
the results for types (a) and (b) were similar, we
show only the results of type (a), for the sake of con-
ciseness. In Figure 2, the number of correct answers
in the top 10 for PL+RT was smaller than that for
PL+IK, but was greater than that for PL.
In Figure 3, the number of correct answers in the
top 10 for PL+RT was greater than those for PL and
PL+IK. Because in Figure 3, the correct answers
were defined in the dictionary and were independent
of the assessor judgments, PL+IK was not as effec-
tive as in Figure 2.
In summary, the use of automatically extracted re-
lated terms was more effective than the method that
does not use the impression model. We also reduced
the manual cost of providing impression keywords,
while maintaining the transliteration accuracy.
Table 4 shows examples of related terms or im-
pression keywords for answer type (c). In Table
4, the column ?Rank? denotes the average rank
of correct answers for PL+RT and PL+IK, respec-
tively. For ??? (mass)?, the rank for PL+RT was
higher than that for PL+IK. However, for ????
? (the State of Qatar)?, the rank for PL+RT was
lower than that for PL+IK. One reason for this is
that most related terms for PL+RT were names of
countries that border Qatar, which do not describe
Qatar well, compared with impression keywords for
PL+IK, such as ??? (desert)? and ? ?? (oil)?.
This example indicates room for improvement in the
related term extraction algorithm.
6 Conclusion
For transliterating foreign words into Chinese, the
pronunciation of a source word is spelled out with
Kanji characters. Because Kanji is an ideogram-
matic script, different Kanji characters are associ-
647
Table 3: Average rank of correct answers for different methods in different cases.
Category
Answer type (a) Answer type (b) Answer type (c)
PL PL+RT PL+IK PL PL+RT PL+IK PL PL+RT PL+IK
General 189 165 167 44 49 52 84 61 65
Company 232 208 203 33 29 27 317 391 325
Product 197 175 166 34 27 21 313 198 198
Person 98 69 44 4 4 4 114 154 75
Place 85 133 95 13 14 16 76 98 89
Avg. 160 150 135 26 25 24 181 160 150
?
??
??
??
???
???
???? ????? ?????? ??????? ???????
?
???????
?? ?????????
????????
????????
???? PL PL?RT PL?IK
Figure 2: Rank for correct answer type (a).
???
????
?????
???
???? ????? ?????? ??????? ???????? ????????
? ?????????
????????
????????
???? PL PL?RT PL?IK
Figure 3: Rank for correct answer type (c).
Table 4: Examples of related terms and impression keywords used for experiments.
Source word Answer Method Rank Examples of related terms or impression keywords
??
??
PL+RT 8 ?? (ceremony),?? (bishop),?? (dedication),?? (church)
(mass) PL+IK 10 ?? (ceremony),?? (bishop),?? (belief),?? (church)
????
???
PL+RT 103 ??? (State of Kuwait),?? (Republic of Yemen)
(State of Qatar) PL+IK 61 ??? (Arab),?? (desert),?? (oil),?? (dryness)
ated with the same pronunciation, but can poten-
tially convey different meanings and impressions.
In this paper, to select appropriate characters for
transliterating into Chinese, we automatically ex-
tracted related terms for source words using the
Web. We showed the effectiveness of our method
experimentally.
References
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 160?167.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning,
pages 419?502.
Stephen Wan and Cornelia Maria Verspoor. 1998. Auto-
matic English-Chinese name transliteration for devel-
opment of multilingual resources. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics and the 17th International Con-
ference on Computational Linguistics, pages 1352?
1356.
LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006.
Modeling Impression in Probabilistic Transliteration
into Chinese. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 242?249.
648
Organizing Encyclopedic Knowledge based on the Web and its
Application to Question Answering
Atsushi Fujii
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
CREST, Japan Science and
Technology Corporation
fujii@ulis.ac.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
We propose a method to generate large-scale
encyclopedic knowledge, which is valuable
for much NLP research, based on the Web.
We first search the Web for pages contain-
ing a term in question. Then we use lin-
guistic patterns and HTML structures to ex-
tract text fragments describing the term. Fi-
nally, we organize extracted term descrip-
tions based on word senses and domains. In
addition, we apply an automatically gener-
ated encyclopedia to a question answering
system targeting the Japanese Information-
Technology Engineers Examination.
1 Introduction
Reflecting the growth in utilization of the World Wide
Web, a number of Web-based language processing
methods have been proposed within the natural lan-
guage processing (NLP), information retrieval (IR)
and artificial intelligence (AI) communities. A sam-
ple of these includes methods to extract linguistic
resources (Fujii and Ishikawa, 2000; Resnik, 1999;
Soderland, 1997), retrieve useful information in re-
sponse to user queries (Etzioni, 1997; McCallum et
al., 1999) and mine/discover knowledge latent in the
Web (Inokuchi et al, 1999).
In this paper, mainly from an NLP point of view,
we explore a method to produce linguistic resources.
Specifically, we enhance the method proposed by Fu-
jii and Ishikawa (2000), which extracts encyclopedic
knowledge (i.e., term descriptions) from the Web.
In brief, their method searches the Web for pages
containing a term in question, and uses linguistic ex-
pressions and HTML layouts to extract fragments de-
scribing the term. They also use a language model to
discard non-linguistic fragments. In addition, a clus-
tering method is used to divide descriptions into a spe-
cific number of groups.
On the one hand, their method is expected to en-
hance existing encyclopedias, where vocabulary size
is relatively limited, and therefore the quantity prob-
lems has been resolved.
On the other hand, encyclopedias extracted from the
Web are not comparable with existing ones in terms of
quality. In hand-crafted encyclopedias, term descrip-
tions are carefully organized based on domains and
word senses, which are especially effective for human
usage. However, the output of Fujii?s method is simply
a set of unorganized term descriptions. Although clus-
tering is optionally performed, resultant clusters are
not necessarily related to explicit criteria, such as word
senses and domains.
To sum up, our belief is that by combining extrac-
tion and organization methods, we can enhance both
quantity and quality of Web-based encyclopedias.
Motivated by this background, we introduce an or-
ganization model to Fujii?s method and reformalize
the whole framework. In other words, our proposed
method is not only extraction but generation of ency-
clopedic knowledge.
Section 2 explains the overall design of our ency-
clopedia generation system, and Section 3 elaborates
on our organization model. Section 4 then explores
a method for applying our resultant encyclopedia to
NLP research, specifically, question answering. Sec-
tion 5 performs a number of experiments to evaluate
our methods.
2 System Design
2.1 Overview
Figure 1 depicts the overall design of our system,
which generates an encyclopedia for input terms.
Our system, which is currently implemented for
Japanese, consists of three modules: ?retrieval,? ?ex-
traction? and ?organization,? among which the orga-
nization module is newly introduced in this paper. In
principle, the remaining two modules (?retrieval? and
?extraction?) are the same as proposed by Fujii and
Ishikawa (2000).
In Figure 1, terms can be submitted either on-line or
off-line. A reasonable method is that while the system
periodically updates the encyclopedia off-line, terms
unindexed in the encyclopedia are dynamically pro-
cessed in real-time usage. In either case, our system
processes input terms one by one.
We briefly explain each module in the following
three sections, respectively.
domain
model
Web
extraction
rules
organization
encyclopedia
retrieval
extraction
term(s)
description
model
Figure 1: The overall design of our Web-based ency-
clopedia generation system.
2.2 Retrieval
The retrieval module searches the Web for pages con-
taining an input term, for which existing Web search
engines can be used, and those with broad coverage
are desirable.
However, search engines performing query expan-
sion are not always desirable, because they usually re-
trieve a number of pages which do not contain an in-
put keyword. Since the extraction module (see Sec-
tion 2.3) analyzes the usage of the input term in re-
trieved pages, pages not containing the term are of no
use for our purpose.
Thus, we use as the retrieval module ?Google,?
which is one of the major search engines and does not
conduct query expansion1.
2.3 Extraction
In the extraction module, given Web pages containing
an input term, newline codes, redundant white spaces
and HTML tags that are not used in the following pro-
cesses are discarded to standardize the page format.
Second, we approximately identify a region describ-
ing the term in the page, for which two rules are used.
1http://www.google.com/
The first rule is based on Japanese linguistic patterns
typically used for term descriptions, such as ?X toha
Y dearu (X is Y).? Following the method proposed
by Fujii and Ishikawa (2000), we semi-automatically
produced 20 patterns based on the Japanese CD-ROM
World Encyclopedia (Heibonsha, 1998), which in-
cludes approximately 80,000 entries related to various
fields. It is expected that a region including the sen-
tence that matched with one of those patterns can be a
term description.
The second rule is based on HTML layout. In a typ-
ical case, a term in question is highlighted as a heading
with tags such as <DT>, <B> and <Hx> (?x? denotes
a digit), followed by its description. In some cases,
terms are marked with the anchor <A> tag, providing
hyperlinks to pages where they are described.
Finally, based on the region briefly identified by the
above method, we extract a page fragment as a term
description. Since term descriptions usually consist of
a logical segment (such as a paragraph) rather than a
single sentence, we extract a fragment that matched
with one of the following patterns, which are sorted
according to preference in descending order:
1. description tagged with <DD> in the case where
the term is tagged with <DT>2,
2. paragraph tagged with <P>,
3. itemization tagged with <UL>,
4. N sentences, where we empirically set N = 3.
2.4 Organization
As discussed in Section 1, organizing information ex-
tracted from the Web is crucial in our framework. For
this purpose, we classify extracted term descriptions
based on word senses and domains.
Although a number of methods have been proposed
to generate word senses (for example, one based on the
vector space model (Schu?tze, 1998)), it is still difficult
to accurately identify word senses without explicit dic-
tionaries that define sense candidates.
In addition, since word senses are often associated
with domains (Yarowsky, 1995), word senses can be
consequently distinguished by way of determining the
domain of each description. For example, different
senses for ?pipeline (processing method/transportation
pipe)? are associated with the computer and construc-
tion domains (fields), respectively.
To sum up, the organization module classifies term
descriptions based on domains, for which we use do-
main and description models. In Section 3, we elabo-
rate on our organization model.
2<DT> and <DD> are inherently provided to describe
terms in HTML.
3 Statistical Organization Model
3.1 Overview
Given one or more (in most cases more than one)
descriptions for a single input term, the organization
module selects appropriate description(s) for each do-
main related to the term.
We do not need all the extracted descriptions as fi-
nal outputs, because they are usually similar to one
another, and thus are redundant.
For the moment, we assume that we know a priori
which domains are related to the input term.
From the viewpoint of probability theory, our task
here is to select descriptions with greater probability
for given domains. The probability for description d
given domain c, P (d|c), is commonly transformed as
in Equation (1), through use of the Bayesian theorem.
P (d|c) = P (c|d) ? P (d)P (c) (1)
In practice, P (c) can be omitted because this factor is
a constant, and thus does not affect the relative proba-
bility for different descriptions.
In Equation (1), P (c|d) models a probability that d
corresponds to domain c. P (d) models a probability
that d can be a description for the term in question,
disregarding the domain. We shall call them domain
and description models, respectively.
To sum up, in principle we select d?s that are
strongly associated with a specific domain, and are
likely to be descriptions themselves.
Extracted descriptions are not linguistically under-
standable in the case where the extraction process is
unsuccessful and retrieved pages inherently contain
non-linguistic information (such as special characters
and e-mail addresses).
To resolve this problem, Fujii and Ishikawa (2000)
used a language model to filter out descriptions with
low perplexity. However, in this paper we integrated
a description model, which is practically the same as
a language model, with an organization model. The
new framework is more understandable with respect
to probability theory.
In practice, we first use Equation (1) to compute
P (d|c) for all the c?s predefined in the domain model.
Then we discard such c?s whose P (d|c) is below a spe-
cific threshold. As a result, for the input term, related
domains and descriptions are simultaneously selected.
Thus, we do not have to know a priori which domains
are related to each term.
In the following two sections, we explain methods
to realize the domain and description models, respec-
tively.
3.2 Domain Model
The domain model quantifies the extent to which de-
scription d is associated with domain c, which is fun-
damentally a categorization task. Among a number
of existing categorization methods, we experimentally
used one proposed by Iwayama and Tokunaga (1994),
which formulates P (c|d) as in Equation (2).
P (c|d) = P (c) ?
?
t
P (t|c) ? P (t|d)
P (t) (2)
Here, P (t|d), P (t|c) and P (t) denote probabilities
that word t appears in d, c and all the domains, respec-
tively. We regard P (c) as a constant. While P (t|d) is
simply a relative frequency of t in d, we need prede-
fined domains to compute P (t|c) and P (t). For this
purpose, the use of large-scale corpora annotated with
domains is desirable.
However, since those resources are prohibitively
expensive, we used the ?Nova? dictionary for
Japanese/English machine translation systems3, which
includes approximately one million entries related to
19 technical fields as listed below:
aeronautics, biotechnology, business, chem-
istry, computers, construction, defense,
ecology, electricity, energy, finance, law,
mathematics, mechanics, medicine, metals,
oceanography, plants, trade.
We extracted words from dictionary entries to esti-
mate P (t|c) and P (t), which are relative frequencies
of t in c and all the domains, respectively. We used
the ChaSen morphological analyzer (Matsumoto et al,
1997) to extract words from Japanese entries. We also
used English entries because Japanese descriptions of-
ten contain English words.
It may be argued that statistics extracted from dic-
tionaries are unreliable, because word frequencies in
real word usage are missing. However, words that are
representative for a domain tend to be frequently used
in compound word entries associated with the domain,
and thus our method is a practical approximation.
3.3 Description Model
The description model quantifies the extent to which a
given page fragment is feasible as a description for the
input term. In principle, we decompose the description
model into language and quality properties, as shown
in Equation (3).
P (d) = P
L
(d) ? P
Q
(d) (3)
Here, P
L
(d) and P
Q
(d) denote language and quality
models, respectively.
3Produced by NOVA, Inc.
It is expected that the quality model discards in-
correct or misleading information contained in Web
pages. For this purpose, a number of quality rating
methods for Web pages (Amento et al, 2000; Zhu and
Gauch, 2000) can be used.
However, since Google (i.e., the search engine used
in our system) rates the quality of pages based on
hyperlink information, and selectively retrieves those
with higher quality (Brin and Page, 1998), we tenta-
tively regarded P
Q
(d) as a constant. Thus, in practice
the description model is approximated solely with the
language model as in Equation (4).
P (d) ? P
L
(d) (4)
Statistical approaches to language modeling have
been used in much NLP research, such as machine
translation (Brown et al, 1993) and speech recogni-
tion (Bahl et al, 1983). Our model is almost the same
as existing models, but is different in two respects.
First, while general language models quantify the
extent to which a given word sequence is linguisti-
cally acceptable, our model also quantifies the extent
to which the input is acceptable as a term description.
Thus, we trained the model based on an existing ma-
chine readable encyclopedia.
We used the ChaSen morphological analyzer to
segment the Japanese CD-ROM World Encyclope-
dia (Heibonsha, 1998) into words (we replaced head-
words with a common symbol), and then used the
CMU-Cambridge toolkit (Clarkson and Rosenfeld,
1997) to model a word-based trigram.
Consequently, descriptions in which word se-
quences are more similar to those in the World En-
cyclopedia are assigned greater probability scores
through our language model.
Second, P (d), which is a product of probabilities
for N -grams in d, is quite sensitive to the length of d.
In the cases of machine translation and speech recog-
nition, this problem is less crucial because multiple
candidates compared based on the language model are
almost equivalent in terms of length.
However, since in our case length of descriptions are
significantly different, shorter descriptions are more
likely to be selected, regardless of the quality. To avoid
this problem, we normalize P (d) by the number of
words contained in d.
4 Application
4.1 Overview
Encyclopedias generated through our Web-based
method can be used in a number of applications, in-
cluding human usage, thesaurus production (Hearst,
1992; Nakamura and Nagao, 1988) and natural lan-
guage understanding in general.
Among the above applications, natural language un-
derstanding (NLU) is the most challenging from a sci-
entific point of view. Current practical NLU research
includes dialogue, information extraction and question
answering, among which we focus solely on question
answering (QA) in this paper.
A straightforward application is to answer inter-
rogative questions like ?What is X?? in which a QA
system searches the encyclopedia database for one or
more descriptions related to X (this application is also
effective for dialog systems).
In general, the performance of QA systems are eval-
uated based on coverage and accuracy. Coverage is
the ratio between the number of questions answered
(disregarding their correctness) and the total number
of questions. Accuracy is the ratio between the num-
ber of correct answers and the total number of answers
made by the system.
While coverage can be estimated objectively and
systematically, estimating accuracy relies on human
subjects (because there is no absolute description for
term X), and thus is expensive.
In view of this problem, we targeted Information
Technology Engineers Examinations4, which are bian-
nual (spring and autumn) examinations necessary for
candidates to qualify to be IT engineers in Japan.
Among a number of classes, we focused on the
?Class II? examination, which requires fundamental
and general knowledge related to information technol-
ogy. Approximately half of questions are associated
with IT technical terms.
Since past examinations and answers are open to the
public, we can evaluate the performance of our QA
system with minimal cost.
4.2 Analyzing IT Engineers Examinations
The Class II examination consists of quadruple-choice
questions, among which technical term questions can
be subdivided into two types.
In the first type of question, examinees choose
the most appropriate description for a given technical
term, such as ?memory interleave? and ?router.?
In the second type of question, examinees choose
the most appropriate term for a given question, for
which we show examples collected from the exami-
nation in the autumn of 1999 (translated into English
by one of the authors) as follows:
1. Which data structure is most appropriate for
FIFO (First-In First-Out)?
a) binary trees, b) queues, c) stacks, d) heaps
2. Choose the LAN access method in which mul-
tiple terminals transmit data simultaneously and
4Japan Information-Technology Engineers Examination
Center. http://www.jitec.jipdec.or.jp/
thus they potentially collide.
a) ATM, b) CSM/CD, c) FDDI, d) token ring
In the autumn of 1999, out of 80 questions, the num-
ber of the first and second types were 22 and 18, re-
spectively.
4.3 Implementing a QA system
For the first type of question, human examinees would
search their knowledge base (i.e., memory) for the de-
scription of a given term, and compare that description
with four candidates. Then they would choose the can-
didate that is most similar to the description.
For the second type of question, human examinees
would search their knowledge base for the description
of each of four candidate terms. Then they would
choose the candidate term whose description is most
similar to the question description.
The mechanism of our QA system is analogous to
the above human methods. However, unlike human
examinees, our system uses an encyclopedia generated
from the Web as a knowledge base.
In addition, our system selectively uses term de-
scriptions categorized into domains related to infor-
mation technology. In other words, the description
of ?pipeline (transportation pipe)? is irrelevant or mis-
leading to answer questions associated with ?pipeline
(processing method).?
To compute the similarity between two descriptions,
we used techniques developed in IR research, in which
the similarity between a user query and each document
in a collection is usually quantified based on word fre-
quencies. In our case, a question and four possible
answers correspond to query and document collection,
respectively. We used a probabilistic method (Robert-
son and Walker, 1994), which is one of the major IR
methods.
To sum up, given a question, its type and four
choices, our QA system chooses one of four candi-
dates as the answer, in which the resolution algorithm
varies depending on the question type.
4.4 Related Work
Motivated partially by the TREC-8 QA collec-
tion (Voorhees and Tice, 2000), question answering
has of late become one of the major topics within the
NLP/IR communities.
In fact, a number of QA systems targeting
the TREC QA collection have recently been pro-
posed (Harabagiu et al, 2000; Moldovan and
Harabagiu, 2000; Prager et al, 2000). Those sys-
tems are commonly termed ?open-domain? systems,
because questions expressed in natural language are
not necessarily limited to explicit axes, including who,
what, when, where, how and why.
However, Moldovan and Harabagiu (2000) found
that each of the TREC questions can be recast as ei-
ther a single axis or a combination of axes. They also
found that out of the 200 TREC questions, 64 ques-
tions (approximately one third) were associated with
the what axis, for which the Web-based encyclopedia
is expected to improve the quality of answers.
Although Harabagiu et al (2000) proposed a
knowledge-based QA system, most existing systems
rely on conventional IR and shallow NLP methods.
The use of encyclopedic knowledge for QA systems,
as we demonstrated, needs to be further explored.
5 Experimentation
5.1 Methodology
We conducted a number of experiments to investigate
the effectiveness of our methods.
First, we generated an encyclopedia by way of our
Web-based method (see Sections 2 and 3), and evalu-
ated the quality of the encyclopedia itself.
Second, we applied the generated encyclopedia to
our QA system (see Section 4), and evaluated its per-
formance. The second experiment can be seen as a
task-oriented evaluation for our encyclopedia genera-
tion method.
In the first experiment, we collected 96 terms from
technical term questions in the Class II examination
(the autumn of 1999). We used as test inputs those 96
terms and generated an encyclopedia, which was used
in the second experiment.
For all the 96 test terms, Google (see Section 2.2)
retrieved a positive number of pages, and the average
number of pages for one term was 196,503. Since
Google practically outputs contents of the top 1,000
pages, the remaining pages were not used in our ex-
periments.
In the following two sections, we explain the first
and second experiments, respectively.
5.2 Evaluating Encyclopedia Generation
For each test term, our method first computed P (d|c)
using Equation (1) and discarded domains whose
P (d|c) was below 0.05. Then, for each remaining do-
main, descriptions with higher P (d|c) were selected as
the final outputs.
We selected the top three (not one) descriptions for
each domain, because reading a couple of descriptions,
which are short paragraphs, is not laborious for human
users in real-world usage. As a result, at least one de-
scription was generated for 85 test terms, disregarding
the correctness. The number of resultant descriptions
was 326 (3.8 per term). We analyzed those descrip-
tions from different perspectives.
First, we analyzed the distribution of the Google
ranks for the Web pages from which the top three de-
scriptions were eventually retained. Figure 2 shows
the result, where we have combined the pages in
groups of 50, so that the leftmost bar, for example, de-
notes the number of used pages whose original Google
ranks ranged from 1 to 50.
Although the first group includes the largest number
of pages, other groups are also related to a relatively
large number of pages. In other words, our method
exploited a number of low ranking pages, which are
not browsed or utilized by most Web users.
0
10
20
30
40
50
60
70
0 100 200 300 400 500 600 700 800 900 1000
# 
of
 p
ag
es
ranking
Figure 2: Distribution of rankings for original pages in
Google.
Second, we analyzed the distribution of domains
assigned to the 326 resultant descriptions. Figure 3
shows the result, in which, as expected, most descrip-
tions were associated with the computer domain.
However, the law domain was unexpectedly asso-
ciated with a relatively great number of descriptions.
We manually analyzed the resultant descriptions and
found that descriptions for which appropriate domains
are not defined in our domain model, such as sports,
tended to be categorized into the law domain.
computers (200), law (41), electricity (28),
plants (15), medicine (10), finance (8),
mathematics (8), mechanics (5), biotechnology (4),
construction (2), ecology (2), chemistry (1),
energy (1), oceanography (1)
Figure 3: Distribution of domains related to the 326
resultant descriptions.
Third, we evaluated the accuracy of our method,
that is, the quality of an encyclopedia our method gen-
erated. For this purpose, each of the resultant descrip-
tions was judged as to whether or not it is a correct de-
scription for a term in question. Each domain assigned
to descriptions was also judged correct or incorrect.
We analyzed the result on a description-by-
description basis, that is, all the generated descriptions
were considered independent of one another. The ratio
of correct descriptions, disregarding the domain cor-
rectness, was 58.0% (189/326), and the ratio of cor-
rect descriptions categorized into the correct domain
was 47.9% (156/326).
However, since all the test terms are inherently re-
lated to the IT field, we focused solely on descriptions
categorized into the computer domain. In this case,
the ratio of correct descriptions, disregarding the do-
main correctness, was 62.0% (124/200), and the ratio
of correct descriptions categorized into the correct do-
main was 61.5% (123/200).
In addition, we analyzed the result on a term-by-
term basis, because reading only a couple of descrip-
tions is not crucial. In other words, we evaluated
each term (not description), and in the case where at
least one correct description categorized into the cor-
rect domain was generated for a term in question, we
judged it correct. The ratio of correct terms was 89.4%
(76/85), and in the case where we focused solely on the
computer domain, the ratio was 84.8% (67/79).
In other words, by reading a couple of descriptions
(3.8 descriptions per term), human users can obtain
knowledge of approximately 90% of input terms.
Finally, we compared the resultant descriptions with
an existing dictionary. For this purpose, we used the
?Nichigai? computer dictionary (Nichigai Associates,
1996), which lists approximately 30,000 Japanese
technical terms related to the computer field, and con-
tains descriptions for 13,588 terms. In the Nichigai
dictionary, 42 out of the 96 test terms were described.
Our method, which generated correct descriptions as-
sociated with the computer domain for 67 input terms,
enhanced the Nichigai dictionary in terms of quantity.
These results indicate that our method for generat-
ing encyclopedias is of operational quality.
5.3 Evaluating Question Answering
We used as test inputs 40 questions, which are related
to technical terms collected from the Class II exami-
nation in the autumn of 1999.
The objective here is not only to evaluate the perfor-
mance of our QA system itself, but also to evaluate the
quality of the encyclopedia generated by our method.
Thus, as performed in the first experiment (Sec-
tion 5.2), we used the Nichigai computer dictionary as
a baseline encyclopedia. We compared the following
three different resources as a knowledge base:
? the Nichigai dictionary (?Nichigai?),
? the descriptions generated in the first experiment
(?Web?),
? combination of both resources (?Nichigai +
Web?).
Table 1 shows the result of our comparative exper-
iment, in which ?C? and ?A? denote coverage and ac-
curacy, respectively, for variations of our QA system.
Since all the questions we used are quadruple-
choice, in case the system cannot answer the question,
random choice can be performed to improve the cov-
erage to 100%. Thus, for each knowledge resource we
compared cases without/with random choice, which
are denoted ?w/o Random? and ?w/ Random? in Ta-
ble 1, respectively.
Table 1: Coverage and accuracy (%) for different ques-
tion answering methods.
w/o Random w/ Random
Resource C A C A
Nichigai 50.0 65.0 100 45.0
Web 92.5 48.6 100 46.9
Nichigai + Web 95.0 63.2 100 61.3
In the case where random choice was not per-
formed, the Web-based encyclopedia noticeably im-
proved the coverage for the Nichigai dictionary, but
decreased the accuracy. However, by combining both
resources, the accuracy was noticeably improved, and
the coverage was comparable with that for the Nichi-
gai dictionary.
On the other hand, in the case where random choice
was performed, the Nichigai dictionary and the Web-
based encyclopedia were comparable in terms of both
the coverage and accuracy. Additionally, by combin-
ing both resources, the accuracy was further improved.
We also investigated the performance of our QA
system where descriptions related to the computer do-
main are solely used. However, coverage/accuracy did
not significantly change, because as shown in Figure 3,
most of the descriptions were inherently related to the
computer domain.
6 Conclusion
The World Wide Web has been an unprecedentedly
enormous information source, from which a number
of language processing methods have been explored
to extract, retrieve and discover various types of infor-
mation.
In this paper, we aimed at generating encyclopedic
knowledge, which is valuable for many applications
including human usage and natural language under-
standing. For this purpose, we reformalized an exist-
ing Web-based extraction method, and proposed a new
statistical organization model to improve the quality of
extracted data.
Given a term for which encyclopedic knowledge
(i.e., descriptions) is to be generated, our method se-
quentially performs a) retrieval of Web pages contain-
ing the term, b) extraction of page fragments describ-
ing the term, and c) organizing extracted descriptions
based on domains (and consequently word senses).
In addition, we proposed a question answering sys-
tem, which answers interrogative questions associated
with what, by using a Web-based encyclopedia as a
knowledge base. For the purpose of evaluation, we
used as test inputs technical terms collected from the
Class II IT engineers examination, and found that the
encyclopedia generated through our method was of
operational quality and quantity.
We also used test questions from the Class II exam-
ination, and evaluated the Web-based encyclopedia in
terms of question answering. We found that our Web-
based encyclopedia improved the system coverage ob-
tained solely with an existing dictionary. In addition,
when we used both resources, the performance was
further improved.
Future work would include generating information
associated with more complex interrogations, such as
ones related to how and why, so as to enhance Web-
based natural language understanding.
Acknowledgments
The authors would like to thank NOVA, Inc. for their
support with the Nova dictionary and Katunobu Itou
(The National Institute of Advanced Industrial Science
and Technology, Japan) for his insightful comments on
this paper.
References
Brian Amento, Loren Terveen, and Will Hill. 2000.
Does ?authority? mean quality? predicting expert
quality ratings of Web documents. In Proceedings
of the 23rd Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 296?303.
Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum linklihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
5(2):179?190.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal language modeling using the CMU-Cambridge
toolkit. In Proceedings of EuroSpeech?97, pages
2707?2710.
Oren Etzioni. 1997. Moving up the information food
chain. AI Magazine, 18(2):11?18.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.
Maiorano. 2000. Experiments with open-domain
textual question answering. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 292?298.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th International Conference on Computa-
tional Linguistics, pages 539?545.
Hitachi Digital Heibonsha. 1998. CD-ROM World
Encyclopedia. (In Japanese).
Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda,
Kouhei Kumasawa, and Naohide Arai. 1999. Bas-
ket analysis for graph structured data. In Proceed-
ings of the 3rd Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pages 420?431.
Makoto Iwayama and Takenobu Tokunaga. 1994. A
probabilistic model for text categorization: Based
on a single random variable with multiple values. In
Proceedings of the 4th Conference on Applied Nat-
ural Language Processing, pages 162?167.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-
IS-TR97007, NAIST. (In Japanese).
Andrew McCallum, Kamal Nigam, Jason Rennie, and
Kristie Seymore. 1999. A machine learning ap-
proach to building domain-specific search engines.
In Proceedings of the 16th International Joint Con-
ference on Artificial Intelligence, pages 662?667.
Dan Moldovan and Sanda Harabagiu. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 563?570.
Jun?ichi Nakamura and Makoto Nagao. 1988. Extrac-
tion of semantic information from an ordinary En-
glish dictionary and its evaluation. In Proceedings
of the 10th International Conference on Computa-
tional Linguistics, pages 459?464.
Nichigai Associates. 1996. English-Japanese com-
puter terminology dictionary. (In Japanese).
John Prager, Eric Brown, and Anni Coden. 2000.
Question-answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 184?191.
Philip Resnik. 1999. Mining the Web for bilingual
texts. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 527?534.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 232?241.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Stephen Soderland. 1997. Learning to extract text-
based information from the World Wide Web. In
Proceedings of 3rd International Conference on
Knowledge Discovery and Data Mining.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 200?207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196.
Xiaolan Zhu and Susan Gauch. 2000. Incorporating
quality metrics in centralized/distributed informa-
tion retrieval on the World Wide Web. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 288?295.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 631?642, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Enlarging Paraphrase Collections through Generalization and Instantiation
Atsushi Fujita
Future University Hakodate
116-2 Kameda-nakano-cho,
Hakodate, Hokkaido, 041-8655, Japan
fujita@fun.ac.jp
Pierre Isabelle Roland Kuhn
National Research Council Canada
283 Alexandre-Tache? Boulevard,
Gatineau, QC, J8X 3X7, Canada
{Pierre.Isabelle, Roland.Kuhn}@nrc.ca
Abstract
This paper presents a paraphrase acquisition
method that uncovers and exploits generali-
ties underlying paraphrases: paraphrase pat-
terns are first induced and then used to col-
lect novel instances. Unlike existing methods,
ours uses both bilingual parallel and monolin-
gual corpora. While the former are regarded as
a source of high-quality seed paraphrases, the
latter are searched for paraphrases that match
patterns learned from the seed paraphrases.
We show how one can use monolingual cor-
pora, which are far more numerous and larger
than bilingual corpora, to obtain paraphrases
that rival in quality those derived directly from
bilingual corpora. In our experiments, the
number of paraphrase pairs obtained in this
way from monolingual corpora was a large
multiple of the number of seed paraphrases.
Human evaluation through a paraphrase sub-
stitution test demonstrated that the newly ac-
quired paraphrase pairs are of reasonable qual-
ity. Remaining noise can be further reduced
by filtering seed paraphrases.
1 Introduction
Paraphrases are semantically equivalent expressions
in the same language. Because ?equivalence? is the
most fundamental semantic relationship, techniques
for generating and recognizing paraphrases play an
important role in a wide range of natural language
processing tasks (Madnani and Dorr, 2010).
In the last decade, automatic acquisition of knowl-
edge about paraphrases from corpora has been draw-
ing the attention of many researchers. Typically, the
acquired knowledge is simply represented as pairs of
semantically equivalent sub-sentential expressions
as in (1).
(1) a. look like ? resemble
b. control system ? controller
The challenge in acquiring paraphrases is to ensure
good coverage of the targeted classes of paraphrases
along with a low proportion of incorrect pairs. How-
ever, no matter what type of resource has been used,
it has proven difficult to acquire paraphrase pairs
with both high recall and high precision.
Among various types of corpora, monolingual
corpora can be considered the best source for high-
coverage paraphrase acquisition, because there is
far more monolingual than bilingual text avail-
able. Most methods that exploit monolingual cor-
pora rely on the Distributional Hypothesis (Harris,
1968): expressions that appear in similar contexts
are expected to have similar meaning. However,
if one uses purely distributional criteria, it is dif-
ficult to distinguish real paraphrases from pairs of
expressions that are related in other ways, such as
antonyms and cousin words.
In contrast, since the work in (Bannard and
Callison-Burch, 2005), bilingual parallel corpora
have been acknowledged as a good source of high-
quality paraphrases: paraphrases are obtained by
putting together expressions that receive the same
translation in the other language (pivot language).
Because translation expresses a specific meaning
more directly than context in the aforementioned ap-
proach, pairs of expressions acquired in this manner
tend to be correct paraphrases. However, the cov-
erage problem remains: there is much less bilingual
parallel than monolingual text available.
Our objective in this paper is to obtain para-
phrases that have high quality (like those extracted
from bilingual parallel corpora via pivoting) but can
be generated in large quantity (like those extracted
631
from monolingual corpora via contextual similar-
ity). To achieve this, we propose a method that ex-
ploits general patterns underlying paraphrases and
uses both bilingual parallel and monolingual sources
of information. Given a relatively high-quality set of
paraphrases obtained from a bilingual parallel cor-
pus, a set of paraphrase patterns is first induced.
Then, appropriate instances of such patterns, i.e.,
potential paraphrases, are harvested from a mono-
lingual corpus.
After reviewing existing methods in Section 2,
our method is presented in Section 3. Section 4
describes our experiments in acquiring paraphrases
and presents statistics summarizing the coverage of
our method. Section 5 describes a human evaluation
of the quality of the acquired paraphrases. Finally,
Section 6 concludes this paper.
2 Literature on Paraphrase Acquisition
This section summarizes existing corpus-based
methods for paraphrase acquisition, following the
classification in (Hashimoto et al2011): similarity-
based and alignment-based methods.
2.1 Similarity-based Methods
Techniques that use monolingual (non-parallel) cor-
pora mostly rely on the Distributional Hypothesis
(Harris, 1968). Because a large quantity of mono-
lingual data is available for many languages, a large
number of paraphrase candidates can be acquired
(Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bha-
gat and Ravichandran, 2008, etc.). The recipes pro-
posed so far are based on three main ingredients, i.e.,
features used for representing context of target ex-
pression (contextual features), criteria for weighting
and filtering features, and aggregation functions.
A drawback of relying only on contextual simi-
larity is that it tends to give high scores to semanti-
cally related but non-equivalent expressions, such as
antonyms and cousin words. To enhance the preci-
sion of the results, filtering mechanisms need to be
introduced (Marton et al2011).
2.2 Alignment-based Methods
Pairs of expressions that get translated to the same
expression in a different language can be regarded as
paraphrases. On the basis of this hypothesis, Barzi-
lay and McKeown (2001) and Pang et al2003)
created monolingual parallel corpora from multiple
human translations of the same source. Then, they
extracted corresponding parts of such parallel sen-
tences as sub-sentential paraphrases.
Leveraging recent advances in statistical ma-
chine translation (SMT), Bannard and Callison-
Burch (2005) proposed a method for acquiring sub-
sentential paraphrases from bilingual parallel cor-
pora. As in SMT, a translation table is first built on
the basis of alignments between expressions, such as
words, phrases, and subtrees, across a parallel sen-
tence pair. Then, pairs of expressions (e1, e2) in the
same language that are aligned with the same ex-
pressions in the other language (pivot language) are
extracted as paraphrases. The likelihood of e2 being
a paraphrase of e1 is given by
p(e2|e1) =
?
f?Tr(e1,e2)
p(e2|f)p(f |e1), (1)
where Tr(e1, e2) stands for the set of shared trans-
lations of e1 and e2. Each factor p(e|f) and p(f |e)
is estimated from the number of times e and f are
aligned and the number of occurrences of each ex-
pression in each language. Kok and Brockett (2010)
showed how one can discover paraphrases that do
not share any translation in one language by travers-
ing a graph created from multiple translation tables,
each corresponding to a bilingual parallel corpus.
This approach, however, suffers from a cover-
age problem, because both monolingual parallel and
bilingual parallel corpora tend to be significantly
smaller than monolingual non-parallel corpora. The
acquired pairs of expressions include some non-
paraphrases as well. Many of these come from er-
roneous alignments, which are particularly frequent
when the given corpus is small.
Monolingual comparable corpora have also been
exploited as sources of paraphrases using alignment-
based methods. For instance, multiple news arti-
cles covering the same event (Shinyama et al2002;
Barzilay and Lee, 2003; Dolan et al2004; Wubben
et al2009) have been used. Such corpora have
also been created manually through crowdsourcing
(Chen and Dolan, 2011). However, the availabil-
ity of monolingual comparable corpora is very lim-
ited for most languages; thus, approaches relying
on these corpora have typically produced only very
632
small collections of paraphrases. Hashimoto et al
(2011) found a way around this limitation by collect-
ing sentences that constitute explicit definitions of
particular words or phrases from monolingual non-
parallel Web documents, pairing sentences that de-
fine the same noun phrase, and then finding corre-
sponding phrases in each sentence pair. One limita-
tion of this approach is that it requires a considerable
amount of labeled data for both the corpus construc-
tion and the paraphrase extraction steps.
2.3 Summary
Existing methods have investigated one of the fol-
lowing four types of corpora as their principal re-
source1: monolingual non-parallel corpora, mono-
lingual parallel corpora, monolingual comparable
corpora, and bilingual parallel corpora. No matter
what type of resource has been used, however, it
has proven difficult to acquire paraphrases with both
high recall and precision, with the possible excep-
tion of the method in (Hashimoto et al2011) which
requires large amounts of labeled data.
3 Proposed Method
While most existing methods deal with expressions
only at the surface level, ours exploits generalities
underlying paraphrases to achieve better coverage
while retaining high precision. Furthermore, unlike
existing methods, ours uses both bilingual parallel
and monolingual non-parallel corpora as sources for
acquiring paraphrases.
The process is illustrated in Figure 1. First, a
set of high-quality seed paraphrases, PSeed , is ac-
quired from bilingual parallel corpora by using an
alignment-based method. Then, our method collects
further paraphrases through the following two steps.
Generalization (Step 2): Paraphrase patterns are
learned from the seed paraphrases, PSeed .
Instantiation (Step 3): A novel set of paraphrase
pairs, PHvst , is finally harvested from mono-
lingual non-parallel corpora using the learned
patterns; each newly acquired paraphrase pair
is assessed by contextual similarity.
1Chan et al2011) used monolingual corpora only for re-
ranking paraphrases obtained from bilingual parallel corpora.
To the best of our knowledge, bilingual comparable corpora
have never been used as sources for acquiring paraphrases.
Monolingual Non-parallel Corpus
Step 1. Seed Paraphrase Acquisition
Step 2. Paraphrase Pattern Induction
Step 3. Paraphrase Instance Acquisition
?health issue? ? ?health problem? ?look like? ? ?resemble? ?regional issue? ? ?regional problem? 
?health issue? ? ?probl?me de sant?? ?health problem? ? ?probl?me de sant?? ?look like? ? ?ressemble? ?regional issue? ? ?probl?me r?gional? ?regional problem? ? ?probl?me r?gional? ?resemble? ? ?ressemble? 
?X issue? ? ?X problem?;         {food, regional, ...}
?backlog issue? ? ?backlog problem? ?communal issue? ? ?communal problem? ?phishing issue? ? ?phishing problem? ?spatial issue? ? ?spatial problem?
Translation Table
PSeed: Seed Paraphrases
Paraphrase Patterns
PHvst: Novel Paraphrases
Bilingual Parallel Corpus
Figure 1: Process of paraphrase acquisition.
The set PSeed acquired early in the process can be
pooled with the set PHvst harvested in the last stage
of the process.
3.1 Step 1. Seed Paraphrase Acquisition
The goal of the first step is to obtain a set of high-
quality paraphrase pairs, PSeed .
For this purpose, alignment-based methods with
bilingual or monolingual parallel corpora are prefer-
able to similarity-based methods applied to non-
parallel corpora. Among various options, in this pa-
per, we start from the standard technique proposed
by Bannard and Callison-Burch (2005) with bilin-
gual parallel corpora (see also Section 2.2). In par-
ticular, we assume the phrase-based SMT frame-
work (Koehn et al2003). Then, we purify the re-
sults with several filtering methods.
The phrase pair extraction process of phrase-
based SMT systems aims at high recall for increased
robustness of the translation process. As a result,
a naive application of the paraphrase acquisition
method produces pairs of expressions that are not
exact paraphrases. For instance, the algorithm ex-
plained in Koehn (2009, p.134) extracts both ?dass?
and ?, dass? as counterparts of ?that? from the sen-
tence pair. To reduce that kind of noise, we apply
some filtering techniques to the candidate translation
pairs. First, statistically unreliable translation pairs
(Johnson et al2007) are filtered out. Then, we also
filter out phrases made up entirely of stop words (in-
cluding punctuation marks), both in the language of
interest and in the pivot language.
Let PRaw be the initial set of paraphrase pairs ex-
tracted from the sanitized translation table. We first
633
lp: control apparatus
rp: control devicep(rp|lp)
.172
rp: control system
.032
rp: the control device
.015
rp: control device of the.005
rp: controlling device.004
rp: control system of
.003
rp: a control system for an
.001
rp: a controlling device
.001
Figure 2: RHS-filtering for ?control apparatus?.
rp: control device
lp: controller p(lp|rp)
.153
lp: control apparatus
.135
lp: the control apparatus
.010
lp: control apparatus of .008
lp: controlling unit .004
lp: control equipment
.002
lp: controller for a
.001
lp: to the control apparatus
.001
Figure 3: LHS-filtering for ?control device?.
discard pairs whose difference comprises only stop
words, such as ?the schools? ? ?schools and?. We
also remove pairs containing only singular-plural
differences, such as ?family unit? ? ?family units?.
Depending on the language of interest, other types of
morphological variants, such as those shown in (2),
may also be ignored.
(2) a. ?europe?enne? ? ?europe?en?
(Gender in French)
b. ?guten Lo?sungen? ? ?gute Lo?sungen?
(Case in German)
We further filter out less reliable pairs, such as
those shown with dotted lines in Figures 2 and 3.
This is carried out by comparing the right-hand side
(RHS) phrases of each left-hand side (LHS) phrase,
and vice versa2. Given a set of paraphrase pairs,
RHS phrases corresponding to the same LHS phrase
lp are compared. A RHS phrase rp is not licensed iff
lp has another RHS phrase rp? (?= rp) which satis-
fies the following two conditions (see also Figure 2).
? rp? is a word sub-sequence of rp
? rp? is a more likely paraphrase than rp,
i.e., p(rp ?|lp) > p(rp|lp)
LHS phrases for each RHS phrase rp are also com-
pared in a similar manner, i.e., a LHS phrase lp is
not qualified as a legitimate source of rp iff rp has
another LHS phrase lp? (?= lp) which satisfies the
following conditions (see also Figure 3).
? lp? is a word sub-sequence of lp
? lp? is a more likely source than lp,
i.e., p(lp ?|rp) > p(lp|rp)
The two directions of filtering are separately applied
and the intersection of their results is retained.
2cf. Denkowski and Lavie (2011); they only compared each
RHS phrase to its corresponding LHS phrase.
Candidate pairs are finally filtered on the basis
of their reliability score. Traditionally, a threshold
(thp) on the conditional probability given by Eq. (1)
is used (Du et al2010; Max, 2010; Denkowski
and Lavie, 2011, etc.). Furthermore, we also re-
quire that LHS and RHS phrases exceed a thresh-
old (ths ) on their contextual similarity in a mono-
lingual corpus. This paper neither proposes a spe-
cific recipe nor makes a comprehensive comparison
of existing recipes for computing contextual simi-
larity, although one particular recipe is used in our
experiments (see Section 4.1).
3.2 Step 2. Paraphrase Pattern Induction
From a set of seed paraphrases, PSeed , paraphrase
patterns are induced. For instance, from paraphrases
in (3), we induce paraphrase patterns in (4).
(3) a. ?restraint system? ? ?restraint apparatus?
b. ?movement against racism?
? ?anti-racism movement?
c. ?middle eastern countries?
? ?countries in the middle east?
(4) a. ?X system? ? ?X apparatus?
b. ?X against Y ? ? ?anti-Y X?
c. ?X eastern Y ? ? ?Y in the X east?
Word pairs of LHS and RHS phrases will be re-
placed with variable slots iff they are fully identi-
cal or singular-plural variants. Note that stop words
are retained. While a deeper level of lexical cor-
respondences, such as ?eastern? and ?east? in (3c)
and ?system? and ?apparatus? in (3a), could be cap-
tured, this would require the use of rich language
resources, thereby making the method less portable
to resource-poor languages.
634
Note that our aim is to automatically capture gen-
eral paraphrase patterns of the kind that have some-
times been manually described (Jacquemin, 1999;
Fujita et al2007). This is different from ap-
proaches that attach variable slots to paraphrases for
calculating their similarity (Lin and Pantel, 2001;
Szpektor and Dagan, 2008) or for constraining
the context in which they are regarded legitimate
(Callison-Burch, 2008; Zhao et al2009).
3.3 Step 3. Paraphrase Instance Acquisition
Given a set of paraphrase patterns, such as those
shown in (4), a set of novel instances, i.e., novel
paraphrases, PHvst , will now be harvested from
monolingual non-parallel corpora. In other words,
a set of appropriate slot-fillers will be extracted.
First, expressions that match both elements of
the pattern, except stop words, are collected from
a given monolingual corpus. Pattern matching alone
may generate inappropriate pairs, so we then assess
the legitimacy of each collected slot-filler.
Let LHS (w) and RHS (w) be the expressions
generated by instantiating the k variable slots in
LHS and RHS phrases of the pattern with a k-tuple
of slot-fillers w (= w1, . . . , wk), respectively. We
estimate how likelyRHS (w) is to be a paraphrase of
LHS (w) based on the contextual similarity between
them using a monolingual corpus; a pair of phrases
is discarded if they are used in substantially dissim-
ilar contexts. We use the same recipe and threshold
value for ths with Step 1 in our experiments.
Contextual similarity of antonyms and cousin
words can also be high, as they are often used in sim-
ilar contexts. However, this is not a problem in our
framework, because semantic equivalence between
LHS (w) and RHS (w) is almost entirely guaran-
teed as a result of the way the corresponding patterns
were learned from a bilingual parallel corpus.
3.4 Characteristics
In terms of coverage, PHvst is expected to be greatly
larger than PSeed , although it will not cover to-
tally different pairs of paraphrases, such as those
shown in (1). On the other hand, the quality of
PHvst depends on that of PSeed . Unlike in the pure
similarity-based method, PHvst is constrained by the
paraphrase patterns derived from the set of high-
quality paraphrases, PSeed , and will therefore gen-
erally exclude the kind of semantically similar but
non-equivalent pairs that contextual similarity alone
tends to extract alongside real paraphrases.
As mentioned in Section 3.1, other types of meth-
ods can be used for obtaining high-quality seed
paraphrases, PSeed . For instance, the supervised
method proposed by Hashimoto et al2011) uses
the existence of shared words as a feature to deter-
mine whether the given pair of expressions are para-
phrases, and thereby extracts many pairs sharing the
same words. Thus, their output has a high potential
to be used as an alternative seed for our method.
Another advantage of our method is that it does
not require any labeled data, unlike the super-
vised methods proposed by Zhao et al2009) and
Hashimoto et al2011).
4 Quantitative Impact
4.1 Experimental Settings
Two different sets of corpora were used as data
sources; in both settings, we acquired English para-
phrases.
Europarl: The English-French version of the Eu-
roparl Parallel Corpus3 consisting of 1.8M sen-
tence pairs (51M words in English and 56M
words in French) was used as a bilingual par-
allel corpus, while its English side and the En-
glish side of the 109 French-English corpus4
consisting of 23.8M sentences (649M words)
were used as monolingual data.
Patent: The Japanese-English Patent Translation
data (Fujii et al2010) consisting of 3.2M sen-
tence pairs (122M morphemes in Japanese and
106Mwords in English) was used as a bilingual
parallel corpus, while its English side and the
30.0M sentences (626M words) from the 2007
chapter of NTCIR unaligned patent documents
were used as monolingual data.
To study the behavior of our method for different
amounts of bilingual parallel data, we carried out
learning curve experiments.
We used our in-house tokenizer for segmentation
of English and French sentences and MeCab5 for
Japanese sentences.
3http://statmt.org/europarl/, release 6
4http://statmt.org/wmt10/training-giga-fren.tar
5http://mecab.sourceforge.net/, version 0.98
635
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent).
Stop word lists for sanitizing translation pairs and
paraphrase pairs were manually compiled: we enu-
merated 442 English words, 193 French words, and
149 Japanese morphemes, respectively.
From a bilingual parallel corpus, a translation ta-
ble was created by our in-house phrase-based SMT
system, PORTAGE (Sadat et al2005). Phrase
alignments of each sentence pair were identified by
the heuristic ?grow-diag-final?6 with a maximum
phrase length 8. The resulting translation pairs were
then filtered with the significance pruning technique
of (Johnson et al2007), using ? + ? as threshold.
As contextual features for computing similarity
of each paraphrase pair, all of the 1- to 4-grams of
words adjacent to each occurrence of a phrase were
counted. This is a compromise between less expen-
sive but noisier approaches, such as bag-of-words,
and more accurate but more expensive approaches
that incorporate syntactic features (Lin and Pantel,
2001; Shinyama et al2002; Pang et al2003;
Szpektor and Dagan, 2008). Contextual similarity is
finally measured by taking cosine between two fea-
ture vectors.
4.2 Statistics on Acquired Paraphrases
Seed Paraphrases (PSeed )
Figure 4 shows the number of paraphrase pairs
PSeed obtained from the bilingual parallel corpora.
The general trend is simply that the larger the cor-
pus is, the more paraphrases are acquired.
Given the initial set of paraphrases, PRaw (???),
our filtering techniques (?
2
?) discarded a large por-
tion (63-75% in Europarl and 43-64% in Patent) of
them. Pairs with zero similarity were also filtered
out, i.e., ths = ?. This suggests that many incorrect
6http://statmt.org/moses/?n=FactoredTraining.AlignWords
and/or relatively useless pairs, such as those shown
in Figures 2 and 3, had originally been acquired.
Lines with ??? show the results based on a
widely-used threshold value on the conditional prob-
ability in Eq. (1), i.e., thp = 0.01 (Du et al2010;
Max, 2010; Denkowski and Lavie, 2011, etc.). The
percentage of paraphrase pairs thereby discarded
varied greatly depending on the corpus size (17-78%
in Europarl and 31-82% in Patent), suggesting that
the threshold value should be determined depending
on the given corpus. In the following experiment,
however, we conform to the convention thp = 0.01
(???) to ensure the quality of PSeed that we will be
using for inducing paraphrase patterns, even though
this results in discarding some less frequent but cor-
rect paraphrase pairs, such as ?control apparatus?
? ?controlling device? in Figure 2.
Paraphrase Patterns
Figures 5 and 6 show the number of paraphrase
patterns that our method induced and their cover-
age against PSeed , respectively. Due to their rather
rigid form, the patterns covered no more than 15%
of PSeed in Europarl. In contrast, a higher propor-
tion of PSeed in Patent was generalized into patterns.
We speculate it is because the patent domain con-
tains many expressions, including technical terms,
that have similar variations of constructions.
The acquired patterns were mostly one-variable
patterns: 88-93% and 80-91% of total patterns for
different variants of the Europarl and Patent set-
tings, respectively. Given that there are far more
one-variable patterns than other types, and that one-
variable patterns are the simplest type, we hence-
forth focus on them. More complex patterns, includ-
ing two-variable patterns (7-11% and 8-17% in each
setting), will be investigated in our future work.
636
102
103
104
105
106
106 107 108
# o
f pa
rap
hra
se 
pat
tern
s
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 5: # of paraphrase patterns.
 0
 10
 20
 30
 40
106 107 108
Co
ver
age
 [%]
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 6: Coverage of the paraphrase patterns.
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
18.1M
1.22M
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
28.7M
1.41M
Figure 7: # of paraphrase pairs and unique LHS phrases in PSeed and PHvst (left: Europarl, right: Patent).
Novel Paraphrases (PHvst )
Using the paraphrase patterns, novel paraphrase
pairs, PHvst , were harvested from the monolingual
non-parallel corpora. In this experiment, we only
retained one-variable patterns and regarded only sin-
gle words as slot-fillers for them. Nevertheless, we
managed to acquire a large number of paraphrase
pairs as depicted in Figure 7, where pairs having
zero similarity were excluded. For instance, when
the full size of bilingual parallel corpus in Patent was
used, we acquired 1.41M pairs of seed paraphrases,
PSeed , and 28.7M pairs of novel paraphrases, PHvst .
In other words, our method expanded PSeed by about
21 times. The number of unique LHS phrases that
PHvst covers was also significantly larger than that
of PSeed .
Figure 8 highlights the remarkably large ratio of
PHvst to PSeed in terms of the number of paraphrase
pairs and the number of unique LHS phrases. The
smaller the bilingual corpus is, the higher the ratio
is, except when there is only a very small amount of
Europarl data. This demonstrates that our method is
quite powerful, given a minimum amount of data.
Another striking difference between PSeed and
PHvst is the average number of RHS phrases per
unique LHS phrase, i.e., their relative yield. As
displayed in Figure 9, the yield for PHvst increased
rapidly with the scaling up of the bilingual cor-
pus, while that of PSeed only grew slowly. The
alignment-based method with bilingual corpora can-
not produce very many RHS phrases per unique
LHS phrase due to its reliance on conditional prob-
ability and the surface level processing. In con-
trast, our method does not limit the number of RHS
phrases: each RHS phrase is separately assessed by
its similarity to the corresponding LHS phrase. One
limitation of our method is that it cannot achieve
high yield for PHvst whenever only a small num-
ber of paraphrase patterns can be extracted from the
bilingual corpus (see also Figure 5).
Both the ratio of PHvst to PSeed and the relative
yield could probably be increased by scaling up the
monolingual corpus. For instance, in the patent do-
main, monolingual documents 10 times larger than
the one used in the above experiments are avail-
able at the NTCIR project7. It would be interesting
to compare the relative gains brought by in-domain
versus general-purpose corpora.
7http://ntcir.nii.ac.jp/PatentMT-2/
637
 0
 20
 40
 60
 80
106 107 108
Ra
tio 
of P
Hv
st t
o P
Se
ed
# of words in the English side of bilingual corpus
LHS (Patent)Pair (Patent)LHS (Europarl)Pair (Europarl)
Figure 8: Ratio of PHvst to PSeed .
 1
 2
 3
 4
 5
106 107 108
Av
g. #
 of 
RH
S p
hra
ses
# of words in the English side of bilingual corpus
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 9: Average # of RHS phrases per LHS phrase.
103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Probability threshold thp
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl) 103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Similarity threshold ths
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 10: # of acquired paraphrase pairs against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
Finally, we investigated how the number of para-
phrase pairs varies depending on the values for the
two thresholds, i.e., thp on the conditional probabil-
ity and ths on the contextual similarity, respectively.
Figure 10 shows the results when the full sizes of
bilingual corpora are used. When the pairs were fil-
tered only with thp , the number of paraphrase pairs
in PHvst decreased more slowly than that of PSeed
according to the increase of the threshold value. This
is a benefit from our generalization and instantiation
method. The same paraphrase pattern is often in-
duced from more than one paraphrase pair in PSeed .
Thus, as long as at least one of them has a proba-
bility higher than the given threshold value, corre-
sponding novel paraphrases can be harvested.
On the other hand, as a results of assessing each
individual paraphrase pair by the contextual similar-
ity, many pairs in PHvst , which are supposed to be
incorrect instances of their corresponding pattern,
are filtered out by a larger threshold value for ths .
In contrast, many pairs in PSeed have a relatively
high similarity, e.g., 40% of all pairs have similarity
higher than 0.4. This indicates the quality of PSeed
is highly guaranteed by the shared translations.
5 Human Evaluation of Quality
To confirm that the quality of PHvst is sufficiently
high, we carried out a substitution test.
First, by substituting sub-sentential paraphrases
to existing sentences in a given test corpus, pairs
of slightly different sentences were automatically
generated. For instance, by applying ?looks like?
? ?resembles? to (5), (6) was generated.
(5) The roof looks like a prehistoric lizard?s spine.
(6) The roof resembles a prehistoric lizard?s spine.
Human evaluators were then asked to score each
pair of an original sentence and a paraphrased sen-
tence with the following two 5-point scale grades
proposed by Callison-Burch (2008):
Grammaticality: whether the paraphrased sen-
tence is grammatical (1: horrible, 5: perfect)
Meaning: whether the meaning of the original sen-
tence is properly retained by the paraphrased
sentence (1: totally different, 5: equivalent)
To make results more consistent and reduce the
human labor, evaluators were asked to rate at the
same time several paraphrases for the same source
phrase. For instance, given a source sentence (5), the
638
evaluators might be given the following sentences in
addition to a paraphrased sentence (6).
(7) The roof seems like a prehistoric lizard?s spine.
(8) The roofwould look like a prehistoric lizard?s spine.
In this experiment, we showed five paraphrases
per source phrase, assuming that evaluators would
get confused if too large a number of paraphrase
candidates were presented at the same time.
5.1 Data for Evaluation
As in previous work (Callison-Burch, 2008; Chan
et al2011), we evaluated paraphrases acquired
from the Europarl corpus on news sentences. Para-
phrase examples were automatically generated from
the English part ofWMT 2008-2011 ?newstest? data
(10,050 unique sentences) by applying the union of
PSeed and PHvst of the Europarl setting (19.3M para-
phrases for 5.95M phrases).
On the other hand, paraphrases acquired from
patent documents are much more difficult to eval-
uate due to the following reasons. First, they may
be too domain-specific to be of any use in general
areas such as news sentences. However, conduct-
ing an in-domain evaluation would be difficult with-
out enrolling domain experts. We expect that para-
phrases from a domain can be used safely in that
domain. Nevertheless, deciding under what circum-
stances they can be used safely in another domain is
an interesting research question.
To reduce the human labor for the evaluation, sen-
tences were restricted to those with moderate length:
10-30 words, which are expected to provide suf-
ficient but succinct context. To propose multiple
paraphrase candidates at the same time, we also re-
stricted phrases to be paraphrased (LHS phrases) to
those having at least five paraphrases including ones
from PHvst . This resulted in 60,421 paraphrases for
988 phrase tokens (353 unique phrases).
Finally, we randomly sampled 80 unique phrase
tokens and five unique paraphrases for each phrase
token (400 examples in total), and asked six people
having a high level of English proficiency to evalu-
ate them. Inter-evaluator agreement was calculated
from five different pairs of evaluators, each judging
the same 10 examples. The remaining 350 exam-
ples were divided into six chunks of slightly unequal
length, with each chunk being judged by one of the
six evaluators.
5-point Binary
n G M G M Both
PSeed 55 4.60 4.35 0.85 0.93 0.78
PHvst 295 4.22 3.35 0.74 0.67 0.55
Total 350 4.28 3.50 0.76 0.71 0.58
Table 1: Avg. score and precision of binary classification.
5.2 Results
Table 1 shows the average of the original 5-point
scale scores and the percentage of examples that
are judged correct based on a binary judgment
(Callison-Burch, 2008): an example is considered to
be correct iff the grammaticality score is 4 or above
and/or the meaning score is 3 or above. Paraphrases
based on PSeed achieved a quite high performance
in both grammaticality (?G?) and meaning (?M?) in
part because of the effectiveness of our filtering tech-
niques. The performance of paraphrases drawn from
PHvst was reasonably high and similar to the scores
0.68 for grammaticality, 0.61 for meaning, and 0.55
for both, of the best model reported in (Callison-
Burch, 2008), although it was inferior to PSeed .
Despite the fact that all of our evaluators had a
high-level command of English, the agreement was
not very high. This was true even when the col-
lected scores were mapped into binary classes. In
this case, the ? values (Cohen, 1960) for each crite-
rion were 0.45 and 0.45, respectively, which indicate
the agreement was ?fair?. To obtain a better ? value,
the criteria for grading will need to be improved.
However, we think that was not too low either8.
The most promising way for improving the qual-
ity of PHvst is to ensure that paraphrase patterns
cover only legitimate paraphrases. We investigated
this by filtering the manually scored paraphrase ex-
amples with two thresholds for cleaning seed para-
phrases PSeed : thp on the conditional probability es-
timated using the bilingual parallel corpus and ths
on the contextual similarity in the monolingual non-
parallel corpus. Figure 11 shows the average score
of the examples whose corresponding paraphrase is
obtainable with the given threshold values. Note that
the points in the figure with higher threshold values
are less reliable than the others, because filtering re-
duces the number of the manually scored examples
8Note that Callison-Burch (2008) might possibly underesti-
mate the chance agreement and overestimate the ? values, be-
cause the distribution of human scores would not be uniform.
639
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Probability threshold thp
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Similarity threshold ths
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
Figure 11: Average score of paraphrase examples against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
The points with higher threshold values are less reliable than the others,
because filtering reduces the number of the manually scored examples used to calculate scores.
used to calculate scores. Nevertheless, it indicates
that better filtering of PSeed with higher threshold
values is likely to produce a better-quality set of
paraphrases PHvst . For instance, an inappropriate
paraphrase pattern (9a) was excluded with thp = 0.1
or ths = 0.1, while correct ones (9b) and (9c) re-
mained even when a large threshold value is used.
(9) a. ?X years? ? ?turn X?
b. ?X supplied? ? ?X provided?
c. ?main X? ? ?most significant X?
Kendall?s correlation coefficient ?B (Kendall,
1938) between the contextual similarity and each of
the human scores were 0.24 for grammaticality and
0.21 for meaning, respectively. Although they are ri-
valing the best results reported in (Chan et al2011),
i.e., 0.24 and 0.21, similarity metrics should be fur-
ther investigated to realize a more accurate filtering.
6 Conclusion
In this paper, we exploited general patterns under-
lying paraphrases to acquire automatically a large
number of high-quality paraphrase pairs using both
bilingual parallel and monolingual non-parallel cor-
pora. Experiments using two sets of corpora demon-
strated that our method is able to leverage informa-
tion in a relatively small bilingual parallel corpus
to exploit large amounts of information in a rela-
tively large monolingual non-parallel corpus. Hu-
man evaluation through a paraphrase substitution
test revealed that the acquired paraphrases are gen-
erally of reasonable quality. Our original objective
was to extract from monolingual corpora a large
quantity of paraphrases whose quality is as high as
that of paraphrases from bilingual parallel corpora.
We have met the quantity part of the objective, and
have come close to meeting the quality part.
There are three main directions for our future
work. First, we intend to carry out in-depth anal-
yses of the proposed method. For instance, while
we showed that the performance of phrase substi-
tution could be improved by removing noisy seed
paraphrases, this also strongly affected the quan-
tity. We will therefore investigate similarity metrics
in our future work. Other interesting questions re-
lated to the work presented here are, as mentioned in
Section 4.2, exploitation of patterns with more than
one variable, learning curve experiments with dif-
ferent amounts of monolingual data, and compari-
son of in-domain and general-purpose monolingual
corpora. Second, we have an interest in exploiting
sophisticated paraphrase patterns; for instance, by
inducing patterns hierarchically (recursively) and in-
corporating lexical resources such as those exempli-
fied in (4). Finally, the developed paraphrase col-
lection will be attested through applications, such
as sentence compression (Cohn and Lapata, 2008;
Ganitkevitch et al2011) and machine translation
(Callison-Burch et al2006; Marton et al2009).
Acknowledgments
We are deeply grateful to our colleagues at National
Research Council Canada, especially George Foster,
Eric Joanis, and Samuel Larkin, for their technical
support. The first author is currently a JSPS (the
Japan Society for the Promotion of Science) Post-
doctoral Fellow for Research Abroad.
640
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 597?604.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Hu-
man Language Technology Conference and the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 16?23.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 17?24.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
196?205.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van-
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
Proceedings of the Workshop on Geometrical Models
of Natual Language Semantics (GEMS), pages 33?42.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 190?200.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING), pages 137?144.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalua-
tion of machine translation systems. In Proceedings of
the 6th Workshop on Statistical Machine Translation
(WMT), pages 85?91.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Conference
on Computational Linguistics (COLING), pages 350?
356.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 420?429.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the patent
translation task at the NTCIR-8 workshop. In Pro-
ceedings of NTCIR-8 Workshop Meeting, pages 371?
376.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi
Sato. 2007. A compositional approach toward dy-
namic phrasal thesaurus. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing (WTEP), pages 151?158.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel cor-
pora for text-to-text generation. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1168?1179.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. John Wiley & Sons.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
Web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1087?1097.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 341?348.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the 2007 Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975.
Maurice Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1-2):81?93.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Con-
ference and the North American Chapter of the Asso-
641
ciation for Computational Linguistics (HLT-NAACL),
pages 48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT), pages 145?
153.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 381?390.
Yuval Marton, Ahmed El Kholy, and Nizar Habash.
2011. Filtering antonymous, trend-contrasting, and
polarity-dissimilar distributional paraphrases for im-
proving statistical machine translation. In Proceedings
of the 6th Workshop on Statistical Machine Translation
(WMT), pages 237?249.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 656?666.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the Web.
In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 119?130.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of the 2003 Human Language Technol-
ogy Conference and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 102?109.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.
2005. PORTAGE: A phrase-based machine transla-
tion system. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 129?132.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the 2002
Human Language Technology Conference (HLT).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING). 849-856.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation, pages 122?125.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(4):503?526.
642
Proceedings of the 8th Workshop on Asian Language Resources, pages 1?8,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Thesaurus of Predicate-Argument Structure for Japanese Verbs
to Deal with Granularity of Verb Meanings
Koichi Takeuchi
Okayama University /
koichi@cl.cs.
okayama-u.ac.jp
Kentaro Inui
Tohoku University /
inui@ecei.
tohoku.ac.jp
Nao Takeuchi
Free Language
Analyst
Atsushi Fujita
Future University Hakodate /
fujita@fun.ac.jp
Abstract
In this paper we propose a framework
of verb semantic description in order to
organize different granularity of similar-
ity between verbs. Since verb mean-
ings highly depend on their arguments
we propose a verb thesaurus on the ba-
sis of possible shared meanings with
predicate-argument structure. Motiva-
tions of this work are to (1) construct a
practical lexicon for dealing with alter-
nations, paraphrases and entailment re-
lations between predicates, and (2) pro-
vide a basic database for statistical learn-
ing system as well as a theoretical lex-
icon study such as Generative Lexicon
and Lexical Conceptual Structure. One
of the characteristics of our description
is that we assume several granularities
of semantic classes to characterize verb
meanings. The thesaurus form allows us
to provide several granularities of shared
meanings; thus, this gives us a further re-
vision for applying more detailed analy-
ses of verb meanings.
1 Introduction
In natural language processing, to deal with
similarities/differences between verbs is essen-
tial not only for paraphrase but also textual en-
tailment and QA system which are expected
to extract more valuable facts from massively
large texts such as the Web. For example, in
the QA system, assuming that the body text
says ?He lent her a bicycle?, the answer of the
question ?He gave her a bicycle?? should be
?No?, however the answer of ?She rented the
bicycle?? should be ?Yes?. Thus construct-
ing database of verb similarities/differences en-
ables us to deal with detailed paraphrase/non-
paraphrase relations in NLP.
From the view of the current language re-
source, how the shared/different meanings of
?He lent her a bicycle? and ?He gave her a bi-
cycle? can be described? The shared mean-
ing of lend and give in the above sentences is
that they are categorized to Giving Verbs, as
in Levin?s English Verb Classes and Alterna-
tions (EVCA) (Levin, 1993), while the different
meaning will be that lend does not imply own-
ership of the theme, i.e., a bicycle. One of the
problematic issues with describing shared mean-
ing among verbs is that semantic classes such as
Giving Verbs should be dependent on the gran-
ularity of meanings we assumed. For example,
the meaning of lend and give in the above sen-
tences is not categorized into the same Frame
in FrameNet (Baker et al, 1998). The reason
for this different categorization can be consid-
ered to be that the granularity of the semantic
class of Giving Verbs is larger than that of the
Giving Frame in FrameNet1. From the view of
natural language processing, especially dealing
the with propositional meaning of verbs, all of
the above classes, i.e., the wider class of Giv-
ing Verbs containing lend and give as well as
the narrower class of Giving Frame containing
give and donate, are needed. Therefore, in this
work, in order to describe verb meanings with
several granularities of semantic classes, a the-
saurus form is adopted for our verb dictionary.
Based on the background, this paper presents
a thesaurus of predicate-argument structure for
verbs on the basis of a lexical decompo-
sitional framework such as Lexical Concep-
tual Structure (Jackendoff, 1990); thus our
1We agree with the concept of Frame and FrameEle-
ments in FrameNet but what we propose in this paper is the
necessity for granularities of Frames and FrameElements.
1
proposed thesaurus can deal with argument
structure level alternations such as causative,
transitive/intransitive, stative. Besides, tak-
ing a thesaurus form enables us to deal with
shared/differenciate meaning of verbs with con-
sistency, e.g., a verb class node of ?lend? and
?rent? can be described in the detailed layer of
the node ?give?.
We constructed this thesaurus on Japanese
verbs and the current status of the verb thesaurus
is this: we have analyzed 7,473 verb meanings
(4,425 verbs) and organized the semantic classes
in a ve-layer thesaurus with 71 semantic roles
types. Below, we describe background issues,
basic design issues, what kind of problems re-
main, limitations and perspectives of applica-
tions.
2 Existing Lexical Resources and
Drawbacks
2.1 Lexical Resources in English
From the view of previous lexical databases
In English, several well-considered lexical
databases are available, e.g., EVCA, Dorr?s
LCS (Dorr, 1997), FrameNet, WordNet (Fell-
baum, 1998), VerbNet (Kipper-Schuler, 2005)
and PropBank (Palmer et al, 2005). Be-
sides there is the research project (Pustejovsky
and Meyers, 2005) to nd general descriptional
framework of predicate argument structure by
merging several lexical databases such as Prop-
Bank, NomBank, TimeBank and PennDiscouse
Treebank.
Our approach corresponds partly to each
lexical database, (i.e., FrameNet?s Frame and
FrameElements correspond to our verb class
and semantic role labels, and the way to orga-
nize verb similarity classes with thesaurus cor-
responds with WordNet?s synset), but is not
exactly the same; namely, there is no lex-
ical database describing several granularities
of semantic classes between verbs with argu-
ments. Of course, since the above English lex-
ical databases have links with each other, it is
possible to produce a verb dictionary with sev-
eral granularities of semantic classes with argu-
ments. However, the basic categories of classify-
ing verbs would be little different due to the dif-
ferent background theory of each English lexical
database; it must be not easy to add another level
of semantic granularity with keeping consistency
for all the lexical databases; thus, thesaurus form
is needed to be a core form for describing verb
meanings2.
2.2 Lexical Resources in Japanese
In previous studies, several Japanese lexicons
were published: IPAL (IPA, 1986) focuses on
morpho-syntactic classes but IPAL is small3.
EDR (Jap, 1995) consists of a large-scale lex-
icon and corpus (See Section 3.4). EDR is
a well-considered and wide coverage dictio-
nary focusing on translation between Japanese
and English, but EDR?s semantic classes were
not designed with linguistically-motivated lex-
ical relations between verbs, e.g., alternations,
causative, transitive, and detransitive relations
between verbs. We believe these relations must
be key for dealing with paraphrase in NLP.
Recently Japanese FrameNet (Ohara et al,
2006) and Japanese WordNet (Bond et al, 2008)
are proposed. Japanese FrameNet currently
published only less than 100 verbs4. Besides
Japanese WordNet contains 87000 words and
46000 synsets, however, there are three major
difculty of dealing with paraphrase relations
between verbs: (1) there is no argument informa-
tion; (2) existing many similar synsets force us to
solve ne disambiguation between verbs when
we map a verb in a sentence to WordNet; (3) the
basic verbs of Japanese (i.e., highly ambiguous
verbs) are wrongly assigned to unrelated synsets
because they are constructed by translation from
English to Japanese.
2As Kipper (Kipper-Schuler, 2005) showed in their
examples mapping between VerbNet and WordNet verb
senses, most of the mappings are many-to-many relations;
this indicates that some two verbs grouped in a same se-
mantic type in VerbNet can be categorized into different
synsets in WordNet. Since WordNet does not have argu-
ment structure nor syntactic information, we cannot pur-
chase what is the different features for between the synsets.
3It contains 861 verbs and 136 adjectives.
4We are supplying our database to Japanese FrameNet
project.
2
3 Thesaurus of Predicate-Argument
Structure
The proposed thesaurus of predicate-argument
structure can deal with several levels of verb
classes on the basis of granularity of dened verb
meaning. In the thesaurus we incorporate LCS-
based semantic description for each verb class
that can provide several argument structure such
as construction grammar (Goldberg, 1995). This
must be high advantage to describe the different
factors from the view of not only syntactic func-
tions but also internal semantic relations. Thus
this characteristics of the proposed thesaurus can
be powerful framework for calculating similar-
ity and difference between verb senses. In the
following sections we explain the total design of
thesaurus and the details.
3.1 Design of Thesaurus
The proposed thesaurus consists of hierarchy of
verb classes we assumed. A verb class, which
is a conceptual class, has verbs with a shared
meaning. A parent verb class includes concepts
of subordinate verb class; thus a subordinate
verb class is a concretization of the parent verb
class. A verb class has a semantic description
that is a kind of semantic skeleton inspired from
lexical conceptual structure (Jackendoff, 1990;
Kageyama, 1996; Dorr, 1997). Thus a seman-
tic description in a verb class describes core se-
mantic relations between arguments and shadow
arguments of a shared meaning of the verb class.
Since verb can be polysemous, each verb sense is
designated with example sentences. Verb senses
with a shared meaning are assigned to a verb
class. Every example sentence is analyzed into
their arguments and semantic role types; and
then their arguments are linked to variables in se-
mantic description of verb class. This indicates
that one semantic description in a verb class can
provide several argument structure on the basis
of syntactic structure. This architecture is related
to construction grammar.
Here we explain this structure using verbs
such as rent, lend, give, hire, borrow, lease. We
assume that each verb sense we focus on here is
designated by example sentences, e.g., ?Mother
gives a book to her child?, ?Kazuko rents a bicy-
cle from her friend?, and ?Taro lend a car to his
friend?. As Figure 1 shows that all of the above
verb senses are involved in the verb classMoving
of One?s Possession 5. The semantic description,
which expresses core meaning of the verb class
Moving of One?s Possession is
([Agent] CAUSE) ?
BECOME [Theme] BE AT [Goal].
Where the brackets [] denote variables that can
be lled with arguments in example sentences.
Likewise parentheses () denote occasional factor.
?Agent? and ?Theme? are semantic role labels
that can be annotated to all example sentences.
Figure 1 shows that the children of the verb
class Moving of One?s Possession are the two
verb classesMoving of One?s Possession/Renting
and Moving of One?s Possession/Lending. In the
Renting class, rent, hire and borrow are there,
while in the Lending class, lend and lease exist.
Both of the semantic descriptions in the children
verb classes are more detailed ones than the par-
ent?s description.
	






	
