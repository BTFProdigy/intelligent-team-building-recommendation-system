Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960?1970,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Of words, eyes and brains:
Correlating image-based distributional semantic models
with neural representations of concepts
Andrew J. Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto, Italy)
first.last@unitn.it
Abstract
Traditional distributional semantic models ex-
tract word meaning representations from co-
occurrence patterns of words in text cor-
pora. Recently, the distributional approach has
been extended to models that record the co-
occurrence of words with visual features in
image collections. These image-based models
should be complementary to text-based ones,
providing a more cognitively plausible view
of meaning grounded in visual perception. In
this study, we test whether image-based mod-
els capture the semantic patterns that emerge
from fMRI recordings of the neural signal.
Our results indicate that, indeed, there is a
significant correlation between image-based
and brain-based semantic similarities, and that
image-based models complement text-based
ones, so that the best correlations are achieved
when the two modalities are combined. De-
spite some unsatisfactory, but explained out-
comes (in particular, failure to detect differ-
ential association of models with brain areas),
the results show, on the one hand, that image-
based distributional semantic models can be a
precious new tool to explore semantic repre-
sentation in the brain, and, on the other, that
neural data can be used as the ultimate test set
to validate artificial semantic models in terms
of their cognitive plausibility.
1 Introduction
Many recent neuroscientific studies have brought
support to the view that concepts are represented
in terms of patterns of neural activation over broad
areas, naturally encoded as vectors in a neural se-
mantic space (Haxby et al, 2001; Huth et al, 2012).
Similar representations are also widely used in com-
putational linguistics, and in particular in distribu-
tional semantics (Clark, 2012; Erk, 2012; Turney
and Pantel, 2010), that captures meaning in terms
of vectors recording the patterns of co-occurrence
of words in large corpora, under the hypothesis that
words that occur in similar contexts are similar in
meaning.
Since the seminal work of Mitchell et al (2008),
there has thus being interest in investigating whether
corpus-harvested semantic representations can con-
tribute to the study of concepts in the brain. The
relation is mutually beneficial: From the point of
view of brain activity decoding, a strong correlation
between corpus-based and brain-derived conceptual
representations would mean that we could use the
former (much easier to construct on a very large
scale) to make inferences about the second: e.g., us-
ing corpus-based representations to reconstruct the
likely neural signal associated to words we have no
direct brain data for. From the point of view of com-
putational linguistics, neural data provide the ulti-
mate testing ground for models that strive to cap-
ture important aspects of human semantic mem-
ory (much more so than the commonly used ex-
plicit semantic rating benchmarks). If we found that
a corpus-based model of meaning can make non-
trivial predictions about the structure of the semantic
space in the brain, that would make a pretty strong
case for the intriguing idea that the model is approx-
imating, in interesting ways, the way in which hu-
mans acquire and represent semantic knowledge.
1960
We take as our starting point the extensive experi-
ments reported in Murphy et al (2012), who showed
that purely corpus-based distributional models are at
least as good at brain signal prediction tasks as ear-
lier models that made use of manually-generated or
controlled knowledge sources (Chang et al, 2011;
Palatucci et al, 2009; Pereira et al, 2011), and we
evaluate a very recent type of distributional model,
namely one that is not extracted from textual data
but from image collections through automated vi-
sual feature extraction techniques. It has been ar-
gued that this new generation of image-based dis-
tributional models (Bruni et al, 2011; Bruni et al,
2012b; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011) provides a more realistic view of mean-
ing, since humans obviously acquire a large propor-
tion of their semantic knowledge from perceptual
data. The first question that we ask, thus, is whether
the more ?grounded? image-based models can help
us in interpreting conceptual representations in the
brain. More specifically, we will compare the per-
formance of different image-based representations,
and we will test whether text- and image-based rep-
resentations are complementary, so that when used
together they can better account for patterns in neu-
ral data. Finally, we will check for differences be-
tween anatomical regions in the degree to which text
and/or image models are effective, as one might ex-
pect given the well-known functional specializations
of different anatomical regions.
2 Brain data
We use the data that were recorded and preprocessed
by Mitchell et al (2008), available for download in
their supporting online material.1 Full details of the
experimental protocol, data acquisition and prepro-
cessing can be found in Mitchell et al (2008) and
the supporting material. Key points are that there
were nine right-handed adult participants (5 female,
age between 18 and 32). The experimental task was
to actively think about the properties of sixty objects
that were presented visually, each as a line drawing
in combination with a text label. The entire set of
objects was presented in a random order in six ses-
sions, each object remained on screen for 3 seconds
with a seven second fixation gap between presenta-
1http://www.cs.cmu.edu/?tom/science2008/
tions.
Mitchell and colleagues examined 12 categories,
five objects per category, for a total of 60 concepts
(words). Due to coverage limitations, we use 51/60
words representing 11/12 categories. Table 1 con-
tains the full list of 51 words organized by category.
fMRI acquisition and preprocessing Mitchell et
al. (2008) acquired functional images on a Siemens
Allegra 3.0T scanner using a gradient echo EPI
pulse sequence with TR=1000 ms, TE=30 ms and
a 60? angle. Seventeen 5-mm thick oblique-axial
slices were imaged with a gap of 1-mm between
slices. The acquisition matrix was 64?64 with
3.125?3.125?5-mm voxels. They subsequently
corrected data for slice timing, motion, linear trend,
and performed temporal smoothing with a high-pass
filter at 190s cutoff. The data were normalized to
the MNI template brain image, spatially normalized
into MNI space and resampled to 3?3?6 mm3 vox-
els. The voxel-wise percent signal change relative to
the fixation condition was computed for each object
presentation. The mean of the four images acquired
4s post stimulus presentation was used for analysis.
To create a single representation per object per
participant, we took the voxel-wise mean of the six
presentations of each word. Likewise to create a sin-
gle representation per category per participant, we
took the voxel-wise mean of all word models per
category, per participant.
Anatomical parcellation Analysis was conducted
on the whole brain, and to address the question of
whether there are differences in models? effective-
ness between anatomical regions, brains were fur-
ther partitioned into frontal, parietal, temporal and
occipital lobes. This partitioning is coarse (each lobe
is large and serves many diverse functions), but, for
an initial test, appropriate, given that each lobe has
specialisms that on face value are amenable to inter-
pretation by our different distributional models and
the exact nature of specialist processing in localised
areas is often subject to debate (so being overly re-
strictive may be risky). Formulation of the distribu-
tional models is described in detail in the Section 3,
but for now it is sufficient to know that the Object
model is derived from image statistics of the object
depicted in images, Context from image statistics of
the background scene, Object&Context is a com-
1961
Animals Bear, Cat, Cow, Dog Horse
Building Apartment, Barn, Church, House
Building parts Arch, Chimney, Closet, Door, Window
Clothing Coat, Dress, Pants, Shirt, Skirt
Furniture Bed, Chair, Desk, Dresser, Table
Insect Ant, Bee, Beetle, Butterfly, Fly
Kitchen utensils Bottle, Cup, Glass, Knife, Spoon
Man made objects Bell, Key, Refrigerator, Telephone, Watch
Tool Chisel, Hammer, Screwdriver
Vegetable Celery, Corn, Lettuce, Tomato
Vehicle Airplane, Bicycle, Car, Train, Truck
Table 1: The 51 words represented by the brain and the distributional models, organized by category.
bination of the two, and Window2 is a text-based
model.
The occipital lobe houses the primary visual pro-
cessing system and consequently it is reasonable
to expect some bias toward image-based semantic
models. Furthermore, given that experimental stim-
uli incorporated line drawings of the object,and the
visual cortex has a well-established role in process-
ing low-level visual statistics including edge detec-
tion (Bruce et al, 2003), we naturally expected a
good performance from Object (formulated from
edge orientation histograms of similar objects).
Following Goodale and Milner (1992)?s influ-
ential perception-action model (see McIntosh and
Schenk (2009) for recent discussion), visual infor-
mation is channeled from the occipital lobe in two
streams: a perceptual stream, serving object identi-
fication and recognition; and an action stream, spe-
cialist in processing egocentric spatial relationships
and ultimately supporting interaction with the world.
The perceptual stream leads to the temporal lobe.
Here the fusiform gyrus (shared with the occipital
lobe) plays a general role in object categorisation
(e.g., animals and tools (Chao et al, 1999), faces
(Kanwisher and Yovel, 2006), body parts (Peelen
and Downing, 2005) and even word form percep-
tion (McCandliss et al, 2003)). As the parahip-
pocampus is strongly associated with scene repre-
sentation (Epstein, 2008), we expect both the Object
and Context models to capture variability in the tem-
poral lobe. Of wider relevance to semantic process-
ing, the medial temporal gyrus, inferior temporal
gyrus and ventral temporal lobe have generally been
implicated to have roles in supramodal integration
and concept retrieval (Binder et al, 2009). Given
this, we expected that incorporating text would also
be valuable and that the Window2&Object&Context
combination would be a good model.
The visual action stream leads from the occipi-
tal lobe to the parietal lobe to support spatial cog-
nition tasks and action control (Sack, 2009). In
that there seems to be an egocentric frame of ref-
erence, placing actor in environment, it is tempt-
ing to speculate that the Context model is more ap-
propriate than the Object model here. As the pari-
etal lobe also contains the angular gyrus, thought
to be involved in complex, supra-modal information
integration and knowledge retrieval (Binder et al,
2009), we might again forecast that integrating text
and image information would boost performance, so
Window2&Context was earmarked as a strong can-
didate.
The frontal lobe, is traditionally associated with
high-level processing and manipulation of abstract
knowledge and rules and controlled behaviour
(Miller et al, 2002). Regarding semantics, the dor-
somedial prefrontal cortex has been implicated in
self-guided retrieval of semantic information (e.g.,
uncued speech production), the ventromedial pre-
frontal cortex in motivation and emotional process-
ing, the inferior frontal gyrus in phonological and
syntactic processing, (Binder et al, 2009) and in-
tegration of lexical information (Hagoort, 2005).
Given the association with linguistic processing we
anticipated a bias in favour of Window2.
The four lobes were identified and partitioned
using Tzourio-Mazoyer et al (2002)?s automatic
anatomical labelling scheme.
1962
Voxel selection The set of 500 most stable voxels,
both within the whole brain and from within each
region of interest were identified for analysis. The
most stable voxels were those showing consistent
variation across the different stimuli between scan-
ning sessions. Specifically, and following a similar
strategy to Mitchell et al (2008), for each voxel, the
set of 51 words from each unique pair of scanning
sessions were correlated using Pearson?s correlation
(6 sessions and therefore 15 unique pairs), and the
mean of the 15 resulting correlation coefficients was
taken as the measure of stability. The 500 voxels
with highest mean correlations were selected.
3 Distributional models
Distributional semantic models approximate word
meaning by keeping track of word co-occurrence
statistics from large textual input, relying on the dis-
tributional hypothesis: The meaning of a word can
be induced by the context in which it occurs (Turney
and Pantel, 2010). Despite their great success, these
models still rely on verbal input only, while humans
base their meaning representation also on perceptual
information (Louwerse, 2011).
Thanks to recent developments in computer vi-
sion, it is nowadays possible to take the visual per-
ceptual channel into account, and build new com-
putational models of semantics enhanced with vi-
sual information (Feng and Lapata, 2010; Bruni et
al., 2011; Leong and Mihalcea, 2011; Bergsma and
Goebel, 2011; Bruni et al, 2012a). Given a set of
target concepts and a collection of images depicting
those concepts, it is indeed possible to first encode
the image content into low-level features, and subse-
quently convert it into a higher-level representation
based on the bag-of-visual-words method (Grauman
and Leibe, 2011). Recently, Bruni et al (2012b)
have shown that better semantic representations can
be extracted if we first localize the concept in the
image, and then extract distinct higher-level features
(visual words) from the box containing the concept
and from the surrounding context. We also follow
this strategy here.
In our experiments we utilize both traditional text-
based models and experimental image-based mod-
els, as well as their combination.
3.1 Textual models
Verb We experiment with the original text-based
semantic model used to predict fMRI patterns by
Mitchell et al (2008). Each object stimulus word
is represented as a 25-dimensional vector, with each
value corresponding to the normalized sentence-
wide co-occurrence of that word with one of 25
manually-picked sensorimotor verbs (such as see,
hear, eat, . . . ) in a trillion word text corpus.
Window2 To create this model, we collect text
co-occurrence statistics from the freely available
ukWaC and Wackypedia corpora combined (about 3
billion words in total).2 As collocates of our distri-
butional model we select a set of 30K words, namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs.
In the tradition of HAL (Lund and Burgess, 1996),
the model is based on co-occurrence statistics with
collocates within a fixed-size window of 2 to the left
and right of each target word. Despite their sim-
plicity, narrow-window-based models have shown
to achieve state-of-the-art results in various stan-
dard semantic tasks (Bullinaria and Levy, 2007)
and to outperform both document-based and syntax-
based models trained on the same corpus (Bruni et
al., 2012a). Moreover, in Murphy et al (2012) a
window-based model very similar to ours was not
significantly worse than their best model for brain
decoding. We tried also a few variations, e.g., us-
ing a larger window or different transformations on
the raw co-occurrences from those presented below,
but with little, insignificant changes in performance.
Given that our focus here is on visual information,
we only report results for Window2 and its combi-
nation with visual models.
3.2 Visual models
Our visual models are inspired by Bruni et al
(2012b), that have explored to what extent extract-
ing features from images where objects are local-
ized results in better semantic representations. They
found that extracting visual features separately from
the object and its surrounding context leads to bet-
ter performance than not using localization, and us-
ing only object- and, more surprisingly, context-
extracted features also results in performant models
2http://wacky.sslmit.unibo.it/
1963
(especially when evaluating inter-object similarity,
the context in which an object is located can signif-
icantly contribute to semantic representation, in cer-
tain cases carrying even more information than the
depicted object itself).
More in detail, with localization the visual fea-
tures (visual words) can be extracted from the ob-
ject bounding box (in our experiments, the Object
model) or from only outside the object box (the
Context model). A combined model is obtained
by concatenating the two feature vectors (the Ob-
ject&Context model).
Visual model construction pipeline To extract
visual co-occurrence statistics, we use images from
ImageNet (Deng et al, 2009),3 a very large im-
age database organized on top of the WordNet hi-
erarchy (Fellbaum, 1998). ImageNet has more than
14 million images, covering 21K WordNet nominal
synsets. ImageNet stands out for the high quality of
its images, both in terms of resolution and concept
annotations. Moreover, for around 3K concepts, an-
notations of object bounding boxes is provided. This
last feature allows us to exploit object localization
within our experiments.
To build visual distributional models, we utilize
the bag-of-visual-words (BoVW) representation of
images (Sivic and Zisserman, 2003; Csurka et al,
2004). Inspired by NLP, BoVW discretizes the im-
age content in terms of a histogram of visual word
counts. Differently from NLP, in vision there is not a
natural notion of visual words, hence a visual vocab-
ulary has to be built from scratch. The process works
as follows. First, a large set of low-level features is
extracted from a corpus of images. The low-level
feature vectors are subsequently clustered into dif-
ferent regions (visual words). Given then a new im-
age, each of the low-level feature vectors extracted
from the patches that compose it is mapped to the
nearest visual word (e.g., in terms of Euclidean dis-
tance from the cluster centroid) such that the image
can be represented with a histogram counting the in-
stances of each visual word in the image.
As low-level features we use SIFT, the Scale In-
variant Feature Transform (Lowe, 2004). SIFT fea-
tures are good at capturing parts of objects and are
designed to be invariant to image transformations
3http://www.image-net.org/
such as change in scale, rotation and illumination.
To construct the visual vocabulary, we cluster the
SIFT features into 25K different clusters.4 We add
also spatial information by dividing the image into
several subregions, representing each of them in
terms of BoVW and then stacking the resulting his-
tograms (Lazebnik et al, 2006). We use in total 8
different regions, obtaining a final vector of 200K
dimensions (25K visual words ? 8 regions). Since
each concept in our dataset is represented by mul-
tiple images, we pool the visual word occurrences
across images by summing them up into a single
vector.
To perform the entire visual pipeline we use
VSEM, an open library for visual semantics (Bruni
et al, 2013).5
3.3 Model transformations and combination
Once both the textual and the visual models are built,
we perform two different transformations on the raw
co-occurrence counts. First, we transform them into
nonnegative Pointwise Mutual Information (PMI)
association scores (Church and Hanks, 1990). As a
second transformation, we apply dimensionality re-
duction to the two matrices. In particular, we adopt
the Singular Value Decomposition (SVD), one of the
most effective methods to approximate the original
data in lower dimensionality space (Schu?tze, 1997),
and reduce the vectors to 50 dimensions.
To combine text- and image-based semantic mod-
els in a joint representation, we separately normalize
their vectors to unit length, and concatenate them,
along the lines of Bruni et al (2011). More sophis-
ticated combination models have been proposed in
the recent literature on multimodal semantics. For
example, Bruni et al (2012a) use SVD as a mix-
ing strategy, given its ability to smooth the matrices
and uncover latent dimensions. Another example is
Silberer and Lapata (2013), where Canonical Corre-
lation Analysis is used. We reserve the exploration
of more advanced combination methods for further
studies.
Finally, to represent the 11 categories we experi-
ment with (see Table 1), we average the vectors of
the concepts they include.
4We use k-means, the most commonly employed clustering
algorithm for this task.
5http://clic.cimec.unitn.it/vsem/
1964
4 Experiments
A question is posed over how to evaluate the rela-
tionship between the different distributional models
and brain data. Comparing each model?s predictive
performance using the same strategy as Mitchell et
al. (2008) (also followed by Murphy et al (2012))
is one possibility: they used multiple regression to
relate distributional codes to individual voxel activa-
tions, thus allowing brain states to be estimated from
previously unseen distributional codes. Regression
models were trained on 58/60 words and in testing
the regression models estimated the brain state as-
sociated with the 2 unseen distributional codes. The
predicted brain states were compared with the actual
fMRI data, and the process repeated for each per-
mutation of left-out words, to build a metric of pre-
diction accuracy. For our purposes, a fair compari-
son of models using this strategy is complicated by
differences in dimensionality between both seman-
tic models and lobes (which we compare to other
lobes) in association with the comparatively small
number of words in the fMRI data set. Large dimen-
sionality models risk overfitting the data, and it is a
nuisance to try to reliably correct for the effects of
overfitting in performance comparisons. Not least,
to thoroughly evaluate all possible cross-validation
permutations is demanding in processing time, and
we have many models to compare.
An alternative approach, and that which we
have adopted, is representational similarity analy-
sis (Kriegeskorte et al, 2008). Representational
similarity analysis circumvents the previous prob-
lems by abstracting each fMRI/distributional data
source to a common structure capturing the inter-
relationships between each pair of data items (e.g.,
words). Specifically, for each model/participant?s
fMRI data/anatomical region, the similarity struc-
ture was evaluated by taking the pairwise correla-
tion (Pearson?s correlation coefficient) between all
unique category or word combinations. This pro-
duced a list of 55 category pair correlations and 121
word pair correlations for each data source. For all
brain data, correlation lists were averaged across the
nine participants to produce a single list of mean
word pair correlations and a single list of mean cat-
egory pair correlations for each anatomical region
and the whole brain. Then to provide a measure of
similarity between models and brain data, the cor-
relation lists for respective data sources were them-
selves correlated using Spearman?s rank correlation.
Statistical significance was tested using a permuta-
tion test: The word-pair (or category-pair) labels
were randomly shuffled 10,000 times to estimate a
null distribution when the two similarity lists are
not correlated. The p-value is calculated as the pro-
portion of random correlation coefficients that are
greater than or equal to the observed coefficient.
5 Results
5.1 Category-level analyses
Do image models correlate with brain data? Ta-
ble 2 displays results of Spearman?s correlations be-
tween the per-category similarity structure of dis-
tributional models and brain data. There is a sig-
nificant correlation between every purely image-
based model and the occipital, parietal and tempo-
ral lobes, and also the whole brain (.38? ? ?.51,
all p?.01). The frontal lobe is less well described.
Still, whilst not significant, correlations are only
marginally above the conventional p = .05 cutoff
(all are less than p = .064). This strongly suggests
that the answer to our first question is yes: distri-
butional models derived from images can be used
to explain concept fMRI data. Otherwise Window2
significantly correlates with the whole brain and all
anatomical regions except for the frontal lobe where
?=.34, p = .07. In contrast Verb (the original, par-
tially hand-crafted model used by Mitchell and col-
leagues) captures inter-relationships poorly and nei-
ther correlates with the whole brain or any lobe.
Do different models correlate with different
anatomical regions? 2-way ANOVA without
replication was used to test for differences in cor-
relation coefficients between the five pure-modality
models (Verb, Window2, Object, Context and Ob-
ject&Context), and the four brain lobes. This re-
vealed a highly significant difference between mod-
els F(4,12)=45.2, p<.001. Post-hoc 2-tailed t-tests
comparing model pairs found that Verb differed sig-
nificantly from all other models (correlations were
lower). There was a clear difference even when Verb
(mean?sd over lobes = .1?.1) was compared to the
second weakest model, Object (mean?sd=.4?.09),
where t =-7.7, p <.01, df=4. There were no
1965
Frontal Parietal Occipital Temporal Whole-Brain
Verb 0.00 (0.51) 0.06 (0.37) 0.24 (0.10) 0.07 (0.35) 0.17 (0.17)
Window2 0.34 (0.06) 0.49 (0.00) 0.47 (0.01) 0.47 (0.00) 0.44 (0.00)
Object 0.27 (0.07) 0.38 (0.02) 0.45 (0.00) 0.47 (0.00) 0.43 (0.01)
Context 0.33 (0.06) 0.50 (0.00) 0.44 (0.00) 0.44 (0.01) 0.44 (0.01)
Object&Context 0.32 (0.05) 0.48 (0.00) 0.51 (0.00) 0.49 (0.00) 0.49 (0.00)
Window2&Object 0.32 (0.06) 0.45 (0.00) 0.52 (0.00) 0.53 (0.00) 0.49 (0.00)
Window2&Context 0.39 (0.04) 0.57 (0.00) 0.53 (0.00) 0.55 (0.00) 0.51 (0.00)
Window2&Object&Context 0.37 (0.04) 0.52 (0.00) 0.55 (0.00) 0.55 (0.00) 0.53 (0.00)
Table 2: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 11 categories. In each column the first value corre-
sponds to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
Frontal Parietal Occipital Temporal Whole-Brain
Verb -0.04 (0.72) 0.09 (0.06) 0.07 (0.20) 0.03 (0.31) 0.07 (0.18)
Window2 0.07 (0.13) 0.19 (0.00) 0.12 (0.06) 0.21 (0.00) 0.13 (0.04)
Object 0.01 (0.40) 0.08 (0.07) 0.17 (0.01) 0.18 (0.00) 0.17 (0.01)
Context 0.04 (0.24) 0.14 (0.01) 0.01 (0.44) 0.12 (0.02) 0.02 (0.38)
Object&Context 0.03 (0.31) 0.13 (0.01) 0.10 (0.07) 0.17 (0.00) 0.11 (0.06)
Window2&Object 0.04 (0.24) 0.16 (0.00) 0.16 (0.01) 0.23 (0.00) 0.17 (0.00)
Window2&Context 0.07 (0.12) 0.20 (0.00) 0.09 (0.11) 0.22 (0.00) 0.11 (0.07)
Window2&Object&Context 0.05 (0.18) 0.18 (0.00) 0.12 (0.05) 0.23 (0.00) 0.13 (0.02)
Table 3: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 51 words. In each column the first value corresponds
to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
other significant differences between models. How-
ever there was a highly significant difference be-
tween lobes F(3,12)=13.77, p <.001. Post-hoc 2-
tailed t-tests comparing lobe pairs found that the
frontal lobe yielded significantly different correla-
tions (lower) than each other lobe. When the frontal
lobe (mean?sd over models = .25?.14) was com-
pared to the second weakest anatomical region, the
parietal lobe (mean?sd=.38?.19), the difference
was highly significant, t =-8, df=3, p <.01. This
introduces the question of whether this difference in
correlations is the result of differences in neural cat-
egory organisation and representation, or differences
in the quality of the signal, which we address next.
Category-level inter-correlations between lobes
were all relatively strong and highly significant. The
occipital lobe was found to be the most distinct, be-
ing similar to the temporal lobe (?=.71, p <.001),
but less so to the parietal and frontal lobes (?=.53,
p <.001 and ?=.57, p <.001 respectively). The
temporal lobe shows roughly similar levels of cor-
relation to each other lobe (all .71? ? ?.73, all
p <.001). The frontal and parietal lobes are related
most strongly to each other (?=.77, p <.001), to a
slightly lesser extent to the temporal lobe (in both
cases ?=.73, p <.001) and least so to the occipital
lobe. These strong relationships are consistent with
there being a broadly similar category organisation
across lobes.
To appraise this assertion in the context of the
previously detected difference between the frontal
lobe and all other lobes, we examine the raw cat-
egory pair similarity matrices derived from the oc-
cipital lobe and the frontal lobe (Figure 1). All the
below observations are qualitative. Although it is
difficult to have intuitions about the relative differ-
ences between all category pairs (e.g., whether tools
or furniture should be more similar to animals), we
might reasonably expect some obvious similarities.
For instance, for animals to be visually similar to in-
1966
sects and clothing, because all have legs and arms
and curves (of course we would not expect a strong
relationship between insects and clothes in function
or other modalities such as sound), buildings to be
similar to building parts and vehicles (hard edges
and windows), building parts to be similar to furni-
ture (e.g., from Table 1 we see there is some overlap
in category membership between these categories,
such as closet and door) and tools to be similar to
kitchen utensils. All of these relationships are main-
tained in the occipital lobe, and many are visible in
the frontal lobe (including the similarity between in-
sects and clothes), however there are exceptions that
are difficult to explain e.g., within the frontal lobe,
building parts are not similar to furniture, kitchen
utensils are closer to clothing than to tools and ve-
hicles are more similar to clothing than anything
else. As such we conclude that category-level rep-
resentations were similar across lobes with differ-
ences likely due to variation in signal quality be-
tween lobes.
Are text- and image-based semantic models com-
plementary? Turning to the question of whether
text- and image-derived semantic information can
be complementary, we observe from Table 2 that
there is not a single instance of a joint model with
a weaker correlation than its pure-image counter-
part. The Window2 model showed a stronger cor-
relation than the Window2&Object model for the
frontal and parietal lobes, but was weaker than Win-
dow2&Object&Context and Window2&Context in
all tests and was also weaker than any joint model
in whole-brain comparisons. The mean?sd correla-
tions for all purely image-based results pooled over
lobes (3 models * 4 lobes) was .42?.08 in com-
parison to .49?.08 for the joint models. The rel-
ative performance of Object vs. Context vs. Ob-
ject&Context on the four different lobes is preserved
between image-based and joint models: correlating
the 12 combinations using Spearman?s correlation
gives ?=.85, p <.001. Differences can be statis-
tically quantified by pooling all image related cor-
relation coefficients for each anatomical region (3
models * 4 regions), as for the respective joint mod-
els, and comparing with a 2-tailed Wilcoxon signed
rank test. Differences were highly significant (W=0,
p <.001,n=12). This evidence accumulates to sug-
Figure 1: Similarity (Pearson correlation) between each
category pair in (top) occipital and (bottom) frontal lobes.
gest that text and image-derived semantic informa-
tion can be complementary in interpreting concept
fMRI data.
5.2 Word-level analyses
Do image models capture word pair similari-
ties? Per-word results generally corroborate the
relationships observed in the previous section in
the sense that Spearman?s correlation between per-
word and per-category results for the 40 combina-
tions of models and lobes was ?=.78, p <.001.
There were differences, most obviously a dramatic
drop in the strength of correlation coefficients for
the per-word results, visible in Table 3. Subsets
1967
of per-word image-based models correlated with
three lobes and the whole brain. Correlations corre-
sponding to significance values of p <.05 were ob-
served in the temporal and parietal lobes, for Con-
text, Object&Context and Window2 whereas Ob-
ject was correlated with the occipital and temporal
lobes (p <.05). 2-way ANOVA without replica-
tion was used to test for differences between mod-
els and lobes. This revealed a significant differ-
ence between models (F(4,12)=4.05, p=.027). Post-
hoc t-tests showed that the Window2 model signifi-
cantly differed from (was stronger than) the Context
(t=3.8, p =.03, df=3) and Object&Context models
(t =4.5, p =.02, df=3). There were no other signifi-
cant differences between models. There was again a
significant difference between lobes (F(3,12)=7.89,
p < .01), with the frontal lobe showing the weak-
est correlations. Post-hoc 2-tailed t-tests comparing
lobe-pairs found that the frontal lobe differed signif-
icantly (correlations were weaker) from the parietal
(t =-9, p <.001, df=4) and temporal lobes (t =-6.4,
p <.01, df=4) but not from the occipital lobe (t =-
2.18, p =.09, df=4). No other significant differences
between lobes were observed.
Are there differences between models/lobes?
Word-level inter-correlations between lobes were all
significant and the pattern of differences in correla-
tion strength largely resembled that of the category-
level analyses. The occipital lobe was again most
similar to the temporal lobe (?=.57, p <.001), but
less so to the parietal and frontal lobes (?=.47,
p <.001 and ?=.34, p <.001 respectively). The
temporal lobe this time showed stronger correlation
to the parietal (?=.68, p <.001) and frontal lobes
(?=.61, p <.001) than the occipital lobe. The frontal
and parietal lobes were again strongly related to one
another (?=.67, p <.001). These results echo the
category-level findings, that word-level brain activ-
ity is also organised in a similar way across lobes.
Consequently this diminishes our chances of uncov-
ering neat interactions between models and brain ar-
eas (where for instance the Window2 model corre-
lates with the frontal lobe and Object model matches
the occipital lobe). It is however noteworthy that
we can observe some interpretable selectivity in
lobe*model combinations. In particular the Con-
text model better matches the parietal lobe than the
Object model, which in turn better captures the oc-
cipital and temporal lobes (Observations are quali-
tative). Also as we see next, adding text informa-
tion boosts performance in both parietal and tempo-
ral lobes (see Section 2 on our expectations about
information encoded in the lobes).
Does joining text and image models help word-
level interpretation? As concerns the benefits of
joining Text and Image information, per-word joint
models were generally stronger than the respective
image-based models. There was one exception:
adding text to the Object model weakened corre-
lation with the occipital lobe. Joint models were
exclusively stronger than Window2 for the tempo-
ral and occipital lobes, and were stronger in 1/3 of
cases for the frontal and parietal lobes. In an anal-
ogous comparison to the per-category analysis, a
Wilcoxon signed rank test was used to examine the
difference made by adding text information to image
models (pooling 3 models over 4 anatomical areas
for both image and joint models). The mean?sd of
image models was .1?.06 whereas for Joint models
it was .15?.07. The difference was highly signifi-
cant (W=1, p <.001, n=12).
6 Conclusion
This study brought together, for the first time, two
recent research lines: The exploration of ?seman-
tic spaces? in the brain using distributional semantic
models extracted from corpora, and the extension
of the latter to image-based features. We showed
that image-based distributional semantic measures
significantly correlate with fMRI-based neural sim-
ilarity patterns pertaining to categories of concrete
concepts as well as concrete basic-level concepts ex-
pressed by specific words (although correlations, es-
pecially at the basic-concept level, are rather low,
which might signify the need to develop still more
performant distributional models and/or noise inher-
ent to neural data). Moreover, image-based mod-
els complement a state-of-the-art text-based model,
with the best performance achieved when the two
modalities are combined. This not only presents an
optimistic outlook for the future use of image-based
models as an interpretative tool to explore issues of
cognitive grounding, but also demonstrates that they
are capturing useful additional aspects of meaning to
1968
the text models, which are likely relevant for com-
putational semantic tasks.
The weak comparative performance of the origi-
nal Mitchell et al?s Verb model is perhaps surprising
given its previous success in prediction (Mitchell et
al., 2008), but a useful reminder that a good predic-
tor does not necessarily have to capture the internal
structure of the data it predicts.
The lack of finding organisational differences be-
tween anatomical regions differentially described by
the various models is perhaps disappointing, but not
uncontroversial, given that the dataset was not origi-
nally designed to tease apart visual information from
linguistic context. It is however interesting that
in the more challenging word-level analysis some
meaningful trend was visible. In future experiments
it may prove valuable to configure a fMRI stimulus
set where text-based and image-based interrelation-
ships are maximally different. Collecting our own
fMRI data will also allow us to move beyond ex-
ploratory analysis, to test sharper predictions about
distributional models and their brain area correlates.
There are also many opportunities for focusing anal-
yses on different subsets of brain regions, with the
semantic system identified by Binder et al (2009) in
particular presenting one interesting avenue for in-
vestigation.
Acknowledgments
This research was partially funded by a Google Re-
search Award to the fifth author.
References
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of RANLP, pages 399?405, Hissar, Bulgaria.
Jeffrey R. Binder, Rutvik H. Desai, William W. Graves,
and Lisa L. Conant. 2009. Where is the semantic
system? a critical review and meta-analysis of 120
functional neuroimaging studies. Cerebral Cortexl,
12:2767?2796.
Vicki Bruce, Patrick R Green, and Georgeson Mark A.
2003. Visual perception: Physiology, psychology, and
ecology. Psychology Pr.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22?
32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics in
Technicolor. In Proceedings of ACL, pages 136?145,
Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings of ACM Mul-
timedia, pages 1219?1228, Nara, Japan.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open li-
brary for visual semantics representation. In Proceed-
ings of ACL, Sofia, Bulgaria.
John Bullinaria and Joseph Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39:510?526.
Kai-min Chang, Tom Mitchell, and Marcel Just. 2011.
Quantitative modeling of the neural representation of
objects: How semantic feature norms can account for
fMRI activation. NeuroImage, 56:716?727.
Linda L Chao, James V Haxby, and Alex Martin. 1999.
Attribute-based neural substrates in temporal cortex
for perceiving and knowing about objects. Nature neu-
roscience, 2(10):913?919.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22, Prague, Czech Republic.
Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248?255, Miami Beach, FL.
Russell A Epstein. 2008. Parahippocampal and ret-
rosplenial contributions to human spatial navigation.
Trends in cognitive sciences, 12(10):388?396.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91?99, Los Angeles, CA.
1969
Melvyn A. Goodale and David Milner. 1992. Separate
visual pathways for perception and action. Trends in
Neurosciences, 15:20?25.
Kristen Grauman and Bastian Leibe. 2011. Visual Object
Recognition. Morgan & Claypool, San Francisco.
Peter Hagoort. 2005. On Broca, brain, and bind-
ing: a new framework. Trends in cognitive sciences,
9(9):416?423.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces and
objects in ventral temporal cortex. Science, 293:2425?
2430.
Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gal-
lant. 2012. A continuous semantic space describes the
representation of thousands of object and action cate-
gories across the human brain. Neuron, 76(6):1210?
1224.
Nancy Kanwisher and Galit Yovel. 2006. The fusiform
face area: a cortical region specialized for the percep-
tion of faces. Philosophical Transactions of the Royal
Society B: Biological Sciences, 361(1476):2109?2128.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008. Representational similarity analysis?
connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of CVPR, pages 2169?2178, Washington,
DC.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David G Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60:91?110.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Bruce D McCandliss, Laurent Cohen, and Stanislas De-
haene. 2003. The visual word form area: expertise
for reading in the fusiform gyrus. Trends in cognitive
sciences, 7(7):293?299.
Robert D McIntosh and Thomas Schenk. 2009. Two vi-
sual streams for perception and action: Current trends.
Neuropsychologia, 47(6):1391?1396.
Earl K Miller, David J Freedman, and Jonathan D Wal-
lis. 2002. The prefrontal cortex: categories, con-
cepts and cognition. Philosophical Transactions of
the Royal Society of London. Series B: Biological Sci-
ences, 357(1424):1123?1136.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason, and
Marcel Just. 2008. Predicting human brain activ-
ity associated with the meanings of nouns. Science,
320:1191?1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012.
Selecting corpus-semantic models for neurolinguistic
decoding. In Proceedings of *SEM, pages 114?123,
Montreal, Canada.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and
Tom Mitchell. 2009. Zero-shot learning with seman-
tic output codes. In Proceedings of NIPS, pages 1410?
1418, Vancouver, Canada.
Marius V Peelen and Paul E Downing. 2005. Selectivity
for the human body in the fusiform gyrus. Journal of
Neurophysiology, 93(1):603?608.
Francisco Pereira, Greg Detre, and Matthew Botvinick.
2011. Generating text from functional brain images.
Frontiers in Human Neuroscience, 5(72). Published
online: http://www.frontiersin.org/
human_neuroscience/10.3389/fnhum.
2011.00072/abstract.
Alexander T Sack. 2009. Parietal cortex and spatial cog-
nition. Behavioural brain research, 202(2):153?161.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Carina Silberer and Mirella Lapata. 2013. Models of
semantic representation with visual attributes. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of ICCV, pages 1470?1477, Nice,
France.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
N Tzourio-Mazoyer, B Landeau, D Papathanassiou,
F Crivello, O Etard, N Delcroix, B Mazoyer, and M Jo-
liot. 2002. Automated anatomical labeling of activa-
tions in SPM using a macroscopic anatomical parcel-
lation of the MNI MRI single-subject brain. Neuroim-
age, 15(1):273?289.
1970
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136?145,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Distributional Semantics in Technicolor
Elia Bruni
University of Trento
elia.bruni@unitn.it
Gemma Boleda
University of Texas at Austin
gemma.boleda@utcompling.com
Marco Baroni
Nam-Khanh Tran
University of Trento
name.surname@unitn.it
Abstract
Our research aims at building computational
models of word meaning that are perceptually
grounded. Using computer vision techniques,
we build visual and multimodal distributional
models and compare them to standard textual
models. Our results show that, while visual
models with state-of-the-art computer vision
techniques perform worse than textual models
in general tasks (accounting for semantic re-
latedness), they are as good or better models
of the meaning of words with visual correlates
such as color terms, even in a nontrivial task
that involves nonliteral uses of such words.
Moreover, we show that visual and textual in-
formation are tapping on different aspects of
meaning, and indeed combining them in mul-
timodal models often improves performance.
1 Introduction
Traditional semantic space models represent mean-
ing on the basis of word co-occurrence statistics in
large text corpora (Turney and Pantel, 2010). These
models (as well as virtually all work in computa-
tional lexical semantics) rely on verbal information
only, while human semantic knowledge also relies
on non-verbal experience and representation (Louw-
erse, 2011), crucially on the information gathered
through perception. Recent developments in com-
puter vision make it possible to computationally
model one vital human perceptual channel: vision
(Mooney, 2008). A few studies have begun to use
visual information extracted from images as part of
distributional semantic models (Bergsma and Van
Durme, 2011; Bergsma and Goebel, 2011; Bruni et
al., 2011; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011). These preliminary studies all focus on
how vision may help text-based models in general
terms, by evaluating performance on, for instance,
word similarity datasets such as WordSim353.
This paper contributes to connecting language and
perception, focusing on how to exploit visual infor-
mation to build better models of word meaning, in
three ways: (1) We carry out a systematic compari-
son of models using textual, visual, and both types of
information. (2) We evaluate the models on general
semantic relatedness tasks and on two specific tasks
where visual information is highly relevant, as they
focus on color terms. (3) Unlike previous work, we
study the impact of using different kinds of visual
information for these semantic tasks.
Our results show that, while visual models with
state-of-the-art computer vision techniques perform
worse than textual models in general semantic tasks,
they are as good or better models of the mean-
ing of words with visual correlates such as color
terms, even in a nontrivial task that involves nonlit-
eral uses of such words. Moreover, we show that vi-
sual and textual information are tapping on different
aspects of meaning, such that they are complemen-
tary sources of information, and indeed combining
them in multimodal models often improves perfor-
mance. We also show that ?hybrid? models exploit-
ing the patterns of co-occurrence of words as tags
of the same images can be a powerful surrogate of
visual information under certain circumstances.
The rest of the paper is structured as follows. Sec-
tion 2 introduces the textual, visual, multimodal,
136
and hybrid models we use for our experiments. We
present our experiments in sections 3 to 5. Section
6 reviews related work, and section 7 finishes with
conclusions and future work.
2 Distributional semantic models
2.1 Textual models
For the current project, we constructed a set of
textual distributional models that implement vari-
ous standard ways to extract them from a corpus,
chosen to be representative of the state of the art.
In all cases, occurrence and co-occurrence statis-
tics are extracted from the freely available ukWaC
and Wackypedia corpora combined (size: 1.9B and
820M tokens, respectively).1 Moreover, in all mod-
els the raw co-occurrence counts are transformed
into nonnegative Local Mutual Information (LMI)
scores.2 Finally, in all models we harvest vector rep-
resentations for the same words (lemmas), namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs in the com-
bined corpora (for coherence with the vision-based
models, that cannot exploit contextual information
to distinguish nouns and adjectives, we merge nom-
inal and adjectival usages of the color adjectives in
the text-based models as well). The same 30K tar-
get nouns, verbs and adjectives are also employed as
contextual elements.
The Window2 and Window20 models are based
on counting co-occurrences with collocates within
a window of fixed width, in the tradition of HAL
(Lund and Burgess, 1996). Window2 records
sentence-internal co-occurrence with the nearest 2
content words to the left and right of each target con-
cept, a narrow context definition expected to capture
taxonomic relations. Window20 considers a larger
window of 20 words to the left and right of the target,
and should capture broader topical relations. The
Document model corresponds to a ?topic-based?
approach in which words are represented as distri-
butions over documents. It is based on a word-by-
document matrix, recording the distribution of the
1http://wacky.sslmit.unibo.it/
2LMI is obtained by multiplying raw counts by Pointwise
Mutual Information, and it is a close approximation to the Log-
Likelihood Ratio (Evert, 2005). It counteracts the tendency of
PMI to favour extremely rare events.
30K target words across the 30K documents in the
concatenated corpus that have the largest cumulative
LMI mass. This model is thus akin to traditional
Latent Semantic Analysis (Landauer and Dumais,
1997), without dimensionality reduction.
We add to the models we constructed the freely
available Distributional Memory (DM) model,3 that
has been shown to reach state-of-the-art perfor-
mance in many semantic tasks (Baroni and Lenci,
2010). DM is an example of a more complex text-
based model that exploits lexico-syntactic and de-
pendency relations between words (see Baroni and
Lenci?s article for details), and we use it as an in-
stance of a grammar-based model. DM is based
on the same corpora we used plus the 100M-word
British National Corpus,4 and it also uses LMI
scores.
2.2 Visual models
The visual models use information extracted from
images instead of textual corpora. We use image
data where each image is associated with one or
more words or tags (we use ?tag? for each word as-
sociated to the image, and ?label? for the set of tags
of an image). We use the ESP-Game dataset,5 con-
taining 100K images labeled through a game with a
purpose in which two people partnered online must
independently and rapidly agree on an appropriate
word to label randomly selected images. Once a
word is entered by both partners in a certain num-
ber of game matches, that word is added to the label
for that image, and it becomes a taboo word for the
following rounds of the game (von Ahn and Dab-
bish, 2004). There are 20,515 distinct tags in the
dataset, with an average of 4 tags per image. We
build one vector with visual features for each tag in
the dataset.
The visual features are extracted with the use of
a standard bag-of-visual-words (BoVW) represen-
tation of images, inspired by NLP (Sivic and Zisser-
man, 2003; Csurka et al, 2004; Nister and Stewe-
nius, 2006; Bosch et al, 2007; Yang et al, 2007).
This approach relies on the notion of a common vo-
cabulary of ?visual words? that can serve as discrete
representations for all images. Contrary to what hap-
3http://clic.cimec.unitn.it/dm
4http://www.natcorp.ox.ac.uk/
5http://www.espgame.org
137
pens in NLP, where words are (mostly) discrete and
easy to identify, in vision the visual words need to
be first defined. The process is completely induc-
tive. In a nutshell, BoVW works as follows. From
every image in a dataset, relevant areas are identified
and a low-level feature vector (called a ?descriptor?)
is built to represent each area. These vectors, living
in what is sometimes called a descriptor space, are
then grouped into a number of clusters. Each cluster
is treated as a discrete visual word, and the clusters
will be the vocabulary of visual words used to rep-
resent all the images in the collection. Now, given
a new image, the nearest visual word is identified
for each descriptor extracted from it, such that the
image can be represented as a BoVW feature vec-
tor, by counting the instances of each visual word
in the image (note that an occurrence of a low-level
descriptor vector in an image, after mapping to the
nearest cluster, will increment the count of a single
dimension of the higher-level BoVW vector). In our
work, the representation of each word (tag) is a also
a BoVW vector. The values of each dimension are
obtained by summing the occurrences of the relevant
visual word in all the images tagged with the word.
Again, raw counts are transformed into Local Mu-
tual Information scores. The process to extract vi-
sual words and use them to create image-based vec-
tors to represent (real) words is illustrated in Figure
1, for a hypothetical example in which there is only
one image in the collection labeled with the word
horse.
! !
!"#$%&'()*&!$(+%#
!! !!!"#$%
,*&$# - . / .!!!!0#%)*&!&#(&#$#1)+)'*1
!!!!!!!!2345!.6.
Figure 1: Procedure to build a visual representation for a
word, exemplified with SIFT features.
We extract descriptor features of two types.6
First, the standard Scale-Invariant Feature Trans-
form (SIFT) feature vectors (Lowe, 1999; Lowe,
2004), good at characterizing parts of objects. Sec-
ond, LAB features (Fairchild, 2005), which encode
only color information. We also experimented with
other visual features, such as those focusing on
edges (Canny, 1986), texture (Zhu et al, 2002), and
shapes (Oliva and Torralba, 2001), but they were
not useful for the color tasks. Moreover, we ex-
perimented also with different color scales, such as
LUV, HSV and RGB, obtaining significantly worse
performance compared to LAB. Further details on
feature extraction follow.
SIFT features are designed to be invariant to im-
age scale and rotation, and have been shown to pro-
vide a robust matching across affine distortion, noise
and change in illumination. The version of SIFT fea-
tures that we use is sensitive to color (RGB scale;
LUV, LAB and OPPONENT gave worse results).
We automatically identified keypoints for each im-
age and extracted SIFT features on a regular grid de-
fined around the keypoint with five pixels spacing,
at four multiple scales (10, 15, 20, 25 pixel radii),
zeroing the low contrast ones. To obtain the visual
word vocabulary, we cluster the SIFT feature vec-
tors with the standardly used k-means clustering al-
gorithm. We varied the number k of visual words
between 500 and 2,500 in steps of 500.
For the SIFT-based representation of images, we
used spatial histograms to introduce weak geometry
(Grauman and Darrell, 2005; Lazebnik et al, 2006),
dividing the image into several (spatial) regions, rep-
resenting each region in terms of BoVW, and then
concatenating the vectors. In our experiments, the
spatial regions were obtained by dividing the image
in 4? 4, for a total of 16 regions (other values and a
global representation did not perform as well). Note
that, following standard practice, descriptor cluster-
ing was performed ignoring the region partition, but
the resulting visual words correspond to different di-
mensions in the concatenated BoVW vectors, de-
pending on the region in which they occur. Con-
sequently, a vocabulary of k visual words results in
BoVW vectors with k ? 16 dimensions.
6We use VLFeat (http://www.vlfeat.org/) for fea-
ture extraction (Vedaldi and Fulkerson, 2008).
138
The LAB color space plots image data in 3 di-
mensions along 3 independent (orthogonal) axes,
one for brightness (luminance) and two for color
(chrominance). Luminance corresponds closely to
brightness as recorded by the brain-eye system;
the chrominance (red-green and yellow-blue) axes
mimic the oppositional color sensations the retina
reports to the brain (Szeliski, 2010). LAB features
are densely sampled for each pixel. Also here we use
the k-means algorithm to build the descriptor space.
We varied the number of k visual words between
128 and 1,024 in steps of 128.
2.3 Multimodal models
To assemble the textual and visual representations in
multimodal semantic spaces, we concatenate the two
vectors after normalizing them. We use the linear
weighted combination function proposed by Bruni
et al (2011): Given a word that is present both in
the textual model and in the visual model, we sepa-
rately normalize the two vectors Ft and Fv and we
combine them as follows:
F = ?? Ft ? (1? ?)? Fv
where ? is the vector concatenate operator. The
weighting parameter ? (0 ? ? ? 1) is tuned on the
MEN development data (2,000 word pairs; details
on the MEN dataset in the next section). We find the
optimal value to be close to ? = 0.5 for most model
combinations, suggesting that textual and visual in-
formation should have similar weight. Our imple-
mentation of the proposed method is open source
and publicly available.7
2.4 Hybrid models
We further introduce hybrid models that exploit the
patterns of co-occurrence of words as tags of the
same images. Like textual models, these mod-
els are based on word co-occurrence; like visual
models, they consider co-occurrence in images (im-
age labels). In one model (ESP-Win, analogous
to window-based models), words tagging an im-
age were represented in terms of co-occurrence with
the other tags in the image label (Baroni and Lenci
(2008) are a precedent for the use of ESP-Win).
The other (ESP-Doc, analogous to document-based
7https://github.com/s2m/FUSE
models) represented words in terms of their co-
occurrence with images, using each image as a dif-
ferent dimension. This information is very easy to
extract, as it does not require the sophisticated tech-
niques used in computer vision. We expected these
models to perform very bad; however, as we will
show, they perform relatively well in all but one of
the tasks tested.
3 Textual and visual models as general
semantic models
We test the models just presented in two different
ways: First, as general models of word meaning,
testing their correlation to human judgements on
word similarity and relatedness (this section). Sec-
ond, as models of the meaning of color terms (sec-
tions 4 and 5).
We use one standard dataset (WordSim353) and
one new dataset (MEN). WordSim353 (Finkelstein
et al, 2002) is a widely used benchmark constructed
by asking 16 subjects to rate a set of 353 word pairs
on a 10-point similarity scale and averaging the rat-
ings (dollar/buck receives a high 9.22 average rat-
ing, professor/cucumber a low 0.31). MEN is a
new evaluation benchmark with a better coverage of
our multimodal semantic models.8 It contains 3,000
pairs of randomly selected words that occur as ESP
tags (pairs sampled to ensure a balanced range of re-
latedness levels according to a text-based semantic
score). Each pair is scored on a [0, 1]-normalized
semantic relatedness scale via ratings obtained by
crowdsourcing on the Amazon Mechanical Turk (re-
fer to the online MEN documentation for more de-
tails). For example, cold/frost has a high 0.9 MEN
score, eat/hair a low 0.1. We evaluate the models
in terms of their Spearman correlation to the human
ratings. Our models have a perfect MEN coverage
and a coverage of 252 WordSim pairs.
We used the development set of MEN to test
the effect of varying the number k of visual words
in SIFT and LAB. We restrict the discussion to
SIFT with the optimal k (2.5K words) and to LAB
with the optimal (256), lowest (128), and highest
k (1024). We report the results of the multimodal
8An updated version of MEN is available from http://
clic.cimec.unitn.it/?elia.bruni/MEN.html.
The version used here contained 10 judgements per word pair.
139
models built with these visual models and the best
textual models (Window2 and Window20).
Columns WS and MEN in Table 1 report corre-
lations with the WordSim and MEN ratings, respec-
tively. As expected, because they are more mature
and capture a broader range of semantic informa-
tion, textual models perform much better than purely
visual models. Also as expected, SIFT features out-
perform the simpler LAB features for this task.
A first indication that visual information helps is
the fact that, for MEN, multimodal models perform
best. Note that all models that are sensitive to vi-
sual information perform better for MEN than for
WordSim, and the reverse is true for textual models.
Because of its design, word pairs in MEN can be
expected to be more imageable than those in Word-
Sim, so the visual information is more relevant for
this dataset. Also recall that we did some parameter
tuning on held-out MEN data.
Surprisingly, hybrid models perform quite well:
They are around 10 points worse than textual and
multimodal models for WordSim, and only slightly
worse than multimodal models for MEN.
4 Experiment 1: Discovering the color of
concrete objects
In Experiment 1, we test the hypothesis that the re-
lation between words denoting concrete things and
words denoting their typical color is reflected by the
distance of the corresponding vectors better when
the models are sensitive to visual information.
4.1 Method
Two authors labeled by consensus a list of concrete
nouns (extracted from the BLESS dataset9 and the
nouns in the BNC occurring with color terms more
than 100 times) with one of the 11 colors from
the basic set proposed by Berlin and Kay (1969):
black, blue, brown, green, grey, orange, pink, pur-
ple, red, white, yellow. Objects that do not have
an obvious characteristic color (computer) and those
with more than one characteristic color (zebra, bear)
were eliminated. Moreover, only nouns covered by
all the models were preserved. The final list con-
9http://sites.google.com/site/
geometricalmodels/shared-evaluation
Model WS MEN E1 E2
DM .44 .42 3 (09) .14
Document .63 .62 3 (07) .06
Window2 .70 .66 5 (13) .49***
Window20 .70 .62 3 (11) .53***
LAB128 .21 .41 1 (27) .25*
LAB256 .21 .41 2 (24) .24*
LAB1024 .19 .41 2 (24) .28**
SIFT2.5K .33 .44 3 (15) .57***
W2-LAB128 .40 .59 1 (27) .40***
W2-LAB256 .41 .60 2 (23) .40***
W2-LAB1024 .39 .61 2 (24) .44***
W20-LAB128 .40 .60 1 (27) .36***
W20-LAB256 .41 .60 2 (23) .36***
W20-LAB1024 .39 .62 2 (24) .40***
W2-SIFT2.5K .64 .69 2.5 (19) .68***
W20-SIFT2.5K .64 .68 2 (17) .73***
ESP-Doc .52 .66 1 (37) .29*
ESP-Win .55 .68 4 (15) .16
Table 1: Results of the textual, visual, multimodal, and
hybrid models on the general semantic tasks (first two
columns, section 3; Pearson ?) and Experiments 1 (E1,
section 4) and 2 (E2, section 5). E1 reports the median
rank of the correct color and the number of top matches
(in parentheses), and E2 the average difference in nor-
malized cosines between literal and nonliteral adjective-
noun phrases, with the significance of a t-test (*** for
p< 0.001, ** < 0.01, * < 0.05).
tains 52 nouns.10 Some random examples are fog?
grey, crow?black, wood?brown, parsley?green, and
grass?green.
For evaluation, we measured the cosine of each
noun with the 11 basic color words in the space pro-
duced by each model, and recorded the rank of the
correct color in the resulting ordered list.
4.2 Results
Column E1 in Table 1 reports the median rank for
each model (the smaller the rank, the better the
model), as well as the number of exact matches (that
is, number of nouns for which the model ranks the
correct color first).
Discovering knowledge such that grass is green
is arguably a simple task but Experiment 1 shows
10Dataset available from the second author?s webpage, under
resources.
140
that textual models fail this simple task, with median
ranks around 3.11 This is consistent with the findings
in Baroni and Lenci (2008) that standard distribu-
tional models do not capture the association between
concrete concepts and their typical attributes. Visual
models, as expected, are better at capturing the as-
sociation between concepts and visual attributes. In
fact, all models that are sensitive to visual informa-
tion achieve median rank 1.
Multimodal models do not increase performance
with respect to visual models: For instance, both
W2-LAB128 and W20-LAB128 have the same me-
dian rank and number of exact matches as LAB128
alone. Textual information in this case is not com-
plementary to visual information, but simply poorer.
Also note that LAB features do better than SIFT
features. This is probably due to the fact that Exper-
iment 1 is basically about identifying a large patch
of color. The SIFT features we are using are also
sensitive to color, but they seem to be misguided by
the other cues that they extract from images. For
example, pigs are pink in LAB space but brown in
SIFT space, perhaps because SIFT focused on the
color of the typical environment of a pig. We can
thus confirm that, by limiting multimodal spaces to
SIFT features, as has been done until now in the lit-
erature, we are missing important semantic informa-
tion, such as the color information that we can mine
with LAB.
Again we find that hybrid models do very well,
in fact in this case they have the top performance,
as they perform better than LAB128 (the differ-
ence, which can be noticed in the number of exact
matches, is highly significant according to a paired
Mann-Whitney test, with p<0.001).
5 Experiment 2
Experiment 2 requires more sophisticated informa-
tion than Experiment 1, as it involves distinguishing
between literal and nonliteral uses of color terms.
11We also experimented with a model based on direct co-
occurrence of adjectives and nouns, obtaining promising results
in a preliminary version of Exp. 1. We abandoned this approach
because such a model inherently lacks scalability, as it will not
generalize behind cases where the training data contain direct
examples of co-occurrences of the target pairs.
5.1 Method
We test the performance of the different models
with a dataset consisting of color adjective-noun
phrases, randomly drawn from the most frequent 8K
nouns and 4K adjectives in the concatenated ukWaC,
Wackypedia, and BNC corpora (four color terms are
not among these, so the dataset includes phrases for
black, blue, brown, green, red, white, and yellow
only). These were tagged by consensus by two hu-
man judges as literal (white towel, black feather)
or nonliteral (white wine, white musician, green fu-
ture). Some phrases had both literal and nonliteral
uses, such as blue book in ?book that is blue? vs.
?automobile price guide?. In these cases, only the
most common sense (according to the judges) was
taken into account for the present experiment. The
dataset consists of 370 phrases, of which our models
cover 342, 227 literal and 115 nonliteral.12
The prediction is that, in good semantic models,
literal uses will in general result in a higher simi-
larity between the noun and color term vectors: A
white towel is white, while wine or musicians are
not white in the same manner. We test this prediction
by comparing the average cosine between the color
term and the nouns across the literal and nonliteral
pairs (similar results were obtained in an evaluation
in terms of prediction accuracy of a simple classi-
fier).
5.2 Results
Column E2 in Table 1 summarizes the results of
the experiment, reporting the mean difference be-
tween the normalized cosines (that is, how large
the difference is between the literal and nonliteral
uses of color terms), as well as the significance of
the differences according to a t-test. Window-based
models perform best among textual models, partic-
ularly Window20, while the rest can?t discriminate
between the two uses. This is particularly striking
for the Document model, which performs quite well
in general semantic tasks but bad in visual tasks.
Visual models are all able to discriminate between
the two uses, suggesting that indeed visual infor-
mation can capture nonliteral aspects of meaning.
However, in this case SIFT features perform much
better than LAB features, as Experiment 2 involves
12Dataset available upon request to the second author.
141
tackling much more sophisticated information than
Experiment 1. This is consistent with the fact that,
for LAB, a lower k (lower granularity of the in-
formation) performs better for Experiment 1 and a
higher k (higher granularity) for Experiment 2.
One crucial question to ask, given the goals of
our research, is whether textual and visual models
are doing essentially the same job, only using dif-
ferent types of information. Note that, in this case,
multimodal models increase performance over the
individual modalities, and are the best models for
this task. This suggests that the information used in
the individual models is complementary, and indeed
there is no correlation between the cosines obtained
with the best textual and visual models (Pearson?s
? = .09, p = .11).
Figure 2 depicts the results broken down by
color.13 Both modalities can capture the differ-
ences for black and green, probably because nonlit-
eral uses of these color terms have also clear textual
correlates (more concretely, topical correlates, as
they are related to race and ecology, respectively).14
Significantly, however, vision can capture nonliteral
uses of blue and red, while text can?t. Note that
these uses (blue note, shark, shield, red meat, dis-
trict, face) do not have a clear topical correlate, and
thus it makes sense that vision does a better job.
Finally, note that for this more sophisticated task,
hybrid models perform quite bad, which shows their
limitations as models of word meaning.15 Overall,
13Yellow and brown are excluded because the dataset contains
only one and two instances of nonliteral cases for these terms,
respectively. The significance of the differences as explained in
the text has been tested via t-tests.
14It?s not entirely clear why neither modality can capture
the differences for white; for text, it may be because the non-
literal cases are not so tied to race as is the cases for black,
but they also contain many other types of nonliteral uses, such
as type-referring (white wine/rice/cell) or metonymical ones
(white smile).
15The hybrid model that performs best in the color tasks is
ESP-Doc. This model can only detect a relation between an ad-
jective and a noun if they directly co-occur in the label of at least
one image (a ?document? in this setting). The more direct co-
occurrences there are, the more related the words will be for the
model. This works for Exp. 1: Since the ESP labels are lists of
what subjects saw in a picture, and the adjectives of Exp. 1 are
typical colors of objects, there is a high co-occurrence, as all but
one adjective-noun pairs co-occur in at least one ESP label. For
the model to perform well in Exp. 2 too, literal phrases should
occur in the same labels and non-literal pairs should not. We
our results suggest that co-occurrence in an image
label can be used as a surrogate of true visual infor-
mation to some extent, but the behavior of hybrid
models depends on ad-hoc aspects of the labeled
dataset, and, from an empirical perspective, they are
more limited than truly multimodal models, because
they require large amounts of rich verbal picture de-
scriptions to reach good coverage.
6 Related work
There is an increasing amount of work in com-
puter vision that exploits text-derived information
for image retrieval and annotation tasks (Farhadi
et al, 2010; Kulkarni et al, 2011). One particu-
lar techinque inspired by NLP that has acted as a
very effective proxy from CV to NLP is precisely
the BoVW. Recently, NLPers have begun exploit-
ing BoVW to enrich distributional models that rep-
resent word meaning with visual features automati-
cally extracted from images (Feng and Lapata, 2010;
Bruni et al, 2011; Leong and Mihalcea, 2011). Pre-
vious work in this area relied on SIFT features only,
whereas we have enriched the visual representation
of words with other kinds of features from computer
vision, namely, color-related features (LAB). More-
over, earlier evaluation of multimodal models has
focused only on standard word similarity tasks (us-
ing mainly WordSim353), whereas we have tested
them on both general semantic tasks and specific
tasks that tap directly into aspects of semantics (such
as color) where we expect visual information to be
crucial.
The most closely related work to ours is that re-
cently presented by O?zbal et al (2011). Like us,
O?zbal and colleagues use both a textual model and a
visual model (as well as Google adjective-noun co-
occurrence counts) to find the typical color of an ob-
ject. However, their visual model works by analyz-
ing pictures associated with an object, and determin-
ing the color of the object directly by image analysis.
We attempt the more ambitious goal of separately
associating a vector to nouns and adjectives, and de-
find no such difference (89% of adjective-noun pairs co-occur
in at least one image in the literal set, 86% in the nonliteral set),
because many of the relevant pairs describe concrete concepts
that, while not necessarily of the ?right? literal colour, are per-
fectly fit to be depicted in images (?blue shark?, ?black boy?,
?white wine?).
142
L N
0.05
0.10
0.15
0.20
0.25
0.30
Vision: black
l
ll
l
ll
l
L N0
.0
0.1
0.2
0.3
0.4
0.5
Text: black
L N
0.10
0.15
0.20
0.25
0.30
0.35
Vision: blue
l
l
L N0.
0
0.1
0.2
0.3
Text: blue
l
l
L N
0.05
0.15
0.25
Vision: green
l
l
l
L N0.0
0
0.04
0.08
0.12
Text: green
L N
0.05
0.10
0.15
0.20
0.25
0.30
Vision: red
l
l
l
L N0.0
0
0.10
0.20
0.30
Text: red
l
L N0
.05
0.10
0.15
0.20
0.25
0.30
Vision: white
l
ll
l
l
ll
L N0.
00
0.05
0.10
0.15
Text: white
Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models.
termining the color of an object by the nearness of
the noun denoting the object to the color term. In
other words, we are trying to model the meaning of
color terms and how they relate to other words, and
not to directly extract the color of an object from pic-
tures depicting them. Our second experiment is con-
nected to the literature on the automated detection of
figurative language (Shutova, 2010). There is in par-
ticular some similarity with the tasks studied by Tur-
ney et al (2011). Turney and colleagues try, among
other things, to distinguish literal and metaphorical
usages of adjectives when combined with nouns, in-
cluding the highly visual adjective dark (dark hair
vs. dark humour). Their method, based on automat-
ically quantifying the degree of abstractness of the
noun, is complementary to ours. Future work could
combine our approach and theirs.
7 Conclusion
We have presented evidence that distributional se-
mantic models based on text, while providing a
good general semantic representation of word mean-
ing, can be outperformed by models using visual
information for semantic aspects of words where
vision is relevant. More generally, this suggests
that computer vision is mature enough to signifi-
cantly contribute to perceptually grounded compu-
tational models of language. We have also shown
that different types of visual features (LAB, SIFT)
are appropriate for different tasks. Future research
should investigate automated methods to discover
which (if any) kind of visual information should be
highlighted in which task, more sophisticated mul-
timodal models, visual properties other than color,
and larger color datasets, such as the one recently
introduced by Mohammad (2011).
Acknowledgments
E.B. and M.B. are partially supported by a Google
Research Award. G.B. is partially supported
by the Spanish Ministry of Science and Innova-
tion (FFI2010-15006, TIN2009-14715-C04-04), the
EU PASCAL2 Network of Excellence (FP7-ICT-
216886) and the AGAUR (2010 BP-A 00070). The
E2 evaluation set was created by G.B. with Louise
McNally and Eva Maria Vecchi. Fig. 1 was adapted
from a figure by Jasper Uijlings. G. B. thanks Mar-
garita Torrent for taking care of her children while
she worked hard to meet the Sunday deadline.
References
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
143
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of Recent Advances in Natural Language Process-
ing, pages 399?405, Hissar.
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing bilingual lexicons using the visual similarity of la-
beled web images. In Proc. IJCAI, pages 1764?1769,
Barcelona, Spain, July.
Brent Berlin and Paul Key. 1969. Basic Color Terms:
Their Universality and Evolution. University of Cali-
fornia Press, Berkeley, CA.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image Classification using Random Forests and
Ferns. In Computer Vision, 2007. ICCV 2007. IEEE
11th International Conference on, pages 1?8.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22?
32, Edinburgh.
John Canny. 1986. A computational approach to edge
detection. IEEE Trans. Pattern Anal. Mach. Intell,
36(4):679?698.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Mark D. Fairchild. 2005. Status of cie color appearance
models.
A. Farhadi, M. Hejrati, M. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In Proceedings of ECCV.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91?99, Los Angeles, CA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Kristen Grauman and Trevor Darrell. 2005. The pyramid
match kernel: Discriminative classification with sets
of image features. In In ICCV, pages 1458?1465.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. Berg,
and T. Berg. 2011. Baby talk: Understanding and
generating simple image descriptions. In Proceedings
of CVPR.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of the 2006 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition
- Volume 2, CVPR 2006, pages 2169?2178, Washing-
ton, DC, USA. IEEE Computer Society.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407, Chiang Mai, Thailand.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David Lowe. 1999. Object Recognition from Local
Scale-Invariant Features. Computer Vision, IEEE In-
ternational Conference on, 2:1150?1157 vol.2, Au-
gust.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Saif Mohammad. 2011. Colourful language: Measuring
word-colour associations. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computational
Linguistics, pages 97?106, Portland, Oregon.
Raymond J. Mooney. 2008. Learning to connect lan-
guage and perception.
David Nister and Henrik Stewenius. 2006. Scalable
recognition with a vocabulary tree. In Proceedings
of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 2,
CVPR ?06, pages 2161?2168.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. Int. J. Comput. Vision, 42:145?175.
Go?zde O?zbal, Carlo Strapparava, Rada Mihalcea, and
Daniele Pighin. 2011. A comparison of unsupervised
methods to associate colors with words. In Proceed-
ings of ACII, pages 42?51, Memphis, TN.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL, pages 688?697, Uppsala, Swe-
den.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of the International Conference on
Computer Vision, volume 2, pages 1470?1477, Octo-
ber.
144
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer-Verlag New York Inc.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of EMNLP, pages 680?690, Edinburgh, UK.
Andrea Vedaldi and Brian Fulkerson. 2008. VLFeat:
An open and portable library of computer vision algo-
rithms. http://www.vlfeat.org/.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems, pages 319?326, Vienna, Austria.
Jun Yang, Yu-Gang Jiang, Alexander G. Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In Mul-
timedia Information Retrieval, pages 197?206.
Song Chun Zhu, Cheng en Guo, Ying Nian Wu, and
Yizhou Wang. 2002. What are textons? In Computer
Vision - ECCV 2002, 7th European Conference on
Computer Vision, Copenhagen, Denmark, May 28-31,
2002, Proceedings, Part IV, pages 793?807. Springer.
145
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187?192,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
VSEM: An open library for visual semantics representation
Elia Bruni
University of Trento
elia.bruni@unitn.it
Jasper Uijlings
University of Trento
jrr@disi.unitn.it
Ulisse Bordignon
University of Trento
ulisse.bordignon@unitn.it
Irina Sergienya
University of Trento
irina.sergienya@unitn.it
Adam Liska
University of Trento
adam.liska@unitn.it
Abstract
VSEM is an open library for visual se-
mantics. Starting from a collection of
tagged images, it is possible to auto-
matically construct an image-based rep-
resentation of concepts by using off-the-
shelf VSEM functionalities. VSEM is en-
tirely written in MATLAB and its object-
oriented design allows a large flexibility
and reusability. The software is accompa-
nied by a website with supporting docu-
mentation and examples.
1 Introduction
In the last years we have witnessed great progress
in the area of automated image analysis. Important
advances, such as the introduction of local features
for a robust description of the image content (see
Mikolajczyk et al (2005) for a systematic review)
and the bag-of-visual-words method (BoVW)1 for
a standard representation across multiple images
(Sivic and Zisserman, 2003), have contributed to
make image analysis ubiquitous, with applications
ranging from robotics to biology, from medicine to
photography.
Two facts have played a key role in the rapid ad-
vance of these ideas. First, the introduction of very
well defined challenges which have been attracting
also a wide community of ?outsiders" specialized
in a variety of disciplines (e.g., machine learning,
neural networks, graphical models and natural lan-
guage processing). Second, the sharing of effec-
tive, well documented implementations of cutting
edge image analysis algorithms, such as OpenCV2
1Bag-of-visual-words model is a popular technique for
image classification inspired by the traditional bag-of-words
model in Information Retrieval. It represents an image with
discrete image-describing features. Visual words are iden-
tified by clustering a large corpus of lower-level continuous
features.
2http://opencv.org/
and VLFeat.3
A comparable story can be told about automatic
text analysis. The last decades have seen a long
series of successes in the processing of large text
corpora in order to extract more or less structured
semantic knowledge. In particular, under the as-
sumption that meaning can be captured by patterns
of co-occurrences of words, distributional seman-
tic models such as Latent Semantic Analysis (Lan-
dauer and Dumais, 1997) or Topic Models (Blei
et al, 2003) have been shown to be very effective
both in general semantic tasks such as approximat-
ing human intuitions about meaning, as well as in
more application-driven tasks such as information
retrieval, word disambiguation and query expan-
sion (Turney and Pantel, 2010). And also in the
case of automated text analysis, a wide range of
method implementations are at the disposal of the
scientific community.4
Nowadays, given the parallel success of the two
disciplines, there is growing interest in making
the visual and textual channels interact for mutual
benefit. If we look at the image analysis commu-
nity, we discover a well established tradition of
studies that exploit both channels of information.
For example, there is a relatively extended amount
of literature about enhancing the performance on
visual tasks such as object recognition or image re-
trieval by replacing a purely image-based pipeline
with hybrid methods augmented with textual in-
formation (Barnard et al, 2003; Farhadi et al,
2009; Berg et al, 2010; Kulkarni et al, 2011).
Unfortunately, the same cannot be said of the
exploitation of image analysis from within the text
community. Despite the huge potential that au-
tomatically induced visual features could repre-
sent as a new source of perceptually grounded
3http://www.vlfeat.org/
4See for example the annotated list of corpus-based
computational linguistics resources at http://www-nlp.
stanford.edu/links/statnlp.html.
187
semantic knowledge,5 image-enhanced models of
semantics developed so far (Feng and Lapata,
2010; Bruni et al, 2011; Leong and Mihalcea,
2011; Bergsma and Goebel, 2011; Bruni et al,
2012a; Bruni et al, 2012b) have only scratched
this great potential and are still considered as
proof-of-concept studies only.
One possible reason of this delay with respect to
the image analysis community might be ascribed
to the high entry barriers that NLP researchers
adopting image analysis methods have to face. Al-
though many of the image analysis toolkits are
open source and well documented, they mainly ad-
dress users within the same community and there-
fore their use is not as intuitive for others. The
final goal of libraries such VLFeat and OpenCV
is the representation and classification of images.
Therefore, they naturally lack of a series of com-
plementary functionalities that are necessary to
bring the visual representation to the level of se-
mantic concepts.6
To fill the gap we just described, we present
hereby VSEM,7 a novel toolkit which allows the
extraction of image-based representations of con-
cepts in an easy fashion. VSEM is equipped with
state-of-the-art algorithms, from low-level feature
detection and description up to the BoVW repre-
sentation of images, together with a set of new rou-
tines necessary to move from an image-wise to a
concept-wise representation of image content. In
a nutshell, VSEM extracts visual information in a
way that resembles how it is done for automatic
text analysis. Thanks to BoVW, the image con-
tent is indeed discretized and visual units some-
how comparable to words in text are produced (the
visual words). In this way, from a corpus of im-
ages annotated with a set of concepts, it is pos-
sible to derive semantic vectors of co-occurrence
counts of concepts and visual words akin to the
representations of words in terms of textual collo-
cates in standard distributional semantics. Impor-
5In recent years, a conspicuous literature of studies has
surfaced, wherein demonstration was made of how text based
models are not sufficiently good at capturing the environment
we acquire language from. This is due to the fact that they
are lacking of perceptual information (Andrews et al, 2009;
Baroni et al, 2010; Baroni and Lenci, 2008; Riordan and
Jones, 2011).
6The authors of the aforementioned studies usually refer
to words instead of concepts. We chose to call them concepts
to account for the both theoretical and practical differences
standing between a word and the perceptual information it
brings along, which we define its concept.
7http://clic.cimec.unitn.it/vsem/
tantly, the obtained visual semantic vectors can be
easily combined with more traditional text-based
vectors to arrive at a multimodal representation of
meaning (see e.g. (Bruni et al, 2011)). It has
been shown that the resulting multimodal models
perform better than text-only models in semantic
tasks such as approximating semantic similarity
and relatedness ((Feng and Lapata, 2010; Bruni et
al., 2012b)).
VSEM functionalities concerning image anal-
ysis is based on VLFeat (Vedaldi and Fulkerson,
2010). This guarantees that the image analysis un-
derpinnings of the library are well maintained and
state-of-the-art.
The rest of the paper is organized as follows.
In Section 2 we introduce the procedure to obtain
an image-based representation of a concept. Sec-
tion 3 describes the VSEM architecture. Section
4 shows how to install and run VSEM through
an example that uses the Pascal VOC data set.
Section 5 concludes summarizing the material and
discussing further directions.
2 Background
As shown by Feng and Lapata (2010), Bruni et
al. (2011) and Leong and Mihalcea (2011), it is
possible to construct an image-based representa-
tion of a set of target concepts by starting from a
collection of images depicting those concepts, en-
coding the image contents into low-level features
(e.g., SIFT) and scaling up to a higher level rep-
resentation, based on the well-established BoVW
method to represent images. In addition, as shown
by Bruni et al (2012b), better representations can
be extracted if the object depicting the concept is
first localized in the image.
More in detail, the pipeline encapsulating the
whole process mentioned above takes as input a
collection of images together with their associated
tags and optionally object location annotations. Its
output is a set of concept representation vectors
for individual tags. The following steps are in-
volved: (i) extraction of local image features, (ii)
visual vocabulary construction, (iii) encoding the
local features in a BoVW histogram, (iv) including
spatial information with spatial binning, (v) aggre-
gation of visual words on a per-concept basis in
order to obtain the co-occurrence counts for each
concept and (vi) transforming the counts into asso-
ciation scores and/or reducing the dimensionality
of the data. A brief description of the individual
188
  
feature extraction
Figure 1: An example of a visual vocabulary cre-
ation pipeline. From a set of images, a larger set
of features are extracted and clustered, forming the
visual vocabulary.
steps follows.
Local features Local features are designed to
find local image structures in a repeatable fash-
ion and to represent them in robust ways that are
invariant to typical image transformations, such
as translation, rotation, scaling, and affine defor-
mation. Local features constitute the basis of
approaches developed to automatically recognize
specific objects (Grauman and Leibe, 2011). The
most popular local feature extraction method is the
Scale Invariant Feature Transform (SIFT), intro-
duced by Lowe (2004). VSEM uses the VLFeat
implementation of SIFT.
Visual vocabulary To obtain a BoVW repre-
sentation of the image content, a large set of lo-
cal features extracted from a large corpus of im-
ages are clustered. In this way the local fea-
ture space is divided into informative regions (vi-
sual words) and the collection of the obtained vi-
sual words is called visual vocabulary. k-means
is the most commonly used clustering algorithm
(Grauman and Leibe, 2011). In the special case
of Fisher encoding (see below), the clustering of
the features is performed with a Gaussian mixture
model (GMM), see Perronnin et al (2010). Fig-
ure 1 exemplifies a visual vocabulary construction
pipeline. VSEM contains both the k-means and
the GMM implementations.
Encoding The encoding step maps the local fea-
tures extracted from an image to the correspond-
ing visual words of the previously created vocab-
ulary. The most common encoding strategy is
called hard quantization, which assigns each fea-
ture to the nearest visual word?s centroid (in Eu-
clidean distance). Recently, more effective encod-
ing methods have been introduced, among which
the Fisher encoding (Perronnin et al, 2010) has
been shown to outperform all the others (Chatfield
et al, 2011). VSEM uses both the hard quantiza-
tion and the Fisher encoding.
Spatial binning A consolidated way of intro-
ducing spatial information in BoVW is the use of
spatial histograms (Lazebnik et al, 2006). The
main idea is to divide the image into several (spa-
tial) regions, compute the encoding for each region
and stack the resulting histograms. This technique
is referred to as spatial binning and it is imple-
mented in VSEM. Figure 2 exemplifies the BoVW
pipeline for a single image, involving local fea-
tures extraction, encoding and spatial binning.
  
feature extraction spatial binningencoding
Figure 2: An example of a BoVW representation
pipeline for an image. Figure inspired by Chatfield
et al (2011). Each feature extracted from the tar-
get image is assigned to the corresponding visual
word(s). Then, spatial binning is performed.
Moreover, the input of spatial binning can be
further refined by introducing localization. Three
different types of localization are typically used:
global, object, and surrounding. Global extracts
visual information from the whole image and it is
also the default option when the localization in-
formation is missing. Object extracts visual infor-
mation from the object location only and the sur-
rounding extracts visual information from outside
the object location. Localization itself can either
be done by humans (or ground truth annotation)
but also by existing localization methods (Uijlings
et al, 2013).
For localization, VSEM uses annotated object
locations (in the format of bounding boxes) of the
target object.
Aggregation Since each concept is represented
by multiple images, an aggregation function for
pooling the visual word occurrences across images
has to be defined. As far as we know, the sum
function has been the only function utilized so far.
An example for the aggregation step is sketched in
189
  =cat
aggregationr ti++
+
Figure 3: An example of a concept representa-
tion pipeline for cat. First, several images depict-
ing a cat are represented as vectors of visual word
counts and, second, the vectors are aggregated into
one single concept vector.
figure 3. VSEM offers an implementation of the
sum function.
Transformations Once the concept-
representing visual vectors are built, two types
of transformation can be performed over them to
refine their raw visual word counts: association
scores and dimensionality reduction. So far,
the vectors that we have obtained represent co-
occurrence counts of visual words with concepts.
The goal of association scores is to distinguish
interesting co-occurrences from those that are due
to chance. In order to do this, VSEM implements
two versions of mutual information (pointwise
and local), see Evert (2005).
On the other hand, dimensionality reduction
leads to matrices that are smaller and easier to
work with. Moreover, some techniques are able
to smooth the matrices and uncover latent dimen-
sions. Common dimensionality reduction methods
are singular value decomposition (Manning et al,
2008), non-negative matrix factorization (Lee and
Seung, 2001) and neural networks (Hinton and
Salakhutdinov, 2006). VSEM implements the sin-
gular value decomposition method.
3 Framework design
VSEM offers a friendly implementation of the
pipeline described in Section 2. The framework is
organized into five parts, which correspond to an
equal number of MATLAB packages and it is writ-
ten in object-oriented programming to encourage
reusability. A description of the packages follows.
? datasets This package contains the code
that manages the image data sets. We al-
ready provide a generic wrapper for sev-
eral possible dataset formats (VsemDataset
). Therefore, to use a new image data set
two solutions are possible: either write a
new class which extends GenericDataset or
use directly VsemDataset after having rear-
ranged the new data as described in help
VsemDataset.
? vision This package contains the code for
extracting the bag-of-visual-words represen-
tation of images. In the majority of cases,
it can be used as a ?black box? by the user.
Nevertheless, if the user wants to add new
functionalities such as new features or encod-
ings, this is possible by simply extending the
corresponding generic classes and the class
VsemHistogramExtractor.
? concepts This is the package that deals
with the construction of the image-based rep-
resentation of concepts. concepts is the
most important package of VSEM. It ap-
plies the image analysis methods to obtain the
BoVW representation of the image data and
then aggregates visual word counts concept-
wise. The main class of this package is
ConceptSpace, which takes care of storing
concepts names and vectors and provides
managing and transformation utilities as its
methods.
? benchmarks VSEM offers a benchmarking
suite to assess the quality of the visual con-
cept representations. For example, it can be
used to find the optimal parametrization of
the visual pipeline.
? helpers This package contains supporting
classes. There is a general helpers with
functionalities shared across packages and
several package specific helpers.
4 Getting started
Installation VSEM can be easily installed by
running the file vsemSetup.m. Moreover, pascal-
DatasetSetup.m can be run to download and place
the popular dataset, integrating it in the current
pipeline.
190
Documentation All the MATLAB commands
of VSEM are self documented (e.g. help vsem)
and an HTML version of the MATLAB command
documentation is available from the VSEM web-
site.
The Pascal VOC demo The Pascal VOC demo
provides a comprehensive example of the work-
ings of VSEM. From the demo file pascalVQDemo
.mmultiple configurations are accessible. Addi-
tional settings are available and documented for
each function, class or package in the toolbox (see
Documentation).
Running the demo file executes the following
lines of code and returns as output ConceptSpace,
which contains the visual concept representations
for the Pascal data set.
% Create a matlab structure with the
% whole set of images in the Pascal
% dataset alng with their annotation
dataset = datasets.VsemDataset(
configuration.imagesPath,?
annotationFolder?,configuration.
annotationPath);
% Initiate the class that handles
% the extraction of visual features.
featureExtractor = vision.features.
PhowFeatureExtractor();
% Create the visual vocabulary
vocabulary = KmeansVocabulary.
trainVocabulary(dataset,
featureExtractor);
% Calculate semantic vectors
conceptSpace = conceptExtractor.
extractConcepts(dataset,
histogramExtractor);
% Compute pointwise mutual
% information
conceptSpace = conceptSpace.reweight();
% Conclude the demo, computing
% the similarity of correlation
% measures of the 190 possible
% pair of concepts from the Pascal
% dataset against a gold standard
[correlationScore, p-value] =
similarityBenchmark.computeBenchmark
(conceptSpace,similarityExtractor);
5 Conclusions
We have introduced VSEM, an open library for vi-
sual semantics. With VSEM it is possible to ex-
tract visual semantic information from tagged im-
ages and arrange such information into concept
representations according to the tenets of distri-
butional semantics, as applied to images instead
of text. To analyze images, it uses state-of-the-art
techniques such as the SIFT features and the bag-
of-visual-words with spatial pyramid and Fisher
encoding. In the future, we would like to add
automatic localization strategies, new aggregation
functions and a completely new package for fusing
image- and text-based representations.
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
Kobus Barnard, Pinar Duygulu, David Forsyth, Nando
de Freitas, David Blei, and Michael Jordan. 2003.
Matching words and pictures. Journal of Machine
Learning Research, 3:1107?1135.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of
Linguistics, 20(1):55?88.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Tamara Berg, Alexander Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteri-
zation from noisy Web data. In ECCV, pages 663?
676, Crete, Greece.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP, pages 399?405, Hissar, Bul-
garia.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP GEMS Workshop, pages
22?32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proceedings of
ACM Multimedia, pages 1219?1228, Nara, Japan.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
191
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778?
1785, Miami Beach, FL.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91?99, Los Angeles, CA.
Kristen Grauman and Bastian Leibe. 2011. Visual Ob-
ject Recognition. Morgan & Claypool, San Fran-
cisco.
Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Re-
ducing the dimensionality of data with neural net-
works. Science, 313(5786):504 ? 507.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In Proceedings of
CVPR, Colorado Springs, MSA.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169?2178, Wash-
ington, DC.
Daniel D. Lee and H. Sebastian Seung. 2001. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556?562. MIT Press.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
Chris Manning, Prabhakar Raghavan, and Hinrich
Sch?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge,
UK.
K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisser-
man, J. Matas, F. Schaffalitzky, T. Kadir, and L. V.
Gool. 2005. A Comparison of Affine Region De-
tectors. International Journal of Computer Vision,
65(1).
Florent Perronnin, Jorge Sanchez, and Thomas
Mensink. 2010. Improving the fisher kernel for
large-scale image classification. In Proceedings of
ECCV, pages 143?156, Berlin, Heidelberg.
Brian Riordan and Michael Jones. 2011. Redundancy
in perceptual and linguistic experience: Comparing
feature-based and distributional models of semantic
representation. Topics in Cognitive Science, 3(2):1?
43.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470?
1477, Nice, France.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and
A.W.M. Smeulders. 2013. Selective search for ob-
ject recognition. IJCV.
Andrea Vedaldi and Brian Fulkerson. 2010. Vlfeat
? an open and portable library of computer vision
algorithms. In Proceedings of ACM Multimedia,
pages 1469?1472, Firenze, Italy.
192
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, page 1,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Visual Features for Linguists:
Basic image analysis techniques for multimodally-curious NLPers
Elia Bruni
University of Trento
elia.bruni@unitn.it
Marco Baroni
University of Trento
marco.baroni@unitn.it
Description
Features automatically extracted from images con-
stitute a new and rich source of semantic knowl-
edge that can complement information extracted
from text. The convergence between vision- and
text-based information can be exploited in scenar-
ios where the two modalities must be combined
to solve a target task (e.g., generating verbal de-
scriptions of images, or finding the right images
to illustrate a story). However, the potential ap-
plications for integrated visual features go beyond
mixed-media scenarios: Because of their comple-
mentary nature with respect to language, visual
features might provide perceptually grounded se-
mantic information that can be exploited in purely
linguistic domains.
The tutorial will first introduce basic techniques
to encode image contents in terms of low-level fea-
tures, such as the widely adopted SIFT descriptors.
We will then show how these low-level descriptors
are used to induce more abstract features, focus-
ing on the well-established bags-of-visual-words
method to represent images, but also briefly in-
troducing more recent developments, that include
capturing spatial information with pyramid repre-
sentations, soft visual word clustering via Fisher
encoding and attribute-based image representa-
tion. Next, we will discuss some example appli-
cations, and we will conclude with a brief practi-
cal illustration of visual feature extraction using a
software package we developed.
The tutorial is addressed to computational lin-
guists without any background in computer vi-
sion. It provides enough background material to
understand the vision-and-language literature and
the less technical articles on image analysis. After
the tutorial, the participants should also be able to
autonomously incorporate visual features in their
NLP pipelines using off-the-shelf tools.
Outline
1. Why image analysis?
? The grounding problem
? Multimodal datasets (Pascal, SUN, Im-
ageNet and ESP-Game)
2. Extraction of low-level features from images
? Challenges (viewpoint, illumination,
scale, occlusion, etc.)
? Feature detectors
? Feature descriptors
3. Visual words for higher-level representation
of visual information
? Constructing a vocabulary of visual
words
? Classic Bags-of-visual-words represen-
tation
? Recent advances
? Computer vision applications: Object
recognition and emotion analysis
4. Going multimodal: Example applications of
visual features in NLP
? Generating image descriptions
? Semantic relatedness
? Modeling selectional preference
1
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403?1414,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Is this a wampimuk?
Cross-modal mapping between distributional semantics
and the visual world
Angeliki Lazaridou and Elia Bruni and Marco Baroni
Center for Mind/Brain Sciences
University of Trento
{angeliki.lazaridou|elia.bruni|marco.baroni}@unitn.it
Abstract
Following up on recent work on estab-
lishing a mapping between vector-based
semantic embeddings of words and the
visual representations of the correspond-
ing objects from natural images, we first
present a simple approach to cross-modal
vector-based semantics for the task of
zero-shot learning, in which an image
of a previously unseen object is mapped
to a linguistic representation denoting its
word. We then introduce fast mapping, a
challenging and more cognitively plausi-
ble variant of the zero-shot task, in which
the learner is exposed to new objects and
the corresponding words in very limited
linguistic contexts. By combining prior
linguistic and visual knowledge acquired
about words and their objects, as well as
exploiting the limited new evidence avail-
able, the learner must learn to associate
new objects with words. Our results on
this task pave the way to realistic simula-
tions of how children or robots could use
existing knowledge to bootstrap grounded
semantic knowledge about new concepts.
1 Introduction
Computational models of meaning that rely on
corpus-extracted context vectors, such as LSA
(Landauer and Dumais, 1997), HAL (Lund and
Burgess, 1996), Topic Models (Griffiths et al,
2007) and more recent neural-network approaches
(Collobert and Weston, 2008; Mikolov et al,
2013b) have successfully tackled a number of lex-
ical semantics tasks, where context vector sim-
ilarity highly correlates with various indices of
semantic relatedness (Turney and Pantel, 2010).
Given that these models are learned from natu-
rally occurring data using simple associative tech-
niques, various authors have advanced the claim
that they might be also capturing some crucial as-
pects of how humans acquire and use language
(Landauer and Dumais, 1997; Lenci, 2008).
However, the models induce the meaning of
words entirely from their co-occurrence with other
words, without links to the external world. This
constitutes a serious blow to claims of cogni-
tive plausibility in at least two respects. One
is the grounding problem (Harnad, 1990; Searle,
1984). Irrespective of their relatively high per-
formance on various semantic tasks, it is debat-
able whether models that have no access to visual
and perceptual information can capture the holis-
tic, grounded knowledge that humans have about
concepts. However, a possibly even more serious
pitfall of vector models is lack of reference: natu-
ral language is, fundamentally, a means to commu-
nicate, and thus our words must be able to refer to
objects, properties and events in the outside world
(Abbott, 2010). Current vector models are purely
language-internal, solipsistic models of meaning.
Consider the very simple scenario in which visual
information is being provided to an agent about
the current state of the world, and the agent?s task
is to determine the truth of a statement similar to
There is a dog in the room. Although the agent
is equipped with a powerful context vector model,
this will not suffice to successfully complete the
task. The model might suggest that the concepts
of dog and cat are semantically related, but it has
no means to determine the visual appearance of
dogs, and consequently no way to verify the truth
of such a simple statement.
Mapping words to the objects they denote is
such a core function of language that humans are
highly optimized for it, as shown by the so-called
fast mapping phenomenon, whereby children can
learn to associate a word to an object or prop-
erty by a single exposure to it (Bloom, 2000;
Carey, 1978; Carey and Bartlett, 1978; Heibeck
and Markman, 1987). But lack of reference is not
1403
only a theoretical weakness: Without the ability to
refer to the outside world, context vectors are ar-
guably useless for practical goals such as learning
to execute natural language instructions (Brana-
van et al, 2009; Chen and Mooney, 2011), that
could greatly benefit from the rich network of lex-
ical meaning such vectors encode, in order to scale
up to real-life challenges.
Very recently, a number of papers have ex-
ploited advances in automated feature extraction
form images and videos to enrich context vectors
with visual information (Bruni et al, 2014; Feng
and Lapata, 2010; Leong and Mihalcea, 2011;
Regneri et al, 2013; Silberer et al, 2013). This
line of research tackles the grounding problem:
Word representations are no longer limited to their
linguistic contexts but also encode visual informa-
tion present in images associated with the corre-
sponding objects. In this paper, we rely on the
same image analysis techniques but instead focus
on the reference problem: We do not aim at en-
riching word representations with visual informa-
tion, although this might be a side effect of our
approach, but we address the issue of automati-
cally mapping objects, as depicted in images, to
the context vectors representing the correspond-
ing words. This is achieved by means of a simple
neural network trained to project image-extracted
feature vectors to text-based vectors through a hid-
den layer that can be interpreted as a cross-modal
semantic space.
We first test the effectiveness of our cross-
modal semantic space on the so-called zero-shot
learning task (Palatucci et al, 2009), which has re-
cently been explored in the machine learning com-
munity (Frome et al, 2013; Socher et al, 2013). In
this setting, we assume that our system possesses
linguistic and visual information for a set of con-
cepts in the form of text-based representations of
words and image-based vectors of the correspond-
ing objects, used for vision-to-language-mapping
training. The system is then provided with visual
information for a previously unseen object, and the
task is to associate it with a word by cross-modal
mapping. Our approach is competitive with re-
spect to the recently proposed alternatives, while
being overall simpler.
The aforementioned task is very demanding and
interesting from an engineering point of view.
However, from a cognitive angle, it relies on
strong, unrealistic assumptions: The learner is
asked to establish a link between a new object and
a word for which they possess a full-fledged text-
based vector extracted from a billion-word cor-
pus. On the contrary, the first time a learner is
exposed to a new object, the linguistic informa-
tion available is likely also very limited. Thus, in
order to consider vision-to-language mapping un-
der more plausible conditions, similar to the ones
that children or robots in a new environment are
faced with, we next simulate a scenario akin to fast
mapping. We show that the induced cross-modal
semantic space is powerful enough that sensible
guesses about the correct word denoting an object
can be made, even when the linguistic context vec-
tor representing the word has been created from as
little as 1 sentence containing it.
The contributions of this work are three-fold.
First, we conduct experiments with simple image-
and text-based vector representations and compare
alternative methods to perform cross-modal map-
ping. Then, we complement recent work (Frome
et al, 2013) and show that zero-shot learning
scales to a large and noisy dataset. Finally, we pro-
vide preliminary evidence that cross-modal pro-
jections can be used effectively to simulate a fast
mapping scenario, thus strengthening the claims
of this approach as a full-fledged, fully inductive
theory of meaning acquisition.
2 Related Work
The problem of establishing word reference has
been extensively explored in computational sim-
ulations of cross-situational learning (see Fazly et
al. (2010) for a recent proposal and extended re-
view of previous work). This line of research has
traditionally assumed artificial models of the ex-
ternal world, typically a set of linguistic or logi-
cal labels for objects, actions and possibly other
aspects of a scene (Siskind, 1996). Recently,
Yu and Siskind (2013) presented a system that
induces word-object mappings from features ex-
tracted from short videos paired with sentences.
Our work complements theirs in two ways. First,
unlike Yu and Siskind (2013) who considered a
limited lexicon of 15 items with only 4 nouns, we
conduct experiments in a large search space con-
taining a highly ambiguous set of potential target
words for every object (see Section 4.1). Most im-
portantly, by projecting visual representations of
objects into a shared semantic space, we do not
limit ourselves to establishing a link between ob-
1404
jects and words. We induce a rich semantic rep-
resentation of the multimodal concept, that can
lead, among other things, to the discovery of im-
portant properties of an object even when we lack
its linguistic label. Nevertheless, Yu and Siskind?s
system could in principle be used to initialize the
vision-language mapping that we rely upon.
Closer to the spirit of our work are two very
recent studies coming from the machine learning
community. Socher et al (2013) and Frome et al
(2013) focus on zero-shot learning in the vision-
language domain by exploiting a shared visual-
linguistic semantic space. Socher et al (2013)
learn to project unsupervised vector-based image
representations onto a word-based semantic space
using a neural network architecture. Unlike us,
Socher and colleagues train an outlier detector
to decide whether a test image should receive a
known-word label by means of a standard super-
vised object classifier, or be assigned an unseen
label by vision-to-language mapping. In our zero-
shot experiments, we assume no access to an out-
lier detector, and thus, the search for the correct
label is performed in the full concept space. Fur-
thermore, Socher and colleagues present a much
more constrained evaluation setup, where only 10
concepts are considered, compared to our experi-
ments with hundreds or thousands of concepts.
Frome et al (2013) use linear regression to
transform vector-based image representations onto
vectors representing the same concepts in linguis-
tic semantic space. Unlike Socher et al (2013) and
the current study that adopt simple unsupervised
techniques for constructing image representations,
Frome et al (2013) rely on a supervised state-of-
the-art method: They feed low-level features to a
deep neural network trained on a supervised object
recognition task (Krizhevsky et al, 2012). Fur-
thermore, their text-based vectors encode very rich
information, such as
~
king ? ~man + ~woman =
~queen (Mikolov et al, 2013c). A natural ques-
tion we aim to answer is whether the success of
cross-modal mapping is due to the high-quality
embeddings or to the general algorithmic design.
If the latter is the case, then these results could be
extended to traditional distributional vectors bear-
ing other desirable properties, such as high inter-
pretability of dimensions.
(a) (b)
Figure 1: A potential wampimuk (a) together with
its projection onto the linguistic space (b).
3 Zero-shot learning and fast mapping
?We found a cute, hairy wampimuk sleeping be-
hind the tree.? Even though the previous state-
ment is certainly the first time one hears about
wampimuks, the linguistic context already creates
some visual expectations: Wampimuks probably
resemble small animals (Figure 1a). This is the
scenario of zero-shot learning. Moreover, if this is
also the first linguistic encounter of that concept,
then we refer to the task as fast mapping.
Concretely, we assume that concepts, denoted
for convenience by word labels, are represented in
linguistic terms by vectors in a text-based distri-
butional semantic space (see Section 4.3). Objects
corresponding to concepts are represented in vi-
sual terms by vectors in an image-based semantic
space (Section 4.2). For a subset of concepts (e.g.,
a set of animals, a set of vehicles), we possess in-
formation related to both their linguistic and visual
representations. During training, this cross-modal
vocabulary is used to induce a projection func-
tion (Section 4.4), which ? intuitively ? represents
a mapping between visual and linguistic dimen-
sions. Thus, this function, given a visual vector,
returns its corresponding linguistic representation.
At test time, the system is presented with a previ-
ously unseen object (e.g., wampimuk). This object
is projected onto the linguistic space and associ-
ated with the word label of the nearest neighbor in
that space (degus in Figure 1b).
The fast mapping setting can be seen as a spe-
cial case of the zero-shot task. Whereas for the lat-
ter our system assumes that all concepts have rich
linguistic representations (i.e., representations es-
timated from a large corpus), in the case of the for-
mer, new concepts are assumed to be encounted in
a limited linguistic context and therefore lacking
rich linguistic representations. This is operational-
ized by constructing the text-based vector for these
1405
Figure 2: Images of chair as extracted from
CIFAR-100 (left) and ESP (right).
concepts from a context of just a few occurrences.
In this way, we simulate the first encounter of a
learner with a concept that is new in both visual
and linguistic terms.
4 Experimental Setup
4.1 Visual Datasets
CIFAR-100 The CIFAR-100 dataset
(Krizhevsky, 2009) consists of 60,000 32x32
colour images (note the extremely small size)
representing 100 distinct concepts, with 600
images per concept. The dataset covers a wide
range of concrete domains and is organized into
20 broader categories. Table 1 lists the concepts
used in our experiments organized by category.
ESP Our second dataset consists of 100K im-
ages from the ESP-Game data set, labeled through
a ?game with a purpose? (Von Ahn, 2006).
1
The
ESP image tags form a vocabulary of 20,515
unique words. Unlike other datasets used for zero-
shot learning, it covers adjectives and verbs in ad-
dition to nouns. On average, an image has 14
tags and a word appears as a tag for 70 images.
Unlike the CIFAR-100 images, which were cho-
sen specifically for image object recognition tasks
(i.e., each image is clearly depicting a single ob-
ject in the foreground), ESP contains a random se-
lection of images from the Web. Consequently,
objects do not appear in most images in their pro-
totypical display, but rather as elements of com-
plex scenes (see Figure 2). Thus, ESP constitutes
a more realistic, and at the same time more chal-
lenging, simulation of how things are encountered
in real life, testing the potentials of cross-modal
mapping in dealing with the complex scenes that
one would encounter in event recognition and cap-
tion generation tasks.
1
http://www.cs.cmu.edu/
?
biglou/
resources/
4.2 Visual Semantic Spaces
Image-based vectors are extracted using the unsu-
pervised bag-of-visual-words (BoVW) represen-
tational architecture (Sivic and Zisserman, 2003;
Csurka et al, 2004), that has been widely and suc-
cessfully applied to computer vision tasks such as
object recognition and image retrieval (Yang et al,
2007). First, low-level visual features (Szeliski,
2010) are extracted from a large collection of im-
ages and clustered into a set of ?visual words?.
The low-level features of a specific image are then
mapped to the corresponding visual words, and the
image is represented by a count vector recording
the number of occurrences of each visual word in
it. We do not attempt any parameter tuning of the
pipeline.
As low-level features, we use Scale Invariant
Feature Transform (SIFT) features (Lowe, 2004).
SIFT features are tailored to capture object parts
and to be invariant to several image transfor-
mations such as rotation, illumination and scale
change. These features are clustered into vocab-
ularies of 5,000 (ESP) and 4,096 (CIFAR-100) vi-
sual words.
2
To preserve spatial information in the
BoVW representation, we use the spatial pyramid
technique (Lazebnik et al, 2006), which consists
in dividing the image into several regions, comput-
ing BoVW vectors for each region and concatenat-
ing them. In particular, we divide ESP images into
16 regions and the smaller CIFAR-100 images into
4. The vectors resulting from region concatenation
have dimensionality 5000 ? 16 = 80, 000 (ESP)
and 4, 096 ? 4 = 16, 384 (CIFAR-100), respec-
tively. We apply Local Mutual Information (LMI,
(Evert, 2005)) as weighting scheme and reduce the
full co-occurrence space to 300 dimensions using
the Singular Value Decomposition.
For CIFAR-100, we extract distinct visual vec-
tors for single images. For ESP, given the size
and amount of noise in this dataset, we build vec-
tors for visual concepts, by normalizing and sum-
ming the BoVW vectors of all the images that have
the relevant concept as a tag. Note that relevant
literature (Pereira et al, 2010) has emphasized
the importance of learners self-generating multi-
ple views when faced with new objects. Thus, our
multiple-image assumption should not be consid-
ered as problematic in the current setup.
2
For selecting the size of the vocabulary size, we relied on
standard settings found in the relevant literature (Bruni et al,
2014; Chatfield et al, 2011).
1406
Category Seen Concepts Unseen (Test) Concepts
aquatic mammals beaver, otter, seal, whale dolphin
fish ray, trout shark
flowers orchid, poppy, sunflower, tulip rose
food containers bottle, bowl, can ,plate cup
fruit vegetable apple, mushroom, pear orange
household electrical devices keyboard, lamp, telephone, television clock
household furniture chair, couch, table, wardrobe bed
insects bee, beetle, caterpillar, cockroach butterfly
large carnivores bear, leopard, lion, wolf tiger
large man-made outdoor things bridge, castle, house, road skyscraper
large natural outdoor scenes cloud, mountain, plain, sea forest
large omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephant
medium-sized mammals fox, porcupine, possum, skunk raccoon
non-insect invertebrates crab, snail, spider, worm lobster
people baby, girl, man, woman boy
reptiles crocodile, dinosaur, snake, turtle lizard
small mammals hamster, mouse, rabbit, shrew squirrel
vehicles 1 bicycle, motorcycle, train bus
vehicles 2 rocket, tank, tractor streetcar
Table 1: Concepts in our version of the CIFAR-100 data set
We implement the entire visual pipeline with
VSEM, an open library for visual seman-
tics (Bruni et al, 2013).
3
4.3 Linguistic Semantic Spaces
For constructing the text-based vectors, we fol-
low a standard pipeline in distributional semantics
(Turney and Pantel, 2010) without tuning its pa-
rameters and collect co-occurrence statistics from
the concatenation of ukWaC
4
and the Wikipedia,
amounting to 2.7 billion tokens in total. Seman-
tic vectors are constructed for a set of 30K target
words (lemmas), namely the top 20K most fre-
quent nouns, 5K most frequent adjectives and 5K
most frequent verbs, and the same 30K lemmas are
also employed as contextual elements. We collect
co-occurrences in a symmetric context window of
20 elements around a target word. Finally, simi-
larly to the visual semantic space, raw counts are
transformed by applying LMI and then reduced to
300 dimensions with SVD.
5
4.4 Cross-modal Mapping
The process of learning to map objects to the their
word label is implemented by training a projec-
tion function f
proj
v?w
from the visual onto the lin-
guistic semantic space. For the learning, we use
a set of N
s
seen concepts for which we have both
image-based visual representations V
s
? R
N
s
?d
v
3
http://clic.cimec.unitn.it/vsem/
4
http://wacky.sslmit.unibo.it
5
We also experimented with the image- and text-based
vectors of Socher et al (2013), but achieved better perfor-
mance with the reported setup.
and text-based linguistic representations W
s
?
R
N
s
?d
w
. The projection function is subject to
an objective that aims at minimizing some cost
function between the induced text-based represen-
tations
?
W
s
? R
N
s
?d
w
and the gold ones W
s
.
The induced f
proj
v?w
is then applied to the image-
based representations V
u
? R
N
u
?d
v
of N
u
un-
seen objects to transform them into text-based rep-
resentations
?
W
u
? R
N
u
?d
w
. We implement 4
alternative learning algorithms for inducing the
cross-modal projection function f
proj
v?w
.
Linear Regression (lin) Our first model is a very
simple linear mapping between the two modali-
ties estimated by solving a least-squares problem.
This method is similar to the one introduced by
Mikolov et al (2013a) for estimating a translation
matrix, only solved analytically. In our setup, we
can see the two different modalities as if they were
different languages. By using least-squares regres-
sion, the projection function f
proj
v?w
can be de-
rived as
f
proj
v?w
= (V
T
s
V
s
)
?1
V
T
s
W
s
(1)
Canonical Correlation Analysis (CCA)
CCA (Hardoon et al, 2004; Hotelling, 1936)
and variations thereof have been successfully used
in the past for annotation of regions (Socher and
Fei-Fei, 2010) and complete images (Hardoon et
al., 2006; Hodosh et al, 2013). Given two paired
observation matrices, in our case V
s
and W
s
,
CCA aims at capturing the linear relationship
that exists between these variables. This is
achieved by finding a pair of matrices, in our
1407
case C
V
? R
d
v
?d
and C
W
? R
d
w
?d
, such that
the correlation between the projections of the
two multidimensional variables into a common,
lower-rank space is maximized. The resulting
multimodal space has been shown to provide a
good approximation to human concept similarity
judgments (Silberer and Lapata, 2012). In our
setup, after applying CCA on the two spaces V
s
and W
s
, we obtain the two projection mappings
onto the common space and thus our projection
function can be derived as:
f
proj
v?w
= C
V
C
W
?1
(2)
Singular Value Decomposition (SVD) SVD is
the most widely used dimensionality reduction
technique in distributional semantics (Turney and
Pantel, 2010), and it has recently been exploited
to combine visual and linguistic dimensions in
the multimodal distributional semantic model of
Bruni et al (2014). SVD smoothing is also a way
to infer values of unseen dimensions in partially
incomplete matrices, a technique that has been ap-
plied to the task of inferring word tags of unanno-
tated images (Hare et al, 2008). Assuming that the
concept-representing rows of V
s
and W
s
are or-
dered in the same way, we apply the (k-truncated)
SVD to the concatenated matrix [V
s
W
s
], such
that [
?
V
s
?
W
s
] = U
k
?
k
Z
T
k
is a k-rank approxima-
tion of the original matrix.
6
The projection func-
tion is then:
f
proj
v?w
= Z
k
Z
T
k
(3)
where the input is appropriately padded with 0s
([V
u
0
Nu?W
]) and we discard the visual block of
the output matrix [
?
V
u
?
W
u
].
Neural Network (NNet) The last model that we
introduce is a neural network with one hidden
layer. The projection function in this model can
be described as:
f
proj
v?w
= ?
v?w
(4)
where ?
v?w
consists of the model weights ?
(1)
?
R
d
v
?h
and ?
(2)
? R
h?d
w
that map the in-
put image-based vectors V
s
first to the hid-
den layer and then to the output layer in or-
der to obtain text-based vectors, i.e.,
?
W
s
=
?
(2)
(?
(1)
(Vs?
(1)
)?
(2)
), where ?
(1)
and ?
(2)
are
6
We denote the right singular vectors matrix by Z instead
of the customaryV to avoid confusion with the visual matrix.
the non-linear activation functions. We experi-
mented with sigmoid, hyperbolic tangent and lin-
ear; hyperbolic tangent yielded the highest perfor-
mance. The weights are estimated by minimizing
the objective function
J(?
v?w
) =
1
2
(1? sim(W
s
,
?
W
s
)) (5)
where sim is some similarity function. In our ex-
periments we used cosine as similarity function,
so that sim(A,B) =
AB
?A??B?
, thus penalizing pa-
rameter settings leading to a low cosine between
the target linguistic representations W
s
and those
produced by the projection function
?
W
s
. The co-
sine has been widely used in the distributional se-
mantic literature, and it has been shown to out-
perform Euclidean distance (Bullinaria and Levy,
2007).
7
Parameters were estimated with standard
backpropagation and L-BFGS.
5 Results
Our experiments focus on the tasks of zero-shot
learning (Sections 5.1 and 5.2) and fast mapping
(Section 5.3). In both tasks, the projected vector of
the unseen concept is labeled with the word asso-
ciated to its cosine-based nearest neighbor vector
in the corresponding semantic space.
For the zero-shot task we report the accuracy
of retrieving the correct label among the top k
neighbors from a semantic space populated with
the union of seen and unseen concepts. For fast
mapping, we report the mean rank of the correct
concept among fast mapping candidates.
5.1 Zero-shot Learning in CIFAR-100
For this experiment, we use the intersection of
our linguistic space with the concepts present in
CIFAR-100, containing a total of 90 concepts. For
each concept category, we treat all concepts but
one as seen concepts (Table 1). The 71 seen con-
cepts correspond to 42,600 distinct visual vectors
and are used to induce the projection function. Ta-
ble 2 reports results obtained by averaging the per-
formance on the 11,400 distinct vectors of the 19
unseen concepts.
Our 4 models introduced in Section 4.4 are
compared to a theoretically derived baseline
Chance simulating selecting a label at random. For
the neural network NN, we use prior knowledge
7
We also experimented with the same objective func-
tion as Socher et al (2013), however, our objective function
yielded consistently better results in all experimental settings.
1408
PP
P
P
P
P
Model
k
1 2 3 5 10 20
Chance 1.1 2.2 3.3 5.5 11.0 22.0
SVD 1.9 5.0 8.1 14.5 29.0 48.6
CCA 3.0 6.9 10.7 17.9 31.7 51.7
lin 2.4 6.4 10.5 18.7 33.0 55.0
NN 3.9 6.6 10.6 21.9 37.9 58.2
Table 2: Percentage accuracy among top k nearest
neighbors on CIFAR-100.
about the number of concept categories to set the
number of hidden units to 20 in order to avoid
tuning of this parameter. For the SVD model, we
set the number of dimensions to 300, a common
choice in distributional semantics, coherent with
the settings we used for the visual and linguistic
spaces.
First and foremost, all 4 models outperform
Chance by a large margin. Surprisingly, the very
simple lin method outperforms both CCA and SVD.
However, NN, an architecture that can capture
more complex, non-linear relations in features
across modalities, emerges as the best performing
model, confirming on a larger scale the recent find-
ings of Socher et al (2013).
5.1.1 Concept Categorization
In order to gain qualitative insights into the perfor-
mance of the projection process of NN, we attempt
to investigate the role and interpretability of the
hidden layer. We achieve this by looking at which
visual concepts result in the highest hidden unit
activation.
8
This is inspired by analogous quali-
tative analysis conducted in Topic Models (Grif-
fiths et al, 2007), where ?topics? are interpreted
in terms of the words with the highest probability
under each of them.
Table 3 presents both seen and unseen con-
cepts corresponding to visual vectors that trigger
the highest activation for a subset of hidden units.
The table further reports, for each hidden unit, the
?correct? unseen concept for the category of the
top seen concepts, together with its rank in terms
of activation of the unit. The analysis demon-
strates that, although prior knowledge about cat-
egories was not explicitly used to train the net-
work, the latter induced an organization of con-
cepts into superordinate categories in which the
8
For this post-hoc analysis, we include a sparsity param-
eter in the objective function of Equation 5 in order to get
more interpretable results; hidden units are therefore maxi-
mally activated by a only few concepts.
Unseen Concept Nearest Neighbors
tiger cat, microchip, kitten, vet, pet
bike spoke, wheel, brake, tyre, motorcycle
blossom bud, leaf, jasmine, petal, dandelion
bakery quiche, bread, pie, bagel, curry
Table 4: Top 5 neighbors in linguistic space after
visual vector projection of 4 unseen concepts.
hidden layer acts as a cross-modal concept cate-
gorization/organization system. When the induced
projection function maps an object onto the lin-
guistic space, the derived text vector will inherit
a mixture of textual features from the concepts
that activated the same hidden unit as the object.
This suggests a bias towards seen concepts. Fur-
thermore, in many cases of miscategorization, the
concepts are still semantically coherent with the
induced category, confirming that the projection
function is indeed capturing a latent, cross-modal
semantic space. A squirrel, although not a ?large
omnivore?, is still an animal, while butterflies are
not flowers but often feed on their nectar.
5.2 Zero-shot Learning in ESP
For this experiment, we focus on NN, the best per-
forming model in the previous experiment. We
use a set of approximately 9,500 concepts, the in-
tersection of the ESP-based visual semantic space
with the linguistic space. For tuning the number
of hidden units of NN, we use the MEN-concrete
dataset of Bruni et al (2014). Finally, we ran-
domly pick 70% of the concepts to induce the pro-
jection function f
proj
v?w
and report results on the
remaining 30%. Note that the search space for the
correct label in this experiment is approximately
95 times larger than the one used for the experi-
ment presented in Section 5.1.
Although our experimental setup differs from
the one of Frome et al (2013), thus preventing a
direct comparison, the results reported in Table 5
are on a comparable scale to theirs. We note that
previous work on zero-shot learning has used stan-
dard object recognition benchmarks. To the best
of our knowledge, this is the first time this task has
been performed on a dataset as noisy as ESP. Over-
all, the results suggest that cross-modal mapping
could be applied in tasks where images exhibit a
more complex structure, e.g., caption generation
and event recognition.
1409
Seen Concepts Unseen Concept Rank of Correct CIFAR-100 Category
Unseen Concept
Unit 1 sunflower, tulip, pear butterfly 2 (rose) flowers
Unit 2 cattle, camel, bear squirrel 2 (elephant) large omnivores and herbivores
Unit 3 castle, bridge, house bus 4 (skyscraper) large man-made outdoor things
Unit 4 man, girl, baby boy 1 people
Unit 5 motorcycle, bicycle, tractor streetcar 2 (bus) vehicles 1
Unit 6 sea, plain, cloud forest 1 large natural outdoor scenes
Unit 7 chair, couch, table bed 1 household furniture
Unit 8 plate, bowl, can clock 3 (cup) food containers
Unit 9 apple, pear, mushroom orange 1 fruit and vegetables
Table 3: Categorization induced by the hidden layer of the NN; concepts belonging in the same CIFAR-
100 categories, reported in the last column, are marked in bold. Example: Unit 1 receives the highest
activation during training by the category flowers and at test time by butterfly, belonging to insects. The
same unit receives the second highest activation by the ?correct? test concept, the flower rose.
P
P
P
P
P
P
Model
k
1 2 5 10 50
Chance 0.01 0.02 0.05 0.10 0.5
NN 0.8 1.9 5.6 9.7 30.9
Table 5: Percentage accuracy among top k nearest
neighbors on ESP.
5.3 Fast Mapping in ESP
In this section, we aim at simulating a fast map-
ping scenario in which the learner has been just
exposed to a new concept, and thus has limited lin-
guistic evidence for that concept. We operational-
ize this by considering the 34 concrete concepts
introduced by Frassinelli and Keller (2012), and
deriving their text-based representations from just
a few sentences randomly picked from the corpus.
Concretely, we implement 5 models: context 1, con-
text 5, context 10, context 20 and context full, where
the name of the model denotes the number of sen-
tences used to construct the text-based representa-
tions. The derived vectors were reduced with the
same SVD projection induced from the complete
corpus. Cross-modal mapping is done via NN.
The zero-shot framework leads us to frame fast
mapping as the task of projecting visual represen-
tations of new objects onto language space for re-
trieving their word labels (v? w). This mapping
from visual to textual representations is arguably
a more plausible task than vice versa. If we think
about how linguistic reference is acquired, a sce-
nario in which a learner first encounters a new ob-
ject and then seeks its reference in the language of
the surrounding environment (e.g., adults having a
conversation, the text of a book with an illustration
of an unknown object) is very natural. Further-
more, since not all new concepts in the linguistic
environment refer to new objects (they might de-
note abstract concepts or out-of-scene objects), it
seems more reasonable for the learner to be more
alerted to linguistic cues about a recently-spotted
new object than vice versa. Moreover, once the
learner observes a new object, she can easily con-
struct a full visual representation for it (and the
acquisition literature has shown that humans are
wired for good object segmentation and recogni-
tion (Spelke, 1994)) ? the more challenging task is
to scan the ongoing and very ambiguous linguistic
communication for contexts that might be relevant
and informative about the new object. However,
fast mapping is often described in the psycholog-
ical literature as the opposite task: The learner
is exposed to a new word in context and has to
search for the right object referring to it. We im-
plement this second setup (w? v) by training the
projection function f
proj
w?v
which maps linguis-
tic vectors to visual ones. The adaptation of NN is
straightforward; the new objective function is de-
rived as
J(?
w?v
) =
1
2
(1? sim(V
s
,
?
V
s
)) (6)
where
?
V
s
= ?
(2)
(?
(1)
(Ws?
(1)
)?
(2)
), ?
(1)
?
R
d
w
?h
and ?
(2)
? R
h?d
v
.
Table 7 presents the results. Not surprisingly,
performance increases with the number of sen-
tences that are used to construct the textual repre-
sentations. Furthermore, all models perform bet-
ter than Chance, including those that are based on
just 1 or 5 sentences. This suggests that the system
can make reasonable inferences about object-word
connections even when linguistic evidence is very
scarce.
Regarding the sources of error, a qualitative
analysis of predicted word labels and objects as
1410
v?w w?v
cooker?potato dishwasher? corkscrew
clarinet? drum potato? corn
gorilla? elephant guitar? violin
scooter? car scarf? trouser
Table 6: Top-ranked concepts in cases where the
gold concepts received numerically high ranks.
X
X
X
X
X
X
X
X
Context
Mapping
v? w w? v
Chance 17 17
context 1 12.6 14.5
context 5 8.08 13.29
context 10 7.29 13.44
context 20 6.02 12.17
context full 5.52 5.88
Table 7: Mean rank results averaged across 34
concepts when mapping an image-based vector
and retrieving its linguistic neighbors (v? w) as
well as when mapping a text-based vector and
retrieving its visual neighbors (w? v). Lower
numbers cue better performance.
presented in Table 6 suggests that both textual
and visual representations, although capturing rel-
evant ?topical? or ?domain? information, are not
enough to single out the properties of the target
concept. As an example, the textual vector of dish-
washer contains kitchen-related dimensions such
as ?fridge, oven, gas, hob, ..., sink?. After projecting
onto the visual space, its nearest visual neighbours
are the visual ones of the same-domain concepts
corkscrew and kettle. The latter is shown in Figure
3a, with a gas hob well in evidence. As a further
example, the visual vector for cooker is extracted
from pictures such as the one in Figure 3b. Not
surprisingly, when projecting it onto the linguis-
tic space, the nearest neighbours are other kitchen-
related terms, i.e., potato and dishwasher.
6 Conclusion
At the outset of this work, we considered the
problem of linking purely language-based distri-
(a) A kettle
(b) A cooker
Figure 3: Two images from ESP.
butional semantic spaces with objects in the vi-
sual world by means of cross-modal mapping. We
compared recent models for this task both on a
benchmark object recognition dataset and on a
more realistic and noisier dataset covering a wide
range of concepts. The neural network architec-
ture emerged as the best performing approach, and
our qualitative analysis revealed that it induced a
categorical organization of concepts. Most impor-
tantly, our results suggest the viability of cross-
modal mapping for grounded word-meaning ac-
quisition in a simulation of fast mapping.
Given the success of NN, we plan to experi-
ment in the future with more sophisticated neural
network architectures inspired by recent work in
machine translation (Gao et al, 2013) and mul-
timodal deep learning (Srivastava and Salakhut-
dinov, 2012). Furthermore, we intend to adopt
visual attributes (Farhadi et al, 2009; Silberer
et al, 2013) as visual representations, since they
should allow a better understanding of how cross-
modal mapping works, thanks to their linguistic
interpretability. The error analysis in Section 5.3
suggests that automated localization techniques
(van de Sande et al, 2011), distinguishing an ob-
ject from its surroundings, might drastically im-
prove mapping accuracy. Similarly, in the textual
domain, models that extract collocates of a word
that are more likely to denote conceptual proper-
ties (Kelly et al, 2012) might lead to more infor-
mative and discriminative linguistic vectors. Fi-
nally, the lack of large child-directed speech cor-
pora constrained the experimental design of fast
mapping simulations; we plan to run more realis-
tic experiments with true nonce words and using
source corpora (e.g., the Simple Wikipedia, child
stories, portions of CHILDES) that contain sen-
tences more akin to those a child might effectively
hear or read in her word-learning years.
Acknowledgments
We thank Adam Li?ska for helpful discussions and
the 3 anonymous reviewers for useful comments.
This work was supported by ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES).
References
Barbara Abbott. 2010. Reference. Oxford University
Press, Oxford, UK.
1411
Paul Bloom. 2000. How Children Learn the Meanings
of Words. MIT Press, Cambridge, MA.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL/IJCNLP, pages 82?90.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open
library for visual semantics representation. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word. Papers and Reports on Child Lan-
guage Development, 15:17?29.
Susan Carey. 1978. The child as a word learner. In
M. Halle, J. Bresnan, and G. Miller, editors, Linguis-
tics Theory and Psychological Reality. MIT Press,
Cambridge, MA.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of AAAI, pages
859?865, San Francisco, CA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and C?edric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop
on Statistical Learning in Computer Vision, ECCV,
pages 1?22, Prague, Czech Republic.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778?
1785, Miami Beach, FL.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34:1017?1063.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91?99, Los Angeles, CA.
Diego Frassinelli and Frank Keller. 2012. The plausi-
bility of semantic properties generated by a distribu-
tional model: Evidence from a visual world experi-
ment. In Proceedings of CogSci, pages 1560?1565.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639?2664.
David R Hardoon, Craig Saunders, Sandor Szedmak,
and John Shawe-Taylor. 2006. A correlation ap-
proach for automatic image annotation. In Ad-
vanced Data Mining and Applications, pages 681?
692. Springer.
Jonathon Hare, Sina Samangooei, Paul Lewis, and
Mark Nixon. 2008. Semantic spaces revisited: In-
vestigating the performance of auto-annotation and
semantic retrieval using semantic spaces. In Pro-
ceedings of CIVR, pages 359?368, Niagara Falls,
Canada.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?
346.
Tracy Heibeck and Ellen Markman. 1987. Word learn-
ing in children: an examination of fast mapping.
Child Development, 58:1021?1024.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853?899.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11?20, Montreal, Canada.
1412
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Proceedings of NIPS,
pages 1106?1114.
Alex Krizhevsky. 2009. Learning multiple layers of
features from tiny images. Master?s thesis.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169?2178, Wash-
ington, DC.
Alessandro Lenci. 2008. Distributional approaches in
linguistic and cognitive research. Italian Journal of
Linguistics, 20(1):1?31.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Proceedings of NIPS,
pages 1410?1418, Vancouver, Canada.
Alfredo F Pereira, Karin H James, Susan S Jones,
and Linda B Smith. 2010. Early biases and de-
velopmental changes in self-generated object views.
Journal of vision, 10(11).
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25?36.
John Searle. 1984. Minds, Brains and Science. Har-
vard University Press, Cambridge, MA.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423?1433, Jeju, Korea.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of ACL, pages 572?582,
Sofia, Bulgaria.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39?91.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470?
1477, Nice, France.
Richard Socher and Li Fei-Fei. 2010. Connecting
modalities: Semi-supervised segmentation and an-
notation of images using unaligned text corpora. In
Proceedings of CVPR, pages 966?973.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Elizabeth Spelke. 1994. Initial knowledge: Six sug-
gestions. Cognition, 50:431?445.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In Proceedings of NIPS, pages 2231?2239.
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer, Berlin.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Koen van de Sande, Jasper Uijlings, Theo Gevers, and
Arnold Smeulders. 2011. Segmentation as selec-
tive search for object recognition. In Proceedings of
ICCV, pages 1879?1886, Barcelona, Spain.
Luis Von Ahn. 2006. Games with a purpose. Com-
puter, 29(6):92?94.
Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In
James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197?206. ACM.
1413
Haonan Yu and Jeffrey Siskind. 2013. Grounded lan-
guage learning from video described with sentences.
In Proceedings of ACL, pages 53?63, Sofia, Bul-
garia.
1414
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 22?32,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Distributional semantics from text and images
Elia Bruni
CIMeC, University of Trento
elia.bruni@unitn.it
Giang Binh Tran
EMLCT, Free University of Bolzano &
CIMeC, University of Trento
Giang.Tran@stud-inf.unibz.it
Marco Baroni
CIMeC, University of Trento
marco.baroni@unitn.it
Abstract
We present a distributional semantic model
combining text- and image-based features. We
evaluate this multimodal semantic model on
simulating similarity judgments, concept clus-
tering and the BLESS benchmark. When inte-
grated with the same core text-based model,
image-based features are at least as good as
further text-based features, and they capture
different qualitative aspects of the tasks, sug-
gesting that the two sources of information are
complementary.
1 Introduction
Distributional semantic models use large text cor-
pora to derive estimates of semantic similarities be-
tween words. The basis of these procedures lies in
the hypothesis that semantically similar words tend
to appear in similar contexts (Miller and Charles,
1991; Wittgenstein, 1953). For example, the mean-
ing of spinach (primarily) becomes the result of sta-
tistical computations based on the association be-
tween spinach and words like plant, green, iron,
Popeye, muscles. Alongside their applications in
NLP areas such as information retrieval or word
sense disambiguation (Turney and Pantel, 2010), a
strong debate has arisen on whether distributional
semantic models are also reflecting human cogni-
tive processes (Griffiths et al, 2007; Baroni et al,
2010). Many cognitive scientists have however ob-
served that these techniques relegate the process of
meaning extraction solely to linguistic regularities,
forgetting that humans can also rely on non-verbal
experience, and comprehension also involves the ac-
tivation of non-linguistic representations (Barsalou
et al, 2008; Glenberg, 1997; Zwaan, 2004). They
argue that, without grounding words to bodily ac-
tions and perceptions in the environment, we can
never get past defining a symbol by simply pointing
to covariation of amodal symbolic patterns (Harnad,
1990). Going back to our example, the meaning of
spinach should come (at least partially) from our ex-
perience with spinach, its colors, smell and the oc-
casions in which we tend to encounter it.
We can thus distinguish two different views of
how meaning emerges, one stating that it emerges
from association between linguistic units reflected
by statistical computations on large bodies of text,
the other stating that meaning is still the result of an
association process, but one that concerns the asso-
ciation between words and perceptual information.
In our work, we try to make these two appar-
ently mutually exclusive accounts communicate, to
construct a richer and more human-like notion of
meaning. In particular, we concentrate on percep-
tual information coming from images, and we cre-
ate a multimodal distributional semantic model ex-
tracted from texts and images, putting side by side
techniques from NLP and computer vision. In a nut-
shell, our technique is based on using a collection
of labeled pictures to build vectors recording the co-
occurrences of words with image-based features, ex-
actly as we would do with textual co-occurrences.
We then concatenate the image-based vector with
a standard text-based distributional vector, to ob-
tain our multimodal representation. The prelimi-
nary results reported in this paper indicate that en-
22
riching a text-based model with image-based fea-
tures is at least not damaging, with respect to en-
larging the purely textual component, and it leads to
qualitatively different results, indicating that the two
sources of information are not redundant.
The rest of the paper is structured as follows. Sec-
tion 2 reviews relevant work including distributional
semantic models, computer vision techniques suit-
able to our purpose and systems combining text and
image information, including the only work we are
aware of that attempts something similar to what we
try here. We introduce our multimodal distributional
semantic model in Section 3, and our experimental
setup and procedure in Section 4. Our experiments?
results are discussed in Section 5. Section 6 con-
cludes summarizing current achievements and dis-
cussing next directions.
2 Related Work
2.1 Text-based distributional semantic models
Traditional corpus-based models of semantic repre-
sentation base their analysis on textual input alone
(Turney and Pantel, 2010). Assuming the distribu-
tional hypothesis (Miller and Charles, 1991), they
represent semantic similarity between words as a
function of the degree of overlap among their lin-
guistic contexts. Similarity is computed in a seman-
tic space represented as a matrix, with words as rows
and contextual elements as columns/dimensions.
Thanks to the geometrical nature of the represen-
tation, words are compared using a distance met-
ric, such as the cosine of the angle between vectors
(Landauer and Dumais, 1997).
2.2 Bag of visual words
In NLP, ?bag of words? (BoW) is a dictionary-based
method in which a document is represented as a
?bag? (i.e., order is not considered), which contains
words from the dictionary. In computer vision, ?bag
of visual words? (BoVW) is a similar idea for image
representation (Sivic and Zisserman, 2003; Csurka
et al, 2004; Nister and Stewenius, 2006; Bosch et
al., 2007; Yang et al, 2007).
Here, an image is treated as a document, and fea-
tures from a dictionary of visual elements extracted
from the image are considered as the ?words? repre-
senting the image. The following pipeline is typ-
ically adopted in order to group the local interest
points into types (visual words) within and across
images, so that then an image can be represented
by the number of occurrences of each visual word
type in it, analogously to BoW. From every image
of a data set, keypoints are automatically detected
and represented as vectors of various descriptors.
Keypoint vectors are then projected into a common
space and grouped into a number of clusters. Each
cluster is treated as a discrete visual word (this tech-
nique is generally known as vector quantization).
With its keypoints mapped onto visual words, each
image can then be represented as a BoVW feature
vector according to the count of each visual word. In
this way, we move from representing the image by
a varying number of high-dimensional keypoint de-
scriptor vectors to a representation in terms of a sin-
gle sparse vector of fixed dimensionality across all
images. What kind of image content a visual word
captures exactly depends on a number of factors, in-
cluding the descriptors used to identify and represent
local interest points, the quantization algorithm and
the number of target visual words selected. In gen-
eral, local interest points assigned to the same visual
word tend to be patches with similar low-level ap-
pearance; but these common types of local patterns
need not be correlated with object-level parts present
in the images. Figure 1 illustrates the procedure to
form bags of visual words. Importantly for our pur-
poses, the BoVW representation, despite its unre-
lated origin in computer vision, is entirely analogous
to the BoW representation, making the integration of
text- and image-based features very straightforward.
2.3 Integrating textual and perceptual
information
Louwerse (2011), contributing to the debate on sym-
bol grounding in cognitive science, theorizes the in-
terdependency account, which suggests a conver-
gence of symbolic theories (such as distributional
semantics) and perceptual theories of meaning, but
lacks of a concrete way to harvest perceptual infor-
mation computationally. Andrews et al (2009) com-
plement text-based models with experiential infor-
mation, by combining corpus-based statistics with
speaker-generated feature norms as a proxy of per-
ceptual experience. However, the latter are an un-
satisfactory proxy, since they are still verbally pro-
23
Figure 1: Illustration of bag of visual words procedure: (a) detect and represent local interest points as descriptor
vectors (b) quantize vectors (c) histogram computation to form BoVW vector for the image
duced descriptions, and they are expensive to collect
from subjects via elicitation techniques.
Taking inspiration from methods originally used
in text processing, algorithms for image labeling,
search and retrieval have been built upon the connec-
tion between text and visual features. Such models
learn the statistical models which characterize the
joint statistical distribution of observed visual fea-
tures and verbal image tags (Hofmann, 2001; Hare
et al, 2008). This line of research is pursuing the re-
verse of what we are interested in: using text to im-
prove the semantic description of images, whereas
we want to exploit images to improve our approxi-
mation to word meaning.
Feng and Lapata are the first trying to integrate
authentic visual information in a text-based distribu-
tional model (Feng and Lapata, 2010). Using a col-
lection of BBC news with pictures as corpus, they
train a Topic model where text and visual words are
represented in terms of the same shared latent di-
mensions (topics). In this framework, word meaning
is modeled as a probability distribution over a set of
latent multimodal topics and the similarity between
two words can be estimated by measuring the topics
they have in common. A better correlation with se-
mantic intuitions is obtainable when visual modality
is taken into account, in comparison to estimating
the topic structure from text only.
Although Feng and Lapata?s work is very promis-
ing and the main inspiration for our own, their
method requires the extraction of a single distribu-
tional model from the same mixed-media corpus.
This has two important drawbacks: First, the tex-
tual model must be extracted from the same corpus
images are taken from, and the text context extrac-
tion methods must be compatible with the overall
multimodal approach. Thus, image features can-
not be added to a state-of-the-art text-based distri-
butional model ? e.g., a model computed on the
whole Wikipedia or larger corpora using syntactic
dependency information ? to assess whether visual
information is helping even when purely textual fea-
tures are already very good. Second, by training a
joint model with latent dimensions that mix textual
and visual information, it becomes hard to assess,
quantitatively and qualitatively, the separate effect
of image-based features on the overall performance.
In order to overcome these issues, we propose a
somewhat simpler approach, in which the text- and
image-based models are independently constructed
from different sources, and then concatenated.
3 Proposed method
Figure 2 presents a diagram of our overall sys-
tem. The main idea is to construct text-based and
image-based co-occurrence models separately and
then combine them. We first describe our proce-
dure to build both text-based and image-based mod-
els. However, we stress the latter since it is the
more novel part of the procedure. Then, we describe
our simple combination technique to integrate both
models and create a multimodal distributional se-
mantic space. Our implementation of the proposed
method is open-source1.
1https://github.com/s2m
24
Image Data
Visual feature extraction
Bag of visual words
Image-based distributional vectorText-based distributional vector
Text feature extraction
Normalize and concatenate
Multimodal distributional semantic vector
Tag modeling
Text corpus
Figure 2: Overview of our system architecture
3.1 Text-based distributional model
Instead of proposing yet another model, we pick one
that is publicly available off-the-shelf and has been
shown to be at the state of the art on a number of
benchmarks. The picked model (DM)2 is encoded
in a matrix in which each target word is represented
by a row vector of weights representing its associa-
tion with collocates in a corpus. See Section 4.1 for
details about the text-based model.
3.2 Image-based distributional model
We assume image data where each image is associ-
ated with word labels (somehow related to the im-
age) that we call tags.
The primary approach to form the image-based
vector space is to use the BoVW method to rep-
resent images. Having represented each image in
our data set in terms of the frequency of occurrence
of each visual word in it, we construct the image-
based distributional vector of each tag as follows.
Each tag (textual word) is associated to the list of
images which are tagged with it; we then sum visual
word occurrences across that list of images to ob-
tain the co-occurrence counts associated with each
tag. For uniformity with the treatment of textual
co-occurrences (see Section 4.1), the raw counts are
transformed into Local Mutual Information scores
computed between each tag and visual word. Lo-
cal Mutual Information is an association measure
that closely approximates the commonly used Log-
Likelihood Ratio while being simpler to compute
(Evert, 2005).
In this way, we obtain an image-based distribu-
2http://clic.cimec.unitn.it/dm
tional semantic model, that is, a matrix where each
row corresponds to a tag vector, summarizing the
distributional history of the tag in the image collec-
tion in terms of its association with the visual words.
3.3 Integrating distributional models
We assemble the two distributional vectors to con-
struct the multimodal semantic space. Given a word
that is present both in the text-based model and
(as a tag) in the image-based model, we separately
normalize the two vectors representing the word to
length 1 (so that the text and image components will
have equal weight), and we concatenate them to ob-
tain the multimodal distributional semantic vector
representing the word. The matrix of concatenated
text- and image-based vectors is our multimodal dis-
tributional semantic model. We leave it to future
work to consider more sophisticated combination
techniques (preliminary experiments on differential
weighting of the text and image components did not
lead to promising results).
4 Experimental setup
4.1 The DM text-based model
DM has been shown to be near or at the state of
the art in a great variety of semantic tasks, ranging
from modeling similarity judgments to concept cat-
egorization, predicting selectional preferences, rela-
tion classification and more.
The DM model is described in detail by Baroni
and Lenci (2010), where it is referred to as TypeDM.
In brief, the model is trained on a large corpus
of about 2.8 billion tokens that include Web docu-
ments, the Wikipedia and the BNC. DM is a struc-
tured model, where the collocates are labeled with
the link that connect them to the target words. The
links are determined by a mixture of dependency
parse information and lexico-syntactic patterns, re-
sulting in distributional features (the dimensions of
the semantic space) such as subject kill, with gun or
as sharp as. The score of a target word with a fea-
ture is not based on the absolute number of times
they co-occur in the corpus, but on the variety of
different surface realizations of the feature the word
co-occurs with. For example, for the word fat and
the feature of animal, the raw score is 9 because fat
co-occurs with 9 different forms of the feature (a
25
fat of the animal, the fat of the animal, fats of an-
imal. . . ). Refer to Baroni and Lenci (2010) for how
the surface realizations of a feature are determined.
Raw scores are then transformed into Local Mutual
Information values.
The DM semantic space is a matrix with 30K
rows (target words) represented in a space of more
than 700M dimensions. Since our visual dimension
extraction algorithms are maximally producing 32K
dimensions (see Section 4.2 below), we make the
impact of text features on the combined model di-
rectly comparable to the one of visual features by
selecting only the top n DM dimensions (with n
varying as explained below). The top dimensions
are picked based on their cumulative Local Mutual
Information mass. We show in the experiments be-
low that trimming DM in this way does not have a
negative impact on its performance, so that we are
justified in claiming that we are adding visual in-
formation to a state-of-the-art text-based semantic
space.
4.2 Visual Information Extraction
For our experiments, we use the ESP-Game data
set.3 It contains 50K images, labeled through the
famous ?game with a purpose? developed by Louis
von Ahn (von Ahn and Dabbish, 2004). The tags
of images in the data set form a vocabulary of 11K
distinct word types. Image labels contain 6.686 tags
on average (2.357 s.d.). The ESP-Game corpus is
an interesting data set from our point of view since,
on the one hand, it is rather large and we know that
the tags it contains are related to the images. On
the other hand, it is not the product of experts la-
belling representative images, but of a noisy anno-
tation process of often poor-quality or uninteresting
images (e.g., logos) randomly downloaded from the
Web. Thus, analogously to the characteristics of a
textual corpus, our algorithms must be able to ex-
ploit large-scale statistical information, while being
robust to noise.
Following what has become an increasingly stan-
dard procedure in computer vision, we use the Dif-
ference of Gaussian (DoG) detector to automatically
detect keypoints from images and consequently map
them to visual words (Lowe, 1999; Lowe, 2004). We
3http://www.espgame.org
use the Scale-Invariant Feature Transform (SIFT) to
depict the keypoints in terms of a 128-dimensional
real-valued descriptor vector. Color version SIFT
descriptors are extracted on a regular grid with five
pixels spacing, at four multiple scales (10, 15, 20,
25 pixel radii), zeroing the low contrast ones. We
chose SIFT for its invariance to image scale, ori-
entation, noise, distortion and partial invariance to
illumination changes. To map the descriptors to vi-
sual words, we cluster the keypoints in their 128-
dimensional space using the K-means clustering al-
gorithm, and encode each keypoint by the index of
the cluster (visual word) to which it belongs. We
varied the number of visual words between 250 and
2000 in steps of 250. We then computed a one-level
4x4 pyramid of spatial histograms (Grauman and
Darrell, 2005), consequently increasing the features
dimensions 16 times, for a number that varies be-
tween 4K and 32K, in steps of 4K. From the point of
view of our distributional semantic model construc-
tion, the important point to keep in mind is that stan-
dard parameter choices such as the ones we adopted
lead to distributional vectors with 4K, 8K, . . . , 32K
dimensions, where a higher number of features cor-
responds, roughly, to a more granular analysis of an
image. We used the VLFeat implementation for the
entire pipeline (Vedaldi and Fulkerson, 2008). See
the references in Section 2.2 above for technical de-
tails.
4.3 Model integration
We remarked above that the visual word extraction
procedure naturally leads to 8 kinds of image-based
vectors of dimensionalities from 4K to 32K in steps
of 4K. To balance text and image information, we
use DM vectors made of top n features ranging from
4K to 32K in the same 4K steps. By combining,
we obtain 64 combined models (4K text and 4K im-
age dimensions, 4K text and 8K image dimensions,
etc.). Since in the experiments on WordSim (Section
5.1 below) we observe best performance with 32K
text-based features, we report here only experiments
with (at least) 32K dimensions. Similar patterns to
the ones we report are observed when adding image-
based dimensions to text-based vectors of different
dimensionalities.
For a thoroughly fair comparison, if we add n vi-
sual features to the text-based model and we notice
26
an improvement, we must ask whether the same im-
provement could also be obtained by adding more
text-based features. To control for this possibility,
we also consider a set of purely text-based mod-
els that have the same number of dimensions of the
combined models, that is, the top 32K DM features
plus 8K, . . . , 32K further DM features (the next top
features in the cumulative Local Mutual Information
score ranking). In the experiments below, we refer to
the purely textual model as text (always 32K dimen-
sions), to the purely image-based model as image, to
the combined models as combined, and to the con-
trol in which further text dimensions are added for
comparability with combined as text+.
4.4 Evaluation benchmarks
We conduct our most extensive evaluation on the
WordSim353 data set (Finkelstein et al, 2002),
a widely used benchmark constructed by asking
16 subjects to rate a set of word pairs on a 10-
point similarity scale and averaging the ratings (dol-
lar/buck receive a high 9.22 average rating, profes-
sor/cucumber a low 0.31). We cover 260 Word-
Sim (mostly noun/noun) pairs. We evaluate models
in terms of the Spearman correlation of the cosines
they produce for the WordSim pairs with the average
human ratings for the same pairs (here and below,
we do not report comparisons with the state of the
art in the literature, because we have reduced cov-
erage of the data sets, making the comparison not
meaningful).
To verify if the conclusions reached on WordSim
extend to different semantic tasks, we use two con-
cept categorization benchmarks, where the goal is
to cluster a set of (nominal) concepts into broader
categories. The Almuhareb-Poesio (AP) concept set
(Almuhareb, 2006), in the version we cover, con-
tains 230 concepts to be clustered into 21 classes
such as vehicle (airplane, car. . . ), time (aeon, fu-
ture. . . ) or social unit (brigade, nation). The Battig
set (Baroni et al, 2010), in the version we cover,
contains 72 concepts to be clustered into 10 classes.
Unlike AP, Battig only contains concrete basic-level
concepts belonging to categories such as bird (ea-
gle, owl. . . ), kitchenware (bowl, spoon. . . ) or veg-
etable (broccoli, potato. . . ). For both sets, follow-
ing the original proponents and others, we clus-
ter the words based on their pairwise cosines in
the semantic space defined by a model using the
CLUTO toolkit (Karypis, 2003). We use CLUTO?s
built-in repeated bisections with global optimiza-
tion method, accepting all of CLUTO?s default val-
ues. Cluster quality is evaluated by percentage pu-
rity (Zhao and Karypis, 2003). If nir is the num-
ber of items from the i-th true (gold standard) class
that were assigned to the r-th cluster, n is the total
number of items and k the number of clusters, then:
Purity = 1n
?k
r=1 maxi
(nir). In the best case (per-
fect clusters), purity is 100% and as cluster quality
deteriorates, purity approaches 0.
Finally, we use the Baroni-Lenci Evaluation of
Semantic Similarity (BLESS) data set made avail-
able by the GEMS 2011 organizers.4 In the ver-
sion we cover, the data set contains 174 concrete
nominal concepts, each paired with a set of words
that instantiate the following 6 relations: hyper-
nymy (spear/weapon), coordination (tiger/coyote),
meronymy (castle/hall), typical attribute (an ad-
jective: grapefruit/tart) and typical event (a verb:
cat/hiss). Concepts are moreover matched with 3
sets of randomly picked unrelated words (nouns, ad-
jectives and verbs). For each true and random rela-
tion, the data set contains at least one word per con-
cept, typically more. Following the GEMS guide-
lines, we apply a model to BLESS as follows. Given
the similarity scores provided by the model for a
concept with all associated words within a relation,
we pick the term with the highest score. We then z-
standardize the 8 scores we obtain for each concept
(one per relation), and we produce a boxplot summa-
rizing the distribution of z scores per relation across
the concepts (i.e., each box of the plot summarizes
the distribution of the 174 scores picked for each re-
lation, standardized as we just described). Boxplots
are produced accepting the default boxplotting op-
tion of the R statistical package5 (boxes extend from
first to third quartile, median is horizontal line inside
the box).
4http://sites.google.com/site/geometricalmodels/shared-
evaluation
5http://www.r-project.org/
27
5 Results
5.1 WordSim
The WordSim results for our models across dimen-
sionalities as well as for the full DM are summarized
in Figure 3.
+4K +8K +12K +16K +20K +24K +28K +32K25
30
35
40
45
50
55
Adding more features to top 32K DM
Sp
ear
ma
n c
oef
fici
ent
(%
)
Performance of distributional models on WordSim
 
 
DM
combined
text
image
text+
Figure 3: Performance of distributional models on Word-
Sim
The purely image-based model is having the
worst performance in all settings, although even the
lowest image-based Spearman score (0.29) is signif-
icantly above chance (p. < 0.05), suggesting that
the model does capture some semantic information.
Contrarily, adding image-based dimensions to a tex-
tual model (combined) consistently reaches the best
performance, also better ? for all choices of dimen-
sionality ? than adding an equal number of text fea-
tures (text+) or using the full DM matrix. Inter-
estingly, the same overall result pattern is observed
if we limit evaluation to the WordSim subsets that
Agirre et al (2009) have identified as semantically
similar (e.g., synonyms or coordinate terms) and se-
mantically related (e.g., meronyms or topically re-
lated concepts).
Based on the results reported in Figure 3, fur-
ther analyses will focus on the combined model with
+20K image-based features, since performance of
combined does not seem to be greatly affected by the
dimensionality parameter, and performance around
this value looks quite stable (it is better only at the
boundary +4K value, and with +28K, where, how-
ever, there is a dip for the image model). The text+
performance is not essentially affected by the di-
mensionality parameter, and we pick the +20K ver-
sion for maximum comparability with combined.
The difference between combined and text+, al-
though consistent, is not statistically significant
according to a two-tailed paired permutation test
(Moore and McCabe, 2005) conducted on the re-
sults for the +20K versions of the models. Still, very
interesting qualitative differences emerge. Table 1
reports those WordSim pairs (among the ones with
above-median human-judged similarity) that have
the highest and lowest combined-to-text+ cosine ra-
tios, i.e., pairs that are correctly treated as similar by
combined but not by text+, and vice versa. Strik-
ingly, the pairs characterizing the image-feature-
enriched combined are all made of concrete, highly
imageable concepts, whereas the text+ pairs refer to
very abstract notions. We thus see here the first ev-
idence of the complementary nature of visual and
textual information.
combined text+
tennis/racket physics/proton
planet/sun championship/tournament
closet/clothes profit/loss
king/rook registration/arrangement
cell/phone mile/kilometer
Table 1: WordSim pairs with highest (first column) and
lowest (second column) combined-to-text+ cosine ratios
5.2 Concept categorization
Table 2 reports percentage purities in the AP and
Battig clustering tasks for full DM and the represen-
tative models discussed above.
model AP Battig
DM 81 96
text 79 83
text+ 80 86
image 25 36
combined 78 96
Table 2: Percentage AP and Battig purities of distribu-
tional models
Once more, we see that the image model alone
is not at the level of the text models, although both
its AP and Battig purities are significantly above
28
chance (p < 0.05 based on simulated distributions
for random cluster assignment). Thus, even alone,
image-based vectors do capture aspects of meaning.
For AP, adding image features does not improve per-
formance, although it does not significantly worsen
it either (a two-tailed paired permutation test con-
firms that the difference between text+ and com-
bined is far from significance). For Battig, adding
visual features improves on the purely text-based
models based on a comparable number of features
(although the difference between text+ and com-
bined is not significant), reaching the same perfor-
mance obtained with the full DM model (that in
these categorization tests is slightly above that of the
trimmed models). Intriguingly, the Battig test is en-
tirely composed of concrete concepts, so the differ-
ence in performance for combined might be related
to its preference for concrete things we already ob-
served for WordSim.
5.3 BLESS
The BLESS distributions of text-based models (in-
cluding combined) are very similar, so we use here
the full DM model as representative of the text-based
set ? its histogram is compared to the one of the
purely image-based model in Figure 4.
We see that purely text-based DM cosines capture
a reasonable scale of taxonomic similarity among
nominal neighbours (coordinates then hypernyms
then meronyms then random nouns), whereas verbs
and adjectives are uniformly very distant, whether
they are related or not. This is not surprising be-
cause the DM links mostly reflect syntactic patterns,
that will be disjoint across parts of speech (e.g., a
feature like subject kill will only apply to nouns,
save for parsing errors). Looking at the image-
only model, we first observe that it can capture dif-
ferences between related attributes/events and ran-
dom adjectives/verbs (according to a Tukey HSD
test for all pairwise comparisons, these differences
are highly significant, whereas DM only signifi-
cantly distinguishes attributes from random verbs).
In this respect, image is arguably the ?best? model
on BLESS. However, perhaps more interestingly,
the image model also shows a bias for nouns, cap-
turing the same taxonomic hierarchy found for DM.
This suggests that image analysis is providing a de-
composition of concepts into attributes shared by
similar entities, that capture ontological similarity
beyond mere syntagmatic co-occurrence in an im-
age description.
To support this latter claim, we counted the av-
erage number of times that the related terms picked
by the image model directly co-occur with the target
concepts in an ESP-Game label. It turns out that this
count is higher for both attributes (10.6) and hyper-
nyms (7.5) than for coordinates (6.5). So, the higher
similarity of coordinates in the image model demon-
strates that its features do generalize across images,
allowing us to capture ?attributional? or ?paradig-
matic? similarity in visual space. More in general,
we find that, among all the related terms picked by
the image model that have an above-average cosine
with the target concept, almost half (41%) never co-
occur with the concept in the image set, again sup-
porting the claim that, by our featural analysis, we
are capturing visual properties of similar concepts
beyond their co-occurrence as descriptions of the
same image.
A final interesting point pertains to the specific in-
stances of each (non-random) relation picked by the
textual and visual models: of 870 related term pairs
in total, almost half (418) differ between DM and
image, suggesting that the boxplots in Figure 4 hide
larger differences in what the models are doing. The
randomly picked examples of mismatches in top at-
tributes from Table 3 clearly illustrate the qualitative
difference between the models, and, once more, the
tendency of image-based representations to favour
(not surprisingly!) highly visual properties such as
colours and shapes, vs. the well-known tendency of
text-based models to extract systemic or functional
characteristics such as powerful or elegant (Baroni
et al, 2010). By combining the two sources of infor-
mation, we should be able to develop distributional
models that come with more well-rounded charac-
terizations of the concepts they describe.
6 Conclusion
We proposed a simple method to augment a state-
of-the-art text-based distributional semantic model
with information extracted from image analysis.
The method is based on the standard bag-of-visual-
words representation of images in computer vision.
The image-based distributional profile of a word is
29
ll
l
llll
l
l
l
ll
l
l
l
l
l
ll
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?
1.0
?
0.5
0.0
0.5
1.0
1.5
2.0
DM
ll
ll
l
ll
ll
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
ll
l
l
l
l
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?
1
0
1
2
Image
Figure 4: Distribution of z-normalized cosines of words instantiating various relations across BLESS concepts.
concept DM image concept DM image
ant small black potato edible red
axe powerful old rifle short black
cathedral ancient dark scooter cheap white
cottage little old shirt fancy black
dresser new square sparrow wild brown
fighter fast old squirrel fluffy brown
fork dangerous shiny sweater elegant old
goose white old truck new heavy
jet fast old villa new cosy
pistol dangerous black whale large gray
Table 3: Randomly selected cases where nearest at-
tributes picked by DM and image differ.
encoded in a vector of co-occurrences with ?visual
words?, that we concatenate with a text-based co-
occurrence vector. A cautious interpretation of our
results is that adding image-based features is at least
not damaging, when compared to adding further
text-based features, and possibly beneficial. Impor-
tantly, in all experiments we find that image-based
features lead to interesting qualitative differences in
performance: Models including image-based infor-
mation are more oriented towards capturing similar-
ities between concrete concepts, and focus on their
more imageable properties, whereas the text-based
features are more geared towards abstract concepts
and properties. Coming back to the discussion of
symbol grounding at the beginning of the paper, we
consider this (very!) preliminary evidence for an in-
tegrated view of semantics where the more concrete
aspects of meaning derive from perceptual experi-
ence, whereas verbal associations mostly account
for abstraction.
In future work, we plan first of all to improve per-
formance, by focusing on visual word extraction and
on how the text- and image-based vectors are com-
bined (possibly using supervision to optimize both
feature extraction and integration with respect to se-
mantic tasks). However, the most exciting direction
we intend to follow next will concern evaluation,
and in particular devising new benchmarks that ad-
dress the special properties of image-enhanced mod-
els directly. For example, Baroni and Lenci (2008)
observe that text-based distributional models are se-
riously lacking when it comes to characterize phys-
ical properties of concepts such as their colors or
parts. These are exactly the aspects of conceptual
knowledge where image-based information should
help most, and we will devise new test sets that will
focus specifically on verifying this hypothesis.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, Boulder, CO.
30
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Lawrence Barsalou, Ava Santos, Kyle Simmons, and
Christine Wilson, 2008. Language and Simulation in
Conceptual Processing, chapter 13, pages 245?283.
Oxford University Press, USA, 1 edition.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image Classification using Random Forests and
Ferns. In Computer Vision, 2007. ICCV 2007. IEEE
11th International Conference on, pages 1?8.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91?99, Los Angeles,
California. Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Arthur Glenberg. 1997. What memory is for. Behav
Brain Sci, 20(1), March.
Kristen Grauman and Trevor Darrell. 2005. The pyramid
match kernel: Discriminative classification with sets
of image features. In In ICCV, pages 1458?1465.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psychologi-
cal Review, 114:211?244.
Jonathon Hare, Sina Samangooei, Paul Lewis, and Mark
Nixon. 2008. Semantic spaces revisited: investigat-
ing the performance of auto-annotation and semantic
retrieval using semantic spaces. In Proceedings of the
2008 international conference on Content-based im-
age and video retrieval, CIVR ?08, pages 359?368,
New York, NY, USA. ACM.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?346,
June.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42(1-2):177?196, January.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota De-
partment of Computer Science.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David Lowe. 1999. Object Recognition from Local
Scale-Invariant Features. Computer Vision, IEEE In-
ternational Conference on, 2:1150?1157 vol.2, Au-
gust.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
George Miller and Walter Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
David Moore and George McCabe. 2005. Introduction
to the Practice of Statistics. Freeman, New York, 5
edition.
David Nister and Henrik Stewenius. 2006. Scalable
recognition with a vocabulary tree. In Proceedings
of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 2,
CVPR ?06, pages 2161?2168.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of the International Conference on
Computer Vision, volume 2, pages 1470?1477, Octo-
ber.
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer-Verlag New York Inc.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Andrea Vedaldi and Brian Fulkerson. 2008. VLFeat:
An open and portable library of computer vision algo-
rithms. http://www.vlfeat.org/.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
31
SIGCHI conference on Human factors in computing
systems, CHI ?04, pages 319?326, New York, NY,
USA. ACM.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwell, Oxford. Translated by G.E.M.
Anscombe.
Jun Yang, Yu-Gang Jiang, Alexander G. Hauptmann,
and Chong-Wah Ngo. 2007. Evaluating bag-of-
visual-words representations in scene classification.
In James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197?206. ACM.
Ying Zhao and George Karypis. 2003. Criterion func-
tions for document clustering: Experiments and analy-
sis. Technical Report 01-40, University of Minnesota
Department of Computer Science.
Rolf Zwaan. 2004. The immersed experiencer: Toward
an embodied theory of language comprehension. Psy-
chology of Learning and Motivation: Advances in Re-
search and Theory, Vol 44, 44.
32
